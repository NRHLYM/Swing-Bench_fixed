Metadata-Version: 2.4
Name: Swingbench
Version: 0.0.0
Summary: The official Swingbench package - a benchmark for evaluating LMs on software engineering
Author: Anonymous
Author-email: anonymous@anonymous.com
Keywords: nlp,benchmark,code
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: beautifulsoup4
Requires-Dist: chardet
Requires-Dist: datasets
Requires-Dist: docker
Requires-Dist: ghapi
Requires-Dist: GitPython
Requires-Dist: modal
Requires-Dist: pre-commit
Requires-Dist: python-dotenv
Requires-Dist: requests
Requires-Dist: rich
Requires-Dist: tenacity
Requires-Dist: tqdm
Requires-Dist: unidiff
Requires-Dist: openai
Requires-Dist: tiktoken
Requires-Dist: rouge
Requires-Dist: torch
Requires-Dist: transformers
Requires-Dist: accelerate
Requires-Dist: evaluate
Requires-Dist: xopen
Requires-Dist: python-dotenv
Requires-Dist: tree-sitter==0.24.0
Requires-Dist: tree-sitter-javascript==0.23.1
Requires-Dist: tree-sitter-python==0.23.6
Requires-Dist: tree-sitter-rust==0.21.1
Requires-Dist: tree-sitter-typescript==0.23.2
Provides-Extra: inference
Requires-Dist: anthropic; extra == "inference"
Requires-Dist: flash_attn; extra == "inference"
Requires-Dist: jedi; extra == "inference"
Requires-Dist: openai; extra == "inference"
Requires-Dist: peft; extra == "inference"
Requires-Dist: protobuf; extra == "inference"
Requires-Dist: sentencepiece; extra == "inference"
Requires-Dist: tiktoken; extra == "inference"
Requires-Dist: torch; extra == "inference"
Requires-Dist: transformers; extra == "inference"
Requires-Dist: triton; extra == "inference"
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Provides-Extra: ci-tools
Requires-Dist: pre-commit>=3.0.0; extra == "ci-tools"
Requires-Dist: docker>=6.0.0; extra == "ci-tools"
Requires-Dist: pyyaml>=6.0; extra == "ci-tools"
Requires-Dist: jsonschema>=4.0.0; extra == "ci-tools"
Provides-Extra: all
Requires-Dist: anthropic; extra == "all"
Requires-Dist: flash_attn; extra == "all"
Requires-Dist: jedi; extra == "all"
Requires-Dist: openai; extra == "all"
Requires-Dist: peft; extra == "all"
Requires-Dist: protobuf; extra == "all"
Requires-Dist: sentencepiece; extra == "all"
Requires-Dist: tiktoken; extra == "all"
Requires-Dist: torch; extra == "all"
Requires-Dist: transformers; extra == "all"
Requires-Dist: triton; extra == "all"
Requires-Dist: pytest; extra == "all"
Requires-Dist: pytest-cov; extra == "all"
Requires-Dist: pre-commit>=3.0.0; extra == "all"
Requires-Dist: docker>=6.0.0; extra == "all"
Requires-Dist: pyyaml>=6.0; extra == "all"
Requires-Dist: jsonschema>=4.0.0; extra == "all"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: keywords
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

<p align="center">
  <a href="https://swebench.com">
    <img src="figures_swing/github_logo_pot.png" style="height: 10em" alt="Kawi the SWE-Llama" />
  </a>
</p>

<div align="center">

 [English](https://github.com/menik1126/Swing-Bench/) 

</div>



<p align="center">
Code and data for our paper <a href="https://arxiv.org/abs/2505.23932">SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving</a>
    </br>
    </br>
    <a href="https://www.python.org/">
        <img alt="Build" src="https://img.shields.io/badge/Python-3.8+-1f425f.svg?color=purple">
    </a>
    <a href="https://copyright.princeton.edu/policy">
        <img alt="License" src="https://img.shields.io/badge/License-MIT-blue">
    </a>
    <a href="https://badge.fury.io/py/swebench">
        <img src="https://badge.fury.io/py/swebench.svg">
    </a>
</p>

Please refer our [website](https://swing-bench.github.io/) for the public leaderboard.

## üì∞ News

* **[June. 5, 2024]**: We have released [SwingArena](https://arxiv.org/abs/2505.23932)! 

## üëã Overview
SwingArena is a realistic, *CI-driven* evaluation framework for LLMs that simulates real-world software development by pairing models as patch *submitters* and *reviewers*, enhanced with *retrieval-augmented code generation* for multi-language support and long-context handling.

<img src="figures_swing/main_pot.png">

To access SwingArena, copy and run the following code:
```from datasets import load_dataset
languages = ['rust', 'cpp', 'python', 'go', 'java', 'javascript', 'php']
for lang in languages:
    swingbench[lang] = load_dataset('SwingBench/SwingBench-data', split=lang)
```

## üõ†Ô∏è Technical Architecture & Environment Setup

SwingArena employs an advanced containerized evaluation architecture that ensures cross-platform reproducibility and consistency. The system core relies on **Docker** for isolated environment management, combined with **CI tools** (such as GitHub Actions simulated through `act`) to achieve real-world software development workflow evaluation.

### üîß System Requirements
Before getting started, please ensure your system meets the following requirements:
- **Docker**: Follow the [Docker official installation guide](https://docs.docker.com/engine/install/) to install Docker Engine. Linux users are recommended to refer to the [post-installation steps](https://docs.docker.com/engine/install/linux-postinstall/) for optimal experience.
- **Hardware Configuration**: Recommended `x86_64` architecture machine with at least 120GB available storage, 16GB RAM, and 8 CPU cores (`arm64` support is still experimental)
- **Python Environment**: Python 3.8+ and related dependency packages

### üèóÔ∏è Core Technology Stack
SwingArena integrates multiple cutting-edge technologies:

**AI Model Integration**: Supports various large language model APIs (OpenAI, Anthropic, Claude, etc.) and local model serving through a flexible model proxy system for seamless switching.

**Retrieval-Augmented Generation**: Built-in BM25 retriever provides precise relevant information retrieval for long-context code generation, supporting multi-language codebase indexing (Python, Rust, C++, Go, JavaScript, TypeScript, PHP, etc.).

**Distributed Evaluation**: Adopts multi-process parallel evaluation architecture with Modal cloud execution support, dynamically adjusting worker processes based on system resources (recommended not to exceed `min(0.75 * os.cpu_count(), 24)`).

**Arena Mechanism**: Pioneering dual-agent battle evaluation mode where one agent acts as a patch submitter and another as a code reviewer, simulating real collaborative development scenarios.

### üìä Data Processing Pipeline
The system includes a complete data collection, annotation, and evaluation pipeline:
- **Data Crawling**: Automated GitHub repository issue collection and PR analysis
- **Multi-round Annotation**: Quality control mechanism supporting collaborative annotation by multiple annotators
- **CI-driven Validation**: Validates patches and test cases effectiveness through real CI environments
- **Statistical Analysis**: Provides detailed performance metrics and failure mode analysis

Through this technical architecture, SwingArena provides the industry's closest-to-real development environment benchmark platform for evaluating large language models in software engineering domains.

## üî® CI Tools Setup

SwingArena requires CI tools for realistic software development workflow simulation. The primary tools are **Docker** (for containerized environments) and **`act`** (for GitHub Actions simulation).

> **üí° Quick Setup**: If you followed the [Quick Start](#-quick-start) with `pip install -e ".[ci-tools]"`, most CI tools are already installed automatically.

### Prerequisites
- **Git** (required for repository operations)
- **Docker** (required for act to run GitHub Actions and containerized environments)
- **sudo/admin privileges** (for system-level tool installation)

### üöÄ Installation Options

**Option 1: Automatic installation during pip install (Recommended)**
```bash
# This is the same command from Quick Start
pip install -e ".[ci-tools]"
```

**Option 2: Using the dedicated installer script**
```bash
python install_ci_tools.py
```



Expected output after successful installation:
```
üîç Checking CI tools installation status...

act (GitHub Actions): ‚úÖ Installed
Docker: ‚úÖ Installed  
Git: ‚úÖ Installed
Python docker: ‚úÖ Installed
Python yaml: ‚úÖ Installed

üìä Overall status: ‚úÖ All tools ready
```

### üê≥ Docker Installation Details

Our installer supports automatic Docker installation on major Linux distributions:

**Supported Linux Distributions:**
- **Ubuntu/Debian**: Uses official Docker APT repository
- **CentOS/RHEL**: Uses official Docker YUM repository  
- **Arch Linux**: Uses pacman package manager
- **Other Linux**: Falls back to Docker's convenience script

**macOS**: 
- Automatic installation via Homebrew (`brew install --cask docker`)
- Manual download required if Homebrew unavailable

**Windows**:
- Automatic installation via Chocolatey or winget
- Manual download required if neither available

### Manual Installation
If automatic installation fails, install manually:

**Installing Docker:**

- **Linux (Ubuntu/Debian)**:
  ```bash
  curl -fsSL https://get.docker.com -o get-docker.sh
  sudo sh get-docker.sh
  sudo usermod -aG docker $USER
  ```

- **Linux (CentOS/RHEL)**:
  ```bash
  sudo yum install -y yum-utils
  sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  sudo yum install -y docker-ce docker-ce-cli containerd.io
  sudo systemctl start docker && sudo systemctl enable docker
  sudo usermod -aG docker $USER
  ```

- **macOS**: [Download Docker Desktop](https://docs.docker.com/desktop/mac/install/)
- **Windows**: [Download Docker Desktop](https://docs.docker.com/desktop/windows/install/)

**Installing act (GitHub Actions Local Runner):**

- **Linux**: `curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash`
- **macOS**: `brew install act` (or use curl method above)
- **Windows**: `choco install act-cli` or `winget install nektos.act`

For detailed troubleshooting and advanced configuration, see [CI_TOOLS_SETUP.md](CI_TOOLS_SETUP.md).

## üöÄ Quick Start

To build SwingArena from source, follow these steps:

### üîß Basic Installation
```bash
git clone https://github.com/menik1126/Swing-Bench.git
cd Swing-Bench
pip install -e .
```

### üõ†Ô∏è Installation with CI Tools (Recommended)
For full SwingArena functionality including agent battles and CI simulation:
```bash
pip install -e ".[ci-tools]"
```

This enhanced installation will automatically:
- ‚úÖ Install all Python dependencies 
- üê≥ **Install Docker** (on supported Linux distributions)
- üîß **Install `act`** (GitHub Actions local runner)
- üì¶ Install Docker SDK for Python
- üîó Set up pre-commit hooks

> **üí° Note**: The basic `pip install -e .` only installs Python dependencies. For CI-driven evaluation and agent battles, use the `[ci-tools]` extra or run the dedicated installer script.

### ‚úÖ Installation Verification

Test your installation by running:
```bash
python -m swingarena.harness.run_evaluation \
    --predictions_path gold \
    --max_workers 1 \
    --instance_ids sympy__sympy-20590 \
    --run_id validate-gold
```

For CI tools verification:
```bash
python install_ci_tools.py --check
```

### üå©Ô∏è Evaluation with Modal
You can also run evaluations entirely on the cloud using [Modal](https://modal.com/) to avoid local setup and resource constraints:
```bash
python -m swingarena.harness.run_evaluation \
    --predictions_path gold \
    --run_id validate-gold-modal \
    --instance_ids sympy__sympy-20590 \
    --modal true
```
This will execute the evaluation harness on Modal's cloud infrastructure, eliminating the need for local Docker setup and resource management.

> [!NOTE]
> Modal for SwingArena Multimodal is currently experimental and may not be fully supported yet.

### ü•ä Agent Battle Mode

SwingArena's unique agent battle evaluation mode can be used as follows:

> **‚ö†Ô∏è Prerequisites**: Ensure CI tools are installed with `pip install -e ".[ci-tools]"` or verify with `python install_ci_tools.py --check`

```bash
export CI_TOOL_NAME=act
python swingarena/harness/agent_battle.py \
    --ci_tool_name act \
    --dataset_name SwingBench/SwingBench \
    --split Rust \
    --model_lhs "gpt-4" \
    --model_rhs "claude-3" \
    --api_key_lhs "your-api-key-1" \
    --api_key_rhs "your-api-key-2"
```

Or use the provided script:
```bash
# Set environment variables
export SWING_TESTBED_PATH="/path/to/workdir"
export SWING_REPOS_DIR_PATH="/path/to/repos" 
export SWING_INDEXES_PATH="/path/to/indexes"

./start_battle.sh
```

## üíΩ Usage
> [!WARNING]
> Running fast evaluations on SWE-bench can be resource intensive
> We recommend running the evaluation harness on an `x86_64` machine with at least 120GB of free storage, 16GB of RAM, and 8 CPU cores.
> You may need to experiment with the `--max_workers` argument to find the optimal number of workers for your machine, but we recommend using fewer than `min(0.75 * os.cpu_count(), 24)`.
>
> If running with docker desktop, make sure to increase your virtual disk space to have ~120 free GB available, and set max_workers to be consistent with the above for the CPUs available to docker.
>
> Support for `arm64` machines is experimental.

Evaluate model predictions on SwingArena using the evaluation harness with the following command:
```bash
python -m swingarena.harness.run_evaluation \
    --dataset_name SwingBench/SwingBench \
    --predictions_path <path_to_predictions> \
    --max_workers <num_workers> \
    --run_id <run_id>
    # use --predictions_path 'gold' to verify the gold patches
    # use --run_id to name the evaluation run
```

This command will generate docker build logs (`logs/build_images`) and evaluation logs (`logs/run_evaluation`) in the current directory.

The final evaluation results will be stored in the `evaluation_results` directory.

To see the full list of arguments for the evaluation harness, run:
```bash
python -m swingarena.harness.run_evaluation --help
```

Additionally, the SwingArena repo can help you:
* Train your own models on our pre-processed datasets
* Run [inference](https://github.com/menik1126/Swing-Bench/blob/main/swingarena/inference/README.md) on existing models (either models you have on-disk like LLaMA, or models you have access to through an API like GPT-4). The inference step is where you get a repo and an issue and have the model try to generate a fix for it.
*  Run SwingArena's [data collection procedure](https://github.com/menik1126/Swing-Bench/blob/main/swingarena/collect/) on your own repositories, to make new SwingArena tasks.

## ‚¨áÔ∏è Downloads
| Datasets | Models |
| - | - |
| [ü§ó SWE-bench](https://huggingface.co/datasets/princeton-nlp/SWE-bench) | [ü¶ô SWE-Llama 13b](https://huggingface.co/princeton-nlp/SWE-Llama-13b) |
| [ü§ó "Oracle" Retrieval](https://huggingface.co/datasets/princeton-nlp/SWE-bench_oracle) | [ü¶ô SWE-Llama 13b (PEFT)](https://huggingface.co/princeton-nlp/SWE-Llama-13b-peft) |
| [ü§ó BM25 Retrieval 13K](https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_13K) | [ü¶ô SWE-Llama 7b](https://huggingface.co/princeton-nlp/SWE-Llama-7b) |
| [ü§ó BM25 Retrieval 27K](https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_27K) | [ü¶ô SWE-Llama 7b (PEFT)](https://huggingface.co/princeton-nlp/SWE-Llama-7b-peft) |
| [ü§ó BM25 Retrieval 40K](https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_40K) | |
| [ü§ó BM25 Retrieval 50K (Llama tokens)](https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_50k_llama)   | |

## üçé Tutorials
We've also written the following blog posts on how to use different parts of SWE-bench.
If you'd like to see a post about a particular topic, please let us know via an issue.
* [Nov 1. 2023] Collecting Evaluation Tasks for SWE-Bench ([üîó](https://github.com/princeton-nlp/SWE-bench/blob/main/assets/collection.md))
* [Nov 6. 2023] Evaluating on SWE-bench ([üîó](https://github.com/princeton-nlp/SWE-bench/blob/main/assets/evaluation.md))

## üö® Troubleshooting

### Common CI Tool Issues

**1. "act: command not found"**
- Ensure `/usr/local/bin` is in your PATH
- Reinstall: `python install_ci_tools.py --force`

**2. "Docker daemon not running"**
- Start Docker service: `sudo systemctl start docker` (Linux)
- Start Docker Desktop (macOS/Windows)

**3. Permission denied errors**
- Add user to docker group: `sudo usermod -aG docker $USER`
- Log out and back in

For detailed troubleshooting, see [CI_TOOLS_SETUP.md](CI_TOOLS_SETUP.md).

## üí´ Contributions
We would love to hear from the broader NLP, Machine Learning, and Software Engineering research communities, and we welcome any contributions, pull requests, or issues!
To do so, please either file a new pull request or issue and fill in the corresponding templates accordingly. We'll be sure to follow up shortly!

Contact person: [Carlos E. Jimenez](http://www.carlosejimenez.com/) and [John Yang](https://john-b-yang.github.io/) (Email: carlosej@princeton.edu, johnby@stanford.edu).

## ‚úçÔ∏è Citation
If you find our work helpful, please use the following citations.
```
@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}
```

## ü™™ License
MIT. Check `LICENSE.md`.
