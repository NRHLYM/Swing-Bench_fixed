{"problem_statement": "Backward compatibility for configs of pools/providers\n\r\n## Rationale\r\n\r\nUMF should provide backward compatible interfaces.\r\n\r\n## Description\r\n\r\n### Background\r\n\r\nMPI team experienced the issue after the PR #692 extends `level_zero_memory_provider_params_t` config structure. The root cause of the issue relates to how MPI instantiate/initialize the L0 provider config. They did the following:\r\n```c\r\nlevel_zero_memory_provider_params_t l0_config = {\r\n    .level_zero_context_handle = hContext;\r\n    .level_zero_device_handle = hDevice;\r\n    .memory_type = UMF_MEMORY_TYPE_DEVICE;\r\n};\r\n\r\numf_memory_provider_handle_t hProvider = NULL;\r\numf_memory_provider_ops_t *l0_ops = umfLevelZeroMemoryProviderOps();\r\numfMemoryProviderCreate(l0_ops, &l0_config, &hProvider);\r\n```\r\n\r\nAfter PR #692 the code above start crashing because two new fields were added to the `level_zero_memory_provider_params_t` data structure and the example above does not initialize these new fields:\r\n```c\r\ntypedef struct level_zero_memory_provider_params_t {\r\n    ze_context_handle_t level_zero_context_handle;\r\n    ze_device_handle_t level_zero_device_handle;\r\n    umf_usm_memory_type_t memory_type;\r\n\r\n    // New fields\r\n    ze_device_handle_t *resident_device_handles;\r\n    uint32_t resident_device_count;\r\n} level_zero_memory_provider_params_t;\r\n```\r\n\r\nA quick fix for the issue was to init the config data structure with `0` and than assign the required fields:\r\n```c\r\nlevel_zero_memory_provider_params_t l0_config = { 0 }; // zero-initialize\r\nl0_config.level_zero_context_handle = hContext;\r\nl0_config.level_zero_device_handle = hDevice;\r\nl0_config.memory_type = UMF_MEMORY_TYPE_DEVICE;\r\n```\r\n\r\n### Open Questions\r\n\r\nThe quick fix above works only because it is OK to init fields of the `level_zero_memory_provider_params_t` structure with zeroes. But there are 2 related major question we should address:\r\n\r\n#### 1. How to initialize the configs with default values?\r\n\r\nIn general case, not every field of the config data structure could/should be initialized with zeroes.\r\n\r\n#### 2. How to support backward compatibility?\r\n\r\nConfig data structures are defined in the headers. If application was built with old version of UMF but on the system there is a newer version than even the application initialize all config's fields properly it initializes only fields that exists in the old version of UMF.\r\nConsider an example. There is a `provider_foo` and corresponding `foo_config_t` structure that contains an int field in the 1st version:\r\n\r\n```c\r\n// provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n} foo_config_t;\r\n\r\n// provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, void **provider) {\r\n    foo_params_t *foo_params = (foo_params_t *)params;\r\n    // expected sizeof(foo_params_t) is equal sizeof(int) that is 4 bytes\r\n\r\n    assert(foo_params->field1 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params = {0};\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, &hProvider);\r\n}\r\n```\r\n\r\nNow in the 2nd version of UMF we extend the `foo_params_t` structure with additional field:\r\n\r\n```c\r\n// provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\n// provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, void **provider) {\r\n    foo_params_t *foo_params = (foo_params_t *)params;\r\n    // expected sizeof(foo_params_t) is equal 2*sizeof(int) that is 8 bytes\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0); // ERROR: if application was compiled with 1st version of the UMF lib then the size of memory pointed by `params` is 4 bytes\r\n}\r\n```\r\n\r\n## Possible API Changes\r\n\r\n### Option 1: handle-based approach\r\nDo not expose the config structure in interfaces. The config object is allocated inside libumf.so and handle to the config is returned to the client code. Setter/getter APIs are used to setup config parameters. For example:\r\n\r\n```c\r\nfoo_params_handle_t foo_params = umfCreateFooParams();\r\numfFooParamsSetField1(foo_params, 7);\r\n\r\numf_memory_provider_handle_t hProvider = NULL;\r\numf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\numfMemoryProviderCreate(foo_ops, foo_params, &hProvider);\r\n```\r\n\r\nClient code does not depend on the layout of the `foo_params_t` structure only UMF knows it and can change in different versions.\r\n\r\n### Option 2: explicitly pass the size of params \r\n\r\nData structures that define configs for the pools/providers remains in headers (the same as today). The new fields can be added only to the end. Application explicitly passes the size of the config data structure. Provider/pool implementation determine the version of the config based on the size. To init the config data structure we need to introduce a special macros or header-based inline functions. For example:\r\n\r\n```c\r\n// Version 1 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n     params->field1 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n     params->field1 = 0;\r\n     params->field2 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, size_t params_size, void **provider) {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(& foo_params);\r\n\r\n    if(sizeof(foo_params_t) == params_size) {\r\n         // current version, just copy input params as is.\r\n        foo_params = *(foo_params_t*)params;\r\n    } else {\r\n        // old version, copy only first `params_size` bytes and keep the rest default initialized\r\n        memcpy(& foo_params, params, params_size);\r\n    }\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(&params);\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, sizeof(foo_params_t), &hProvider);\r\n}\r\n```\r\n\r\n### Option 3: store version in the params data structure\r\n\r\nSimilar to Option 2, but instead of using the size of params data structure to determine the version store it explicitly as a first field in the params data structure.\r\n\r\n```c\r\n// Version 1 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int version;\r\n    int field 1;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n    version = 1;\r\n    params->field1 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n    version = 2;\r\n    params->field1 = 0;\r\n    params->field2 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, size_t params_size, void **provider) {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(& foo_params);\r\n\r\n    int params_version = *(int *)params;\r\n\r\n    switch(params_version) {\r\n        case 1:\r\n            ...\r\n            break;\r\n        case 2:\r\n            ...\r\n            break;\r\n        default:\r\n            LOG_ERR(\"Wrong version\");\r\n    }\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(&params);\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, &hProvider);\r\n}\r\n```\r\n\r\n### Design Considerations\r\n\r\nI prefer Option 1 over Option 2 and Option 3. I think it provides best ABI compatibility. Another advantage is that the config data structure always initialized when user calls `umfCreateFooParams()`. There is no way to get non-initialized fields in the config, while in case of Option 2 or 3 user might forget to call `umfFooParamsInit(&params)` function.\r\n\r\nBetween Options 2 and 3, I prefer Option 3 because of two things:\r\n\r\n- it stores version explicitly.\r\n- the `umfMemoryProviderCreate` API remains unchanged.\r\n\r\n## Implementation details\r\n\r\nTBD\r\n\r\n\n", "patch": "diff --git a/examples/cuda_shared_memory/cuda_shared_memory.c b/examples/cuda_shared_memory/cuda_shared_memory.c\nindex 22e5e3f10..50c8f9240 100644\n--- a/examples/cuda_shared_memory/cuda_shared_memory.c\n+++ b/examples/cuda_shared_memory/cuda_shared_memory.c\n@@ -43,24 +43,51 @@ int main(void) {\n     // Create a context on the device\n     cuCtxCreate(&cuContext, 0, cuDevice);\n \n-    // Setup parameters for the CUDA memory provider. It will be used for\n+    // Setup parameters for the CUDA Memory Provider. It will be used for\n     // allocating memory from CUDA devices.\n-    cuda_memory_provider_params_t cu_memory_provider_params;\n-    cu_memory_provider_params.cuda_context_handle = cuContext;\n-    cu_memory_provider_params.cuda_device_handle = cuDevice;\n+    umf_cuda_memory_provider_params_handle_t cu_memory_provider_params = NULL;\n+    res = umfCUDAMemoryProviderParamsCreate(&cu_memory_provider_params);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to create memory provider params!\\n\");\n+        ret = -1;\n+        goto cuda_destroy;\n+    }\n+\n+    res = umfCUDAMemoryProviderParamsSetContext(cu_memory_provider_params,\n+                                                cuContext);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to set context in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n+\n+    res = umfCUDAMemoryProviderParamsSetDevice(cu_memory_provider_params,\n+                                               cuDevice);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to set device in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n     // Set the memory type to shared to allow the memory to be accessed on both\n     // CPU and GPU.\n-    cu_memory_provider_params.memory_type = UMF_MEMORY_TYPE_SHARED;\n+    res = umfCUDAMemoryProviderParamsSetMemoryType(cu_memory_provider_params,\n+                                                   UMF_MEMORY_TYPE_SHARED);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"Failed to set memory type in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n \n     // Create CUDA memory provider\n     umf_memory_provider_handle_t cu_memory_provider;\n-    res = umfMemoryProviderCreate(umfCUDAMemoryProviderOps(),\n-                                  &cu_memory_provider_params,\n-                                  &cu_memory_provider);\n+    res =\n+        umfMemoryProviderCreate(umfCUDAMemoryProviderOps(),\n+                                cu_memory_provider_params, &cu_memory_provider);\n     if (res != UMF_RESULT_SUCCESS) {\n         fprintf(stderr, \"Failed to create a memory provider!\\n\");\n         ret = -1;\n-        goto cuda_destroy;\n+        goto provider_params_destroy;\n     }\n \n     printf(\"CUDA memory provider created at %p\\n\", (void *)cu_memory_provider);\n@@ -147,6 +174,9 @@ int main(void) {\n memory_provider_destroy:\n     umfMemoryProviderDestroy(cu_memory_provider);\n \n+provider_params_destroy:\n+    umfCUDAMemoryProviderParamsDestroy(cu_memory_provider_params);\n+\n cuda_destroy:\n     ret = cuCtxDestroy(cuContext);\n     return ret;\ndiff --git a/include/umf/providers/provider_cuda.h b/include/umf/providers/provider_cuda.h\nindex 2f6a07d81..5f1d5a6e2 100644\n--- a/include/umf/providers/provider_cuda.h\n+++ b/include/umf/providers/provider_cuda.h\n@@ -14,12 +14,44 @@\n extern \"C\" {\n #endif\n \n-/// @brief CUDA Memory Provider settings struct\n-typedef struct cuda_memory_provider_params_t {\n-    void *cuda_context_handle;         ///< Handle to the CUDA context\n-    int cuda_device_handle;            ///< Handle to the CUDA device\n-    umf_usm_memory_type_t memory_type; ///< Allocation memory type\n-} cuda_memory_provider_params_t;\n+struct umf_cuda_memory_provider_params_t;\n+\n+typedef struct umf_cuda_memory_provider_params_t\n+    *umf_cuda_memory_provider_params_handle_t;\n+\n+/// @brief  Create a struct to store parameters of the CUDA Memory Provider.\n+/// @param  hParams [out] handle to the newly created parameters struct.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfCUDAMemoryProviderParamsCreate(\n+    umf_cuda_memory_provider_params_handle_t *hParams);\n+\n+/// @brief  Destroy parameters struct.\n+/// @param  hParams handle to the parameters of the CUDA Memory Provider.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfCUDAMemoryProviderParamsDestroy(\n+    umf_cuda_memory_provider_params_handle_t hParams);\n+\n+/// @brief  Set the CUDA context handle in the parameters struct.\n+/// @param  hParams handle to the parameters of the CUDA Memory Provider.\n+/// @param  hContext handle to the CUDA context.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfCUDAMemoryProviderParamsSetContext(\n+    umf_cuda_memory_provider_params_handle_t hParams, void *hContext);\n+\n+/// @brief  Set the CUDA device handle in the parameters struct.\n+/// @param  hParams handle to the parameters of the CUDA Memory Provider.\n+/// @param  hDevice handle to the CUDA device.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfCUDAMemoryProviderParamsSetDevice(\n+    umf_cuda_memory_provider_params_handle_t hParams, int hDevice);\n+\n+/// @brief  Set the memory type in the parameters struct.\n+/// @param  hParams handle to the parameters of the CUDA Memory Provider.\n+/// @param  memoryType memory type.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfCUDAMemoryProviderParamsSetMemoryType(\n+    umf_cuda_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType);\n \n umf_memory_provider_ops_t *umfCUDAMemoryProviderOps(void);\n \ndiff --git a/src/libumf.def b/src/libumf.def\nindex 56e26050c..8f29f4579 100644\n--- a/src/libumf.def\n+++ b/src/libumf.def\n@@ -17,6 +17,11 @@ EXPORTS\n     umfCoarseMemoryProviderGetStats\n     umfCoarseMemoryProviderOps\n     umfCUDAMemoryProviderOps\n+    umfCUDAMemoryProviderParamsCreate\n+    umfCUDAMemoryProviderParamsDestroy\n+    umfCUDAMemoryProviderParamsSetContext\n+    umfCUDAMemoryProviderParamsSetDevice\n+    umfCUDAMemoryProviderParamsSetMemoryType\n     umfDevDaxMemoryProviderOps\n     umfFree\n     umfFileMemoryProviderOps\ndiff --git a/src/libumf.map b/src/libumf.map\nindex 19235705c..f70d247b5 100644\n--- a/src/libumf.map\n+++ b/src/libumf.map\n@@ -11,6 +11,11 @@ UMF_1.0 {\n         umfCoarseMemoryProviderGetStats;\n         umfCoarseMemoryProviderOps;\n         umfCUDAMemoryProviderOps;\n+        umfCUDAMemoryProviderParamsCreate;\n+        umfCUDAMemoryProviderParamsDestroy;\n+        umfCUDAMemoryProviderParamsSetContext;\n+        umfCUDAMemoryProviderParamsSetDevice;\n+        umfCUDAMemoryProviderParamsSetMemoryType;\n         umfDevDaxMemoryProviderOps;\n         umfFree;\n         umfFileMemoryProviderOps;\ndiff --git a/src/provider/provider_cuda.c b/src/provider/provider_cuda.c\nindex 5a686d857..3c4e39451 100644\n--- a/src/provider/provider_cuda.c\n+++ b/src/provider/provider_cuda.c\n@@ -14,6 +14,40 @@\n \n #if defined(UMF_NO_CUDA_PROVIDER)\n \n+umf_result_t umfCUDAMemoryProviderParamsCreate(\n+    umf_cuda_memory_provider_params_handle_t *hParams) {\n+    (void)hParams;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsDestroy(\n+    umf_cuda_memory_provider_params_handle_t hParams) {\n+    (void)hParams;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetContext(\n+    umf_cuda_memory_provider_params_handle_t hParams, void *hContext) {\n+    (void)hParams;\n+    (void)hContext;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetDevice(\n+    umf_cuda_memory_provider_params_handle_t hParams, int hDevice) {\n+    (void)hParams;\n+    (void)hDevice;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetMemoryType(\n+    umf_cuda_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType) {\n+    (void)hParams;\n+    (void)memoryType;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n umf_memory_provider_ops_t *umfCUDAMemoryProviderOps(void) {\n     // not supported\n     return NULL;\n@@ -48,6 +82,13 @@ typedef struct cu_memory_provider_t {\n     size_t min_alignment;\n } cu_memory_provider_t;\n \n+// CUDA Memory Provider settings struct\n+typedef struct umf_cuda_memory_provider_params_t {\n+    void *cuda_context_handle;         ///< Handle to the CUDA context\n+    int cuda_device_handle;            ///< Handle to the CUDA device\n+    umf_usm_memory_type_t memory_type; ///< Allocation memory type\n+} umf_cuda_memory_provider_params_t;\n+\n typedef struct cu_ops_t {\n     CUresult (*cuMemGetAllocationGranularity)(\n         size_t *granularity, const CUmemAllocationProp *prop,\n@@ -158,14 +199,81 @@ static void init_cu_global_state(void) {\n     }\n }\n \n+umf_result_t umfCUDAMemoryProviderParamsCreate(\n+    umf_cuda_memory_provider_params_handle_t *hParams) {\n+    if (!hParams) {\n+        LOG_ERR(\"CUDA Memory Provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    umf_cuda_memory_provider_params_handle_t params_data =\n+        umf_ba_global_alloc(sizeof(umf_cuda_memory_provider_params_t));\n+    if (!params_data) {\n+        LOG_ERR(\"Cannot allocate memory for CUDA Memory Provider params\");\n+        return UMF_RESULT_ERROR_OUT_OF_HOST_MEMORY;\n+    }\n+\n+    params_data->cuda_context_handle = NULL;\n+    params_data->cuda_device_handle = -1;\n+    params_data->memory_type = UMF_MEMORY_TYPE_UNKNOWN;\n+\n+    *hParams = params_data;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsDestroy(\n+    umf_cuda_memory_provider_params_handle_t hParams) {\n+    umf_ba_global_free(hParams);\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetContext(\n+    umf_cuda_memory_provider_params_handle_t hParams, void *hContext) {\n+    if (!hParams) {\n+        LOG_ERR(\"CUDA Memory Provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->cuda_context_handle = hContext;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetDevice(\n+    umf_cuda_memory_provider_params_handle_t hParams, int hDevice) {\n+    if (!hParams) {\n+        LOG_ERR(\"CUDA Memory Provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->cuda_device_handle = hDevice;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfCUDAMemoryProviderParamsSetMemoryType(\n+    umf_cuda_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType) {\n+    if (!hParams) {\n+        LOG_ERR(\"CUDA Memory Provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->memory_type = memoryType;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n static umf_result_t cu_memory_provider_initialize(void *params,\n                                                   void **provider) {\n     if (params == NULL) {\n         return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n     }\n \n-    cuda_memory_provider_params_t *cu_params =\n-        (cuda_memory_provider_params_t *)params;\n+    umf_cuda_memory_provider_params_handle_t cu_params =\n+        (umf_cuda_memory_provider_params_handle_t)params;\n \n     if (cu_params->memory_type == UMF_MEMORY_TYPE_UNKNOWN ||\n         cu_params->memory_type > UMF_MEMORY_TYPE_SHARED) {\n", "instance_id": "oneapi-src__unified-memory-framework-921", "clarity": 3, "difficulty": 0.65, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly outlines the goal of ensuring backward compatibility for configuration structures in the UMF library, provides detailed background on the issue with concrete examples (e.g., the `level_zero_memory_provider_params_t` structure issue), and explains the root cause of the problem. The statement also includes multiple potential solutions (Options 1, 2, and 3) with detailed descriptions, code snippets, and design considerations, which further clarify the problem and possible approaches. Constraints and challenges, such as the need to handle uninitialized fields and version mismatches, are explicitly discussed. There are no significant ambiguities, and the problem's logic is well-articulated with examples of real-world implications. The only minor gap is the lack of finalized implementation details (marked as TBD), but this does not detract from the overall clarity given the depth of the discussion and proposed solutions.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of the code changes involves multiple files and components, including headers, provider implementations, and example code, as seen in the diffs for CUDA memory provider configurations. This requires understanding and modifying interactions between the library's public API and internal structures. Second, the problem demands a deep understanding of ABI (Application Binary Interface) compatibility, dynamic library versioning, and memory layout issues in C, which are advanced concepts. Third, the solution (implementing a handle-based approach as per Option 1) requires significant refactoring of how configuration parameters are managed, moving from direct struct access to an opaque handle with getter/setter APIs, which impacts the library's design and usage. Fourth, edge cases such as invalid handles, memory allocation failures, and ensuring backward compatibility with existing client code add complexity to the implementation and error handling. While the problem does not reach the \"Very Hard\" level (0.8-1.0) due to the absence of system-level or highly domain-specific challenges beyond ABI compatibility, it still requires a solid grasp of C programming nuances, library design, and careful handling of versioned interfaces. A score of 0.65 reflects the need for deep technical knowledge and careful implementation across multiple parts of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[ENH] - Old develop branch references after default branch renaming\n### Feature description\n\nRecently, we renamed the default branch from `develop` to `main` and [updated our GHA workflows accordingly](https://github.com/nebari-dev/nebari/pull/2748).\r\n\r\nHowever, it seems we still have some `develop` references across our GHA workflow files. See for example:\r\n* https://github.com/nebari-dev/nebari/blob/baef3b4733a583174dc77e24e2d7050b1fa45cee/.github/workflows/test_aws_integration.yaml#L11\r\n* https://github.com/nebari-dev/nebari/blob/baef3b4733a583174dc77e24e2d7050b1fa45cee/.github/workflows/test_azure_integration.yaml#L11\r\n* https://github.com/nebari-dev/nebari/blob/baef3b4733a583174dc77e24e2d7050b1fa45cee/.github/workflows/typing.yaml#L12\r\n\r\nInitially, I thought this was causing our [cloud deployment tests to fail](https://github.com/nebari-dev/nebari/issues/2768), but that was fixes by itself, probably because of branch redirects.\n\n### Value and/or benefit\n\nIt's clearer and more maintainable if we don't have reference to old branches in our codebase.\n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/.github/workflows/run-precommit.yaml b/.github/workflows/run-precommit.yaml\nindex 9592a5837..eab62ce83 100644\n--- a/.github/workflows/run-precommit.yaml\n+++ b/.github/workflows/run-precommit.yaml\n@@ -4,7 +4,6 @@ on:\n   push:\n     branches:\n       - main\n-      - develop\n       - release/\\d{4}.\\d{1,2}.\\d{1,2}\n   pull_request:\n \ndiff --git a/.github/workflows/typing.yaml b/.github/workflows/typing.yaml\nindex de70d6948..3ce1bbc78 100644\n--- a/.github/workflows/typing.yaml\n+++ b/.github/workflows/typing.yaml\n@@ -9,7 +9,6 @@ on:\n   push:\n     branches:\n       - main\n-      - develop\n       - release/\\d{4}.\\d{1,2}.\\d{1,2}\n     paths:\n       - \".github/workflows/typing.yaml\"\n", "instance_id": "nebari-dev__nebari-2769", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in describing the goal of removing outdated references to the 'develop' branch in GitHub Actions (GHA) workflow files after renaming the default branch to 'main'. It provides specific examples of files and lines where these references exist, which helps in understanding the scope of the issue. The value and benefit of making these changes are also articulated, emphasizing clarity and maintainability. However, there are minor ambiguities: the statement does not explicitly mention whether all instances of 'develop' should be removed or if there are specific contexts where they should remain. Additionally, it lacks discussion on potential side effects or dependencies in the workflows that might be affected by these changes. Overall, the problem is valid and clear, but these minor missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, as it involves straightforward modifications to a few configuration files in the GitHub Actions workflows. The code changes provided show simple deletions of the 'develop' branch reference in two files, which requires minimal understanding of the codebase or complex logic. The scope of the changes is limited to specific lines in a small number of files, with no impact on the broader system architecture or interactions between modules. No advanced programming concepts, algorithms, or domain-specific knowledge are required beyond basic familiarity with GitHub Actions syntax. There are no edge cases or error handling considerations mentioned in the problem statement or evident in the code changes, as the task is purely about updating branch references. Overall, this is a very easy task that can be completed quickly by someone with basic knowledge of version control and configuration files.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "The setter for rng seed of an instance of `ProbInput` is ineffective.\nConsider the following test:\r\n\r\n```python\r\nimport uqtestfuns as uqtf\r\na = uqtf.HigdonSine()\r\nb = a.prob_input\r\n\r\nb.rng_seed = 42  # By intention, this should reset the RNG with a seed number of 42\r\nx = b.get_sample(10)\r\nb.rng_seed = 42  # Once again, this should reset the RNG with a seed number of 42\r\ny = b.get_sample(10)\r\n\r\nassert np.all(x == y)\r\n```\r\n\r\nthe assertion will fail because in `probabilistic_input.py` l. 86:\r\n\r\n```\r\n    def rng_seed(self, value: Optional[int]):\r\n        \"\"\"Set/reset the seed for RNG.\"\"\"\r\n        self.reset_rng(self._rng_seed)\r\n```\r\n\r\nIn other words, the value is never assigned.\r\n\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex f41ec72..a7f70a9 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -61,6 +61,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ## Fixed\n \n+- Assigning an integer value to `rng_seed` property of `ProbInput` now\n+  correctly reset the RNG with the assigned seed number.\n - The fifth input variable of the Friedman is now active instead of the sixth.\n   Only the first five input variables of the Friedman functions are active.\n - The argument `input_dimension` to filter the output of `list_functions()`\n", "instance_id": "damar-wicaksono__uqtestfuns-443", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the setter for `rng_seed` in the `ProbInput` class does not correctly assign the provided seed value, leading to ineffective resetting of the random number generator (RNG). The provided test code snippet effectively demonstrates the expected behavior versus the actual behavior, and the specific line of code causing the issue is pointed out. However, there are minor ambiguities, such as the lack of explicit mention of the expected behavior of `reset_rng` or whether there are any constraints on the seed value (e.g., must it be positive?). Additionally, edge cases or potential side effects of changing the seed value are not discussed. Overall, the problem is understandable, but some minor details are missing that could affect the implementation or testing of the fix.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is a straightforward bug in the setter method for `rng_seed`, where the provided value is not being assigned or used to reset the RNG. Fixing this likely involves a simple modification to a single line or function in `probabilistic_input.py` to correctly assign the input value to `self._rng_seed` before calling `reset_rng`. The scope of the code change is minimal, confined to a single property setter, and does not impact the broader architecture or require understanding complex interactions within the codebase. The technical concepts involved are basic\u2014understanding Python property setters and possibly the behavior of RNG seeding (likely using `numpy.random` or similar). There are no significant edge cases or error handling requirements mentioned in the problem statement, and the fix does not appear to introduce complexity in these areas. Overall, this is a very easy task suitable for a junior developer or someone with basic familiarity with Python.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove bracket in the application tags when printing out an instance of test functions\nCurrently the string representation of a test function is as follows:\r\n\r\n```\r\n>>> my_testfun = uqtf.Borehole()\r\n>>> print(my_testfun)\r\nFunction ID      : Borehole\r\nInput Dimension  : 8 (fixed)\r\nOutput Dimension : 1\r\nParameterized    : False\r\nDescription      : Borehole function from Harper and Gupta (1983)\r\nApplications     : ['metamodeling', 'sensitivity']\r\n```\r\n\r\nThe content of \"Applications\" should be printed out without brackets and without the quotation marks.\n", "patch": "diff --git a/README.md b/README.md\nindex ac98de9..f954b67 100644\n--- a/README.md\n+++ b/README.md\n@@ -65,7 +65,7 @@ Input Dimension  : 8 (fixed)\n Output Dimension : 1\n Parameterized    : False\n Description      : Borehole function from Harper and Gupta (1983)\n-Applications     : ['metamodeling', 'sensitivity']\n+Applications     : metamodeling, sensitivity\n ```\n \n The probabilistic input specification of this test function is built-in:\n", "instance_id": "damar-wicaksono__uqtestfuns-397", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to modify the string representation of a test function by removing brackets and quotation marks from the \"Applications\" field when printing. It provides a concrete example of the current and desired output, which helps in understanding the expected result. However, there are minor ambiguities, such as the lack of information about where in the codebase this string representation is generated or formatted. Additionally, it does not specify if there are any constraints or edge cases (e.g., empty application lists or special characters in application names) that need to be handled. While the intent is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The task appears to involve a simple modification to the string formatting logic for the \"Applications\" field in the string representation of a test function. Based on the provided code changes, it seems to be a documentation update in the README.md file rather than an actual code change in the application logic, which suggests the actual implementation might not be reflected here or is trivial (e.g., updating a format string or joining a list with a different delimiter). The scope of the change is minimal, likely confined to a single function or method responsible for formatting the output. No complex technical concepts, algorithms, or deep understanding of the codebase architecture are required beyond basic string manipulation. There are no edge cases or error handling requirements mentioned in the problem statement, and the provided diff does not indicate any complexity in this regard. Overall, this is a very easy task that requires only basic modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "chore(deps): bump prost-types from 0.12.6 to 0.13.1\nBumps [prost-types](https://github.com/tokio-rs/prost) from 0.12.6 to 0.13.1.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tokio-rs/prost/blob/master/CHANGELOG.md\">prost-types's changelog</a>.</em></p>\n<blockquote>\n<h1>Prost version 0.13.1</h1>\n<p><em>PROST!</em> is a <a href=\"https://developers.google.com/protocol-buffers/\">Protocol Buffers</a> implementation for the <a href=\"https://www.rust-lang.org/\">Rust Language</a>. <code>prost</code> generates simple, idiomatic Rust code from <code>proto2</code> and <code>proto3</code> files.</p>\n<h2>Bug fixes</h2>\n<ul>\n<li>Enum variant named Error causes ambiguous item (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1098\">#1098</a>)</li>\n</ul>\n<h1>PROST version 0.13.0</h1>\n<p><strong>note</strong>: this version was yanked in favor of 0.13.1</p>\n<p><em>PROST!</em> is a <a href=\"https://developers.google.com/protocol-buffers/\">Protocol Buffers</a> implementation for the <a href=\"https://www.rust-lang.org/\">Rust Language</a>. <code>prost</code> generates simple, idiomatic Rust code from <code>proto2</code> and <code>proto3</code> files.</p>\n<p>This major update brings new features and fixes:</p>\n<h2>Breaking changes</h2>\n<ul>\n<li>\n<p>derive Copy trait for messages where possible (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/950\">#950</a>)</p>\n<p><code>prost-build</code> will automatically derive <code>trait Copy</code> for some messages. If you manually implement <code>Copy</code> you should remove your implementation.</p>\n</li>\n<li>\n<p>Change generated functions signatures to remove type parameters (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1045\">#1045</a>)</p>\n<p>The function signature of <code>trait Message</code> is changed to use <code>impl Buf</code> instead of a named generic type. If you implement <code>trait Message</code>, you should change the function signature.</p>\n</li>\n<li>\n<p>Lightweight error value in TryFrom<!-- raw HTML omitted --> for enums (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1010\">#1010</a>)</p>\n<p>When a <code>impl TryFrom&lt;i32&gt;</code> is generated by <code>prost</code> derive macros, it will now return the error type <code>UnknownEnumValue</code> instead of <code>DecodeError</code>. The new error can be used to retreive the integer value that failed to convert.</p>\n</li>\n</ul>\n<h2>Features</h2>\n<ul>\n<li>\n<p>fix: Only touch include file if contents is changed (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1058\">#1058</a>)</p>\n<p>Most generated files are untouched when the contents doesn't change. Use the same mechanism for include file as well.</p>\n</li>\n</ul>\n<h2>Dependencies</h2>\n<ul>\n<li>update env_logger requirement from 0.10 to 0.11 (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1074\">#1074</a>)</li>\n<li>update criterion requirement from 0.4 to 0.5 (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1071\">#1071</a>)</li>\n<li>Remove unused libz-sys (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1077\">#1077</a>)</li>\n<li>build(deps): update itertools requirement from &gt;=0.10, <!-- raw HTML omitted -->=0.10, &lt;=0.13 (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1070\">#1070</a>)</li>\n</ul>\n<h2>Documentation</h2>\n<ul>\n<li>better checking of tag duplicates, avoid discarding invalid variant errs (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/951\">#951</a>)</li>\n<li>docs: Fix broken link warnings (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1056\">#1056</a>)</li>\n<li>Add missing LICENSE symlink (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1086\">#1086</a>)</li>\n</ul>\n<h2>Internal</h2>\n<ul>\n<li>workspace package metadata (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1036\">#1036</a>)</li>\n<li>fix: Build error due to merge conflict (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1068\">#1068</a>)</li>\n<li>build: Fix release scripts (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1055\">#1055</a>)</li>\n<li>chore: Add ci to check MSRV (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1057\">#1057</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/f19104a3d4345f4635bbc5fd54cb5089572e2660\"><code>f19104a</code></a> chore: prepare 0.13.1 release (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1099\">#1099</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/26463f437e1a3445c9d3119cf9cf0f64f5b23dfa\"><code>26463f4</code></a> fix: Enum variant named <code>Error</code> causes ambiguous item (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1098\">#1098</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/23f71749696d5836afa143cbbb860fa8943e0640\"><code>23f7174</code></a> chore: Release version 0.13.0 (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1093\">#1093</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/7a1424cf8c71c0c276970d1b8f1ef17c424d818d\"><code>7a1424c</code></a> build: Fix prepare-release.sh (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1094\">#1094</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/7790799b0dfc65f70cfde0a26e8f29f77510744a\"><code>7790799</code></a> build(deps): update itertools requirement from &gt;=0.10, &lt;=0.12 to &gt;=0.10, &lt;=0....</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/4a0cc17102700d1215c791f1ee4b8df22481a19f\"><code>4a0cc17</code></a> Add missing LICENSE symlink (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1086\">#1086</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/ae33a5ea97ff2df8a508b6f80fa5b88c4bc0c0a6\"><code>ae33a5e</code></a> ci: Set rust version of clippy job to a fixed version (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1090\">#1090</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/ba776540834b0cecbbaa6ca98bd79d9682cd7e92\"><code>ba77654</code></a> fix: Only touch include file if contents is changed (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1058\">#1058</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/e7049d3eb20cf33bb1ac60bff19cc3a7bd48f6f2\"><code>e7049d3</code></a> workspace package metadata (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1036\">#1036</a>)</li>\n<li><a href=\"https://github.com/tokio-rs/prost/commit/ef4930c140c639eaaa0362662d90e55a31977eb3\"><code>ef4930c</code></a> docs: Fix broken link warnings (<a href=\"https://redirect.github.com/tokio-rs/prost/issues/1056\">#1056</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tokio-rs/prost/compare/v0.12.6...v0.13.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=prost-types&package-manager=cargo&previous-version=0.12.6&new-version=0.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex dff7f07d8..aa765c81f 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -121,9 +121,9 @@ dependencies = [\n \n [[package]]\n name = \"anstyle\"\n-version = \"1.0.6\"\n+version = \"1.0.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8901269c6307e8d93993578286ac0edf7f195079ffff5ebdeea6a59ffb7e36bc\"\n+checksum = \"1bec1de6f59aedf83baf9ff929c98f2ad654b97c9510f4e70cf6f661d49fd5b1\"\n \n [[package]]\n name = \"anstyle-parse\"\n@@ -484,13 +484,14 @@ dependencies = [\n \n [[package]]\n name = \"assert_cmd\"\n-version = \"2.0.15\"\n+version = \"2.0.16\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bc65048dd435533bb1baf2ed9956b9a278fbfdcf90301b39ee117f06c0199d37\"\n+checksum = \"dc1835b7f27878de8525dc71410b5a31cdcc5f230aed5ba5df968e09c201b23d\"\n dependencies = [\n  \"anstyle\",\n  \"bstr\",\n  \"doc-comment\",\n+ \"libc\",\n  \"predicates\",\n  \"predicates-core\",\n  \"predicates-tree\",\n@@ -1929,7 +1930,7 @@ dependencies = [\n  \"num-traits\",\n  \"serde\",\n  \"wasm-bindgen\",\n- \"windows-targets 0.52.4\",\n+ \"windows-targets 0.52.6\",\n ]\n \n [[package]]\n@@ -2015,9 +2016,9 @@ dependencies = [\n \n [[package]]\n name = \"clap\"\n-version = \"4.5.13\"\n+version = \"4.5.14\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0fbb260a053428790f3de475e304ff84cdbc4face759ea7a3e64c1edd938a7fc\"\n+checksum = \"c937d4061031a6d0c8da4b9a4f98a172fc2976dfb1c19213a9cf7d0d3c837e36\"\n dependencies = [\n  \"clap_builder\",\n  \"clap_derive\",\n@@ -2025,9 +2026,9 @@ dependencies = [\n \n [[package]]\n name = \"clap_builder\"\n-version = \"4.5.13\"\n+version = \"4.5.14\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"64b17d7ea74e9f833c7dbf2cbe4fb12ff26783eda4782a8975b72f895c9b4d99\"\n+checksum = \"85379ba512b21a328adf887e85f7742d12e96eb31f3ef077df4ffc26b506ffed\"\n dependencies = [\n  \"anstream\",\n  \"anstyle\",\n@@ -3667,9 +3668,9 @@ dependencies = [\n \n [[package]]\n name = \"flate2\"\n-version = \"1.0.30\"\n+version = \"1.0.31\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5f54427cfd1c7829e2a139fcefea601bf088ebca651d2bf53ebc600eac295dae\"\n+checksum = \"7f211bbe8e69bbd0cfdea405084f128ae8b4aaa6b0b522fc8f2b009084797920\"\n dependencies = [\n  \"crc32fast\",\n  \"miniz_oxide\",\n@@ -5125,7 +5126,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"0c2a198fb6b0eada2a8df47933734e6d35d350665a33a3593d7164fa52c75c19\"\n dependencies = [\n  \"cfg-if\",\n- \"windows-targets 0.52.4\",\n+ \"windows-targets 0.48.5\",\n ]\n \n [[package]]\n@@ -5669,7 +5670,7 @@ dependencies = [\n  \"pin-project\",\n  \"rand\",\n  \"rustls 0.22.3\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"serde\",\n  \"serde_json\",\n  \"socket2\",\n@@ -5989,7 +5990,7 @@ dependencies = [\n  \"rand\",\n  \"reqwest 0.11.27\",\n  \"ring 0.17.8\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"serde\",\n  \"serde_json\",\n  \"snafu\",\n@@ -6314,7 +6315,7 @@ dependencies = [\n  \"proxyutil\",\n  \"reqwest 0.12.4\",\n  \"rustls 0.23.12\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"serde\",\n  \"sqlexec\",\n  \"tempfile\",\n@@ -7065,9 +7066,9 @@ dependencies = [\n \n [[package]]\n name = \"regex\"\n-version = \"1.10.5\"\n+version = \"1.10.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b91213439dad192326a0d7c6ee3955910425f441d7038e0d6933b0aec5c4517f\"\n+checksum = \"4219d74c6b67a3654a9fbebc4b419e22126d13d2f3c4a07ee0cb61ff79a79619\"\n dependencies = [\n  \"aho-corasick\",\n  \"memchr\",\n@@ -7218,7 +7219,7 @@ dependencies = [\n  \"pin-project-lite\",\n  \"rustls 0.22.3\",\n  \"rustls-native-certs 0.7.0\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"rustls-pki-types\",\n  \"serde\",\n  \"serde_json\",\n@@ -7398,9 +7399,9 @@ dependencies = [\n \n [[package]]\n name = \"rstest\"\n-version = \"0.21.0\"\n+version = \"0.22.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"9afd55a67069d6e434a95161415f5beeada95a01c7b815508a82dcb0e1593682\"\n+checksum = \"7b423f0e62bdd61734b67cd21ff50871dfaeb9cc74f869dcd6af974fbcb19936\"\n dependencies = [\n  \"futures\",\n  \"futures-timer\",\n@@ -7410,9 +7411,9 @@ dependencies = [\n \n [[package]]\n name = \"rstest_macros\"\n-version = \"0.21.0\"\n+version = \"0.22.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"4165dfae59a39dd41d8dec720d3cbfbc71f69744efb480a3920f5d4e0cc6798d\"\n+checksum = \"c5e1711e7d14f74b12a58411c542185ef7fb7f2e7f8ee6e2940a883628522b42\"\n dependencies = [\n  \"cfg-if\",\n  \"glob\",\n@@ -7606,7 +7607,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"8f1fb85efa936c42c6d5fc28d2629bb51e4b2f4b8a5211e297d599cc5a093792\"\n dependencies = [\n  \"openssl-probe\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"rustls-pki-types\",\n  \"schannel\",\n  \"security-framework\",\n@@ -7623,9 +7624,9 @@ dependencies = [\n \n [[package]]\n name = \"rustls-pemfile\"\n-version = \"2.1.2\"\n+version = \"2.1.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"29993a25686778eb88d4189742cd713c9bce943bc54251a33509dc63cbacf73d\"\n+checksum = \"196fe16b00e106300d3e45ecfcb764fa292a535d7326a29a5875c579c7417425\"\n dependencies = [\n  \"base64 0.22.1\",\n  \"rustls-pki-types\",\n@@ -7913,9 +7914,9 @@ checksum = \"a3f0bf26fd526d2a95683cd0f87bf103b8539e2ca1ef48ce002d67aad59aa0b4\"\n \n [[package]]\n name = \"serde\"\n-version = \"1.0.204\"\n+version = \"1.0.205\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bc76f558e0cbb2a839d37354c575f1dc3fdc6546b5be373ba43d95f231bf7c12\"\n+checksum = \"e33aedb1a7135da52b7c21791455563facbbcc43d0f0f66165b42c21b3dfb150\"\n dependencies = [\n  \"serde_derive\",\n ]\n@@ -7931,9 +7932,9 @@ dependencies = [\n \n [[package]]\n name = \"serde_derive\"\n-version = \"1.0.204\"\n+version = \"1.0.205\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e0cd7e117be63d3c3678776753929474f3b04a43a080c744d6b0ae2a8c28e222\"\n+checksum = \"692d6f5ac90220161d6774db30c662202721e64aed9058d2c394f451261420c1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -8787,14 +8788,15 @@ dependencies = [\n \n [[package]]\n name = \"tempfile\"\n-version = \"3.10.1\"\n+version = \"3.12.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"85b77fafb263dd9d05cbeac119526425676db3784113aa9295c88498cbf8bff1\"\n+checksum = \"04cbcdd0c794ebb0d4cf35e88edd2f7d2c4c3e9a5a6dab322839b321c6a87a64\"\n dependencies = [\n  \"cfg-if\",\n  \"fastrand\",\n+ \"once_cell\",\n  \"rustix\",\n- \"windows-sys 0.52.0\",\n+ \"windows-sys 0.59.0\",\n ]\n \n [[package]]\n@@ -9189,7 +9191,7 @@ dependencies = [\n  \"pin-project\",\n  \"prost 0.12.6\",\n  \"rustls-native-certs 0.7.0\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"rustls-pki-types\",\n  \"tokio\",\n  \"tokio-rustls 0.25.0\",\n@@ -9910,7 +9912,7 @@ version = \"0.52.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"33ab640c8d7e35bf8ba19b884ba838ceb4fba93a4e8c65a9059d08afcfc683d9\"\n dependencies = [\n- \"windows-targets 0.52.4\",\n+ \"windows-targets 0.52.6\",\n ]\n \n [[package]]\n@@ -9928,7 +9930,16 @@ version = \"0.52.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d\"\n dependencies = [\n- \"windows-targets 0.52.4\",\n+ \"windows-targets 0.52.6\",\n+]\n+\n+[[package]]\n+name = \"windows-sys\"\n+version = \"0.59.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1e38bc4d79ed67fd075bcc251a1c39b32a1776bbe92e5bef1f0bf1f8c531853b\"\n+dependencies = [\n+ \"windows-targets 0.52.6\",\n ]\n \n [[package]]\n@@ -9948,17 +9959,18 @@ dependencies = [\n \n [[package]]\n name = \"windows-targets\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"7dd37b7e5ab9018759f893a1952c9420d060016fc19a472b4bb20d1bdd694d1b\"\n+checksum = \"9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973\"\n dependencies = [\n- \"windows_aarch64_gnullvm 0.52.4\",\n- \"windows_aarch64_msvc 0.52.4\",\n- \"windows_i686_gnu 0.52.4\",\n- \"windows_i686_msvc 0.52.4\",\n- \"windows_x86_64_gnu 0.52.4\",\n- \"windows_x86_64_gnullvm 0.52.4\",\n- \"windows_x86_64_msvc 0.52.4\",\n+ \"windows_aarch64_gnullvm 0.52.6\",\n+ \"windows_aarch64_msvc 0.52.6\",\n+ \"windows_i686_gnu 0.52.6\",\n+ \"windows_i686_gnullvm\",\n+ \"windows_i686_msvc 0.52.6\",\n+ \"windows_x86_64_gnu 0.52.6\",\n+ \"windows_x86_64_gnullvm 0.52.6\",\n+ \"windows_x86_64_msvc 0.52.6\",\n ]\n \n [[package]]\n@@ -9969,9 +9981,9 @@ checksum = \"2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8\"\n \n [[package]]\n name = \"windows_aarch64_gnullvm\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bcf46cf4c365c6f2d1cc93ce535f2c8b244591df96ceee75d8e83deb70a9cac9\"\n+checksum = \"32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3\"\n \n [[package]]\n name = \"windows_aarch64_msvc\"\n@@ -9981,9 +9993,9 @@ checksum = \"dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc\"\n \n [[package]]\n name = \"windows_aarch64_msvc\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"da9f259dd3bcf6990b55bffd094c4f7235817ba4ceebde8e6d11cd0c5633b675\"\n+checksum = \"09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469\"\n \n [[package]]\n name = \"windows_i686_gnu\"\n@@ -9993,9 +10005,15 @@ checksum = \"a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e\"\n \n [[package]]\n name = \"windows_i686_gnu\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b\"\n+\n+[[package]]\n+name = \"windows_i686_gnullvm\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b474d8268f99e0995f25b9f095bc7434632601028cf86590aea5c8a5cb7801d3\"\n+checksum = \"0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66\"\n \n [[package]]\n name = \"windows_i686_msvc\"\n@@ -10005,9 +10023,9 @@ checksum = \"8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406\"\n \n [[package]]\n name = \"windows_i686_msvc\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1515e9a29e5bed743cb4415a9ecf5dfca648ce85ee42e15873c3cd8610ff8e02\"\n+checksum = \"240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66\"\n \n [[package]]\n name = \"windows_x86_64_gnu\"\n@@ -10017,9 +10035,9 @@ checksum = \"53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e\"\n \n [[package]]\n name = \"windows_x86_64_gnu\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5eee091590e89cc02ad514ffe3ead9eb6b660aedca2183455434b93546371a03\"\n+checksum = \"147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78\"\n \n [[package]]\n name = \"windows_x86_64_gnullvm\"\n@@ -10029,9 +10047,9 @@ checksum = \"0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc\"\n \n [[package]]\n name = \"windows_x86_64_gnullvm\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"77ca79f2451b49fa9e2af39f0747fe999fcda4f5e241b2898624dca97a1f2177\"\n+checksum = \"24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d\"\n \n [[package]]\n name = \"windows_x86_64_msvc\"\n@@ -10041,9 +10059,9 @@ checksum = \"ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538\"\n \n [[package]]\n name = \"windows_x86_64_msvc\"\n-version = \"0.52.4\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"32b752e52a2da0ddfbdbcc6fceadfeede4c939ed16d13e648833a61dfb611ed8\"\n+checksum = \"589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec\"\n \n [[package]]\n name = \"winnow\"\n@@ -10188,7 +10206,7 @@ dependencies = [\n  \"log\",\n  \"percent-encoding\",\n  \"rustls 0.23.12\",\n- \"rustls-pemfile 2.1.2\",\n+ \"rustls-pemfile 2.1.3\",\n  \"seahash\",\n  \"serde\",\n  \"serde_json\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 7180f2f8d..e52e805b7 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -17,7 +17,7 @@ strip = true\n wildcard_imports = \"deny\"\n \n [workspace.dependencies]\n-clap = { version = \"4.5.13\", features = [\"derive\"] }\n+clap = { version = \"4.5.14\", features = [\"derive\"] }\n datafusion = { version = \"36.0.0\", features = [\"avro\"] }\n arrow-flight = { version = \"50.0.0\", features = [\"flight-sql-experimental\"] }\n datafusion-proto = { version = \"36.0.0\" }\n@@ -38,7 +38,7 @@ prost-build = \"0.12\"\n prost-types = \"0.12\"\n rustls = \"0.23.12\"\n serde_json = \"1.0.122\"\n-tempfile = \"3.10.1\"\n+tempfile = \"3.12.0\"\n thiserror = \"1.0\"\n tracing = \"0.1\"\n url = \"2.5.2\"\ndiff --git a/crates/bench_runner/Cargo.toml b/crates/bench_runner/Cargo.toml\nindex 9eea1bb32..8128ffb15 100644\n--- a/crates/bench_runner/Cargo.toml\n+++ b/crates/bench_runner/Cargo.toml\n@@ -21,4 +21,4 @@ tokio = { workspace = true }\n tracing = { workspace = true }\n tokio-postgres = \"0.7.11\"\n glob = \"0.3.1\"\n-regex = \"1.10.5\"\n+regex = \"1.10.6\"\ndiff --git a/crates/datafusion_ext/Cargo.toml b/crates/datafusion_ext/Cargo.toml\nindex 334ca99d7..7ed5c0fdc 100644\n--- a/crates/datafusion_ext/Cargo.toml\n+++ b/crates/datafusion_ext/Cargo.toml\n@@ -39,4 +39,4 @@ ctor = \"0.2.8\"\n env_logger = \"0.11\"\n paste = \"^1.0\"\n rand = \"0.8.5\"\n-rstest = \"0.21\"\n+rstest = \"0.22\"\ndiff --git a/crates/datasources/Cargo.toml b/crates/datasources/Cargo.toml\nindex f43bc354f..a2fd982d2 100644\n--- a/crates/datasources/Cargo.toml\n+++ b/crates/datasources/Cargo.toml\n@@ -50,7 +50,7 @@ mongodb = \"3.0.1\"\n once_cell = \"1.19.0\"\n parking_lot = \"0.12.3\"\n rand = \"0.8.5\"\n-regex = \"1.10.5\"\n+regex = \"1.10.6\"\n ring = \"0.17.8\"\n scylla = \"0.13.1\"\n serde_with = \"3.9.0\"\ndiff --git a/crates/decimal/Cargo.toml b/crates/decimal/Cargo.toml\nindex 56a7a48ff..b3f4234d8 100644\n--- a/crates/decimal/Cargo.toml\n+++ b/crates/decimal/Cargo.toml\n@@ -13,4 +13,4 @@ doctest = false\n [dependencies]\n thiserror.workspace = true\n num-traits = \"0.2.19\"\n-regex = \"1.10.5\"\n+regex = \"1.10.6\"\ndiff --git a/crates/pgsrv/Cargo.toml b/crates/pgsrv/Cargo.toml\nindex a586bfedb..53beee03e 100644\n--- a/crates/pgsrv/Cargo.toml\n+++ b/crates/pgsrv/Cargo.toml\n@@ -28,7 +28,7 @@ pgrepr = { path = \"../pgrepr\" }\n proxyutil = { path = \"../proxyutil\" }\n sqlexec = { path = \"../sqlexec\" }\n once_cell = \"1.19.0\"\n-rustls-pemfile = \"2.1.2\"\n+rustls-pemfile = \"2.1.3\"\n tokio-postgres = \"0.7.11\"\n tokio-rustls = \"0.26.0\"\n tokio-util = { version = \"0.7.11\", features = [\"codec\"] }\ndiff --git a/crates/slt/Cargo.toml b/crates/slt/Cargo.toml\nindex cba18de81..a8de59afb 100644\n--- a/crates/slt/Cargo.toml\n+++ b/crates/slt/Cargo.toml\n@@ -35,7 +35,7 @@ telemetry = { path = \"../telemetry\" }\n uuid = { version = \"1.10\", features = [\"v4\", \"fast-rng\", \"macro-diagnostics\"] }\n glob = \"0.3.1\"\n once_cell = \"1.19.0\"\n-regex = \"1.10.5\"\n+regex = \"1.10.6\"\n sqllogictest = \"0.21.0\"\n tokio-postgres = \"0.7.11\"\n walkdir = \"2.5.0\"\ndiff --git a/crates/snowflake_connector/Cargo.toml b/crates/snowflake_connector/Cargo.toml\nindex 7cd8c02a4..b9666ee07 100644\n--- a/crates/snowflake_connector/Cargo.toml\n+++ b/crates/snowflake_connector/Cargo.toml\n@@ -23,7 +23,7 @@ datafusion = { workspace = true }\n hex = \"0.4.3\"\n base64 = \"0.22.1\"\n bytes = \"1.7.1\"\n-flate2 = \"1.0.30\"\n+flate2 = \"1.0.31\"\n decimal = { path = \"../decimal\" }\n \n [dev-dependencies]\n", "instance_id": "GlareDB__glaredb-3147", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear as it describes a dependency update for `prost-types` from version 0.12.6 to 0.13.1, including a detailed changelog that outlines breaking changes, bug fixes, and new features. The intent of the task (updating a dependency) and the scope (specific library version bump) are evident. Additionally, the provided code changes in the `Cargo.lock` and `Cargo.toml` files align with the described update. However, there are minor ambiguities: the problem statement does not explicitly discuss the potential impact of the breaking changes on the codebase (e.g., whether manual adjustments are needed due to the `Copy` trait derivation or function signature changes in `prost`). It also lacks specific instructions on testing or validating the update, which could be critical given the breaking changes noted in the changelog. Thus, while the goal is clear, some minor details about handling potential issues are missing.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it primarily involves a straightforward dependency update. The code changes are limited to version bumps in `Cargo.toml` files across multiple crates and corresponding updates in `Cargo.lock`, which are mostly automated by tools like Dependabot. The scope of changes is narrow, affecting only configuration files without requiring modifications to the core logic or architecture of the codebase. However, there is a slight increase in difficulty due to the breaking changes mentioned in the `prost-types` changelog (e.g., `Copy` trait derivation, function signature changes, and error type changes in `TryFrom`). These could potentially require minor manual adjustments if the codebase relies on the affected APIs, though no such issues are evident in the provided diff. The task requires basic familiarity with Rust's dependency management via Cargo and a cursory understanding of the changelog to identify potential impacts, but no deep technical concepts, complex algorithms, or extensive edge case handling are involved. There are also no significant error handling requirements beyond ensuring the build and tests pass after the update. Overall, this is a very easy task that might require minimal follow-up if compatibility issues arise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Repo migration is failing\nMigrating repo from 1.4.0 to 1.5.0 is failing with:\r\n```\r\nfailed to setup repo: failed to reconcile repo: failed to open repo: failed to migrate repo: rename /Users/walid/.bacalhau/orchestrator_store/jobs.db /Users/walid/.bacalhau/orchestrator_store/state_boltdb.db: no such file or directory\r\n```\r\n\r\n### How to reproduce\r\n```\r\n# Start with fresh repo\r\nexport BACALHAU_DIR=$(mktemp -d)\r\n\r\n# run v1.4.0\r\n\u2192 bacalhau-1.4.0 version\r\n12:25:01.37 | INF pkg/repo/fs.go:99 > Initializing repo at '/var/folders/vz/pk7pk9qx1c5gbktg8nhyl88h0000gn/T/tmp.XpUn1hDbW2' for environment 'production'\r\n CLIENT  SERVER  UPDATE MESSAGE\r\n v1.4.0  v1.4.0\r\n\r\n\r\n# run v1.5.0\r\n\u2192 bacalhau-1.5.0 version\r\n12:25:27.158 | INF pkg/repo/migration.go:46 > Migrating repo from version 3 to 4\r\nError: failed to setup repo: failed to reconcile repo: failed to open repo: failed to migrate repo: rename /var/folders/vz/pk7pk9qx1c5gbktg8nhyl88h0000gn/T/tmp.XpUn1hDbW2/orchestrator_store/jobs.db /var/folders/vz/pk7pk9qx1c5gbktg8nhyl88h0000gn/T/tmp.XpUn1hDbW2/orchestrator_store/state_boltdb.db: no such file or directory\r\n```\r\n\n", "patch": "diff --git a/cmd/cli/serve/serve.go b/cmd/cli/serve/serve.go\nindex b36cadadc5..737b1c1f47 100644\n--- a/cmd/cli/serve/serve.go\n+++ b/cmd/cli/serve/serve.go\n@@ -114,6 +114,15 @@ func serve(cmd *cobra.Command, cfg types.Bacalhau, fsRepo *repo.FsRepo) error {\n \tif err != nil {\n \t\treturn fmt.Errorf(\"failed to get node name: %w\", err)\n \t}\n+\tif nodeName == \"\" {\n+\t\tnodeName, err = config.GenerateNodeID(ctx, cfg.NameProvider)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to generate node name for provider %s: %w\", cfg.NameProvider, err)\n+\t\t}\n+\t\tif err := fsRepo.WriteNodeName(nodeName); err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to write node name %s: %w\", nodeName, err)\n+\t\t}\n+\t}\n \tctx = logger.ContextWithNodeIDLogger(ctx, nodeName)\n \n \t// configure node type\ndiff --git a/pkg/config/config.go b/pkg/config/config.go\nindex 0778d098b2..c98dbf11c2 100644\n--- a/pkg/config/config.go\n+++ b/pkg/config/config.go\n@@ -1,16 +1,19 @@\n package config\n \n import (\n+\t\"context\"\n \t\"fmt\"\n \t\"os\"\n \t\"strings\"\n \n \t\"github.com/mitchellh/mapstructure\"\n \t\"github.com/rs/zerolog/log\"\n+\t\"github.com/samber/lo\"\n \t\"github.com/spf13/pflag\"\n \t\"github.com/spf13/viper\"\n \n \t\"github.com/bacalhau-project/bacalhau/pkg/config/types\"\n+\t\"github.com/bacalhau-project/bacalhau/pkg/util/idgen\"\n )\n \n const (\n@@ -249,3 +252,25 @@ func KeyAsEnvVar(key string) string {\n \t\tfmt.Sprintf(\"%s_%s\", environmentVariablePrefix, environmentVariableReplace.Replace(key)),\n \t)\n }\n+\n+func GenerateNodeID(ctx context.Context, nodeNameProviderType string) (string, error) {\n+\tnodeNameProviders := map[string]idgen.NodeNameProvider{\n+\t\t\"hostname\": idgen.HostnameProvider{},\n+\t\t\"aws\":      idgen.NewAWSNodeNameProvider(),\n+\t\t\"gcp\":      idgen.NewGCPNodeNameProvider(),\n+\t\t\"uuid\":     idgen.UUIDNodeNameProvider{},\n+\t\t\"puuid\":    idgen.PUUIDNodeNameProvider{},\n+\t}\n+\tnodeNameProvider, ok := nodeNameProviders[nodeNameProviderType]\n+\tif !ok {\n+\t\treturn \"\", fmt.Errorf(\n+\t\t\t\"unknown node name provider: %s. Supported providers are: %s\", nodeNameProviderType, lo.Keys(nodeNameProviders))\n+\t}\n+\n+\tnodeName, err := nodeNameProvider.GenerateNodeName(ctx)\n+\tif err != nil {\n+\t\treturn \"\", err\n+\t}\n+\n+\treturn nodeName, nil\n+}\ndiff --git a/pkg/repo/fs.go b/pkg/repo/fs.go\nindex 42bac4d330..0215f134b1 100644\n--- a/pkg/repo/fs.go\n+++ b/pkg/repo/fs.go\n@@ -5,17 +5,13 @@ import (\n \t\"fmt\"\n \t\"os\"\n \t\"path/filepath\"\n-\t\"time\"\n \n \t\"github.com/mitchellh/go-homedir\"\n \t\"github.com/rs/zerolog/log\"\n-\t\"github.com/samber/lo\"\n \n-\t\"github.com/bacalhau-project/bacalhau/pkg/config/types\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/config_legacy\"\n \tlegacy_types \"github.com/bacalhau-project/bacalhau/pkg/config_legacy/types\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/telemetry\"\n-\t\"github.com/bacalhau-project/bacalhau/pkg/util/idgen\"\n )\n \n const (\n@@ -85,7 +81,7 @@ func (fsr *FsRepo) Version() (int, error) {\n }\n \n // Init initializes a new repo, returning an error if the repo already exists.\n-func (fsr *FsRepo) Init(cfg types.Bacalhau) error {\n+func (fsr *FsRepo) Init() error {\n \tif exists, err := fsr.Exists(); err != nil {\n \t\treturn err\n \t} else if exists {\n@@ -94,14 +90,6 @@ func (fsr *FsRepo) Init(cfg types.Bacalhau) error {\n \n \tlog.Info().Msgf(\"Initializing repo at %s\", fsr.path)\n \n-\t// if it takes longer than 5 seconds to get the node name from a provider, fail\n-\tnameCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n-\tdefer cancel()\n-\tnodeName, err := getNodeID(nameCtx, cfg.NameProvider)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n \t// 0755: Owner can read, write, execute. Others can read and execute.\n \tif err := os.MkdirAll(fsr.path, repoPermission); err != nil && !os.IsExist(err) {\n \t\treturn err\n@@ -114,9 +102,6 @@ func (fsr *FsRepo) Init(cfg types.Bacalhau) error {\n \t\treturn fmt.Errorf(\"failed to persist repo version: %w\", err)\n \t}\n \n-\tif err := fsr.WriteNodeName(nodeName); err != nil {\n-\t\treturn fmt.Errorf(\"failed to persist node name: %w\", err)\n-\t}\n \treturn nil\n }\n \n@@ -216,25 +201,3 @@ func (fsr *FsRepo) EnsureRepoPathsConfigured(c config_legacy.ReadWriter) {\n func (fsr *FsRepo) join(paths ...string) string {\n \treturn filepath.Join(append([]string{fsr.path}, paths...)...)\n }\n-\n-func getNodeID(ctx context.Context, nodeNameProviderType string) (string, error) {\n-\tnodeNameProviders := map[string]idgen.NodeNameProvider{\n-\t\t\"hostname\": idgen.HostnameProvider{},\n-\t\t\"aws\":      idgen.NewAWSNodeNameProvider(),\n-\t\t\"gcp\":      idgen.NewGCPNodeNameProvider(),\n-\t\t\"uuid\":     idgen.UUIDNodeNameProvider{},\n-\t\t\"puuid\":    idgen.PUUIDNodeNameProvider{},\n-\t}\n-\tnodeNameProvider, ok := nodeNameProviders[nodeNameProviderType]\n-\tif !ok {\n-\t\treturn \"\", fmt.Errorf(\n-\t\t\t\"unknown node name provider: %s. Supported providers are: %s\", nodeNameProviderType, lo.Keys(nodeNameProviders))\n-\t}\n-\n-\tnodeName, err := nodeNameProvider.GenerateNodeName(ctx)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\treturn nodeName, nil\n-}\ndiff --git a/pkg/repo/migrations/v3_4.go b/pkg/repo/migrations/v3_4.go\nindex d212d63379..93fb20c4cc 100644\n--- a/pkg/repo/migrations/v3_4.go\n+++ b/pkg/repo/migrations/v3_4.go\n@@ -116,7 +116,7 @@ var V3Migration = repo.NewMigration(\n \t\t\t\tuserConfigDir, err := os.UserConfigDir()\n \t\t\t\tif err == nil {\n \t\t\t\t\tnewConfigDir := filepath.Join(userConfigDir, \"bacalhau\")\n-\t\t\t\t\tif err := os.Mkdir(newConfigDir, util.OS_USER_RWX); err != nil {\n+\t\t\t\t\tif err := os.MkdirAll(newConfigDir, util.OS_USER_RWX); err != nil {\n \t\t\t\t\t\treturn err\n \t\t\t\t\t}\n \t\t\t\t\tnewConfigFilePath := filepath.Join(newConfigDir, config_legacy.FileName)\n@@ -136,6 +136,9 @@ var V3Migration = repo.NewMigration(\n \t\t\t\t\t\treturn err\n \t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t} else if !os.IsNotExist(err) {\n+\t\t\t\t// if there was an error other than the file not existing, abort.\n+\t\t\t\treturn fmt.Errorf(\"failed to read config file %s while migrating: %w\", oldConfigFilePath, err)\n \t\t\t}\n \n \t\t}\n@@ -149,11 +152,15 @@ func migrateComputeStore(repoPath string, config legacy_types.JobStoreConfig) er\n \tif oldExecutionStorePath == \"\" {\n \t\toldExecutionStorePath = filepath.Join(oldComputeDir, \"executions.db\")\n \t}\n-\tnewExecutionStorePath := filepath.Join(oldComputeDir, \"state_boltdb.db\")\n-\tif err := os.Rename(oldExecutionStorePath, newExecutionStorePath); err != nil {\n+\t// if a store is present migrate it.\n+\tif _, err := os.Stat(oldExecutionStorePath); err == nil {\n+\t\tnewExecutionStorePath := filepath.Join(oldComputeDir, \"state_boltdb.db\")\n+\t\tif err := os.Rename(oldExecutionStorePath, newExecutionStorePath); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t} else if !os.IsNotExist(err) {\n \t\treturn err\n \t}\n-\n \tnewComputeDir := filepath.Join(repoPath, types.ComputeDirName)\n \tif err := os.Rename(oldComputeDir, newComputeDir); err != nil {\n \t\treturn err\n@@ -173,11 +180,15 @@ func migrateOrchestratorStore(repoPath string, config legacy_types.JobStoreConfi\n \tif oldJobStorePath == \"\" {\n \t\toldJobStorePath = filepath.Join(oldOrchestratorDir, \"jobs.db\")\n \t}\n-\tnewJobStorePath := filepath.Join(oldOrchestratorDir, \"state_boltdb.db\")\n-\tif err := os.Rename(oldJobStorePath, newJobStorePath); err != nil {\n+\t// if a store is present migrate it.\n+\tif _, err := os.Stat(oldJobStorePath); err == nil {\n+\t\tnewJobStorePath := filepath.Join(oldOrchestratorDir, \"state_boltdb.db\")\n+\t\tif err := os.Rename(oldJobStorePath, newJobStorePath); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t} else if !os.IsNotExist(err) {\n \t\treturn err\n \t}\n-\n \tnewOrchestratorDir := filepath.Join(repoPath, types.OrchestratorDirName)\n \tif err := os.Rename(oldOrchestratorDir, newOrchestratorDir); err != nil {\n \t\treturn err\ndiff --git a/pkg/setup/setup.go b/pkg/setup/setup.go\nindex a266fb11f8..d5c98c3f02 100644\n--- a/pkg/setup/setup.go\n+++ b/pkg/setup/setup.go\n@@ -46,7 +46,7 @@ func SetupBacalhauRepo(cfg types.Bacalhau) (*repo.FsRepo, error) {\n \n \t\treturn nil, fmt.Errorf(\"failed to check if repo exists: %w\", err)\n \t} else if !exists {\n-\t\tif err := fsRepo.Init(cfg); err != nil {\n+\t\tif err := fsRepo.Init(); err != nil {\n \t\t\treturn nil, fmt.Errorf(\"failed to initialize repo: %w\", err)\n \t\t}\n \t} else {\n", "instance_id": "bacalhau-project__bacalhau-4398", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a repository migration from version 1.4.0 to 1.5.0 fails with a specific error related to a file rename operation. It provides steps to reproduce the issue, which is helpful for understanding the context and verifying the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior after migration or the specific conditions under which the error occurs (e.g., whether the file `jobs.db` is expected to exist in all cases). Additionally, it lacks clarity on the broader impact of the migration failure (e.g., does it affect specific functionalities or data integrity?). While the error message and reproduction steps provide a good starting point, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`serve.go`, `config.go`, `fs.go`, `migrations/v3_4.go`, `setup.go`), indicating a need to understand interactions across different parts of the codebase. The changes primarily address file renaming logic during migration and node ID generation, requiring modifications to error handling and initialization logic. Second, the technical concepts involved include file system operations (e.g., `os.Rename`, `os.Stat`), configuration management, and migration logic, which are moderately complex but not overly advanced. Third, the problem requires handling specific edge cases, such as when files do not exist during migration, as evidenced by the added checks for file existence before renaming. However, the changes do not appear to impact the core architecture significantly, nor do they involve highly intricate algorithms or domain-specific knowledge beyond typical file system and configuration handling in Go. Overall, this problem requires a solid understanding of the codebase and careful handling of edge cases, but it does not reach the level of hard or very hard due to the absence of deep architectural changes or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add the IEA-22MW turbine\nUsing the geometry and mass from https://github.com/IEAWindTask37/IEA-22-280-RWT, create a 22MW_generic turbine yaml file and add to the library. Update any tests along the way to ensure that the size/scale of the turbine does not raise any warnings/errors.\n", "patch": "diff --git a/library/turbines/22MW_generic.yaml b/library/turbines/22MW_generic.yaml\nnew file mode 100644\nindex 00000000..7e6639e5\n--- /dev/null\n+++ b/library/turbines/22MW_generic.yaml\n@@ -0,0 +1,22 @@\n+# Most Data comes from IEA Wind 22-MW Reference Turbine (IEA-22MW-RWT)\n+blade:\n+  deck_space: 744.12      # m^2 [IEA-22MW-RWT Value] assuming area = blade length * max chord * 0.75 (assume 25% overhang) = 137.8 * 7.2 * 0.75\n+  length: 137.8          # m tonnes [IEA-22MW-RWT Value]\n+  type: Blade\n+  mass: 82.301         # tonnes [IEA-22MW-RWT Value]\n+hub_height: 170        # m [IEA-22MW-RWT Value]\n+nacelle:\n+  deck_space: 198      # m^2 [IEA-22MW-RWT Value] \"The outer dimenesions of the nacelle are estimated equal to 18 m in length, 11 m in width, and 11 m in height\" area = 18 * 11\n+  type: Nacelle\n+  mass: 941.2        # tonnes [IEA-22MW-RWT Value] Nacelle assembly mass + Hub system mass = 120.0 +  821.2\n+name: 22MW_generic\n+rated_windspeed: 11.00 # m/s [IEA-22MW-RWT Value]\n+rotor_diameter: 284   # m [IEA-22MW-RWT Value]\n+tower:\n+  deck_space: 78.54    # m^2 [IEA-22MW-RWT Value] and assuming erected tower, deck space = tower base = pi * max outer tower diameter^2 /4 = pi * 10^2 / 4\n+  sections: 3        # guess\n+  type: Tower\n+  length: 164.4        # m [IEA-22MW-RWT Value] Hub height - Vertical distance between tower top and hub center\n+  mass: 1574         # tonnes [IEA-22MW-RWT Value]\n+turbine_rating: 22     # MW \n+\n", "instance_id": "WISDEM__ORBIT-152", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: to add a new turbine configuration (IEA-22MW) to a library using provided data from a referenced GitHub repository and ensure that tests pass without warnings or errors related to the turbine's size/scale. The goal is straightforward, and the code changes provided align with the task of creating a new YAML file with specific turbine parameters. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the exact structure or schema of the YAML file (though it can be inferred from the code changes), nor does it mention how the tests should be updated or what specific warnings/errors to watch for. Additionally, there is no mention of edge cases or constraints related to the turbine data (e.g., acceptable ranges for values like mass or rotor diameter). While the referenced GitHub repository provides a source for the data, it is not clear if any transformation or validation of the data is required beyond direct transcription. Overall, the statement is valid and mostly clear but lacks some minor details that could affect implementation.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is limited to creating a single new YAML file with predefined values sourced from a reference repository. There is no indication of complex logic or computation required to derive these values, as the data appears to be directly transcribed with minor calculations or assumptions (e.g., deck space calculations). The impact on the codebase is minimal, as it does not involve modifying existing files (beyond potentially updating tests, which is not detailed in the provided diff) or altering the system's architecture. The number of technical concepts required is low; it primarily involves understanding YAML file structure and possibly some domain-specific knowledge about wind turbines (e.g., interpreting terms like \"deck_space\" or \"nacelle\"), but no advanced programming language features, algorithms, or design patterns are needed. The problem statement mentions updating tests, but since no specific test modifications are shown in the diff, I assume this involves minimal effort, such as ensuring the new turbine data passes existing validation checks. Edge cases and error handling are not explicitly mentioned in the problem statement, and the code changes do not suggest any complex error handling logic is required. Overall, this task requires basic familiarity with the codebase structure and minimal effort to implement, making it an easy problem for a developer with moderate experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Problem using new immutable API with Slab-based DOM tree\nI am using a `slab::Slab` to store nodes for my DOM implementation. The new immutable API in html5ever 0.28 has required me to wrap my node tree in a `RefCell` so that I can mutate it from html5ever's trait methods. However, html5ever's `ExpandedName<'a>` type also requires me to return a (plain) reference into my tree, which is not possible if my tree is wrapped in a `RefCell`.\r\n\r\nI believe the best fix for this (assuming that the new interior mutability-based API is indeed necessary - it seems awkward!) is to make `ExpandedName` an associated type on the `TreeSink` trait which implements `AsRef<ExpandedName>` or alternatively has methods to access the fields of ExpandedName individually. This would allow implementers of `TreeSink` to choose one of the following strategies:\r\n\r\n- Use the existing ExpandedName type if their tree supports it.\r\n- Use a variant of ExpandedName that uses a RefCell `Ref` (or `MutexGuard`, etc)\r\n- Return cloned/owned names\r\n\r\nI would be happy to put up a PR for this if this seems like a reasonable solution. ~I have not yet completed my migration, so I am unsure if I will run into further problems.~ I have now migrated the rest of the code. This seems to be the only thing I couldn't solve with a `RefCell`.\n", "patch": "diff --git a/html5ever/examples/arena.rs b/html5ever/examples/arena.rs\nindex cfdb0416..36b790d3 100644\n--- a/html5ever/examples/arena.rs\n+++ b/html5ever/examples/arena.rs\n@@ -12,7 +12,7 @@ extern crate typed_arena;\n \n use html5ever::interface::tree_builder::{ElementFlags, NodeOrText, QuirksMode, TreeSink};\n use html5ever::tendril::{StrTendril, TendrilSink};\n-use html5ever::{parse_document, Attribute, ExpandedName, QualName};\n+use html5ever::{parse_document, Attribute, QualName};\n use std::borrow::Cow;\n use std::cell::{Cell, RefCell};\n use std::collections::HashSet;\n@@ -183,6 +183,7 @@ impl<'arena> Sink<'arena> {\n impl<'arena> TreeSink for Sink<'arena> {\n     type Handle = Ref<'arena>;\n     type Output = Ref<'arena>;\n+    type ElemName<'a> = &'a QualName where Self : 'a;\n \n     fn finish(self) -> Ref<'arena> {\n         self.document\n@@ -202,9 +203,9 @@ impl<'arena> TreeSink for Sink<'arena> {\n         ptr::eq::<Node>(*x, *y)\n     }\n \n-    fn elem_name<'a>(&self, target: &'a Ref<'arena>) -> ExpandedName<'a> {\n+    fn elem_name(&self, target: &Ref<'arena>) -> Self::ElemName<'_> {\n         match target.data {\n-            NodeData::Element { ref name, .. } => name.expanded(),\n+            NodeData::Element { ref name, .. } => name,\n             _ => panic!(\"not an element!\"),\n         }\n     }\ndiff --git a/html5ever/examples/noop-tree-builder.rs b/html5ever/examples/noop-tree-builder.rs\nindex ab7fc531..ee09155a 100644\n--- a/html5ever/examples/noop-tree-builder.rs\n+++ b/html5ever/examples/noop-tree-builder.rs\n@@ -40,6 +40,7 @@ impl Sink {\n impl TreeSink for Sink {\n     type Handle = usize;\n     type Output = Self;\n+    type ElemName<'a> = ExpandedName<'a>;\n     fn finish(self) -> Self {\n         self\n     }\ndiff --git a/html5ever/examples/print-tree-actions.rs b/html5ever/examples/print-tree-actions.rs\nindex 104d6444..cae76cfc 100644\n--- a/html5ever/examples/print-tree-actions.rs\n+++ b/html5ever/examples/print-tree-actions.rs\n@@ -11,7 +11,7 @@\n extern crate html5ever;\n \n use std::borrow::Cow;\n-use std::cell::{Cell, RefCell};\n+use std::cell::{Cell, Ref, RefCell};\n use std::collections::HashMap;\n use std::io;\n \n@@ -20,7 +20,7 @@ use html5ever::tendril::*;\n use html5ever::tree_builder::{\n     AppendNode, AppendText, ElementFlags, NodeOrText, QuirksMode, TreeSink,\n };\n-use html5ever::{Attribute, ExpandedName, QualName};\n+use html5ever::{Attribute, QualName};\n \n struct Sink {\n     next_id: Cell<usize>,\n@@ -38,6 +38,7 @@ impl Sink {\n impl TreeSink for Sink {\n     type Handle = usize;\n     type Output = Self;\n+    type ElemName<'a> = Ref<'a, QualName>;\n     fn finish(self) -> Self {\n         self\n     }\n@@ -68,13 +69,10 @@ impl TreeSink for Sink {\n         x == y\n     }\n \n-    fn elem_name(&self, target: &usize) -> ExpandedName {\n-        self.names\n-            .borrow()\n-            .get(target)\n-            .cloned()\n-            .expect(\"not an element\")\n-            .expanded()\n+    fn elem_name(&self, target: &usize) -> Self::ElemName<'_> {\n+        Ref::map(self.names.borrow(), |map| {\n+            *map.get(target).expect(\"not an element\")\n+        })\n     }\n \n     fn create_element(&self, name: QualName, _: Vec<Attribute>, _: ElementFlags) -> usize {\ndiff --git a/html5ever/src/tree_builder/mod.rs b/html5ever/src/tree_builder/mod.rs\nindex 49dfc062..348c211d 100644\n--- a/html5ever/src/tree_builder/mod.rs\n+++ b/html5ever/src/tree_builder/mod.rs\n@@ -9,7 +9,9 @@\n \n //! The HTML5 tree builder.\n \n-pub use crate::interface::{create_element, ElementFlags, NextParserState, Tracer, TreeSink};\n+pub use crate::interface::{\n+    create_element, ElemName, ElementFlags, NextParserState, Tracer, TreeSink,\n+};\n pub use crate::interface::{AppendNode, AppendText, Attribute, NodeOrText};\n pub use crate::interface::{LimitedQuirks, NoQuirks, Quirks, QuirksMode};\n \n@@ -187,7 +189,8 @@ where\n         opts: TreeBuilderOpts,\n     ) -> TreeBuilder<Handle, Sink> {\n         let doc_handle = sink.get_document();\n-        let context_is_template = sink.elem_name(&context_elem) == expanded_name!(html \"template\");\n+        let context_is_template =\n+            sink.elem_name(&context_elem).expanded() == expanded_name!(html \"template\");\n         let template_modes = if context_is_template {\n             RefCell::new(vec![InTemplate])\n         } else {\n@@ -231,7 +234,8 @@ where\n     pub fn tokenizer_state_for_context_elem(&self) -> tok_state::State {\n         let context_elem = self.context_elem.borrow();\n         let elem = context_elem.as_ref().expect(\"no context element\");\n-        let name = match self.sink.elem_name(elem) {\n+        let elem_name = self.sink.elem_name(elem);\n+        let name = match elem_name.expanded() {\n             ExpandedName {\n                 ns: &ns!(html),\n                 local,\n@@ -296,8 +300,8 @@ where\n         print!(\"    open_elems:\");\n         for node in self.open_elems.borrow().iter() {\n             let name = self.sink.elem_name(node);\n-            match *name.ns {\n-                ns!(html) => print!(\" {}\", name.local),\n+            match *name.ns() {\n+                ns!(html) => print!(\" {}\", name.local_name()),\n                 _ => panic!(),\n             }\n         }\n@@ -308,8 +312,8 @@ where\n                 &Marker => print!(\" Marker\"),\n                 Element(h, _) => {\n                     let name = self.sink.elem_name(h);\n-                    match *name.ns {\n-                        ns!(html) => print!(\" {}\", name.local),\n+                    match *name.ns() {\n+                        ns!(html) => print!(\" {}\", name.local_name()),\n                         _ => panic!(),\n                     }\n                 },\n@@ -541,7 +545,7 @@ where\n \n     fn adjusted_current_node_present_but_not_in_html_namespace(&self) -> bool {\n         !self.open_elems.borrow().is_empty()\n-            && self.sink.elem_name(&self.adjusted_current_node()).ns != &ns!(html)\n+            && *self.sink.elem_name(&self.adjusted_current_node()).ns() != ns!(html)\n     }\n }\n \n@@ -689,7 +693,7 @@ where\n     where\n         TagSet: Fn(ExpandedName) -> bool,\n     {\n-        set(self.sink.elem_name(&self.current_node()))\n+        set(self.sink.elem_name(&self.current_node()).expanded())\n     }\n \n     // Insert at the \"appropriate place for inserting a node\".\n@@ -1014,7 +1018,8 @@ where\n         for elem in self.open_elems.borrow().iter() {\n             let error;\n             {\n-                let name = self.sink.elem_name(elem);\n+                let elem_name = self.sink.elem_name(elem);\n+                let name = elem_name.expanded();\n                 if body_end_ok(name) {\n                     continue;\n                 }\n@@ -1041,7 +1046,7 @@ where\n             if pred(node.clone()) {\n                 return true;\n             }\n-            if scope(self.sink.elem_name(node)) {\n+            if scope(self.sink.elem_name(node).expanded()) {\n                 return false;\n             }\n         }\n@@ -1055,12 +1060,12 @@ where\n     where\n         TagSet: Fn(ExpandedName) -> bool,\n     {\n-        set(self.sink.elem_name(elem))\n+        set(self.sink.elem_name(elem).expanded())\n     }\n \n     fn html_elem_named(&self, elem: &Handle, name: LocalName) -> bool {\n-        let expanded = self.sink.elem_name(elem);\n-        *expanded.ns == ns!(html) && *expanded.local == name\n+        let elem_name = self.sink.elem_name(elem);\n+        *elem_name.ns() == ns!(html) && *elem_name.local_name() == name\n     }\n \n     fn in_html_elem_named(&self, name: LocalName) -> bool {\n@@ -1090,8 +1095,8 @@ where\n             {\n                 let open_elems = self.open_elems.borrow();\n                 let elem = unwrap_or_return!(open_elems.last());\n-                let nsname = self.sink.elem_name(elem);\n-                if !set(nsname) {\n+                let elem_name = self.sink.elem_name(elem);\n+                if !set(elem_name.expanded()) {\n                     return;\n                 }\n             }\n@@ -1132,7 +1137,7 @@ where\n             match self.open_elems.borrow_mut().pop() {\n                 None => break,\n                 Some(elem) => {\n-                    if pred(self.sink.elem_name(&elem)) {\n+                    if pred(self.sink.elem_name(&elem).expanded()) {\n                         break;\n                     }\n                 },\n@@ -1217,7 +1222,8 @@ where\n             if let (true, Some(ctx)) = (last, context_elem.as_ref()) {\n                 node = ctx;\n             }\n-            let name = match self.sink.elem_name(node) {\n+            let elem_name = self.sink.elem_name(node);\n+            let name = match elem_name.expanded() {\n                 ExpandedName {\n                     ns: &ns!(html),\n                     local,\n@@ -1470,7 +1476,8 @@ where\n         }\n \n         let current = self.adjusted_current_node();\n-        let name = self.sink.elem_name(&current);\n+        let elem_name = self.sink.elem_name(&current);\n+        let name = elem_name.expanded();\n         if let ns!(html) = *name.ns {\n             return false;\n         }\n@@ -1681,7 +1688,7 @@ where\n         let current_ns = self\n             .sink\n             .elem_name(&self.adjusted_current_node())\n-            .ns\n+            .ns()\n             .clone();\n         match current_ns {\n             ns!(mathml) => self.adjust_mathml_attributes(&mut tag),\ndiff --git a/html5ever/src/tree_builder/rules.rs b/html5ever/src/tree_builder/rules.rs\nindex a8e8d610..9bce3570 100644\n--- a/html5ever/src/tree_builder/rules.rs\n+++ b/html5ever/src/tree_builder/rules.rs\n@@ -9,15 +9,18 @@\n \n // The tree builder rules, as a single, enormous nested match expression.\n \n-use markup5ever::{expanded_name, local_name, namespace_prefix, namespace_url, ns};\n+use crate::interface::Quirks;\n use crate::tokenizer::states::{Plaintext, Rawtext, Rcdata, ScriptData};\n+use crate::tokenizer::TagKind::{EndTag, StartTag};\n use crate::tree_builder::tag_sets::*;\n use crate::tree_builder::types::*;\n+use crate::tree_builder::{\n+    create_element, html_elem, ElemName, NodeOrText::AppendNode, StrTendril, Tag, TreeBuilder,\n+    TreeSink,\n+};\n use crate::QualName;\n-use crate::tree_builder::{create_element, html_elem, TreeSink, Tag, NodeOrText::AppendNode, StrTendril, TreeBuilder};\n-use crate::tokenizer::TagKind::{StartTag, EndTag};\n+use markup5ever::{expanded_name, local_name, namespace_prefix, namespace_url, ns};\n use std::borrow::Cow::Borrowed;\n-use crate::interface::Quirks;\n \n use std::borrow::ToOwned;\n \n@@ -402,7 +405,8 @@ where\n \n                     let mut to_close = None;\n                     for node in self.open_elems.borrow().iter().rev() {\n-                        let name = self.sink.elem_name(node);\n+                        let elem_name = self.sink.elem_name(node);\n+                        let name = elem_name.expanded();\n                         let can_close = if list {\n                             close_list(name)\n                         } else {\n@@ -1441,8 +1445,8 @@ where\n                     {\n                         let open_elems = self.open_elems.borrow();\n                         let node_name = self.sink.elem_name(&open_elems[stack_idx]);\n-                        html = *node_name.ns == ns!(html);\n-                        eq = node_name.local.eq_ignore_ascii_case(&tag.name);\n+                        html = *node_name.ns() == ns!(html);\n+                        eq = node_name.local_name().eq_ignore_ascii_case(&tag.name);\n                     }\n                     if !first && html {\n                         let mode = self.mode.get();\ndiff --git a/html5ever/src/tree_builder/types.rs b/html5ever/src/tree_builder/types.rs\nindex da2bf992..f6bb588d 100644\n--- a/html5ever/src/tree_builder/types.rs\n+++ b/html5ever/src/tree_builder/types.rs\n@@ -58,6 +58,7 @@ pub(crate) enum SplitStatus {\n /// A subset/refinement of `tokenizer::Token`.  Everything else is handled\n /// specially at the beginning of `process_token`.\n #[derive(PartialEq, Eq, Clone, Debug)]\n+#[allow(clippy::enum_variant_names)]\n pub(crate) enum Token {\n     TagToken(Tag),\n     CommentToken(StrTendril),\ndiff --git a/markup5ever/interface/mod.rs b/markup5ever/interface/mod.rs\nindex 0cac03db..d036d47b 100644\n--- a/markup5ever/interface/mod.rs\n+++ b/markup5ever/interface/mod.rs\n@@ -8,12 +8,13 @@\n // except according to those terms.\n //! Types for tag and attribute names, and tree-builder functionality.\n \n+use std::cell::Ref;\n use std::fmt;\n use tendril::StrTendril;\n \n pub use self::tree_builder::{create_element, AppendNode, AppendText, ElementFlags, NodeOrText};\n+pub use self::tree_builder::{ElemName, NextParserState, Tracer, TreeSink};\n pub use self::tree_builder::{LimitedQuirks, NoQuirks, Quirks, QuirksMode};\n-pub use self::tree_builder::{NextParserState, Tracer, TreeSink};\n use super::{LocalName, Namespace, Prefix};\n \n /// An [expanded name], containing the tag and the namespace.\n@@ -25,6 +26,30 @@ pub struct ExpandedName<'a> {\n     pub local: &'a LocalName,\n }\n \n+impl<'a> ElemName for ExpandedName<'a> {\n+    #[inline(always)]\n+    fn ns(&self) -> &Namespace {\n+        self.ns\n+    }\n+\n+    #[inline(always)]\n+    fn local_name(&self) -> &LocalName {\n+        self.local\n+    }\n+}\n+\n+impl<'a> ElemName for Ref<'a, ExpandedName<'a>> {\n+    #[inline(always)]\n+    fn ns(&self) -> &Namespace {\n+        self.ns\n+    }\n+\n+    #[inline(always)]\n+    fn local_name(&self) -> &LocalName {\n+        self.local\n+    }\n+}\n+\n impl<'a> fmt::Debug for ExpandedName<'a> {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n         if self.ns.is_empty() {\n@@ -241,6 +266,30 @@ pub struct QualName {\n     pub local: LocalName,\n }\n \n+impl<'a> ElemName for Ref<'a, QualName> {\n+    #[inline(always)]\n+    fn ns(&self) -> &Namespace {\n+        &self.ns\n+    }\n+\n+    #[inline(always)]\n+    fn local_name(&self) -> &LocalName {\n+        &self.local\n+    }\n+}\n+\n+impl<'a> ElemName for &'a QualName {\n+    #[inline(always)]\n+    fn ns(&self) -> &Namespace {\n+        &self.ns\n+    }\n+\n+    #[inline(always)]\n+    fn local_name(&self) -> &LocalName {\n+        &self.local\n+    }\n+}\n+\n impl QualName {\n     /// Basic constructor function.\n     ///\ndiff --git a/markup5ever/interface/tree_builder.rs b/markup5ever/interface/tree_builder.rs\nindex a0d24265..f1a30a8e 100644\n--- a/markup5ever/interface/tree_builder.rs\n+++ b/markup5ever/interface/tree_builder.rs\n@@ -12,7 +12,9 @@\n //! It can be used by a parser to create the DOM graph structure in memory.\n \n use crate::interface::{Attribute, ExpandedName, QualName};\n+use crate::{LocalName, Namespace};\n use std::borrow::Cow;\n+use std::fmt::Debug;\n use tendril::StrTendril;\n \n pub use self::NodeOrText::{AppendNode, AppendText};\n@@ -97,6 +99,20 @@ where\n     sink.create_element(name, attrs, flags)\n }\n \n+/// An abstraction over any type that can represent an element's local name and namespace.\n+pub trait ElemName: Debug {\n+    fn ns(&self) -> &Namespace;\n+    fn local_name(&self) -> &LocalName;\n+\n+    #[inline(always)]\n+    fn expanded(&self) -> ExpandedName {\n+        ExpandedName {\n+            ns: self.ns(),\n+            local: self.local_name(),\n+        }\n+    }\n+}\n+\n /// Methods a parser can use to create the DOM. The DOM provider implements this trait.\n ///\n /// Having this as a trait potentially allows multiple implementations of the DOM to be used with\n@@ -113,6 +129,11 @@ pub trait TreeSink {\n     /// [rust-lang/rust#29661](https://github.com/rust-lang/rust/issues/29661)\n     type Output;\n \n+    //\n+    type ElemName<'a>: ElemName\n+    where\n+        Self: 'a;\n+\n     /// Consume this sink and return the overall result of parsing.\n     ///\n     /// TODO:This should default to `fn finish(self) -> Self::Output { self }`,\n@@ -130,7 +151,7 @@ pub trait TreeSink {\n     ///\n     /// Should never be called on a non-element node;\n     /// feel free to `panic!`.\n-    fn elem_name<'a>(&'a self, target: &'a Self::Handle) -> ExpandedName<'a>;\n+    fn elem_name<'a>(&'a self, target: &'a Self::Handle) -> Self::ElemName<'a>;\n \n     /// Create an element.\n     ///\ndiff --git a/markup5ever/serialize.rs b/markup5ever/serialize.rs\nindex e65e998f..e91cb264 100644\n--- a/markup5ever/serialize.rs\n+++ b/markup5ever/serialize.rs\n@@ -6,7 +6,9 @@\n // <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n-//! Traits for serializing elements. The serializer expects the data to be xml-like (with a name,\n+//! Traits for serializing elements.\n+//!\n+//! The serializer expects the data to be xml-like (with a name,\n //! and optional children, attrs, text, comments, doctypes, and [processing instructions]). It uses\n //! the visitor pattern, where the serializer and the serializable objects are decoupled and\n //! implement their own traits.\ndiff --git a/rcdom/lib.rs b/rcdom/lib.rs\nindex 45553cc1..96f26220 100644\n--- a/rcdom/lib.rs\n+++ b/rcdom/lib.rs\n@@ -225,6 +225,8 @@ impl TreeSink for RcDom {\n \n     type Handle = Handle;\n \n+    type ElemName<'a> = ExpandedName<'a> where Self : 'a;\n+\n     fn parse_error(&self, msg: Cow<'static, str>) {\n         self.errors.borrow_mut().push(msg);\n     }\ndiff --git a/xml5ever/src/tree_builder/mod.rs b/xml5ever/src/tree_builder/mod.rs\nindex b76eff0b..19427277 100644\n--- a/xml5ever/src/tree_builder/mod.rs\n+++ b/xml5ever/src/tree_builder/mod.rs\n@@ -20,7 +20,7 @@ use std::collections::{BTreeMap, HashSet, VecDeque};\n use std::fmt::{Debug, Error, Formatter};\n use std::mem;\n \n-pub use self::interface::{NextParserState, NodeOrText, Tracer, TreeSink};\n+pub use self::interface::{ElemName, NextParserState, NodeOrText, Tracer, TreeSink};\n use self::types::*;\n use crate::interface::{self, create_element, AppendNode, Attribute, QualName};\n use crate::interface::{AppendText, ExpandedName};\n@@ -535,7 +535,7 @@ where\n         self.open_elems\n             .borrow()\n             .iter()\n-            .any(|a| self.sink.elem_name(a) == tag.name.expanded())\n+            .any(|a| self.sink.elem_name(a).expanded() == tag.name.expanded())\n     }\n \n     // Pop elements until an element from the set has been popped.  Returns the\n@@ -557,7 +557,7 @@ where\n         TagSet: Fn(ExpandedName) -> bool,\n     {\n         // FIXME: take namespace into consideration:\n-        set(self.sink.elem_name(&self.current_node()))\n+        set(self.sink.elem_name(&self.current_node()).expanded())\n     }\n \n     fn close_tag(&self, tag: Tag) -> XmlProcessResult {\n@@ -567,7 +567,7 @@ where\n             &tag.name\n         );\n \n-        if *self.sink.elem_name(&self.current_node()).local != tag.name.local {\n+        if *self.sink.elem_name(&self.current_node()).local_name() != tag.name.local {\n             self.sink\n                 .parse_error(Borrowed(\"Current node doesn't match tag\"));\n         }\n", "instance_id": "servo__html5ever-553", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the new immutable API in html5ever 0.28 and the conflict arising from using `RefCell` with `ExpandedName`. The goal is evident: to modify the `TreeSink` trait to allow flexibility in how `ExpandedName` is handled by making it an associated type. The statement also outlines potential strategies for implementers, which adds clarity to the intent. However, there are minor ambiguities and missing details. For instance, it does not explicitly discuss potential downstream impacts or constraints of the proposed solution (e.g., compatibility with existing implementations or performance implications). Additionally, edge cases or specific scenarios where the issue manifests are not detailed, which could leave room for interpretation. Overall, the problem is valid and mostly clear, but it lacks some depth in constraints and examples, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files and examples within the html5ever and markup5ever libraries, as well as affecting core traits like `TreeSink`. This requires a deep understanding of the library's architecture and how changes to the trait system impact various implementations. Second, the technical concepts involved are advanced, including Rust's trait system, associated types, lifetime management, and interior mutability patterns (e.g., `RefCell`, `Ref`). Additionally, familiarity with DOM tree structures and html5ever's parsing model is necessary, which adds domain-specific complexity. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest the need to handle different representations of element names (e.g., owned vs. borrowed data), which introduces subtle error-handling and compatibility concerns. Finally, the changes impact the system's core API, requiring careful consideration of backward compatibility and potential performance implications. A score of 0.75 reflects the need for deep Rust expertise, library-specific knowledge, and the complexity of modifying a critical component with wide-reaching effects, though it does not reach the \"Very Hard\" threshold as it does not involve system-level or highly intricate domain problems like distributed systems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "The computed duration of `erfsquare` waveform templates is incorrect\n`ScheduledBasicBlock::get_waveform_duration_seconds` assumes that the duration of all built-in waveforms is equal to their `duration` parameter.  This isn't true for `erfsquare`, though; the duration is just that of the main pulse, and the `padleft` and `padright` parameters specify silent padding on either side, which contributes to the total duration.\r\n\r\nTo see this, consider the following program:\r\n\r\n```\r\nDEFFRAME 0 \"a\":\r\n    SAMPLE-RATE: 1e9\r\nPULSE 0 \"a\" erfsquare(duration: 1.0, padleft: 0.2, padright: 0.3)\r\nPULSE 0 \"a\" erfsquare(duration: 0.1, padleft: 0.7, padright: 0.7)\r\nPULSE 0 \"a\" erfsquare(duration: 0.5, padleft: 0.6, padright: 0.4)\r\nFENCE\r\n```\r\n\r\nIf provided as a test case to `program::schedule::tests::schedule_seconds`, it will produce instruction timings of `[0.0, 1.0, 1.1, 1.6]`, when it should instead produce `[0.0, 1.5, 3.0, 4.5]`.\n", "patch": "diff --git a/quil-rs/src/program/analysis/control_flow_graph.rs b/quil-rs/src/program/analysis/control_flow_graph.rs\nindex 98f60170..1128d9d0 100644\n--- a/quil-rs/src/program/analysis/control_flow_graph.rs\n+++ b/quil-rs/src/program/analysis/control_flow_graph.rs\n@@ -745,8 +745,8 @@ DEFCAL CZ q0 q1:\n     FENCE q0 q1\n     SET-PHASE q0 \"flux_tx_cz\" 0.0\n     SET-PHASE q1 \"flux_tx_iswap\" 0.0\n-    NONBLOCKING PULSE q0 \"flux_tx_cz\" erf_square(duration: 6.000000000000001e-08)\n-    NONBLOCKING PULSE q1 \"flux_tx_iswap\" erf_square(duration: 6.000000000000001e-08)\n+    NONBLOCKING PULSE q0 \"flux_tx_cz\" erf_square(duration: 6.000000000000001e-08, pad_left: 0, pad_right: 0)\n+    NONBLOCKING PULSE q1 \"flux_tx_iswap\" erf_square(duration: 6.000000000000001e-08, pad_left: 0, pad_right: 0)\n     SHIFT-PHASE q0 \"flux_tx_cz\" 1.0\n     SHIFT-PHASE q1 \"flux_tx_iswap\" 1.0\n     FENCE q0 q1\ndiff --git a/quil-rs/src/program/scheduling/schedule.rs b/quil-rs/src/program/scheduling/schedule.rs\nindex 58d762c9..ac848fc0 100644\n--- a/quil-rs/src/program/scheduling/schedule.rs\n+++ b/quil-rs/src/program/scheduling/schedule.rs\n@@ -210,9 +210,9 @@ impl<'p> ScheduledBasicBlock<'p> {\n     fn get_waveform_duration_seconds(\n         program: &Program,\n         instruction: &Instruction,\n-        waveform_invocation: &WaveformInvocation,\n+        WaveformInvocation { name, parameters }: &WaveformInvocation,\n     ) -> Option<Seconds> {\n-        if let Some(definition) = program.waveforms.get(&waveform_invocation.name) {\n+        if let Some(definition) = program.waveforms.get(name) {\n             let sample_count = definition.matrix.len();\n             let common_sample_rate =\n                 program\n@@ -243,11 +243,22 @@ impl<'p> ScheduledBasicBlock<'p> {\n                 .map(|sample_rate| sample_count as f64 / sample_rate)\n                 .map(Seconds)\n         } else {\n-            waveform_invocation\n-                .parameters\n-                .get(\"duration\")\n-                .and_then(|v| v.to_real().ok())\n-                .map(Seconds)\n+            // Per the Quil spec, all waveform templates have a \"duration\"\n+            // parameter, and \"erf_square\" also has \"pad_left\" and \"pad_right\".\n+            // We explicitly choose to be more flexible here, and allow any\n+            // built-in waveform templates to have \"pad_*\" parameters, as well\n+            // as allow \"erf_square\" to omit them.\n+            let parameter = |parameter_name| {\n+                parameters\n+                    .get(parameter_name)\n+                    .and_then(|v| v.to_real().ok())\n+                    .map(Seconds)\n+            };\n+            Some(\n+                parameter(\"duration\")?\n+                    + parameter(\"pad_left\").unwrap_or(Seconds::zero())\n+                    + parameter(\"pad_right\").unwrap_or(Seconds::zero()),\n+            )\n         }\n     }\n \n@@ -404,6 +415,16 @@ PULSE 0 \"a\" flat(duration: 1.0)\n     #[case(\n         r#\"DEFFRAME 0 \"a\":\n     SAMPLE-RATE: 1e9\n+PULSE 0 \"a\" erf_square(duration: 1.0, pad_left: 0.2, pad_right: 0.3)\n+PULSE 0 \"a\" erf_square(duration: 0.1, pad_left: 0.7, pad_right: 0.7)\n+PULSE 0 \"a\" erf_square(duration: 0.5, pad_left: 0.6, pad_right: 0.4)\n+FENCE\n+\"#,\n+        Ok(vec![0.0, 1.5, 3.0, 4.5])\n+    )]\n+    #[case(\n+        r#\"DEFFRAME 0 \"a\":\n+    SAMPLE-RATE: 1e9\n DEFFRAME 0 \"b\":\n     SAMPLE-RATE: 1e9\n NONBLOCKING PULSE 0 \"a\" flat(duration: 1.0)\n", "instance_id": "rigetti__quil-rs-406", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly identifies the issue with the `erfsquare` waveform duration calculation in `ScheduledBasicBlock::get_waveform_duration_seconds`, explaining that the current implementation ignores the `padleft` and `padright` parameters, which should contribute to the total duration. The statement provides a concrete example with a test program and expected versus actual output, making the goal unambiguous. Inputs, outputs, and constraints are implicitly clear from the context and example. There are no significant ambiguities, and the inclusion of a test case further enhances clarity. All critical details are present, and the problem's logic is straightforward to grasp with the provided information.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting the `get_waveform_duration_seconds` function in the `schedule.rs` file. The modification involves updating the logic to account for padding parameters in waveform duration calculations, which is a straightforward adjustment. There are minor updates in another file (`control_flow_graph.rs`) to include padding parameters in a specific waveform invocation, but this is trivial. The changes do not impact the broader system architecture or require extensive refactoring across multiple modules. The overall amount of code change is small, with only a few lines modified or added.\n\n2. **Number of Technical Concepts**: Solving this problem requires basic familiarity with Rust syntax and control flow, as well as an understanding of the domain-specific concept of waveform durations and padding in the context of the Quil programming language. The logic for summing duration parameters (`duration`, `pad_left`, `pad_right`) is simple and does not involve complex algorithms, design patterns, or advanced language features. The test case addition in the code changes also requires minimal effort to understand and implement.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention complex edge cases beyond the general behavior of `erfsquare` waveforms with padding. The code changes handle the possibility of missing padding parameters by using `unwrap_or(Seconds::zero())`, which is a simple and effective way to manage optional values. No intricate error handling or performance considerations are required. The problem's focus is narrow, and the provided test case covers the primary scenario adequately.\n\n4. **Overall Complexity**: The task involves understanding a specific bug in duration calculation and applying a targeted fix. It requires minimal deep diving into the codebase beyond the affected function and does not necessitate advanced technical knowledge or extensive experience. The fix is logical and aligns with the problem description, making it accessible to developers with moderate experience in Rust or similar languages.\n\nGiven these points, a difficulty score of 0.35 reflects the simplicity of the logic and implementation, while acknowledging the need for some domain-specific understanding of waveform parameters and basic error handling. It is not a trivial typo fix (which would be closer to 0.0-0.2), but it also does not approach medium difficulty due to the limited scope and straightforward nature of the changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Allow for retry when tasks are stuck in queued\nTasks can get stuck in queued for a wide variety of reasons (e.g. celery loses\r\ntrack of a task, a cluster can't further scale up its workers, etc.), but tasks\r\nshould not be stuck in queued for a long time.\r\n\r\nOriginally, we simply marked a task as failed when it was stuck in queued for\r\ntoo long. We found that this led to suboptimal outcomes as ideally we would like \"failed\"\r\nto mean that a task was unable to run, instead of it meaning that we were unable to run the task.\r\n\r\nAs a compromise between always failing a stuck task and always rescheduling a stuck task (which could\r\nlead to tasks being stuck in queued forever without informing the user), we have creating the config\r\n`[core] num_stuck_reschedules`. With this new configuration, an airflow admin can decide how\r\nsensitive they would like their airflow to be WRT failing stuck tasks.\r\n\r\n\r\nHere is an example of what it looks like after trying this out with celery executor\r\n\r\n<img width=\"1109\" alt=\"image\" src=\"https://github.com/user-attachments/assets/233af08e-703a-4b1f-82e1-3340c52bf43a\">\r\n\n", "patch": "diff --git a/airflow/executors/base_executor.py b/airflow/executors/base_executor.py\nindex 57568af199710..5a5cf2d73f15d 100644\n--- a/airflow/executors/base_executor.py\n+++ b/airflow/executors/base_executor.py\n@@ -26,6 +26,7 @@\n from typing import TYPE_CHECKING, Any, List, Optional, Sequence, Tuple\n \n import pendulum\n+from deprecated import deprecated\n \n from airflow.cli.cli_config import DefaultHelpParser\n from airflow.configuration import conf\n@@ -545,7 +546,12 @@ def terminate(self):\n         \"\"\"Get called when the daemon receives a SIGTERM.\"\"\"\n         raise NotImplementedError()\n \n-    def cleanup_stuck_queued_tasks(self, tis: list[TaskInstance]) -> list[str]:  # pragma: no cover\n+    @deprecated(\n+        reason=\"Replaced by function `revoke_task`.\",\n+        category=RemovedInAirflow3Warning,\n+        action=\"ignore\",\n+    )\n+    def cleanup_stuck_queued_tasks(self, tis: list[TaskInstance]) -> list[str]:\n         \"\"\"\n         Handle remnants of tasks that were failed because they were stuck in queued.\n \n@@ -556,7 +562,23 @@ def cleanup_stuck_queued_tasks(self, tis: list[TaskInstance]) -> list[str]:  # p\n         :param tis: List of Task Instances to clean up\n         :return: List of readable task instances for a warning message\n         \"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n+\n+    def revoke_task(self, *, ti: TaskInstance):\n+        \"\"\"\n+        Attempt to remove task from executor.\n+\n+        It should attempt to ensure that the task is no longer running on the worker,\n+        and ensure that it is cleared out from internal data structures.\n+\n+        It should *not* change the state of the task in airflow, or add any events\n+        to the event buffer.\n+\n+        It should not raise any error.\n+\n+        :param ti: Task instance to remove\n+        \"\"\"\n+        raise NotImplementedError\n \n     def try_adopt_task_instances(self, tis: Sequence[TaskInstance]) -> Sequence[TaskInstance]:\n         \"\"\"\ndiff --git a/airflow/jobs/scheduler_job_runner.py b/airflow/jobs/scheduler_job_runner.py\nindex aa4e8d4f26aea..c9afd40f719ed 100644\n--- a/airflow/jobs/scheduler_job_runner.py\n+++ b/airflow/jobs/scheduler_job_runner.py\n@@ -25,12 +25,14 @@\n import time\n import warnings\n from collections import Counter, defaultdict, deque\n+from contextlib import suppress\n from dataclasses import dataclass\n from datetime import timedelta\n from functools import lru_cache, partial\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, Collection, Iterable, Iterator\n \n+from deprecated import deprecated\n from sqlalchemy import and_, delete, func, not_, or_, select, text, update\n from sqlalchemy.exc import OperationalError\n from sqlalchemy.orm import lazyload, load_only, make_transient, selectinload\n@@ -97,6 +99,9 @@\n DR = DagRun\n DM = DagModel\n \n+TASK_STUCK_IN_QUEUED_RESCHEDULE_EVENT = \"stuck in queued reschedule\"\n+\"\"\":meta private:\"\"\"\n+\n \n @dataclass\n class ConcurrencyMap:\n@@ -228,6 +233,13 @@ def __init__(\n             stalled_task_timeout, task_adoption_timeout, worker_pods_pending_timeout, task_queued_timeout\n         )\n \n+        # this param is intentionally undocumented\n+        self._num_stuck_queued_retries = conf.getint(\n+            section=\"scheduler\",\n+            key=\"num_stuck_in_queued_retries\",\n+            fallback=2,\n+        )\n+\n         self.do_pickle = do_pickle\n \n         if log:\n@@ -1093,7 +1105,7 @@ def _run_scheduler_loop(self) -> None:\n \n         timers.call_regular_interval(\n             conf.getfloat(\"scheduler\", \"task_queued_timeout_check_interval\"),\n-            self._fail_tasks_stuck_in_queued,\n+            self._handle_tasks_stuck_in_queued,\n         )\n \n         timers.call_regular_interval(\n@@ -1141,6 +1153,7 @@ def _run_scheduler_loop(self) -> None:\n                 for executor in self.job.executors:\n                     try:\n                         # this is backcompat check if executor does not inherit from BaseExecutor\n+                        # todo: remove in airflow 3.0\n                         if not hasattr(executor, \"_task_event_logs\"):\n                             continue\n                         with create_session() as session:\n@@ -1772,48 +1785,132 @@ def _send_sla_callbacks_to_processor(self, dag: DAG) -> None:\n         self.job.executor.send_callback(request)\n \n     @provide_session\n-    def _fail_tasks_stuck_in_queued(self, session: Session = NEW_SESSION) -> None:\n+    def _handle_tasks_stuck_in_queued(self, session: Session = NEW_SESSION) -> None:\n         \"\"\"\n-        Mark tasks stuck in queued for longer than `task_queued_timeout` as failed.\n+        Handle the scenario where a task is queued for longer than `task_queued_timeout`.\n \n         Tasks can get stuck in queued for a wide variety of reasons (e.g. celery loses\n         track of a task, a cluster can't further scale up its workers, etc.), but tasks\n-        should not be stuck in queued for a long time. This will mark tasks stuck in\n-        queued for longer than `self._task_queued_timeout` as failed. If the task has\n-        available retries, it will be retried.\n+        should not be stuck in queued for a long time.\n+\n+        We will attempt to requeue the task (by revoking it from executor and setting to\n+        scheduled) up to 2 times before failing the task.\n         \"\"\"\n-        self.log.debug(\"Calling SchedulerJob._fail_tasks_stuck_in_queued method\")\n+        tasks_stuck_in_queued = self._get_tis_stuck_in_queued(session)\n+        for executor, stuck_tis in self._executor_to_tis(tasks_stuck_in_queued).items():\n+            try:\n+                for ti in stuck_tis:\n+                    executor.revoke_task(ti=ti)\n+                    self._maybe_requeue_stuck_ti(\n+                        ti=ti,\n+                        session=session,\n+                    )\n+            except NotImplementedError:\n+                # this block only gets entered if the executor has not implemented `revoke_task`.\n+                # in which case, we try the fallback logic\n+                # todo: remove the call to _stuck_in_queued_backcompat_logic in airflow 3.0.\n+                #   after 3.0, `cleanup_stuck_queued_tasks` will be removed, so we should\n+                #   just continue immediately.\n+                self._stuck_in_queued_backcompat_logic(executor, stuck_tis)\n+                continue\n \n-        tasks_stuck_in_queued = session.scalars(\n+    def _get_tis_stuck_in_queued(self, session) -> Iterable[TaskInstance]:\n+        \"\"\"Query db for TIs that are stuck in queued.\"\"\"\n+        return session.scalars(\n             select(TI).where(\n                 TI.state == TaskInstanceState.QUEUED,\n                 TI.queued_dttm < (timezone.utcnow() - timedelta(seconds=self._task_queued_timeout)),\n                 TI.queued_by_job_id == self.job.id,\n             )\n-        ).all()\n+        )\n \n-        for executor, stuck_tis in self._executor_to_tis(tasks_stuck_in_queued).items():\n-            try:\n-                cleaned_up_task_instances = set(executor.cleanup_stuck_queued_tasks(tis=stuck_tis))\n-                for ti in stuck_tis:\n-                    if repr(ti) in cleaned_up_task_instances:\n-                        self.log.warning(\n-                            \"Marking task instance %s stuck in queued as failed. \"\n-                            \"If the task instance has available retries, it will be retried.\",\n-                            ti,\n-                        )\n-                        session.add(\n-                            Log(\n-                                event=\"stuck in queued\",\n-                                task_instance=ti.key,\n-                                extra=(\n-                                    \"Task will be marked as failed. If the task instance has \"\n-                                    \"available retries, it will be retried.\"\n-                                ),\n-                            )\n-                        )\n-            except NotImplementedError:\n-                self.log.debug(\"Executor doesn't support cleanup of stuck queued tasks. Skipping.\")\n+    def _maybe_requeue_stuck_ti(self, *, ti, session):\n+        \"\"\"\n+        Requeue task if it has not been attempted too many times.\n+\n+        Otherwise, fail it.\n+        \"\"\"\n+        num_times_stuck = self._get_num_times_stuck_in_queued(ti, session)\n+        if num_times_stuck < self._num_stuck_queued_retries:\n+            self.log.info(\"Task stuck in queued; will try to requeue. task_id=%s\", ti.task_id)\n+            session.add(\n+                Log(\n+                    event=TASK_STUCK_IN_QUEUED_RESCHEDULE_EVENT,\n+                    task_instance=ti.key,\n+                    extra=(\n+                        f\"Task was in queued state for longer than {self._task_queued_timeout} \"\n+                        \"seconds; task state will be set back to scheduled.\"\n+                    ),\n+                )\n+            )\n+            self._reschedule_stuck_task(ti)\n+        else:\n+            self.log.info(\n+                \"Task requeue attempts exceeded max; marking failed. task_instance=%s\",\n+                ti,\n+            )\n+            session.add(\n+                Log(\n+                    event=\"stuck in queued tries exceeded\",\n+                    task_instance=ti.key,\n+                    extra=f\"Task was requeued more than {self._num_stuck_queued_retries} times and will be failed.\",\n+                )\n+            )\n+            ti.set_state(TaskInstanceState.FAILED, session=session)\n+\n+    @deprecated(\n+        reason=\"This is backcompat layer for older executor interface. Should be removed in 3.0\",\n+        category=RemovedInAirflow3Warning,\n+        action=\"ignore\",\n+    )\n+    def _stuck_in_queued_backcompat_logic(self, executor, stuck_tis):\n+        \"\"\"\n+        Try to invoke stuck in queued cleanup for older executor interface.\n+\n+        TODO: remove in airflow 3.0\n+\n+        Here we handle case where the executor pre-dates the interface change that\n+        introduced `cleanup_tasks_stuck_in_queued` and deprecated `cleanup_stuck_queued_tasks`.\n+\n+        \"\"\"\n+        with suppress(NotImplementedError):\n+            for ti_repr in executor.cleanup_stuck_queued_tasks(tis=stuck_tis):\n+                self.log.warning(\n+                    \"Task instance %s stuck in queued. Will be set to failed.\",\n+                    ti_repr,\n+                )\n+\n+    @provide_session\n+    def _reschedule_stuck_task(self, ti, session=NEW_SESSION):\n+        session.execute(\n+            update(TI)\n+            .where(TI.filter_for_tis([ti]))\n+            .values(\n+                state=TaskInstanceState.SCHEDULED,\n+                queued_dttm=None,\n+            )\n+            .execution_options(synchronize_session=False)\n+        )\n+\n+    @provide_session\n+    def _get_num_times_stuck_in_queued(self, ti: TaskInstance, session: Session = NEW_SESSION) -> int:\n+        \"\"\"\n+        Check the Log table to see how many times a taskinstance has been stuck in queued.\n+\n+        We can then use this information to determine whether to reschedule a task or fail it.\n+        \"\"\"\n+        return (\n+            session.query(Log)\n+            .where(\n+                Log.task_id == ti.task_id,\n+                Log.dag_id == ti.dag_id,\n+                Log.run_id == ti.run_id,\n+                Log.map_index == ti.map_index,\n+                Log.try_number == ti.try_number,\n+                Log.event == TASK_STUCK_IN_QUEUED_RESCHEDULE_EVENT,\n+            )\n+            .count()\n+        )\n \n     @provide_session\n     def _emit_pool_metrics(self, session: Session = NEW_SESSION) -> None:\n@@ -2102,7 +2199,7 @@ def _orphan_unreferenced_datasets(self, session: Session = NEW_SESSION) -> None:\n         updated_count = sum(self._set_orphaned(dataset) for dataset in orphaned_dataset_query)\n         Stats.gauge(\"dataset.orphaned\", updated_count)\n \n-    def _executor_to_tis(self, tis: list[TaskInstance]) -> dict[BaseExecutor, list[TaskInstance]]:\n+    def _executor_to_tis(self, tis: Iterable[TaskInstance]) -> dict[BaseExecutor, list[TaskInstance]]:\n         \"\"\"Organize TIs into lists per their respective executor.\"\"\"\n         _executor_to_tis: defaultdict[BaseExecutor, list[TaskInstance]] = defaultdict(list)\n         for ti in tis:\ndiff --git a/docs/spelling_wordlist.txt b/docs/spelling_wordlist.txt\nindex 1ca4c8dc4557d..6f8d5015ed4d5 100644\n--- a/docs/spelling_wordlist.txt\n+++ b/docs/spelling_wordlist.txt\n@@ -1359,6 +1359,7 @@ repos\n repr\n req\n reqs\n+requeued\n Reserialize\n reserialize\n reserialized\n", "instance_id": "apache__airflow-44158", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of handling tasks stuck in a queued state by introducing a retry mechanism before failing them. It provides context on why tasks get stuck and the motivation behind the new configuration (`num_stuck_reschedules`). The inclusion of an image (though not visible in the text) suggests an attempt to provide a visual example of the behavior with the Celery executor. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes \"stuck in queued for too long\" (though it references a timeout in the code), nor does it specify edge cases or constraints around the retry mechanism (e.g., maximum retry duration, behavior under specific failure conditions). Additionally, the expected behavior for different executors or configurations is not fully detailed. These gaps prevent it from being comprehensive, but the overall intent and approach are understandable.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files (`base_executor.py` and `scheduler_job_runner.py`) and involving core components of the Apache Airflow scheduler and executor system. The changes require a deep understanding of Airflow's task lifecycle, state management, and executor interactions, which are non-trivial concepts. The introduction of a retry mechanism (`revoke_task`, `num_stuck_queued_retries`) and associated logic for rescheduling or failing tasks adds complexity, as it impacts how tasks are tracked and managed in the system. Additionally, the code handles backward compatibility with older executor interfaces, requiring careful consideration of deprecated methods and transitional logic (e.g., `_stuck_in_queued_backcompat_logic`), which increases the cognitive load. The number of technical concepts involved is substantial, including database queries (SQLAlchemy), task state transitions, logging, configuration management, and executor-specific behavior. While edge cases are not explicitly detailed in the problem statement, the code changes suggest the need to handle scenarios like repeated retries, executor-specific failures, and ensuring no tasks are stuck indefinitely, which adds to the complexity of error handling. However, it does not reach the \"Very Hard\" category as it does not involve system-level redesign or highly specialized domain knowledge beyond Airflow's architecture. A score of 0.65 reflects the challenging nature of the task, requiring significant expertise in Airflow and careful implementation to avoid unintended side effects.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Type coercion of inequality joins propagates wrong types to literals in expressions referencing columns of right table\n### Checks\n\n- [X] I have checked that this issue has not already been reported.\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\n\n### Reproducible example\n\n```python\r\nimport polars as pl\r\n\r\nleft = pl.LazyFrame({\"a\": pl.Series([1, 2, 3], dtype=pl.Float64)})\r\n\r\nright = pl.LazyFrame({\"a\": pl.Series([1, 2, 3], dtype=pl.Int64), \"b\": pl.Series([1, 2, 3], dtype=pl.Int8)})\r\n\r\nq_bad_1 = left.join_where(right, pl.col(\"a\") <= pl.col(\"a_right\") * 2)\r\n\r\nq_bad_2 = left.join_where(right, pl.col(\"a\") <= pl.col(\"b\") * 2)\r\n\r\nprint(q_bad_1.explain())\r\n\r\nprint(q_bad_2.explain())\r\n```\r\n\n\n### Log output\n\n_No response_\n\n### Issue description\n\nOutput of those prints is:\r\n\r\n```\r\nIEJOIN JOIN:\r\nLEFT PLAN ON: [col(\"a\")]\r\n  DF [\"a\"]; PROJECT */1 COLUMNS; SELECTION: None\r\nRIGHT PLAN ON: [[(col(\"a\")) * (2.0)]]\r\n  DF [\"a\", \"b\"]; PROJECT */2 COLUMNS; SELECTION: None\r\nEND IEJOIN JOIN\r\n```\r\n\r\nAnd\r\n```\r\nIEJOIN JOIN:\r\nLEFT PLAN ON: [col(\"a\")]\r\n  DF [\"a\"]; PROJECT */1 COLUMNS; SELECTION: None\r\nRIGHT PLAN ON: [[(col(\"b\")) * (dyn int: 2)]]\r\n  DF [\"a\", \"b\"]; PROJECT */2 COLUMNS; SELECTION: None\r\nEND IEJOIN JOIN\r\n```\r\n\r\nNotice how in the first case, the right plan expression has a float literal, where it should be an int literal. In the second case, the right plan expression hasn't managed to determine the type of the literal (it's still a `dyn int`).\r\n\r\nI did some printf-debugging:\r\n\r\n```diff\r\ndiff --git a/crates/polars-plan/src/plans/conversion/type_coercion/binary.rs b/crates/polars-plan/src/plans/conversion/type_coercion/binary.rs\r\nindex 24f65b3465..9c95826416 100644\r\n--- a/crates/polars-plan/src/plans/conversion/type_coercion/binary.rs\r\n+++ b/crates/polars-plan/src/plans/conversion/type_coercion/binary.rs\r\n@@ -124,6 +124,9 @@ pub(super) fn process_binary(\r\n     node_right: Node,\r\n ) -> PolarsResult<Option<AExpr>> {\r\n     let input_schema = get_schema(lp_arena, lp_node);\r\n+    println!(\"{:?}\", input_schema);\r\n+    println!(\"{:?}\", expr_arena.get(node_left));\r\n+    println!(\"{:?}\", expr_arena.get(node_right));\r\n     let (left, type_left): (&AExpr, DataType) =\r\n         unpack!(get_aexpr_and_type(expr_arena, node_left, &input_schema));\r\n     let (right, type_right): (&AExpr, DataType) =\r\n```\r\n\r\nThe output from the two queries is:\r\n\r\n```\r\nSchema:\r\nname: a, field: Float64\r\n\r\nColumn(\"a\")\r\nLiteral(dyn int: 2)\r\nSchema:\r\nname: a, field: Float64\r\n\r\nColumn(\"a\")\r\nCast { expr: Node(2), dtype: Float64, options: NonStrict }\r\n```\r\n\r\nand\r\n```\r\nSchema:\r\nname: a, field: Float64\r\n\r\nColumn(\"b\")\r\nLiteral(dyn int: 2)\r\n```\r\n\r\nSo it looks like `run_coercion` for join nodes is always sending down the schema for the left table, but then I got a bit lost.\n\n### Expected behavior\n\nI expect the correct dtypes to be propagated to literals in both cases. I stumbled into this implementing conditional joins in the GPU engine, I trust the dtypes that polars has given me, but in these cases, I get mismatches and then libcudf doesn't like it. I can work around the bugs by doing my own type propagation, but it feels like this is an actual bug, even though it is probably quite benign.\n\n### Installed versions\n\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.12.0\r\nIndex type:          UInt32\r\nPlatform:            Linux-6.8.0-48-generic-x86_64-with-glibc2.35\r\nPython:              3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]\r\nLTS CPU:             False\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  1.2.0\r\naltair               5.4.1\r\ncloudpickle          3.1.0\r\nconnectorx           0.3.3\r\ndeltalake            0.21.0\r\nfastexcel            0.12.0\r\nfsspec               2024.10.0\r\ngevent               24.10.3\r\ngreat_tables         0.13.0\r\nmatplotlib           3.9.2\r\nnest_asyncio         1.6.0\r\nnumpy                2.0.2\r\nopenpyxl             3.1.5\r\npandas               2.2.3\r\npyarrow              17.0.0\r\npydantic             2.9.2\r\npyiceberg            <not installed>\r\nsqlalchemy           2.0.36\r\ntorch                2.4.1.post302\r\nxlsx2csv             0.8.3\r\nxlsxwriter           3.2.0\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/crates/polars-plan/src/plans/conversion/join.rs b/crates/polars-plan/src/plans/conversion/join.rs\nindex 5222ca73cda1..8a10db7de177 100644\n--- a/crates/polars-plan/src/plans/conversion/join.rs\n+++ b/crates/polars-plan/src/plans/conversion/join.rs\n@@ -109,9 +109,31 @@ pub fn resolve_join(\n \n     ctxt.conversion_optimizer\n         .fill_scratch(&left_on, ctxt.expr_arena);\n+    ctxt.conversion_optimizer\n+        .coerce_types(ctxt.expr_arena, ctxt.lp_arena, input_left)\n+        .map_err(|e| e.context(\"'join' failed\".into()))?;\n     ctxt.conversion_optimizer\n         .fill_scratch(&right_on, ctxt.expr_arena);\n-\n+    ctxt.conversion_optimizer\n+        .coerce_types(ctxt.expr_arena, ctxt.lp_arena, input_right)\n+        .map_err(|e| e.context(\"'join' failed\".into()))?;\n+\n+    let get_dtype = |expr: &ExprIR, schema: &SchemaRef| {\n+        ctxt.expr_arena\n+            .get(expr.node())\n+            .get_type(schema, Context::Default, ctxt.expr_arena)\n+            .ok()\n+            .unwrap()\n+    };\n+    for (lnode, rnode) in left_on.iter().zip(right_on.iter()) {\n+        let ltype = get_dtype(lnode, &schema_left);\n+        let rtype = get_dtype(rnode, &schema_right);\n+        polars_ensure!(\n+            ltype == rtype,\n+            SchemaMismatch: \"datatypes of join keys don't match - `{}`: {} on left does not match `{}`: {} on right\",\n+            lnode.output_name(), ltype, rnode.output_name(), rtype\n+        )\n+    }\n     // Every expression must be elementwise so that we are\n     // guaranteed the keys for a join are all the same length.\n     let all_elementwise =\n@@ -128,7 +150,7 @@ pub fn resolve_join(\n         right_on,\n         options,\n     };\n-    run_conversion(lp, ctxt, \"join\")\n+    Ok(ctxt.lp_arena.add(lp))\n }\n \n #[cfg(feature = \"iejoin\")]\n", "instance_id": "pola-rs__polars-19625", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue related to type coercion in inequality joins within the Polars library. It includes a reproducible example with Python code, expected behavior, and actual output, which helps in understanding the bug. The issue description also highlights the impact on downstream systems (e.g., GPU engine integration) and provides context on why this is a problem. However, there are minor ambiguities: the problem statement does not explicitly define all edge cases or constraints that might need to be handled (e.g., specific data types or join conditions beyond the example). Additionally, while the debugging output and explanation are helpful, they are somewhat incomplete as the author admits to getting \"a bit lost,\" leaving some uncertainty about the root cause. Overall, the statement is valid and clear but lacks exhaustive detail on edge cases or a fully fleshed-out root cause analysis, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of the code changes, while localized to a single file (`join.rs`), involves critical logic in the Polars library related to type coercion and join operations. The modification requires understanding and altering the type coercion process for join conditions, which impacts how expressions are evaluated across different schemas (left and right tables). This is not a trivial change as it affects core functionality and could have downstream effects on query execution and performance.\n\nSecond, the number of technical concepts involved is significant. Solving this requires a deep understanding of Rust (given the codebase), Polars' internal architecture (e.g., expression arena, logical plan arena), schema handling, and type coercion mechanisms. Additionally, knowledge of data type propagation in database systems and how joins (especially inequality joins) are processed is necessary. The code changes also introduce explicit type matching logic, which requires careful handling to avoid introducing new bugs.\n\nThird, while the problem statement provides a specific reproducible case, the potential for edge cases is high. Different data types, complex expressions in join conditions, and interactions with other parts of the system (e.g., GPU engine as mentioned) could reveal additional issues. The code changes already include error handling for type mismatches, but ensuring robustness across all scenarios adds to the complexity.\n\nFinally, the impact on the system's architecture, while not a complete overhaul, is non-trivial. The change affects how join operations handle type coercion, which is a fundamental part of query processing in Polars. A score of 0.75 reflects the need for deep codebase knowledge, careful handling of type systems, and consideration of broader implications, placing this problem on the higher end of the \"Hard\" spectrum but not quite at \"Very Hard\" since it does not involve system-wide refactoring or advanced domain-specific challenges beyond database query processing.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update dependency docker to v7.1.0\n[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [docker](https://togithub.com/docker/docker-py) ([changelog](https://docker-py.readthedocs.io/en/stable/change-log.html)) | `==7.0.0` -> `==7.1.0` | [![age](https://developer.mend.io/api/mc/badges/age/pypi/docker/7.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/pypi/docker/7.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/pypi/docker/7.0.0/7.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/pypi/docker/7.0.0/7.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n### Release Notes\n\n<details>\n<summary>docker/docker-py (docker)</summary>\n\n### [`v7.1.0`](https://togithub.com/docker/docker-py/releases/tag/7.1.0)\n\n[Compare Source](https://togithub.com/docker/docker-py/compare/7.0.0...7.1.0)\n\n#### Upgrade Notes\n\n-   Bumped minimum engine API version to 1.24\n-   Bumped default engine API version to 1.44 (Moby 25.0)\n\n#### Bugfixes\n\n-   Fixed issue with tag parsing when the registry address includes ports that resulted in `invalid tag format` errors\n-   Fixed issue preventing creating new configs (`ConfigCollection`), which failed with a `KeyError` due to the `name` field\n-   Fixed an issue due to an update in the [requests](https://togithub.com/psf/requests) package breaking `docker-py` by applying the [suggested fix](https://togithub.com/psf/requests/pull/6710)\n\n#### Miscellaneous\n\n-   Documentation improvements\n-   Updated Ruff (linter) and fixed minor linting issues\n-   Packaging/CI updates\n    -   Started using hatch for packaging (https://github.com/pypa/hatch)\n    -   Updated `setup-python` github action\n-   Updated tests\n    -   Stopped checking for deprecated container and image related fields (`Container` and `ContainerConfig`)\n    -   Updated tests that check `NetworkSettings.Networks.<network>.Aliases` due to engine changes\n\n#### What's Changed\n\n-   Bump default API version to 1.43 (Moby 24.0) by [@&#8203;vvoland](https://togithub.com/vvoland) in [https://github.com/docker/docker-py/pull/3199](https://togithub.com/docker/docker-py/pull/3199)\n-   integration/commit: Don't check for deprecated fields by [@&#8203;vvoland](https://togithub.com/vvoland) in [https://github.com/docker/docker-py/pull/3203](https://togithub.com/docker/docker-py/pull/3203)\n-   fix: tag regex should allow ports by [@&#8203;artificial-intelligence](https://togithub.com/artificial-intelligence) in [https://github.com/docker/docker-py/pull/3196](https://togithub.com/docker/docker-py/pull/3196)\n-   utils: Fix datetime_to_timestamp by [@&#8203;vvoland](https://togithub.com/vvoland) in [https://github.com/docker/docker-py/pull/3208](https://togithub.com/docker/docker-py/pull/3208)\n-   fix: keyerror when creating new config([#&#8203;3110](https://togithub.com/docker/docker-py/issues/3110)) by [@&#8203;Khushiyant](https://togithub.com/Khushiyant) in [https://github.com/docker/docker-py/pull/3200](https://togithub.com/docker/docker-py/pull/3200)\n-   Update Ruff, fix some minor issues by [@&#8203;akx](https://togithub.com/akx) in [https://github.com/docker/docker-py/pull/3206](https://togithub.com/docker/docker-py/pull/3206)\n-   docs: change image.history() return type to list ([#&#8203;3076](https://togithub.com/docker/docker-py/issues/3076)) by [@&#8203;Khushiyant](https://togithub.com/Khushiyant) in [https://github.com/docker/docker-py/pull/3202](https://togithub.com/docker/docker-py/pull/3202)\n-   lint: fix string formatting by [@&#8203;milas](https://togithub.com/milas) in [https://github.com/docker/docker-py/pull/3211](https://togithub.com/docker/docker-py/pull/3211)\n-   Drop `packaging` dependency by [@&#8203;akx](https://togithub.com/akx) in [https://github.com/docker/docker-py/pull/3205](https://togithub.com/docker/docker-py/pull/3205)\n-   Bump default API version to 1.44 (Moby 25.0) by [@&#8203;vvoland](https://togithub.com/vvoland) in [https://github.com/docker/docker-py/pull/3231](https://togithub.com/docker/docker-py/pull/3231)\n-   Modernize build to use `hatchling` and `hatch-vcs` by [@&#8203;akx](https://togithub.com/akx) in [https://github.com/docker/docker-py/pull/3207](https://togithub.com/docker/docker-py/pull/3207)\n-   Enable Ruff I (import sort), autofix by [@&#8203;akx](https://togithub.com/akx) in [https://github.com/docker/docker-py/pull/3212](https://togithub.com/docker/docker-py/pull/3212)\n-   chore(ci): fix-ups across Make / Docker / GitHub Actions by [@&#8203;milas](https://togithub.com/milas) in [https://github.com/docker/docker-py/pull/3241](https://togithub.com/docker/docker-py/pull/3241)\n-   chore: fix return type docs for `container.logs()` by [@&#8203;BobDu](https://togithub.com/BobDu) in [https://github.com/docker/docker-py/pull/2240](https://togithub.com/docker/docker-py/pull/2240)\n-   CI fix  by [@&#8203;krissetto](https://togithub.com/krissetto) in [https://github.com/docker/docker-py/pull/3260](https://togithub.com/docker/docker-py/pull/3260)\n-   Fix for requests 2.32 by [@&#8203;felixfontein](https://togithub.com/felixfontein) in [https://github.com/docker/docker-py/pull/3257](https://togithub.com/docker/docker-py/pull/3257)\n-   Added 7.1.0 changelog by [@&#8203;krissetto](https://togithub.com/krissetto) in [https://github.com/docker/docker-py/pull/3262](https://togithub.com/docker/docker-py/pull/3262)\n-   Fix env var name in release pipeline by [@&#8203;krissetto](https://togithub.com/krissetto) in [https://github.com/docker/docker-py/pull/3263](https://togithub.com/docker/docker-py/pull/3263)\n-   Change env var name in release pipeline to match hatch expectations by [@&#8203;krissetto](https://togithub.com/krissetto) in [https://github.com/docker/docker-py/pull/3264](https://togithub.com/docker/docker-py/pull/3264)\n\n#### New Contributors\n\n-   [@&#8203;vvoland](https://togithub.com/vvoland) made their first contribution in [https://github.com/docker/docker-py/pull/3199](https://togithub.com/docker/docker-py/pull/3199)\n-   [@&#8203;artificial-intelligence](https://togithub.com/artificial-intelligence) made their first contribution in [https://github.com/docker/docker-py/pull/3196](https://togithub.com/docker/docker-py/pull/3196)\n-   [@&#8203;Khushiyant](https://togithub.com/Khushiyant) made their first contribution in [https://github.com/docker/docker-py/pull/3200](https://togithub.com/docker/docker-py/pull/3200)\n-   [@&#8203;BobDu](https://togithub.com/BobDu) made their first contribution in [https://github.com/docker/docker-py/pull/2240](https://togithub.com/docker/docker-py/pull/2240)\n-   [@&#8203;krissetto](https://togithub.com/krissetto) made their first contribution in [https://github.com/docker/docker-py/pull/3260](https://togithub.com/docker/docker-py/pull/3260)\n\n**Full Changelog**: https://github.com/docker/docker-py/compare/7.0.0...7.1.0\n\n</details>\n\n---\n\n### Configuration\n\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\n\u267b **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/anarkiwi/desidulate).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy4zNjguMTAiLCJ1cGRhdGVkSW5WZXIiOiIzNy4zNjguMTAiLCJ0YXJnZXRCcmFuY2giOiJtYWluIiwibGFiZWxzIjpbXX0=-->\n\n", "patch": "diff --git a/.github/workflows/periodic.yml b/.github/workflows/periodic.yml\nindex dd41b95..f9fc002 100644\n--- a/.github/workflows/periodic.yml\n+++ b/.github/workflows/periodic.yml\n@@ -8,7 +8,7 @@ jobs:\n   test:\n     runs-on: ubuntu-latest\n     steps:\n-    - uses: actions/checkout@0ad4b8fadaa221de15dcec353f45205ec38ea70b # v4\n+    - uses: actions/checkout@a5ac7e51b41094c92402da3b24376905380afc29 # v4\n     - name: test\n       run: |\n         sudo apt-get update && sudo apt-get install python3-dev && \\\ndiff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nindex e47ea95..649c965 100644\n--- a/.github/workflows/release.yml\n+++ b/.github/workflows/release.yml\n@@ -16,7 +16,7 @@ jobs:\n       name: \"release\"\n     steps:\n       - name: Checkout repo\n-        uses: actions/checkout@0ad4b8fadaa221de15dcec353f45205ec38ea70b # v4\n+        uses: actions/checkout@a5ac7e51b41094c92402da3b24376905380afc29 # v4\n       - name: Set up python-${{ env.RELEASE_PY_VER }}\n         uses: actions/setup-python@v5\n         with:\ndiff --git a/requirements.txt b/requirements.txt\nindex a501f67..c22c3fa 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,8 +1,8 @@\n-docker==7.0.0\n+docker==7.1.0\n music21==9.1.0\n numpy==1.26.4\n pandas==2.2.2\n-pyarrow==16.0.0\n+pyarrow==16.1.0\n pyresidfp==0.9.0\n scipy==1.13.0\n zstandard==0.22.0\n", "instance_id": "anarkiwi__desidulate-408", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and clear. It explicitly states the goal of updating the Docker dependency from version 7.0.0 to 7.1.0, providing detailed release notes, upgrade notes, bug fixes, and miscellaneous changes associated with the new version. The input (current version) and output (target version) are well-defined, and the changelog offers insight into potential impacts (e.g., API version bumps, bug fixes). Additionally, the context provided by Mend Renovate, including confidence and compatibility badges, further clarifies the intent and reliability of the update. There are no significant ambiguities, and the problem description includes all necessary details to understand the scope of the update, making it a well-documented and straightforward task.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a simple dependency update with minimal code changes. The scope of the modification is limited to updating a version number in the `requirements.txt` file (from `docker==7.0.0` to `docker==7.1.0`) and incidental updates to GitHub Actions checkout references in workflow files, which are unrelated to the core problem but included in the diff. The task does not require deep understanding of the codebase architecture or complex logic, as it is a routine maintenance update. The release notes indicate minor breaking changes (e.g., API version bump to 1.44), but no specific edge cases or error handling modifications are required based on the provided information. The technical concepts involved are basic\u2014understanding dependency management and version updates in Python projects. There is no significant impact on the system's architecture or interactions between modules. Overall, this is a very easy task that requires minimal effort and expertise, primarily involving verification that the updated library version does not introduce breaking changes in the project.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Maintenance: add preliminary support for Python3.13\nInitially, add Python 3.13.0-rc.3 to the `unittest` continuous integration matrix.\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex dbb9b07c5..0457523a5 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -18,6 +18,7 @@ classifiers = [\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n     \"Programming Language :: Python :: 3.12\",\n+    \"Programming Language :: Python :: 3.13\",\n     \"License :: OSI Approved :: MIT License\",\n     \"Intended Audience :: Developers\",\n     \"Operating System :: OS Independent\",\n", "instance_id": "hhursev__recipe-scrapers-1292", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent to add preliminary support for Python 3.13 by including it in the unittest continuous integration matrix. The goal is straightforward, and the code change provided aligns with this intent by adding Python 3.13 to the list of supported versions in the `pyproject.toml` file. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether additional configuration or testing is required beyond updating the matrix (e.g., compatibility checks, dependency updates, or specific test cases for Python 3.13). It also does not mention potential constraints or challenges, such as known incompatibilities with Python 3.13.0-rc.3. While the task is clear at a high level, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a minimal and straightforward code change. The modification is limited to a single line in the `pyproject.toml` file, adding Python 3.13 to the list of supported versions. This does not require deep understanding of the codebase, complex logic, or interaction between multiple modules. No advanced programming concepts, algorithms, or domain-specific knowledge are needed beyond basic familiarity with Python project configuration files. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the change does not appear to impact the system's architecture or performance. The task is essentially a simple update to a configuration file, fitting within the very easy range of difficulty (0.0-0.2). I assign a score of 0.1 to reflect the minimal effort and expertise required, acknowledging that it is slightly more involved than a typo fix but still extremely basic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "spirv-opt crashes when optimizing function declarations\nWhen spv input to spirv-opt contains functions without blocks (declarations), some optimization passes crash. It seems that they assume functions will have blocks.\r\n\r\n```\r\n %normalFunc = OpFunction %int None %26\r\n          %a = OpFunctionParameter %int\r\n               OpFunctionEnd\r\n```\r\n\r\n```\r\n; SPIR-V\r\n; Version: 1.5\r\n; Generator: Khronos; 40\r\n; Bound: 36\r\n; Schema: 0\r\n               OpCapability Linkage\r\n               OpCapability RayTracingKHR\r\n               OpCapability Shader\r\n               OpExtension \"SPV_KHR_ray_tracing\"\r\n               OpMemoryModel Logical GLSL450\r\n               OpEntryPoint AnyHitKHR %anyhit \"main\" %Constants %8\r\n               OpSource Slang 1\r\n               OpName %Payload \"Payload\"\r\n               OpMemberName %Payload 0 \"val\"\r\n               OpName %SLANG_ParameterGroup_Constants_std140 \"SLANG_ParameterGroup_Constants_std140\"\r\n               OpMemberName %SLANG_ParameterGroup_Constants_std140 0 \"x\"\r\n               OpMemberName %SLANG_ParameterGroup_Constants_std140 1 \"y\"\r\n               OpName %Constants \"Constants\"\r\n               OpName %a \"a\"\r\n               OpName %normalFunc \"normalFunc\"\r\n               OpName %a_0 \"a\"\r\n               OpName %normalFuncUsesGeneric \"normalFuncUsesGeneric\"\r\n               OpName %anyhit \"anyhit\"\r\n               OpMemberDecorate %Payload 0 Offset 0\r\n               OpDecorate %SLANG_ParameterGroup_Constants_std140 Block\r\n               OpMemberDecorate %SLANG_ParameterGroup_Constants_std140 0 Offset 0\r\n               OpMemberDecorate %SLANG_ParameterGroup_Constants_std140 1 Offset 4\r\n               OpDecorate %Constants Binding 0\r\n               OpDecorate %Constants DescriptorSet 0\r\n               OpDecorate %normalFunc LinkageAttributes \"_SR29export_2Dxlibrary_2Dxgenerics10normalFuncp1pi_ii\" Import\r\n               OpDecorate %normalFuncUsesGeneric LinkageAttributes \"_SR29export_2Dxlibrary_2Dxgenerics21normalFuncUsesGenericp1pi_ii\" Import\r\n       %void = OpTypeVoid\r\n          %3 = OpTypeFunction %void\r\n        %int = OpTypeInt 32 1\r\n    %Payload = OpTypeStruct %int\r\n%_ptr_IncomingRayPayloadKHR_Payload = OpTypePointer IncomingRayPayloadKHR %Payload\r\n      %int_0 = OpConstant %int 0\r\n%_ptr_IncomingRayPayloadKHR_int = OpTypePointer IncomingRayPayloadKHR %int\r\n      %float = OpTypeFloat 32\r\n%SLANG_ParameterGroup_Constants_std140 = OpTypeStruct %float %float\r\n%_ptr_Uniform_SLANG_ParameterGroup_Constants_std140 = OpTypePointer Uniform %SLANG_ParameterGroup_Constants_std140\r\n%_ptr_Uniform_float = OpTypePointer Uniform %float\r\n      %int_1 = OpConstant %int 1\r\n         %26 = OpTypeFunction %int %int\r\n          %8 = OpVariable %_ptr_IncomingRayPayloadKHR_Payload IncomingRayPayloadKHR\r\n  %Constants = OpVariable %_ptr_Uniform_SLANG_ParameterGroup_Constants_std140 Uniform\r\n %normalFunc = OpFunction %int None %26\r\n          %a = OpFunctionParameter %int\r\n               OpFunctionEnd\r\n%normalFuncUsesGeneric = OpFunction %int None %26\r\n        %a_0 = OpFunctionParameter %int\r\n               OpFunctionEnd\r\n     %anyhit = OpFunction %void None %3\r\n          %4 = OpLabel\r\n         %11 = OpAccessChain %_ptr_IncomingRayPayloadKHR_int %8 %int_0\r\n         %17 = OpAccessChain %_ptr_Uniform_float %Constants %int_0\r\n         %18 = OpLoad %float %17\r\n         %20 = OpAccessChain %_ptr_Uniform_float %Constants %int_1\r\n         %21 = OpLoad %float %20\r\n         %22 = OpFMul %float %18 %21\r\n         %23 = OpConvertFToS %int %22\r\n         %24 = OpFunctionCall %int %normalFunc %23\r\n         %28 = OpLoad %float %20\r\n         %29 = OpConvertFToS %int %28\r\n         %30 = OpFunctionCall %int %normalFuncUsesGeneric %29\r\n         %33 = OpIAdd %int %24 %30\r\n               OpStore %11 %33\r\n               OpReturn\r\n               OpFunctionEnd\r\n```\n", "patch": "diff --git a/source/opt/aggressive_dead_code_elim_pass.cpp b/source/opt/aggressive_dead_code_elim_pass.cpp\nindex 7eb158d5e6..6e86c378b9 100644\n--- a/source/opt/aggressive_dead_code_elim_pass.cpp\n+++ b/source/opt/aggressive_dead_code_elim_pass.cpp\n@@ -267,6 +267,7 @@ void AggressiveDCEPass::AddBreaksAndContinuesToWorklist(\n }\n \n bool AggressiveDCEPass::AggressiveDCE(Function* func) {\n+  if (func->IsDeclaration()) return false;\n   std::list<BasicBlock*> structured_order;\n   cfg()->ComputeStructuredOrder(func, &*func->begin(), &structured_order);\n   live_local_vars_.clear();\ndiff --git a/source/opt/mem_pass.cpp b/source/opt/mem_pass.cpp\nindex 80ec8da18e..65f45ec3b8 100644\n--- a/source/opt/mem_pass.cpp\n+++ b/source/opt/mem_pass.cpp\n@@ -415,6 +415,7 @@ void MemPass::RemoveBlock(Function::iterator* bi) {\n }\n \n bool MemPass::RemoveUnreachableBlocks(Function* func) {\n+  if (func->IsDeclaration()) return false;\n   bool modified = false;\n \n   // Mark reachable all blocks reachable from the function's entry block.\n", "instance_id": "KhronosGroup__SPIRV-Tools-5796", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: `spirv-opt` crashes when optimizing SPIR-V input containing function declarations without blocks. The provided SPIR-V code snippet and the description of the crash scenario help in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem does not specify which optimization passes are crashing or under what specific conditions beyond the presence of function declarations. Additionally, there are no explicit examples of expected behavior or output after the fix, nor are there detailed constraints or edge cases mentioned (e.g., specific SPIR-V versions or other language features that might interact with this issue). While the goal is clear\u2014prevent crashes during optimization\u2014the lack of comprehensive details about the scope of affected passes or test cases slightly reduces the clarity.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is relatively narrow, affecting only two files (`aggressive_dead_code_elim_pass.cpp` and `mem_pass.cpp`), with minimal lines of code modified (adding a check for function declarations). However, the changes require a deep understanding of the SPIR-V intermediate representation and the internal workings of the `spirv-opt` tool, which is a specialized domain within graphics programming and compiler optimization. The technical concepts involved include knowledge of SPIR-V semantics (e.g., function declarations vs. definitions), optimization pass design, and the specific behavior of passes like Aggressive Dead Code Elimination (DCE) and memory-related optimizations. Additionally, while the provided code changes are straightforward (adding a guard condition), ensuring correctness likely involves understanding the broader implications on other optimization passes and potential edge cases, such as interactions with other SPIR-V features or different types of function declarations. Error handling is implicitly addressed by preventing crashes, but the problem does not explicitly discuss other error conditions or performance impacts. The need for domain-specific knowledge and the potential for subtle bugs or regressions in a complex system like `spirv-opt` justifies a difficulty score of 0.65, placing it in the lower end of the \"Hard\" range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ARM64 support\nAs mentioned in #1782 -- arm64 and it's variants (armv7, armv8, aarch64) are here to stay -- and likely in much higher usage these days with IoT (ie esp32, etc), Raspberry PIs and it's alternatives, and of course AWS with it's Graviton 2 & 3 processors (which have been steadily & rapidly growing in usage especially given higher performance and lower cost compared to their non-Graviton instance types).\r\n\r\nOpening this request hoping that `psutil` will reconsider providing wheels for these types of processors. Many companies don't allow software such as gcc, devel packages, etc on production servers and so for many, including myself, it is impossible to deploy `psutil` into a production environment.\r\n\r\nGiven the high value of `psutil` and that it is often embedded into other packages (meaning you can't use alternatives), hopefully you will reconsider.\r\n\r\nWhile I realize #1972 exists, this request is to support more then just the `aarch64` variant.\nwheels: add arm64 builds\n## Summary\r\n\r\n* OS: windows, arm, mac\r\n* Bug fix: no\r\n* Type: wheels\r\n* Fixes: #2206, #1972, #1966 \r\n\r\n## Description\r\n\r\n- add arm64 builds via qemu\r\n- add macos universal2 build\r\n- add windows arm64 build\r\n- bump cancel workflow action to latest to remove nodejs warnings\r\n- bump cibuildwheel action to latest\r\n\n", "patch": "diff --git a/.github/workflows/build.yml b/.github/workflows/build.yml\nindex c4e9c745e..d23bd179c 100644\n--- a/.github/workflows/build.yml\n+++ b/.github/workflows/build.yml\n@@ -18,23 +18,20 @@ concurrency:\n jobs:\n   # Linux + macOS + Windows Python 3\n   py3:\n-    name: py3-${{ matrix.os }}-${{ startsWith(matrix.os, 'ubuntu') && 'all' || matrix.archs }}\n+    name: \"py3-${{ matrix.os }}-${{ matrix.arch }}\"\n     runs-on: ${{ matrix.os }}\n     timeout-minutes: 30\n     strategy:\n       fail-fast: false\n       matrix:\n         include:\n-        - os: ubuntu-latest\n-          archs: \"x86_64 i686\"\n-        - os: macos-12\n-          archs: \"x86_64\"\n-        - os: macos-14\n-          archs: \"arm64\"\n-        - os: windows-2019\n-          archs: \"AMD64\"\n-        - os: windows-2019\n-          archs: \"x86\"\n+        - {os: ubuntu-latest, arch: x86_64}\n+        - {os: ubuntu-latest, arch: i686}\n+        - {os: ubuntu-latest, arch: aarch64}\n+        - {os: macos-12, arch: x86_64}\n+        - {os: macos-14, arch: arm64}\n+        - {os: windows-2019, arch: AMD64}\n+        - {os: windows-2019, arch: x86}\n     steps:\n     - uses: actions/checkout@v4\n \n@@ -51,16 +48,20 @@ jobs:\n       with:\n         python-version: 3.11\n \n+    - name: Set up QEMU\n+      uses: docker/setup-qemu-action@v3\n+      if: matrix.arch == 'aarch64'\n+\n     - name: Create wheels + run tests\n-      uses: pypa/cibuildwheel@v2.18.0\n+      uses: pypa/cibuildwheel@v2.19.1\n       env:\n-        CIBW_ARCHS: \"${{ matrix.archs }}\"\n+        CIBW_ARCHS: \"${{ matrix.arch }}\"\n         CIBW_PRERELEASE_PYTHONS: True\n \n     - name: Upload wheels\n       uses: actions/upload-artifact@v4\n       with:\n-        name: wheels-py3-${{ matrix.os }}-${{ startsWith(matrix.os, 'ubuntu') && 'all' || matrix.archs }}\n+        name: wheels-py3-${{ matrix.os }}-${{ matrix.arch }}\n         path: wheelhouse\n \n     - name: Generate .tar.gz\ndiff --git a/HISTORY.rst b/HISTORY.rst\nindex ae664900f..5107b1345 100644\n--- a/HISTORY.rst\n+++ b/HISTORY.rst\n@@ -18,6 +18,7 @@\n - 2407_: `Process.connections()`_ was renamed to `Process.net_connections()`_.\n   The old name is still available, but it's deprecated (triggers a\n   ``DeprecationWarning``) and will be removed in the future.\n+- 2425_: [Linux]: provide aarch64 wheels.  (patch by Matthieu Darbois / Ben Raz)\n \n **Bug fixes**\n \ndiff --git a/pyproject.toml b/pyproject.toml\nindex b03392071..ba13f2829 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -207,6 +207,7 @@ trailing_comma_inline_array = true\n skip = [\n     \"*-musllinux*\",\n     \"cp313-win*\",  # pywin32 is not available on cp313 yet\n+    \"cp3{7,8,9,10,11,12}-*linux_{aarch64,ppc64le,s390x}\",  # Only test cp36/cp313 on qemu tested architectures\n     \"pp*\",\n ]\n test-command = [\n@@ -218,6 +219,9 @@ test-extras = \"test\"\n [tool.cibuildwheel.macos]\n archs = [\"arm64\", \"x86_64\"]\n \n+[tool.cibuildwheel.linux]\n+before-all = \"yum install -y net-tools\"\n+\n [build-system]\n build-backend = \"setuptools.build_meta\"\n requires = [\"setuptools>=43\", \"wheel\"]\n", "instance_id": "giampaolo__psutil-2425", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to add ARM64 support (including variants like aarch64) for the `psutil` library by providing wheels for multiple platforms (Windows, macOS, and Linux). It references specific issues (#2206, #1972, #1966) and provides a high-level summary of the goal (adding ARM64 builds via QEMU, macOS universal2 build, and Windows ARM64 build). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the exact scope of \"ARM64 variants\" beyond aarch64, nor does it specify constraints or requirements for compatibility, performance, or testing on these architectures. Additionally, there are no examples or detailed expectations for how the builds should behave or what specific challenges might arise during implementation. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting the CI/CD configuration in `.github/workflows/build.yml` and minor updates to `pyproject.toml` and `HISTORY.rst`. The changes involve adding support for new architectures (e.g., aarch64 on Linux) using QEMU for emulation, updating build configurations, and ensuring compatibility across platforms. This requires understanding of CI/CD workflows (GitHub Actions), cross-platform build tools (cibuildwheel), and platform-specific build requirements (e.g., macOS universal2, Windows ARM64). The technical concepts involved include cross-compilation, emulation with QEMU, and managing build artifacts, which are moderately complex but not overly specialized. The changes do not significantly impact the core architecture of the `psutil` library itself, as they are confined to the build system. However, potential challenges arise in ensuring compatibility across diverse hardware and OS combinations, which may involve handling edge cases related to architecture-specific bugs or build failures. Error handling in the build process might need to be considered, though it is not explicitly mentioned in the problem statement. Overall, this task requires a moderate level of expertise in build systems and cross-platform development, but it does not demand deep architectural changes or advanced domain-specific knowledge, placing it at a difficulty of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Allow 'integer' in select_dtypes include argument \n**Describe the bug**\r\n#900 added better typing for the select_dtypes include and exclude arguments.\r\nHowever, it missed at least one allowable include variable type in the AstypeArgExt literal list.\r\n\"integer\" should be added to that literal list as it is allowed in the select_dtypes include/exclude arguments and will include or exclude all integer types.\r\nThere are likely a few other numpy scalar abstract base classes are also valid here, probably everything within a dashed-line box on the Hierarchy of type objects diagram located here: https://numpy.org/doc/stable/reference/arrays.scalars.html\r\nOf those, only \"number\" has been added to the Literal list, but it is likely that anyone may want to include/exclude any of the other numpy \r\n\r\n**To Reproduce**\r\nFollowing code produces an error in mypy:\r\n```\r\nimport pandas as pd\r\nfrom numpy import double, int16\r\n\r\ncol_cat = pd.Series(['a', 'b', 'c', 'a'], dtype='category', name='col_cat')\r\ncol_bool = pd.Series([True, True, False, True], dtype=bool, name='col_bool')\r\ncol_int_0 = pd.Series([1, 2, 3, 4], dtype=int, name='col_int_0')\r\ncol_int_1 = pd.Series([1, 2, 3, 4], dtype=int16, name='col_int_1')\r\ncol_float_1 = pd.Series([1.1, 2.2, 3.3, 4.4], dtype=double, name='col_float_0')\r\n\r\nX = pd.concat([col_cat, col_bool, col_int_0, col_int_1, col_float_1], axis=1)\r\n\r\ninteger_cols_ = list(X.select_dtypes(include=['integer']).columns)\r\n```\r\n\r\nmypy error is on the line `inter_cols_ = ...` and is:\r\n```\r\nerror: List item 0 has incompatible type \"Literal['integer']\"; expected \"Literal['bool', 'boolean', '?', 'b1', 'bool8', 'bool_', 'bool[pyarrow]', 'boolean[pyarrow]', 'int', 'Int8', 'Int16', 'Int32', 'Int64', 'b', 'i1', 'int8', 'byte', 'h', 'i2', 'int16', 'short', 'i', 'i4', 'int32', 'intc', 'l', 'i8', 'int64', 'int_', 'long', 'q', 'longlong', 'p', 'intp', 'int0', 'int8[pyarrow]', 'int16[pyarrow]', 'int32[pyarrow]', 'int64[pyarrow]', 'UInt8', 'UInt16', 'UInt32', 'UInt64', 'B', 'u1', 'uint8', 'ubyte', 'H', 'u2', 'uint16', 'ushort', 'I', 'u4', 'uint32', 'uintc', 'L', 'u8', 'uint', 'ulong', 'uint64', 'Q', 'ulonglong', 'P', 'uintp', 'uint0', 'uint8[pyarrow]', 'uint16[pyarrow]', 'uint32[pyarrow]', 'uint64[pyarrow]', 'str', 'string', 'U', 'str_', 'str0', 'unicode', 'unicode_', 'string[pyarrow]', 'bytes', 'S', 'a', 'bytes_', 'bytes0', 'string_', 'binary[pyarrow]', 'float', 'Float32', 'Float64', 'e', 'f2', '<f2', 'float16', 'half', 'f', 'f4', 'float32', 'single', 'd', 'f8', 'float64', 'double', 'float_', 'g', 'f16', 'float128', 'longdouble', 'longfloat', 'float[pyarrow]', 'double[pyarrow]', 'float16[pyarrow]', 'float32[pyarrow]', 'float64[pyarrow]', 'complex', 'F', 'c8', 'complex64', 'csingle', 'singlecomplex', 'D', 'c16', 'complex128', 'cdouble', 'cfloat', 'complex_', 'G', 'c32', 'complex256', 'clongdouble', 'clongfloat', 'longcomplex', 'timedelta64[Y]', 'timedelta64[M]', 'timedelta64[W]', 'timedelta64[D]', 'timedelta64[h]', 'timedelta64[m]', 'timedelta64[s]', 'timedelta64[ms]', 'timedelta64[us]', 'timedelta64[\u03bcs]', 'timedelta64[ns]', 'timedelta64[ps]', 'timedelta64[fs]', 'timedelta64[as]', 'm8[Y]', 'm8[M]', 'm8[W]', 'm8[D]', 'm8[h]', 'm8[m]', 'm8[s]', 'm8[ms]', 'm8[us]', 'm8[\u03bcs]', 'm8[ns]', 'm8[ps]', 'm8[fs]', 'm8[as]', '<m8[Y]', '<m8[M]', '<m8[W]', '<m8[D]', '<m8[h]', '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[\u03bcs]', '<m8[ns]', '<m8[ps]', '<m8[fs]', '<m8[as]', 'duration[s][pyarrow]', 'duration[ms][pyarrow]', 'duration[us][pyarrow]', 'duration[ns][pyarrow]', 'datetime64[Y]', 'datetime64[M]', 'datetime64[W]', 'datetime64[D]', 'datetime64[h]', 'datetime64[m]', 'datetime64[s]', 'datetime64[ms]', 'datetime64[us]', 'datetime64[\u03bcs]', 'datetime64[ns]', 'datetime64[ps]', 'datetime64[fs]', 'datetime64[as]', 'M8[Y]', 'M8[M]', 'M8[W]', 'M8[D]', 'M8[h]', 'M8[m]', 'M8[s]', 'M8[ms]', 'M8[us]', 'M8[\u03bcs]', 'M8[ns]', 'M8[ps]', 'M8[fs]', 'M8[as]', '<M8[Y]', '<M8[M]', '<M8[W]', '<M8[D]', '<M8[h]', '<M8[m]', '<M8[s]', '<M8[ms]', '<M8[us]', '<M8[\u03bcs]', '<M8[ns]', '<M8[ps]', '<M8[fs]', '<M8[as]', 'date32[pyarrow]', 'date64[pyarrow]', 'timestamp[s][pyarrow]', 'timestamp[ms][pyarrow]', 'timestamp[us][pyarrow]', 'timestamp[ns][pyarrow]', 'category', 'object', 'O', 'V', 'void', 'void0', 'number', 'datetime64', 'datetime', 'timedelta', 'timedelta64', 'datetimetz'] | type[object] | dtype[generic] | ExtensionDtype\"  [list-item]\r\n```\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Linux\r\n - OS Version: python:3.12.3-slim docker container\r\n - python version: 3.12.3\r\n - version of type checker: 1.10.0\r\n - version of installed `pandas-stubs`: 2.2.2.24054 \r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 20879b00..8607343c 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -624,6 +624,7 @@ class DataFrame(NDFrame, OpsMixin):\n             \"number\",\n             \"datetime64\",\n             \"datetime\",\n+            \"integer\",\n             \"timedelta\",\n             \"timedelta64\",\n             \"datetimetz\",\n", "instance_id": "pandas-dev__pandas-stubs-968", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `select_dtypes` method in pandas does not accept 'integer' as a valid argument in the type hints for `include` and `exclude` parameters, leading to a mypy error. The goal is to update the type hints to include 'integer' as a valid literal. The statement provides a reproducible example and the specific error message, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, while it mentions that other numpy scalar abstract base classes might also be valid (referencing a diagram), it does not specify whether the task includes adding those types or just 'integer'. Additionally, edge cases or potential side effects of adding 'integer' to the type hints are not discussed. Overall, the problem is valid and mostly clear but lacks comprehensive detail on the full scope and potential implications.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to a single line in a type hint definition within the `pandas-stubs` repository. The code change is minimal, requiring only the addition of 'integer' to a list of literals in a `.pyi` file, as shown in the diff. The scope is limited to a single file and does not impact the broader codebase architecture or functionality, since it is purely a type hint update for static type checking with mypy. The technical concepts involved are basic\u2014understanding Python type hints and the purpose of stub files in the `pandas-stubs` library. No complex algorithms, design patterns, or domain-specific knowledge beyond basic familiarity with pandas and numpy are required. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the change itself does not introduce new logic that would necessitate such considerations. Overall, this is a very easy task that requires minimal effort and expertise, fitting within the 0.0-0.2 range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Inference for `gaussian_hmm` is broken on latest jax version (0.4.30)\nThe following works with jax/jaxlib 0.4.29, but is broken with jax/jaxlib 0.4.30:\r\n\r\nCopying from the [current documentation](https://num.pyro.ai/en/stable/primitives.html#numpyro.contrib.control_flow.scan):\r\n\r\n```python\r\nimport numpy as np\r\nimport numpyro\r\nimport numpyro.distributions as dist\r\nfrom numpyro.contrib.control_flow import scan\r\n\r\ndef gaussian_hmm(y=None, T=10):\r\n    def transition(x_prev, y_curr):\r\n        x_curr = numpyro.sample('x', dist.Normal(x_prev, 1))\r\n        y_curr = numpyro.sample('y', dist.Normal(x_curr, 1), obs=y_curr)\r\n        return x_curr, (x_curr, y_curr)\r\n\r\n    x0 = numpyro.sample('x_0', dist.Normal(0, 1))\r\n    _, (x, y) = scan(transition, x0, y, length=T)\r\n    return (x, y)\r\n\r\n\r\nwith numpyro.handlers.seed(rng_seed=0):  # generative\r\n    x, y = gaussian_hmm()\r\n```\r\n\r\nIf I then try to perform inference:\r\n```python\r\nfrom jax import random\r\nfrom numpyro.infer import MCMC, NUTS\r\nnuts_kernel = NUTS(gaussian_hmm)\r\n\r\nmcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\r\nrng_key = random.PRNGKey(0)\r\n\r\nmcmc.run(rng_key, y=y)\r\n```\r\n\r\nI get the error\r\n```pycon\r\nTypeError: body_fun output and input must have identical types, got\r\n('ShapedArray(int32[], weak_type=True)', ['ShapedArray(float32[10])', 'DIFFERENT ShapedArray(int32[], weak_type=True) vs. ShapedArray(float0[])', 'ShapedArray(float32[])'], []).\r\n```\n", "patch": "diff --git a/examples/annotation.py b/examples/annotation.py\nindex 3341dbe72..a28ab48be 100644\n--- a/examples/annotation.py\n+++ b/examples/annotation.py\n@@ -320,7 +320,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Bayesian Models of Annotation\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/ar2.py b/examples/ar2.py\nindex b6f0e737a..6ebb5949e 100644\n--- a/examples/ar2.py\n+++ b/examples/ar2.py\n@@ -138,7 +138,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"AR2 example\")\n     parser.add_argument(\"--num-data\", nargs=\"?\", default=142, type=int)\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/baseball.py b/examples/baseball.py\nindex 1588d5898..47615c37a 100644\n--- a/examples/baseball.py\n+++ b/examples/baseball.py\n@@ -210,7 +210,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Baseball batting average using MCMC\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=3000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1500, type=int)\ndiff --git a/examples/bnn.py b/examples/bnn.py\nindex 9dd29d291..022e88d9e 100644\n--- a/examples/bnn.py\n+++ b/examples/bnn.py\n@@ -160,7 +160,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Bayesian neural network example\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/covtype.py b/examples/covtype.py\nindex 8b59b99dc..424eac7f7 100644\n--- a/examples/covtype.py\n+++ b/examples/covtype.py\n@@ -206,7 +206,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"parse args\")\n     parser.add_argument(\n         \"-n\", \"--num-samples\", default=1000, type=int, help=\"number of samples\"\ndiff --git a/examples/funnel.py b/examples/funnel.py\nindex 2e9c3d1b6..bb28e7dfa 100644\n--- a/examples/funnel.py\n+++ b/examples/funnel.py\n@@ -139,7 +139,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(\n         description=\"Non-centered reparameterization example\"\n     )\ndiff --git a/examples/gaussian_shells.py b/examples/gaussian_shells.py\nindex 98870b167..9c2184eb4 100644\n--- a/examples/gaussian_shells.py\n+++ b/examples/gaussian_shells.py\n@@ -120,7 +120,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Nested sampler for Gaussian shells\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=10000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/gp.py b/examples/gp.py\nindex 0f70400db..dfa22f624 100644\n--- a/examples/gp.py\n+++ b/examples/gp.py\n@@ -180,7 +180,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Gaussian Process example\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/hmm.py b/examples/hmm.py\nindex c414ba59d..3d17b9366 100644\n--- a/examples/hmm.py\n+++ b/examples/hmm.py\n@@ -263,7 +263,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Semi-supervised Hidden Markov Model\")\n     parser.add_argument(\"--num-categories\", default=3, type=int)\n     parser.add_argument(\"--num-words\", default=10, type=int)\ndiff --git a/examples/holt_winters.py b/examples/holt_winters.py\nindex 621f0eb35..fb97e62bd 100644\n--- a/examples/holt_winters.py\n+++ b/examples/holt_winters.py\n@@ -180,7 +180,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Holt-Winters\")\n     parser.add_argument(\"--T\", nargs=\"?\", default=6, type=int)\n     parser.add_argument(\"--future\", nargs=\"?\", default=1, type=int)\ndiff --git a/examples/horseshoe_regression.py b/examples/horseshoe_regression.py\nindex 834483aa6..ee1624cc2 100644\n--- a/examples/horseshoe_regression.py\n+++ b/examples/horseshoe_regression.py\n@@ -162,7 +162,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Horseshoe regression example\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/minipyro.py b/examples/minipyro.py\nindex aeea67d0d..8d7094de3 100644\n--- a/examples/minipyro.py\n+++ b/examples/minipyro.py\n@@ -58,7 +58,7 @@ def body_fn(i, val):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Mini Pyro demo\")\n     parser.add_argument(\"-f\", \"--full-pyro\", action=\"store_true\", default=False)\n     parser.add_argument(\"-n\", \"--num-steps\", default=1001, type=int)\ndiff --git a/examples/mortality.py b/examples/mortality.py\nindex 27a624511..0ee18dc06 100644\n--- a/examples/mortality.py\n+++ b/examples/mortality.py\n@@ -220,7 +220,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n \n     parser = argparse.ArgumentParser(description=\"Mortality regression model\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=500, type=int)\ndiff --git a/examples/neutra.py b/examples/neutra.py\nindex 780e420f2..c47d8f94e 100644\n--- a/examples/neutra.py\n+++ b/examples/neutra.py\n@@ -197,7 +197,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"NeuTra HMC\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=4000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/ode.py b/examples/ode.py\nindex cd5a6eabe..3845dcd9c 100644\n--- a/examples/ode.py\n+++ b/examples/ode.py\n@@ -117,7 +117,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Predator-Prey Model\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\ndiff --git a/examples/prodlda.py b/examples/prodlda.py\nindex 1c82b27a7..0a30d202d 100644\n--- a/examples/prodlda.py\n+++ b/examples/prodlda.py\n@@ -315,7 +315,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(\n         description=\"Probabilistic topic modelling with Flax and Haiku\"\n     )\ndiff --git a/examples/sparse_regression.py b/examples/sparse_regression.py\nindex 84f1ae8b5..f0c9781d4 100644\n--- a/examples/sparse_regression.py\n+++ b/examples/sparse_regression.py\n@@ -384,7 +384,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Gaussian Process example\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=500, type=int)\ndiff --git a/examples/stochastic_volatility.py b/examples/stochastic_volatility.py\nindex 7f1583b94..a4ce6d7fd 100644\n--- a/examples/stochastic_volatility.py\n+++ b/examples/stochastic_volatility.py\n@@ -122,7 +122,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Stochastic Volatility Model\")\n     parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=600, type=int)\n     parser.add_argument(\"--num-warmup\", nargs=\"?\", default=600, type=int)\ndiff --git a/examples/thompson_sampling.py b/examples/thompson_sampling.py\nindex 1e61e4890..e10205b72 100644\n--- a/examples/thompson_sampling.py\n+++ b/examples/thompson_sampling.py\n@@ -292,7 +292,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Thompson sampling example\")\n     parser.add_argument(\n         \"--num-random\", nargs=\"?\", default=2, type=int, help=\"number of random draws\"\ndiff --git a/examples/toy_mixture_model_discrete_enumeration.py b/examples/toy_mixture_model_discrete_enumeration.py\nindex ae68d170a..f64bdb98b 100644\n--- a/examples/toy_mixture_model_discrete_enumeration.py\n+++ b/examples/toy_mixture_model_discrete_enumeration.py\n@@ -126,7 +126,7 @@ def get_true_pred_CPDs(CPD, posterior_param):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"Toy mixture model\")\n     parser.add_argument(\"-n\", \"--num-steps\", default=4000, type=int)\n     parser.add_argument(\"-o\", \"--num-obs\", default=10000, type=int)\ndiff --git a/examples/ucbadmit.py b/examples/ucbadmit.py\nindex f35cb03f7..4147067c6 100644\n--- a/examples/ucbadmit.py\n+++ b/examples/ucbadmit.py\n@@ -151,7 +151,7 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(\n         description=\"UCBadmit gender discrimination using HMC\"\n     )\ndiff --git a/examples/vae.py b/examples/vae.py\nindex f9f5732d0..5543c30d0 100644\n--- a/examples/vae.py\n+++ b/examples/vae.py\n@@ -160,7 +160,7 @@ def reconstruct_img(epoch, rng_key):\n \n \n if __name__ == \"__main__\":\n-    assert numpyro.__version__.startswith(\"0.15.0\")\n+    assert numpyro.__version__.startswith(\"0.15.1\")\n     parser = argparse.ArgumentParser(description=\"parse args\")\n     parser.add_argument(\n         \"-n\", \"--num-epochs\", default=15, type=int, help=\"number of training epochs\"\ndiff --git a/notebooks/source/bad_posterior_geometry.ipynb b/notebooks/source/bad_posterior_geometry.ipynb\nindex 3d61fd14e..68c2f0c56 100644\n--- a/notebooks/source/bad_posterior_geometry.ipynb\n+++ b/notebooks/source/bad_posterior_geometry.ipynb\n@@ -49,7 +49,7 @@\n     \"import numpyro.distributions as dist\\n\",\n     \"from numpyro.infer import MCMC, NUTS\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\\n\",\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\\n\",\n     \"\\n\",\n     \"# NB: replace cpu by gpu to run this notebook on gpu\\n\",\n     \"numpyro.set_platform(\\\"cpu\\\")\"\ndiff --git a/notebooks/source/bayesian_hierarchical_linear_regression.ipynb b/notebooks/source/bayesian_hierarchical_linear_regression.ipynb\nindex 8d29d9158..36d0f743f 100644\n--- a/notebooks/source/bayesian_hierarchical_linear_regression.ipynb\n+++ b/notebooks/source/bayesian_hierarchical_linear_regression.ipynb\n@@ -246,7 +246,7 @@\n     \"import numpyro.distributions as dist\\n\",\n     \"from numpyro.infer import MCMC, NUTS, Predictive\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/bayesian_hierarchical_stacking.ipynb b/notebooks/source/bayesian_hierarchical_stacking.ipynb\nindex e03174ef7..dff9d67ad 100644\n--- a/notebooks/source/bayesian_hierarchical_stacking.ipynb\n+++ b/notebooks/source/bayesian_hierarchical_stacking.ipynb\n@@ -97,7 +97,7 @@\n     \"    set_matplotlib_formats(\\\"svg\\\")\\n\",\n     \"\\n\",\n     \"numpyro.set_host_device_count(4)\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/bayesian_imputation.ipynb b/notebooks/source/bayesian_imputation.ipynb\nindex ab223bdf1..b1b0381da 100644\n--- a/notebooks/source/bayesian_imputation.ipynb\n+++ b/notebooks/source/bayesian_imputation.ipynb\n@@ -52,7 +52,7 @@\n     \"if \\\"NUMPYRO_SPHINXBUILD\\\" in os.environ:\\n\",\n     \"    set_matplotlib_formats(\\\"svg\\\")\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/bayesian_regression.ipynb b/notebooks/source/bayesian_regression.ipynb\nindex f1a8bcf20..1c7a8ae40 100644\n--- a/notebooks/source/bayesian_regression.ipynb\n+++ b/notebooks/source/bayesian_regression.ipynb\n@@ -73,7 +73,7 @@\n     \"if \\\"NUMPYRO_SPHINXBUILD\\\" in os.environ:\\n\",\n     \"    set_matplotlib_formats(\\\"svg\\\")\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/censoring.ipynb b/notebooks/source/censoring.ipynb\nindex a1c2d6dff..204a02c1f 100644\n--- a/notebooks/source/censoring.ipynb\n+++ b/notebooks/source/censoring.ipynb\n@@ -60,7 +60,7 @@\n     \"\\n\",\n     \"rng_key = random.PRNGKey(seed=0)\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\\n\",\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\\n\",\n     \"\\n\",\n     \"%load_ext autoreload\\n\",\n     \"%autoreload 2\\n\",\ndiff --git a/notebooks/source/gmm.ipynb b/notebooks/source/gmm.ipynb\nindex b64e65167..f45a07b1f 100644\n--- a/notebooks/source/gmm.ipynb\n+++ b/notebooks/source/gmm.ipynb\n@@ -54,7 +54,7 @@\n     \"%matplotlib inline\\n\",\n     \"\\n\",\n     \"smoke_test = \\\"CI\\\" in os.environ\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/hsgp_example.ipynb b/notebooks/source/hsgp_example.ipynb\nindex 0bfcafc23..d5bef357f 100644\n--- a/notebooks/source/hsgp_example.ipynb\n+++ b/notebooks/source/hsgp_example.ipynb\n@@ -62,7 +62,7 @@\n     \"\\n\",\n     \"rng_key = random.PRNGKey(seed=42)\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\\n\",\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\\n\",\n     \"\\n\",\n     \"%load_ext autoreload\\n\",\n     \"%autoreload 2\\n\",\ndiff --git a/notebooks/source/logistic_regression.ipynb b/notebooks/source/logistic_regression.ipynb\nindex 0432ca010..917004657 100644\n--- a/notebooks/source/logistic_regression.ipynb\n+++ b/notebooks/source/logistic_regression.ipynb\n@@ -41,7 +41,7 @@\n     \"from numpyro.examples.datasets import COVTYPE, load_dataset\\n\",\n     \"from numpyro.infer import HMC, MCMC, NUTS\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\\n\",\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\\n\",\n     \"\\n\",\n     \"# NB: replace gpu by cpu to run this notebook in cpu\\n\",\n     \"numpyro.set_platform(\\\"gpu\\\")\"\ndiff --git a/notebooks/source/model_rendering.ipynb b/notebooks/source/model_rendering.ipynb\nindex 359d6886a..d06abaf33 100644\n--- a/notebooks/source/model_rendering.ipynb\n+++ b/notebooks/source/model_rendering.ipynb\n@@ -38,7 +38,7 @@\n     \"import numpyro.distributions as dist\\n\",\n     \"import numpyro.distributions.constraints as constraints\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/ordinal_regression.ipynb b/notebooks/source/ordinal_regression.ipynb\nindex fcb610095..174de9f9c 100644\n--- a/notebooks/source/ordinal_regression.ipynb\n+++ b/notebooks/source/ordinal_regression.ipynb\n@@ -54,7 +54,7 @@\n     \"from numpyro.infer import MCMC, NUTS\\n\",\n     \"from numpyro.infer.reparam import TransformReparam\\n\",\n     \"\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/notebooks/source/time_series_forecasting.ipynb b/notebooks/source/time_series_forecasting.ipynb\nindex 40f467ee0..f535cecd5 100644\n--- a/notebooks/source/time_series_forecasting.ipynb\n+++ b/notebooks/source/time_series_forecasting.ipynb\n@@ -48,7 +48,7 @@\n     \"    set_matplotlib_formats(\\\"svg\\\")\\n\",\n     \"\\n\",\n     \"numpyro.set_host_device_count(4)\\n\",\n-    \"assert numpyro.__version__.startswith(\\\"0.15.0\\\")\"\n+    \"assert numpyro.__version__.startswith(\\\"0.15.1\\\")\"\n    ]\n   },\n   {\ndiff --git a/numpyro/version.py b/numpyro/version.py\nindex c44b61ed5..4618c3c4f 100644\n--- a/numpyro/version.py\n+++ b/numpyro/version.py\n@@ -1,4 +1,4 @@\n # Copyright Contributors to the Pyro project.\n # SPDX-License-Identifier: Apache-2.0\n \n-__version__ = \"0.15.0\"\n+__version__ = \"0.15.1\"\n", "instance_id": "pyro-ppl__numpyro-1830", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: inference for the `gaussian_hmm` model is broken in the latest JAX version (0.4.30), while it works in version 0.4.29. It provides a specific code snippet from the documentation, demonstrates the error during inference with MCMC and NUTS, and includes the exact error message. This gives a clear starting point for understanding the issue. However, there are minor ambiguities and missing details. For instance, it does not explicitly state whether the issue lies in JAX's internal changes, NumPyro's compatibility, or the specific usage of the `scan` primitive. Additionally, there are no mentions of specific edge cases, constraints, or expected behavior beyond the error message. While the goal (fixing the inference) is implied, it could benefit from a more explicit statement of the desired outcome or potential root causes. Hence, I rate it as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of the issue involves compatibility between NumPyro and a specific version of JAX (0.4.30), which requires a deep understanding of both libraries' internals, particularly how NumPyro's control flow primitives like `scan` interact with JAX's JIT compilation and type system. The error message indicates a type mismatch in the `body_fun` output/input of the `scan` operation, suggesting a need to debug and potentially modify the implementation of the `gaussian_hmm` model or NumPyro's handling of JAX arrays. This involves advanced knowledge of JAX's `ShapedArray` and type system, as well as NumPyro's inference mechanisms (e.g., MCMC with NUTS).\n\nSecond, while the provided code changes only update the version string from \"0.15.0\" to \"0.15.1\" across multiple files (a trivial change), this does not address the core issue described in the problem statement. The actual fix is not reflected in the diff, implying that the real solution likely requires significant modifications to the `gaussian_hmm` implementation or NumPyro's internals, potentially impacting multiple modules. Understanding the interaction between JAX's updates and NumPyro's control flow would be critical, and this could involve tracing through the codebase to identify where type mismatches occur.\n\nThird, the technical concepts involved are complex, including probabilistic programming, hidden Markov models (HMMs), JAX's functional programming paradigm, and NumPyro's inference algorithms. Solving this may require familiarity with advanced topics like automatic differentiation, JIT compilation, and type consistency in functional transformations.\n\nFinally, while the problem statement does not explicitly mention edge cases, the nature of inference in probabilistic models inherently involves handling numerical stability, convergence issues, and data-dependent behaviors, which could complicate the fix. Given the lack of a direct solution in the code changes and the need for deep library-specific knowledge, I assign a difficulty score of 0.75, reflecting a challenging problem that requires significant expertise and effort to resolve.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Body negative local drag coefficients\nStrictly speaking, this is a bug that I encountered on our internal MoorDyn fork, which is somewhat out of date. I don't see anything in the latest public code that would address this, but I haven't directly tested this behavior with the latest branch of this repo.\r\n\r\nWhen bodies are in certain orientations, the computed global drag coefficients can end up having negative values.\r\n\r\nThe code at fault is\r\nhttps://github.com/FloatingArrayDesign/MoorDyn/blob/99cd2b6e6afa7e5d58e2ac7ac3a4b05e127f1f72/source/Body.cpp#L496-L501\r\n\r\nAn example of a case where this fails to work is when the body's pitch is -90 degrees. The rotation matrix in this case will be\r\n```\r\n[  0.0,  0.0, -1.0;\r\n   0.0,  1.0,  0.0;\r\n   1.0,  0.0,  0.0 ]\r\n```\r\n\r\nThe `cda` variable will look like `bodyCda * [1, 1, -1, 1, 1, -1]`. Which leads to a negative drag coefficient in the z axis, which is a destabilizing force that is both not physical and will eventually cause the simulation to error out.\r\n\r\nI'm making this issue because I'm not sure what the correct solution to this issue is. I think it's valid to just take the absolute value of the rotated drag coefficients, but I'll defer to others for how they want to address this.\nPrevent body drag coeffs from being negative\nfixes #205 \r\n\r\nI think this should be a sufficient change to address the issue of getting negative drag coefficients in certain orientations.\r\n\r\nThe new body test fails without this change because its z drag coefficient is negative, causing the body to fly off the infinity. With the fix the test passes normally. Once others are happy with this change, I'll fix the merge conflict.\n", "patch": "diff --git a/docs/inputs.rst b/docs/inputs.rst\nindex 3568a63b..c08d3f17 100644\n--- a/docs/inputs.rst\n+++ b/docs/inputs.rst\n@@ -353,12 +353,18 @@ The columns are as follows:\n  - CG \u2013 Body center of gravity. If one value given, it is along the Z axis. To specify a coordinate \n    point, the XYZ values are listed separated by | with no spaces (m)\n  - I \u2013 Body moment of inertia diagonals for the 3x3 inertia matrix. If one value given, it is used \n-   for all three values. To specify different values, the inputs are listed separated by | with no \n+   for all three values. To specify three different values, the inputs are listed separated by | with no \n    spaces (kg-m^2)\n  - Volume \u2013 The body displaced volume used in buoyancy calculations excluding attached rod and \n    point volume contributions (m^3)\n- - CdA \u2013 The product of drag coefficient and frontal area of body (m^2)\n- - Ca \u2013 The body added mass coefficient\n+ - CdA \u2013 The product of drag coefficient and frontal area of body. If one value given, it is used \n+   for all six values. If two values are given, the first is used for the translational drag and \n+   the second for the rotational drag. If three values are given, they are used as both the \n+   translational and rotational drag coefficicents for the respective axis. If six values are given, \n+   they are used as the drag coefficicnets in the respective degree of freedom. To specify different \n+   values, the inputs are listed separated by | with no spaces (m^2)\n+ - Ca \u2013 The body added mass coefficient. If one value given, it is used for all three values. To \n+   specify three different values, the inputs are listed separated by | with no spaces\n \n Rods list\n ^^^^^^^^^\ndiff --git a/source/Body.cpp b/source/Body.cpp\nindex 836c9b69..0ef4baa6 100644\n--- a/source/Body.cpp\n+++ b/source/Body.cpp\n@@ -578,10 +578,10 @@ Body::doRHS()\n \n \t// Rotational DOFs drag coefficients are also defined on bodyCdA\n \tvec6 cda;\n-\tcda(Eigen::seqN(0, 3)) = OrMat.transpose() * bodyCdA.head<3>();\n-\tcda(Eigen::seqN(3, 3)) = OrMat.transpose() * bodyCdA.tail<3>();\n+\tcda(Eigen::seqN(0, 3)) = (OrMat * bodyCdA.head<3>().asDiagonal() * OrMat.transpose()) * vi.cwiseProduct(vi.cwiseAbs()).head<3>();\n+\tcda(Eigen::seqN(3, 3)) = (OrMat * bodyCdA.tail<3>().asDiagonal() * OrMat.transpose()) * vi.cwiseProduct(vi.cwiseAbs()).tail<3>();\n \tF6net +=\n-\t    0.5 * env->rho_w * vi.cwiseProduct(vi.cwiseAbs()).cwiseProduct(cda);\n+\t    0.5 * env->rho_w * cda;\n \n \t// Get contributions from any points attached to the body\n \tfor (auto attached : attachedP) {\n", "instance_id": "FloatingArrayDesign__MoorDyn-208", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue of negative drag coefficients in certain body orientations within a simulation context, specifically in the MoorDyn project. It provides a specific example (body pitch at -90 degrees) and points to the exact code location causing the issue, which aids in understanding the problem. However, there are minor ambiguities and missing details. For instance, the statement does not fully clarify the expected physical behavior or constraints for drag coefficients beyond suggesting that negative values are unphysical and destabilizing. Additionally, while the issue is described with an example, it lacks comprehensive test cases or edge case descriptions that could further clarify the scope of the problem. The proposed solution (taking absolute values) is mentioned, but it is not definitively endorsed, leaving some uncertainty about the correctness or completeness of the approach. Overall, the problem is valid and mostly clear, but minor details regarding broader implications, alternative solutions, or additional edge cases are missing.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is relatively small, primarily affecting a specific section of the `Body.cpp` file with a targeted modification to the drag coefficient calculation. The change involves adjusting the formula to account for orientation and velocity, ensuring non-negative coefficients, which requires a basic understanding of linear algebra (rotation matrices) and physics (drag forces). The documentation update in `inputs.rst` is also straightforward, merely expanding the description of input parameters for drag coefficients. Second, the technical concepts involved are not overly complex\u2014familiarity with Eigen (a C++ library for linear algebra) and basic matrix operations is sufficient, along with a rudimentary understanding of simulation physics. Third, the problem does not appear to impact the broader system architecture or require extensive cross-module changes; it is localized to a specific computation within the `Body` class. Finally, while edge cases are implied (e.g., specific orientations leading to negative coefficients), the problem statement and code changes do not delve into complex error handling or performance considerations beyond the immediate fix. The difficulty is slightly elevated within the \"Easy\" range due to the need for domain-specific knowledge (physics of drag in simulations) and the use of a specialized library like Eigen, but it remains a relatively straightforward bug fix for someone familiar with the codebase or similar systems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for PEP517 builds\n### Summary\r\n\r\nUsing [PEP517](https://peps.python.org/pep-0517/) to build the source distribution fails, because the build system dependencies are not specified.\r\n\r\n```bash\r\npip wheel --no-cache-dir --use-pep517 dymos\r\n```\r\n\r\n```bash\r\nCollecting dymos\r\n  Downloading dymos-1.10.0.tar.gz (567 kB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 567.3/567.3 kB 5.1 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [17 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"PATH_TO_VIRTUAL_ENV\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"PATH_TO_VIRTUAL_ENV\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"PATH_TO_VIRTUAL_ENV\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"PATH_TO_TEMP\\pip-build-env-p10744e1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=['wheel'])\r\n        File \"PATH_TO_TEMP\\pip-build-env-p10744e1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\r\n          self.run_setup()\r\n        File \"PATH_TO_TEMP\\pip-build-env-p10744e1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\r\n          super().run_setup(setup_script=setup_script)\r\n        File \"PATH_TO_TEMP\\pip-build-env-p10744e1\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 1, in <module>\r\n      ModuleNotFoundError: No module named 'packaging'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\n```\r\n\r\nSupport for [PEP517](https://peps.python.org/pep-0517/) is required as it is the recommended standard and dependency management systems such as [Poetry](https://python-poetry.org/) use the `--use-pep517` flag.\r\n\r\nThe error can be easily solved by adding a `pyproject.toml` file that specifies the required build dependencies. This is suggested in this pull request.\r\n\r\n### Related Issues\r\n\r\nNone\r\n\r\n### Backwards incompatibilities\r\n\r\nNone\r\n\r\n### New Dependencies\r\n\r\nNone, [setuptools](https://setuptools.pypa.io/) is already required. The requirement is only specified.\r\n\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nnew file mode 100644\nindex 000000000..ce3cb5646\n--- /dev/null\n+++ b/pyproject.toml\n@@ -0,0 +1,77 @@\n+[build-system]\n+requires = [\"hatchling\"]\n+build-backend = \"hatchling.build\"\n+\n+[project]\n+name = \"dymos\"\n+dynamic = [\"version\"]\n+description = \"Open-Source Optimization of Dynamic Multidisciplinary Systems\"\n+license = \"Apache-2.0\"\n+requires-python = \">=3.8\"\n+classifiers = [\n+    \"Development Status :: 5 - Production/Stable\",\n+    \"Intended Audience :: Science/Research\",\n+    \"License :: OSI Approved :: Apache Software License\",\n+    \"Natural Language :: English\",\n+    \"Operating System :: MacOS :: MacOS X\",\n+    \"Operating System :: Microsoft :: Windows\",\n+    \"Operating System :: POSIX :: Linux\",\n+    \"Programming Language :: Python\",\n+    \"Programming Language :: Python :: 3\",\n+    \"Programming Language :: Python :: Implementation :: CPython\",\n+    \"Topic :: Scientific/Engineering\",\n+]\n+dependencies = [\n+    \"numpy\",\n+    \"openmdao>=3.27.0\",\n+    \"scipy\",\n+]\n+\n+[project.optional-dependencies]\n+all = [\n+    \"dymos[docs]\",\n+    \"dymos[notebooks]\",\n+    \"dymos[test]\",\n+]\n+docs = [\n+    \"bokeh\",\n+    \"ipython\",\n+    \"jax\",\n+    \"jaxlib\",\n+    \"jupyter\",\n+    \"jupyter-book==0.14\",\n+    \"matplotlib\",\n+    \"nbconvert\",\n+    \"notebook\",\n+    \"numpydoc>=1.1\",\n+    \"redbaron\",\n+    \"tabulate\",\n+]\n+notebooks = [\n+    \"ipython\",\n+    \"notebook\",\n+    \"tabulate\",\n+]\n+test = [\n+    \"aiounittest\",\n+    \"matplotlib\",\n+    \"numpydoc>=1.1\",\n+    \"packaging\",\n+    \"playwright>=1.20\",\n+    \"pycodestyle\",\n+    \"testflo>=1.3.6\",\n+]\n+\n+[project.entry-points.openmdao_report]\n+\"dymos.linkage\" = \"dymos.visualization.linkage.report:_linkage_report_register\"\n+\n+[project.urls]\n+Homepage = \"https://github.com/OpenMDAO/dymos\"\n+\n+[tool.hatch.version]\n+path = \"dymos/__init__.py\"\n+\n+[tool.hatch.build.targets.sdist]\n+include = [\n+    \"/dymos\",\n+]\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 05710be1e..000000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-from setuptools import find_packages, setup\n-\n-# Setup optional dependencies\n-optional_dependencies = {\n-    'docs': [\n-        'matplotlib',\n-        'bokeh',\n-        'jupyter',\n-        'jupyter-book==0.14',\n-        'nbconvert',\n-        'notebook',\n-        'ipython',\n-        'numpydoc>=1.1',\n-        'redbaron',\n-        'tabulate',\n-        'jaxlib',\n-        'jax'\n-    ],\n-    'notebooks': [\n-        'notebook',\n-        'tabulate',\n-        'ipython'\n-    ],\n-    'test': [\n-        'packaging',\n-        'pycodestyle',\n-        'testflo>=1.3.6',\n-        'matplotlib',\n-        'numpydoc>=1.1',\n-        'playwright>=1.20',\n-        'aiounittest'\n-    ]\n-}\n-\n-# Add an optional dependency that concatenates all others\n-optional_dependencies['all'] = sorted([\n-    dependency\n-    for dependencies in optional_dependencies.values()\n-    for dependency in dependencies\n-])\n-\n-\n-setup(name='dymos',\n-    version='1.10.1-dev',\n-    description='Open-Source Optimization of Dynamic Multidisciplinary Systems',\n-    long_description='''\n-Dymos is a framework for the simulation and optimization of dynamical systems within the OpenMDAO Multidisciplinary Analysis and Optimization environment.\n-Dymos leverages implicit and explicit simulation techniques to simulate generic dynamic systems of arbitary complexity.\n-\n-The software has two primary objectives:\n--   Provide a generic ODE integration interface that allows for the analysis of dynamical systems.\n--   Allow the user to solve optimal control problems involving dynamical multidisciplinary systems.''',\n-    long_description_content_type='text/markdown',\n-    url='https://github.com/OpenMDAO/dymos',\n-    classifiers=[\n-        'Development Status :: 5 - Production/Stable',\n-        'Intended Audience :: Science/Research',\n-        'License :: OSI Approved :: Apache Software License',\n-        'Natural Language :: English',\n-        'Operating System :: MacOS :: MacOS X',\n-        'Operating System :: POSIX :: Linux',\n-        'Operating System :: Microsoft :: Windows',\n-        'Topic :: Scientific/Engineering',\n-        'Programming Language :: Python',\n-        'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: Implementation :: CPython',\n-    ],\n-    license='Apache License',\n-    packages=find_packages(),\n-    python_requires=\">=3.8\",\n-    install_requires=[\n-        'openmdao>=3.27.0',\n-        'numpy',\n-        'scipy'\n-    ],\n-    extras_require=optional_dependencies,\n-    zip_safe=False,\n-    package_data={\n-        'dymos.examples.aircraft_steady_flight.aero': ['data/CRM_aero_inputs.dat', 'data/CRM_aero_outputs.dat'],\n-        'dymos.visualization.linkage': ['report_template.html', 'js/*', 'style/*'],\n-        'dymos.visualization.linkage.test': ['model_data.dat']\n-    },\n-    entry_points = {\n-        'openmdao_report': [\n-            'dymos.linkage=dymos.visualization.linkage.report:_linkage_report_register'\n-        ],\n-    },\n-)\ndiff --git a/travis_deploy_rsa.enc b/travis_deploy_rsa.enc\ndeleted file mode 100644\nindex 98d1a9638..000000000\nBinary files a/travis_deploy_rsa.enc and /dev/null differ\n", "instance_id": "OpenMDAO__dymos-1052", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly outlines the issue with PEP517 builds failing due to unspecified build system dependencies when using tools like Poetry or pip with the `--use-pep517` flag. The goal is explicit: add support for PEP517 by specifying build dependencies in a `pyproject.toml` file. The statement includes relevant error logs to illustrate the problem, links to the PEP517 standard for context, and mentions that there are no related issues or backwards incompatibilities. Additionally, it specifies that no new dependencies are introduced beyond what is already required. The provided code changes align directly with the problem description, showing the creation of a `pyproject.toml` file and the removal of the old `setup.py` file. There are no ambiguities in the goal, input, output, or constraints, and the problem is presented with sufficient detail and context, warranting a clarity score of 3.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The scope of the code changes is limited to creating a new `pyproject.toml` file to define build dependencies and project metadata, and deleting the outdated `setup.py` file, which previously handled the same configuration. This involves minimal interaction with the broader codebase, as it does not impact the core functionality or architecture of the `dymos` library\u2014it's purely a build system update. The technical concepts required are straightforward: understanding Python's build system standards (PEP517), familiarity with `pyproject.toml` configuration, and basic knowledge of dependency management tools like Hatchling. These are not particularly complex for a developer with moderate Python experience. There are no significant edge cases or error handling requirements mentioned in the problem statement, and the changes do not introduce new logic that could fail under specific conditions. The overall amount of code change is small, and the task does not require deep architectural understanding or extensive debugging. Therefore, a difficulty score of 0.25 is appropriate, reflecting a simple but non-trivial task that requires some understanding of Python packaging standards.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support for mypy\nThis issue consists of two major parts: First, integrate mypy as a static type checker in the dev and build process of the library (and fix issues that mypy finds). Second, try to find out how to make validataclass **compatible** with mypy (i.e. in projects that use validataclass and mypy).\r\n\r\n### Use mypy to check the library code\r\n\r\n- [x] Add mypy to the dev environment (tox, Makefile) as an **optional** step for now.\r\n  - Only add `/src` to the target files for now, but add a comment to add `/tests` later too.\r\n  - (The tests will probably require the full validataclass-mypy compatibility.)\r\n- [x] Fix all low-hanging fruit issues detected by mypy (some issues might need a bit more work)\r\n- [x] Try to fix the more complicated issues (if necessary, mark them as `# type: ignore` for now, maybe create issues for them)\r\n- [x] When all issues are resolved or ignored, make mypy a **required** (i.e. default) step in the Tox config.\r\n- [x] Integrate mypy into the test and build CI pipelines\r\n- [x] Fix typing issues in the unit tests that are **not** related to how typing in validataclasses works (don't permanently add `/tests` to the mypy config yet)\r\n\r\n### Make validataclass fully mypy-compatible\r\n\r\n- [x] Make package PEP 561 compatible: #125\r\n- [ ] Run mypy on validataclass example code (e.g. the unit tests) and examine the issues.\r\n  - Assigning validators to fields in validataclasses: `foo: int = IntegerValidator()` - it's a validator, not an int.\r\n  - Subclasses of validataclasses with incompatible field type overrides (e.g. subclass changes field type from `int` to `OptionalUnset[int]`)\r\n  - Sentinels: Checking `x is not UnsetValue` does not narrow down the type, and evaluating as boolean `if x:` doesn't either. (For the latter: would it help to declare the type of `UnsetValueType.__bool__` as `Literal[False]`?)\r\n- [ ] Write mypy plugin.\r\n- [ ] ???\r\n- [ ] Tests and documentation\r\n  - [ ] Document how to use mypy with validataclass (best practices etc.)\nIntegrate mypy into development process (part of #116)\nThis PR integrates mypy as a static type checker into the development environment and the CI pipeline. To do that, it fixes a bunch of typing issues in the library code. It solves a part of #116 (but not all of it as that would be too big of a scope).\r\n\r\nList of general changes:\r\n\r\n- Add mypy to dev environment (tox, Makefile)\r\n- Add mypy to CI pipeline (GitHub Actions)\r\n- Set mypy rules to strict (apart from `no_implicit_reexport`)\r\n\r\nList of code changes:\r\n\r\n- Add a lot of missing type hints and fix some incorrect/inaccurate ones\r\n- Replace some type checks with more type-safe ones (e.g. `isinstance` instead of `type(...) is`)\r\n- Exception classes: use relative imports to import base ValidationError\r\n- Improve dynamic keyword argument construction in some classes like [here](https://github.com/binary-butterfly/validataclass/pull/120/commits/027ab1a9f273d43844308ae4f37f57dae8e6ffa1#diff-35059a70cee3bcbed31c465028dd5d37727743ac93a0e066e29cc72c58070ffcR61-R66)\r\n- `FloatToDecimalValidator`: `min_value`/`max_value` will not be cast to string if specified as integer anymore (see [commit](https://github.com/binary-butterfly/validataclass/pull/120/commits/027ab1a9f273d43844308ae4f37f57dae8e6ffa1#diff-35942654b8e76bfaba18cfca910da74f8cb750a03ad76f5f778dc77bdf436be1R99-R100))\r\n- `EnumValidator`: Improve type checking of Enum class when creating the validator (https://github.com/binary-butterfly/validataclass/pull/120/commits/9cbf9f9d1eab9f66d2fefd1b4d0a96e7b926e9c2)\r\n- `DataclassValidator`: (https://github.com/binary-butterfly/validataclass/pull/120/commits/0b6ed6ebe0b37bc2c7125729684da57d5da10bf9)\r\n  - Improve check if given dataclass is actually a dataclass\r\n  - Simplified dataclass check and error handling: There is now only one error message for \"parameter is a class that is not a dataclass\" and \"parameter is an instance of a dataclass\".\r\n  - Properly type `dataclasses.Field` with type parameter (requires extra code because in Python 3.8 it is not a Generic yet)\r\n- Use parametrized `Pattern` type hints for regex patterns (uses `typing.Pattern` for now, because `re.Pattern` is not yet parametrized in Python 3.8) (https://github.com/binary-butterfly/validataclass/pull/120/commits/a0d0f546d58a9d22ccb7adcde6f3d5fea8e70893)\r\n- `ValidataclassMixin`: Add a `# type: ignore[call-overload]` for the warning about `asdict` (https://github.com/binary-butterfly/validataclass/pull/120/commits/28110ce9da57958d694d210f31a73e0c5f549cca)\r\n- Add `# type: ignore[override]` for `validate()` overrides in validator classes that return different types than their base validators (e.g. `DecimalValidator` returning `Decimal`, which is not compatible with `StringValidator` returning `str`) (https://github.com/binary-butterfly/validataclass/pull/120/commits/712ac98d798bb159e2ce7c4ec2d3fdbb08dd1fff)\r\n- Add `# type: ignore` for sentinel object creation (https://github.com/binary-butterfly/validataclass/pull/120/commits/3965efe2d8ccb8df372ea05c25dae4afdff9c086)\r\n  - There might be a better way to do this, but sentinels are weird, there should be language support for it.\r\n- `unset_to_none()`: Use `isinstance(value, UnsetValueType)` instead of `is UnsetValue` (again, sentinels are weird) (https://github.com/binary-butterfly/validataclass/pull/120/commits/5ceea7434e1d50abfbd52102e9195529690becbd)\r\n- Fix typing in `DateTimeRange` and `DateTimeOffsetRange` (https://github.com/binary-butterfly/validataclass/pull/120/commits/f7dd402be20a49d19f987a8a8fd387a163bc8f85)\r\n- Various small changes to fix mypy issues\nSupport for mypy\nThis issue consists of two major parts: First, integrate mypy as a static type checker in the dev and build process of the library (and fix issues that mypy finds). Second, try to find out how to make validataclass **compatible** with mypy (i.e. in projects that use validataclass and mypy).\r\n\r\n### Use mypy to check the library code\r\n\r\n- [x] Add mypy to the dev environment (tox, Makefile) as an **optional** step for now.\r\n  - Only add `/src` to the target files for now, but add a comment to add `/tests` later too.\r\n  - (The tests will probably require the full validataclass-mypy compatibility.)\r\n- [x] Fix all low-hanging fruit issues detected by mypy (some issues might need a bit more work)\r\n- [x] Try to fix the more complicated issues (if necessary, mark them as `# type: ignore` for now, maybe create issues for them)\r\n- [x] When all issues are resolved or ignored, make mypy a **required** (i.e. default) step in the Tox config.\r\n- [x] Integrate mypy into the test and build CI pipelines\r\n- [x] Fix typing issues in the unit tests that are **not** related to how typing in validataclasses works (don't permanently add `/tests` to the mypy config yet)\r\n\r\n### Make validataclass fully mypy-compatible\r\n\r\n- [x] Make package PEP 561 compatible: #125\r\n- [ ] Run mypy on validataclass example code (e.g. the unit tests) and examine the issues.\r\n  - Assigning validators to fields in validataclasses: `foo: int = IntegerValidator()` - it's a validator, not an int.\r\n  - Subclasses of validataclasses with incompatible field type overrides (e.g. subclass changes field type from `int` to `OptionalUnset[int]`)\r\n  - Sentinels: Checking `x is not UnsetValue` does not narrow down the type, and evaluating as boolean `if x:` doesn't either. (For the latter: would it help to declare the type of `UnsetValueType.__bool__` as `Literal[False]`?)\r\n- [ ] Write mypy plugin.\r\n- [ ] ???\r\n- [ ] Tests and documentation\r\n  - [ ] Document how to use mypy with validataclass (best practices etc.)\nSupport for mypy\nThis issue consists of two major parts: First, integrate mypy as a static type checker in the dev and build process of the library (and fix issues that mypy finds). Second, try to find out how to make validataclass **compatible** with mypy (i.e. in projects that use validataclass and mypy).\r\n\r\n### Use mypy to check the library code\r\n\r\n- [x] Add mypy to the dev environment (tox, Makefile) as an **optional** step for now.\r\n  - Only add `/src` to the target files for now, but add a comment to add `/tests` later too.\r\n  - (The tests will probably require the full validataclass-mypy compatibility.)\r\n- [x] Fix all low-hanging fruit issues detected by mypy (some issues might need a bit more work)\r\n- [x] Try to fix the more complicated issues (if necessary, mark them as `# type: ignore` for now, maybe create issues for them)\r\n- [x] When all issues are resolved or ignored, make mypy a **required** (i.e. default) step in the Tox config.\r\n- [x] Integrate mypy into the test and build CI pipelines\r\n- [x] Fix typing issues in the unit tests that are **not** related to how typing in validataclasses works (don't permanently add `/tests` to the mypy config yet)\r\n\r\n### Make validataclass fully mypy-compatible\r\n\r\n- [x] Make package PEP 561 compatible: #125\r\n- [ ] Run mypy on validataclass example code (e.g. the unit tests) and examine the issues.\r\n  - Assigning validators to fields in validataclasses: `foo: int = IntegerValidator()` - it's a validator, not an int.\r\n  - Subclasses of validataclasses with incompatible field type overrides (e.g. subclass changes field type from `int` to `OptionalUnset[int]`)\r\n  - Sentinels: Checking `x is not UnsetValue` does not narrow down the type, and evaluating as boolean `if x:` doesn't either. (For the latter: would it help to declare the type of `UnsetValueType.__bool__` as `Literal[False]`?)\r\n- [ ] Write mypy plugin.\r\n- [ ] ???\r\n- [ ] Tests and documentation\r\n  - [ ] Document how to use mypy with validataclass (best practices etc.)\nSupport for mypy\nThis issue consists of two major parts: First, integrate mypy as a static type checker in the dev and build process of the library (and fix issues that mypy finds). Second, try to find out how to make validataclass **compatible** with mypy (i.e. in projects that use validataclass and mypy).\r\n\r\n### Use mypy to check the library code\r\n\r\n- [x] Add mypy to the dev environment (tox, Makefile) as an **optional** step for now.\r\n  - Only add `/src` to the target files for now, but add a comment to add `/tests` later too.\r\n  - (The tests will probably require the full validataclass-mypy compatibility.)\r\n- [x] Fix all low-hanging fruit issues detected by mypy (some issues might need a bit more work)\r\n- [x] Try to fix the more complicated issues (if necessary, mark them as `# type: ignore` for now, maybe create issues for them)\r\n- [x] When all issues are resolved or ignored, make mypy a **required** (i.e. default) step in the Tox config.\r\n- [x] Integrate mypy into the test and build CI pipelines\r\n- [x] Fix typing issues in the unit tests that are **not** related to how typing in validataclasses works (don't permanently add `/tests` to the mypy config yet)\r\n\r\n### Make validataclass fully mypy-compatible\r\n\r\n- [x] Make package PEP 561 compatible: #125\r\n- [ ] Run mypy on validataclass example code (e.g. the unit tests) and examine the issues.\r\n  - Assigning validators to fields in validataclasses: `foo: int = IntegerValidator()` - it's a validator, not an int.\r\n  - Subclasses of validataclasses with incompatible field type overrides (e.g. subclass changes field type from `int` to `OptionalUnset[int]`)\r\n  - Sentinels: Checking `x is not UnsetValue` does not narrow down the type, and evaluating as boolean `if x:` doesn't either. (For the latter: would it help to declare the type of `UnsetValueType.__bool__` as `Literal[False]`?)\r\n- [ ] Write mypy plugin.\r\n- [ ] ???\r\n- [ ] Tests and documentation\r\n  - [ ] Document how to use mypy with validataclass (best practices etc.)\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex d483313..3951594 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -11,10 +11,23 @@ write_to = \"src/validataclass/_version.py\"\n version_scheme = \"post-release\"\n \n [tool.mypy]\n-files = \"src/\"\n+files = [\"src/\", \"tests/\"]\n+mypy_path = \"src/\"\n+explicit_package_bases = true\n \n # Enable strict type checking\n strict = true\n \n # Ignore errors like `Module \"validataclass.exceptions\" does not explicitly export attribute \"...\"`\n no_implicit_reexport = false\n+\n+[[tool.mypy.overrides]]\n+module = 'tests.*'\n+\n+# Don't enforce typed definitions in tests, this is a lot of unnecessary work (most parameters would be Any anyway).\n+allow_untyped_defs = true\n+\n+# TODO: This is the main issue with mypy and validataclass right now.\n+# Defining dataclasses with validators using the @validataclass decorator, like `some_field: str = StringValidator()`,\n+# will cause \"Incompatible types in assignment\" errors. Until we find a way to solve this, ignore this error for now.\n+disable_error_code = \"assignment\"\ndiff --git a/setup.cfg b/setup.cfg\nindex 70668f8..ae8085e 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -46,3 +46,4 @@ testing =\n     coverage-conditional-plugin ~= 0.5\n     flake8 ~= 7.0\n     mypy ~= 1.9\n+    types-python-dateutil\ndiff --git a/tox.ini b/tox.ini\nindex 90371bd..298ef83 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -24,8 +24,7 @@ deps = flake8\n commands = flake8 src/ tests/\n \n [testenv:mypy,py{312,311,310,39,38}-mypy]\n-skip_install = true\n-deps = mypy\n+extras = testing\n commands = mypy\n \n [testenv:clean]\n", "instance_id": "binary-butterfly__validataclass-121", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in outlining the two primary goals: integrating mypy as a static type checker into the development and build process of the `validataclass` library, and making the library compatible with mypy for external projects. The statement provides a detailed breakdown of tasks, such as adding mypy to the dev environment, fixing typing issues, and integrating it into CI pipelines. It also identifies specific challenges in making `validataclass` mypy-compatible, such as issues with validators in dataclass fields and sentinel values. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully specify the expected behavior or constraints for mypy compatibility (e.g., how to handle type narrowing with sentinels or the exact scope of the mypy plugin). Additionally, while examples of issues are provided, there are no comprehensive examples or test cases to clarify the desired outcomes. The repeated sections in the problem statement also reduce readability. Overall, the statement is valid and mostly clear but lacks some finer details and examples for full clarity, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated at 0.75, placing it in the \"Hard\" category (0.6-0.8). This assessment is based on several factors. First, the scope of code changes is significant, involving multiple files and aspects of the codebase, including configuration files (`pyproject.toml`, `tox.ini`, `setup.cfg`), source code in `/src`, and potentially test files. The changes include adding type hints, fixing typing issues, and modifying logic for type safety (e.g., replacing type checks with `isinstance`, handling sentinel objects). While the provided diffs are relatively small, the referenced commits suggest a broader impact across the codebase, including complex modifications in validator and dataclass logic. Second, the problem requires understanding several technical concepts, such as Python's type system, mypy's strict mode and configuration, PEP 561 compliance, and the intricacies of dataclasses and custom validators. Additionally, creating a mypy plugin (though not yet implemented) hints at advanced type system manipulation, which is a specialized skill. Third, edge cases and error handling are explicitly mentioned, such as handling incompatible type overrides in subclasses and type narrowing issues with sentinels, which add complexity to the solution. Finally, while the problem does not seem to fundamentally alter the system's architecture, it impacts the development process and CI pipeline, requiring a deep understanding of the library's structure and build tools (tox, Makefile, GitHub Actions). The combination of these factors\u2014broad scope, multiple technical concepts, and specific edge cases\u2014makes this a challenging task that requires significant expertise in Python typing and static analysis tools, justifying a difficulty score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fix test_format_exception_as_html on python 3.13\nThe Fedora project has rebuilt all of its python packages with python 3.13.0b3.  This fixes the test failure reported in https://github.com/zopefoundation/zope.exceptions/issues/34.\ntest_format_exception_as_html fails with Python 3.13\n## BUG/PROBLEM REPORT / FEATURE REQUEST\r\n\r\n### What I did:\r\n\r\nRun tests with Python 3.13.0b2 (`tox -e py313`).\r\n\r\n### What I expect to happen:\r\n\r\nTests passing.\r\n\r\n### What actually happened:\r\n\r\n```pytb\r\nFailure in test test_format_exception_as_html (zope.exceptions.tests.test_exceptionformatter.Test_format_exception.test_format_exception_as_html)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 651, in run\r\n    self._callTestMethod(testMethod)\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 606, in _callTestMethod\r\n    if method() is not None:\r\n  File \"/tmp/zope.exceptions/src/zope/exceptions/tests/test_exceptionformatter.py\", line 779, in test_format_exception_as_html\r\n    self.assertEqual(expected, result)\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 907, in assertEqual\r\n    assertion_func(first, second, msg=msg)\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 1273, in assertMultiLineEqual\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n  File \"/usr/lib/python3.13/unittest/case.py\", line 732, in fail\r\n    raise self.failureException(msg)\r\nAssertionError: '<p>T[272 chars]Error: invalid syntax<br />\\n</p>' != '<p>T[272 chars]Error: Expected one or more names after \\'import\\'<br />\\n</p>'\r\n  <p>Traceback (most recent call last):</p>\r\n  <ul>\r\n  <li>  Module zope.exceptions.tests.test_exceptionformatter, line ABC, in test_format_exception_as_html<br />\r\n      exec('import')</li>\r\n  </ul><p>  File \"&lt;string&gt;\", line 1<br />\r\n      import<br />\r\n            ^<br />\r\n- SyntaxError: invalid syntax<br />\r\n+ SyntaxError: Expected one or more names after 'import'<br />\r\n  </p>\r\n```\r\n\r\nApparently it's just a matter of improved exception message.\r\n\r\n### What version of Python and Zope/Addons I am using:\r\n\r\nGentoo Linux amd64\r\nPython 3.13.0b2\r\nzope.exceptions 69ee8af262c40432e237202dc20fc3ae5d5ee5a6\n", "patch": "diff --git a/.meta.toml b/.meta.toml\nindex f819477..43e43c6 100644\n--- a/.meta.toml\n+++ b/.meta.toml\n@@ -2,14 +2,14 @@\n # https://github.com/zopefoundation/meta/tree/master/config/pure-python\n [meta]\n template = \"pure-python\"\n-commit-id = \"a6fb3886\"\n+commit-id = \"994c74d7\"\n \n [python]\n with-pypy = true\n with-docs = true\n with-sphinx-doctests = true\n with-windows = false\n-with-future-python = false\n+with-future-python = true\n with-macos = false\n \n [tox]\ndiff --git a/CHANGES.rst b/CHANGES.rst\nindex 490937b..63c7617 100644\n--- a/CHANGES.rst\n+++ b/CHANGES.rst\n@@ -7,6 +7,8 @@\n \n - Drop support for Python 3.7.\n \n+- Add preliminary support for Python 3.13 as of 3.13.rc1\n+\n \n 5.1 (2024-06-07)\n ================\ndiff --git a/tox.ini b/tox.ini\nindex 46baa20..d2b7639 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -10,6 +10,7 @@ envlist =\n     py310\n     py311\n     py312\n+    py313\n     pypy3\n     docs\n     coverage\n@@ -18,8 +19,10 @@ envlist =\n usedevelop = true\n package = wheel\n wheel_build_env = .pkg\n+pip_pre = py313: true\n deps =\n     setuptools < 69\n+    Sphinx\n setenv =\n     py312: VIRTUALENV_PIP=23.1.2\n     py312: PIP_REQUIRE_VIRTUALENV=0\n@@ -80,6 +83,7 @@ allowlist_externals =\n     mkdir\n deps =\n     coverage\n+    Sphinx\n commands =\n     mkdir -p {toxinidir}/parts/htmlcov\n     coverage run -m zope.testrunner --test-path=src {posargs:-vc}\n", "instance_id": "zopefoundation__zope.exceptions-37", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a test failure in `test_format_exception_as_html` with Python 3.13 due to a change in the exception message for a `SyntaxError`. The goal is to fix the test to account for the updated error message. The input (running tests with Python 3.13) and expected output (tests passing) are implied, and the provided traceback offers context about the failure. However, there are minor ambiguities: the problem statement does not explicitly specify whether the fix should involve updating the expected output in the test or modifying the exception formatting logic to handle Python 3.13's new message. Additionally, edge cases or other potential impacts of Python 3.13's changes are not mentioned. Despite these minor gaps, the issue is well-documented with a clear reproduction path and relevant version information, justifying a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The issue appears to be a straightforward compatibility fix for Python 3.13, likely requiring a simple update to the expected test output to match the new `SyntaxError` message (\"Expected one or more names after 'import'\" instead of \"invalid syntax\"). The code changes provided in the diff are minimal and do not directly address the test failure but rather update configuration files (e.g., `tox.ini`, `.meta.toml`) to include Python 3.13 in the testing matrix and add preliminary support notes in `CHANGES.rst`. These changes suggest the scope is limited to configuration adjustments rather than complex logic modifications. The technical concepts involved are basic: understanding Python exception messages, test assertions, and tox configuration. No deep architectural changes, complex algorithms, or extensive edge case handling are apparent from the problem or changes. The primary challenge might be verifying if other tests or exception messages are affected by Python 3.13, but this is not indicated as a significant concern in the statement. Therefore, a difficulty score of 0.25 reflects the simplicity of the required fix and the limited scope of the changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "MetaDataDictionary::Print only prints [UNKNOWN_PRINT_CHARACTERISTICS]\n### Description\r\n\r\n\r\n<!-- Description of the bug. -->\r\n\r\nFor any MetaDataObject calling `obj->Print(std::cout)` produces \"[UNKNOWN_PRINT_CHARACTERISTICS]\" on the console. \r\n\r\nhttps://github.com/InsightSoftwareConsortium/ITK/blob/60469760ee13f48b265ca7424e3e9fd652672710/Modules/Core/Common/include/itkMetaDataObject.hxx#L63-L69\r\n\r\n### Expected behavior\r\n\r\nFor `itk::Object` base objects the `Print` method should be used while those which support `operator<<`, that method should be used.\r\n\r\nIt is expected to be able to iterate over the metadata dictionary and print the key and value of all types and get descriptive output.\n", "patch": "diff --git a/Modules/Core/Common/include/itkMetaDataObject.h b/Modules/Core/Common/include/itkMetaDataObject.h\nindex bbf2f7bd8cf..3b2c80117e2 100644\n--- a/Modules/Core/Common/include/itkMetaDataObject.h\n+++ b/Modules/Core/Common/include/itkMetaDataObject.h\n@@ -58,15 +58,8 @@ namespace itk\n  * and redefining the copy constructor and initializing constructor and the Get/Set functions\n  * to work around those deficiencies.\n  *\n- * The behavior of the MetaDataObject<Type>::Print() function has many plausible\n- * application dependent implementations.  The default implementation prints the\n- * string \"[UNKNOWN PRINT CHARACTERISTICS]\" that works for all possible\n- * MetaDataObject types.\n- *\n- * The application developer may overload the default implementation to provide\n- * a specialized Print() characteristics to produce results desirable for their application.\n- * A set of very crude Macros {NATIVE_TYPE_METADATAPRINT, ITK_OBJECT_TYPE_METADATAPRINT_1COMMA,\n- * ITK_IMAGE_TYPE_METADATAPRINT  } are provided to facilitate a very simple implementation, and as an example.\n+ * The default implementation prints uses the MetaDataObjectType's Print or operator<< if available. Otherwise, it\n+ * prints string \"[UNKNOWN PRINT CHARACTERISTICS]\".\n  *\n  * \\ingroup ITKCommon\n  *\n@@ -264,52 +257,6 @@ ExposeMetaData(const MetaDataDictionary & Dictionary, const std::string key, T &\n \n } // end namespace itk\n \n-/**\n- * \\def ITK_NATIVE_TYPE_METADATAPRINT( TYPE_NAME )\n- * \\brief An ugly macro to facilitate creating a simple implementation of\n- * the MetaDataObject<Type>::Print() function for types that\n- * have operator<< defined.\n- * \\param TYPE_NAME the native type parameter type\n- */\n-#define ITK_NATIVE_TYPE_METADATAPRINT(TYPE_NAME)                      \\\n-  template <>                                                         \\\n-  void itk::MetaDataObject<TYPE_NAME>::Print(std::ostream & os) const \\\n-  {                                                                   \\\n-    os << this->m_MetaDataObjectValue << std::endl;                   \\\n-  }\n-\n-/**\n- * \\def ITK_OBJECT_TYPE_METADATAPRINT_1COMMA( TYPE_NAME_PART1, TYPE_NAME_PART2 )\n- * \\brief An ugly macro to facilitate creating a simple implementation of\n- * the MetaDataObject< Type >::Print() function for\n- * itk::Objects that have 1 comma in their type definition\n- * \\param TYPE_NAME_PART1\n- * \\param TYPE_NAME_PART2\n- */\n-#define ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(TYPE_NAME_PART1, TYPE_NAME_PART2)               \\\n-  template <>                                                                                \\\n-  void itk::MetaDataObject<TYPE_NAME_PART1, TYPE_NAME_PART2>::Print(std::ostream & os) const \\\n-  {                                                                                          \\\n-    this->m_MetaDataObjectValue->Print(os);                                                  \\\n-  }\n-\n-/**\n- * \\def ITK_IMAGE_TYPE_METADATAPRINT( STORAGE_TYPE )\n- * An ugly macro to facilitate creating a simple implementation of\n- * the MetaDataObject<Type>::Print() function for\n- * itk::Image\\<STORAGE_TYPE,[1-8]\\>\\::Pointer\n- * \\param STORAGE_TYPE The storage type of the image type to print.\n- */\n-#define ITK_IMAGE_TYPE_METADATAPRINT(STORAGE_TYPE)                           \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 1>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 2>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 3>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 4>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 5>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 6>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 7>::Pointer) \\\n-  ITK_OBJECT_TYPE_METADATAPRINT_1COMMA(itk::Image<STORAGE_TYPE, 8>::Pointer)\n-\n #ifndef ITK_MANUAL_INSTANTIATION\n #  include \"itkMetaDataObject.hxx\"\n #endif\ndiff --git a/Modules/Core/Common/include/itkMetaDataObject.hxx b/Modules/Core/Common/include/itkMetaDataObject.hxx\nindex 23b92f234db..47c2d548956 100644\n--- a/Modules/Core/Common/include/itkMetaDataObject.hxx\n+++ b/Modules/Core/Common/include/itkMetaDataObject.hxx\n@@ -59,11 +59,45 @@ MetaDataObject<MetaDataObjectType>::SetMetaDataObjectValue(const MetaDataObjectT\n   Self::Assign(m_MetaDataObjectValue, newValue);\n }\n \n+namespace\n+{\n+template <class T, class = void>\n+struct has_Print : std::false_type\n+{};\n+\n+template <class T>\n+struct has_Print<T, std::void_t<decltype(std::declval<T>().Print(std::declval<std::ostream &>()))>> : std::true_type\n+{};\n+\n+template <class T, class = void>\n+struct has_output_operator : std::false_type\n+{};\n+\n+template <class T>\n+struct has_output_operator<T, std::void_t<decltype(std::declval<std::ostream &>() << std::declval<T>())>>\n+  : std::true_type\n+{};\n+} // namespace\n+\n template <typename MetaDataObjectType>\n void\n MetaDataObject<MetaDataObjectType>::Print(std::ostream & os) const\n {\n-  Superclass::Print(os);\n+  // future c++20 feature\n+  // constexpr bool hasPrint = false; requires( const &MetaDataObjectType obj ) { obj.Print(os); };\n+\n+  if constexpr (has_Print<MetaDataObjectType>::value)\n+  {\n+    m_MetaDataObjectValue.Print(os);\n+  }\n+  else if constexpr (has_output_operator<MetaDataObjectType>::value)\n+  {\n+    os << m_MetaDataObjectValue;\n+  }\n+  else\n+  {\n+    Superclass::Print(os);\n+  }\n }\n \n } // end namespace itk\n", "instance_id": "InsightSoftwareConsortium__ITK-4814", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `MetaDataDictionary::Print` method, which currently outputs \"[UNKNOWN_PRINT_CHARACTERISTICS]\" for all `MetaDataObject` instances. The expected behavior is articulated: for `itk::Object` base objects, the `Print` method should be used, and for types supporting `operator<<`, that should be used instead. Additionally, the goal of iterating over a metadata dictionary to print descriptive key-value pairs is mentioned. However, there are minor ambiguities and missing details. For instance, the problem does not specify how to handle cases where neither `Print` nor `operator<<` is available beyond falling back to the default behavior, nor does it provide examples of expected output formats for different types. Edge cases, such as handling null or invalid objects, are not addressed. Overall, while the core issue and goal are understandable, the lack of detailed examples and edge case specifications prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting the `MetaDataObject` class implementation across two files (`itkMetaDataObject.h` and `itkMetaDataObject.hxx`). The changes involve removing outdated macro-based print implementations and introducing a modern C++ template metaprogramming approach using SFINAE (Substitution Failure Is Not An Error) to detect the availability of `Print` or `operator<<` for a given type. This requires a solid understanding of advanced C++ concepts like `std::void_t`, type traits, and `constexpr if`, which are not trivial for less experienced developers. Additionally, the developer must understand the ITK library's class hierarchy and how `MetaDataObject` interacts with other components, though the changes do not appear to impact the broader system architecture significantly. The amount of code change is moderate, involving both deletion of legacy code and addition of new logic. While the problem statement does not explicitly mention edge cases, the implementation must handle fallback behavior when neither `Print` nor `operator<<` is available, which adds a layer of complexity. Overall, this task requires a good grasp of C++ metaprogramming and familiarity with the ITK library, placing it in the medium difficulty range (0.4-0.6), slightly above the midpoint due to the advanced language features involved.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug]: Maximum recursing depth exceeded\n### Actual Behavior\n\nWhen using the openapi spec for [Azure OpenAI](https://raw.githubusercontent.com/Azure/azure-rest-api-specs/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-02-15-preview/inference.json) with any supported body results in a `RecursionError: maximum recursion depth exceeded.` and the code execution stops\r\n\r\nI had code as part of a fastapi, but kept running into this issue so I isolated the code in a small project for which the link is in the repro steps.\r\n\r\nAlso other specs for versions of the same API have this issue. All of them are valid when parsed on https://validator.swagger.io/\r\nThe list of all specs can be found [here](https://github.com/Azure/azure-rest-api-specs/tree/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference)\r\n\r\nthe full error I get is posted below.\r\n\r\nI hope this helps!\r\n\r\n```\r\nC:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\security\\providers.py:20: UserWarning: Unsupported scheme type\r\n  warnings.warn(\"Unsupported scheme type\")\r\nTraceback (most recent call last):\r\n  File \"c:\\Code\\gaia-retrieval-gateway-poc\\testing\\specvalidator.py\", line 29, in <module>\r\n    result = openapi.unmarshal_request(openapi_request)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\app.py\", line 398, in unmarshal_request\r\n    return self.unmarshal_apicall_request(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\app.py\", line 413, in unmarshal_apicall_request\r\n    return self.request_unmarshaller.unmarshal(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\request\\unmarshallers.py\", line 249, in unmarshal\r\n    return self._unmarshal(request, operation, path)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\request\\unmarshallers.py\", line 156, in _unmarshal\r\n    body = self._get_body(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\validation\\decorators.py\", line 31, in wrapper\r\n    return f(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\validation\\request\\validators.py\", line 268, in _get_body   \r\n    value, _ = self._get_content_and_schema(raw_body, content, mimetype)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\unmarshallers.py\", line 118, in _get_content_and_schema\r\n    return self._unmarshal_schema(schema, casted), schema\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\unmarshallers.py\", line 95, in _unmarshal_schema\r\n    return unmarshaller.unmarshal(value)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 249, in unmarshal\r\n    typed = type_unmarshaller(value)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 51, in __call__\r\n    properties = self._unmarshal_properties(value)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 94, in _unmarshal_properties\r\n    )._unmarshal_properties(value, schema_only=True)\r\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 107, in _unmarshal_properties\r\n    ).unmarshal(prop_value)\r\n      ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 249, in unmarshal\r\n    typed = type_unmarshaller(value)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 143, in __call__\r\n    return unmarshaller(value)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 143, in __call__\r\n    return unmarshaller(value)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 143, in __call__\r\n    return unmarshaller(value)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  [Previous line repeated 976 more times]\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\unmarshalling\\schemas\\unmarshallers.py\", line 139, in __call__\r\n    primitive_type = self.schema_validator.get_primitive_type(value)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\openapi_core\\validation\\schemas\\validators.py\", line 82, in get_primitive_type\r\n    schema_types = self.schema.getkey(\"type\")\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\pathable\\paths.py\", line 177, in getkey\r\n    with self.open() as d:\r\n  File \"C:\\Users\\M63H578\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 137, in __enter__\r\n    return next(self.gen)\r\n           ^^^^^^^^^^^^^^\r\n  File \"C:\\Code\\gaia-retrieval-gateway-poc\\venv\\Lib\\site-packages\\jsonschema_path\\paths.py\", line 119, in open\r\n    with self.resolve() as resolved:\r\n         ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\M63H578\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 301, in helper\r\n    return _GeneratorContextManager(func, args, kwds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\M63H578\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 105, in __init__\r\n    self.gen = func(*args, **kwds)\r\n               ^^^^^^^^^^^^^^^^^^^\r\nRecursionError: maximum recursion depth exceeded\r\n```\n\n### Expected Behavior\n\nThe validation to succeed when passing a valid body in the request.\n\n### Steps to Reproduce\n\nI created a temporary repo with my [code](https://github.com/mpriem/openapi-core-recursion-error/tree/main), so you can reproduce.\r\n\r\nPlease let me know if there is anything else you need. Thanks in advance!\n\n### OpenAPI Core Version\n\n0.19.2\n\n### OpenAPI Core Integration\n\nnone but had the same issue with fastapi\n\n### Affected Area(s)\n\nunmarschaling\n\n### References\n\nN/A\n\n### Anything else we need to know?\n\n_No response_\n\n### Would you like to implement a fix?\n\nNone\n", "patch": "diff --git a/openapi_core/unmarshalling/schemas/unmarshallers.py b/openapi_core/unmarshalling/schemas/unmarshallers.py\nindex 53ddcc34..1df9ed09 100644\n--- a/openapi_core/unmarshalling/schemas/unmarshallers.py\n+++ b/openapi_core/unmarshalling/schemas/unmarshallers.py\n@@ -137,6 +137,9 @@ def _unmarshal_properties(\n class MultiTypeUnmarshaller(PrimitiveUnmarshaller):\n     def __call__(self, value: Any) -> Any:\n         primitive_type = self.schema_validator.get_primitive_type(value)\n+        # OpenAPI 3.0: handle no type for None\n+        if primitive_type is None:\n+            return None\n         unmarshaller = self.schema_unmarshaller.get_type_unmarshaller(\n             primitive_type\n         )\n@@ -247,6 +250,9 @@ def unmarshal(self, value: Any) -> Any:\n         schema_type = self.schema.getkey(\"type\")\n         type_unmarshaller = self.get_type_unmarshaller(schema_type)\n         typed = type_unmarshaller(value)\n+        # skip finding format for None\n+        if typed is None:\n+            return None\n         schema_format = self.find_format(value)\n         if schema_format is None:\n             return typed\ndiff --git a/openapi_core/validation/schemas/validators.py b/openapi_core/validation/schemas/validators.py\nindex c822a82f..7a764e31 100644\n--- a/openapi_core/validation/schemas/validators.py\n+++ b/openapi_core/validation/schemas/validators.py\n@@ -94,6 +94,7 @@ def get_primitive_type(self, value: Any) -> Optional[str]:\n                 continue\n             assert isinstance(schema_type, (str, type(None)))\n             return schema_type\n+        # OpenAPI 3.0: None is not a primitive type so None value will not find any type\n         return None\n \n     def iter_valid_schemas(self, value: Any) -> Iterator[SchemaPath]:\n", "instance_id": "python-openapi__openapi-core-876", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a `RecursionError` occurs when processing certain OpenAPI specifications, specifically with Azure OpenAI inference specs. The actual behavior, expected behavior, and steps to reproduce are provided, along with a link to a minimal reproduction repository and the full error traceback. This helps in understanding the context and replicating the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the root cause of the recursion (e.g., cyclic references in the schema or specific schema structures causing the issue), nor does it specify constraints or edge cases beyond the provided specs. Additionally, while the affected area (\"unmarshalling\") is mentioned, there is no deeper insight into which part of the unmarshalling process (e.g., schema traversal, type handling) is problematic. Overall, the statement is valid and clear but lacks some finer details that could aid in quicker diagnosis.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the issue involves understanding the unmarshalling process in the `openapi-core` library, which requires familiarity with how OpenAPI schemas are parsed, validated, and processed. The code changes provided are relatively small and localized to two files (`unmarshallers.py` and `validators.py`), focusing on handling `None` values to prevent recursion. However, the impact of these changes is significant as they touch core logic for schema validation and type unmarshalling, which are critical to the library's functionality. \n\nSecond, the technical concepts involved include a deep understanding of OpenAPI 3.0 specifications, schema traversal, type handling, and recursion issues in nested data structures. The developer must also be aware of Python's recursion depth limits and how they manifest in real-world scenarios like schema processing. Additionally, knowledge of the `openapi-core` library's internal architecture is necessary to ensure that the fix does not introduce regressions or break compatibility with other specs.\n\nThird, while the provided code changes address the immediate issue (handling `None` values to break recursion), the problem hints at broader challenges, such as potential cyclic references or deeply nested schemas in the Azure OpenAI specs. This suggests that edge cases (e.g., malformed schemas, extremely nested structures) need to be considered, and the solution may require more robust error handling or recursion safeguards beyond the current patch.\n\nOverall, solving this problem requires a deep understanding of the library's internals and the OpenAPI domain, along with careful consideration of the broader implications of the fix. While not at the extreme end of difficulty (e.g., requiring a full rewrite or distributed system design), it is a challenging task that justifies a score of 0.65, placing it in the \"Hard\" range due to the complexity of concepts and potential for subtle bugs or regressions.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update completion model fields immediately on save\n## Description\r\n\r\n- Follow-up to #1125. Fixes an issue identified by @krassowski.\r\n- Fixes completion model fields by reading from the correct config key when returning model arguments for a completion model.\r\n- Adds unit test coverage for verification.\r\n\r\n## Testing instructions\r\n\r\n1. Checkout this branch, and make the following changes near line 459 in `config_manager.py` to mimic the existing behavior on `main`:\r\n\r\n    ```diff\r\n            # get config fields (e.g. base API URL, etc.)\r\n    -       if completions:\r\n    -           fields = config.completions_fields.get(model_uid, {})\r\n    -       else:\r\n    -           fields = config.fields.get(model_uid, {})\r\n    +       fields = config.fields.get(model_uid, {})\r\n    ```\r\n\r\n2. Run `pytest`, verify that the new unit test fails.\r\n3. Revert your local changes, re-run `pytest`, and verify that the new unit test passes.\n", "patch": "diff --git a/packages/jupyter-ai/jupyter_ai/config_manager.py b/packages/jupyter-ai/jupyter_ai/config_manager.py\nindex 71ca3f185..7b309faae 100644\n--- a/packages/jupyter-ai/jupyter_ai/config_manager.py\n+++ b/packages/jupyter-ai/jupyter_ai/config_manager.py\n@@ -442,10 +442,10 @@ def em_provider_params(self):\n     @property\n     def completions_lm_provider_params(self):\n         return self._provider_params(\n-            \"completions_model_provider_id\", self._lm_providers\n+            \"completions_model_provider_id\", self._lm_providers, completions=True\n         )\n \n-    def _provider_params(self, key, listing):\n+    def _provider_params(self, key, listing, completions: bool = False):\n         # read config\n         config = self._read_config()\n \n@@ -457,7 +457,10 @@ def _provider_params(self, key, listing):\n         model_id = model_uid.split(\":\", 1)[1]\n \n         # get config fields (e.g. base API URL, etc.)\n-        fields = config.fields.get(model_uid, {})\n+        if completions:\n+            fields = config.completions_fields.get(model_uid, {})\n+        else:\n+            fields = config.fields.get(model_uid, {})\n \n         # get authn fields\n         _, Provider = get_em_provider(model_uid, listing)\n", "instance_id": "jupyterlab__jupyter-ai-1142", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to fix an issue related to reading the correct configuration fields for a completion model in a Jupyter AI project. It provides a description of the issue, references a related issue (#1125), and includes testing instructions to verify the behavior before and after the fix. The code diff provided in the testing instructions and the actual code changes align with the described goal. However, there are minor ambiguities: the problem statement does not explicitly define what \"completion model fields\" are or provide context about the broader system (e.g., what `config.fields` vs. `config.completions_fields` represent). Additionally, edge cases or potential side effects of this change are not mentioned, which could be critical for a configuration-related fix. Overall, while the goal is understandable, some contextual details and potential constraints are missing, leading to a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change is localized to a single file (`config_manager.py`) and involves a small, targeted modification to the `_provider_params` method by adding a parameter and conditional logic to choose between `config.fields` and `config.completions_fields`. The scope of the change is minimal, affecting only one function, and does not appear to impact the broader system architecture. The technical concepts required are basic: understanding Python function parameters, conditional statements, and dictionary access. No advanced algorithms, design patterns, or domain-specific knowledge are necessary beyond familiarity with the project's configuration structure. The problem statement and testing instructions imply a straightforward bug fix with a provided unit test to validate the change, further reducing complexity. Edge cases or error handling are not explicitly mentioned or required in the code changes, keeping the difficulty low. Therefore, I assign a difficulty score of 0.25, reflecting a simple bug fix with minimal cognitive load.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Alignment should affect size of absolutely positioned element\n```html\r\n<!DOCTYPE html>\r\n<style>\r\n.wrapper { display: inline-block; position: relative; width: 100px; height: 100px; border: 3px solid; }\r\n.abspos { position: absolute; inset: 0; width: auto; height: auto; background: cyan; }\r\n.abspos::before { content: \"\"; display: block; width: 50px; height: 50px; }\r\n</style>\r\n<div class=\"wrapper\"><div class=\"abspos\" style=\"place-self: normal\"></div></div>\r\n<div class=\"wrapper\"><div class=\"abspos\" style=\"place-self: stretch\"></div></div>\r\n<div class=\"wrapper\"><div class=\"abspos\" style=\"place-self: center\"></div></div>\r\n<br>\r\n<div class=\"wrapper\"><canvas width=\"50\" height=\"50\" class=\"abspos\" style=\"place-self: normal\"></canvas></div>\r\n<div class=\"wrapper\"><canvas width=\"50\" height=\"50\" class=\"abspos\" style=\"place-self: stretch\"></canvas></div>\r\n<div class=\"wrapper\"><canvas width=\"50\" height=\"50\" class=\"abspos\" style=\"place-self: center\"></canvas></div>\r\n```\r\n\r\n| Servo (bad) | Blink (good) |\r\n| - | - |\r\n| ![](https://github.com/user-attachments/assets/b65106d2-2bde-45cf-ba43-12a9f96e79c6) | ![](https://github.com/user-attachments/assets/04d5d62c-14cb-41db-8dec-caeb284cfc62) |\r\n\r\nThe replaced and non-replaced cases can be addressed in different PRs.\r\n\r\nThe non-replaced case (1st row in testcase) seems simple, currently it always stretches because of https://github.com/servo/servo/blob/3fd1a229df65406699c5795cf504948cdb314320/components/layout_2020/positioned.rs#L849\r\n\r\nIt should use `Size::FitContent` when the alignment isn't `normal` or `stretch`.\n", "patch": "diff --git a/components/layout_2020/positioned.rs b/components/layout_2020/positioned.rs\nindex 7219463691487..5a85733a60363 100644\n--- a/components/layout_2020/positioned.rs\n+++ b/components/layout_2020/positioned.rs\n@@ -846,10 +846,16 @@ impl<'a> AbsoluteAxisSolver<'a> {\n                 let stretch_size = free_space -\n                     self.computed_margin_start.auto_is(Au::zero) -\n                     self.computed_margin_end.auto_is(Au::zero);\n-                let used_size = solve_size(Size::Stretch, stretch_size)\n-                    .to_definite()\n-                    .unwrap();\n-                free_space -= used_size;\n+                let initial_behavior = match self.alignment.value() {\n+                    AlignFlags::STRETCH | AlignFlags::NORMAL | AlignFlags::AUTO => Size::Stretch,\n+                    _ => Size::FitContent,\n+                };\n+                let size = solve_size(initial_behavior, stretch_size);\n+                if let Some(used_size) = size.to_definite() {\n+                    free_space -= used_size;\n+                } else {\n+                    free_space = Au::zero();\n+                }\n                 let (margin_start, margin_end) =\n                     match (self.computed_margin_start, self.computed_margin_end) {\n                         (AuOrAuto::Auto, AuOrAuto::Auto) => {\n@@ -872,7 +878,7 @@ impl<'a> AbsoluteAxisSolver<'a> {\n                     };\n                 AxisResult {\n                     anchor: Anchor::Start(start),\n-                    size: SizeConstraint::Definite(used_size),\n+                    size,\n                     margin_start,\n                     margin_end,\n                 }\n", "instance_id": "servo__servo-34264", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in terms of its goal: to adjust the sizing behavior of absolutely positioned elements based on their alignment properties (e.g., `normal`, `stretch`, `center`) in the Servo browser engine to match the behavior observed in Blink (Chrome's rendering engine). The provided HTML test case and visual comparison between Servo (incorrect) and Blink (correct) effectively illustrate the desired outcome. Additionally, the statement points to a specific line of code in the Servo repository that causes the current incorrect behavior, which is helpful. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for each alignment value beyond a general mention of using `Size::FitContent` for non-`normal` or non-`stretch` alignments. It also lacks discussion of potential edge cases or constraints (e.g., how this change might interact with other CSS properties or different types of content). While the issue is scoped to non-replaced elements in this PR, there is no mention of how this might relate to replaced elements (like `<canvas>`) in future PRs or whether there are dependencies to consider. Overall, the problem is understandable, but minor clarifications would make it more comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is relatively small and localized to a single file (`positioned.rs`) and a specific function or block of logic within it. The diff provided shows a modification of about 10-15 lines of code, which is minimal in terms of overall impact to the codebase. The change does not appear to affect the broader system architecture or require understanding complex interactions between multiple modules; it is a targeted fix to the sizing logic for absolutely positioned elements. \n\nSecond, the technical concepts involved are moderately straightforward for someone familiar with Rust and browser layout engines. The solution requires understanding CSS alignment properties (`place-self`), how they map to internal layout logic (`AlignFlags`, `Size` enum), and basic manipulation of layout constraints (`Size::Stretch` vs. `Size::FitContent`). While this involves domain-specific knowledge of CSS and browser rendering, it does not require advanced algorithms, design patterns, or complex library usage. The provided code change already demonstrates a clear approach to solving the issue by conditionally setting the size behavior based on alignment flags.\n\nThird, regarding edge cases and error handling, the problem statement does not explicitly mention specific edge cases to handle, and the code change does not introduce new error handling logic. However, there is an implicit need to ensure that the change does not break other alignment behaviors or interact poorly with other CSS properties, which adds a slight layer of complexity. The modification to how `free_space` is calculated and the handling of `size.to_definite()` suggests some consideration of fallback behavior, but this is not overly intricate.\n\nOverall, I rate this as 0.35 because, while it is not a trivial typo fix, it is a relatively contained bug fix that requires understanding a specific part of the layout logic and making a targeted change. It is suitable for a developer with intermediate experience in Rust and some familiarity with browser layout concepts. The primary challenge lies in verifying that the change behaves correctly across different alignment values and does not introduce regressions, but this does not push it into the \"Medium\" difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "New audit: un-pinned action audit\n### Pre-submission checks\n\n- [X] I am **not** a bug (crash, false positive/negative, etc). These must be filed via the bug report template.\n- [X] I have looked through the open issues for a duplicate request.\n\n### What's the problem this feature will solve?\n\nGitHub Actions may become compromised due to account takeover, social engineering, etc. It's a good idea to pin actions to known-good versions so that a workflow isn't silently compromised by a change in an action.\n\n### Describe the solution you'd like\n\nAn audit that checks for actions without a version. For example, it would flag `actions/checkout`. For extra security, it could require a commit ID, since tags could be overwritten by an attacker.\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/docs/audits.md b/docs/audits.md\nindex 44c2e7f..c36fdc5 100644\n--- a/docs/audits.md\n+++ b/docs/audits.md\n@@ -259,3 +259,69 @@ and possible.\n * <https://docs.pypi.org/trusted-publishers/>\n * <https://guides.rubygems.org/trusted-publishing/>\n * <https://blog.trailofbits.com/2023/05/23/trusted-publishing-a-new-benchmark-for-packaging-security/>\n+\n+## `unpinned-uses`\n+\n+| Type     | Examples                     | Introduced in | Works offline  | Enabled by default |\n+|----------|------------------------------|---------------|----------------|--------------------|\n+| Workflow  | [unpinned.yml]              | v0.4.0        | \u2705             | \u2705                 |\n+\n+[unpinned.yml]: https://github.com/woodruffw/gha-hazmat/blob/main/.github/workflows/unpinned.yml\n+\n+Detects \"unpinned\" `uses:` clauses.\n+\n+When a `uses:` clause is not pinned by branch, tag, or SHA reference,\n+GitHub Actions will use the latest commit on the referenced repository\n+(or, in the case of Docker actions, the `:latest` tag).\n+\n+### Remediation\n+\n+For repository actions (like @actions/checkout): add a branch, tag, or SHA\n+reference.\n+\n+For Docker actions (like `docker://ubuntu`): add an appropriate\n+`:{version}` suffix.\n+\n+A before/after example is shown below.\n+\n+=== \"Before\"\n+\n+    ```yaml title=\"unpinned-uses.yml\" hl_lines=\"8 12\"\n+    name: unpinned-uses\n+    on: [push]\n+\n+    jobs:\n+    unpinned-uses:\n+        runs-on: ubuntu-latest\n+        steps:\n+        - uses: actions/checkout\n+          with:\n+          persist-credentials: false\n+\n+        - uses: docker://ubuntu\n+          with:\n+          entrypoint: /bin/echo\n+          args: hello!\n+    ```\n+\n+=== \"After\"\n+\n+    ```yaml title=\"unpinned-uses.yml\" hl_lines=\"8 12\"\n+    name: unpinned-uses\n+    on: [push]\n+\n+    jobs:\n+    unpinned-uses:\n+        runs-on: ubuntu-latest\n+        steps:\n+        - uses: actions/checkout@v4 # (1)!\n+          with:\n+          persist-credentials: false\n+\n+        - uses: docker://ubuntu:24.04\n+          with:\n+          entrypoint: /bin/echo\n+          args: hello!\n+    ```\n+\n+    1. Or `actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683` for a SHA-pinned action.\ndiff --git a/mkdocs.yml b/mkdocs.yml\nindex ec83097..042fb1a 100644\n--- a/mkdocs.yml\n+++ b/mkdocs.yml\n@@ -67,6 +67,11 @@ markdown_extensions:\n   - admonition\n   - pymdownx.details\n   - pymdownx.superfences\n+  - pymdownx.tabbed:\n+      alternate_style: true\n+      slugify: !!python/object/apply:pymdownx.slugs.slugify\n+        kwds:\n+          case: lower\n   - pymdownx.snippets:\n       base_path:\n         - \"docs/snippets\"\n@@ -75,6 +80,10 @@ markdown_extensions:\n   - attr_list\n   - toc:\n       permalink: \ud83d\udd17\n+  - pymdownx.highlight:\n+      anchor_linenums: true\n+      line_spans: __span\n+      pygments_lang_class: true\n \n validation:\n   omitted_files: warn\ndiff --git a/src/audit/impostor_commit.rs b/src/audit/impostor_commit.rs\nindex 7c806d7..d48f148 100644\n--- a/src/audit/impostor_commit.rs\n+++ b/src/audit/impostor_commit.rs\n@@ -5,16 +5,14 @@\n //!\n //! [`clank`]: https://github.com/chainguard-dev/clank\n \n-use std::ops::Deref;\n-\n use anyhow::{anyhow, Result};\n-use github_actions_models::workflow::{job::StepBody, Job};\n+use github_actions_models::workflow::Job;\n \n use super::WorkflowAudit;\n use crate::{\n     finding::{Confidence, Finding, Severity},\n     github_api::{self, Branch, ComparisonStatus, Tag},\n-    models::{Uses, Workflow},\n+    models::{RepositoryUses, Uses, Workflow},\n     state::AuditState,\n };\n \n@@ -25,7 +23,7 @@ pub(crate) struct ImpostorCommit {\n }\n \n impl ImpostorCommit {\n-    fn named_refs(&self, uses: Uses<'_>) -> Result<(Vec<Branch>, Vec<Tag>)> {\n+    fn named_refs(&self, uses: RepositoryUses<'_>) -> Result<(Vec<Branch>, Vec<Tag>)> {\n         let branches = self.client.list_branches(uses.owner, uses.repo)?;\n         let tags = self.client.list_tags(uses.owner, uses.repo)?;\n         Ok((branches, tags))\n@@ -33,7 +31,7 @@ impl ImpostorCommit {\n \n     fn named_ref_contains_commit(\n         &self,\n-        uses: &Uses<'_>,\n+        uses: &RepositoryUses<'_>,\n         base_ref: &str,\n         head_ref: &str,\n     ) -> Result<bool> {\n@@ -57,7 +55,7 @@ impl ImpostorCommit {\n     /// Returns a boolean indicating whether or not this commit is an \"impostor\",\n     /// i.e. resolves due to presence in GitHub's fork network but is not actually\n     /// present in any of the specified `owner/repo`'s tags or branches.\n-    fn impostor(&self, uses: Uses<'_>) -> Result<bool> {\n+    fn impostor(&self, uses: RepositoryUses<'_>) -> Result<bool> {\n         // If there's no ref or the ref is not a commit, there's nothing to impersonate.\n         let Some(head_ref) = uses.commit_ref() else {\n             return Ok(false);\n@@ -141,11 +139,7 @@ impl WorkflowAudit for ImpostorCommit {\n             match *job {\n                 Job::NormalJob(_) => {\n                     for step in job.steps() {\n-                        let StepBody::Uses { uses, .. } = &step.deref().body else {\n-                            continue;\n-                        };\n-\n-                        let Some(uses) = Uses::from_step(uses) else {\n+                        let Some(Uses::Repository(uses)) = step.uses() else {\n                             continue;\n                         };\n \ndiff --git a/src/audit/known_vulnerable_actions.rs b/src/audit/known_vulnerable_actions.rs\nindex e5edcc3..c21953e 100644\n--- a/src/audit/known_vulnerable_actions.rs\n+++ b/src/audit/known_vulnerable_actions.rs\n@@ -6,12 +6,12 @@\n //! See: <https://docs.github.com/en/rest/security-advisories/global-advisories?apiVersion=2022-11-28>\n \n use anyhow::{anyhow, Context, Result};\n-use github_actions_models::workflow::{job::StepBody, Job};\n+use github_actions_models::workflow::Job;\n \n use crate::{\n     finding::{Confidence, Severity},\n     github_api,\n-    models::Uses,\n+    models::{RepositoryUses, Uses},\n     state::AuditState,\n };\n \n@@ -22,7 +22,10 @@ pub(crate) struct KnownVulnerableActions {\n }\n \n impl KnownVulnerableActions {\n-    fn action_known_vulnerabilities(&self, uses: &Uses<'_>) -> Result<Vec<(Severity, String)>> {\n+    fn action_known_vulnerabilities(\n+        &self,\n+        uses: &RepositoryUses<'_>,\n+    ) -> Result<Vec<(Severity, String)>> {\n         let version = match uses.git_ref {\n             // If `uses` is pinned to a symbolic ref, we need to perform\n             // feats of heroism to figure out what's going on.\n@@ -154,11 +157,7 @@ impl WorkflowAudit for KnownVulnerableActions {\n             };\n \n             for step in job.steps() {\n-                let StepBody::Uses { uses, .. } = &step.body else {\n-                    continue;\n-                };\n-\n-                let Some(uses) = Uses::from_step(uses) else {\n+                let Some(Uses::Repository(uses)) = step.uses() else {\n                     continue;\n                 };\n \ndiff --git a/src/audit/mod.rs b/src/audit/mod.rs\nindex d0e6445..9fca985 100644\n--- a/src/audit/mod.rs\n+++ b/src/audit/mod.rs\n@@ -17,6 +17,7 @@ pub(crate) mod known_vulnerable_actions;\n pub(crate) mod ref_confusion;\n pub(crate) mod self_hosted_runner;\n pub(crate) mod template_injection;\n+pub(crate) mod unpinned_uses;\n pub(crate) mod use_trusted_publishing;\n \n pub(crate) trait WorkflowAudit {\ndiff --git a/src/audit/ref_confusion.rs b/src/audit/ref_confusion.rs\nindex 68e8036..c961e37 100644\n--- a/src/audit/ref_confusion.rs\n+++ b/src/audit/ref_confusion.rs\n@@ -9,13 +9,13 @@\n use std::ops::Deref;\n \n use anyhow::{anyhow, Result};\n-use github_actions_models::workflow::{job::StepBody, Job};\n+use github_actions_models::workflow::Job;\n \n use super::WorkflowAudit;\n use crate::{\n     finding::{Confidence, Severity},\n     github_api,\n-    models::Uses,\n+    models::{RepositoryUses, Uses},\n     state::AuditState,\n };\n \n@@ -27,7 +27,7 @@ pub(crate) struct RefConfusion {\n }\n \n impl RefConfusion {\n-    fn confusable(&self, uses: &Uses) -> Result<bool> {\n+    fn confusable(&self, uses: &RepositoryUses) -> Result<bool> {\n         let Some(sym_ref) = uses.symbolic_ref() else {\n             return Ok(false);\n         };\n@@ -90,11 +90,7 @@ impl WorkflowAudit for RefConfusion {\n             match job.deref() {\n                 Job::NormalJob(_) => {\n                     for step in job.steps() {\n-                        let StepBody::Uses { uses, .. } = &step.deref().body else {\n-                            continue;\n-                        };\n-\n-                        let Some(uses) = Uses::from_step(uses) else {\n+                        let Some(Uses::Repository(uses)) = step.uses() else {\n                             continue;\n                         };\n \ndiff --git a/src/audit/unpinned_uses.rs b/src/audit/unpinned_uses.rs\nnew file mode 100644\nindex 0000000..5a159ef\n--- /dev/null\n+++ b/src/audit/unpinned_uses.rs\n@@ -0,0 +1,64 @@\n+use github_actions_models::workflow::Job;\n+\n+use crate::finding::{Confidence, Severity};\n+\n+use super::{AuditState, Finding, Workflow, WorkflowAudit};\n+\n+pub(crate) struct UnpinnedUses {}\n+\n+impl WorkflowAudit for UnpinnedUses {\n+    fn ident() -> &'static str\n+    where\n+        Self: Sized,\n+    {\n+        \"unpinned-uses\"\n+    }\n+\n+    fn desc() -> &'static str\n+    where\n+        Self: Sized,\n+    {\n+        \"unpinned action reference\"\n+    }\n+\n+    fn new(_state: AuditState) -> anyhow::Result<Self>\n+    where\n+        Self: Sized,\n+    {\n+        Ok(Self {})\n+    }\n+\n+    fn audit<'w>(&self, workflow: &'w Workflow) -> anyhow::Result<Vec<Finding<'w>>> {\n+        let mut findings = vec![];\n+\n+        for job in workflow.jobs() {\n+            // No point in checking reusable workflows, since they\n+            // require a ref pin when used outside of the local repo.\n+            let Job::NormalJob(_) = *job else {\n+                continue;\n+            };\n+\n+            for step in job.steps() {\n+                let Some(uses) = step.uses() else {\n+                    continue;\n+                };\n+\n+                if uses.unpinned() {\n+                    findings.push(\n+                        Self::finding()\n+                            .confidence(Confidence::High)\n+                            .severity(Severity::Informational)\n+                            .add_location(\n+                                step.location().with_keys(&[\"uses\".into()]).annotated(\n+                                    \"action is not pinned to a tag, branch, or hash ref\",\n+                                ),\n+                            )\n+                            .build(workflow)?,\n+                    );\n+                }\n+            }\n+        }\n+\n+        Ok(findings)\n+    }\n+}\ndiff --git a/src/main.rs b/src/main.rs\nindex 2986a2a..6a9878b 100644\n--- a/src/main.rs\n+++ b/src/main.rs\n@@ -153,6 +153,7 @@ fn run() -> Result<ExitCode> {\n     register_audit!(audit::hardcoded_container_credentials::HardcodedContainerCredentials);\n     register_audit!(audit::self_hosted_runner::SelfHostedRunner);\n     register_audit!(audit::known_vulnerable_actions::KnownVulnerableActions);\n+    register_audit!(audit::unpinned_uses::UnpinnedUses);\n \n     let bar = ProgressBar::new((workflow_registry.len() * audit_registry.len()) as u64);\n \ndiff --git a/src/models.rs b/src/models.rs\nindex 445b4a9..12031f5 100644\n--- a/src/models.rs\n+++ b/src/models.rs\n@@ -4,7 +4,7 @@\n use std::{collections::hash_map, iter::Enumerate, ops::Deref, path::Path};\n \n use anyhow::{anyhow, Context, Result};\n-use github_actions_models::workflow;\n+use github_actions_models::workflow::{self, job::StepBody};\n \n use crate::finding::{Route, SymbolicLocation};\n \n@@ -145,6 +145,15 @@ impl<'w> Step<'w> {\n         }\n     }\n \n+    /// Returns a [`Uses`] for this [`Step`], if it has one.\n+    pub(crate) fn uses(&self) -> Option<Uses<'w>> {\n+        let StepBody::Uses { uses, .. } = &self.inner.body else {\n+            return None;\n+        };\n+\n+        Uses::from_step(uses)\n+    }\n+\n     /// Returns a symbolic location for this [`Step`].\n     pub(crate) fn location(&self) -> SymbolicLocation<'w> {\n         self.parent.with_step(self)\n@@ -194,104 +203,167 @@ impl<'w> Iterator for Steps<'w> {\n     }\n }\n \n-/// Represents the components of an \"action ref\", i.e. the value\n-/// of a `uses:` clause in a normal job step or a reusable workflow job.\n-/// Does not support `docker://` refs, or \"local\" (i.e. `./`) refs.\n #[derive(Copy, Clone, Debug, PartialEq)]\n-pub(crate) struct Uses<'a> {\n+pub(crate) struct DockerUses<'a> {\n+    pub(crate) registry: Option<&'a str>,\n+    pub(crate) image: &'a str,\n+    pub(crate) tag: Option<&'a str>,\n+    pub(crate) hash: Option<&'a str>,\n+}\n+\n+#[derive(Copy, Clone, Debug, PartialEq)]\n+pub(crate) struct RepositoryUses<'a> {\n     pub(crate) owner: &'a str,\n     pub(crate) repo: &'a str,\n     pub(crate) subpath: Option<&'a str>,\n     pub(crate) git_ref: Option<&'a str>,\n }\n \n-impl<'a> Uses<'a> {\n-    /// Create a new `Uses` manually. No validation of the constituent\n-    /// parts is performed.\n-    fn new(\n-        owner: &'a str,\n-        repo: &'a str,\n-        subpath: Option<&'a str>,\n-        git_ref: Option<&'a str>,\n-    ) -> Self {\n-        Self {\n-            owner,\n-            repo,\n-            subpath,\n-            git_ref,\n+impl<'a> RepositoryUses<'a> {\n+    pub(crate) fn ref_is_commit(&self) -> bool {\n+        match self.git_ref {\n+            Some(git_ref) => git_ref.len() == 40 && git_ref.chars().all(|c| c.is_ascii_hexdigit()),\n+            None => false,\n         }\n     }\n \n-    fn from_common(uses: &'a str) -> Option<Self> {\n-        // We don't currently have enough context to resolve local actions.\n-        if uses.starts_with(\"./\") {\n-            return None;\n+    pub(crate) fn commit_ref(&self) -> Option<&str> {\n+        match self.git_ref {\n+            Some(git_ref) if self.ref_is_commit() => Some(git_ref),\n+            _ => None,\n         }\n+    }\n+\n+    pub(crate) fn symbolic_ref(&self) -> Option<&str> {\n+        match self.git_ref {\n+            Some(git_ref) if !self.ref_is_commit() => Some(git_ref),\n+            _ => None,\n+        }\n+    }\n+}\n \n-        // NOTE: Technically both git refs and action paths can contain `@`,\n-        // so this isn't guaranteed to be correct. In practice, however,\n-        // splitting on the last `@` is mostly reliable.\n-        let (path, git_ref) = match uses.rsplit_once('@') {\n-            Some((path, git_ref)) => (path, Some(git_ref)),\n-            None => (uses, None),\n+/// Represents the components of an \"action ref\", i.e. the value\n+/// of a `uses:` clause in a normal job step or a reusable workflow job.\n+/// Supports Docker (`docker://`) and repository (`actions/checkout`)\n+/// style references, but not local (`./foo`) references.\n+#[derive(Copy, Clone, Debug, PartialEq)]\n+pub(crate) enum Uses<'a> {\n+    Docker(DockerUses<'a>),\n+    Repository(RepositoryUses<'a>),\n+}\n+\n+impl<'a> Uses<'a> {\n+    fn is_registry(registry: &str) -> bool {\n+        // https://stackoverflow.com/a/42116190\n+        registry == \"localhost\" || registry.contains('.') || registry.contains(':')\n+    }\n+\n+    /// Parses a Docker image reference.\n+    /// See: <https://docs.docker.com/reference/cli/docker/image/tag/>\n+    fn from_image_ref(image: &'a str) -> Option<Self> {\n+        let (registry, image) = match image.split_once('/') {\n+            Some((registry, image)) if Self::is_registry(registry) => (Some(registry), image),\n+            _ => (None, image),\n         };\n \n-        let components = path.splitn(3, '/').collect::<Vec<_>>();\n-        if components.len() < 2 {\n-            log::debug!(\"malformed `uses:` ref: {uses}\");\n-            return None;\n+        // NOTE(ww): hashes aren't mentioned anywhere in Docker's own docs,\n+        // but appear to be an OCI thing. GitHub doesn't support them\n+        // yet either, but we expect them to soon (with \"immutable actions\").\n+        if let Some(at_pos) = image.find('@') {\n+            let (image, hash) = image.split_at(at_pos);\n+\n+            let hash = if hash.is_empty() {\n+                None\n+            } else {\n+                Some(&hash[1..])\n+            };\n+\n+            Some(Self::Docker(DockerUses {\n+                registry,\n+                image,\n+                tag: None,\n+                hash,\n+            }))\n+        } else {\n+            let (image, tag) = match image.split_once(':') {\n+                Some((image, \"\")) => (image, None),\n+                Some((image, tag)) => (image, Some(tag)),\n+                _ => (image, None),\n+            };\n+\n+            Some(Self::Docker(DockerUses {\n+                registry,\n+                image,\n+                tag,\n+                hash: None,\n+            }))\n         }\n-\n-        Some(Self::new(\n-            components[0],\n-            components[1],\n-            components.get(2).copied(),\n-            git_ref,\n-        ))\n     }\n \n-    pub(crate) fn from_step(uses: &'a str) -> Option<Self> {\n-        if uses.starts_with(\"docker://\") {\n-            return None;\n+    fn from_common(uses: &'a str) -> Option<Self> {\n+        if uses.starts_with(\"./\") {\n+            None\n+        } else if let Some(image) = uses.strip_prefix(\"docker://\") {\n+            Self::from_image_ref(image)\n+        } else {\n+            // NOTE: Technically both git refs and action paths can contain `@`,\n+            // so this isn't guaranteed to be correct. In practice, however,\n+            // splitting on the last `@` is mostly reliable.\n+            let (path, git_ref) = match uses.rsplit_once('@') {\n+                Some((path, git_ref)) => (path, Some(git_ref)),\n+                None => (uses, None),\n+            };\n+\n+            let components = path.splitn(3, '/').collect::<Vec<_>>();\n+            if components.len() < 2 {\n+                log::debug!(\"malformed `uses:` ref: {uses}\");\n+                return None;\n+            }\n+\n+            Some(Self::Repository(RepositoryUses {\n+                owner: components[0],\n+                repo: components[1],\n+                subpath: components.get(2).copied(),\n+                git_ref,\n+            }))\n         }\n+    }\n \n+    pub(crate) fn from_step(uses: &'a str) -> Option<Self> {\n         Self::from_common(uses)\n     }\n \n-    pub(crate) fn from_reusable(uses: &'a str) -> Option<Self> {\n+    /// Parse a [`Uses`] from a reusable workflow `uses:` clause.\n+    ///\n+    /// Returns only the [`RepositoryUses`] variant since Docker actions\n+    /// can't be used in reusable workflows.\n+    pub(crate) fn from_reusable(uses: &'a str) -> Option<RepositoryUses> {\n         match Self::from_common(uses) {\n+            // Reusable workflows don't support Docker actions.\n+            Some(Uses::Docker(DockerUses { .. })) => None,\n             // Reusable workflows require a git ref.\n-            Some(uses) if uses.git_ref.is_none() => None,\n-            Some(uses) => Some(uses),\n+            Some(Uses::Repository(RepositoryUses {\n+                owner: _,\n+                repo: _,\n+                subpath: _,\n+                git_ref,\n+            })) if git_ref.is_none() => None,\n+            Some(Uses::Repository(repo)) => Some(repo),\n             None => None,\n         }\n     }\n \n-    pub(crate) fn ref_is_commit(&self) -> bool {\n-        match self.git_ref {\n-            Some(git_ref) => git_ref.len() == 40 && git_ref.chars().all(|c| c.is_ascii_hexdigit()),\n-            None => false,\n-        }\n-    }\n-\n-    pub(crate) fn commit_ref(&self) -> Option<&str> {\n-        match self.git_ref {\n-            Some(git_ref) if self.ref_is_commit() => Some(git_ref),\n-            _ => None,\n-        }\n-    }\n-\n-    pub(crate) fn symbolic_ref(&self) -> Option<&str> {\n-        match self.git_ref {\n-            Some(git_ref) if !self.ref_is_commit() => Some(git_ref),\n-            _ => None,\n+    pub(crate) fn unpinned(&self) -> bool {\n+        match self {\n+            Uses::Docker(docker) => docker.hash.is_none() && docker.tag.is_none(),\n+            Uses::Repository(repo) => repo.git_ref.is_none(),\n         }\n     }\n }\n \n #[cfg(test)]\n mod tests {\n-    use super::Uses;\n+    use super::{DockerUses, RepositoryUses, Uses};\n \n     #[test]\n     fn uses_from_step() {\n@@ -299,46 +371,141 @@ mod tests {\n             (\n                 // Valid: fully pinned.\n                 \"actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3\",\n-                Some(Uses::new(\n-                    \"actions\",\n-                    \"checkout\",\n-                    None,\n-                    Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n-                )),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"actions\",\n+                    repo: \"checkout\",\n+                    subpath: None,\n+                    git_ref: Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n+                })),\n             ),\n             (\n                 // Valid: fully pinned, subpath\n                 \"actions/aws/ec2@8f4b7f84864484a7bf31766abe9204da3cbe65b3\",\n-                Some(Uses::new(\n-                    \"actions\",\n-                    \"aws\",\n-                    Some(\"ec2\"),\n-                    Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n-                )),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"actions\",\n+                    repo: \"aws\",\n+                    subpath: Some(\"ec2\"),\n+                    git_ref: Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n+                })),\n             ),\n             (\n                 // Valid: fully pinned, complex subpath\n                 \"example/foo/bar/baz/quux@8f4b7f84864484a7bf31766abe9204da3cbe65b3\",\n-                Some(Uses::new(\n-                    \"example\",\n-                    \"foo\",\n-                    Some(\"bar/baz/quux\"),\n-                    Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n-                )),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"example\",\n+                    repo: \"foo\",\n+                    subpath: Some(\"bar/baz/quux\"),\n+                    git_ref: Some(\"8f4b7f84864484a7bf31766abe9204da3cbe65b3\"),\n+                })),\n             ),\n             (\n                 // Valid: pinned with branch/tag\n                 \"actions/checkout@v4\",\n-                Some(Uses::new(\"actions\", \"checkout\", None, Some(\"v4\"))),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"actions\",\n+                    repo: \"checkout\",\n+                    subpath: None,\n+                    git_ref: Some(\"v4\"),\n+                })),\n             ),\n             (\n                 \"actions/checkout@abcd\",\n-                Some(Uses::new(\"actions\", \"checkout\", None, Some(\"abcd\"))),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"actions\",\n+                    repo: \"checkout\",\n+                    subpath: None,\n+                    git_ref: Some(\"abcd\"),\n+                })),\n             ),\n             (\n                 // Valid: unpinned\n                 \"actions/checkout\",\n-                Some(Uses::new(\"actions\", \"checkout\", None, None)),\n+                Some(Uses::Repository(RepositoryUses {\n+                    owner: \"actions\",\n+                    repo: \"checkout\",\n+                    subpath: None,\n+                    git_ref: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, implicit registry\n+                \"docker://alpine:3.8\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: None,\n+                    image: \"alpine\",\n+                    tag: Some(\"3.8\"),\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, localhost\n+                \"docker://localhost/alpine:3.8\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: Some(\"localhost\"),\n+                    image: \"alpine\",\n+                    tag: Some(\"3.8\"),\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, localhost w/ port\n+                \"docker://localhost:1337/alpine:3.8\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: Some(\"localhost:1337\"),\n+                    image: \"alpine\",\n+                    tag: Some(\"3.8\"),\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, custom registry\n+                \"docker://ghcr.io/foo/alpine:3.8\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: Some(\"ghcr.io\"),\n+                    image: \"foo/alpine\",\n+                    tag: Some(\"3.8\"),\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, missing tag\n+                \"docker://ghcr.io/foo/alpine\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: Some(\"ghcr.io\"),\n+                    image: \"foo/alpine\",\n+                    tag: None,\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Invalid, but allowed: Docker ref, empty tag\n+                \"docker://ghcr.io/foo/alpine:\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: Some(\"ghcr.io\"),\n+                    image: \"foo/alpine\",\n+                    tag: None,\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, bare\n+                \"docker://alpine\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: None,\n+                    image: \"alpine\",\n+                    tag: None,\n+                    hash: None,\n+                })),\n+            ),\n+            (\n+                // Valid: Docker ref, hash\n+                \"docker://alpine@hash\",\n+                Some(Uses::Docker(DockerUses {\n+                    registry: None,\n+                    image: \"alpine\",\n+                    tag: None,\n+                    hash: Some(\"hash\"),\n+                })),\n             ),\n             // Invalid: missing user/repo\n             (\"checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3\", None),\n@@ -347,8 +514,6 @@ mod tests {\n                 \"./.github/actions/hello-world-action@172239021f7ba04fe7327647b213799853a9eb89\",\n                 None,\n             ),\n-            // Invalid: Docker refs not supported\n-            (\"docker://alpine:3.8\", None),\n         ];\n \n         for (input, expected) in vectors {\n@@ -363,30 +528,30 @@ mod tests {\n             (\n                 \"octo-org/this-repo/.github/workflows/workflow-1.yml@\\\n                  172239021f7ba04fe7327647b213799853a9eb89\",\n-                Some(Uses::new(\n-                    \"octo-org\",\n-                    \"this-repo\",\n-                    Some(\".github/workflows/workflow-1.yml\"),\n-                    Some(\"172239021f7ba04fe7327647b213799853a9eb89\"),\n-                )),\n+                Some(RepositoryUses {\n+                    owner: \"octo-org\",\n+                    repo: \"this-repo\",\n+                    subpath: Some(\".github/workflows/workflow-1.yml\"),\n+                    git_ref: Some(\"172239021f7ba04fe7327647b213799853a9eb89\"),\n+                }),\n             ),\n             (\n                 \"octo-org/this-repo/.github/workflows/workflow-1.yml@notahash\",\n-                Some(Uses::new(\n-                    \"octo-org\",\n-                    \"this-repo\",\n-                    Some(\".github/workflows/workflow-1.yml\"),\n-                    Some(\"notahash\"),\n-                )),\n+                Some(RepositoryUses {\n+                    owner: \"octo-org\",\n+                    repo: \"this-repo\",\n+                    subpath: Some(\".github/workflows/workflow-1.yml\"),\n+                    git_ref: Some(\"notahash\"),\n+                }),\n             ),\n             (\n                 \"octo-org/this-repo/.github/workflows/workflow-1.yml@abcd\",\n-                Some(Uses::new(\n-                    \"octo-org\",\n-                    \"this-repo\",\n-                    Some(\".github/workflows/workflow-1.yml\"),\n-                    Some(\"abcd\"),\n-                )),\n+                Some(RepositoryUses {\n+                    owner: \"octo-org\",\n+                    repo: \"this-repo\",\n+                    subpath: Some(\".github/workflows/workflow-1.yml\"),\n+                    git_ref: Some(\"abcd\"),\n+                }),\n             ),\n             // Invalid: no ref at all\n             (\"octo-org/this-repo/.github/workflows/workflow-1.yml\", None),\n@@ -410,16 +575,16 @@ mod tests {\n     #[test]\n     fn uses_ref_is_commit() {\n         assert!(\n-            Uses::from_step(\"actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3\")\n+            Uses::from_reusable(\"actions/checkout@8f4b7f84864484a7bf31766abe9204da3cbe65b3\")\n                 .unwrap()\n                 .ref_is_commit()\n         );\n \n-        assert!(!Uses::from_step(\"actions/checkout@v4\")\n+        assert!(!Uses::from_reusable(\"actions/checkout@v4\")\n             .unwrap()\n             .ref_is_commit());\n \n-        assert!(!Uses::from_step(\"actions/checkout@abcd\")\n+        assert!(!Uses::from_reusable(\"actions/checkout@abcd\")\n             .unwrap()\n             .ref_is_commit());\n     }\n", "instance_id": "woodruffw__zizmor-161", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of implementing an audit to detect unpinned GitHub Actions in workflows, which could pose a security risk due to potential compromises. It specifies the problem (unpinned actions), the desired solution (an audit to flag such actions), and provides a basic example of what should be flagged (e.g., `actions/checkout` without a version). However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes a \"pinned\" action beyond a general mention of version, branch, or commit ID, nor does it clarify the expected behavior for different types of actions (e.g., Docker actions are mentioned in the code changes but not in the problem statement). Additionally, edge cases or specific constraints (e.g., handling of local actions or malformed `uses` clauses) are not addressed, which could lead to implementation uncertainties. Despite these minor gaps, the intent and primary requirements are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files, including adding a new audit module (`unpinned_uses.rs`), updating existing models to support both Docker and repository action references, and modifying documentation. This requires understanding and integrating with the existing codebase structure, particularly the workflow parsing and audit framework. Second, the technical concepts involved include parsing and validating GitHub Actions workflow syntax (specifically `uses` clauses), handling different reference types (Docker and repository), and integrating a new audit check into an existing registry of audits. While these concepts are not overly complex for a senior engineer, they do require familiarity with Rust's type system, pattern matching, and the specific domain of GitHub Actions workflows. Third, the problem involves moderate edge case handling, such as distinguishing between different types of references and identifying unpinned actions, though the code changes suggest these are handled straightforwardly without deep complexity. The overall impact on the system's architecture is minimal, as this is an additive feature rather than a refactor of core components. A score of 0.45 reflects a medium difficulty task that requires understanding multiple concepts and making targeted modifications across several files, but does not demand deep architectural changes or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Lambda Support as CloudWatch Alarm Action\n### Feature scope\n\nCloudWatch\n\n### Describe your suggested feature\n\nSince the end of December 2023, AWS now supports the triggering of a Lambda function as a CloudWatch Alarm action target (cf. https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-cloudwatch-alarms-lambda-change-action/). This is useful, for example, when you want to modify the email body before sending it to another SNS topic where a list of email addresses is subscribed.\r\n\r\nCurrently, `cdk-monitoring-constructs` only supports SNS topics like CloudWatch Alarms action. Would it be possible to implement Lambda as an Alarm action as well?\n", "patch": "diff --git a/API.md b/API.md\nindex 42307b75..c2b6bc70 100644\n--- a/API.md\n+++ b/API.md\n@@ -65597,6 +65597,55 @@ public readonly streamUrl: string;\n ---\n \n \n+### LambdaAlarmActionStrategy <a name=\"LambdaAlarmActionStrategy\" id=\"cdk-monitoring-constructs.LambdaAlarmActionStrategy\"></a>\n+\n+- *Implements:* <a href=\"#cdk-monitoring-constructs.IAlarmActionStrategy\">IAlarmActionStrategy</a>\n+\n+Alarm action strategy that triggers a Lambda function.\n+\n+#### Initializers <a name=\"Initializers\" id=\"cdk-monitoring-constructs.LambdaAlarmActionStrategy.Initializer\"></a>\n+\n+```typescript\n+import { LambdaAlarmActionStrategy } from 'cdk-monitoring-constructs'\n+\n+new LambdaAlarmActionStrategy(lambdaFunction: IAlias | IFunction | IVersion)\n+```\n+\n+| **Name** | **Type** | **Description** |\n+| --- | --- | --- |\n+| <code><a href=\"#cdk-monitoring-constructs.LambdaAlarmActionStrategy.Initializer.parameter.lambdaFunction\">lambdaFunction</a></code> | <code>aws-cdk-lib.aws_lambda.IAlias \\| aws-cdk-lib.aws_lambda.IFunction \\| aws-cdk-lib.aws_lambda.IVersion</code> | *No description.* |\n+\n+---\n+\n+##### `lambdaFunction`<sup>Required</sup> <a name=\"lambdaFunction\" id=\"cdk-monitoring-constructs.LambdaAlarmActionStrategy.Initializer.parameter.lambdaFunction\"></a>\n+\n+- *Type:* aws-cdk-lib.aws_lambda.IAlias | aws-cdk-lib.aws_lambda.IFunction | aws-cdk-lib.aws_lambda.IVersion\n+\n+---\n+\n+#### Methods <a name=\"Methods\" id=\"Methods\"></a>\n+\n+| **Name** | **Description** |\n+| --- | --- |\n+| <code><a href=\"#cdk-monitoring-constructs.LambdaAlarmActionStrategy.addAlarmActions\">addAlarmActions</a></code> | *No description.* |\n+\n+---\n+\n+##### `addAlarmActions` <a name=\"addAlarmActions\" id=\"cdk-monitoring-constructs.LambdaAlarmActionStrategy.addAlarmActions\"></a>\n+\n+```typescript\n+public addAlarmActions(props: AlarmActionStrategyProps): void\n+```\n+\n+###### `props`<sup>Required</sup> <a name=\"props\" id=\"cdk-monitoring-constructs.LambdaAlarmActionStrategy.addAlarmActions.parameter.props\"></a>\n+\n+- *Type:* <a href=\"#cdk-monitoring-constructs.AlarmActionStrategyProps\">AlarmActionStrategyProps</a>\n+\n+---\n+\n+\n+\n+\n ### LambdaFunctionEnhancedMetricFactory <a name=\"LambdaFunctionEnhancedMetricFactory\" id=\"cdk-monitoring-constructs.LambdaFunctionEnhancedMetricFactory\"></a>\n \n #### Initializers <a name=\"Initializers\" id=\"cdk-monitoring-constructs.LambdaFunctionEnhancedMetricFactory.Initializer\"></a>\n@@ -77818,7 +77867,7 @@ public readonly humanReadableName: string;\n \n ### IAlarmActionStrategy <a name=\"IAlarmActionStrategy\" id=\"cdk-monitoring-constructs.IAlarmActionStrategy\"></a>\n \n-- *Implemented By:* <a href=\"#cdk-monitoring-constructs.MultipleAlarmActionStrategy\">MultipleAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.NoopAlarmActionStrategy\">NoopAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.OpsItemAlarmActionStrategy\">OpsItemAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.SnsAlarmActionStrategy\">SnsAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.IAlarmActionStrategy\">IAlarmActionStrategy</a>\n+- *Implemented By:* <a href=\"#cdk-monitoring-constructs.LambdaAlarmActionStrategy\">LambdaAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.MultipleAlarmActionStrategy\">MultipleAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.NoopAlarmActionStrategy\">NoopAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.OpsItemAlarmActionStrategy\">OpsItemAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.SnsAlarmActionStrategy\">SnsAlarmActionStrategy</a>, <a href=\"#cdk-monitoring-constructs.IAlarmActionStrategy\">IAlarmActionStrategy</a>\n \n An object that appends actions to alarms.\n \ndiff --git a/lib/common/alarm/action/LambdaAlarmActionStrategy.ts b/lib/common/alarm/action/LambdaAlarmActionStrategy.ts\nnew file mode 100644\nindex 00000000..4be0a129\n--- /dev/null\n+++ b/lib/common/alarm/action/LambdaAlarmActionStrategy.ts\n@@ -0,0 +1,28 @@\n+import { LambdaAction } from \"aws-cdk-lib/aws-cloudwatch-actions\";\n+import { IAlias, IFunction, IVersion } from \"aws-cdk-lib/aws-lambda\";\n+\n+import {\n+  AlarmActionStrategyProps,\n+  IAlarmActionStrategy,\n+} from \"./IAlarmActionStrategy\";\n+\n+export function triggerLambda(\n+  lambdaFunction: IAlias | IVersion | IFunction,\n+): IAlarmActionStrategy {\n+  return new LambdaAlarmActionStrategy(lambdaFunction);\n+}\n+\n+/**\n+ * Alarm action strategy that triggers a Lambda function.\n+ */\n+export class LambdaAlarmActionStrategy implements IAlarmActionStrategy {\n+  protected readonly lambdaFunction: IAlias | IVersion | IFunction;\n+\n+  constructor(lambdaFunction: IAlias | IVersion | IFunction) {\n+    this.lambdaFunction = lambdaFunction;\n+  }\n+\n+  addAlarmActions(props: AlarmActionStrategyProps): void {\n+    props.alarm.addAlarmAction(new LambdaAction(this.lambdaFunction));\n+  }\n+}\ndiff --git a/lib/common/alarm/action/index.ts b/lib/common/alarm/action/index.ts\nindex 7dc6bb1f..9b0e98bb 100644\n--- a/lib/common/alarm/action/index.ts\n+++ b/lib/common/alarm/action/index.ts\n@@ -1,4 +1,5 @@\n export * from \"./IAlarmActionStrategy\";\n+export * from \"./LambdaAlarmActionStrategy\";\n export * from \"./MultipleAlarmActionStrategy\";\n export * from \"./NoopAlarmActionStrategy\";\n export * from \"./OpsItemAlarmActionStrategy\";\n", "instance_id": "cdklabs__cdk-monitoring-constructs-597", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the feature request to add support for Lambda functions as CloudWatch Alarm actions in the `cdk-monitoring-constructs` library. It provides context about the AWS feature update and a practical use case (modifying email content before sending to an SNS topic). The goal is evident: extend the library to support Lambda as an alarm action alongside the existing SNS support. However, there are minor ambiguities and missing details. For instance, the statement does not specify any constraints or requirements for the Lambda function (e.g., permissions, input format for the alarm data, or expected behavior). Additionally, edge cases or potential failure scenarios (e.g., Lambda execution failures or timeouts) are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, primarily involving the addition of a new `LambdaAlarmActionStrategy` class and updates to related files (e.g., API documentation and export index). The changes span a few files but do not impact the broader system architecture or require extensive refactoring. The amount of code added is modest, focusing on integrating an existing AWS CDK construct (`LambdaAction`) into the library's alarm action framework.\n\n2. **Technical Concepts Involved**: Solving this problem requires familiarity with AWS CDK (specifically the `aws-cloudwatch-actions` and `aws-lambda` modules), TypeScript (the language used in the codebase), and the library's existing alarm action strategy pattern. These concepts are not overly complex for someone with moderate experience in cloud infrastructure or CDK-based development. The implementation mirrors existing strategies like `SnsAlarmActionStrategy`, reducing the learning curve.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases or error handling requirements, and the provided code changes do not address potential issues like Lambda invocation failures or permission errors. While some error handling might be implicitly handled by the AWS CDK constructs, the lack of explicit focus on edge cases keeps the complexity low at this stage.\n\n4. **Overall Complexity**: The task involves straightforward feature addition by extending an existing interface (`IAlarmActionStrategy`) and leveraging AWS CDK's built-in functionality. It requires understanding the library's design pattern for alarm actions but does not demand deep architectural changes or advanced problem-solving skills.\n\nA score of 0.35 reflects that this is slightly more involved than a trivial change (e.g., updating a constant) due to the need to understand the CDK library and the project's structure, but it remains an easy task for a developer with basic to intermediate experience in AWS CDK and TypeScript.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Can only be used with specific aws-cdk-lib versions due to alpha module peer dependencies\n### Version\n\nv1.18.4\n\n### Steps and/or minimal code example to reproduce\n\nThe peer-dependencies are specified in a way that is not working when using any cdk version higher than 2.18.0.\r\n\r\n```\r\n    \"@aws-cdk/aws-apigatewayv2-alpha\": \"^2.18.0-alpha.0\",\r\n    \"@aws-cdk/aws-appsync-alpha\": \"^2.18.0-alpha.0\",\r\n    \"@aws-cdk/aws-redshift-alpha\": \"^2.18.0-alpha.0\",\r\n    \"@aws-cdk/aws-synthetics-alpha\": \"^2.18.0-alpha.0\",\r\n```\r\n\r\nThese only match `2.18.0-alpha.0`, `2.18.0-alpha.1`, `2.18.0-alpha.2` and so on but not `2.19.0-alpha.0` or in my case `2.22.0-alpha.0`\n\n### Expected behavior\n\nAny minor version higher than 2.18.0-alpha.0 can be used.\n\n### Actual behavior\n\nError: Error: Declared dependency on version ^2.18.0-alpha.0 of @aws-cdk/aws-apigatewayv2-alpha, but version 2.22.0-alpha.0 was found\n\n### Other details\n\nI couldn't find a way to make it work with overrides or resolutions and to make it even worse there is no notation that matches `2.18.0-alpha.0` and `2.19.0-alpha.0` other than `2.18.0-alpha.0 || 2.19.0-alpha.0`.\n", "patch": "diff --git a/.projen/deps.json b/.projen/deps.json\nindex 0dd3a5f4..ea47b889 100644\n--- a/.projen/deps.json\n+++ b/.projen/deps.json\n@@ -119,11 +119,6 @@\n       \"version\": \"2.6.0\",\n       \"type\": \"override\"\n     },\n-    {\n-      \"name\": \"@aws-cdk/aws-redshift-alpha\",\n-      \"version\": \"^2.112.0-alpha.0\",\n-      \"type\": \"peer\"\n-    },\n     {\n       \"name\": \"aws-cdk-lib\",\n       \"version\": \"^2.112.0\",\ndiff --git a/.projenrc.ts b/.projenrc.ts\nindex 4dca79ff..4da782af 100644\n--- a/.projenrc.ts\n+++ b/.projenrc.ts\n@@ -79,10 +79,6 @@ _By submitting this pull request, I confirm that my contribution is made under t\n \n // Experimental modules\n [\"@aws-cdk/aws-redshift-alpha\"].forEach((dep) => {\n-  project.deps.addDependency(\n-    `${dep}@^${CDK_VERSION}-alpha.0`,\n-    DependencyType.PEER\n-  );\n   project.deps.addDependency(\n     `${dep}@${CDK_VERSION}-alpha.0`,\n     DependencyType.DEVENV\ndiff --git a/README.md b/README.md\nindex c2040ff9..88077b67 100644\n--- a/README.md\n+++ b/README.md\n@@ -29,7 +29,6 @@ In your `package.json`:\n     \"cdk-monitoring-constructs\": \"^7.0.0\",\n \n     // peer dependencies of cdk-monitoring-constructs\n-    \"@aws-cdk/aws-redshift-alpha\": \"^2.112.0-alpha.0\",\n     \"aws-cdk-lib\": \"^2.112.0\",\n     \"constructs\": \"^10.0.5\"\n \ndiff --git a/lib/facade/MonitoringAspect.ts b/lib/facade/MonitoringAspect.ts\nindex da7e08e4..e678ad9c 100644\n--- a/lib/facade/MonitoringAspect.ts\n+++ b/lib/facade/MonitoringAspect.ts\n@@ -1,4 +1,3 @@\n-import * as redshift from \"@aws-cdk/aws-redshift-alpha\";\n import { IAspect } from \"aws-cdk-lib\";\n import * as apigw from \"aws-cdk-lib/aws-apigateway\";\n import * as apigwv2 from \"aws-cdk-lib/aws-apigatewayv2\";\n@@ -17,6 +16,7 @@ import * as kinesisfirehose from \"aws-cdk-lib/aws-kinesisfirehose\";\n import * as lambda from \"aws-cdk-lib/aws-lambda\";\n import * as opensearch from \"aws-cdk-lib/aws-opensearchservice\";\n import * as rds from \"aws-cdk-lib/aws-rds\";\n+import { CfnCluster } from \"aws-cdk-lib/aws-redshift\";\n import * as s3 from \"aws-cdk-lib/aws-s3\";\n import * as secretsmanager from \"aws-cdk-lib/aws-secretsmanager\";\n import * as sns from \"aws-cdk-lib/aws-sns\";\n@@ -341,15 +341,24 @@ export class MonitoringAspect implements IAspect {\n \n   private monitorRedshift(node: IConstruct) {\n     const [isEnabled, props] = this.getMonitoringDetails(this.props.redshift);\n-    if (isEnabled && node instanceof redshift.Cluster) {\n+    if (isEnabled && this.isProbablyL2RedshiftCluster(node)) {\n+      const cfnCluster = (node as any).cluster as CfnCluster;\n       this.monitoringFacade.monitorRedshiftCluster({\n-        clusterIdentifier: node.clusterName,\n-        alarmFriendlyName: node.node.path,\n+        clusterIdentifier: cfnCluster.ref,\n+        alarmFriendlyName: cfnCluster.node.path,\n         ...props,\n       });\n     }\n   }\n \n+  private isProbablyL2RedshiftCluster(node: IConstruct): boolean {\n+    return (\n+      (node as any).cluster instanceof CfnCluster &&\n+      !!(node as any).clusterName &&\n+      !!(node as any).node?.path\n+    );\n+  }\n+\n   private monitorS3(node: IConstruct) {\n     const [isEnabled, props] = this.getMonitoringDetails(this.props.s3);\n     if (isEnabled && node instanceof s3.Bucket) {\ndiff --git a/package.json b/package.json\nindex 05ff7394..9b5a6a2a 100644\n--- a/package.json\n+++ b/package.json\n@@ -65,7 +65,6 @@\n     \"typescript\": \"^4.9.5\"\n   },\n   \"peerDependencies\": {\n-    \"@aws-cdk/aws-redshift-alpha\": \"^2.112.0-alpha.0\",\n     \"aws-cdk-lib\": \"^2.112.0\",\n     \"constructs\": \"^10.0.5\"\n   },\n", "instance_id": "cdklabs__cdk-monitoring-constructs-530", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue with peer dependency versioning in the AWS CDK library, specifically with alpha modules not being compatible with versions higher than 2.18.0. It provides a clear description of the expected behavior (support for higher minor versions) and the actual behavior (dependency version mismatch error). Steps to reproduce are included with relevant dependency declarations, and the issue is contextualized with version-specific details. However, there are minor ambiguities: the problem statement does not explicitly discuss potential workarounds or constraints beyond dependency notation issues, nor does it specify if there are broader implications for other alpha modules or CDK constructs. Additionally, edge cases or alternative scenarios (e.g., compatibility with non-alpha versions or other dependency management tools) are not mentioned. Thus, while the core issue is well-articulated, some minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (e.g., `.projen/deps.json`, `.projenrc.ts`, `package.json`, `README.md`, and a source file `MonitoringAspect.ts`), indicating a moderate level of codebase interaction. The changes include removing peer dependency declarations for `@aws-cdk/aws-redshift-alpha` and modifying logic in the `MonitoringAspect.ts` file to handle Redshift clusters without directly importing the alpha module, instead using a lower-level `CfnCluster` construct. This requires understanding AWS CDK constructs, dependency management, and the implications of shifting from L2 to L1 constructs, which adds to the complexity.\n\nSecond, the technical concepts involved include familiarity with AWS CDK (specifically alpha modules and peer dependencies), TypeScript/JavaScript, and dependency management in Node.js (e.g., understanding version specifiers like `^` and their impact on compatibility). Additionally, the code change in `MonitoringAspect.ts` introduces a type assertion and a custom type-checking method (`isProbablyL2RedshiftCluster`), which requires a nuanced understanding of CDK's construct hierarchy and runtime behavior.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes imply potential risks, such as ensuring that the replacement logic for detecting Redshift clusters works across different CDK versions and construct configurations. Error handling is not directly modified, but the removal of a direct dependency on the alpha module could introduce subtle compatibility issues that need to be tested.\n\nFinally, the impact on the system's architecture is moderate; it does not fundamentally alter the system's design but does change how a specific AWS service (Redshift) is monitored, which could affect users relying on this functionality. Given these considerations\u2014multiple file changes, moderate technical depth in CDK and dependency management, and implicit edge case concerns\u2014I assign a difficulty score of 0.55, placing it in the medium range (0.4-0.6), as it requires understanding multiple concepts and making non-trivial modifications across several files.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Infinite watch stream loop\nHi! The below snippet works just fine on version 30.1.1 when I invoke on a pod in \"Succeeded\" state:\r\n\r\n```python\r\nimport asyncio\r\n\r\nfrom kubernetes_asyncio import client, watch, config\r\nfrom kubernetes_asyncio.client.api_client import ApiClient\r\n\r\nif __name__ == \"__main__\":\r\n    async def main():\r\n        pod = \"...\"\r\n        namespace = \"...\"\r\n        await config.load_kube_config()\r\n\r\n        async with ApiClient() as api:\r\n            watcher = watch.Watch()\r\n            async for log in watcher.stream(client.CoreV1Api(api).read_namespaced_pod_log, pod, namespace):\r\n                print(log, end=\"\")\r\n\r\n    asyncio.run(main())\r\n```\r\nthe loop ends and execution stops after the logs finish.\r\n\r\nHowever same snippet keeps looping forever on version 30.3.0. Once the logs finish, it starts over and repeats.\n", "patch": "diff --git a/kubernetes_asyncio/watch/watch.py b/kubernetes_asyncio/watch/watch.py\nindex 493c700c..ab09f2e3 100644\n--- a/kubernetes_asyncio/watch/watch.py\n+++ b/kubernetes_asyncio/watch/watch.py\n@@ -174,16 +174,20 @@ async def next(self):\n \n             line = line.decode('utf8')\n \n+            # Special case for faster log streaming\n+            if self.return_type == 'str':\n+                if line == '':\n+                    # end of log\n+                    raise StopAsyncIteration\n+                return line\n+\n             # Stop the iterator if K8s sends an empty response. This happens when\n             # eg the supplied timeout has expired.\n             if line == '':\n                 if watch_forever:\n                     self._reconnect()\n                     continue\n-\n-            # Special case for faster log streaming\n-            if self.return_type == 'str':\n-                return line\n+                raise StopAsyncIteration\n \n             # retry 410 error only once\n             try:\n", "instance_id": "tomplus__kubernetes_asyncio-330", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a change in behavior between versions 30.1.1 and 30.3.0 of a Kubernetes asyncio library, where a log streaming loop that previously terminated now runs indefinitely. The goal (fixing the infinite loop) is implied, and a code snippet is provided to reproduce the issue. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly state whether the desired behavior is to always stop after logs finish (as in 30.1.1) or if there are specific conditions under which the loop should continue. Additionally, constraints or expectations around timeouts and reconnection behavior are not mentioned, which could impact the solution. Edge cases, such as empty logs or connection interruptions, are also not addressed in the statement. Despite these minor gaps, the issue is understandable with the provided context and code changes.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a small, localized change in a single file (`watch.py`) within the `kubernetes_asyncio` library. The modification involves reordering logic for handling log streaming and raising a `StopAsyncIteration` exception when an empty line is encountered, indicating the end of logs. The change is minimal (a few lines) and does not impact the broader architecture or multiple modules.\n\n2. **Technical Concepts Involved**: Solving this requires understanding basic Python asyncio concepts, such as asynchronous iteration and exception handling (`StopAsyncIteration`). Familiarity with the Kubernetes client library's `watch` module and its behavior for log streaming is necessary, but this is not overly complex for someone with moderate experience in Python or async programming. No advanced algorithms, design patterns, or domain-specific knowledge beyond Kubernetes API interactions are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change addresses a specific condition (empty line as end-of-log marker). The modification introduces a simple check to terminate the loop, which suggests minimal complexity in error handling. However, potential edge cases like network interruptions or malformed log data are not addressed in the diff or problem statement, and the solution does not appear to require extensive handling of such scenarios.\n\n4. **Overall Complexity**: The issue is a straightforward bug fix involving a clear cause (change in loop termination logic between versions) and a simple solution (repositioning the termination condition). It requires understanding the intent of the `watch_forever` flag and log streaming behavior, but this is not a deep or intricate problem. The impact is limited to the log streaming functionality, and there are no indications of performance or system-wide considerations.\n\nGiven these points, a difficulty score of 0.35 reflects an \"Easy\" problem that requires moderate understanding of the code logic and a small, targeted modification. It is slightly above the lower end of the range due to the need to understand asyncio and Kubernetes client behavior, but it does not approach medium difficulty as the scope and complexity remain limited.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FR] Button to remove all pending/unapplied match recommendations\nThe match recommendations are currently shared by manual processing and _Unify_ jobs.\r\n\r\nThis makes it very risky if _Unify_ jobs are scheduled because manual processing must end before the next scheduled run.\r\n\r\nWe might get way too many recommendations to handle manually. In such cases, _Unify_ jobs have to be de-scheduled to prevent massive uncontrolled unifications that could cause snowballs.\r\n\r\nAnd match recommendations can only be removed by accessing the database directly. There's no option in the WUI, nor in the API to do so.\r\n\r\nThe most basic solution would be a button in the interface next to the recommendations count (or at least an option in the API) that disables (removes) the unapplied recommendations.\n", "patch": "diff --git a/releases/unreleased/remove-merge-recommendations.yml b/releases/unreleased/remove-merge-recommendations.yml\nnew file mode 100644\nindex 00000000..505b62f4\n--- /dev/null\n+++ b/releases/unreleased/remove-merge-recommendations.yml\n@@ -0,0 +1,9 @@\n+---\n+title: Remove merge recommendations\n+category: added\n+author: Eva Mill\u00e1n <evamillan@bitergia.com>\n+issue: 883\n+notes: >\n+  Recommendations can now be deleted from the database\n+  and not just dismissed. This is useful in case there\n+  are too many recommendations to handle manually. \ndiff --git a/sortinghat/core/api.py b/sortinghat/core/api.py\nindex 88930284..06fdb94b 100644\n--- a/sortinghat/core/api.py\n+++ b/sortinghat/core/api.py\n@@ -47,6 +47,7 @@\n                  delete_scheduled_task as delete_scheduled_task_db,\n                  delete_domain as delete_domain_db,\n                  delete_alias as delete_alias_db,\n+                 delete_merge_recommendations as delete_merge_recommendations_db,\n                  update_profile as update_profile_db,\n                  update_scheduled_task as update_scheduled_task_db,\n                  move_identity as move_identity_db,\n@@ -63,7 +64,7 @@\n                      DuplicateRangeError,\n                      EqualIndividualError)\n from .log import TransactionsLog\n-from .models import Identity, MIN_PERIOD_DATE, MAX_PERIOD_DATE\n+from .models import Identity, MergeRecommendation, MIN_PERIOD_DATE, MAX_PERIOD_DATE\n from .aux import merge_datetime_ranges\n from .decorators import atomic_using_tenant\n from ..utils import generate_uuid\n@@ -1617,3 +1618,24 @@ def delete_scheduled_task(ctx, task_id):\n     logger.info(f\"Task {task_id} deleted\")\n \n     return task\n+\n+\n+@atomic_using_tenant\n+def delete_merge_recommendations(ctx):\n+    \"\"\"Remove merge recommendations from the registry.\n+\n+    This function removes all the merge recommendations that have not been\n+    processed from the registry.\n+\n+    :param ctx: context from where this method is called\n+    \"\"\"\n+    trxl = TransactionsLog.open('delete_merge_recommendations', ctx)\n+\n+    recommendations = MergeRecommendation.objects.filter(applied__isnull=True)\n+    delete_merge_recommendations_db(trxl, recommendations=recommendations)\n+\n+    trxl.close()\n+\n+    logger.info(\"Merge recommendations deleted\")\n+\n+    return recommendations\ndiff --git a/sortinghat/core/db.py b/sortinghat/core/db.py\nindex 830cfdca..4eabb073 100644\n--- a/sortinghat/core/db.py\n+++ b/sortinghat/core/db.py\n@@ -46,7 +46,8 @@\n                      Enrollment,\n                      Operation,\n                      ScheduledTask,\n-                     Alias)\n+                     Alias,\n+                     MergeRecommendation)\n from .aux import validate_field\n \n \n@@ -1328,3 +1329,26 @@ def delete_alias(trxl, alias):\n     trxl.log_operation(op_type=Operation.OpType.DELETE, entity_type='alias',\n                        timestamp=datetime_utcnow(), args=op_args,\n                        target=op_args['alias'])\n+\n+\n+def delete_merge_recommendations(trxl, recommendations):\n+    \"\"\"Remove merge recommendations from the database.\n+\n+    :param trxl: TransactionsLog object from the method calling this one\n+    :param recommendations: MergeRecommendation queryset to remove\n+\n+    :raises ValueError: raised when `recommendations` is not a MergeRecommendation queryset;\n+    \"\"\"\n+    # Setting operation arguments before they are modified\n+    op_args = {\n+        'merge_recommendations': [rec.id for rec in recommendations]\n+    }\n+\n+    if recommendations.model is not MergeRecommendation:\n+        raise ValueError(\"'recommendations' must be a MergeRecommendation queryset\")\n+\n+    recommendations.delete()\n+\n+    trxl.log_operation(op_type=Operation.OpType.DELETE, entity_type='merge_recommendation',\n+                       timestamp=datetime_utcnow(), args=op_args,\n+                       target='merge_recommendations')\ndiff --git a/sortinghat/core/schema.py b/sortinghat/core/schema.py\nindex 72b52788..d7baf1f7 100644\n--- a/sortinghat/core/schema.py\n+++ b/sortinghat/core/schema.py\n@@ -58,6 +58,7 @@\n                   delete_team,\n                   delete_domain,\n                   delete_alias,\n+                  delete_merge_recommendations,\n                   enroll,\n                   withdraw,\n                   update_enrollment,\n@@ -1374,6 +1375,22 @@ def mutate(self, info, recommendation_id, apply):\n         )\n \n \n+class DeleteMergeRecommendations(graphene.Mutation):\n+    deleted = graphene.Boolean()\n+\n+    @check_permissions(['core.delete_mergerecommendation'])\n+    def mutate(self, info):\n+        user = info.context.user\n+        tenant = get_db_tenant()\n+        ctx = SortingHatContext(user=user, tenant=tenant)\n+\n+        rec = delete_merge_recommendations(ctx)\n+\n+        return DeleteMergeRecommendations(\n+            deleted=True\n+        )\n+\n+\n class ManageAffiliationRecommendation(graphene.Mutation):\n     class Arguments:\n         recommendation_id = graphene.Int()\n@@ -2197,6 +2214,9 @@ class SortingHatMutation(graphene.ObjectType):\n     update_scheduled_task = UpdateScheduledTask.Field(\n         description='Update a periodic task.'\n     )\n+    delete_merge_recommendations = DeleteMergeRecommendations.Field(\n+        description='Remove all unapplied merge recommendations.'\n+    )\n \n     # JWT authentication\n     token_auth = graphql_jwt.ObtainJSONWebToken.Field()\ndiff --git a/ui/src/apollo/mutations.js b/ui/src/apollo/mutations.js\nindex 51a4a420..f054b1d1 100644\n--- a/ui/src/apollo/mutations.js\n+++ b/ui/src/apollo/mutations.js\n@@ -442,6 +442,14 @@ const ADD_LINKEDIN_PROFILE = gql`\n   ${FULL_INDIVIDUAL}\n `;\n \n+const DELETE_MERGE_RECOMMENDATIONS = gql`\n+  mutation deleteMergeRecommendations {\n+    deleteMergeRecommendations {\n+      deleted\n+    }\n+  }\n+`;\n+\n const tokenAuth = (apollo, username, password) => {\n   const response = apollo.mutate({\n     mutation: TOKEN_AUTH,\n@@ -854,6 +862,12 @@ const addLinkedinProfile = (apollo, uuid, username) => {\n   });\n };\n \n+const deleteMergeRecommendations = (apollo) => {\n+  return apollo.mutate({\n+    mutation: DELETE_MERGE_RECOMMENDATIONS,\n+  });\n+};\n+\n export {\n   tokenAuth,\n   lockIndividual,\n@@ -888,4 +902,5 @@ export {\n   addAlias,\n   deleteAlias,\n   addLinkedinProfile,\n+  deleteMergeRecommendations,\n };\ndiff --git a/ui/src/components/IndividualsTable.stories.js b/ui/src/components/IndividualsTable.stories.js\nindex aad7c8fe..a32d19cd 100644\n--- a/ui/src/components/IndividualsTable.stories.js\n+++ b/ui/src/components/IndividualsTable.stories.js\n@@ -412,6 +412,7 @@ export const Default = () => ({\n       };\n     },\n     manageRecommendation: () => {},\n+    deleteMergeRecommendations: () => {},\n   }),\n   data: () => ({\n     query: query,\n@@ -478,6 +479,7 @@ export const Expandable = () => ({\n       };\n     },\n     manageRecommendation: () => {},\n+    deleteMergeRecommendations: () => {},\n   }),\n   data: () => ({\n     query: query,\n@@ -535,6 +537,7 @@ export const Outlined = () => ({\n       };\n     },\n     manageRecommendation: () => {},\n+    deleteMergeRecommendations: () => {},\n   }),\n   data: () => ({\n     query: query,\n@@ -582,6 +585,7 @@ export const HiddenHeader = () => ({\n     getRecommendations: () => {},\n     getRecommendationsCount: () => {},\n     manageRecommendation: () => {},\n+    deleteMergeRecommendations: () => {},\n   }),\n   data: () => ({\n     query: query,\ndiff --git a/ui/src/components/Recommendation.stories.js b/ui/src/components/Recommendation.stories.js\nindex a8d9b834..966f8c65 100644\n--- a/ui/src/components/Recommendation.stories.js\n+++ b/ui/src/components/Recommendation.stories.js\n@@ -188,6 +188,7 @@ export const Default = () => ({\n       getRecommendationsCount: this.getRecommendations,\n       getRecommendations: this.getRecommendations,\n       manageRecommendation: this.manageRecommendation,\n+      deleteMergeRecommendations: () => {},\n     };\n   },\n });\n@@ -208,12 +209,16 @@ export const CustomModalActivator = () => ({\n       this.index = +!this.index;\n       return true;\n     },\n+    deleteRecommendations() {\n+      return true;\n+    },\n   },\n   provide() {\n     return {\n       getRecommendationsCount: this.getRecommendations,\n       getRecommendations: this.getRecommendations,\n       manageRecommendation: this.manageRecommendation,\n+      deleteMergeRecommendations: () => {},\n     };\n   },\n });\ndiff --git a/ui/src/components/Recommendations.vue b/ui/src/components/Recommendations.vue\nindex 1ce17cb7..0a057b7b 100644\n--- a/ui/src/components/Recommendations.vue\n+++ b/ui/src/components/Recommendations.vue\n@@ -1,17 +1,68 @@\n <template>\n   <v-dialog v-model=\"isOpen\" width=\"700\" persistent>\n-    <template v-slot:activator=\"{ props }\">\n-      <v-btn\n+    <template v-slot:activator=\"{ props: dialog }\">\n+      <v-btn-group\n         v-show=\"count !== 0\"\n-        v-bind=\"props\"\n         class=\"mr-4\"\n-        height=\"34\"\n+        density=\"comfortable\"\n         variant=\"outlined\"\n-        size=\"small\"\n+        divided\n       >\n-        <v-icon size=\"small\" start>mdi-lightbulb-on-outline</v-icon>\n-        {{ count }} recommendation{{ count > 1 ? \"s\" : \"\" }}\n-      </v-btn>\n+        <v-btn v-bind=\"dialog\" height=\"34\" variant=\"outlined\" size=\"small\">\n+          <v-icon size=\"small\" start>mdi-lightbulb-on-outline</v-icon>\n+          Recommendations\n+          <v-chip class=\"ml-1\" size=\"x-small\">{{ count }}</v-chip>\n+        </v-btn>\n+        <v-menu v-model=\"menu\" location=\"bottom\">\n+          <template v-slot:activator=\"{ props: menu }\">\n+            <v-btn\n+              size=\"small\"\n+              height=\"34\"\n+              icon=\"mdi-menu-down\"\n+              aria-label=\"Open menu\"\n+              v-bind=\"menu\"\n+            />\n+          </template>\n+          <v-list density=\"comfortable\" nav>\n+            <v-list-item v-bind=\"dialog\">\n+              <v-list-item-title>Review recommendations</v-list-item-title>\n+            </v-list-item>\n+            <v-dialog\n+              activator=\"parent\"\n+              max-width=\"500\"\n+              aria-label=\"Delete recommendations confirmation\"\n+            >\n+              <template v-slot:activator=\"{ props: activator }\">\n+                <v-list-item v-bind=\"activator\">\n+                  <v-list-item-title>\n+                    Delete all recommendations\n+                  </v-list-item-title>\n+                </v-list-item>\n+              </template>\n+              <template v-slot:default=\"{ isActive }\">\n+                <v-card\n+                  title=\"Delete all recommendations?\"\n+                  :text=\"errorMessage\"\n+                >\n+                  <template v-slot:actions>\n+                    <v-btn\n+                      class=\"ml-auto\"\n+                      text=\"Cancel\"\n+                      @click=\"isActive.value = false\"\n+                    ></v-btn>\n+                    <v-btn\n+                      color=\"primary\"\n+                      text=\"Delete\"\n+                      variant=\"flat\"\n+                      @click=\"deleteRecommendations(isActive)\"\n+                    ></v-btn>\n+                  </template>\n+                </v-card>\n+              </template>\n+            </v-dialog>\n+          </v-list>\n+        </v-menu>\n+      </v-btn-group>\n     </template>\n \n     <v-card v-if=\"currentItem\" class=\"section\">\n@@ -98,6 +149,7 @@ export default {\n     \"getRecommendations\",\n     \"getRecommendationsCount\",\n     \"manageRecommendation\",\n+    \"deleteMergeRecommendations\",\n   ],\n   data() {\n     return {\n@@ -105,6 +157,7 @@ export default {\n       currentItem: null,\n       isOpen: false,\n       errorMessage: null,\n+      menu: null,\n     };\n   },\n   methods: {\n@@ -163,6 +216,18 @@ export default {\n       this.$emit(\"updateTable\");\n       this.$emit(\"updateWorkspace\");\n     },\n+    async deleteRecommendations(isActive) {\n+      try {\n+        await this.deleteMergeRecommendations();\n+        this.count = 0;\n+        this.menu = false;\n+        this.errorMessage = null;\n+        isActive.value = false;\n+      } catch (error) {\n+        this.$logger.error(`Error removing recommendations: ${error}`);\n+        this.errorMessage = this.$getErrorMessage(error);\n+      }\n+    },\n   },\n   watch: {\n     isOpen(value) {\n@@ -183,4 +248,8 @@ export default {\n .col {\n   max-width: 290px;\n }\n+\n+.v-btn-group--density-comfortable.v-btn-group {\n+  height: 34px;\n+}\n </style>\ndiff --git a/ui/src/components/WorkSpace.stories.js b/ui/src/components/WorkSpace.stories.js\nindex d7d9512a..1f4c9f07 100644\n--- a/ui/src/components/WorkSpace.stories.js\n+++ b/ui/src/components/WorkSpace.stories.js\n@@ -274,6 +274,7 @@ export const DragAndDrop = () => ({\n       };\n     },\n     manageRecommendation: () => {},\n+    deleteMergeRecommendations: () => {},\n   }),\n   data: () => ({\n     individuals: [],\ndiff --git a/ui/src/views/Dashboard.vue b/ui/src/views/Dashboard.vue\nindex efb6dfa5..56865559 100644\n--- a/ui/src/views/Dashboard.vue\n+++ b/ui/src/views/Dashboard.vue\n@@ -114,6 +114,7 @@ import {\n   mergeOrganizations,\n   addAlias,\n   deleteAlias,\n+  deleteMergeRecommendations,\n } from \"../apollo/mutations\";\n import IndividualsTable from \"../components/IndividualsTable\";\n import OrganizationsTable from \"../components/OrganizationsTable\";\n@@ -369,6 +370,10 @@ export default {\n       const response = await deleteAlias(this.$apollo, alias);\n       return response;\n     },\n+    async deleteRecommendations() {\n+      const response = await deleteMergeRecommendations(this.$apollo);\n+      return response;\n+    },\n   },\n   async mounted() {\n     if (this.workspace && this.workspace.length > 0) {\n@@ -386,6 +391,7 @@ export default {\n       getRecommendations: this.getRecommendations,\n       getRecommendationsCount: this.getRecommendationsCount,\n       manageRecommendation: this.manageRecommendation,\n+      deleteMergeRecommendations: this.deleteRecommendations,\n     };\n   },\n };\n", "instance_id": "chaoss__grimoirelab-sortinghat-910", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal: to add a feature (a button or API option) to remove all unapplied merge recommendations due to the risk of overwhelming manual processing when Unify jobs are scheduled. It identifies the current limitation (no UI or API option to remove recommendations, requiring direct database access) and suggests a basic solution (a button next to the recommendations count or an API option). However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes \"unapplied recommendations\" in technical terms (though the code changes clarify this as recommendations with `applied__isnull=True`). Additionally, there are no explicit mentions of edge cases, error handling requirements, or constraints such as performance expectations or user permissions. While the intent is understandable, these missing details prevent it from being fully comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.50, placing this problem in the medium range. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes span multiple files and layers of the application, including backend database operations (`db.py`), API logic (`api.py`), GraphQL schema updates (`schema.py`), and frontend UI modifications (`Recommendations.vue` and related files). This requires understanding and modifying several parts of the codebase, from database queries to user interface interactions. However, the changes are relatively straightforward within each file, focusing on adding a new feature rather than refactoring existing logic or altering the system's architecture significantly. The overall amount of code change is moderate, with new functions and UI components added but no deep structural impact.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including Django ORM for database operations (filtering and deleting records), GraphQL mutations for API exposure, transaction logging for audit purposes, and Vue.js for frontend development (adding a button group and dialog for user confirmation). Additionally, understanding permission checks (`check_permissions` decorator) and context management (`atomic_using_tenant`) is necessary. While these concepts are not overly complex individually, their combination across backend and frontend increases the cognitive load to a moderate level.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce basic error handling (e.g., catching errors in the frontend during deletion and displaying an error message). Potential edge cases include scenarios like concurrent modifications to recommendations, handling large numbers of recommendations (performance concerns), or ensuring proper user authorization before deletion. The code addresses some of these implicitly (e.g., permission checks), but there\u2019s no explicit handling of performance or concurrency issues, which might need consideration depending on the scale of data. The complexity of edge cases is moderate, as they are not fully explored in the problem or code.\n\n4. **Overall Assessment**: This problem requires a solid understanding of multiple parts of the codebase and the integration of backend and frontend changes. It is not a simple bug fix or trivial feature addition (which would score lower), nor does it involve deep architectural changes, complex algorithms, or advanced domain-specific knowledge (which would score higher). The need to coordinate changes across several files and handle basic error conditions places it in the medium difficulty range. A score of 0.50 reflects the balance between the moderate scope of changes and the manageable complexity of the required technical concepts.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] `segmented_reduce` does not accept empty offsets\nWhen the input is all empty, i.e., we have empty input values and empty offsets, `segmented_reduce` throws an exception:\n```\nC++ exception with description \"CUDF failure at: ../../src/reductions/segmented/reductions.cpp:163: `offsets` should have at least 1 element.\"\n```\n\nThis is wrong, as it should be able to handle the case of empty input.\n", "patch": "diff --git a/cpp/src/reductions/segmented/reductions.cpp b/cpp/src/reductions/segmented/reductions.cpp\nindex c4f6c135dde..4843d95346f 100644\n--- a/cpp/src/reductions/segmented/reductions.cpp\n+++ b/cpp/src/reductions/segmented/reductions.cpp\n@@ -13,6 +13,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+#include <cudf/column/column_factories.hpp>\n #include <cudf/detail/aggregation/aggregation.hpp>\n #include <cudf/detail/nvtx/ranges.hpp>\n #include <cudf/reduction.hpp>\n@@ -118,6 +119,11 @@ std::unique_ptr<column> segmented_reduce(column_view const& segmented_values,\n     CUDF_FAIL(\n       \"Initial value is only supported for SUM, PRODUCT, MIN, MAX, ANY, and ALL aggregation types\");\n   }\n+\n+  if (segmented_values.is_empty() && offsets.empty()) {\n+    return cudf::make_empty_column(output_dtype);\n+  }\n+\n   CUDF_EXPECTS(offsets.size() > 0, \"`offsets` should have at least 1 element.\");\n \n   return cudf::detail::aggregation_dispatcher(\n", "instance_id": "rapidsai__cudf-17437", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `segmented_reduce` function fails to handle empty inputs (both empty values and empty offsets) and throws an exception, which is undesirable behavior. The goal is implicitly understood as fixing this bug to allow the function to handle empty inputs gracefully. However, the statement lacks explicit details about the expected behavior or output in the case of empty inputs (e.g., should it return an empty column of a specific type?). Additionally, there are no examples beyond the error message, and edge cases or constraints are not explicitly mentioned. While the intent is clear, these minor ambiguities prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal, confined to a single file and a small modification in the `segmented_reduce` function. The change involves adding a simple early-return condition to handle the empty input case by returning an empty column, which is a straightforward fix. Second, the technical concepts required are basic: familiarity with C++ (specifically exception handling and conditional logic) and a minimal understanding of the CUDF library's column factory functions. Third, the edge case (empty inputs) is explicitly mentioned and directly addressed by the code change, with no additional complex error handling or unforeseen edge cases apparent from the problem or diff. Finally, there is no significant impact on the broader codebase or architecture, as the change is localized and does not alter the core logic of the function. Overall, this is a simple bug fix requiring basic code modification and minimal deep understanding of the surrounding system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove memory resource parameter from `cudf::io::write_csv` and `cudf::io::write_json()` APIs\nNeither  `cudf::io::write_csv` nor `cudf::io::write_json()` return any data to the caller so the passed in memory resource should never be used. https://github.com/rapidsai/cudf/blob/branch-24.08/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md#temporary-memory\r\n\r\nAlso, both the `cudf::io::write_orc()` and `cudf::io::write_parquet()` do not have memory resource parameters.\r\nhttps://docs.rapids.ai/api/libcudf/stable/group__io__writers\n", "patch": "diff --git a/cpp/include/cudf/io/csv.hpp b/cpp/include/cudf/io/csv.hpp\nindex 68bb7fba00e..cc361f0918e 100644\n--- a/cpp/include/cudf/io/csv.hpp\n+++ b/cpp/include/cudf/io/csv.hpp\n@@ -1756,11 +1756,9 @@ class csv_writer_options_builder {\n  *\n  * @param options Settings for controlling writing behavior\n  * @param stream CUDA stream used for device memory operations and kernel launches\n- * @param mr Device memory resource to use for device memory allocation\n  */\n void write_csv(csv_writer_options const& options,\n-               rmm::cuda_stream_view stream      = cudf::get_default_stream(),\n-               rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n+               rmm::cuda_stream_view stream = cudf::get_default_stream());\n \n /** @} */  // end of group\n }  // namespace io\ndiff --git a/cpp/include/cudf/io/detail/csv.hpp b/cpp/include/cudf/io/detail/csv.hpp\nindex 50c1a7c163d..2a70fa888f4 100644\n--- a/cpp/include/cudf/io/detail/csv.hpp\n+++ b/cpp/include/cudf/io/detail/csv.hpp\n@@ -49,14 +49,12 @@ table_with_metadata read_csv(std::unique_ptr<cudf::io::datasource>&& source,\n  * @param column_names Column names for the output CSV\n  * @param options Settings for controlling behavior\n  * @param stream CUDA stream used for device memory operations and kernel launches.\n- * @param mr Device memory resource to use for device memory allocation\n  */\n void write_csv(data_sink* sink,\n                table_view const& table,\n                host_span<std::string const> column_names,\n                csv_writer_options const& options,\n-               rmm::cuda_stream_view stream,\n-               rmm::device_async_resource_ref mr);\n+               rmm::cuda_stream_view stream);\n \n }  // namespace csv\n }  // namespace detail\ndiff --git a/cpp/include/cudf/io/detail/json.hpp b/cpp/include/cudf/io/detail/json.hpp\nindex 540a584908d..6ff1c12831b 100644\n--- a/cpp/include/cudf/io/detail/json.hpp\n+++ b/cpp/include/cudf/io/detail/json.hpp\n@@ -46,13 +46,11 @@ table_with_metadata read_json(host_span<std::unique_ptr<datasource>> sources,\n  * @param table The set of columns\n  * @param options Settings for controlling behavior\n  * @param stream CUDA stream used for device memory operations and kernel launches.\n- * @param mr Device memory resource to use for device memory allocation\n  */\n void write_json(data_sink* sink,\n                 table_view const& table,\n                 json_writer_options const& options,\n-                rmm::cuda_stream_view stream,\n-                rmm::device_async_resource_ref mr);\n+                rmm::cuda_stream_view stream);\n \n /**\n  * @brief Normalize single quotes to double quotes using FST\ndiff --git a/cpp/include/cudf/io/json.hpp b/cpp/include/cudf/io/json.hpp\nindex 8de690482f9..7af90766ad0 100644\n--- a/cpp/include/cudf/io/json.hpp\n+++ b/cpp/include/cudf/io/json.hpp\n@@ -1018,11 +1018,9 @@ class json_writer_options_builder {\n  *\n  * @param options Settings for controlling writing behavior\n  * @param stream CUDA stream used for device memory operations and kernel launches\n- * @param mr Device memory resource to use for device memory allocation\n  */\n void write_json(json_writer_options const& options,\n-                rmm::cuda_stream_view stream      = cudf::get_default_stream(),\n-                rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n+                rmm::cuda_stream_view stream = cudf::get_default_stream());\n \n /** @} */  // end of group\n }  // namespace io\ndiff --git a/cpp/src/io/csv/writer_impl.cu b/cpp/src/io/csv/writer_impl.cu\nindex 63eb0b03c5f..00a6dcb2286 100644\n--- a/cpp/src/io/csv/writer_impl.cu\n+++ b/cpp/src/io/csv/writer_impl.cu\n@@ -430,13 +430,13 @@ void write_csv(data_sink* out_sink,\n                table_view const& table,\n                host_span<std::string const> user_column_names,\n                csv_writer_options const& options,\n-               rmm::cuda_stream_view stream,\n-               rmm::device_async_resource_ref mr)\n+               rmm::cuda_stream_view stream)\n {\n   // write header: column names separated by delimiter:\n   // (even for tables with no rows)\n   //\n-  write_chunked_begin(out_sink, table, user_column_names, options, stream, mr);\n+  write_chunked_begin(\n+    out_sink, table, user_column_names, options, stream, rmm::mr::get_current_device_resource());\n \n   if (table.num_rows() > 0) {\n     // no need to check same-size columns constraint; auto-enforced by table_view\ndiff --git a/cpp/src/io/functions.cpp b/cpp/src/io/functions.cpp\nindex 5daa55d4552..d1811267942 100644\n--- a/cpp/src/io/functions.cpp\n+++ b/cpp/src/io/functions.cpp\n@@ -215,9 +215,7 @@ table_with_metadata read_json(json_reader_options options,\n   return json::detail::read_json(datasources, options, stream, mr);\n }\n \n-void write_json(json_writer_options const& options,\n-                rmm::cuda_stream_view stream,\n-                rmm::device_async_resource_ref mr)\n+void write_json(json_writer_options const& options, rmm::cuda_stream_view stream)\n {\n   auto sinks = make_datasinks(options.get_sink());\n   CUDF_EXPECTS(sinks.size() == 1, \"Multiple sinks not supported for JSON writing\");\n@@ -226,8 +224,7 @@ void write_json(json_writer_options const& options,\n     sinks[0].get(),\n     options.get_table(),\n     options,\n-    stream,\n-    mr);\n+    stream);\n }\n \n table_with_metadata read_csv(csv_reader_options options,\n@@ -252,9 +249,7 @@ table_with_metadata read_csv(csv_reader_options options,\n }\n \n // Freeform API wraps the detail writer class API\n-void write_csv(csv_writer_options const& options,\n-               rmm::cuda_stream_view stream,\n-               rmm::device_async_resource_ref mr)\n+void write_csv(csv_writer_options const& options, rmm::cuda_stream_view stream)\n {\n   using namespace cudf::io::detail;\n \n@@ -266,8 +261,7 @@ void write_csv(csv_writer_options const& options,\n     options.get_table(),\n     options.get_names(),\n     options,\n-    stream,\n-    mr);\n+    stream);\n }\n \n raw_orc_statistics read_raw_orc_statistics(source_info const& src_info,\ndiff --git a/cpp/src/io/json/write_json.cu b/cpp/src/io/json/write_json.cu\nindex 997d6fd99f8..c688c809e04 100644\n--- a/cpp/src/io/json/write_json.cu\n+++ b/cpp/src/io/json/write_json.cu\n@@ -805,8 +805,7 @@ void write_chunked(data_sink* out_sink,\n                    strings_column_view const& str_column_view,\n                    int const skip_last_chars,\n                    json_writer_options const& options,\n-                   rmm::cuda_stream_view stream,\n-                   rmm::device_async_resource_ref mr)\n+                   rmm::cuda_stream_view stream)\n {\n   CUDF_FUNC_RANGE();\n   CUDF_EXPECTS(str_column_view.size() > 0, \"Unexpected empty strings column.\");\n@@ -829,8 +828,7 @@ void write_chunked(data_sink* out_sink,\n void write_json(data_sink* out_sink,\n                 table_view const& table,\n                 json_writer_options const& options,\n-                rmm::cuda_stream_view stream,\n-                rmm::device_async_resource_ref mr)\n+                rmm::cuda_stream_view stream)\n {\n   CUDF_FUNC_RANGE();\n   std::vector<column_name_info> user_column_names = [&]() {\n@@ -912,7 +910,7 @@ void write_json(data_sink* out_sink,\n       bool const include_line_terminator =\n         (&sub_view != &vector_views.back()) or options.is_enabled_lines();\n       auto const skip_last_chars = (include_line_terminator ? 0 : line_terminator.size());\n-      write_chunked(out_sink, str_concat_col->view(), skip_last_chars, options, stream, mr);\n+      write_chunked(out_sink, str_concat_col->view(), skip_last_chars, options, stream);\n     }\n   } else {\n     if (options.is_enabled_lines()) {\n", "instance_id": "rapidsai__cudf-16231", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to remove the memory resource parameter from the `cudf::io::write_csv` and `cudf::io::write_json` APIs, as it is not used and aligns with the design of similar APIs like `write_orc` and `write_parquet`. It provides references to relevant documentation and developer guides, which help in understanding the context. However, there are minor ambiguities: the statement does not explicitly discuss potential side effects or dependencies of removing the memory resource parameter, nor does it mention any specific edge cases or testing requirements to ensure the change does not break existing functionality. Additionally, it lacks detailed examples of how the API is used before and after the change. Overall, while the goal is clear, some minor details are missing that could affect implementation or validation of the solution.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting a small number of files (primarily headers and implementation files for CSV and JSON writers in the `cudf` library). The modifications involve removing a parameter (`mr` for memory resource) from function signatures and updating the corresponding call sites. The changes span multiple files (7 files as per the diff), but they are repetitive and straightforward, with no significant impact on the system's architecture. The amount of code change is minimal, mostly involving deletion of parameter declarations and updating function calls.\n\n2. **Number of Technical Concepts:** The problem requires a basic understanding of C++ function signatures, CUDA streams (`rmm::cuda_stream_view`), and the `rmm` (RAPIDS Memory Manager) library's memory resource handling. However, the concepts are not particularly complex for someone familiar with CUDA or GPU programming. No advanced algorithms, design patterns, or domain-specific knowledge beyond the `cudf` library's IO module are needed. The change is more about API consistency than deep technical innovation.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error handling requirements related to removing the memory resource parameter. The code changes also do not introduce new error handling logic; they simply remove an unused parameter. However, there is a minor implicit concern: ensuring that removing the memory resource does not affect downstream users or internal implementations that might rely on custom memory resources (though the problem states it is unused). This risk is mitigated by the fact that a default memory resource is still provided in some internal calls (e.g., `rmm::mr::get_current_device_resource()`), as seen in the diffs.\n\n4. **Overall Complexity:** The task requires understanding the purpose of the memory resource parameter and ensuring its removal does not break functionality, but it does not involve complex logic, extensive refactoring, or deep architectural changes. It is a straightforward API update that can be accomplished with moderate familiarity with the codebase.\n\nGiven these factors, a difficulty score of 0.30 reflects the simplicity of the task while acknowledging the need for some understanding of the `cudf` library's IO module and CUDA-related concepts. It is not a trivial typo fix (which would be closer to 0.0-0.2), but it is far from requiring deep architectural changes or advanced technical knowledge (which would push it toward 0.6 or higher).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Netlist device combination / simplify requests\nThis is a takeaway of a number of discussions regarding extraction of C elements.\r\n\r\n1. Netlist#simplify could eliminate shorted elements such as R, L or C\r\n\r\nExample:\r\n\r\n```\r\nnetlist = pya.Netlist()\r\n\r\ncircuit = pya.Circuit()\r\ncircuit.name = \"TOP\"\r\n\r\nnetlist.add(circuit)\r\n\r\ncap_device = pya.DeviceClassCapacitor()\r\ncap_device.name = \"model_name\"\r\n\r\nnetlist.add(cap_device)\r\n\r\nn1 = circuit.create_net(\"n1\")\r\nn2 = circuit.create_net(\"n2\")\r\n\r\nc1 = circuit.create_device(cap_device, \"c1\")\r\nc1.set_parameter(\"C\", 1e-15)\r\n\r\nc2 = circuit.create_device(cap_device, \"c2\")\r\nc2.set_parameter(\"C\", 2e-15)\r\n\r\n# shorted cap\r\nc1.connect_terminal(\"A\", n1)\r\nc1.connect_terminal(\"B\", n1)\r\n\r\n# non-shorted one\r\nc2.connect_terminal(\"A\", n1)\r\nc2.connect_terminal(\"B\", n2)\r\n\r\nnetlist.simplify()\r\n\r\nprint(netlist)\r\n```\r\n\r\nCurrent output:\r\n\r\n```\r\ncircuit TOP (n1=n1,n2=n2);\r\n  device model_name c1 (A=n1,B=n1) (C=1e-15,A=0,P=0);\r\n  device model_name c2 (A=n1,B=n2) (C=2e-15,A=0,P=0);\r\nend;\r\n```\r\n\r\nProposed output:\r\n\r\n\r\n```\r\ncircuit TOP (n1=n1,n2=n2);\r\n  device model_name c2 (A=n1,B=n2) (C=2e-15,A=0,P=0);\r\nend;\r\n```\r\n\r\n2. Netlist#combine_devices does not work properly on small (typical cap values):\r\n\r\nExample:\r\n\r\n```\r\n\r\nnetlist = pya.Netlist()\r\n\r\ncircuit = pya.Circuit()\r\ncircuit.name = \"TOP\"\r\n\r\nnetlist.add(circuit)\r\n\r\ncap_device = pya.DeviceClassCapacitor()\r\ncap_device.name = \"model_name\"\r\n\r\nnetlist.add(cap_device)\r\n\r\nn1 = circuit.create_net(\"n1\")\r\nn2 = circuit.create_net(\"n2\")\r\nn3 = circuit.create_net(\"n3\")\r\n\r\np1 = circuit.create_pin(\"p1\")\r\np2 = circuit.create_pin(\"p2\")\r\n\r\ncircuit.connect_pin(p1, n1)\r\ncircuit.connect_pin(p2, n3)\r\n\r\nc1 = circuit.create_device(cap_device, \"c1\")\r\nc1.set_parameter(\"C\", 1e-15)\r\n\r\nc2 = circuit.create_device(cap_device, \"c2\")\r\nc2.set_parameter(\"C\", 2e-15)\r\n\r\nc3 = circuit.create_device(cap_device, \"c3\")\r\nc3.set_parameter(\"C\", 3e-15)\r\n\r\n# C1 and C2 are series caps\r\nc1.connect_terminal(\"A\", n1)\r\nc1.connect_terminal(\"B\", n2)\r\n\r\nc2.connect_terminal(\"A\", n2)\r\nc2.connect_terminal(\"B\", n3)\r\n\r\n# C3 connects across\r\nc3.connect_terminal(\"A\", n1)\r\nc3.connect_terminal(\"B\", n3)\r\n\r\nprint(netlist)\r\n\r\nnetlist.combine_devices()\r\n\r\nprint(netlist)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ncircuit TOP (p1=n1,p2=n3);\r\n  device model_name c1 (A=n1,B=n2) (C=1e-15,A=0,P=0);\r\n  device model_name c2 (A=n2,B=n3) (C=2e-15,A=0,P=0);\r\n  device model_name c3 (A=n1,B=n3) (C=3e-15,A=0,P=0);\r\nend;\r\n\r\ncircuit TOP (p1=n1,p2=n3);\r\n  device model_name c1 (A=n1,B=n3) (C=3e-15,A=0,P=0);\r\nend;\r\n```\r\n\r\nExpected output for combined-device netlist\r\n\r\n```\r\ncircuit TOP (p1=n1,p2=n3);\r\n  device model_name c1 (A=n1,B=n3) (C=3.66666666667e-15,A=0,P=0);\r\nend;\r\n```\n", "patch": "diff --git a/src/db/db/dbCircuit.cc b/src/db/db/dbCircuit.cc\nindex c9c4d2153..49d3e91d5 100644\n--- a/src/db/db/dbCircuit.cc\n+++ b/src/db/db/dbCircuit.cc\n@@ -825,6 +825,41 @@ void Circuit::do_purge_nets (bool keep_pins)\n   }\n }\n \n+static bool can_purge_device (const db::Device &device)\n+{\n+  if (! device.device_class ()) {\n+    return false;\n+  }\n+\n+  const std::vector<db::DeviceTerminalDefinition> &tdefs = device.device_class ()->terminal_definitions ();\n+  if (tdefs.size () <= 1) {\n+    return false;\n+  }\n+\n+  const db::Net *net = device.net_for_terminal (tdefs.front ().id ());\n+  for (auto t = tdefs.begin () + 1; t != tdefs.end (); ++t) {\n+    if (net != device.net_for_terminal (t->id ())) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+void Circuit::purge_devices ()\n+{\n+  std::vector<db::Device *> devices_to_be_purged;\n+  for (device_iterator d = begin_devices (); d != end_devices (); ++d) {\n+    if (can_purge_device (*d)) {\n+      devices_to_be_purged.push_back (d.operator-> ());\n+    }\n+  }\n+\n+  for (auto d = devices_to_be_purged.begin (); d != devices_to_be_purged.end (); ++d) {\n+    remove_device (*d);\n+  }\n+}\n+\n /**\n  *  @brief Sanity check for device to be removed\n  */\ndiff --git a/src/db/db/dbCircuit.h b/src/db/db/dbCircuit.h\nindex abcb74274..9ebc5f83d 100644\n--- a/src/db/db/dbCircuit.h\n+++ b/src/db/db/dbCircuit.h\n@@ -732,6 +732,14 @@ class DB_PUBLIC Circuit\n    */\n   void purge_nets_keep_pins ();\n \n+  /**\n+   *  @brief Purges invalid devices\n+   *\n+   *  This method will purge all invalid devices, i.e. those\n+   *  whose terminals are all connected to the same net.\n+   */\n+  void purge_devices ();\n+\n   /**\n    *  @brief Combine devices\n    *\ndiff --git a/src/db/db/dbNetlist.cc b/src/db/db/dbNetlist.cc\nindex e5e801863..3c038eb77 100644\n--- a/src/db/db/dbNetlist.cc\n+++ b/src/db/db/dbNetlist.cc\n@@ -619,6 +619,13 @@ void Netlist::purge_nets ()\n   }\n }\n \n+void Netlist::purge_devices ()\n+{\n+  for (bottom_up_circuit_iterator c = begin_bottom_up (); c != end_bottom_up (); ++c) {\n+    c->purge_devices ();\n+  }\n+}\n+\n void Netlist::make_top_level_pins ()\n {\n   size_t ntop = top_circuit_count ();\n@@ -684,6 +691,8 @@ void Netlist::simplify ()\n {\n   make_top_level_pins ();\n   purge ();\n+\n+  //  combine devices are purge nets that are created in that step\n   combine_devices ();\n   purge_nets ();\n }\ndiff --git a/src/db/db/dbNetlist.h b/src/db/db/dbNetlist.h\nindex 0bff649e1..d21115838 100644\n--- a/src/db/db/dbNetlist.h\n+++ b/src/db/db/dbNetlist.h\n@@ -508,6 +508,14 @@ class DB_PUBLIC Netlist\n    */\n   void purge_nets ();\n \n+  /**\n+   *  @brief Purges invalid devices\n+   *\n+   *  This method will purge all invalid devices, i.e. those\n+   *  whose terminals are all connected to the same net.\n+   */\n+  void purge_devices ();\n+\n   /**\n    *  @brief Creates pins for top-level circuits\n    *\ndiff --git a/src/db/db/dbNetlistDeviceClasses.cc b/src/db/db/dbNetlistDeviceClasses.cc\nindex 2a1fa9573..5252557d0 100644\n--- a/src/db/db/dbNetlistDeviceClasses.cc\n+++ b/src/db/db/dbNetlistDeviceClasses.cc\n@@ -116,7 +116,7 @@ class ResistorDeviceCombiner\n   {\n     double va = a->parameter_value (0);\n     double vb = b->parameter_value (0);\n-    a->set_parameter_value (0, va + vb < 1e-10 ? 0.0 : va * vb / (va + vb));\n+    a->set_parameter_value (0, va + vb < 1e-30 ? 0.0 : va * vb / (va + vb));\n \n     //  parallel width is sum of both, length is the one that gives the same value of resistance\n     //    R = 1/(1/R1 + 1/R2)\n@@ -204,7 +204,7 @@ class CapacitorDeviceCombiner\n   {\n     double va = a->parameter_value (0);\n     double vb = b->parameter_value (0);\n-    a->set_parameter_value (0, va + vb < 1e-10 ? 0.0 : va * vb / (va + vb));\n+    a->set_parameter_value (0, va + vb < 1e-30 ? 0.0 : va * vb / (va + vb));\n \n     //  TODO: does this implementation make sense?\n     double aa = a->parameter_value (1);\n@@ -259,7 +259,7 @@ class InductorDeviceCombiner\n   {\n     double va = a->parameter_value (0);\n     double vb = b->parameter_value (0);\n-    a->set_parameter_value (0, va + vb < 1e-10 ? 0.0 : va * vb / (va + vb));\n+    a->set_parameter_value (0, va + vb < 1e-30 ? 0.0 : va * vb / (va + vb));\n   }\n \n   void serial (Device *a, Device *b) const\ndiff --git a/src/db/db/gsiDeclDbNetlist.cc b/src/db/db/gsiDeclDbNetlist.cc\nindex 1573631dc..2687fec45 100644\n--- a/src/db/db/gsiDeclDbNetlist.cc\n+++ b/src/db/db/gsiDeclDbNetlist.cc\n@@ -1809,6 +1809,12 @@ Class<db::Circuit> decl_dbCircuit (decl_dbNetlistObject, \"db\", \"Circuit\",\n     \"For example, serial or parallel resistors can be combined into \"\n     \"a single resistor.\\n\"\n   ) +\n+  gsi::method (\"purge_devices\", &db::Circuit::purge_devices,\n+    \"@brief Purges invalid devices.\\n\"\n+    \"Purges devices which are considered invalid. Such devices are for example those whose terminals are all connected to a single net.\\n\"\n+    \"\\n\"\n+    \"This method has been added in version 0.29.7.\"\n+  ) +\n   gsi::method (\"purge_nets\", &db::Circuit::purge_nets,\n     \"@brief Purges floating nets.\\n\"\n     \"Floating nets are nets with no device or subcircuit attached to. Such floating \"\n@@ -2176,6 +2182,12 @@ Class<db::Netlist> decl_dbNetlist (\"db\", \"Netlist\",\n     \"Floating nets can be created as effect of reconnections of devices or pins. \"\n     \"This method will eliminate all nets that make less than two connections.\"\n   ) +\n+  gsi::method (\"purge_devices\", &db::Netlist::purge_devices,\n+    \"@brief Purges invalid devices.\\n\"\n+    \"Purges devices which are considered invalid. Such devices are for example those whose terminals are all connected to a single net.\\n\"\n+    \"\\n\"\n+    \"This method has been added in version 0.29.7.\"\n+  ) +\n   gsi::method (\"simplify\", &db::Netlist::simplify,\n     \"@brief Convenience method that combines the simplification.\\n\"\n     \"This method is a convenience method that runs \\\\make_top_level_pins, \\\\purge, \\\\combine_devices and \\\\purge_nets.\"\n", "instance_id": "KLayout__klayout-1851", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear, providing specific goals for the `Netlist#simplify` and `Netlist#combine_devices` methods with detailed examples of current and expected outputs. The intent to eliminate shorted elements (like capacitors connected to the same net) and to correctly combine device parameters (like capacitance values) is well-articulated through code snippets. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes an \"invalid\" device beyond the example of shorted elements, nor does it address potential edge cases such as handling devices with zero or negative parameter values, or how to handle numerical precision issues with very small values (though the code changes hint at a threshold adjustment). Additionally, the statement lacks clarity on whether there are broader implications or constraints for combining devices beyond capacitors (e.g., resistors or inductors), even though the code changes suggest a generic approach. Overall, while the examples and intent are clear, these minor gaps prevent a perfect score.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`dbCircuit.cc`, `dbNetlist.cc`, `dbNetlistDeviceClasses.cc`, etc.), requiring modifications to core functionality like device purging and parameter combination logic. This indicates a need to understand interactions between different parts of the codebase, such as how circuits, nets, and devices are managed. Second, the technical concepts involved include object-oriented design in C++ (e.g., working with device classes and terminal definitions), numerical computations (e.g., adjusting thresholds for small values from 1e-10 to 1e-30), and domain-specific knowledge of netlist simplification in electronic design automation (EDA), which adds a layer of complexity. Third, the changes impact critical functionality like `simplify()` and `combine_devices()`, suggesting potential architectural considerations, though not a full system overhaul. Finally, while edge cases like numerical precision for small capacitance values are partially addressed in the code changes, the problem statement does not fully explore other potential issues (e.g., handling invalid or malformed netlists), which might require additional error handling. Overall, this problem requires a moderate level of expertise in C++ and domain knowledge, along with careful implementation across multiple modules, justifying a score of 0.55, slightly above the medium threshold due to the domain-specific complexity and cross-file modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] Deprecations warnings should be errors\n**Describe the bug**\nRMM test and benchmark CMakeLists.txt disable warnings as errors for deprecated declarations by specifying `-Wno-error=deprecated-declarations. Currently we have direct usage of deprecated `rmm::logger()` in the replay benchmark `replay.cpp` which results in warnings, but no errors. \n\nSee also rapidsai/build-planning#26\n\n**Steps/Code to reproduce bug**\nBuild with `-DBUILD_BENCHMARKS=ON`\n\n**Expected behavior**\nAll warnings should be treated as errors and there should be no warnings.\n\n**Environment details (please complete the following information):**\n - Environment location: [Docker]\n - Method of RMM install: [from source]\n   - If method of install is [Docker]: RAPIDS combined dev container\n\nRemove all explicit usage of fmtlib\n## Description\r\nFixes #1717\r\nAlso fixes #1710 in 5330063\r\n\r\nI have replaced fmt-style format string placeholders (`\"... {} ...\"`) with printf-style placeholders by adding a function `rmm::detail::formatted_log()`, which I modified from @vyasr 's #1722.\r\n\r\n~The only remaining mention of fmt is in CMakeLists.txt. Do we still need to explicitly fetch fmt?~\r\nRemoved.\r\n\r\n## Checklist\r\n- [X] I am familiar with the [Contributing Guidelines](https://github.com/rapidsai/rmm/blob/HEAD/CONTRIBUTING.md).\r\n- [X] New or existing tests cover these changes.\r\n- [X] The documentation is up to date with these changes.\r\n\n", "patch": "diff --git a/benchmarks/CMakeLists.txt b/benchmarks/CMakeLists.txt\nindex 9dfb2c538..0487a2dfa 100644\n--- a/benchmarks/CMakeLists.txt\n+++ b/benchmarks/CMakeLists.txt\n@@ -1,5 +1,5 @@\n # =============================================================================\n-# Copyright (c) 2018-2020, NVIDIA CORPORATION.\n+# Copyright (c) 2018-2024, NVIDIA CORPORATION.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\n # in compliance with the License. You may obtain a copy of the License at\n@@ -42,9 +42,8 @@ function(ConfigureBench BENCH_NAME)\n     target_compile_definitions(${BENCH_NAME} PUBLIC CUDA_API_PER_THREAD_DEFAULT_STREAM)\n   endif()\n \n-  target_compile_options(\n-    ${BENCH_NAME} PUBLIC $<$<COMPILE_LANG_AND_ID:CXX,GNU,Clang>:-Wall -Werror\n-                         -Wno-error=deprecated-declarations -Wno-unknown-pragmas>)\n+  target_compile_options(${BENCH_NAME} PUBLIC $<$<COMPILE_LANG_AND_ID:CXX,GNU,Clang>:-Wall -Werror\n+                                              -Wno-unknown-pragmas>)\n   if(DISABLE_DEPRECATION_WARNING)\n     target_compile_options(\n       ${BENCH_NAME} PUBLIC $<$<COMPILE_LANGUAGE:CUDA>:-Xcompiler=-Wno-deprecated-declarations>)\ndiff --git a/benchmarks/replay/replay.cpp b/benchmarks/replay/replay.cpp\nindex 5afed036a..d80841321 100644\n--- a/benchmarks/replay/replay.cpp\n+++ b/benchmarks/replay/replay.cpp\n@@ -172,7 +172,7 @@ struct replay_benchmark {\n   void SetUp(const ::benchmark::State& state)\n   {\n     if (state.thread_index() == 0) {\n-      rmm::logger().log(spdlog::level::info, \"------ Start of Benchmark -----\");\n+      RMM_LOG_INFO(\"------ Start of Benchmark -----\");\n       mr_ = factory_(simulated_size_);\n     }\n   }\n@@ -181,7 +181,7 @@ struct replay_benchmark {\n   void TearDown(const ::benchmark::State& state)\n   {\n     if (state.thread_index() == 0) {\n-      rmm::logger().log(spdlog::level::info, \"------ End of Benchmark -----\");\n+      RMM_LOG_INFO(\"------ End of Benchmark -----\");\n       // clean up any leaked allocations\n       std::size_t total_leaked{0};\n       std::size_t num_leaked{0};\n@@ -402,7 +402,7 @@ int main(int argc, char** argv)\n     auto const num_threads = per_thread_events.size();\n \n     // Uncomment to enable / change default log level\n-    // rmm::logger().set_level(spdlog::level::trace);\n+    // rmm::detail::logger().set_level(spdlog::level::trace);\n \n     if (args.count(\"resource\") > 0) {\n       std::string mr_name = args[\"resource\"].as<std::string>();\n", "instance_id": "rapidsai__rmm-1728", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: deprecated declaration warnings in the RMM library are not being treated as errors, which is inconsistent with the expected behavior of treating all warnings as errors. The goal (removing deprecated usage and ensuring warnings are errors) and steps to reproduce the bug (building with a specific flag) are provided. Additionally, the environment details and related issues are mentioned, which adds context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly clarify the full scope of expected changes beyond the provided code diff (e.g., whether other files or deprecated usages need to be addressed). It also lacks specific examples of expected output or detailed constraints. Furthermore, the mention of removing fmtlib usage feels slightly disconnected or underexplained in the context of the primary bug about deprecation warnings. Overall, while the intent is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively localized, primarily affecting two files: `CMakeLists.txt` (removing the flag to suppress deprecation warnings as errors) and `replay.cpp` (replacing deprecated logging calls with updated macros or functions). The changes do not appear to impact the broader system architecture or require modifications across multiple modules beyond these files. The amount of code change is small, focusing on configuration and simple replacements.\n\n2. **Number of Technical Concepts**: Solving this problem requires a basic understanding of CMake configuration (specifically, compiler flags like `-Wno-error=deprecated-declarations`), familiarity with the RMM library's logging mechanisms, and potentially some knowledge of deprecation handling in C++. These concepts are not particularly complex for a developer with moderate experience in C++ and build systems. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases or error conditions to handle beyond ensuring that warnings are treated as errors. The code changes do not introduce new error handling logic or modify existing error paths in a significant way. The focus is on configuration and API replacement, which minimizes the complexity of edge case handling.\n\n4. **Overall Complexity**: The task involves straightforward modifications\u2014removing a compiler flag and updating deprecated API usage with provided alternatives (e.g., replacing `rmm::logger()` with `RMM_LOG_INFO`). While it requires some understanding of the codebase's logging system and build configuration, it does not demand deep architectural knowledge or complex refactoring. The impact is limited to ensuring build-time correctness rather than runtime behavior or performance.\n\nGiven these factors, a difficulty score of 0.30 reflects an \"Easy\" problem that requires understanding some code logic and making simple modifications. It is slightly above the \"Very Easy\" range due to the need to understand build system configurations and library-specific logging mechanisms, but it remains far from medium or hard due to the limited scope and straightforward nature of the changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Running tests with pytest-asyncio 0.23.5.post1 makes test_asyncio.py tests fail due to DeprecationWarning\nWe intend to update `pytest-asyncio` to the latest release (0.23.5.post1) in Fedora. Testing packages that depend on it, showed `pytest-subprocess` tests failing, because warnings are turned into errors as per:\r\n\r\nhttps://github.com/aklajnert/pytest-subprocess/blob/cc26247fb37e53dbd06a54860d5a5094eed8b679/pytest.ini#L3-L5\r\n\r\nresulting in:\r\n\r\n```python\r\n==================================== ERRORS ====================================\r\n__________________ ERROR at setup of test_basic_usage[shell] ___________________\r\nfixturedef = <FixtureDef argname='event_loop' scope='function' baseid='tests/test_asyncio.py'>\r\n    @pytest.hookimpl(hookwrapper=True)\r\n    def pytest_fixture_setup(\r\n        fixturedef: FixtureDef,\r\n    ) -> Generator[None, Any, None]:\r\n        \"\"\"Adjust the event loop policy when an event loop is produced.\"\"\"\r\n        if fixturedef.argname == \"event_loop\":\r\n            # The use of a fixture finalizer is preferred over the\r\n            # pytest_fixture_post_finalizer hook. The fixture finalizer is invoked once\r\n            # for each fixture, whereas the hook may be invoked multiple times for\r\n            # any specific fixture.\r\n            # see https://github.com/pytest-dev/pytest/issues/5848\r\n            _add_finalizers(\r\n                fixturedef,\r\n                _close_event_loop,\r\n                _restore_event_loop_policy(asyncio.get_event_loop_policy()),\r\n                _provide_clean_event_loop,\r\n            )\r\n            outcome = yield\r\n            loop: asyncio.AbstractEventLoop = outcome.get_result()\r\n            # Weird behavior was observed when checking for an attribute of FixtureDef.func\r\n            # Instead, we now check for a special attribute of the returned event loop\r\n            fixture_filename = inspect.getsourcefile(fixturedef.func)\r\n            if not getattr(loop, \"__original_fixture_loop\", False):\r\n                _, fixture_line_number = inspect.getsourcelines(fixturedef.func)\r\n>               warnings.warn(\r\n                    _REDEFINED_EVENT_LOOP_FIXTURE_WARNING\r\n                    % (fixture_filename, fixture_line_number),\r\n                    DeprecationWarning,\r\n                )\r\nE               DeprecationWarning: The event_loop fixture provided by pytest-asyncio has been redefined in\r\nE               /builddir/build/BUILD/pytest-subprocess-1.5.0/tests/test_asyncio.py:14\r\nE               Replacing the event_loop fixture with a custom implementation is deprecated\r\nE               and will lead to errors in the future.\r\nE               If you want to request an asyncio event loop with a scope other than function\r\nE               scope, use the \"scope\" argument to the asyncio mark when marking the tests.\r\nE               If you want to return different types of event loops, use the event_loop_policy\r\nE               fixture.\r\n/usr/lib/python3.12/site-packages/pytest_asyncio/plugin.py:769: DeprecationWarning\r\n_________________ ERROR at teardown of test_basic_usage[shell] _________________\r\n    def _close_event_loop() -> None:\r\n        policy = asyncio.get_event_loop_policy()\r\n        try:\r\n>           loop = policy.get_event_loop()\r\n/usr/lib/python3.12/site-packages/pytest_asyncio/plugin.py:823: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nself = <asyncio.unix_events._UnixDefaultEventLoopPolicy object at 0x7fd70d9466c0>\r\n    def get_event_loop(self):\r\n        \"\"\"Get the event loop for the current context.\r\n    \r\n        Returns an instance of EventLoop or raises an exception.\r\n        \"\"\"\r\n        if (self._local._loop is None and\r\n                not self._local._set_called and\r\n                threading.current_thread() is threading.main_thread()):\r\n            stacklevel = 2\r\n            try:\r\n                f = sys._getframe(1)\r\n            except AttributeError:\r\n                pass\r\n            else:\r\n                # Move up the call stack so that the warning is attached\r\n                # to the line outside asyncio itself.\r\n                while f:\r\n                    module = f.f_globals.get('__name__')\r\n                    if not (module == 'asyncio' or module.startswith('asyncio.')):\r\n                        break\r\n                    f = f.f_back\r\n                    stacklevel += 1\r\n            import warnings\r\n>           warnings.warn('There is no current event loop',\r\n                          DeprecationWarning, stacklevel=stacklevel)\r\nE           DeprecationWarning: There is no current event loop\r\n/usr/lib64/python3.12/asyncio/events.py:697: DeprecationWarning\r\n```\r\n(Above is a sample. The same happens for all tests from `test_asyncio.py`)\r\n\r\nWe've turned off that setting for now.\n", "patch": "diff --git a/setup.cfg b/setup.cfg\nindex c9da5a2..ca4dac3 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -3,7 +3,7 @@ ignore = E231,W503\n max-line-length = 89\n \n [mypy]\n-python_version = 3.6\n+python_version = 3.8\n warn_return_any = True\n warn_unused_configs = True\n disallow_untyped_defs = True\n", "instance_id": "aklajnert__pytest-subprocess-148", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: updating `pytest-asyncio` to version 0.23.5.post1 causes test failures in `pytest-subprocess` due to `DeprecationWarning` being treated as errors. It provides detailed error logs and references to the relevant configuration in the `pytest.ini` file, which helps in understanding the root cause. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the desired resolution or expected outcome (e.g., whether to suppress warnings, modify the test configuration, or update the codebase to avoid deprecated behavior). Additionally, while it mentions turning off a setting temporarily, it lacks clarity on whether this is the intended final solution or just a workaround. Edge cases or specific constraints related to the update are also not mentioned. Overall, the statement is valid and mostly clear but could benefit from explicitly stating the goal of the fix and any potential constraints or side effects of the solution.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to several factors. First, the scope of the code change provided in the diff is minimal, involving only a small update to the Python version in `setup.cfg` from 3.6 to 3.8. This change appears to be unrelated to the core issue described (DeprecationWarning with `pytest-asyncio`), suggesting that the provided diff might not fully address the problem or is only a part of a larger solution. Second, the technical concepts involved are relatively straightforward: understanding Python warnings, pytest configuration, and potentially asyncio event loop handling. These are not overly complex for a developer familiar with Python testing frameworks. Third, the problem does not seem to require deep architectural changes or extensive modifications across multiple files based on the provided diff, though the full resolution might involve additional changes not shown (e.g., modifying `pytest.ini` or test code to handle warnings). Finally, while there are implicit edge cases related to event loop handling and warnings in asyncio, they are not explicitly detailed in the problem statement, and the error handling requirements appear manageable. Overall, this problem requires understanding some code logic and making simple modifications, likely involving configuration tweaks or minor code adjustments, hence a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support a body template specifically for when there are no contributing commits\n### Is there an existing issue or pull request for this?\r\n\r\n- [X] I have searched the existing issues and pull requests\r\n\r\n### Feature description\r\n\r\nIn the case where no commits contribute to the new version, which is fairly likely with `--include-path` and monorepos, the `body` template is apparently not evaluated at all. Then when the changelog is modified (for example with `--prepend \u2026`) no changes occur, and then there is nothing to stage, which `git commit` doesn't like and will exit with a non-zero status.\r\n\r\n### Desired solution\r\n\r\nA template specifically for the case where there are in fact no commits contributing, then it would be possible to still output a changelog for this, which would a) record the new version's existence and b) be explicit about the fact that there are no changes.\r\n\r\n### Alternatives considered\r\n\r\n#### Alternative 1: `git commit --allow-empty`\r\n\r\nThis is the workaround we opted for, but unfortunately it means the version isn't recorded at all in the changelog.\r\n\r\n#### Alternative 2: Reuse the existing `body` template\r\n\r\nRequiring everyone to cater for branching on zero commits is breaking the current contract for the `body` template, and seems slightly annoying to write the template for.\r\n\r\n### Additional context\r\n\r\nI'm basing my assumption that `body` isn't run at all by placing some literal text at the very top of `body` and noting that it does not appear in the changelog until I remove filters/include paths to eventually include at least one commit.\n", "patch": "diff --git a/config/cliff.toml b/config/cliff.toml\nindex fc5def181c..3d562041ba 100644\n--- a/config/cliff.toml\n+++ b/config/cliff.toml\n@@ -38,6 +38,8 @@ trim = true\n postprocessors = [\n   # { pattern = '<REPO>', replace = \"https://github.com/orhun/git-cliff\" }, # replace repository URL\n ]\n+# render body even when there are no releases to process\n+# render_always = true\n # output file path\n # output = \"test.md\"\n \ndiff --git a/git-cliff-core/src/changelog.rs b/git-cliff-core/src/changelog.rs\nindex 6ff289242e..9649bf8767 100644\n--- a/git-cliff-core/src/changelog.rs\n+++ b/git-cliff-core/src/changelog.rs\n@@ -148,7 +148,7 @@ impl<'a> Changelog<'a> {\n \n \t/// Processes the releases and filters them out based on the configuration.\n \tfn process_releases(&mut self) {\n-\t\tdebug!(\"Processing the releases...\");\n+\t\tdebug!(\"Processing {} release(s)...\", self.releases.len());\n \t\tlet skip_regex = self.config.git.skip_tags.as_ref();\n \t\tlet mut skipped_tags = Vec::new();\n \t\tself.releases = self\n@@ -161,7 +161,12 @@ impl<'a> Changelog<'a> {\n \t\t\t\t\tif let Some(version) = release.version.clone() {\n \t\t\t\t\t\ttrace!(\"Release doesn't have any commits: {}\", version);\n \t\t\t\t\t}\n-\t\t\t\t\tfalse\n+\t\t\t\t\tmatch &release.previous {\n+\t\t\t\t\t\tSome(prev_release) if prev_release.commits.is_empty() => {\n+\t\t\t\t\t\t\tself.config.changelog.render_always.unwrap_or(false)\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\t_ => false,\n+\t\t\t\t\t}\n \t\t\t\t} else if let Some(version) = &release.version {\n \t\t\t\t\t!skip_regex.is_some_and(|r| {\n \t\t\t\t\t\tlet skip_tag = r.is_match(version);\n@@ -670,6 +675,7 @@ mod test {\n \t\t\t\t\treplace:         Some(String::from(\"exciting\")),\n \t\t\t\t\treplace_command: None,\n \t\t\t\t}]),\n+\t\t\t\trender_always:  None,\n \t\t\t\toutput:         None,\n \t\t\t},\n \t\t\tgit:       GitConfig {\ndiff --git a/git-cliff-core/src/config.rs b/git-cliff-core/src/config.rs\nindex 3afcc2b99e..fd0000d9c2 100644\n--- a/git-cliff-core/src/config.rs\n+++ b/git-cliff-core/src/config.rs\n@@ -77,6 +77,8 @@ pub struct ChangelogConfig {\n \tpub footer:         Option<String>,\n \t/// Trim the template.\n \tpub trim:           Option<bool>,\n+\t/// Always render the body template.\n+\tpub render_always:  Option<bool>,\n \t/// Changelog postprocessors.\n \tpub postprocessors: Option<Vec<TextProcessor>>,\n \t/// Output file path.\ndiff --git a/git-cliff/src/lib.rs b/git-cliff/src/lib.rs\nindex c71007dfa2..73715c55ac 100644\n--- a/git-cliff/src/lib.rs\n+++ b/git-cliff/src/lib.rs\n@@ -235,6 +235,7 @@ fn process_repository<'a>(\n \t}\n \n \t// Update tags.\n+\tlet mut releases = vec![Release::default()];\n \tlet mut tag_timestamp = None;\n \tif let Some(ref tag) = args.tag {\n \t\tif let Some(commit_id) = commits.first().map(|c| c.id().to_string()) {\n@@ -247,11 +248,16 @@ fn process_repository<'a>(\n \t\t\t\t\ttags.insert(commit_id, repository.resolve_tag(tag));\n \t\t\t\t}\n \t\t\t}\n+\t\t} else {\n+\t\t\treleases[0].version = Some(tag.to_string());\n+\t\t\treleases[0].timestamp = SystemTime::now()\n+\t\t\t\t.duration_since(UNIX_EPOCH)?\n+\t\t\t\t.as_secs()\n+\t\t\t\t.try_into()?;\n \t\t}\n \t}\n \n \t// Process releases.\n-\tlet mut releases = vec![Release::default()];\n \tlet mut previous_release = Release::default();\n \tlet mut first_processed_tag = None;\n \tfor git_commit in commits.iter().rev() {\ndiff --git a/website/docs/configuration/changelog.md b/website/docs/configuration/changelog.md\nindex 8636eafbfd..45f9cb7e8b 100644\n--- a/website/docs/configuration/changelog.md\n+++ b/website/docs/configuration/changelog.md\n@@ -50,6 +50,10 @@ If set to `true`, leading and trailing whitespace are removed from the [`body`](\n \n It is useful for adding indentation to the template for readability, as shown [in the example](#changelog).\n \n+### render_always\n+\n+If set to `true`, the changelog [body](#body) will be rendered even if there are no releases to process.\n+\n ### postprocessors\n \n An array of commit postprocessors for manipulating the changelog before outputting.\n", "instance_id": "orhun__git-cliff-859", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue and the desired outcome. It identifies a specific scenario where no commits contribute to a new version (especially in monorepos with filters like `--include-path`), resulting in the `body` template not being evaluated and causing issues with changelog generation and `git commit`. The goal of introducing a template or mechanism to handle this \"no commits\" case is evident, and the desired solution (a specific template for this case) is articulated. Additionally, alternatives like `git commit --allow-empty` and reusing the existing `body` template are discussed, which adds context. However, there are minor ambiguities: the problem statement does not explicitly define the expected format or content of the \"no commits\" template, nor does it mention specific edge cases or constraints (e.g., how this should interact with other configuration options or filters). While the issue is valid and the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (`config/cliff.toml`, `git-cliff-core/src/changelog.rs`, `git-cliff-core/src/config.rs`, `git-cliff/src/lib.rs`, and documentation), indicating a need to understand and modify different parts of the codebase. However, the changes themselves are relatively straightforward\u2014adding a new configuration option (`render_always`), updating logic to handle rendering the body template when there are no commits, and initializing releases with a default version in specific cases. The amount of code change is moderate, with small but meaningful modifications. Second, the technical concepts required are not overly complex: they involve understanding Rust's struct and option handling, basic control flow, and configuration parsing, which are intermediate-level skills. Third, the problem does not appear to impact the broader system architecture significantly, as it is more of a feature enhancement than a core refactoring. Finally, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest handling scenarios like empty previous releases, which adds a slight layer of complexity but not to an extreme degree. Overall, this task requires a moderate understanding of the codebase and careful implementation across several files, justifying a score of 0.45, on the lower end of the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Serialise as EMMO datasets\n# Description\r\nThis PR implements a new `dlite.dataset` module for serialisation of DLite datamodels and instances to an RDF representation based on EMMO following the representation shown in the figure below.\r\n\r\nThe main interface is exposed by four new functions:\r\n- `add_dataset()`: stores datamodel+mappings to a triplestore\r\n- `add_data()`: stores an instance (or datamodel) to a triplestore\r\n- `get_dataset()`: loads datamodel+mappings from a triplestore\r\n- `get_data()`: loads an instance (or datamodel) from a triplestore\r\n\r\n**Question**: Is the naming of these functions understandable? The term *dataset* comes from EMMO, but as a user of DLite it may be confusing. Maybe `save_datamodel()`, `save_instance()`, `load_datamodel()`, `load_instance()` would be more intuitive?\r\n\r\nTwo tests are added using a datamodel matching what is shown in the figure.\r\n- [test_dataset1_dave.py](https://github.com/SINTEF/dlite/blob/652-serialise-data-models-to-tbox/bindings/python/tests/test_dataset1_save.py) loads first a [`FluidData`]( https://github.com/SINTEF/dlite/blob/652-serialise-data-models-to-tbox/bindings/python/tests/entities/FluidData.json) datamodel and documents it semantically with the following mappings\r\n  ```python\r\n  mappings = [\r\n    (FLUID,                  EMMO.isDescriptionFor, EMMO.Fluid),\r\n    (FLUID.LJPotential,      MAP.mapsTo,            EMMO.String),\r\n    (FLUID.LJPotential,      EMMO.isDescriptionFor, EMMO.MolecularEntity),\r\n    (FLUID.TemperatureField, MAP.mapsTo,            EMMO.ThermodynamicTemperature),\r\n    (FLUID.ntimes,           MAP.mapsTo,            EMMO.Time),\r\n    (FLUID.npositions,       MAP.mapsTo,            EMMO.Position),\r\n  ]\r\n  ```\r\n  Note the use of `emmo:isDescriptionFor` relations in the mappings. They are stored as-is in the triplestore.\r\n  The `map:mapsTo` are translated to `rdfs:subClassOf` when serialised to the triplestore.\r\n\r\n  Then it uses the `add_dataset()` function in the new `dlite.dataset` module and stores it as RDF in a local triplestore. The content of the triplestore corresponds now to the figure below.\r\n\r\n  Then it creates two `FluidData` instances and store them (using the `add_data()` function) as RDF in a local triplestore as well. The instances are represented as an individual with a rdf:JSON data property containing the instance data. \r\n\r\n  Finally the triplestore is serialised to a turtle file. \r\n\r\n- [test_dataset2_load.py](https://github.com/SINTEF/dlite/blob/652-serialise-data-models-to-tbox/bindings/python/tests/test_dataset2_load.py) loads the turtle file into a local triplestore and reconstruct the `FluidData` datamodel as well as the mappings using the `get_dataset()` function. \r\n\r\n  Using the `get_hash()` method, it is checked that the reconstruct the `FluidData` datamodel is exactly equal to the original datamodel.\r\n\r\n  Finally it loads the two instances using the `get_data()` function and check that they are exactly equal to the two original instances.\r\n\r\n![](https://raw.githubusercontent.com/SINTEF/dlite/652-serialise-data-models-to-tbox/doc/_static/dataset-v2.svg)\r\n\r\n\r\n\r\n\r\n\r\n## Type of change\r\n- [ ] Bug fix & code cleanup\r\n- [x] New feature\r\n- [ ] Documentation update\r\n- [x] Test update\r\n\r\n## Checklist for the reviewer\r\nThis checklist should be used as a help for the reviewer.\r\n\r\n- [ ] Is the change limited to one issue?\r\n- [ ] Does this PR close the issue?\r\n- [ ] Is the code easy to read and understand?\r\n- [ ] Do all new feature have an accompanying new test?\r\n- [ ] Has the documentation been updated as necessary?\r\n\n", "patch": "diff --git a/examples/dataset/README.md b/examples/dataset/README.md\nindex 29611ee82..c0cb01a33 100644\n--- a/examples/dataset/README.md\n+++ b/examples/dataset/README.md\n@@ -27,6 +27,9 @@ properties:\n     description: Array of measured temperatures.\n ```\n \n+A DLite datamodel is represented with an emmo:DataSet, and its properties are represented with emmo:Datum as illustrated in the figure above.\n+It shows how a simple [`FluidData`] datamodel is represented.\n+\n The datamodel is semantically enhanced using the following mappings\n   ```python\n   mappings = [\n@@ -45,5 +48,4 @@ Some comments:\n - The `map:mapsTo` are translated to `rdfs:subClassOf` when serialised to the triplestore.\n \n \n-\n [`FluidData`]: https://github.com/SINTEF/dlite/blob/652-serialise-data-models-to-tbox/examples/dataset/datamodels/FluidData.json\n", "instance_id": "SINTEF__dlite-877", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal of implementing a new `dlite.dataset` module for serializing DLite datamodels and instances to an RDF representation based on EMMO. It provides a high-level overview of the four main functions (`add_dataset()`, `add_data()`, `get_dataset()`, `get_data()`) and their purpose. Additionally, it includes a detailed description of the test cases and mappings, along with a visual representation (figure) to aid understanding. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement raises a question about the intuitiveness of function naming but does not provide a definitive direction or requirement for resolution. Furthermore, critical details such as the exact input/output formats for the functions, specific constraints on the triplestore (e.g., supported formats or limitations), and potential edge cases (e.g., handling invalid mappings or triplestore failures) are not explicitly mentioned. While the overall intent is clear, these gaps could lead to misinterpretation or require additional clarification during implementation.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" category, due to several factors. First, the scope of the code changes appears to involve creating a new module (`dlite.dataset`) with four key functions, as well as adding associated tests. Although the provided diff only shows a minor update to a README file, the problem statement implies significant code additions in Python, likely spanning multiple files (e.g., the module implementation and test files). This suggests a moderate-to-high impact on the codebase, requiring integration with existing DLite systems and interaction with a triplestore for RDF serialization. Second, the technical concepts involved are relatively complex, including familiarity with RDF, EMMO ontology, triplestore operations, and semantic mappings (e.g., translating `map:mapsTo` to `rdfs:subClassOf`). Additionally, domain-specific knowledge of DLite datamodels and EMMO is necessary, which adds to the learning curve for someone unfamiliar with these concepts. Third, while edge cases are not explicitly mentioned in the problem statement, the nature of serialization and triplestore interactions inherently introduces potential challenges such as handling malformed data, ensuring data consistency, and managing errors during storage or retrieval. Finally, the need to ensure exact reconstruction of datamodels and instances (as verified by `get_hash()`) implies a requirement for precision and possibly performance considerations. Overall, this problem requires a deep understanding of specific libraries and domain concepts, along with careful implementation across multiple components, justifying a difficulty score of 0.65.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Protocol 21 SDK Support\n## Protocol 21 SDK Support\n\nOnce voted in, the release of Protocol 21 will introduce following new CAPs:\n\n- [CAP-51](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0051.md): Smart Contract Host Functionality\n- [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0053.md): Separate host functions to extend the TTL for contract instance and contract code\n- [CAP-54](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0054.md): Soroban refined VM instantiation cost model\n- [CAP-55 ](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0055.md): Soroban streamlined linking\n- [CAP-56](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0056.md): Soroban intra-transaction module caching\n\n\n### **XDR changes:**\n\nXDR update [issue details](https://github.com/stellar/stellar-xdr/pull/184) - https://github.com/stellar/stellar-xdr/commit/1a04392432dacc0092caaeae22a600ea1af3c6a5\n\nReference Implementations:\njs-sdk - https://github.com/stellar/js-stellar-base/pull/738\n\n\n### **Soroban API Changes:**\n\n#### Breaking Changes:\n\n- simulateTransaction: https://github.com/stellar/soroban-rpc/issues/13\n\n#### Non Breaking changes:\n\n-  Add StateDiff to simulateTransaction Response: https://github.com/stellar/soroban-rpc/pull/120\n\nNew optional field in `SimulateTransactionResponse` which indicates how the state (ledger entries) will change as a result of the transaction execution.\n\n```\nStateDiff       []LedgerEntryDiff            `json:\"stateDiff,omitempty\"`   \n\ntype LedgerEntryDiff struct {\n\tBefore string `json:\"before,omitempty\"` // LedgerEntry XDR in base64\n\tAfter  string `json:\"after,omitempty\"`  // LedgerEntry XDR in base64\n}\n```\n\n\n- Add ledger Range to getHealth RPC: https://github.com/stellar/soroban-rpc/pull/133\n\nNew Ledger Range and Ledger Retention Window in getHealth Response\n\n```\n\ntype HealthCheckResult struct {\n\tStatus                string `json:\"status\"`\n\tLatestLedger          uint32 `json:\"latestLedger\"` // New\n\tOldestLedger          uint32 `json:\"oldestLedger\"` // New\n\tLedgerRetentionWindow uint32 `json:\"ledgerRetentionWindow\"` // New\n}\n\n```\n\n### **Horizon API Changes:**\n\n- There are no changes to Horizon APIs\n\n\n\n\n## Reference Implementations\n\nReference implementation authored by SDF:\n\n  - JavaScript: https://github.com/stellar/js-stellar-sdk/issues/935\n\nYou can follow each respective issue to its implementation PRs.\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 7b61a5e5..6ea5792a 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -3,6 +3,7 @@ Release History\n \n ### Pending\n #### Update\n+- feat: add support for Soroban-RPC 21.\n - improve: The function in `stellar_xdr.scval` can accept `sc_val` of `base64` and `bytes` types.\n \n ### Version 9.4.0\ndiff --git a/stellar_sdk/soroban_rpc.py b/stellar_sdk/soroban_rpc.py\nindex 7c5e7f4c..67779f65 100644\n--- a/stellar_sdk/soroban_rpc.py\n+++ b/stellar_sdk/soroban_rpc.py\n@@ -64,6 +64,7 @@ class EventInfo(BaseModel):\n     topic: List[str] = Field(alias=\"topic\")\n     value: str = Field(alias=\"value\")\n     in_successful_contract_call: bool = Field(alias=\"inSuccessfulContractCall\")\n+    transaction_hash: str = Field(alias=\"txHash\")\n \n \n class PaginationOptions(BaseModel):\n@@ -142,6 +143,9 @@ class GetHealthResponse(BaseModel):\n     \"\"\"\n \n     status: str\n+    latest_ledger: int = Field(alias=\"latestLedger\")\n+    oldest_ledger: int = Field(alias=\"oldestLedger\")\n+    ledger_retention_window: int = Field(alias=\"ledgerRetentionWindow\")\n \n \n # simulate_transaction\n@@ -194,6 +198,21 @@ class RestorePreamble(BaseModel):\n     min_resource_fee: int = Field(alias=\"minResourceFee\")\n \n \n+class LedgerEntryChange(BaseModel):\n+    \"\"\"LedgerEntryChange designates a change in a ledger entry. Before and After cannot be omitted at the same time.\n+    If Before is omitted, it constitutes a creation, if After is omitted, it constitutes a deletion.\n+    \"\"\"\n+\n+    # LedgerEntryChangeType\n+    type: str\n+    # LedgerEntryKey in base64\n+    key: str\n+    # LedgerEntry XDR in base64\n+    before: Optional[str] = None\n+    # LedgerEntry XDR in base64\n+    after: Optional[str] = None\n+\n+\n class SimulateTransactionResponse(BaseModel):\n     \"\"\"Response for JSON-RPC method simulateTransaction.\n \n@@ -215,6 +234,10 @@ class SimulateTransactionResponse(BaseModel):\n     restore_preamble: Optional[RestorePreamble] = Field(\n         alias=\"restorePreamble\", default=None\n     )\n+    # If present, it indicates how the state (ledger entries) will change as a result of the transaction execution.\n+    state_changes: Optional[List[LedgerEntryChange]] = Field(\n+        alias=\"stateChanges\", default=None\n+    )\n     # If present, it indicates that a prior RestoreFootprint is required\n     latest_ledger: int = Field(alias=\"latestLedger\")\n \n", "instance_id": "StellarCN__py-stellar-base-933", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement for \"Protocol 21 SDK Support\" is mostly clear, providing a high-level overview of the changes required to support new CAPs (Core Advancement Proposals) and associated API updates for the Soroban-RPC in the Stellar ecosystem. It includes references to specific CAPs, links to related issues and pull requests, and details about breaking and non-breaking changes to the API. The XDR updates and reference implementations are also mentioned, which provide additional context. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the expected behavior or constraints for the new fields added to the API responses (e.g., `stateDiff` in `SimulateTransactionResponse` or `ledgerRetentionWindow` in `GetHealthResponse`). Additionally, there is no mention of specific edge cases or error conditions that need to be handled with these updates. While the links to external resources are helpful, they require the developer to dig through additional documentation to fully understand the requirements, which introduces some friction. Overall, the statement is valid and mostly clear but lacks some finer details and explicit guidance on implementation challenges or edge cases.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes provided are relatively focused, primarily affecting the `soroban_rpc.py` file in the Stellar SDK. The modifications involve adding new fields to existing data models (e.g., `transaction_hash` in `EventInfo`, new ledger-related fields in `GetHealthResponse`, and `state_changes` in `SimulateTransactionResponse`) and updating the changelog. The changes are localized to a single file and do not appear to impact the broader architecture of the system significantly. The amount of code change is moderate, involving structural updates to Pydantic models (or similar data classes) rather than complex logic or algorithmic modifications.\n\n2. **Number of Technical Concepts**: Solving this problem requires a basic understanding of the Stellar protocol, specifically the Soroban-RPC API and its data structures. Familiarity with Python and data modeling libraries (e.g., Pydantic for serialization/deserialization) is necessary. Additionally, developers need to understand the context of the new CAPs (e.g., CAP-51 to CAP-56) and their implications on the API, which introduces some domain-specific knowledge. However, the concepts involved are not overly complex for someone with experience in blockchain SDKs or API development. No advanced algorithms, design patterns, or low-level system programming are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases or error handling requirements for the new fields. However, the nature of the changes (e.g., optional fields like `state_changes` and nullable values in `LedgerEntryChange`) suggests that developers need to ensure proper handling of missing or malformed data in API responses. This adds a moderate layer of complexity, but it is not particularly challenging compared to other aspects of blockchain development. The code changes provided do not include explicit error handling logic, which might need to be addressed depending on the broader codebase standards.\n\n4. **Overall Complexity**: The task requires understanding the context of Protocol 21 and its impact on the Soroban-RPC API, which adds some complexity beyond a simple code update. However, the implementation itself is straightforward, involving mostly data structure updates rather than intricate logic or cross-module interactions. The difficulty is elevated slightly due to the need to align with external specifications (CAPs and XDR changes) and ensure compatibility with reference implementations, but it does not reach the level of a hard or very hard problem.\n\nIn summary, this problem is of medium difficulty (0.45), as it involves moderate scope, a few technical concepts related to Stellar and API modeling, and some implicit edge case considerations. It is suitable for a developer with intermediate experience in Python and familiarity with blockchain SDKs, but it does not require deep architectural changes or advanced technical expertise.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Replace EnhancedFileSystem with fsspec\nThe current implementation of `EnhancedFileSystem` is based on the legacy pyarrow filesystem interface that was removed in pyarrow 16.0.0 (https://github.com/apache/arrow/pull/39825).\r\n\r\nWe can entirely replace `EnhancedFileSystem` with [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html). For HDFS fsspec relies on the new pyarrow [filesystem interface](https://arrow.apache.org/docs/python/filesystems.html#hadoop-distributed-file-system-hdfs).\r\n\r\nResolves #87\n", "patch": "diff --git a/cluster_pack/filesystem.py b/cluster_pack/filesystem.py\nindex a03ab33..2c5b068 100644\n--- a/cluster_pack/filesystem.py\n+++ b/cluster_pack/filesystem.py\n@@ -1,16 +1,32 @@\n import logging\r\n from typing import Tuple, Any\r\n+from urllib.parse import urlparse\r\n \r\n import fsspec\r\n-import fsspec.implementations.arrow\r\n-\r\n-# Support viewfs:// protocol for HDFS\r\n-fsspec.register_implementation(\"viewfs\", fsspec.implementations.arrow.HadoopFileSystem)\r\n \r\n _logger = logging.getLogger(__name__)\r\n \r\n \r\n def resolve_filesystem_and_path(uri: str, **kwargs: Any) -> Tuple[fsspec.AbstractFileSystem, str]:\r\n-    fs, fs_path = fsspec.url_to_fs(uri, **kwargs)\r\n+    parsed_uri = urlparse(uri)\r\n+    fs_path = parsed_uri.path\r\n+    if parsed_uri.scheme == 'hdfs' or parsed_uri.scheme == 'viewfs':\r\n+        netloc_split = parsed_uri.netloc.split(':')\r\n+        host = netloc_split[0]\r\n+        if host == '':\r\n+            host = 'default'\r\n+        else:\r\n+            host = parsed_uri.scheme + \"://\" + host\r\n+        port = 0\r\n+        if len(netloc_split) == 2 and netloc_split[1].isnumeric():\r\n+            port = int(netloc_split[1])\r\n+\r\n+        fs = fsspec.filesystem(\"hdfs\", host=host, port=port, **kwargs)\r\n+    elif parsed_uri.scheme == '':\r\n+        # Input is local path such as /home/user/myfile.parquet\r\n+        fs = fsspec.filesystem('file', **kwargs)\r\n+    else:\r\n+        fs = fsspec.filesystem(parsed_uri.scheme, **kwargs)\r\n+\r\n     _logger.info(f\"Resolved base filesystem: {type(fs)}\")\r\n     return fs, fs_path\r\n", "instance_id": "criteo__cluster-pack-131", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the legacy `EnhancedFileSystem` with `fsspec` due to the removal of the pyarrow filesystem interface in version 16.0.0. It provides context about the dependency on pyarrow's new filesystem interface for HDFS and references relevant documentation and a related issue (#87). However, it lacks specific details about the expected behavior of the replacement, such as how `fsspec` should handle various filesystem protocols beyond HDFS, potential compatibility issues, or specific performance requirements. Additionally, edge cases or constraints (e.g., handling malformed URIs or unsupported schemes) are not mentioned, which could lead to ambiguity during implementation. Overall, while the goal is clear, minor details are missing that would make the problem statement comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single function (`resolve_filesystem_and_path`) in one file (`filesystem.py`), with a moderate amount of code modification (around 30 lines changed). The changes involve replacing the existing filesystem resolution logic with a custom implementation using `fsspec`, which requires understanding how `fsspec` handles different protocols (e.g., HDFS, local files) and parsing URIs using `urlparse`. \n\nTechnically, the problem requires familiarity with Python's standard library (`urllib.parse`), the `fsspec` library, and filesystem concepts (e.g., HDFS, local file paths). The logic for handling different URI schemes and constructing the appropriate filesystem object adds moderate complexity, as it involves conditional branching and parameter parsing (e.g., host and port extraction). However, it does not appear to impact the broader system architecture or require deep refactoring across multiple modules.\n\nEdge cases, such as malformed URIs, unsupported schemes, or invalid port numbers, are not explicitly mentioned in the problem statement but are evident in the code changes (e.g., fallback to 'default' host). Handling these cases adds a slight layer of complexity, though not extensively. Overall, this task requires understanding multiple concepts and making targeted, moderately complex modifications, but it does not demand deep architectural changes or advanced domain-specific knowledge, placing it in the 0.4-0.6 range with a score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update of Numpy to 2.0 has broken schemachange\n**Describe the bug**\r\nA ValueError occurs when attempting to run SchemaChange, this did not occur last Friday (6/14) but is occurring this Monday (6/17).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. pip install schemachange\r\n2. run schemachange\r\n\r\n**Expected behavior**\r\nApplication of a versioned migration and repeatable files.\r\n\r\n**Screenshots**\r\n2024-06-17T15:57:56.5457992Z Traceback (most recent call last):\r\n2024-06-17T15:57:56.5458535Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/bin/schemachange\", line 5, in <module>\r\n2024-06-17T15:57:56.5458837Z     from schemachange.cli import main\r\n2024-06-17T15:57:56.5459520Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/schemachange/cli.py\", line 15, in <module>\r\n2024-06-17T15:57:56.5459791Z     import snowflake.connector\r\n2024-06-17T15:57:56.5460319Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/__init__.py\", line 19, in <module>\r\n2024-06-17T15:57:56.5460671Z     from .connection import SnowflakeConnection\r\n2024-06-17T15:57:56.5461162Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/connection.py\", line 35, in <module>\r\n2024-06-17T15:57:56.5461731Z     from . import errors, proxy\r\n2024-06-17T15:57:56.5462137Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/errors.py\", line 15, in <module>\r\n2024-06-17T15:57:56.5462514Z     from .compat import BASE_EXCEPTION_CLASS\r\n2024-06-17T15:57:56.5462948Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/compat.py\", line 19, in <module>\r\n2024-06-17T15:57:56.5463285Z     from . import constants\r\n2024-06-17T15:57:56.5463719Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/constants.py\", line 12, in <module>\r\n2024-06-17T15:57:56.5463989Z     from .options import pyarrow as pa\r\n2024-06-17T15:57:56.5464397Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/options.py\", line 135, in <module>\r\n2024-06-17T15:57:56.5464708Z     pandas, pyarrow, installed_pandas = _import_or_missing_pandas_option()\r\n2024-06-17T15:57:56.5465295Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/snowflake/connector/options.py\", line 77, in _import_or_missing_pandas_option\r\n2024-06-17T15:57:56.5465683Z     pandas = importlib.import_module(\"pandas\")\r\n2024-06-17T15:57:56.5465951Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n2024-06-17T15:57:56.5515033Z     return _bootstrap._gcd_import(name[level:], package, level)\r\n2024-06-17T15:57:56.5516339Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/__init__.py\", line 22, in <module>\r\n2024-06-17T15:57:56.5516827Z     from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\r\n2024-06-17T15:57:56.5517385Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 18, in <module>\r\n2024-06-17T15:57:56.5518015Z     from pandas.compat.numpy import (\r\n2024-06-17T15:57:56.5518522Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py\", line 4, in <module>\r\n2024-06-17T15:57:56.5518894Z     from pandas.util.version import Version\r\n2024-06-17T15:57:56.5519393Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/util/__init__.py\", line 2, in <module>\r\n2024-06-17T15:57:56.5519762Z     from pandas.util._decorators import (  # noqa:F401\r\n2024-06-17T15:57:56.5520269Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 14, in <module>\r\n2024-06-17T15:57:56.5520620Z     from pandas._libs.properties import cache_readonly\r\n2024-06-17T15:57:56.5521222Z   File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pandas/_libs/__init__.py\", line 13, in <module>\r\n2024-06-17T15:57:56.5521536Z     from pandas._libs.interval import Interval\r\n2024-06-17T15:57:56.5521835Z   File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\r\n2024-06-17T15:57:56.5522229Z ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\r\n\r\n**Schemachange (please complete the following information):**\r\n- Version: 3.6.1\n", "patch": "diff --git a/.github/CONTRIBUTING.md b/.github/CONTRIBUTING.md\nindex e6a49e39..3ded2980 100644\n--- a/.github/CONTRIBUTING.md\n+++ b/.github/CONTRIBUTING.md\n@@ -4,7 +4,7 @@\n \n When reporting issues please include as much detail as possible about your\n operating environment, schemachange version and python version. Whenever possible, please\n-also include a brief, self-contained code example that demonstrates the problem. \n+also include a brief, self-contained code example that demonstrates the problem.\n \n We have included [issue templates for reporting bugs, requesting features and seeking clarifications.](https://github.com/Snowflake-Labs/schemachange/issues/new/choose)\n Choose the appropriate issue template to contribute to the repository.\n@@ -21,50 +21,50 @@ Thanks for your interest in contributing code to schemachange!\n 1. If you are a first-time contributor\n     + Go to [Snowflake-Labs/Schemachange](https://github.com/Snowflake-Labs/schemachange) and click the \"fork\" button to create your own copy of the project.\n     + [Clone](https://github.com/git-guides/git-clone) the project to your local computer\n-  \n+\n     ```shell\n     git clone https://github.com/your-username/schemachange.git\n-    \n+\n     ```\n-    \n+\n     + Change the directory\n-   \n+\n     ```shell\n     cd schemachange\n-    \n+\n     ```\n     + Add upstream repository:\n \n     ```shell\n     git remote add upstream https://github.com/Snowflake-Labs/schemachange\n-    \n+\n     ```\n-    \n+\n     + Now, `git remote -v` will show two [remote](https://github.com/git-guides/git-remote) repositories named:\n       + `upstream`, which refers to the `schemachange` repository\n-      + `origin`, which refers to your personal fork \n+      + `origin`, which refers to your personal fork\n     + [Pull](https://github.com/git-guides/git-pull) the latest changes from upstream, including tags:\n-      \n+\n     ```shell\n     git checkout main\n     git pull upstream main --tags\n-    \n+\n     ```\n-    \n+\n 3. Develop your contribution\n     + Create a branch for the features you want to work on. Since the branch name will appear in the merge message, use a sensible name such as 'update-build-library-dependencies':\n-      \n+\n     ```shell\n     git checkout -b update-build-library-dependencies\n     ```\n-    \n+\n     + Commit locally as you progress ( [git add](https://github.com/git-guides/git-add) and [git commit](https://github.com/git-guides/git-commit) ). Use a properly formatted commit message. Be sure to document any changed behavior.\n 4. To submit your contribution\n     + [Push](https://github.com/git-guides/git-push) your changes back to your fork on GitHub\n \n     ```shell\n     git push origin update-build-library-dependencies\n-    \n+\n     ```\n \n     + Go to GitHub. The new branch will show up with a green [Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#initiating-the-pull-request) button. Make sure the title and message are clear, concise and self explanatory. Then click the button to submit it.\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 63e4b8fe..c707b5ef 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -3,6 +3,11 @@ All notable changes to this project will be documented in this file.\n \n *The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).*\n \n+## [3.6.2] - 2024-06-10\n+### Changed\n+- Updated pandas version dependency\n+- Pinned NumPy version dependency\n+\n ## [3.6.1] - 2023-11-15\n ### Added\n - Allow passing snowflake schema as config or CLI parameter\ndiff --git a/requirements.txt b/requirements.txt\nindex 5e8d0440..161d51b6 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,4 +1,5 @@\n Jinja2~=3.0\n-pandas~=1.3\n+numpy=>=1.23.2,<2\n+pandas>=1.5.3,<3\n PyYAML~=6.0\n snowflake-connector-python>=2.8,<4.0\ndiff --git a/setup.cfg b/setup.cfg\nindex b3f8ec01..66a0f9bc 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -1,6 +1,6 @@\n [metadata]\n name = schemachange\n-version = 3.6.1\n+version = 3.6.2\n description = A Database Change Management tool for Snowflake\n long_description = file: README.md\n long_description_content_type = text/markdown\n@@ -18,7 +18,8 @@ classifiers =\n packages = schemachange\n install_requires =\n     jinja2~=3.0\n-    pandas~=1.3\n+    numpy>=1.23.2,<2\n+    pandas>=1.5.3,<3\n     pyyaml~=6.0\n     snowflake-connector-python>=2.8,<4.0\n python_requires = >=3.8\n", "instance_id": "Snowflake-Labs__schemachange-260", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a `ValueError` occurs when running `schemachange` after a NumPy update to version 2.0, which did not happen previously. It provides steps to reproduce the issue, the expected behavior, and a detailed stack trace that points to a binary incompatibility issue with NumPy's dtype size. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the root cause (though it can be inferred from the stack trace and title), nor does it specify the exact environment details (e.g., Python version constraints or other dependency versions beyond `schemachange` 3.6.1). Additionally, there are no explicit mentions of edge cases or specific constraints that might need to be considered when addressing the issue. Despite these minor gaps, the intent and context of the problem are understandable, especially with the provided stack trace, making it \"Mostly Clear.\"", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to several factors. First, the scope of the code changes is minimal and localized, primarily involving updates to dependency versions in `requirements.txt` and `setup.cfg` to pin NumPy to a version below 2.0 and adjust pandas compatibility. These changes do not impact the broader architecture of the system or require deep modifications across multiple modules; they are straightforward updates to configuration files. Second, the technical concepts involved are basic\u2014understanding Python dependency management and version constraints, which are fundamental skills for any developer working with Python projects. No complex algorithms, design patterns, or domain-specific knowledge beyond basic library compatibility are required. Third, the problem does not explicitly mention edge cases or require significant error handling beyond ensuring compatibility with the pinned versions. The stack trace clearly points to a binary incompatibility issue with NumPy, and the solution (pinning versions) is a common and well-understood fix for such issues. Lastly, the amount of code change is trivial, involving only a few lines in configuration files. While some understanding of the interaction between `schemachange`, `pandas`, and `snowflake-connector-python` is helpful, it is not critical to implementing the fix. Therefore, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and minimal codebase understanding.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "(sfnp) add permute host op.\nSome use cases require a bit of array handling in between shortfin program invocations or as pre/post-processing steps. In SD workflows, it's common to see a permutation of the image output array in preparation for image library (e.g. pillow) \r\nconsumption.\r\n\r\nPerhaps implementing the equivalent of np.transpose ([docs](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)) would achieve what we need for most cases.\r\n\r\nThe following is a snippet showing how we use np.transpose in SDXL:\r\n```\r\npermuted = (\r\n    np.transpose(vae_output, (0, 2, 3, 1))[0]\r\n)\r\n```\r\n\n", "patch": "diff --git a/shortfin/python/array_binding.cc b/shortfin/python/array_binding.cc\nindex a05232674..08a4071a8 100644\n--- a/shortfin/python/array_binding.cc\n+++ b/shortfin/python/array_binding.cc\n@@ -531,22 +531,26 @@ void BindArray(py::module_ &m) {\n                  ->AddAsInvocationArgument(\n                      inv, static_cast<local::ProgramResourceBarrier>(barrier));\n            })\n-      .def_static(\"for_device\",\n-                  [](local::ScopedDevice &device, std::span<const size_t> shape,\n-                     DType dtype) {\n-                    return custom_new_keep_alive<device_array>(\n-                        py::type<device_array>(),\n-                        /*keep_alive=*/device.fiber(),\n-                        device_array::for_device(device, shape, dtype));\n-                  })\n-      .def_static(\"for_host\",\n-                  [](local::ScopedDevice &device, std::span<const size_t> shape,\n-                     DType dtype) {\n-                    return custom_new_keep_alive<device_array>(\n-                        py::type<device_array>(),\n-                        /*keep_alive=*/device.fiber(),\n-                        device_array::for_host(device, shape, dtype));\n-                  })\n+      .def_static(\n+          \"for_device\",\n+          [](local::ScopedDevice &device, std::span<const size_t> shape,\n+             DType dtype) {\n+            return custom_new_keep_alive<device_array>(\n+                py::type<device_array>(),\n+                /*keep_alive=*/device.fiber(),\n+                device_array::for_device(device, shape, dtype));\n+          },\n+          py::arg(\"device\"), py::arg(\"shape\"), py::arg(\"dtype\"))\n+      .def_static(\n+          \"for_host\",\n+          [](local::ScopedDevice &device, std::span<const size_t> shape,\n+             DType dtype) {\n+            return custom_new_keep_alive<device_array>(\n+                py::type<device_array>(),\n+                /*keep_alive=*/device.fiber(),\n+                device_array::for_host(device, shape, dtype));\n+          },\n+          py::arg(\"device\"), py::arg(\"shape\"), py::arg(\"dtype\"))\n       .def(\"for_transfer\",\n            [](device_array &self) {\n              return custom_new_keep_alive<device_array>(\ndiff --git a/shortfin/python/array_host_ops.cc b/shortfin/python/array_host_ops.cc\nindex 86385cfee..3e2a8ebe3 100644\n--- a/shortfin/python/array_host_ops.cc\n+++ b/shortfin/python/array_host_ops.cc\n@@ -91,6 +91,18 @@ static const char DOCSTRING_RANDOM_GENERATOR[] =\n       fixed number.\n   )\";\n \n+static const char DOCSTRING_TRANSPOSE[] =\n+    R\"(Transposes axes of an array according to a permutation vector.\n+\n+Args:\n+  input: Array to transpose.\n+  permutation: New sequence of axes. Must have same number of elements as the\n+    rank of input.\n+  out: If given, then the results are written to this array.\n+  device_visible: Whether to make the result array visible to devices. Defaults\n+    to False.\n+)\";\n+\n #define SF_UNARY_FUNCTION_CASE(dtype_name, cpp_type) \\\n   case DType::dtype_name():                          \\\n     return compute.template operator()<cpp_type>()\n@@ -100,6 +112,25 @@ static const char DOCSTRING_RANDOM_GENERATOR[] =\n     compute.template operator()<cpp_type>();      \\\n     break\n \n+#define SF_MOVEMENT_OP_SWITCH(dtype)                                     \\\n+  if (!dtype.is_byte_aligned())                                          \\\n+    throw std::invalid_argument(                                         \\\n+        \"data movement ops are only defined for byte aligned dtypes\");   \\\n+  switch (dtype.dense_byte_count()) {                                    \\\n+    case 1:                                                              \\\n+      return compute.template operator()<uint8_t>();                     \\\n+    case 2:                                                              \\\n+      return compute.template operator()<uint16_t>();                    \\\n+    case 4:                                                              \\\n+      return compute.template operator()<uint32_t>();                    \\\n+    case 8:                                                              \\\n+      return compute.template operator()<uint64_t>();                    \\\n+    default:                                                             \\\n+      throw std::invalid_argument(                                       \\\n+          \"data movement ops are only defined for dtypes of size 1, 2, \" \\\n+          \"4, 8\");                                                       \\\n+  }\n+\n struct PyRandomGenerator {\n  public:\n   using SeedType = xt::random::default_engine_type::result_type;\n@@ -374,6 +405,227 @@ struct ConvertTruncFunctor {\n   }\n };\n \n+void OptionalArrayCast(py::handle handle,\n+                       std::optional<device_array> &maybe_array) {\n+  if (py::isinstance<device_array>(handle)) {\n+    maybe_array.emplace(py::cast<device_array>(handle));\n+  }\n+}\n+\n+int DTypePromotionRank(DType dtype) {\n+  int rank = 1;\n+  if (dtype.is_boolean())\n+    rank *= 1000;\n+  else if (dtype.is_integer())\n+    rank *= 2000;\n+  else if (dtype.is_float())\n+    rank *= 4000;\n+  else if (dtype.is_complex())\n+    rank *= 8000;\n+  return rank + dtype.bit_count();\n+}\n+\n+DType PromoteArithmeticTypes(std::optional<DType> lhs_dtype,\n+                             std::optional<DType> rhs_dtype) {\n+  if (!lhs_dtype && !rhs_dtype) {\n+    throw std::invalid_argument(\n+        \"Elementwise operators require at least one argument to be a \"\n+        \"device_array\");\n+  }\n+\n+  // One not an array: promote to the array type.\n+  if (!lhs_dtype)\n+    return *rhs_dtype;\n+  else if (!rhs_dtype)\n+    return *lhs_dtype;\n+\n+  int lhs_rank = DTypePromotionRank(*lhs_dtype);\n+  int rhs_rank = DTypePromotionRank(*rhs_dtype);\n+  DType promoted_dtype = lhs_rank < rhs_rank ? *rhs_dtype : *lhs_dtype;\n+\n+  // If mismatched signed/unsigned, then need to promote to the next signed\n+  // dtype.\n+  if (promoted_dtype.is_integer()) {\n+    bool lhs_unsigned = iree_all_bits_set(\n+        lhs_dtype->numerical_type(), IREE_HAL_NUMERICAL_TYPE_INTEGER_UNSIGNED);\n+    bool rhs_unsigned = iree_all_bits_set(\n+        rhs_dtype->numerical_type(), IREE_HAL_NUMERICAL_TYPE_INTEGER_UNSIGNED);\n+    if ((lhs_unsigned || rhs_unsigned) && !(lhs_unsigned && rhs_unsigned)) {\n+      // Signed/unsigned mismatch. Promote to next.\n+      switch (promoted_dtype) {\n+        case DType::uint8():\n+        case DType::int8():\n+          return DType::int16();\n+        case DType::uint16():\n+        case DType::int16():\n+          return DType::int32();\n+        case DType::uint32():\n+        case DType::int32():\n+          return DType::int64();\n+        default:\n+          // Jax's type promotion chart says this goes to a weak FP type, but\n+          // we don't implement such a construct and I don't really see how\n+          // that makes sense in a system setting like this, so we just saturate\n+          // to 64bit.\n+          return DType::int64();\n+      }\n+    }\n+  }\n+\n+  return promoted_dtype;\n+}\n+\n+// ---------------------------------------------------------------------------//\n+// Elementwise support\n+// ---------------------------------------------------------------------------//\n+\n+// Python element type scalar conversion functions.\n+uint8_t ConvertPyToEltTy(py::handle py_value, uint8_t zero) {\n+  return py::cast<uint8_t>(py_value);\n+}\n+\n+int8_t ConvertPyToEltTy(py::handle py_value, int8_t zero) {\n+  return py::cast<int8_t>(py_value);\n+}\n+\n+uint16_t ConvertPyToEltTy(py::handle py_value, uint16_t zero) {\n+  return py::cast<uint16_t>(py_value);\n+}\n+\n+int16_t ConvertPyToEltTy(py::handle py_value, int16_t zero) {\n+  return py::cast<int16_t>(py_value);\n+}\n+\n+uint32_t ConvertPyToEltTy(py::handle py_value, uint32_t zero) {\n+  return py::cast<uint32_t>(py_value);\n+}\n+\n+int32_t ConvertPyToEltTy(py::handle py_value, int32_t zero) {\n+  return py::cast<int32_t>(py_value);\n+}\n+\n+uint64_t ConvertPyToEltTy(py::handle py_value, uint64_t zero) {\n+  return py::cast<uint64_t>(py_value);\n+}\n+\n+int64_t ConvertPyToEltTy(py::handle py_value, int64_t zero) {\n+  return py::cast<int64_t>(py_value);\n+}\n+\n+float ConvertPyToEltTy(py::handle py_value, float zero) {\n+  return py::cast<float>(py_value);\n+}\n+\n+double ConvertPyToEltTy(py::handle py_value, double zero) {\n+  return py::cast<double>(py_value);\n+}\n+\n+half_float::half ConvertPyToEltTy(py::handle py_value, half_float::half zero) {\n+  // Python can't cast directly to half so first go to double.\n+  return static_cast<half_float::half>(py::cast<double>(py_value));\n+}\n+\n+struct AddFunctor {\n+  template <typename Lhs, typename Rhs>\n+  static auto Invoke(Lhs &&lhs, Rhs &&rhs) {\n+    return lhs + rhs;\n+  }\n+};\n+\n+struct DivideFunctor {\n+  template <typename Lhs, typename Rhs>\n+  static auto Invoke(Lhs &&lhs, Rhs &&rhs) {\n+    return lhs / rhs;\n+  }\n+};\n+\n+struct MultiplyFunctor {\n+  template <typename Lhs, typename Rhs>\n+  static auto Invoke(Lhs &&lhs, Rhs &&rhs) {\n+    return lhs * rhs;\n+  }\n+};\n+\n+struct SubtractFunctor {\n+  template <typename Lhs, typename Rhs>\n+  static auto Invoke(Lhs &&lhs, Rhs &&rhs) {\n+    return lhs - rhs;\n+  }\n+};\n+\n+template <typename ElementwiseFunctor>\n+device_array ElementwiseOperation(py::handle lhs, py::handle rhs,\n+                                  std::optional<device_array> out,\n+                                  bool device_visible) {\n+  std::optional<device_array> lhs_array;\n+  OptionalArrayCast(lhs, lhs_array);\n+  std::optional<device_array> rhs_array;\n+  OptionalArrayCast(rhs, rhs_array);\n+  auto dtype = PromoteArithmeticTypes(\n+      lhs_array ? std::optional<DType>(lhs_array->dtype()) : std::nullopt,\n+      rhs_array ? std::optional<DType>(rhs_array->dtype()) : std::nullopt);\n+  if (lhs_array && lhs_array->dtype() != dtype) {\n+    auto converted = GenericElementwiseConvert<ConvertFunctor>(\n+        *lhs_array, dtype, /*out=*/std::nullopt,\n+        /*device_visible=*/false);\n+    lhs_array.reset();\n+    lhs_array.emplace(std::move(converted));\n+  }\n+  if (rhs_array && rhs_array->dtype() != dtype) {\n+    auto converted = GenericElementwiseConvert<ConvertFunctor>(\n+        *rhs_array, dtype, /*out=*/std::nullopt,\n+        /*device_visible=*/false);\n+    rhs_array.reset();\n+    rhs_array.emplace(std::move(converted));\n+  }\n+\n+  auto compute = [&]<typename EltTy>() -> device_array {\n+    auto handle_result = [&]<typename D, typename A>(\n+                             D &&device, A &&result) -> device_array {\n+      if (!out) {\n+        out.emplace(device_array::for_host(device, result.shape(), dtype,\n+                                           device_visible));\n+      }\n+      auto out_t = out->map_xtensor_w<EltTy>();\n+      *out_t = result;\n+      return *out;\n+    };\n+    if (!rhs_array) {\n+      auto lhs_t = lhs_array->map_xtensor<EltTy>();\n+      xt::xarray<EltTy> rhs_scalar = ConvertPyToEltTy(rhs, EltTy());\n+      return handle_result(lhs_array->device(),\n+                           ElementwiseFunctor::Invoke(*lhs_t, rhs_scalar));\n+    } else if (!lhs_array) {\n+      xt::xarray<EltTy> lhs_scalar = ConvertPyToEltTy(lhs, EltTy());\n+      auto rhs_t = rhs_array->map_xtensor<EltTy>();\n+      return handle_result(rhs_array->device(),\n+                           ElementwiseFunctor::Invoke(lhs_scalar, *rhs_t));\n+    } else {\n+      auto lhs_t = lhs_array->map_xtensor<EltTy>();\n+      auto rhs_t = rhs_array->map_xtensor<EltTy>();\n+      return handle_result(lhs_array->device(),\n+                           ElementwiseFunctor::Invoke(*lhs_t, *rhs_t));\n+    }\n+  };\n+\n+  switch (dtype) {\n+    SF_UNARY_FUNCTION_CASE(float16, half_float::half);\n+    SF_UNARY_FUNCTION_CASE(float32, float);\n+    SF_UNARY_FUNCTION_CASE(float64, double);\n+    SF_UNARY_FUNCTION_CASE(uint8, uint8_t);\n+    SF_UNARY_FUNCTION_CASE(int8, int8_t);\n+    SF_UNARY_FUNCTION_CASE(uint16, uint16_t);\n+    SF_UNARY_FUNCTION_CASE(int16, int16_t);\n+    SF_UNARY_FUNCTION_CASE(uint32, uint32_t);\n+    SF_UNARY_FUNCTION_CASE(int32, uint32_t);\n+    SF_UNARY_FUNCTION_CASE(uint64, uint64_t);\n+    SF_UNARY_FUNCTION_CASE(int64, int64_t);\n+    default:\n+      throw std::invalid_argument(fmt::format(\n+          \"Unsupported dtype({}) for in elementwise op\", dtype.name()));\n+  }\n+}\n+\n }  // namespace\n \n void BindArrayHostOps(py::module_ &m) {\n@@ -457,6 +709,39 @@ void BindArrayHostOps(py::module_ &m) {\n   SF_DEF_CONVERT(\"floor\", GenericElementwiseConvert<ConvertFloorFunctor>);\n   SF_DEF_CONVERT(\"round\", GenericElementwiseConvert<ConvertRoundFunctor>);\n   SF_DEF_CONVERT(\"trunc\", GenericElementwiseConvert<ConvertTruncFunctor>);\n-}\n+\n+  // Transpose.\n+  m.def(\n+      \"transpose\",\n+      [](device_array input, std::vector<size_t> permutation,\n+         std::optional<device_array> out, bool device_visible) {\n+        auto compute = [&]<typename EltTy>() -> device_array {\n+          auto input_t = input.map_xtensor<EltTy>();\n+          auto permuted_t =\n+              xt::transpose(*input_t, permutation, xt::check_policy::full());\n+          if (!out) {\n+            out.emplace(device_array::for_host(input.device(),\n+                                               permuted_t.shape(),\n+                                               input.dtype(), device_visible));\n+          }\n+          auto out_t = out->map_xtensor_w<EltTy>();\n+          *out_t = permuted_t;\n+          return *out;\n+        };\n+        SF_MOVEMENT_OP_SWITCH(input.dtype());\n+      },\n+      py::arg(\"input\"), py::arg(\"permutation\"), py::arg(\"out\") = py::none(),\n+      py::arg(\"device_visible\") = false, DOCSTRING_TRANSPOSE);\n+\n+// Elementwise.\n+#define SF_DEF_ELEMENTWISE(py_name, target)                             \\\n+  m.def(py_name, target, py::arg(\"lhs\"), py::arg(\"rhs\"), py::kw_only(), \\\n+        py::arg(\"out\") = py::none(), py::arg(\"device_visible\") = false)\n+  SF_DEF_ELEMENTWISE(\"add\", ElementwiseOperation<AddFunctor>);\n+  SF_DEF_ELEMENTWISE(\"divide\", ElementwiseOperation<DivideFunctor>);\n+  SF_DEF_ELEMENTWISE(\"multiply\", ElementwiseOperation<MultiplyFunctor>);\n+  SF_DEF_ELEMENTWISE(\"subtract\", ElementwiseOperation<SubtractFunctor>);\n+\n+}  // namespace shortfin::python\n \n }  // namespace shortfin::python\ndiff --git a/shortfin/python/shortfin/array/__init__.py b/shortfin/python/shortfin/array/__init__.py\nindex 6079541c8..670102dfe 100644\n--- a/shortfin/python/shortfin/array/__init__.py\n+++ b/shortfin/python/shortfin/array/__init__.py\n@@ -44,11 +44,16 @@\n \n # Ops.\n argmax = _sfl.array.argmax\n+add = _sfl.array.add\n ceil = _sfl.array.ceil\n convert = _sfl.array.convert\n+divide = _sfl.array.divide\n fill_randn = _sfl.array.fill_randn\n floor = _sfl.array.floor\n+multiply = _sfl.array.multiply\n round = _sfl.array.round\n+subtract = _sfl.array.subtract\n+transpose = _sfl.array.transpose\n trunc = _sfl.array.trunc\n RandomGenerator = _sfl.array.RandomGenerator\n \n@@ -86,12 +91,17 @@\n     \"storage\",\n     \"DType\",\n     # Ops.\n+    \"add\",\n     \"argmax\",\n     \"ceil\",\n     \"convert\",\n+    \"divide\",\n     \"fill_randn\",\n     \"floor\",\n+    \"multiply\",\n     \"round\",\n+    \"subtract\",\n+    \"transpose\",\n     \"trunc\",\n     \"RandomGenerator\",\n ]\ndiff --git a/shortfin/src/shortfin/array/dtype.h b/shortfin/src/shortfin/array/dtype.h\nindex d746d69bf..de1763698 100644\n--- a/shortfin/src/shortfin/array/dtype.h\n+++ b/shortfin/src/shortfin/array/dtype.h\n@@ -49,6 +49,9 @@ class SHORTFIN_API DType {\n   bool is_integer_bitwidth(size_t bitwidth) const {\n     return iree_hal_element_type_is_integer(et_, bitwidth);\n   }\n+  uint32_t numerical_type() const {\n+    return iree_hal_element_numerical_type(et_);\n+  }\n \n   // Computes the size in bytes required to store densely packed nd-dims.\n   // This presently only supports byte aligned dtypes. In the future, when\n", "instance_id": "nod-ai__shark-ai-578", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to add a permutation operation for arrays, specifically mimicking the functionality of `np.transpose` from NumPy for use in image processing workflows (e.g., SDXL). It provides a specific use case and a code snippet to contextualize the requirement, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats beyond referencing NumPy's transpose function, nor does it mention specific constraints or edge cases (e.g., handling of non-contiguous arrays, invalid permutation vectors, or performance requirements). Additionally, it lacks detailed requirements for integration with the existing `shortfin` framework, such as whether the operation should support specific data types or devices. Despite these gaps, the overall intent and scope are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files (`array_binding.cc`, `array_host_ops.cc`, `dtype.h`, and a Python module), indicating a need to understand and modify various parts of the codebase. The changes involve not only adding the `transpose` operation but also introducing supporting infrastructure like type promotion logic, element-wise operations, and data movement operations, which suggests a broader impact on the system. Second, the technical concepts required are moderately complex, including familiarity with C++ (particularly with templates and type handling), Python bindings using `pybind11`, array manipulation libraries like `xtensor`, and domain-specific knowledge of array operations in machine learning workflows (e.g., image processing for SDXL). Third, the code changes require handling edge cases, such as validating permutation vectors, ensuring byte-aligned data types for movement operations, and managing optional output arrays, which adds to the complexity. While the problem does not appear to require deep architectural refactoring or advanced system-level considerations (e.g., distributed systems or low-level optimizations), the combination of cross-file modifications, integration with existing array handling logic, and the need for robust error handling pushes this into the harder spectrum of medium-to-hard difficulty. A score of 0.65 reflects the need for a solid understanding of the codebase and careful implementation across multiple components, without reaching the extreme complexity of very hard problems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] Single quotes in the url path are generated as &#39;\n#### Bug Report Checklist\r\n\r\n- [x] Have you provided a full/minimal spec to reproduce the issue?\r\n- [ ] Have you validated the input using an OpenAPI validator ([example](https://apidevtools.org/swagger-parser/online/))?\r\n- [ ] Have you [tested with the latest master](https://github.com/OpenAPITools/openapi-generator/wiki/FAQ#how-to-test-with-the-latest-master-of-openapi-generator) to confirm the issue still exists?\r\n- [x] Have you searched for related issues/PRs?\r\n- [ ] What's the actual output vs expected output?\r\n\r\n##### Description\r\n\r\nSingle quotes in the url path are generated as `&#39;` \r\n\r\nBased on the given specs, the plugin generates \r\n\r\n```kotlin\r\nfun articleIdGetRequestConfig(id: kotlin.String) : RequestConfig<Unit> {\r\n        val localVariableBody = null\r\n        val localVariableQuery = mutableMapOf<kotlin.String, kotlin.collections.List<kotlin.String>>()\r\n        val localVariableHeaders: MutableMap<String, String> = mutableMapOf()\r\n        \r\n        val params = mutableMapOf<String, Any>(\r\n            \"Id\" to id,\r\n        )\r\n\r\n        return RequestConfig(\r\n            method = RequestMethod.GET,\r\n            path = \"/article(&#39;{Id}&#39;)\",\r\n            params = params,\r\n            query = localVariableQuery,\r\n            headers = localVariableHeaders,\r\n            requiresAuthentication = false,\r\n            body = localVariableBody\r\n        )\r\n    }\r\n```\r\n\r\nwhile it should be\r\n```kotlin\r\nfun articleIdGetRequestConfig(id: kotlin.String) : RequestConfig<Unit> {\r\n        val localVariableBody = null\r\n        val localVariableQuery = mutableMapOf<kotlin.String, kotlin.collections.List<kotlin.String>>()\r\n        val localVariableHeaders: MutableMap<String, String> = mutableMapOf()\r\n        \r\n        val params = mutableMapOf<String, Any>(\r\n            \"Id\" to id,\r\n        )\r\n\r\n        return RequestConfig(\r\n            method = RequestMethod.GET,\r\n            path = \"/article('{Id}')\",\r\n            params = params,\r\n            query = localVariableQuery,\r\n            headers = localVariableHeaders,\r\n            requiresAuthentication = false,\r\n            body = localVariableBody\r\n        )\r\n    }\r\n\r\n```\r\n\r\n\r\n##### openapi-generator version\r\n\r\n7.9.0\r\n\r\n##### OpenAPI declaration file content or url\r\nhttps://gist.github.com/detomarco/3bfe686176d9978ee31809eaf70654b7\r\n  -->\r\n\r\n##### Generation Details\r\n\r\nGradle plugin with this configs\r\n```kotlin\r\nopenApiGenerate {\r\n    val groupId = \"******\"\r\n    generatorName.set(\"kotlin\")\r\n    inputSpec.set(\"****\")\r\n    outputDir.set(\"****\")\r\n    apiPackage.set(\"$groupId.api\")\r\n    invokerPackage.set(\"$groupId.invoker\")\r\n    modelPackage.set(\"$groupId.model\")\r\n    additionalProperties.set(\r\n        mapOf(\r\n            \"library\" to \"jvm-spring-webclient\",\r\n            \"omitGradleWrapper\" to \"true\",\r\n            \"serializationLibrary\" to \"jackson\",\r\n            \"useSpringBoot3\" to \"true\",\r\n            \"useSettingsGradle\" to \"true\",\r\n            \"omitGradleWrapper\" to \"true\",\r\n        )\r\n    )\r\n}\r\n```\r\n\r\n##### Steps to reproduce\r\n\r\n<!-- unambiguous set of steps to reproduce the bug.-->\r\n\r\n##### Related issues/PRs\r\n\r\n\r\n##### Suggest a fix\r\n\r\n<!-- if you can't fix the bug yourself, perhaps you can point to what might be\r\n  causing the problem (line of code or commit), or simply make a suggestion -->\r\n\n", "patch": "diff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-ktor/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-ktor/api.mustache\nindex c6ca0ce0c06d..c246003421c6 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-ktor/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-ktor/api.mustache\n@@ -134,7 +134,7 @@ import com.fasterxml.jackson.databind.ObjectMapper\n \n             val localVariableConfig = RequestConfig<kotlin.Any?>(\n             RequestMethod.{{httpMethod}},\n-            \"{{path}}\"{{#pathParams}}.replace(\"{\" + \"{{baseName}}\" + \"}\", \"${{{paramName}}}\"){{/pathParams}},\n+            \"{{{path}}}\"{{#pathParams}}.replace(\"{\" + \"{{baseName}}\" + \"}\", \"${{{paramName}}}\"){{/pathParams}},\n             query = localVariableQuery,\n             headers = localVariableHeaders,\n             requiresAuthentication = {{#hasAuthMethods}}true{{/hasAuthMethods}}{{^hasAuthMethods}}false{{/hasAuthMethods}},\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-okhttp/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-okhttp/api.mustache\nindex 642794399125..bd262322887c 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-okhttp/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-okhttp/api.mustache\n@@ -232,7 +232,7 @@ import {{packageName}}.infrastructure.toMultiValue\n \n         return RequestConfig(\n             method = RequestMethod.{{httpMethod}},\n-            path = \"{{path}}\"{{#pathParams}}.replace(\"{\"+\"{{#lambda.escapeDollar}}{{baseName}}{{/lambda.escapeDollar}}\"+\"}\", encodeURIComponent({{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{{paramName}}}{{#isEnum}}{{^required}}?{{/required}}.value{{/isEnum}}.toString(){{/isContainer}})){{/pathParams}},\n+            path = \"{{{path}}}\"{{#pathParams}}.replace(\"{\"+\"{{#lambda.escapeDollar}}{{baseName}}{{/lambda.escapeDollar}}\"+\"}\", encodeURIComponent({{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{{paramName}}}{{#isEnum}}{{^required}}?{{/required}}.value{{/isEnum}}.toString(){{/isContainer}})){{/pathParams}},\n             query = localVariableQuery,\n             headers = localVariableHeaders,\n             requiresAuthentication = {{#hasAuthMethods}}true{{/hasAuthMethods}}{{^hasAuthMethods}}false{{/hasAuthMethods}},\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-restclient/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-restclient/api.mustache\nindex 1c533aae70a5..e1ad1abe32fb 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-restclient/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-restclient/api.mustache\n@@ -133,7 +133,7 @@ import {{packageName}}.infrastructure.*\n \n         return RequestConfig(\n             method = RequestMethod.{{httpMethod}},\n-            path = \"{{path}}\",\n+            path = \"{{{path}}}\",\n             params = params,\n             query = localVariableQuery,\n             headers = localVariableHeaders,\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-webclient/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-webclient/api.mustache\nindex 556397463513..177a918b06ef 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-webclient/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-spring-webclient/api.mustache\n@@ -135,7 +135,7 @@ import {{packageName}}.infrastructure.*\n \n         return RequestConfig(\n             method = RequestMethod.{{httpMethod}},\n-            path = \"{{path}}\",\n+            path = \"{{{path}}}\",\n             params = params,\n             query = localVariableQuery,\n             headers = localVariableHeaders,\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-vertx/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-vertx/api.mustache\nindex c84f15f0b3fb..1ccca7f298f1 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-vertx/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/jvm-vertx/api.mustache\n@@ -136,7 +136,7 @@ import {{packageName}}.infrastructure.*\n     {{/isDeprecated}}\n     fun {{operationId}}WithHttpInfo({{#allParams}}{{{paramName}}}: {{#isEnum}}{{#isContainer}}kotlin.collections.List<{{enumName}}{{operationIdCamelCase}}>{{/isContainer}}{{^isContainer}}{{enumName}}{{operationIdCamelCase}}{{/isContainer}}{{/isEnum}}{{^isEnum}}{{{dataType}}}{{/isEnum}}{{^required}}?{{/required}}{{^-last}}, {{/-last}}{{/allParams}}) : Future<ApiResponse<{{#returnType}}{{{returnType}}}?{{/returnType}}{{^returnType}}Unit?{{/returnType}}>> {\n         val vertxClient = WebClient.create(vertx)\n-        val request = vertxClient.requestAbs(HttpMethod.{{httpMethod}}, UriTemplate.of(\"$basePath{{path}}\"{{#pathParams}}.replace(\"{\"+\"{{baseName}}\"+\"}\", encodeURIComponent({{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{{paramName}}}{{#isEnum}}{{^required}}?{{/required}}.value{{/isEnum}}.toString(){{/isContainer}})){{/pathParams}}))\n+        val request = vertxClient.requestAbs(HttpMethod.{{httpMethod}}, UriTemplate.of(\"$basePath{{{path}}}\"{{#pathParams}}.replace(\"{\"+\"{{baseName}}\"+\"}\", encodeURIComponent({{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{{paramName}}}{{#isEnum}}{{^required}}?{{/required}}.value{{/isEnum}}.toString(){{/isContainer}})){{/pathParams}}))\n \n         {{#hasFormParams}}request.putHeader(\"Content-Type\", {{^consumes}}\"multipart/form-data\"{{/consumes}}{{#consumes.0}}\"{{{mediaType}}}\"{{/consumes.0}}){{/hasFormParams}}\n         {{#headerParams}}{{{paramName}}}{{^required}}?{{/required}}.apply { request.putHeader(\"{{baseName}}\", {{#isContainer}}this.joinToString(separator = collectionDelimiter(\"{{collectionFormat}}\")){{/isContainer}}{{^isContainer}}this.toString(){{/isContainer}})}{{/headerParams}}\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/multiplatform/api.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/multiplatform/api.mustache\nindex 249f8edbb632..6566f40d9651 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/libraries/multiplatform/api.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/libraries/multiplatform/api.mustache\n@@ -105,7 +105,7 @@ import kotlinx.serialization.encoding.*\n \n         val localVariableConfig = RequestConfig<kotlin.Any?>(\n             RequestMethod.{{httpMethod}},\n-            \"{{path}}\"{{#pathParams}}.replace(\"{\" + \"{{baseName}}\" + \"}\", {{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{^isEnum}}\"${{{paramName}}}\"{{/isEnum}}{{#isEnum}}\"${ {{paramName}}.value }\"{{/isEnum}}{{/isContainer}}){{/pathParams}},\n+            \"{{{path}}}\"{{#pathParams}}.replace(\"{\" + \"{{baseName}}\" + \"}\", {{#isContainer}}{{paramName}}.joinToString(\",\"){{/isContainer}}{{^isContainer}}{{^isEnum}}\"${{{paramName}}}\"{{/isEnum}}{{#isEnum}}\"${ {{paramName}}.value }\"{{/isEnum}}{{/isContainer}}){{/pathParams}},\n             query = localVariableQuery,\n             headers = localVariableHeaders,\n             requiresAuthentication = {{#hasAuthMethods}}true{{/hasAuthMethods}}{{^hasAuthMethods}}false{{/hasAuthMethods}},\n", "instance_id": "OpenAPITools__openapi-generator-19937", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: single quotes in the URL path are being incorrectly encoded as `&#39;` in the generated Kotlin code by the OpenAPI Generator. The expected output and actual output are provided with code snippets, which helps in understanding the desired behavior. Additionally, the OpenAPI declaration file URL, generator version, and configuration details are included, which provide context for reproduction. However, there are minor ambiguities and missing details. For instance, the steps to reproduce the issue are not explicitly detailed (the section is left blank), and there is no mention of specific edge cases or constraints related to the URL path encoding beyond the single quote issue. While the problem goal is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to several factors. First, the scope of the code changes is relatively small and focused: the provided diff modifies template files (Mustache templates) in the OpenAPI Generator to handle the path string with triple curly braces `{{{path}}}` instead of double `{{path}}`, which prevents escaping of special characters like single quotes. This change spans multiple files (six template files for different Kotlin client libraries), but the modification in each file is straightforward and repetitive, requiring minimal understanding of the broader codebase architecture. Second, the technical concepts involved are basic: familiarity with Mustache templating and string escaping in code generation is sufficient, and no complex algorithms, design patterns, or domain-specific knowledge are required. Third, the problem does not explicitly mention edge cases beyond the single quote encoding, and the changes do not involve intricate error handling or performance considerations. Overall, this is a simple bug fix that requires understanding a specific templating quirk and applying a consistent change across a few files, making it an easy task for someone with moderate experience in code generation tools or template systems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Dynamic schema allows appending non-matching RangeIndexes\n### Describe the bug\r\n\r\nWhen dynamic schema is enabled it is possible to append dataframe with ranged index to symbol even if the ranges are not matching. This makes it possible to\r\n1. Append ranged index with start different than the stop of the symbol\r\n2. Append ranged index with step different than the step of the symbol\r\nBoth actions change the index even for the old rows and are forbidden with static_schema.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\n>>> import arcticdb\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> ac = arcticdb.Arctic(\"lmdb://test\")\r\n>>> opts = arcticdb.LibraryOptions(dynamic_schema=True)\r\n>>> lib = ac.get_library(\"test\", create_if_missing=True, library_options=opts)\r\n>>> lib.write(\"test\", pd.DataFrame({\"a\": [1, 2, 3]}))\r\nVersionedItem(symbol='test', library='test', data=n/a, version=0, metadata=None, host='LMDB(path=...test)', timestamp=1713515824376569000)\r\n>>> lib.read(\"test\").data\r\n   a\r\n0  1\r\n1  2\r\n2  3\r\n>>> lib.append(\"test\", pd.DataFrame({\"a\": [1]}, pd.RangeIndex(start=5, stop=6, step=1)))\r\nVersionedItem(symbol='test', library='test', data=n/a, version=1, metadata=None, host='LMDB(path=...test)', timestamp=1713515904432580000)\r\n>>> lib.read(\"test\").data\r\n   a\r\n5  1\r\n6  2\r\n7  3\r\n8  1\r\n>>> lib.append(\"test\", pd.DataFrame({\"a\": [1]}, pd.RangeIndex(start=5, stop=6, step=10)))\r\nVersionedItem(symbol='test', library='test', data=n/a, version=6, metadata=None, host='LMDB(path=...test)', timestamp=1713517240251682100)\r\n>>> lib.read(\"test\").data\r\n    a\r\n5   1\r\n15  2\r\n25  3\r\n35  1\r\n45  1\r\n```\r\n\r\n### Expected Results\r\n\r\nDo not allow appending RangeIndex-ed dataframes if the ranges are not matching.\r\n\r\n### OS, Python Version and ArcticDB Version\r\n\r\nPython: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\r\nOS: Windows-10-10.0.22631-SP0\r\nArcticDB: 4.4.0\r\nNumpy: 1.26.3\r\nPandas: 2.1.4\r\n\r\n### Backend storage used\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_\n", "patch": "diff --git a/cpp/arcticdb/python/normalization_checks.cpp b/cpp/arcticdb/python/normalization_checks.cpp\nindex 109c3b4acc..17584bc941 100644\n--- a/cpp/arcticdb/python/normalization_checks.cpp\n+++ b/cpp/arcticdb/python/normalization_checks.cpp\n@@ -122,17 +122,15 @@ void update_rowcount_normalization_data(\n             );\n \n             size_t new_start = new_index->start();\n-            if (new_start != 0) {\n-                auto stop = old_index->start() + old_length * old_index->step();\n-                normalization::check<ErrorCode::E_INCOMPATIBLE_INDEX>(\n-                    new_start == stop,\n-                    \"The appending data has a RangeIndex.start={} that is not contiguous with the {}\"\n-                    \"stop ({}) of\",\n-                    error_suffix,\n-                    new_start,\n-                    stop\n-                );\n-            }\n+            auto stop = old_index->start() + old_length * old_index->step();\n+            normalization::check<ErrorCode::E_INCOMPATIBLE_INDEX>(\n+                new_start == stop || (new_start == 0 && new_index->step() == 1),\n+                \"The appending data has a RangeIndex.start={} that is not contiguous with the \"\n+                \"stop ({}) of {}\",\n+                new_start,\n+                stop,\n+                error_suffix\n+            );\n \n             new_pandas->get().mutable_index()->set_start(old_index->start());\n         }\ndiff --git a/cpp/arcticdb/version/schema_checks.hpp b/cpp/arcticdb/version/schema_checks.hpp\nindex 9b81d54058..28a7e3ed1d 100644\n--- a/cpp/arcticdb/version/schema_checks.hpp\n+++ b/cpp/arcticdb/version/schema_checks.hpp\n@@ -74,9 +74,10 @@ inline void check_normalization_index_match(\n             // Having this will not allow appending RowCont indexed pd.DataFrames to DateTime indexed pd.DataFrames because they would\n             // have different field size (the rowcount index is not stored as a field). This logic is bug prone and will become better\n             // after we enable the empty index.\n+            const bool input_frame_is_series = frame.norm_meta.has_series();\n             normalization::check<ErrorCode::E_INCOMPATIBLE_INDEX>(\n                 common_index_type != IndexDescriptor::UNKNOWN ||\n-                    (old_idx_kind == IndexDescriptor::TIMESTAMP && new_idx_kind == IndexDescriptor::ROWCOUNT),\n+                    (input_frame_is_series && old_idx_kind == IndexDescriptor::TIMESTAMP && new_idx_kind == IndexDescriptor::ROWCOUNT),\n                 \"Cannot append {} index to {} index\",\n                 index_type_to_str(new_idx_kind),\n                 index_type_to_str(old_idx_kind)\n@@ -121,12 +122,9 @@ inline void fix_descriptor_mismatch_or_throw(\n     const auto &old_sd = existing_isr.tsd().as_stream_descriptor();\n     check_normalization_index_match(operation, old_sd, new_frame, empty_types);\n \n-    if (dynamic_schema)\n-        return; // TODO: dynamic schema may need some of the checks as below\n-\n     fix_normalization_or_throw(operation == APPEND, existing_isr, new_frame);\n \n-    if (!columns_match(old_sd, new_frame.desc)) {\n+    if (!dynamic_schema && !columns_match(old_sd, new_frame.desc)) {\n         throw StreamDescriptorMismatch(\n             \"The columns (names and types) in the argument are not identical to that of the existing version\",\n             StreamDescriptor{old_sd},\n", "instance_id": "man-group__ArcticDB-1529", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the bug related to appending non-matching RangeIndexes in a DataFrame when dynamic schema is enabled. It includes steps to reproduce the issue with example Python code, expected results, and relevant version information for the environment and libraries used (ArcticDB, Pandas, etc.). However, there are minor ambiguities: the problem statement does not explicitly define what constitutes a \"matching\" range (e.g., exact conditions for start, stop, and step compatibility), and it lacks detailed discussion of edge cases or potential side effects of the bug (e.g., impact on data integrity or performance). Additionally, while the expected result is stated as \"do not allow appending non-matching RangeIndex-ed DataFrames,\" it does not specify how the error should be communicated to the user (e.g., specific exception type or message). These minor gaps prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves modifications in two C++ files (`normalization_checks.cpp` and `schema_checks.hpp`), indicating a need to understand and alter logic in multiple parts of the codebase. The changes are not trivial; they involve conditional logic updates for index compatibility checks and schema handling, requiring a solid understanding of ArcticDB's internal normalization and schema management mechanisms. Second, the technical concepts involved include knowledge of C++ (specifically how ArcticDB handles DataFrame indices and schema validation), familiarity with Pandas' RangeIndex behavior, and domain-specific knowledge of data storage and versioning in ArcticDB. Third, the problem touches on edge cases like non-contiguous index ranges and differing step values, which are partially addressed in the code changes but require careful validation to ensure no new issues are introduced. However, the changes do not appear to impact the broader system architecture significantly, nor do they involve advanced algorithms or performance-critical optimizations, which prevents the difficulty from reaching the \"hard\" range. The overall amount of code change is moderate, focused on specific conditional checks, and does not require extensive refactoring. Therefore, a score of 0.55 reflects a medium difficulty level, requiring a decent understanding of the codebase and careful handling of index compatibility logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "yapf FileNotFoundError\n<!--\r\nWelcome to the PyInstaller issue tracker! Before creating an issue, please heed the following:\r\n\r\n1. This tracker should only be used to report bugs and request features / enhancements to PyInstaller\r\n    - For questions and general support, use the discussions forum.\r\n2. Use the search function before creating a new issue. Duplicates will be closed and directed to\r\n   the original discussion.\r\n3. When making a bug report, make sure you provide all required information. The easier it is for\r\n   maintainers to reproduce, the faster it'll be fixed.\r\n-->\r\n\r\n<!-- +++ ONLY TEXT +++ DO NOT POST IMAGES +++ -->\r\n\r\n## Description of the issue\r\nyapf is used in the groundingDINO project.\r\nUsing pyinstaller to build a onedir executable.\r\nyapf is not included in the onedir/_internal packages, and so module not found error is encountered on running the frozen application.\r\n\r\n### Context information (for bug reports)\r\n\r\n* Output of `pyinstaller --version`: ```6.10```\r\n* Version of Python: <!-- e.g. 3.11 --> 3.9.13\r\n* Platform: <!-- e.g GNU/Linux (distribution), Windows (language settings), OS X, FreeBSD --> Windows 11\r\n* How you installed Python: <!-- e.g. python.org/downloads, conda, brew, pyenv, apt, Windows store --> python.org/downloads\r\n* Did you also try this on another platform? Does it work there? Have not tried on another platform\r\n\r\nI have already tried adding yapf to hiddenimports.\r\n\r\n  * [x] start with clean installation: TRIED\r\n  * [ ] use the latest development version\r\n  * [x] Run your frozen program **from a command window (shell)** \u2014 instead of double-clicking on it: TRIED\r\n  * [x] Package your program in **--onedir mode**: TRIED\r\n  * [ ] Package **without UPX**, say: use the option `--noupx` or set `upx=False` in your .spec-file\r\n  * [x] Repackage you application in **verbose/debug mode**. For this, pass the option `--debug` to `pyi-makespec` or `pyinstaller` or use `EXE(..., debug=1, ...)` in your .spec file.: TRIED (that's how discovered the error msg)\r\n\r\nOn manually copy-pasting the yapf module from my python installation into the onedir/_internal directory, the application works as expected.\r\nSo this seems a problem of yapf not being recognised and exported properly.\r\n\r\nI'm using a .spec file to construct the executable.\r\nExe runs fine until the feature/piece of code that uses the yapf module is encountered.\r\n### A minimal example program which shows the error\r\n\r\n```\r\nmain.py:\r\nimport yapf\r\n\r\npyinstaller main.py --onedir\r\n```\r\nFileNotFoundError: No such file or directory: `path/to/dist/main/_internal/yapf_third_party/_ylib2to3/Grammar.txt`\r\n### Reported earlier in pyinstaller/pyinstaller#4939 in 2020 but closed as stale\n", "patch": "diff --git a/_pyinstaller_hooks_contrib/stdhooks/hook-yapf_third_party.py b/_pyinstaller_hooks_contrib/stdhooks/hook-yapf_third_party.py\nnew file mode 100644\nindex 00000000..d138c53b\n--- /dev/null\n+++ b/_pyinstaller_hooks_contrib/stdhooks/hook-yapf_third_party.py\n@@ -0,0 +1,15 @@\n+# ------------------------------------------------------------------\n+# Copyright (c) 2024 PyInstaller Development Team.\n+#\n+# This file is distributed under the terms of the GNU General Public\n+# License (version 2.0 or later).\n+#\n+# The full license is available in LICENSE, distributed with\n+# this software.\n+#\n+# SPDX-License-Identifier: GPL-2.0-or-later\n+# ------------------------------------------------------------------\n+\n+from PyInstaller.utils.hooks import collect_data_files\n+\n+datas = collect_data_files('yapf_third_party')\ndiff --git a/news/792.new.rst b/news/792.new.rst\nnew file mode 100644\nindex 00000000..112ddb38\n--- /dev/null\n+++ b/news/792.new.rst\n@@ -0,0 +1,2 @@\n+Add hook for ``yapf_third_party`` (part of ``yapf``) to collect its\n+data files.\n", "instance_id": "pyinstaller__pyinstaller-hooks-contrib-792", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: yapf, a dependency in the groundingDINO project, is not being included in the PyInstaller onedir executable, leading to a FileNotFoundError when running the frozen application. The context provided includes relevant details such as PyInstaller version, Python version, platform, and steps already attempted (e.g., adding yapf to hiddenimports, running in debug mode). It also provides a minimal reproducible example and references a previously reported issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly clarify the expected behavior of PyInstaller with respect to data files for third-party modules like yapf_third_party, nor does it discuss potential edge cases or constraints (e.g., compatibility with different Python or PyInstaller versions). Additionally, while the error message is provided, the exact cause (missing data files) is inferred rather than explicitly stated in the description. Overall, the statement is valid and mostly clear but lacks some minor details for full comprehensiveness.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are minimal and localized. The solution involves adding a new hook file (hook-yapf_third_party.py) with just a few lines of code to collect data files for the yapf_third_party module using PyInstaller's utility function. Additionally, a small news entry is added to document the change. The changes do not impact the broader architecture of PyInstaller or require modifications across multiple complex modules. The overall amount of code change is very small (less than 20 lines).\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of PyInstaller's hook mechanism and the collect_data_files utility, which are specific to PyInstaller's packaging process. While this is a niche area, it is not inherently complex for someone familiar with Python packaging or PyInstaller. No advanced algorithms, design patterns, or domain-specific knowledge beyond PyInstaller hooks are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement and code changes do not explicitly address edge cases or additional error handling. The solution focuses on ensuring data files are included, and there is no indication of complex edge cases (e.g., version-specific behavior or conflicts with other modules) that need to be handled. The error (FileNotFoundError) is straightforward and resolved by including the missing files.\n\n4. **Overall Complexity**: The problem requires understanding a specific tool (PyInstaller) and its hook system, but the fix is relatively simple and does not involve deep architectural changes or intricate logic. It is a typical bug fix in the context of Python packaging, requiring moderate familiarity with the tool rather than advanced technical expertise.\n\nA score of 0.35 reflects an easy problem that goes slightly beyond trivial due to the need to understand PyInstaller's hook system, but it remains a straightforward fix with minimal impact and complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "DisplayIO Example Wanted\nAdd a Basic DisplayIO Based Example\n\nWe would like to have a basic displayio example for this library. The example should be written for microcontrollers with a built-in display. At a minimum it should show a Label on the display and update it with live readings from the sensor. \n\nThe example should not be overly complex, it's intended to be a good starting point for displayio based projects that utilize this sensor library. Try to keep all visual content as near to the top left corner as possible in order to best fascilitate devices with small built-in display resolutions.\n\nThe new example should follow the naming convention `examples/libraryname_displayio_simpletest.py` with \"libraryname\" being replaced by the actual name of this library.\n\nYou can find an example of a Pull Request that adds this kind of example here: https://github.com/adafruit/Adafruit_CircuitPython_BME680/pull/72\n\nWe have a guide that covers the process of contributing with git and github: https://learn.adafruit.com/contribute-to-circuitpython-with-git-and-github\n\nIf you're interested in contributing but need additional help, or just want to say hi, feel free to join the Discord server to ask questions: https://adafru.it/discord\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 70ade69..4fdf32f 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -4,21 +4,24 @@\n \n repos:\n   - repo: https://github.com/python/black\n-    rev: 23.3.0\n+    rev: 24.10.0\n     hooks:\n       - id: black\n   - repo: https://github.com/fsfe/reuse-tool\n-    rev: v1.1.2\n+    rev: v4.0.3\n     hooks:\n       - id: reuse\n   - repo: https://github.com/pre-commit/pre-commit-hooks\n-    rev: v4.4.0\n+    rev: v5.0.0\n     hooks:\n       - id: check-yaml\n       - id: end-of-file-fixer\n       - id: trailing-whitespace\n+      - id: mixed-line-ending\n+        args:\n+         - --fix=lf\n   - repo: https://github.com/pycqa/pylint\n-    rev: v2.17.4\n+    rev: v3.3.1\n     hooks:\n       - id: pylint\n         name: pylint (library code)\n", "instance_id": "adafruit__Adafruit_CircuitPython_VL53L1X-19", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add a basic DisplayIO-based example for a sensor library targeting microcontrollers with built-in displays. It specifies the goal (display a label with live sensor readings), provides constraints (keep visual content near the top-left corner for small displays), and offers a naming convention for the example file. Additionally, it references a similar pull request and provides resources for contributors, which aids in understanding the expected outcome. However, there are minor ambiguities: the problem does not explicitly define the specific sensor library (\"libraryname\" is a placeholder), nor does it detail the expected format or structure of the code beyond a general description. Edge cases, such as handling display initialization failures or sensor reading errors, are not mentioned. These missing details prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The task involves creating a new example file rather than modifying existing codebase components, which limits the scope to a single, isolated addition. The provided diff in the code changes appears unrelated to the core task (it updates pre-commit hooks and tool versions), suggesting the actual implementation of the DisplayIO example is not shown here. Based on the problem statement, the code change is expected to be a standalone Python script, likely under 100 lines, with no impact on the broader system architecture.\n\n2. **Technical Concepts Required:** Solving this requires familiarity with Python, the CircuitPython framework, and the DisplayIO library for rendering text on a microcontroller display. Additionally, basic knowledge of sensor data reading (likely via an existing library API) is needed. These concepts are relatively straightforward for someone with experience in embedded systems or CircuitPython, though they may pose a slight learning curve for beginners unfamiliar with DisplayIO.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases or error handling requirements, such as handling sensor read failures or display initialization issues. However, a robust implementation might need to consider these, adding minor complexity. Given the goal of a \"basic\" example, extensive error handling is likely not required.\n\n4. **Overall Complexity:** The task involves writing a simple script to initialize a display, create a label, read sensor data, and update the display periodically. This requires understanding a few library APIs and basic event loops, but it does not involve complex algorithms, design patterns, or deep architectural changes. The referenced pull request suggests a pattern to follow, further reducing the difficulty.\n\nA score of 0.35 reflects an Easy task that requires understanding some specific library logic (DisplayIO and sensor interaction) and writing a small, self-contained script. It is slightly above the lower end of the Easy range due to the need to learn or recall specific CircuitPython APIs, but it remains a straightforward contribution for someone with moderate Python experience or familiarity with embedded programming.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Flink: Pre-create fieldGetters to avoid constructing them for each row\nI found that RowDataUtil.clone consumes a significant amount of CPU time by analyzing the flame graph. It constructs a FieldGetter object for each field of every row being processed. This commit optimizes this by constructing all FieldGetter objects upfront, avoiding repeated object creation.\n", "patch": "diff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\nindex c5cb51b7eae4..3a8f5ccc6c03 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\n@@ -79,22 +79,45 @@ public static Object convertConstant(Type type, Object value) {\n    * the arity check.\n    */\n   public static RowData clone(\n-      RowData from, RowData reuse, RowType rowType, TypeSerializer[] fieldSerializers) {\n+      RowData from,\n+      RowData reuse,\n+      RowType rowType,\n+      TypeSerializer[] fieldSerializers,\n+      RowData.FieldGetter[] fieldGetters) {\n     GenericRowData ret;\n     if (reuse instanceof GenericRowData) {\n       ret = (GenericRowData) reuse;\n     } else {\n       ret = new GenericRowData(from.getArity());\n     }\n+\n     ret.setRowKind(from.getRowKind());\n     for (int i = 0; i < rowType.getFieldCount(); i++) {\n       if (!from.isNullAt(i)) {\n-        RowData.FieldGetter getter = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n-        ret.setField(i, fieldSerializers[i].copy(getter.getFieldOrNull(from)));\n+        ret.setField(i, fieldSerializers[i].copy(fieldGetters[i].getFieldOrNull(from)));\n       } else {\n         ret.setField(i, null);\n       }\n     }\n+\n     return ret;\n   }\n+\n+  /**\n+   * @deprecated will be removed in 1.7.0; Not reusing FieldGetter in this method could lead to\n+   *     performance degradation, use {@link #clone(RowData, RowData, RowType, TypeSerializer[],\n+   *     RowData.FieldGetter[])} instead.\n+   */\n+  @Deprecated\n+  public static RowData clone(\n+      RowData from, RowData reuse, RowType rowType, TypeSerializer[] fieldSerializers) {\n+    RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[rowType.getFieldCount()];\n+    for (int i = 0; i < rowType.getFieldCount(); ++i) {\n+      if (!from.isNullAt(i)) {\n+        fieldGetters[i] = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n+      }\n+    }\n+\n+    return clone(from, reuse, rowType, fieldSerializers, fieldGetters);\n+  }\n }\ndiff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\nindex 1e265b2663ce..40d5c28d7bc7 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\n@@ -28,10 +28,12 @@\n class RowDataRecordFactory implements RecordFactory<RowData> {\n   private final RowType rowType;\n   private final TypeSerializer[] fieldSerializers;\n+  private final RowData.FieldGetter[] fieldGetters;\n \n   RowDataRecordFactory(RowType rowType) {\n     this.rowType = rowType;\n     this.fieldSerializers = createFieldSerializers(rowType);\n+    this.fieldGetters = createFieldGetters(rowType);\n   }\n \n   static TypeSerializer[] createFieldSerializers(RowType rowType) {\n@@ -40,6 +42,15 @@ static TypeSerializer[] createFieldSerializers(RowType rowType) {\n         .toArray(TypeSerializer[]::new);\n   }\n \n+  static RowData.FieldGetter[] createFieldGetters(RowType rowType) {\n+    RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[rowType.getFieldCount()];\n+    for (int i = 0; i < rowType.getFieldCount(); ++i) {\n+      fieldGetters[i] = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n+    }\n+\n+    return fieldGetters;\n+  }\n+\n   @Override\n   public RowData[] createBatch(int batchSize) {\n     RowData[] arr = new RowData[batchSize];\n@@ -56,6 +67,7 @@ public void clone(RowData from, RowData[] batch, int position) {\n     // Clone method will allocate a new GenericRowData object\n     // if the target object is NOT a GenericRowData.\n     // So we should always set the clone return value back to the array.\n-    batch[position] = RowDataUtil.clone(from, batch[position], rowType, fieldSerializers);\n+    batch[position] =\n+        RowDataUtil.clone(from, batch[position], rowType, fieldSerializers, fieldGetters);\n   }\n }\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\nindex c5cb51b7eae4..3a8f5ccc6c03 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/RowDataUtil.java\n@@ -79,22 +79,45 @@ public static Object convertConstant(Type type, Object value) {\n    * the arity check.\n    */\n   public static RowData clone(\n-      RowData from, RowData reuse, RowType rowType, TypeSerializer[] fieldSerializers) {\n+      RowData from,\n+      RowData reuse,\n+      RowType rowType,\n+      TypeSerializer[] fieldSerializers,\n+      RowData.FieldGetter[] fieldGetters) {\n     GenericRowData ret;\n     if (reuse instanceof GenericRowData) {\n       ret = (GenericRowData) reuse;\n     } else {\n       ret = new GenericRowData(from.getArity());\n     }\n+\n     ret.setRowKind(from.getRowKind());\n     for (int i = 0; i < rowType.getFieldCount(); i++) {\n       if (!from.isNullAt(i)) {\n-        RowData.FieldGetter getter = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n-        ret.setField(i, fieldSerializers[i].copy(getter.getFieldOrNull(from)));\n+        ret.setField(i, fieldSerializers[i].copy(fieldGetters[i].getFieldOrNull(from)));\n       } else {\n         ret.setField(i, null);\n       }\n     }\n+\n     return ret;\n   }\n+\n+  /**\n+   * @deprecated will be removed in 1.7.0; Not reusing FieldGetter in this method could lead to\n+   *     performance degradation, use {@link #clone(RowData, RowData, RowType, TypeSerializer[],\n+   *     RowData.FieldGetter[])} instead.\n+   */\n+  @Deprecated\n+  public static RowData clone(\n+      RowData from, RowData reuse, RowType rowType, TypeSerializer[] fieldSerializers) {\n+    RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[rowType.getFieldCount()];\n+    for (int i = 0; i < rowType.getFieldCount(); ++i) {\n+      if (!from.isNullAt(i)) {\n+        fieldGetters[i] = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n+      }\n+    }\n+\n+    return clone(from, reuse, rowType, fieldSerializers, fieldGetters);\n+  }\n }\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\nindex 1e265b2663ce..40d5c28d7bc7 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataRecordFactory.java\n@@ -28,10 +28,12 @@\n class RowDataRecordFactory implements RecordFactory<RowData> {\n   private final RowType rowType;\n   private final TypeSerializer[] fieldSerializers;\n+  private final RowData.FieldGetter[] fieldGetters;\n \n   RowDataRecordFactory(RowType rowType) {\n     this.rowType = rowType;\n     this.fieldSerializers = createFieldSerializers(rowType);\n+    this.fieldGetters = createFieldGetters(rowType);\n   }\n \n   static TypeSerializer[] createFieldSerializers(RowType rowType) {\n@@ -40,6 +42,15 @@ static TypeSerializer[] createFieldSerializers(RowType rowType) {\n         .toArray(TypeSerializer[]::new);\n   }\n \n+  static RowData.FieldGetter[] createFieldGetters(RowType rowType) {\n+    RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[rowType.getFieldCount()];\n+    for (int i = 0; i < rowType.getFieldCount(); ++i) {\n+      fieldGetters[i] = RowData.createFieldGetter(rowType.getTypeAt(i), i);\n+    }\n+\n+    return fieldGetters;\n+  }\n+\n   @Override\n   public RowData[] createBatch(int batchSize) {\n     RowData[] arr = new RowData[batchSize];\n@@ -56,6 +67,7 @@ public void clone(RowData from, RowData[] batch, int position) {\n     // Clone method will allocate a new GenericRowData object\n     // if the target object is NOT a GenericRowData.\n     // So we should always set the clone return value back to the array.\n-    batch[position] = RowDataUtil.clone(from, batch[position], rowType, fieldSerializers);\n+    batch[position] =\n+        RowDataUtil.clone(from, batch[position], rowType, fieldSerializers, fieldGetters);\n   }\n }\n", "instance_id": "apache__iceberg-10676", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to optimize performance by pre-creating `FieldGetter` objects to avoid repeated construction for each row in a Flink-based data processing system. It identifies the performance bottleneck (via flame graph analysis) and briefly describes the solution. However, it lacks critical details such as specific input/output expectations, constraints on the data or system (e.g., row sizes, field types), and any potential edge cases or risks associated with the optimization. Additionally, there are no examples or test cases provided to illustrate the impact of the change. While the goal is understandable, these missing details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves modifications across multiple files (`RowDataUtil.java` and `RowDataRecordFactory.java`) and versions (Flink v1.18 and v1.19), requiring an understanding of how these components interact within the Apache Iceberg and Flink ecosystems. The changes include adding a new method signature, pre-creating `FieldGetter` arrays, and maintaining backward compatibility via a deprecated method, which adds moderate complexity. Second, the technical concepts involved include familiarity with Flink's `RowData` API, `TypeSerializer`, and performance optimization techniques (e.g., object reuse to reduce CPU overhead), which are not trivial but also not highly advanced. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential considerations, such as handling null fields and ensuring compatibility with existing code that might not use the new method signature. However, the overall impact on the system's architecture is limited, as this is primarily a performance optimization rather than a structural change. The amount of code change is moderate, with logical additions rather than deep refactoring. Therefore, a difficulty score of 0.45 reflects the need for a solid understanding of the codebase and moderate complexity in implementation, without reaching the level of hard or very hard challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[change] Update subnet division docs link\nOnce we will publish the [new comprehensive documentation website](https://github.com/openwisp/openwisp2-docs/pull/189), the link in the subnet division rule admin web UI has to be updated (it now points to the openwisp-controller README):\r\n\r\n![Screenshot from 2024-05-29 14-09-39](https://github.com/openwisp/openwisp-controller/assets/841044/6f297797-655e-4093-9b2d-4cc5fc0e6349)\r\n\n", "patch": "diff --git a/openwisp_controller/subnet_division/admin.py b/openwisp_controller/subnet_division/admin.py\nindex 131ae3076..b1182d501 100644\n--- a/openwisp_controller/subnet_division/admin.py\n+++ b/openwisp_controller/subnet_division/admin.py\n@@ -32,8 +32,8 @@ class SubnetDivisionRuleInlineAdmin(\n             '\"Number of IPs\" will not be possible.'\n         ),\n         'documentation_url': (\n-            'https://github.com/openwisp/openwisp-controller'\n-            '#limitations-of-subnet-division'\n+            'https://openwisp.io/docs/dev/controller/user/subnet-division-rules.html'\n+            '#limitations-of-subnet-division-rules'\n         ),\n     }\n \n", "instance_id": "openwisp__openwisp-controller-911", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes the need to update a documentation link in the subnet division rule admin web UI to point to a new comprehensive documentation website. The inclusion of a screenshot provides additional context about where the link appears in the UI, which is helpful. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly confirm whether the new link is already live or if there are any dependencies (e.g., waiting for the documentation website to be published). Additionally, it lacks explicit mention of any potential impact on users or other parts of the system due to this change. Despite these minor gaps, the goal and the required action are evident from the description and the provided code changes.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a straightforward update of a hardcoded URL in a single file. The code change is minimal, affecting only two lines in the `subnet_division/admin.py` file, and does not require understanding complex logic, interactions with other parts of the codebase, or any advanced technical concepts. No algorithms, design patterns, or domain-specific knowledge are needed beyond basic familiarity with Python and string literals. There are no edge cases or error handling requirements mentioned or implied in the problem statement or code changes. The task does not impact the system's architecture or performance and is essentially a cosmetic update. Therefore, I assign a difficulty score of 0.1, placing it in the \"very easy\" category, as it is a simple modification akin to fixing a typo or updating a constant.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add file py.typed to enable mypy to find project specific types\nCreate empty file `py.typed` in directory `exasol_integration_test_docker_environment` to enable mypy to find project specific types, see https://blog.whtsky.me/tech/2021/dont-forget-py.typed-for-your-typed-python-package/\r\n\n", "patch": "diff --git a/doc/changes/changelog.md b/doc/changes/changelog.md\nindex ec5251007..9c207e7c8 100644\n--- a/doc/changes/changelog.md\n+++ b/doc/changes/changelog.md\n@@ -1,5 +1,6 @@\n # Changes\n \n+* [3.3.0](changes_3.3.0.md)\n * [3.2.0](changes_3.2.0.md)\n * [3.1.0](changes_3.1.0.md)\n * [3.0.0](changes_3.0.0.md)\n@@ -34,6 +35,7 @@\n hidden:\n ---\n changes_3.2.0\n+changes_3.3.0\n changes_3.1.0\n changes_3.0.0\n changes_2.1.0\ndiff --git a/doc/changes/changes_3.3.0.md b/doc/changes/changes_3.3.0.md\nnew file mode 100644\nindex 000000000..8f1aad633\n--- /dev/null\n+++ b/doc/changes/changes_3.3.0.md\n@@ -0,0 +1,10 @@\n+# Integration-Test-Docker-Environment 3.3.0, released 2024-??-??\n+\n+Code name:\n+\n+## Summary\n+\n+## Refactorings\n+\n+* #119: Refactored `pkg_resources` usage to `importlib.resources`\n+* #420: Added file `py.typed` to enable mypy to find project specific types\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 8e2794ede..f0a331c4a 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -3,7 +3,7 @@ name = \"exasol-integration-test-docker-environment\"\n packages = [\n     { include = \"exasol_integration_test_docker_environment\" },\n ]\n-version = \"3.2.0\"\n+version = \"3.3.0\"\n description = \"Integration Test Docker Environment for Exasol\"\n \n license = \"MIT\"\n@@ -44,7 +44,7 @@ fabric = \"^3.0.1\"\n portalocker = \"^2.7.0\"\n exasol-error-reporting = \"^0.4.0\"\n # The current combination of dependencies for ITDE and Luigi is not compatible with projects that run on Python 3.8.\n-# The latest version of docutils, version 0.21.1, is required to run on Python 3.9 or higher. \n+# The latest version of docutils, version 0.21.1, is required to run on Python 3.9 or higher.\n # As a temporary fix, until support for Python 3.8 is dropped (which is estimated to be around 6 months),\n # we are explicitly requiring a version of docutils that is less than or equal to 0.20.1 in ITDE.\n # Once support for Python 3.8 is dropped, this dependency can be removed as it is only needed as a transitive dependency.\n", "instance_id": "exasol__integration-test-docker-environment-419", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: it specifies the task of adding an empty `py.typed` file to a specific directory to enable mypy to find project-specific types, and it provides a reference link for further context. However, it lacks some minor details, such as an explanation of why this file is necessary beyond the reference link, or any potential implications of adding this file (e.g., whether it affects other tools or workflows). Additionally, there are no examples or explicit mentions of how this change integrates with the rest of the project. Despite these minor omissions, the goal is straightforward and understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is very low, as it involves minimal code changes and no complex logic. The primary task is to create an empty `py.typed` file, which is a trivial operation. The provided code changes also include updates to the changelog, version number in `pyproject.toml`, and documentation, which are standard housekeeping tasks associated with a release or minor update. These changes are confined to a few files and do not require deep understanding of the codebase, advanced technical concepts, or handling of edge cases. There are no significant architectural impacts or interactions with other modules to consider. The task requires only basic familiarity with Python packaging and version control practices, making it fall into the very easy category with a score of 0.1.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Inconsistent results when using ORDER BY with PIVOT\n### What happens?\r\n\r\nPIVOT statement does not look to correctly handle ORDER BY clause.\r\nResults sometimes sorted correctly and sometimes not. \r\nRunning PIVOT multiple times produces different results\r\n\r\n### To Reproduce\r\n\r\n```sql\r\ncreate table p (col1 timestamp, col2 int);\r\ninsert into p values ('2024-12-04 09:30:01', 100);\r\ninsert into p values ('2024-12-04 09:30:02', 100);\r\ninsert into p values ('2024-12-04 09:30:03', 100);\r\ninsert into p values ('2024-12-04 09:30:04', 100);\r\n\r\npivot p using sum (col2)\r\ngroup by col1\r\norder by col1;\r\n```\r\n\r\n### OS:\r\n\r\nWindows\r\n\r\n### DuckDB Version:\r\n\r\n1.1.3\r\n\r\n### DuckDB Client:\r\n\r\nbuild from sourcce\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nAlex G\r\n\r\n### Affiliation:\r\n\r\nFintech\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/parser/transform/statement/transform_pivot_stmt.cpp b/src/parser/transform/statement/transform_pivot_stmt.cpp\nindex 8b798a83c9fb..94bafd72f203 100644\n--- a/src/parser/transform/statement/transform_pivot_stmt.cpp\n+++ b/src/parser/transform/statement/transform_pivot_stmt.cpp\n@@ -163,6 +163,8 @@ unique_ptr<QueryNode> Transformer::TransformPivotStatement(duckdb_libpgquery::PG\n \t\tif (pivot->aggrs) {\n \t\t\tTransformExpressionList(*pivot->aggrs, select_node->select_list);\n \t\t}\n+\t\t// transform order by/limit modifiers\n+\t\tTransformModifiers(select, *select_node);\n \t\treturn std::move(select_node);\n \t}\n \n", "instance_id": "duckdb__duckdb-15155", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the PIVOT statement in DuckDB does not consistently handle the ORDER BY clause, leading to inconsistent sorting results when run multiple times. The statement includes a reproducible SQL example, which is helpful, along with relevant details about the OS, DuckDB version, and client build. However, there are minor ambiguities and missing details. For instance, the expected output or behavior of the ORDER BY clause with PIVOT is not explicitly stated (e.g., should it always sort ascending or descending?). Additionally, there is no mention of specific edge cases or constraints that might influence the inconsistency (e.g., data types, large datasets, or specific configurations). While the issue is reproducible, the lack of clarity on the desired outcome and potential edge cases prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal, as shown in the diff, which involves adding a single line to call `TransformModifiers` to handle ORDER BY and LIMIT clauses in the PIVOT statement transformation. This suggests the fix is localized to a specific part of the parser logic in `transform_pivot_stmt.cpp` and does not impact multiple modules or the broader system architecture. Second, the technical concepts required are relatively straightforward: understanding SQL parser transformations and how modifiers like ORDER BY are handled in DuckDB's query processing pipeline. This does not require advanced algorithms, design patterns, or domain-specific knowledge beyond familiarity with database query parsing. Third, the problem statement and code change do not explicitly address complex edge cases or error handling requirements, though there might be implicit considerations (e.g., ensuring ORDER BY works with various data types or PIVOT configurations). Overall, solving this issue requires understanding some code logic and making a simple modification, but it does not demand deep architectural changes or advanced expertise, justifying a score of 0.30.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Inconsistent Rounding Behavior When Casting String to DECIMAL\n### What happens?\r\n\r\nWhen casting a string to DECIMAL with various precisions and scales, DuckDB exhibits inconsistent rounding behavior. Specifically, casting the string `'1.0029999999999999'` to DECIMAL types with different precisions and scales sometimes results in unexpected rounding, e.g., `1.0031` instead of the anticipated `1.0030`.\r\nIn contrast, PostgreSQL consistently produces expected `1.0030`.\r\n\r\nI've tested on two duckdb environments, both produce the same results:\r\n1. execute duckdb raw sql in Python environment, with both stable version `0.10.1` and latest build `0.10.2-dev311`;\r\n2. execute duckdb raw sql in duckdb-wasm at https://shell.duckdb.org/\r\n\r\n### To Reproduce\r\n\r\nExecute the following queries in DuckDB:\r\n\r\n```sql\r\nSELECT '1.0029999999999999'::DECIMAL(38, 4);  -- Expected: 1.0030,  Actual: 1.0030,  GOOD\r\nSELECT '1.0029999999999999'::DECIMAL(10, 4);  -- Expected: 1.0030,  Actual: 1.0031,  BAD\r\nSELECT '1.0029999999999999'::DECIMAL( 5, 4);  -- Expected: 1.0030,  Actual: 1.0030,  GOOD\r\nSELECT '1.0029999999999999'::DECIMAL(19, 4);  -- Expected: 1.0030,  Actual: 1.0030,  GOOD\r\nSELECT '1.0029999999999999'::DECIMAL(18, 4);  -- Expected: 1.0030,  Actual: 1.0031,  BAD\r\nSELECT '1.002999999999999' ::DECIMAL(10, 4);  -- Expected: 1.0030,  Actual: 1.0030,  GOOD\r\nSELECT '1.0029999999999999'::DECIMAL(10, 5);  -- Expected: 1.00300, Actual: 1.00301, BAD\r\nSELECT '1.00299999999999'  ::DECIMAL(10, 4);  -- Expected: 1.0030,  Actual: 1.0030,  GOOD\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 22.04.4 LTS on Windows 10 x86_64 (WSL2)\r\n\r\n### DuckDB Version:\r\n\r\n0.10.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nJunjie Wang\r\n\r\n### Affiliation:\r\n\r\ndsm-firmenich\r\n\r\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\r\n\r\nI have tested with a nightly build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/function/cast/decimal_cast.cpp b/src/function/cast/decimal_cast.cpp\nindex 7f9f2b493518..21f80bf0475a 100644\n--- a/src/function/cast/decimal_cast.cpp\n+++ b/src/function/cast/decimal_cast.cpp\n@@ -155,7 +155,7 @@ struct DecimalScaleDownCheckOperator {\n \t\t\t                                data->result.GetType().ToString());\n \t\t\treturn HandleVectorCastError::Operation<RESULT_TYPE>(std::move(error), mask, idx, data->vector_cast_data);\n \t\t}\n-\t\treturn Cast::Operation<INPUT_TYPE, RESULT_TYPE>(input / data->factor);\n+\t\treturn DecimalScaleDownOperator::Operation<INPUT_TYPE, RESULT_TYPE>(input, mask, idx, dataptr);\n \t}\n };\n \n", "instance_id": "duckdb__duckdb-14860", "clarity": 3, "difficulty": 0.75, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue of inconsistent rounding behavior when casting strings to DECIMAL types in DuckDB, with specific examples of expected versus actual outputs for various precision and scale combinations. The statement includes detailed steps to reproduce the issue, specifies the environments and versions tested, and compares the behavior with PostgreSQL for context. Additionally, it provides relevant metadata such as OS, DuckDB version, and client used. There are no significant ambiguities, and the problem goal (fixing inconsistent rounding) is evident. The provided SQL queries serve as concrete test cases, making the requirements and expected behavior very clear.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code change appears focused on a specific part of the codebase (decimal casting logic in `decimal_cast.cpp`), but the provided diff suggests a modification to how scaling down operations are handled, which likely requires a deep understanding of DuckDB's internal decimal representation and arithmetic operations. Second, the problem involves nuanced numerical precision and rounding behavior, which is inherently complex and requires knowledge of floating-point arithmetic, decimal type implementations, and potentially domain-specific standards for decimal rounding (e.g., aligning with PostgreSQL's behavior). Third, the number of technical concepts involved is significant, including C++ intricacies, DuckDB's type system, and possibly custom operators for decimal manipulation. Fourth, edge cases are implicitly critical here\u2014handling various precision and scale combinations, as shown in the test queries, indicates that the solution must account for a wide range of inputs to ensure consistent rounding behavior. While the code change itself appears small, the impact on the system's correctness is high, and debugging or validating the fix (e.g., ensuring no regressions across other decimal operations) adds to the difficulty. This problem requires a deep understanding of the codebase and careful consideration of numerical accuracy, justifying a score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Failed to create prepared statement for insert-statement after called ToString method\n### What happens?\r\n\r\n`duckdb::Connection` has `Prepare` method accepting `duckdb::SQLStatement`\r\nCalling `Prepare` method after `duckdb::SQLStatement::ToString` led to throwing an exception: \r\n\r\n> GetUsingBinding: expected non-empty binding_name.\r\n\r\nit seems to forget reverting `alias` property.\r\nhttps://github.com/duckdb/duckdb/blob/45559f5eeb1834454a30490fc4ffad1807e13f3b/src/parser/statement/insert_statement.cpp#L99-L103\r\n\r\nIs this expected feature?\r\n\r\n### To Reproduce\r\n\r\n## Code\r\n\r\n```cpp\r\nauto db = duckdb::Duckdb(nullptr);\r\nauto conn = duckdb::Connection(db);\r\n\r\n// It supporses to exist `Foo` table\r\n// `Foo` table is `CREATE TABLE Foo (id int primary key, name varchar, age int)`\r\nstd::string sql(\"INSERT INTO Foo values (1, 'qwerty', 42)\";\r\n\r\n// get parsed statements\r\nauto stmts = conn.ExtractStatements(sql);\r\nauto &stmt = stmts[0];\r\n\r\n// Call Tostring\r\nstmt->ToString();\r\n\r\n// Call prepare\r\nauto prepared_stmt = conn.Prepare(stmt); // <- `duckdb::InternalException` is throws here\r\n```\r\n\r\n## Expected result\r\n`duckdb::unique_ptr<duckdb::PreparedStatement>` is returned successfully.\r\n\r\n## Actual result\r\n`duckdb::InternalException` is throws\r\nfrom https://github.com/duckdb/duckdb/blob/45559f5eeb1834454a30490fc4ffad1807e13f3b/src/planner/bind_context.cpp#L100\r\n\r\n## Workaround\r\n\r\n```cpp\r\nstmt->Copy()->ToString();\r\nauto prepared_stmt = conn.Prepare(stmt);\r\n```\r\n\r\n### OS:\r\n\r\nMacOS Ventura 13.6.7 (x86_64 16GB RAM)\r\n\r\n### DuckDB Version:\r\n\r\n1.1.0\r\n\r\n### DuckDB Client:\r\n\r\nC++\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nKazuhiko TAMURA\r\n\r\n### Affiliation:\r\n\r\nFree\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex b162b27a54bb..76d7d141eeec 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -124,6 +124,7 @@ duckdb_extension_load(sqlsmith\n         DONT_LINK LOAD_TESTS\n         GIT_URL https://github.com/duckdb/duckdb_sqlsmith\n         GIT_TAG f24be8b5b0cd0eeed7541e10cff42d7050771afc\n+        APPLY_PATCHES\n         )\n \n ################# SUBSTRAIT\ndiff --git a/.github/patches/extensions/sqlsmith/insert_statement_to_string.patch b/.github/patches/extensions/sqlsmith/insert_statement_to_string.patch\nnew file mode 100644\nindex 000000000000..30845ddf62ce\n--- /dev/null\n+++ b/.github/patches/extensions/sqlsmith/insert_statement_to_string.patch\n@@ -0,0 +1,13 @@\n+diff --git a/test/sql/sql_reduce.test b/test/sql/sql_reduce.test\n+index 428ae92..af0530f 100644\n+--- a/test/sql/sql_reduce.test\n++++ b/test/sql/sql_reduce.test\n+@@ -48,7 +48,7 @@ INSERT INTO tbl (VALUES (1))\n+ INSERT INTO tbl (VALUES (1, 2))\n+ INSERT INTO tbl (VALUES (2))\n+ INSERT INTO tbl SELECT *\n+-INSERT INTO tbl SELECT NULL FROM (VALUES (1, 2))\n++INSERT INTO tbl SELECT NULL FROM (VALUES (1, 2)) AS valueslist\n+ INSERT INTO tbl SELECT NULL FROM (VALUES (1, 2)) AS valueslist\n+ \n+ query I\ndiff --git a/src/parser/statement/insert_statement.cpp b/src/parser/statement/insert_statement.cpp\nindex 4255ebb97b61..4035632975fd 100644\n--- a/src/parser/statement/insert_statement.cpp\n+++ b/src/parser/statement/insert_statement.cpp\n@@ -98,8 +98,10 @@ string InsertStatement::ToString() const {\n \tauto values_list = GetValuesList();\n \tif (values_list) {\n \t\tD_ASSERT(!default_values);\n+\t\tauto saved_alias = values_list->alias;\n \t\tvalues_list->alias = string();\n \t\tresult += values_list->ToString();\n+\t\tvalues_list->alias = saved_alias;\n \t} else if (select_statement) {\n \t\tD_ASSERT(!default_values);\n \t\tresult += select_statement->ToString();\n", "instance_id": "duckdb__duckdb-14171", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: calling `ToString()` on a `duckdb::SQLStatement` object before `Prepare()` results in an exception due to a failure in reverting the `alias` property. The goal (fixing the exception), input (a specific sequence of method calls), output (successful preparation of the statement), and reproduction steps are provided with a clear code example. Additionally, the actual and expected results are explicitly stated, and relevant environment details (OS, DuckDB version, client language) are included. However, there are minor ambiguities: the problem statement does not fully explain the root cause beyond a vague reference to the `alias` property, nor does it discuss potential edge cases or constraints (e.g., whether this issue affects only `INSERT` statements or other statement types). These missing details prevent it from being comprehensive, but the statement is still actionable and clear enough for a developer to understand the issue.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.35, placing it in the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single file (`insert_statement.cpp`) and involve a small, targeted modification\u2014saving and restoring the `alias` property of a `values_list` object during the `ToString()` operation. The diff shows only a few lines of code added, and there is no impact on the broader system architecture or multiple modules. Additionally, a minor test file update is included to adjust a query alias, which is trivial.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of C++ (object manipulation, string handling) and a minimal grasp of DuckDB's internal parser logic (specifically how `ToString()` interacts with statement properties like `alias`). No advanced algorithms, design patterns, or domain-specific knowledge beyond the immediate context of the DuckDB parser are needed. The concept of preserving state during method calls is straightforward for an experienced developer.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code change does not introduce new error handling logic. The fix is a simple state preservation mechanism, and while there might be implicit edge cases (e.g., different types of statements or empty `values_list`), they are not addressed in the provided diff or problem description. The complexity of edge cases appears minimal at this stage.\n\n4. **Overall Complexity**: The issue is a bug fix that requires understanding a specific interaction in the DuckDB parser but does not demand deep knowledge of the entire codebase or complex refactoring. The solution is a small, logical adjustment to prevent state loss, making it accessible to developers with moderate experience in C++ and familiarity with the DuckDB codebase.\n\nThis problem falls on the higher end of \"Easy\" due to the need for some contextual understanding of DuckDB's parser internals, but it remains a relatively simple fix without significant architectural or systemic impact.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Binder error when manually materializing a CTE\n### What happens?\n\nThe new automatic CTE materialization mechanism clashes with manually requesting a CTE to be materialized.\r\n\r\n\n\n### To Reproduce\n\nRun\r\n\r\n```sql\r\ncreate table batch (\r\n    entity text,\r\n    start_ts timestamptz,\r\n    duration interval\r\n)\r\n;\r\n\r\ncreate table active_events (\r\n    entity text,\r\n    start_ts timestamptz,\r\n    end_ts timestamptz\r\n)\r\n;\r\n\r\ncreate table active_events as\r\nwith\r\n    new_events as materialized (  -- Does not make much sense in this example, but my original query was a union of a bunch of things\r\n        select\r\n            entity,\r\n            start_ts as start_ts,\r\n            start_ts + duration as end_ts\r\n        from\r\n            batch\r\n    ), events_to_deduplicate as (\r\n        select\r\n            *\r\n        from  -- We need to deduplicate events for old entites with new data\r\n            active_events old\r\n            semi join new_events new on\r\n                old.entity = new.entity\r\n        union all\r\n        select *\r\n        from\r\n            new_events\r\n    ), combined_deduplicated_events as (\r\n        select\r\n            entity,\r\n            min(start_ts) as start_ts,\r\n            max(end_ts) as end_ts\r\n        from\r\n            events_to_deduplicate\r\n        group by\r\n            entity\r\n    ), all_events as (\r\n        select\r\n            entity,\r\n            start_ts,\r\n            end_ts\r\n        from\r\n            combined_deduplicated_events\r\n        where\r\n            end_ts > start_ts\r\n        union all\r\n        select\r\n            *\r\n        from  -- The data we did not need to deduplicate\r\n            active_events old\r\n            anti join new_events new on\r\n                old.entity = new.entity\r\n    )\r\nselect\r\n    *\r\nfrom\r\n    combined_leases\r\n;\r\n```\r\n\r\nand observe a Binder Error: Duplicate alias \"new_events\" in query!\r\n\r\nRemoving the `materialized` clause makes the query pass.\r\n\r\n\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\nv1.1.0 fa5c2fe15f\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nArista Networks\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\nINTERNAL Error: Attempted to dereference unique_ptr that is NULL\n### What happens?\n\nThe below test case (after some seconds creduce) causes an INTERNAL error in DuckDB. This [test case](https://github.com/user-attachments/files/17001454/backup.txt) before further reduced could sometimes cause segmentation fault.\r\n\n\n### To Reproduce\n\n```sql\r\nCREATE TABLE t0(c0 TIMESTAMP, c1 VARCHAR[]);\r\nCREATE TABLE t1(c0 FLOAT, c1 TIMESTAMP, c2 FLOAT);\r\nINSERT INTO t0 VALUES('2023-10-10 00:00:00+00:00', NULL);\r\nINSERT INTO t0 VALUES('2025-12-25 12:00:00+02:00', []), ('2004-07-27 10:00:00+02', []);\r\nINSERT INTO t0(c1, c0) VALUES([], '2023-01-01 00:00:00+00:00'), ([], '2021-01-01 00:00:00+01');\r\nINSERT INTO t0(c1, c0) VALUES([], '2021-01-01 00:00:00+00');\r\nINSERT INTO t1 VALUES(2.71, '1999-12-31 23:59:59', 1.41421356237);\r\nINSERT INTO t1 VALUES(1.61803, '1970-01-01 00:00:00', 1.61803);\r\nINSERT INTO t1(c0) VALUES(1064961652.34), (123.45);\r\nINSERT INTO t1(c0) VALUES('514332609.12');\r\nINSERT INTO t1(c0, c2, c1) VALUES(2.71828, 2.345, '1995-05-23 08:45:00'), ('1308880808', 12.34, '2021-01-01 15:30:45');\r\nINSERT INTO t1(c0) VALUES(92857950), (840458867);\r\nINSERT INTO t1 VALUES('3.14', '1999-12-31 23:59:59', 3.1415);\r\n\r\nSELECT * FROM t0 RIGHT JOIN t1 ON(CAST(t1.c1 AS TIMESTAMP) BETWEEN t0.c0 AND t0.c0);\r\n-- INTERNAL Error: Attempted to dereference unique_ptr that is NULL! This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic. For more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\n\n### OS:\n\nUbuntu 22.04\n\n### DuckDB Version:\n\nv1.1.1-dev122 b369bcb4e0\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nSuyang Zhong\n\n### Affiliation:\n\nNUS\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/extension/jemalloc/jemalloc/README.md b/extension/jemalloc/jemalloc/README.md\nindex 292d2e4bbae6..9bf08821e76a 100644\n--- a/extension/jemalloc/jemalloc/README.md\n+++ b/extension/jemalloc/jemalloc/README.md\n@@ -81,9 +81,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/jemalloc/jemalloc/src/jemalloc.c b/extension/jemalloc/jemalloc/src/jemalloc.c\nindex bac0880f9deb..9d57c2e42e73 100644\n--- a/extension/jemalloc/jemalloc/src/jemalloc.c\n+++ b/extension/jemalloc/jemalloc/src/jemalloc.c\n@@ -4277,9 +4277,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/json/include/json_common.hpp b/extension/json/include/json_common.hpp\nindex bcc8d7c23db7..872058023c22 100644\n--- a/extension/json/include/json_common.hpp\n+++ b/extension/json/include/json_common.hpp\n@@ -241,12 +241,16 @@ struct JSONCommon {\n \t};\n \n \t//! Get JSON value using JSON path query (safe, checks the path query)\n-\tstatic inline yyjson_val *Get(yyjson_val *val, const string_t &path_str) {\n+\tstatic inline yyjson_val *Get(yyjson_val *val, const string_t &path_str, bool integral_argument) {\n \t\tauto ptr = path_str.GetData();\n \t\tauto len = path_str.GetSize();\n \t\tif (len == 0) {\n \t\t\treturn GetUnsafe(val, ptr, len);\n \t\t}\n+\t\tif (integral_argument) {\n+\t\t\tauto str = \"$[\" + path_str.GetString() + \"]\";\n+\t\t\treturn GetUnsafe(val, str.c_str(), str.length());\n+\t\t}\n \t\tswitch (*ptr) {\n \t\tcase '/': {\n \t\t\t// '/' notation must be '\\0'-terminated\n@@ -260,9 +264,15 @@ struct JSONCommon {\n \t\t\t}\n \t\t\treturn GetUnsafe(val, ptr, len);\n \t\t}\n-\t\tdefault:\n-\t\t\tauto str = \"/\" + string(ptr, len);\n-\t\t\treturn GetUnsafe(val, str.c_str(), len + 1);\n+\t\tdefault: {\n+\t\t\tstring path;\n+\t\t\tif (memchr(ptr, '\"', len)) {\n+\t\t\t\tpath = \"/\" + string(ptr, len);\n+\t\t\t} else {\n+\t\t\t\tpath = \"$.\\\"\" + path_str.GetString() + \"\\\"\";\n+\t\t\t}\n+\t\t\treturn GetUnsafe(val, path.c_str(), path.length());\n+\t\t}\n \t\t}\n \t}\n \ndiff --git a/extension/json/include/json_executors.hpp b/extension/json/include/json_executors.hpp\nindex 0eeff5e40bc6..3290a95ede6a 100644\n--- a/extension/json/include/json_executors.hpp\n+++ b/extension/json/include/json_executors.hpp\n@@ -8,6 +8,7 @@\n \n #pragma once\n \n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"json_functions.hpp\"\n \n@@ -88,11 +89,18 @@ struct JSONExecutors {\n \t\t\t}\n \t\t} else { // Columnref path\n \t\t\tD_ASSERT(info.path_type == JSONCommon::JSONPathType::REGULAR);\n-\t\t\tauto &paths = args.data[1];\n+\t\t\tunique_ptr<Vector> casted_paths;\n+\t\t\tif (args.data[1].GetType().id() == LogicalTypeId::VARCHAR) {\n+\t\t\t\tcasted_paths = make_uniq<Vector>(args.data[1]);\n+\t\t\t} else {\n+\t\t\t\tcasted_paths = make_uniq<Vector>(LogicalTypeId::VARCHAR);\n+\t\t\t\tVectorOperations::DefaultCast(args.data[1], *casted_paths, args.size(), true);\n+\t\t\t}\n \t\t\tBinaryExecutor::ExecuteWithNulls<string_t, string_t, T>(\n-\t\t\t    inputs, paths, result, args.size(), [&](string_t input, string_t path, ValidityMask &mask, idx_t idx) {\n+\t\t\t    inputs, *casted_paths, result, args.size(),\n+\t\t\t    [&](string_t input, string_t path, ValidityMask &mask, idx_t idx) {\n \t\t\t\t    auto doc = JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());\n-\t\t\t\t    auto val = JSONCommon::Get(doc->root, path);\n+\t\t\t\t    auto val = JSONCommon::Get(doc->root, path, args.data[1].GetType().IsIntegral());\n \t\t\t\t    if (SET_NULL_IF_NOT_FOUND && !val) {\n \t\t\t\t\t    mask.SetInvalid(idx);\n \t\t\t\t\t    return T {};\ndiff --git a/extension/json/json_extension.cpp b/extension/json/json_extension.cpp\nindex b594c26d9c15..e0665260a5bf 100644\n--- a/extension/json/json_extension.cpp\n+++ b/extension/json/json_extension.cpp\n@@ -27,7 +27,7 @@ static DefaultMacro json_macros[] = {\n      \"json_group_structure\",\n      {\"x\", nullptr},\n      {{nullptr, nullptr}},\n-     \"json_structure(json_group_array(x))->'0'\"},\n+     \"json_structure(json_group_array(x))->0\"},\n     {DEFAULT_SCHEMA, \"json\", {\"x\", nullptr}, {{nullptr, nullptr}}, \"json_extract(x, '$')\"},\n     {nullptr, nullptr, {nullptr}, {{nullptr, nullptr}}, nullptr}};\n \ndiff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex 0ad6837690b6..2b8b7828e17c 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -21,21 +21,25 @@ static JSONPathType CheckPath(const Value &path_val, string &path, size_t &len)\n \tconst auto path_str_val = path_val.DefaultCastAs(LogicalType::VARCHAR);\n \tauto path_str = path_str_val.GetValueUnsafe<string_t>();\n \tlen = path_str.GetSize();\n-\tauto ptr = path_str.GetData();\n+\tconst auto ptr = path_str.GetData();\n \t// Empty strings and invalid $ paths yield an error\n \tif (len == 0) {\n \t\tthrow BinderException(\"Empty JSON path\");\n \t}\n \tJSONPathType path_type = JSONPathType::REGULAR;\n-\tif (*ptr == '$') {\n-\t\tpath_type = JSONCommon::ValidatePath(ptr, len, true);\n-\t}\n \t// Copy over string to the bind data\n \tif (*ptr == '/' || *ptr == '$') {\n \t\tpath = string(ptr, len);\n-\t} else {\n+\t} else if (path_val.type().IsIntegral()) {\n+\t\tpath = \"$[\" + string(ptr, len) + \"]\";\n+\t} else if (memchr(ptr, '\"', len)) {\n \t\tpath = \"/\" + string(ptr, len);\n-\t\tlen++;\n+\t} else {\n+\t\tpath = \"$.\\\"\" + string(ptr, len) + \"\\\"\";\n+\t}\n+\tlen = path.length();\n+\tif (*path.c_str() == '$') {\n+\t\tpath_type = JSONCommon::ValidatePath(path.c_str(), len, true);\n \t}\n \treturn path_type;\n }\n@@ -67,7 +71,11 @@ unique_ptr<FunctionData> JSONReadFunctionData::Bind(ClientContext &context, Scal\n \t\t\tpath_type = CheckPath(path_val, path, len);\n \t\t}\n \t}\n-\tbound_function.arguments[1] = LogicalType::VARCHAR;\n+\tif (arguments[1]->return_type.IsIntegral()) {\n+\t\tbound_function.arguments[1] = LogicalType::BIGINT;\n+\t} else {\n+\t\tbound_function.arguments[1] = LogicalType::VARCHAR;\n+\t}\n \tif (path_type == JSONCommon::JSONPathType::WILDCARD) {\n \t\tbound_function.return_type = LogicalType::LIST(bound_function.return_type);\n \t}\n@@ -117,6 +125,7 @@ unique_ptr<FunctionData> JSONReadManyFunctionData::Bind(ClientContext &context,\n \n JSONFunctionLocalState::JSONFunctionLocalState(Allocator &allocator) : json_allocator(allocator) {\n }\n+\n JSONFunctionLocalState::JSONFunctionLocalState(ClientContext &context)\n     : JSONFunctionLocalState(BufferAllocator::Get(context)) {\n }\ndiff --git a/src/common/types/column/column_data_allocator.cpp b/src/common/types/column/column_data_allocator.cpp\nindex 0f2c63849e6e..bec0751e0d6c 100644\n--- a/src/common/types/column/column_data_allocator.cpp\n+++ b/src/common/types/column/column_data_allocator.cpp\n@@ -2,6 +2,7 @@\n \n #include \"duckdb/common/types/column/column_data_collection_segment.hpp\"\n #include \"duckdb/storage/buffer/block_handle.hpp\"\n+#include \"duckdb/storage/buffer/buffer_pool.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n \n namespace duckdb {\n@@ -45,6 +46,21 @@ ColumnDataAllocator::ColumnDataAllocator(ColumnDataAllocator &other) {\n \t}\n }\n \n+ColumnDataAllocator::~ColumnDataAllocator() {\n+\tif (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {\n+\t\treturn;\n+\t}\n+\tfor (auto &block : blocks) {\n+\t\tblock.handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);\n+\t}\n+\tconst auto data_size = SizeInBytes();\n+\tblocks.clear();\n+\tif (Allocator::SupportsFlush() &&\n+\t    data_size > alloc.buffer_manager->GetBufferPool().GetAllocatorBulkDeallocationFlushThreshold()) {\n+\t\tAllocator::FlushAll();\n+\t}\n+}\n+\n BufferHandle ColumnDataAllocator::Pin(uint32_t block_id) {\n \tD_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);\n \tshared_ptr<BlockHandle> handle;\ndiff --git a/src/common/types/row/tuple_data_segment.cpp b/src/common/types/row/tuple_data_segment.cpp\nindex 82e25001ee2b..ddec1323846e 100644\n--- a/src/common/types/row/tuple_data_segment.cpp\n+++ b/src/common/types/row/tuple_data_segment.cpp\n@@ -1,6 +1,7 @@\n #include \"duckdb/common/types/row/tuple_data_segment.hpp\"\n \n #include \"duckdb/common/types/row/tuple_data_allocator.hpp\"\n+#include \"duckdb/storage/buffer/buffer_pool.hpp\"\n \n namespace duckdb {\n \n@@ -118,6 +119,10 @@ TupleDataSegment::~TupleDataSegment() {\n \t}\n \tpinned_row_handles.clear();\n \tpinned_heap_handles.clear();\n+\tif (Allocator::SupportsFlush() && allocator &&\n+\t    data_size > allocator->GetBufferManager().GetBufferPool().GetAllocatorBulkDeallocationFlushThreshold()) {\n+\t\tAllocator::FlushAll();\n+\t}\n \tallocator.reset();\n }\n \ndiff --git a/src/execution/operator/join/physical_piecewise_merge_join.cpp b/src/execution/operator/join/physical_piecewise_merge_join.cpp\nindex d7b30423e3a0..8216d91ab18b 100644\n--- a/src/execution/operator/join/physical_piecewise_merge_join.cpp\n+++ b/src/execution/operator/join/physical_piecewise_merge_join.cpp\n@@ -618,7 +618,12 @@ OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionConte\n \n \t\t\t\tif (tail_count < result_count) {\n \t\t\t\t\tresult_count = tail_count;\n-\t\t\t\t\tchunk.Slice(*sel, result_count);\n+\t\t\t\t\tif (result_count == 0) {\n+\t\t\t\t\t\t// Need to reset here otherwise we may use the non-flat chunk when constructing LEFT/OUTER\n+\t\t\t\t\t\tchunk.Reset();\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tchunk.Slice(*sel, result_count);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \ndiff --git a/src/include/duckdb/common/types/column/column_data_allocator.hpp b/src/include/duckdb/common/types/column/column_data_allocator.hpp\nindex dc49d2db0ad9..194b40ca3ea1 100644\n--- a/src/include/duckdb/common/types/column/column_data_allocator.hpp\n+++ b/src/include/duckdb/common/types/column/column_data_allocator.hpp\n@@ -32,6 +32,7 @@ class ColumnDataAllocator {\n \texplicit ColumnDataAllocator(BufferManager &buffer_manager);\n \tColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type);\n \tColumnDataAllocator(ColumnDataAllocator &allocator);\n+\t~ColumnDataAllocator();\n \n \t//! Returns an allocator object to allocate with. This returns the allocator in IN_MEMORY_ALLOCATOR, and a buffer\n \t//! allocator in case of BUFFER_MANAGER_ALLOCATOR.\ndiff --git a/src/include/duckdb/storage/buffer/buffer_pool.hpp b/src/include/duckdb/storage/buffer/buffer_pool.hpp\nindex 955f1aa4ecee..50166a51fa05 100644\n--- a/src/include/duckdb/storage/buffer/buffer_pool.hpp\n+++ b/src/include/duckdb/storage/buffer/buffer_pool.hpp\n@@ -50,6 +50,7 @@ class BufferPool {\n \n \t//! If bulk deallocation larger than this occurs, flush outstanding allocations\n \tvoid SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold);\n+\tidx_t GetAllocatorBulkDeallocationFlushThreshold();\n \n \tvoid UpdateUsedMemory(MemoryTag tag, int64_t size);\n \ndiff --git a/src/optimizer/filter_pushdown.cpp b/src/optimizer/filter_pushdown.cpp\nindex 0744c67091f8..4af2a075ff53 100644\n--- a/src/optimizer/filter_pushdown.cpp\n+++ b/src/optimizer/filter_pushdown.cpp\n@@ -93,9 +93,14 @@ unique_ptr<LogicalOperator> FilterPushdown::Rewrite(unique_ptr<LogicalOperator>\n \t\t// we can just push directly through these operations without any rewriting\n \t\top->children[0] = Rewrite(std::move(op->children[0]));\n \t\treturn op;\n-\tcase LogicalOperatorType::LOGICAL_MATERIALIZED_CTE:\n+\tcase LogicalOperatorType::LOGICAL_MATERIALIZED_CTE: {\n+\t\t// we can't push filters into the materialized CTE (LHS), but we do want to recurse into it\n+\t\tFilterPushdown pushdown(optimizer, convert_mark_joins);\n+\t\top->children[0] = pushdown.Rewrite(std::move(op->children[0]));\n+\t\t// we can push filters into the rest of the query plan (RHS)\n \t\top->children[1] = Rewrite(std::move(op->children[1]));\n \t\treturn op;\n+\t}\n \tcase LogicalOperatorType::LOGICAL_GET:\n \t\treturn PushdownGet(std::move(op));\n \tcase LogicalOperatorType::LOGICAL_LIMIT:\ndiff --git a/src/optimizer/join_order/cardinality_estimator.cpp b/src/optimizer/join_order/cardinality_estimator.cpp\nindex 7b3151d4049d..fb65c0baf739 100644\n--- a/src/optimizer/join_order/cardinality_estimator.cpp\n+++ b/src/optimizer/join_order/cardinality_estimator.cpp\n@@ -2,10 +2,10 @@\n #include \"duckdb/common/enums/join_type.hpp\"\n #include \"duckdb/common/limits.hpp\"\n #include \"duckdb/common/printer.hpp\"\n-#include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/function/table/table_scan.hpp\"\n #include \"duckdb/optimizer/join_order/join_node.hpp\"\n #include \"duckdb/optimizer/join_order/query_graph_manager.hpp\"\n+#include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n \n@@ -291,10 +291,18 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t// and we start to choose the filters that join relations in the set.\n \n \t// edges are guaranteed to be in order of largest tdom to smallest tdom.\n+\tunordered_set<idx_t> unused_edge_tdoms;\n \tauto edges = GetEdges(relations_to_tdoms, set);\n \tfor (auto &edge : edges) {\n-\t\tauto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);\n+\t\tif (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {\n+\t\t\t// the first subgraph has connected all the desired relations, just skip the rest of the edges\n+\t\t\tif (edge.has_tdom_hll) {\n+\t\t\t\tunused_edge_tdoms.insert(edge.tdom_hll);\n+\t\t\t}\n+\t\t\tcontinue;\n+\t\t}\n \n+\t\tauto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);\n \t\tif (subgraph_connections.empty()) {\n \t\t\t// create a subgraph out of left and right, then merge right into left and add left to subgraphs.\n \t\t\t// this helps cover a case where there are no subgraphs yet, and the only join filter is a SEMI JOIN\n@@ -342,13 +350,11 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t\t\t                                   [](Subgraph2Denominator &s) { return !s.relations; });\n \t\t\tsubgraphs.erase(remove_start, subgraphs.end());\n \t\t}\n-\t\tif (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {\n-\t\t\t// the first subgraph has connected all the desired relations, no need to iterate\n-\t\t\t// through the rest of the edges.\n-\t\t\tbreak;\n-\t\t}\n \t}\n \n+\t// Slight penalty to cardinality for unused edges\n+\tauto denom_multiplier = 1.0 + static_cast<double>(unused_edge_tdoms.size());\n+\n \t// It's possible cross-products were added and are not present in the filters in the relation_2_tdom\n \t// structures. When that's the case, merge all remaining subgraphs.\n \tif (subgraphs.size() > 1) {\n@@ -367,7 +373,7 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t\t// denominator is 1 and numerators are a cross product of cardinalities.\n \t\treturn DenomInfo(set, 1, 1);\n \t}\n-\treturn DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom);\n+\treturn DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom * denom_multiplier);\n }\n \n template <>\ndiff --git a/src/planner/binder.cpp b/src/planner/binder.cpp\nindex ea751bade659..9f0abf3986ea 100644\n--- a/src/planner/binder.cpp\n+++ b/src/planner/binder.cpp\n@@ -342,7 +342,8 @@ unique_ptr<BoundQueryNode> Binder::BindNode(QueryNode &node) {\n \n BoundStatement Binder::Bind(QueryNode &node) {\n \tBoundStatement result;\n-\tif (context.db->config.options.disabled_optimizers.find(OptimizerType::MATERIALIZED_CTE) ==\n+\tif (node.type != QueryNodeType::CTE_NODE && // Issue #13850 - Don't auto-materialize if users materialize (for now)\n+\t    context.db->config.options.disabled_optimizers.find(OptimizerType::MATERIALIZED_CTE) ==\n \t        context.db->config.options.disabled_optimizers.end() &&\n \t    context.config.enable_optimizer && OptimizeCTEs(node)) {\n \t\tswitch (node.type) {\ndiff --git a/src/storage/buffer/buffer_pool.cpp b/src/storage/buffer/buffer_pool.cpp\nindex 3e7e11c496dc..9ed31f6f3262 100644\n--- a/src/storage/buffer/buffer_pool.cpp\n+++ b/src/storage/buffer/buffer_pool.cpp\n@@ -413,6 +413,10 @@ void BufferPool::SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold) {\n \tallocator_bulk_deallocation_flush_threshold = threshold;\n }\n \n+idx_t BufferPool::GetAllocatorBulkDeallocationFlushThreshold() {\n+\treturn allocator_bulk_deallocation_flush_threshold;\n+}\n+\n BufferPool::MemoryUsage::MemoryUsage() {\n \tfor (auto &v : memory_usage) {\n \t\tv = 0;\n", "instance_id": "duckdb__duckdb-13982", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear, as it provides detailed reproduction steps for two distinct issues in DuckDB: a binder error related to materialized CTEs and an internal error involving a NULL unique_ptr dereference during a RIGHT JOIN operation. The SQL queries provided are comprehensive, and the environment details (OS, DuckDB version, etc.) are included, which aids in reproducibility. However, there are minor ambiguities: the problem statement does not explicitly connect the reported issues to the provided code changes, leaving it unclear whether the changes fully address the issues or are part of a broader fix. Additionally, edge cases or expected behavior post-fix are not discussed, which could be critical for understanding the full scope of the problem. Overall, the statement is valid and clear but lacks some minor contextual details tying the issues to the code changes and potential edge case considerations.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.75) due to several factors. First, the scope of code changes spans multiple files and modules within the DuckDB codebase, including JSON parsing, memory allocation (jemalloc), buffer pool management, join operations, and query optimization. This indicates a need to understand interactions across different parts of the system, increasing the complexity. Second, the number of technical concepts involved is significant: it requires knowledge of SQL query binding and optimization (e.g., materialized CTE handling), memory management (e.g., buffer pool deallocation and jemalloc configuration), join operation logic (e.g., handling edge cases in merge joins), and JSON path query processing. These concepts are moderately to highly complex, especially in the context of a database engine like DuckDB. Third, the changes suggest handling of specific edge cases, such as NULL pointer dereferences and JSON path formatting, which require careful error handling and validation logic. While the changes themselves are not overly extensive in terms of lines of code, their impact on the system's architecture (e.g., optimizer behavior, memory management) and the depth of understanding required push this into the hard category. It falls short of very hard (0.8-1.0) as it does not appear to involve system-level redesign or highly specialized domain knowledge beyond database internals, but it still demands a deep understanding of the codebase and careful consideration of side effects.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\"Failed to bind column reference\" when inserting into a table with a virtual column\n### What happens?\r\n\r\nWhen inserting into a table with a virtual column duckdb throws error and blocks further interactions with the table\r\n\r\n![Screenshot from 2024-08-19 18-51-34](https://github.com/user-attachments/assets/01ef18cc-bb9f-4190-8133-434c420f0432)\r\n\r\n\r\n### To Reproduce\r\n\r\n### How to reproduce in cli\r\n\r\n```sql\r\nCREATE TABLE t1 (a CHAR NOT NULL, c CHAR GENERATED ALWAYS AS (a) VIRTUAL, b INT);\r\nCREATE UNIQUE INDEX t1_idx ON t1 (a);\r\nINSERT OR REPLACE INTO t1 (a, b) VALUES ('a', 1);\r\n```\r\n\r\nThe output after executing last line from the above:\r\n```\r\nINTERNAL Error: Failed to bind column reference \"b\" [10.2] (bindings: {#[10.0], #[10.1], #[2.0], #[2.1]})\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\r\n\r\n### Observations\r\n#### There is no issue if virtual column is last in table so below runs without issue\r\n``` \r\nCREATE TABLE t1 (a CHAR NOT NULL, b INT, c CHAR GENERATED ALWAYS AS (a) VIRTUAL);\r\nCREATE UNIQUE INDEX t1_idx ON t1 (a);\r\nINSERT OR REPLACE INTO t1 (a, b) VALUES ('a', 1);\r\n```\r\n#### There is no issue if index is not created so below runs ok\r\n```\r\nCREATE TABLE t1 (a CHAR NOT NULL, b INT, c CHAR GENERATED ALWAYS AS (a) VIRTUAL);\r\nINSERT INTO t1 (a, b) VALUES ('a', 1);\r\n```\r\n#### Issue is also reproduced when inserting BY NAME\r\n```\r\nCREATE TABLE t1 (a CHAR NOT NULL, c CHAR GENERATED ALWAYS AS (a) VIRTUAL, b INT);\r\nCREATE UNIQUE INDEX t1_idx ON t1 (a);\r\nINSERT OR REPLACE INTO t1 BY NAME (SELECT 'a' as a, 1 as b);\r\n```\r\n\r\n### OS:\r\n\r\nLinux, x86_64\r\n\r\n### DuckDB Version:\r\n\r\n1.0.0 and also nightly build e1568a2\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Full Name:\r\n\r\nBoris Urman\r\n\r\n### Affiliation:\r\n\r\nLogility\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a nightly build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/planner/binder/statement/bind_insert.cpp b/src/planner/binder/statement/bind_insert.cpp\nindex ac430edee321..f02be80009d1 100644\n--- a/src/planner/binder/statement/bind_insert.cpp\n+++ b/src/planner/binder/statement/bind_insert.cpp\n@@ -295,7 +295,7 @@ void Binder::BindOnConflictClause(LogicalInsert &insert, TableCatalogEntry &tabl\n \t\t\tauto entry = specified_columns.find(col.Name());\n \t\t\tif (entry != specified_columns.end()) {\n \t\t\t\t// column was specified, set to the index\n-\t\t\t\tinsert.on_conflict_filter.insert(col.Oid());\n+\t\t\t\tinsert.on_conflict_filter.insert(col.Physical().index);\n \t\t\t}\n \t\t}\n \t\tbool index_references_columns = false;\n@@ -353,8 +353,12 @@ void Binder::BindOnConflictClause(LogicalInsert &insert, TableCatalogEntry &tabl\n \t// add a bind context entry for it\n \tauto excluded_index = GenerateTableIndex();\n \tinsert.excluded_table_index = excluded_index;\n-\tauto table_column_names = columns.GetColumnNames();\n-\tauto table_column_types = columns.GetColumnTypes();\n+\tvector<string> table_column_names;\n+\tvector<LogicalType> table_column_types;\n+\tfor (auto &col : columns.Physical()) {\n+\t\ttable_column_names.push_back(col.Name());\n+\t\ttable_column_types.push_back(col.Type());\n+\t}\n \tbind_context.AddGenericBinding(excluded_index, \"excluded\", table_column_names, table_column_types);\n \n \tif (on_conflict.condition) {\n", "instance_id": "duckdb__duckdb-13759", "clarity": 3, "difficulty": 0.65, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue of \"Failed to bind column reference\" when inserting into a table with a virtual column in DuckDB, providing detailed steps to reproduce the issue using SQL queries. The statement includes specific conditions under which the issue occurs (e.g., virtual column not being last, presence of a unique index) and conditions under which it does not occur, which helps narrow down the root cause. Additionally, it provides relevant context such as the OS, DuckDB version, and client used, along with visual evidence (screenshot) and confirmation of testing with the latest build. There are no significant ambiguities, and all necessary information to understand and reproduce the issue is included. The only minor omission is a lack of explicit mention of expected behavior, but the implication (successful insertion without errors) is clear from the context.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the clarity of the problem statement helps, but solving it requires a deep understanding of DuckDB's internal architecture, specifically how column binding, virtual columns, and conflict resolution (INSERT OR REPLACE with ON CONFLICT) interact within the planner and binder components. The code changes provided are focused on a single file (`bind_insert.cpp`), which limits the scope, but the modifications involve critical logic related to column indexing and binding context management. The changes address physical column indexing (`col.Physical().index`) and dynamic construction of column names and types for the binding context, indicating a need to understand DuckDB's column storage model (logical vs. physical) and how excluded table bindings work in conflict clauses.\n\nThe technical concepts involved include database internals (column binding, virtual columns, index handling), C++ programming (working with vectors, string manipulation, and custom data structures like `LogicalInsert`), and DuckDB-specific APIs and conventions. While the code change itself is relatively small (a few lines), the impact is significant as it affects core functionality related to SQL query execution. Edge cases are implicitly present due to the nature of virtual columns and conflict resolution, though not explicitly detailed in the problem statement beyond the reproduction steps. Error handling modifications are not directly evident in the diff, but understanding the error (\"Failed to bind column reference\") suggests a need to ensure robust binding logic.\n\nOverall, this problem requires a solid grasp of database internals and DuckDB's codebase, placing it above medium difficulty (0.4-0.6) but not at the extreme end of very hard (0.8-1.0) since the scope of changes is localized and does not involve system-wide refactoring or advanced distributed systems concepts. A score of 0.65 reflects the need for specialized knowledge and careful handling of core logic, with moderate complexity in debugging and implementation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[python/r/c++] Revisit `shape` for component arrays\n## PRs\r\n\r\n**Merged PRs:**\r\n\r\n<details>\r\n\r\n* #2909 `kerl/schevo-timestamp-methodize`\r\n* #2913 `kerl/name-neaten`\r\n* #2908 `kerl/ut-soma-exc-simplify`\r\n* #2910 `kerl/test-common-parameterize`\r\n* #2918 `kerl/cpp-test-deadstrip`\r\n* #2919 `kerl/minor-unit-test-helper-mod`\r\n* #2936 `kerl/cpp-ut-helper-neaten`\r\n* #2938 `kerl/more-cur-dom-parameterize`\r\n* #2915 `kerl/cpp-strict-int64-shape`\r\n* #2911 `kerl/arrow-util-current-domain-optional`\r\n* #2939 `kerl/step-two-temp`\r\n* #2947 `kerl/cpp-ndarray-resize-testing`\r\n* #2945 `kerl/dataframe-test-fixture`\r\n* #2944 `kerl/cpp-variant-indexed-dataframes`\r\n* #2916 `kerl/sdf-shape`\r\n* #2917 `kerl/cpp-resizes`\r\n* #2948 `kerl/upgrade-shape-int64`\r\n* #2951 `kerl/sdf-test-accessors`\r\n* #2953 `kerl/py-r-accessor-plumbing`\r\n* #2957 `kerl/sdf-domain-accessors`\r\n* #2960 `kerl/dense-link`\r\n* #2963 `kerl/nightly-fix`\r\n* #2970 `kerl/dense-writeable-after-create`\r\n* #2968 `kerl/minor-trim`\r\n* #2969 `kerl/more-py-domain-name-neaten`\r\n* #2972 `kerl/libtiledbsoma-env-logging-level`\r\n* #2962 `kerl/py-r-creation-paths`\r\n* #2950 `kerl/py-r-test-2`\r\n* #2994 `kerl/nanoarrow-helpers`\r\n* #3011 `kerl/polydom3`\r\n* #3017 `kerl/polydom5`\r\n* #3018 `kerl/polydom6`\r\n* #2990 `kerl/variant-nnz-bug`\r\n* #3019 `kerl/index-swap`\r\n* #3020 `kerl/ut-max-shape`\r\n* #3012 `kerl/polydom4`\r\n* #3025 `kerl/fix-3020-merge`\r\n* #3026 `kerl/one-more-rename`\r\n* #3028 `kerl/ff-not`\r\n* #3029 `kerl/ut-vg`\r\n* #3030 `kerl/table-utils-memory`\r\n* #3067 `kerl/improve-sdf-test-field-names`\r\n* #3068 `kerl/ut-generate`\r\n* #3069 `kerl/cpp-sdf-domain-at-create`\r\n* #3027 `kerl/hll-domainish`\r\n* #3088 `kerl/max-domain-int64`\r\n* #3090 `kerl/maybe-resize-soma-joinid-cpp-tweak`\r\n* #3032 `kerl/sdf-domain-at-create` -- fixes #2967\r\n* #3091 `kerl/maybe-resize-soma-joinid-py-r`\r\n* #3095 `kerl/cpp-exp-resize-prep`\r\n* #3089 `kerl/r-dataframe-shapeable`\r\n* https://github.com/single-cell-data/SOMA/pull/233\r\n* #3125 `kerl/cpp-ut-name-shortens`\r\n* #3127 `kerl/helper-rename`\r\n* #3130 `kerl/cpp-can-resizers-names`\r\n* #3132 `kerl/cpp-dataframe-sizing-helpers`\r\n* #3139 `kerl/cpp-dataframe-upgrade-test`\r\n* #3140 `kerl/py-resizer-connects`\r\n* #3151 `kerl/py-can-upgrade-shape`\r\n* #3152 `kerl/registration-shape-acceessors`\r\n* #3156 `kerl/py-exp-shaping`\r\n* #3157 `kerl/py-exp-shaping2`\r\n* #3148 `kerl/py-exp-resize`\r\n* #3191 `kerl/py-domain-at-create-ut-1`\r\n* #3190 `kerl/py-domain-at-create-ut-2`\r\n* #3192 `kerl/py-domain-at-create-ut-3`\r\n* #3193 `kerl/py-domain-at-create-ut-4`\r\n* #3194 `kerl/py-domain-at-create-ut-5`\r\n* #3203 `kerl/min-size-2`\r\n* #3208 `kerl/r-min-sizing`\r\n* #3211 `kerl/cpp-ugr-dom`\r\n* #3232 `kerl/ff-interop`\r\n* #3230 `kerl/ffon`\r\n* #3234 `kerl/docstring-prune`\r\n* #3236 `kerl/prefixing`\r\n* #3241 `kerl/fix-bad-merge`\r\n* #3235 `kerl/py-r-ugr-dom`\r\n* #3237 `kerl/py-r-ugr-dom-2`\r\n* #3238 `kerl/py-r-ugr-dom-3`\r\n* #3253 `kerl/set-coords-rename`\r\n* #3261 `kerl/pybind11-nda-sizing`\r\n* #3265 `kerl/dense-227-a`\r\n* #3263 `kerl/dense-range-trim`\r\n* #3268 `kerl/dim-explosion`\r\n* #3269 `kerl/python-227-dense-ned-read`\r\n* #3270 `kerl/r-227-dense-fixes`\r\n* #3280 `kerl/r-dense-227-more`\r\n* #3286 `kerl/more-fn4m`\r\n* #3283 `kerl/readthedocs-pre-1.15`\r\n* #3288 `kerl/dense-ugrsh`\r\n* #3289 `kerl/notebook-shape-upgrade`\r\n* #3285 `kerl/new-shape-doc-updates`\r\n* #3295 `kerl/notebook-data-refresh`\r\n* #3290 `kerl/notebook-new-shape-refresh`\r\n* #3301 `kerl/ffena`\r\n* #3303 `kerl/r-data-refresh`\r\n* #3300 `kerl/sdf-sjid-lower-zero`\r\n* #3296 `kerl/dense-example-data-refresh`\r\n* #3294 `kerl/new-shape-notebook-and-vignette`\r\n* #3308 `kerl/upgrade-experiment-resources`\r\n* #3309 `kerl/fix-notebook-merge`\r\n* #3321 `kerl/more-use-shape`\r\n* #3358 `kerl/revert-3300`\r\n* #3368 `kerl/227a`\r\n* #3369 `kerl/ucd1`\r\n* #3370 `kerl/ucd2`\r\n* #3371 `kerl/ucd3`\r\n* #3372 `kerl/ucd4`\r\n* #3396 `kerl/domain-at-create-docstrings`\r\n* #3302 `kerl/new-shape-vignette`\r\n* https://github.com/single-cell-data/SOMA/pull/250\r\n* #3399 `kerl/new-shape-more-docstrings`\r\n* #3400 `kerl/check-only-r`\r\n* https://github.com/TileDB-Inc/somacore-feedstock/pull/27\r\n\r\n</details>\r\n\r\n**Closed/abandoned PRs:**\r\n\r\n<details>\r\n\r\n* #2785 -- This was only dogfooding for the core 2.25 release -- not to be merged\r\n* #2952 `kerl/feature-flag-temp` -- folded into 2962\r\n* #2995 `kerl/polydom`\r\n* #2964 `kerl/tiledbsoma-io-test`\r\n* #3189 `kerl/min-size`\r\n* #3220 `kerl/cpp-ugr-dom-2`\r\n* #3244 `kerl/dense-227-fixes`\r\n* #3276 `kerl/dataframe-shape`\r\n\r\n</details>\r\n\r\n**Issues which are related but non-blocking:**\r\n\r\n* https://github.com/single-cell-data/SOMA/pull/216\r\n* https://github.com/TileDB-Inc/TileDB/pull/5303\r\n* #2966\r\n* #3081\r\n* Note: R append mode does not exist yet -- see #1630 -- so an experiment-level resizer is not a priority in R\r\n* #3271 \r\n* #3272\r\n* #3273\r\n\r\n**See also:** [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Problem to be solved\r\n\r\nUsers want to know the `shape` of an array, in the SciPy sense:\r\n\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`.\r\n  * Some users need the ability to grow their datasets later, using either `tiledbsoma.io`'s append mode, or subsequent writes using the `tiledbsoma` API.\r\n  * Note that the cellxgene census doesn't need this: eact week's published census has fixed shape, and any updates will happen in new storage, on a new week.\r\n\r\nUsing TileDB-SOMA up until the present:\r\n\r\n* The TIleDB `domain` is immutable after array creation\r\n  * This does bounds-checking for reads and writes, which is good\r\n  * To leverage this to function as a `shape`, users would need to set the `domain` at array-creation time. However, users lose the ability to grow their datasets later.\r\n* There is a `non_empty_domain` accessor\r\n  * This only indicates min/max coordinates _at which data exists_. Consider an `X` array for 100 cells and 200 genes. If non-zero expression counts exist only for cell join IDs 2-17, then the `non_empty_domain` will indicate `(2,17)` along `soma_dim_0`.\r\n  * Consider an `obms[\"X_pca\"]` within the same experiment. This may be 100 cells by 50 PCA components: we need a placd to store the number 50.\r\n  * Therefore users cannot leverage this to function as a `shape` accessor.\r\n* We have offered a `used_shape` accessor since TileDB-SOMA 1.5.\r\n  * This functions as a `shape` accessor, in the SciPy sense, but it is not multi-writer safe.\r\n\r\nNew feature for TileDB-SOMA 1.15:\r\n\r\n* Arrays will have a  `shape`\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`\r\n* The `used_shape` accessor will be deprecated in TileDB-SOMA 1.13, and slated for removal in TileDB-SOMA 1.14.\r\n\r\nCompatiblity:\r\n\r\nThis will now require users to do an explicit `resize` before appending/growing TileDB-SOMA Experiments. Guidance in the form of example notebooks will be provided.\r\n\r\n##  Tracking\r\n\r\nSee also: [[sc-41074]](https://app.shortcut.com/tiledb-inc/story/41074) and [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Scheduling\r\n\r\nSupport arrives in TileDB Core 2.25. Deprecations for TileDB-SOMA will be released with 1.13. Full support within TileDB-SOMA will be release in 1.14.\r\n\r\n## Details\r\n\r\nSOMA API mods as we've discussed in a Google doc are as follows.\r\n\r\n### `SOMADataFrame`\r\n\r\n* `create`: Retain the `domain` argument\r\n  * Issue:\r\n    * Core has a `(lo, hi)` tuple per dim, e.g. `(0,99)` or `(10,19)`\r\n    * SOMA has count per dim, with 0 implicit: e.g. 100 or 20\r\n    * For `SparseNDArray` and `DenseNDArray` core can have `(lo, hi)` and SOMA can have `count`\r\n    * For `DataFrame` there can be multiple dims --- default is a single `soma_joinid`\r\n    * That could be treated either in `(lo, hi)` fashion or `count` fashion\r\n    * However additional dims (e.g. `cell_type`) can be on any type, including strings, floats, etc. where there is no implicit lo=0\r\n    * Therefore we need to keep the current SOMA API wherein `DataFrame` takes a `domain` argument (in `(lo, hi)` fashion) and not a `shape` argument (in `count` fashion)\r\n\r\n### `SparseNDArray and DenseNDArray`\r\n\r\n* `create`\r\n  * Have an optional shape argument which is of type `Tuple[Int,...]` where each element is the cell count of the corresponding dimension\r\n    * If unsupplied, or if supplied but None in any slot: use the minimum 0 in each slot \u2013 nothing larger makes sense since we will not support downsize\r\n  * User guidance should make clear that it will not be possible to create an \u2018old\u2019 style array with the \u2018new style\u2019 API. (See also the upgrade logic below.)\r\n\r\n### All three of `SOMADataFrame`, `SparseNDArray`, `DenseNDArray`\r\n\r\n* `write`\r\n  * For new arrays, created with the new shape feature:\r\n    * Core will bounds-check that coordinates provided at `write` time are within the current shape\r\n    * Core will raise `tiledb.cc.TileDBError` to TileDB-SOMA, which will catch and raise `IndexError`, and R-standard behavior on the R side\r\n  * For old arrays created before this feature:\r\n    * Core will not bounds-check that coordinates provided at write time are within the current shape\r\n* Existing `used_shape` accessor\r\n  * TileDB-SOMA will deprecate this over a release cycle.\r\n  * For new arrays: raise `NotImplementedError`\r\n  * For old arrays: return what\u2019s currently returned, with a deprecation warning.\r\n  * Mechanism for determining old vs. new: `array.schema.version` (the core storage version).\r\n* Existing `shape` accessor\r\n  * For new arrays:\r\n    * Have this return the new shape as proposed by core, no longer returning the TileDB domain.\r\n  * For old arrays created before this feature:\r\n    * Return the TileDB domain as now.\r\n* Existing `non_empty_domain` accessor\r\n  * Same behavior for old and new arrays (unaffected by this proposal).\r\n  * Keep this accessor supported, but, with user notes that it\u2019s generally non-useful\r\n  * This should return None (or R equivalent) when there is a schema but no data have been written.\r\n* New `maxshape` accessor\r\n  * Maps the core-level `(lo, hi)` accessor for domain to count-style accessor hi+1. E.g. if the core domain is either `(0,99)` or `(50,99)` then TileDB-SOMA `maxshape` will say 100.\r\n  * Same behavior for old and new arrays.\r\n  * Let users query for what the TileDB domain is, with user notes that it\u2019s the maximum that users can reshape to.\r\n  * Issac suggests: maybe `domain` or `maxshape` (see h5py).\r\n* New `resize` mutator\r\n  * Note: `reshape` means something else in the community (numpy, zarr, h5py), e.g. a 5x20 (total 100 cells) being reinterpreted as 4x25 (still 100 cells). The standard name for changing cell-count is `resize`.\r\n  * For old arrays created before this feature: raise `NotImplementedError`.\r\n  * For new arrays:\r\n    * Will raise `ValueError` if the new shape is smaller on any dim than currently in storage\r\n    * Regardless of whether any data have been written whatsoever\r\n    * Will raise `ValueError` if the new shape exceeds the TileDB domain from create time, which will serve TileDB-SOMA in a role of \u201cmax possible shape the user can reshape to\u201d\r\n    * Otherwise, any calls to write from this point will bounds-check writes within this new shape\r\n    * **We don\u2019t expect resize to be multi-writer safe with regard to write ; user notes must be clear on this point**\r\n* New `tiledbsoma_upgrade_shape` method for SparseNDArray and DenseNDArray\r\n  * This will leverage `array.schema.version` to see if an upgrade is needed\r\n  * Leverage core support for storage-version updates\r\n  * This will take a shape argument as in `create`\r\n  * For arrays created with \u201cjust-right\u201d size: this will succeed\r\n  * For arrays created with \u201croom-for-growth\u201d / \u201ctwo billion-ish\u201d size: this will succeed\r\n  * If the user passes a shape which exceeds the current TileDB domain: this will fail\r\n* New `tiledbsoma_upgrade_domain method` for `DataFrame`\r\n  * Same as for `SparseNDArray`/`DenseNDArray` except it will take a domain at the SOMA-API level just as `DataFrame`'s create method\r\n\r\n### `tiledbsoma.io`\r\n\r\n* The user-facing API has no shape arguments and thus won\u2019t need changing.\r\n* Internally to `tiledbsoma.io`, we\u2019ll still ask the tiledbsoma API for the \u201cbig domain\u201d (2 billionish)\r\n* Append mode:\r\n  * Will need a new `resize` method at the `Experiment` level\r\n  * Users will need to:\r\n    * Register as now\r\n    * Call the experiment-level `resize`\r\n      * Could be `exp.resize(...)`, or (better) this could be `tiledbsoma.io.reshape_experiment`\r\n  * In either case: this method will take the new `obs` and `var` counts as inputs:\r\n    * `exp.obs.reshape` to new `obs` count\r\n    * `exp.ms[name].var.reshape` to new `var` count\r\n    * `exp.ms[name].X[name].reshape` to new `obs` count x `var` count\r\n    * `exp.ms[name].obsm[name].reshape` to new `obs` count x same width\r\n    * `exp.ms[name].obsp[name].reshape` to new `obs` count x `obs` count\r\n    * `exp.ms[name].varm[name].reshape` to new `var` count x same width\r\n    * `exp.ms[name].varp[name].reshape` to new `var` count x `var` count\r\n  * Do the individual append-mode writes as now\r\n\n", "patch": "diff --git a/libtiledbsoma/src/soma/soma_array.cc b/libtiledbsoma/src/soma/soma_array.cc\nindex e19aa4703d..c49c3a9167 100644\n--- a/libtiledbsoma/src/soma/soma_array.cc\n+++ b/libtiledbsoma/src/soma/soma_array.cc\n@@ -1443,7 +1443,7 @@ std::vector<int64_t> SOMAArray::maxshape() {\n std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n     const std::vector<int64_t>& newshape,\n     bool is_resize,\n-    std::string method_name_for_messages) {\n+    std::string function_name_for_messages) {\n     // E.g. it's an error to try to upgrade_domain or resize specifying\n     // a 3-D shape on a 2-D array.\n     auto arg_ndim = newshape.size();\n@@ -1453,7 +1453,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n             false,\n             fmt::format(\n                 \"cannot {}: provided shape has ndim {}, while the array has {}\",\n-                method_name_for_messages,\n+                function_name_for_messages,\n                 arg_ndim,\n                 array_ndim));\n     }\n@@ -1471,7 +1471,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n                 fmt::format(\n                     \"{}: array currently has no shape: please use \"\n                     \"tiledbsoma_upgrade_shape.\",\n-                    method_name_for_messages));\n+                    function_name_for_messages));\n         }\n     } else {\n         // They're trying to do upgrade_shape on an array that already has a\n@@ -1483,7 +1483,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n                     \"{}: array already has a shape: please use resize rather \"\n                     \"than \"\n                     \"tiledbsoma_upgrade_shape.\",\n-                    method_name_for_messages));\n+                    function_name_for_messages));\n         }\n     }\n \n@@ -1498,7 +1498,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n     // if the requested shape fits in the array's core domain, it's good to go\n     // as a new shape.\n     auto domain_check = _can_set_shape_domainish_helper(\n-        newshape, false, method_name_for_messages);\n+        newshape, false, function_name_for_messages);\n     if (!domain_check.first) {\n         return domain_check;\n     }\n@@ -1507,7 +1507,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n     // shape (core current domain) isn't a downsize of the current one.\n     if (has_shape) {\n         auto current_domain_check = _can_set_shape_domainish_helper(\n-            newshape, true, method_name_for_messages);\n+            newshape, true, function_name_for_messages);\n         if (!current_domain_check.first) {\n             return current_domain_check;\n         }\n@@ -1522,7 +1522,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_helper(\n std::pair<bool, std::string> SOMAArray::_can_set_shape_domainish_helper(\n     const std::vector<int64_t>& newshape,\n     bool check_current_domain,\n-    std::string method_name_for_messages) {\n+    std::string function_name_for_messages) {\n     Domain domain = arr_->schema().domain();\n \n     for (unsigned i = 0; i < domain.ndim(); i++) {\n@@ -1538,7 +1538,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_domainish_helper(\n             throw TileDBSOMAError(fmt::format(\n                 \"{}: internal error: expected {} dim to \"\n                 \"be {}; got {}\",\n-                method_name_for_messages,\n+                function_name_for_messages,\n                 dim_name,\n                 tiledb::impl::type_to_str(TILEDB_INT64),\n                 tiledb::impl::type_to_str(dim.type())));\n@@ -1554,7 +1554,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_domainish_helper(\n                     false,\n                     fmt::format(\n                         \"cannot {} for {}: new {} < existing shape {}\",\n-                        method_name_for_messages,\n+                        function_name_for_messages,\n                         dim_name,\n                         newshape[i],\n                         old_dim_shape));\n@@ -1570,7 +1570,7 @@ std::pair<bool, std::string> SOMAArray::_can_set_shape_domainish_helper(\n                     false,\n                     fmt::format(\n                         \"cannot {} for {}: new {} < maxshape {}\",\n-                        method_name_for_messages,\n+                        function_name_for_messages,\n                         dim_name,\n                         newshape[i],\n                         old_dim_shape));\ndiff --git a/libtiledbsoma/src/soma/soma_array.h b/libtiledbsoma/src/soma/soma_array.h\nindex 22617948ae..1619936ac6 100644\n--- a/libtiledbsoma/src/soma/soma_array.h\n+++ b/libtiledbsoma/src/soma/soma_array.h\n@@ -1193,7 +1193,7 @@ class SOMAArray : public SOMAObject {\n     std::pair<bool, std::string> _can_set_shape_helper(\n         const std::vector<int64_t>& newshape,\n         bool is_resize,\n-        std::string method_name_for_messages);\n+        std::string function_name_for_messages);\n \n     /**\n      * This is a second-level code-dedupe helper for _can_set_shape_helper.\n@@ -1201,7 +1201,7 @@ class SOMAArray : public SOMAObject {\n     std::pair<bool, std::string> _can_set_shape_domainish_helper(\n         const std::vector<int64_t>& newshape,\n         bool check_current_domain,\n-        std::string method_name_for_messages);\n+        std::string function_name_for_messages);\n \n     /**\n      * This is a code-dedupe helper method for resize and upgrade_shape.\n", "instance_id": "single-cell-data__TileDB-SOMA-3125", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in defining the goal of introducing a `shape` feature for TileDB-SOMA arrays, which allows bounds-checking for reads and writes, retains value regardless of data occupancy, and supports resizing. It provides detailed descriptions of the current limitations (e.g., immutable `domain`, limitations of `non_empty_domain` and `used_shape`), the new feature's behavior, and API modifications for various array types (`SOMADataFrame`, `SparseNDArray`, `DenseNDArray`). It also outlines compatibility concerns, deprecation plans, and user guidance. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, while the general behavior of `resize` and `upgrade_shape` is described, specific edge cases (e.g., behavior during concurrent writes, exact error messages for various failure conditions) are not fully specified. Additionally, the problem statement includes an extensive list of related PRs and issues, which, while helpful for context, can overwhelm the reader and dilute focus on the core problem. Lastly, the exact interaction with TileDB Core's version-specific features (e.g., storage version updates) could be elaborated further for clarity.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors. First, the scope and depth of code changes are significant. Although the provided diff is relatively small and focused on a single file (`soma_array.cc`), the problem statement indicates that this feature spans multiple components of the TileDB-SOMA library, including API modifications for different array types, integration with TileDB Core, and updates to user-facing tools like `tiledbsoma.io`. The extensive list of related PRs (over 100 merged PRs) suggests that the overall feature implementation involves a large codebase with changes across multiple modules and languages (C++, Python, R). Second, the number of technical concepts involved is substantial, requiring a deep understanding of TileDB's domain and shape concepts, array storage formats, bounds-checking mechanisms, versioning, and multi-language API bindings (C++, Python, R). Additionally, developers must handle compatibility between old and new array formats, deprecation of existing features (`used_shape`), and integration with core library updates (TileDB Core 2.25). Third, the problem involves complex edge cases and error handling, such as ensuring `resize` operations do not downsize below existing data, handling domain limits, and managing multi-writer safety (explicitly noted as unsupported for `resize`). While the provided code change itself is relatively straightforward (mostly renaming and error message updates), the broader context indicates a need for architectural awareness and careful handling of system invariants. This combination of factors\u2014broad scope, multiple technical concepts, and complex edge cases\u2014makes this a challenging problem, though not at the extreme end of difficulty (e.g., implementing a distributed system from scratch), hence the score of 0.75.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[python] Fix nightly-build failure / `pybind11` exception-mapping\n**Issue and/or context:** Resolves #2961 \r\n\r\n**Changes:**\r\n\r\nDo explicit try-catch. See also #783.\r\n\r\n**Notes for Reviewer:**\r\n\r\nIt's not necessarily causal that this triggered on https://github.com/jdblischak/centralized-tiledb-nightlies/issues/21. It's true that the `apis/python/tests/test_shape.py` was recently introduced on the #2407 stack. However, whether un-rethrown errors (on implicit pybind body, as in the \"before\" parts of this PR) reach the Python level as `tiledbsoma.SOMAError` or `RuntimeError` or `tiledb.cc.TileDBError` has some element of randomness.\r\n\r\nThere's an attempt to globally register a rethrow:\r\nhttps://github.com/single-cell-data/TileDB-SOMA/blob/1.13.1/apis/python/src/tiledbsoma/pytiledbsoma.cc#L36-L50\r\n\r\nHowever, @nguyenv and I have long found that some things come through as `RuntimeError` on one machine, and `tiledb.cc.TileDBError` in another. We suspect that TileDB-Py's exception registrar has a race condition with our own.\r\n\r\nRegardless, a safe practice is explicit catch-and-rethrow at the boundary between C++ and Python, which is what we do more of here.\n[c++] Refine concurrency configuration (`soma.compute_concurrency_level`)\n**Issue and/or context:**\r\n\r\nIntroduces direct control over the native thread pool concurrency level in SOMA. Previously, it was indirectly controlled by the TileDB Core `sm.compute_currency_control` configuration, and would create a thread pool that was 1/2 the size of that value.\r\n\r\n**Changes:**\r\n\r\nThis PR introduces a new context configuration setting:  `soma.compute_currency_level`, which defaults to the host CPU count. It can be set with an numeric value,eg., in Python:\r\n\r\n```python\r\ncontext = tiledbsoma.SOMATileDBContext(tiledb_config={\"soma.compute_currency_level\": \"20\"})\r\n```\r\n\r\n**Notes for Reviewer:**\r\n\r\nsc-54891\r\n\r\nthe PR also adds:\r\n* improved error handling for malformed config values\r\n* bound the value between 1 and 1024\r\n\r\n\n", "patch": "diff --git a/apis/python/src/tiledbsoma/common.h b/apis/python/src/tiledbsoma/common.h\nindex 4b6632788f..e4b38e692c 100644\n--- a/apis/python/src/tiledbsoma/common.h\n+++ b/apis/python/src/tiledbsoma/common.h\n@@ -13,22 +13,7 @@ using namespace std;\n using namespace tiledb;\n namespace py = pybind11;\n \n-#define TPY_ERROR_LOC(m) throw TileDBSOMAPyError(m);\n-\n-class TileDBSOMAPyError : std::runtime_error {\n-   public:\n-    explicit TileDBSOMAPyError(const char* m)\n-        : std::runtime_error(m) {\n-    }\n-    explicit TileDBSOMAPyError(std::string m)\n-        : std::runtime_error(m.c_str()) {\n-    }\n-\n-   public:\n-    virtual const char* what() const noexcept override {\n-        return std::runtime_error::what();\n-    }\n-};\n+#define TPY_ERROR_LOC(m) throw TileDBSOMAError(m);\n \n namespace tiledbsoma {\n \ndiff --git a/apis/python/src/tiledbsoma/pytiledbsoma.cc b/apis/python/src/tiledbsoma/pytiledbsoma.cc\nindex f63ebde488..f9d56ff7fe 100644\n--- a/apis/python/src/tiledbsoma/pytiledbsoma.cc\n+++ b/apis/python/src/tiledbsoma/pytiledbsoma.cc\n@@ -49,8 +49,6 @@ PYBIND11_MODULE(pytiledbsoma, m) {\n                 std::rethrow_exception(p);\n         } catch (const TileDBSOMAError& e) {\n             PyErr_SetString(tiledb_soma_error.ptr(), e.what());\n-        } catch (const TileDBSOMAPyError& e) {\n-            PyErr_SetString(tiledb_soma_error.ptr(), e.what());\n         } catch (py::builtin_exception& e) {\n             throw;\n         };\ndiff --git a/apis/python/src/tiledbsoma/reindexer.cc b/apis/python/src/tiledbsoma/reindexer.cc\nindex 100bf1231d..d22a0314b7 100644\n--- a/apis/python/src/tiledbsoma/reindexer.cc\n+++ b/apis/python/src/tiledbsoma/reindexer.cc\n@@ -47,7 +47,7 @@ using namespace tiledbsoma;\n  * @param lookups input values to be looked up\n  * @return looked up values\n  */\n-py::array_t<int64_t> get_indexer_general(\n+py::array_t<int64_t> get_indexer_general_aux(\n     IntIndexer& indexer, py::array_t<int64_t> lookups) {\n     auto input_buffer = lookups.request();\n     int64_t* input_ptr = static_cast<int64_t*>(input_buffer.ptr);\n@@ -58,6 +58,14 @@ py::array_t<int64_t> get_indexer_general(\n     indexer.lookup(input_ptr, results_ptr, size);\n     return results;\n }\n+py::array_t<int64_t> get_indexer_general(\n+    IntIndexer& indexer, py::array_t<int64_t> lookups) {\n+    try {\n+        return get_indexer_general_aux(indexer, lookups);\n+    } catch (const std::exception& e) {\n+        throw TileDBSOMAError(e.what());\n+    }\n+}\n \n /***\n  * Helper function to provide data and schema for an arrow object\n@@ -83,7 +91,7 @@ void extract_py_array_schema(\n  * @py_arrow_array pyarrow inputs to be looked up\n  * @return looked up values\n  */\n-py::array_t<int64_t> get_indexer_py_arrow(\n+py::array_t<int64_t> get_indexer_py_arrow_aux(\n     IntIndexer& indexer, py::object py_arrow_array) {\n     // Check if it is not a pyarrow array or pyarrow chunked array\n     if (!py::hasattr(py_arrow_array, \"_export_to_c\") &&\n@@ -134,6 +142,15 @@ py::array_t<int64_t> get_indexer_py_arrow(\n     return results;\n }\n \n+py::array_t<int64_t> get_indexer_py_arrow(\n+    IntIndexer& indexer, py::object py_arrow_array) {\n+    try {\n+        return get_indexer_py_arrow_aux(indexer, py_arrow_array);\n+    } catch (const std::exception& e) {\n+        throw TileDBSOMAError(e.what());\n+    }\n+}\n+\n void load_reindexer(py::module& m) {\n     // Efficient C++ re-indexing (aka hashing unique key values to an index\n     // between 0 and number of keys - 1) based on khash\n@@ -143,7 +160,11 @@ void load_reindexer(py::module& m) {\n         .def(\n             \"map_locations\",\n             [](IntIndexer& indexer, py::array_t<int64_t> keys) {\n-                indexer.map_locations(keys.data(), keys.size());\n+                try {\n+                    indexer.map_locations(keys.data(), keys.size());\n+                } catch (const std::exception& e) {\n+                    throw TileDBSOMAError(e.what());\n+                }\n             })\n         // Perform lookup for a large input array of keys and writes the\n         // looked up values into previously allocated array (works for the\ndiff --git a/apis/python/src/tiledbsoma/soma_array.cc b/apis/python/src/tiledbsoma/soma_array.cc\nindex f301f0947a..afad4bc758 100644\n--- a/apis/python/src/tiledbsoma/soma_array.cc\n+++ b/apis/python/src/tiledbsoma/soma_array.cc\n@@ -46,9 +46,13 @@ void write(SOMAArray& array, py::handle py_batch, bool sort_coords = true) {\n     uintptr_t arrow_array_ptr = (uintptr_t)(&arrow_array);\n     py_batch.attr(\"_export_to_c\")(arrow_array_ptr, arrow_schema_ptr);\n \n-    array.set_array_data(\n-        std::make_unique<ArrowSchema>(arrow_schema),\n-        std::make_unique<ArrowArray>(arrow_array));\n+    try {\n+        array.set_array_data(\n+            std::make_unique<ArrowSchema>(arrow_schema),\n+            std::make_unique<ArrowArray>(arrow_array));\n+    } catch (const std::exception& e) {\n+        TPY_ERROR_LOC(e.what());\n+    }\n \n     try {\n         array.write(sort_coords);\n", "instance_id": "single-cell-data__TileDB-SOMA-2980", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear, providing context for two distinct issues: fixing a nightly build failure related to `pybind11` exception mapping and introducing a new concurrency configuration setting in the SOMA library. The goals are outlined, with references to related issues and PRs, and the changes are described at a high level (e.g., explicit try-catch for exception handling, new configuration setting for concurrency). However, there are minor ambiguities and missing details. For instance, the exception mapping issue lacks a clear explanation of the root cause or specific test failures beyond referencing randomness in error types across machines. Additionally, the concurrency configuration change does not specify potential impacts on existing code or performance, nor does it detail how the new setting interacts with other configurations. Edge cases or constraints are not explicitly mentioned for either issue. Despite these gaps, the overall intent and scope of the changes are understandable, especially with the provided code diffs and reviewer notes.\n", "difficulty_explanation": "\nI rate the difficulty of this problem at 0.65, placing it in the \"Hard\" category due to the combination of technical complexity and the need for deep understanding of specific domains. Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of Problem Description**: While the problem statement is mostly clear, the inherent logic behind exception mapping in `pybind11` and the randomness of error types across environments introduces complexity. Understanding why exceptions behave inconsistently requires knowledge of Python-C++ bindings and potential race conditions in exception registration.\n\n2. **Scope and Depth of Code Changes**: The code changes span multiple files in the `apis/python/src/tiledbsoma/` directory, affecting exception handling logic and configuration settings. The exception handling modifications involve adding explicit try-catch blocks across several functions, which is a moderate change but requires consistency. The concurrency configuration change (though not fully shown in the provided diff) suggests a broader impact on the system's threading model, potentially affecting performance and requiring careful integration with existing TileDB Core settings. The changes do not appear to significantly alter the system's architecture but do touch critical boundary areas (C++-Python interface).\n\n3. **Number of Technical Concepts**: Solving this problem requires understanding several advanced concepts:\n   - `pybind11` for Python-C++ bindings and exception translation mechanisms.\n   - TileDB and SOMA library internals, particularly how exceptions are propagated and registered.\n   - Concurrency and thread pool management in a native library context, including configuration parsing and validation.\n   - Error handling patterns at language boundaries, which can be non-trivial due to differences in error representation between Python and C++.\n   These concepts are moderately to highly complex, especially for developers without prior experience in mixed-language programming or TileDB internals.\n\n4. **Edge Cases and Error Handling**: The problem explicitly involves improving error handling by adding try-catch blocks to ensure consistent exception propagation to Python as `TileDBSOMAError`. The concurrency configuration change mentions bounds checking (1 to 1024) and improved error handling for malformed config values, indicating attention to edge cases. However, the complexity of these edge cases is moderate; for instance, handling invalid config values is straightforward, but ensuring exception consistency across environments might involve subtle edge cases related to runtime behavior.\n\nOverall, this problem requires a deep understanding of the TileDB-SOMA codebase, particularly the Python-C++ interface, and involves moderately complex changes across multiple files. The need to handle exceptions consistently and introduce a new configuration setting with proper validation pushes this into the \"Hard\" range. However, it does not reach \"Very Hard\" as it does not involve system-level redesign or extremely intricate domain-specific challenges.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] Unable to bind mount when mount source is within the daemon root\n### Description\n\nAfter upgrading Compose v2.29.6 from v2.29.2, the following service began failing:\r\n\r\n```\r\nservices:\r\n  cadvisor:\r\n    image: gcr.io/cadvisor/cadvisor:latest\r\n    privileged: true\r\n    volumes:\r\n      - /:/rootfs:ro\r\n      - /var/run:/var/run:ro\r\n      - /sys:/sys:ro\r\n      - /var/lib/docker/:/var/lib/docker:ro\r\n      - /dev/disk/:/dev/disk:ro\r\n```\r\n\r\nRunning `docker compose up -d` gives the following error message:\r\n\r\n```\r\nError response from daemon: invalid mount config: must use either propagation mode \"rslave\" or \"rshared\" when mount source is within the daemon root, daemon root: \"/var/lib/docker\", bind mount source: \"/\", propagation: \"rprivate\"\r\n```\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\nDocker Compose version v2.29.6\n```\n\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\r\n Version:    27.3.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.17.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.29.6\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 26.1.3\r\n Storage Driver: vfs\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8b3b7ca2e5ce38e8f31a34f35b2b68ceb8470d89\r\n runc version: v1.1.14-0-g2c9f560\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.6.16-linuxkit\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 12\r\n Total Memory: 7.657GiB\r\n Name: d4d8c60ee1fe\r\n ID: 186ebd80-8c59-49b8-92ae-007c1c620318\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\n```\n\n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/pkg/compose/compose.go b/pkg/compose/compose.go\nindex a93c95bc24..10322fc7d8 100644\n--- a/pkg/compose/compose.go\n+++ b/pkg/compose/compose.go\n@@ -321,23 +321,6 @@ func (s *composeService) RuntimeVersion(ctx context.Context) (string, error) {\n \n }\n \n-var windowsContainer = struct {\n-\tonce sync.Once\n-\tval  bool\n-\terr  error\n-}{}\n-\n-func (s *composeService) isWindowsContainer(ctx context.Context) (bool, error) {\n-\twindowsContainer.once.Do(func() {\n-\t\tinfo, err := s.apiClient().Info(ctx)\n-\t\tif err != nil {\n-\t\t\twindowsContainer.err = err\n-\t\t}\n-\t\twindowsContainer.val = info.OSType == \"windows\"\n-\t})\n-\treturn windowsContainer.val, windowsContainer.err\n-}\n-\n func (s *composeService) isDesktopIntegrationActive() bool {\n \treturn s.desktopCli != nil\n }\ndiff --git a/pkg/compose/create.go b/pkg/compose/create.go\nindex 08b9a0aa44..c4d1734fa5 100644\n--- a/pkg/compose/create.go\n+++ b/pkg/compose/create.go\n@@ -801,28 +801,28 @@ func (s *composeService) buildContainerVolumes(\n \t\treturn nil, nil, err\n \t}\n \n-\tmountOptions, err := s.buildContainerMountOptions(ctx, p, service, imgInspect, inherit)\n+\tmountOptions, err := buildContainerMountOptions(p, service, imgInspect, inherit)\n \tif err != nil {\n \t\treturn nil, nil, err\n \t}\n \n-\tversion, err := s.RuntimeVersion(ctx)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tif versions.GreaterThan(version, \"1.42\") {\n-\t\t// We can fully leverage `Mount` API as a replacement for legacy `Bind`\n-\t\treturn nil, mountOptions, nil\n-\t}\n-\n MOUNTS:\n \tfor _, m := range mountOptions {\n+\t\tif m.Type == mount.TypeNamedPipe {\n+\t\t\tmounts = append(mounts, m)\n+\t\t\tcontinue\n+\t\t}\n \t\tif m.Type == mount.TypeBind {\n-\t\t\t// `Mount` does not offer option to created host path if missing\n+\t\t\t// `Mount` is preferred but does not offer option to created host path if missing\n \t\t\t// so `Bind` API is used here with raw volume string\n+\t\t\t// see https://github.com/moby/moby/issues/43483\n \t\t\tfor _, v := range service.Volumes {\n \t\t\t\tif v.Target == m.Target {\n-\t\t\t\t\tif v.Bind != nil && v.Bind.CreateHostPath {\n+\t\t\t\t\tswitch {\n+\t\t\t\t\tcase string(m.Type) != v.Type:\n+\t\t\t\t\t\tv.Source = m.Source\n+\t\t\t\t\t\tfallthrough\n+\t\t\t\t\tcase v.Bind != nil && v.Bind.CreateHostPath:\n \t\t\t\t\t\tbinds = append(binds, v.String())\n \t\t\t\t\t\tcontinue MOUNTS\n \t\t\t\t\t}\n@@ -834,7 +834,7 @@ MOUNTS:\n \treturn binds, mounts, nil\n }\n \n-func (s *composeService) buildContainerMountOptions(ctx context.Context, p types.Project, service types.ServiceConfig, img moby.ImageInspect, inherit *moby.Container) ([]mount.Mount, error) {\n+func buildContainerMountOptions(p types.Project, s types.ServiceConfig, img moby.ImageInspect, inherit *moby.Container) ([]mount.Mount, error) {\n \tvar mounts = map[string]mount.Mount{}\n \tif inherit != nil {\n \t\tfor _, m := range inherit.Mounts {\n@@ -859,7 +859,7 @@ func (s *composeService) buildContainerMountOptions(ctx context.Context, p types\n \t\t\t\t}\n \t\t\t}\n \t\t\tvolumes := []types.ServiceVolumeConfig{}\n-\t\t\tfor _, v := range service.Volumes {\n+\t\t\tfor _, v := range s.Volumes {\n \t\t\t\tif v.Target != m.Destination || v.Source != \"\" {\n \t\t\t\t\tvolumes = append(volumes, v)\n \t\t\t\t\tcontinue\n@@ -872,11 +872,11 @@ func (s *composeService) buildContainerMountOptions(ctx context.Context, p types\n \t\t\t\t\tReadOnly: !m.RW,\n \t\t\t\t}\n \t\t\t}\n-\t\t\tservice.Volumes = volumes\n+\t\t\ts.Volumes = volumes\n \t\t}\n \t}\n \n-\tmounts, err := s.fillBindMounts(ctx, p, service, mounts)\n+\tmounts, err := fillBindMounts(p, s, mounts)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -888,27 +888,27 @@ func (s *composeService) buildContainerMountOptions(ctx context.Context, p types\n \treturn values, nil\n }\n \n-func (s *composeService) fillBindMounts(ctx context.Context, p types.Project, service types.ServiceConfig, m map[string]mount.Mount) (map[string]mount.Mount, error) {\n-\tfor _, v := range service.Volumes {\n-\t\tbindMount, err := s.buildMount(ctx, p, v)\n+func fillBindMounts(p types.Project, s types.ServiceConfig, m map[string]mount.Mount) (map[string]mount.Mount, error) {\n+\tfor _, v := range s.Volumes {\n+\t\tbindMount, err := buildMount(p, v)\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n \t\tm[bindMount.Target] = bindMount\n \t}\n \n-\tsecrets, err := s.buildContainerSecretMounts(ctx, p, service)\n+\tsecrets, err := buildContainerSecretMounts(p, s)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\tfor _, secret := range secrets {\n-\t\tif _, found := m[secret.Target]; found {\n+\tfor _, s := range secrets {\n+\t\tif _, found := m[s.Target]; found {\n \t\t\tcontinue\n \t\t}\n-\t\tm[secret.Target] = secret\n+\t\tm[s.Target] = s\n \t}\n \n-\tconfigs, err := s.buildContainerConfigMounts(ctx, p, service)\n+\tconfigs, err := buildContainerConfigMounts(p, s)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -921,11 +921,11 @@ func (s *composeService) fillBindMounts(ctx context.Context, p types.Project, se\n \treturn m, nil\n }\n \n-func (s *composeService) buildContainerConfigMounts(ctx context.Context, p types.Project, service types.ServiceConfig) ([]mount.Mount, error) {\n+func buildContainerConfigMounts(p types.Project, s types.ServiceConfig) ([]mount.Mount, error) {\n \tvar mounts = map[string]mount.Mount{}\n \n \tconfigsBaseDir := \"/\"\n-\tfor _, config := range service.Configs {\n+\tfor _, config := range s.Configs {\n \t\ttarget := config.Target\n \t\tif config.Target == \"\" {\n \t\t\ttarget = configsBaseDir + config.Source\n@@ -953,7 +953,7 @@ func (s *composeService) buildContainerConfigMounts(ctx context.Context, p types\n \t\t\tcontinue\n \t\t}\n \n-\t\tbindMount, err := s.buildMount(ctx, p, types.ServiceVolumeConfig{\n+\t\tbindMount, err := buildMount(p, types.ServiceVolumeConfig{\n \t\t\tType:     types.VolumeTypeBind,\n \t\t\tSource:   definedConfig.File,\n \t\t\tTarget:   target,\n@@ -971,11 +971,11 @@ func (s *composeService) buildContainerConfigMounts(ctx context.Context, p types\n \treturn values, nil\n }\n \n-func (s *composeService) buildContainerSecretMounts(ctx context.Context, p types.Project, service types.ServiceConfig) ([]mount.Mount, error) {\n+func buildContainerSecretMounts(p types.Project, s types.ServiceConfig) ([]mount.Mount, error) {\n \tvar mounts = map[string]mount.Mount{}\n \n \tsecretsDir := \"/run/secrets/\"\n-\tfor _, secret := range service.Secrets {\n+\tfor _, secret := range s.Secrets {\n \t\ttarget := secret.Target\n \t\tif secret.Target == \"\" {\n \t\t\ttarget = secretsDir + secret.Source\n@@ -1003,7 +1003,7 @@ func (s *composeService) buildContainerSecretMounts(ctx context.Context, p types\n \t\t\tcontinue\n \t\t}\n \n-\t\tmnt, err := s.buildMount(ctx, p, types.ServiceVolumeConfig{\n+\t\tmnt, err := buildMount(p, types.ServiceVolumeConfig{\n \t\t\tType:     types.VolumeTypeBind,\n \t\t\tSource:   definedSecret.File,\n \t\t\tTarget:   target,\n@@ -1039,7 +1039,7 @@ func isWindowsAbs(p string) bool {\n \treturn false\n }\n \n-func (s *composeService) buildMount(ctx context.Context, project types.Project, volume types.ServiceVolumeConfig) (mount.Mount, error) {\n+func buildMount(project types.Project, volume types.ServiceVolumeConfig) (mount.Mount, error) {\n \tsource := volume.Source\n \t// on windows, filepath.IsAbs(source) is false for unix style abs path like /var/run/docker.sock.\n \t// do not replace these with  filepath.Abs(source) that will include a default drive.\n@@ -1060,10 +1060,7 @@ func (s *composeService) buildMount(ctx context.Context, project types.Project,\n \t\t}\n \t}\n \n-\tbind, vol, tmpfs, err := s.buildMountOptions(ctx, volume)\n-\tif err != nil {\n-\t\treturn mount.Mount{}, err\n-\t}\n+\tbind, vol, tmpfs := buildMountOptions(project, volume)\n \n \tvolume.Target = path.Clean(volume.Target)\n \n@@ -1083,7 +1080,7 @@ func (s *composeService) buildMount(ctx context.Context, project types.Project,\n \t}, nil\n }\n \n-func (s *composeService) buildMountOptions(ctx context.Context, volume types.ServiceVolumeConfig) (*mount.BindOptions, *mount.VolumeOptions, *mount.TmpfsOptions, error) {\n+func buildMountOptions(project types.Project, volume types.ServiceVolumeConfig) (*mount.BindOptions, *mount.VolumeOptions, *mount.TmpfsOptions) {\n \tswitch volume.Type {\n \tcase \"bind\":\n \t\tif volume.Volume != nil {\n@@ -1092,8 +1089,7 @@ func (s *composeService) buildMountOptions(ctx context.Context, volume types.Ser\n \t\tif volume.Tmpfs != nil {\n \t\t\tlogrus.Warnf(\"mount of type `bind` should not define `tmpfs` option\")\n \t\t}\n-\t\toption, err := s.buildBindOption(ctx, volume.Bind)\n-\t\treturn option, nil, nil, err\n+\t\treturn buildBindOption(volume.Bind), nil, nil\n \tcase \"volume\":\n \t\tif volume.Bind != nil {\n \t\t\tlogrus.Warnf(\"mount of type `volume` should not define `bind` option\")\n@@ -1101,7 +1097,12 @@ func (s *composeService) buildMountOptions(ctx context.Context, volume types.Ser\n \t\tif volume.Tmpfs != nil {\n \t\t\tlogrus.Warnf(\"mount of type `volume` should not define `tmpfs` option\")\n \t\t}\n-\t\treturn nil, buildVolumeOptions(volume.Volume), nil, nil\n+\t\tif v, ok := project.Volumes[volume.Source]; ok && v.DriverOpts[\"o\"] == types.VolumeTypeBind {\n+\t\t\treturn buildBindOption(&types.ServiceVolumeBind{\n+\t\t\t\tCreateHostPath: true,\n+\t\t\t}), nil, nil\n+\t\t}\n+\t\treturn nil, buildVolumeOptions(volume.Volume), nil\n \tcase \"tmpfs\":\n \t\tif volume.Bind != nil {\n \t\t\tlogrus.Warnf(\"mount of type `tmpfs` should not define `bind` option\")\n@@ -1109,30 +1110,19 @@ func (s *composeService) buildMountOptions(ctx context.Context, volume types.Ser\n \t\tif volume.Volume != nil {\n \t\t\tlogrus.Warnf(\"mount of type `tmpfs` should not define `volume` option\")\n \t\t}\n-\t\treturn nil, nil, buildTmpfsOptions(volume.Tmpfs), nil\n+\t\treturn nil, nil, buildTmpfsOptions(volume.Tmpfs)\n \t}\n-\treturn nil, nil, nil, nil\n+\treturn nil, nil, nil\n }\n \n-func (s *composeService) buildBindOption(ctx context.Context, bind *types.ServiceVolumeBind) (*mount.BindOptions, error) {\n+func buildBindOption(bind *types.ServiceVolumeBind) *mount.BindOptions {\n \tif bind == nil {\n-\t\treturn nil, nil\n-\t}\n-\n-\tpropagation := bind.Propagation\n-\tisWindowsContainer, err := s.isWindowsContainer(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif propagation == \"\" && !isWindowsContainer {\n-\t\tpropagation = types.PropagationRPrivate\n+\t\treturn nil\n \t}\n-\n \treturn &mount.BindOptions{\n-\t\tPropagation:      mount.Propagation(propagation),\n-\t\tCreateMountpoint: bind.CreateHostPath,\n+\t\tPropagation: mount.Propagation(bind.Propagation),\n \t\t// NonRecursive: false, FIXME missing from model ?\n-\t}, nil\n+\t}\n }\n \n func buildVolumeOptions(vol *types.ServiceVolumeVolume) *mount.VolumeOptions {\n", "instance_id": "docker__compose-12141", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a bug in Docker Compose v2.29.6 where bind mounts fail when the mount source is within the daemon root, with a specific error message provided. The context of the issue (upgrade from v2.29.2) and the affected configuration (cadvisor service with specific volume mounts) are detailed, along with relevant environment information. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or resolution (e.g., should the propagation mode be automatically adjusted to \"rslave\" or \"rshared\"?). Additionally, edge cases or specific constraints related to different environments or configurations are not mentioned. While the issue is reproducible based on the provided configuration and error message, the lack of \"Steps to Reproduce\" content and explicit desired outcome slightly lowers the clarity. Hence, a score of 2 (Mostly Clear) is appropriate.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving modifications across multiple functions in the `create.go` file of the Docker Compose codebase, particularly in how mount options and bind configurations are handled. The changes impact core functionality related to container volume mounting, which is a critical part of the system\u2019s architecture. Second, the technical concepts required to solve this include a deep understanding of Docker\u2019s mount and bind APIs, propagation modes (e.g., \"rprivate\", \"rslave\", \"rshared\"), and the interaction between Compose configurations and the Docker daemon\u2019s root directory constraints. Additionally, the removal of runtime version checks and Windows container detection logic suggests a need to understand backward compatibility and platform-specific behaviors, which adds complexity. Third, while the problem statement does not explicitly mention edge cases, the code changes imply handling of special cases like named pipes and missing host path creation, requiring careful consideration of error handling and configuration validation. Overall, solving this requires a solid grasp of the Docker Compose internals and mount mechanics, justifying a difficulty score of 0.65, leaning towards the lower end of \"Hard\" due to the focused nature of the changes within a single module, despite their complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Segfault on Python versions >=3.9\n**Describe the bug**\r\nCollisionMesh constructor segfaults on Python versions >=3.9.\r\n\r\n**To Reproduce**\r\nI used pyenv to get python versions 3.8.19 and 3.9.19 and ran the same test file (below) with both.  Python 3.8.19 raised no errors while 3.9.19 gave a segfault.\r\n\r\n```python\r\nfrom numpy import array\r\nimport ipctk\r\n\r\npoints = array([[-0.5, 0.5], [0.5, 0.5], [0.5, -0.5]])\r\nedges = array([[0, 1], [1, 2], [2, 0]])\r\nfaces = array([[0, 1, 2]])\r\n\r\nmesh = ipctk.CollisionMesh(points, edges, faces)\r\n```\r\n\r\n**Environment:**\r\n Tested on both Debian Sid and macOS Monterey 12.2.1 with both raising the same issue\r\n\r\n**Additional context**\r\nI haven't tested any other functions, but I assume that there might be other issues as well.\n", "patch": "diff --git a/.github/workflows/clang-format-check.yml b/.github/workflows/clang-format-check.yml\nindex 17def9a66..af3092c93 100644\n--- a/.github/workflows/clang-format-check.yml\n+++ b/.github/workflows/clang-format-check.yml\n@@ -1,6 +1,12 @@\n name: clang-format Check\n on:\n   push:\n+    paths:\n+      - '.github/workflows/clang-format-check.yml'\n+      - '.clang-format'\n+      - 'src/**'\n+      - 'tests/src/**'\n+      - 'python/src/**'\n   pull_request:\n     paths:\n       - '.github/workflows/clang-format-check.yml'\ndiff --git a/.github/workflows/python.yml b/.github/workflows/python.yml\nindex ccf2c772e..a48fa4956 100644\n--- a/.github/workflows/python.yml\n+++ b/.github/workflows/python.yml\n@@ -23,7 +23,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         os: [ubuntu-latest, macos-latest, windows-latest]\n-        python-version: [\"3.10\"]\n+        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n         include:\n           - os: ubuntu-latest\n             name: Linux\n@@ -65,8 +65,7 @@ jobs:\n         uses: actions/cache@v3.0.11\n         with:\n           path: ${{ env.CACHE_PATH }}\n-          key: ${{ runner.os }}-Python-cache\n-          restore-keys: ${{ runner.os }}-Release-cache\n+          key: ${{ runner.os }}-Python${{ matrix.python-version }}-cache\n \n       - name: Prepare Ccache\n         run: |\ndiff --git a/cmake/recipes/pybind11.cmake b/cmake/recipes/pybind11.cmake\nindex 747696283..6d9c46eaa 100644\n--- a/cmake/recipes/pybind11.cmake\n+++ b/cmake/recipes/pybind11.cmake\n@@ -26,4 +26,4 @@ find_package(Python COMPONENTS Interpreter Development.Module REQUIRED)\n set(PYTHON_EXECUTABLE ${Python_EXECUTABLE})\n \n include(CPM)\n-CPMAddPackage(\"gh:pybind/pybind11@2.11.1\")\n+CPMAddPackage(\"gh:pybind/pybind11@2.13.1\")\n", "instance_id": "ipc-sim__ipc-toolkit-100", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a segfault occurs in the `CollisionMesh` constructor of the `ipctk` library when using Python versions >= 3.9. It provides a reproducible code snippet, specifies the environment (Debian Sid and macOS Monterey), and contrasts the behavior between Python 3.8 (no error) and 3.9 (segfault). However, there are minor ambiguities and missing details. For instance, it does not specify whether the segfault occurs consistently across all inputs or only under specific conditions, nor does it mention any error messages or stack traces that could help pinpoint the root cause. Additionally, edge cases or specific configurations (e.g., numpy versions, library dependencies) are not discussed, which could be critical for debugging. Despite these gaps, the core issue is well-defined with a clear reproduction path, justifying a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of the issue involves a segfault, which typically indicates a low-level memory issue in the underlying C++ code (likely via pybind11 bindings) rather than a simple Python-level bug. This requires deep knowledge of C++ memory management, Python C extensions, and pybind11's binding mechanisms, especially since the issue is version-specific (Python >= 3.9). The code changes provided are minimal\u2014updating the pybind11 version from 2.11.1 to 2.13.1 and expanding CI testing to cover multiple Python versions (3.8 to 3.12)\u2014suggesting the fix might be related to compatibility issues addressed in newer pybind11 versions. However, identifying and verifying the root cause (e.g., changes in Python's internal API or memory handling between 3.8 and 3.9) demands significant expertise in debugging across language boundaries. \n\nSecond, while the code changes are small and localized (a version bump and CI updates), the impact is potentially architectural, as pybind11 is a core dependency for Python-C++ interoperability in this project. Understanding the interaction between Python's runtime and the native code, especially with version-specific behavior, adds complexity. Third, the technical concepts involved include advanced topics like Python C API changes, memory safety in C++, and binding library intricacies, which are non-trivial. Finally, edge cases and error handling are not explicitly mentioned in the problem statement, but segfaults inherently require careful consideration of memory-related edge cases (e.g., invalid inputs, large datasets, or concurrent access), which may need to be investigated or mitigated. \n\nGiven these factors\u2014a deep but focused technical challenge, limited code changes but high conceptual complexity, and potential for subtle edge cases\u2014I assign a difficulty score of 0.65, placing it in the lower end of the \"Hard\" category. It does not reach \"Very Hard\" (0.8-1.0) as it likely does not require extensive codebase refactoring or domain-specific knowledge beyond Python-C++ interop, assuming the pybind11 update resolves the issue.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Optional dependency for database requirements\n**Motivation: describe the problem to be solved**\r\nThe most recent version bumped up some requirements, specifically SQLAlchemy 2, which is a good thing and keeping dependencies up to date is important. However, it now forces me to migrate to SQLAlchemy 2 (which is something I should do), and install other database requirements, when it's not part of the library that I use. Something [similar is mentioned here](https://github.com/NannyML/nannyml/issues/331#issuecomment-1804704060).\r\n\r\n**Describe the solution you'd like**\r\nI'd like to add [optional dependencies](https://python-poetry.org/docs/pyproject/#extras) for the database module. As far as I can tell, it's only `sqlmodel` and maybe `psycopg2-binary` that would need an extra.\r\n\r\n**Describe alternatives you've considered**\r\n_just upgrading my internal SQLAlchemy library to handle version 2_, which is something on my todo list.\r\n\n", "patch": "File: NannyML__nannyml-415\nType: block\nName: \nLines: 136-209\nCode:\nfor more information.\n\n We construct the :class:`~nannyml.io.db.database_writer.DatabaseWriter` by providing a database connection string.\n We construct the :class:`~nannyml.io.db.database_writer.DatabaseWriter` by providing a database connection string.\n Upon calling the :meth:`~nannyml.io.base.Writer.write` method, all results will be written into\n Upon calling the :meth:`~nannyml.io.base.Writer.write` method, all results will be written into\n the database, in this case, an `SQLite` database.\n the database, in this case, an `SQLite` database.\nindex 224078b0..e96e2cd2 100644\nindex 224078b0..e96e2cd2 100644\n # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n # 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n # 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n #\n #\n__version__ = '0.11.0'\n__version__ = '0.12.0'\n \n \n import logging\n import logging\n \n \n from dotenv import load_dotenv\n from dotenv import load_dotenv\nfrom importlib import import_module\n\n \n \n from .calibration import Calibrator, IsotonicCalibrator, needs_calibration\n from .calibration import Calibrator, IsotonicCalibrator, needs_calibration\n from .chunk import Chunk, Chunker, CountBasedChunker, DefaultChunker, PeriodBasedChunker, SizeBasedChunker\n from .chunk import Chunk, Chunker, CountBasedChunker, DefaultChunker, PeriodBasedChunker, SizeBasedChunker\nfrom .data_quality import MissingValuesCalculator, UnseenValuesCalculator, NumericalRangeCalculator\nfrom .data_quality import MissingValuesCalculator, NumericalRangeCalculator, UnseenValuesCalculator\n from .datasets import (\n from .datasets import (\n     load_modified_california_housing_dataset,\n     load_modified_california_housing_dataset,\n     load_synthetic_binary_classification_dataset,\n     load_synthetic_binary_classification_dataset,\n     UnivariateDriftCalculator,\n     UnivariateDriftCalculator,\n )\n )\n from .exceptions import ChunkerException, InvalidArgumentsException, MissingMetadataException\n from .exceptions import ChunkerException, InvalidArgumentsException, MissingMetadataException\nfrom .io import DatabaseWriter, PickleFileWriter, RawFilesWriter\nfrom .io import PickleFileWriter, RawFilesWriter\n from .performance_calculation import PerformanceCalculator\n from .performance_calculation import PerformanceCalculator\n from .performance_estimation import CBPE, DLE\n from .performance_estimation import CBPE, DLE\n from .stats import (\n from .stats import (\n )\n )\n from .usage_logging import UsageEvent, disable_usage_logging, enable_usage_logging, log_usage\n from .usage_logging import UsageEvent, disable_usage_logging, enable_usage_logging, log_usage\n \n \n\n_optional_dependencies = {\n    'DatabaseWriter': '.io.db',\n}\n\n\ndef __getattr__(name: str):\n    optional_module_path = _optional_dependencies.get(name)\n    if optional_module_path is not None:\n        module = import_module(optional_module_path, package=__name__)\n        return getattr(module, name)\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n\nFile: NannyML__nannyml-415\nType: block\nName: \nLines: 267-272\nCode:\ntry:\n    from sqlmodel import Session, SQLModel, create_engine, select\nexcept ImportError:\n    raise ImportError(\n        \"`sqlmodel` module is not available. Please install the `nannyml[db]` extra to use this functionality.\"\n    )\n\nFile: NannyML__nannyml-415\nType: block\nName: \nLines: 298-303\nCode:\ntry:\n    from sqlmodel import Field, Relationship, SQLModel\nexcept ImportError:\n    raise ImportError(\n        \"`sqlmodel` module is not available. Please install the `nannyml[db]` extra to use this functionality.\"\n    )\n", "instance_id": "NannyML__nannyml-415", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the motivation and the desired solution. The goal of adding optional dependencies for database requirements (specifically for SQLAlchemy 2 and related libraries like `sqlmodel` and `psycopg2-binary`) is articulated, along with a reference to a similar issue for context. The solution proposed\u2014using Python Poetry's optional dependencies (extras)\u2014is also mentioned. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or constraints for users who do not install the optional dependencies. Additionally, there are no examples or detailed requirements about how the optional dependencies should integrate with the existing codebase or how they should be tested. Edge cases, such as compatibility with different database backends or handling installation failures, are not addressed. Despite these gaps, the overall intent and scope of the problem are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes provided indicate modifications primarily in a single module or area of the codebase, focusing on the dynamic import of optional dependencies and error handling for missing libraries. The changes involve adding logic for lazy loading of the `DatabaseWriter` module and raising informative errors if dependencies like `sqlmodel` are not installed. The impact is localized and does not appear to affect the broader system architecture significantly. The amount of code change is relatively small, involving a few blocks of code.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding Python's dynamic module loading (`__getattr__` and `import_module`), Python packaging concepts (optional dependencies or \"extras\" in Poetry), and basic error handling. These are moderately advanced Python features but are well-documented and commonly used in library development. No complex algorithms, design patterns, or domain-specific knowledge beyond Python packaging are required.\n\n3. **Edge Cases and Error Handling**: The problem does not explicitly mention specific edge cases in the statement, but the code changes show a focus on error handling for missing dependencies (e.g., raising an `ImportError` with a helpful message). This is a straightforward error condition to handle, and no particularly complex edge cases (e.g., partial installations, version conflicts) are addressed in the provided changes or statement.\n\n4. **Overall Complexity**: The task requires a basic-to-intermediate understanding of Python library development and dependency management. It involves implementing a feature (optional dependencies) that is conceptually simple but requires careful integration to ensure users are not forced to install unnecessary packages. The changes do not impact performance, scalability, or core functionality in a significant way, and there is no need for deep architectural refactoring.\n\nA score of 0.35 reflects an Easy problem that requires some understanding of Python's module system and packaging but does not pose significant technical challenges or require extensive modifications across the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
