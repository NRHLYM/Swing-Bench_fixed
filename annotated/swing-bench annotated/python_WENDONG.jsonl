{"problem_statement": "[help wanted]: rename vllm/logging module to avoid shadowing builtin logging module \n### Anything you want to discuss about vllm.\n\nsee https://github.com/vllm-project/vllm/issues/2021#issuecomment-2463435586 for more details.\r\n\r\nwe'd better avoid having a module that can potentially shadow python's builtin module, so that error trace can be clear.\r\n\r\nsolution could be:\r\n\r\nrename `vllm.logging` to `vllm.logging_utils`\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex bae8645502dea..db373a4b1dd02 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -68,7 +68,7 @@ files = [\n     \"vllm/entrypoints\",\n     \"vllm/core\",\n     \"vllm/inputs\",\n-    \"vllm/logging\",\n+    \"vllm/logging_utils\",\n     \"vllm/multimodal\",\n     \"vllm/platforms\",\n     \"vllm/transformers_utils\",\ndiff --git a/vllm/logger.py b/vllm/logger.py\nindex d6fcda02a0fb3..80b9fcc59272d 100644\n--- a/vllm/logger.py\n+++ b/vllm/logger.py\n@@ -24,7 +24,7 @@\n DEFAULT_LOGGING_CONFIG = {\n     \"formatters\": {\n         \"vllm\": {\n-            \"class\": \"vllm.logging.NewLineFormatter\",\n+            \"class\": \"vllm.logging_utils.NewLineFormatter\",\n             \"datefmt\": _DATE_FORMAT,\n             \"format\": _FORMAT,\n         },\ndiff --git a/vllm/logging/__init__.py b/vllm/logging/__init__.py\ndeleted file mode 100644\nindex b9aec380776f3..0000000000000\n--- a/vllm/logging/__init__.py\n+++ /dev/null\n@@ -1,5 +0,0 @@\n-from vllm.logging.formatter import NewLineFormatter\n-\n-__all__ = [\n-    \"NewLineFormatter\",\n-]\ndiff --git a/vllm/logging_utils/__init__.py b/vllm/logging_utils/__init__.py\nnew file mode 100644\nindex 0000000000000..576ccf78a8117\n--- /dev/null\n+++ b/vllm/logging_utils/__init__.py\n@@ -0,0 +1,5 @@\n+from vllm.logging_utils.formatter import NewLineFormatter\n+\n+__all__ = [\n+    \"NewLineFormatter\",\n+]\ndiff --git a/vllm/logging/formatter.py b/vllm/logging_utils/formatter.py\nsimilarity index 100%\nrename from vllm/logging/formatter.py\nrename to vllm/logging_utils/formatter.py\n", "instance_id": "vllm-project__vllm-10134", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to rename the `vllm/logging` module to `vllm/logging_utils` to avoid shadowing Python's built-in `logging` module, which could cause confusion in error traces. The goal is explicitly stated, and a specific solution is suggested. However, there are minor ambiguities and missing details. For instance, the problem statement does not discuss potential downstream impacts of renaming the module (e.g., compatibility issues with existing codebases or third-party dependencies that might rely on the old module name). Additionally, there are no examples or detailed steps provided for ensuring a smooth transition, such as handling backward compatibility or updating documentation. While the linked issue provides some context, the statement itself lacks comprehensive details about constraints or edge cases that might arise during the rename. Thus, it falls into the \"Mostly Clear\" category with minor details missing.", "difficulty_explanation": "The difficulty of this task is very low, falling into the 0.0-0.2 range. The code changes involve a straightforward renaming of a module from `vllm/logging` to `vllm/logging_utils`, which is reflected in the provided diff. The scope of changes is limited to a few files (`pyproject.toml`, `logger.py`, and the module directory itself), with minimal impact on the overall codebase architecture. The modifications are mostly mechanical\u2014updating file paths and references\u2014and do not require deep understanding of the system's logic or interactions between modules beyond the renamed component. The technical concepts involved are basic, limited to understanding Python module naming and import mechanisms, with no complex algorithms, design patterns, or domain-specific knowledge required. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the code changes do not introduce new logic that would necessitate such considerations. The primary challenge might be ensuring that all references to the old module name are updated, but this is a routine task that can be handled with simple search-and-replace operations or refactoring tools. Overall, this is a very easy task requiring only basic code modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Curly brackets are not being escaped\nHi guys,\r\n\r\nCurly brackets are not being escaped and a couple of sigma rules are causing syntax errors.\r\n\r\nSource:\r\nhttps://github.com/SigmaHQ/pySigma-backend-crowdstrike/blob/1fdd29d5cac228687398769a09a8a4f68680b0b4/sigma/backends/crowdstrike/logscale.py#L127\r\n\r\nSuggested fix:\r\n```\r\nadd_escaped_re: ClassVar[str] = \"*$^.|?()[]{}+/\"\r\n```\r\n\r\nAffected sigma rules:\r\n* https://github.com/SigmaHQ/sigma/blob/598d29f811c1859ba18e05b8c419cc94410c9a55/rules/windows/process_creation/proc_creation_win_hktl_relay_attacks_tools.yml#L47\r\n* https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_powershell_iex_patterns.yml#L25\n", "patch": "diff --git a/sigma/backends/crowdstrike/logscale.py b/sigma/backends/crowdstrike/logscale.py\nindex 53a7b17..129aeeb 100644\n--- a/sigma/backends/crowdstrike/logscale.py\n+++ b/sigma/backends/crowdstrike/logscale.py\n@@ -124,7 +124,7 @@ class LogScaleBackend(TextQueryBackend):\n     escape_char_re: ClassVar[str] = \"\\\\\"\n     wildcard_multi_re: ClassVar[str] = \".*\"\n     wildcard_single_re: ClassVar[str] = \".\"\n-    add_escaped_re: ClassVar[str] = \"*$^.|?()[]+/\"\n+    add_escaped_re: ClassVar[str] = \"*$^.|?()[]+/{}\"\n     filter_chars_re: ClassVar[str] = \"\"\n     bool_values_re: ClassVar[Dict[bool, str]] = {\n         True: \"true\",\n", "instance_id": "SigmaHQ__pySigma-backend-crowdstrike-16", "clarity": 1, "difficulty": 0.1, "clarity_explanation": "The problem statement is valid but lacks critical details, leading to significant ambiguities. While the issue of curly brackets not being escaped is mentioned, along with references to specific Sigma rules causing syntax errors, the statement does not clearly define the expected behavior or the exact nature of the syntax errors. There is no explanation of why escaping curly brackets is necessary, what the impact of not escaping them is, or how the suggested fix resolves the issue. Additionally, there are no examples of input/output or specific test cases to validate the fix. The links to the source code and affected rules provide some context, but they require the reader to investigate further to fully understand the problem. Overall, the problem description is incomplete and leaves room for misinterpretation.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a minimal and straightforward code change. The modification is limited to a single line in a single file, where curly brackets are added to a string constant (`add_escaped_re`) that defines characters to be escaped. This change does not require deep understanding of the codebase, complex logic, or advanced technical concepts beyond basic string manipulation. There are no apparent edge cases or error handling considerations mentioned or required based on the provided diff. The scope of the change is extremely narrow, with no impact on the system's architecture or interactions between modules. This task falls into the \"very easy\" category, as it is essentially a simple fix to a constant value.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "load_sheet_by_name raises generic CalamineError instead of SheetNotFoundError\nHi,\r\n\r\nWhile loading a sheet from a reader, if a missing sheet name is provided a generic `CalamineError` is raised instead of the more specific `SheetNotFoundError`.\r\n\r\nMy use case is:\r\n\r\n```python\r\nsheets = []\r\nfor reader in readers:\r\n    try:\r\n        sheet = reader.load_sheet_by_name(\"sheet_name\")\r\n    except SheetNotFoundError:\r\n        continue\r\n    # Do some other stuff with sheet before appending to list.\r\n```\n", "patch": "diff --git a/src/types/python/excelreader.rs b/src/types/python/excelreader.rs\nindex 8de50de..e295e6c 100644\n--- a/src/types/python/excelreader.rs\n+++ b/src/types/python/excelreader.rs\n@@ -144,14 +144,25 @@ impl ExcelReader {\n         let name = idx_or_name\n             .try_into()\n             .and_then(|idx_or_name| match idx_or_name {\n-                IdxOrName::Name(name) => Ok(name),\n+                IdxOrName::Name(name) => {\n+                    if self.sheet_names.contains(&name) {\n+                        Ok(name)\n+                    } else {\n+                        Err(FastExcelErrorKind::SheetNotFound(IdxOrName::Name(name.clone())).into()).with_context(||  {\n+                            let available_sheets = self.sheet_names.iter().map(|s| format!(\"\\\"{s}\\\"\")).collect::<Vec<_>>().join(\", \");\n+                            format!(\n+                                \"Sheet \\\"{name}\\\" not found in file. Available sheets: {available_sheets}.\"\n+                            )\n+                        })\n+                    }\n+                }\n                 IdxOrName::Idx(idx) => self\n                     .sheet_names\n                     .get(idx)\n                     .ok_or_else(|| FastExcelErrorKind::SheetNotFound(IdxOrName::Idx(idx)).into())\n                     .with_context(|| {\n                         format!(\n-                            \"Sheet index {idx} is out of range. File has {} sheets\",\n+                            \"Sheet index {idx} is out of range. File has {} sheets.\",\n                             self.sheet_names.len()\n                         )\n                     })\n", "instance_id": "ToucanToco__fastexcel-222", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a generic `CalamineError` is raised instead of a more specific `SheetNotFoundError` when a sheet name is not found. The goal is evident\u2014modify the error type to be more specific for better error handling in user code, as demonstrated by the provided Python snippet. However, there are minor ambiguities. The statement does not explicitly mention whether this change should apply only to name-based lookups or also to index-based lookups (though the code changes clarify this). Additionally, it lacks details on potential edge cases, such as empty sheet names or case sensitivity in name matching, and does not specify if the error message should include additional context (though the code changes address this). Overall, the problem is valid and mostly clear, but these minor missing details prevent a perfect score.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modification is localized to a single file (`excelreader.rs`) and specifically to a small section of the code handling sheet name lookups. The diff shows a focused change of about 10-15 lines, adding a conditional check and a detailed error message. There is no impact on the broader system architecture or interactions with other modules, making the scope very narrow.\n\n2. **Number of Technical Concepts:** The solution requires basic familiarity with Rust's error handling mechanisms (e.g., `Result`, custom error types like `FastExcelErrorKind`), string manipulation, and collection operations (checking if a name exists in a vector). These are fundamental concepts in Rust and do not involve advanced language features, libraries, or complex algorithms. No design patterns or domain-specific knowledge beyond basic file parsing logic are needed.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code change introduces a more specific error type (`SheetNotFoundError`) and enhances the error message with a list of available sheets, which is a straightforward improvement. There are no complex edge cases to handle beyond checking for the existence of a sheet name, and the error handling logic added is simple.\n\n4. **Overall Complexity:** The task involves understanding a small part of the codebase and making a simple modification to improve error specificity. It does not require deep knowledge of the entire system or intricate logic, aligning with an easy difficulty level. I assign a score of 0.25 within the easy range because, while it is not as trivial as fixing a typo, it still involves minimal complexity and risk of introducing bugs.\n\nIn summary, this is an easy problem requiring a small, focused change with basic Rust knowledge and minimal impact on the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add nebari upgrade basic test workflow\nFrom an offline convo I had with @dcmcand:\r\n\r\n- We want the workflow to be triggered by pre-releases or optionally manually\r\n- We want to use local provider\r\n\r\nThe steps should be\r\n\r\n1. Deploy the most recent released version of nebari\r\n2. Perform a health check (`curl` some endpoints for now)\r\n3. Run `nebari upgrade` for the version that triggered the workflow\r\n4. Perform the health check again\r\n\r\nDepending on the scope of the health checks, which we want to keep minimal for the initial version of this test, this might or might not detect whether the upgrade does not break anything.\r\n\r\nIn any case, it will not be able to prevent the data loss described in the top comment that happened because a resource was deleted and recreated. To test that we would need to add some state in between step 1. and 2. above and verify its integrity after step 4.\r\n\r\n_Originally posted by @pmeier in https://github.com/nebari-dev/nebari/issues/2701#issuecomment-2419405929_\r\n            \n", "patch": "diff --git a/.github/actions/health-check/action.yml b/.github/actions/health-check/action.yml\nnew file mode 100644\nindex 000000000..14602b3d7\n--- /dev/null\n+++ b/.github/actions/health-check/action.yml\n@@ -0,0 +1,19 @@\n+name: health-check\n+description: \"Check health of Nebari deployment\"\n+\n+inputs:\n+  domain:\n+    description: Domain name\n+    required: true\n+\n+runs:\n+  using: composite\n+\n+  steps:\n+    - name: List kubernetes components\n+      shell: bash\n+      run: kubectl get --all-namespaces all,cm,secret,pv,pvc,ing\n+\n+    - name: Check if JupyterHub login page is accessible\n+      shell: bash\n+      run: curl --insecure --include 'https://${{ inputs.domain }}/hub/home'\ndiff --git a/.github/actions/init-local/action.yml b/.github/actions/init-local/action.yml\nnew file mode 100644\nindex 000000000..306876973\n--- /dev/null\n+++ b/.github/actions/init-local/action.yml\n@@ -0,0 +1,81 @@\n+name: init-local\n+description: \"Initialize Nebari config for local deployment\"\n+\n+inputs:\n+  directory:\n+    description: \"Path to directory to initialize in\"\n+    required: false\n+    default: './local-deployment'\n+\n+outputs:\n+  directory:\n+    description: \"Path to config directory\"\n+    value: ${{ steps.metadata.outputs.directory }}\n+  config:\n+    description: \"Path to Nebari config\"\n+    value: ${{ steps.metadata.outputs.config }}\n+  project:\n+    description: \"Project name\"\n+    value: ${{ steps.metadata.outputs.project }}\n+  domain:\n+    description: \"Domain name\"\n+    value: ${{ steps.metadata.outputs.domain }}\n+\n+runs:\n+  using: composite\n+\n+  steps:\n+    - shell: bash\n+      id: metadata\n+      run: |\n+        # Setup metadata\n+        DIRECTORY=$(realpath '${{ inputs.directory }}')\n+        mkdir --parents \"${DIRECTORY}\"\n+        echo \"directory=${DIRECTORY}\" | tee --append \"${GITHUB_OUTPUT}\"\n+\n+        CONFIG=\"${DIRECTORY}/nebari-config.yaml\"\n+        echo \"config=${CONFIG}\" | tee --append \"${GITHUB_OUTPUT}\"\n+\n+        PROJECT='github-actions'\n+        echo \"project=${PROJECT}\" | tee --append \"${GITHUB_OUTPUT}\"\n+\n+        DOMAIN='github-actions.nebari.dev'\n+        nslookup \"${DOMAIN}\"\n+        echo \"domain=${DOMAIN}\" | tee --append \"${GITHUB_OUTPUT}\"\n+\n+    - shell: bash -l {0}\n+      id: init\n+      working-directory: ${{ steps.metadata.outputs.directory }}\n+      run: |\n+        nebari init local \\\n+          --project-name '${{ steps.metadata.outputs.project }}' \\\n+          --domain-name '${{ steps.metadata.outputs.domain }}' \\\n+          --auth-provider password \\\n+          --output '${{ steps.metadata.outputs.config }}'\n+\n+    - shell: bash\n+      run: |\n+        # Update nebari config for CI\n+\n+        # Change default JupyterLab theme\n+        cat >> '${{ steps.metadata.outputs.config }}' <<- EOM\n+        jupyterlab:\n+          default_settings:\n+            \"@jupyterlab/apputils-extension:themes\":\n+              theme: JupyterLab Dark\n+        EOM\n+\n+        # Change default value for minio persistence size\n+        cat >> '${{ steps.metadata.outputs.config }}' <<- EOM\n+        monitoring:\n+          enabled: true\n+          overrides:\n+            minio:\n+              persistence:\n+                size: 1Gi\n+        EOM\n+\n+    - shell: bash\n+      run: |\n+        # Display Nebari config\n+        cat '${{ steps.metadata.outputs.config }}'\ndiff --git a/.github/actions/setup-local/action.yml b/.github/actions/setup-local/action.yml\nnew file mode 100644\nindex 000000000..87a12f45d\n--- /dev/null\n+++ b/.github/actions/setup-local/action.yml\n@@ -0,0 +1,30 @@\n+name: setup-local\n+description: \"Setup runner for local deployment\"\n+\n+inputs:\n+  kubectl-version:\n+    description: \"Version of kubectl to install\"\n+    required: false\n+    default: \"1.19.16\"\n+\n+runs:\n+  using: composite\n+\n+  steps:\n+    - uses: azure/setup-kubectl@v4\n+      with:\n+        version: v${{ inputs.kubectl-version }}\n+\n+    - shell: bash\n+      run: |\n+        # Enable docker permissions for user\n+        sudo docker ps\n+        sudo usermod -aG docker $USER && newgrp docker\n+\n+        docker info\n+        docker ps\n+\n+    - shell: bash\n+      run: |\n+        # Get routing table for docker pods\n+        ip route\n", "instance_id": "nebari-dev__nebari-2780", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "\nThe problem statement is mostly clear in outlining the goal of adding a basic test workflow for upgrading Nebari. It specifies the triggering conditions (pre-releases or manual), the use of a local provider, and the high-level steps involved (deploy, health check, upgrade, health check again). However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not specify the exact nature of the health checks beyond a basic `curl` to endpoints, leaving room for interpretation on what constitutes a successful check. Additionally, there is a mention of potential data loss issues, but no clear guidance on whether addressing this is in scope for the initial implementation. Constraints or expectations around the local provider setup and environment are also not detailed. While the intent and primary steps are clear, these minor gaps in specificity result in a score of 2 (Mostly Clear).\n", "difficulty_explanation": "\nI assign a difficulty score of 0.55, placing this problem in the medium range, due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes involve creating new GitHub Actions workflows across multiple files (`health-check`, `init-local`, `setup-local`). These are self-contained additions rather than modifications to existing code, which limits the need to understand a broader codebase. However, the changes span multiple aspects of deployment and testing (initialization, setup, and health checking), requiring a moderate understanding of how these components interact. The amount of code added is not trivial but also not extensive, focusing on scripting and configuration rather than deep architectural changes. There is no significant impact on the system's core architecture, as these are test workflows.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including GitHub Actions (workflow syntax, composite actions, inputs/outputs), Kubernetes (`kubectl` commands), Docker (permissions and routing), shell scripting, and the Nebari tool itself (commands like `nebari init` and `nebari upgrade`). Additionally, there is domain-specific knowledge related to CI/CD pipelines and deployment testing. While none of these concepts are extremely advanced individually, the combination and integration across tools increase the complexity to a moderate level.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases beyond the potential for data loss (which is out of scope for the initial version). The code changes include basic health checks (e.g., `curl` to endpoints) but lack explicit error handling for scenarios like network failures, deployment issues, or unexpected responses from endpoints. Implementing robust error handling or expanding health checks could add complexity, but the current scope keeps this minimal. The local provider setup might introduce edge cases (e.g., environment inconsistencies), but these are not addressed in the provided changes.\n\n4. **Overall Complexity**: The task requires a moderate level of expertise to integrate deployment, testing, and health checking in a CI/CD context. It is not a simple bug fix or feature addition (which would score lower), nor does it involve deep architectural refactoring or advanced algorithmic challenges (which would score higher). The need to coordinate multiple tools and ensure the workflow functions as a cohesive test suite places it in the medium difficulty range.\n\nThus, a score of 0.55 reflects a medium difficulty task that requires understanding multiple concepts and making coordinated changes across several files, with moderate complexity in integration and potential for unaddressed edge cases.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Errors with --error_format  gcc contains only proto filename without --proto_path prefix.\n\r\n**What version of protobuf and what language are you using?**\r\nVersion: main (libprotoc 29.0-dev)\r\nLanguage: C++\r\n**What operating system (Linux, Windows, ...) and version?**\r\nLinux\r\n**What runtime / compiler are you using (e.g., python version or gcc version)**\r\ngcc I guess\r\n\r\n**What did you do?**\r\nConsider following file structure\r\n``` console\r\n$ tree /tmp/protoc_test/\r\n/tmp/protoc_test/\r\n\u251c\u2500\u2500 output\r\n\u2514\u2500\u2500 protos\r\n    \u2514\u2500\u2500 simple.proto\r\n 2 directories, 1 file\r\n```\r\nContent of simple.proto with error(missing semicolon)\r\n```proto\r\nmessage MsgResponse {\r\n     required string msg1 = 1\r\n     required string msg2 = 2;\r\n}\r\n```\r\nCompilation command\r\n```\r\n$ protoc --error_format gcc --cpp_out ./output --proto_path ./protos simple.proto\r\nsimple.proto:3:6: Expected \";\".\r\n```\r\nError is expected, but path to .proto file does not contain `--proto_path` prefix which makes it harder to goto/jump to the file with error.\r\n\r\n**What did you expect to see**\r\nPath to proto with provided `--proto_path` prefix, similar to what `--error_format msvs` provides\r\n**Current** output with `--error_format msvs`:\r\n```bash\r\n# relative\r\n$ protoc --error_format msvs --cpp_out ./output --proto_path ./protos simple.proto\r\nprotos/simple.proto(3) : error in column=6: Expected \";\".\r\n# absolute\r\n$ protoc --error_format msvs --cpp_out ./output --proto_path $PWD/protos simple.proto\r\n/tmp/protoc_test/protos/simple.proto(3) : error in column=6: Expected \";\".\r\n```\r\n\r\n**Expected** output with --error_format gcc:\r\n```\r\n# relative\r\n$ protoc --error_format gcc --cpp_out ./output --proto_path ./protos simple.proto\r\nprotos/simple.proto:3:6: Expected \";\".\r\n# absolute\r\nprotoc --error_format gcc --cpp_out ./output --proto_path $PWD/protos simple.proto\r\n/tmp/protoc_test/protos/simple.proto:3:6: Expected \";\".\r\n```\r\n\nErrors with --error_format  gcc contains only proto filename without --proto_path prefix.\n\r\n**What version of protobuf and what language are you using?**\r\nVersion: main (libprotoc 29.0-dev)\r\nLanguage: C++\r\n**What operating system (Linux, Windows, ...) and version?**\r\nLinux\r\n**What runtime / compiler are you using (e.g., python version or gcc version)**\r\ngcc I guess\r\n\r\n**What did you do?**\r\nConsider following file structure\r\n``` console\r\n$ tree /tmp/protoc_test/\r\n/tmp/protoc_test/\r\n\u251c\u2500\u2500 output\r\n\u2514\u2500\u2500 protos\r\n    \u2514\u2500\u2500 simple.proto\r\n 2 directories, 1 file\r\n```\r\nContent of simple.proto with error(missing semicolon)\r\n```proto\r\nmessage MsgResponse {\r\n     required string msg1 = 1\r\n     required string msg2 = 2;\r\n}\r\n```\r\nCompilation command\r\n```\r\n$ protoc --error_format gcc --cpp_out ./output --proto_path ./protos simple.proto\r\nsimple.proto:3:6: Expected \";\".\r\n```\r\nError is expected, but path to .proto file does not contain `--proto_path` prefix which makes it harder to goto/jump to the file with error.\r\n\r\n**What did you expect to see**\r\nPath to proto with provided `--proto_path` prefix, similar to what `--error_format msvs` provides\r\n**Current** output with `--error_format msvs`:\r\n```bash\r\n# relative\r\n$ protoc --error_format msvs --cpp_out ./output --proto_path ./protos simple.proto\r\nprotos/simple.proto(3) : error in column=6: Expected \";\".\r\n# absolute\r\n$ protoc --error_format msvs --cpp_out ./output --proto_path $PWD/protos simple.proto\r\n/tmp/protoc_test/protos/simple.proto(3) : error in column=6: Expected \";\".\r\n```\r\n\r\n**Expected** output with --error_format gcc:\r\n```\r\n# relative\r\n$ protoc --error_format gcc --cpp_out ./output --proto_path ./protos simple.proto\r\nprotos/simple.proto:3:6: Expected \";\".\r\n# absolute\r\nprotoc --error_format gcc --cpp_out ./output --proto_path $PWD/protos simple.proto\r\n/tmp/protoc_test/protos/simple.proto:3:6: Expected \";\".\r\n```\r\n\n", "patch": "diff --git a/src/google/protobuf/compiler/command_line_interface.cc b/src/google/protobuf/compiler/command_line_interface.cc\nindex ae6b87e4ad1f..f85814197732 100644\n--- a/src/google/protobuf/compiler/command_line_interface.cc\n+++ b/src/google/protobuf/compiler/command_line_interface.cc\n@@ -394,10 +394,6 @@ class CommandLineInterface::ErrorPrinter\n                          std::ostream& out) {\n     std::string dfile;\n     if (\n-#ifndef PROTOBUF_OPENSOURCE\n-        // Print full path when running under MSVS\n-        format_ == CommandLineInterface::ERROR_FORMAT_MSVS &&\n-#endif  // !PROTOBUF_OPENSOURCE\n         tree_ != nullptr && tree_->VirtualFileToDiskFile(filename, &dfile)) {\n       out << dfile;\n     } else {\ndiff --git a/src/google/protobuf/message_lite.h b/src/google/protobuf/message_lite.h\nindex cd1ad9846cbb..58c5c2bdddd0 100644\n--- a/src/google/protobuf/message_lite.h\n+++ b/src/google/protobuf/message_lite.h\n@@ -1206,11 +1206,6 @@ PROTOBUF_ALWAYS_INLINE inline MessageLite* MessageCreator::PlacementNew(\n   //  - We know the minimum size is 16. We have a fallback for when it is not.\n   //  - We can \"underflow\" the buffer because those are the MessageLite bytes\n   //    we will set later.\n-#ifndef PROTO2_OPENSOURCE\n-  // This manual handling shows a 1.85% improvement in the parsing\n-  // microbenchmark.\n-  // TODO: Verify this is still the case.\n-#endif  // !PROTO2_OPENSOUCE\n   if (as_tag == kZeroInit) {\n     // Make sure the input is really all zeros.\n     ABSL_DCHECK(std::all_of(src + sizeof(MessageLite), src + size,\n", "instance_id": "protocolbuffers__protobuf-18614", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when using the `--error_format gcc` option with the `protoc` compiler, the error messages do not include the `--proto_path` prefix in the file path, making it harder to navigate to the file with the error. The statement provides a detailed setup, including file structure, command used, current output, and expected output, which helps in understanding the problem. Examples of both relative and absolute paths are provided for comparison with the `--error_format msvs` output, which is a good reference. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., multiple `--proto_path` entries, nested directories, or symbolic links) or constraints on how the path should be formatted in the error message. Additionally, the statement repeats itself, which slightly hampers readability. Overall, while the goal and context are clear, the lack of discussion on edge cases or additional constraints prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively small and localized. The primary change is in `command_line_interface.cc`, where a conditional block related to error formatting is modified to remove a restriction on full path printing for the MSVS format. This change impacts only a single file and a specific part of the error reporting logic. The second change in `message_lite.h` appears unrelated to the problem statement (it deals with parsing optimization) and may be a part of a larger diff not directly tied to the issue at hand. Thus, the relevant modification is minimal and does not affect the broader system architecture.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of C++ and the Protobuf compiler's internals, specifically how error messages are formatted and how file paths are resolved using the `--proto_path` option. Familiarity with the `DiskSourceTree` class (used for mapping virtual files to disk files) and command-line argument handling in the Protobuf toolchain is necessary but not overly complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond the Protobuf compiler's error reporting mechanism are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but implementing the solution might require considering scenarios like multiple `--proto_path` entries, absolute vs. relative paths, or invalid paths. However, the provided code change does not introduce new error handling logic; it simply adjusts the condition for printing the full path. The complexity of handling such edge cases appears minimal based on the diff.\n\n4. **Overall Complexity**: The task involves understanding a specific part of the Protobuf compiler's error formatting logic and making a straightforward modification to include the full path in the GCC error format. It does not require deep knowledge of the entire codebase or significant refactoring. The change is more about adjusting an existing behavior than implementing a new feature or solving a complex problem.\n\nGiven these points, a difficulty score of 0.35 reflects an \"Easy\" problem that requires some understanding of the codebase and a simple modification, but does not pose significant technical challenges or require extensive changes across multiple modules.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "switch {pycodestlye, pydocstyle, pylint} to ruff\n[pycodestyle](https://github.com/PyCQA/pycodestyle) was archived and suggested switching to [ruff](https://github.com/astral-sh/ruff), which integrates various linters. This PR removes `pycodestlye`, `pydocstyle`, and `pylint` and adds `ruff` in github action workflow. New lint errors in existing python files are also fixed. \r\n\r\n**Note:** We could selectively disable some unnecessarily strict lint rules as we further develop this project. To disable lint rules for all python files, add corresponding codes to `ignore` under `[tool.ruff.lint]` in `pyproject.toml`. To disable lint rules for test files only, add corresponding codes to `\"**/test/**/*.py\"` under `[tool.ruff.lint.extend-per-file-ignores]`.\n", "patch": "diff --git a/.github/workflows/darglint.yml b/.github/workflows/darglint.yml\nnew file mode 100644\nindex 00000000..e16bb2e0\n--- /dev/null\n+++ b/.github/workflows/darglint.yml\n@@ -0,0 +1,23 @@\n+name: Darglint\n+\n+on: [pull_request, workflow_dispatch]\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    strategy:\n+      matrix:\n+        python-version: [\"3.10\"]\n+    steps:\n+    - uses: actions/checkout@v3\n+    - name: Set up Python ${{ matrix.python-version }}\n+      uses: actions/setup-python@v3\n+      with:\n+        python-version: ${{ matrix.python-version }}\n+    - name: Install dependencies\n+      run: |\n+        python -m pip install --upgrade pip\n+        pip install darglint\n+    - name: Analysing the code with darglint\n+      run: |\n+        darglint $(git ls-files 'dattri/*.py')\ndiff --git a/dattri/func/ihvp.py b/dattri/func/ihvp.py\nindex dadc8185..704e04a6 100644\n--- a/dattri/func/ihvp.py\n+++ b/dattri/func/ihvp.py\n@@ -1,34 +1,60 @@\n-\"\"\"ihvp (inverse hessian-vector product) calculation for an arbitrary function.\n+\"\"\"IHVP (inverse hessian-vector product) calculation.\n \n This module contains:\n-- `ihvp_direct`: Direct algorithm for ihvp.\n+- `ihvp_explicit`: IHVP via explicit Hessian calculation.\n \"\"\"\n+from __future__ import annotations\n \n-from collections.abc import Callable\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Callable\n+    from typing import Tuple, Union\n \n import torch\n from torch import Tensor\n from torch.func import hessian\n \n \n-def ihvp_direct(func: Callable, *args, argnums: int = 0) -> Callable:\n-    \"\"\"Direct ihvp algorithm function.\n-\n-    Standing for the inverse-hessian-vector product, returns a function that,\n-    when given vectors, computes the product of inverse-hessian and vector.\n-\n-    Direct algorithm calcualte the hessian matrix explicitly and then use\n-    `torch.linagl.solve` for each vector production.\n-\n-    :param func: A Python function that takes one or more arguments.\n-           Must return a single-element Tensor. The hessian will\n-           be calculated on this function.\n-    :param argnums: An integer default to 0. Specifies arguments to compute\n-           gradients with respect to.\n+def ihvp_explicit(func: Callable,\n+                  *args,\n+                  argnums: Union[int, Tuple[int, ...]] = 0) -> Callable:\n+    \"\"\"IHVP via explicit Hessian calculation.\n+\n+    IHVP stands for inverse-hessian-vector product. For a given function\n+    `func`, this method first calculates the Hessian matrix explicitly\n+    and then wraps the Hessian in a function that uses `torch.linalg.solve` to\n+    calculate the IHVP for any given vector.\n+\n+    Args:\n+        func (Callable): A function taking one or more arguments and returning\n+            a single-element Tensor. The Hessian will be calculated based on\n+            this function.\n+        *args: List of arguments for `func`.\n+        argnums (int or Tuple[int], optional): An integer or a tuple of integers\n+            deciding which arguments in `*args` to get the Hessian with respect\n+            to. Default: 0.\n+\n+    Returns:\n+        A function that takes a vector `vec` and returns the IHVP of the Hessian\n+        of `func` and `vec`.\n+\n+    Note:\n+        This method stores the Hessian matrix explicitly and is not computationally\n+        efficient.\n     \"\"\"\n     hessian_tensor = hessian(func, argnums=argnums)(*args)\n \n     def _ihvp_direct_func(vec: Tensor) -> Tensor:\n+        \"\"\"The IHVP function based on `hessian_tensor`.\n+\n+        Args:\n+            vec (Tensor): A vector with the same dimension as the first dim of\n+                `hessian_tensor`.\n+\n+        Returns:\n+            The IHVP value, i.e., inverse of `hessian_tensor` times `vec`.\n+        \"\"\"\n         return torch.linalg.solve(hessian_tensor, vec.T).T\n \n     return _ihvp_direct_func\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 923cbf7e..ca7bf4d8 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,10 +1,10 @@\n [project]\n name = \"dattri\"\n-requires-python = \">=3.10\"\n+requires-python = \">=3.8\"\n \n [tool.ruff.lint]\n select = [\"ALL\"]\n-ignore = [\"ANN002\", \"D203\", \"D213\", \"D407\"]  # ignores: ANN002 (unnecessarily strict rule), D203 (conflict with D211), D213 (conflict with D212), D407 (supporting Google style docstrings)\n+ignore = [\"ANN002\", \"D203\", \"D213\", \"D407\", \"UP\"]  # ignores: ANN002 (unnecessarily strict rule), D203 (conflict with D211), D213 (conflict with D212), D407 (supporting Google style docstrings), UP (compatibility with Python 3.8)\n \n [tool.ruff.lint.extend-per-file-ignores]\n \"__init__.py\" = [\"E401\", \"E402\"]\n", "instance_id": "TRAIS-Lab__dattri-8", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to switch from multiple Python linters (pycodestyle, pydocstyle, pylint) to a unified tool (ruff) within a GitHub Actions workflow, and to fix existing lint errors in Python files. It provides a high-level goal and some actionable guidance on disabling lint rules via configuration in `pyproject.toml`. However, there are minor ambiguities and missing details. For instance, it does not specify which specific lint rules were causing errors in the existing codebase or how they were fixed, nor does it mention any potential challenges or compatibility issues with switching to ruff. Additionally, there are no examples of the lint errors fixed or detailed instructions on testing the new setup. While the overall objective is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task falls in the easy range (0.2-0.4) due to the relatively straightforward nature of the changes required. The scope of code changes involves updating configuration files (e.g., `pyproject.toml`), modifying GitHub Actions workflows (e.g., adding a new workflow for darglint), and making minor code adjustments to fix lint errors (e.g., updating function documentation and type hints in `ihvp.py`). The changes span a few files but do not significantly impact the system's architecture or require deep understanding of the codebase's core logic. The technical concepts involved are basic: familiarity with Python linting tools, GitHub Actions, and configuration file syntax. No complex algorithms, design patterns, or domain-specific knowledge are required. Edge cases and error handling are minimal, as the task primarily deals with tooling and formatting rather than runtime logic. However, it is slightly more involved than a very easy task due to the need to understand and configure a new tool (ruff) and ensure compatibility with existing code, which requires some research and testing. Hence, a score of 0.35 reflects this balance of simplicity with minor complexity in tool migration.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add the one-dimensional test function from Holsclaw et al. (2013)\nThe test function reads:\r\n\r\n$$\r\n\\mathcal{M}(x) = \\frac{x \\sin{ \\left( x \\right) } }{10},\r\n$$\r\n\r\nwhere $x$ is uniformly distributed in $[0, 10]$.\r\n\r\nThe function was introduced in [^Holsclaw2013] as a test function for modeling derivative curves using Gaussian process.\r\nIn the paper, an i.i.d noise of $\\mathcal{N}(0, 0.3)$ is added to the response.\r\n\r\n[^Holsclaw2013]: T. Holsclaw *et al.*, \u201cGaussian Process Modeling of Derivative Curves,\u201d *Technometrics*, vol. 55, no. 1, pp. 57\u201367, Feb. 2013, doi: [10.1080/00401706.2012.723918](https://doi.org/10.1080/00401706.2012.723918).\r\n[^location]: See the second example of Section 4 in [^Holsclaw2013].\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 89fc96e..bf71e5a 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -9,6 +9,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Added\n \n+- The One-dimensional sine function from Holsclaw et al. (2013) for\n+  metamodeling exercises.\n - The M-dimensional discontinuous function from Genz (1984) for integration and\n   sensitivity analysis exercises; one parameter set from the literature\n   is included.\ndiff --git a/docs/_toc.yml b/docs/_toc.yml\nindex 8d35e34..a685320 100644\n--- a/docs/_toc.yml\n+++ b/docs/_toc.yml\n@@ -108,6 +108,8 @@ parts:\n             title: Genz (Product Peak)\n           - file: test-functions/gramacy-1d-sine\n             title: Gramacy (2007) 1D Sine\n+          - file: test-functions/holsclaw-sine\n+            title: Holsclaw Sine\n           - file: test-functions/hyper-sphere\n             title: Hyper-sphere Bound\n           - file: test-functions/ishigami\ndiff --git a/docs/fundamentals/metamodeling.md b/docs/fundamentals/metamodeling.md\nindex 813290a..5d53fcf 100644\n--- a/docs/fundamentals/metamodeling.md\n+++ b/docs/fundamentals/metamodeling.md\n@@ -26,7 +26,7 @@ in the comparison of metamodeling approaches.\n |               {ref}`Borehole <test-functions:borehole>`                |        8        |      `Borehole()`      |\n |       {ref}`Cheng and Sandu (2010) 2D <test-functions:cheng2d>`        |        2        |       `Cheng2D`        |\n |          {ref}`Coffee Cup Model <test-functions:coffee-cup>`           |        2        |     `CoffeeCup()`      |\n-|            {ref}`Currin Sine <test-functions:currin-sine>`             |        1        |     `CurrinSine()`     |\n+|     {ref}`Currin et al. (1988) Sine <test-functions:currin-sine>`      |        1        |     `CurrinSine()`     |\n |          {ref}`Damped Cosine <test-functions:damped-cosine>`           |        1        |    `DampedCosine()`    |\n |      {ref}`Damped Oscillator <test-functions:damped-oscillator>`       |        7        |  `DampedOscillator()`  |\n |                  {ref}`Flood <test-functions:flood>`                   |        8        |       `Flood()`        |\n@@ -41,6 +41,7 @@ in the comparison of metamodeling approaches.\n |          {ref}`Friedman (10D) <test-functions:friedman-10d>`           |       10        |    `Friedman10D()`     |\n |      {ref}`Genz (Corner Peak) <test-functions:genz-corner-peak>`       |        M        |   `GenzCornerPeak()`   |\n |     {ref}`Gramacy (2007) 1D Sine <test-functions:gramacy-1d-sine>`     |        1        |   `Gramacy1DSine()`    |\n+|   {ref}`Holsclaw et al. (2013) Sine <test-functions:holsclaw-sine>`    |        1        |    `HolsclawSine()`    |\n | {ref}`Lim et al. (2002) Non-Polynomial <test-functions:lim-non-poly>`  |        2        |     `LimNonPoly()`     |\n |     {ref}`Lim et al. (2002) Polynomial <test-functions:lim-poly>`      |        2        |      `LimPoly()`       |\n |              {ref}`McLain S1 <test-functions:mclain-s1>`               |        2        |      `McLainS1()`      |\ndiff --git a/docs/references.bib b/docs/references.bib\nindex 3b89416..cc4a60e 100644\n--- a/docs/references.bib\n+++ b/docs/references.bib\n@@ -1009,4 +1009,15 @@ @Article{Jakeman2015\n   doi     = {10.1016/j.jcp.2015.02.025},\n }\n \n+@Article{Holsclaw2013,\n+  author  = {Holsclaw, Tracy and Sans\u00f3, Bruno and Lee, Herbert K. H. and Heitmann, Katrin and Habib, Salman and Higdon, David and Alam, Ujjaini},\n+  journal = {Technometrics},\n+  title   = {Gaussian process modeling of derivative curves},\n+  year    = {2013},\n+  number  = {1},\n+  pages   = {57--67},\n+  volume  = {55},\n+  doi     = {10.1080/00401706.2012.723918},\n+}\n+\n @Comment{jabref-meta: databaseType:bibtex;}\n", "instance_id": "damar-wicaksono__uqtestfuns-422", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to add a one-dimensional test function from Holsclaw et al. (2013) for metamodeling exercises, with the mathematical formulation provided and the context of its use (Gaussian process modeling of derivative curves) briefly described. The input domain (x uniformly distributed in [0, 10]) and the addition of i.i.d. noise (N(0, 0.3)) are specified, which helps in understanding the requirements. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define how the function should be implemented (e.g., as a callable function, class, or other structure), nor does it specify the expected output format or how the noise should be applied (e.g., deterministically or as part of a sampling process). Additionally, edge cases or specific implementation constraints (e.g., numerical precision, handling of boundary values) are not mentioned. While the code changes provide some context on integration into the codebase, the problem statement itself lacks these finer details, leading to a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is rated as Easy (0.25) based on the provided factors. First, the scope and depth of code changes appear limited, as the diffs primarily involve documentation updates (CHANGELOG.md, _toc.yml, metamodeling.md, references.bib) rather than actual implementation code. This suggests that the core implementation (likely in a separate file not shown in the diff) is straightforward and localized, not requiring extensive modifications across multiple modules or impacting the system's architecture. Second, the number of technical concepts involved is minimal; implementing the function requires basic mathematical computation (evaluating x * sin(x) / 10), familiarity with a programming language's math library (e.g., sin function), and potentially a random number generator for noise (N(0, 0.3)), which are standard and not complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic statistics are needed. Third, potential edge cases and error handling requirements seem negligible; the problem statement does not highlight specific edge cases, and the input domain [0, 10] is well-defined with no apparent numerical instability in the function. The task likely involves adding a simple function with minimal interaction with other parts of the codebase, aligning with an Easy difficulty level (0.2-0.4). The score of 0.25 reflects that while the task is straightforward, it may require slight attention to detail in integrating with the existing test function framework and documentation style.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Potentially wrong queries\nthere are existing queries that point out non-issues.\r\nspecifically, having requests.memory/cpu != limits.memory/cpu is not an issue (but a best practice).\r\nresource limits are supposed to help users avoid OOM/throttling, while resource requests make sure resources are actually allocated for the containers.\r\n\r\neven in the [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](link attached) as help for these queries, the example shows limits>requests.\r\n\r\n### Expected Behavior\r\ncontainers having requests.memory/cpu != limits.memory/cpu will not trigger an issue.\r\n\r\nqueries:\r\n\r\naafa7d94-62de-4fbf-8838-b69ee217b0e6\r\n9d43040e-e703-4e16-8bfe-8d4da10fa7e6\r\naee3c7d2-a811-4201-90c7-11c028be9a46\r\n\r\nshould be removed\r\n\r\n### Actual Behavior\r\n\r\ncontainers having requests.memory/cpu != limits.memory/cpu trigger an issue.\n", "patch": "diff --git a/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/metadata.json b/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/metadata.json\ndeleted file mode 100644\nindex c25235b526b..00000000000\n--- a/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/metadata.json\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-{\n-  \"id\": \"9d43040e-e703-4e16-8bfe-8d4da10fa7e6\",\n-  \"queryName\": \"Container CPU Requests Not Equal To Its Limits\",\n-  \"severity\": \"LOW\",\n-  \"category\": \"Best Practices\",\n-  \"descriptionText\": \"A Pod's Containers must have the same CPU requests as limits set, which is recommended to avoid resource DDOS of the node during spikes. This means the 'requests.cpu' must equal 'limits.cpu', and both be defined.\",\n-  \"descriptionUrl\": \"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\",\n-  \"platform\": \"Kubernetes\",\n-  \"descriptionID\": \"3e1c6d16\",\n-  \"cwe\": \"\"\n-}\ndiff --git a/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/query.rego b/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/query.rego\ndeleted file mode 100644\nindex b539d26fc71..00000000000\n--- a/assets/queries/k8s/container_cpu_requests_not_equal_to_its_limits/query.rego\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-package Cx\n-\n-import data.generic.common as common_lib\n-import data.generic.k8s as k8sLib\n-\n-types := {\"initContainers\", \"containers\"}\n-rec := {\"requests\", \"limits\"}\n-\n-CxPolicy[result] {\n-\tdocument := input.document[i]\n-\tdocument.kind == k8sLib.valid_pod_spec_kind_list[_]\n-\tspecInfo := k8sLib.getSpecInfo(document)\n-\tcontainer := specInfo.spec[types[x]][c]\n-\n-\thas_request_or_limits(container)\n-\tnot common_lib.valid_key(container.resources[rec[t]], \"cpu\")\n-\n-\tresult := {\n-\t\t\"documentId\": document.id,\n-\t\t\"resourceType\": document.kind,\n-\t\t\"resourceName\": document.metadata.name,\n-\t\t\"searchKey\": sprintf(\"metadata.name={{%s}}.%s.%s.name={{%s}}.resources.%s\", [document.metadata.name, specInfo.path, types[x], container.name, rec[t]]),\n-\t\t\"issueType\": \"MissingAttribute\",\n-\t\t\"keyExpectedValue\": sprintf(\"spec.%s[%s].resources.%s.cpu should be defined\", [types[x], container.name, rec[t]]),\n-\t\t\"keyActualValue\": sprintf(\"spec.%s[%s].resources.%s.cpu is not defined\", [types[x], container.name, rec[t]]),\n-\t\t\"searchLine\": common_lib.build_search_line(split(specInfo.path, \".\"), [types[x], c, \"resources\", rec[t]]),\n-\t}\n-}\n-\n-CxPolicy[result] {\n-\tdocument := input.document[i]\n-\tdocument.kind == k8sLib.valid_pod_spec_kind_list[_]\n-\tspecInfo := k8sLib.getSpecInfo(document)\n-\tcontainer := specInfo.spec[types[x]][c]\n-\n-\tcontainer.resources.requests.cpu != container.resources.limits.cpu\n-\n-\tresult := {\n-\t\t\"documentId\": document.id,\n-\t\t\"resourceType\": document.kind,\n-\t\t\"resourceName\": document.metadata.name,\n-\t\t\"searchKey\": sprintf(\"metadata.name={{%s}}.%s.%s.name={{%s}}.resources\", [document.metadata.name, specInfo.path, types[x], container.name]),\n-\t\t\"issueType\": \"IncorrectValue\",\n-\t\t\"keyExpectedValue\": sprintf(\"spec.%s[%s].resources.requests.cpu is equal to spec.%s[%s].resources.limits.cpu\", [types[x], container.name, types[x], container.name]),\n-\t\t\"keyActualValue\": sprintf(\"spec.%s[%s].resources.requests.cpu is not equal to spec.%s[%s].resources.limits.cpu\", [types[x], container.name, types[x], container.name]),\n-\t\t\"searchLine\": common_lib.build_search_line(split(specInfo.path, \".\"), [types[x], c, \"resources\"]),\n-\t}\n-}\n-\n-has_request_or_limits(x){\n-\tcommon_lib.valid_key(x.resources[rec[\"requests\"]],\"cpu\")\n-}else{\n-\tcommon_lib.valid_key(x.resources[rec[\"limits\"]],\"cpu\")\n-}\ndiff --git a/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/metadata.json b/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/metadata.json\ndeleted file mode 100644\nindex 8a08d4a9654..00000000000\n--- a/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/metadata.json\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-{\n-  \"id\": \"aafa7d94-62de-4fbf-8838-b69ee217b0e6\",\n-  \"queryName\": \"Container Memory Requests Not Equal To Its Limits\",\n-  \"severity\": \"LOW\",\n-  \"category\": \"Resource Management\",\n-  \"descriptionText\": \"A Pod's Containers must have the same Memory requests as limits set, which is recommended to avoid resource DDOS of the node during spikes. This means the 'requests.memory' must equal 'limits.memory', and both be defined.\",\n-  \"descriptionUrl\": \"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\",\n-  \"platform\": \"Kubernetes\",\n-  \"descriptionID\": \"0c15063c\",\n-  \"cwe\": \"\"\n-}\ndiff --git a/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/query.rego b/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/query.rego\ndeleted file mode 100644\nindex dc2b504268a..00000000000\n--- a/assets/queries/k8s/container_memory_requests_not_equal_to_its_limits/query.rego\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-package Cx\n-\n-import data.generic.common as common_lib\n-import data.generic.k8s as k8sLib\n-\n-types := {\"initContainers\", \"containers\"}\n-rec := {\"requests\", \"limits\"}\n-\n-CxPolicy[result] {\n-\tdocument := input.document[i]\n-\tdocument.kind == k8sLib.valid_pod_spec_kind_list[_]\n-\tspecInfo := k8sLib.getSpecInfo(document)\n-\tcontainer := specInfo.spec[types[x]][c]\n-\n-    has_request_or_limits(container)\n-\tnot common_lib.valid_key(container.resources[rec[t]], \"memory\")\n-\n-\tresult := {\n-\t\t\"documentId\": document.id,\n-\t\t\"resourceType\": document.kind,\n-\t\t\"resourceName\": document.metadata.name,\n-\t\t\"searchKey\": sprintf(\"metadata.name={{%s}}.%s.%s.name={{%s}}.resources.%s\", [document.metadata.name,specInfo.path, types[x], container.name, rec[t]]),\n-\t\t\"issueType\": \"MissingAttribute\",\n-\t\t\"keyExpectedValue\": sprintf(\"spec.%s[%s].resources.%s.memory should be defined\", [types[x], container.name, rec[t]]),\n-\t\t\"keyActualValue\": sprintf(\"spec.%s[%s].resources.%s.memory is not defined\", [types[x], container.name, rec[t]]),\n-\t\t\"searchLine\": common_lib.build_search_line(split(specInfo.path, \".\"), [types[x], c, \"resources\", rec[t]])\n-\t}\n-}\n-\n-CxPolicy[result] {\n-\tdocument := input.document[i]\n-\tdocument.kind == k8sLib.valid_pod_spec_kind_list[_]\n-\n-\tspecInfo := k8sLib.getSpecInfo(document)\n-\ttypes := {\"initContainers\", \"containers\"}\n-\n-\tcontainer := specInfo.spec[types[x]][c]\n-\n-\tcontainer.resources.requests.memory != container.resources.limits.memory\n-\n-\tresult := {\n-\t\t\"documentId\": document.id,\n-\t\t\"resourceType\": document.kind,\n-\t\t\"resourceName\": document.metadata.name,\n-\t\t\"searchKey\": sprintf(\"metadata.name={{%s}}.%s.%s.name={{%s}}.resources\", [document.metadata.name, specInfo.path,types[x], container.name]),\n-\t\t\"issueType\": \"IncorrectValue\",\n-\t\t\"keyExpectedValue\": sprintf(\"spec.%s[%s].resources.requests.memory is equal to spec.%s[%s].resources.limits.memory\", [types[x], container.name, types[x], container.name]),\n-\t\t\"keyActualValue\": sprintf(\"spec.%s[%s].resources.requests.memory is not equal to spec.%s[%s].resources.limits.memory\", [types[x], container.name, types[x], container.name]),\n-\t\t\"searchLine\": common_lib.build_search_line(split(specInfo.path, \".\"), [types[x], c, \"resources\"])\n-\t}\n-}\n-\n-has_request_or_limits(x){\n-\tcommon_lib.valid_key(x.resources[rec[\"requests\"]],\"memory\")\n-}else{\n-\tcommon_lib.valid_key(x.resources[rec[\"limits\"]],\"memory\")\n-}\n", "instance_id": "Checkmarx__kics-6974", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to address a specific issue with Kubernetes resource queries. It identifies the problem (incorrect flagging of containers where requests and limits for CPU/memory differ) and provides the expected behavior (not triggering an issue for such cases). It also references Kubernetes documentation to support the argument that differing requests and limits are a best practice, not an issue. Additionally, specific query IDs are provided for removal, which adds clarity to the scope of the change. However, there are minor ambiguities: the problem statement does not explicitly discuss the broader context of the codebase or potential side effects of removing these queries. It also lacks details on whether there are any specific conditions or exceptions where flagging such differences might still be relevant. Overall, the statement is valid and clear but misses some minor contextual details and potential edge cases.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves straightforward code modifications with minimal complexity. The scope of the change is limited to deleting specific query files related to Kubernetes resource checks, as seen in the provided diff (removal of metadata.json and query.rego files for CPU and memory checks). This does not require deep understanding of the codebase architecture or complex logic modifications, as it is purely a removal of existing rules without the need to add or modify functionality. The technical concepts involved are minimal\u2014basic familiarity with file structures and version control (e.g., Git) is sufficient. There are no significant edge cases or error handling requirements to consider since the task is to remove functionality rather than implement or modify it. The impact on the system is negligible, as it does not alter the core architecture or introduce new dependencies. Overall, this is a very easy task that can be completed quickly by a developer with basic experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Rename ListX responses to `Items`\nInstead of having ListJob, ListJobExecutions,... return an array of `Jobs`, `Executions`,... , unify this to always return `Items` array along with `NextToken`\r\n\r\nPreferably we should do this before announcing V1 API deprecation. Also would be nice to do this while maintaining backward compatibility with existing clients. Maybe we should explore adding bacalhau version as an http header, and return `Items` only for new versions? Up for discussion\n", "patch": "diff --git a/cmd/cli/job/describe.go b/cmd/cli/job/describe.go\nindex b5347727ea..5c3a2209cd 100644\n--- a/cmd/cli/job/describe.go\n+++ b/cmd/cli/job/describe.go\n@@ -102,7 +102,7 @@ func (o *DescribeOptions) run(cmd *cobra.Command, args []string, api client.API)\n \tvar executions []*models.Execution\n \tif response.Executions != nil {\n \t\t// TODO: #520 rename Executions.Executions to Executions.Items\n-\t\texecutions = response.Executions.Executions\n+\t\texecutions = response.Executions.Items\n \t}\n \t// Show most relevant execution first: sort by time DESC\n \tslices.SortFunc(executions, func(a, b *models.Execution) int {\n@@ -111,7 +111,7 @@ func (o *DescribeOptions) run(cmd *cobra.Command, args []string, api client.API)\n \n \tvar history []*models.JobHistory\n \tif response.History != nil {\n-\t\thistory = response.History.History\n+\t\thistory = response.History.Items\n \t}\n \n \to.printHeaderData(cmd, job)\ndiff --git a/cmd/cli/job/executions.go b/cmd/cli/job/executions.go\nindex da9c4f8b32..3a337299bf 100644\n--- a/cmd/cli/job/executions.go\n+++ b/cmd/cli/job/executions.go\n@@ -150,7 +150,7 @@ func (o *ExecutionOptions) run(cmd *cobra.Command, args []string, api client.API\n \t\treturn err\n \t}\n \n-\tif err = output.Output(cmd, executionColumns, o.OutputOptions, response.Executions); err != nil {\n+\tif err = output.Output(cmd, executionColumns, o.OutputOptions, response.Items); err != nil {\n \t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n \ndiff --git a/cmd/cli/job/history.go b/cmd/cli/job/history.go\nindex ffa224540e..8fc9507aa5 100644\n--- a/cmd/cli/job/history.go\n+++ b/cmd/cli/job/history.go\n@@ -188,7 +188,7 @@ func (o *HistoryOptions) run(cmd *cobra.Command, args []string, api client.API)\n \t\treturn err\n \t}\n \n-\tif err = output.Output(cmd, historyColumns, o.OutputOptions, response.History); err != nil {\n+\tif err = output.Output(cmd, historyColumns, o.OutputOptions, response.Items); err != nil {\n \t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n \ndiff --git a/cmd/cli/job/list.go b/cmd/cli/job/list.go\nindex 61b11af016..d72e7f14a0 100644\n--- a/cmd/cli/job/list.go\n+++ b/cmd/cli/job/list.go\n@@ -150,7 +150,7 @@ func (o *ListOptions) run(cmd *cobra.Command, api client.API) error {\n \t\treturn fmt.Errorf(\"failed request: %w\", err)\n \t}\n \n-\tif err = output.Output(cmd, listColumns, o.OutputOptions, response.Jobs); err != nil {\n+\tif err = output.Output(cmd, listColumns, o.OutputOptions, response.Items); err != nil {\n \t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n \ndiff --git a/cmd/util/download.go b/cmd/util/download.go\nindex 36ee59b604..c20d3d4fde 100644\n--- a/cmd/util/download.go\n+++ b/cmd/util/download.go\n@@ -36,7 +36,7 @@ func DownloadResultsHandler(\n \t\tFatal(cmd, fmt.Errorf(\"could not get results for job %s: %w\", jobID, err), 1)\n \t}\n \n-\tif len(response.Results) == 0 {\n+\tif len(response.Items) == 0 {\n \t\t// No results doesn't mean error, so we should print out a message and return nil\n \t\tcmd.Println(\"No results found\")\n \t\tcmd.Println(\"You can check the logged output of the job using the logs command.\")\n@@ -50,11 +50,11 @@ func DownloadResultsHandler(\n \t}\n \n \t// check if we don't support downloading the results\n-\tfor _, result := range response.Results {\n+\tfor _, result := range response.Items {\n \t\tif !downloaderProvider.Has(ctx, result.Type) {\n \t\t\tcmd.PrintErrln(\n \t\t\t\t\"No supported downloader found for the published results. You will have to download the results differently.\")\n-\t\t\tb, err := json.MarshalIndent(response.Results, \"\", \"    \")\n+\t\t\tb, err := json.MarshalIndent(response.Items, \"\", \"    \")\n \t\t\tif err != nil {\n \t\t\t\treturn err\n \t\t\t}\n@@ -70,7 +70,7 @@ func DownloadResultsHandler(\n \n \terr = downloader.DownloadResults(\n \t\tctx,\n-\t\tresponse.Results,\n+\t\tresponse.Items,\n \t\tdownloaderProvider,\n \t\t(*downloader.DownloaderSettings)(processedDownloadSettings),\n \t)\ndiff --git a/cmd/util/printer/print.go b/cmd/util/printer/print.go\nindex 1d3ea65417..4719e7c419 100644\n--- a/cmd/util/printer/print.go\n+++ b/cmd/util/printer/print.go\n@@ -90,7 +90,7 @@ func PrintJobExecution(\n \t\t\treturn fmt.Errorf(\"failed getting job history: %w\", err)\n \t\t}\n \n-\t\thistorySummary := summariseHistoryEvents(history.History)\n+\t\thistorySummary := summariseHistoryEvents(history.Items)\n \t\tif len(historySummary) > 0 {\n \t\t\tfor _, event := range historySummary {\n \t\t\t\tprintEvent(cmd, event)\n@@ -107,7 +107,7 @@ func PrintJobExecution(\n \t\tif err != nil {\n \t\t\treturn fmt.Errorf(\"failed getting job executions: %w\", err)\n \t\t}\n-\t\tsummary := summariseExecutions(executions.Executions)\n+\t\tsummary := summariseExecutions(executions.Items)\n \t\tif len(summary) > 0 {\n \t\t\tcmd.Println(\"\\nJob Results By Node:\")\n \t\t\tfor message, runs := range summary {\ndiff --git a/pkg/publicapi/apimodels/helpers.go b/pkg/publicapi/apimodels/helpers.go\nnew file mode 100644\nindex 0000000000..d161a45d2a\n--- /dev/null\n+++ b/pkg/publicapi/apimodels/helpers.go\n@@ -0,0 +1,23 @@\n+package apimodels\n+\n+import (\n+\t\"net/http\"\n+\n+\t\"github.com/Masterminds/semver\"\n+\n+\t\"github.com/bacalhau-project/bacalhau/pkg/version\"\n+)\n+\n+// GetClientVersion extracts the client version from the `X-Bacalhau-Git-Version` header in the request.\n+// If the header is present and the version string can be parsed, it returns the parsed version.\n+// If the header is missing or the version string cannot be parsed, it returns version.Unknown.\n+func GetClientVersion(req *http.Request) *semver.Version {\n+\tif clientVerStr := req.Header.Get(HTTPHeaderBacalhauGitVersion); clientVerStr != \"\" {\n+\t\tclientVersion, err := semver.NewVersion(clientVerStr)\n+\t\tif err != nil {\n+\t\t\treturn version.Unknown\n+\t\t}\n+\t\treturn clientVersion\n+\t}\n+\treturn version.Unknown\n+}\ndiff --git a/pkg/publicapi/apimodels/job.go b/pkg/publicapi/apimodels/job.go\nindex 24de5431e9..69eec677fd 100644\n--- a/pkg/publicapi/apimodels/job.go\n+++ b/pkg/publicapi/apimodels/job.go\n@@ -3,8 +3,9 @@ package apimodels\n import (\n \t\"strconv\"\n \n-\t\"github.com/bacalhau-project/bacalhau/pkg/models\"\n \t\"k8s.io/apimachinery/pkg/labels\"\n+\n+\t\"github.com/bacalhau-project/bacalhau/pkg/models\"\n )\n \n type PutJobRequest struct {\n@@ -89,7 +90,9 @@ func (o *ListJobsRequest) ToHTTPRequest() *HTTPRequest {\n \n type ListJobsResponse struct {\n \tBaseListResponse\n-\tJobs []*models.Job `json:\"Jobs\"`\n+\t// Deprecated, use Items\n+\tJobs  []*models.Job\n+\tItems []*models.Job\n }\n \n // Normalize is used to canonicalize fields in the ListJobsResponse.\n@@ -98,6 +101,9 @@ func (r *ListJobsResponse) Normalize() {\n \tfor _, job := range r.Jobs {\n \t\tjob.Normalize()\n \t}\n+\tfor _, job := range r.Items {\n+\t\tjob.Normalize()\n+\t}\n }\n \n type ListJobHistoryRequest struct {\n@@ -130,7 +136,9 @@ func (o *ListJobHistoryRequest) ToHTTPRequest() *HTTPRequest {\n \n type ListJobHistoryResponse struct {\n \tBaseListResponse\n+\t// Deprecated: use Items\n \tHistory []*models.JobHistory\n+\tItems   []*models.JobHistory\n }\n \n type ListJobExecutionsRequest struct {\n@@ -140,7 +148,9 @@ type ListJobExecutionsRequest struct {\n \n type ListJobExecutionsResponse struct {\n \tBaseListResponse\n+\t// Deprecated: use Items\n \tExecutions []*models.Execution\n+\tItems      []*models.Execution\n }\n \n type ListJobResultsRequest struct {\n@@ -150,7 +160,9 @@ type ListJobResultsRequest struct {\n \n type ListJobResultsResponse struct {\n \tBaseListResponse\n+\t// Deprecated: use Items\n \tResults []*models.SpecConfig\n+\tItems   []*models.SpecConfig\n }\n \n type StopJobRequest struct {\ndiff --git a/pkg/publicapi/endpoint/orchestrator/job.go b/pkg/publicapi/endpoint/orchestrator/job.go\nindex f3502c2286..dc83b85e00 100644\n--- a/pkg/publicapi/endpoint/orchestrator/job.go\n+++ b/pkg/publicapi/endpoint/orchestrator/job.go\n@@ -5,7 +5,7 @@ import (\n \t\"net/http\"\n \t\"strings\"\n \n-\t\"github.com/bacalhau-project/bacalhau/pkg/lib/concurrency\"\n+\t\"github.com/Masterminds/semver\"\n \t\"github.com/gorilla/websocket\"\n \t\"github.com/labstack/echo/v4\"\n \t\"github.com/rs/zerolog/log\"\n@@ -13,11 +13,13 @@ import (\n \t\"golang.org/x/exp/slices\"\n \n \t\"github.com/bacalhau-project/bacalhau/pkg/jobstore\"\n+\t\"github.com/bacalhau-project/bacalhau/pkg/lib/concurrency\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/models\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/orchestrator\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/publicapi\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/publicapi/apimodels\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/util\"\n+\t\"github.com/bacalhau-project/bacalhau/pkg/version\"\n )\n \n // godoc for Orchestrator PutJob\n@@ -69,7 +71,7 @@ func (e *Endpoint) putJob(c echo.Context) error {\n // @Failure\t\t400\t{object}\tstring\n // @Failure\t\t500\t{object}\tstring\n // @Router\t\t\t/api/v1/orchestrator/jobs [get]\n-func (e *Endpoint) getJob(c echo.Context) error {\n+func (e *Endpoint) getJob(c echo.Context) error { //nolint: gocyclo\n \tctx := c.Request().Context()\n \tjobID := c.Param(\"id\")\n \tvar args apimodels.GetJobRequest\n@@ -97,10 +99,15 @@ func (e *Endpoint) getJob(c echo.Context) error {\n \t\t\t\treturn err\n \t\t\t}\n \t\t\tresponse.History = &apimodels.ListJobHistoryResponse{\n-\t\t\t\tHistory: make([]*models.JobHistory, len(history)),\n+\t\t\t\tItems: make([]*models.JobHistory, len(history)),\n \t\t\t}\n \t\t\tfor i := range history {\n-\t\t\t\tresponse.History.History[i] = &history[i]\n+\t\t\t\tresponse.History.Items[i] = &history[i]\n+\t\t\t}\n+\t\t\t// migrate to old response if required\n+\t\t\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\t\t\tresponse.History.History = response.History.Items //nolint: staticcheck\n+\t\t\t\tresponse.History.Items = make([]*models.JobHistory, 0)\n \t\t\t}\n \t\tcase \"executions\":\n \t\t\t// ignore if user requested executions twice\n@@ -114,13 +121,19 @@ func (e *Endpoint) getJob(c echo.Context) error {\n \t\t\t\treturn err\n \t\t\t}\n \t\t\tresponse.Executions = &apimodels.ListJobExecutionsResponse{\n-\t\t\t\tExecutions: make([]*models.Execution, len(executions)),\n+\t\t\t\tItems: make([]*models.Execution, len(executions)),\n \t\t\t}\n \t\t\tfor i := range executions {\n-\t\t\t\tresponse.Executions.Executions[i] = &executions[i]\n+\t\t\t\tresponse.Executions.Items[i] = &executions[i]\n+\t\t\t}\n+\t\t\t// migrate to old response if required\n+\t\t\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\t\t\tresponse.Executions.Executions = response.Executions.Items //nolint: staticcheck\n+\t\t\t\tresponse.Executions.Items = make([]*models.Execution, 0)\n \t\t\t}\n \t\t}\n \t}\n+\n \treturn c.JSON(http.StatusOK, response)\n }\n \n@@ -209,7 +222,7 @@ func (e *Endpoint) listJobs(c echo.Context) error {\n \t}\n \n \tres := &apimodels.ListJobsResponse{\n-\t\tJobs: lo.Map[models.Job, *models.Job](response.Jobs, func(item models.Job, _ int) *models.Job {\n+\t\tItems: lo.Map[models.Job, *models.Job](response.Jobs, func(item models.Job, _ int) *models.Job {\n \t\t\treturn &item\n \t\t}),\n \t\tBaseListResponse: apimodels.BaseListResponse{\n@@ -217,6 +230,13 @@ func (e *Endpoint) listJobs(c echo.Context) error {\n \t\t},\n \t}\n \n+\t// migrate to old response if required\n+\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\tres.Jobs = res.Items\n+\t\tres.Items = make([]*models.Job, 0)\n+\t\tres.Normalize()\n+\t}\n+\n \treturn c.JSON(http.StatusOK, res)\n }\n \n@@ -298,10 +318,17 @@ func (e *Endpoint) jobHistory(c echo.Context) error {\n \t\treturn err\n \t}\n \tres := &apimodels.ListJobHistoryResponse{\n-\t\tHistory: make([]*models.JobHistory, len(history)),\n+\t\tItems: make([]*models.JobHistory, len(history)),\n \t}\n \tfor i := range history {\n-\t\tres.History[i] = &history[i]\n+\t\tres.Items[i] = &history[i]\n+\t}\n+\n+\t// migrate to old response if required\n+\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\tres.History = res.Items //nolint: staticcheck\n+\t\tres.Items = make([]*models.JobHistory, 0)\n+\t\tres.Normalize()\n \t}\n \n \treturn c.JSON(http.StatusOK, res)\n@@ -384,10 +411,17 @@ func (e *Endpoint) jobExecutions(c echo.Context) error {\n \n \t// prepare result\n \tres := &apimodels.ListJobExecutionsResponse{\n-\t\tExecutions: make([]*models.Execution, len(executions)),\n+\t\tItems: make([]*models.Execution, len(executions)),\n \t}\n \tfor i := range executions {\n-\t\tres.Executions[i] = &executions[i]\n+\t\tres.Items[i] = &executions[i]\n+\t}\n+\n+\t// migrate to old response if required\n+\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\tres.Executions = res.Items //nolint: staticcheck\n+\t\tres.Items = make([]*models.Execution, 0)\n+\t\tres.Normalize()\n \t}\n \n \treturn c.JSON(http.StatusOK, res)\n@@ -428,9 +462,15 @@ func (e *Endpoint) jobResults(c echo.Context) error {\n \t\treturn err\n \t}\n \n-\treturn publicapi.UnescapedJSON(c, http.StatusOK, &apimodels.ListJobResultsResponse{\n-\t\tResults: resp.Results,\n-\t})\n+\tresult := &apimodels.ListJobResultsResponse{Items: resp.Results}\n+\t// migrate to old response if required\n+\tif listResponseRequiresMigration(apimodels.GetClientVersion(c.Request())) {\n+\t\tresult.Results = result.Items //nolint: staticcheck\n+\t\tresult.Items = make([]*models.SpecConfig, 0)\n+\t\tresult.Normalize()\n+\t}\n+\n+\treturn publicapi.UnescapedJSON(c, http.StatusOK, result)\n }\n \n // godoc for Orchestrator JobLogs\n@@ -498,3 +538,15 @@ func (e *Endpoint) logsWS(c echo.Context, ws *websocket.Conn) error {\n \t}\n \treturn nil\n }\n+\n+// listResponseRequiresMigration determines if the response to the request needs\n+// to be migrated based on the client version. It returns true if the client\n+// version is less than or equal to version.V1_3_2 without a pre-release version,\n+// or if the client version is not development, or is unknown.\n+// Otherwise, it returns false.\n+func listResponseRequiresMigration(clientVersion *semver.Version) bool {\n+\treturn !(clientVersion.GreaterThan(version.V1_3_2) ||\n+\t\t(clientVersion.Equal(version.V1_3_2) && clientVersion.Prerelease() != \"\") ||\n+\t\tclientVersion.Equal(version.Development) ||\n+\t\tclientVersion.Equal(version.Unknown))\n+}\ndiff --git a/pkg/version/version.go b/pkg/version/version.go\nindex 77fbead493..aeae1da90a 100644\n--- a/pkg/version/version.go\n+++ b/pkg/version/version.go\n@@ -19,6 +19,13 @@ import (\n )\n \n const DevelopmentGitVersion = \"v0.0.0-xxxxxxx\"\n+const UnknownGitVersion = \"v0.0.0\"\n+\n+var (\n+\tDevelopment = semver.MustParse(DevelopmentGitVersion)\n+\tUnknown     = semver.MustParse(UnknownGitVersion)\n+\tV1_3_2      = semver.MustParse(\"v1.3.2\") //nolint: stylecheck\n+)\n \n var (\n \t// GITVERSION is the Git tag that Bacalhau was built from. This is expected to be populated via the `ldflags` flag,\n", "instance_id": "bacalhau-project__bacalhau-4111", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to rename response fields from specific names like `Jobs`, `Executions`, etc., to a unified `Items` field across various API responses. It also mentions maintaining backward compatibility and suggests using a version header to handle different client versions, which shows an awareness of the need for a transition strategy. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the full scope of affected endpoints or provide concrete examples of the expected input/output formats before and after the change. Additionally, the discussion around versioning and backward compatibility is left open-ended (\"up for discussion\"), which introduces uncertainty about the final implementation approach. Edge cases, such as how to handle clients that do not provide version headers or potential conflicts in data structures, are not addressed. Overall, while the goal is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (e.g., CLI commands, API models, and endpoint handlers), requiring a broad understanding of the codebase structure, particularly how data flows between API responses and client-side processing. The changes involve not just renaming fields but also implementing backward compatibility logic using version headers, which adds complexity. This requires familiarity with semantic versioning (using the `semver` library), HTTP header handling, and conditional response formatting based on client versions. The technical concepts involved include API design, data structure normalization, and maintaining compatibility in a live system, which are moderately complex but not overly advanced. Edge cases, such as handling unknown or missing client versions, are addressed in the code (e.g., fallback to older response formats), but the problem statement does not explicitly call out other potential issues like performance impacts of dual response formats or client-side errors during transition. The changes do not appear to significantly impact the system's architecture but do require careful coordination to avoid breaking existing clients. Overall, this task demands a solid understanding of multiple components and careful implementation across several files, justifying a score of 0.55, slightly above the midpoint of the medium difficulty range (0.4-0.6).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Consider consistent cobra.Command construction\nCurrently some of our commands use the Run field in `cobra.Command` to specify the function to run. They explicitly handle an error and then call util.Fatal.\r\n\r\nOthers of our commands use the RunE field, which allows the function to return an error, which is then handled in root and calls util.Fatal in one place.\r\n\r\nWe should introduce some consistency in using RunE so that:\r\n\r\n* We are only calling Fatal in one place\r\n* The commands are easier to test as an error is available.\n", "patch": "diff --git a/cmd/cli/agent/alive.go b/cmd/cli/agent/alive.go\nindex 93199bac50..53ea1f6cd1 100644\n--- a/cmd/cli/agent/alive.go\n+++ b/cmd/cli/agent/alive.go\n@@ -28,22 +28,24 @@ func NewAliveCmd() *cobra.Command {\n \t\tUse:   \"alive\",\n \t\tShort: \"Get the agent's liveness and health info.\",\n \t\tArgs:  cobra.NoArgs,\n-\t\tRun:   o.runAlive,\n+\t\tRunE:  o.runAlive,\n \t}\n \taliveCmd.Flags().AddFlagSet(cliflags.OutputNonTabularFormatFlags(&o.OutputOpts))\n \treturn aliveCmd\n }\n \n // Run executes alive command\n-func (o *AliveOptions) runAlive(cmd *cobra.Command, _ []string) {\n+func (o *AliveOptions) runAlive(cmd *cobra.Command, _ []string) error {\n \tctx := cmd.Context()\n \tresponse, err := util.GetAPIClientV2(cmd).Agent().Alive(ctx)\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"could not get server alive: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"could not get server alive: %w\", err)\n \t}\n \n \twriteErr := output.OutputOneNonTabular(cmd, o.OutputOpts, response)\n \tif writeErr != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write alive: %w\", writeErr), 1)\n+\t\treturn fmt.Errorf(\"failed to write alive: %w\", writeErr)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/agent/node.go b/cmd/cli/agent/node.go\nindex 4a01f7bad1..4fe10f36c5 100644\n--- a/cmd/cli/agent/node.go\n+++ b/cmd/cli/agent/node.go\n@@ -29,22 +29,24 @@ func NewNodeCmd() *cobra.Command {\n \t\tUse:   \"node\",\n \t\tShort: \"Get the agent's node info.\",\n \t\tArgs:  cobra.NoArgs,\n-\t\tRun:   o.runNode,\n+\t\tRunE:  o.runNode,\n \t}\n \tnodeCmd.Flags().AddFlagSet(cliflags.OutputNonTabularFormatFlags(&o.OutputOpts))\n \treturn nodeCmd\n }\n \n // Run executes node command\n-func (o *NodeOptions) runNode(cmd *cobra.Command, _ []string) {\n+func (o *NodeOptions) runNode(cmd *cobra.Command, _ []string) error {\n \tctx := cmd.Context()\n \tresponse, err := util.GetAPIClientV2(cmd).Agent().Node(ctx, &apimodels.GetAgentNodeRequest{})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"could not get server node: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"could not get server node: %w\", err)\n \t}\n \n \twriteErr := output.OutputOneNonTabular(cmd, o.OutputOpts, response.NodeInfo)\n \tif writeErr != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write node: %w\", writeErr), 1)\n+\t\treturn fmt.Errorf(\"failed to write node: %w\", writeErr)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/agent/version.go b/cmd/cli/agent/version.go\nindex b53ad854f8..c794afa7fd 100644\n--- a/cmd/cli/agent/version.go\n+++ b/cmd/cli/agent/version.go\n@@ -29,18 +29,18 @@ func NewVersionCmd() *cobra.Command {\n \t\tUse:   \"version\",\n \t\tShort: \"Get the agent version.\",\n \t\tArgs:  cobra.NoArgs,\n-\t\tRun:   oV.runVersion,\n+\t\tRunE:  oV.runVersion,\n \t}\n \tversionCmd.Flags().AddFlagSet(cliflags.OutputNonTabularFormatFlags(&oV.OutputOpts))\n \treturn versionCmd\n }\n \n // Run executes version command\n-func (oV *VersionOptions) runVersion(cmd *cobra.Command, _ []string) {\n+func (oV *VersionOptions) runVersion(cmd *cobra.Command, _ []string) error {\n \tctx := cmd.Context()\n \tserverVersionResponse, err := util.GetAPIClientV2(cmd).Agent().Version(ctx)\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"could not get server version: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"could not get server version: %w\", err)\n \t}\n \n \tv := serverVersionResponse.BuildVersionInfo\n@@ -58,6 +58,8 @@ func (oV *VersionOptions) runVersion(cmd *cobra.Command, _ []string) {\n \t}\n \n \tif writeErr != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write version: %w\", writeErr), 1)\n+\t\treturn fmt.Errorf(\"failed to write version: %w\", writeErr)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/create/create.go b/cmd/cli/create/create.go\nindex 45313ae93d..2a9c1bf5b4 100644\n--- a/cmd/cli/create/create.go\n+++ b/cmd/cli/create/create.go\n@@ -67,10 +67,8 @@ func NewCmd() *cobra.Command {\n \t\tArgs:     cobra.MinimumNArgs(0),\n \t\tPreRunE:  hook.RemoteCmdPreRunHooks,\n \t\tPostRunE: hook.RemoteCmdPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) {\n-\t\t\tif err := create(cmd, cmdArgs, OC); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n+\t\t\treturn create(cmd, cmdArgs, OC)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/describe/describe.go b/cmd/cli/describe/describe.go\nindex b737bb080a..88fbdeee3b 100644\n--- a/cmd/cli/describe/describe.go\n+++ b/cmd/cli/describe/describe.go\n@@ -59,10 +59,8 @@ func NewCmd() *cobra.Command {\n \t\tArgs:     cobra.ExactArgs(1),\n \t\tPreRunE:  hook.RemoteCmdPreRunHooks,\n \t\tPostRunE: hook.RemoteCmdPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) { // nolintunparam // incorrectly suggesting unused\n-\t\t\tif err := describe(cmd, cmdArgs, OD); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n+\t\t\treturn describe(cmd, cmdArgs, OD)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/devstack/devstack.go b/cmd/cli/devstack/devstack.go\nindex 42310e1b02..ecd854c50c 100644\n--- a/cmd/cli/devstack/devstack.go\n+++ b/cmd/cli/devstack/devstack.go\n@@ -81,15 +81,11 @@ func NewCmd() *cobra.Command {\n \t\tShort:   \"Start a cluster of bacalhau nodes for testing and development\",\n \t\tLong:    devStackLong,\n \t\tExample: devstackExample,\n-\t\tPreRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := configflags.BindFlags(cmd, devstackFlags); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tPreRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn configflags.BindFlags(cmd, devstackFlags)\n \t\t},\n-\t\tRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := runDevstack(cmd, ODs, IsNoop); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn runDevstack(cmd, ODs, IsNoop)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/docker/docker_run.go b/cmd/cli/docker/docker_run.go\nindex f9c5cef7a2..ec60c78cdf 100644\n--- a/cmd/cli/docker/docker_run.go\n+++ b/cmd/cli/docker/docker_run.go\n@@ -105,10 +105,8 @@ func newDockerRunCmd() *cobra.Command { //nolint:funlen\n \t\tArgs:     cobra.MinimumNArgs(1),\n \t\tPreRunE:  hook.Chain(hook.RemoteCmdPreRunHooks, configflags.PreRun(dockerRunFlags)),\n \t\tPostRunE: hook.RemoteCmdPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) {\n-\t\t\tif err := dockerRun(cmd, cmdArgs, opts); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n+\t\t\treturn dockerRun(cmd, cmdArgs, opts)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/exec/exec.go b/cmd/cli/exec/exec.go\nindex 9e9e944127..d3039b3a39 100644\n--- a/cmd/cli/exec/exec.go\n+++ b/cmd/cli/exec/exec.go\n@@ -80,15 +80,13 @@ func NewCmdWithOptions(options *ExecOptions) *cobra.Command {\n \t\tPreRunE:            hook.RemoteCmdPreRunHooks,\n \t\tPostRunE:           hook.RemoteCmdPostRunHooks,\n \t\tFParseErrWhitelist: cobra.FParseErrWhitelist{UnknownFlags: true},\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) {\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n \t\t\t// Find the unknown arguments from the original args.  We only want to find the\n \t\t\t// flags that are unknown. We will only support the long form for custom\n \t\t\t// job types as we will want to use them as keys in template completions.\n \t\t\tunknownArgs := ExtractUnknownArgs(cmd.Flags(), os.Args[1:])\n \n-\t\t\tif err := exec(cmd, cmdArgs, unknownArgs, options); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\t\treturn exec(cmd, cmdArgs, unknownArgs, options)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/get/get.go b/cmd/cli/get/get.go\nindex 0978ce187e..381fd2dd33 100644\n--- a/cmd/cli/get/get.go\n+++ b/cmd/cli/get/get.go\n@@ -55,10 +55,8 @@ func NewCmd() *cobra.Command {\n \t\tArgs:     cobra.ExactArgs(1),\n \t\tPreRunE:  hook.Chain(hook.RemoteCmdPreRunHooks, configflags.PreRun(getFlags)),\n \t\tPostRunE: hook.RemoteCmdPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) {\n-\t\t\tif err := get(cmd, cmdArgs, OG); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n+\t\t\treturn get(cmd, cmdArgs, OG)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/id/id.go b/cmd/cli/id/id.go\nindex b914d5a8d4..09b9e1b097 100644\n--- a/cmd/cli/id/id.go\n+++ b/cmd/cli/id/id.go\n@@ -31,15 +31,11 @@ func NewCmd() *cobra.Command {\n \tidCmd := &cobra.Command{\n \t\tUse:   \"id\",\n \t\tShort: \"Show bacalhau node id info\",\n-\t\tPreRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := configflags.BindFlags(cmd, idFlags); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tPreRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn configflags.BindFlags(cmd, idFlags)\n \t\t},\n-\t\tRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := id(cmd, outputOpts); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn id(cmd, outputOpts)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/job/describe.go b/cmd/cli/job/describe.go\nindex c76088caee..9eebe1e29f 100644\n--- a/cmd/cli/job/describe.go\n+++ b/cmd/cli/job/describe.go\n@@ -54,13 +54,13 @@ func NewDescribeCmd() *cobra.Command {\n \t\tLong:    describeLong,\n \t\tExample: describeExample,\n \t\tArgs:    cobra.ExactArgs(1),\n-\t\tRun:     o.run,\n+\t\tRunE:    o.run,\n \t}\n \tjobCmd.Flags().AddFlagSet(cliflags.OutputNonTabularFormatFlags(&o.OutputOpts))\n \treturn jobCmd\n }\n \n-func (o *DescribeOptions) run(cmd *cobra.Command, args []string) {\n+func (o *DescribeOptions) run(cmd *cobra.Command, args []string) error {\n \tctx := cmd.Context()\n \tjobID := args[0]\n \tresponse, err := util.GetAPIClientV2(cmd).Jobs().Get(ctx, &apimodels.GetJobRequest{\n@@ -69,14 +69,14 @@ func (o *DescribeOptions) run(cmd *cobra.Command, args []string) {\n \t})\n \n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"could not get job %s: %w\", jobID, err), 1)\n+\t\treturn fmt.Errorf(\"could not get job %s: %w\", jobID, err)\n \t}\n \n \tif o.OutputOpts.Format != \"\" {\n \t\tif err = output.OutputOneNonTabular(cmd, o.OutputOpts, response); err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write job %s: %w\", jobID, err), 1)\n+\t\t\treturn fmt.Errorf(\"failed to write job %s: %w\", jobID, err)\n \t\t}\n-\t\treturn\n+\t\treturn nil\n \t}\n \n \tjob := response.Job\n@@ -89,9 +89,11 @@ func (o *DescribeOptions) run(cmd *cobra.Command, args []string) {\n \to.printHeaderData(cmd, job)\n \to.printExecutionsSummary(cmd, executions)\n \tif err = o.printExecutions(cmd, executions); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write job executions %s: %w\", jobID, err), 1)\n+\t\treturn fmt.Errorf(\"failed to write job executions %s: %w\", jobID, err)\n \t}\n \to.printOutputs(cmd, executions)\n+\n+\treturn nil\n }\n \n func (o *DescribeOptions) printHeaderData(cmd *cobra.Command, job *models.Job) {\ndiff --git a/cmd/cli/job/executions.go b/cmd/cli/job/executions.go\nindex 82804eb60e..699b333009 100644\n--- a/cmd/cli/job/executions.go\n+++ b/cmd/cli/job/executions.go\n@@ -59,7 +59,7 @@ func NewExecutionCmd() *cobra.Command {\n \t\tLong:    executionLong,\n \t\tExample: executionExample,\n \t\tArgs:    cobra.ExactArgs(1),\n-\t\tRun:     o.run,\n+\t\tRunE:    o.run,\n \t}\n \n \tnodeCmd.Flags().AddFlagSet(cliflags.ListFlags(&o.ListOptions))\n@@ -120,7 +120,7 @@ var executionColumns = []output.TableColumn[*models.Execution]{\n \texecutionColumnDesired,\n }\n \n-func (o *ExecutionOptions) run(cmd *cobra.Command, args []string) {\n+func (o *ExecutionOptions) run(cmd *cobra.Command, args []string) error {\n \tctx := cmd.Context()\n \tjobID := args[0]\n \tresponse, err := util.GetAPIClientV2(cmd).Jobs().Executions(ctx, &apimodels.ListJobExecutionsRequest{\n@@ -133,10 +133,12 @@ func (o *ExecutionOptions) run(cmd *cobra.Command, args []string) {\n \t\t},\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, err, 1)\n+\t\treturn err\n \t}\n \n \tif err = output.Output(cmd, executionColumns, o.OutputOptions, response.Executions); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to output: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/job/history.go b/cmd/cli/job/history.go\nindex 616d17a875..dbe5cad02a 100644\n--- a/cmd/cli/job/history.go\n+++ b/cmd/cli/job/history.go\n@@ -63,7 +63,7 @@ func NewHistoryCmd() *cobra.Command {\n \t\tLong:    historyLong,\n \t\tExample: historyExample,\n \t\tArgs:    cobra.ExactArgs(1),\n-\t\tRun:     o.run,\n+\t\tRunE:    o.run,\n \t}\n \n \tnodeCmd.Flags().StringVar(&o.EventType, \"event-type\", o.EventType,\n@@ -123,7 +123,7 @@ var historyColumns = []output.TableColumn[*models.JobHistory]{\n \t},\n }\n \n-func (o *HistoryOptions) run(cmd *cobra.Command, args []string) {\n+func (o *HistoryOptions) run(cmd *cobra.Command, args []string) error {\n \tctx := cmd.Context()\n \tjobID := args[0]\n \tresponse, err := util.GetAPIClientV2(cmd).Jobs().History(ctx, &apimodels.ListJobHistoryRequest{\n@@ -139,10 +139,12 @@ func (o *HistoryOptions) run(cmd *cobra.Command, args []string) {\n \t\t},\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, err, 1)\n+\t\treturn err\n \t}\n \n \tif err = output.Output(cmd, historyColumns, o.OutputOptions, response.History); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to output: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/job/list.go b/cmd/cli/job/list.go\nindex 58e799b50a..9eef6a12bc 100644\n--- a/cmd/cli/job/list.go\n+++ b/cmd/cli/job/list.go\n@@ -68,7 +68,7 @@ func NewListCmd() *cobra.Command {\n \t\tLong:    listLong,\n \t\tExample: listExample,\n \t\tArgs:    cobra.NoArgs,\n-\t\tRun:     o.run,\n+\t\tRunE:    o.run,\n \t}\n \n \tlistCmd.Flags().StringVar(&o.Labels, \"labels\", o.Labels,\n@@ -113,7 +113,7 @@ var listColumns = []output.TableColumn[*models.Job]{\n \t},\n }\n \n-func (o *ListOptions) run(cmd *cobra.Command, _ []string) {\n+func (o *ListOptions) run(cmd *cobra.Command, _ []string) error {\n \tctx := cmd.Context()\n \n \tvar err error\n@@ -121,7 +121,7 @@ func (o *ListOptions) run(cmd *cobra.Command, _ []string) {\n \tif o.Labels != \"\" {\n \t\tlabelRequirements, err = labels.ParseToRequirements(o.Labels)\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"could not parse labels: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"could not parse labels: %w\", err)\n \t\t}\n \t}\n \tresponse, err := util.GetAPIClientV2(cmd).Jobs().List(ctx, &apimodels.ListJobsRequest{\n@@ -134,15 +134,17 @@ func (o *ListOptions) run(cmd *cobra.Command, _ []string) {\n \t\t},\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed request: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed request: %w\", err)\n \t}\n \n \tif err = output.Output(cmd, listColumns, o.OutputOptions, response.Jobs); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to output: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n \n \tif response.NextToken != \"\" {\n \t\tmsg := fmt.Sprintf(\"To fetch more records use `--next-token %s`\", response.NextToken)\n \t\tcmd.Printf(\"\\n%s\\n\", msg)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/job/logs.go b/cmd/cli/job/logs.go\nindex 2ff0f0d308..bb964ef3cd 100644\n--- a/cmd/cli/job/logs.go\n+++ b/cmd/cli/job/logs.go\n@@ -39,16 +39,14 @@ func NewLogCmd() *cobra.Command {\n \t\tShort:   logsShortDesc,\n \t\tExample: logsExample,\n \t\tArgs:    cobra.ExactArgs(1),\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) {\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n \t\t\topts := util.LogOptions{\n \t\t\t\tJobID:       cmdArgs[0],\n \t\t\t\tExecutionID: options.ExecutionID,\n \t\t\t\tFollow:      options.Follow,\n \t\t\t\tTail:        options.Tail,\n \t\t\t}\n-\t\t\tif err := util.Logs(cmd, opts); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\t\treturn util.Logs(cmd, opts)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/job/run.go b/cmd/cli/job/run.go\nindex d629c23cc7..040b7b4549 100644\n--- a/cmd/cli/job/run.go\n+++ b/cmd/cli/job/run.go\n@@ -61,7 +61,7 @@ func NewRunCmd() *cobra.Command {\n \t\tLong:    runLong,\n \t\tExample: runExample,\n \t\tArgs:    cobra.MinimumNArgs(0),\n-\t\tRun:     o.run,\n+\t\tRunE:    o.run,\n \t}\n \n \trunCmd.Flags().AddFlagSet(cliflags.NewRunTimeSettingsFlags(o.RunTimeSettings))\n@@ -77,7 +77,7 @@ func NewRunCmd() *cobra.Command {\n \treturn runCmd\n }\n \n-func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n+func (o *RunOptions) run(cmd *cobra.Command, args []string) error {\n \tctx := cmd.Context()\n \n \t// read the job spec from stdin or file\n@@ -86,23 +86,23 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \tif len(args) == 0 {\n \t\tbyteResult, err = util.ReadFromStdinIfAvailable(cmd)\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"unknown error reading from file or stdin: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"unknown error reading from file or stdin: %w\", err)\n \t\t}\n \t} else {\n \t\tvar fileContent *os.File\n \t\tfileContent, err = os.Open(args[0])\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"error opening file: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"error opening file: %w\", err)\n \t\t}\n \t\tdefer fileContent.Close()\n \n \t\tbyteResult, err = io.ReadAll(fileContent)\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"error reading file: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"error reading file: %w\", err)\n \t\t}\n \t}\n \tif len(byteResult) == 0 {\n-\t\tutil.Fatal(cmd, errors.New(userstrings.JobSpecBad), 1)\n+\t\treturn errors.New(userstrings.JobSpecBad)\n \t}\n \n \tif !o.NoTemplate {\n@@ -111,13 +111,11 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \t\t\tEnvPattern:   o.TemplateEnvVarsPattern,\n \t\t})\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to create template parser: %w\", err), 1)\n-\t\t\treturn\n+\t\t\treturn fmt.Errorf(\"failed to create template parser: %w\", err)\n \t\t}\n \t\tbyteResult, err = parser.ParseBytes(byteResult)\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err), 1)\n-\t\t\treturn\n+\t\t\treturn fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err)\n \t\t}\n \t}\n \n@@ -126,16 +124,14 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \tvar j *models.Job\n \terr = marshaller.YAMLUnmarshalWithMax(byteResult, &j)\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err), 1)\n-\t\treturn\n+\t\treturn fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err)\n \t}\n \n \t// Normalize and validate the job spec\n \tj.Normalize()\n \terr = j.ValidateSubmission()\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err), 1)\n-\t\treturn\n+\t\treturn fmt.Errorf(\"%s: %w\", userstrings.JobSpecBad, err)\n \t}\n \n \tif o.RunTimeSettings.DryRun {\n@@ -145,9 +141,9 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \t\t}\n \t\toutputOps := output.NonTabularOutputOptions{Format: output.YAMLFormat}\n \t\tif err = output.OutputOneNonTabular(cmd, outputOps, j); err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write job: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"failed to write job: %w\", err)\n \t\t}\n-\t\treturn\n+\t\treturn nil\n \t}\n \n \t// Submit the job\n@@ -156,8 +152,7 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \t\tJob: j,\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed request: %w\", err), 1)\n-\t\treturn\n+\t\treturn fmt.Errorf(\"failed request: %w\", err)\n \t}\n \n \tif o.ShowWarnings && len(resp.Warnings) > 0 {\n@@ -165,9 +160,10 @@ func (o *RunOptions) run(cmd *cobra.Command, args []string) {\n \t}\n \n \tif err := printer.PrintJobExecution(ctx, resp.JobID, cmd, o.RunTimeSettings, client); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to print job execution: %w\", err), 1)\n-\t\treturn\n+\t\treturn fmt.Errorf(\"failed to print job execution: %w\", err)\n \t}\n+\n+\treturn nil\n }\n \n func (o *RunOptions) printWarnings(cmd *cobra.Command, warnings []string) {\ndiff --git a/cmd/cli/list/list.go b/cmd/cli/list/list.go\nindex faaecf0f8d..eed9dd1d2c 100644\n--- a/cmd/cli/list/list.go\n+++ b/cmd/cli/list/list.go\n@@ -75,10 +75,8 @@ func NewCmd() *cobra.Command {\n \t\tExample:  listExample,\n \t\tPreRunE:  hook.RemoteCmdPreRunHooks,\n \t\tPostRunE: hook.RemoteCmdPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := list(cmd, OL); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn list(cmd, OL)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/node/describe.go b/cmd/cli/node/describe.go\nindex 6206e3a63a..9ff61903ee 100644\n--- a/cmd/cli/node/describe.go\n+++ b/cmd/cli/node/describe.go\n@@ -29,24 +29,26 @@ func NewDescribeCmd() *cobra.Command {\n \t\tUse:   \"describe [id]\",\n \t\tShort: \"Get the info of a node by id.\",\n \t\tArgs:  cobra.ExactArgs(1),\n-\t\tRun:   o.runDescribe,\n+\t\tRunE:  o.runDescribe,\n \t}\n \tnodeCmd.Flags().AddFlagSet(cliflags.OutputNonTabularFormatFlags(&o.OutputOpts))\n \treturn nodeCmd\n }\n \n // Run executes node command\n-func (o *DescribeOptions) runDescribe(cmd *cobra.Command, args []string) {\n+func (o *DescribeOptions) runDescribe(cmd *cobra.Command, args []string) error {\n \tctx := cmd.Context()\n \tnodeID := args[0]\n \tresponse, err := util.GetAPIClientV2(cmd).Nodes().Get(ctx, &apimodels.GetNodeRequest{\n \t\tNodeID: nodeID,\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"could not get node %s: %w\", nodeID, err), 1)\n+\t\treturn fmt.Errorf(\"could not get node %s: %w\", nodeID, err)\n \t}\n \n \tif err = output.OutputOneNonTabular(cmd, o.OutputOpts, response.Node); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to write node %s: %w\", nodeID, err), 1)\n+\t\treturn fmt.Errorf(\"failed to write node %s: %w\", nodeID, err)\n \t}\n+\n+\treturn nil\n }\ndiff --git a/cmd/cli/node/list.go b/cmd/cli/node/list.go\nindex 7a1fe44c04..cd6a104c5e 100644\n--- a/cmd/cli/node/list.go\n+++ b/cmd/cli/node/list.go\n@@ -69,7 +69,7 @@ func (o *ListOptions) run(cmd *cobra.Command, _ []string) error {\n \tif o.Labels != \"\" {\n \t\tlabelRequirements, err = labels.ParseToRequirements(o.Labels)\n \t\tif err != nil {\n-\t\t\tutil.Fatal(cmd, fmt.Errorf(\"could not parse labels: %w\", err), 1)\n+\t\t\treturn fmt.Errorf(\"could not parse labels: %w\", err)\n \t\t}\n \t}\n \n@@ -97,7 +97,7 @@ func (o *ListOptions) run(cmd *cobra.Command, _ []string) error {\n \t\t},\n \t})\n \tif err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed request: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed request: %w\", err)\n \t}\n \n \tcolumns := alwaysColumns\n@@ -106,7 +106,7 @@ func (o *ListOptions) run(cmd *cobra.Command, _ []string) error {\n \t}\n \n \tif err = output.Output(cmd, columns, o.OutputOptions, response.Nodes); err != nil {\n-\t\tutil.Fatal(cmd, fmt.Errorf(\"failed to output: %w\", err), 1)\n+\t\treturn fmt.Errorf(\"failed to output: %w\", err)\n \t}\n \n \treturn nil\ndiff --git a/cmd/cli/serve/serve.go b/cmd/cli/serve/serve.go\nindex 68222b4f1b..6168238cea 100644\n--- a/cmd/cli/serve/serve.go\n+++ b/cmd/cli/serve/serve.go\n@@ -126,7 +126,7 @@ func NewCmd() *cobra.Command {\n \t\tShort:   \"Start the bacalhau compute node\",\n \t\tLong:    serveLong,\n \t\tExample: serveExample,\n-\t\tPreRun: func(cmd *cobra.Command, args []string) {\n+\t\tPreRunE: func(cmd *cobra.Command, args []string) error {\n \t\t\t/*\n \t\t\t\tNB(forrest):\n \t\t\t\t(I learned a lot more about viper and cobra than was intended...)\n@@ -150,9 +150,7 @@ func NewCmd() *cobra.Command {\n \t\t\t\treturn the value of the last flag bound to it. This is why it's important to manage\n \t\t\t\tflag binding thoughtfully, ensuring each command's context is respected.\n \t\t\t*/\n-\t\t\tif err := configflags.BindFlags(cmd, serveFlags); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\t\treturn configflags.BindFlags(cmd, serveFlags)\n \t\t},\n \t\tRunE: func(cmd *cobra.Command, _ []string) error {\n \t\t\treturn serve(cmd)\ndiff --git a/cmd/cli/validate/validate.go b/cmd/cli/validate/validate.go\nindex a52e4ddb1c..e6eb5c8047 100644\n--- a/cmd/cli/validate/validate.go\n+++ b/cmd/cli/validate/validate.go\n@@ -10,7 +10,6 @@ import (\n \t\"github.com/invopop/jsonschema\"\n \t\"github.com/spf13/cobra\"\n \n-\t\"github.com/bacalhau-project/bacalhau/cmd/util\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/model\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/util/templates\"\n \n@@ -67,10 +66,8 @@ func NewCmd() *cobra.Command {\n \t\tLong:    validateLong,\n \t\tExample: validateExample,\n \t\tArgs:    cobra.MinimumNArgs(0),\n-\t\tRun: func(cmd *cobra.Command, cmdArgs []string) { //nolint:unparam // incorrect that cmd is unused.\n-\t\t\tif err := validate(cmd, cmdArgs, OV); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, cmdArgs []string) error {\n+\t\t\treturn validate(cmd, cmdArgs, OV)\n \t\t},\n \t}\n \ndiff --git a/cmd/cli/version/version.go b/cmd/cli/version/version.go\nindex f3010d03db..d26f8b8d67 100644\n--- a/cmd/cli/version/version.go\n+++ b/cmd/cli/version/version.go\n@@ -50,10 +50,8 @@ func NewCmd() *cobra.Command {\n \t\tShort:  \"Get the client and server version.\",\n \t\tArgs:   cobra.NoArgs,\n \t\tPreRun: hook.ApplyPorcelainLogLevel,\n-\t\tRun: func(cmd *cobra.Command, _ []string) {\n-\t\t\tif err := runVersion(cmd, oV); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, _ []string) error {\n+\t\t\treturn runVersion(cmd, oV)\n \t\t},\n \t}\n \tversionCmd.Flags().BoolVar(&oV.ClientOnly, \"client\", oV.ClientOnly, \"If true, shows client version only (no server required).\")\ndiff --git a/cmd/cli/wasm/wasm_run.go b/cmd/cli/wasm/wasm_run.go\nindex d45049801f..cae8df28ae 100644\n--- a/cmd/cli/wasm/wasm_run.go\n+++ b/cmd/cli/wasm/wasm_run.go\n@@ -102,10 +102,8 @@ func newRunCmd() *cobra.Command {\n \t\tArgs:     cobra.MinimumNArgs(1),\n \t\tPreRunE:  hook.Chain(hook.ClientPreRunHooks, configflags.PreRun(wasmRunFlags)),\n \t\tPostRunE: hook.ClientPostRunHooks,\n-\t\tRun: func(cmd *cobra.Command, args []string) {\n-\t\t\tif err := runWasm(cmd, args, opts); err != nil {\n-\t\t\t\tutil.Fatal(cmd, err, 1)\n-\t\t\t}\n+\t\tRunE: func(cmd *cobra.Command, args []string) error {\n+\t\t\treturn runWasm(cmd, args, opts)\n \t\t},\n \t}\n \ndiff --git a/go.work.sum b/go.work.sum\nindex eee5b88d45..32aae5d08e 100644\n--- a/go.work.sum\n+++ b/go.work.sum\n@@ -1218,6 +1218,7 @@ github.com/cncf/xds/go v0.0.0-20230105202645-06c439db220b h1:ACGZRIr7HsgBKHsueQ1\n github.com/cncf/xds/go v0.0.0-20230105202645-06c439db220b/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n github.com/cncf/xds/go v0.0.0-20230607035331-e9ce68804cb4 h1:/inchEIKaYC1Akx+H+gqO04wryn5h75LSazbRlnya1k=\n github.com/cncf/xds/go v0.0.0-20230607035331-e9ce68804cb4/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n+github.com/cncf/xds/go v0.0.0-20231109132714-523115ebc101/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\n github.com/cockroachdb/apd v1.1.0 h1:3LFP3629v+1aKXU5Q37mxmRxX/pIu1nijXydLShEq5I=\n github.com/cockroachdb/cockroach-go/v2 v2.1.1 h1:3XzfSMuUT0wBe1a3o5C0eOTcArhmmFAg2Jzh/7hhKqo=\n github.com/cockroachdb/cockroach-go/v2 v2.1.1/go.mod h1:7NtUnP6eK+l6k483WSYNrq3Kb23bWV10IRV1TyeSpwM=\n", "instance_id": "bacalhau-project__bacalhau-3767", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to standardize the usage of `RunE` over `Run` in `cobra.Command` for error handling consistency across commands. It specifies the goal of centralizing error handling with `util.Fatal` in one place and improving testability by making errors available. However, it lacks specific details on how the error handling should be centralized (e.g., where exactly `util.Fatal` should be called in the root), and it does not provide examples of the desired final structure or mention any potential edge cases or constraints. Additionally, there is no discussion of potential impacts on existing tests or other parts of the codebase. Despite these minor ambiguities, the overall objective and reasoning are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n1. **Scope and Depth of Code Changes:** The changes span multiple files (around 20 files as seen in the diff), but the modifications are repetitive and straightforward\u2014primarily changing `Run` to `RunE`, updating function signatures to return errors, and removing explicit calls to `util.Fatal` in favor of returning errors. The changes are localized to command definitions and do not appear to impact the broader system architecture or require deep understanding of inter-module interactions.\n2. **Technical Concepts Involved:** The problem requires basic familiarity with the `cobra` library (a popular Go library for building CLI applications) and its `Run` vs. `RunE` fields. It also involves understanding error handling in Go, which is a fundamental concept. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes focus on standardizing error returns rather than introducing new error handling logic. The task is more about refactoring for consistency than addressing complex error scenarios.\n4. **Overall Complexity:** While the number of files touched is significant, the nature of the changes is mechanical and does not require deep analysis or complex decision-making. A developer with moderate Go experience and familiarity with CLI tools could implement these changes with minimal risk of introducing bugs.\nA score of 0.35 reflects the slightly higher effort due to the number of files involved, but the simplicity of the changes keeps it within the Easy category.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Rule `variable_011` does not enforce on procedure calls\n**Environment**\r\nv3.27.0\r\n\r\n**Describe the bug**\r\nThe rule `variable_011` enforces consistent case for variables. However, this does not work on cases where a variable is used in a procedure call.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Create a file called test.vhd with the following contents:\r\n```vhdl\r\nARCHITECTURE RTL OF TEST IS\r\n\r\n  PROCEDURE MY_PROC (\r\n    SIGNAL PARAM1   : INTEGER;\r\n    VARIABLE PARAM2 : INTEGER\r\n  ) IS\r\n  BEGIN\r\n\r\n  END PROCEDURE MY_PROC;\r\n\r\n  SIGNAL SIG1 : INTEGER;\r\n\r\nBEGIN\r\n\r\n  PROCESS1 : PROCESS IS\r\n\r\n    VARIABLE VAR1 : INTEGER;\r\n\r\n  BEGIN\r\n\r\n    MY_PROC(\r\n            PARAM1 => SIG1,\r\n            PARAM2 => VAR1\r\n          );\r\n\r\n  END PROCESS PROCESS1;\r\n\r\nEND ARCHITECTURE RTL;\r\n```\r\n2. Run `vsg -f test.vhd --fix`\r\n3. Read test.vhd again\r\n```vhdl\r\narchitecture rtl of test is\r\n\r\n  procedure my_proc (\r\n    signal param1   : integer;\r\n    variable param2 : integer\r\n  ) is\r\n  begin\r\n\r\n  end procedure my_proc;\r\n\r\n  signal sig1 : integer;\r\n\r\nbegin\r\n\r\n  process1 : process is\r\n\r\n    variable var1 : integer;\r\n\r\n  begin\r\n\r\n    my_proc(\r\n            param1 => sig1,\r\n            param2 => VAR1\r\n          );\r\n\r\n  end process process1;\r\n\r\nend architecture rtl;\r\n```\r\nAll of the code other than the variable name in the procedure call has been corrected to lowercase.\r\n\r\n**Expected behavior**\r\nI expect this rule to act on variable names in procedure calls.\r\n\n", "patch": "diff --git a/vsg/rules/variable/rule_011.py b/vsg/rules/variable/rule_011.py\nindex c1e2f9f76..a0ea6501a 100644\n--- a/vsg/rules/variable/rule_011.py\n+++ b/vsg/rules/variable/rule_011.py\n@@ -12,6 +12,7 @@\n lNames.append(token.simple_variable_assignment.target)\n lNames.append(token.selected_variable_assignment.target)\n lNames.append(token.conditional_variable_assignment.target)\n+lNames.append(token.association_element.actual_part)\n \n \n class rule_011(Rule):\n", "instance_id": "jeremiah-c-leary__vhdl-style-guide-1336", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the `variable_011` rule, which fails to enforce consistent casing for variable names in procedure calls within VHDL code. The goal is explicitly stated: the rule should apply to variable names in procedure calls. The input and output are demonstrated through a detailed example, including the steps to reproduce the issue and the expected behavior. The provided VHDL code snippets before and after running the tool illustrate the problem effectively. There are no significant ambiguities, and the constraints (i.e., the rule's scope) are implicitly clear from the context. The inclusion of environment details (version 3.27.0) and a specific test case further enhances clarity. Overall, the problem description leaves little room for misinterpretation, justifying a score of 3.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code change is minimal and localized to a single line in a single file (`vsg/rules/variable/rule_011.py`). The modification involves adding a new token type (`token.association_element.actual_part`) to a list of targets for the rule's enforcement. This does not impact the broader architecture of the system or require understanding complex interactions between modules. The change is straightforward and does not involve significant refactoring or extensive code modifications.\n\n2. **Number of Technical Concepts**: Solving this problem requires basic familiarity with the structure of the VHDL parser or linter being used (likely a custom tool named `vsg`). The developer needs to understand the token system and how rules are applied to specific parts of the code. However, no advanced language features, complex algorithms, design patterns, or domain-specific knowledge beyond basic VHDL syntax and tool configuration are necessary. The concept of token-based rule enforcement is relatively simple for someone familiar with linters or static analysis tools.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code change does not appear to introduce new error handling requirements. However, a developer might need to consider whether applying the rule to `actual_part` in association elements could lead to unintended consequences (e.g., misapplying casing rules to non-variable elements). This is a minor concern and does not significantly increase the difficulty, as the change aligns with the existing pattern of rule application.\n\n4. **Overall Complexity**: The task is a simple bug fix that extends an existing rule to cover an additional case. It requires minimal debugging or investigation beyond identifying the correct token type to include. The impact is limited to the specific rule's behavior, and there are no performance or scalability concerns evident from the change.\n\nGiven these factors, a difficulty score of 0.25 reflects the ease of the task. It requires understanding some code logic (the token system and rule application) and making a simple modification, but it does not involve complex concepts, extensive changes, or significant edge case handling. This is a straightforward fix suitable for a junior or intermediate developer with basic familiarity with the tool's codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Different behavior of hidden status inheritance between Jsonnet and Go-Jsonnet\n## Summary\r\n\r\nI've been evaluating switching to Go-Jsonnet for our tooling and stumbled across a difference in how hidden status inheritance (https://jsonnet.org/ref/spec.html#hidden_inheritance) is implemented in Jsonnet and Go-Jsonnet.\r\n\r\nHere's an example Jsonnet script which demonstrates the difference:\r\n\r\n```jsonnet\r\n// `makeMergeable` is taken from our tooling. We use this extensively to make user inputs loaded from YAML mergeable\r\nlocal makeMergeable(o) = {\r\n  [key]+: makeMergeable(o[key])\r\n  for key in std.objectFields(o)\r\n  if std.isObject(o[key])\r\n} + {\r\n  [key]+: o[key]\r\n  for key in std.objectFields(o)\r\n  if std.isArray(o[key])\r\n} + {\r\n  [key]: o[key]\r\n  for key in std.objectFields(o)\r\n  if !std.isObject(o[key]) && !std.isArray(o[key])\r\n\r\n};\r\n\r\nlocal base = { field:: 'data' };\r\n\r\n{\r\n  regular: base { field: 'other' }, // hidden status of `field` is inherited in both implementations\r\n  makeMergeable: base + makeMergeable({\r\n    field: 'other',\r\n  }), // hidden status of `field` is inherited through the object comprehension in go-jsonnet, but the hidden status of the field is lost in jsonnet\r\n}\r\n```\r\n\r\nFrom what I can tell, the go-jsonnet implementation implements the specification correctly.\r\n\r\n## Example script evaluation and output\r\n\r\nI've wrapped the example in Python (because that's what I had readily available) to run both implementations side-by-side:\r\n\r\n```python\r\nimport _jsonnet\r\nimport _gojsonnet\r\n\r\nprog = \"\"\"\r\n// `makeMergeable` is taken from our tooling. We use this extensively to make user inputs mergeable\r\nlocal makeMergeable(o) = {\r\n  [key]+: makeMergeable(o[key])\r\n  for key in std.objectFields(o)\r\n  if std.isObject(o[key])\r\n} + {\r\n  [key]+: o[key]\r\n  for key in std.objectFields(o)\r\n  if std.isArray(o[key])\r\n} + {\r\n  [key]: o[key]\r\n  for key in std.objectFields(o)\r\n  if !std.isObject(o[key]) && !std.isArray(o[key])\r\n\r\n};\r\n\r\nlocal base = { field:: 'data' };\r\n\r\n{\r\n  regular: base { field: 'other' }, // hidden status of `field` is inherited in both implementations\r\n  makeMergeable: base + makeMergeable({\r\n    field: 'other',\r\n  }), // hidden status of `field` is inherited through the object comprehension in go-jsonnet, but the hidden status of the field is lost in jsonnet\r\n}\r\n\"\"\"\r\n\r\nprint(\"Jsonnet:\\n\" + _jsonnet.evaluate_snippet(\"test.jsonnet\", prog))\r\nprint(\"Go-Jsonnet:\\n\" + _gojsonnet.evaluate_snippet(\"test.jsonnet\", prog))\r\n```\r\n\r\nThis gives the following in a virtualenv (setup with `python3 -m venv venv && source venv/bin/activate && pip install jsonnet gojsonnet`):\r\n\r\n```console\r\n(venv) simon@phoenix:~/tmp/jsonnet-issue $ pip list\r\nPackage    Version\r\n---------- -------\r\ngojsonnet  0.20.0\r\njsonnet    0.20.0\r\npip        22.0.2\r\nsetuptools 59.6.0\r\n(venv) simon@phoenix:~/tmp/jsonnet-issue $ python test.py\r\nJsonnet:\r\n{\r\n   \"makeMergeable\": {\r\n      \"field\": \"other\"\r\n   },\r\n   \"regular\": { }\r\n}\r\n\r\nGo-Jsonnet:\r\n{\r\n   \"makeMergeable\": { },\r\n   \"regular\": { }\r\n}\r\n\r\n```\r\n\n", "patch": "diff --git a/core/vm.cpp b/core/vm.cpp\nindex 21418c475..62966e1d0 100644\n--- a/core/vm.cpp\n+++ b/core/vm.cpp\n@@ -748,7 +748,7 @@ class Interpreter {\n \n         } else if (auto *obj = dynamic_cast<const HeapComprehensionObject *>(obj_)) {\n             for (const auto &f : obj->compValues)\n-                r[f.first] = ObjectField::VISIBLE;\n+                r[f.first] = ObjectField::INHERIT;\n         }\n         return r;\n     }\n", "instance_id": "google__jsonnet-1140", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue of differing behavior in hidden status inheritance between Jsonnet and Go-Jsonnet implementations. It provides a detailed example script, along with the expected outputs from both implementations, which helps in understanding the discrepancy. The summary and example evaluation effectively highlight the core issue. However, there are minor ambiguities: the problem statement does not explicitly define what the \"correct\" behavior should be beyond referencing the Go-Jsonnet implementation as adhering to the specification. Additionally, it lacks explicit mention of edge cases or constraints that might affect the behavior of hidden status inheritance in other scenarios. While a link to the specification is provided, a brief explanation of the expected behavior per the spec would have made the statement more comprehensive. Overall, the statement is valid and clear but misses some minor details that could enhance understanding.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the provided factors. First, the scope and depth of code changes are minimal, involving a single-line modification in a specific file (vm.cpp) to change the field visibility status from VISIBLE to INHERIT. This change does not impact multiple modules or the broader system architecture, and the amount of code change is trivial. Second, the number of technical concepts required to solve this problem is low; it primarily involves understanding the Jsonnet language specification regarding hidden field inheritance and a basic grasp of C++ to interpret and modify the interpreter's behavior in the VM. No advanced algorithms, design patterns, or domain-specific knowledge beyond Jsonnet internals are needed. Third, the problem statement and code change do not explicitly address edge cases or complex error handling requirements, and the fix appears straightforward without introducing new error conditions. Overall, this task requires understanding some code logic in the Jsonnet interpreter but involves a simple and localized modification, fitting within the easy difficulty range (0.2-0.4).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "disable allowing for <root> in tach.toml?\nHey, is there a way of configuring tach to not allow dependencies on `<root>`? I would like the following behavior:\r\n\r\n- when running tach sync, never introduce `<root>` but fail instead\r\n- when tach.toml was manually added to include `<root>`, fail on `tach check`\r\n\r\nThis would make it easier for people in my repo to start using tach and not by accident introduce `<root>` deps because they forgot to explicitly add a new module.\n", "patch": "diff --git a/docs/usage/configuration.mdx b/docs/usage/configuration.mdx\nindex eab6d3a8..f4104921 100644\n--- a/docs/usage/configuration.mdx\n+++ b/docs/usage/configuration.mdx\n@@ -22,6 +22,8 @@ This is the project-level configuration file which should be in the root of your\n \n `forbid_circular_dependencies` is a boolean which, when enabled, causes `tach check` to fail if any circular dependencies are detected.\n \n+`root_module` takes a string enum value, and determines how Tach treats code which lives within the project but is not covered by an explicit module. This is described in detail [below](#the_root_module)\n+\n `use_regex_matching` is a boolean which, when enabled, uses regex (default) matching to exclude patterns else uses globs matching.\n \n ```toml\n@@ -39,6 +41,8 @@ exact = true\n ignore_type_checking_imports = true\n forbid_circular_dependencies = true\n \n+root_module = \"allow\"\n+\n [[modules]]\n path = \"tach\"\n depends_on = []\n@@ -93,6 +97,39 @@ Each module listed under the `modules` key above can accept the following attrib\n   Tach also supports [deprecating individual dependencies](../usage/deprecate).\n </Note>\n \n+## The Root Module\n+\n+By default, Tach checks all of the source files beneath all of the configured [source roots](#source_roots).\n+This means that some code may not be contained within any configured [module](#modules).\n+\n+For example, given the file tree below:\n+\n+```\n+my_repo/\n+  tach.toml\n+  script.py\n+  lib/\n+    module1.py\n+    module2/\n+      __init__.py\n+      service.py\n+    module3.py\n+  docs/\n+  tests/\n+```\n+\n+If `lib.module1`, `lib.module2`, and `lib.module3` are the only configured modules, then the code in `script.py` would be automatically part of the `<root>` module.\n+\n+This module can declare its own dependencies with `depends_on` and use the rest of the available module configuration.\n+Further, other modules need to declare an explicit dependency on `<root>` to use code which rolls up to the root.\n+\n+Tach allows configuring how the root module should be treated through the `root_module` key in `tach.toml`. It may take one of the following values:\n+\n+- **(default)** `\"allow\"`: Treat `<root>` as a catch-all rollup module which must be explicitly declared as a dependency and must declare its own dependencies on other modules.\n+- **(permissive)** `\"ignore\"`: Disable all checks related to the `<root>` module. `tach check` will never fail due to code in the `<root>` module, and `tach sync` will never add `<root>` to `tach.toml`\n+- **(stricter)** `\"dependenciesonly\"`: Forbid any module from listing `<root>` as a dependency, but allow `<root>` to declare its own dependencies.\n+- **(strictest)** `\"forbid\"`: Forbid any reference to the `<root>` module in tach.toml. This means that all code in [source roots](#source_roots) MUST be contained within an explicitly configured [module](#modules).\n+\n ## Source Roots\n \n The `source_roots` key is required for Tach to understand the imports within your project.\ndiff --git a/src/check_int.rs b/src/check_int.rs\nindex 623d4207..471464a0 100644\n--- a/src/check_int.rs\n+++ b/src/check_int.rs\n@@ -9,7 +9,7 @@ use thiserror::Error;\n \n use crate::{\n     core::{\n-        config::ProjectConfig,\n+        config::{ProjectConfig, RootModuleTreatment},\n         module::{ModuleNode, ModuleTree},\n     },\n     exclusion::{self, is_path_excluded, set_excluded_paths},\n@@ -139,6 +139,7 @@ fn check_import(\n     file_mod_path: &str,\n     module_tree: &ModuleTree,\n     file_nearest_module: Option<Arc<ModuleNode>>,\n+    root_module_treatment: RootModuleTreatment,\n ) -> Result<(), ImportCheckError> {\n     let import_nearest_module = match module_tree.find_nearest(import_mod_path) {\n         Some(module) => module,\n@@ -147,6 +148,10 @@ fn check_import(\n         None => return Ok(()),\n     };\n \n+    if import_nearest_module.is_root() && root_module_treatment == RootModuleTreatment::Ignore {\n+        return Ok(());\n+    }\n+\n     let file_nearest_module = file_nearest_module\n         // Lookup file_mod_path if module not given\n         .or_else(|| module_tree.find_nearest(file_mod_path))\n@@ -265,6 +270,7 @@ pub fn check(\n         &source_roots,\n         valid_modules,\n         project_config.forbid_circular_dependencies,\n+        project_config.root_module.clone(),\n     )?;\n \n     set_excluded_paths(\n@@ -283,6 +289,11 @@ pub fn check(\n             let Some(nearest_module) = module_tree.find_nearest(&mod_path) else {\n                 continue;\n             };\n+\n+            if nearest_module.is_root() && project_config.root_module == RootModuleTreatment::Ignore\n+            {\n+                continue;\n+            }\n             let project_imports = match get_project_imports(\n                 &source_roots,\n                 abs_file_path,\n@@ -319,6 +330,7 @@ pub fn check(\n                     &mod_path,\n                     &module_tree,\n                     Some(Arc::clone(&nearest_module)),\n+                    project_config.root_module.clone(),\n                 ) else {\n                     continue;\n                 };\n@@ -381,14 +393,26 @@ mod tests {\n         #[case] import_mod_path: &str,\n         #[case] expected_result: bool,\n     ) {\n-        let check_error = check_import(import_mod_path, file_mod_path, &module_tree, None);\n+        let check_error = check_import(\n+            import_mod_path,\n+            file_mod_path,\n+            &module_tree,\n+            None,\n+            RootModuleTreatment::Allow,\n+        );\n         let result = check_error.is_ok();\n         assert_eq!(result, expected_result);\n     }\n \n     #[rstest]\n     fn test_check_deprecated_import(module_tree: ModuleTree) {\n-        let check_error = check_import(\"domain_one.subdomain\", \"domain_one\", &module_tree, None);\n+        let check_error = check_import(\n+            \"domain_one.subdomain\",\n+            \"domain_one\",\n+            &module_tree,\n+            None,\n+            RootModuleTreatment::Allow,\n+        );\n         assert!(check_error.is_err());\n         assert!(check_error.unwrap_err().is_deprecated());\n     }\ndiff --git a/src/core/config.rs b/src/core/config.rs\nindex 804ab67f..98b3ee26 100644\n--- a/src/core/config.rs\n+++ b/src/core/config.rs\n@@ -183,6 +183,33 @@ pub struct UnusedDependencies {\n     pub dependencies: Vec<DependencyConfig>,\n }\n \n+#[derive(Debug, Serialize, Default, Deserialize, Clone, PartialEq)]\n+#[serde(rename_all = \"lowercase\")]\n+pub enum RootModuleTreatment {\n+    #[default]\n+    Allow,\n+    Forbid,\n+    Ignore,\n+    DependenciesOnly,\n+}\n+\n+impl RootModuleTreatment {\n+    fn is_default(&self) -> bool {\n+        *self == Self::default()\n+    }\n+}\n+\n+impl IntoPy<PyObject> for RootModuleTreatment {\n+    fn into_py(self, py: Python) -> PyObject {\n+        match self {\n+            Self::Allow => \"allow\".to_object(py),\n+            Self::Forbid => \"forbid\".to_object(py),\n+            Self::Ignore => \"ignore\".to_object(py),\n+            Self::DependenciesOnly => \"dependenciesonly\".to_object(py),\n+        }\n+    }\n+}\n+\n #[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n #[serde(deny_unknown_fields)]\n #[pyclass(get_all, module = \"tach.extension\")]\n@@ -209,6 +236,8 @@ pub struct ProjectConfig {\n     pub forbid_circular_dependencies: bool,\n     #[serde(default = \"default_true\", skip_serializing_if = \"is_true\")]\n     pub use_regex_matching: bool,\n+    #[serde(default, skip_serializing_if = \"RootModuleTreatment::is_default\")]\n+    pub root_module: RootModuleTreatment,\n }\n \n impl Default for ProjectConfig {\n@@ -224,6 +253,7 @@ impl Default for ProjectConfig {\n             ignore_type_checking_imports: default_true(),\n             forbid_circular_dependencies: Default::default(),\n             use_regex_matching: default_true(),\n+            root_module: Default::default(),\n         }\n     }\n }\n@@ -291,6 +321,7 @@ impl ProjectConfig {\n             ignore_type_checking_imports: self.ignore_type_checking_imports,\n             forbid_circular_dependencies: self.forbid_circular_dependencies,\n             use_regex_matching: self.use_regex_matching,\n+            root_module: self.root_module.clone(),\n         }\n     }\n \ndiff --git a/src/core/module.rs b/src/core/module.rs\nindex a1719e54..f7b30573 100644\n--- a/src/core/module.rs\n+++ b/src/core/module.rs\n@@ -46,6 +46,10 @@ impl ModuleNode {\n         }\n     }\n \n+    pub fn is_root(&self) -> bool {\n+        self.full_path == \".\" && self.is_end_of_path\n+    }\n+\n     pub fn fill(\n         &mut self,\n         config: ModuleConfig,\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 85862946..04a98038 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -95,6 +95,8 @@ impl From<sync::SyncError> for PyErr {\n         match err {\n             SyncError::FileWrite(err) => PyOSError::new_err(err.to_string()),\n             SyncError::TomlSerialize(err) => PyOSError::new_err(err.to_string()),\n+            SyncError::CheckError(err) => err.into(),\n+            SyncError::RootModuleViolation(err) => PyValueError::new_err(err.to_string()),\n         }\n     }\n }\n@@ -298,7 +300,7 @@ fn sync_dependency_constraints(\n     project_config: ProjectConfig,\n     exclude_paths: Vec<String>,\n     prune: bool,\n-) -> ProjectConfig {\n+) -> Result<ProjectConfig, SyncError> {\n     sync::sync_dependency_constraints(project_root, project_config, exclude_paths, prune)\n }\n \ndiff --git a/src/parsing/config.rs b/src/parsing/config.rs\nindex 7a05a4ac..7067f01c 100644\n--- a/src/parsing/config.rs\n+++ b/src/parsing/config.rs\n@@ -40,7 +40,7 @@ mod tests {\n \n     use super::*;\n     use crate::{\n-        core::config::{CacheConfig, DependencyConfig, ExternalDependencyConfig, ModuleConfig},\n+        core::config::{DependencyConfig, ModuleConfig},\n         tests::fixtures::example_dir,\n     };\n     use filesystem::DEFAULT_EXCLUDE_PATHS;\n@@ -79,7 +79,7 @@ mod tests {\n                         ..Default::default()\n                     }\n                 ],\n-                cache: CacheConfig::default(),\n+                cache: Default::default(),\n                 exclude: DEFAULT_EXCLUDE_PATHS\n                     .into_iter()\n                     .chain([\"domain_four\"].into_iter())\n@@ -91,7 +91,8 @@ mod tests {\n                 ignore_type_checking_imports: true,\n                 forbid_circular_dependencies: true,\n                 use_regex_matching: true,\n-                external: ExternalDependencyConfig::default(),\n+                external: Default::default(),\n+                root_module: Default::default(),\n             }\n         );\n     }\ndiff --git a/src/parsing/error.rs b/src/parsing/error.rs\nindex 9c846eb2..21f0dc51 100644\n--- a/src/parsing/error.rs\n+++ b/src/parsing/error.rs\n@@ -50,6 +50,8 @@ pub enum ModuleTreeError {\n     VisibilityViolation(Vec<VisibilityErrorInfo>),\n     #[error(\"Circular dependency detected: {0:?}\")]\n     CircularDependency(Vec<String>),\n+    #[error(\"Root module violation: {0:?}\")]\n+    RootModuleViolation(String),\n     #[error(\"Parsing Error while building module tree.\\n{0}\")]\n     ParseError(#[from] ParsingError),\n     #[error(\"Cannot insert module with empty path.\")]\ndiff --git a/src/parsing/module.rs b/src/parsing/module.rs\nindex e9d03e4b..8b693209 100644\n--- a/src/parsing/module.rs\n+++ b/src/parsing/module.rs\n@@ -1,8 +1,9 @@\n use std::collections::{HashMap, HashSet};\n use std::path::PathBuf;\n \n-use crate::core::config::{global_visibility, ModuleConfig};\n+use crate::core::config::{global_visibility, ModuleConfig, RootModuleTreatment};\n use crate::core::module::ModuleTree;\n+use crate::filesystem::ROOT_MODULE_SENTINEL_TAG;\n use petgraph::algo::kosaraju_scc;\n use petgraph::graphmap::DiGraphMap;\n \n@@ -108,10 +109,63 @@ pub fn find_modules_with_cycles(modules: &[ModuleConfig]) -> Vec<&String> {\n     modules_with_cycles\n }\n \n+fn validate_root_module_treatment(\n+    root_module_treatment: RootModuleTreatment,\n+    modules: &[ModuleConfig],\n+) -> Result<(), ModuleTreeError> {\n+    match root_module_treatment {\n+        RootModuleTreatment::Allow | RootModuleTreatment::Ignore => Ok(()),\n+        RootModuleTreatment::Forbid => {\n+            let root_module_violations: Vec<String> = modules\n+                .iter()\n+                .filter_map(|module| {\n+                    if module.path == ROOT_MODULE_SENTINEL_TAG\n+                        || module\n+                            .depends_on\n+                            .iter()\n+                            .any(|dep| dep.path == ROOT_MODULE_SENTINEL_TAG)\n+                    {\n+                        return Some(module.path.clone());\n+                    }\n+                    None\n+                })\n+                .collect();\n+\n+            if root_module_violations.is_empty() {\n+                Ok(())\n+            } else {\n+                Err(ModuleTreeError::RootModuleViolation(format!(\n+                    \"The root module ('{}') is forbidden, but was found in module configuration for modules: {}.\",\n+                    ROOT_MODULE_SENTINEL_TAG,\n+                    root_module_violations.into_iter().map(|module| format!(\"'{}'\", module)).collect::<Vec<_>>().join(\", \")\n+                )))\n+            }\n+        }\n+        RootModuleTreatment::DependenciesOnly => {\n+            let root_module_violations: Vec<String> = modules\n+                .iter()\n+                .filter(|module| module.path == ROOT_MODULE_SENTINEL_TAG)\n+                .map(|module| module.path.clone())\n+                .collect();\n+\n+            if root_module_violations.is_empty() {\n+                Ok(())\n+            } else {\n+                Err(ModuleTreeError::RootModuleViolation(format!(\n+                    \"The root module ('{}') is set to allow dependencies only, but was found as a dependency in: {}.\",\n+                    ROOT_MODULE_SENTINEL_TAG,\n+                    root_module_violations.into_iter().map(|module| format!(\"'{}'\", module)).collect::<Vec<_>>().join(\", \")\n+                )))\n+            }\n+        }\n+    }\n+}\n+\n pub fn build_module_tree(\n     source_roots: &[PathBuf],\n     modules: Vec<ModuleConfig>,\n     forbid_circular_dependencies: bool,\n+    root_module_treatment: RootModuleTreatment,\n ) -> Result<ModuleTree, ModuleTreeError> {\n     // Check for duplicate modules\n     let duplicate_modules = find_duplicate_modules(&modules);\n@@ -127,6 +181,9 @@ pub fn build_module_tree(\n         return Err(ModuleTreeError::VisibilityViolation(visibility_error_info));\n     }\n \n+    // Check for root module treatment errors\n+    validate_root_module_treatment(root_module_treatment, &modules)?;\n+\n     // Check for circular dependencies if forbidden\n     if forbid_circular_dependencies {\n         let module_paths = find_modules_with_cycles(&modules);\ndiff --git a/src/sync.rs b/src/sync.rs\nindex 70a848c8..bf8c195d 100644\n--- a/src/sync.rs\n+++ b/src/sync.rs\n@@ -1,8 +1,10 @@\n use thiserror::Error;\n \n-use crate::check_int::check;\n-use crate::core::config::{global_visibility, DependencyConfig, ModuleConfig, ProjectConfig};\n-use crate::filesystem as fs;\n+use crate::check_int::{check, CheckError};\n+use crate::core::config::{\n+    global_visibility, DependencyConfig, ModuleConfig, ProjectConfig, RootModuleTreatment,\n+};\n+use crate::filesystem::{self as fs, ROOT_MODULE_SENTINEL_TAG};\n use crate::parsing::config::dump_project_config_to_toml;\n use std::collections::HashMap;\n use std::path::PathBuf;\n@@ -13,6 +15,43 @@ pub enum SyncError {\n     FileWrite(#[from] std::io::Error),\n     #[error(\"Failed to serialize project configuration to TOML.\\n{0}\")]\n     TomlSerialize(#[from] toml::ser::Error),\n+    #[error(\"Failed to sync project.\\n{0}\")]\n+    CheckError(#[from] CheckError),\n+    #[error(\"Failed to sync project configuration due to root module violation.\\n{0}\")]\n+    RootModuleViolation(String),\n+}\n+\n+fn handle_detected_dependency(\n+    module_path: &str,\n+    dependency: DependencyConfig,\n+    project_config: &mut ProjectConfig,\n+) -> Result<(), SyncError> {\n+    let module_is_root = module_path == ROOT_MODULE_SENTINEL_TAG;\n+    let dependency_is_root = dependency.path == ROOT_MODULE_SENTINEL_TAG;\n+\n+    if !module_is_root && !dependency_is_root {\n+        project_config.add_dependency_to_module(module_path, dependency);\n+        return Ok(());\n+    }\n+\n+    match project_config.root_module {\n+        RootModuleTreatment::Ignore => Ok(()),\n+        RootModuleTreatment::Allow => {\n+            project_config.add_dependency_to_module(module_path, dependency);\n+            Ok(())\n+        }\n+        RootModuleTreatment::Forbid => Err(SyncError::RootModuleViolation(format!(\n+            \"The root module is forbidden, but it was found that '{}' depends on '{}'.\",\n+            module_path, dependency.path\n+        ))),\n+        RootModuleTreatment::DependenciesOnly => {\n+            if dependency_is_root {\n+                return Err(SyncError::RootModuleViolation(format!(\"No module may depend on the root module, but it was found that '{}' depends on the root module.\", module_path)));\n+            }\n+            project_config.add_dependency_to_module(module_path, dependency);\n+            Ok(())\n+        }\n+    }\n }\n \n /// Update project configuration with auto-detected dependency constraints.\n@@ -23,7 +62,7 @@ pub fn sync_dependency_constraints(\n     mut project_config: ProjectConfig,\n     exclude_paths: Vec<String>,\n     prune: bool,\n-) -> ProjectConfig {\n+) -> Result<ProjectConfig, SyncError> {\n     let mut deprecation_map: HashMap<String, Vec<String>> = HashMap::new();\n     let mut visibility_map: HashMap<String, Vec<String>> = HashMap::new();\n     let mut new_project_config = None;\n@@ -67,8 +106,7 @@ pub fn sync_dependency_constraints(\n     let mut new_project_config = new_project_config.unwrap_or(project_config);\n \n     // If prune is false, the existing project config is reused without changes\n-    let check_result = check(project_root, &new_project_config, exclude_paths)\n-        .expect(\"Failed to run the check function\");\n+    let check_result = check(project_root, &new_project_config, exclude_paths)?;\n \n     // Iterate through the check results to add dependencies to the config\n     for error in check_result.errors {\n@@ -87,7 +125,8 @@ pub fn sync_dependency_constraints(\n                 deprecated,\n             };\n \n-            new_project_config.add_dependency_to_module(source_path, dependency);\n+            // The project config determines whether the sync fails, ignores, or adds this dependency\n+            handle_detected_dependency(source_path, dependency, &mut new_project_config)?\n         }\n     }\n \n@@ -98,7 +137,7 @@ pub fn sync_dependency_constraints(\n         }\n     }\n \n-    new_project_config\n+    Ok(new_project_config)\n }\n \n pub fn sync_project(\n@@ -108,7 +147,7 @@ pub fn sync_project(\n     add: bool,\n ) -> Result<String, SyncError> {\n     let mut project_config =\n-        sync_dependency_constraints(project_root, project_config, exclude_paths, !add);\n+        sync_dependency_constraints(project_root, project_config, exclude_paths, !add)?;\n \n     Ok(dump_project_config_to_toml(&mut project_config)?)\n }\n", "instance_id": "gauge-sh__tach-362", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in expressing the desired behavior: preventing the use of `<root>` as a dependency in `tach.toml` during `tach sync` and failing `tach check` if `<root>` is manually added. The goal is to avoid accidental dependencies on `<root>` in a repository using the `tach` tool. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define what `<root>` represents in the context of the tool (though it can be inferred from the code changes as a catch-all module for unconfigured code). Additionally, there are no examples of input/output configurations or specific error messages expected when the behavior is triggered. Edge cases, such as how existing configurations with `<root>` should be handled during migration, are not addressed. Despite these minor gaps, the intent and high-level requirements are understandable, especially when paired with the code changes, which provide context on the implementation approach.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (e.g., configuration parsing, sync logic, module validation, and documentation), requiring a moderate understanding of the codebase's architecture and interactions between components like `check`, `sync`, and configuration handling. The changes involve adding a new configuration option (`root_module`) with multiple behavior modes (`allow`, `ignore`, `dependenciesonly`, `forbid`), which introduces complexity in validation and dependency handling logic. \n\nFrom a technical concepts perspective, the solution requires familiarity with Rust's enum handling, serialization/deserialization (via `serde`), error handling, and modification of existing logic for dependency checks and synchronization. The implementation also touches on domain-specific knowledge related to module dependency management in a tool like `tach`, which adds a layer of complexity for someone unfamiliar with the project's purpose.\n\nEdge cases and error handling are significant in this problem. The code changes address different treatments of the `<root>` module, requiring careful handling of scenarios like existing configurations, forbidden dependencies, and ignored checks. The logic for failing `tach sync` or `tach check` based on the configuration mode introduces potential pitfalls if not thoroughly tested (e.g., ensuring no unintended side effects on other modules or dependency graphs).\n\nWhile the problem does not require advanced algorithms or system-level considerations, it does demand a solid grasp of the existing codebase structure and careful implementation to avoid breaking existing functionality. The amount of code change is moderate, with both new additions (e.g., `RootModuleTreatment` enum, validation logic) and modifications to existing flows (e.g., `sync` and `check` functions). It does not significantly impact the system's architecture but does introduce a new behavioral control mechanism. Overall, I rate this as a medium difficulty task (0.55), suitable for a developer with intermediate experience in Rust and some familiarity with the project's design.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Nice in systemd unit is always 5\nHello \r\nI noticed that `nice` in global section is not correctly [passed](https://github.com/creativeprojects/resticprofile/blob/3476fbce1425f3368527c05a9130d511fe530139/systemd/generate.go#L31) to systemd unit template. \r\n\r\nfor example with this config\r\n\r\n```yaml\r\nversion: \"1\"\r\nglobal:\r\n  nice: 17\r\ndefault:\r\n  repository: \"my_repo\"\r\n  password-file: \"key\"\r\n  backup:\r\n    exclude-file: \"excludes\"\r\n    exclude-caches: true\r\n    one-file-system: true\r\n    tag:\r\n      - \"test\"\r\n    source:\r\n      - \"/tmp/\"\r\n    schedule: \"*-*-* 12:15:00\"\r\n    schedule-permission: system\r\n    schedule-priority: background\r\n    schedule-lock-mode: default\r\n    schedule-lock-wait: 15m30s\r\n ```\r\nAfter running `schedule` the systemd unit file is \r\n\r\n```\r\ncat /etc/systemd/system/resticprofile-backup@profile-default.service\r\n[Unit]\r\nDescription=resticprofile backup for profile default in ./profiles.yaml\r\n\r\n[Service]\r\nType=notify\r\nWorkingDirectory=/tmp\r\nExecStart=/usr/local/bin/resticprofile --no-prio --no-ansi --config ./profiles.yaml --name default --lock-wait 15m30s backup\r\nNice=5\r\nEnvironment=\"HOME=/root\"\r\nEnvironment=\"SUDO_USER=test\"\r\n```\r\nNice is set to 5 and not to 17 as expected\r\n\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex 2f67aba0..e12be48b 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -39,3 +39,5 @@ status.json\n \n go.work\n go.work.sum\n+\n+/*.txt\ndiff --git a/Makefile b/Makefile\nindex 1cea6565..6247f6c8 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -309,3 +309,17 @@ fix:\n \tGOOS=darwin golangci-lint run --fix\n \tGOOS=linux golangci-lint run --fix\n \tGOOS=windows golangci-lint run --fix\n+\n+.PHONY: deploy-current\n+deploy-current: build-linux build-pi\n+\t@echo \"[*] $@\"\n+\tfor server in $$(cat targets_amd64.txt); do \\\n+\t\techo \"Deploying to $$server\" ; \\\n+\t\trsync -avz --progress $(BINARY_LINUX_AMD64) $$server: ; \\\n+\t\tssh $$server \"sudo -S install $(BINARY_LINUX_AMD64) /usr/local/bin/resticprofile\" ; \\\n+\tdone\n+\tfor server in $$(cat targets_armv6.txt); do \\\n+\t\techo \"Deploying to $$server\" ; \\\n+\t\trsync -avz --progress $(BINARY_PI) $$server: ; \\\n+\t\tssh $$server \"sudo -S install $(BINARY_PI) /usr/local/bin/resticprofile\" ; \\\n+\tdone\ndiff --git a/config/global.go b/config/global.go\nindex 91108061..d3da9f51 100644\n--- a/config/global.go\n+++ b/config/global.go\n@@ -9,9 +9,9 @@ import (\n \n // Global holds the configuration from the global section\n type Global struct {\n-\tIONice               bool                `mapstructure:\"ionice\" default:\"false\" description:\"Enables setting the unix IO priority class and level for resticprofile and child processes (only on unix OS).\"`\n-\tIONiceClass          int                 `mapstructure:\"ionice-class\" default:\"2\" range:\"[1:3]\" description:\"Sets the unix \\\"ionice-class\\\" to apply when \\\"ionice\\\" is enabled\"`\n-\tIONiceLevel          int                 `mapstructure:\"ionice-level\" default:\"0\" range:\"[0:7]\" description:\"Sets the unix \\\"ionice-level\\\" to apply when \\\"ionice\\\" is enabled\"`\n+\tIONice               bool                `mapstructure:\"ionice\" default:\"false\" description:\"Enables setting the linux IO priority class and level for resticprofile and child processes (only on linux OS).\"`\n+\tIONiceClass          int                 `mapstructure:\"ionice-class\" default:\"2\" range:\"[1:3]\" description:\"Sets the linux \\\"ionice-class\\\" (I/O scheduling class) to apply when \\\"ionice\\\" is enabled (1=realtime, 2=best-effort, 3=idle)\"`\n+\tIONiceLevel          int                 `mapstructure:\"ionice-level\" default:\"0\" range:\"[0:7]\" description:\"Sets the linux \\\"ionice-level\\\" (I/O priority within the scheduling class) to apply when \\\"ionice\\\" is enabled (0=highest priority, 7=lowest priority)\"`\n \tNice                 int                 `mapstructure:\"nice\" default:\"0\" range:\"[-20:19]\" description:\"Sets the unix \\\"nice\\\" value for resticprofile and child processes (on any OS)\"`\n \tPriority             string              `mapstructure:\"priority\" default:\"normal\" enum:\"idle;background;low;normal;high;highest\" description:\"Sets process priority class for resticprofile and child processes (on any OS)\"`\n \tDefaultCommand       string              `mapstructure:\"default-command\" default:\"snapshots\" description:\"The restic or resticprofile command to use when no command was specified\"`\n@@ -20,8 +20,8 @@ type Global struct {\n \tResticBinary         string              `mapstructure:\"restic-binary\" description:\"Full path of the restic executable (detected if not set)\"`\n \tResticVersion        string              // not configurable at the moment. To be set after ResticBinary is known.\n \tFilterResticFlags    bool                `mapstructure:\"restic-arguments-filter\" default:\"true\" description:\"Remove unknown flags instead of passing all configured flags to restic\"`\n-\tResticLockRetryAfter time.Duration       `mapstructure:\"restic-lock-retry-after\" default:\"1m\" description:\"Time to wait before trying to get a lock on a restic repositoey - see https://creativeprojects.github.io/resticprofile/usage/locks/\"`\n-\tResticStaleLockAge   time.Duration       `mapstructure:\"restic-stale-lock-age\" default:\"1h\" description:\"The age an unused lock on a restic repository must have at least before resiticprofile attempts to unlock - see https://creativeprojects.github.io/resticprofile/usage/locks/\"`\n+\tResticLockRetryAfter time.Duration       `mapstructure:\"restic-lock-retry-after\" default:\"1m\" description:\"Time to wait before trying to get a lock on a restic repository - see https://creativeprojects.github.io/resticprofile/usage/locks/\"`\n+\tResticStaleLockAge   time.Duration       `mapstructure:\"restic-stale-lock-age\" default:\"1h\" description:\"The age an unused lock on a restic repository must have at least before resticprofile attempts to unlock - see https://creativeprojects.github.io/resticprofile/usage/locks/\"`\n \tShellBinary          []string            `mapstructure:\"shell\" default:\"auto\" examples:\"sh;bash;pwsh;powershell;cmd\" description:\"The shell that is used to run commands (default is OS specific)\"`\n \tMinMemory            uint64              `mapstructure:\"min-memory\" default:\"100\" description:\"Minimum available memory (in MB) required to run any commands - see https://creativeprojects.github.io/resticprofile/usage/memory/\"`\n \tScheduler            string              `mapstructure:\"scheduler\" default:\"auto\" examples:\"auto;launchd;systemd;taskscheduler;crond;crond:/usr/bin/crontab;crontab:*:/etc/cron.d/resticprofile\" description:\"Selects the scheduler. Blank or \\\"auto\\\" uses the default scheduler of your operating system: \\\"launchd\\\", \\\"systemd\\\", \\\"taskscheduler\\\" or \\\"crond\\\" (as fallback). Alternatively you can set \\\"crond\\\" for cron compatible schedulers supporting the crontab executable API or \\\"crontab:[user:]file\\\" to write into a crontab file directly. The need for a user is detected if missing and can be set to a name, \\\"-\\\" (no user) or \\\"*\\\" (current user).\"`\ndiff --git a/docs/content/configuration/priority.md b/docs/content/configuration/priority.md\nindex b942f9e1..0f4643a1 100644\n--- a/docs/content/configuration/priority.md\n+++ b/docs/content/configuration/priority.md\n@@ -10,7 +10,7 @@ You can lower the priority of restic to avoid slowing down other processes. This\n \n ## Nice\n \n-You can use these values for the `priority` parameter:\n+You can use these values for the `priority` parameter, string or numeric values are both valid:\n \n | String value | \"nice\" equivalent on unixes | Notes |\n |--------------|-----------------------------|------|\ndiff --git a/docs/content/schedules/systemd.md b/docs/content/schedules/systemd.md\nindex 07efc3d5..14cb0b03 100644\n--- a/docs/content/schedules/systemd.md\n+++ b/docs/content/schedules/systemd.md\n@@ -1,7 +1,7 @@\n ---\n title: \"Systemd\"\n weight: 105\n-tags: [\"v0.25.0\"]\n+tags: [\"v0.25.0\", \"v0.29.0\"]\n ---\n \n \n@@ -48,6 +48,7 @@ Specifying the profile option `schedule-after-network-online: true` means that t\n for a network connection before running.\n This is done via an [After=network-online.target](https://systemd.io/NETWORK_ONLINE/) entry in the service.\n \n+\n ## systemd drop-in files\n \n It is possible to automatically populate `*.conf.d`\n@@ -158,6 +159,22 @@ pass = $(systemd-ask-password -n \"smb restic user password\" | rclone obscure -)\n EOF\n ```\n \n+## Priority and CPU scheduling\n+\n+resticprofile allows you to set the `nice` value, the CPU scheduling policy and IO nice values for the systemd service.\n+This is only working properly for resticprofile >= 0.29.0.\n+\n+| systemd unit option  | resticprofile option |\n+|----------------------|----------------------|\n+| CPUSchedulingPolicy  | set to `idle` if schedule `priority` = `background` , otherwise default to standard policy |\n+| Nice                 | `nice` from `global` section |\n+| IOSchedulingClass    | `ionice-class` from `global` section |\n+| IOSchedulingPriority | `ionice-level` from `global` section |\n+\n+{{% notice note %}}\n+When setting the `CPUSchedulingPolicy` to `idle` (by setting `priority` to `background`), the backup might never execute if all your CPU cores are always busy.\n+{{% /notice %}}\n+\n ## How to change the default systemd unit and timer file using a template\n \n By default, an opinionated systemd unit and timer are automatically generated by resticprofile.\n@@ -219,12 +236,18 @@ Here are the defaults if you don't specify your own (which I recommend to use as\n Description={{ .JobDescription }}\n {{ if .AfterNetworkOnline }}After=network-online.target\n {{ end }}\n-\n [Service]\n Type=notify\n WorkingDirectory={{ .WorkingDirectory }}\n ExecStart={{ .CommandLine }}\n-{{ if .Nice }}Nice={{ .Nice }}{{ end }}\n+{{ if .Nice }}Nice={{ .Nice }}\n+{{ end -}}\n+{{ if .CPUSchedulingPolicy }}CPUSchedulingPolicy={{ .CPUSchedulingPolicy }}\n+{{ end -}}\n+{{ if .IOSchedulingClass }}IOSchedulingClass={{ .IOSchedulingClass }}\n+{{ end -}}\n+{{ if .IOSchedulingPriority }}IOSchedulingPriority={{ .IOSchedulingPriority }}\n+{{ end -}}\n {{ range .Environment -}}\n Environment=\"{{ . }}\"\n {{ end -}}\ndiff --git a/examples/linux.yaml b/examples/linux.yaml\nindex e67d6447..ffc80720 100644\n--- a/examples/linux.yaml\n+++ b/examples/linux.yaml\n@@ -8,6 +8,10 @@ global:\n     priority: low\n     systemd-unit-template: sample.service\n     prevent-sleep: false\n+    ionice: true\n+    ionice-class: 3\n+    ionice-level: 7\n+    nice: 19\n \n default:\n     password-file: key\n@@ -57,12 +61,12 @@ test2:\n     backup:\n         source: ./\n         schedule: \"*:05,20,35,50\"\n-        schedule-permission: system\n+        schedule-permission: user\n         schedule-log: backup-test2.log\n         run-after: \"chown -R $SUDO_USER $HOME/.cache/restic /tmp/backup\"\n     check:\n         schedule: \"*-*-2\"\n-        schedule-permission: system\n+        schedule-permission: user\n         schedule-log: check-test2.log\n \n test3:\ndiff --git a/examples/sample.service b/examples/sample.service\nindex 4cb59b55..747ccd21 100644\n--- a/examples/sample.service\n+++ b/examples/sample.service\n@@ -6,7 +6,14 @@ OnFailure=unit-status-mail@%n.service\n Type=notify\n WorkingDirectory={{ .WorkingDirectory }}\n ExecStart={{ .CommandLine }}\n-{{ if .Nice }}Nice={{ .Nice }}{{ end }}\n+{{ if .Nice }}Nice={{ .Nice }}\n+{{ end -}}\n+{{ if .CPUSchedulingPolicy }}CPUSchedulingPolicy={{ .CPUSchedulingPolicy }}\n+{{ end -}}\n+{{ if .IOSchedulingClass }}IOSchedulingClass={{ .IOSchedulingClass }}\n+{{ end -}}\n+{{ if .IOSchedulingPriority }}IOSchedulingPriority={{ .IOSchedulingPriority }}\n+{{ end -}}\n {{ range .Environment -}}\n Environment=\"{{ . }}\"\n {{ end -}}\ndiff --git a/schedule/handler_systemd.go b/schedule/handler_systemd.go\nindex ded48e18..55023407 100644\n--- a/schedule/handler_systemd.go\n+++ b/schedule/handler_systemd.go\n@@ -107,20 +107,23 @@ func (h *HandlerSystemd) CreateJob(job *Config, schedules []*calendar.Event, per\n \t}\n \n \terr := systemd.Generate(systemd.Config{\n-\t\tCommandLine:        job.Command + \" --no-prio \" + strings.Join(job.Arguments, \" \"),\n-\t\tEnvironment:        job.Environment,\n-\t\tWorkingDirectory:   job.WorkingDirectory,\n-\t\tTitle:              job.ProfileName,\n-\t\tSubTitle:           job.CommandName,\n-\t\tJobDescription:     job.JobDescription,\n-\t\tTimerDescription:   job.TimerDescription,\n-\t\tSchedules:          job.Schedules,\n-\t\tUnitType:           unitType,\n-\t\tPriority:           job.GetPriority(),\n-\t\tUnitFile:           h.config.UnitTemplate,\n-\t\tTimerFile:          h.config.TimerTemplate,\n-\t\tAfterNetworkOnline: job.AfterNetworkOnline,\n-\t\tDropInFiles:        job.SystemdDropInFiles,\n+\t\tCommandLine:          job.Command + \" \" + strings.Join(append([]string{\"--no-prio\"}, job.Arguments...), \" \"),\n+\t\tEnvironment:          job.Environment,\n+\t\tWorkingDirectory:     job.WorkingDirectory,\n+\t\tTitle:                job.ProfileName,\n+\t\tSubTitle:             job.CommandName,\n+\t\tJobDescription:       job.JobDescription,\n+\t\tTimerDescription:     job.TimerDescription,\n+\t\tSchedules:            job.Schedules,\n+\t\tUnitType:             unitType,\n+\t\tPriority:             job.GetPriority(),\n+\t\tUnitFile:             h.config.UnitTemplate,\n+\t\tTimerFile:            h.config.TimerTemplate,\n+\t\tAfterNetworkOnline:   job.AfterNetworkOnline,\n+\t\tDropInFiles:          job.SystemdDropInFiles,\n+\t\tNice:                 h.config.Nice,\n+\t\tIOSchedulingClass:    h.config.IONiceClass,\n+\t\tIOSchedulingPriority: h.config.IONiceLevel,\n \t})\n \tif err != nil {\n \t\treturn err\ndiff --git a/schedule/scheduler_config.go b/schedule/scheduler_config.go\nindex 8521dba5..3ea542a1 100644\n--- a/schedule/scheduler_config.go\n+++ b/schedule/scheduler_config.go\n@@ -52,6 +52,9 @@ func (s SchedulerCrond) Convert(_ string) SchedulerConfig { return s }\n type SchedulerSystemd struct {\n \tUnitTemplate  string\n \tTimerTemplate string\n+\tNice          int\n+\tIONiceClass   int\n+\tIONiceLevel   int\n }\n \n func (s SchedulerSystemd) Type() string                     { return constants.SchedulerSystemd }\n@@ -70,6 +73,7 @@ func NewSchedulerConfig(global *config.Global) SchedulerConfig {\n \t\t} else {\n \t\t\treturn SchedulerCrond{}\n \t\t}\n+\n \tcase constants.SchedulerCrontab:\n \t\tif len(resource) > 0 {\n \t\t\tif user, location, found := strings.Cut(resource, \":\"); found {\n@@ -85,27 +89,38 @@ func NewSchedulerConfig(global *config.Global) SchedulerConfig {\n \t\t} else {\n \t\t\tpanic(fmt.Errorf(\"invalid schedule %q, no crontab file was specified, expecting \\\"%s: filename\\\"\", scheduler, scheduler))\n \t\t}\n+\n \tcase constants.SchedulerLaunchd:\n \t\treturn SchedulerLaunchd{}\n+\n \tcase constants.SchedulerSystemd:\n-\t\treturn SchedulerSystemd{\n-\t\t\tUnitTemplate:  global.SystemdUnitTemplate,\n-\t\t\tTimerTemplate: global.SystemdTimerTemplate,\n-\t\t}\n+\t\treturn getSchedulerSystemdDefaultConfig(global)\n+\n \tcase constants.SchedulerWindows:\n \t\treturn SchedulerWindows{}\n+\n \tdefault:\n \t\treturn SchedulerDefaultOS{\n \t\t\tdefaults: []SchedulerConfig{\n-\t\t\t\tSchedulerSystemd{\n-\t\t\t\t\tUnitTemplate:  global.SystemdUnitTemplate,\n-\t\t\t\t\tTimerTemplate: global.SystemdTimerTemplate,\n-\t\t\t\t},\n+\t\t\t\tgetSchedulerSystemdDefaultConfig(global),\n \t\t\t},\n \t\t}\n \t}\n }\n \n+func getSchedulerSystemdDefaultConfig(global *config.Global) SchedulerSystemd {\n+\tscheduler := SchedulerSystemd{\n+\t\tUnitTemplate:  global.SystemdUnitTemplate,\n+\t\tTimerTemplate: global.SystemdTimerTemplate,\n+\t\tNice:          global.Nice,\n+\t}\n+\tif global.IONice {\n+\t\tscheduler.IONiceClass = global.IONiceClass\n+\t\tscheduler.IONiceLevel = global.IONiceLevel\n+\t}\n+\treturn scheduler\n+}\n+\n var (\n \t_ SchedulerConfig = SchedulerDefaultOS{}\n \t_ SchedulerConfig = SchedulerCrond{}\ndiff --git a/systemd/generate.go b/systemd/generate.go\nindex 1535407c..672b8cb7 100644\n--- a/systemd/generate.go\n+++ b/systemd/generate.go\n@@ -31,7 +31,14 @@ Description={{ .JobDescription }}\n Type=notify\n WorkingDirectory={{ .WorkingDirectory }}\n ExecStart={{ .CommandLine }}\n-{{ if .Nice }}Nice={{ .Nice }}{{ end }}\n+{{ if .Nice }}Nice={{ .Nice }}\n+{{ end -}}\n+{{ if .CPUSchedulingPolicy }}CPUSchedulingPolicy={{ .CPUSchedulingPolicy }}\n+{{ end -}}\n+{{ if .IOSchedulingClass }}IOSchedulingClass={{ .IOSchedulingClass }}\n+{{ end -}}\n+{{ if .IOSchedulingPriority }}IOSchedulingPriority={{ .IOSchedulingPriority }}\n+{{ end -}}\n {{ range .Environment -}}\n Environment=\"{{ . }}\"\n {{ end -}}\n@@ -66,33 +73,40 @@ var fs afero.Fs\n // templateInfo to create systemd unit\n type templateInfo struct {\n \ttemplates.DefaultData\n-\tJobDescription     string\n-\tTimerDescription   string\n-\tWorkingDirectory   string\n-\tCommandLine        string\n-\tOnCalendar         []string\n-\tSystemdProfile     string\n-\tNice               int\n-\tEnvironment        []string\n-\tAfterNetworkOnline bool\n+\tJobDescription       string\n+\tTimerDescription     string\n+\tWorkingDirectory     string\n+\tCommandLine          string\n+\tOnCalendar           []string\n+\tSystemdProfile       string\n+\tNice                 int\n+\tEnvironment          []string\n+\tAfterNetworkOnline   bool\n+\tCPUSchedulingPolicy  string\n+\tIOSchedulingClass    int\n+\tIOSchedulingPriority int\n }\n \n // Config for generating systemd unit and timer files\n type Config struct {\n-\tCommandLine        string\n-\tEnvironment        []string\n-\tWorkingDirectory   string\n-\tTitle              string\n-\tSubTitle           string\n-\tJobDescription     string\n-\tTimerDescription   string\n-\tSchedules          []string\n-\tUnitType           UnitType\n-\tPriority           string\n-\tUnitFile           string\n-\tTimerFile          string\n-\tDropInFiles        []string\n-\tAfterNetworkOnline bool\n+\tCommandLine          string\n+\tEnvironment          []string\n+\tWorkingDirectory     string\n+\tTitle                string\n+\tSubTitle             string\n+\tJobDescription       string\n+\tTimerDescription     string\n+\tSchedules            []string\n+\tUnitType             UnitType\n+\tPriority             string // standard or background\n+\tUnitFile             string\n+\tTimerFile            string\n+\tDropInFiles          []string\n+\tAfterNetworkOnline   bool\n+\tNice                 int\n+\tCPUSchedulingPolicy  string\n+\tIOSchedulingClass    int\n+\tIOSchedulingPriority int\n }\n \n func init() {\n@@ -123,22 +137,25 @@ func Generate(config Config) error {\n \t\tenvironment = append(environment, fmt.Sprintf(\"SUDO_USER=%s\", sudoUser))\n \t}\n \n-\tnice := constants.DefaultBackgroundNiceFlag\n-\tif config.Priority == constants.SchedulePriorityStandard {\n-\t\tnice = constants.DefaultStandardNiceFlag\n+\tpolicy := \"\"\n+\tif config.Priority == constants.SchedulePriorityBackground {\n+\t\tpolicy = \"idle\"\n \t}\n \n \tinfo := templateInfo{\n-\t\tDefaultData:        templates.NewDefaultData(nil),\n-\t\tJobDescription:     config.JobDescription,\n-\t\tTimerDescription:   config.TimerDescription,\n-\t\tWorkingDirectory:   config.WorkingDirectory,\n-\t\tCommandLine:        config.CommandLine,\n-\t\tOnCalendar:         config.Schedules,\n-\t\tAfterNetworkOnline: config.AfterNetworkOnline,\n-\t\tSystemdProfile:     systemdProfile,\n-\t\tNice:               nice,\n-\t\tEnvironment:        environment,\n+\t\tDefaultData:          templates.NewDefaultData(nil),\n+\t\tJobDescription:       config.JobDescription,\n+\t\tTimerDescription:     config.TimerDescription,\n+\t\tWorkingDirectory:     config.WorkingDirectory,\n+\t\tCommandLine:          config.CommandLine,\n+\t\tOnCalendar:           config.Schedules,\n+\t\tAfterNetworkOnline:   config.AfterNetworkOnline,\n+\t\tSystemdProfile:       systemdProfile,\n+\t\tNice:                 config.Nice,\n+\t\tEnvironment:          environment,\n+\t\tCPUSchedulingPolicy:  policy,\n+\t\tIOSchedulingClass:    config.IOSchedulingClass,\n+\t\tIOSchedulingPriority: config.IOSchedulingPriority,\n \t}\n \n \tvar data bytes.Buffer\n", "instance_id": "creativeprojects__resticprofile-409", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `nice` value in the systemd unit file is hardcoded to 5 and does not reflect the user-configured value (e.g., 17 as shown in the example configuration). The goal is evident\u2014ensure the configured `nice` value from the global section of the configuration is correctly passed to the systemd unit template. The provided example configuration and the resulting systemd unit file output effectively illustrate the discrepancy. However, there are minor ambiguities: the problem statement does not explicitly mention other related settings like `ionice` or CPU scheduling policies, which are also addressed in the code changes. Additionally, edge cases or constraints (e.g., valid ranges for `nice`, behavior on non-Unix systems) are not discussed, leaving some room for interpretation. Overall, the statement is valid and clear but lacks comprehensive detail on the full scope of related configurations and potential edge cases.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (e.g., `schedule/handler_systemd.go`, `systemd/generate.go`, configuration templates, and documentation), requiring a moderate understanding of the codebase structure and interactions between configuration parsing, scheduling, and systemd unit generation. The changes involve passing the `nice` value (and related `ionice` settings) from the global configuration to the systemd unit template, which necessitates modifications in how configuration data is handled and applied in the template generation logic. \n\nFrom a technical concepts perspective, the problem requires familiarity with Go (struct handling, template rendering), systemd unit file syntax, and Unix process priority concepts (`nice`, `ionice`, CPU scheduling policies). While these concepts are not overly complex for an experienced developer, they do require domain-specific knowledge of Linux system administration and process scheduling.\n\nThe amount of code change is moderate, involving updates to structs, template files, and documentation, but it does not significantly impact the system's architecture. Edge case handling appears minimal in the problem statement and code changes\u2014there are no explicit mentions of handling invalid `nice` values or platform-specific behaviors (e.g., non-Linux systems), though the changes do account for conditional rendering in templates. Error handling modifications are not evident in the provided diff.\n\nOverall, this problem is of medium difficulty (0.45) as it requires understanding multiple components of the codebase and applying specific technical knowledge, but it does not involve deep architectural changes or highly complex logic. It is approachable for a developer with intermediate experience in Go and Linux system concepts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Consider pruning publisher entries even if `prune_imports` is disabled when a new publisher entry is being added for a crate\nIn order to avoid unnecessary changes to files such as `imports.lock` cargo-vet makes some effort to avoid removing entries from the file unless requested. The main way this happens in normal operation is when `cargo vet certify` is used to add a new audit for a crate, which can lead to other exemptions, imports, etc. for the crate to be removed.\r\n\r\nUnfortunately this pattern doesn't work super well for crates with wildcard audits or trusted entries. For these crates, generally adding a dependency does not require the user running any command other than `cargo vet`, which does not prune unused entries, which means that the previous `publisher` entries are left behind and not cleaned up.\r\n\r\nWe could consider automatically pruning `publisher` entries for crates even when `prune_imports` is disabled so long as a new publisher entry is being added for that crate. This still avoids unnecessary updates, but means we can automatically clean up publisher entries when new entries are being added anyway.\r\n\r\nWe could also consider doing this for other forms of imports such as normal and wildcard audits, though removing those from `imports.lock` due to them becoming unnecessary is less frequent during normal dependency updates.\n", "patch": "diff --git a/src/resolver.rs b/src/resolver.rs\nindex 1c79cb57..daf0ba1f 100644\n--- a/src/resolver.rs\n+++ b/src/resolver.rs\n@@ -2858,6 +2858,39 @@ pub fn update_store(\n     get_store_updates(cfg, store, mode).apply(store);\n }\n \n+/// Helper function to determine if we should be pruning imports.\n+///\n+/// We always prune if requested, but will also prune imports if a new audit or\n+/// publisher entry is being added for the crate.\n+fn should_prune_imports(\n+    store: &Store,\n+    required_entries: &Option<SortedMap<RequiredEntry, CriteriaSet>>,\n+    mode: UpdateMode,\n+    pkgname: PackageStr<'_>,\n+) -> bool {\n+    if mode.prune_imports {\n+        true\n+    } else if let Some(required_entries) = required_entries {\n+        let import = |idx| store.imported_audits().values().nth(idx).unwrap();\n+        required_entries.keys().any(|entry| match entry {\n+            RequiredEntry::Audit {\n+                import_index,\n+                audit_index,\n+            } => import(*import_index).audits[pkgname][*audit_index].is_fresh_import,\n+            RequiredEntry::WildcardAudit {\n+                import_index,\n+                audit_index,\n+            } => import(*import_index).wildcard_audits[pkgname][*audit_index].is_fresh_import,\n+            RequiredEntry::Publisher { publisher_index } => {\n+                store.publishers()[pkgname][*publisher_index].is_fresh_import\n+            }\n+            _ => false,\n+        })\n+    } else {\n+        false\n+    }\n+}\n+\n /// The non-mutating core of `update_store` for use in non-mutating situations.\n pub(crate) fn get_store_updates(\n     cfg: &Config,\n@@ -2932,10 +2965,11 @@ pub(crate) fn get_store_updates(\n                 .wildcard_audits\n                 .iter()\n                 .map(|(pkgname, wildcard_audits)| {\n-                    let prune_imports = mode(&pkgname[..]).prune_imports;\n                     let required_entries = required_entries\n                         .get(&pkgname[..])\n                         .unwrap_or(&no_required_entries);\n+                    let prune_imports =\n+                        should_prune_imports(store, required_entries, mode(&pkgname[..]), pkgname);\n                     (\n                         pkgname,\n                         wildcard_audits\n@@ -2974,12 +3008,13 @@ pub(crate) fn get_store_updates(\n                 .audits\n                 .iter()\n                 .map(|(pkgname, audits)| {\n-                    let prune_imports = mode(&pkgname[..]).prune_imports;\n                     let (uses_package, required_entries) = match required_entries.get(&pkgname[..])\n                     {\n                         Some(e) => (true, e),\n                         None => (false, &no_required_entries),\n                     };\n+                    let prune_imports =\n+                        should_prune_imports(store, required_entries, mode(&pkgname[..]), pkgname);\n                     (\n                         pkgname,\n                         audits\n@@ -3029,10 +3064,11 @@ pub(crate) fn get_store_updates(\n \n     // Determine which live publisher information to keep in the imports.lock file.\n     for (pkgname, publishers) in store.publishers() {\n-        let prune_imports = mode(&pkgname[..]).prune_imports;\n         let required_entries = required_entries\n             .get(&pkgname[..])\n             .unwrap_or(&no_required_entries);\n+        let prune_imports =\n+            should_prune_imports(store, required_entries, mode(&pkgname[..]), pkgname);\n         let mut publishers: Vec<_> = publishers\n             .iter()\n             .enumerate()\n", "instance_id": "mozilla__cargo-vet-621", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of automatically pruning publisher entries for crates when a new entry is added, even if `prune_imports` is disabled. It provides context about the issue with unused entries in `imports.lock` for crates with wildcard audits or trusted entries and explains the motivation behind the proposed change. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define what constitutes a \"fresh import\" or how the system determines when an entry is \"new.\" Additionally, while it mentions considering similar pruning for other forms of imports (normal and wildcard audits), it does not clarify whether this is part of the current scope or a future consideration. Edge cases, such as potential conflicts or performance impacts of pruning, are not addressed. Overall, the problem is understandable, but these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`resolver.rs`) with modifications to logic around pruning imports. The changes involve adding a helper function (`should_prune_imports`) and updating several sections of the code to use this function, which requires understanding the existing logic for managing imports and audits in the codebase. The amount of code change is moderate, with around 30-40 lines added or modified, and it does not appear to impact the broader system architecture significantly.\n\nTechnically, the problem requires familiarity with Rust's syntax and standard library (e.g., working with iterators, pattern matching, and data structures like `SortedMap`), as well as an understanding of the domain-specific logic around package management and audit entries in the context of `cargo-vet`. Concepts such as determining \"fresh imports\" and managing required entries add a layer of complexity, but they are not overly advanced. The code changes also involve conditional logic to decide when to prune, which requires careful consideration of the existing `UpdateMode` and `Store` structures.\n\nEdge cases and error handling do not seem to be explicitly addressed in the problem statement or code changes, but the logic in `should_prune_imports` could potentially introduce subtle bugs if the conditions for pruning are not well-defined or if there are unexpected interactions with other parts of the system (e.g., performance overhead from repeated checks). However, no explicit error handling modifications are required based on the provided diff.\n\nOverall, this problem requires a moderate level of understanding of the codebase and Rust, along with careful implementation of the pruning logic, but it does not involve deep architectural changes or highly complex concepts. A score of 0.45 reflects this balance of moderate complexity and focused scope.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Alternative ``concat`` way several times faster than ``with_columns()`` when adding many columns to the df\n### Checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\r\n\r\n### Reproducible example\r\n\r\n```python\r\nimport polars as pl\r\nimport numpy as np\r\n\r\n# Set the number of rows and columns\r\nnum_rows = 10000\r\nnum_columns = 10000\r\n\r\nnp.random.seed(42)\r\n\r\ndata1 = {f\"col_{i}\": np.random.rand(num_rows) for i in range(num_columns)}\r\ndf1 = pl.DataFrame(data1)\r\n\r\ndata2 = {f\"feature_{i}\": np.random.rand(num_rows) for i in range(num_columns)}\r\ndf2=pl.DataFrame(data2)\r\n\r\n%timeit df1.with_columns(df2)\r\n\r\n%timeit pl.concat([df1,df2], how=\"horizontal\")\r\n```\r\n### Log output\r\n\r\n```shell\r\n1.28 s \u00b1 31.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n1.85 ms \u00b1 57.8 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n### Issue description\r\n\r\nSo when adding multiple columns, ``concat`` is way faster than the normal ``with_columns()`` method. Would be nice if these two methods were equally as fast.\r\n\r\n### Expected behavior\r\n\r\nThat these functions become equally as fast\r\n\r\n### Installed versions\r\n\r\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.12.0\r\nIndex type:          UInt32\r\nPlatform:            Windows-11-10.0.22631-SP0\r\nPython:              3.13.0 | packaged by Anaconda, Inc. | (main, Oct  7 2024, 21:21:52) [MSC v.1929 64 bit (AMD64)]\r\nLTS CPU:             False\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  <not installed>\r\naltair               <not installed>\r\ncloudpickle          3.1.0\r\nconnectorx           <not installed>\r\ndeltalake            <not installed>\r\nfastexcel            <not installed>\r\nfsspec               <not installed>\r\ngevent               <not installed>\r\ngreat_tables         <not installed>\r\nmatplotlib           3.9.2\r\nnest_asyncio         1.6.0\r\nnumpy                2.1.2\r\nopenpyxl             3.1.5\r\npandas               2.2.3\r\npyarrow              18.0.0\r\npydantic             <not installed>\r\npyiceberg            <not installed>\r\nsqlalchemy           <not installed>\r\ntorch                <not installed>\r\nxlsx2csv             <not installed>\r\nxlsxwriter           <not installed>\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/crates/polars-core/src/frame/mod.rs b/crates/polars-core/src/frame/mod.rs\nindex aa434fb07df7..e3b969a81756 100644\n--- a/crates/polars-core/src/frame/mod.rs\n+++ b/crates/polars-core/src/frame/mod.rs\n@@ -1382,12 +1382,24 @@ impl DataFrame {\n         self\n     }\n \n+    // Note: Schema can be both input or output_schema\n     fn add_column_by_schema(&mut self, c: Column, schema: &Schema) -> PolarsResult<()> {\n         let name = c.name();\n         if let Some((idx, _, _)) = schema.get_full(name.as_str()) {\n-            // schema is incorrect fallback to search\n             if self.columns.get(idx).map(|s| s.name()) != Some(name) {\n-                self.add_column_by_search(c)?;\n+                // Given schema is output_schema and we can push.\n+                if idx == self.columns.len() {\n+                    if self.width() == 0 {\n+                        self.height = c.len();\n+                    }\n+\n+                    self.columns.push(c);\n+                }\n+                // Schema is incorrect fallback to search\n+                else {\n+                    debug_assert!(false);\n+                    self.add_column_by_search(c)?;\n+                }\n             } else {\n                 self.replace_column(idx, c)?;\n             }\n@@ -1401,6 +1413,7 @@ impl DataFrame {\n         Ok(())\n     }\n \n+    // Note: Schema can be both input or output_schema\n     pub fn _add_series(&mut self, series: Vec<Series>, schema: &Schema) -> PolarsResult<()> {\n         for (i, s) in series.into_iter().enumerate() {\n             // we need to branch here\n@@ -1430,6 +1443,8 @@ impl DataFrame {\n     /// Add a new column to this [`DataFrame`] or replace an existing one.\n     /// Uses an existing schema to amortize lookups.\n     /// If the schema is incorrect, we will fallback to linear search.\n+    ///\n+    /// Note: Schema can be both input or output_schema\n     pub fn with_column_and_schema<C: IntoColumn>(\n         &mut self,\n         column: C,\ndiff --git a/crates/polars-mem-engine/src/executors/stack.rs b/crates/polars-mem-engine/src/executors/stack.rs\nindex ba6fa8111402..a93d4fc72d89 100644\n--- a/crates/polars-mem-engine/src/executors/stack.rs\n+++ b/crates/polars-mem-engine/src/executors/stack.rs\n@@ -8,6 +8,7 @@ pub struct StackExec {\n     pub(crate) has_windows: bool,\n     pub(crate) exprs: Vec<Arc<dyn PhysicalExpr>>,\n     pub(crate) input_schema: SchemaRef,\n+    pub(crate) output_schema: SchemaRef,\n     pub(crate) options: ProjectionOptions,\n     // Can run all operations elementwise\n     pub(crate) streamable: bool,\n@@ -19,7 +20,7 @@ impl StackExec {\n         state: &ExecutionState,\n         mut df: DataFrame,\n     ) -> PolarsResult<DataFrame> {\n-        let schema = &*self.input_schema;\n+        let schema = &*self.output_schema;\n \n         // Vertical and horizontal parallelism.\n         let df = if self.streamable\ndiff --git a/crates/polars-mem-engine/src/planner/lp.rs b/crates/polars-mem-engine/src/planner/lp.rs\nindex 3a5e525867fb..0d438b5f5bd1 100644\n--- a/crates/polars-mem-engine/src/planner/lp.rs\n+++ b/crates/polars-mem-engine/src/planner/lp.rs\n@@ -629,7 +629,7 @@ fn create_physical_plan_impl(\n         HStack {\n             input,\n             exprs,\n-            schema: _schema,\n+            schema: output_schema,\n             options,\n         } => {\n             let input_schema = lp_arena.get(input).schema(lp_arena).into_owned();\n@@ -659,6 +659,7 @@ fn create_physical_plan_impl(\n                 has_windows: state.has_windows,\n                 exprs: phys_exprs,\n                 input_schema,\n+                output_schema,\n                 options,\n                 streamable,\n             }))\n", "instance_id": "pola-rs__polars-19701", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in identifying the performance discrepancy between `concat` and `with_columns()` methods in the Polars library when adding multiple columns to a DataFrame. It provides a reproducible example with Python code and performance metrics, which helps in understanding the issue. The goal is explicitly stated: to make `with_columns()` as fast as `concat` for the given use case. However, there are minor ambiguities and missing details. For instance, the problem does not specify whether the performance improvement should maintain all existing functionality of `with_columns()` (e.g., handling expressions or aliases), nor does it discuss potential trade-offs or constraints like memory usage. Additionally, edge cases or specific scenarios beyond the provided example (e.g., DataFrames with different heights or schemas) are not mentioned. Thus, while the core issue is clear, some minor details that could impact the solution's design are absent, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated as Hard (0.75) due to several factors. First, the scope of code changes involves multiple files in the Polars library, a high-performance DataFrame library written in Rust. The changes affect core components like `DataFrame` operations and execution planning in the `polars-core` and `polars-mem-engine` crates, indicating a need to understand interactions between different parts of the codebase. The modifications are not trivial; they involve optimizing how columns are added or replaced, potentially impacting performance-critical paths. Second, the technical concepts required include a deep understanding of Rust (memory management, ownership), Polars' internal architecture (schema handling, execution engine), and performance optimization techniques (e.g., avoiding unnecessary lookups or copies). Third, while the problem statement does not explicitly mention edge cases, the code changes suggest handling scenarios like schema mismatches or empty DataFrames, which adds complexity to ensure correctness. Finally, optimizing `with_columns()` to match `concat`\u2019s performance without breaking existing functionality requires careful consideration of trade-offs and potential regressions, making this a challenging task. A score of 0.75 reflects the need for significant expertise in Rust and Polars, as well as the complexity of performance optimization in a mature library, though it falls short of Very Hard (0.8-1.0) as it does not appear to require groundbreaking algorithmic innovation or system-level redesign.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Upgrade deprecated gh actions\nThese github actions trigger a deprecation warning and need to be updated:\r\n\r\n- actions/checkout@v2\r\n- actions/setup-python@v2\r\n- actions/upload-artifact@v2\n", "patch": "diff --git a/.github/workflows/deploy_site.yml b/.github/workflows/deploy_site.yml\nindex a5c8a12..107474e 100644\n--- a/.github/workflows/deploy_site.yml\n+++ b/.github/workflows/deploy_site.yml\n@@ -12,7 +12,7 @@ jobs:\n   build_docs:\n     runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Generate HTML docs\n         uses: ammaraskar/sphinx-action@master\n         with:\n@@ -23,7 +23,7 @@ jobs:\n             pip install torch --index-url https://download.pytorch.org/whl/cpu\n             pip install -e \".[doc]\"\n       - name: Upload generated HTML as artifact\n-        uses: actions/upload-artifact@v2\n+        uses: actions/upload-artifact@v4\n         with:\n           name: DocHTML\n           path: doc/_build/html/\n@@ -33,9 +33,9 @@ jobs:\n     needs: build_docs\n     runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Download artifacts\n-        uses: actions/download-artifact@v2\n+        uses: actions/download-artifact@v4\n         with:\n           name: DocHTML\n           path: doc/_build/html/\n@@ -50,7 +50,7 @@ jobs:\n           git add .\n           git commit -m \"Update documentation\" -a || true\n       - name: Push changes\n-        uses: ad-m/github-push-action@v0.6.0\n+        uses: ad-m/github-push-action@v0.8.0\n         with:\n           branch: gh-pages\n           directory: gh-pages\n", "instance_id": "alexisthual__fugw-72", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to upgrade deprecated GitHub Actions to newer versions. It specifies the exact actions that need to be updated (actions/checkout@v2, actions/setup-python@v2, actions/upload-artifact@v2), which aligns with the provided code changes. However, it lacks some critical details, such as the reason for the deprecation, potential compatibility issues with newer versions, or any specific requirements for the upgrade (e.g., whether certain features or behaviors need to be preserved). Additionally, there are no examples or references to documentation for the new versions, which could help in understanding the changes needed. While the goal is clear, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The problem involves straightforward updates to version numbers in a GitHub Actions workflow file, requiring minimal code changes (just updating the version tags from v2 to v4 for most actions, and a minor version bump for another). The scope is limited to a single file (deploy_site.yml), with no impact on the broader codebase or system architecture. The technical concepts involved are basic\u2014understanding GitHub Actions syntax and versioning\u2014which are trivial for anyone with minimal experience in CI/CD workflows. There are no complex algorithms, design patterns, or domain-specific knowledge required. Additionally, the problem statement and code changes do not indicate any specific edge cases or error handling requirements beyond ensuring the new versions are compatible, which is a routine consideration for such updates. Overall, this is a very easy task that requires only basic modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "example code in user documentation for pyscript WebSocket fails with pyodide\n### Checklist\n\n- [X] I added a descriptive title\n- [X] I searched for other issues and couldn't find a solution or duplication\n- [X] I already searched in Google and didn't find any good information or help\n\n### What happened?\n\nThe example code for WebSocket given in the pyscript user documentation works without problems under micropython but gives an error with pyodide.\n\n### What browsers are you seeing the problem on? (if applicable)\n\nChrome\n\n### Console info\n\n```shell\nTraceback (most recent call last):\r\n  File \"/lib/python312.zip/_pyodide/_base.py\", line 596, in eval_code_async\r\n    await CodeRunner(\r\n  File \"/lib/python312.zip/_pyodide/_base.py\", line 410, in run_async\r\n    coroutine = eval(self.code, globals, locals)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<exec>\", line 17, in <module>\r\n  File \"/home/pyodide/pyscript/websocket.py\", line 50, in __setattr__\r\n    self._ws[attr] = value\r\n    ~~~~~~~~^^^^^^\r\nTypeError: 'pyodide.ffi.JsProxy' object does not support item assignment\n```\n\n\n### Additional Context\n\nUsing pyscript 2024.8.2\n", "patch": "diff --git a/pyscript.core/package-lock.json b/pyscript.core/package-lock.json\nindex ea94c12cc4f..859efda3f33 100644\n--- a/pyscript.core/package-lock.json\n+++ b/pyscript.core/package-lock.json\n@@ -1,17 +1,17 @@\n {\n     \"name\": \"@pyscript/core\",\n-    \"version\": \"0.5.5\",\n+    \"version\": \"0.5.6\",\n     \"lockfileVersion\": 3,\n     \"requires\": true,\n     \"packages\": {\n         \"\": {\n             \"name\": \"@pyscript/core\",\n-            \"version\": \"0.5.5\",\n+            \"version\": \"0.5.6\",\n             \"license\": \"APACHE-2.0\",\n             \"dependencies\": {\n                 \"@ungap/with-resolvers\": \"^0.1.0\",\n                 \"basic-devtools\": \"^0.1.6\",\n-                \"polyscript\": \"^0.15.0\",\n+                \"polyscript\": \"^0.15.1\",\n                 \"sticky-module\": \"^0.1.1\",\n                 \"to-json-callback\": \"^0.1.1\",\n                 \"type-checked-collections\": \"^0.1.7\"\n@@ -21,7 +21,7 @@\n                 \"@codemirror/lang-python\": \"^6.1.6\",\n                 \"@codemirror/language\": \"^6.10.2\",\n                 \"@codemirror/state\": \"^6.4.1\",\n-                \"@codemirror/view\": \"^6.30.0\",\n+                \"@codemirror/view\": \"^6.33.0\",\n                 \"@playwright/test\": \"1.45.3\",\n                 \"@rollup/plugin-commonjs\": \"^26.0.1\",\n                 \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n@@ -29,12 +29,12 @@\n                 \"@webreflection/toml-j0.4\": \"^1.1.3\",\n                 \"@xterm/addon-fit\": \"^0.10.0\",\n                 \"@xterm/addon-web-links\": \"^0.11.0\",\n-                \"bun\": \"^1.1.21\",\n+                \"bun\": \"^1.1.26\",\n                 \"chokidar\": \"^3.6.0\",\n                 \"codemirror\": \"^6.0.1\",\n-                \"eslint\": \"^9.8.0\",\n+                \"eslint\": \"^9.9.1\",\n                 \"flatted\": \"^3.3.1\",\n-                \"rollup\": \"^4.20.0\",\n+                \"rollup\": \"^4.21.1\",\n                 \"rollup-plugin-postcss\": \"^4.0.2\",\n                 \"rollup-plugin-string\": \"^3.0.0\",\n                 \"static-handler\": \"^0.4.3\",\n@@ -136,9 +136,9 @@\n             \"license\": \"MIT\"\n         },\n         \"node_modules/@codemirror/view\": {\n-            \"version\": \"6.30.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@codemirror/view/-/view-6.30.0.tgz\",\n-            \"integrity\": \"sha512-96Nmn8OeLh6aONQprIeYk8hGVnEuYpWuxKSkdsODOx9hWPxyuyZGvmvxV/JmLsp+CubMO1PsLaN5TNNgrl0UrQ==\",\n+            \"version\": \"6.33.0\",\n+            \"resolved\": \"https://registry.npmjs.org/@codemirror/view/-/view-6.33.0.tgz\",\n+            \"integrity\": \"sha512-AroaR3BvnjRW8fiZBalAaK+ZzB5usGgI014YKElYZvQdNH5ZIidHlO+cyf/2rWzyBFRkvG6VhiXeAEbC53P2YQ==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n@@ -187,9 +187,9 @@\n             }\n         },\n         \"node_modules/@eslint/config-array\": {\n-            \"version\": \"0.17.1\",\n-            \"resolved\": \"https://registry.npmjs.org/@eslint/config-array/-/config-array-0.17.1.tgz\",\n-            \"integrity\": \"sha512-BlYOpej8AQ8Ev9xVqroV7a02JK3SkBAaN9GfMMH9W6Ch8FlQlkjGw4Ir7+FgYwfirivAf4t+GtzuAxqfukmISA==\",\n+            \"version\": \"0.18.0\",\n+            \"resolved\": \"https://registry.npmjs.org/@eslint/config-array/-/config-array-0.18.0.tgz\",\n+            \"integrity\": \"sha512-fTxvnS1sRMu3+JjXwJG0j/i4RT9u4qJ+lqS/yCGap4lH4zZGzQ7tu+xZqQmcMZq5OBZDL4QRxQzRjkWcGt8IVw==\",\n             \"dev\": true,\n             \"license\": \"Apache-2.0\",\n             \"dependencies\": {\n@@ -226,9 +226,9 @@\n             }\n         },\n         \"node_modules/@eslint/js\": {\n-            \"version\": \"9.8.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@eslint/js/-/js-9.8.0.tgz\",\n-            \"integrity\": \"sha512-MfluB7EUfxXtv3i/++oh89uzAr4PDI4nn201hsp+qaXqsjAWzinlZEHEfPgAX4doIlKvPG/i0A9dpKxOLII8yA==\",\n+            \"version\": \"9.9.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@eslint/js/-/js-9.9.1.tgz\",\n+            \"integrity\": \"sha512-xIDQRsfg5hNBqHz04H1R3scSVwmI+KUbqjsQKHKQ1DAUSaUjYPReZZmS/5PNiKu1fUvzDd6H7DEDKACSEhu+TQ==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"engines\": {\n@@ -462,9 +462,9 @@\n             }\n         },\n         \"node_modules/@oven/bun-darwin-aarch64\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-aarch64/-/bun-darwin-aarch64-1.1.21.tgz\",\n-            \"integrity\": \"sha512-n1hZewJPZg5XcubisWDaKn/wLaldgagAWya3ZuMBuFwsz4PnGTeQ7Wl3aBe7XzW6fNUAd+ZIfvfNYBRNv1R7Rw==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-aarch64/-/bun-darwin-aarch64-1.1.26.tgz\",\n+            \"integrity\": \"sha512-E8/3i0RIvsIWS+kyeIlbwBh+4qB5DsQIfcO6xr4p3t7tEzvRWnrFkJrbJthru/eB1UsVV9PJ/hsxTrp3m3za4A==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -476,9 +476,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-darwin-x64\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64/-/bun-darwin-x64-1.1.21.tgz\",\n-            \"integrity\": \"sha512-Vr7tz6UBrtkJ0UMCQBRhKH/JThWxkZWnGAmcGFf8h3zFgMfCaTmmWzB4PSCad1wu+4GCrmVoEG8P7MY8+TmS7w==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64/-/bun-darwin-x64-1.1.26.tgz\",\n+            \"integrity\": \"sha512-ENRAAGBr2zh0VfETZXqcNPO3ZnnKDX3U6E/oWY+J70uWa9dJqRlRaj1oLB63AGoYJBNdhEcsSmTAk7toCJ+PGQ==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -490,9 +490,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-darwin-x64-baseline\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64-baseline/-/bun-darwin-x64-baseline-1.1.21.tgz\",\n-            \"integrity\": \"sha512-4MhDFYONGIg2MqO56u6H/X9TD3+hbDQpOjlGdl7J0aUiV47b3k7vLn5hENYEjAIBR3g744E23rIw4FQAXakFMw==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64-baseline/-/bun-darwin-x64-baseline-1.1.26.tgz\",\n+            \"integrity\": \"sha512-36HQlQfbrwP//xOS5VFN9AR/iH6BDQo3y8j5282DmRO+h6jylwlg+2+Sfz+1uXDOLDQWCbnNv3Mpl8+Ltso6cQ==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -504,9 +504,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-aarch64\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-aarch64/-/bun-linux-aarch64-1.1.21.tgz\",\n-            \"integrity\": \"sha512-0avxsNle8QOLsDwo1lqO1o2Mv1bLp3RlVr83XNV2yGVnzCwZmupQcI76fcc2e+Y+YU173xCUasMkiIbguS271g==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-aarch64/-/bun-linux-aarch64-1.1.26.tgz\",\n+            \"integrity\": \"sha512-MqE/ClaEMW6B5i5UIYJnHbadWLt6QQQHV3NBlXd78Mhx1OiZY0YmARQmAItPUp9mxIEgGuA2QyrKvgGD3pzWPQ==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -518,9 +518,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-x64\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64/-/bun-linux-x64-1.1.21.tgz\",\n-            \"integrity\": \"sha512-zmps8oWLE2L+9Cn6oQPbcxIWDIjOT1txbYAv9zlcd84I12DXiB++e/PEE8dPe/3powygCpwZM9b7gZfTv9sx0w==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64/-/bun-linux-x64-1.1.26.tgz\",\n+            \"integrity\": \"sha512-sD/ZegJpnBg93qsKsiGnJgTROc68CWONwZpvtL65cBROLBqKb965ofhPUaM5oV8HckfaTDmT37cks59hG+tHvw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -532,9 +532,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-x64-baseline\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64-baseline/-/bun-linux-x64-baseline-1.1.21.tgz\",\n-            \"integrity\": \"sha512-HT+PEWa2PY73gBrNuUHrihsGNOBQKp6s6IzAqHUfmDlIyXYaEvRYUZg6vEqyRRSuNcCC6PiQDHWZP99OT2VMZg==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64-baseline/-/bun-linux-x64-baseline-1.1.26.tgz\",\n+            \"integrity\": \"sha512-jQeSLodwfQu5pG529jYG73VSFq26hdrTspxo9E/1B1WvwKrs2Vtz3w32zv+JWH+gvZqc28A/yK6pAmzQMiscNg==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -546,9 +546,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-windows-x64\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64/-/bun-windows-x64-1.1.21.tgz\",\n-            \"integrity\": \"sha512-p9rjwZPiJJtBafJ7MoJvmqyCA4QxVVpM7QaDx6Lhqua7b+i7dsigog8BgeCxGXAMpSKqoBuAuziqnLh0pcdAYQ==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64/-/bun-windows-x64-1.1.26.tgz\",\n+            \"integrity\": \"sha512-EkyW6JYnZPFxD9XsdEDqFxVCnWnAoyacUAiOEUYAiz8LsnbHLMlOfbdw7KYzvm7UPFoEkUZKD78eSdpg6q6c+Q==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -560,9 +560,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-windows-x64-baseline\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64-baseline/-/bun-windows-x64-baseline-1.1.21.tgz\",\n-            \"integrity\": \"sha512-xwPqSrcdSAJVmCnDlpvEWVHDSf9lmCBIcL5PtM9udrqTJOAVxiyQm0cpXjuv/h6MAZxt7rtt9YqrcK0ixA2xIQ==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64-baseline/-/bun-windows-x64-baseline-1.1.26.tgz\",\n+            \"integrity\": \"sha512-qb593xu9WIKBCHd47z7ZaZTC9h8r4T6qDbBV/XGLhxdZEJb24ePWdhW8WoHxa9hsATio9SByozqwblXb2tJncw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -699,9 +699,9 @@\n             }\n         },\n         \"node_modules/@rollup/rollup-android-arm-eabi\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.20.0.tgz\",\n-            \"integrity\": \"sha512-TSpWzflCc4VGAUJZlPpgAJE1+V60MePDQnBd7PPkpuEmOy8i87aL6tinFGKBFKuEDikYpig72QzdT3QPYIi+oA==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.21.1.tgz\",\n+            \"integrity\": \"sha512-2thheikVEuU7ZxFXubPDOtspKn1x0yqaYQwvALVtEcvFhMifPADBrgRPyHV0TF3b+9BgvgjgagVyvA/UqPZHmg==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -713,9 +713,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-android-arm64\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.20.0.tgz\",\n-            \"integrity\": \"sha512-u00Ro/nok7oGzVuh/FMYfNoGqxU5CPWz1mxV85S2w9LxHR8OoMQBuSk+3BKVIDYgkpeOET5yXkx90OYFc+ytpQ==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.21.1.tgz\",\n+            \"integrity\": \"sha512-t1lLYn4V9WgnIFHXy1d2Di/7gyzBWS8G5pQSXdZqfrdCGTwi1VasRMSS81DTYb+avDs/Zz4A6dzERki5oRYz1g==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -727,9 +727,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-darwin-arm64\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.20.0.tgz\",\n-            \"integrity\": \"sha512-uFVfvzvsdGtlSLuL0ZlvPJvl6ZmrH4CBwLGEFPe7hUmf7htGAN+aXo43R/V6LATyxlKVC/m6UsLb7jbG+LG39Q==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.21.1.tgz\",\n+            \"integrity\": \"sha512-AH/wNWSEEHvs6t4iJ3RANxW5ZCK3fUnmf0gyMxWCesY1AlUj8jY7GC+rQE4wd3gwmZ9XDOpL0kcFnCjtN7FXlA==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -741,9 +741,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-darwin-x64\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.20.0.tgz\",\n-            \"integrity\": \"sha512-xbrMDdlev53vNXexEa6l0LffojxhqDTBeL+VUxuuIXys4x6xyvbKq5XqTXBCEUA8ty8iEJblHvFaWRJTk/icAQ==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.21.1.tgz\",\n+            \"integrity\": \"sha512-dO0BIz/+5ZdkLZrVgQrDdW7m2RkrLwYTh2YMFG9IpBtlC1x1NPNSXkfczhZieOlOLEqgXOFH3wYHB7PmBtf+Bg==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -755,9 +755,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm-gnueabihf\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.20.0.tgz\",\n-            \"integrity\": \"sha512-jMYvxZwGmoHFBTbr12Xc6wOdc2xA5tF5F2q6t7Rcfab68TT0n+r7dgawD4qhPEvasDsVpQi+MgDzj2faOLsZjA==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.21.1.tgz\",\n+            \"integrity\": \"sha512-sWWgdQ1fq+XKrlda8PsMCfut8caFwZBmhYeoehJ05FdI0YZXk6ZyUjWLrIgbR/VgiGycrFKMMgp7eJ69HOF2pQ==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -769,9 +769,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm-musleabihf\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.20.0.tgz\",\n-            \"integrity\": \"sha512-1asSTl4HKuIHIB1GcdFHNNZhxAYEdqML/MW4QmPS4G0ivbEcBr1JKlFLKsIRqjSwOBkdItn3/ZDlyvZ/N6KPlw==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.21.1.tgz\",\n+            \"integrity\": \"sha512-9OIiSuj5EsYQlmwhmFRA0LRO0dRRjdCVZA3hnmZe1rEwRk11Jy3ECGGq3a7RrVEZ0/pCsYWx8jG3IvcrJ6RCew==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -783,9 +783,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm64-gnu\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.20.0.tgz\",\n-            \"integrity\": \"sha512-COBb8Bkx56KldOYJfMf6wKeYJrtJ9vEgBRAOkfw6Ens0tnmzPqvlpjZiLgkhg6cA3DGzCmLmmd319pmHvKWWlQ==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.21.1.tgz\",\n+            \"integrity\": \"sha512-0kuAkRK4MeIUbzQYu63NrJmfoUVicajoRAL1bpwdYIYRcs57iyIV9NLcuyDyDXE2GiZCL4uhKSYAnyWpjZkWow==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -797,9 +797,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm64-musl\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.20.0.tgz\",\n-            \"integrity\": \"sha512-+it+mBSyMslVQa8wSPvBx53fYuZK/oLTu5RJoXogjk6x7Q7sz1GNRsXWjn6SwyJm8E/oMjNVwPhmNdIjwP135Q==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.21.1.tgz\",\n+            \"integrity\": \"sha512-/6dYC9fZtfEY0vozpc5bx1RP4VrtEOhNQGb0HwvYNwXD1BBbwQ5cKIbUVVU7G2d5WRE90NfB922elN8ASXAJEA==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -811,9 +811,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-powerpc64le-gnu\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-powerpc64le-gnu/-/rollup-linux-powerpc64le-gnu-4.20.0.tgz\",\n-            \"integrity\": \"sha512-yAMvqhPfGKsAxHN8I4+jE0CpLWD8cv4z7CK7BMmhjDuz606Q2tFKkWRY8bHR9JQXYcoLfopo5TTqzxgPUjUMfw==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-powerpc64le-gnu/-/rollup-linux-powerpc64le-gnu-4.21.1.tgz\",\n+            \"integrity\": \"sha512-ltUWy+sHeAh3YZ91NUsV4Xg3uBXAlscQe8ZOXRCVAKLsivGuJsrkawYPUEyCV3DYa9urgJugMLn8Z3Z/6CeyRQ==\",\n             \"cpu\": [\n                 \"ppc64\"\n             ],\n@@ -825,9 +825,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-riscv64-gnu\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.20.0.tgz\",\n-            \"integrity\": \"sha512-qmuxFpfmi/2SUkAw95TtNq/w/I7Gpjurx609OOOV7U4vhvUhBcftcmXwl3rqAek+ADBwSjIC4IVNLiszoj3dPA==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.21.1.tgz\",\n+            \"integrity\": \"sha512-BggMndzI7Tlv4/abrgLwa/dxNEMn2gC61DCLrTzw8LkpSKel4o+O+gtjbnkevZ18SKkeN3ihRGPuBxjaetWzWg==\",\n             \"cpu\": [\n                 \"riscv64\"\n             ],\n@@ -839,9 +839,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-s390x-gnu\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.20.0.tgz\",\n-            \"integrity\": \"sha512-I0BtGXddHSHjV1mqTNkgUZLnS3WtsqebAXv11D5BZE/gfw5KoyXSAXVqyJximQXNvNzUo4GKlCK/dIwXlz+jlg==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.21.1.tgz\",\n+            \"integrity\": \"sha512-z/9rtlGd/OMv+gb1mNSjElasMf9yXusAxnRDrBaYB+eS1shFm6/4/xDH1SAISO5729fFKUkJ88TkGPRUh8WSAA==\",\n             \"cpu\": [\n                 \"s390x\"\n             ],\n@@ -853,9 +853,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-x64-gnu\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.20.0.tgz\",\n-            \"integrity\": \"sha512-y+eoL2I3iphUg9tN9GB6ku1FA8kOfmF4oUEWhztDJ4KXJy1agk/9+pejOuZkNFhRwHAOxMsBPLbXPd6mJiCwew==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.21.1.tgz\",\n+            \"integrity\": \"sha512-kXQVcWqDcDKw0S2E0TmhlTLlUgAmMVqPrJZR+KpH/1ZaZhLSl23GZpQVmawBQGVhyP5WXIsIQ/zqbDBBYmxm5w==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -867,9 +867,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-x64-musl\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.20.0.tgz\",\n-            \"integrity\": \"sha512-hM3nhW40kBNYUkZb/r9k2FKK+/MnKglX7UYd4ZUy5DJs8/sMsIbqWK2piZtVGE3kcXVNj3B2IrUYROJMMCikNg==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.21.1.tgz\",\n+            \"integrity\": \"sha512-CbFv/WMQsSdl+bpX6rVbzR4kAjSSBuDgCqb1l4J68UYsQNalz5wOqLGYj4ZI0thGpyX5kc+LLZ9CL+kpqDovZA==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -881,9 +881,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-arm64-msvc\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.20.0.tgz\",\n-            \"integrity\": \"sha512-psegMvP+Ik/Bg7QRJbv8w8PAytPA7Uo8fpFjXyCRHWm6Nt42L+JtoqH8eDQ5hRP7/XW2UiIriy1Z46jf0Oa1kA==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.21.1.tgz\",\n+            \"integrity\": \"sha512-3Q3brDgA86gHXWHklrwdREKIrIbxC0ZgU8lwpj0eEKGBQH+31uPqr0P2v11pn0tSIxHvcdOWxa4j+YvLNx1i6g==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -895,9 +895,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-ia32-msvc\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.20.0.tgz\",\n-            \"integrity\": \"sha512-GabekH3w4lgAJpVxkk7hUzUf2hICSQO0a/BLFA11/RMxQT92MabKAqyubzDZmMOC/hcJNlc+rrypzNzYl4Dx7A==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.21.1.tgz\",\n+            \"integrity\": \"sha512-tNg+jJcKR3Uwe4L0/wY3Ro0H+u3nrb04+tcq1GSYzBEmKLeOQF2emk1whxlzNqb6MMrQ2JOcQEpuuiPLyRcSIw==\",\n             \"cpu\": [\n                 \"ia32\"\n             ],\n@@ -909,9 +909,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-x64-msvc\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.20.0.tgz\",\n-            \"integrity\": \"sha512-aJ1EJSuTdGnM6qbVC4B5DSmozPTqIag9fSzXRNNo+humQLG89XpPgdt16Ia56ORD7s+H8Pmyx44uczDQ0yDzpg==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.21.1.tgz\",\n+            \"integrity\": \"sha512-xGiIH95H1zU7naUyTKEyOA/I0aexNMUdO9qRv0bLKN3qu25bBdrxZHqA3PTJ24YNN/GdMzG4xkDcd/GvjuhfLg==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -1202,9 +1202,9 @@\n             }\n         },\n         \"node_modules/bun\": {\n-            \"version\": \"1.1.21\",\n-            \"resolved\": \"https://registry.npmjs.org/bun/-/bun-1.1.21.tgz\",\n-            \"integrity\": \"sha512-mvqYEvafGskIVTjlftbKvsXtyR6z/SQnhJsVw0xCU46pc56oX1sAGvaemWKOy/sy/gGMHcgLE0KUidDQQzqXWQ==\",\n+            \"version\": \"1.1.26\",\n+            \"resolved\": \"https://registry.npmjs.org/bun/-/bun-1.1.26.tgz\",\n+            \"integrity\": \"sha512-dWSewAqE7sVbYmflJxgG47dW4vmsbar7VAnQ4ao45y3ulr3n7CwdsMLFnzd28jhPRtF+rsaVK2y4OLIkP3OD4A==\",\n             \"cpu\": [\n                 \"arm64\",\n                 \"x64\"\n@@ -1222,14 +1222,14 @@\n                 \"bunx\": \"bin/bun.exe\"\n             },\n             \"optionalDependencies\": {\n-                \"@oven/bun-darwin-aarch64\": \"1.1.21\",\n-                \"@oven/bun-darwin-x64\": \"1.1.21\",\n-                \"@oven/bun-darwin-x64-baseline\": \"1.1.21\",\n-                \"@oven/bun-linux-aarch64\": \"1.1.21\",\n-                \"@oven/bun-linux-x64\": \"1.1.21\",\n-                \"@oven/bun-linux-x64-baseline\": \"1.1.21\",\n-                \"@oven/bun-windows-x64\": \"1.1.21\",\n-                \"@oven/bun-windows-x64-baseline\": \"1.1.21\"\n+                \"@oven/bun-darwin-aarch64\": \"1.1.26\",\n+                \"@oven/bun-darwin-x64\": \"1.1.26\",\n+                \"@oven/bun-darwin-x64-baseline\": \"1.1.26\",\n+                \"@oven/bun-linux-aarch64\": \"1.1.26\",\n+                \"@oven/bun-linux-x64\": \"1.1.26\",\n+                \"@oven/bun-linux-x64-baseline\": \"1.1.26\",\n+                \"@oven/bun-windows-x64\": \"1.1.26\",\n+                \"@oven/bun-windows-x64-baseline\": \"1.1.26\"\n             }\n         },\n         \"node_modules/callsites\": {\n@@ -1344,9 +1344,9 @@\n             }\n         },\n         \"node_modules/coincident\": {\n-            \"version\": \"2.0.6\",\n-            \"resolved\": \"https://registry.npmjs.org/coincident/-/coincident-2.0.6.tgz\",\n-            \"integrity\": \"sha512-NMTtehZKCKfYj6X4EAv9e+Rpjg84/gyM/O2M1culPcMZuCKrUnJql99KqDEE7m7V1W16Ou3vmLj8MzAJQB4Gcg==\",\n+            \"version\": \"2.0.7\",\n+            \"resolved\": \"https://registry.npmjs.org/coincident/-/coincident-2.0.7.tgz\",\n+            \"integrity\": \"sha512-kfQTQFShg8znBkorQs8VfDhpHbUsFRauufeoabssPfpp6TUQx5hgNWfTgPxO1szaZb/V/9FkxRl3BqN2t0SAKg==\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"gc-hook\": \"^0.4.1\",\n@@ -1748,17 +1748,17 @@\n             }\n         },\n         \"node_modules/eslint\": {\n-            \"version\": \"9.8.0\",\n-            \"resolved\": \"https://registry.npmjs.org/eslint/-/eslint-9.8.0.tgz\",\n-            \"integrity\": \"sha512-K8qnZ/QJzT2dLKdZJVX6W4XOwBzutMYmt0lqUS+JdXgd+HTYFlonFgkJ8s44d/zMPPCnOOk0kMWCApCPhiOy9A==\",\n+            \"version\": \"9.9.1\",\n+            \"resolved\": \"https://registry.npmjs.org/eslint/-/eslint-9.9.1.tgz\",\n+            \"integrity\": \"sha512-dHvhrbfr4xFQ9/dq+jcVneZMyRYLjggWjk6RVsIiHsP8Rz6yZ8LvZ//iU4TrZF+SXWG+JkNF2OyiZRvzgRDqMg==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"@eslint-community/eslint-utils\": \"^4.2.0\",\n                 \"@eslint-community/regexpp\": \"^4.11.0\",\n-                \"@eslint/config-array\": \"^0.17.1\",\n+                \"@eslint/config-array\": \"^0.18.0\",\n                 \"@eslint/eslintrc\": \"^3.1.0\",\n-                \"@eslint/js\": \"9.8.0\",\n+                \"@eslint/js\": \"9.9.1\",\n                 \"@humanwhocodes/module-importer\": \"^1.0.1\",\n                 \"@humanwhocodes/retry\": \"^0.3.0\",\n                 \"@nodelib/fs.walk\": \"^1.2.8\",\n@@ -1797,6 +1797,14 @@\n             },\n             \"funding\": {\n                 \"url\": \"https://eslint.org/donate\"\n+            },\n+            \"peerDependencies\": {\n+                \"jiti\": \"*\"\n+            },\n+            \"peerDependenciesMeta\": {\n+                \"jiti\": {\n+                    \"optional\": true\n+                }\n             }\n         },\n         \"node_modules/eslint-scope\": {\n@@ -2407,9 +2415,9 @@\n             }\n         },\n         \"node_modules/js-proxy\": {\n-            \"version\": \"0.4.3\",\n-            \"resolved\": \"https://registry.npmjs.org/js-proxy/-/js-proxy-0.4.3.tgz\",\n-            \"integrity\": \"sha512-zSkaj+H6V8YiNF/CnHMYDarE+uNFYWEgW9R7xXjSPUchpCdKipS4MEW4HLJ0FA6oQeI7y2nUwx3g0WpRXZTi0w==\",\n+            \"version\": \"0.4.4\",\n+            \"resolved\": \"https://registry.npmjs.org/js-proxy/-/js-proxy-0.4.4.tgz\",\n+            \"integrity\": \"sha512-f2Av1PIsa2Wtgb0VfDehK7aglp8eby2rLsD5juLWcYPlJsWp0WUxyK9r18hjzV9N0ejA76x6kw1v/YlJvKwycw==\",\n             \"license\": \"MIT\",\n             \"dependencies\": {\n                 \"gc-hook\": \"^0.3.1\",\n@@ -2932,9 +2940,9 @@\n             }\n         },\n         \"node_modules/polyscript\": {\n-            \"version\": \"0.15.0\",\n-            \"resolved\": \"https://registry.npmjs.org/polyscript/-/polyscript-0.15.0.tgz\",\n-            \"integrity\": \"sha512-HCxHqr3Yyoudu1FvxzzCFAwHj51OeS3duga3eGXPsogTB0s79ACw9V6dsiwId02F7vVrcHLVyWcl0od+FtAK/A==\",\n+            \"version\": \"0.15.1\",\n+            \"resolved\": \"https://registry.npmjs.org/polyscript/-/polyscript-0.15.1.tgz\",\n+            \"integrity\": \"sha512-4vbltiMwQFxN8QMtW+Kgz+dYgoU1ba46B8VNYliv2S2WWppJUKQ4QXaLi0NOtX+A6lCLXSU2/sZJAK40KbUpXA==\",\n             \"license\": \"APACHE-2.0\",\n             \"dependencies\": {\n                 \"@ungap/structured-clone\": \"^1.2.0\",\n@@ -2943,7 +2951,7 @@\n                 \"@webreflection/idb-map\": \"^0.3.1\",\n                 \"basic-devtools\": \"^0.1.6\",\n                 \"codedent\": \"^0.1.2\",\n-                \"coincident\": \"^2.0.6\",\n+                \"coincident\": \"^2.0.7\",\n                 \"gc-hook\": \"^0.4.1\",\n                 \"html-escaper\": \"^3.0.3\",\n                 \"proxy-target\": \"^3.0.2\",\n@@ -3667,9 +3675,9 @@\n             }\n         },\n         \"node_modules/rollup\": {\n-            \"version\": \"4.20.0\",\n-            \"resolved\": \"https://registry.npmjs.org/rollup/-/rollup-4.20.0.tgz\",\n-            \"integrity\": \"sha512-6rbWBChcnSGzIlXeIdNIZTopKYad8ZG8ajhl78lGRLsI2rX8IkaotQhVas2Ma+GPxJav19wrSzvRvuiv0YKzWw==\",\n+            \"version\": \"4.21.1\",\n+            \"resolved\": \"https://registry.npmjs.org/rollup/-/rollup-4.21.1.tgz\",\n+            \"integrity\": \"sha512-ZnYyKvscThhgd3M5+Qt3pmhO4jIRR5RGzaSovB6Q7rGNrK5cUncrtLmcTTJVSdcKXyZjW8X8MB0JMSuH9bcAJg==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n@@ -3683,22 +3691,22 @@\n                 \"npm\": \">=8.0.0\"\n             },\n             \"optionalDependencies\": {\n-                \"@rollup/rollup-android-arm-eabi\": \"4.20.0\",\n-                \"@rollup/rollup-android-arm64\": \"4.20.0\",\n-                \"@rollup/rollup-darwin-arm64\": \"4.20.0\",\n-                \"@rollup/rollup-darwin-x64\": \"4.20.0\",\n-                \"@rollup/rollup-linux-arm-gnueabihf\": \"4.20.0\",\n-                \"@rollup/rollup-linux-arm-musleabihf\": \"4.20.0\",\n-                \"@rollup/rollup-linux-arm64-gnu\": \"4.20.0\",\n-                \"@rollup/rollup-linux-arm64-musl\": \"4.20.0\",\n-                \"@rollup/rollup-linux-powerpc64le-gnu\": \"4.20.0\",\n-                \"@rollup/rollup-linux-riscv64-gnu\": \"4.20.0\",\n-                \"@rollup/rollup-linux-s390x-gnu\": \"4.20.0\",\n-                \"@rollup/rollup-linux-x64-gnu\": \"4.20.0\",\n-                \"@rollup/rollup-linux-x64-musl\": \"4.20.0\",\n-                \"@rollup/rollup-win32-arm64-msvc\": \"4.20.0\",\n-                \"@rollup/rollup-win32-ia32-msvc\": \"4.20.0\",\n-                \"@rollup/rollup-win32-x64-msvc\": \"4.20.0\",\n+                \"@rollup/rollup-android-arm-eabi\": \"4.21.1\",\n+                \"@rollup/rollup-android-arm64\": \"4.21.1\",\n+                \"@rollup/rollup-darwin-arm64\": \"4.21.1\",\n+                \"@rollup/rollup-darwin-x64\": \"4.21.1\",\n+                \"@rollup/rollup-linux-arm-gnueabihf\": \"4.21.1\",\n+                \"@rollup/rollup-linux-arm-musleabihf\": \"4.21.1\",\n+                \"@rollup/rollup-linux-arm64-gnu\": \"4.21.1\",\n+                \"@rollup/rollup-linux-arm64-musl\": \"4.21.1\",\n+                \"@rollup/rollup-linux-powerpc64le-gnu\": \"4.21.1\",\n+                \"@rollup/rollup-linux-riscv64-gnu\": \"4.21.1\",\n+                \"@rollup/rollup-linux-s390x-gnu\": \"4.21.1\",\n+                \"@rollup/rollup-linux-x64-gnu\": \"4.21.1\",\n+                \"@rollup/rollup-linux-x64-musl\": \"4.21.1\",\n+                \"@rollup/rollup-win32-arm64-msvc\": \"4.21.1\",\n+                \"@rollup/rollup-win32-ia32-msvc\": \"4.21.1\",\n+                \"@rollup/rollup-win32-x64-msvc\": \"4.21.1\",\n                 \"fsevents\": \"~2.3.2\"\n             }\n         },\ndiff --git a/pyscript.core/package.json b/pyscript.core/package.json\nindex c2832c9b3d5..fb7913b5417 100644\n--- a/pyscript.core/package.json\n+++ b/pyscript.core/package.json\n@@ -1,6 +1,6 @@\n {\n     \"name\": \"@pyscript/core\",\n-    \"version\": \"0.5.5\",\n+    \"version\": \"0.5.6\",\n     \"type\": \"module\",\n     \"description\": \"PyScript\",\n     \"module\": \"./index.js\",\n@@ -45,7 +45,7 @@\n     \"dependencies\": {\n         \"@ungap/with-resolvers\": \"^0.1.0\",\n         \"basic-devtools\": \"^0.1.6\",\n-        \"polyscript\": \"^0.15.0\",\n+        \"polyscript\": \"^0.15.1\",\n         \"sticky-module\": \"^0.1.1\",\n         \"to-json-callback\": \"^0.1.1\",\n         \"type-checked-collections\": \"^0.1.7\"\n@@ -55,7 +55,7 @@\n         \"@codemirror/lang-python\": \"^6.1.6\",\n         \"@codemirror/language\": \"^6.10.2\",\n         \"@codemirror/state\": \"^6.4.1\",\n-        \"@codemirror/view\": \"^6.30.0\",\n+        \"@codemirror/view\": \"^6.33.0\",\n         \"@playwright/test\": \"1.45.3\",\n         \"@rollup/plugin-commonjs\": \"^26.0.1\",\n         \"@rollup/plugin-node-resolve\": \"^15.2.3\",\n@@ -63,12 +63,12 @@\n         \"@webreflection/toml-j0.4\": \"^1.1.3\",\n         \"@xterm/addon-fit\": \"^0.10.0\",\n         \"@xterm/addon-web-links\": \"^0.11.0\",\n-        \"bun\": \"^1.1.21\",\n+        \"bun\": \"^1.1.26\",\n         \"chokidar\": \"^3.6.0\",\n         \"codemirror\": \"^6.0.1\",\n-        \"eslint\": \"^9.8.0\",\n+        \"eslint\": \"^9.9.1\",\n         \"flatted\": \"^3.3.1\",\n-        \"rollup\": \"^4.20.0\",\n+        \"rollup\": \"^4.21.1\",\n         \"rollup-plugin-postcss\": \"^4.0.2\",\n         \"rollup-plugin-string\": \"^3.0.0\",\n         \"static-handler\": \"^0.4.3\",\ndiff --git a/pyscript.core/src/stdlib/pyscript/websocket.py b/pyscript.core/src/stdlib/pyscript/websocket.py\nindex 7ce46ddd049..8e6b759a0d3 100644\n--- a/pyscript.core/src/stdlib/pyscript/websocket.py\n+++ b/pyscript.core/src/stdlib/pyscript/websocket.py\n@@ -1,4 +1,5 @@\n import js\n+from pyscript.ffi import create_proxy\n from pyscript.util import as_bytearray\n \n code = \"code\"\n@@ -38,7 +39,8 @@ def __init__(self, **kw):\n \n         for t in [\"onclose\", \"onerror\", \"onmessage\", \"onopen\"]:\n             if t in kw:\n-                socket[t] = kw[t]\n+                # Pyodide fails at setting socket[t] directly\n+                setattr(socket, t, create_proxy(kw[t]))\n \n     def __getattr__(self, attr):\n         return getattr(self._ws, attr)\n", "instance_id": "pyscript__pyscript-2148", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the WebSocket example code in the PyScript user documentation fails under Pyodide with a specific error related to item assignment on a JsProxy object. It provides relevant context such as the PyScript version, the browser used, and a detailed error traceback, which helps in understanding the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the expected behavior of the WebSocket example or provide a link to the exact documentation or code snippet causing the issue. Additionally, it lacks mention of specific edge cases or constraints that might need to be considered for a complete fix. Despite these minor gaps, the intent and core issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4). The issue involves a specific error in Pyodide related to setting attributes on a JavaScript proxy object, which is resolved by using `create_proxy` from `pyscript.ffi` to wrap the callback functions before assignment. The code changes are minimal and localized to a single file (`websocket.py`), specifically modifying how event handlers are set on the WebSocket object. The solution requires understanding a specific Pyodide limitation (JsProxy objects not supporting direct item assignment) and familiarity with PyScript's FFI utilities, but it does not involve complex logic, multiple modules, or deep architectural changes. The provided diff also includes version bumps in `package.json` and `package-lock.json`, but these are routine dependency updates and do not add significant complexity to the core fix. Edge case handling or error conditions are not explicitly mentioned or required in the changes, further keeping the difficulty low. Overall, this is a straightforward bug fix that requires moderate understanding of Pyodide and PyScript internals, justifying a score of 0.30.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Might Need Disscusion] Refine Time Limit\n# Time Limit Implementation Strategies\r\n\r\nissue #130 \r\n\r\n## Core Principle\r\nThe core principle is that when a user specifies a time_limit, we should complete both training and prediction with the best available model, even if the time limit is reached. But as prediction time is hard to estimate, we have several options:\r\n\r\n### Option 1: Excluding Prediction Time from Time Limit (Current)\r\n- **Approach**: time_limit applies only to training (end_of_fit - start_of_init), excluding prediction time\r\n- **Advantages**:\r\n  - Simple to implement\r\n  - Maximizes training time utilization\r\n- **Disadvantages**:\r\n  - Will always exceed specified time_limit by the prediction duration\r\n  - Unpredictable total execution time from user perspective\r\n\r\n### Option 2: Percentage-Based Allocation\r\n- **Approach**: Reserve a percentage (e.g., 10%) of time_limit for prediction\r\n- **Advantages**:\r\n  - Scales proportionally with total time\r\n  - Least likely to exceed time_limit\r\n- **Disadvantages**:\r\n  - May unnecessarily restrict training time for long time_limits\r\n  - Example: With 12-hour time_limit, reserving 1.2 hours for prediction is excessive\r\n\r\n### Option 3: Fixed Time Reservation\r\n- **Approach**: Reserve a fixed duration (e.g., 5 minutes) for prediction\r\n- **Advantages**:\r\n  - Predictable prediction time allocation\r\n  - Works well for medium-sized datasets\r\n- **Disadvantages**:\r\n  - Requires minimum time_limit threshold\r\n  - Inefficient for small datasets\r\n  - May be insufficient for large datasets\r\n\r\n### Option 4: Hybrid Approach (Percentage with Cap)\r\n- **Approach**: Reserve percentage of time_limit (e.g., 10%) with maximum cap (e.g., 20 minutes)\r\n- **Advantages**:\r\n  - Balances scaling and practicality\r\n  - Prevents excessive prediction time reservation\r\n- **Disadvantages**:\r\n  - Introduces multiple hardcoded parameters\r\n  - More complex to maintain and explain\r\n\r\nBy submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.\r\n\n[Might Need Disscusion] Refine Time Limit\n# Time Limit Implementation Strategies\r\n\r\nissue #130 \r\n\r\n## Core Principle\r\nThe core principle is that when a user specifies a time_limit, we should complete both training and prediction with the best available model, even if the time limit is reached. But as prediction time is hard to estimate, we have several options:\r\n\r\n### Option 1: Excluding Prediction Time from Time Limit (Current)\r\n- **Approach**: time_limit applies only to training (end_of_fit - start_of_init), excluding prediction time\r\n- **Advantages**:\r\n  - Simple to implement\r\n  - Maximizes training time utilization\r\n- **Disadvantages**:\r\n  - Will always exceed specified time_limit by the prediction duration\r\n  - Unpredictable total execution time from user perspective\r\n\r\n### Option 2: Percentage-Based Allocation\r\n- **Approach**: Reserve a percentage (e.g., 10%) of time_limit for prediction\r\n- **Advantages**:\r\n  - Scales proportionally with total time\r\n  - Least likely to exceed time_limit\r\n- **Disadvantages**:\r\n  - May unnecessarily restrict training time for long time_limits\r\n  - Example: With 12-hour time_limit, reserving 1.2 hours for prediction is excessive\r\n\r\n### Option 3: Fixed Time Reservation\r\n- **Approach**: Reserve a fixed duration (e.g., 5 minutes) for prediction\r\n- **Advantages**:\r\n  - Predictable prediction time allocation\r\n  - Works well for medium-sized datasets\r\n- **Disadvantages**:\r\n  - Requires minimum time_limit threshold\r\n  - Inefficient for small datasets\r\n  - May be insufficient for large datasets\r\n\r\n### Option 4: Hybrid Approach (Percentage with Cap)\r\n- **Approach**: Reserve percentage of time_limit (e.g., 10%) with maximum cap (e.g., 20 minutes)\r\n- **Advantages**:\r\n  - Balances scaling and practicality\r\n  - Prevents excessive prediction time reservation\r\n- **Disadvantages**:\r\n  - Introduces multiple hardcoded parameters\r\n  - More complex to maintain and explain\r\n\r\nBy submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.\r\n\n", "patch": "diff --git a/README.md b/README.md\nindex 0a2d569..c44133c 100644\n--- a/README.md\n+++ b/README.md\n@@ -111,9 +111,9 @@ the `config_overrides` parameter with format `\"key1=value1, key2.nested=value2\"`\n Here are some example commands on using configuration overrides:\n \n ```bash\n-aga run toy_data --config_overrides \"feature_transformers.enabled_models=None, autogluon.predictor_fit_kwargs.time_limit=3600\"\n+aga run toy_data --config_overrides \"feature_transformers.enabled_models=None, time_limit=3600\"\n \n # OR\n \n-aga run toy_data --config_overrides \"feature_transformers.enabled_models=None\" --config_overrides \"autogluon.predictor_fit_kwargs.time_limit=3600\"\n+aga run toy_data --config_overrides \"feature_transformers.enabled_models=None\" --config_overrides \"time_limit=3600\"\n ```\ndiff --git a/src/autogluon/assistant/ui/pages/task.py b/src/autogluon/assistant/ui/pages/task.py\nindex 2662272..671b967 100644\n--- a/src/autogluon/assistant/ui/pages/task.py\n+++ b/src/autogluon/assistant/ui/pages/task.py\n@@ -44,9 +44,7 @@ def update_config_overrides():\n     \"\"\"\n     config_overrides = []\n     if st.session_state.time_limit:\n-        config_overrides.append(\n-            f\"autogluon.predictor_fit_kwargs.time_limit={TIME_LIMIT_MAPPING[st.session_state.time_limit]}\"\n-        )\n+        config_overrides.append(f\"time_limit={TIME_LIMIT_MAPPING[st.session_state.time_limit]}\")\n     if st.session_state.llm:\n         config_overrides.append(f\"llm.model={LLM_MAPPING[st.session_state.llm]}\")\n         config_overrides.append(f\"llm.provider={PROVIDER_MAPPING[st.session_state.llm]}\")\n", "instance_id": "autogluon__autogluon-assistant-148", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in outlining the core principle of managing a time limit for training and prediction in a machine learning context. It provides a detailed comparison of four different strategies (excluding prediction time, percentage-based allocation, fixed time reservation, and hybrid approach) with their respective advantages and disadvantages. This helps in understanding the trade-offs and the intent behind refining the time limit implementation. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not specify which option (or combination of options) is to be implemented or prioritized in the current pull request. Additionally, there are no explicit mentions of input/output formats, specific constraints (e.g., minimum or maximum time limits), or edge cases to handle (e.g., extremely short time limits or very large datasets). The statement also lacks examples of expected behavior in different scenarios, which would aid in fully grasping the requirements. Thus, while the problem is valid and mostly clear, these minor gaps result in a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated as Easy (0.25) based on the provided code changes and the scope of the task. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are minimal and localized to two files: `README.md` and `task.py`. The modifications involve updating configuration override strings to simplify the way `time_limit` is specified (removing nested dictionary access). This is a straightforward change that does not impact the broader architecture of the system or require understanding complex interactions between modules. The amount of code change is small, focusing on string replacements.\n\n2. **Number of Technical Concepts**: The changes require basic knowledge of string manipulation and configuration handling in Python. There are no advanced language features, algorithms, design patterns, or domain-specific knowledge needed beyond understanding how configuration overrides work in the context of the application (likely an AutoML system like AutoGluon). The concepts involved are simple and accessible to developers with basic experience.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and code changes do not explicitly address edge cases or error handling related to the time limit implementation. The code modifications themselves do not introduce new logic that would require handling edge cases (e.g., invalid time limits or negative values). Any complexity around edge cases seems to be outside the scope of this specific pull request, as the focus is on updating how the configuration is passed rather than implementing the time limit logic itself.\n\n4. **Overall Complexity**: While the problem statement discusses a conceptually interesting challenge (balancing training and prediction time under a time limit), the actual code changes in this pull request are trivial and do not address the core logic of implementing any of the discussed strategies. The task is limited to updating documentation and configuration syntax, which is a surface-level modification.\n\nGiven these points, the task is rated as Easy (0.25). It requires minimal understanding of the codebase, involves simple modifications, and does not touch on the deeper complexities hinted at in the problem statement (e.g., implementing the time limit strategies). If the pull request were to include the actual implementation of one of the proposed options, the difficulty score would likely increase significantly due to the need for deeper system understanding and handling of edge cases.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug] ci_cpu docker image build fail\n### Expected behavior\r\nThe docker image (ci_cpu) builds successfully.\r\n\r\n### Actual behavior\r\n\r\nfailed with the following error\r\n```sh\r\n40.25 [ 69%] Built target nnpack_reference_layers\r\n40.88 [ 71%] Generating src/x86_64-fma/2d-fourier-16x16.py.o\r\n43.07 [ 72%] Generating src/x86_64-fma/2d-winograd-8x8-3x3.py.o\r\n43.44 [ 73%] Generating src/x86_64-fma/blas/s8gemm.py.o\r\n43.56 [ 74%] Generating src/x86_64-fma/blas/c8gemm.py.o\r\n43.76 [ 75%] Generating src/x86_64-fma/blas/s4c6gemm.py.o\r\n43.94 [ 77%] Generating src/x86_64-fma/blas/conv1x1.py.o\r\n44.04 [ 78%] Generating src/x86_64-fma/blas/sgemm.py.o\r\n44.18 [ 79%] Generating src/x86_64-fma/max-pooling.py.o\r\n44.25 [ 80%] Generating src/x86_64-fma/relu.py.o\r\n44.30 [ 81%] Generating src/x86_64-fma/softmax.py.o\r\n44.57 [ 83%] Generating src/x86_64-fma/blas/sdotxf.py.o\r\n44.78 [ 84%] Generating src/x86_64-fma/blas/shdotxf.py.o\r\n44.81 Traceback (most recent call last):\r\n44.81   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n44.81     return _run_code(code, main_globals, None,\r\n44.81   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n44.81     exec(code, run_globals)\r\n44.81   File \"/NNPACK/deps/peachpy/peachpy/x86_64/__main__.py\", line 282, in <module>\r\n44.81     main()\r\n44.81   File \"/NNPACK/deps/peachpy/peachpy/x86_64/__main__.py\", line 269, in main\r\n44.81     execute_script(writers, options.input[0])\r\n44.81   File \"/NNPACK/deps/peachpy/peachpy/x86_64/__main__.py\", line 200, in execute_script\r\n44.81     execute_script(writers, source_filename)\r\n44.81   File \"/NNPACK/deps/peachpy/peachpy/x86_64/__main__.py\", line 200, in execute_script\r\n44.81     execute_script(writers, source_filename)\r\n44.81   File \"/NNPACK/deps/peachpy/peachpy/x86_64/__main__.py\", line 204, in execute_script\r\n44.81     exec(code, globals())\r\n44.81   File \"/NNPACK/src/x86_64-fma/blas/shdotxf.py\", line 4, in <module>\r\n44.81     from fp16.avx import fp16_alt_xmm_to_fp32_xmm\r\n44.81 ModuleNotFoundError: No module named 'fp16.avx'\r\n44.81 make[2]: *** [CMakeFiles/nnpack.dir/build.make:188: src/x86_64-fma/blas/shdotxf.py.o] Error 1\r\n44.81 make[1]: *** [CMakeFiles/Makefile2:175: CMakeFiles/nnpack.dir/all] Error 2\r\n44.81 make: *** [Makefile:136: all] Error 2\r\n------\r\nDockerfile.ci_cpu:82\r\n--------------------\r\n  80 |     # NNPACK deps\r\n  81 |     COPY install/ubuntu_install_nnpack.sh /install/ubuntu_install_nnpack.sh\r\n  82 | >>> RUN bash /install/ubuntu_install_nnpack.sh\r\n  83 |\r\n  84 |     # ANTLR deps\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c bash /install/ubuntu_install_nnpack.sh\" did not complete successfully: exit code: 2\r\nERROR: docker build failed.\r\n```\r\n### Environment\r\nUbuntu 22.04\r\nLatest TVM code at 1f4c568bdd6f5392466a05921b2ff7ef600010fe\r\n\r\n### Steps to reproduce\r\n```sh\r\n./docker/build.sh tvm.ci_cpu\r\n\r\n\r\n### Triage\r\n\r\nPlease refer to the list of label tags [here](https://github.com/apache/tvm/wiki/Issue-Triage-Labels) to find the relevant tags and add them below in a bullet format (example below).\r\n\r\n* needs-triage\r\n\n", "patch": "diff --git a/cmake/modules/contrib/TFLite.cmake b/cmake/modules/contrib/TFLite.cmake\nindex b8d6a0daff19..255dc5fde780 100644\n--- a/cmake/modules/contrib/TFLite.cmake\n+++ b/cmake/modules/contrib/TFLite.cmake\n@@ -39,6 +39,10 @@ if(NOT USE_TFLITE STREQUAL \"OFF\")\n   endif()\n   find_library(TFLITE_CONTRIB_LIB libtensorflow-lite.a ${USE_TFLITE})\n   file(GLOB_RECURSE TFLITE_DEPS \"${USE_TFLITE}/*.a\")\n+  # the order of the next libs are important for correct build\n+  list(REMOVE_ITEM TFLITE_DEPS \"${USE_TFLITE}/_deps/clog-build/libclog.a\" \"${USE_TFLITE}/_deps/cpuinfo-build/libcpuinfo.a\")\n+  list(APPEND TFLITE_DEPS \"${USE_TFLITE}/_deps/cpuinfo-build/libcpuinfo.a\")\n+  list(APPEND TFLITE_DEPS \"${USE_TFLITE}/_deps/clog-build/libclog.a\")\n \n   list(APPEND TVM_RUNTIME_LINKER_LIBS ${TFLITE_CONTRIB_LIB})\n   list(APPEND TVM_RUNTIME_LINKER_LIBS ${TFLITE_DEPS})\ndiff --git a/docker/Dockerfile.ci_adreno b/docker/Dockerfile.ci_adreno\nindex 961977c54286..30e095b27aac 100644\n--- a/docker/Dockerfile.ci_adreno\n+++ b/docker/Dockerfile.ci_adreno\n@@ -20,11 +20,6 @@ FROM tlcpack/ci-gpu\n \n COPY utils/apt-install-and-clear.sh /usr/local/bin/apt-install-and-clear\n \n-# Android SDK\n-COPY install/ubuntu_install_androidsdk.sh /install/ubuntu_install_androidsdk.sh\n-RUN bash /install/ubuntu_install_androidsdk.sh 25.2.9519653 3.22.1 33.0.2 33\n-ENV PATH /opt/android-sdk-linux/platform-tools:$PATH\n-\n # Clang tool for CLML source codegen\n RUN apt-get update && apt-install-and-clear -y clang-format-15\n \ndiff --git a/docker/Dockerfile.ci_cpu b/docker/Dockerfile.ci_cpu\nindex ae088f5c9e63..17344f7dac22 100644\n--- a/docker/Dockerfile.ci_cpu\n+++ b/docker/Dockerfile.ci_cpu\n@@ -77,10 +77,6 @@ COPY install/ubuntu_install_golang.sh /install/ubuntu_install_golang.sh\n RUN bash /install/ubuntu_install_golang.sh\n ENV PATH $PATH:/usr/lib/go-1.18/bin\n \n-# NNPACK deps\n-COPY install/ubuntu_install_nnpack.sh /install/ubuntu_install_nnpack.sh\n-RUN bash /install/ubuntu_install_nnpack.sh\n-\n # ANTLR deps\n COPY install/ubuntu_install_java.sh /install/ubuntu_install_java.sh\n RUN bash /install/ubuntu_install_java.sh\n@@ -129,10 +125,6 @@ RUN bash /install/ubuntu_install_ethosn_driver_stack.sh\n COPY install/ubuntu_install_vitis_ai_packages_ci.sh /install/ubuntu_install_vitis_ai_packages_ci.sh\n RUN bash /install/ubuntu_install_vitis_ai_packages_ci.sh\n \n-# Android SDK\n-COPY install/ubuntu_install_androidsdk.sh /install/ubuntu_install_androidsdk.sh\n-RUN bash /install/ubuntu_install_androidsdk.sh\n-\n # PaddlePaddle deps\n COPY install/ubuntu_install_paddle.sh /install/ubuntu_install_paddle.sh\n RUN bash /install/ubuntu_install_paddle.sh\ndiff --git a/docker/Dockerfile.ci_gpu b/docker/Dockerfile.ci_gpu\nindex acb0310a41e2..8d11882098fb 100644\n--- a/docker/Dockerfile.ci_gpu\n+++ b/docker/Dockerfile.ci_gpu\n@@ -133,10 +133,6 @@ RUN bash /install/ubuntu_install_wasmtime.sh\n COPY install/ubuntu_install_redis.sh /install/ubuntu_install_redis.sh\n RUN bash /install/ubuntu_install_redis.sh\n \n-# NNPACK deps\n-COPY install/ubuntu_install_nnpack.sh /install/ubuntu_install_nnpack.sh\n-RUN bash /install/ubuntu_install_nnpack.sh\n-\n # BYODT deps\n COPY install/ubuntu_install_universal.sh /install/ubuntu_install_universal.sh\n RUN bash /install/ubuntu_install_universal.sh\ndiff --git a/docker/Dockerfile.ci_hexagon b/docker/Dockerfile.ci_hexagon\nindex 3b4c58ef43c9..1855e3a9c231 100644\n--- a/docker/Dockerfile.ci_hexagon\n+++ b/docker/Dockerfile.ci_hexagon\n@@ -58,12 +58,6 @@ RUN bash /install/ubuntu_install_python_package.sh\n COPY install/ubuntu_install_java.sh /install/ubuntu_install_java.sh\n RUN bash /install/ubuntu_install_java.sh\n \n-# Android SDK\n-COPY install/ubuntu_install_androidsdk.sh /install/ubuntu_install_androidsdk.sh\n-RUN bash /install/ubuntu_install_androidsdk.sh\n-ENV ANDROID_HOME=/opt/android-sdk-linux\n-ENV PATH /opt/android-sdk-linux/platform-tools:$PATH\n-\n # Hexagon\n COPY install/ubuntu_install_hexagon.sh /install/ubuntu_install_hexagon.sh\n RUN bash /install/ubuntu_install_hexagon.sh\ndiff --git a/docker/Dockerfile.demo_vitis_ai b/docker/Dockerfile.demo_vitis_ai\nindex b82076dbdf9c..01b0b494bd9e 100644\n--- a/docker/Dockerfile.demo_vitis_ai\n+++ b/docker/Dockerfile.demo_vitis_ai\n@@ -45,10 +45,6 @@ RUN bash /install/ubuntu_install_python_package.sh\n COPY install/ubuntu_install_llvm.sh /install/ubuntu_install_llvm.sh\n RUN bash /install/ubuntu_install_llvm.sh\n \n-# NNPACK deps\n-COPY install/ubuntu_install_nnpack.sh /install/ubuntu_install_nnpack.sh\n-RUN bash /install/ubuntu_install_nnpack.sh\n-\n ENV PATH $PATH:$CARGO_HOME/bin:/usr/lib/go-1.10/bin\n \n # ANTLR deps\ndiff --git a/docker/install/ubuntu_install_androidsdk.sh b/docker/install/ubuntu_install_androidsdk.sh\nindex 5e7278c5d631..193a02745f3a 100755\n--- a/docker/install/ubuntu_install_androidsdk.sh\n+++ b/docker/install/ubuntu_install_androidsdk.sh\n@@ -25,6 +25,8 @@ ANDROID_HOME=/opt/android-sdk-linux\n ASDKTOOLS_HOME=/opt/android-sdk-tools\n ASDKTOOLS_VERSION=3859397\n ASDKTOOLS_SHA256=444e22ce8ca0f67353bda4b85175ed3731cae3ffa695ca18119cbacef1c1bea0\n+COMMANDLINETOOLS_VERSION=11076708\n+COMMANDLINETOOLS_SHA256=2d2d50857e4eb553af5a6dc3ad507a17adf43d115264b1afc116f95c92e5e258\n \n ANDROID_NDK_VERSION=21.3.6528147\n CMAKE_VERSION=3.6.4111459\n@@ -52,11 +54,11 @@ echo \"Cmake Version: ${CMAKE_VERSION}\"\n echo \"Build Tools: ${BUILD_TOOLS_VERSION}\"\n echo \"Android Platform: ${ANDROID_PLATFORM}\"\n \n-wget -q http://dl.google.com/android/repository/sdk-tools-linux-${ASDKTOOLS_VERSION}.zip -O sdk-tools-linux.zip\n-echo \"${ASDKTOOLS_SHA256} *sdk-tools-linux.zip\" | sha256sum --check -\n-unzip sdk-tools-linux.zip\n-rm sdk-tools-linux.zip\n-mv tools \"${ASDKTOOLS_HOME}/\"\n+wget -q https://dl.google.com/android/repository/commandlinetools-linux-${COMMANDLINETOOLS_VERSION}_latest.zip  -O commandlinetools-linux.zip\n+echo \"${COMMANDLINETOOLS_SHA256} commandlinetools-linux.zip\" | sha256sum --check -\n+unzip commandlinetools-linux.zip\n+rm commandlinetools-linux.zip\n+mv cmdline-tools/ \"${ASDKTOOLS_HOME}/\"\n # The following popular fix makes sdkmanager honour $http_proxy variables\n mv ${ASDKTOOLS_HOME}/bin/sdkmanager ${ASDKTOOLS_HOME}/bin/sdkmanager-vanilla\n cat >${ASDKTOOLS_HOME}/bin/sdkmanager <<\"EOF\"\n@@ -90,8 +92,6 @@ extras;google;market_apk_expansion\n extras;google;market_licensing\n extras;google;simulators\n extras;google;webdriver\n-extras;m2repository;com;android;support;constraint;constraint-layout;1.0.2\n-extras;m2repository;com;android;support;constraint;constraint-layout-solver;1.0.2\n platforms;android-26\n platforms;android-${ANDROID_PLATFORM}\n tools\ndiff --git a/docker/install/ubuntu_install_java.sh b/docker/install/ubuntu_install_java.sh\nindex 5556f0d8fed5..c4a8c5f9acb5 100755\n--- a/docker/install/ubuntu_install_java.sh\n+++ b/docker/install/ubuntu_install_java.sh\n@@ -20,7 +20,7 @@ set -o errexit -o nounset\n set -o pipefail\n \n apt-get update\n-apt-install-and-clear -y openjdk-8-jdk maven\n+apt-install-and-clear -y openjdk-17-jdk maven\n arch=$(uname -m)\n jre_arch=\"unknown\"\n case $arch in\n@@ -36,8 +36,8 @@ case $arch in\n         ;;\n esac\n \n-if [ ! -d \"/usr/lib/jvm/java-8-openjdk-$jre_arch/jre\" ]; then\n+if [ ! -d \"/usr/lib/jvm/java-17-openjdk-$jre_arch\" ]; then\n   echo \"error: missing openjdk for $jre_arch\" >&2\n   exit 1\n fi\n-echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-$jre_arch/jre\" >> /etc/profile\n+echo \"export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-$jre_arch\" >> /etc/profile\n", "instance_id": "apache__tvm-17337", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the Docker image build for `ci_cpu` fails due to a specific error related to a missing module (`fp16.avx`) during the NNPACK build process. The expected behavior (successful build) and actual behavior (build failure with detailed error logs) are provided, along with the environment details (Ubuntu 22.04, specific TVM commit) and steps to reproduce the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss the root cause of the missing module or suggest a specific fix, leaving it to the developer to infer from the error logs. Additionally, there are no mentions of potential edge cases or constraints related to the build environment or dependencies that might affect the solution. Despite these minor gaps, the problem is well-defined enough to understand the goal and context, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of the code changes is significant, as they span multiple Dockerfiles and installation scripts across the codebase, affecting the build process for various environments (e.g., `ci_cpu`, `ci_gpu`, `ci_adreno`, etc.). The changes involve removing NNPACK and Android SDK installation steps, updating Java versions, and modifying dependency linking order in TFLite CMake configurations, indicating a need to understand interactions between different build components. Second, the number of technical concepts involved is moderate to high, requiring knowledge of Docker, build systems (CMake), dependency management, and specific libraries like NNPACK and TFLite, as well as familiarity with Python module dependencies and system-level package installations. Third, while the problem statement does not explicitly mention edge cases, the nature of build system modifications implies potential issues like compatibility across different environments, dependency conflicts, or downstream effects on other CI pipelines, which adds complexity to error handling and testing. Finally, the changes impact the build architecture of the system, a critical part of the development workflow, necessitating careful consideration to avoid breaking other components. A score of 0.65 reflects the need for a deep understanding of the build process and moderate-to-complex modifications, though it does not reach the \"Very Hard\" range as it does not involve advanced algorithmic or domain-specific challenges beyond build system expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "No validation against `OpStore` being used to store to an object with `OpTypeImage`.\nWe found some invalid SPIR-V being generated by DXC. It turns out that `spirv-val` does not catch the invalid SPIR-V either.\r\nI have attached an example shader to illustrate the issue.\r\n\r\n[bin_lines.cs.hlsl#main#SPIR-V#cs_6_6.txt](https://github.com/KhronosGroup/SPIRV-Tools/files/8660269/bin_lines.cs.hlsl.main.SPIR-V.cs_6_6.txt)\r\n\r\n```spirv\r\n%type_3d_image = OpTypeImage %uint 3D 2 0 0 2 R32ui\r\n...\r\n%_ptr_Function_type_3d_image = OpTypePointer Function %type_3d_image\r\n...\r\n%131 = OpAccessChain %_ptr_UniformConstant_type_3d_image %g_rwTexture3d %130\r\n%132 = OpLoad %type_3d_image %131\r\n...\r\n`%69 = OpVariable %_ptr_Function_type_3d_image Function`\r\n...\r\nOpStore %69 %132\r\n```\r\n \r\nThis contradicts Vulkan's spec, https://www.khronos.org/registry/vulkan/specs/1.1-extensions/html/vkspec.html#VUID-StandaloneSpirv-OpTypeImage-04661 :\r\n\"Objects of types OpTypeImage, OpTypeSampler, OpTypeSampledImage, and arrays of these types must not be stored to or modified\"\r\n\n", "patch": "diff --git a/source/val/validate_memory.cpp b/source/val/validate_memory.cpp\nindex 9bfa3c2158..3a3b870478 100644\n--- a/source/val/validate_memory.cpp\n+++ b/source/val/validate_memory.cpp\n@@ -1163,6 +1163,23 @@ spv_result_t ValidateStore(ValidationState_t& _, const Instruction* inst) {\n     }\n   }\n \n+  if (spvIsVulkanEnv(_.context()->target_env) &&\n+      !_.options()->before_hlsl_legalization) {\n+    const auto isForbiddenType = [](const Instruction* type_inst) {\n+      auto opcode = type_inst->opcode();\n+      return opcode == spv::Op::OpTypeImage ||\n+             opcode == spv::Op::OpTypeSampler ||\n+             opcode == spv::Op::OpTypeSampledImage ||\n+             opcode == spv::Op::OpTypeAccelerationStructureKHR;\n+    };\n+    if (_.ContainsType(object_type->id(), isForbiddenType)) {\n+      return _.diag(SPV_ERROR_INVALID_ID, inst)\n+             << _.VkErrorID(6924)\n+             << \"Cannot store to OpTypeImage, OpTypeSampler, \"\n+                \"OpTypeSampledImage, or OpTypeAccelerationStructureKHR objects\";\n+    }\n+  }\n+\n   return SPV_SUCCESS;\n }\n \ndiff --git a/source/val/validation_state.cpp b/source/val/validation_state.cpp\nindex 6f425310ce..2afcacc337 100644\n--- a/source/val/validation_state.cpp\n+++ b/source/val/validation_state.cpp\n@@ -2368,6 +2368,8 @@ std::string ValidationState_t::VkErrorID(uint32_t id,\n       return VUID_WRAP(VUID-StandaloneSpirv-Uniform-06807);\n     case 6808:\n       return VUID_WRAP(VUID-StandaloneSpirv-PushConstant-06808);\n+    case 6924:\n+      return VUID_WRAP(VUID-StandaloneSpirv-OpTypeImage-06924);\n     case 6925:\n       return VUID_WRAP(VUID-StandaloneSpirv-Uniform-06925);\n     case 7041:\n", "instance_id": "KhronosGroup__SPIRV-Tools-5368", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: invalid SPIR-V code generation related to storing objects of type `OpTypeImage` and similar types, which violates the Vulkan specification. It provides a specific example of the problematic SPIR-V code and references the relevant Vulkan spec section, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or constraints beyond the Vulkan spec reference, nor does it mention specific edge cases or scenarios to test against. Additionally, the context of `spirv-val` not catching the issue is noted but not elaborated upon, leaving some uncertainty about the broader implications or related issues. Overall, while the core issue is clear, the lack of detailed requirements or edge case specifications prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively limited, affecting only two files (`validate_memory.cpp` and `validation_state.cpp`) with a small number of lines added (around 20 lines total). The changes are localized to the `ValidateStore` function and involve adding a validation check for specific SPIR-V types, which does not impact the broader system architecture. Second, the technical concepts required include an understanding of SPIR-V (a domain-specific intermediate representation for shaders), Vulkan API specifications, and the internal validation logic of the SPIRV-Tools codebase. While these concepts are moderately complex, they are not overly advanced for someone familiar with graphics programming or compiler toolchains. Third, the problem does not explicitly mention edge cases beyond the provided SPIR-V snippet, and the code changes do not introduce complex error handling beyond a straightforward type check and error message. However, the need to understand the Vulkan spec and SPIR-V semantics adds a layer of complexity beyond a simple bug fix. Overall, this problem requires a moderate level of expertise and understanding of specific domain knowledge, placing it in the 0.4-0.6 range, with a score of 0.45 reflecting the balance between localized changes and domain-specific complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Excel does not support timezones in datetimes. The tzinfo in the datetime/time object must be set to None.\n**Describe the bug**\r\nUpdating an existing Django project from `django-import-export` from 4.1.1 to 4.2 raises the following exception while using the admin export action for models having `datetime` fields.\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\Program Files\\Python311\\Lib\\wsgiref\\handlers.py\", line 137, in run\r\n    self.result = application(self.environ, self.start_response)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\contrib\\staticfiles\\handlers.py\", line 80, in __call__\r\n    return self.application(environ, start_response)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\wsgi.py\", line 124, in __call__\r\n    response = self.get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\base.py\", line 140, in get_response\r\n    response = self._middleware_chain(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 42, in __call__\r\n    return self.get_response(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 54, in __call__\r\n    response = self.get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 85, in __call__\r\n    return self.get_response(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 170, in __call__\r\n    return self.get_response(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 135, in __call__\r\n    return self.get_response(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Workspace\\test\\src\\test\\core\\middleware.py\", line 193, in __call__\r\n    response = self.get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\simple_history\\middleware.py\", line 41, in middleware\r\n    return get_response(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\allauth\\account\\middleware.py\", line 36, in middleware\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\deprecation.py\", line 129, in __call__\r\n    response = response or self.get_response(request)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 57, in inner\r\n    response = response_for_exception(request, exc)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 140, in response_for_exception\r\n    response = handle_uncaught_exception(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\exception.py\", line 55, in inner\r\n    response = get_response(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\core\\handlers\\base.py\", line 197, in _get_response\r\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\decorators.py\", line 188, in _view_wrapper\r\n    result = _process_exception(request, e)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\utils\\decorators.py\", line 186, in _view_wrapper\r\n    response = view_func(request, *args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\views\\decorators\\cache.py\", line 80, in _view_wrapper\r\n    response = view_func(request, *args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\django\\contrib\\admin\\sites.py\", line 241, in inner\r\n    return view(request, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\import_export\\admin.py\", line 773, in export_action\r\n    return self._do_file_export(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\import_export\\admin.py\", line 837, in _do_file_export\r\n    export_data = self.get_export_data(\r\n                  ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\import_export\\admin.py\", line 718, in get_export_data\r\n    export_data = file_format.export_data(data)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\import_export\\formats\\base_formats.py\", line 213, in export_data\r\n    return super().export_data(dataset, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\import_export\\formats\\base_formats.py\", line 91, in export_data\r\n    return dataset.export(self.get_title(), **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\tablib\\core.py\", line 434, in export\r\n    return fmt.export_set(self, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\tablib\\formats\\_xlsx.py\", line 63, in export_set\r\n    wb.save(stream)\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py\", line 386, in save\r\n    save_workbook(self, filename)\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py\", line 294, in save_workbook\r\n    writer.save()\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py\", line 275, in save\r\n    self.write_data()\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py\", line 77, in write_data\r\n    self._write_worksheets()\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py\", line 215, in _write_worksheets\r\n    self.write_worksheet(ws)\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\writer\\excel.py\", line 200, in write_worksheet\r\n    writer.write()\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py\", line 359, in write\r\n    self.write_rows()\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py\", line 125, in write_rows\r\n    self.write_row(xf, row, row_idx)\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py\", line 147, in write_row\r\n    write_cell(xf, self.ws, cell, cell.has_style)\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\cell\\_writer.py\", line 90, in lxml_write_cell\r\n    value, attributes = _set_attributes(cell, styled)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\Workspace\\test\\.venv\\Lib\\site-packages\\openpyxl\\cell\\_writer.py\", line 30, in _set_attributes\r\n    raise TypeError(\"Excel does not support timezones in datetimes. \"\r\nTypeError: Excel does not support timezones in datetimes. The tzinfo in the datetime/time object must be set to None.\r\n```\r\n\r\n**Settings**\r\n```\r\nTIME_ZONE = \"Europe/Berlin\"\r\nLANGUAGE_CODE = \"en\"\r\nUSE_I18N = False\r\nUSE_TZ = True\r\nDATETIME_FORMAT = \"D, j M Y H:i e\"\r\nSHORT_DATETIME_FORMAT = \"j F Y H:i\"\r\nDATE_FORMAT = \"j F Y\"\r\nSHORT_DATE_FORMAT = \"j F\"\r\nTIME_FORMAT = \"H:i:s\"\r\nFIRST_DAY_OF_WEEK = 1\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n - Django Import Export: 4.2\r\n - Python 3.11.7\r\n - Django 5.1.3\r\n - tablib 3.7.0\r\n\r\n**Expected behavior**\r\nWorking export without errors as in version 4.1.1\r\n\r\n\n", "patch": "diff --git a/import_export/widgets.py b/import_export/widgets.py\nindex 4fa8d9969..5e0c0469f 100644\n--- a/import_export/widgets.py\n+++ b/import_export/widgets.py\n@@ -307,13 +307,16 @@ def clean(self, value, row=None, **kwargs):\n \n     def render(self, value, obj=None, **kwargs):\n         self._obj_deprecation_warning(obj)\n-        force_native_type = kwargs.get(\"force_native_type\")\n         if not value or not isinstance(value, datetime):\n             return \"\"\n-        if self.coerce_to_string is False or force_native_type:\n-            return value.replace(tzinfo=None) if force_native_type else value\n         if settings.USE_TZ:\n             value = timezone.localtime(value)\n+\n+        force_native_type = kwargs.get(\"force_native_type\")\n+        if self.coerce_to_string is False or force_native_type:\n+            # binary formats such as xlsx must not have tz set\n+            return value.replace(tzinfo=None) if force_native_type else value\n+\n         return format_datetime(value, self.formats[0])\n \n \n", "instance_id": "django-import-export__django-import-export-1999", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an error occurs when exporting data to Excel using `django-import-export` version 4.2 due to timezone information in datetime objects, which Excel does not support. The goal is evident (fix the error to allow successful export), and the context is provided through a detailed traceback, settings, and version information. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the expected behavior beyond \"working export without errors as in version 4.1.1,\" nor does it mention specific edge cases or constraints (e.g., how to handle different timezone configurations or non-standard datetime formats). Additionally, while the traceback is helpful, it is overly verbose and could be summarized for clarity. Overall, the statement is valid and mostly clear but lacks some minor details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`import_export/widgets.py`) and involves a small modification to the `render` method of a widget class. The change reorders logic to ensure timezone information is stripped when necessary (e.g., for binary formats like XLSX) and does not impact the broader system architecture. The amount of code change is minimal, consisting of a few lines.\n\n2. **Number of Technical Concepts**: Solving this requires understanding basic Python datetime handling, specifically the `tzinfo` attribute and the `replace()` method to remove timezone information. Additionally, familiarity with Django settings (`USE_TZ`) and the `django-import-export` library's widget rendering logic is needed. These concepts are relatively straightforward for a developer with moderate experience in Python and Django.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change implies handling scenarios where datetime objects might or might not have timezone information. The modification ensures compatibility with Excel by stripping `tzinfo`, which addresses the primary error condition. No complex error handling or additional edge case logic appears to be required beyond this fix.\n\n4. **Overall Complexity**: The fix is a simple adjustment to existing logic, requiring minimal debugging or architectural understanding beyond the immediate context of the `render` method. It does not involve multiple modules or deep interactions within the codebase.\n\nGiven these points, a difficulty score of 0.25 reflects the ease of the task, as it involves understanding some code logic and making a straightforward modification to handle timezone information for Excel compatibility.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Replace device_memory_resource* with device_async_resource_ref\nUpdate occurences of `rmm::mr::device_memory_resource*` to\n`rmm::device_async_resource_ref` in this repo. The rationale is explained\nin [this document](https://docs.google.com/document/d/1QsLQn17qexczFGbzjLzTkIiGztfaQvLu-DpNu2EPw1E/edit#heading=h.wdftxmu1lxt).\n\n[BUG]: Strange behavior with quadtree point in polygon\n### Version\n\n23.04\n\n### On which installation method(s) does this occur?\n\nRapids-Compose\n\n### Describe the issue\n\nquadtree pip seems to experience a silent OOM or other error in certain larger data cases. This particular example requires the availability of our demo datasets `taxi2015.csv` and `taxi_zones.zip`. I'm not sure of another way to reproduce it, so I'm including the full example here. It doesn't always appear so I'm just remembering the issue here.\r\n\r\nAs you can see from the Relevant log output, quadtree returns 3.7m samples when only the first 120 polygons in `zones` are used. If the entire `zones` polygons list (263 polygons) is used, _something_ happens and only 16314 rows are returned. This is unexpected behavior and needs investigation.\n\n### Minimum reproducible example\n\n```shell\n# https://github.com/rapidsai/cuspatial/issues/890\r\nimport cuspatial\r\nimport cudf\r\nimport geopandas\r\nhost_zones = geopandas.read_file('taxi_zones.zip')\r\nhost_lonlat = host_zones.to_crs(epsg=4326)\r\nzones = cuspatial.from_geopandas(host_lonlat)\r\ntaxi2015 = cudf.read_csv('taxi2015.csv')\r\ndef quadtree(polygons, points):\r\n    poly_points_x = polygons.polygons.x\r\n    poly_points_y = polygons.polygons.y\r\n    poly_offsets = polygons.polygons.part_offset\r\n    poly_ring_offsets = polygons.polygons.ring_offset\r\n    test_points_x = points.points.x\r\n    test_points_y = points.points.y\r\n    scale = 50\r\n    max_depth = 7\r\n    min_size = 125\r\n    x_max = poly_points_x.max()\r\n    x_min = poly_points_x.min()\r\n    y_max = poly_points_y.max()\r\n    y_min = poly_points_y.min()\r\n    point_indices, quadtree = cuspatial.quadtree_on_points(\r\n        test_points_x,\r\n        test_points_y,\r\n        x_min,\r\n        x_max,\r\n        y_min,\r\n        y_max,\r\n        scale,\r\n        max_depth,\r\n        min_size,\r\n    )\r\n    poly_bboxes = cuspatial.polygon_bounding_boxes(\r\n        poly_offsets, poly_ring_offsets, poly_points_x, poly_points_y\r\n    )\r\n    intersections = cuspatial.join_quadtree_and_bounding_boxes(\r\n        quadtree, poly_bboxes, x_min, x_max, y_min, y_max, scale, max_depth\r\n    )\r\n    polygons_and_points = cuspatial.quadtree_point_in_polygon(\r\n        intersections,\r\n        quadtree,\r\n        point_indices,\r\n        test_points_x,\r\n        test_points_y,\r\n        poly_offsets,\r\n        poly_ring_offsets,\r\n        poly_points_x,\r\n        poly_points_y,\r\n    )\r\n    return polygons_and_points\r\ndef make_geoseries_from_lon_lat(lon, lat):\r\n    # Scatter the two columns into one column\r\n    assert len(lon) == len(lat)\r\n    xy = cudf.Series(cp.zeros(len(lon) * 2))\r\n    xy[::2] = lon\r\n    xy[1::2] = lat\r\n\r\n    return cuspatial.GeoSeries(cuspatial.core._column.geocolumn.GeoColumn._from_points_xy(xy._column))\r\ndropoffs = make_geoseries_from_lon_lat(\r\n    taxi2015['dropoff_longitude'],\r\n    taxi2015['dropoff_latitude']\r\n)\r\nprint(quadtree(zones['geometry'].iloc[0:120], dropoffs))\r\nprint(quadtree(zones['geometry'], dropoffs))\n```\n\n\n### Relevant log output\n\n```shell\npolygon_index  point_index\r\n0                    0          116\r\n1                    0          387\r\n2                    0          685\r\n3                    0         2607\r\n4                    0         3141\r\n...                ...          ...\r\n3686177            167     12304018\r\n3686178            167     12323531\r\n3686179            167     12351800\r\n3686180            167     12444884\r\n3686181            167     12484251\r\n\r\n[3686182 rows x 2 columns]\r\n       polygon_index  point_index\r\n0                  0          116\r\n1                  0          387\r\n2                  0          685\r\n3                  0         2608\r\n4                  0         3142\r\n...              ...          ...\r\n16309              0     12504645\r\n16310              1      7456107\r\n16311              1      7530752\r\n16312              1      7704910\r\n16313              1     11938181\r\n\r\n[16314 rows x 2 columns]\n```\n\n\n### Environment details\n\n```shell\nrapids-compose 23.04\n```\n\n\n### Other/Misc.\n\n_No response_\n", "patch": "diff --git a/.devcontainer/cuda11.8-conda/devcontainer.json b/.devcontainer/cuda11.8-conda/devcontainer.json\nindex 281859a2d..5e203a384 100644\n--- a/.devcontainer/cuda11.8-conda/devcontainer.json\n+++ b/.devcontainer/cuda11.8-conda/devcontainer.json\n@@ -5,12 +5,17 @@\n     \"args\": {\n       \"CUDA\": \"11.8\",\n       \"PYTHON_PACKAGE_MANAGER\": \"conda\",\n-      \"BASE\": \"rapidsai/devcontainers:24.04-cpp-llvm16-cuda11.8-mambaforge-ubuntu22.04\"\n+      \"BASE\": \"rapidsai/devcontainers:24.06-cpp-cuda11.8-mambaforge-ubuntu22.04\"\n     }\n   },\n+  \"runArgs\": [\n+    \"--rm\",\n+    \"--name\",\n+    \"${localEnv:USER}-rapids-${localWorkspaceFolderBasename}-24.06-cuda11.8-conda\"\n+  ],\n   \"hostRequirements\": {\"gpu\": \"optional\"},\n   \"features\": {\n-    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.4\": {}\n+    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.6\": {}\n   },\n   \"overrideFeatureInstallOrder\": [\n     \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils\"\ndiff --git a/.devcontainer/cuda11.8-pip/devcontainer.json b/.devcontainer/cuda11.8-pip/devcontainer.json\nindex 5f042ffd4..682b2d155 100644\n--- a/.devcontainer/cuda11.8-pip/devcontainer.json\n+++ b/.devcontainer/cuda11.8-pip/devcontainer.json\n@@ -5,12 +5,17 @@\n     \"args\": {\n       \"CUDA\": \"11.8\",\n       \"PYTHON_PACKAGE_MANAGER\": \"pip\",\n-      \"BASE\": \"rapidsai/devcontainers:24.04-cpp-cuda11.8-ubuntu22.04\"\n+      \"BASE\": \"rapidsai/devcontainers:24.06-cpp-cuda11.8-ubuntu22.04\"\n     }\n   },\n+  \"runArgs\": [\n+    \"--rm\",\n+    \"--name\",\n+    \"${localEnv:USER}-rapids-${localWorkspaceFolderBasename}-24.06-cuda11.8-pip\"\n+  ],\n   \"hostRequirements\": {\"gpu\": \"optional\"},\n   \"features\": {\n-    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.4\": {}\n+    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.6\": {}\n   },\n   \"overrideFeatureInstallOrder\": [\n     \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils\"\ndiff --git a/.devcontainer/cuda12.2-conda/devcontainer.json b/.devcontainer/cuda12.2-conda/devcontainer.json\nindex ab04745b7..d3bd5b769 100644\n--- a/.devcontainer/cuda12.2-conda/devcontainer.json\n+++ b/.devcontainer/cuda12.2-conda/devcontainer.json\n@@ -5,12 +5,17 @@\n     \"args\": {\n       \"CUDA\": \"12.2\",\n       \"PYTHON_PACKAGE_MANAGER\": \"conda\",\n-      \"BASE\": \"rapidsai/devcontainers:24.04-cpp-mambaforge-ubuntu22.04\"\n+      \"BASE\": \"rapidsai/devcontainers:24.06-cpp-mambaforge-ubuntu22.04\"\n     }\n   },\n+  \"runArgs\": [\n+    \"--rm\",\n+    \"--name\",\n+    \"${localEnv:USER}-rapids-${localWorkspaceFolderBasename}-24.06-cuda12.2-conda\"\n+  ],\n   \"hostRequirements\": {\"gpu\": \"optional\"},\n   \"features\": {\n-    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.4\": {}\n+    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.6\": {}\n   },\n   \"overrideFeatureInstallOrder\": [\n     \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils\"\ndiff --git a/.devcontainer/cuda12.2-pip/devcontainer.json b/.devcontainer/cuda12.2-pip/devcontainer.json\nindex d4f567086..a667a3fd0 100644\n--- a/.devcontainer/cuda12.2-pip/devcontainer.json\n+++ b/.devcontainer/cuda12.2-pip/devcontainer.json\n@@ -5,12 +5,17 @@\n     \"args\": {\n       \"CUDA\": \"12.2\",\n       \"PYTHON_PACKAGE_MANAGER\": \"pip\",\n-      \"BASE\": \"rapidsai/devcontainers:24.04-cpp-cuda12.2-ubuntu22.04\"\n+      \"BASE\": \"rapidsai/devcontainers:24.06-cpp-cuda12.2-ubuntu22.04\"\n     }\n   },\n+  \"runArgs\": [\n+    \"--rm\",\n+    \"--name\",\n+    \"${localEnv:USER}-rapids-${localWorkspaceFolderBasename}-24.06-cuda12.2-pip\"\n+  ],\n   \"hostRequirements\": {\"gpu\": \"optional\"},\n   \"features\": {\n-    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.4\": {}\n+    \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils:24.6\": {}\n   },\n   \"overrideFeatureInstallOrder\": [\n     \"ghcr.io/rapidsai/devcontainers/features/rapids-build-utils\"\ndiff --git a/.github/workflows/build.yaml b/.github/workflows/build.yaml\nindex ba2bf3bc8..06cb83f7d 100644\n--- a/.github/workflows/build.yaml\n+++ b/.github/workflows/build.yaml\n@@ -28,7 +28,7 @@ concurrency:\n jobs:\n   cpp-build:\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-build.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -37,7 +37,7 @@ jobs:\n   python-build:\n     needs: [cpp-build]\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-python-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-python-build.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -46,7 +46,7 @@ jobs:\n   upload-conda:\n     needs: [cpp-build, python-build]\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-upload-packages.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-upload-packages.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -56,7 +56,7 @@ jobs:\n     if: github.ref_type == 'branch'\n     needs: python-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.06\n     with:\n       arch: \"amd64\"\n       branch: ${{ inputs.branch }}\n@@ -68,7 +68,7 @@ jobs:\n       sha: ${{ inputs.sha }}\n   wheel-build-cuspatial:\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -78,7 +78,7 @@ jobs:\n   wheel-publish-cuspatial:\n     needs: wheel-build-cuspatial\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-publish.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-publish.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -87,7 +87,7 @@ jobs:\n       package-name: cuspatial\n   wheel-build-cuproj:\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\n@@ -97,7 +97,7 @@ jobs:\n   wheel-publish-cuproj:\n     needs: wheel-build-cuproj\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-publish.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-publish.yaml@branch-24.06\n     with:\n       build_type: ${{ inputs.build_type || 'branch' }}\n       branch: ${{ inputs.branch }}\ndiff --git a/.github/workflows/pr.yaml b/.github/workflows/pr.yaml\nindex 10b8ccff9..be33cfd04 100644\n--- a/.github/workflows/pr.yaml\n+++ b/.github/workflows/pr.yaml\n@@ -25,40 +25,40 @@ jobs:\n       - wheel-tests-cuproj\n       - devcontainer\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/pr-builder.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/pr-builder.yaml@branch-24.06\n   checks:\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/checks.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/checks.yaml@branch-24.06\n     with:\n       enable_check_generated_files: false\n   conda-cpp-build:\n     needs: checks\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-build.yaml@branch-24.06\n     with:\n       build_type: pull-request\n   conda-cpp-tests:\n     needs: conda-cpp-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-tests.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-cpp-tests.yaml@branch-24.06\n     with:\n       build_type: pull-request\n   conda-python-build:\n     needs: conda-cpp-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-python-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-python-build.yaml@branch-24.06\n     with:\n       build_type: pull-request\n   conda-python-tests:\n     needs: conda-python-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/conda-python-tests.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/conda-python-tests.yaml@branch-24.06\n     with:\n       build_type: pull-request\n   conda-notebook-tests:\n     needs: conda-python-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       node_type: \"gpu-v100-latest-1\"\n@@ -68,7 +68,7 @@ jobs:\n   docs-build:\n     needs: conda-python-build\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/custom-job.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       node_type: \"gpu-v100-latest-1\"\n@@ -78,34 +78,34 @@ jobs:\n   wheel-build-cuspatial:\n     needs: checks\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       script: ci/build_wheel_cuspatial.sh\n   wheel-tests-cuspatial:\n     needs: wheel-build-cuspatial\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-test.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-test.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       script: ci/test_wheel_cuspatial.sh\n   wheel-build-cuproj:\n     needs: checks\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-build.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       script: ci/build_wheel_cuproj.sh\n   wheel-tests-cuproj:\n     needs: [wheel-build-cuspatial, wheel-build-cuproj]\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/wheels-test.yaml@branch-24.04\n+    uses: rapidsai/shared-workflows/.github/workflows/wheels-test.yaml@branch-24.06\n     with:\n       build_type: pull-request\n       script: ci/test_wheel_cuproj.sh\n   devcontainer:\n     secrets: inherit\n-    uses: rapidsai/shared-workflows/.github/workflows/build-in-devcontainer.yaml@fix/devcontainer-json-location\n+    uses: rapidsai/shared-workflows/.github/workflows/build-in-devcontainer.yaml@branch-24.06\n     with:\n       arch: '[\"amd64\"]'\n       cuda: '[\"12.2\"]'\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex fa7e8b254..264cef755 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,3 +1,27 @@\n+# cuspatial 24.06.00 (5 Jun 2024)\n+\n+## \ud83d\udea8 Breaking Changes\n+\n+- Replace rmm::mr::device_memory_resource* with rmm::device_async_resource_ref ([#1373](https://github.com/rapidsai/cuspatial/pull/1373)) [@harrism](https://github.com/harrism)\n+\n+## \ud83d\udc1b Bug Fixes\n+\n+- create conda ci test env in one step ([#1387](https://github.com/rapidsai/cuspatial/pull/1387)) [@msarahan](https://github.com/msarahan)\n+\n+## \ud83d\udee0\ufe0f Improvements\n+\n+- Fix up imports for cudf changes ([#1383](https://github.com/rapidsai/cuspatial/pull/1383)) [@vyasr](https://github.com/vyasr)\n+- Fix building cuspatial with CCCL main ([#1382](https://github.com/rapidsai/cuspatial/pull/1382)) [@trxcllnt](https://github.com/trxcllnt)\n+- Fix quadtree spatial join OOMs on large numbers of input polygons ([#1381](https://github.com/rapidsai/cuspatial/pull/1381)) [@trxcllnt](https://github.com/trxcllnt)\n+- Enable warnings-as-errors for cuproj tests ([#1379](https://github.com/rapidsai/cuspatial/pull/1379)) [@mroeschke](https://github.com/mroeschke)\n+- Always use a static gtest and gbench ([#1377](https://github.com/rapidsai/cuspatial/pull/1377)) [@trxcllnt](https://github.com/trxcllnt)\n+- Migrate to `{{ stdlib(&quot;c&quot;) }}` ([#1376](https://github.com/rapidsai/cuspatial/pull/1376)) [@hcho3](https://github.com/hcho3)\n+- add --rm and --name to devcontainer run args ([#1375](https://github.com/rapidsai/cuspatial/pull/1375)) [@trxcllnt](https://github.com/trxcllnt)\n+- Replace rmm::mr::device_memory_resource* with rmm::device_async_resource_ref ([#1373](https://github.com/rapidsai/cuspatial/pull/1373)) [@harrism](https://github.com/harrism)\n+- Enable all tests for `arm` jobs ([#1365](https://github.com/rapidsai/cuspatial/pull/1365)) [@galipremsagar](https://github.com/galipremsagar)\n+- Enable pytest failures on warnings on FutureWarnings (Replace deprecated `geopandas.dataset`  module) ([#1360](https://github.com/rapidsai/cuspatial/pull/1360)) [@mroeschke](https://github.com/mroeschke)\n+- Fix `JOIN_POINT_IN_POLYGON_LARGE_TEST_EXP` test ([#1346](https://github.com/rapidsai/cuspatial/pull/1346)) [@trxcllnt](https://github.com/trxcllnt)\n+\n # cuSpatial 24.04.00 (10 Apr 2024)\n \n ## \ud83d\udc1b Bug Fixes\ndiff --git a/README.md b/README.md\nindex 5ceb75c38..97ba2859a 100644\n--- a/README.md\n+++ b/README.md\n@@ -113,7 +113,7 @@ An example command from the Release Selector:\n docker run --gpus all --pull always --rm -it \\\n     --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n     -p 8888:8888 -p 8787:8787 -p 8786:8786 \\\n-    nvcr.io/nvidia/rapidsai/notebooks:24.04-cuda11.8-py3.10\n+    nvcr.io/nvidia/rapidsai/notebooks:24.06-cuda11.8-py3.10\n ```\n \n ### Install with Conda\n@@ -125,7 +125,7 @@ cuSpatial can be installed with conda (miniconda, or the full Anaconda distribut\n \n ```shell\n conda install -c rapidsai -c conda-forge -c nvidia \\\n-    cuspatial=24.04 python=3.11 cudatoolkit=11.8\n+    cuspatial=24.06 python=3.11 cudatoolkit=11.8\n ```\n We also provide nightly Conda packages built from the HEAD of our latest development branch.\n \n@@ -184,7 +184,7 @@ To build and install cuSpatial from source please see the [build documentation](\n If you find cuSpatial useful in your published work, please consider citing the repository.\n \n ```bibtex\n-@misc{cuspatial:24.04,\n+@misc{cuspatial:24.06,\n     author = {{NVIDIA Corporation}},\n     title = {cuSpatial: GPU-Accelerated Geospatial and Spatiotemporal Algorithms},\n     year = {2023},\ndiff --git a/VERSION b/VERSION\nindex 4a2fe8aa5..0bff6981a 100644\n--- a/VERSION\n+++ b/VERSION\n@@ -1,1 +1,1 @@\n-24.04.00\n+24.06.00\ndiff --git a/ci/build_docs.sh b/ci/build_docs.sh\nindex c0d99969a..0d4e97488 100755\n--- a/ci/build_docs.sh\n+++ b/ci/build_docs.sh\n@@ -6,30 +6,24 @@ set -euo pipefail\n rapids-logger \"Create test conda environment\"\n . /opt/conda/etc/profile.d/conda.sh\n \n+rapids-logger \"Downloading artifacts from previous jobs\"\n+CPP_CHANNEL=\"$(rapids-download-conda-from-s3 cpp)\"\n+PYTHON_CHANNEL=\"$(rapids-download-conda-from-s3 python)\"\n+\n rapids-dependency-file-generator \\\n   --output conda \\\n   --file_key docs \\\n-  --matrix \"cuda=${RAPIDS_CUDA_VERSION%.*};arch=$(arch);py=${RAPIDS_PY_VERSION}\" | tee env.yaml\n+  --matrix \"cuda=${RAPIDS_CUDA_VERSION%.*};arch=$(arch);py=${RAPIDS_PY_VERSION}\" \\\n+  --prepend-channel \"${CPP_CHANNEL}\" --prepend-channel \"${PYTHON_CHANNEL}\" | tee env.yaml\n \n rapids-mamba-retry env create --yes -f env.yaml -n docs\n conda activate docs\n \n rapids-print-env\n \n-rapids-logger \"Downloading artifacts from previous jobs\"\n-CPP_CHANNEL=$(rapids-download-conda-from-s3 cpp)\n-PYTHON_CHANNEL=$(rapids-download-conda-from-s3 python)\n-\n-rapids-mamba-retry install \\\n-  --channel \"${CPP_CHANNEL}\" \\\n-  --channel \"${PYTHON_CHANNEL}\" \\\n-  libcuspatial \\\n-  cuspatial \\\n-  cuproj\n-\n export RAPIDS_VERSION=\"$(rapids-version)\"\n export RAPIDS_VERSION_MAJOR_MINOR=\"$(rapids-version-major-minor)\"\n-export RAPIDS_VERSION_NUMBER=\"$RAPIDS_VERSION_MAJOR_MINOR\"\n+export RAPIDS_VERSION_NUMBER=\"24.06\"\n export RAPIDS_DOCS_DIR=\"$(mktemp -d)\"\n \n rapids-logger \"Build cuSpatial CPP docs\"\ndiff --git a/ci/release/update-version.sh b/ci/release/update-version.sh\nindex 8bf47d77f..ae13d7629 100755\n--- a/ci/release/update-version.sh\n+++ b/ci/release/update-version.sh\n@@ -45,7 +45,10 @@ NEXT_SHORT_TAG_PEP440=$(python -c \"from setuptools.extern import packaging; prin\n DEPENDENCIES=(\n   cudf\n   cuml\n+  cuspatial\n   libcudf\n+  libcuspatial\n+  libcuspatial-tests\n   librmm\n   rmm\n   cuspatial\n@@ -81,4 +84,5 @@ sed_runner \"s/notebooks:[0-9]\\+\\.[0-9]\\+/notebooks:${NEXT_SHORT_TAG}/g\" README.m\n find .devcontainer/ -type f -name devcontainer.json -print0 | while IFS= read -r -d '' filename; do\n     sed_runner \"s@rapidsai/devcontainers:[0-9.]*@rapidsai/devcontainers:${NEXT_SHORT_TAG}@g\" \"${filename}\"\n     sed_runner \"s@rapidsai/devcontainers/features/rapids-build-utils:[0-9.]*@rapidsai/devcontainers/features/rapids-build-utils:${NEXT_SHORT_TAG_PEP440}@\" \"${filename}\"\n+    sed_runner \"s@rapids-\\${localWorkspaceFolderBasename}-[0-9.]*@rapids-\\${localWorkspaceFolderBasename}-${NEXT_SHORT_TAG}@g\" \"${filename}\"\n done\ndiff --git a/conda/environments/all_cuda-118_arch-x86_64.yaml b/conda/environments/all_cuda-118_arch-x86_64.yaml\nindex 506060cf9..ae933a67c 100644\n--- a/conda/environments/all_cuda-118_arch-x86_64.yaml\n+++ b/conda/environments/all_cuda-118_arch-x86_64.yaml\n@@ -11,8 +11,8 @@ dependencies:\n - cmake>=3.26.4\n - cuda-version=11.8\n - cudatoolkit\n-- cudf==24.4.*\n-- cuml==24.4.*\n+- cudf==24.6.*\n+- cuml==24.6.*\n - cupy>=12.0.0\n - curl\n - cxx-compiler\n@@ -20,12 +20,12 @@ dependencies:\n - doxygen\n - gcc_linux-64=11.*\n - geopandas>=0.11.0\n-- gmock>=1.13.0\n-- gtest>=1.13.0\n - ipython\n - ipywidgets\n-- libcudf==24.4.*\n-- librmm==24.4.*\n+- libcudf==24.6.*\n+- libcuspatial-tests==24.6.*\n+- libcuspatial==24.6.*\n+- librmm==24.6.*\n - myst-parser\n - nbsphinx\n - ninja\n@@ -41,7 +41,7 @@ dependencies:\n - pytest-cov\n - pytest-xdist\n - python>=3.9,<3.12\n-- rmm==24.4.*\n+- rmm==24.6.*\n - scikit-build-core>=0.7.0\n - scikit-image\n - shapely\ndiff --git a/conda/environments/all_cuda-122_arch-x86_64.yaml b/conda/environments/all_cuda-122_arch-x86_64.yaml\nindex 566aa2327..4f48db253 100644\n--- a/conda/environments/all_cuda-122_arch-x86_64.yaml\n+++ b/conda/environments/all_cuda-122_arch-x86_64.yaml\n@@ -14,8 +14,8 @@ dependencies:\n - cuda-nvcc\n - cuda-nvrtc-dev\n - cuda-version=12.2\n-- cudf==24.4.*\n-- cuml==24.4.*\n+- cudf==24.6.*\n+- cuml==24.6.*\n - cupy>=12.0.0\n - curl\n - cxx-compiler\n@@ -23,12 +23,12 @@ dependencies:\n - doxygen\n - gcc_linux-64=11.*\n - geopandas>=0.11.0\n-- gmock>=1.13.0\n-- gtest>=1.13.0\n - ipython\n - ipywidgets\n-- libcudf==24.4.*\n-- librmm==24.4.*\n+- libcudf==24.6.*\n+- libcuspatial-tests==24.6.*\n+- libcuspatial==24.6.*\n+- librmm==24.6.*\n - myst-parser\n - nbsphinx\n - ninja\n@@ -43,7 +43,7 @@ dependencies:\n - pytest-cov\n - pytest-xdist\n - python>=3.9,<3.12\n-- rmm==24.4.*\n+- rmm==24.6.*\n - scikit-build-core>=0.7.0\n - scikit-image\n - shapely\ndiff --git a/conda/recipes/cuproj/conda_build_config.yaml b/conda/recipes/cuproj/conda_build_config.yaml\nindex 92f617ce1..67f968412 100644\n--- a/conda/recipes/cuproj/conda_build_config.yaml\n+++ b/conda/recipes/cuproj/conda_build_config.yaml\n@@ -10,7 +10,10 @@ cuda_compiler:\n cuda11_compiler:\n   - nvcc\n \n-sysroot_version:\n+c_stdlib:\n+  - sysroot\n+\n+c_stdlib_version:\n   - \"2.17\"\n \n cmake_version:\ndiff --git a/conda/recipes/cuproj/meta.yaml b/conda/recipes/cuproj/meta.yaml\nindex 22309116e..4721daefb 100644\n--- a/conda/recipes/cuproj/meta.yaml\n+++ b/conda/recipes/cuproj/meta.yaml\n@@ -1,4 +1,4 @@\n-# Copyright (c) 2018-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2018-2024, NVIDIA CORPORATION.\n \n {% set version = environ['RAPIDS_PACKAGE_VERSION'].lstrip('v') %}\n {% set minor_version = version.split('.')[0] + '.' + version.split('.')[1] %}\n@@ -52,7 +52,7 @@ requirements:\n     {% endif %}\n     - cuda-version ={{ cuda_version }}\n     - ninja\n-    - sysroot_{{ target_platform }} {{ sysroot_version }}\n+    - {{ stdlib(\"c\") }}\n   host:\n     {% if cuda_major != \"11\" %}\n     - cuda-cudart-dev\ndiff --git a/conda/recipes/cuspatial/conda_build_config.yaml b/conda/recipes/cuspatial/conda_build_config.yaml\nindex e28b98da7..e3ca633eb 100644\n--- a/conda/recipes/cuspatial/conda_build_config.yaml\n+++ b/conda/recipes/cuspatial/conda_build_config.yaml\n@@ -10,7 +10,10 @@ cuda_compiler:\n cuda11_compiler:\n   - nvcc\n \n-sysroot_version:\n+c_stdlib:\n+  - sysroot\n+\n+c_stdlib_version:\n   - \"2.17\"\n \n cmake_version:\ndiff --git a/conda/recipes/cuspatial/meta.yaml b/conda/recipes/cuspatial/meta.yaml\nindex c2c60133d..1bc68677f 100644\n--- a/conda/recipes/cuspatial/meta.yaml\n+++ b/conda/recipes/cuspatial/meta.yaml\n@@ -52,7 +52,7 @@ requirements:\n     {% endif %}\n     - cuda-version ={{ cuda_version }}\n     - ninja\n-    - sysroot_{{ target_platform }} {{ sysroot_version }}\n+    - {{ stdlib(\"c\") }}\n   host:\n     {% if cuda_major != \"11\" %}\n     - cuda-cudart-dev\ndiff --git a/conda/recipes/libcuspatial/conda_build_config.yaml b/conda/recipes/libcuspatial/conda_build_config.yaml\nindex 37d54ccc2..51cfd482f 100644\n--- a/conda/recipes/libcuspatial/conda_build_config.yaml\n+++ b/conda/recipes/libcuspatial/conda_build_config.yaml\n@@ -16,5 +16,8 @@ cmake_version:\n gtest_version:\n   - \">=1.13.0\"\n \n-sysroot_version:\n+c_stdlib:\n+  - sysroot\n+\n+c_stdlib_version:\n   - \"2.17\"\ndiff --git a/conda/recipes/libcuspatial/meta.yaml b/conda/recipes/libcuspatial/meta.yaml\nindex 659aa511c..3e212690c 100644\n--- a/conda/recipes/libcuspatial/meta.yaml\n+++ b/conda/recipes/libcuspatial/meta.yaml\n@@ -1,4 +1,4 @@\n-# Copyright (c) 2018-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2018-2024, NVIDIA CORPORATION.\n \n {% set version = environ['RAPIDS_PACKAGE_VERSION'].lstrip('v') %}\n {% set minor_version = version.split('.')[0] + '.' + version.split('.')[1] %}\n@@ -42,12 +42,10 @@ requirements:\n     - cuda-version ={{ cuda_version }}\n     - cmake {{ cmake_version }}\n     - ninja\n-    - sysroot_{{ target_platform }} {{ sysroot_version }}\n+    - {{ stdlib(\"c\") }}\n   host:\n     - cuda-version ={{ cuda_version }}\n     - doxygen\n-    - gmock {{ gtest_version }}\n-    - gtest {{ gtest_version }}\n     - libcudf ={{ minor_version }}\n     - librmm ={{ minor_version }}\n     - sqlite\n@@ -129,5 +127,3 @@ outputs:\n         - cuda-cudart\n         {% endif %}\n         - {{ pin_compatible('cuda-version', max_pin='x', min_pin='x') }}\n-        - gmock {{ gtest_version }}\n-        - gtest {{ gtest_version }}\ndiff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt\nindex 1de36d515..8908617b1 100644\n--- a/cpp/CMakeLists.txt\n+++ b/cpp/CMakeLists.txt\n@@ -107,7 +107,8 @@ rapids_cpm_init()\n include(cmake/thirdparty/get_cudf.cmake)\n # find or install GoogleTest\n if (CUSPATIAL_BUILD_TESTS)\n-    include(cmake/thirdparty/get_gtest.cmake)\n+  include(${rapids-cmake-dir}/cpm/gtest.cmake)\n+  rapids_cpm_gtest(BUILD_STATIC)\n endif()\n # find or add ranger\n include (cmake/thirdparty/get_ranger.cmake)\n@@ -222,7 +223,7 @@ endif()\n if(CUSPATIAL_BUILD_BENCHMARKS)\n     # Find or install GoogleBench\n     include(${rapids-cmake-dir}/cpm/gbench.cmake)\n-    rapids_cpm_gbench()\n+    rapids_cpm_gbench(BUILD_STATIC)\n \n     # Find or install NVBench Temporarily force downloading of fmt because current versions of nvbench\n     # do not support the latest version of fmt, which is automatically pulled into our conda\ndiff --git a/cpp/benchmarks/CMakeLists.txt b/cpp/benchmarks/CMakeLists.txt\nindex 99780a677..10e626f3f 100644\n--- a/cpp/benchmarks/CMakeLists.txt\n+++ b/cpp/benchmarks/CMakeLists.txt\n@@ -1,5 +1,5 @@\n #=============================================================================\n-# Copyright (c) 2019-2021, NVIDIA CORPORATION.\n+# Copyright (c) 2019-2024, NVIDIA CORPORATION.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -23,6 +23,17 @@ add_library(cuspatial_benchmark_common OBJECT\n \n target_compile_features(cuspatial_benchmark_common PUBLIC cxx_std_17 cuda_std_17)\n \n+set_target_properties(cuspatial_benchmark_common\n+    PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n+               INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n+               CXX_STANDARD                        17\n+               CXX_STANDARD_REQUIRED               ON\n+               CUDA_STANDARD                       17\n+               CUDA_STANDARD_REQUIRED              ON\n+               POSITION_INDEPENDENT_CODE           ON\n+               INTERFACE_POSITION_INDEPENDENT_CODE ON\n+)\n+\n target_link_libraries(cuspatial_benchmark_common\n     PUBLIC benchmark::benchmark\n            cudf::cudftestutil\n@@ -43,6 +54,10 @@ function(ConfigureBench CMAKE_BENCH_NAME)\n     set_target_properties(${CMAKE_BENCH_NAME}\n         PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n                    INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n+                   CXX_STANDARD 17\n+                   CXX_STANDARD_REQUIRED ON\n+                   CUDA_STANDARD 17\n+                   CUDA_STANDARD_REQUIRED ON\n         )\n     target_link_libraries(${CMAKE_BENCH_NAME} PRIVATE benchmark::benchmark_main cuspatial_benchmark_common)\n     install(\n@@ -61,7 +76,11 @@ function(ConfigureNVBench CMAKE_BENCH_NAME)\n     ${CMAKE_BENCH_NAME}\n     PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n                INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n-  )\n+               CXX_STANDARD 17\n+               CXX_STANDARD_REQUIRED ON\n+               CUDA_STANDARD 17\n+               CUDA_STANDARD_REQUIRED ON\n+)\n   target_link_libraries(\n     ${CMAKE_BENCH_NAME} PRIVATE cuspatial_benchmark_common nvbench::main\n   )\ndiff --git a/cpp/cuproj/CMakeLists.txt b/cpp/cuproj/CMakeLists.txt\nindex 35ba0277e..061a627ca 100644\n--- a/cpp/cuproj/CMakeLists.txt\n+++ b/cpp/cuproj/CMakeLists.txt\n@@ -95,8 +95,9 @@ include(cmake/thirdparty/get_rmm.cmake)\n \n # find or install GoogleTest and Proj\n if (CUPROJ_BUILD_TESTS)\n-    include(cmake/thirdparty/get_gtest.cmake)\n-    include(cmake/thirdparty/get_proj.cmake)\n+  include(${rapids-cmake-dir}/cpm/gtest.cmake)\n+  rapids_cpm_gtest(BUILD_STATIC)\n+  include(cmake/thirdparty/get_proj.cmake)\n endif()\n \n ###################################################################################################\n@@ -162,7 +163,7 @@ endif()\n if(CUPROJ_BUILD_BENCHMARKS)\n     # Find or install GoogleBench\n     include(${rapids-cmake-dir}/cpm/gbench.cmake)\n-    rapids_cpm_gbench()\n+    rapids_cpm_gbench(BUILD_STATIC)\n \n     # Find or install NVBench Temporarily force downloading of fmt because current versions of nvbench\n     # do not support the latest version of fmt, which is automatically pulled into our conda\ndiff --git a/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md b/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md\nindex 74db9350f..325c9ce79 100644\n--- a/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md\n+++ b/cpp/doxygen/developer_guide/DEVELOPER_GUIDE.md\n@@ -84,7 +84,7 @@ Examples:\n \n ```c++\n template <typename IteratorType>\n-void algorithm_function(int x, rmm::cuda_stream_view s, rmm::device_memory_resource* mr)\n+void algorithm_function(int x, rmm::cuda_stream_view s, rmm::device_async_resource_ref mr)\n {\n   ...\n }\n@@ -233,9 +233,10 @@ std::unique_ptr<cudf::table> points_in_spatial_window(\n   cudf::column_view const& y);\n ```\n \n-## RMM Memory Resources (`rmm::device_memory_resource`)\n+## Memory Resources (`rmm::device_memory_resource`)\n \n-libcuspatial allocates all device memory via RMM memory resources (MR). See the\n+libcuspatial allocates all device memory via RMM memory resources (MR) or CUDA MRs. Either type\n+can be passed to libcuspatial functions via `rmm::device_async_resource_ref` parameters. See the\n [RMM documentation](https://github.com/rapidsai/rmm/blob/main/README.md) for details.\n \n ### Current Device Memory Resource\n@@ -245,6 +246,27 @@ RMM provides a \"default\" memory resource for each device that can be accessed an\n respectively. All memory resource parameters should be defaulted to use the return value of\n `rmm::mr::get_current_device_resource()`.\n \n+### Resource Refs\n+\n+Memory resources are passed via resource ref parameters. A resource ref is memory resource wrapper\n+that enables consumers to specify properties of resources that they expect. These are defined\n+in the `cuda::mr` namespace of libcu++, but RMM provides some convenience wrappers in\n+`rmm/resource_ref.hpp`:\n+ - `rmm::device_resource_ref` accepts a memory resource that provides synchronous allocation\n+    of device-accessible memory.\n+ - `rmm::device_async_resource_ref` accepts a memory resource that provides stream-ordered allocation\n+    of device-accessible memory.\n+ - `rmm::host_resource_ref` accepts a memory resource that provides synchronous allocation of host- \n+    accessible memory.\n+ - `rmm::host_async_resource_ref` accepts a memory resource that provides stream-ordered allocation\n+    of host-accessible memory.\n+ - `rmm::host_device_resource_ref` accepts a memory resource that provides synchronous allocation of\n+    host- and device-accessible memory.\n+ - `rmm::host_device_async_resource_ref` accepts a memory resource that provides stream-ordered\n+    allocation of host- and device-accessible memory.\n+\n+See the libcu++ [docs on `resource_ref`](https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_resource/resource_ref.html) for more information.\n+\n # libcuspatial API and Implementation\n \n This section provides specifics about the structure and implementation of cuSpatial API functions.\n@@ -439,8 +461,8 @@ There are a few key points to notice.\n      cuSpatial APIs will not need to use this returned iterator.\n   9. All APIs that run CUDA device code (including Thrust algorithms) or allocate memory take a CUDA\n      stream on which to execute the device code and allocate memory.\n-  10. Any API that allocate and return device data (not shown here) should also take an\n-      `rmm::device_memory_resource` to use for output memory allocation.\n+  10. Any API that allocates and returns device data (not shown here) should also take an\n+      `rmm::device_async_resource_ref` to use for output memory allocation.\n \n ### (Multiple) Return Values\n \n@@ -542,22 +564,28 @@ control how device memory is allocated.\n \n ### Output Memory\n \n-Any libcuspatial API that allocates memory that is *returned* to a user must accept a pointer to a\n-`device_memory_resource` as the last parameter. Inside the API, this memory resource must be used\n-to allocate any memory for returned objects. It should therefore be passed into functions whose\n-outputs will be returned. Example:\n+Any libcuspatial API that allocates memory that is *returned* to a user must accept a\n+`rmm::device_async_resource_ref` as the last parameter. Inside the API, this memory resource must\n+be used to allocate any memory for returned objects. It should therefore be passed into functions\n+whose outputs will be returned. Example:\n \n ```c++\n // Returned `column` contains newly allocated memory,\n // therefore the API must accept a memory resource pointer\n std::unique_ptr<column> returns_output_memory(\n-  ..., rmm::device_memory_resource * mr = rmm::mr::get_current_device_resource());\n+  ..., rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n // This API does not allocate any new *output* memory, therefore\n // a memory resource is unnecessary\n void does_not_allocate_output_memory(...);\n ```\n \n+This rule automatically applies to all detail APIs that allocate memory. Any detail API may be\n+called by any public API, and therefore could be allocating memory that is returned to the user.\n+To support such uses cases, all detail APIs allocating memory resources should accept an `mr`\n+parameter. Callers are responsible for either passing through a provided `mr` or\n+`rmm::mr::get_current_device_resource()` as needed.\n+\n ### Temporary Memory\n \n Not all memory allocated within a libcuspatial API is returned to the caller. Often algorithms must\n@@ -566,7 +594,7 @@ obtained from `rmm::mr::get_current_device_resource()` for temporary memory allo\n \n ```c++\n rmm::device_buffer some_function(\n-  ..., rmm::mr::device_memory_resource mr * = rmm::mr::get_current_device_resource()) {\n+  ..., rmm::mr::device_async_resource_ref mr = rmm::mr::get_current_device_resource()) {\n     rmm::device_buffer returned_buffer(..., mr); // Returned buffer uses the passed in MR\n     ...\n     rmm::device_buffer temporary_buffer(...); // Temporary buffer uses default MR\n@@ -578,12 +606,12 @@ rmm::device_buffer some_function(\n ### Memory Management\n \n libcuspatial code eschews raw pointers and direct memory allocation. Use RMM classes built to\n-use [`device_memory_resource`](https://github.com/rapidsai/rmm/#device_memory_resource) for device\n+use [memory resources](https://github.com/rapidsai/rmm/#device_memory_resource) for device\n memory allocation with automated lifetime management.\n \n #### rmm::device_buffer\n Allocates a specified number of bytes of untyped, uninitialized device memory using a\n-`device_memory_resource`. If no resource is explicitly provided, uses\n+memory resource. If no `rmm::device_async_resource_ref` is explicitly provided, uses\n `rmm::mr::get_current_device_resource()`.\n \n `rmm::device_buffer` is movable and copyable on a stream. A copy performs a deep copy of the\ndiff --git a/cpp/include/cuspatial/bounding_boxes.hpp b/cpp/include/cuspatial/bounding_boxes.hpp\nindex 9229eb7c2..2134bc1d1 100644\n--- a/cpp/include/cuspatial/bounding_boxes.hpp\n+++ b/cpp/include/cuspatial/bounding_boxes.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -53,7 +54,7 @@ std::unique_ptr<cudf::table> linestring_bounding_boxes(\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n   double expansion_radius,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute minimum bounding box for each polygon in a list.\n@@ -80,8 +81,8 @@ std::unique_ptr<cudf::table> polygon_bounding_boxes(\n   cudf::column_view const& ring_offsets,\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n-  double expansion_radius             = 0.0,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  double expansion_radius           = 0.0,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/detail/index/construction/phase_1.cuh b/cpp/include/cuspatial/detail/index/construction/phase_1.cuh\nindex 3d06c93b1..a444ae64a 100644\n--- a/cpp/include/cuspatial/detail/index/construction/phase_1.cuh\n+++ b/cpp/include/cuspatial/detail/index/construction/phase_1.cuh\n@@ -24,6 +24,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <cuda/functional>\n #include <thrust/copy.h>\n@@ -65,7 +66,7 @@ compute_point_keys_and_sorted_indices(PointIt points_first,\n                                       T scale,\n                                       int8_t max_depth,\n                                       rmm::cuda_stream_view stream,\n-                                      rmm::mr::device_memory_resource* mr)\n+                                      rmm::device_async_resource_ref mr)\n {\n   auto num_points = thrust::distance(points_first, points_last);\n   rmm::device_uvector<uint32_t> keys(num_points, stream);\n@@ -259,7 +260,7 @@ inline auto make_full_levels(PointIt points_first,\n                              T scale,\n                              int8_t max_depth,\n                              rmm::cuda_stream_view stream,\n-                             rmm::mr::device_memory_resource* mr)\n+                             rmm::device_async_resource_ref mr)\n {\n   auto num_points = thrust::distance(points_first, points_last);\n   // Compute point keys and sort into bottom-level quadrants\ndiff --git a/cpp/include/cuspatial/detail/index/construction/phase_2.cuh b/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\nindex ffbb79c0d..df28ecca1 100644\n--- a/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\n+++ b/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2021, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -21,6 +21,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/copy.h>\n #include <thrust/count.h>\n@@ -84,9 +85,9 @@ inline rmm::device_uvector<uint32_t> flatten_point_keys(\n                     keys_and_levels + num_valid_nodes,\n                     flattened_keys.begin(),\n                     [last_level = max_depth - 1] __device__(auto const& val) {\n-                      bool is_parent{false};\n-                      uint32_t key{}, level{};\n-                      thrust::tie(key, level, is_parent) = val;\n+                      auto& key       = thrust::get<0>(val);\n+                      auto& level     = thrust::get<1>(val);\n+                      auto& is_parent = thrust::get<2>(val);\n                       // if this is a parent node, return max_key. otherwise\n                       // compute the key for one level up the tree. Leaf nodes\n                       // whose keys are zero will be removed in a subsequent\n@@ -309,7 +310,7 @@ inline rmm::device_uvector<bool> construct_non_leaf_indicator(\n   int32_t num_parent_nodes,\n   int32_t num_valid_nodes,\n   int32_t max_size,\n-  rmm::mr::device_memory_resource* mr,\n+  rmm::device_async_resource_ref mr,\n   rmm::cuda_stream_view stream)\n {\n   //\ndiff --git a/cpp/include/cuspatial/detail/intersection/linestring_intersection.cuh b/cpp/include/cuspatial/detail/intersection/linestring_intersection.cuh\nindex 1d88e5e36..818c6560b 100644\n--- a/cpp/include/cuspatial/detail/intersection/linestring_intersection.cuh\n+++ b/cpp/include/cuspatial/detail/intersection/linestring_intersection.cuh\n@@ -34,6 +34,7 @@\n #include <rmm/exec_policy.hpp>\n #include <rmm/mr/device/device_memory_resource.hpp>\n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <cuda/atomic>\n #include <thrust/binary_search.h>\n@@ -142,7 +143,7 @@ std::unique_ptr<rmm::device_uvector<types_t>> compute_types_buffer(\n   OffsetRangeB points_offset,\n   OffsetRangeB segments_offset,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   auto types_buffer = std::make_unique<rmm::device_uvector<types_t>>(union_column_size, stream, mr);\n   thrust::tabulate(rmm::exec_policy(stream),\n@@ -162,7 +163,7 @@ std::unique_ptr<rmm::device_uvector<types_t>> compute_types_buffer(\n template <typename index_t, typename types_t>\n std::unique_ptr<rmm::device_uvector<index_t>> compute_offset_buffer(\n   rmm::device_uvector<types_t> const& types_buffer,\n-  rmm::mr::device_memory_resource* mr,\n+  rmm::device_async_resource_ref mr,\n   rmm::cuda_stream_view stream)\n {\n   auto N                                = types_buffer.size();\n@@ -202,7 +203,7 @@ template <typename T,\n linestring_intersection_result<T, index_t> pairwise_linestring_intersection(\n   MultiLinestringRange1 multilinestrings1,\n   MultiLinestringRange2 multilinestrings2,\n-  rmm::mr::device_memory_resource* mr,\n+  rmm::device_async_resource_ref mr,\n   rmm::cuda_stream_view stream)\n {\n   using types_t = typename linestring_intersection_result<T, index_t>::types_t;\ndiff --git a/cpp/include/cuspatial/detail/intersection/linestring_intersection_with_duplicates.cuh b/cpp/include/cuspatial/detail/intersection/linestring_intersection_with_duplicates.cuh\nindex e513cf651..f1a365555 100644\n--- a/cpp/include/cuspatial/detail/intersection/linestring_intersection_with_duplicates.cuh\n+++ b/cpp/include/cuspatial/detail/intersection/linestring_intersection_with_duplicates.cuh\n@@ -29,6 +29,7 @@\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n #include <rmm/mr/device/device_memory_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/binary_search.h>\n #include <thrust/distance.h>\n@@ -228,7 +229,7 @@ struct linestring_intersection_intermediates {\n   /** @brief Construct a zero-pair, zero-geometry intermediate object\n    */\n   linestring_intersection_intermediates(rmm::cuda_stream_view stream,\n-                                        rmm::mr::device_memory_resource* mr)\n+                                        rmm::device_async_resource_ref mr)\n     : offsets(std::make_unique<rmm::device_uvector<IndexType>>(1, stream)),\n       geoms(std::make_unique<rmm::device_uvector<GeomType>>(0, stream, mr)),\n       lhs_linestring_ids(std::make_unique<rmm::device_uvector<IndexType>>(0, stream)),\n@@ -244,7 +245,7 @@ struct linestring_intersection_intermediates {\n                                         std::size_t num_geoms,\n                                         rmm::device_uvector<IndexType> const& num_geoms_per_pair,\n                                         rmm::cuda_stream_view stream,\n-                                        rmm::mr::device_memory_resource* mr)\n+                                        rmm::device_async_resource_ref mr)\n     : offsets(std::make_unique<rmm::device_uvector<IndexType>>(num_pairs + 1, stream)),\n       geoms(std::make_unique<rmm::device_uvector<GeomType>>(num_geoms, stream, mr)),\n       lhs_linestring_ids(std::make_unique<rmm::device_uvector<IndexType>>(num_geoms, stream)),\n@@ -472,7 +473,7 @@ std::pair<linestring_intersection_intermediates<vec_2d<T>, index_t>,\n           linestring_intersection_intermediates<segment<T>, index_t>>\n pairwise_linestring_intersection_with_duplicates(MultiLinestringRange1 multilinestrings1,\n                                                  MultiLinestringRange2 multilinestrings2,\n-                                                 rmm::mr::device_memory_resource* mr,\n+                                                 rmm::device_async_resource_ref mr,\n                                                  rmm::cuda_stream_view stream)\n {\n   static_assert(std::is_integral_v<index_t>, \"Index type must be integral.\");\ndiff --git a/cpp/include/cuspatial/detail/join/quadtree_bbox_filtering.cuh b/cpp/include/cuspatial/detail/join/quadtree_bbox_filtering.cuh\nindex 43ff7b0f6..472bb29b0 100644\n--- a/cpp/include/cuspatial/detail/join/quadtree_bbox_filtering.cuh\n+++ b/cpp/include/cuspatial/detail/join/quadtree_bbox_filtering.cuh\n@@ -22,6 +22,8 @@\n #include <cuspatial/point_quadtree.cuh>\n #include <cuspatial/traits.hpp>\n \n+#include <rmm/resource_ref.hpp>\n+\n #include <cuda/functional>\n #include <thrust/iterator/discard_iterator.h>\n \n@@ -39,7 +41,7 @@ join_quadtree_and_bounding_boxes(point_quadtree_ref quadtree,\n                                  T scale,\n                                  int8_t max_depth,\n                                  rmm::cuda_stream_view stream,\n-                                 rmm::mr::device_memory_resource* mr)\n+                                 rmm::device_async_resource_ref mr)\n {\n   static_assert(is_same<T, cuspatial::iterator_vec_base_type<BoundingBoxIterator>>(),\n                 \"Iterator value_type mismatch\");\ndiff --git a/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh b/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\nindex 1ae0ff1bf..68a2e9596 100644\n--- a/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\n+++ b/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2023-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -24,13 +24,16 @@\n #include <cuspatial/range/multipolygon_range.cuh>\n #include <cuspatial/traits.hpp>\n \n+#include <rmm/cuda_device.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/permutation_iterator.h>\n+#include <thrust/iterator/transform_iterator.h>\n #include <thrust/scan.h>\n \n-#include <cstdint>\n+#include <limits>\n \n namespace cuspatial {\n namespace detail {\n@@ -56,7 +59,7 @@ struct compute_poly_and_point_indices {\n   using IndexType = iterator_value_type<QuadOffsetsIterator>;\n \n   inline thrust::tuple<IndexType, IndexType> __device__\n-  operator()(IndexType const global_index) const\n+  operator()(std::uint64_t const global_index) const\n   {\n     auto const [quad_poly_index, local_point_index] =\n       get_quad_and_local_point_indices(global_index, point_offsets_begin, point_offsets_end);\n@@ -108,7 +111,7 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n   PointIterator points_first,\n   MultiPolygonRange polygons,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   using T = iterator_vec_base_type<PointIterator>;\n \n@@ -117,16 +120,26 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n \n   auto num_poly_quad_pairs = std::distance(poly_indices_first, poly_indices_last);\n \n-  auto quad_lengths_iter =\n-    thrust::make_permutation_iterator(quadtree.length_begin(), quad_indices_first);\n+  // The quadtree length is an iterator of uint32_t, but we have to transform into uint64_t values\n+  // so the thrust::inclusive_scan accumulates into uint64_t outputs. Changing the output iterator\n+  // to uint64_t isn't sufficient to achieve this behavior.\n+  auto quad_lengths_iter = thrust::make_transform_iterator(\n+    thrust::make_permutation_iterator(quadtree.length_begin(), quad_indices_first),\n+    cuda::proclaim_return_type<std::uint64_t>([] __device__(IndexType const& i) -> std::uint64_t {\n+      return static_cast<std::uint64_t>(i);\n+    }));\n \n   auto quad_offsets_iter =\n     thrust::make_permutation_iterator(quadtree.offset_begin(), quad_indices_first);\n \n-  // Compute a \"local\" set of zero-based point offsets from number of points in each quadrant\n+  // Compute a \"local\" set of zero-based point offsets from the number of points in each quadrant.\n+  //\n   // Use `num_poly_quad_pairs + 1` as the length so that the last element produced by\n   // `inclusive_scan` is the total number of points to be tested against any polygon.\n-  rmm::device_uvector<IndexType> local_point_offsets(num_poly_quad_pairs + 1, stream);\n+  //\n+  // Accumulate into uint64_t, because the prefix sums can overflow the size of uint32_t\n+  // when testing a large number of polygons against a large quadtree.\n+  rmm::device_uvector<std::uint64_t> local_point_offsets(num_poly_quad_pairs + 1, stream);\n \n   // inclusive scan of quad_lengths_iter\n   thrust::inclusive_scan(rmm::exec_policy(stream),\n@@ -135,21 +148,27 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n                          local_point_offsets.begin() + 1);\n \n   // Ensure local point offsets starts at 0\n-  IndexType init{0};\n+  std::uint64_t init{0};\n   local_point_offsets.set_element_async(0, init, stream);\n \n   // The last element is the total number of points to test against any polygon.\n   auto num_total_points = local_point_offsets.back_element(stream);\n \n-  // Allocate the output polygon and point index pair vectors\n-  rmm::device_uvector<IndexType> poly_indices(num_total_points, stream);\n-  rmm::device_uvector<IndexType> point_indices(num_total_points, stream);\n-\n-  auto poly_and_point_indices =\n-    thrust::make_zip_iterator(poly_indices.begin(), point_indices.begin());\n-\n-  // Enumerate the point X/Ys using the sorted `point_indices` (from quadtree construction)\n-  auto point_xys_iter = thrust::make_permutation_iterator(points_first, point_indices_first);\n+  // The largest supported input size for thrust::count_if/copy_if is INT32_MAX.\n+  // This functor iterates over the input space and processes up to INT32_MAX elements at a time.\n+  std::uint64_t max_points_to_test = std::numeric_limits<std::int32_t>::max();\n+  auto count_in_chunks             = [&](auto const& func) {\n+    std::uint64_t memo{};\n+    for (std::uint64_t offset{0}; offset < num_total_points; offset += max_points_to_test) {\n+      memo += func(memo, offset, std::min(max_points_to_test, num_total_points - offset));\n+    }\n+    return memo;\n+  };\n+\n+  detail::test_poly_point_intersection test_poly_point_pair{\n+    // Enumerate the point X/Ys using the sorted `point_indices` (from quadtree construction)\n+    thrust::make_permutation_iterator(points_first, point_indices_first),\n+    polygons};\n \n   // Compute the combination of polygon and point index pairs. For each polygon/quadrant pair,\n   // enumerate pairs of (poly_index, point_index) for each point in each quadrant.\n@@ -162,28 +181,57 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n   //     pp_pairs.append((polygon, point))\n   // ```\n   //\n-  auto global_to_poly_and_point_indices = detail::make_counting_transform_iterator(\n-    0,\n-    detail::compute_poly_and_point_indices{quad_offsets_iter,\n-                                           local_point_offsets.begin(),\n-                                           local_point_offsets.end(),\n-                                           poly_indices_first});\n-\n-  // Compute the number of intersections by removing (poly, point) pairs that don't intersect\n-  auto num_intersections = thrust::distance(\n-    poly_and_point_indices,\n-    thrust::copy_if(rmm::exec_policy(stream),\n-                    global_to_poly_and_point_indices,\n-                    global_to_poly_and_point_indices + num_total_points,\n-                    poly_and_point_indices,\n-                    detail::test_poly_point_intersection{point_xys_iter, polygons}));\n-\n-  poly_indices.resize(num_intersections, stream);\n-  poly_indices.shrink_to_fit(stream);\n-  point_indices.resize(num_intersections, stream);\n-  point_indices.shrink_to_fit(stream);\n-\n-  return std::pair{std::move(poly_indices), std::move(point_indices)};\n+  auto global_to_poly_and_point_indices = [&](auto offset = 0) {\n+    return detail::make_counting_transform_iterator(\n+      offset,\n+      detail::compute_poly_and_point_indices{quad_offsets_iter,\n+                                             local_point_offsets.begin(),\n+                                             local_point_offsets.end(),\n+                                             poly_indices_first});\n+  };\n+\n+  auto run_quadtree_point_in_polygon = [&](auto output_size) {\n+    // Allocate the output polygon and point index pair vectors\n+    rmm::device_uvector<IndexType> poly_indices(output_size, stream);\n+    rmm::device_uvector<IndexType> point_indices(output_size, stream);\n+\n+    auto num_intersections = count_in_chunks([&](auto memo, auto offset, auto size) {\n+      auto poly_and_point_indices =\n+        thrust::make_zip_iterator(poly_indices.begin(), point_indices.begin()) + memo;\n+      // Remove (poly, point) pairs that don't intersect\n+      return thrust::distance(poly_and_point_indices,\n+                              thrust::copy_if(rmm::exec_policy(stream),\n+                                              global_to_poly_and_point_indices(offset),\n+                                              global_to_poly_and_point_indices(offset) + size,\n+                                              poly_and_point_indices,\n+                                              test_poly_point_pair));\n+    });\n+\n+    if (num_intersections < output_size) {\n+      poly_indices.resize(num_intersections, stream);\n+      point_indices.resize(num_intersections, stream);\n+      poly_indices.shrink_to_fit(stream);\n+      point_indices.shrink_to_fit(stream);\n+    }\n+\n+    return std::pair{std::move(poly_indices), std::move(point_indices)};\n+  };\n+\n+  try {\n+    // First attempt to run the hit test assuming allocating space for all possible intersections\n+    // fits into the available memory.\n+    return run_quadtree_point_in_polygon(num_total_points);\n+  } catch (rmm::out_of_memory const&) {\n+    // If we OOM the first time, pre-compute the number of hits and allocate only that amount of\n+    // space for the output buffers. This halves performance, but it should at least return valid\n+    // results.\n+    return run_quadtree_point_in_polygon(count_in_chunks([&](auto memo, auto offset, auto size) {\n+      return thrust::count_if(rmm::exec_policy(stream),\n+                              global_to_poly_and_point_indices(offset),\n+                              global_to_poly_and_point_indices(offset) + size,\n+                              test_poly_point_pair);\n+    }));\n+  }\n }\n \n }  // namespace cuspatial\ndiff --git a/cpp/include/cuspatial/detail/join/quadtree_point_to_nearest_linestring.cuh b/cpp/include/cuspatial/detail/join/quadtree_point_to_nearest_linestring.cuh\nindex 6335f133f..5b61882b6 100644\n--- a/cpp/include/cuspatial/detail/join/quadtree_point_to_nearest_linestring.cuh\n+++ b/cpp/include/cuspatial/detail/join/quadtree_point_to_nearest_linestring.cuh\n@@ -27,6 +27,7 @@\n \n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <cuda/functional>\n #include <thrust/detail/raw_reference_cast.h>\n@@ -156,7 +157,7 @@ quadtree_point_to_nearest_linestring(LinestringIndexIterator linestring_indices_\n                                      PointIterator points_first,\n                                      MultiLinestringRange linestrings,\n                                      rmm::cuda_stream_view stream,\n-                                     rmm::mr::device_memory_resource* mr)\n+                                     rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(linestrings.num_multilinestrings() == linestrings.num_linestrings(),\n                     \"Only one linestring per multilinestring currently supported.\");\ndiff --git a/cpp/include/cuspatial/detail/point_quadtree.cuh b/cpp/include/cuspatial/detail/point_quadtree.cuh\nindex 7e0d70dac..19922af35 100644\n--- a/cpp/include/cuspatial/detail/point_quadtree.cuh\n+++ b/cpp/include/cuspatial/detail/point_quadtree.cuh\n@@ -24,6 +24,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <cuda/functional>\n #include <thrust/distance.h>\n@@ -48,7 +49,7 @@ inline point_quadtree make_quad_tree(rmm::device_uvector<uint32_t>& keys,\n                                      int32_t max_size,\n                                      int32_t level_1_size,\n                                      rmm::cuda_stream_view stream,\n-                                     rmm::mr::device_memory_resource* mr)\n+                                     rmm::device_async_resource_ref mr)\n {\n   // count the number of child nodes\n   auto num_child_nodes = thrust::reduce(rmm::exec_policy(stream),\n@@ -155,7 +156,7 @@ inline point_quadtree make_leaf_tree(rmm::device_uvector<uint32_t>& keys,\n                                      rmm::device_uvector<uint32_t>& lengths,\n                                      int32_t num_top_quads,\n                                      rmm::cuda_stream_view stream,\n-                                     rmm::mr::device_memory_resource* mr)\n+                                     rmm::device_async_resource_ref mr)\n {\n   rmm::device_uvector<uint8_t> levels(num_top_quads, stream, mr);\n   rmm::device_uvector<bool> is_internal_node(num_top_quads, stream, mr);\n@@ -195,7 +196,7 @@ inline std::pair<rmm::device_uvector<uint32_t>, point_quadtree> construct_quadtr\n   T scale,\n   int8_t max_depth,\n   int32_t max_size,\n-  rmm::mr::device_memory_resource* mr,\n+  rmm::device_async_resource_ref mr,\n   rmm::cuda_stream_view stream)\n {\n   // Construct the full set of non-empty subquadrants starting from the lowest level.\n@@ -243,7 +244,7 @@ std::pair<rmm::device_uvector<uint32_t>, point_quadtree> quadtree_on_points(\n   int8_t max_depth,\n   int32_t max_size,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   auto num_points = thrust::distance(points_first, points_last);\n   if (num_points <= 0) {\ndiff --git a/cpp/include/cuspatial/detail/range/multilinestring_range.cuh b/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\nindex b9b53bfc0..03ad0fe27 100644\n--- a/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\n+++ b/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -74,6 +74,7 @@ template <typename GeometryIterator, typename PartIterator, typename VecIterator\n class multilinestring_range;\n \n template <typename GeometryIterator, typename PartIterator, typename VecIterator>\n+CUSPATIAL_HOST_DEVICE\n multilinestring_range<GeometryIterator, PartIterator, VecIterator>::multilinestring_range(\n   GeometryIterator geometry_begin,\n   GeometryIterator geometry_end,\ndiff --git a/cpp/include/cuspatial/detail/trajectory/derive_trajectories.cuh b/cpp/include/cuspatial/detail/trajectory/derive_trajectories.cuh\nindex 6da914181..11b8a594d 100644\n--- a/cpp/include/cuspatial/detail/trajectory/derive_trajectories.cuh\n+++ b/cpp/include/cuspatial/detail/trajectory/derive_trajectories.cuh\n@@ -22,6 +22,7 @@\n #include <rmm/device_uvector.hpp>\n #include <rmm/device_vector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <cub/device/device_merge_sort.cuh>\n #include <thrust/gather.h>\n@@ -63,7 +64,7 @@ void order_trajectories(IdInputIt ids_first,\n                         PointOutputIt points_out_first,\n                         TimestampOutputIt timestamps_out_first,\n                         rmm::cuda_stream_view stream,\n-                        rmm::mr::device_memory_resource* mr)\n+                        rmm::device_async_resource_ref mr)\n {\n   using id_type        = iterator_value_type<IdInputIt>;\n   using timestamp_type = iterator_value_type<TimestampInputIt>;\n@@ -116,7 +117,7 @@ std::unique_ptr<rmm::device_uvector<OffsetType>> derive_trajectories(\n   PointOutputIt points_out_first,\n   TimestampOutputIt timestamps_out_first,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   detail::order_trajectories(ids_first,\n                              ids_last,\ndiff --git a/cpp/include/cuspatial/distance.hpp b/cpp/include/cuspatial/distance.hpp\nindex 11cec4fae..76c920b11 100644\n--- a/cpp/include/cuspatial/distance.hpp\n+++ b/cpp/include/cuspatial/distance.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -25,6 +25,7 @@\n #include <cudf/utilities/span.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <optional>\n \n@@ -53,8 +54,8 @@ std::unique_ptr<cudf::column> haversine_distance(\n   cudf::column_view const& a_lat,\n   cudf::column_view const& b_lon,\n   cudf::column_view const& b_lat,\n-  double const radius                 = EARTH_RADIUS_KM,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  double const radius               = EARTH_RADIUS_KM,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief computes Hausdorff distances for all pairs in a collection of spaces\n@@ -124,7 +125,7 @@ std::pair<std::unique_ptr<cudf::column>, cudf::table_view> directed_hausdorff_di\n   cudf::column_view const& xs,\n   cudf::column_view const& ys,\n   cudf::column_view const& space_offsets,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)point-to-(multi)point Cartesian distance\n@@ -144,7 +145,7 @@ std::pair<std::unique_ptr<cudf::column>, cudf::table_view> directed_hausdorff_di\n std::unique_ptr<cudf::column> pairwise_point_distance(\n   geometry_column_view const& multipoints1,\n   geometry_column_view const& multipoints2,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)points-to-(multi)linestrings Cartesian distance\n@@ -166,7 +167,7 @@ std::unique_ptr<cudf::column> pairwise_point_distance(\n std::unique_ptr<cudf::column> pairwise_point_linestring_distance(\n   geometry_column_view const& multipoints,\n   geometry_column_view const& multilinestrings,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)point-to-(multi)polygon Cartesian distance\n@@ -190,7 +191,7 @@ std::unique_ptr<cudf::column> pairwise_point_linestring_distance(\n std::unique_ptr<cudf::column> pairwise_point_polygon_distance(\n   geometry_column_view const& multipoints,\n   geometry_column_view const& multipolygons,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)linestring-to-(multi)linestring Cartesian distance\n@@ -212,7 +213,7 @@ std::unique_ptr<cudf::column> pairwise_point_polygon_distance(\n std::unique_ptr<cudf::column> pairwise_linestring_distance(\n   geometry_column_view const& multilinestrings1,\n   geometry_column_view const& multilinestrings2,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)linestring-to-(multi)polygon Cartesian distance\n@@ -237,7 +238,7 @@ std::unique_ptr<cudf::column> pairwise_linestring_distance(\n std::unique_ptr<cudf::column> pairwise_linestring_polygon_distance(\n   geometry_column_view const& multilinestrings,\n   geometry_column_view const& multipolygons,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute pairwise (multi)polygon-to-(multi)polygon Cartesian distance\n@@ -256,7 +257,7 @@ std::unique_ptr<cudf::column> pairwise_linestring_polygon_distance(\n std::unique_ptr<cudf::column> pairwise_polygon_distance(\n   geometry_column_view const& multipolygons1,\n   geometry_column_view const& multipolygons2,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/geometry/box.hpp b/cpp/include/cuspatial/geometry/box.hpp\nindex 1041c4de2..4a9f97639 100644\n--- a/cpp/include/cuspatial/geometry/box.hpp\n+++ b/cpp/include/cuspatial/geometry/box.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -40,9 +40,9 @@ class alignas(sizeof(Vertex)) box {\n \n  private:\n   /**\n-   * @brief Output stream operator for `vec_2d<T>` for human-readable formatting\n+   * @brief Output stream operator for `box<T>` for human-readable formatting\n    */\n-  friend std::ostream& operator<<(std::ostream& os, cuspatial::box<T> const& b)\n+  friend std::ostream& operator<<(std::ostream& os, cuspatial::box<T, Vertex> const& b)\n   {\n     return os << \"{\" << b.v1 << \", \" << b.v2 << \"}\";\n   }\ndiff --git a/cpp/include/cuspatial/intersection.cuh b/cpp/include/cuspatial/intersection.cuh\nindex 331e6eb0a..c086d84cc 100644\n--- a/cpp/include/cuspatial/intersection.cuh\n+++ b/cpp/include/cuspatial/intersection.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -22,6 +22,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/pair.h>\n \n@@ -87,8 +88,8 @@ template <typename T,\n linestring_intersection_result<T, index_t> pairwise_linestring_intersection(\n   MultiLinestringRange1 multilinestrings1,\n   MultiLinestringRange2 multilinestrings2,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource(),\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default);\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource(),\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default);\n \n }  // namespace cuspatial\n \ndiff --git a/cpp/include/cuspatial/intersection.hpp b/cpp/include/cuspatial/intersection.hpp\nindex aedaeb521..1869768b1 100644\n--- a/cpp/include/cuspatial/intersection.hpp\n+++ b/cpp/include/cuspatial/intersection.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2023-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -21,6 +21,7 @@\n #include <cudf/column/column.hpp>\n \n #include <rmm/mr/device/device_memory_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n namespace cuspatial {\n /**\n@@ -54,6 +55,6 @@ struct linestring_intersection_column_result {\n linestring_intersection_column_result pairwise_linestring_intersection(\n   geometry_column_view const& multilinestrings_lhs,\n   geometry_column_view const& multilinestrings_rhs,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n }  // namespace cuspatial\ndiff --git a/cpp/include/cuspatial/nearest_points.hpp b/cpp/include/cuspatial/nearest_points.hpp\nindex 640a1c9b6..46f0c14a2 100644\n--- a/cpp/include/cuspatial/nearest_points.hpp\n+++ b/cpp/include/cuspatial/nearest_points.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,8 @@\n #include <cudf/column/column_view.hpp>\n #include <cudf/utilities/span.hpp>\n \n+#include <rmm/resource_ref.hpp>\n+\n #include <optional>\n \n namespace cuspatial {\n@@ -165,7 +167,7 @@ point_linestring_nearest_points_result pairwise_point_linestring_nearest_points(\n   std::optional<cudf::device_span<cudf::size_type const>> multilinestring_geometry_offsets,\n   cudf::device_span<cudf::size_type const> linestring_part_offsets,\n   cudf::column_view linestring_points_xy,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/pairwise_multipoint_equals_count.hpp b/cpp/include/cuspatial/pairwise_multipoint_equals_count.hpp\nindex 9bfb13700..0df60c269 100644\n--- a/cpp/include/cuspatial/pairwise_multipoint_equals_count.hpp\n+++ b/cpp/include/cuspatial/pairwise_multipoint_equals_count.hpp\n@@ -21,6 +21,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -69,6 +70,6 @@ namespace cuspatial {\n std::unique_ptr<cudf::column> pairwise_multipoint_equals_count(\n   geometry_column_view const& lhs,\n   geometry_column_view const& rhs,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n }  // namespace cuspatial\ndiff --git a/cpp/include/cuspatial/point_in_polygon.hpp b/cpp/include/cuspatial/point_in_polygon.hpp\nindex 11e7381c3..c66e2e260 100644\n--- a/cpp/include/cuspatial/point_in_polygon.hpp\n+++ b/cpp/include/cuspatial/point_in_polygon.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -21,6 +21,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -78,7 +79,7 @@ std::unique_ptr<cudf::column> point_in_polygon(\n   cudf::column_view const& poly_ring_offsets,\n   cudf::column_view const& poly_points_x,\n   cudf::column_view const& poly_points_y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Given (point, polygon pairs), tests whether the point of each pair is inside the polygon\n@@ -127,7 +128,7 @@ std::unique_ptr<cudf::column> pairwise_point_in_polygon(\n   cudf::column_view const& poly_ring_offsets,\n   cudf::column_view const& poly_points_x,\n   cudf::column_view const& poly_points_y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/point_quadtree.cuh b/cpp/include/cuspatial/point_quadtree.cuh\nindex a84b07558..f64cee6f0 100644\n--- a/cpp/include/cuspatial/point_quadtree.cuh\n+++ b/cpp/include/cuspatial/point_quadtree.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -22,6 +22,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n #include <thrust/tuple.h>\n@@ -175,8 +176,8 @@ std::pair<rmm::device_uvector<uint32_t>, point_quadtree> quadtree_on_points(\n   T scale,\n   int8_t max_depth,\n   int32_t max_size,\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/point_quadtree.hpp b/cpp/include/cuspatial/point_quadtree.hpp\nindex 17e156ea6..72306730f 100644\n--- a/cpp/include/cuspatial/point_quadtree.hpp\n+++ b/cpp/include/cuspatial/point_quadtree.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -74,7 +75,7 @@ std::pair<std::unique_ptr<cudf::column>, std::unique_ptr<cudf::table>> quadtree_\n   double scale,\n   int8_t max_depth,\n   cudf::size_type max_size,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/points_in_range.hpp b/cpp/include/cuspatial/points_in_range.hpp\nindex 7bd23c2c0..8637e2b24 100644\n--- a/cpp/include/cuspatial/points_in_range.hpp\n+++ b/cpp/include/cuspatial/points_in_range.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -21,6 +21,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -60,7 +61,7 @@ std::unique_ptr<cudf::table> points_in_range(\n   double range_max_y,\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/projection.hpp b/cpp/include/cuspatial/projection.hpp\nindex 1158d8c7c..e71164148 100644\n--- a/cpp/include/cuspatial/projection.hpp\n+++ b/cpp/include/cuspatial/projection.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2019-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2019-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -50,7 +51,7 @@ std::pair<std::unique_ptr<cudf::column>, std::unique_ptr<cudf::column>> sinusoid\n   double origin_lat,\n   cudf::column_view const& input_lon,\n   cudf::column_view const& input_lat,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/spatial_join.cuh b/cpp/include/cuspatial/spatial_join.cuh\nindex 9815d5f4f..096af5fb2 100644\n--- a/cpp/include/cuspatial/spatial_join.cuh\n+++ b/cpp/include/cuspatial/spatial_join.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -23,6 +23,7 @@\n #include <cuspatial/traits.hpp>\n \n #include <rmm/device_uvector.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <iterator>\n #include <utility>\n@@ -69,8 +70,8 @@ join_quadtree_and_bounding_boxes(\n   vec_2d<T> const& v_min,\n   T scale,\n   int8_t max_depth,\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Test whether the specified points are inside any of the specified polygons.\n@@ -127,8 +128,8 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n   PointIndexIterator point_indices_last,\n   PointIterator points_first,\n   MultiPolygonRange polygons,\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Finds the nearest linestring to each point in a quadrant, and computes the distances\n@@ -184,8 +185,8 @@ quadtree_point_to_nearest_linestring(\n   PointIndexIterator point_indices_last,\n   PointIterator points_first,\n   MultiLinestringRange linestrings,\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n }  // namespace cuspatial\n \ndiff --git a/cpp/include/cuspatial/spatial_join.hpp b/cpp/include/cuspatial/spatial_join.hpp\nindex 579a3c4f1..e3bec92d8 100644\n--- a/cpp/include/cuspatial/spatial_join.hpp\n+++ b/cpp/include/cuspatial/spatial_join.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -71,7 +72,7 @@ std::unique_ptr<cudf::table> join_quadtree_and_bounding_boxes(\n   double y_max,\n   double scale,\n   int8_t max_depth,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Test whether the specified points are inside any of the specified polygons.\n@@ -122,7 +123,7 @@ std::unique_ptr<cudf::table> quadtree_point_in_polygon(\n   cudf::column_view const& ring_offsets,\n   cudf::column_view const& poly_points_x,\n   cudf::column_view const& poly_points_y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Finds the nearest linestring to each point in a quadrant, and computes the distances\n@@ -171,7 +172,7 @@ std::unique_ptr<cudf::table> quadtree_point_to_nearest_linestring(\n   cudf::column_view const& linestring_offsets,\n   cudf::column_view const& linestring_points_x,\n   cudf::column_view const& linestring_points_y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/include/cuspatial/trajectory.cuh b/cpp/include/cuspatial/trajectory.cuh\nindex f6c5d76f4..dc5ec9708 100644\n--- a/cpp/include/cuspatial/trajectory.cuh\n+++ b/cpp/include/cuspatial/trajectory.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -19,6 +19,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/mr/device/device_memory_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <iterator>\n #include <memory>\n@@ -85,8 +86,8 @@ std::unique_ptr<rmm::device_uvector<OffsetType>> derive_trajectories(\n   IdOutputIt ids_output_first,\n   PointOutputIt points_output_first,\n   TimestampOutputIt timestamps_output_first,\n-  rmm::cuda_stream_view stream        = rmm::cuda_stream_default,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::cuda_stream_view stream      = rmm::cuda_stream_default,\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute the total distance (in meters) and average speed (in m/s) of objects in\ndiff --git a/cpp/include/cuspatial/trajectory.hpp b/cpp/include/cuspatial/trajectory.hpp\nindex 5eb5eaf0d..308063569 100644\n--- a/cpp/include/cuspatial/trajectory.hpp\n+++ b/cpp/include/cuspatial/trajectory.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -22,6 +22,7 @@\n #include <cudf/types.hpp>\n \n #include <rmm/mr/device/per_device_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -62,7 +63,7 @@ std::pair<std::unique_ptr<cudf::table>, std::unique_ptr<cudf::column>> derive_tr\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n   cudf::column_view const& timestamp,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute the distance and speed of objects in a trajectory. Groups the\n@@ -95,7 +96,7 @@ std::unique_ptr<cudf::table> trajectory_distances_and_speeds(\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n   cudf::column_view const& timestamp,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @brief Compute the spatial bounding boxes of trajectories. Groups the x, y,\n@@ -127,7 +128,7 @@ std::unique_ptr<cudf::table> trajectory_bounding_boxes(\n   cudf::column_view const& object_id,\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n-  rmm::mr::device_memory_resource* mr = rmm::mr::get_current_device_resource());\n+  rmm::device_async_resource_ref mr = rmm::mr::get_current_device_resource());\n \n /**\n  * @} // end of doxygen group\ndiff --git a/cpp/src/bounding_boxes/linestring_bounding_boxes.cu b/cpp/src/bounding_boxes/linestring_bounding_boxes.cu\nindex 86b6aeb75..7716d231c 100644\n--- a/cpp/src/bounding_boxes/linestring_bounding_boxes.cu\n+++ b/cpp/src/bounding_boxes/linestring_bounding_boxes.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -26,6 +26,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n \n@@ -43,7 +44,7 @@ std::unique_ptr<cudf::table> compute_linestring_bounding_boxes(\n   cudf::column_view const& y,\n   T expansion_radius,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   auto num_linestrings = linestring_offsets.size() > 0 ? linestring_offsets.size() - 1 : 0;\n \n@@ -93,7 +94,7 @@ struct dispatch_compute_linestring_bounding_boxes {\n              cudf::column_view const& y,\n              double expansion_radius,\n              rmm::cuda_stream_view stream,\n-             rmm::mr::device_memory_resource* mr)\n+             rmm::device_async_resource_ref mr)\n   {\n     return compute_linestring_bounding_boxes<T>(\n       linestring_offsets, x, y, static_cast<T>(expansion_radius), stream, mr);\n@@ -109,7 +110,7 @@ std::unique_ptr<cudf::table> linestring_bounding_boxes(cudf::column_view const&\n                                                        cudf::column_view const& y,\n                                                        double expansion_radius,\n                                                        rmm::cuda_stream_view stream,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(x.type(),\n                                dispatch_compute_linestring_bounding_boxes{},\n@@ -127,7 +128,7 @@ std::unique_ptr<cudf::table> linestring_bounding_boxes(cudf::column_view const&\n                                                        cudf::column_view const& x,\n                                                        cudf::column_view const& y,\n                                                        double expansion_radius,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   auto num_linestrings = linestring_offsets.size() > 0 ? linestring_offsets.size() - 1 : 0;\n \ndiff --git a/cpp/src/bounding_boxes/polygon_bounding_boxes.cu b/cpp/src/bounding_boxes/polygon_bounding_boxes.cu\nindex e38e12dfd..c4c092e55 100644\n--- a/cpp/src/bounding_boxes/polygon_bounding_boxes.cu\n+++ b/cpp/src/bounding_boxes/polygon_bounding_boxes.cu\n@@ -27,6 +27,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n \n@@ -43,7 +44,7 @@ std::unique_ptr<cudf::table> compute_polygon_bounding_boxes(cudf::column_view co\n                                                             cudf::column_view const& y,\n                                                             T expansion_radius,\n                                                             rmm::cuda_stream_view stream,\n-                                                            rmm::mr::device_memory_resource* mr)\n+                                                            rmm::device_async_resource_ref mr)\n {\n   auto num_polygons = poly_offsets.size() > 0 ? poly_offsets.size() - 1 : 0;\n \n@@ -96,7 +97,7 @@ struct dispatch_compute_polygon_bounding_boxes {\n              cudf::column_view const& y,\n              T expansion_radius,\n              rmm::cuda_stream_view stream,\n-             rmm::mr::device_memory_resource* mr)\n+             rmm::device_async_resource_ref mr)\n   {\n     return compute_polygon_bounding_boxes<T>(\n       poly_offsets, ring_offsets, x, y, expansion_radius, stream, mr);\n@@ -113,7 +114,7 @@ std::unique_ptr<cudf::table> polygon_bounding_boxes(cudf::column_view const& pol\n                                                     cudf::column_view const& y,\n                                                     double expansion_radius,\n                                                     rmm::cuda_stream_view stream,\n-                                                    rmm::mr::device_memory_resource* mr)\n+                                                    rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(x.type(),\n                                dispatch_compute_polygon_bounding_boxes{},\n@@ -133,7 +134,7 @@ std::unique_ptr<cudf::table> polygon_bounding_boxes(cudf::column_view const& pol\n                                                     cudf::column_view const& x,\n                                                     cudf::column_view const& y,\n                                                     double expansion_radius,\n-                                                    rmm::mr::device_memory_resource* mr)\n+                                                    rmm::device_async_resource_ref mr)\n {\n   auto num_polys = poly_offsets.size() > 0 ? poly_offsets.size() - 1 : 0;\n   auto num_rings = ring_offsets.size() > 0 ? ring_offsets.size() - 1 : 0;\ndiff --git a/cpp/src/distance/hausdorff.cu b/cpp/src/distance/hausdorff.cu\nindex 9706b5bf6..b4f2a62d1 100644\n--- a/cpp/src/distance/hausdorff.cu\n+++ b/cpp/src/distance/hausdorff.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2019-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2019-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -32,6 +32,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/binary_search.h>\n #include <thrust/distance.h>\n@@ -81,7 +82,7 @@ struct hausdorff_functor {\n              cudf::column_view const& ys,\n              cudf::column_view const& space_offsets,\n              rmm::cuda_stream_view stream,\n-             rmm::mr::device_memory_resource* mr)\n+             rmm::device_async_resource_ref mr)\n   {\n     auto const num_points = static_cast<uint32_t>(xs.size());\n     auto const num_spaces = static_cast<uint32_t>(space_offsets.size());\n@@ -120,7 +121,7 @@ std::pair<std::unique_ptr<cudf::column>, cudf::table_view> directed_hausdorff_di\n   cudf::column_view const& xs,\n   cudf::column_view const& ys,\n   cudf::column_view const& space_offsets,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(xs.type() == ys.type(), \"Inputs `xs` and `ys` must have same type.\");\n   CUSPATIAL_EXPECTS(xs.size() == ys.size(), \"Inputs `xs` and `ys` must have same length.\");\ndiff --git a/cpp/src/distance/haversine.cu b/cpp/src/distance/haversine.cu\nindex 3c4e631da..18a55cd2d 100644\n--- a/cpp/src/distance/haversine.cu\n+++ b/cpp/src/distance/haversine.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -27,6 +27,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/mr/device/device_memory_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -49,7 +50,7 @@ struct haversine_functor {\n     cudf::column_view const& b_lat,\n     T radius,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     if (a_lon.is_empty()) { return cudf::empty_like(a_lon); }\n \n@@ -81,7 +82,7 @@ std::unique_ptr<cudf::column> haversine_distance(cudf::column_view const& a_lon,\n                                                  cudf::column_view const& b_lat,\n                                                  double radius,\n                                                  rmm::cuda_stream_view stream,\n-                                                 rmm::mr::device_memory_resource* mr)\n+                                                 rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(radius > 0, \"radius must be positive.\");\n \n@@ -108,7 +109,7 @@ std::unique_ptr<cudf::column> haversine_distance(cudf::column_view const& a_lon,\n                                                  cudf::column_view const& b_lon,\n                                                  cudf::column_view const& b_lat,\n                                                  double radius,\n-                                                 rmm::mr::device_memory_resource* mr)\n+                                                 rmm::device_async_resource_ref mr)\n {\n   return cuspatial::detail::haversine_distance(\n     a_lon, a_lat, b_lon, b_lat, radius, rmm::cuda_stream_default, mr);\ndiff --git a/cpp/src/distance/linestring_distance.cu b/cpp/src/distance/linestring_distance.cu\nindex fe7b99fda..840d82b43 100644\n--- a/cpp/src/distance/linestring_distance.cu\n+++ b/cpp/src/distance/linestring_distance.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -28,6 +28,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -42,7 +43,7 @@ struct pairwise_linestring_distance_launch {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multilinestrings1,\n                                            geometry_column_view const& multilinestrings2,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto size = multilinestrings1.size();\n \n@@ -71,7 +72,7 @@ struct pairwise_linestring_distance_functor {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multilinestrings1,\n                                            geometry_column_view const& multilinestrings2,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(multilinestrings1.geometry_type() == geometry_type_id::LINESTRING &&\n                         multilinestrings2.geometry_type() == geometry_type_id::LINESTRING,\n@@ -96,7 +97,7 @@ struct pairwise_linestring_distance_functor {\n std::unique_ptr<cudf::column> pairwise_linestring_distance(\n   geometry_column_view const& multilinestrings1,\n   geometry_column_view const& multilinestrings2,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return multi_geometry_double_dispatch<detail::pairwise_linestring_distance_functor>(\n     multilinestrings1.collection_type(),\ndiff --git a/cpp/src/distance/linestring_polygon_distance.cu b/cpp/src/distance/linestring_polygon_distance.cu\nindex b6d370b90..552ab792d 100644\n--- a/cpp/src/distance/linestring_polygon_distance.cu\n+++ b/cpp/src/distance/linestring_polygon_distance.cu\n@@ -34,6 +34,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/counting_iterator.h>\n \n@@ -54,7 +55,7 @@ struct pairwise_linestring_polygon_distance_impl {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multilinestrings,\n                                            geometry_column_view const& multipolygons,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto multilinestrings_range =\n       make_multilinestring_range<is_multi_linestring, T, cudf::size_type>(multilinestrings);\n@@ -87,7 +88,7 @@ struct pairwise_linestring_polygon_distance {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multilinestrings,\n                                            geometry_column_view const& multipolygons,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(multilinestrings.geometry_type() == geometry_type_id::LINESTRING &&\n                         multipolygons.geometry_type() == geometry_type_id::POLYGON,\n@@ -114,7 +115,7 @@ struct pairwise_linestring_polygon_distance {\n std::unique_ptr<cudf::column> pairwise_linestring_polygon_distance(\n   geometry_column_view const& multilinestrings,\n   geometry_column_view const& multipolygons,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return multi_geometry_double_dispatch<detail::pairwise_linestring_polygon_distance>(\n     multilinestrings.collection_type(),\ndiff --git a/cpp/src/distance/point_distance.cu b/cpp/src/distance/point_distance.cu\nindex d2349b7a2..960c500d1 100644\n--- a/cpp/src/distance/point_distance.cu\n+++ b/cpp/src/distance/point_distance.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -27,6 +27,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -40,7 +41,7 @@ struct pairwise_point_distance_impl {\n     geometry_column_view const& multipoints1,\n     geometry_column_view const& multipoints2,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto size = multipoints1.size();\n \n@@ -68,7 +69,7 @@ struct pairwise_point_distance_functor {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multipoints1,\n                                            geometry_column_view const& multipoints2,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(multipoints1.geometry_type() == geometry_type_id::POINT &&\n                         multipoints2.geometry_type() == geometry_type_id::POINT,\n@@ -94,7 +95,7 @@ struct pairwise_point_distance_functor {\n \n std::unique_ptr<cudf::column> pairwise_point_distance(geometry_column_view const& multipoints1,\n                                                       geometry_column_view const& multipoints2,\n-                                                      rmm::mr::device_memory_resource* mr)\n+                                                      rmm::device_async_resource_ref mr)\n {\n   return multi_geometry_double_dispatch<detail::pairwise_point_distance_functor>(\n     multipoints1.collection_type(),\ndiff --git a/cpp/src/distance/point_linestring_distance.cu b/cpp/src/distance/point_linestring_distance.cu\nindex 59cb1b460..42d45b152 100644\n--- a/cpp/src/distance/point_linestring_distance.cu\n+++ b/cpp/src/distance/point_linestring_distance.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -30,6 +30,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -48,7 +49,7 @@ struct pairwise_point_linestring_distance_impl {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multipoints,\n                                            geometry_column_view const& multilinestrings,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto size = multipoints.size();\n \n@@ -79,7 +80,7 @@ struct pairwise_point_linestring_distance_functor {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multipoints,\n                                            geometry_column_view const& multilinestrings,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(multipoints.geometry_type() == geometry_type_id::POINT &&\n                         multilinestrings.geometry_type() == geometry_type_id::LINESTRING,\n@@ -106,7 +107,7 @@ struct pairwise_point_linestring_distance_functor {\n std::unique_ptr<cudf::column> pairwise_point_linestring_distance(\n   geometry_column_view const& multipoints,\n   geometry_column_view const& multilinestrings,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return multi_geometry_double_dispatch<detail::pairwise_point_linestring_distance_functor>(\n     multipoints.collection_type(),\ndiff --git a/cpp/src/distance/point_polygon_distance.cu b/cpp/src/distance/point_polygon_distance.cu\nindex dd7364fbd..b75b328bc 100644\n--- a/cpp/src/distance/point_polygon_distance.cu\n+++ b/cpp/src/distance/point_polygon_distance.cu\n@@ -34,6 +34,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/counting_iterator.h>\n \n@@ -54,7 +55,7 @@ struct pairwise_point_polygon_distance_impl {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multipoints,\n                                            geometry_column_view const& multipolygons,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto multipoints_range = make_multipoint_range<is_multi_point, T, cudf::size_type>(multipoints);\n     auto multipolygons_range =\n@@ -83,7 +84,7 @@ struct pairwise_point_polygon_distance {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& multipoints,\n                                            geometry_column_view const& multipolygons,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     return cudf::type_dispatcher(\n       multipoints.coordinate_type(),\n@@ -100,7 +101,7 @@ struct pairwise_point_polygon_distance {\n std::unique_ptr<cudf::column> pairwise_point_polygon_distance(\n   geometry_column_view const& multipoints,\n   geometry_column_view const& multipolygons,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(multipoints.geometry_type() == geometry_type_id::POINT &&\n                       multipolygons.geometry_type() == geometry_type_id::POLYGON,\ndiff --git a/cpp/src/distance/polygon_distance.cu b/cpp/src/distance/polygon_distance.cu\nindex 678aa3f56..c5a6b0ed1 100644\n--- a/cpp/src/distance/polygon_distance.cu\n+++ b/cpp/src/distance/polygon_distance.cu\n@@ -32,6 +32,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/counting_iterator.h>\n \n@@ -52,7 +53,7 @@ struct pairwise_polygon_distance_impl {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& lhs,\n                                            geometry_column_view const& rhs,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto lhs_range = make_multipolygon_range<is_multi_polygon_lhs, T, cudf::size_type>(lhs);\n     auto rhs_range = make_multipolygon_range<is_multi_polygon_rhs, T, cudf::size_type>(rhs);\n@@ -80,7 +81,7 @@ struct pairwise_polygon_distance {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& lhs,\n                                            geometry_column_view const& rhs,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(lhs.geometry_type() == geometry_type_id::POLYGON &&\n                         rhs.geometry_type() == geometry_type_id::POLYGON,\n@@ -106,7 +107,7 @@ struct pairwise_polygon_distance {\n \n std::unique_ptr<cudf::column> pairwise_polygon_distance(geometry_column_view const& lhs,\n                                                         geometry_column_view const& rhs,\n-                                                        rmm::mr::device_memory_resource* mr)\n+                                                        rmm::device_async_resource_ref mr)\n {\n   return multi_geometry_double_dispatch<detail::pairwise_polygon_distance>(\n     lhs.collection_type(), rhs.collection_type(), lhs, rhs, rmm::cuda_stream_default, mr);\ndiff --git a/cpp/src/equality/pairwise_multipoint_equals_count.cu b/cpp/src/equality/pairwise_multipoint_equals_count.cu\nindex 71e2b6d2b..abfde9e9c 100644\n--- a/cpp/src/equality/pairwise_multipoint_equals_count.cu\n+++ b/cpp/src/equality/pairwise_multipoint_equals_count.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2023-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -26,6 +26,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n #include <thrust/pair.h>\n@@ -47,7 +48,7 @@ struct pairwise_multipoint_equals_count_impl {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view const& lhs,\n                                            geometry_column_view const& rhs,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto size = lhs.size();  // lhs is a buffer of xy coords\n     auto type = cudf::data_type(cudf::type_to_id<uint32_t>());\n@@ -78,7 +79,7 @@ struct pairwise_multipoint_equals_count {\n   std::unique_ptr<cudf::column> operator()(geometry_column_view lhs,\n                                            geometry_column_view rhs,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     return cudf::type_dispatcher(\n       lhs.coordinate_type(),\n@@ -94,7 +95,7 @@ struct pairwise_multipoint_equals_count {\n \n std::unique_ptr<cudf::column> pairwise_multipoint_equals_count(geometry_column_view const& lhs,\n                                                                geometry_column_view const& rhs,\n-                                                               rmm::mr::device_memory_resource* mr)\n+                                                               rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(lhs.geometry_type() == geometry_type_id::POINT &&\n                       rhs.geometry_type() == geometry_type_id::POINT,\ndiff --git a/cpp/src/indexing/point_quadtree.cu b/cpp/src/indexing/point_quadtree.cu\nindex 4e2f5987f..a8e53f1e1 100644\n--- a/cpp/src/indexing/point_quadtree.cu\n+++ b/cpp/src/indexing/point_quadtree.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2021, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -26,6 +26,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n \n@@ -72,7 +73,7 @@ struct dispatch_construct_quadtree {\n     int8_t max_depth,\n     cudf::size_type max_size,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto points = cuspatial::make_vec_2d_iterator(x.begin<T>(), y.begin<T>());\n     auto [point_indices, tree] =\n@@ -131,7 +132,7 @@ std::pair<std::unique_ptr<cudf::column>, std::unique_ptr<cudf::table>> quadtree_\n   int8_t max_depth,\n   cudf::size_type max_size,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(x.type(),\n                                dispatch_construct_quadtree{},\n@@ -160,7 +161,7 @@ std::pair<std::unique_ptr<cudf::column>, std::unique_ptr<cudf::table>> quadtree_\n   double scale,\n   int8_t max_depth,\n   cudf::size_type max_size,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(x.size() == y.size(), \"x and y columns must have the same length\");\n   if (x.is_empty() || y.is_empty()) {\ndiff --git a/cpp/src/intersection/linestring_intersection.cu b/cpp/src/intersection/linestring_intersection.cu\nindex 30e60e798..46734f69d 100644\n--- a/cpp/src/intersection/linestring_intersection.cu\n+++ b/cpp/src/intersection/linestring_intersection.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2023-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -34,6 +34,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -43,7 +44,7 @@ namespace detail {\n \n std::unique_ptr<cudf::column> even_sequence(cudf::size_type size,\n                                             rmm::cuda_stream_view stream,\n-                                            rmm::mr::device_memory_resource* mr)\n+                                            rmm::device_async_resource_ref mr)\n {\n   auto res = cudf::make_numeric_column(cudf::data_type{cudf::type_to_id<cudf::size_type>()},\n                                        size,\n@@ -67,7 +68,7 @@ struct pairwise_linestring_intersection_launch {\n   operator()(geometry_column_view const& multilinestrings1,\n              geometry_column_view const& multilinestrings2,\n              rmm::cuda_stream_view stream,\n-             rmm::mr::device_memory_resource* mr)\n+             rmm::device_async_resource_ref mr)\n   {\n     using index_t = cudf::size_type;\n \n@@ -155,7 +156,7 @@ struct pairwise_linestring_intersection {\n   linestring_intersection_column_result operator()(geometry_column_view const& linestrings1,\n                                                    geometry_column_view const& linestrings2,\n                                                    rmm::cuda_stream_view stream,\n-                                                   rmm::mr::device_memory_resource* mr)\n+                                                   rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(linestrings1.coordinate_type() == linestrings2.coordinate_type(),\n                       \"Input linestring coordinates must be the same type.\");\n@@ -176,7 +177,7 @@ struct pairwise_linestring_intersection {\n linestring_intersection_column_result pairwise_linestring_intersection(\n   geometry_column_view const& lhs,\n   geometry_column_view const& rhs,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(lhs.geometry_type() == geometry_type_id::LINESTRING &&\n                       rhs.geometry_type() == geometry_type_id::LINESTRING,\ndiff --git a/cpp/src/join/quadtree_bbox_filtering.cu b/cpp/src/join/quadtree_bbox_filtering.cu\nindex 59bd369cd..1757938f3 100644\n--- a/cpp/src/join/quadtree_bbox_filtering.cu\n+++ b/cpp/src/join/quadtree_bbox_filtering.cu\n@@ -27,6 +27,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <tuple>\n \n@@ -42,7 +43,7 @@ struct dispatch_quadtree_bounding_box_join {\n                                                  double y_min,\n                                                  double scale,\n                                                  int8_t max_depth,\n-                                                 rmm::mr::device_memory_resource* mr,\n+                                                 rmm::device_async_resource_ref mr,\n                                                  rmm::cuda_stream_view stream)\n   {\n     auto bbox_min = cuspatial::make_vec_2d_iterator(bbox.column(0).template begin<T>(),\n@@ -94,7 +95,7 @@ std::unique_ptr<cudf::table> join_quadtree_and_bounding_boxes(cudf::table_view c\n                                                               double y_max,\n                                                               double scale,\n                                                               int8_t max_depth,\n-                                                              rmm::mr::device_memory_resource* mr)\n+                                                              rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(quadtree.num_columns() == 5, \"quadtree table must have 5 columns\");\n   CUSPATIAL_EXPECTS(bbox.num_columns() == 4, \"bbox table must have 4 columns\");\ndiff --git a/cpp/src/join/quadtree_point_in_polygon.cu b/cpp/src/join/quadtree_point_in_polygon.cu\nindex 75b723aa8..48f9055ba 100644\n--- a/cpp/src/join/quadtree_point_in_polygon.cu\n+++ b/cpp/src/join/quadtree_point_in_polygon.cu\n@@ -28,6 +28,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_buffer.hpp>\n+#include <rmm/resource_ref.hpp>\n \n namespace cuspatial {\n namespace detail {\n@@ -52,7 +53,7 @@ struct compute_quadtree_point_in_polygon {\n     cudf::column_view const& poly_points_x,\n     cudf::column_view const& poly_points_y,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto poly_indices = poly_quad_pairs.column(0);\n     auto quad_indices = poly_quad_pairs.column(1);\n@@ -120,7 +121,7 @@ std::unique_ptr<cudf::table> quadtree_point_in_polygon(cudf::table_view const& p\n                                                        cudf::column_view const& poly_points_x,\n                                                        cudf::column_view const& poly_points_y,\n                                                        rmm::cuda_stream_view stream,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(point_x.type(),\n                                compute_quadtree_point_in_polygon{},\n@@ -148,7 +149,7 @@ std::unique_ptr<cudf::table> quadtree_point_in_polygon(cudf::table_view const& p\n                                                        cudf::column_view const& ring_offsets,\n                                                        cudf::column_view const& poly_points_x,\n                                                        cudf::column_view const& poly_points_y,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(poly_quad_pairs.num_columns() == 2,\n                     \"a quadrant-polygon table must have 2 columns\");\ndiff --git a/cpp/src/join/quadtree_point_to_nearest_linestring.cu b/cpp/src/join/quadtree_point_to_nearest_linestring.cu\nindex c423c1cb4..8288ecbc3 100644\n--- a/cpp/src/join/quadtree_point_to_nearest_linestring.cu\n+++ b/cpp/src/join/quadtree_point_to_nearest_linestring.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -28,6 +28,7 @@\n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <limits>\n #include <memory>\n@@ -54,7 +55,7 @@ struct compute_quadtree_point_to_nearest_linestring {\n     cudf::column_view const& linestring_points_x,\n     cudf::column_view const& linestring_points_y,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto linestring_indices = linestring_quad_pairs.column(0);\n     auto quad_indices       = linestring_quad_pairs.column(1);\n@@ -120,7 +121,7 @@ std::unique_ptr<cudf::table> quadtree_point_to_nearest_linestring(\n   cudf::column_view const& linestring_points_x,\n   cudf::column_view const& linestring_points_y,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(point_x.type(),\n                                compute_quadtree_point_to_nearest_linestring{},\n@@ -147,7 +148,7 @@ std::unique_ptr<cudf::table> quadtree_point_to_nearest_linestring(\n   cudf::column_view const& linestring_offsets,\n   cudf::column_view const& linestring_points_x,\n   cudf::column_view const& linestring_points_y,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(linestring_quad_pairs.num_columns() == 2,\n                     \"a quadrant-linestring table must have 2 columns\");\ndiff --git a/cpp/src/nearest_points/point_linestring_nearest_points.cu b/cpp/src/nearest_points/point_linestring_nearest_points.cu\nindex 9e15cc225..00a4d365f 100644\n--- a/cpp/src/nearest_points/point_linestring_nearest_points.cu\n+++ b/cpp/src/nearest_points/point_linestring_nearest_points.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -30,6 +30,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/counting_iterator.h>\n #include <thrust/iterator/discard_iterator.h>\n@@ -56,7 +57,7 @@ struct pairwise_point_linestring_nearest_points_impl {\n     cudf::device_span<cudf::size_type const> linestring_offsets,\n     cudf::column_view linestring_points_xy,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto num_points            = static_cast<SizeType>(points_xy.size() / 2);\n     auto num_linestring_points = static_cast<SizeType>(linestring_points_xy.size() / 2);\n@@ -219,7 +220,7 @@ struct pairwise_point_linestring_nearest_points_functor {\n     cudf::device_span<cudf::size_type const> linestring_part_offsets,\n     cudf::column_view linestring_points_xy,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     CUSPATIAL_EXPECTS(points_xy.size() % 2 == 0 && linestring_points_xy.size() % 2 == 0,\n                       \"Points array must contain even number of coordinates.\");\n@@ -258,7 +259,7 @@ point_linestring_nearest_points_result pairwise_point_linestring_nearest_points(\n   std::optional<cudf::device_span<cudf::size_type const>> multilinestring_geometry_offsets,\n   cudf::device_span<cudf::size_type const> linestring_part_offsets,\n   cudf::column_view linestring_points_xy,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return double_boolean_dispatch<detail::pairwise_point_linestring_nearest_points_functor>(\n     multipoint_geometry_offsets.has_value(),\ndiff --git a/cpp/src/point_in_polygon/point_in_polygon.cu b/cpp/src/point_in_polygon/point_in_polygon.cu\nindex e6eb56d08..c22a8da94 100644\n--- a/cpp/src/point_in_polygon/point_in_polygon.cu\n+++ b/cpp/src/point_in_polygon/point_in_polygon.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -28,6 +28,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/mr/device/device_memory_resource.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -56,7 +57,7 @@ struct point_in_polygon_functor {\n                                            cudf::column_view const& poly_points_y,\n                                            bool pairwise,\n                                            rmm::cuda_stream_view stream,\n-                                           rmm::mr::device_memory_resource* mr)\n+                                           rmm::device_async_resource_ref mr)\n   {\n     auto size = test_points_x.size();\n     auto tid  = pairwise ? cudf::type_to_id<uint8_t>() : cudf::type_to_id<int32_t>();\n@@ -112,7 +113,7 @@ std::unique_ptr<cudf::column> point_in_polygon(cudf::column_view const& test_poi\n                                                cudf::column_view const& poly_points_y,\n                                                bool pairwise,\n                                                rmm::cuda_stream_view stream,\n-                                               rmm::mr::device_memory_resource* mr)\n+                                               rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(\n     test_points_x.size() == test_points_y.size() and poly_points_x.size() == poly_points_y.size(),\n@@ -155,7 +156,7 @@ std::unique_ptr<cudf::column> point_in_polygon(cudf::column_view const& test_poi\n                                                cudf::column_view const& poly_ring_offsets,\n                                                cudf::column_view const& poly_points_x,\n                                                cudf::column_view const& poly_points_y,\n-                                               rmm::mr::device_memory_resource* mr)\n+                                               rmm::device_async_resource_ref mr)\n {\n   return cuspatial::detail::point_in_polygon(test_points_x,\n                                              test_points_y,\n@@ -174,7 +175,7 @@ std::unique_ptr<cudf::column> pairwise_point_in_polygon(cudf::column_view const&\n                                                         cudf::column_view const& poly_ring_offsets,\n                                                         cudf::column_view const& poly_points_x,\n                                                         cudf::column_view const& poly_points_y,\n-                                                        rmm::mr::device_memory_resource* mr)\n+                                                        rmm::device_async_resource_ref mr)\n {\n   return cuspatial::detail::point_in_polygon(test_points_x,\n                                              test_points_y,\ndiff --git a/cpp/src/points_in_range/points_in_range.cu b/cpp/src/points_in_range/points_in_range.cu\nindex df6f60c6a..edb843a93 100644\n--- a/cpp/src/points_in_range/points_in_range.cu\n+++ b/cpp/src/points_in_range/points_in_range.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -25,6 +25,7 @@\n #include <cudf/table/table.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <type_traits>\n@@ -42,7 +43,7 @@ struct points_in_range_dispatch {\n                                           cudf::column_view const& x,\n                                           cudf::column_view const& y,\n                                           rmm::cuda_stream_view stream,\n-                                          rmm::mr::device_memory_resource* mr)\n+                                          rmm::device_async_resource_ref mr)\n   {\n     auto points_begin = cuspatial::make_vec_2d_iterator(x.begin<T>(), y.begin<T>());\n \n@@ -98,7 +99,7 @@ std::unique_ptr<cudf::table> points_in_range(double range_min_x,\n                                              cudf::column_view const& x,\n                                              cudf::column_view const& y,\n                                              rmm::cuda_stream_view stream,\n-                                             rmm::mr::device_memory_resource* mr)\n+                                             rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(x.type() == y.type(), \"Type mismatch between x and y arrays\");\n   CUSPATIAL_EXPECTS(x.size() == y.size(), \"Size mismatch between x and y arrays\");\n@@ -129,7 +130,7 @@ std::unique_ptr<cudf::table> points_in_range(double range_min_x,\n                                              double range_max_y,\n                                              cudf::column_view const& x,\n                                              cudf::column_view const& y,\n-                                             rmm::mr::device_memory_resource* mr)\n+                                             rmm::device_async_resource_ref mr)\n {\n   return detail::points_in_range(\n     range_min_x, range_max_x, range_min_y, range_max_y, x, y, rmm::cuda_stream_default, mr);\ndiff --git a/cpp/src/projection/sinusoidal_projection.cu b/cpp/src/projection/sinusoidal_projection.cu\nindex 1be4ade40..2552601a4 100644\n--- a/cpp/src/projection/sinusoidal_projection.cu\n+++ b/cpp/src/projection/sinusoidal_projection.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2019-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2019-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -26,6 +26,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n #include <thrust/pair.h>\n@@ -53,7 +54,7 @@ struct dispatch_sinusoidal_projection {\n     cudf::column_view const& input_lon,\n     cudf::column_view const& input_lat,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto size = input_lon.size();\n     auto type = cudf::data_type{cudf::type_to_id<T>()};\n@@ -87,7 +88,7 @@ pair_of_columns sinusoidal_projection(double origin_lon,\n                                       cudf::column_view const& input_lon,\n                                       cudf::column_view const& input_lat,\n                                       rmm::cuda_stream_view stream,\n-                                      rmm::mr::device_memory_resource* mr)\n+                                      rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(\n     origin_lon >= -180 && origin_lon <= 180 && origin_lat >= -90 && origin_lat <= 90,\n@@ -116,7 +117,7 @@ pair_of_columns sinusoidal_projection(double origin_lon,\n                                       double origin_lat,\n                                       cudf::column_view const& input_lon,\n                                       cudf::column_view const& input_lat,\n-                                      rmm::mr::device_memory_resource* mr)\n+                                      rmm::device_async_resource_ref mr)\n {\n   return detail::sinusoidal_projection(\n     origin_lon, origin_lat, input_lon, input_lat, rmm::cuda_stream_default, mr);\ndiff --git a/cpp/src/trajectory/derive_trajectories.cu b/cpp/src/trajectory/derive_trajectories.cu\nindex 8356369ef..e4e6f3745 100644\n--- a/cpp/src/trajectory/derive_trajectories.cu\n+++ b/cpp/src/trajectory/derive_trajectories.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -25,6 +25,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <memory>\n #include <vector>\n@@ -43,7 +44,7 @@ struct derive_trajectories_dispatch {\n     cudf::column_view const& y,\n     cudf::column_view const& timestamp,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     auto cols = std::vector<std::unique_ptr<cudf::column>>{};\n     cols.reserve(4);\n@@ -95,7 +96,7 @@ std::pair<std::unique_ptr<cudf::table>, std::unique_ptr<cudf::column>> derive_tr\n   cudf::column_view const& y,\n   cudf::column_view const& timestamp,\n   rmm::cuda_stream_view stream,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   return cudf::double_type_dispatcher(x.type(),\n                                       timestamp.type(),\n@@ -114,7 +115,7 @@ std::pair<std::unique_ptr<cudf::table>, std::unique_ptr<cudf::column>> derive_tr\n   cudf::column_view const& x,\n   cudf::column_view const& y,\n   cudf::column_view const& timestamp,\n-  rmm::mr::device_memory_resource* mr)\n+  rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(\n     x.size() == y.size() && x.size() == object_id.size() && x.size() == timestamp.size(),\ndiff --git a/cpp/src/trajectory/trajectory_bounding_boxes.cu b/cpp/src/trajectory/trajectory_bounding_boxes.cu\nindex 8d441b9c4..baeea23e4 100644\n--- a/cpp/src/trajectory/trajectory_bounding_boxes.cu\n+++ b/cpp/src/trajectory/trajectory_bounding_boxes.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -27,6 +27,7 @@\n #include <cudf/utilities/type_dispatcher.hpp>\n \n #include <rmm/cuda_stream_view.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/zip_iterator.h>\n \n@@ -45,7 +46,7 @@ struct dispatch_element {\n     cudf::column_view const& x,\n     cudf::column_view const& y,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     // Construct output columns\n     auto type = cudf::data_type{cudf::type_to_id<T>()};\n@@ -101,7 +102,7 @@ std::unique_ptr<cudf::table> trajectory_bounding_boxes(cudf::size_type num_traje\n                                                        cudf::column_view const& x,\n                                                        cudf::column_view const& y,\n                                                        rmm::cuda_stream_view stream,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(\n     x.type(), dispatch_element{}, num_trajectories, object_id, x, y, stream, mr);\n@@ -112,7 +113,7 @@ std::unique_ptr<cudf::table> trajectory_bounding_boxes(cudf::size_type num_traje\n                                                        cudf::column_view const& object_id,\n                                                        cudf::column_view const& x,\n                                                        cudf::column_view const& y,\n-                                                       rmm::mr::device_memory_resource* mr)\n+                                                       rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(object_id.size() == x.size() && x.size() == y.size(), \"Data size mismatch\");\n   CUSPATIAL_EXPECTS(x.type().id() == y.type().id(), \"Data type mismatch\");\ndiff --git a/cpp/src/trajectory/trajectory_distances_and_speeds.cu b/cpp/src/trajectory/trajectory_distances_and_speeds.cu\nindex a27f8b7f8..89474e925 100644\n--- a/cpp/src/trajectory/trajectory_distances_and_speeds.cu\n+++ b/cpp/src/trajectory/trajectory_distances_and_speeds.cu\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2020, NVIDIA CORPORATION.\n+ * Copyright (c) 2020-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -29,6 +29,7 @@\n \n #include <rmm/cuda_stream_view.hpp>\n #include <rmm/exec_policy.hpp>\n+#include <rmm/resource_ref.hpp>\n \n #include <thrust/adjacent_difference.h>\n #include <thrust/functional.h>\n@@ -54,7 +55,7 @@ struct dispatch_timestamp {\n     cudf::column_view const& y,\n     cudf::column_view const& timestamp,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     // Construct output columns\n     auto type = cudf::data_type{cudf::type_to_id<T>()};\n@@ -102,7 +103,7 @@ struct dispatch_element {\n     cudf::column_view const& y,\n     cudf::column_view const& timestamp,\n     rmm::cuda_stream_view stream,\n-    rmm::mr::device_memory_resource* mr)\n+    rmm::device_async_resource_ref mr)\n   {\n     return cudf::type_dispatcher(timestamp.type(),\n                                  dispatch_timestamp<Element>{},\n@@ -132,7 +133,7 @@ std::unique_ptr<cudf::table> trajectory_distances_and_speeds(cudf::size_type num\n                                                              cudf::column_view const& y,\n                                                              cudf::column_view const& timestamp,\n                                                              rmm::cuda_stream_view stream,\n-                                                             rmm::mr::device_memory_resource* mr)\n+                                                             rmm::device_async_resource_ref mr)\n {\n   return cudf::type_dispatcher(\n     x.type(), dispatch_element{}, num_trajectories, object_id, x, y, timestamp, stream, mr);\n@@ -144,7 +145,7 @@ std::unique_ptr<cudf::table> trajectory_distances_and_speeds(cudf::size_type num\n                                                              cudf::column_view const& x,\n                                                              cudf::column_view const& y,\n                                                              cudf::column_view const& timestamp,\n-                                                             rmm::mr::device_memory_resource* mr)\n+                                                             rmm::device_async_resource_ref mr)\n {\n   CUSPATIAL_EXPECTS(\n     x.size() == y.size() && x.size() == object_id.size() && x.size() == timestamp.size(),\ndiff --git a/dependencies.yaml b/dependencies.yaml\nindex fabb4149e..bd5d25e11 100644\n--- a/dependencies.yaml\n+++ b/dependencies.yaml\n@@ -18,6 +18,7 @@ files:\n       - depends_on_cuml\n       - depends_on_cupy\n       - run_python_cuspatial\n+      - test_libcuspatial\n       - test_python_cuspatial\n       - test_python_cuproj\n       - notebooks\n@@ -25,6 +26,7 @@ files:\n     output: none\n     includes:\n       - cuda_version\n+      - test_libcuspatial\n   test_python:\n     output: none\n     includes:\n@@ -32,6 +34,7 @@ files:\n       - py_version\n       - test_python_cuspatial\n       - test_python_cuproj\n+      - test_cuspatial\n   test_notebooks:\n     output: none\n     includes:\n@@ -39,6 +42,7 @@ files:\n       - depends_on_cuml\n       - notebooks\n       - py_version\n+      - test_cuspatial\n   checks:\n     output: none\n     includes:\n@@ -50,6 +54,7 @@ files:\n       - cuda_version\n       - docs\n       - py_version\n+      - test_cuspatial\n   py_build_cuspatial:\n     output: [pyproject]\n     pyproject_dir: python/cuspatial\n@@ -105,6 +110,7 @@ files:\n     includes:\n       - test_python_cuproj\n       - depends_on_cuspatial\n+      - test_cuspatial\n \n channels:\n   - rapidsai\n@@ -122,10 +128,8 @@ dependencies:\n         packages:\n           - c-compiler\n           - cxx-compiler\n-          - gmock>=1.13.0\n-          - gtest>=1.13.0\n-          - libcudf==24.4.*\n-          - librmm==24.4.*\n+          - libcudf==24.6.*\n+          - librmm==24.6.*\n           - proj\n           - sqlite\n     specific:\n@@ -167,9 +171,7 @@ dependencies:\n         packages:\n           - c-compiler\n           - cxx-compiler\n-          - gmock>=1.13.0\n-          - gtest>=1.13.0\n-          - librmm==24.4.*\n+          - librmm==24.6.*\n           - proj\n           - sqlite\n     specific:\n@@ -360,7 +362,7 @@ dependencies:\n     common:\n       - output_types: conda\n         packages:\n-          - &rmm_conda rmm==24.4.*\n+          - &rmm_conda rmm==24.6.*\n       - output_types: requirements\n         packages:\n           # pip recognizes the index as a global option for the requirements.txt file\n@@ -371,17 +373,17 @@ dependencies:\n         matrices:\n           - matrix: {cuda: \"12.*\"}\n             packages:\n-              - rmm-cu12==24.4.*\n+              - rmm-cu12==24.6.*\n           - matrix: {cuda: \"11.*\"}\n             packages:\n-              - rmm-cu11==24.4.*\n+              - rmm-cu11==24.6.*\n           - {matrix: null, packages: [*rmm_conda]}\n \n   depends_on_cudf:\n     common:\n       - output_types: conda\n         packages:\n-          - &cudf_conda cudf==24.4.*\n+          - &cudf_conda cudf==24.6.*\n       - output_types: requirements\n         packages:\n           # pip recognizes the index as a global option for the requirements.txt file\n@@ -392,17 +394,17 @@ dependencies:\n         matrices:\n           - matrix: {cuda: \"12.*\"}\n             packages:\n-              - cudf-cu12==24.4.*\n+              - cudf-cu12==24.6.*\n           - matrix: {cuda: \"11.*\"}\n             packages:\n-              - cudf-cu11==24.4.*\n+              - cudf-cu11==24.6.*\n           - {matrix: null, packages: [*cudf_conda]}\n \n   depends_on_cuml:\n     common:\n       - output_types: conda\n         packages:\n-          - &cuml_conda cuml==24.4.*\n+          - &cuml_conda cuml==24.6.*\n       - output_types: requirements\n         packages:\n           # pip recognizes the index as a global option for the requirements.txt file\n@@ -413,17 +415,17 @@ dependencies:\n         matrices:\n           - matrix: {cuda: \"12.*\"}\n             packages:\n-              - cuml-cu12==24.4.*\n+              - cuml-cu12==24.6.*\n           - matrix: {cuda: \"11.*\"}\n             packages:\n-              - cuml-cu11==24.4.*\n+              - cuml-cu11==24.6.*\n           - {matrix: null, packages: [*cuml_conda]}\n \n   depends_on_cuspatial:\n     common:\n       - output_types: conda\n         packages:\n-          - &cuspatial_conda cuspatial==24.4.*\n+          - &cuspatial_conda cuspatial==24.6.*\n       - output_types: requirements\n         packages:\n           # pip recognizes the index as a global option for the requirements.txt file\n@@ -434,10 +436,10 @@ dependencies:\n         matrices:\n           - matrix: {cuda: \"12.*\"}\n             packages:\n-              - cuspatial-cu12==24.4.*\n+              - cuspatial-cu12==24.6.*\n           - matrix: {cuda: \"11.*\"}\n             packages:\n-              - cuspatial-cu11==24.4.*\n+              - cuspatial-cu11==24.6.*\n           - {matrix: null, packages: [*cuspatial_conda]}\n \n   depends_on_cupy:\n@@ -455,3 +457,16 @@ dependencies:\n             packages:\n               - cupy-cuda11x>=12.0.0\n           - {matrix: null, packages: [cupy-cuda11x>=12.0.0]}\n+  test_libcuspatial:\n+    common:\n+      - output_types: conda\n+        packages:\n+          - libcuspatial==24.6.*\n+          - libcuspatial-tests==24.6.*\n+  test_cuspatial:\n+    common:\n+      - output_types: conda\n+        packages:\n+          - libcuspatial==24.6.*\n+          - cuspatial==24.6.*\n+          - cuproj==24.6.*\ndiff --git a/docs/cuproj/source/user_guide/cuproj_api_examples.ipynb b/docs/cuproj/source/user_guide/cuproj_api_examples.ipynb\nindex a319082b4..8dd25cb28 100644\n--- a/docs/cuproj/source/user_guide/cuproj_api_examples.ipynb\n+++ b/docs/cuproj/source/user_guide/cuproj_api_examples.ipynb\n@@ -45,7 +45,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"# !conda create -n rapids-24.04 --solver=libmamba -c rapidsai -c conda-forge -c nvidia \\\\ \\n\",\n+    \"# !conda create -n rapids-24.06 --solver=libmamba -c rapidsai -c conda-forge -c nvidia \\\\ \\n\",\n     \"#     cuproj-23.12 python=3.10 cuda-version=12.0\"\n    ]\n   },\ndiff --git a/docs/source/user_guide/cuspatial_api_examples.ipynb b/docs/source/user_guide/cuspatial_api_examples.ipynb\nindex 4ec66b4b8..a20b8cf1e 100644\n--- a/docs/source/user_guide/cuspatial_api_examples.ipynb\n+++ b/docs/source/user_guide/cuspatial_api_examples.ipynb\n@@ -57,8 +57,8 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"# !conda create -n rapids-24.04 -c rapidsai -c conda-forge -c nvidia \\\\ \\n\",\n-    \"#     cuspatial=24.04 python=3.9 cudatoolkit=11.5 \"\n+    \"# !conda create -n rapids-24.06 -c rapidsai -c conda-forge -c nvidia \\\\ \\n\",\n+    \"#     cuspatial=24.06 python=3.9 cudatoolkit=11.5 \"\n    ]\n   },\n   {\ndiff --git a/python/cuproj/pyproject.toml b/python/cuproj/pyproject.toml\nindex 698482319..54c4a3709 100644\n--- a/python/cuproj/pyproject.toml\n+++ b/python/cuproj/pyproject.toml\n@@ -18,7 +18,7 @@ requires = [\n     \"cmake>=3.26.4\",\n     \"cython>=3.0.0\",\n     \"ninja\",\n-    \"rmm==24.4.*\",\n+    \"rmm==24.6.*\",\n     \"scikit-build-core[pyproject]>=0.7.0\",\n     \"setuptools\",\n     \"wheel\",\n@@ -34,7 +34,7 @@ license = { text = \"Apache 2.0\" }\n requires-python = \">=3.9\"\n dependencies = [\n     \"cupy-cuda11x>=12.0.0\",\n-    \"rmm==24.4.*\",\n+    \"rmm==24.6.*\",\n ] # This list was generated by `rapids-dependency-file-generator`. To make changes, edit ../../dependencies.yaml and run `rapids-dependency-file-generator`.\n classifiers = [\n     \"Intended Audience :: Developers\",\n@@ -49,7 +49,7 @@ classifiers = [\n \n [project.optional-dependencies]\n test = [\n-    \"cuspatial==24.4.*\",\n+    \"cuspatial==24.6.*\",\n     \"geopandas>=0.11.0\",\n     \"numpy>=1.23,<2.0a0\",\n     \"pyproj>=3.6.0,<3.7a0\",\n@@ -111,3 +111,11 @@ wheel.packages = [\"cuproj\"]\n provider = \"scikit_build_core.metadata.regex\"\n input = \"cuproj/VERSION\"\n regex = \"(?P<value>.*)\"\n+\n+[tool.pytest.ini_options]\n+xfail_strict = true\n+filterwarnings = [\n+    \"error\",\n+    # https://github.com/pytest-dev/pytest-cov/issues/557\n+    \"ignore:The --rsyncdir command line argument:DeprecationWarning\",\n+]\ndiff --git a/python/cuspatial/benchmarks/api/bench_api.py b/python/cuspatial/benchmarks/api/bench_api.py\nindex d93e45ebb..d8e24cf33 100644\n--- a/python/cuspatial/benchmarks/api/bench_api.py\n+++ b/python/cuspatial/benchmarks/api/bench_api.py\n@@ -1,4 +1,6 @@\n-# Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2022-2024, NVIDIA CORPORATION.\n+import pathlib\n+\n import cupy\n import geopandas\n import pytest\n@@ -223,12 +225,16 @@ def bench_quadtree_point_to_nearest_linestring(benchmark):\n     SCALE = 3\n     MAX_DEPTH = 7\n     MIN_SIZE = 125\n-    host_countries = geopandas.read_file(\n-        geopandas.datasets.get_path(\"naturalearth_lowres\")\n-    )\n-    host_cities = geopandas.read_file(\n-        geopandas.datasets.get_path(\"naturalearth_cities\")\n+    data_dir = (\n+        pathlib.Path(__file__).parent.parent.parent\n+        / \"cuspatial\"\n+        / \"tests\"\n+        / \"data\"\n     )\n+    naturalearth_lowres = data_dir / \"naturalearth_lowres.shp\"\n+    naturalearth_cities = data_dir / \"naturalearth_cities.shp\"\n+    host_countries = geopandas.read_file(naturalearth_lowres)\n+    host_cities = geopandas.read_file(naturalearth_cities)\n     gpu_countries = cuspatial.from_geopandas(\n         host_countries[host_countries[\"geometry\"].type == \"Polygon\"]\n     )\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/distance.pxd b/python/cuspatial/cuspatial/_lib/cpp/distance.pxd\nindex 1af0f97e0..98925a516 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/distance.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/distance.pxd\n@@ -1,11 +1,11 @@\n-# Copyright (c) 2023, NVIDIA CORPORATION.\n+# Copyright (c) 2023-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.utility cimport pair\n \n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table_view cimport table_view\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table_view cimport table_view\n \n from cuspatial._lib.cpp.column.geometry_column_view cimport (\n     geometry_column_view,\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/distance/polygon_distance.pxd b/python/cuspatial/cuspatial/_lib/cpp/distance/polygon_distance.pxd\nindex 62f3a318c..3a92adae3 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/distance/polygon_distance.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/distance/polygon_distance.pxd\n@@ -1,8 +1,8 @@\n-# Copyright (c) 2023, NVIDIA CORPORATION.\n+# Copyright (c) 2023-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n \n-from cudf._lib.cpp.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n \n from cuspatial._lib.cpp.column.geometry_column_view cimport (\n     geometry_column_view,\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/linestring_bounding_boxes.pxd b/python/cuspatial/cuspatial/_lib/cpp/linestring_bounding_boxes.pxd\nindex fa458a6cf..8622a417c 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/linestring_bounding_boxes.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/linestring_bounding_boxes.pxd\n@@ -1,9 +1,9 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n \n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n \n \n cdef extern from \"cuspatial/bounding_boxes.hpp\" \\\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/points_in_range.pxd b/python/cuspatial/cuspatial/_lib/cpp/points_in_range.pxd\nindex e6534dc7b..d03b3a133 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/points_in_range.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/points_in_range.pxd\n@@ -1,9 +1,9 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n \n from cudf._lib.column cimport column, column_view\n-from cudf._lib.cpp.table.table cimport table, table_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table, table_view\n \n \n cdef extern from \"cuspatial/points_in_range.hpp\" namespace \"cuspatial\" nogil:\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/polygon_bounding_boxes.pxd b/python/cuspatial/cuspatial/_lib/cpp/polygon_bounding_boxes.pxd\nindex a6d28f965..45910626b 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/polygon_bounding_boxes.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/polygon_bounding_boxes.pxd\n@@ -1,9 +1,9 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n \n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n \n \n cdef extern from \"cuspatial/bounding_boxes.hpp\" \\\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/projection.pxd b/python/cuspatial/cuspatial/_lib/cpp/projection.pxd\nindex 3c85461e7..724269486 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/projection.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/projection.pxd\n@@ -1,10 +1,10 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.pair cimport pair\n \n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n \n \n cdef extern from \"cuspatial/projection.hpp\" namespace \"cuspatial\" \\\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/quadtree.pxd b/python/cuspatial/cuspatial/_lib/cpp/quadtree.pxd\nindex be34d98a9..db2dabaca 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/quadtree.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/quadtree.pxd\n@@ -1,13 +1,13 @@\n-# Copyright (c) 2020, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libc.stdint cimport int8_t\n from libcpp.memory cimport unique_ptr\n from libcpp.pair cimport pair\n \n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n-from cudf._lib.cpp.types cimport size_type\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.types cimport size_type\n \n \n cdef extern from \"cuspatial/point_quadtree.hpp\" namespace \"cuspatial\" nogil:\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/spatial_join.pxd b/python/cuspatial/cuspatial/_lib/cpp/spatial_join.pxd\nindex 948aa77c9..ab96cf53c 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/spatial_join.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/spatial_join.pxd\n@@ -1,10 +1,10 @@\n-# Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libc.stdint cimport int8_t\n from libcpp.memory cimport unique_ptr\n \n from cudf._lib.column cimport column_view\n-from cudf._lib.cpp.table.table cimport table, table_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table, table_view\n \n \n cdef extern from \"cuspatial/spatial_join.hpp\" namespace \"cuspatial\" nogil:\ndiff --git a/python/cuspatial/cuspatial/_lib/cpp/trajectory.pxd b/python/cuspatial/cuspatial/_lib/cpp/trajectory.pxd\nindex 9031d17eb..5940349cc 100644\n--- a/python/cuspatial/cuspatial/_lib/cpp/trajectory.pxd\n+++ b/python/cuspatial/cuspatial/_lib/cpp/trajectory.pxd\n@@ -1,12 +1,12 @@\n-# Copyright (c) 2020, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.pair cimport pair\n \n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n-from cudf._lib.cpp.types cimport size_type\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.types cimport size_type\n \n \n cdef extern from \"cuspatial/trajectory.hpp\" namespace \"cuspatial\" nogil:\ndiff --git a/python/cuspatial/cuspatial/_lib/distance.pyx b/python/cuspatial/cuspatial/_lib/distance.pyx\nindex cf61773e0..90e7b5ac0 100644\n--- a/python/cuspatial/cuspatial/_lib/distance.pyx\n+++ b/python/cuspatial/cuspatial/_lib/distance.pyx\n@@ -1,12 +1,12 @@\n-# Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2022-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport make_shared, shared_ptr, unique_ptr\n from libcpp.utility cimport move, pair\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table_view cimport table_view\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table_view cimport table_view\n from cudf._lib.utils cimport columns_from_table_view\n \n from cuspatial._lib.cpp.column.geometry_column_view cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/linestring_bounding_boxes.pyx b/python/cuspatial/cuspatial/_lib/linestring_bounding_boxes.pyx\nindex 28815cc57..4023f4965 100644\n--- a/python/cuspatial/cuspatial/_lib/linestring_bounding_boxes.pyx\n+++ b/python/cuspatial/cuspatial/_lib/linestring_bounding_boxes.pyx\n@@ -1,11 +1,11 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n from cudf._lib.utils cimport columns_from_unique_ptr\n \n from cuspatial._lib.cpp.linestring_bounding_boxes cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/nearest_points.pyx b/python/cuspatial/cuspatial/_lib/nearest_points.pyx\nindex d8ecebc58..da24357d3 100644\n--- a/python/cuspatial/cuspatial/_lib/nearest_points.pyx\n+++ b/python/cuspatial/cuspatial/_lib/nearest_points.pyx\n@@ -1,9 +1,9 @@\n-# Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2022-2024, NVIDIA CORPORATION.\n \n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n \n from cuspatial._lib.cpp.nearest_points cimport (\n     pairwise_point_linestring_nearest_points as c_func,\ndiff --git a/python/cuspatial/cuspatial/_lib/points_in_range.pyx b/python/cuspatial/cuspatial/_lib/points_in_range.pyx\nindex e54b60a6b..4c25c2dbf 100644\n--- a/python/cuspatial/cuspatial/_lib/points_in_range.pyx\n+++ b/python/cuspatial/cuspatial/_lib/points_in_range.pyx\n@@ -1,10 +1,10 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column, column_view\n-from cudf._lib.cpp.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n from cudf._lib.utils cimport data_from_unique_ptr\n \n from cuspatial._lib.cpp.points_in_range cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/polygon_bounding_boxes.pyx b/python/cuspatial/cuspatial/_lib/polygon_bounding_boxes.pyx\nindex d9419fe7b..0f963fd5a 100644\n--- a/python/cuspatial/cuspatial/_lib/polygon_bounding_boxes.pyx\n+++ b/python/cuspatial/cuspatial/_lib/polygon_bounding_boxes.pyx\n@@ -1,11 +1,11 @@\n-# Copyright (c) 2020-2023, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n from cudf._lib.utils cimport columns_from_unique_ptr\n \n from cuspatial._lib.cpp.polygon_bounding_boxes cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/quadtree.pyx b/python/cuspatial/cuspatial/_lib/quadtree.pyx\nindex de8246bc3..eb548a882 100644\n--- a/python/cuspatial/cuspatial/_lib/quadtree.pyx\n+++ b/python/cuspatial/cuspatial/_lib/quadtree.pyx\n@@ -1,4 +1,4 @@\n-# Copyright (c) 2020, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libc.stdint cimport int8_t\n from libcpp.memory cimport unique_ptr\n@@ -6,10 +6,10 @@ from libcpp.pair cimport pair\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n-from cudf._lib.cpp.types cimport size_type\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.types cimport size_type\n from cudf._lib.utils cimport data_from_unique_ptr\n \n from cuspatial._lib.cpp.quadtree cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/spatial.pyx b/python/cuspatial/cuspatial/_lib/spatial.pyx\nindex 916d66cec..6698da904 100644\n--- a/python/cuspatial/cuspatial/_lib/spatial.pyx\n+++ b/python/cuspatial/cuspatial/_lib/spatial.pyx\n@@ -1,12 +1,12 @@\n-# Copyright (c) 2019, NVIDIA CORPORATION.\n+# Copyright (c) 2019-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.pair cimport pair\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n \n from cuspatial._lib.cpp.projection cimport (\n     sinusoidal_projection as cpp_sinusoidal_projection,\ndiff --git a/python/cuspatial/cuspatial/_lib/spatial_join.pyx b/python/cuspatial/cuspatial/_lib/spatial_join.pyx\nindex aab04c938..045014aba 100644\n--- a/python/cuspatial/cuspatial/_lib/spatial_join.pyx\n+++ b/python/cuspatial/cuspatial/_lib/spatial_join.pyx\n@@ -1,11 +1,11 @@\n-# Copyright (c) 2020-2022, NVIDIA CORPORATION.\n+# Copyright (c) 2020-2024, NVIDIA CORPORATION.\n \n from libc.stdint cimport int8_t\n from libcpp.memory cimport unique_ptr\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column, column_view\n-from cudf._lib.cpp.table.table cimport table, table_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table, table_view\n from cudf._lib.utils cimport data_from_unique_ptr, table_view_from_table\n \n from cuspatial._lib.cpp.spatial_join cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/trajectory.pyx b/python/cuspatial/cuspatial/_lib/trajectory.pyx\nindex 012439904..df29dc05a 100644\n--- a/python/cuspatial/cuspatial/_lib/trajectory.pyx\n+++ b/python/cuspatial/cuspatial/_lib/trajectory.pyx\n@@ -1,14 +1,14 @@\n-# Copyright (c) 2019-2020, NVIDIA CORPORATION.\n+# Copyright (c) 2019-2024, NVIDIA CORPORATION.\n \n from libcpp.memory cimport unique_ptr\n from libcpp.pair cimport pair\n from libcpp.utility cimport move\n \n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column cimport column\n-from cudf._lib.cpp.column.column_view cimport column_view\n-from cudf._lib.cpp.table.table cimport table\n-from cudf._lib.cpp.types cimport size_type\n+from cudf._lib.pylibcudf.libcudf.column.column cimport column\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.table.table cimport table\n+from cudf._lib.pylibcudf.libcudf.types cimport size_type\n from cudf._lib.utils cimport data_from_unique_ptr\n \n from cuspatial._lib.cpp.trajectory cimport (\ndiff --git a/python/cuspatial/cuspatial/_lib/utils.pxd b/python/cuspatial/cuspatial/_lib/utils.pxd\nindex 0567434d1..ad1714016 100644\n--- a/python/cuspatial/cuspatial/_lib/utils.pxd\n+++ b/python/cuspatial/cuspatial/_lib/utils.pxd\n@@ -1,5 +1,5 @@\n-# Copyright (c) 2022, NVIDIA CORPORATION.\n-from cudf._lib.cpp.column.column_view cimport column_view\n+# Copyright (c) 2022-2024, NVIDIA CORPORATION.\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n \n from cuspatial._lib.cpp.optional cimport nullopt, optional\n \ndiff --git a/python/cuspatial/cuspatial/_lib/utils.pyx b/python/cuspatial/cuspatial/_lib/utils.pyx\nindex cabad333c..4aea62863 100644\n--- a/python/cuspatial/cuspatial/_lib/utils.pyx\n+++ b/python/cuspatial/cuspatial/_lib/utils.pyx\n@@ -1,6 +1,6 @@\n-# Copyright (c) 2022, NVIDIA CORPORATION.\n+# Copyright (c) 2022-2024, NVIDIA CORPORATION.\n from cudf._lib.column cimport Column\n-from cudf._lib.cpp.column.column_view cimport column_view\n+from cudf._lib.pylibcudf.libcudf.column.column_view cimport column_view\n \n from cuspatial._lib.cpp.optional cimport nullopt, optional\n \ndiff --git a/python/cuspatial/pyproject.toml b/python/cuspatial/pyproject.toml\nindex e58b6bd54..b4ae45e9e 100644\n--- a/python/cuspatial/pyproject.toml\n+++ b/python/cuspatial/pyproject.toml\n@@ -16,10 +16,10 @@\n build-backend = \"scikit_build_core.build\"\n requires = [\n     \"cmake>=3.26.4\",\n-    \"cudf==24.4.*\",\n+    \"cudf==24.6.*\",\n     \"cython>=3.0.0\",\n     \"ninja\",\n-    \"rmm==24.4.*\",\n+    \"rmm==24.6.*\",\n     \"scikit-build-core[pyproject]>=0.7.0\",\n     \"setuptools\",\n     \"wheel\",\n@@ -36,10 +36,10 @@ authors = [\n license = { text = \"Apache 2.0\" }\n requires-python = \">=3.9\"\n dependencies = [\n-    \"cudf==24.4.*\",\n+    \"cudf==24.6.*\",\n     \"geopandas>=0.11.0\",\n     \"numpy>=1.23,<2.0a0\",\n-    \"rmm==24.4.*\",\n+    \"rmm==24.6.*\",\n ] # This list was generated by `rapids-dependency-file-generator`. To make changes, edit ../../dependencies.yaml and run `rapids-dependency-file-generator`.\n classifiers = [\n     \"Intended Audience :: Developers\",\n@@ -124,5 +124,9 @@ regex = \"(?P<value>.*)\"\n [tool.pytest.ini_options]\n xfail_strict = true\n filterwarnings = [\n-    \"error:::cudf\"\n+    \"error:::cudf\",\n+    \"error::FutureWarning\",\n+    \"error::DeprecationWarning\",\n+    # https://github.com/pytest-dev/pytest-cov/issues/557\n+    \"ignore:The --rsyncdir command line argument:DeprecationWarning\",\n ]\n", "instance_id": "rapidsai__cuspatial-1391", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to replace `rmm::mr::device_memory_resource*` with `rmm::device_async_resource_ref` across the repository, as part of an update explained in a linked document. It also includes a bug report about unexpected behavior in the quadtree point-in-polygon functionality with larger datasets, providing a reproducible example and relevant log output. However, there are minor ambiguities and missing details. For the type replacement task, the problem statement lacks specific guidance on potential challenges or edge cases in the replacement process (e.g., compatibility issues or specific functions requiring special handling). For the bug report, while the issue is described with a reproducible example, it does not clearly specify the expected output or root cause hypothesis, nor does it outline constraints or performance expectations for a fix. Additionally, the connection between the type replacement and the bug fix is not explicitly stated, leaving some uncertainty about whether they are directly related or separate tasks. Overall, the statement is valid and mostly clear but misses some critical details for a fully comprehensive understanding.", "difficulty_explanation": "The difficulty score of 0.65 reflects a hard problem that requires a deep understanding of the codebase and involves complex modifications with significant impact. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The task of replacing `rmm::mr::device_memory_resource*` with `rmm::device_async_resource_ref` spans across multiple files and modules, as evidenced by the extensive code changes in the provided diff (e.g., affecting function signatures in headers and implementations across various components like `bounding_boxes.hpp`, `distance.hpp`, etc.). This is not a localized change but a systemic update impacting the memory management strategy of the library. Additionally, the bug fix for the quadtree point-in-polygon issue (related to OOM or silent errors with larger datasets) likely requires modifications in specific areas (e.g., `quadtree_point_in_polygon.cuh`), involving understanding of memory allocation and algorithmic efficiency. The changes also include updates to version numbers, build configurations, and dependency files, indicating a broad impact on the codebase.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several technical concepts, including CUDA memory management (specifically RMM and the transition to asynchronous resource references), GPU-accelerated geospatial algorithms (quadtrees, bounding box intersections, point-in-polygon tests), and build systems (CMake, Conda, dependency management). Understanding the implications of memory resource changes on stream-ordered allocations and ensuring compatibility with existing code is non-trivial. Additionally, debugging the quadtree issue involves knowledge of spatial indexing, data structures, and handling large datasets on GPUs, potentially requiring optimization techniques to prevent OOM errors.\n\n3. **Potential Edge Cases and Error Handling**: The type replacement task may introduce edge cases related to backward compatibility, performance regressions, or incorrect usage of asynchronous memory resources in certain contexts (e.g., older CUDA versions or specific hardware). The bug report highlights a specific edge case with larger datasets causing unexpected output (e.g., reduced row counts), suggesting the need for robust error handling and possibly redesigning memory allocation strategies or algorithmic approaches to handle scale. The provided code changes already show an attempt to address OOM issues by adjusting allocation strategies in `quadtree_point_in_polygon.cuh`, indicating non-trivial error handling considerations.\n\n4. **Overall Complexity**: The combination of a systemic API change and a specific bug fix pushes this task into the hard category. It requires a deep understanding of the RAPIDS ecosystem (cuSpatial, cuDF, RMM), CUDA programming nuances, and spatial algorithms. The impact of the memory resource change could affect performance and correctness across the library, necessitating thorough testing. The bug fix, while more localized, involves complex debugging of GPU code and handling large-scale data, which adds to the difficulty. However, it does not reach the \"very hard\" category (0.8-1.0) as it does not appear to involve system-level redesign or highly specialized domain knowledge beyond GPU geospatial computing.\n\nThus, a score of 0.65 appropriately captures the challenging nature of this task, requiring significant expertise and effort but not reaching the extreme complexity of a complete architectural overhaul or novel algorithm design.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Implement ExtensionArray _accumulate and _reduce\n**Describe the bug**\r\nThe stubs for ExtensionArray (in `pandas-stubs/core/arrays/base.pyi`) does not provide type signatures for _accumulate and _reduce.  To properly add typing information to the Pint-Pandas project, these need to be defined.\r\n\r\n**To Reproduce**\r\n1. Minimal Runnable Example:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom typing import reveal_type\r\nfrom pandas.arrays import IntegerArray\r\nfrom pandas.api.extensions import ExtensionArray\r\n\r\n_data: ExtensionArray = IntegerArray(values=np.array([1, 2, 3], dtype=int), mask=np.array([True, True, True], dtype=bool))\r\nif isinstance(_data, ExtensionArray):\r\n    reveal_type(_data)\r\n    reveal_type(_data._accumulate)\r\n    reveal_type(_data._reduce)\r\n```\r\n2.  Using `mypy`\r\n3.  Show the error message received from that type checker while checking your example.\r\n```\r\n(pint-dev) % pre-commit run mypy --files foo.py\r\nmypy.....................................................................Failed\r\n- hook id: mypy\r\n- duration: 1.41s\r\n- exit code: 1\r\n\r\nfoo.py:9: note: Revealed type is \"pandas.core.arrays.base.ExtensionArray\"\r\nfoo.py:10: error: \"ExtensionArray\" has no attribute \"_accumulate\"  [attr-defined]\r\nfoo.py:10: note: Revealed type is \"Any\"\r\nfoo.py:11: error: \"ExtensionArray\" has no attribute \"_reduce\"  [attr-defined]\r\nfoo.py:11: note: Revealed type is \"Any\"\r\nFound 2 errors in 1 file (checked 1 source file)\r\n```\r\n\r\nNote that running the script in python works, because it uses actual Pandas code, not Pandas-Stubs:\r\n```\r\n(pint-dev) % python foo.py\r\nRuntime type is 'IntegerArray'\r\nRuntime type is 'method'\r\nRuntime type is 'method'\r\n```\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Mac OS\r\n - OS Version  14.1.2\r\n - python 3.11.4\r\n - mypy 1.8.0\r\n - version of installed `pandas-stubs`: 2.1.4.231227\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\nImplement ExtensionArray _accumulate and _reduce\n**Describe the bug**\r\nThe stubs for ExtensionArray (in `pandas-stubs/core/arrays/base.pyi`) does not provide type signatures for _accumulate and _reduce.  To properly add typing information to the Pint-Pandas project, these need to be defined.\r\n\r\n**To Reproduce**\r\n1. Minimal Runnable Example:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom typing import reveal_type\r\nfrom pandas.arrays import IntegerArray\r\nfrom pandas.api.extensions import ExtensionArray\r\n\r\n_data: ExtensionArray = IntegerArray(values=np.array([1, 2, 3], dtype=int), mask=np.array([True, True, True], dtype=bool))\r\nif isinstance(_data, ExtensionArray):\r\n    reveal_type(_data)\r\n    reveal_type(_data._accumulate)\r\n    reveal_type(_data._reduce)\r\n```\r\n2.  Using `mypy`\r\n3.  Show the error message received from that type checker while checking your example.\r\n```\r\n(pint-dev) % pre-commit run mypy --files foo.py\r\nmypy.....................................................................Failed\r\n- hook id: mypy\r\n- duration: 1.41s\r\n- exit code: 1\r\n\r\nfoo.py:9: note: Revealed type is \"pandas.core.arrays.base.ExtensionArray\"\r\nfoo.py:10: error: \"ExtensionArray\" has no attribute \"_accumulate\"  [attr-defined]\r\nfoo.py:10: note: Revealed type is \"Any\"\r\nfoo.py:11: error: \"ExtensionArray\" has no attribute \"_reduce\"  [attr-defined]\r\nfoo.py:11: note: Revealed type is \"Any\"\r\nFound 2 errors in 1 file (checked 1 source file)\r\n```\r\n\r\nNote that running the script in python works, because it uses actual Pandas code, not Pandas-Stubs:\r\n```\r\n(pint-dev) % python foo.py\r\nRuntime type is 'IntegerArray'\r\nRuntime type is 'method'\r\nRuntime type is 'method'\r\n```\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Mac OS\r\n - OS Version  14.1.2\r\n - python 3.11.4\r\n - mypy 1.8.0\r\n - version of installed `pandas-stubs`: 2.1.4.231227\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n", "patch": "diff --git a/pandas-stubs/core/arrays/base.pyi b/pandas-stubs/core/arrays/base.pyi\nindex 7141f70f..9a632700 100644\n--- a/pandas-stubs/core/arrays/base.pyi\n+++ b/pandas-stubs/core/arrays/base.pyi\n@@ -56,6 +56,10 @@ class ExtensionArray:\n     def view(self, dtype=...) -> Self | np.ndarray: ...\n     def ravel(self, order=...) -> Self: ...\n     def tolist(self) -> list: ...\n+    def _reduce(\n+        self, name: str, *, skipna: bool = ..., keepdims: bool = ..., **kwargs\n+    ) -> object: ...\n+    def _accumulate(self, name: str, *, skipna: bool = ..., **kwargs) -> Self: ...\n \n class ExtensionOpsMixin:\n     @classmethod\n", "instance_id": "pandas-dev__pandas-stubs-923", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the type stubs for `ExtensionArray` in the `pandas-stubs` library are missing signatures for the `_accumulate` and `_reduce` methods, which causes errors when using type checkers like `mypy`. The goal is to add these type signatures to improve typing support for the Pint-Pandas project. The statement includes a minimal reproducible example, error messages from `mypy`, and runtime behavior in Python, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, the problem does not specify the expected return types or parameter constraints for `_accumulate` and `_reduce` beyond what is inferred from the code changes. Additionally, there is no mention of potential edge cases or specific behaviors that the type signatures should account for. While the intent is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The task involves adding type signatures for two methods (`_accumulate` and `_reduce`) in a single file (`pandas-stubs/core/arrays/base.pyi`), as shown in the code changes. The scope of the modification is minimal, requiring only a few lines of code with no impact on the broader codebase architecture or interactions between modules. The technical concepts involved are straightforward: familiarity with Python type hints, stub files (`.pyi`), and basic understanding of the `pandas` library's `ExtensionArray` class. No complex algorithms, design patterns, or domain-specific knowledge are required beyond what a developer with intermediate Python experience would know. Edge cases and error handling are not explicitly mentioned in the problem statement, and the provided code changes do not suggest significant complexity in this area. The primary challenge lies in ensuring the type signatures align with the actual runtime behavior of the methods in `pandas`, but this is a relatively simple task given the provided context and minimal example. Overall, this is a straightforward bug fix in the type stub file with limited scope and complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use official Python 3.13.0 for testing once released\nCurrently, we are using Python 3.13.0-rc.1 in the test workflow.\r\n\r\nThis ticket is to move to the officially released Python 3.13.0. Planned GA date is 2024-10-01 (see https://peps.python.org/pep-0719).\r\n\r\nAlso, reduce the set of test combinations for normal tests again.\n", "patch": "diff --git a/Makefile b/Makefile\nindex b75bec5a..82d7a740 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -308,12 +308,12 @@ dist_dependent_files_all := \\\n     README.md \\\n     README_PYPI.md \\\n     requirements.txt \\\n-\t\tpyproject.toml \\\n+    pyproject.toml \\\n     $(wildcard $( package_name)/*.py) \\\n     $(wildcard $(package_name)/*/*.py) \\\n     $(wildcard $(package_name)/*/*/*.py) \\\n \n-# The actually used dependency list, which removes the version file. Reason is that the\n+# The dependency list actually used, which removes the version file. Reason is that the\n # version file is rebuilt during build.\n dist_dependent_files := $(filter-out $(version_file), $(dist_dependent_files_all))\n \ndiff --git a/base-requirements.txt b/base-requirements.txt\nindex 2ae27727..ce35623d 100644\n--- a/base-requirements.txt\n+++ b/base-requirements.txt\n@@ -7,4 +7,4 @@\n pip>=23.3\n setuptools>=70.0.0\n setuptools-scm>=8.1.0\n-wheel>=0.38.1\n+wheel>=0.41.3\ndiff --git a/dev-requirements.txt b/dev-requirements.txt\nindex ef40591f..64b33321 100644\n--- a/dev-requirements.txt\n+++ b/dev-requirements.txt\n@@ -1,8 +1,8 @@\n # Pip requirements file for development dependencies.\n \n \n-# Direct dependencies for development and indirect dependencies for development\n-# that are needed for some reason (must be consistent with minimum-constraints-develop.txt)\n+# Direct and necessary indirect dependencies for development (must be consistent\n+# with minimum-constraints-develop.txt)\n \n # Build distribution archive\n build>=1.1.1\n@@ -56,7 +56,7 @@ sphinxcontrib-serializinghtml>=1.1.5; python_version == '3.8'\n sphinxcontrib-serializinghtml>=1.1.9; python_version >= '3.9'\n sphinxcontrib-websupport>=1.2.4\n autodocsumm>=0.2.12\n-Babel>=2.9.1\n+Babel>=2.11.0\n \n # PyLint (no imports, invoked via pylint script)\n pylint>=2.17.7; python_version <= '3.11'\n@@ -64,7 +64,7 @@ pylint>=3.2.0; python_version >= '3.12'\n astroid>=2.15.8; python_version <= '3.11'\n astroid>=3.2.0; python_version >= '3.12'\n lazy-object-proxy>=1.4.3\n-wrapt>=1.14\n+wrapt>=1.15\n platformdirs>=4.1.0\n isort>=4.3.8\n tomlkit>=0.10.1\ndiff --git a/docs/changes.rst b/docs/changes.rst\nindex 54f9e9b5..7b0eab97 100644\n--- a/docs/changes.rst\n+++ b/docs/changes.rst\n@@ -16,9 +16,10 @@ Released: not yet\n \n * Removed support for Python 2.7, 3.6, and 3.7. (issue #1390)\n \n-* Removed setup.py and therefore the possibility to run setup.py as a command,\n-  for example to install or test pywbemtools. Running setup.py as a command has\n-  been deprecated by Python.\n+* The migration from setup.py to pyproject.toml removed the possibility to run\n+  setup.py as a command, for example to install or test pywbemtools. Note that\n+  running setup.py as a command has been deprecated by the Python setuptools\n+  team.\n \n **Deprecations:**\n \n@@ -32,8 +33,13 @@ Released: not yet\n   with Python 3.9-3.11, by running it without login shell. Added Python 3.11 on\n   MacOS to the normal tests.\n \n+* Increased minimum versions of PyYAML to 6.0.2 and psutil to 6.0.0, to fix\n+  install errors with Python 3.13 on Windows. (related to issue #3225)\n+\n **Enhancements:**\n \n+* Added support for and testing on Python 3.13.0-rc.1. (issue #1429)\n+\n * Development: Changed release process to use a GitHub Actions workflow\n   add as documented in DEVELOP.md. (issue #1395)\n \n@@ -50,8 +56,7 @@ Released: not yet\n \n * Development: The pywbem version during development now uses an automatically\n   calculated dev number and the git commit hash, e.g. ``1.4.0a1.dev9+gad875911``.\n-  Note that the pywbem version numbers for packages released to Pypi is\n-  unchanged: M.N.U.\n+  The pywbem version numbers for packages released to Pypi are unchanged: M.N.U.\n   Updated the release description in DEVELOP.md to no longer edit the version\n   file.\n \ndiff --git a/minimum-constraints-develop.txt b/minimum-constraints-develop.txt\nindex a6bebaec..81d35ff3 100644\n--- a/minimum-constraints-develop.txt\n+++ b/minimum-constraints-develop.txt\n@@ -65,7 +65,7 @@ sphinxcontrib-serializinghtml==1.1.5; python_version == '3.8'\n sphinxcontrib-serializinghtml==1.1.9; python_version >= '3.9'\n sphinxcontrib-websupport==1.2.4\n autodocsumm==0.2.12\n-Babel==2.9.1\n+Babel==2.11.0\n \n # PyLint (no imports, invoked via pylint script)\n pylint==2.17.7; python_version <= '3.11'\n@@ -73,7 +73,7 @@ pylint==3.2.0; python_version >= '3.12'\n astroid==2.15.8; python_version <= '3.11'\n astroid==3.2.0; python_version >= '3.12'\n lazy-object-proxy==1.4.3\n-wrapt==1.14\n+wrapt==1.15\n platformdirs==4.1.0\n isort==4.3.8\n tomlkit==0.10.1\ndiff --git a/minimum-constraints-install.txt b/minimum-constraints-install.txt\nindex 1a4b9182..3fa11b02 100644\n--- a/minimum-constraints-install.txt\n+++ b/minimum-constraints-install.txt\n@@ -13,7 +13,7 @@\n pip==23.3\n setuptools==70.0.0\n setuptools-scm==8.1.0\n-wheel==0.38.1\n+wheel==0.41.3\n \n \n # Direct dependencies for install (must be consistent with requirements.txt)\n@@ -35,12 +35,11 @@ tabulate==0.8.2; python_version <= '3.9'\n tabulate==0.8.8; python_version >= '3.10'\n \n toposort==1.6\n-psutil==5.6.6; python_version <= '3.9'\n-psutil==5.8.0; python_version >= '3.10'\n+psutil==6.0.0\n \n prompt-toolkit==3.0.13\n \n-PyYAML==5.3.1\n+PyYAML==6.0.2\n \n yamlloader==0.5.5\n # Packaging 21.0 is required by safety 2.2.0\ndiff --git a/pyproject.toml b/pyproject.toml\nindex ddfd7db4..a8265625 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -52,6 +52,7 @@ classifiers = [\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n     \"Programming Language :: Python :: 3.12\",\n+    \"Programming Language :: Python :: 3.13\",\n ]\n requires-python = \">=3.8\"\n dynamic = [\"version\", \"dependencies\"]\ndiff --git a/requirements.txt b/requirements.txt\nindex 9e674a8e..5acf415a 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -34,21 +34,18 @@ tabulate>=0.8.2; python_version <= '3.9'\n tabulate>=0.8.8; python_version >= '3.10'\n \n toposort>=1.6\n-# psutil 5.8.0 fixes an install error on Python 3.10\n-psutil>=5.6.6; python_version <= '3.9'\n-psutil>=5.8.0; python_version >= '3.10'\n+# psutil 6.0.0 fixes an install error on Python 3.13 on Windows\n+psutil>=6.0.0\n \n # prompt-toolkit>=3.0 may cause WinError 995 on py38 on Windows (issue #690).\n prompt-toolkit>=3.0.13\n \n # PyYAML is also pulled in by dparse and python-coveralls\n-# PyYAML 5.3 has wheel archives for Python 2.7, 3.5 - 3.9\n-# PyYAML 5.4 has wheel archives for Python 2.7, 3.6 - 3.9\n # PyYAML 6.0 has wheel archives for Python 3.6 - 3.11\n-# PyYAML 5.4 and 6.0.0 fails install since Cython 3 was released, see issue\n+# PyYAML 6.0.0 fails install since Cython 3 was released, see issue\n #   https://github.com/yaml/pyyaml/issues/724.\n-PyYAML>=5.3.1,!=5.4.0,!=5.4.1; python_version <= '3.11'\n-PyYAML>=5.3.1,!=5.4.0,!=5.4.1,!=6.0.0; python_version >= '3.12'\n+# PyYAML 6.0.2 provides wheel archives for Python 3.13 on Windows\n+PyYAML>=6.0.2\n \n yamlloader>=0.5.5\n \ndiff --git a/tox.ini b/tox.ini\nindex 9041cc16..0e796f04 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -15,6 +15,7 @@ envlist =\n     py310\n     py311\n     py312\n+    py313\n     win64_py38_32\n     win64_py38_64\n     win64_py39_32\n@@ -25,12 +26,15 @@ envlist =\n     win64_py311_64\n     win64_py312_32\n     win64_py312_64\n+    win64_py313_32\n+    win64_py313_64\n     cygwin32_py38\n     cygwin64_py38\n     cygwin64_py39\n     cygwin64_py310\n     cygwin64_py311\n     cygwin64_py312\n+    cygwin64_py313\n \n # For Appveyor, missing interpreters should fail. For local use, you may\n # want to allow to skip missing interpreters.\n@@ -102,6 +106,10 @@ basepython = python3.11\n platform = linux2|darwin\n basepython = python3.12\n \n+[testenv:py313]\n+platform = linux2|darwin\n+basepython = python3.13\n+\n # Note: The basepython file paths for the win64* tox environments may need to\n #       be customized.\n \n@@ -145,6 +153,14 @@ basepython = C:\\Python312\\python.exe\n platform = win32\n basepython = C:\\Python312-x64\\python.exe\n \n+[testenv:win64_py313_32]\n+platform = win32\n+basepython = C:\\Python313\\python.exe\n+\n+[testenv:win64_py313_64]\n+platform = win32\n+basepython = C:\\Python313-x64\\python.exe\n+\n [testenv:cygwin32_py38]\n platform = cygwin\n basepython = python3.8\n@@ -168,3 +184,7 @@ basepython = python3.11\n [testenv:cygwin64_py312]\n platform = cygwin\n basepython = python3.12\n+\n+[testenv:cygwin64_py313]\n+platform = cygwin\n+basepython = python3.13\n", "instance_id": "pywbem__pywbemtools-1432", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to update the testing workflow from Python 3.13.0-rc.1 to the official Python 3.13.0 release and to reduce the set of test combinations for normal tests. It provides a specific target date for the Python release (2024-10-01) and references a relevant PEP document for context. However, there are minor ambiguities and missing details. For instance, it does not explicitly define what \"reduce the set of test combinations for normal tests\" entails\u2014whether it means fewer Python versions, platforms, or specific test cases. Additionally, there are no examples or specifics about potential compatibility issues with Python 3.13.0 or constraints to consider during the transition. While the goal is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are spread across multiple files (Makefile, requirements.txt, tox.ini, pyproject.toml, etc.), but they are mostly straightforward updates to version numbers (e.g., Python 3.13, PyYAML 6.0.2, psutil 6.0.0) and configuration settings to include Python 3.13 in testing environments. The changes do not impact the core architecture of the system and are limited to build, dependency, and test configurations. The overall amount of code change is moderate but not complex.\n\n2. **Number of Technical Concepts:** The task requires basic familiarity with Python dependency management (e.g., pip, requirements.txt), build tools (e.g., Makefile, pyproject.toml), and testing frameworks (e.g., tox.ini for environment configuration). It also involves understanding version compatibility issues, as seen in the comments about fixing installation errors on Python 3.13 for Windows. However, these concepts are relatively basic for a software engineer and do not involve advanced algorithms, design patterns, or domain-specific knowledge.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes address specific compatibility issues (e.g., PyYAML and psutil updates for Python 3.13 on Windows). The error handling logic appears to be minimal and is addressed through version updates rather than complex code modifications. The edge cases are not particularly intricate.\n\n4. **Overall Complexity:** This task involves routine maintenance work\u2014updating dependencies and test configurations to support a new Python version. It requires understanding the build and test setup but does not demand deep architectural changes or advanced problem-solving skills. The primary challenge lies in ensuring compatibility across platforms (e.g., Windows, Linux, Cygwin), but the provided changes already address these concerns.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only a basic understanding of the codebase and straightforward modifications to configuration files. It is slightly above the \"Very Easy\" range due to the need to coordinate changes across multiple files and verify compatibility, but it remains an easy task for a developer with moderate experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Build a CSVToDocument Component\n**Is your feature request related to a problem? Please describe.**\r\nHaystack currently supports conversion of several file formats such as `.txt`, `.pdf` and `markdown`. It would be very useful to also have a Component that converts a csv format file into a list of `Document` objects.\r\n\r\n**Describe the solution you'd like**\r\nI would like to implement a CSVToDocument that will load CSV files into a sequence of `Document` objects. Each row of the CSV file should be translated to one document. I think this could be the best choice since it is true that each line of a CSV file is usually a different data record. \r\n\r\nEach row could be converted into a key:value pair so that the Document output could be the following:\r\n\r\n`Document(id=XXX, content: 'column1: value1\\ncolumn2: value2\\ncolumn3: value3', meta: {'row': 0, 'source': './example.csv'})\r\n`\r\n\r\n**Describe alternatives you've considered**\r\n[Unstructured](https://docs.haystack.deepset.ai/docs/unstructuredfileconverter) is already present in Haystack so there is already a method to convert .csv files. Nevertheless, I think it is useful to have a component specifically designed for this purpose that does not go through the generation of an API key to communicate with an external service.\r\n\n", "patch": "diff --git a/docs/pydoc/config/converters_api.yml b/docs/pydoc/config/converters_api.yml\nindex e8ec88c7a2..945ae37430 100644\n--- a/docs/pydoc/config/converters_api.yml\n+++ b/docs/pydoc/config/converters_api.yml\n@@ -13,7 +13,8 @@ loaders:\n         \"txt\",\n         \"output_adapter\",\n         \"openapi_functions\",\n-        \"docx\"\n+        \"docx\",\n+        \"csv\"\n       ]\n     ignore_when_discovered: [\"__init__\"]\n processors:\ndiff --git a/haystack/components/converters/__init__.py b/haystack/components/converters/__init__.py\nindex bde66e1589..681ab85c35 100644\n--- a/haystack/components/converters/__init__.py\n+++ b/haystack/components/converters/__init__.py\n@@ -3,6 +3,7 @@\n # SPDX-License-Identifier: Apache-2.0\n \n from haystack.components.converters.azure import AzureOCRDocumentConverter\n+from haystack.components.converters.csv import CSVToDocument\n from haystack.components.converters.docx import DOCXMetadata, DOCXToDocument\n from haystack.components.converters.html import HTMLToDocument\n from haystack.components.converters.markdown import MarkdownToDocument\n@@ -27,4 +28,5 @@\n     \"DOCXToDocument\",\n     \"DOCXMetadata\",\n     \"PPTXToDocument\",\n+    \"CSVToDocument\",\n ]\ndiff --git a/haystack/components/converters/csv.py b/haystack/components/converters/csv.py\nnew file mode 100644\nindex 0000000000..721d8cf625\n--- /dev/null\n+++ b/haystack/components/converters/csv.py\n@@ -0,0 +1,93 @@\n+# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>\n+#\n+# SPDX-License-Identifier: Apache-2.0\n+\n+import io\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Union\n+\n+from haystack import Document, component, logging\n+from haystack.components.converters.utils import get_bytestream_from_source, normalize_metadata\n+from haystack.dataclasses import ByteStream\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@component\n+class CSVToDocument:\n+    \"\"\"\n+    Converts CSV files to Documents.\n+\n+    By default, it uses UTF-8 encoding when converting files but\n+    you can also set a custom encoding.\n+    It can attach metadata to the resulting documents.\n+\n+    ### Usage example\n+\n+    ```python\n+    from haystack.components.converters.csv import CSVToDocument\n+    converter = CSVToDocument()\n+    results = converter.run(sources=[\"sample.csv\"], meta={\"date_added\": datetime.now().isoformat()})\n+    documents = results[\"documents\"]\n+    print(documents[0].content)\n+    # 'col1,col2\\now1,row1\\nrow2row2\\n'\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, encoding: str = \"utf-8\"):\n+        \"\"\"\n+        Creates a CSVToDocument component.\n+\n+        :param encoding:\n+            The encoding of the csv files to convert.\n+            If the encoding is specified in the metadata of a source ByteStream,\n+            it overrides this value.\n+        \"\"\"\n+        self.encoding = encoding\n+\n+    @component.output_types(documents=List[Document])\n+    def run(\n+        self,\n+        sources: List[Union[str, Path, ByteStream]],\n+        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n+    ):\n+        \"\"\"\n+        Converts a CSV file to a Document.\n+\n+        :param sources:\n+            List of file paths or ByteStream objects.\n+        :param meta:\n+            Optional metadata to attach to the documents.\n+            This value can be either a list of dictionaries or a single dictionary.\n+            If it's a single dictionary, its content is added to the metadata of all produced documents.\n+            If it's a list, the length of the list must match the number of sources, because the two lists will\n+            be zipped.\n+            If `sources` contains ByteStream objects, their `meta` will be added to the output documents.\n+        :returns:\n+            A dictionary with the following keys:\n+            - `documents`: Created documents\n+        \"\"\"\n+        documents = []\n+\n+        meta_list = normalize_metadata(meta, sources_count=len(sources))\n+\n+        for source, metadata in zip(sources, meta_list):\n+            try:\n+                bytestream = get_bytestream_from_source(source)\n+            except Exception as e:\n+                logger.warning(\"Could not read {source}. Skipping it. Error: {error}\", source=source, error=e)\n+                continue\n+            try:\n+                encoding = bytestream.meta.get(\"encoding\", self.encoding)\n+                data = io.BytesIO(bytestream.data).getvalue().decode(encoding=encoding)\n+            except Exception as e:\n+                logger.warning(\n+                    \"Could not convert file {source}. Skipping it. Error message: {error}\", source=source, error=e\n+                )\n+                continue\n+\n+            merged_metadata = {**bytestream.meta, **metadata}\n+            document = Document(content=data, meta=merged_metadata)\n+            documents.append(document)\n+\n+        return {\"documents\": documents}\ndiff --git a/releasenotes/notes/add-csv-converter-5c0d52f180d498f5.yaml b/releasenotes/notes/add-csv-converter-5c0d52f180d498f5.yaml\nnew file mode 100644\nindex 0000000000..e5a01c9457\n--- /dev/null\n+++ b/releasenotes/notes/add-csv-converter-5c0d52f180d498f5.yaml\n@@ -0,0 +1,4 @@\n+---\n+features:\n+  - |\n+    Add a CSV to Document converter component. Loads the file as bytes object. Adds the loaded string as a new document that can be used for further processing by the Document Splitter.\n", "instance_id": "deepset-ai__haystack-8328", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of creating a `CSVToDocument` component to convert CSV files into a list of `Document` objects within the Haystack framework. It specifies the desired output format (each row as a separate `Document` with key-value pairs) and provides a rationale for the feature by comparing it to an existing alternative (`UnstructuredFileConverter`). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly address how to handle CSV-specific complexities such as delimiters, headers, or malformed rows. Additionally, while an example output format is provided, edge cases (e.g., empty files, missing columns, or special characters) are not mentioned. These omissions prevent it from being fully comprehensive, but the overall intent and structure are clear enough to proceed with implementation.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving the creation of a new file (`csv.py`) with a new component (`CSVToDocument`), as well as minor updates to configuration and initialization files. The changes are localized and do not significantly impact the broader system architecture, though they require integration with existing Haystack utilities like `get_bytestream_from_source` and `normalize_metadata`. Second, the technical concepts involved are relatively straightforward for someone familiar with Python and file handling, including encoding/decoding, metadata management, and basic error handling. However, understanding the Haystack framework's conventions for converters and document processing adds a layer of complexity. Third, while the problem statement does not explicitly mention edge cases, the code changes include basic error handling for file reading and encoding issues, suggesting some consideration of potential problems. Overall, this task requires a moderate level of understanding of the codebase and involves implementing a new feature with some complexity, but it does not demand deep architectural changes or advanced technical knowledge, placing it at 0.45 on the difficulty scale.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Lib] implement json.marshal_with_options builtin\nImplement [json.marshal_with_options](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonmarshal_with_options).\r\n\r\nEnsure that `cargo test -r --test opa -- jsonbuiltins` passes.\n", "patch": "diff --git a/README.md b/README.md\nindex fc5ff7ce..83c51b7d 100644\n--- a/README.md\n+++ b/README.md\n@@ -9,7 +9,7 @@\n Regorus is also\n   - *cross-platform* - Written in platform-agnostic Rust.\n   - *current* - We strive to keep Regorus up to date with latest OPA release. Regorus supports `import rego.v1`.\n-  - *compliant* - Regorus is mostly compliant with the latest [OPA release v0.63.0](https://github.com/open-policy-agent/opa/releases/tag/v0.63.0). See [OPA Conformance](#opa-conformance) for details. Note that while we behaviorally produce the same results, we don't yet support all the builtins.\n+  - *compliant* - Regorus is mostly compliant with the latest [OPA release v0.64.0](https://github.com/open-policy-agent/opa/releases/tag/v0.64.0). See [OPA Conformance](#opa-conformance) for details. Note that while we behaviorally produce the same results, we don't yet support all the builtins.\n   - *extensible* - Extend the Rego language by implementing custom stateful builtins in Rust.\n     See [add_extension](https://github.com/microsoft/regorus/blob/fc68bf9c8bea36427dae9401a7d1f6ada771f7ab/src/engine.rs#L352).\n     Support for extensibility using other languages coming soon.\n@@ -69,7 +69,7 @@ $ cargo build -r --example regorus --features \"yaml\" --no-default-features; stri\n -rwxr-xr-x  1 anand  staff   2.9M Jan 19 11:26 target/release/examples/regorus*\n ```\n \n-Regorus passes the [OPA v0.63.0 test-suite](https://www.openpolicyagent.org/docs/latest/ir/#test-suite) barring a few\n+Regorus passes the [OPA v0.64.0 test-suite](https://www.openpolicyagent.org/docs/latest/ir/#test-suite) barring a few\n builtins. See [OPA Conformance](#opa-conformance) below.\n \n ## Bindings\n@@ -245,7 +245,7 @@ Benchmark 1: opa eval -b tests/aci -d tests/aci/data.json -i tests/aci/input.jso\n ```\n ## OPA Conformance\n \n-Regorus has been verified to be compliant with [OPA v0.63.0](https://github.com/open-policy-agent/opa/releases/tag/v0.63.0)\n+Regorus has been verified to be compliant with [OPA v0.64.0](https://github.com/open-policy-agent/opa/releases/tag/v0.64.0)\n using a [test driver](https://github.com/microsoft/regorus/blob/main/tests/opa.rs) that loads and runs the OPA testsuite using Regorus, and verifies that expected outputs are produced.\n \n The test driver can be invoked by running:\ndiff --git a/docs/builtins.md b/docs/builtins.md\nindex 71e99029..9eafd6e0 100644\n--- a/docs/builtins.md\n+++ b/docs/builtins.md\n@@ -159,24 +159,25 @@ In future, each builtin will be associated with a feature (many builtins could b\n   | [type_name](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-types-type_name)   | _       |\n   \n - [Encoding](https://www.openpolicyagent.org/docs/latest/policy-reference/#encoding)\n-  | Builtin                                                                                                                          | Feature     |\n-  |----------------------------------------------------------------------------------------------------------------------------------|-------------|\n-  | [base64.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64is_valid)                 | `base64`    |\n-  | [base64url.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urldecode)               | `base64`    |\n-  | [base64url.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urlencode)               | `base64url` |\n-  | [base64url.encode_no_pad](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urlencode_no_pad) | `base64url` |\n-  | [hex.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-hexdecode)                           | `hex`       |\n-  | [hex.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-hexencode)                           | `hex`       |\n-  | [json.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonis_valid)                     | _           |\n-  | [json.marshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonmarshal)                       | _           |\n-  | [json.unmarshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonunmarshal)                   | _           |\n-  | [urlquery.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlquerydecode)                 | `urlquery`  |\n-  | [urlquery.decode_object](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlquerydecode_object)   | `urlquery`  |\n-  | [urlquery.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlqueryencode)                 | `urlquery`  |\n-  | [urlquery.encode_object](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlqueryencode_object)   | `urlquery`  |\n-  | [yaml.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlis_valid)                     | `yaml`      |\n-  | [yaml.marshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlmarshal)                       | `yaml`      |\n-  | [yaml.unmarshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlunmarshal)                   | `yaml`            |\n+  | Builtin                                                                                                                              | Feature     |\n+  |--------------------------------------------------------------------------------------------------------------------------------------|-------------|\n+  | [base64.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64is_valid)                     | `base64`    |\n+  | [base64url.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urldecode)                   | `base64`    |\n+  | [base64url.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urlencode)                   | `base64url` |\n+  | [base64url.encode_no_pad](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-base64urlencode_no_pad)     | `base64url` |\n+  | [hex.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-hexdecode)                               | `hex`       |\n+  | [hex.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-hexencode)                               | `hex`       |\n+  | [json.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonis_valid)                         | _           |\n+  | [json.marshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonmarshal)                           | _           |\n+  | [json.marshal_with_options](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonmarshal_with_options) | _           |\n+  | [json.unmarshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-jsonunmarshal)                       | _           |\n+  | [urlquery.decode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlquerydecode)                     | `urlquery`  |\n+  | [urlquery.decode_object](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlquerydecode_object)       | `urlquery`  |\n+  | [urlquery.encode](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlqueryencode)                     | `urlquery`  |\n+  | [urlquery.encode_object](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-urlqueryencode_object)       | `urlquery`  |\n+  | [yaml.is_valid](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlis_valid)                         | `yaml`      |\n+  | [yaml.marshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlmarshal)                           | `yaml`      |\n+  | [yaml.unmarshal](https://www.openpolicyagent.org/docs/latest/policy-reference/#builtin-encoding-yamlunmarshal)                       | `yaml`      |\n \n - [Time](https://www.openpolicyagent.org/docs/latest/policy-reference/#time)\n    | Builtin                                                                                                                    | Feature |\ndiff --git a/src/builtins/encoding.rs b/src/builtins/encoding.rs\nindex 16a51d66..7ac34054 100644\n--- a/src/builtins/encoding.rs\n+++ b/src/builtins/encoding.rs\n@@ -42,6 +42,7 @@ pub fn register(m: &mut HashMap<&'static str, builtins::BuiltinFcn>) {\n     }\n     m.insert(\"json.is_valid\", (json_is_valid, 1));\n     m.insert(\"json.marshal\", (json_marshal, 1));\n+    m.insert(\"json.marshal_with_options\", (json_marshal_with_options, 2));\n     m.insert(\"json.unmarshal\", (json_unmarshal, 1));\n \n     #[cfg(feature = \"yaml\")]\n@@ -368,6 +369,74 @@ fn json_marshal(span: &Span, params: &[Ref<Expr>], args: &[Value], _strict: bool\n     ))\n }\n \n+fn json_marshal_with_options(\n+    span: &Span,\n+    params: &[Ref<Expr>],\n+    args: &[Value],\n+    _strict: bool,\n+) -> Result<Value> {\n+    let name = \"json.marshal_with_options\";\n+    ensure_args_count(span, name, params, args, 2)?;\n+\n+    let options = ensure_object(name, &params[1], args[1].clone())?;\n+    let (mut pretty, mut indent, mut prefix) = (true, Some(\"\\t\".to_owned()), None);\n+    for (option, option_value) in options.iter() {\n+        match option {\n+            Value::String(s) if s.as_ref() == \"pretty\" && option_value.as_bool().is_ok() => {\n+                pretty = option_value == &Value::Bool(true);\n+            }\n+            Value::String(s) if s.as_ref() == \"pretty\" => bail!(params[1]\n+                .span()\n+                .error(\"marshaling option `pretty` must be true or false\")),\n+            Value::String(s) if s.as_ref() == \"prefix\" && option_value.as_string().is_ok() => {\n+                prefix = Some(option_value.as_string()?.as_ref().to_string());\n+            }\n+            Value::String(s) if s.as_ref() == \"prefix\" => bail!(params[1]\n+                .span()\n+                .error(\"marshaling option `pretty` must be string\")),\n+            Value::String(s) if s.as_ref() == \"indent\" && option_value.as_string().is_ok() => {\n+                indent = Some(option_value.as_string()?.as_ref().to_string());\n+            }\n+            Value::String(s) if s.as_ref() == \"indent\" => bail!(params[1]\n+                .span()\n+                .error(\"marshaling option `pretty` must be string\")),\n+            _ => bail!(params[1]\n+                .span()\n+                .error(\"marshaling option must be one of `indent`, `prefix` or `pretty`\")),\n+        }\n+    }\n+\n+    if !pretty || options.is_empty() {\n+        return Ok(Value::String(\n+            serde_json::to_string(&args[0])\n+                .with_context(|| span.error(\"could not serialize to json\"))?\n+                .into(),\n+        ));\n+    }\n+\n+    let lines: Vec<String> = serde_json::to_string_pretty(&args[0])\n+        .with_context(|| span.error(\"could not serialize to json\"))?\n+        .split('\\n')\n+        .map(|line| {\n+            let mut line = line.to_string();\n+\n+            if let Some(indent) = &indent {\n+                let start_trimmed = line.trim_start();\n+                let leading_spaces = line.len() - start_trimmed.len();\n+                let indentation_level = leading_spaces / 2;\n+                line = indent.repeat(indentation_level) + start_trimmed;\n+            }\n+\n+            if let Some(prefix) = &prefix {\n+                line = prefix.to_owned() + &line;\n+            }\n+            line\n+        })\n+        .collect();\n+\n+    Ok(Value::from(lines.join(\"\\n\")))\n+}\n+\n fn json_unmarshal(\n     span: &Span,\n     params: &[Ref<Expr>],\n", "instance_id": "microsoft__regorus-219", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to implement the `json.marshal_with_options` built-in function as per the Open Policy Agent (OPA) documentation. It provides a reference link to the OPA documentation and specifies a test command to verify the implementation. However, it lacks critical details such as explicit input/output formats, specific behavior expectations for the options (e.g., `pretty`, `indent`, `prefix`), and any edge cases or constraints that need to be handled. While the referenced documentation can fill some gaps, the problem statement itself does not comprehensively cover these aspects, leaving minor ambiguities for the developer to resolve by inferring from the OPA documentation or test suite.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily involving the addition of a new function (`json.marshal_with_options`) in a single file (`src/builtins/encoding.rs`) and minor documentation updates in `README.md` and `builtins.md`. The implementation requires understanding Rust's `serde_json` library for JSON serialization, handling of custom formatting options (pretty printing, custom indent, and prefix), and integrating with the existing Regorus framework for built-in functions. The logic involves parsing an options object, validating its fields, and applying custom formatting to the JSON output, which introduces moderate complexity. Additionally, the developer must handle potential edge cases such as invalid option values or serialization errors, though these are not explicitly detailed in the problem statement. The changes do not significantly impact the broader system architecture but require a good grasp of Rust's error handling (`Result` type) and string manipulation. Overall, this task requires understanding multiple concepts and making moderately complex modifications, justifying a score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add missing functions for Redshift\n**Describe the bug**\r\nMany redshift functions cannot be mocked, and should be implemented as UDFs\r\n\r\n**To Reproduce**\r\n```python3\r\nfrom pytest_mock_resources import create_redshift_fixture\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy.sql import text\r\n\r\nsession = create_redshift_fixture(session=True)\r\n\r\n\r\ndef test_len(session: Session):\r\n    q = f\"select len('apple')\"\r\n\r\n    res = session.execute(text(q))\r\n    item = res.fetchone()\r\n\r\n    assert item is not None\r\n    assert item[0] == 5\r\n```\r\n\r\n**Expected behavior**\r\nTest success\r\n\r\n**Actual Behavior**\r\n```console\r\nE       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) function len(character varying) does not exist\r\nE       LINE 1: select len('apple')\r\nE                      ^\r\nE       HINT:  No function matches the given name and argument types. You might need to add explicit type casts.\r\nE\r\nE       [SQL: select len('apple')]\r\nE       (Background on this error at: https://sqlalche.me/e/20/f405)\r\n```\r\n\r\n**Additional context**\r\nThis happens because the LEN function is not handled by sqlalchemy as is, and is not defined in [the custom fixtures](https://github.com/schireson/pytest-mock-resources/blob/main/src/pytest_mock_resources/fixture/redshift/udf.py)\r\n\r\nA non-exhaustive list of functions I frequently use that are missing:\r\n- [x] Len\r\n- [ ] Median\r\n- [x] Convert_timezone\r\n- [ ] Listagg\r\n\r\nI have a PR ready for [LEN](https://github.com/schireson/pytest-mock-resources/pull/212) to begin with, I will try to implement the other ones later.\r\n\r\nI can split this into multiple issues if you prefer\n", "patch": "diff --git a/poetry.lock b/poetry.lock\nindex 99115d5..c870216 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1,4 +1,4 @@\n-# This file is automatically @generated by Poetry 1.7.1 and should not be changed by hand.\n+# This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.\n \n [[package]]\n name = \"async-timeout\"\n@@ -862,6 +862,8 @@ files = [\n     {file = \"psycopg2-2.9.9-cp310-cp310-win_amd64.whl\", hash = \"sha256:426f9f29bde126913a20a96ff8ce7d73fd8a216cfb323b1f04da402d452853c3\"},\n     {file = \"psycopg2-2.9.9-cp311-cp311-win32.whl\", hash = \"sha256:ade01303ccf7ae12c356a5e10911c9e1c51136003a9a1d92f7aa9d010fb98372\"},\n     {file = \"psycopg2-2.9.9-cp311-cp311-win_amd64.whl\", hash = \"sha256:121081ea2e76729acfb0673ff33755e8703d45e926e416cb59bae3a86c6a4981\"},\n+    {file = \"psycopg2-2.9.9-cp312-cp312-win32.whl\", hash = \"sha256:d735786acc7dd25815e89cc4ad529a43af779db2e25aa7c626de864127e5a024\"},\n+    {file = \"psycopg2-2.9.9-cp312-cp312-win_amd64.whl\", hash = \"sha256:a7653d00b732afb6fc597e29c50ad28087dcb4fbfb28e86092277a559ae4e693\"},\n     {file = \"psycopg2-2.9.9-cp37-cp37m-win32.whl\", hash = \"sha256:5e0d98cade4f0e0304d7d6f25bbfbc5bd186e07b38eac65379309c4ca3193efa\"},\n     {file = \"psycopg2-2.9.9-cp37-cp37m-win_amd64.whl\", hash = \"sha256:7e2dacf8b009a1c1e843b5213a87f7c544b2b042476ed7755be813eaf4e8347a\"},\n     {file = \"psycopg2-2.9.9-cp38-cp38-win32.whl\", hash = \"sha256:ff432630e510709564c01dafdbe996cb552e0b9f3f065eb89bdce5bd31fabf4c\"},\n@@ -904,6 +906,7 @@ files = [\n     {file = \"psycopg2_binary-2.9.9-cp311-cp311-win32.whl\", hash = \"sha256:dc4926288b2a3e9fd7b50dc6a1909a13bbdadfc67d93f3374d984e56f885579d\"},\n     {file = \"psycopg2_binary-2.9.9-cp311-cp311-win_amd64.whl\", hash = \"sha256:b76bedd166805480ab069612119ea636f5ab8f8771e640ae103e05a4aae3e417\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:8532fd6e6e2dc57bcb3bc90b079c60de896d2128c5d9d6f24a63875a95a088cf\"},\n+    {file = \"psycopg2_binary-2.9.9-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:b0605eaed3eb239e87df0d5e3c6489daae3f7388d455d0c0b4df899519c6a38d\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8f8544b092a29a6ddd72f3556a9fcf249ec412e10ad28be6a0c0d948924f2212\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2d423c8d8a3c82d08fe8af900ad5b613ce3632a1249fd6a223941d0735fce493\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:2e5afae772c00980525f6d6ecf7cbca55676296b580c0e6abb407f15f3706996\"},\n@@ -912,6 +915,8 @@ files = [\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:cb16c65dcb648d0a43a2521f2f0a2300f40639f6f8c1ecbc662141e4e3e1ee07\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:911dda9c487075abd54e644ccdf5e5c16773470a6a5d3826fda76699410066fb\"},\n     {file = \"psycopg2_binary-2.9.9-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:57fede879f08d23c85140a360c6a77709113efd1c993923c59fde17aa27599fe\"},\n+    {file = \"psycopg2_binary-2.9.9-cp312-cp312-win32.whl\", hash = \"sha256:64cf30263844fa208851ebb13b0732ce674d8ec6a0c86a4e160495d299ba3c93\"},\n+    {file = \"psycopg2_binary-2.9.9-cp312-cp312-win_amd64.whl\", hash = \"sha256:81ff62668af011f9a48787564ab7eded4e9fb17a4a6a74af5ffa6a457400d2ab\"},\n     {file = \"psycopg2_binary-2.9.9-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:2293b001e319ab0d869d660a704942c9e2cce19745262a8aba2115ef41a0a42a\"},\n     {file = \"psycopg2_binary-2.9.9-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:03ef7df18daf2c4c07e2695e8cfd5ee7f748a1d54d802330985a78d2a5a6dca9\"},\n     {file = \"psycopg2_binary-2.9.9-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0a602ea5aff39bb9fac6308e9c9d82b9a35c2bf288e184a816002c9fae930b77\"},\n@@ -1236,6 +1241,7 @@ files = [\n     {file = \"PyYAML-6.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938\"},\n     {file = \"PyYAML-6.0.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d\"},\n     {file = \"PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515\"},\n+    {file = \"PyYAML-6.0.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290\"},\n     {file = \"PyYAML-6.0.1-cp310-cp310-win32.whl\", hash = \"sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924\"},\n     {file = \"PyYAML-6.0.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d\"},\n     {file = \"PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007\"},\n@@ -1243,8 +1249,16 @@ files = [\n     {file = \"PyYAML-6.0.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d\"},\n     {file = \"PyYAML-6.0.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc\"},\n     {file = \"PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673\"},\n+    {file = \"PyYAML-6.0.1-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b\"},\n     {file = \"PyYAML-6.0.1-cp311-cp311-win32.whl\", hash = \"sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741\"},\n     {file = \"PyYAML-6.0.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-win32.whl\", hash = \"sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df\"},\n     {file = \"PyYAML-6.0.1-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47\"},\n     {file = \"PyYAML-6.0.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98\"},\n     {file = \"PyYAML-6.0.1-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c\"},\n@@ -1261,6 +1275,7 @@ files = [\n     {file = \"PyYAML-6.0.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5\"},\n     {file = \"PyYAML-6.0.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696\"},\n     {file = \"PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735\"},\n+    {file = \"PyYAML-6.0.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6\"},\n     {file = \"PyYAML-6.0.1-cp38-cp38-win32.whl\", hash = \"sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206\"},\n     {file = \"PyYAML-6.0.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62\"},\n     {file = \"PyYAML-6.0.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8\"},\n@@ -1268,6 +1283,7 @@ files = [\n     {file = \"PyYAML-6.0.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6\"},\n     {file = \"PyYAML-6.0.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0\"},\n     {file = \"PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c\"},\n+    {file = \"PyYAML-6.0.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5\"},\n     {file = \"PyYAML-6.0.1-cp39-cp39-win32.whl\", hash = \"sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c\"},\n     {file = \"PyYAML-6.0.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486\"},\n     {file = \"PyYAML-6.0.1.tar.gz\", hash = \"sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43\"},\n@@ -1801,4 +1817,4 @@ redshift = [\"boto3\", \"filelock\", \"moto\", \"python-on-whales\", \"sqlparse\"]\n [metadata]\n lock-version = \"2.0\"\n python-versions = \">=3.7, <4\"\n-content-hash = \"dd611f684e06611ec8331ba6fadf2a9b9503291833b4c805b86918d7d898d2b5\"\n+content-hash = \"726dd9329ea0e5c4d8b3b8cde7a84a42324de2e91941c2fff47151ebe54aaef7\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 74e69a6..8abff94 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"pytest-mock-resources\"\n-version = \"2.10.3\"\n+version = \"2.10.4\"\n description = \"A pytest plugin for easily instantiating reproducible mock resources.\"\n authors = [\n     \"Omar Khan <oakhan3@gmail.com>\",\n@@ -54,7 +54,7 @@ filelock = {version = \"*\", optional = true}\n python-on-whales = {version = \">=0.22.0\", optional = true}\n \n [tool.poetry.dev-dependencies]\n-botocore = \">=1.31.63\"\n+botocore = \"1.33.13\"\n coverage = \"*\"\n moto = \">=2.3.2\"\n mypy = {version = \"0.982\"}\n", "instance_id": "schireson__pytest-mock-resources-212", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: certain Redshift functions like `LEN` are not implemented in the `pytest-mock-resources` library, leading to errors when trying to mock them in tests. The goal is to implement these as User-Defined Functions (UDFs) to enable proper testing. The statement includes a reproducible example, expected behavior, actual behavior with error output, and additional context about the missing functions. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the full scope of the implementation (e.g., whether all listed functions must be implemented in a single PR or can be split) and lacks specifics on how the UDFs should behave for edge cases or constraints. Additionally, while a PR for `LEN` is mentioned, the broader requirements for other functions like `MEDIAN` or `LISTAGG` are not detailed. Thus, while the problem is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: Based on the provided diff, the changes appear to be limited to dependency updates (e.g., Poetry lock file, version bumps, and dependency constraints in `pyproject.toml`). However, the actual implementation of the `LEN` function (and potentially others) is not shown in the diff, as it likely resides in a separate file or PR (referenced as PR #212). Assuming the implementation of a UDF for `LEN` is straightforward, it would likely involve a single file or module (e.g., `udf.py` as mentioned), with minimal impact on the broader codebase architecture. The changes shown are minor and do not suggest deep systemic modifications.\n\n2. **Technical Concepts Involved**: Solving this requires understanding of Python, SQLAlchemy, and the Redshift database dialect, as well as how to define UDFs for mocking purposes in a testing framework like `pytest-mock-resources`. Additionally, familiarity with the library's fixture system is necessary. While these concepts are not trivial, they are relatively standard for someone with experience in database testing and Python, making this a moderately complex task but not overly challenging.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases for the `LEN` function or others, but implementing a UDF would likely require handling basic edge cases (e.g., null inputs, empty strings). These are not particularly complex for a function like `LEN`, though functions like `MEDIAN` or `LISTAGG` (not yet implemented) could introduce more complexity. Error handling logic would need to be added or modified to match Redshift's behavior, but this is likely straightforward for the current scope.\n\n4. **Overall Complexity**: The task of implementing a single UDF like `LEN` is relatively simple, involving well-defined logic (e.g., returning the length of a string). The broader issue mentions additional functions (`MEDIAN`, `LISTAGG`, etc.), which could increase difficulty, but since the current PR focuses on `LEN`, I am evaluating based on that scope. The dependency updates shown in the diff are trivial and do not add to the difficulty.\n\nGiven these considerations, a score of 0.35 reflects an \"Easy\" problem that requires understanding some specific library and database concepts, along with simple modifications to add a UDF. It does not involve deep architectural changes or highly complex logic, though it is slightly above the lowest end of the range due to the need for domain-specific knowledge of Redshift and testing fixtures.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Make ADD_SPECIAL_TOKENS true by default\nBefore this change the ADD_SPECIAL_TOKENS acted as a tristate variable where the default (not set) would be to add special tokens only if the model didn't have a chat template.\r\n\r\nHowever, this this default broke existing integration tests so, after some deliberation,  the decision was made to make the default to be \"true\". Due to the way that environment variables work, this means that this setting now only has 2 states: true and false.\r\n\nbug: tokenization in Tokenize and Generate does not match if ADD_SPECIAL_TOKENS=False\nThe usage of `add_special_tokens` in Generate is not replicated in Tokenize. This means that the tokenization result/count will be different if `ADD_SPECIAL_TOKENS=False`.\r\n\r\nLet's make sure to add a unit test and integration test for it if applicable!\n", "patch": "diff --git a/src/vllm_tgis_adapter/grpc/grpc_server.py b/src/vllm_tgis_adapter/grpc/grpc_server.py\nindex 1ee9bccc..99c981a5 100644\n--- a/src/vllm_tgis_adapter/grpc/grpc_server.py\n+++ b/src/vllm_tgis_adapter/grpc/grpc_server.py\n@@ -855,7 +855,9 @@ async def Tokenize(\n         # other threads\n         for req in request.requests:\n             batch_encoding = tokenizer.encode_plus(\n-                text=req.text, return_offsets_mapping=request.return_offsets\n+                text=req.text,\n+                return_offsets_mapping=request.return_offsets,\n+                add_special_tokens=ADD_SPECIAL_TOKENS,\n             )\n \n             # Tokenize the input text\n", "instance_id": "opendatahub-io__vllm-tgis-adapter-144", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal of changing the default behavior of `ADD_SPECIAL_TOKENS` to `true` and addressing a bug where tokenization behavior differs between `Tokenize` and `Generate` when `ADD_SPECIAL_TOKENS=False`. It also mentions the need for unit and integration tests. However, there are minor ambiguities: the problem does not explicitly define what \"special tokens\" are or provide examples of their impact on tokenization. Additionally, it lacks details on the expected behavior for edge cases (e.g., empty input strings or invalid inputs) and does not specify the exact scope of the integration/unit tests to be added. Despite these minor gaps, the overall intent and required changes are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change provided is minimal, involving a single-line modification in one file (`grpc_server.py`) to pass the `ADD_SPECIAL_TOKENS` parameter to the `tokenizer.encode_plus` method, ensuring consistent tokenization behavior. The scope of the change is narrow, affecting only a specific function without requiring deep understanding of the broader codebase or system architecture. The technical concepts involved are straightforward\u2014basic parameter passing and familiarity with a tokenizer library (likely Hugging Face's `transformers`). However, the problem also mentions adding unit and integration tests, which slightly increases the effort as it requires understanding the expected behavior and writing test cases for different configurations of `ADD_SPECIAL_TOKENS`. Edge cases and error handling are not explicitly mentioned in the problem or code changes, and the provided diff does not address complex scenarios, keeping the difficulty low. Overall, this task requires minimal code changes and a basic understanding of the tokenization logic, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add a new option: use_tls\nHello !\r\n\r\nThank you for the work done on this plugin ! I am currently using it for accessing JupyterHub in our data science team at my company, but I needed to add an option \"use_tls\" and set it to False to actually make it work. That is because we need to connect to a LDAP server that needs this configuration.\r\n\r\nFor information, such option exists in the LDAP connector used by Airflow, another tool that we are also using. There is distincts \"USE_SSL\" and \"USE_TLS\" options for addressing our use case, so I just applied the same logic.\r\n\r\nI hope that my PR is correct, I simply added the option \"use_tls\" and a small test for testing the option. I also needed to update the black pre-commit hook because it didn't work on my computer.\r\n\r\nObviously, the default value of this option does not change anything on the current behavior. \r\n\r\nThank you ! \nAdd `auto_bind` config option\n<!-- Thank you for contributing. These HTML commments will not render in the issue, but you can delete them once you've read them if you prefer! -->\r\n\r\n### Bug description\r\nMake ldap3 library `auto_bind` config settable.\r\n\r\n#### Expected behaviour\r\nCurrently, if `use_ssl` is set to `False`, `auto_bind` becomes `ldap3.AUTO_BIND_TLS_BEFORE_BIND`. This means that the ldap server must support ssl or starttls. There is no way to use a ldap server which doesn't have either. This is the case for a local LDAP server i.e runs in the same server as jupyterhub and communicates via the internal network.\r\n\r\n#### Actual behaviour\r\nAdd a new config variable `auto_bind` to match upstream ldap3 library configuration.\r\n\r\nI opened this as a bug because it used to because this was the behavior in 1.3.0 but it seems behavior changed because of the discussion in https://github.com/jupyterhub/ldapauthenticator/issues/171\n", "patch": "diff --git a/README.md b/README.md\nindex 4ec50fe..172aeb7 100644\n--- a/README.md\n+++ b/README.md\n@@ -167,15 +167,35 @@ is what most shell username validators do.\n \n #### `LDAPAuthenticator.use_ssl`\n \n-Boolean to specify whether to use SSL encryption when contacting\n-the LDAP server. If it is left to `False` (the default)\n-`LDAPAuthenticator` will try to upgrade connection with StartTLS.\n-Set this to be `True` to start SSL connection.\n+`use_ssl` is deprecated since 2.0. `use_ssl=True` translates to configuring\n+`tls_strategy=\"on_connect\"`, but `use_ssl=False` (previous default) doesn't\n+translate to anything.\n+\n+#### `LDAPAuthenticator.tls_strategy`\n+\n+When LDAPAuthenticator connects to the LDAP server, it can establish a\n+SSL/TLS connection directly, or do it before binding, which is LDAP\n+terminology for authenticating and sending sensitive credentials.\n+\n+The LDAP v3 protocol deprecated establishing a SSL/TLS connection\n+directly (`tls_strategy=\"on_connect\"`) in favor of upgrading the\n+connection to SSL/TLS before binding (`tls_strategy=\"before_bind\"`).\n+\n+Supported `tls_strategy` values are:\n+\n+- \"before_bind\" (default)\n+- \"on_connect\" (deprecated in LDAP v3, associated with use of port 636)\n+- \"insecure\"\n+\n+When configuring `tls_strategy=\"on_connect\"`, the default value of\n+`server_port` becomes 636.\n \n #### `LDAPAuthenticator.server_port`\n \n-Port to use to contact the LDAP server. Defaults to 389 if no SSL\n-is being used, and 636 is SSL is being used.\n+Port on which to contact the LDAP server.\n+\n+Defaults to `636` if `tls_strategy=\"on_connect\"` is set, `389`\n+otherwise.\n \n #### `LDAPAuthenticator.user_search_base`\n \ndiff --git a/ldapauthenticator/ldapauthenticator.py b/ldapauthenticator/ldapauthenticator.py\nindex 140b83b..761ff72 100644\n--- a/ldapauthenticator/ldapauthenticator.py\n+++ b/ldapauthenticator/ldapauthenticator.py\n@@ -1,9 +1,21 @@\n+import enum\n import re\n \n import ldap3\n from jupyterhub.auth import Authenticator\n from ldap3.utils.conv import escape_filter_chars\n-from traitlets import Bool, Int, List, Unicode, Union, validate\n+from traitlets import Bool, Int, List, Unicode, Union, UseEnum, observe, validate\n+\n+\n+class TlsStrategy(enum.Enum):\n+    \"\"\"\n+    Represents a SSL/TLS strategy for LDAPAuthenticator to use when interacting\n+    with the LDAP server.\n+    \"\"\"\n+\n+    before_bind = 1\n+    on_connect = 2\n+    insecure = 3\n \n \n class LDAPAuthenticator(Authenticator):\n@@ -20,23 +32,62 @@ class LDAPAuthenticator(Authenticator):\n         help=\"\"\"\n         Port on which to contact the LDAP server.\n \n-        Defaults to `636` if `use_ssl` is set, `389` otherwise.\n+        Defaults to `636` if `tls_strategy=\"on_connect\"` is set, `389`\n+        otherwise.\n         \"\"\",\n     )\n \n     def _server_port_default(self):\n-        if self.use_ssl:\n+        if self.tls_strategy == TlsStrategy.on_connect:\n             return 636  # default SSL port for LDAP\n         else:\n             return 389  # default plaintext port for LDAP\n \n     use_ssl = Bool(\n-        False,\n+        None,\n+        allow_none=True,\n         config=True,\n         help=\"\"\"\n-        Use SSL to communicate with the LDAP server.\n+        `use_ssl` is deprecated since 2.0. `use_ssl=True` translates to configuring\n+        `tls_strategy=\"on_connect\"`, but `use_ssl=False` (previous default) doesn't\n+        translate to anything.\n+        \"\"\",\n+    )\n \n-        Deprecated in version 3 of LDAP. Your LDAP server must be configured to support this, however.\n+    @observe(\"use_ssl\")\n+    def _observe_use_ssl(self, change):\n+        if change.new:\n+            self.tls_strategy = TlsStrategy.on_connect\n+            self.log.warning(\n+                \"LDAPAuthenticator.use_ssl is deprecated in 2.0 in favor of LDAPAuthenticator.tls_strategy, \"\n+                'instead of configuring use_ssl=True, configure use tls_strategy=\"on_connect\" from now on.'\n+            )\n+        else:\n+            self.log.warning(\n+                \"LDAPAuthenticator.use_ssl is deprecated in 2.0 in favor of LDAPAuthenticator.tls_strategy, \"\n+                \"you can stop configuring use_ssl=False from now on as doing so has no effect.\"\n+            )\n+\n+    tls_strategy = UseEnum(\n+        TlsStrategy,\n+        default_value=TlsStrategy.before_bind,\n+        config=True,\n+        help=\"\"\"\n+        When LDAPAuthenticator connects to the LDAP server, it can establish a\n+        SSL/TLS connection directly, or do it before binding, which is LDAP\n+        terminology for authenticating and sending sensitive credentials.\n+\n+        The LDAP v3 protocol deprecated establishing a SSL/TLS connection\n+        directly (`tls_strategy=\"on_connect\"`) in favor of upgrading the\n+        connection to SSL/TLS before binding (`tls_strategy=\"before_bind\"`).\n+\n+        Supported `tls_strategy` values are:\n+        - \"before_bind\" (default)\n+        - \"on_connect\" (deprecated in LDAP v3, associated with use of port 636)\n+        - \"insecure\"\n+\n+        When configuring `tls_strategy=\"on_connect\"`, the default value of\n+        `server_port` becomes 636.\n         \"\"\",\n     )\n \n@@ -314,14 +365,31 @@ def resolve_username(self, username_supplied_by_user):\n         return (user_dn, response[0][\"dn\"])\n \n     def get_connection(self, userdn, password):\n+        \"\"\"\n+        Returns a ldap3 Connection object automatically bound to the user.\n+\n+        ldap3 Connection ref: https://ldap3.readthedocs.io/en/latest/connection.html\n+        \"\"\"\n+        if self.tls_strategy == TlsStrategy.on_connect:\n+            use_ssl = True\n+            auto_bind = ldap3.AUTO_BIND_NO_TLS\n+        elif self.tls_strategy == TlsStrategy.before_bind:\n+            use_ssl = False\n+            auto_bind = ldap3.AUTO_BIND_TLS_BEFORE_BIND\n+        else:  # TlsStrategy.insecure\n+            use_ssl = False\n+            auto_bind = ldap3.AUTO_BIND_NO_TLS\n+\n         server = ldap3.Server(\n-            self.server_address, port=self.server_port, use_ssl=self.use_ssl\n-        )\n-        auto_bind = (\n-            ldap3.AUTO_BIND_NO_TLS if self.use_ssl else ldap3.AUTO_BIND_TLS_BEFORE_BIND\n+            self.server_address,\n+            port=self.server_port,\n+            use_ssl=use_ssl,\n         )\n         conn = ldap3.Connection(\n-            server, user=userdn, password=password, auto_bind=auto_bind\n+            server,\n+            user=userdn,\n+            password=password,\n+            auto_bind=auto_bind,\n         )\n         return conn\n \n", "instance_id": "jupyterhub__ldapauthenticator-258", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear but has some ambiguities and missing details. The goal of adding a new configuration option for TLS/SSL handling in an LDAP authenticator for JupyterHub is understandable, and the intent to align with upstream LDAP library configurations (like `use_tls` and `auto_bind`) is evident. The statement also references specific use cases (e.g., connecting to a local LDAP server without SSL/TLS support) and prior discussions (e.g., GitHub issue #171), which provide context. However, critical details are missing or vague: the exact behavior of `use_tls` and `auto_bind` in different scenarios isn't fully specified in the problem statement itself, and the expected interaction between the new `tls_strategy` and existing configurations could be clearer. Additionally, edge cases (e.g., invalid configurations or unsupported LDAP server setups) are not mentioned, and there are no explicit examples of input/output or configuration scenarios. Despite these minor gaps, the overall intent and scope are reasonably clear, especially when paired with the code changes, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is moderate, involving updates to a single core module (`ldapauthenticator.py`) and documentation (`README.md`). The changes include adding a new enum class (`TlsStrategy`), deprecating an existing configuration (`use_ssl`), introducing a new configuration (`tls_strategy`), and updating the connection logic to handle different TLS strategies. This requires understanding the interaction between configuration options and the LDAP connection process, which adds some complexity. Second, the technical concepts involved include familiarity with Python's `traitlets` library for configuration management, the `ldap3` library for LDAP interactions, and the nuances of SSL/TLS strategies in LDAP (e.g., `on_connect` vs. `before_bind`). While these concepts are not overly advanced, they require domain-specific knowledge of LDAP authentication protocols. Third, the changes impact a critical part of the system (authentication), but they do not alter the overall architecture significantly, as they are mostly additive and backward-compatible. Finally, potential edge cases (e.g., misconfigured TLS strategies or unsupported LDAP server setups) are not explicitly handled in the code changes, but the problem does not seem to demand extensive error handling beyond what's already implemented. Overall, this task requires a moderate level of understanding and effort, involving multiple concepts and careful implementation, but it does not reach the level of hard or very hard due to the localized scope and lack of deep architectural impact. A score of 0.45 reflects this balance.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Skparagraph does not properly handle newlines on windows\n**Describe the bug**\r\n\r\nWhen given text that has newlines in it, Skparagraph fails to render the newlines and instead renders them as unknown characters.\r\n![test](https://github.com/user-attachments/assets/3612d143-278c-438d-adde-c9e286515831)\r\n\r\nBut it works on linux:\r\n![test](https://github.com/user-attachments/assets/0f69f683-9978-4493-aa68-c61ce687c08d)\r\n\r\nI\u2018m not sure if it's an upstream problem, but when I tried it with rust-skia, it worked on both windows and linux.\r\n\r\n\r\n**To Reproduce**\r\n\r\nCode:\r\n```python\r\nimport math\r\n\r\nimport skia\r\nfrom skia import textlayout\r\n\r\npaint = skia.Paint()\r\npaint.setColor(skia.ColorBLACK)\r\npaint.setAntiAlias(True)\r\n\r\nstyle = textlayout.TextStyle()\r\nstyle.setFontSize(50)\r\nstyle.setForegroundPaint(paint)\r\n\r\nfont_collection = textlayout.FontCollection()\r\nfont_collection.setDefaultFontManager(skia.FontMgr())\r\n\r\npara_style = textlayout.ParagraphStyle()\r\n\r\nbuilder = textlayout.ParagraphBuilder.make(\r\n    para_style, font_collection, skia.Unicodes.ICU.Make()\r\n)\r\nbuilder.pushStyle(style)\r\n\r\nbuilder.addText(\"test\\ntest\")\r\nparagraph = builder.Build()\r\nparagraph.layout(300)\r\n\r\nimage_width = math.ceil(paragraph.LongestLine)\r\nimage_height = math.ceil(paragraph.Height)\r\nsurface = skia.Surfaces.MakeRasterN32Premul(image_width, image_height)\r\ncanvas = surface.getCanvas()\r\ncanvas.clear(skia.ColorWHITE)\r\nparagraph.paint(canvas, 0, 0)\r\nsurface.flushAndSubmit()\r\nimage = surface.makeImageSnapshot()\r\nimage.save(\"test.png\", skia.kPNG)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe text with newlines is rendered properly on windows.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: windows 11, ubuntu 22.04\r\n - Python: 3.10\r\n - skia-python version: v130.0b10\r\n\n", "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex f104578c..434cc580 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -25,7 +25,7 @@ jobs:\n           key: linux-aarch64-skia-${{ github.sha }}-3rd-party\n       - name: Pre-fetch skia deps\n         if: ${{ steps.cache-skia.outputs.cache-hit != 'true' }}\n-        run: git config --global core.compression 0 && cd skia && patch -p1 -i ../patch/skia-m130-minimize-download.patch && python tools/git-sync-deps && patch -p1 -R -i ../patch/skia-m130-minimize-download.patch\n+        run: git config --global core.compression 0 && cd skia && patch -p1 -i ../patch/skia-m132-minimize-download.patch && python tools/git-sync-deps && patch -p1 -R -i ../patch/skia-m132-minimize-download.patch\n       - name: Set up QEMU\n         uses: docker/setup-qemu-action@v3\n       - name: Build skia 3rd-Party\n@@ -73,29 +73,89 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        os: [ubuntu-22.04, windows-2022, macos-12]\n+        os: [ubuntu-22.04, windows-2022, macos-13]\n         arch: [auto64]\n-        cp: [\"cp3{8,9,10,11,12}\"]\n+        cp: [\"cp3{10,11,12,13}\"]\n         include:\n-          - os: macos-12\n+          - os: macos-13\n             arch: arm64\n-            cp: \"cp3{8,9,10,11,12}\"\n+            cp: \"cp3{10,11,12,13}\"\n           # aarch64 is emulated and takes longer, build one wheel per job\n           - os: ubuntu-22.04\n             arch: aarch64\n-            cp: cp38\n+            cp: cp310\n           - os: ubuntu-22.04\n             arch: aarch64\n-            cp: cp39\n+            cp: cp311\n           - os: ubuntu-22.04\n             arch: aarch64\n-            cp: cp310\n+            cp: cp312\n           - os: ubuntu-22.04\n             arch: aarch64\n-            cp: cp311\n+            cp: cp313\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          submodules: true\n+\n+      - uses: actions/cache/restore@v4\n+        if: runner.os == 'Linux' && matrix.arch == 'aarch64'\n+        with:\n+          path: |\n+            gn\n+            skia\n+          key: linux-aarch64-skia-${{ github.sha }}\n+\n+      - name: Set up QEMU\n+        if: runner.os == 'Linux' && matrix.arch == 'aarch64'\n+        uses: docker/setup-qemu-action@v3\n+        with:\n+          platforms: ${{ matrix.arch }}\n+\n+      - name: Build wheels\n+        uses: pypa/cibuildwheel@v2.21.3\n+        env:\n+          CIBW_BUILD: \"${{ matrix.cp }}-*\"\n+          CIBW_SKIP: \"*musllinux*\"\n+          CIBW_ARCHS: ${{ matrix.arch }}\n+          CIBW_ENVIRONMENT_MACOS: TARGET_ARCH=${{ matrix.arch }} MACOSX_DEPLOYMENT_TARGET=11.0\n+          CIBW_BEFORE_ALL: bash scripts/build_${{ runner.os }}.sh\n+          CIBW_BEFORE_BUILD: pip install pybind11 numpy\n+          CIBW_TEST_REQUIRES: pytest pillow glfw\n+          CIBW_TEST_REQUIRES_MACOS: pytest pillow pyopengl\n+          CIBW_TEST_COMMAND: python -m pytest {project}/tests\n+          CIBW_TEST_COMMAND_LINUX: >\n+            xvfb-run -s \"-screen 0 640x480x24\" python -m pytest {project}/tests\n+          CIBW_TEST_SKIP: \"*-macosx_arm64\"\n+\n+      - uses: actions/upload-artifact@v4\n+        with:\n+          name: wheel-${{ matrix.os }}-${{ matrix.arch }}-${{ matrix.cp }}\n+          path: ./wheelhouse/*.whl\n+\n+  # identical to \"build_wheels\", except with the older pypa/cibuildwheel@v2.19.2\n+  build_wheels_old:\n+    name: Build wheels on ${{ matrix.os }} (${{ matrix.arch }}) for ${{ matrix.cp }}\n+    needs: prebuild_linux_aarch64\n+    runs-on: ${{ matrix.os }}\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        os: [ubuntu-22.04, windows-2022, macos-13]\n+        arch: [auto64]\n+        cp: [\"cp3{8,9}\"]\n+        include:\n+          - os: macos-13\n+            arch: arm64\n+            cp: \"cp3{8,9}\"\n+          # aarch64 is emulated and takes longer, build one wheel per job\n           - os: ubuntu-22.04\n             arch: aarch64\n-            cp: cp312\n+            cp: cp38\n+          - os: ubuntu-22.04\n+            arch: aarch64\n+            cp: cp39\n \n     steps:\n       - uses: actions/checkout@v4\n@@ -139,7 +199,7 @@ jobs:\n \n   build_docs:\n     name: Build docs\n-    needs: [build_wheels]\n+    needs: [build_wheels, build_wheels_old]\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v4\n@@ -169,7 +229,7 @@ jobs:\n \n   publish:\n     name: Upload packages to PyPI\n-    needs: [build_wheels]\n+    needs: [build_wheels, build_wheels_old]\n     runs-on: ubuntu-latest\n     if: github.event_name == 'release' && github.event.action == 'published'\n     permissions:\ndiff --git a/README.md b/README.md\nindex 37b2bfa8..5b84501a 100644\n--- a/README.md\n+++ b/README.md\n@@ -15,7 +15,7 @@ Binary package is available on PyPI:\n pip install skia-python\n ```\n \n-Supported platforms: Python 3.8-3.12 (CPython) on\n+Supported platforms: Python 3.8-3.13 (CPython) on\n \n - Linux x86_64, aarch64\n - macOS x86_64, arm64\n@@ -57,7 +57,8 @@ https://kyamagu.github.io/skia-python\n   [README.m120](relnotes/README.m120.md), [README.m121](relnotes/README.m121.md), [README.m122](relnotes/README.m122.md),\n   [README.m123](relnotes/README.m123.md), [README.m124](relnotes/README.m124.md), [README.m125](relnotes/README.m125.md),\n   [README.m126](relnotes/README.m126.md), [README.m127](relnotes/README.m127.md), [README.m128](relnotes/README.m128.md),\n-  [README.m129](relnotes/README.m129.md), [README.m130](relnotes/README.m130.md).\n+  [README.m129](relnotes/README.m129.md), [README.m130](relnotes/README.m130.md), [README.m131](relnotes/README.m131.md),\n+  [README.m132](relnotes/README.m132.md).\n \n ## Contributing\n \ndiff --git a/docs/tutorial/canvas.rst b/docs/tutorial/canvas.rst\nindex 16eb1791..ff658761 100644\n--- a/docs/tutorial/canvas.rst\n+++ b/docs/tutorial/canvas.rst\n@@ -91,10 +91,11 @@ The following example uses glfw package to create an OpenGL context. Install\n         glfw.window_hint(glfw.VISIBLE, glfw.FALSE)\n         glfw.window_hint(glfw.STENCIL_BITS, 8)\n         # see https://www.glfw.org/faq#macos\n-        glfw.window_hint(glfw.CONTEXT_VERSION_MAJOR, 3)\n-        glfw.window_hint(glfw.CONTEXT_VERSION_MINOR, 2)\n-        glfw.window_hint(glfw.OPENGL_FORWARD_COMPAT, True)\n-        glfw.window_hint(glfw.OPENGL_PROFILE, glfw.OPENGL_CORE_PROFILE)\n+        if sys.platform.startswith(\"darwin\"):\n+            glfw.window_hint(glfw.CONTEXT_VERSION_MAJOR, 3)\n+            glfw.window_hint(glfw.CONTEXT_VERSION_MINOR, 2)\n+            glfw.window_hint(glfw.OPENGL_FORWARD_COMPAT, True)\n+            glfw.window_hint(glfw.OPENGL_PROFILE, glfw.OPENGL_CORE_PROFILE)\n         window = glfw.create_window(640, 480, '', None, None)\n         glfw.make_context_current(window)\n         yield window\n@@ -144,10 +145,11 @@ Here's a complete example:\n             raise RuntimeError('glfw.init() failed')\n         glfw.window_hint(glfw.STENCIL_BITS, 8)\n         # see https://www.glfw.org/faq#macos\n-        glfw.window_hint(glfw.CONTEXT_VERSION_MAJOR, 3)\n-        glfw.window_hint(glfw.CONTEXT_VERSION_MINOR, 2)\n-        glfw.window_hint(glfw.OPENGL_FORWARD_COMPAT, True)\n-        glfw.window_hint(glfw.OPENGL_PROFILE, glfw.OPENGL_CORE_PROFILE)\n+        if sys.platform.startswith(\"darwin\"):\n+            glfw.window_hint(glfw.CONTEXT_VERSION_MAJOR, 3)\n+            glfw.window_hint(glfw.CONTEXT_VERSION_MINOR, 2)\n+            glfw.window_hint(glfw.OPENGL_FORWARD_COMPAT, True)\n+            glfw.window_hint(glfw.OPENGL_PROFILE, glfw.OPENGL_CORE_PROFILE)\n         window = glfw.create_window(WIDTH, HEIGHT, '', None, None)\n         glfw.make_context_current(window)\n         yield window\ndiff --git a/patch/skia-m131-minimize-download.patch b/patch/skia-m131-minimize-download.patch\nnew file mode 100644\nindex 00000000..2d3642cc\n--- /dev/null\n+++ b/patch/skia-m131-minimize-download.patch\n@@ -0,0 +1,70 @@\n+diff --git a/DEPS b/DEPS\n+index a30e242..3d8dd72 100644\n+--- a/DEPS\n++++ b/DEPS\n+@@ -31,53 +31,18 @@ vars = {\n+ #     ./tools/git-sync-deps\n+ deps = {\n+   \"buildtools\"                                   : \"https://chromium.googlesource.com/chromium/src/buildtools.git@b138e6ce86ae843c42a1a08f37903207bebcca75\",\n+-  \"third_party/externals/angle2\"                 : \"https://chromium.googlesource.com/angle/angle.git@78a694a1b82a01623226a418cf2f765c75e45c70\",\n+-  \"third_party/externals/brotli\"                 : \"https://skia.googlesource.com/external/github.com/google/brotli.git@6d03dfbedda1615c4cba1211f8d81735575209c8\",\n+-  \"third_party/externals/d3d12allocator\"         : \"https://skia.googlesource.com/external/github.com/GPUOpen-LibrariesAndSDKs/D3D12MemoryAllocator.git@169895d529dfce00390a20e69c2f516066fe7a3b\",\n+-  # Dawn requires jinja2 and markupsafe for the code generator, tint for SPIRV compilation, and abseil for string formatting.\n+-  # When the Dawn revision is updated these should be updated from the Dawn DEPS as well.\n+-  \"third_party/externals/dawn\"                   : \"https://dawn.googlesource.com/dawn.git@f3c7cc5c580eb743829c78bb77df0c1e8f6a6ce3\",\n+-  \"third_party/externals/jinja2\"                 : \"https://chromium.googlesource.com/chromium/src/third_party/jinja2@e2d024354e11cc6b041b0cff032d73f0c7e43a07\",\n+-  \"third_party/externals/markupsafe\"             : \"https://chromium.googlesource.com/chromium/src/third_party/markupsafe@0bad08bb207bbfc1d6f3bbc82b9242b0c50e5794\",\n+-  \"third_party/externals/abseil-cpp\"             : \"https://skia.googlesource.com/external/github.com/abseil/abseil-cpp.git@65a55c2ba891f6d2492477707f4a2e327a0b40dc\",\n+   \"third_party/externals/dng_sdk\"                : \"https://android.googlesource.com/platform/external/dng_sdk.git@c8d0c9b1d16bfda56f15165d39e0ffa360a11123\",\n+-  \"third_party/externals/egl-registry\"           : \"https://skia.googlesource.com/external/github.com/KhronosGroup/EGL-Registry@b055c9b483e70ecd57b3cf7204db21f5a06f9ffe\",\n+-  \"third_party/externals/emsdk\"                  : \"https://skia.googlesource.com/external/github.com/emscripten-core/emsdk.git@a896e3d066448b3530dbcaa48869fafefd738f57\",\n+   \"third_party/externals/expat\"                  : \"https://chromium.googlesource.com/external/github.com/libexpat/libexpat.git@624da0f593bb8d7e146b9f42b06d8e6c80d032a3\",\n+   \"third_party/externals/freetype\"               : \"https://chromium.googlesource.com/chromium/src/third_party/freetype2.git@83af801b552111e37d9466a887e1783a0fb5f196\",\n+   \"third_party/externals/harfbuzz\"               : \"https://chromium.googlesource.com/external/github.com/harfbuzz/harfbuzz.git@a070f9ebbe88dc71b248af9731dd49ec93f4e6e6\",\n+-  \"third_party/externals/highway\"                : \"https://chromium.googlesource.com/external/github.com/google/highway.git@424360251cdcfc314cfc528f53c872ecd63af0f0\",\n+   \"third_party/externals/icu\"                    : \"https://chromium.googlesource.com/chromium/deps/icu.git@364118a1d9da24bb5b770ac3d762ac144d6da5a4\",\n+-  \"third_party/externals/icu4x\"                  : \"https://chromium.googlesource.com/external/github.com/unicode-org/icu4x.git@bcf4f7198d4dc5f3127e84a6ca657c88e7d07a13\",\n+-  \"third_party/externals/imgui\"                  : \"https://skia.googlesource.com/external/github.com/ocornut/imgui.git@55d35d8387c15bf0cfd71861df67af8cfbda7456\",\n+-  \"third_party/externals/libavif\"                : \"https://skia.googlesource.com/external/github.com/AOMediaCodec/libavif.git@55aab4ac0607ab651055d354d64c4615cf3d8000\",\n+-  \"third_party/externals/libgav1\"                : \"https://chromium.googlesource.com/codecs/libgav1.git@5cf722e659014ebaf2f573a6dd935116d36eadf1\",\n+-  \"third_party/externals/libgrapheme\"            : \"https://skia.googlesource.com/external/github.com/FRIGN/libgrapheme/@c0cab63c5300fa12284194fbef57aa2ed62a94c0\",\n+   \"third_party/externals/libjpeg-turbo\"          : \"https://chromium.googlesource.com/chromium/deps/libjpeg_turbo.git@ccfbe1c82a3b6dbe8647ceb36a3f9ee711fba3cf\",\n+-  \"third_party/externals/libjxl\"                 : \"https://chromium.googlesource.com/external/gitlab.com/wg1/jpeg-xl.git@a205468bc5d3a353fb15dae2398a101dff52f2d3\",\n+   \"third_party/externals/libpng\"                 : \"https://skia.googlesource.com/third_party/libpng.git@ed217e3e601d8e462f7fd1e04bed43ac42212429\",\n+   \"third_party/externals/libwebp\"                : \"https://chromium.googlesource.com/webm/libwebp.git@845d5476a866141ba35ac133f856fa62f0b7445f\",\n+-  \"third_party/externals/libyuv\"                 : \"https://chromium.googlesource.com/libyuv/libyuv.git@d248929c059ff7629a85333699717d7a677d8d96\",\n+-  \"third_party/externals/microhttpd\"             : \"https://android.googlesource.com/platform/external/libmicrohttpd@748945ec6f1c67b7efc934ab0808e1d32f2fb98d\",\n+-  \"third_party/externals/oboe\"                   : \"https://chromium.googlesource.com/external/github.com/google/oboe.git@b02a12d1dd821118763debec6b83d00a8a0ee419\",\n+-  \"third_party/externals/opengl-registry\"        : \"https://skia.googlesource.com/external/github.com/KhronosGroup/OpenGL-Registry@14b80ebeab022b2c78f84a573f01028c96075553\",\n+-  \"third_party/externals/partition_alloc\"        : \"https://chromium.googlesource.com/chromium/src/base/allocator/partition_allocator.git@ca4487e127c2e071da5d4a36a9f71fd7b65b1434\",\n+-  \"third_party/externals/perfetto\"               : \"https://android.googlesource.com/platform/external/perfetto@93885509be1c9240bc55fa515ceb34811e54a394\",\n+   \"third_party/externals/piex\"                   : \"https://android.googlesource.com/platform/external/piex.git@bb217acdca1cc0c16b704669dd6f91a1b509c406\",\n+-  \"third_party/externals/swiftshader\"            : \"https://swiftshader.googlesource.com/SwiftShader@7a9a492a38b7c701f7c96a15a76046aed8f8c0c3\",\n+   \"third_party/externals/vulkanmemoryallocator\"  : \"https://chromium.googlesource.com/external/github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator@a6bfc237255a6bac1513f7c1ebde6d8aed6b5191\",\n+-  # vulkan-deps is a meta-repo containing several interdependent Khronos Vulkan repositories.\n+-  # When the vulkan-deps revision is updated, those repos (spirv-*, vulkan-*) should be updated as well.\n+   \"third_party/externals/vulkan-deps\"            : \"https://chromium.googlesource.com/vulkan-deps@8f346c5caf5a624f42324ffb88167fc90992cab5\",\n+-  \"third_party/externals/spirv-cross\"            : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/SPIRV-Cross@b8fcf307f1f347089e3c46eb4451d27f32ebc8d3\",\n+   \"third_party/externals/spirv-headers\"          : \"https://skia.googlesource.com/external/github.com/KhronosGroup/SPIRV-Headers.git@50bc4debdc3eec5045edbeb8ce164090e29b91f3\",\n+-  \"third_party/externals/spirv-tools\"            : \"https://skia.googlesource.com/external/github.com/KhronosGroup/SPIRV-Tools.git@42b315c15b1ff941b46bb3949c105e5386be8717\",\n+-  \"third_party/externals/vello\"                  : \"https://skia.googlesource.com/external/github.com/linebender/vello.git@3ee3bea02164c5a816fe6c16ef4e3a810edb7620\",\n+-  \"third_party/externals/vulkan-headers\"         : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Headers@d91597a82f881d473887b560a03a7edf2720b72c\",\n+-  \"third_party/externals/vulkan-tools\"           : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Tools@eb9b6043be165f06c7ec78fadbb1ff773c5fc19c\",\n+-  \"third_party/externals/vulkan-utility-libraries\": \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Utility-Libraries@bfd85956e1b4c1c79842ce857fc7fb15adb8a573\",\n+-  \"third_party/externals/unicodetools\"           : \"https://chromium.googlesource.com/external/github.com/unicode-org/unicodetools@66a3fa9dbdca3b67053a483d130564eabc5fe095\",\n+-  #\"third_party/externals/v8\"                     : \"https://chromium.googlesource.com/v8/v8.git@5f1ae66d5634e43563b2d25ea652dfb94c31a3b4\",\n+   \"third_party/externals/wuffs\"                  : \"https://skia.googlesource.com/external/github.com/google/wuffs-mirror-release-c.git@e3f919ccfe3ef542cfc983a82146070258fb57f8\",\n+   \"third_party/externals/zlib\"                   : \"https://chromium.googlesource.com/chromium/src/third_party/zlib@646b7f569718921d7d4b5b8e22572ff6c76f2596\",\n+ \n+diff --git a/bin/activate-emsdk b/bin/activate-emsdk\n+index 687ca9f..7167d8d 100755\n+--- a/bin/activate-emsdk\n++++ b/bin/activate-emsdk\n+@@ -17,6 +17,7 @@ EMSDK_PATH = os.path.join(EMSDK_ROOT, 'emsdk.py')\n+ EMSDK_VERSION = '3.1.44'\n+ \n+ def main():\n++    return\n+     if sysconfig.get_platform() in ['linux-aarch64', 'linux-arm64']:\n+         # This platform cannot install emsdk at the provided version. See\n+         # https://github.com/emscripten-core/emsdk/blob/main/emscripten-releases-tags.json#L5\ndiff --git a/patch/skia-m132-colrv1-freetype.diff b/patch/skia-m132-colrv1-freetype.diff\nnew file mode 100644\nindex 00000000..7d818119\n--- /dev/null\n+++ b/patch/skia-m132-colrv1-freetype.diff\n@@ -0,0 +1,122 @@\n+diff --git a/src/core/SkFontDescriptor.h b/src/core/SkFontDescriptor.h\n+index a3018ee..5eadbed 100644\n+--- a/src/core/SkFontDescriptor.h\n++++ b/src/core/SkFontDescriptor.h\n+@@ -82,7 +82,7 @@ private:\n+     skia_private::AutoSTMalloc<4, SkFontArguments::Palette::Override> fPaletteOverrides;\n+ };\n+ \n+-class SkFontDescriptor : SkNoncopyable {\n++class SK_SPI SkFontDescriptor : SkNoncopyable {\n+ public:\n+     SkFontDescriptor();\n+     // Does not affect ownership of SkStream.\n+diff --git a/src/ports/SkFontHost_FreeType.cpp b/src/ports/SkFontHost_FreeType.cpp\n+index 7798b91..7637709 100644\n+--- a/src/ports/SkFontHost_FreeType.cpp\n++++ b/src/ports/SkFontHost_FreeType.cpp\n+@@ -32,7 +32,6 @@\n+ #include \"src/core/SkMask.h\"\n+ #include \"src/core/SkMaskGamma.h\"\n+ #include \"src/core/SkScalerContext.h\"\n+-#include \"src/ports/SkFontHost_FreeType_common.h\"\n+ #include \"src/ports/SkFontScanner_FreeType_priv.h\"\n+ #include \"src/ports/SkTypeface_FreeType.h\"\n+ #include \"src/sfnt/SkOTUtils.h\"\n+@@ -52,6 +51,7 @@\n+ #ifdef FT_COLOR_H  // 2.10.0\n+ #   include <freetype/ftcolor.h>\n+ #endif\n++#include \"src/ports/SkFontHost_FreeType_common.h\"\n+ #include <freetype/freetype.h>\n+ #include <freetype/ftlcdfil.h>\n+ #include <freetype/ftmodapi.h>\n+diff --git a/src/ports/SkFontHost_FreeType_common.cpp b/src/ports/SkFontHost_FreeType_common.cpp\n+index be5bc52..12bd7fe 100644\n+--- a/src/ports/SkFontHost_FreeType_common.cpp\n++++ b/src/ports/SkFontHost_FreeType_common.cpp\n+@@ -6,7 +6,6 @@\n+  * found in the LICENSE file.\n+  */\n+ \n+-#include \"src/ports/SkFontHost_FreeType_common.h\"\n+ \n+ #include \"include/core/SkBitmap.h\"\n+ #include \"include/core/SkCanvas.h\"\n+@@ -34,6 +33,7 @@\n+ #ifdef FT_COLOR_H\n+ #   include <freetype/ftcolor.h>\n+ #endif\n++#include \"src/ports/SkFontHost_FreeType_common.h\"\n+ #include <freetype/ftimage.h>\n+ #include <freetype/ftoutln.h>\n+ #include <freetype/ftsizes.h>\n+@@ -1575,6 +1575,41 @@ bool SkScalerContextFTUtils::drawCOLRv1Glyph(FT_Face face, const SkGlyph& glyph,\n+                               face, glyph.getGlyphID(),\n+                               FT_COLOR_INCLUDE_ROOT_TRANSFORM, &activePaints);\n+ }\n++/*\n++ * This content is mostly just\n++ *       SkTypeface_FreeType::FaceRec::setupPalette()\n++ +     + SkScalerContext_FreeType_Base::drawCOLRv1Glyph()\n++ +*/\n++bool SkScalerContextFTUtils::skia_colrv1_start_glyph(SkCanvas* canvas,\n++                                    FT_Face face,\n++                                    uint16_t glyphId,\n++                                    FT_UShort palette_index,\n++                                    FT_Color_Root_Transform rootTransform\n++                                    ) {\n++    uint32_t fForegroundColor{SK_ColorBLACK};\n++    FT_Palette_Data paletteData;\n++    FT_Palette_Data_Get(face, &paletteData);\n++\n++    FT_Color* ftPalette = nullptr;\n++    FT_Palette_Select(face, palette_index, &ftPalette);\n++    std::unique_ptr<SkColor[]> ptr_palette(new SkColor[paletteData.num_palette_entries]);\n++    for (int i = 0; i < paletteData.num_palette_entries; ++i) {\n++      ptr_palette[i] = SkColorSetARGB(ftPalette[i].alpha,\n++                                  ftPalette[i].red,\n++                                  ftPalette[i].green,\n++                                  ftPalette[i].blue);\n++    }\n++    SkSpan<SkColor> palette(ptr_palette.get(), paletteData.num_palette_entries);\n++\n++    VisitedSet activePaints;\n++    bool haveLayers =  colrv1_start_glyph(canvas, palette,\n++                                          fForegroundColor, // FT_Palette_Get_Foreground_Color?\n++                                          face, glyphId,\n++                                          FT_COLOR_INCLUDE_ROOT_TRANSFORM,\n++                                          &activePaints);\n++    SkASSERTF(haveLayers, \"Could not get COLRv1 layers from '%s'.\", face->family_name);\n++    return haveLayers;\n++}\n+ #endif  // TT_SUPPORT_COLRV1\n+ \n+ #ifdef FT_COLOR_H\n+diff --git a/src/ports/SkFontHost_FreeType_common.h b/src/ports/SkFontHost_FreeType_common.h\n+index cd19ec7..40842c5 100644\n+--- a/src/ports/SkFontHost_FreeType_common.h\n++++ b/src/ports/SkFontHost_FreeType_common.h\n+@@ -20,6 +20,7 @@ class SkCanvas;\n+ // These are forward declared to avoid pimpl but also hide the FreeType implementation.\n+ typedef struct FT_FaceRec_* FT_Face;\n+ typedef signed long FT_Pos;\n++typedef unsigned short  FT_UShort; /* freetype/fttypes.h */\n+ \n+ \n+ #ifdef SK_DEBUG\n+@@ -31,7 +32,13 @@ const char* SkTraceFtrGetError(int);\n+ #define SK_TRACEFTR(ERR, ...) do { sk_ignore_unused_variable(ERR); } while (false)\n+ #endif\n+ \n+-struct SkScalerContextFTUtils {\n++struct SK_SPI SkScalerContextFTUtils {\n++    static bool skia_colrv1_start_glyph(SkCanvas* canvas,\n++                                        FT_Face face,\n++                                        uint16_t glyphId,\n++                                        FT_UShort palette_index,\n++                                        FT_Color_Root_Transform rootTransform\n++                                        );\n+     SkColor                 fForegroundColor;\n+     SkScalerContext::Flags  fFlags;\n+ \ndiff --git a/patch/skia-m132-minimize-download.patch b/patch/skia-m132-minimize-download.patch\nnew file mode 100644\nindex 00000000..ffb54097\n--- /dev/null\n+++ b/patch/skia-m132-minimize-download.patch\n@@ -0,0 +1,70 @@\n+diff --git a/DEPS b/DEPS\n+index 16d496c..eb6a7cb 100644\n+--- a/DEPS\n++++ b/DEPS\n+@@ -31,53 +31,18 @@ vars = {\n+ #     ./tools/git-sync-deps\n+ deps = {\n+   \"buildtools\"                                   : \"https://chromium.googlesource.com/chromium/src/buildtools.git@b138e6ce86ae843c42a1a08f37903207bebcca75\",\n+-  \"third_party/externals/angle2\"                 : \"https://chromium.googlesource.com/angle/angle.git@7fea539cc99bed8fd315cfbc5026952a133ac3ae\",\n+-  \"third_party/externals/brotli\"                 : \"https://skia.googlesource.com/external/github.com/google/brotli.git@6d03dfbedda1615c4cba1211f8d81735575209c8\",\n+-  \"third_party/externals/d3d12allocator\"         : \"https://skia.googlesource.com/external/github.com/GPUOpen-LibrariesAndSDKs/D3D12MemoryAllocator.git@169895d529dfce00390a20e69c2f516066fe7a3b\",\n+-  # Dawn requires jinja2 and markupsafe for the code generator, tint for SPIRV compilation, and abseil for string formatting.\n+-  # When the Dawn revision is updated these should be updated from the Dawn DEPS as well.\n+-  \"third_party/externals/dawn\"                   : \"https://dawn.googlesource.com/dawn.git@2a86250e561c56e9b1b9af5774f1253d9d66be97\",\n+-  \"third_party/externals/jinja2\"                 : \"https://chromium.googlesource.com/chromium/src/third_party/jinja2@e2d024354e11cc6b041b0cff032d73f0c7e43a07\",\n+-  \"third_party/externals/markupsafe\"             : \"https://chromium.googlesource.com/chromium/src/third_party/markupsafe@0bad08bb207bbfc1d6f3bbc82b9242b0c50e5794\",\n+-  \"third_party/externals/abseil-cpp\"             : \"https://skia.googlesource.com/external/github.com/abseil/abseil-cpp.git@65a55c2ba891f6d2492477707f4a2e327a0b40dc\",\n+   \"third_party/externals/dng_sdk\"                : \"https://android.googlesource.com/platform/external/dng_sdk.git@c8d0c9b1d16bfda56f15165d39e0ffa360a11123\",\n+-  \"third_party/externals/egl-registry\"           : \"https://skia.googlesource.com/external/github.com/KhronosGroup/EGL-Registry@b055c9b483e70ecd57b3cf7204db21f5a06f9ffe\",\n+-  \"third_party/externals/emsdk\"                  : \"https://skia.googlesource.com/external/github.com/emscripten-core/emsdk.git@a896e3d066448b3530dbcaa48869fafefd738f57\",\n+   \"third_party/externals/expat\"                  : \"https://chromium.googlesource.com/external/github.com/libexpat/libexpat.git@624da0f593bb8d7e146b9f42b06d8e6c80d032a3\",\n+   \"third_party/externals/freetype\"               : \"https://chromium.googlesource.com/chromium/src/third_party/freetype2.git@83af801b552111e37d9466a887e1783a0fb5f196\",\n+   \"third_party/externals/harfbuzz\"               : \"https://chromium.googlesource.com/external/github.com/harfbuzz/harfbuzz.git@a070f9ebbe88dc71b248af9731dd49ec93f4e6e6\",\n+-  \"third_party/externals/highway\"                : \"https://chromium.googlesource.com/external/github.com/google/highway.git@424360251cdcfc314cfc528f53c872ecd63af0f0\",\n+   \"third_party/externals/icu\"                    : \"https://chromium.googlesource.com/chromium/deps/icu.git@364118a1d9da24bb5b770ac3d762ac144d6da5a4\",\n+-  \"third_party/externals/icu4x\"                  : \"https://chromium.googlesource.com/external/github.com/unicode-org/icu4x.git@bcf4f7198d4dc5f3127e84a6ca657c88e7d07a13\",\n+-  \"third_party/externals/imgui\"                  : \"https://skia.googlesource.com/external/github.com/ocornut/imgui.git@55d35d8387c15bf0cfd71861df67af8cfbda7456\",\n+-  \"third_party/externals/libavif\"                : \"https://skia.googlesource.com/external/github.com/AOMediaCodec/libavif.git@55aab4ac0607ab651055d354d64c4615cf3d8000\",\n+-  \"third_party/externals/libgav1\"                : \"https://chromium.googlesource.com/codecs/libgav1.git@5cf722e659014ebaf2f573a6dd935116d36eadf1\",\n+-  \"third_party/externals/libgrapheme\"            : \"https://skia.googlesource.com/external/github.com/FRIGN/libgrapheme/@c0cab63c5300fa12284194fbef57aa2ed62a94c0\",\n+   \"third_party/externals/libjpeg-turbo\"          : \"https://chromium.googlesource.com/chromium/deps/libjpeg_turbo.git@ccfbe1c82a3b6dbe8647ceb36a3f9ee711fba3cf\",\n+-  \"third_party/externals/libjxl\"                 : \"https://chromium.googlesource.com/external/gitlab.com/wg1/jpeg-xl.git@a205468bc5d3a353fb15dae2398a101dff52f2d3\",\n+   \"third_party/externals/libpng\"                 : \"https://skia.googlesource.com/third_party/libpng.git@ed217e3e601d8e462f7fd1e04bed43ac42212429\",\n+   \"third_party/externals/libwebp\"                : \"https://chromium.googlesource.com/webm/libwebp.git@845d5476a866141ba35ac133f856fa62f0b7445f\",\n+-  \"third_party/externals/libyuv\"                 : \"https://chromium.googlesource.com/libyuv/libyuv.git@d248929c059ff7629a85333699717d7a677d8d96\",\n+-  \"third_party/externals/microhttpd\"             : \"https://android.googlesource.com/platform/external/libmicrohttpd@748945ec6f1c67b7efc934ab0808e1d32f2fb98d\",\n+-  \"third_party/externals/oboe\"                   : \"https://chromium.googlesource.com/external/github.com/google/oboe.git@b02a12d1dd821118763debec6b83d00a8a0ee419\",\n+-  \"third_party/externals/opengl-registry\"        : \"https://skia.googlesource.com/external/github.com/KhronosGroup/OpenGL-Registry@14b80ebeab022b2c78f84a573f01028c96075553\",\n+-  \"third_party/externals/partition_alloc\"        : \"https://chromium.googlesource.com/chromium/src/base/allocator/partition_allocator.git@ce13777cb731e0a60c606d1741091fd11a0574d7\",\n+-  \"third_party/externals/perfetto\"               : \"https://android.googlesource.com/platform/external/perfetto@93885509be1c9240bc55fa515ceb34811e54a394\",\n+   \"third_party/externals/piex\"                   : \"https://android.googlesource.com/platform/external/piex.git@bb217acdca1cc0c16b704669dd6f91a1b509c406\",\n+-  \"third_party/externals/swiftshader\"            : \"https://swiftshader.googlesource.com/SwiftShader@76855a9baecc97fa144ce70d7ae43a9f878e14c8\",\n+   \"third_party/externals/vulkanmemoryallocator\"  : \"https://chromium.googlesource.com/external/github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator@a6bfc237255a6bac1513f7c1ebde6d8aed6b5191\",\n+-  # vulkan-deps is a meta-repo containing several interdependent Khronos Vulkan repositories.\n+-  # When the vulkan-deps revision is updated, those repos (spirv-*, vulkan-*) should be updated as well.\n+   \"third_party/externals/vulkan-deps\"            : \"https://chromium.googlesource.com/vulkan-deps@a2dfb2276ea5f9467eb84c9a19ecf917d92e4135\",\n+-  \"third_party/externals/spirv-cross\"            : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/SPIRV-Cross@b8fcf307f1f347089e3c46eb4451d27f32ebc8d3\",\n+   \"third_party/externals/spirv-headers\"          : \"https://skia.googlesource.com/external/github.com/KhronosGroup/SPIRV-Headers.git@996c728cf7dcfb29845cfa15222822318f047810\",\n+-  \"third_party/externals/spirv-tools\"            : \"https://skia.googlesource.com/external/github.com/KhronosGroup/SPIRV-Tools.git@9117e042b93d4ff08d2406542708170f77aaa2a3\",\n+-  \"third_party/externals/vello\"                  : \"https://skia.googlesource.com/external/github.com/linebender/vello.git@3ee3bea02164c5a816fe6c16ef4e3a810edb7620\",\n+-  \"third_party/externals/vulkan-headers\"         : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Headers@cbcad3c0587dddc768d76641ea00f5c45ab5a278\",\n+-  \"third_party/externals/vulkan-tools\"           : \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Tools@15f2de809304aba619ee327f3273425418ca83de\",\n+-  \"third_party/externals/vulkan-utility-libraries\": \"https://chromium.googlesource.com/external/github.com/KhronosGroup/Vulkan-Utility-Libraries@87ab6b39a97d084a2ef27db85e3cbaf5d2622a09\",\n+-  \"third_party/externals/unicodetools\"           : \"https://chromium.googlesource.com/external/github.com/unicode-org/unicodetools@66a3fa9dbdca3b67053a483d130564eabc5fe095\",\n+-  #\"third_party/externals/v8\"                     : \"https://chromium.googlesource.com/v8/v8.git@5f1ae66d5634e43563b2d25ea652dfb94c31a3b4\",\n+   \"third_party/externals/wuffs\"                  : \"https://skia.googlesource.com/external/github.com/google/wuffs-mirror-release-c.git@e3f919ccfe3ef542cfc983a82146070258fb57f8\",\n+   \"third_party/externals/zlib\"                   : \"https://chromium.googlesource.com/chromium/src/third_party/zlib@646b7f569718921d7d4b5b8e22572ff6c76f2596\",\n+ \n+diff --git a/bin/activate-emsdk b/bin/activate-emsdk\n+index 687ca9f..7167d8d 100755\n+--- a/bin/activate-emsdk\n++++ b/bin/activate-emsdk\n+@@ -17,6 +17,7 @@ EMSDK_PATH = os.path.join(EMSDK_ROOT, 'emsdk.py')\n+ EMSDK_VERSION = '3.1.44'\n+ \n+ def main():\n++    return\n+     if sysconfig.get_platform() in ['linux-aarch64', 'linux-arm64']:\n+         # This platform cannot install emsdk at the provided version. See\n+         # https://github.com/emscripten-core/emsdk/blob/main/emscripten-releases-tags.json#L5\ndiff --git a/relnotes/README.m130.md b/relnotes/README.m130.md\nindex fa8bc7ba..4b61d791 100644\n--- a/relnotes/README.m130.md\n+++ b/relnotes/README.m130.md\n@@ -16,10 +16,10 @@ Since m128 (last beta release):\n     overloaded as an alias to this, too.\n \n   * The `SkUnicode` class is now available under python as `skia.Unicode`.\n-    The constructor is known to fail on windows - It is likely that downloading\n-    a `icudtl.dat` file, renaming from the versioned data-bin-{l,b}.zip in\n-    https://github.com/unicode-org/icu/releases, is needed. Windows users please report\n-    success/failure on this.\n+    The constructor is known to fail on windows on m130, without a bundled\n+    `icudtl.dat` file.\n+\n+    EDIT: Therefore, we bundle a `icudtl.dat` file for windows from m131 onward.\n \n   * There are two examples `shape_text.py` (a python port of upstream's example), and\n     `skparagraph-example.py` hosted [elsewhere](https://github.com/HinTak/skia-python-examples/).\ndiff --git a/relnotes/README.m131.md b/relnotes/README.m131.md\nnew file mode 100644\nindex 00000000..3c2602b8\n--- /dev/null\n+++ b/relnotes/README.m131.md\n@@ -0,0 +1,10 @@\n+Since m130:\n+\n+- We now build for Mac OS 13 (12 deprecated at github), and support python 3.13\n+\n+- GL examples and tests updated, to restrict \"Mac OS friendly\" GL settings to Mac only.\n+  Some Linux installations - specifically, inside github CI - are sensitive to those;\n+  typical desktop linux boxes with real graphic cards seem not to be affected.\n+\n+- `PDF.StructureElementNode.fAdditionalNodeIds` withdrawn.\n+  `SkPDF::StructureElementNode::fAdditionalNodeIds` removed upstream.\ndiff --git a/relnotes/README.m132.md b/relnotes/README.m132.md\nnew file mode 100644\nindex 00000000..3e6f769f\n--- /dev/null\n+++ b/relnotes/README.m132.md\n@@ -0,0 +1,26 @@\n+Since m131:\n+- `PathEffect.{DashType, DashInfo, asADash}` withdrawn.\n+   Upstream removed them from public API in m132.\n+\n+- We now bundle a `icudtl.dat` for windows. This fixes windows-specific problems\n+  with SkUnicode, libSkShaper, and SkParagraph. Thanks @meetwq for the change.\n+  Relevant CI tests re-enabled and added.\n+\n+- Upstream's `SkNamedTransferFn::*` now available as `cms.NamedTransferFn.*`,\n+  and `SkNamedGamut::*` as `cms.NamedGamut.*`. These are used in RuntimeEffect-related\n+  code.\n+\n+- More overloads of `SkImage::makeShader` added, and `SkImage::makeRawShader` added\n+  as `Image.makeRawShader`.\n+\n+- We binds `SkRuntimeEffect` as `skia.RuntimeEffect`. Added some helper classes:\n+  `SkV3` as `skia.V3`, `SkV4` as `skia.V4`,\n+  `SkRuntimeEffect::Result` as `RuntimeEffectResult`,\n+  `SkRuntimeEffect::ChildPtr` as `RuntimeEffectChildPtr`,\n+  `SkRuntimeEffectBuilder` as `RuntimeEffectBuilder`,\n+  `std::vector<SkRuntimeEffect::ChildPtr>` as `VectorRuntimeEffectChildPtr`,\n+  `SkSpan<const SkRuntimeEffect::ChildPtr>` as `SpanRuntimeEffectChildPtr`,\n+  `SkRuntimeEffectBuilder::BuilderUniform` as `RuntimeEffectBuilderUniform`,\n+  `SkRuntimeEffectBuilder::BuilderChild` as `RuntimeEffectBuilderChild`.\n+  Details are subjected to change. We ported all 9 of current upstream SkSL c++ examples,\n+  hosted [elsewhere](https://github.com/HinTak/skia-python-examples/).\ndiff --git a/scripts/build_Linux.sh b/scripts/build_Linux.sh\nindex 8fc6efca..4fda21eb 100644\n--- a/scripts/build_Linux.sh\n+++ b/scripts/build_Linux.sh\n@@ -60,8 +60,8 @@ git clone https://gn.googlesource.com/gn && \\\n \n # Build skia\n cd skia && \\\n-    patch -p1 < ../patch/skia-m130-minimize-download.patch && \\\n-    patch -p1 < ../patch/skia-m123-colrv1-freetype.diff && \\\n+    patch -p1 < ../patch/skia-m132-minimize-download.patch && \\\n+    patch -p1 < ../patch/skia-m132-colrv1-freetype.diff && \\\n     python3 tools/git-sync-deps && \\\n     cp -f ../gn/out/gn bin/gn && \\\n     bin/gn gen out/Release --args=\"\ndiff --git a/scripts/build_Windows.sh b/scripts/build_Windows.sh\nindex ce694e46..e56cc6de 100644\n--- a/scripts/build_Windows.sh\n+++ b/scripts/build_Windows.sh\n@@ -4,8 +4,8 @@ export PATH=\"${PWD}/depot_tools:$PATH\"\n \n # Build skia\n cd skia && \\\n-    patch -p1 < ../patch/skia-m130-minimize-download.patch && \\\n-    patch -p1 < ../patch/skia-m123-colrv1-freetype.diff && \\\n+    patch -p1 < ../patch/skia-m132-minimize-download.patch && \\\n+    patch -p1 < ../patch/skia-m132-colrv1-freetype.diff && \\\n     python tools/git-sync-deps && \\\n     bin/gn gen out/Release --args='\n is_official_build=true\ndiff --git a/scripts/build_macOS.sh b/scripts/build_macOS.sh\nindex 666d1fdb..2286603f 100644\n--- a/scripts/build_macOS.sh\n+++ b/scripts/build_macOS.sh\n@@ -22,8 +22,8 @@ function apply_patch {\n }\n \n cd skia && \\\n-    patch -p1 < ../patch/skia-m130-minimize-download.patch && \\\n-    patch -p1 < ../patch/skia-m123-colrv1-freetype.diff && \\\n+    patch -p1 < ../patch/skia-m132-minimize-download.patch && \\\n+    patch -p1 < ../patch/skia-m132-colrv1-freetype.diff && \\\n     python3 tools/git-sync-deps && \\\n     bin/gn gen out/Release --args=\"\n is_official_build=true\ndiff --git a/setup.py b/setup.py\nindex e4494234..62277de9 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -12,13 +12,14 @@\n     pass\n \n NAME = 'skia-python'\n-__version__ = '130.0b10'\n+__version__ = '132.0b11'\n \n SKIA_PATH = os.getenv('SKIA_PATH', 'skia')\n SKIA_OUT_PATH = os.getenv(\n     'SKIA_OUT_PATH', os.path.join(SKIA_PATH, 'out', 'Release')\n )\n \n+data_files = []\n if sys.platform == 'win32':\n     DEFINE_MACROS = []  # doesn't work for cl.exe\n     LIBRARIES = [\n@@ -54,6 +55,7 @@\n         '/OPT:ICF',\n         '/OPT:REF',\n     ]\n+    data_files = [('Lib/site-packages', [os.path.join(SKIA_OUT_PATH, 'icudtl.dat')])]\n elif sys.platform == 'darwin':\n     DEFINE_MACROS = [\n         ('VERSION_INFO', __version__),\n@@ -173,6 +175,7 @@ def build_extensions(self):\n     long_description=open('README.md', 'r').read(),\n     long_description_content_type='text/markdown',\n     ext_modules=[extension],\n+    data_files=data_files,\n     install_requires=[\n         'numpy',\n         'pybind11>=2.6'\ndiff --git a/skia b/skia\nindex 0f81dc8b..b1fb88ac 160000\n--- a/skia\n+++ b/skia\n@@ -1,1 +1,1 @@\n-Subproject commit 0f81dc8b2e43c060972a8d8d229b346694a674f8\n+Subproject commit b1fb88ac03b77cf36a1aa9802a5bdb673e3ff983\ndiff --git a/src/skia/ColorSpace.cpp b/src/skia/ColorSpace.cpp\nindex 25a5549f..c93bcf79 100644\n--- a/src/skia/ColorSpace.cpp\n+++ b/src/skia/ColorSpace.cpp\n@@ -37,6 +37,16 @@ py::class_<skcms_TransferFunction>(skcms, \"TransferFunction\",\n             };\n         }), py::arg(\"v\"));\n \n+/* Upstream static constexpr's, which function like enum's,\n+   but cannot be bound that way. */\n+m.attr(\"cms\").attr(\"NamedTransferFn\") = m.attr(\"cms\").attr(\"TransferFunction\");\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"kSRGB\")    = SkNamedTransferFn::kSRGB;\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"k2Dot2\")   = SkNamedTransferFn::k2Dot2;\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"kLinear\")  = SkNamedTransferFn::kLinear;\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"kRec2020\") = SkNamedTransferFn::kRec2020;\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"kPQ\")      = SkNamedTransferFn::kPQ;\n+m.attr(\"cms\").attr(\"TransferFunction\").attr(\"kHLG\")     = SkNamedTransferFn::kHLG;\n+\n py::class_<skcms_Matrix3x3>(skcms, \"Matrix3x3\",\n     R\"docstring(\n     A row-major 3x3 matrix (ie vals[row][col])\n@@ -52,6 +62,15 @@ py::class_<skcms_Matrix3x3>(skcms, \"Matrix3x3\",\n             }};\n         }), py::arg(\"v\"));\n \n+/* Upstream static constexpr's, which function like enum's,\n+   but cannot be bound that way. */\n+m.attr(\"cms\").attr(\"NamedGamut\") = m.attr(\"cms\").attr(\"Matrix3x3\");\n+m.attr(\"cms\").attr(\"Matrix3x3\").attr(\"kSRGB\")      = SkNamedGamut::kSRGB;\n+m.attr(\"cms\").attr(\"Matrix3x3\").attr(\"kAdobeRGB\")  = SkNamedGamut::kAdobeRGB;\n+m.attr(\"cms\").attr(\"Matrix3x3\").attr(\"kDisplayP3\") = SkNamedGamut::kDisplayP3;\n+m.attr(\"cms\").attr(\"Matrix3x3\").attr(\"kRec2020\")   = SkNamedGamut::kRec2020;\n+m.attr(\"cms\").attr(\"Matrix3x3\").attr(\"kXYZ\")       = SkNamedGamut::kXYZ;\n+\n py::class_<SkColorSpace, sk_sp<SkColorSpace>>(m, \"ColorSpace\")\n     .def(\"toProfile\",\n         [] (const SkColorSpace& colorspace) {\ndiff --git a/src/skia/Document.cpp b/src/skia/Document.cpp\nindex 102cc10e..cdab79f7 100644\n--- a/src/skia/Document.cpp\n+++ b/src/skia/Document.cpp\n@@ -216,8 +216,11 @@ py::class_<SkPDF::StructureElementNode>(pdf, \"StructureElementNode\",\n     //     &SkPDF::StructureElementNode::fChildVector)\n     .def_readwrite(\"fNodeId\",\n         &SkPDF::StructureElementNode::fNodeId)\n+/* Removed in m131 */\n+/*\n     .def_readonly(\"fAdditionalNodeIds\",\n         &SkPDF::StructureElementNode::fAdditionalNodeIds)\n+*/\n     .def_readonly(\"fAttributes\",\n         &SkPDF::StructureElementNode::fAttributes)\n     .def_readwrite(\"fAlt\",\ndiff --git a/src/skia/GrContext.cpp b/src/skia/GrContext.cpp\nindex caaf7dd4..2c5e88fe 100644\n--- a/src/skia/GrContext.cpp\n+++ b/src/skia/GrContext.cpp\n@@ -1,14 +1,14 @@\n #include \"common.h\"\n #include <include/core/SkTextureCompressionType.h>\n-#include <include/gpu/GrBackendSemaphore.h>\n-#include <include/gpu/GrBackendSurface.h>\n-#include <include/gpu/GrContextThreadSafeProxy.h>\n+#include <include/gpu/ganesh/GrBackendSemaphore.h>\n+#include <include/gpu/ganesh/GrBackendSurface.h>\n+#include <include/gpu/ganesh/GrContextThreadSafeProxy.h>\n #include <include/gpu/GpuTypes.h>\n-#include <include/gpu/mock/GrMockTypes.h>\n-#include <include/gpu/gl/GrGLInterface.h>\n+#include <include/gpu/ganesh/mock/GrMockTypes.h>\n+#include <include/gpu/ganesh/gl/GrGLInterface.h>\n #include <include/gpu/ganesh/gl/GrGLBackendSurface.h>\n #include <include/gpu/ganesh/gl/GrGLDirectContext.h>\n-#include <include/gpu/vk/GrVkTypes.h>\n+#include <include/gpu/ganesh/vk/GrVkTypes.h>\n #include <include/gpu/ganesh/vk/GrVkBackendSemaphore.h>\n #include <include/gpu/ganesh/vk/GrVkBackendSurface.h>\n #include <include/gpu/ganesh/vk/GrVkDirectContext.h>\n@@ -898,7 +898,7 @@ py::class_<GrDirectContext, sk_sp<GrDirectContext>, GrRecordingContext>(m, \"GrDi\n         )docstring\",\n         py::arg(\"info\"))\n     .def(\"flush\", py::overload_cast<>(&GrDirectContext::flush))\n-    .def(\"submit\", &GrDirectContext::submit,\n+    .def(\"submit\", py::overload_cast<GrSyncCpu>(&GrDirectContext::submit),\n         R\"docstring(\n         Submit outstanding work to the gpu from all previously un-submitted\n         flushes. The return value of the submit will indicate whether or not the\ndiff --git a/src/skia/GrContext_gl.cpp b/src/skia/GrContext_gl.cpp\nindex 6024abea..b0413997 100644\n--- a/src/skia/GrContext_gl.cpp\n+++ b/src/skia/GrContext_gl.cpp\n@@ -1,6 +1,6 @@\n #include \"common.h\"\n-#include <include/gpu/gl/GrGLTypes.h>\n-#include <include/gpu/gl/GrGLInterface.h>\n+#include <include/gpu/ganesh/gl/GrGLTypes.h>\n+#include <include/gpu/ganesh/gl/GrGLInterface.h>\n \n void initGrContext_gl(py::module &m) {\n \ndiff --git a/src/skia/GrContext_mock.cpp b/src/skia/GrContext_mock.cpp\nindex 24d94a10..3b9f7307 100644\n--- a/src/skia/GrContext_mock.cpp\n+++ b/src/skia/GrContext_mock.cpp\n@@ -1,6 +1,6 @@\n #include \"common.h\"\n-#include <include/gpu/mock/GrMockTypes.h>\n-#include <include/gpu/GrBackendSurface.h>\n+#include <include/gpu/ganesh/mock/GrMockTypes.h>\n+#include <include/gpu/ganesh/GrBackendSurface.h>\n #include <include/core/SkTextureCompressionType.h>\n \n void initGrContext_mock(py::module &m) {\ndiff --git a/src/skia/GrContext_vk.cpp b/src/skia/GrContext_vk.cpp\nindex 7a0705bd..75bdaa21 100644\n--- a/src/skia/GrContext_vk.cpp\n+++ b/src/skia/GrContext_vk.cpp\n@@ -1,6 +1,6 @@\n #include \"common.h\"\n #include <include/gpu/vk/VulkanBackendContext.h>\n-#include <include/gpu/vk/GrVkTypes.h>\n+#include <include/gpu/ganesh/vk/GrVkTypes.h>\n \n void initGrContext_vk(py::module &m) {\n \ndiff --git a/src/skia/Image.cpp b/src/skia/Image.cpp\nindex 13bb1c97..1d987ee4 100644\n--- a/src/skia/Image.cpp\n+++ b/src/skia/Image.cpp\n@@ -1,7 +1,7 @@\n #include \"common.h\"\n #include <include/codec/SkEncodedImageFormat.h>\n #include <include/core/SkSamplingOptions.h>\n-#include <include/gpu/GrBackendSurface.h>\n+#include <include/gpu/ganesh/GrBackendSurface.h>\n #include <include/gpu/GpuTypes.h>\n #include <include/gpu/ganesh/SkImageGanesh.h>\n #include <include/encode/SkJpegEncoder.h>\n@@ -1230,7 +1230,153 @@ image\n         py::arg_v(\"tmy\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n         py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n         py::arg(\"localMatrix\") = nullptr)\n-    // TODO: Other makeShader overloads.\n+    .def(\"makeShader\",\n+        py::overload_cast<SkTileMode, SkTileMode, const SkSamplingOptions&, const SkMatrix&>(\n+            &SkImage::makeShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        :py:class:`Shader` uses :py:class:`TileMode` rules to fill drawn area\n+        outside :py:class:`Image`. localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.TileMode tmx: tiling in the x direction\n+        :param skia.TileMode tmy: tiling in the y direction\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"tmx\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"tmy\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\"))\n+    .def(\"makeShader\",\n+        py::overload_cast<const SkSamplingOptions&, const SkMatrix&>(\n+            &SkImage::makeShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\"))\n+    .def(\"makeShader\",\n+        py::overload_cast<const SkSamplingOptions&, const SkMatrix*>(\n+            &SkImage::makeShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\") = nullptr)\n+    .def(\"makeRawShader\",\n+        py::overload_cast<SkTileMode, SkTileMode, const SkSamplingOptions&, const SkMatrix*>(\n+            &SkImage::makeRawShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        makeRawShader functions like makeShader, but for images that contain non-color data.\n+        This includes images encoding things like normals, material properties (eg, roughness),\n+        heightmaps, or any other purely mathematical data that happens to be stored in an image.\n+        These types of images are useful with some programmable shaders (see: SkRuntimeEffect).\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        :py:class:`Shader` uses :py:class:`TileMode` rules to fill drawn area\n+        outside :py:class:`Image`. localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.TileMode tmx: tiling in the x direction\n+        :param skia.TileMode tmy: tiling in the y direction\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"tmx\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"tmy\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\") = nullptr)\n+    .def(\"makeRawShader\",\n+        py::overload_cast<SkTileMode, SkTileMode, const SkSamplingOptions&, const SkMatrix&>(\n+            &SkImage::makeRawShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        makeRawShader functions like makeShader, but for images that contain non-color data.\n+        This includes images encoding things like normals, material properties (eg, roughness),\n+        heightmaps, or any other purely mathematical data that happens to be stored in an image.\n+        These types of images are useful with some programmable shaders (see: SkRuntimeEffect).\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        :py:class:`Shader` uses :py:class:`TileMode` rules to fill drawn area\n+        outside :py:class:`Image`. localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.TileMode tmx: tiling in the x direction\n+        :param skia.TileMode tmy: tiling in the y direction\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"tmx\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"tmy\", SkTileMode::kClamp, \"skia.TileMode.kClamp\"),\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\"))\n+    .def(\"makeRawShader\",\n+        py::overload_cast<const SkSamplingOptions&, const SkMatrix&>(\n+            &SkImage::makeRawShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        makeRawShader functions like makeShader, but for images that contain non-color data.\n+        This includes images encoding things like normals, material properties (eg, roughness),\n+        heightmaps, or any other purely mathematical data that happens to be stored in an image.\n+        These types of images are useful with some programmable shaders (see: SkRuntimeEffect).\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\"))\n+    .def(\"makeRawShader\",\n+        py::overload_cast<const SkSamplingOptions&, const SkMatrix*>(\n+            &SkImage::makeRawShader, py::const_),\n+        R\"docstring(\n+        Creates :py:class:`Shader` from :py:class:`Image`.\n+\n+        makeRawShader functions like makeShader, but for images that contain non-color data.\n+        This includes images encoding things like normals, material properties (eg, roughness),\n+        heightmaps, or any other purely mathematical data that happens to be stored in an image.\n+        These types of images are useful with some programmable shaders (see: SkRuntimeEffect).\n+\n+        :py:class:`Shader` dimensions are taken from :py:class:`Image`.\n+        localMatrix permits transforming\n+        :py:class:`Image` before :py:class:`Canvas` matrix is applied.\n+\n+        :param skia.Matrix localMatrix: :py:class:`Image` transformation, or\n+            nullptr\n+        :return: :py:class:`Shader` containing :py:class:`Image`\n+        )docstring\",\n+        py::arg_v(\"samplingOptions\", SkSamplingOptions(), \"skia.SamplingOptions()\"),\n+        py::arg(\"localMatrix\") = nullptr)\n     .def(\"peekPixels\", &SkImage::peekPixels,\n         R\"docstring(\n         Copies :py:class:`Image` pixel address, row bytes, and\ndiff --git a/src/skia/PathEffect.cpp b/src/skia/PathEffect.cpp\nindex 3e6a10ba..a01f2e7d 100644\n--- a/src/skia/PathEffect.cpp\n+++ b/src/skia/PathEffect.cpp\n@@ -138,6 +138,8 @@ py::class_<SkPathEffect, sk_sp<SkPathEffect>, SkFlattenable>\n         ~skia.TrimPathEffect\n     )docstring\");\n \n+/* SkPathEffect::DashInfo withdrawn from public API in m132 */\n+/*\n py::class_<SkPathEffect::DashInfo>(patheffect, \"DashInfo\")\n     .def(py::init<>())\n     .def_property_readonly(\"fIntervals\",\n@@ -160,6 +162,7 @@ py::class_<SkPathEffect::DashInfo>(patheffect, \"DashInfo\")\n         Offset into the dashed interval pattern.\n         )docstring\")\n     ;\n+*/\n \n /*\n py::class_<SkPathEffect::PointData> pointdata(patheffect, \"PointData\",\n@@ -195,6 +198,8 @@ pointdata\n     ;\n */\n \n+/* SkPathEffect::DashType withdrawn from public API in m132 */\n+/*\n py::enum_<SkPathEffect::DashType>(patheffect, \"DashType\",\n     R\"docstring(\n     If the :py:class:`PathEffect` can be represented as a dash pattern, asADash\n@@ -215,6 +220,7 @@ py::enum_<SkPathEffect::DashType>(patheffect, \"DashType\",\n     .value(\"kDash_DashType\", SkPathEffect::DashType::kDash_DashType,\n         \"fills in all of the info parameter\")\n     .export_values();\n+*/\n \n patheffect\n     .def(\"filterPath\", py::overload_cast<SkPath*, const SkPath&, SkStrokeRec*, const SkRect*>(&SkPathEffect::filterPath, py::const_),\n@@ -251,7 +257,10 @@ patheffect\n         py::arg(\"results\"), py::arg(\"src\"), py::arg(\"stroke_rec\"),\n         py::arg(\"matrix\"), py::arg(\"cullR\"))\n */\n+/* SkPathEffect::asADash withdrawn from public API in m132 */\n+/*\n     .def(\"asADash\", &SkPathEffect::asADash, py::arg(\"info\"))\n+*/\n     .def_static(\"MakeSum\",\n         [] (const SkPathEffect& first, const SkPathEffect& second) {\n             auto first_ = first.serialize();\ndiff --git a/src/skia/RuntimeEffect.cpp b/src/skia/RuntimeEffect.cpp\nnew file mode 100644\nindex 00000000..ee40d27a\n--- /dev/null\n+++ b/src/skia/RuntimeEffect.cpp\n@@ -0,0 +1,238 @@\n+#include <stdexcept>\n+#include \"common.h\"\n+#include <include/effects/SkRuntimeEffect.h>\n+//#include <include/core/SkM44.h> // defines SkV3, SkV4 ; M44 used in Matrix/Canvas ; Revisit.\n+#include <pybind11/stl_bind.h>\n+\n+PYBIND11_MAKE_OPAQUE(std::vector<SkRuntimeEffect::ChildPtr>)\n+\n+void initRuntimeEffect(py::module &m) {\n+py::class_<SkRuntimeEffect, sk_sp<SkRuntimeEffect>, SkRefCnt> runtime_effect(m, \"RuntimeEffect\");\n+\n+py::class_<SkRuntimeEffect::ChildPtr> runtime_effect_childptr(m, \"RuntimeEffectChildPtr\");\n+\n+py::bind_vector<std::vector<SkRuntimeEffect::ChildPtr>>(m, \"VectorRuntimeEffectChildPtr\");\n+py::class_<SkSpan<const SkRuntimeEffect::ChildPtr>> span_runtime_effect_childptr(m, \"SpanRuntimeEffectChildPtr\");\n+\n+py::class_<SkRuntimeEffectBuilder> runtime_effect_builder(m, \"RuntimeEffectBuilder\");\n+\n+py::class_<SkV3>(m, \"V3\")\n+    .def(py::init(\n+        [] (float x, float y, float z) {\n+            return SkV3{x, y, z};\n+        }))\n+    .def(py::init(\n+        [] (py::tuple v3) {\n+            if (v3.size() != 3)\n+                throw py::value_error(\"V3 must have exactly three elements.\");\n+            return SkV3{v3[0].cast<float>(), v3[1].cast<float>(), v3[2].cast<float>()};\n+        }),\n+        py::arg(\"v3\"))\n+    ;\n+\n+py::implicitly_convertible<py::tuple, SkV3>();\n+\n+py::class_<SkV4>(m, \"V4\")\n+    .def(py::init(\n+        [] (float x, float y, float z, float w) {\n+            return SkV4{x, y, z, w};\n+        }))\n+    .def(py::init(\n+        [] (py::tuple v4) {\n+            if (v4.size() != 4)\n+                throw py::value_error(\"V4 must have exactly four elements.\");\n+            return SkV4{v4[0].cast<float>(), v4[1].cast<float>(), v4[2].cast<float>(), v4[3].cast<float>()};\n+        }),\n+        py::arg(\"v4\"))\n+    ;\n+\n+py::implicitly_convertible<py::tuple, SkV4>();\n+\n+py::class_<SkRuntimeEffect::Result>(m, \"RuntimeEffectResult\")\n+    .def_readwrite(\"effect\", &SkRuntimeEffect::Result::effect)\n+    .def_readwrite(\"errorText\", &SkRuntimeEffect::Result::errorText)\n+    ;\n+\n+runtime_effect_childptr\n+    .def(py::init<>())\n+    .def(py::init<sk_sp<SkShader>>())\n+    .def(py::init<sk_sp<SkColorFilter>>())\n+    .def(py::init<sk_sp<SkBlender>>())\n+    ;\n+\n+span_runtime_effect_childptr\n+    .def(py::init<>())\n+    .def(py::init<const SkRuntimeEffect::ChildPtr*, size_t>())\n+    .def(py::init<const SkSpan<const SkRuntimeEffect::ChildPtr>&>())\n+    .def(py::init(\n+        [] (std::vector<SkRuntimeEffect::ChildPtr>& v) {\n+            return SkSpan<SkRuntimeEffect::ChildPtr>(&v[0], v.size());\n+        }))\n+    .def(py::init<const SkSpan<SkRuntimeEffect::ChildPtr>&>())\n+    ;\n+\n+py::implicitly_convertible<sk_sp<SkShader>, SkRuntimeEffect::ChildPtr>();\n+py::implicitly_convertible<sk_sp<SkColorFilter>, SkRuntimeEffect::ChildPtr>();\n+py::implicitly_convertible<sk_sp<SkBlender>, SkRuntimeEffect::ChildPtr>();\n+py::implicitly_convertible<std::vector<SkRuntimeEffect::ChildPtr>, SkSpan<const SkRuntimeEffect::ChildPtr>>();\n+\n+/*\n+  All of these static methods check Result.effect being non-null, throw with errorText if null;\n+  they differ from upsteam c++ APIs, which asks clients to check.\n+*/\n+runtime_effect\n+    .def_static(\"MakeForColorFilter\",\n+        [] (SkString sksl, const SkRuntimeEffect::Options& options) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForColorFilter(sksl, options);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"), py::arg(\"options\"))\n+    .def_static(\"MakeForColorFilter\",\n+        [] (SkString sksl) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForColorFilter(sksl);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"))\n+    .def_static(\"MakeForShader\",\n+        [] (SkString sksl, const SkRuntimeEffect::Options& options) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForShader(sksl, options);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"), py::arg(\"options\"))\n+    .def_static(\"MakeForShader\",\n+        [] (SkString sksl) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForShader(sksl);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"))\n+    .def_static(\"MakeForBlender\",\n+        [] (SkString sksl, const SkRuntimeEffect::Options& options) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForBlender(sksl, options);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"), py::arg(\"options\"))\n+    .def_static(\"MakeForBlender\",\n+        [] (SkString sksl) {\n+            auto [effect, err] = SkRuntimeEffect::MakeForBlender(sksl);\n+            if (!effect)\n+                throw std::runtime_error(err.data());\n+            return effect;\n+        },\n+        py::arg(\"sksl\"))\n+    .def(\"makeShader\",\n+        [] (SkRuntimeEffect& runtime_effect, sk_sp<const SkData> uniforms) {\n+            return runtime_effect.makeShader(uniforms, {});\n+        },\n+         py::arg(\"uniforms\"))\n+    .def(\"makeShader\",\n+        py::overload_cast<sk_sp<const SkData>, sk_sp<SkShader>[], size_t, const SkMatrix*>(&SkRuntimeEffect::makeShader, py::const_),\n+        py::arg(\"uniforms\"), py::arg(\"children\"),\n+        py::arg(\"childCount\"), py::arg(\"localMatrix\") = nullptr)\n+    .def(\"makeShader\",\n+        py::overload_cast<sk_sp<const SkData>, SkSpan<const SkRuntimeEffect::ChildPtr>, const SkMatrix*>(&SkRuntimeEffect::makeShader, py::const_),\n+        py::arg(\"uniforms\"), py::arg(\"children\"),\n+        py::arg(\"localMatrix\") = nullptr)\n+    .def(\"makeColorFilter\",\n+        py::overload_cast<sk_sp<const SkData>>(&SkRuntimeEffect::makeColorFilter, py::const_),\n+        py::arg(\"uniforms\"))\n+    .def(\"makeColorFilter\",\n+        py::overload_cast<sk_sp<const SkData>, sk_sp<SkColorFilter>[], size_t>(&SkRuntimeEffect::makeColorFilter, py::const_),\n+        py::arg(\"uniforms\"), py::arg(\"children\"),\n+        py::arg(\"childCount\"))\n+    .def(\"makeColorFilter\",\n+        py::overload_cast<sk_sp<const SkData>, SkSpan<const SkRuntimeEffect::ChildPtr>>(&SkRuntimeEffect::makeColorFilter, py::const_),\n+        py::arg(\"uniforms\"), py::arg(\"children\"))\n+    .def(\"makeBlender\",\n+        [] (SkRuntimeEffect& runtime_effect, sk_sp<const SkData> uniforms) {\n+            return runtime_effect.makeBlender(uniforms, {});\n+        },\n+        py::arg(\"uniforms\"))\n+    .def(\"makeBlender\",\n+        py::overload_cast<sk_sp<const SkData>, SkSpan<const SkRuntimeEffect::ChildPtr>>(&SkRuntimeEffect::makeBlender, py::const_),\n+        py::arg(\"uniforms\"), py::arg(\"children\") = SkSpan<const SkRuntimeEffect::ChildPtr>{})\n+    ;\n+\n+py::class_<SkRuntimeEffectBuilder::BuilderUniform>(m, \"RuntimeEffectBuilderUniform\")\n+    .def(py::init<>())\n+    ;\n+\n+py::class_<SkRuntimeEffectBuilder::BuilderChild>(m, \"RuntimeEffectBuilderChild\")\n+    .def(py::init<>())\n+    ;\n+\n+runtime_effect_builder\n+    .def(py::init<sk_sp<SkRuntimeEffect>>())\n+    .def(py::init<sk_sp<SkRuntimeEffect>, sk_sp<SkData>>())\n+    .def(\"uniform\", &SkRuntimeEffectBuilder::uniform,\n+        py::arg(\"name\"))\n+    .def(\"child\", &SkRuntimeEffectBuilder::child,\n+        py::arg(\"name\"))\n+    .def(\"setUniform\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, int uniform) {\n+            auto v = builder.uniform(name);\n+            v = uniform;\n+        },\n+        py::arg(\"name\"), py::arg(\"uniform\"))\n+    .def(\"setUniform\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, const SkV3& uniform) {\n+            auto v = builder.uniform(name);\n+            v = uniform;\n+        },\n+        py::arg(\"name\"), py::arg(\"uniform\"))\n+    .def(\"setUniform\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, const SkV4& uniform) {\n+            auto v = builder.uniform(name);\n+            v = uniform;\n+        },\n+        py::arg(\"name\"), py::arg(\"uniform\"))\n+    .def(\"setUniform\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, py::list vN) {\n+          if (vN.size() != 3 && vN.size() != 4)\n+                throw py::value_error(\"Input must have exactly three or four elements.\");\n+            auto v = builder.uniform(name);\n+            if (vN.size() == 3)\n+                v = SkV3{vN[0].cast<float>(), vN[1].cast<float>(), vN[2].cast<float>()};\n+            if (vN.size() == 4)\n+                v = SkV4{vN[0].cast<float>(), vN[1].cast<float>(), vN[2].cast<float>(), vN[3].cast<float>()};\n+        },\n+        py::arg(\"name\"), py::arg(\"uniform\"))\n+    .def(\"setChild\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, sk_sp<SkShader> child) {\n+            auto v = builder.child(name);\n+            v = child;\n+        },\n+        py::arg(\"name\"), py::arg(\"child\"))\n+    .def(\"setChild\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, sk_sp<SkColorFilter> child) {\n+            auto v = builder.child(name);\n+            v = child;\n+        },\n+        py::arg(\"name\"), py::arg(\"child\"))\n+    .def(\"setChild\",\n+        [] (SkRuntimeEffectBuilder& builder, std::string_view name, sk_sp<SkBlender> child) {\n+            auto v = builder.child(name);\n+            v = child;\n+        },\n+        py::arg(\"name\"), py::arg(\"child\"))\n+    .def(\"uniforms\", &SkRuntimeEffectBuilder::uniforms)\n+    .def(\"children\", &SkRuntimeEffectBuilder::children)\n+    .def(\"makeShader\", &SkRuntimeEffectBuilder::makeShader,\n+        py::arg(\"localMatrix\") = nullptr)\n+    .def(\"makeColorFilter\", &SkRuntimeEffectBuilder::makeColorFilter)\n+    .def(\"makeBlender\", &SkRuntimeEffectBuilder::makeBlender)\n+    ;\n+\n+m.attr(\"RuntimeShaderBuilder\") = m.attr(\"RuntimeEffectBuilder\");\n+m.attr(\"RuntimeColorFilterBuilder\") = m.attr(\"RuntimeEffectBuilder\");\n+m.attr(\"RuntimeBlendBuilder\") = m.attr(\"RuntimeEffectBuilder\");\n+}\ndiff --git a/src/skia/Shader.cpp b/src/skia/Shader.cpp\nindex e89f95b5..d1b37690 100644\n--- a/src/skia/Shader.cpp\n+++ b/src/skia/Shader.cpp\n@@ -1,5 +1,4 @@\n #include \"common.h\"\n-#include <include/effects/SkRuntimeEffect.h>\n #include <include/effects/SkGradientShader.h>\n #include <include/effects/SkPerlinNoiseShader.h>\n #include <include/effects/SkBlenders.h>\ndiff --git a/src/skia/Surface.cpp b/src/skia/Surface.cpp\nindex 7ecaf7e7..d4bbc026 100644\n--- a/src/skia/Surface.cpp\n+++ b/src/skia/Surface.cpp\n@@ -3,7 +3,7 @@\n #include <include/gpu/ganesh/gl/GrGLBackendSurface.h>\n #include <include/private/chromium/GrSurfaceCharacterization.h>\n #include <include/gpu/GpuTypes.h>\n-#include <include/gpu/GrTypes.h>\n+#include <include/gpu/ganesh/GrTypes.h>\n #include <include/gpu/ganesh/SkSurfaceGanesh.h>\n #include <include/gpu/MutableTextureState.h>\n #include <pybind11/operators.h>\ndiff --git a/src/skia/common.h b/src/skia/common.h\nindex d6f4d9d8..0cd21ab9 100644\n--- a/src/skia/common.h\n+++ b/src/skia/common.h\n@@ -12,7 +12,7 @@\n #include <include/core/SkData.h>\n #include <include/core/SkDocument.h>\n #include <include/core/SkFont.h>\n-#include <include/gpu/GrDirectContext.h>\n+#include <include/gpu/ganesh/GrDirectContext.h>\n #include <include/core/SkImage.h>\n #include <include/core/SkImageFilter.h>\n #include <include/core/SkImageInfo.h>\ndiff --git a/src/skia/main.cpp b/src/skia/main.cpp\nindex 8d7caca5..450e7b1e 100644\n--- a/src/skia/main.cpp\n+++ b/src/skia/main.cpp\n@@ -29,6 +29,7 @@ void initPoint(py::module &);\n void initRect(py::module &);\n void initRefCnt(py::module &);\n void initRegion(py::module &);\n+void initRuntimeEffect(py::module &);\n void initSamplingOptions(py::module &);\n void initScalar(py::module &);\n void initSize(py::module &);\n@@ -75,6 +76,7 @@ PYBIND11_MODULE(skia, m) {\n     initPathMeasure(m);\n     initPicture(m);\n     initPixmap(m);\n+    initRuntimeEffect(m);\n     initScalar(m);\n     initTextBlob(m);\n     initVertices(m);\n", "instance_id": "kyamagu__skia-python-274", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: Skparagraph fails to render newlines correctly on Windows, rendering them as unknown characters, while it works correctly on Linux. The statement includes a reproducible code snippet, expected behavior, and relevant environment details (OS versions, Python version, and library version). Screenshots are provided to visually demonstrate the issue on both platforms, which adds to the clarity. However, there are minor ambiguities that prevent a perfect score. The problem statement does not explicitly discuss potential causes (e.g., whether it's a text encoding issue, a platform-specific rendering bug, or a missing dependency), nor does it mention specific edge cases beyond the newline issue (e.g., different newline formats like CRLF vs LF, or behavior with other special characters). Additionally, while it references an upstream library (rust-skia) working correctly, it lacks detail on whether this suggests a specific fix or dependency to investigate. Overall, the statement is valid and clear but misses some finer details that could aid in diagnosing and resolving the issue.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors. First, the scope of code changes is significant, as seen in the diff provided. The changes span multiple files, including build scripts, CI configurations, documentation, and core library code, indicating a broad impact on the codebase. Notably, the update to a newer Skia version (m132) and the inclusion of a bundled `icudtl.dat` file for Windows suggest that the fix involves addressing a dependency or data file issue critical for Unicode handling, which directly relates to newline rendering in Skparagraph. This is not a trivial change, as it requires understanding the Skia library's internals and platform-specific behaviors.\n\nSecond, the number of technical concepts involved is substantial. The solution touches on Skia's text rendering pipeline (SkUnicode, SkParagraph), platform-specific build configurations (Windows vs Linux vs macOS), and dependency management (updating Skia versions and handling ICU data files for Unicode support). Additionally, there are modifications to Python bindings (e.g., `RuntimeEffect` additions) and CI workflows, which require familiarity with Python C++ extensions (pybind11), GitHub Actions, and cross-platform compatibility. Understanding how newline rendering ties into Unicode data files (`icudtl.dat`) and ensuring compatibility across platforms adds to the complexity.\n\nThird, while the problem statement does not explicitly mention edge cases beyond newline rendering, the code changes imply considerations for platform-specific behaviors (e.g., bundling `icudtl.dat` for Windows to fix Unicode issues). Error handling and testing across different environments (as seen in updated CI configurations) are also necessary, increasing the workload.\n\nFinally, the impact on the system's architecture is moderate but significant. Updating Skia to a newer version and modifying build processes could introduce regressions or compatibility issues, requiring careful validation. The changes also affect user-facing functionality (text rendering), which is a core feature of the library, necessitating thorough testing.\n\nOverall, this problem requires a deep understanding of the Skia library, cross-platform development, and build systems, along with the ability to navigate and modify a complex codebase. It falls short of \"Very Hard\" (0.8-1.0) because it does not appear to involve groundbreaking algorithmic innovation or system-level redesign, but it is still a challenging task due to the breadth of knowledge and precision required.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bindings(Python): Use Python classes for `Fragment` variants \nCurrently, the fragments of `Encoded` (Rust enum variants) are deserializes into variant dictionaries, e.g. `{\"Line\": {\"data\": list(b\"a NOOP\\r\\n\")}}`.\r\n\r\nBy using individual classes for each variant (`LineFragment`, `LiteralFragment`), handling these in the Python code would be greatly improved.\r\n\r\nRelated: This would remove the need for `Fragment` to implement `Serialize`, see #555.\n", "patch": "diff --git a/bindings/imap-codec-python/imap_codec.pyi b/bindings/imap-codec-python/imap_codec.pyi\nindex 89874e45..33e0544a 100644\n--- a/bindings/imap-codec-python/imap_codec.pyi\n+++ b/bindings/imap-codec-python/imap_codec.pyi\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import Tuple\n+from typing import Tuple, Union\n \n class DecodeError(Exception):\n     \"\"\"\n@@ -25,6 +25,72 @@ class DecodeLiteralFound(DecodeError):\n     The decoder stopped at the beginning of literal data.\n     \"\"\"\n \n+class LiteralMode:\n+    \"\"\"\n+    Literal mode, i.e., sync or non-sync.\n+\n+    - Sync: A synchronizing literal, i.e., `{<n>}\\r\\n<data>`.\n+    - NonSync: A non-synchronizing literal according to RFC 7888, i.e., `{<n>+}\\r\\n<data>`.\n+\n+    Warning: The non-sync literal extension must only be used when the server advertised support\n+             for it sending the LITERAL+ or LITERAL- capability.\n+    \"\"\"\n+\n+    Sync: LiteralMode\n+    NonSync: LiteralMode\n+\n+class LineFragment:\n+    \"\"\"\n+    Fragment of a line that is ready to be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes):\n+        \"\"\"\n+        Create a line fragment from data bytes\n+\n+        :param data: Data bytes of fragment\n+        :raises TypeError: `data` is not byte-like\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get line fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+class LiteralFragment:\n+    \"\"\"\n+    Fragment of a literal that may require an action before it should be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes, mode: LiteralMode):\n+        \"\"\"\n+        Create a literal fragment from data bytes and literal mode\n+\n+        :param data: Data bytes of fragment\n+        :param mode: Literal mode\n+        :raises TypeError: `data` is not byte-like\n+        :raises TypeError: `mode` is invalid\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get literal fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+    @property\n+    def mode(self) -> LiteralMode:\n+        \"\"\"\n+        Get literal fragment literal mode\n+\n+        :return: Literal mode\n+        \"\"\"\n+\n class Encoded:\n     \"\"\"\n     An encoded message.\n@@ -36,7 +102,7 @@ class Encoded:\n     \"\"\"\n \n     def __iter__(self) -> Encoded: ...\n-    def __next__(self) -> dict: ...\n+    def __next__(self) -> Union[LineFragment, LiteralFragment]: ...\n     def dump(self) -> bytes:\n         \"\"\"\n         Dump the (remaining) encoded data without being guided by fragments.\ndiff --git a/bindings/imap-codec-python/src/encoded.rs b/bindings/imap-codec-python/src/encoded.rs\nnew file mode 100644\nindex 00000000..d896933e\n--- /dev/null\n+++ b/bindings/imap-codec-python/src/encoded.rs\n@@ -0,0 +1,150 @@\n+use imap_codec::{\n+    encode::{Encoded, Fragment},\n+    imap_types::core::LiteralMode,\n+};\n+use pyo3::{prelude::*, types::PyBytes};\n+\n+/// Python class representing a literal mode\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+#[pyclass(name = \"LiteralMode\", eq)]\n+pub(crate) enum PyLiteralMode {\n+    Sync,\n+    NonSync,\n+}\n+\n+impl std::fmt::Display for PyLiteralMode {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match self {\n+            PyLiteralMode::Sync => f.write_str(\"LiteralMode.Sync\"),\n+            PyLiteralMode::NonSync => f.write_str(\"LiteralMode.NonSync\"),\n+        }\n+    }\n+}\n+\n+impl From<LiteralMode> for PyLiteralMode {\n+    fn from(value: LiteralMode) -> Self {\n+        match value {\n+            LiteralMode::Sync => PyLiteralMode::Sync,\n+            LiteralMode::NonSync => PyLiteralMode::NonSync,\n+        }\n+    }\n+}\n+\n+/// Python class representing a line fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LineFragment\", eq)]\n+pub(crate) struct PyLineFragment {\n+    data: Vec<u8>,\n+}\n+\n+#[pymethods]\n+impl PyLineFragment {\n+    /// Create a new line fragment from data\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn new(data: Vec<u8>) -> Self {\n+        Self { data }\n+    }\n+\n+    /// Retrieve the data from the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// String representation of the fragment, e.g. `b'hello'`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        self.data(py).to_string()\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LineFragment(b'hello')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LineFragment({})\", self.__str__(py))\n+    }\n+}\n+\n+/// Python class representing a literal fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LiteralFragment\", eq)]\n+pub(crate) struct PyLiteralFragment {\n+    data: Vec<u8>,\n+    mode: PyLiteralMode,\n+}\n+\n+#[pymethods]\n+impl PyLiteralFragment {\n+    /// Create a new literal fragment from data and mode\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn try_new(data: Vec<u8>, mode: PyLiteralMode) -> PyResult<Self> {\n+        Ok(Self { data, mode })\n+    }\n+\n+    /// Retrieve the data of the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// Retrieve the mode of the fragment\n+    #[getter]\n+    pub(crate) fn mode(&self) -> PyLiteralMode {\n+        self.mode\n+    }\n+\n+    /// String representation of the fragment, e.g. `(b'hello', 'Sync')`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        format!(\"({}, {})\", self.data(py), self.mode)\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LiteralFragment(b'hello', 'Sync')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LiteralFragment{}\", self.__str__(py))\n+    }\n+}\n+\n+/// Python wrapper classes for `Encoded`\n+///\n+/// This implements a Python iterator over the containing fragments.\n+#[derive(Debug, Clone)]\n+#[pyclass(name = \"Encoded\")]\n+pub(crate) struct PyEncoded(pub(crate) Option<Encoded>);\n+\n+#[pymethods]\n+impl PyEncoded {\n+    /// Initialize iterator\n+    pub(crate) fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n+        slf\n+    }\n+\n+    /// Return next fragment\n+    pub(crate) fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<PyObject>> {\n+        // Try to get next `Fragment` from `Encoded` iterator\n+        let Some(fragment) = slf.0.as_mut().and_then(|encoded| encoded.next()) else {\n+            return Ok(None);\n+        };\n+\n+        // Return instance of `PyLineFragment` or `PyLiteralFragment` as a generic `PyObject`.\n+        Ok(Some(match fragment {\n+            Fragment::Line { data } => {\n+                Bound::new(slf.py(), PyLineFragment::new(data))?.to_object(slf.py())\n+            }\n+            Fragment::Literal { data, mode } => {\n+                Bound::new(slf.py(), PyLiteralFragment::try_new(data, mode.into())?)?\n+                    .to_object(slf.py())\n+            }\n+        }))\n+    }\n+\n+    /// Dump remaining fragment data\n+    pub(crate) fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n+        let encoded = slf.0.take();\n+        let dump = match encoded {\n+            Some(encoded) => encoded.dump(),\n+            None => Vec::new(),\n+        };\n+        Ok(PyBytes::new_bound(slf.py(), &dump))\n+    }\n+}\ndiff --git a/bindings/imap-codec-python/src/lib.rs b/bindings/imap-codec-python/src/lib.rs\nindex 23ee0717..da568fb4 100644\n--- a/bindings/imap-codec-python/src/lib.rs\n+++ b/bindings/imap-codec-python/src/lib.rs\n@@ -1,6 +1,9 @@\n+mod encoded;\n+\n+use encoded::PyEncoded;\n use imap_codec::{\n     decode::{self, Decoder},\n-    encode::{Encoded, Encoder},\n+    encode::Encoder,\n     AuthenticateDataCodec, CommandCodec, GreetingCodec, IdleDoneCodec, ResponseCodec,\n };\n use pyo3::{create_exception, exceptions::PyException, prelude::*, types::PyBytes};\n@@ -11,46 +14,10 @@ create_exception!(imap_codec, DecodeFailed, DecodeError);\n create_exception!(imap_codec, DecodeIncomplete, DecodeError);\n create_exception!(imap_codec, DecodeLiteralFound, DecodeError);\n \n-/// Wrapper for `Encoded`\n-///\n-/// This implements a Python iterator over the containing fragments.\n-#[derive(Debug, Clone)]\n-#[pyclass(name = \"Encoded\")]\n-struct PyEncoded(Option<Encoded>);\n-\n-#[pymethods]\n-impl PyEncoded {\n-    /// Initialize iterator\n-    fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n-        slf\n-    }\n-\n-    /// Return next fragment\n-    fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<Bound<PyAny>>> {\n-        let Some(encoded) = &mut slf.0 else {\n-            return Ok(None);\n-        };\n-        Ok(encoded\n-            .next()\n-            .map(|value| serde_pyobject::to_pyobject(slf.py(), &value))\n-            .transpose()?)\n-    }\n-\n-    /// Dump remaining fragment data\n-    fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n-        let encoded = slf.0.take();\n-        let dump = match encoded {\n-            Some(encoded) => encoded.dump(),\n-            None => Vec::new(),\n-        };\n-        Ok(PyBytes::new_bound(slf.py(), &dump))\n-    }\n-}\n-\n-/// Wrapper for `GreetingCodec`\n+/// Python class for using `GreetingCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"GreetingCodec\")]\n-struct PyGreetingCodec(GreetingCodec);\n+struct PyGreetingCodec;\n \n #[pymethods]\n impl PyGreetingCodec {\n@@ -80,10 +47,10 @@ impl PyGreetingCodec {\n     }\n }\n \n-/// Wrapper for `CommandCodec`\n+/// Python class for using `CommandCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"CommandCodec\")]\n-struct PyCommandCodec(CommandCodec);\n+struct PyCommandCodec;\n \n #[pymethods]\n impl PyCommandCodec {\n@@ -119,10 +86,10 @@ impl PyCommandCodec {\n     }\n }\n \n-/// Wrapper for `AuthenticateDataCodec`\n+/// Python class for using `AuthenticateDataCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"AuthenticateDataCodec\")]\n-struct PyAuthenticateDataCodec(AuthenticateDataCodec);\n+struct PyAuthenticateDataCodec;\n \n #[pymethods]\n impl PyAuthenticateDataCodec {\n@@ -151,10 +118,10 @@ impl PyAuthenticateDataCodec {\n     }\n }\n \n-/// Wrapper for `ResponseCodec`\n+/// Python class for using `ResponseCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"ResponseCodec\")]\n-struct PyResponseCodec(ResponseCodec);\n+struct PyResponseCodec;\n \n #[pymethods]\n impl PyResponseCodec {\n@@ -188,10 +155,10 @@ impl PyResponseCodec {\n     }\n }\n \n-/// Wrapper for `IdleDoneCodec`\n+/// Python class for using `IdleDoneCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"IdleDoneCodec\")]\n-struct PyIdleDoneCodec(IdleDoneCodec);\n+struct PyIdleDoneCodec;\n \n #[pymethods]\n impl PyIdleDoneCodec {\n@@ -233,6 +200,9 @@ fn imap_codec_python(m: &Bound<'_, PyModule>) -> PyResult<()> {\n         \"DecodeLiteralFound\",\n         m.py().get_type_bound::<DecodeLiteralFound>(),\n     )?;\n+    m.add_class::<encoded::PyLiteralMode>()?;\n+    m.add_class::<encoded::PyLineFragment>()?;\n+    m.add_class::<encoded::PyLiteralFragment>()?;\n     m.add_class::<PyEncoded>()?;\n     m.add_class::<PyGreetingCodec>()?;\n     m.add_class::<PyCommandCodec>()?;\n", "instance_id": "duesee__imap-codec-561", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to improve the handling of `Fragment` variants in Python by using dedicated classes (`LineFragment` and `LiteralFragment`) instead of dictionaries. It specifies the current behavior (deserialization into dictionaries) and the desired outcome (using classes for better handling). However, it lacks critical details such as explicit input/output formats, specific constraints, or examples of how the new classes should behave in different scenarios. Additionally, there is no mention of edge cases or potential challenges in integrating these changes with the existing Rust-Python bindings. The reference to issue #555 provides some context but is not detailed in the statement itself. Overall, while the goal is understandable, minor ambiguities and missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files, including Python type hints (`.pyi`), Rust code for Python bindings (`encoded.rs`), and modifications to the main library module (`lib.rs`). This requires understanding the interaction between Rust and Python via `pyo3`, a moderately complex binding framework. Second, the technical concepts involved include Rust enums, Python class definitions, iterator implementations, and type conversions between Rust and Python, which demand a solid grasp of both languages and their interoperability. Third, while the problem does not explicitly mention edge cases, the nature of the changes (e.g., handling different fragment types and literal modes) implies the need to consider potential issues like invalid data inputs or mode mismatches, though these are not overly complex. Finally, the changes do not appear to impact the broader system architecture significantly, as they are confined to the binding layer. Overall, this task requires a moderate level of expertise and effort, involving complex modifications across a few files but not posing deep architectural or domain-specific challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Index folder created in data-dir should explicitly set permission modes\n## Description\r\n\r\nThe index folder for `/learn` is created in the `jupyter_data_dir()`:\r\n\r\nhttps://github.com/jupyterlab/jupyter-ai/blob/2019571d916549262d04b3c02ec13e043935a7d4/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py#L28\r\n\r\nwhich by default has `0o700` mode (see [definition in `jupyter-core`](https://github.com/jupyter/jupyter_core/blob/fa513c1550bbd1ebcc14a4a79eb8c5d95e3e23c9/jupyter_core/application.py#L101-L106)).\r\n\r\nHowever, the code in jupyter-ai does not explicitly impose this restriction which may be worrying to some users: \r\n\r\nhttps://github.com/jupyterlab/jupyter-ai/blob/2019571d916549262d04b3c02ec13e043935a7d4/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py#L66-L67\r\n\r\nThis is because `os.makedirs` uses very lenient `0o777` mode by default ([ref](https://docs.python.org/3/library/os.html#os.makedirs)).\r\n\r\nOn some file systems this may lead to undesirable file permissions. Being explicit ensures that files are only accessible to the single user on multi-tenant systems.\r\n\r\n## Context\r\n\r\n`main` at https://github.com/jupyterlab/jupyter-ai/commit/2019571d916549262d04b3c02ec13e043935a7d4\n", "patch": "diff --git a/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py b/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py\nindex 2d09aa9a5..8d6fb09aa 100644\n--- a/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py\n+++ b/packages/jupyter-ai/jupyter_ai/chat_handlers/learn.py\n@@ -19,6 +19,7 @@\n     IndexMetadata,\n )\n from jupyter_core.paths import jupyter_data_dir\n+from jupyter_core.utils import ensure_dir_exists\n from langchain.schema import BaseRetriever, Document\n from langchain.text_splitter import (\n     LatexTextSplitter,\n@@ -97,11 +98,12 @@ def __init__(self, *args, **kwargs):\n         self.metadata = IndexMetadata(dirs=[])\n         self.prev_em_id = None\n \n-        if not os.path.exists(INDEX_SAVE_DIR):\n-            os.makedirs(INDEX_SAVE_DIR)\n-\n+        self._ensure_dirs()\n         self._load()\n \n+    def _ensure_dirs(self):\n+        ensure_dir_exists(INDEX_SAVE_DIR, mode=0o700)\n+\n     def _load(self):\n         \"\"\"Loads the vector store.\"\"\"\n         if self.index is not None:\n", "instance_id": "jupyterlab__jupyter-ai-887", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly explains the issue with the default permissions of the index folder created by `os.makedirs` in the `jupyter-ai` codebase, referencing specific lines of code and providing links to relevant documentation and commits. The goal is explicit: to enforce a stricter permission mode (`0o700`) for the folder to ensure security on multi-tenant systems. The context, including the default behavior of `os.makedirs` and the desired behavior from `jupyter-core`, is well-articulated. There are no significant ambiguities, and the problem description includes all necessary details about the issue, its implications, and the expected solution. The only minor omission is the lack of explicit mention of potential edge cases (e.g., behavior on different file systems), but this does not detract from the overall clarity given the focused scope of the issue.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue requires a straightforward modification to replace the use of `os.makedirs` with `ensure_dir_exists` from `jupyter_core.utils`, explicitly setting the permission mode to `0o700`. The code change is minimal, confined to a single file (`learn.py`), and involves only a few lines of code: removing the old directory creation logic, adding an import, and introducing a small helper method. It does not require deep understanding of the broader codebase or complex interactions between modules, as the change is isolated to the initialization logic of a specific class. The technical concepts involved are basic\u2014understanding file permissions and Python's standard library functions for directory creation. There are no significant edge cases or error handling requirements mentioned in the problem statement, and the provided code change does not introduce new error handling logic. Overall, this is a very easy task that a junior developer with basic Python knowledge could handle with minimal guidance.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Terms lookup feature no longer supported due to recent release\nA recent release appears to have broken the ability to perform a \"terms lookup\" query which is a special subset of terms query which allows you to specify the index, id, and field so you can limit terms to those found in a specific document in another index:\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html#query-dsl-terms-lookup\n\nI used to be able to create a \"terms lookup\" query like so:\n\n```\nQ('terms', **{'_id': {\n    'index': 'my-other-index',\n    'id': 'my-document-id',\n    'path': 'my-field'\n}})\n```\n\nNow, however, the query generated flattens the dictionary into a list of keys, so that what actually gets built is this:\n\n```\n\"terms\": {\n    \"_id\": [\n        \"index\",\n        \"id\",\n        \"path\"\n    ]\n}\n```\n\nThis seems to be an unintended side-effect of a recent release. Please advise.\n", "patch": "diff --git a/elasticsearch_dsl/query.py b/elasticsearch_dsl/query.py\nindex 607df3ff..4d131d1b 100644\n--- a/elasticsearch_dsl/query.py\n+++ b/elasticsearch_dsl/query.py\n@@ -2647,7 +2647,7 @@ def __init__(\n \n     def _setattr(self, name: str, value: Any) -> None:\n         # here we convert any iterables that are not strings to lists\n-        if hasattr(value, \"__iter__\") and not isinstance(value, (str, list)):\n+        if hasattr(value, \"__iter__\") and not isinstance(value, (str, list, dict)):\n             value = list(value)\n         super()._setattr(name, value)\n \ndiff --git a/utils/templates/query.py.tpl b/utils/templates/query.py.tpl\nindex 23b23809..ca95f5a0 100644\n--- a/utils/templates/query.py.tpl\n+++ b/utils/templates/query.py.tpl\n@@ -365,7 +365,7 @@ EMPTY_QUERY = MatchAll()\n     {% elif k.name == \"Terms\" %}\n     def _setattr(self, name: str, value: Any) -> None:\n         # here we convert any iterables that are not strings to lists\n-        if hasattr(value, \"__iter__\") and not isinstance(value, (str, list)):\n+        if hasattr(value, \"__iter__\") and not isinstance(value, (str, list, dict)):\n             value = list(value)\n         super()._setattr(name, value)\n \n", "instance_id": "elastic__elasticsearch-dsl-py-1921", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a recent release has broken the \"terms lookup\" query functionality in an Elasticsearch DSL library, resulting in incorrect query generation due to dictionary flattening. The goal is evident (fix the query generation for terms lookup), and the problem provides a specific example of the incorrect output. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected correct query structure (though it can be inferred from the context and the provided link to Elasticsearch documentation). Additionally, there are no mentions of edge cases, constraints, or specific requirements for backward compatibility. While the issue is understandable with some domain knowledge of Elasticsearch, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows a very small and targeted change in two files (`query.py` and a template file). The modification is limited to a single line in each file, adjusting a condition in the `_setattr` method to exclude dictionaries from being converted to lists. This change is localized and does not impact the broader architecture of the system or require understanding complex interactions between modules. The amount of code change is minimal.\n\n2. **Technical Concepts Involved:** Solving this issue requires basic knowledge of Python, specifically understanding type checking with `isinstance()` and handling iterables. Familiarity with the Elasticsearch DSL library and its query-building mechanism is helpful but not strictly necessary, as the fix is more about general Python behavior than domain-specific logic. No advanced algorithms, design patterns, or complex library features are involved.\n\n3. **Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error conditions to handle. The code change itself does not introduce new error handling logic; it simply prevents an unintended conversion of dictionaries to lists. While there might be implicit edge cases (e.g., nested dictionaries or other complex data structures), they are not addressed in the problem or the diff, and the fix appears straightforward.\n\n4. **Overall Complexity:** The issue is a simple bug fix that addresses an unintended side effect of a previous change. It requires minimal debugging (the root cause is already identified in the problem statement) and a basic understanding of Python's type system. The impact is limited to a specific feature (terms lookup query), and there are no indications of broader performance or compatibility concerns.\n\nGiven these points, a difficulty score of 0.30 reflects the simplicity of the fix, requiring only a basic understanding of Python and a small, targeted code modification. It is slightly above the \"Very Easy\" range due to the need for some contextual understanding of the library's query-building logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "to_dict() doesn't convert inner_hits\nMaybe related to https://github.com/elastic/elasticsearch-dsl-py/issues/291, I'm attempting to dump my elasticsearch-dsl response by calling `to_dict()`, but it doesn't appear to walk the entire structure. `inner_hits` appear to be `Response` instances, but calling `to_dict()` on the top-level response doesn't convert them. Why does it convert hits but not inner_hits? Is there a simple way to convert the _entire_ response without having to pick apart the structure and manually call `to_dict()` as needed?\r\n\r\nI'm using Python 3.8 and elasticsearch-dsl 7.4.0.\r\n\r\nThanks so much for this library! \u2764\ufe0f It's incredibly useful.\n", "patch": "diff --git a/elasticsearch_dsl/utils.py b/elasticsearch_dsl/utils.py\nindex 021afc99..d9727f1e 100644\n--- a/elasticsearch_dsl/utils.py\n+++ b/elasticsearch_dsl/utils.py\n@@ -86,6 +86,17 @@ def _wrap(val: Any, obj_wrapper: Optional[Callable[[Any], Any]] = None) -> Any:\n     return val\n \n \n+def _recursive_to_dict(value: Any) -> Any:\n+    if hasattr(value, \"to_dict\"):\n+        return value.to_dict()\n+    elif isinstance(value, dict) or isinstance(value, AttrDict):\n+        return {k: _recursive_to_dict(v) for k, v in value.items()}\n+    elif isinstance(value, list) or isinstance(value, AttrList):\n+        return [recursive_to_dict(elem) for elem in value]\n+    else:\n+        return value\n+\n+\n class AttrList(Generic[_ValT]):\n     def __init__(\n         self, l: List[_ValT], obj_wrapper: Optional[Callable[[_ValT], Any]] = None\n@@ -228,8 +239,10 @@ def __setattr__(self, name: str, value: _ValT) -> None:\n     def __iter__(self) -> Iterator[str]:\n         return iter(self._d_)\n \n-    def to_dict(self) -> Dict[str, _ValT]:\n-        return self._d_\n+    def to_dict(self, recursive: bool = False) -> Dict[str, _ValT]:\n+        return cast(\n+            Dict[str, _ValT], _recursive_to_dict(self._d_) if recursive else self._d_\n+        )\n \n     def keys(self) -> Iterable[str]:\n         return self._d_.keys()\n", "instance_id": "elastic__elasticsearch-dsl-py-1892", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `to_dict()` method in the `elasticsearch-dsl` library does not convert `inner_hits` within a response to a dictionary format, unlike `hits`. The user provides context about their environment (Python 3.8, elasticsearch-dsl 7.4.0) and references a related GitHub issue, which adds some clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or output format for `inner_hits` when converted to a dictionary. Additionally, there are no examples of input/output or specific edge cases mentioned (e.g., nested structures beyond `inner_hits` or empty responses). While the goal is understandable, these missing details prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to a single file (`elasticsearch_dsl/utils.py`) and involve modifying the `to_dict()` method of the `AttrDict` class to support recursive conversion of nested structures. A new helper function `_recursive_to_dict()` is introduced to handle nested dictionaries and lists. The changes are relatively small (about 15 lines of code) and do not impact the broader architecture of the library. There is no need to understand complex interactions across multiple modules.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Python's object-oriented programming (e.g., method overriding, attribute checking with `hasattr()`), recursion for handling nested data structures, and familiarity with the library's custom types like `AttrDict` and `AttrList`. These concepts are not particularly advanced for a developer with moderate Python experience. No complex algorithms, design patterns, or domain-specific knowledge (beyond basic familiarity with Elasticsearch responses) are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases, but the code changes handle common scenarios like nested dictionaries and lists. Potential edge cases (e.g., circular references, very deeply nested structures, or unsupported types) are not addressed in the provided diff, and there is no explicit error handling added. However, the simplicity of the change suggests that handling such cases might not be critical for the immediate fix. The complexity of edge cases appears low at this stage.\n\n4. **Overall Complexity**: The task involves understanding the existing behavior of `to_dict()` and extending it to recursively process nested structures. This requires some code logic comprehension and a straightforward modification to add a recursive option. It does not demand deep knowledge of the entire codebase or advanced technical skills.\n\nGiven these points, I assign a difficulty score of 0.35, placing it on the higher end of the Easy range due to the need for recursion and understanding of nested data structures, but still within a scope that a developer with basic to intermediate Python skills can handle without significant challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add .pyi stub files to Python package\n## What\n\nFollowing on from #747, add `.pyi` stub files to our Python package covering the `_openassetio` Python extension module.\n\n## Why\n\nIDEs can use `.pyi` stub files located in the package to greatly enhance code-completion and documentation for Python C extension modules.\n\n## Acceptance Criteria\n\n- Installing via `pip install` includes an `_openassetio` directory containing `.pyi` stub files (satisfying [PEP 561](https://peps.python.org/pep-0561/)), which PyCharm (and other IDEs) can detect and use to enhance code completion.\n- `.pyi` stubs are included in the binary wheels on PyPI.\n-  `.pyi` stubs are installed when doing a from-source install i.e. `pip install .`\n- ~~`.pyi` stubs are installed (to the source tree, just like the extension module itself) when doing a `--editable` install.~~\n- Installing via `cmake --install`  bundles the `.pyi` stubs in the install tree.\n\n# Notes\n\nSee #747 for investigation with proposed implementation.\n", "patch": "diff --git a/.github/workflows/build-wheels.yml b/.github/workflows/build-wheels.yml\nindex 98850999d..9fed1e2c6 100644\n--- a/.github/workflows/build-wheels.yml\n+++ b/.github/workflows/build-wheels.yml\n@@ -72,6 +72,8 @@ jobs:\n           PIP_VERBOSE: 1\n           # Required as we make use of c++17 features\n           MACOSX_DEPLOYMENT_TARGET: 10.15\n+          # Ensure .pyi stub tests won't be skipped\n+          OPENASSETIO_TEST_ENABLE_PYTHON_STUBGEN: 1\n \n       - uses: actions/upload-artifact@v3\n         with:\ndiff --git a/CMakeLists.txt b/CMakeLists.txt\nindex fff7fcb04..db553e0c4 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -127,6 +127,8 @@ if (OPENASSETIO_ENABLE_PYTHON)\n         \" discoverability and prevent overwrite by package managers such as pip\")\n     option(OPENASSETIO_ENABLE_PYTHON_INSTALL_DIST_INFO\n         \"${OPENASSETIO_ENABLE_PYTHON_INSTALL_DIST_INFO_desc}\" ON)\n+\n+    option(OPENASSETIO_ENABLE_PYTHON_STUBGEN \"Enable Python .pyi stub file generation\" ON)\n endif ()\n \n # Enable C bindings, built as a separate library that depends on the\n@@ -323,6 +325,7 @@ message(STATUS \"Enable Python module build         = ${OPENASSETIO_ENABLE_PYTHON\n if (OPENASSETIO_ENABLE_PYTHON)\n     message(STATUS \"Python install .dist-info metadata = ${OPENASSETIO_ENABLE_PYTHON_INSTALL_DIST_INFO}\")\n     message(STATUS \"Python relative install dir        = ${OPENASSETIO_PYTHON_SITEDIR}\")\n+    message(STATUS \"Generate .pyi stubs                = ${OPENASSETIO_ENABLE_PYTHON_STUBGEN}\")\n endif ()\n message(STATUS \"Create test targets                = ${OPENASSETIO_ENABLE_TESTS}\")\n message(STATUS \"Create Python venv during tests    = ${OPENASSETIO_ENABLE_PYTHON_TEST_VENV}\")\ndiff --git a/CMakePresets.json b/CMakePresets.json\nindex 99ea59ea2..669a20437 100644\n--- a/CMakePresets.json\n+++ b/CMakePresets.json\n@@ -31,7 +31,10 @@\n         \"enable-sanitizers\",\n         \"enable-targets-all\",\n         \"enable-tests\"\n-      ]\n+      ],\n+      \"cacheVariables\": {\n+        \"OPENASSETIO_ENABLE_PYTHON_STUBGEN\": \"OFF\"\n+      }\n     },\n     {\n       \"name\": \"test\",\ndiff --git a/RELEASE_NOTES.md b/RELEASE_NOTES.md\nindex 9ddffea44..e8942dab0 100644\n--- a/RELEASE_NOTES.md\n+++ b/RELEASE_NOTES.md\n@@ -27,6 +27,11 @@ v1.0.0-beta.x.y\n   logger.\n   [#1014](https://github.com/OpenAssetIO/OpenAssetIO/issues/1014)\n \n+### Improvements\n+\n+- Added `.pyi` stub files to the Python package to aid IDE code\n+  completion for Python bindings of C++ types.\n+  [#1252](https://github.com/OpenAssetIO/OpenAssetIO/issues/1252)\n \n v1.0.0-beta.2.1\n ---------------\ndiff --git a/cmake/Testing.cmake b/cmake/Testing.cmake\nindex b853e65fe..4e0e26e27 100644\n--- a/cmake/Testing.cmake\n+++ b/cmake/Testing.cmake\n@@ -108,8 +108,11 @@ if (OPENASSETIO_ENABLE_PYTHON)\n     #-------------------------------------------------------------------\n     # Common environment variables for pytest tests.\n \n-    set(_pytest_env\n-        OPENASSETIO_TEST_CPP_PLUGINS_SUBDIR=${OPENASSETIO_TEST_CPP_PLUGINS_SUBDIR})\n+    set(\n+        _pytest_env\n+        OPENASSETIO_TEST_CPP_PLUGINS_SUBDIR=${OPENASSETIO_TEST_CPP_PLUGINS_SUBDIR}\n+        OPENASSETIO_TEST_ENABLE_PYTHON_STUBGEN=$<BOOL:${OPENASSETIO_ENABLE_PYTHON_STUBGEN}>\n+    )\n \n     #-------------------------------------------------------------------\n     # Gather ASan-specific environment variables to prepend to the\ndiff --git a/resources/build/Dockerfile b/resources/build/Dockerfile\nindex a8b8a8a6b..c88f7e75a 100644\n--- a/resources/build/Dockerfile\n+++ b/resources/build/Dockerfile\n@@ -74,8 +74,9 @@ COPY --from=openassetio-dependencies /usr/local/cmake/pcre2-config-version.cmake\n COPY --from=openassetio-dependencies /usr/local/lib64/cmake/trompeloeil /usr/local/lib64/cmake/trompeloeil\n COPY --from=openassetio-dependencies /usr/local/include/trompeloeil.hpp /usr/local/include/trompeloeil.hpp\n \n-# Update CMake. See cmake_minimum_required in top-level CMakeLists.txt.\n-RUN pip install cmake==3.28.3\n+# * Update CMake. See cmake_minimum_required in top-level CMakeLists.txt.\n+# * Add pybind11-stubgen for generating .pyi stub files\n+RUN pip install cmake==3.28.3 pybind11-stubgen==2.5.1\n \n LABEL org.opencontainers.image.name=\"openassetio-build\"\n LABEL org.opencontainers.image.title=\"OpenAssetIO VFX CY2022 Build Docker Image\"\ndiff --git a/resources/build/bootstrap-ubuntu-20.04.sh b/resources/build/bootstrap-ubuntu-20.04.sh\nindex 0ea22aa14..258e5d154 100644\n--- a/resources/build/bootstrap-ubuntu-20.04.sh\n+++ b/resources/build/bootstrap-ubuntu-20.04.sh\n@@ -8,7 +8,7 @@ sudo apt-get update\n sudo apt-get install -y build-essential pkgconf clang-format-12 clang-tidy-12 python3-pip ccache\n \n # Install additional build tools.\n-sudo pip3 install -r \"$WORKSPACE/resources/build/requirements.txt\"\n+pip3 install -r \"$WORKSPACE/resources/build/requirements.txt\"\n # Use explicit predictable conan root path, where packages are cached.\n export CONAN_USER_HOME=\"$HOME/conan\"\n # Create default conan profile so we can configure it before install.\ndiff --git a/resources/build/requirements.txt b/resources/build/requirements.txt\nindex fbea7e958..32ad34d4d 100644\n--- a/resources/build/requirements.txt\n+++ b/resources/build/requirements.txt\n@@ -2,3 +2,4 @@ conan==1.59.0\n cmake==3.28.3\n ninja==1.10.2.3\n pip>=21.3\n+pybind11-stubgen==2.5.1\ndiff --git a/src/openassetio-python/cmodule/CMakeLists.txt b/src/openassetio-python/cmodule/CMakeLists.txt\nindex fac04b101..83e5fd9f0 100644\n--- a/src/openassetio-python/cmodule/CMakeLists.txt\n+++ b/src/openassetio-python/cmodule/CMakeLists.txt\n@@ -124,3 +124,102 @@ if (OPENASSETIO_ENABLE_TESTS)\n     target_compile_definitions(openassetio-python-module PRIVATE OPENASSETIO_ENABLE_TESTS)\n     target_link_libraries(openassetio-python-module PRIVATE openassetio-python-module-test)\n endif ()\n+\n+\n+#-----------------------------------------------------------------------\n+# Generate .pyi stubs\n+#\n+# pybind11-stubgen will create openassetio/_openassetio subdirectories\n+# in the build output directory. We use this knowledge to assemble a\n+# Python pseudo-package in the build output directory, such that stubgen\n+# can import the module to do its thing then dump its output alongside.\n+\n+if (OPENASSETIO_ENABLE_PYTHON_STUBGEN)\n+    # Check pybind11-stubgen is available.\n+    execute_process(\n+        COMMAND \"${Python_EXECUTABLE}\" -m pybind11_stubgen --help\n+        OUTPUT_VARIABLE _pybind11_stubgen_output\n+        ERROR_VARIABLE _pybind11_stubgen_output\n+        RESULT_VARIABLE _pybind11_stubgen_exit_code\n+    )\n+\n+    # Fatal error if pybind11-stubgen isn't available.\n+    if (NOT _pybind11_stubgen_exit_code EQUAL \"0\")\n+        message(\n+            FATAL_ERROR\n+            \"OPENASSETIO_ENABLE_PYTHON_STUBGEN=${OPENASSETIO_ENABLE_PYTHON_STUBGEN} but\"\n+            \" pybind11-stubgen not found: status=${_pybind11_stubgen_exit_code}:\"\n+            \" ${_pybind11_stubgen_output}\"\n+        )\n+    endif()\n+\n+    # Create temporary pseudo-package directory.\n+    add_custom_command(\n+        TARGET openassetio-python-module POST_BUILD\n+        COMMAND\n+        \"${CMAKE_COMMAND}\" -E make_directory \"${CMAKE_CURRENT_BINARY_DIR}/openassetio\"\n+    )\n+\n+    if (WIN32)\n+        # On Windows, copy openassetio.dll dependency into the\n+        # pseudo-package directory, so that stubgen can import\n+        # _openassetio.\n+        # Note that this must be done _after_ the build, otherwise\n+        # Windows (Visual Studio CMake generator) complains that the\n+        # build directory already exists.\n+        add_custom_command(\n+            TARGET openassetio-python-module POST_BUILD\n+            COMMAND\n+            \"${CMAKE_COMMAND}\" -E copy_if_different\n+            \"$<TARGET_FILE:openassetio-core>\"\n+            \"${CMAKE_CURRENT_BINARY_DIR}/openassetio/\"\n+        )\n+    endif()\n+\n+    # Execute commands to generate .pyi stubs.\n+    add_custom_command(\n+        TARGET openassetio-python-module POST_BUILD\n+        COMMAND\n+        \"${CMAKE_COMMAND}\" -E echo \"Generating .pyi stubs with pybind11-stubgen...\"\n+        # Copy the Python extension module under the pseudo-package\n+        # directory.\n+        COMMAND\n+        \"${CMAKE_COMMAND}\" -E copy_if_different\n+        \"$<TARGET_FILE:openassetio-python-module>\"\n+        \"${CMAKE_CURRENT_BINARY_DIR}/openassetio/\"\n+        # Ensure we have a py.typed file at the root of our package, to\n+        # signal to IDEs that stubs exist.\n+        COMMAND\n+        \"${CMAKE_COMMAND}\" -E touch \"${CMAKE_CURRENT_BINARY_DIR}/openassetio/py.typed\"\n+        # Execute pybind11-stubgen, modifying PYTHONPATH so it can\n+        # locate the openassetio._openassetio module.\n+        COMMAND\n+        \"${CMAKE_COMMAND}\" -E env\n+        --modify \"PYTHONPATH=path_list_prepend:${CMAKE_CURRENT_BINARY_DIR}\"\n+        --\n+        \"${Python_EXECUTABLE}\" -m pybind11_stubgen\n+        # Fail the build and abort if any errors generating stubs.\n+        --exit-code\n+        # For whatever reason, stubgen fails to resolve PathType.\n+        --enum-class-locations PathType:openassetio._openassetio.utils\n+        -o \"${CMAKE_CURRENT_BINARY_DIR}\" openassetio._openassetio\n+        VERBATIM\n+    )\n+\n+    # Add stub files to the Python extension module installation\n+    # component, so that\n+    # `cmake --install --component openassetio-python-module` will\n+    # include the stubs.\n+    install(\n+        # pybind11-stubgen generates stubs for openassetio._openassetio\n+        # under an `openassetio/_openassetio` directory structure.\n+        DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/openassetio/_openassetio\"\n+        DESTINATION \"${_install_subdir}\"\n+        COMPONENT openassetio-python-module\n+    )\n+    install(\n+        FILES \"${CMAKE_CURRENT_BINARY_DIR}/openassetio/py.typed\"\n+        DESTINATION \"${_install_subdir}\"\n+        COMPONENT openassetio-python-module\n+    )\n+endif ()\ndiff --git a/src/openassetio-python/pyproject.toml b/src/openassetio-python/pyproject.toml\nindex 99df02886..c7375d8f8 100644\n--- a/src/openassetio-python/pyproject.toml\n+++ b/src/openassetio-python/pyproject.toml\n@@ -7,7 +7,9 @@ requires = [\n     # CMake 3.29 PyPI package requires importlib_metadata, which is not\n     # available in a fresh Python 3.7 environment.\n     \"cmake==3.28.3\",\n-    \"ninja>=1.10.2.4\"\n+    \"ninja>=1.10.2.4\",\n+    # For generating .pyi stub files.\n+    \"pybind11-stubgen==2.5.1\"\n ]\n build-backend = \"setuptools.build_meta\"\n \n@@ -141,9 +143,9 @@ target-version = [\"py39\"]\n [tool.cibuildwheel]\n test-requires = [\"pytest==7.4.4\", \"pytest-subtests==0.11.0\"]\n test-command = \"pytest {package}/tests/package\"\n+environment-pass = [\"PIP_VERBOSE\", \"OPENASSETIO_TEST_ENABLE_PYTHON_STUBGEN\"]\n \n [tool.cibuildwheel.linux]\n # Linux runs in a docker container, with the project at top level\n before-build = \"resources/build/bootstrap-cibuildwheel-manylinux-2014.sh\"\n environment = { CMAKE_TOOLCHAIN_FILE=\".conan/conan_paths.cmake\" }\n-environment-pass = [\"PIP_VERBOSE\"]\ndiff --git a/src/openassetio-python/setup.py b/src/openassetio-python/setup.py\nindex b147ede59..fb0d7592a 100644\n--- a/src/openassetio-python/setup.py\n+++ b/src/openassetio-python/setup.py\n@@ -96,6 +96,7 @@ def __cmake(self, args: List[str]):\n setup(\n     packages=find_packages(where=\"package\"),\n     package_dir={\"\": \"package\"},\n+    package_data={\"\": [\"py.typed\", \"_openassetio/*.pyi\"]},\n     ext_modules=[Extension(\"openassetio._openassetio\", sources=[])],\n     cmdclass={\"build_ext\": build_ext},\n     # See pyproject.toml for other metadata fields.\n", "instance_id": "OpenAssetIO__OpenAssetIO-1306", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in defining the goal of adding `.pyi` stub files to a Python package for enhancing IDE code completion and documentation for a C extension module. It specifies the \"What,\" \"Why,\" and provides detailed acceptance criteria for different installation scenarios (e.g., `pip install`, binary wheels, source install, and `cmake --install`). References to related issues (#747) and PEP 561 provide additional context. However, there are minor ambiguities: the problem statement does not explicitly describe how the `.pyi` files should be generated or integrated into the build process, nor does it mention specific challenges or edge cases related to different environments or IDEs. While the notes reference an investigation, the lack of direct detail in the statement itself leaves some room for interpretation, especially for someone unfamiliar with the codebase or `pybind11-stubgen`. Thus, it is rated as \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of this problem falls into the \"Medium\" range due to several factors. \n\n1. **Scope and Depth of Code Changes**: The changes span multiple files and systems, including CMake configuration (`CMakeLists.txt`), build scripts (`bootstrap-ubuntu-20.04.sh`), Docker setup (`Dockerfile`), Python build configuration (`pyproject.toml`, `setup.py`), and CI workflows (`build-wheels.yml`). This requires understanding and modifying build systems, Python packaging, and installation processes, which adds to the complexity. However, the changes are mostly additive (e.g., enabling stub generation, adding dependencies) and do not appear to impact core architecture or require deep refactoring.\n\n2. **Number of Technical Concepts**: Solving this requires familiarity with several concepts: Python C extensions, `pybind11` for binding C++ to Python, `pybind11-stubgen` for generating type hints, CMake for build configuration, Python packaging (PEP 561, `py.typed` files), and cross-platform build considerations (e.g., handling Windows-specific file copying). Additionally, knowledge of IDE behavior with stub files and installation mechanisms (`pip`, `cmake --install`) is necessary. While none of these concepts are extremely advanced individually, the combination and integration across tools increase the cognitive load.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes reveal considerations like ensuring `pybind11-stubgen` is available (with error checking), handling platform-specific issues (e.g., copying DLLs on Windows), and ensuring stubs are correctly placed for IDE detection. These are moderately complex but not overly challenging, as they are addressed with straightforward conditional logic and file operations in the build scripts.\n\n4. **Overall Complexity**: The task requires a solid understanding of build systems and Python packaging but does not demand deep architectural changes or advanced algorithmic work. It involves coordination across multiple parts of the codebase, but the impact is limited to build and installation processes rather than runtime behavior. The need to test across different installation methods and environments adds some complexity, but the provided code changes suggest a clear path to implementation.\n\nGiven these factors, a difficulty score of 0.55 reflects a medium-level challenge that requires understanding multiple concepts and making coordinated changes across several files, but it does not reach the \"Hard\" threshold due to the lack of deep architectural impact or highly complex logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support `hx-on:event`\nkeys in python can't have `-` or `:`. And while a user can get around this by doing:\r\n\r\n```python\r\nDiv(**{\"hx-on:click\": \"thing\"}\r\n```\r\n\r\nI don't really like that.\r\n\r\nThankfully htmx also supports having `-`s instead of all these colons ref [the doc](https://htmx.org/attributes/hx-on/)\r\n\r\n> Finally, in order to make this feature compatible with some templating languages (e.g. [JSX](https://react.dev/learn/writing-markup-with-jsx)) that do not like having a colon (:) in HTML attributes, you may use dashes in the place of colons for both the long form and the shorthand form:\r\n\r\n```html\r\n<!-- These two are equivalent -->\r\n<button hx-get=\"/info\" hx-on-htmx-before-request=\"alert('Making a request!')\">\r\n    Get Info!\r\n</button>\r\n\r\n<button hx-get=\"/info\" hx-on--before-request=\"alert('Making a request!')\">\r\n    Get Info!\r\n</button>\r\n```\r\n\r\nSo I think we should just keep to underscores and also allow double underscores. \r\n\r\n\r\n```python\r\nDiv(hx_on_click=\"js string\")\r\n\r\nDiv(hx_on_htmx_before_request=\"alert('Making a request!')\")\r\n\r\nDiv(hx_on__before_request=\"alert('Making a request!')\")\r\n```\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 5e54a12..0f8f46d 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -9,6 +9,10 @@\n \n ## Latest Changes\n \n+### Internal\n+\n+* Document how to add attributes with symbols like dash and period.\n+\n ## Version 4.0.0\n \n ### Breaking\ndiff --git a/README.md b/README.md\nindex c31682a..0a8f493 100644\n--- a/README.md\n+++ b/README.md\n@@ -73,6 +73,36 @@ base.dump()\n # '<html><body><menu><ul><li>main</li></ul></menu><header>my header</header><div><div>Some content</div></div></body></html>'\n ```\n \n+## Symbol attributes\n+\n+Attributes with dashes, periods etc can be added by spreading a dictionary\n+\n+```python\n+# Alpine.js\n+Button(**{\"@click\": \"open = ! open\"}).dump()\n+# <button @click='open'></button>\n+\n+# Datastar\n+Div(**{\"x-transition.duration_500ms\": \"$show\"}).dump()\n+# <div x-transition.duration_500ms='$show'></div>\n+```\n+\n+Note: The </> HTMX attributes get special treatment. [The documentation](https://htmx.org/attributes/hx-on/) specifies that all hx attributes can be written with all dashes. Because of that Hypermedia lets users write hx attributes with underscores and Hypermedia changes them to dashes for you.\n+\n+```python\n+\n+Div(hx_on_click='alert(\"Making a request!\")')\n+# <div hx-on-click='alert(\"Making a request!\")'></div>\n+# Which is equivalent to:\n+# <div hx-on:click='alert(\"Making a request!\"'></div>\n+\n+Div(hx_on_htmx_before_request='alert(\"Making a request!\")')\n+# <div hx-on-htmx-before-request='alert(\"Making a request!\")'></div>\n+\n+# shorthand version of above statement\n+Div(hx_on__before_request='alert(\"Making a request!\")')\n+# <div hx-on--before-request='alert(\"Making a request!\")'></div>\n+```\n \n # HTMX\n \n", "instance_id": "thomasborgen__hypermedia-21", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the goal of supporting `hx-on:event` attributes in a Python library, likely for generating HTML with HTMX attributes. It explains the issue with Python not allowing certain characters like `-` or `:` in dictionary keys and proposes a solution using underscores and double underscores to map to HTMX's dash-based syntax. The inclusion of examples and references to HTMX documentation helps in understanding the intent. However, there are minor ambiguities: the statement does not explicitly define the full scope of supported syntax (e.g., are all `hx-on` events covered, or just specific ones?), and it lacks clarity on whether this change affects other attribute types or libraries beyond HTMX. Additionally, edge cases or potential conflicts with existing attribute naming conventions are not mentioned. Overall, while the problem is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes appears limited to documentation updates, as seen in the provided diff, which modifies `README.md` and `CHANGELOG.md` to explain the feature. There are no actual code changes shown for implementing the logic to transform underscores to dashes in attribute names, suggesting that either the implementation is already in place or not included in the diff. If implementation is required, it would likely involve a simple string replacement or mapping logic in a single module or function, which is straightforward in Python. Second, the technical concepts involved are basic: understanding Python string manipulation and dictionary handling, along with familiarity with HTMX attribute syntax. No advanced algorithms, design patterns, or deep architectural changes are implied. Third, the problem does not mention complex edge cases or error handling beyond the basic syntax transformation, though potential issues like naming conflicts (e.g., existing underscores in attribute names) might need consideration. Finally, the impact on the codebase seems minimal, likely confined to a specific utility or rendering function. Overall, this is a relatively simple feature addition or documentation update with low complexity, warranting a score of 0.30.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CloudWatch Log Group Monitoring uses SampleCount statistic when it should use Sum\n### Version\n\nv7.13.1\n\n### Steps and/or minimal code example to reproduce\n\n```typescript\r\nmonitoringFacade.monitorLog({\r\n  logGroupName: \"<someLogGroup>\",\r\n  addMinIncomingLogsAlarm: {\r\n    Warning: {\r\n      minCount: 1\r\n    }\r\n  }\r\n});\r\n```\r\nThis code would set up alarms and dashboard widgets that utilize `IncomingLogEvents` with the `SampleCount` statistic, which presumably represents the number of times that CloudWatch emitted the metric, which is not useful for alarming on the min/max number of actual log events that were written within a period of time. This should instead use the `Sum` statistic.\n\n### Expected behavior\n\n`addMinIncomingLogsAlarm`/`addMaxIncomingLogsAlarm` should create alarms/widgets based on the total number of log events written in a period of time.\n\n### Actual behavior\n\n`addMinIncomingLogsAlarm`/`addMaxIncomingLogsAlarm` create alarms/widgets based on the number _of samples_ of `IncomingLogEvents` metrics emitted in a period of time.\n\n### Other details\n\n_No response_\n", "patch": "diff --git a/lib/monitoring/aws-cloudwatch/CloudWatchLogsMetricFactory.ts b/lib/monitoring/aws-cloudwatch/CloudWatchLogsMetricFactory.ts\nindex 2a141a7e..819ff583 100644\n--- a/lib/monitoring/aws-cloudwatch/CloudWatchLogsMetricFactory.ts\n+++ b/lib/monitoring/aws-cloudwatch/CloudWatchLogsMetricFactory.ts\n@@ -34,7 +34,7 @@ export class CloudWatchLogsMetricFactory extends BaseMetricFactory<CloudWatchLog\n   metricIncomingLogEvents() {\n     return this.metricFactory.createMetric(\n       \"IncomingLogEvents\",\n-      MetricStatistic.N,\n+      MetricStatistic.SUM,\n       \"Logs\",\n       this.dimensionsMap,\n       undefined,\n", "instance_id": "cdklabs__cdk-monitoring-constructs-589", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the current implementation of CloudWatch Log Group Monitoring uses the `SampleCount` statistic instead of the desired `Sum` statistic for the `IncomingLogEvents` metric. It provides a minimal code example to reproduce the issue, specifies the expected behavior (using total log events), and contrasts it with the actual behavior (using the number of samples). However, there are minor ambiguities and missing details. For instance, the problem does not explicitly mention any specific edge cases or constraints related to using `Sum` over `SampleCount`, nor does it discuss potential side effects or dependencies in the system that might be affected by this change. Additionally, while the intent is clear, there is no discussion of whether this change aligns with AWS CloudWatch's intended usage of metrics or if there are any performance implications. Overall, the statement is valid and clear but lacks some depth in addressing potential complexities or edge scenarios.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward modification to a single line of code in a single file (`CloudWatchLogsMetricFactory.ts`). The change is limited to updating the statistic type from `MetricStatistic.N` (presumably representing `SampleCount`) to `MetricStatistic.SUM`. This requires minimal understanding of the codebase\u2014just the specific metric configuration logic in the affected class. The scope of the change is extremely narrow, with no apparent impact on the broader system architecture or interactions between modules, based on the provided diff. There are no complex technical concepts involved beyond basic familiarity with AWS CloudWatch metrics and the library's metric factory pattern, which are relatively simple for a senior engineer. Additionally, the problem statement and code changes do not indicate any specific edge cases or error handling requirements that need to be addressed as part of this fix. The task is essentially a small configuration adjustment, akin to fixing a typo or changing a constant, with no significant logic or design considerations. Therefore, I assign a difficulty score of 0.15 to reflect the minimal effort and expertise required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "BREAKING CHANGE introduced by dependency `lxml` 5.2.0\nHello there,\r\n\r\nThe `lxml` dependency recently introduced a breaking change wit new release 5.2.0 (https://github.com/lxml/lxml/releases/tag/lxml-5.2.0)\r\n\r\n```\r\n* LP#1958539: The ``lxml.html.clean`` implementation suffered from several (only if used)\r\n  security issues in the past and was now extracted into a separate library:\r\n\r\n  https://github.com/fedora-python/lxml_html_clean\r\n\r\n  Projects that use lxml without \"lxml.html.clean\" will not notice any difference,\r\n  except that they won't have potentially vulnerable code installed.\r\n  The module is available as an \"extra\" setuptools dependency \"lxml[html_clean]\",\r\n  so that Projects that need \"lxml.html.clean\" will need to switch their requirements\r\n  from \"lxml\" to \"lxml[html_clean]\", or install the new library themselves.\r\n```\r\n\r\nAs mentionned in the release note, the quick fix would be to install `lxml[html_clean]`.\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex bb45f67..4564272 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -35,7 +35,7 @@ dynamic = [\n ]\n dependencies = [\n   \"beautifulsoup4\",\n-  \"lxml>=4.9.1\",\n+  \"lxml[html_clean]>=5.2.0\",\n ]\n [project.urls]\n Homepage = \"https://github.com/matthiask/html-sanitizer/\"\ndiff --git a/tox.ini b/tox.ini\nindex ad5370a..a1ab64d 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -1,7 +1,7 @@\n [testenv]\n deps =\n     wheel\n-    lxml\n+    lxml[html_clean]\n     beautifulsoup4\n     coverage\n changedir = {toxinidir}\n", "instance_id": "matthiask__html-sanitizer-39", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a breaking change introduced by the `lxml` library version 5.2.0, where the `lxml.html.clean` module was extracted into a separate library. It provides a direct reference to the release notes and suggests a quick fix by updating the dependency to `lxml[html_clean]`. However, there are minor ambiguities and missing details. For instance, it does not explicitly mention the impact on the project (e.g., whether the project actually uses `lxml.html.clean` or if this change is purely precautionary). Additionally, there are no examples or test cases provided to validate the fix, and potential side effects of adopting the new dependency are not discussed. Overall, the goal and required action are clear, but some minor context or validation steps are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward update to dependency specifications in two configuration files (`pyproject.toml` and `tox.ini`). The scope of the code changes is minimal, affecting only a single line in each file with no impact on the broader codebase or system architecture. No deep understanding of the codebase, complex logic, or technical concepts beyond basic dependency management in Python is required. There are no edge cases or error handling considerations mentioned in the problem statement, and the changes do not introduce any new logic that might require such handling. This task is essentially a simple configuration update, fitting into the 0.0-0.2 range (Very Easy), and I assign it a score of 0.1 to reflect the minimal effort and expertise needed.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deviations from Berkeley TestFloat\nSome deviations and bugs in floating-point arithmetic were found when testing against Berkeley TestFloat:\r\n* [x] (#408) As already known, the sign of zero is incorrect as it doesn't consider the quantization mode. \r\n* [x] (#407) The result sometimes becomes infinity instead of the maximum normal number for certain quantization modes. This makes sense when reading the definitions again, e.g. for TO_POS (from the 754 standard): \"the result shall be the format\u2019s floating-point number (possibly +\u221e) closest to and no less than the infinitely precise result\". The result should therefore never become negative infinity for TO_POS.\r\n* [x] (#428) f16_mul sometimes fails for directed quantization modes, e.g. the following for TO_POS:\r\n```Python\r\nAPyFloat(1, 1, 1023, 5, 10) * APyFloat(1, 26, 80, 5, 10)\r\n# Becomes APyFloat(sign=0, exp=13, man=79, exp_bits=5, man_bits=10)\r\n# and not APyFloat(sign=0, exp=13, man=80, exp_bits=5, man_bits=10)\r\n```\r\n* [x] (#411) f32_mul sometimes results in a subnormal number instead of zero, however this isn't seen for f64_mul or f16_mul):\r\n```Python\r\nAPyFloat(0, 53, 32895, 8, 23) * APyFloat(1, 1, 1999049, 8, 23)\r\n# Becomes APyFloat(sign=1, exp=0, man=10184, exp_bits=8, man_bits=23)\r\n# and not APyFloat(sign=1, exp=0, man=0, exp_bits=8, man_bits=23)\r\n```\r\n* [x] Added bullet point. The mantissa is sometimes wrong when casting from a larger format to a smaller, similar to the previous bullet. See comment below.\r\n\r\nThese were found from running\r\n```cmd\r\npython run_berkeley_cases.py -op all -qm all -s 1 -l 1\r\n```\r\nbut dedicated test cases should of course be made while fixing these.\n", "patch": "diff --git a/src/apyfloat.cc b/src/apyfloat.cc\nindex b142b35bc..5ef5eed13 100644\n--- a/src/apyfloat.cc\n+++ b/src/apyfloat.cc\n@@ -256,21 +256,22 @@ void APyFloat::cast_mantissa(std::uint8_t new_man_bits, QuantizationMode quantiz\n     // Check if only zeros should be added\n     if (man_bits_delta <= 0) {\n         if (exp >= max_exp) {\n-            exp = max_exp;\n-            man = 0;\n+            if (do_infinity(quantization, sign)) {\n+                exp = max_exp;\n+                man = 0;\n+            } else {\n+                exp = max_exp - 1;\n+                man = leading_one() - 1;\n+            }\n         } else {\n             man <<= -man_bits_delta;\n         }\n         return;\n     }\n \n-    quantize_mantissa(man, exp, man_bits_delta, sign, leading_one(), quantization);\n-\n-    // Check for overflow\n-    if (exp >= max_exp) {\n-        exp = max_exp;\n-        man = 0;\n-    }\n+    quantize_mantissa(\n+        man, exp, max_exp, man_bits_delta, sign, leading_one(), quantization\n+    );\n }\n \n // Simplified version of cast_mantissa when it is known that new_man_bits is shorter\n@@ -282,13 +283,9 @@ void APyFloat::cast_mantissa_shorter(\n     auto man_bits_delta = man_bits - new_man_bits;\n     man_bits = new_man_bits;\n     const auto max_exp = max_exponent();\n-    quantize_mantissa(man, exp, man_bits_delta, sign, leading_one(), quantization);\n-\n-    // Check for overflow\n-    if (exp >= max_exp) {\n-        exp = max_exp;\n-        man = 0;\n-    }\n+    quantize_mantissa(\n+        man, exp, max_exp, man_bits_delta, sign, leading_one(), quantization\n+    );\n }\n \n // Simplified version of cast_mantissa with exp = 0\n@@ -305,7 +302,10 @@ void APyFloat::cast_mantissa_subnormal(\n         return;\n     }\n \n-    quantize_mantissa(man, exp, man_bits_delta, sign, leading_one(), quantization);\n+    // The overflow check with max_exponent() is not needed here, but send it anyway\n+    quantize_mantissa(\n+        man, exp, max_exponent(), man_bits_delta, sign, leading_one(), quantization\n+    );\n }\n \n APyFloat APyFloat::_cast_to_double() const\n@@ -464,7 +464,13 @@ APyFloat APyFloat::cast_from_double(\n     }\n \n     if (new_exp >= res.max_exponent()) {\n-        return res.construct_inf();\n+        if (do_infinity(get_float_quantization_mode(), res.sign)) {\n+            new_exp = res.max_exponent();\n+            new_man = 0;\n+        } else {\n+            new_exp = res.max_exponent() - 1;\n+            new_man = res.man_mask();\n+        }\n     }\n \n     res.man = new_man;\n@@ -858,7 +864,14 @@ APyFloat APyFloat::operator+(const APyFloat& rhs) const\n \n     // Check for overflow\n     if (new_exp >= res.max_exponent()) {\n-        return res.construct_inf();\n+        if (do_infinity(quantization, res.sign)) {\n+            res.exp = res.max_exponent();\n+            res.man = 0;\n+        } else {\n+            res.exp = res.max_exponent() - 1;\n+            res.man = res.man_mask();\n+        }\n+        return res;\n     }\n \n     // Remove leading one\n@@ -982,8 +995,15 @@ APyFloat& APyFloat::operator+=(const APyFloat& rhs)\n     }\n \n     // Check for overflow\n-    if (exp >= max_exponent()) {\n-        set_to_inf();\n+    const exp_t max_exp = max_exponent();\n+    if (exp >= max_exp) {\n+        if (do_infinity(quantization, sign)) {\n+            exp = max_exp;\n+            man = 0;\n+        } else {\n+            exp = max_exp - 1;\n+            man = leading_one() - 1;\n+        }\n         return *this;\n     }\n \n@@ -1069,18 +1089,17 @@ APyFloat APyFloat::operator*(const APyFloat& y) const\n         exp_t res_exp = static_cast<exp_t>(tmp_exp);\n         new_man &= (two - 1);\n         quantize_mantissa(\n-            new_man, res_exp, man_bits_delta, sign, two_res, quantization\n+            new_man,\n+            res_exp,\n+            res.max_exponent(),\n+            man_bits_delta,\n+            sign,\n+            two_res,\n+            quantization\n         );\n \n-        // Check for overflow\n-        const auto max_exp = res.max_exponent();\n-        if (res_exp >= max_exp) {\n-            res.exp = max_exp;\n-            res.man = 0;\n-        } else {\n-            res.man = new_man;\n-            res.exp = res_exp;\n-        }\n+        res.man = new_man;\n+        res.exp = res_exp;\n         return res;\n     } else {\n         // Normalize both inputs\n@@ -1123,7 +1142,14 @@ APyFloat APyFloat::operator*(const APyFloat& y) const\n         }\n \n         if (new_exp >= res.max_exponent()) {\n-            return res.construct_inf();\n+            if (do_infinity(quantization, res.sign)) {\n+                res.exp = res.max_exponent();\n+                res.man = 0;\n+            } else {\n+                res.exp = res.max_exponent() - 1;\n+                res.man = res.man_mask();\n+            }\n+            return res;\n         }\n \n         if (apy_res.positive_greater_than_equal_pow2(0)) { // Remove leading one\n@@ -1200,7 +1226,14 @@ APyFloat APyFloat::operator/(const APyFloat& y) const\n \n     // Check limits\n     if (new_exp >= res.max_exponent()) {\n-        return res.construct_inf();\n+        if (do_infinity(quantization, res.sign)) {\n+            res.exp = res.max_exponent();\n+            res.man = 0;\n+        } else {\n+            res.exp = res.max_exponent() - 1;\n+            res.man = res.man_mask();\n+        }\n+        return res;\n     }\n \n     // Handle subnormal case\ndiff --git a/src/apyfloat_util.h b/src/apyfloat_util.h\nindex 8c0c6414c..a8bfac408 100644\n--- a/src/apyfloat_util.h\n+++ b/src/apyfloat_util.h\n@@ -12,10 +12,28 @@ static constexpr std::size_t _MAN_T_SIZE_BITS = 8 * _MAN_T_SIZE_BYTES;\n static constexpr std::size_t _EXP_T_SIZE_BYTES = sizeof(exp_t);\n static constexpr std::size_t _EXP_T_SIZE_BITS = 8 * _EXP_T_SIZE_BYTES;\n \n+//! Check if one should saturate to infinity or maximum normal number\n+bool APY_INLINE do_infinity(QuantizationMode mode, bool sign)\n+{\n+    switch (mode) {\n+    case QuantizationMode::TRN_ZERO:     // TO_ZERO\n+    case QuantizationMode::JAM:          // JAM\n+    case QuantizationMode::JAM_UNBIASED: // JAM_UNBIASED\n+        return false;\n+    case QuantizationMode::TRN: // TO_NEG\n+        return sign;\n+    case QuantizationMode::TRN_INF: // TO_POS\n+        return !sign;\n+    default:\n+        return true;\n+    }\n+}\n+\n //! Quantize mantissa\n void APY_INLINE quantize_mantissa(\n     man_t& man,\n     exp_t& exp,\n+    exp_t max_exp,\n     std::uint8_t bits_to_quantize,\n     bool sign,\n     man_t man_msb_constant,\n@@ -99,18 +117,31 @@ void APY_INLINE quantize_mantissa(\n             \"unknown (did you pass `int` as `QuantizationMode`?)\"\n         );\n     }\n+\n     man = res_man;\n     man += B;\n     if (man & man_msb_constant) {\n         ++exp;\n         man = 0;\n     }\n+\n+    // Check for overflow. This must always be checked since other methods depend on it.\n+    if (exp >= max_exp) {\n+        if (do_infinity(quantization, sign)) {\n+            exp = max_exp;\n+            man = 0;\n+        } else {\n+            exp = max_exp - 1;\n+            man = man_msb_constant - 1;\n+        }\n+    }\n }\n \n //! Quantize mantissa\n void APY_INLINE quantize_mantissa(\n     man_t& man,\n     exp_t& exp,\n+    exp_t max_exp,\n     std::uint8_t bits_to_quantize,\n     bool sign,\n     man_t man_msb_constant,\n@@ -120,6 +151,7 @@ void APY_INLINE quantize_mantissa(\n     quantize_mantissa(\n         man,\n         exp,\n+        max_exp,\n         bits_to_quantize,\n         sign,\n         man_msb_constant,\ndiff --git a/src/apyfloatarray.cc b/src/apyfloatarray.cc\nindex e96e37d0a..bda605b6e 100644\n--- a/src/apyfloatarray.cc\n+++ b/src/apyfloatarray.cc\n@@ -218,14 +218,17 @@ APyFloatArray APyFloatArray::operator+(const APyFloatArray& rhs) const\n             new_man &= man_mask;\n \n             quantize_mantissa(\n-                new_man, new_exp, 4, x.sign, final_res_leading_one, 3, 7, quantization\n+                new_man,\n+                new_exp,\n+                res_max_exponent,\n+                4,\n+                x.sign,\n+                final_res_leading_one,\n+                3,\n+                7,\n+                quantization\n             );\n \n-            // Check for overflow\n-            if (new_exp >= res_max_exponent) {\n-                new_exp = res_max_exponent;\n-                new_man = 0;\n-            }\n             res.data[i]\n                 = { x.sign, static_cast<exp_t>(new_exp), static_cast<man_t>(new_man) };\n         }\n@@ -405,14 +408,17 @@ APyFloatArray APyFloatArray::operator+(const APyFloat& rhs) const\n             new_man &= man_mask;\n \n             quantize_mantissa(\n-                new_man, new_exp, 4, res_sign, final_res_leading_one, 3, 7, quantization\n+                new_man,\n+                new_exp,\n+                res_max_exponent,\n+                4,\n+                res_sign,\n+                final_res_leading_one,\n+                3,\n+                7,\n+                quantization\n             );\n \n-            // Check for overflow\n-            if (new_exp >= res_max_exponent) {\n-                new_exp = res_max_exponent;\n-                new_man = 0;\n-            }\n             res.data[i] = { res_sign,\n                             static_cast<exp_t>(new_exp),\n                             static_cast<man_t>(new_man) };\n@@ -615,6 +621,7 @@ void APyFloatArray::hadamard_multiplication(\n             quantize_mantissa(\n                 new_man,\n                 new_exp,\n+                res_max_exponent,\n                 man_bits_delta,\n                 res_sign,\n                 two_res,\n@@ -623,11 +630,6 @@ void APyFloatArray::hadamard_multiplication(\n                 quantization\n             );\n \n-            // Check for overflow\n-            if (new_exp >= res_max_exponent) {\n-                new_exp = res_max_exponent;\n-                new_man = 0;\n-            }\n             res.data[i] = { res_sign,\n                             static_cast<exp_t>(new_exp),\n                             static_cast<man_t>(new_man) };\n@@ -811,6 +813,7 @@ APyFloatArray APyFloatArray::operator*(const APyFloat& rhs) const\n             quantize_mantissa(\n                 new_man,\n                 new_exp,\n+                res_max_exponent,\n                 man_bits_delta,\n                 res_sign,\n                 two_res,\n@@ -819,11 +822,6 @@ APyFloatArray APyFloatArray::operator*(const APyFloat& rhs) const\n                 quantization\n             );\n \n-            // Check for overflow\n-            if (new_exp >= res_max_exponent) {\n-                new_exp = res_max_exponent;\n-                new_man = 0;\n-            }\n             res.data[i] = { res_sign,\n                             static_cast<exp_t>(new_exp),\n                             static_cast<man_t>(new_man) };\n@@ -1457,14 +1455,19 @@ APyFloat APyFloatArray::vector_sum(const QuantizationMode quantization) const\n             sum_man &= man_mask;\n \n             quantize_mantissa(\n-                sum_man, sum_exp, 4, sum_sign, final_res_leading_one, 3, 7, quantization\n+                sum_man,\n+                sum_exp,\n+                res_max_exponent,\n+                4,\n+                sum_sign,\n+                final_res_leading_one,\n+                3,\n+                7,\n+                quantization\n             );\n \n             // Check for overflow\n-            if (sum_exp >= res_max_exponent) {\n-                // Inf\n-                sum_exp = res_max_exponent;\n-                sum_man = 0;\n+            if (sum_exp == res_max_exponent) {\n                 sum_is_max_exponent = true;\n             }\n         }\n", "instance_id": "apytypes__apytypes-407", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issues related to deviations from Berkeley TestFloat in floating-point arithmetic. It lists specific bugs and deviations with examples (e.g., incorrect sign of zero, infinity instead of maximum normal number, and specific test cases for f16_mul and f32_mul). The inclusion of Python code snippets and command-line instructions for reproducing the issues adds clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully specify the expected behavior for all quantization modes or provide a comprehensive list of edge cases to handle. Additionally, the description of the mantissa issue during casting is vague (\"similar to the previous bullet\") and lacks a concrete example or detailed explanation. While the intent is understandable, these gaps prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files (apyfloat.cc, apyfloat_util.h, apyfloatarray.cc) and affecting core functionality related to floating-point arithmetic operations (addition, multiplication, division, casting). The changes involve modifications to quantization logic and overflow handling, which are central to the correctness of the system. Second, the problem requires a deep understanding of technical concepts, including floating-point representation (sign, exponent, mantissa), quantization modes, and IEEE 754 standards. Implementing the fixes necessitates precise handling of numerical edge cases (e.g., infinity vs. maximum normal number, subnormal numbers) and understanding the behavior of different quantization modes (TO_POS, TO_NEG, etc.). Third, the code changes show a non-trivial amount of logic modification, such as introducing the `do_infinity` function to handle saturation behavior and updating multiple call sites of `quantize_mantissa` to include overflow checks. While the problem does not appear to require a complete architectural overhaul or advanced domain-specific knowledge beyond floating-point arithmetic, it does demand careful attention to detail and a solid grasp of the codebase's internal workings. A score of 0.65 reflects the complexity of the concepts involved and the need for meticulous error handling across various operations, placing it on the lower end of the \"Hard\" range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FDS-1725] Missing `entityId` handling testing\n**Description:**\r\nThis PR follows up on #1456, where the `filenameExists` validation rule was implemented. This rule handles cases where a file path present in a manifest does not exist in a dataset and cases where the entityId provided in a manifest row does not match with its corresponding file path. \r\n\r\nThis PR adds additional error handling for situations including:\r\n- The entityId field in a manifest row is empty\r\n- The entityId provided in a manifest row does not exist in the dataset\r\n\r\nPreviously, an outer join was used in the validation rule followed by a line that handled cases where there were more rows in the dataset than the manifest. The outer join implementation did not always maintain the order of manifest rows resulting in incorrect errors on manifest rows. I changed this outer join to a left join on the manifest to handle both issues.\r\n\r\nThe row number indexing for `filename_validation` is also updated to match the other validation rules for consistency.\r\n\r\nI have also updated `generate_filename_error` to handle the new error cases.\r\n\r\n**Testing:**\r\n- New integration test cases for `filename_validation` have been added as needed.\r\n- Unit tests for `filename_validation` and `generate_filename_error` have been added\n", "patch": "diff --git a/.github/workflows/docker.yml b/.github/workflows/docker.yml\nindex 58f903f76..12b355dc0 100644\n--- a/.github/workflows/docker.yml\n+++ b/.github/workflows/docker.yml\n@@ -17,16 +17,16 @@ jobs:\n     steps:\n       -\n         name: Checkout\n-        uses: actions/checkout@v3\n+        uses: actions/checkout@v4\n       -\n         name: Set up QEMU\n-        uses: docker/setup-qemu-action@v2\n+        uses: docker/setup-qemu-action@v3\n       -\n         name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v2\n+        uses: docker/setup-buildx-action@v3\n       -\n         name: Login to DockerHub\n-        uses: docker/login-action@v2\n+        uses: docker/login-action@v3\n         with:\n           username: schematicbot\n           password: ${{ secrets.DOCKER_HUB_TOKEN }}\n@@ -36,7 +36,7 @@ jobs:\n         run: echo \"::set-output name=sha_short::$(git rev-parse --short HEAD)\"\n       -\n         name: Build and push (tagged release)\n-        uses: docker/build-push-action@v3\n+        uses: docker/build-push-action@v6\n         if: ${{ github.event_name == 'push' }}\n         with:\n           platforms: linux/amd64,linux/arm64\n@@ -48,7 +48,7 @@ jobs:\n             ${{ env.DOCKER_ORG }}/${{ env.DOCKER_REPO }}:commit-${{ steps.vars.outputs.sha_short }}\n       -\n         name: Build and push (manual release)\n-        uses: docker/build-push-action@v3\n+        uses: docker/build-push-action@v6\n         if: ${{ github.event_name == 'workflow_dispatch' }}\n         with:\n           platforms: linux/amd64,linux/arm64\ndiff --git a/.github/workflows/docker_build.yml b/.github/workflows/docker_build.yml\nindex 50596b7db..f1beb3489 100644\n--- a/.github/workflows/docker_build.yml\n+++ b/.github/workflows/docker_build.yml\n@@ -23,13 +23,13 @@ jobs:\n \n     steps:\n       - name: Checkout repository\n-        uses: actions/checkout@v2\n+        uses: actions/checkout@v4\n           \n       - name: Set env variable for version tag\n         run: echo \"RELEASE_VERSION=${GITHUB_REF#refs/*/}\" >> $GITHUB_ENV\n \n       - name: Log in to the Container registry\n-        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n+        uses: docker/login-action@v3\n         with:\n           registry: ${{ env.REGISTRY }}\n           username: ${{ github.actor }}\n@@ -37,7 +37,7 @@ jobs:\n \n       - name: Extract metadata (tags, labels) for Docker\n         id: meta\n-        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n+        uses: docker/metadata-action@v5\n         with:\n           images: ${{ env.IMAGE_PATH }}\n           tags: |\n@@ -46,7 +46,7 @@ jobs:\n             type=semver,pattern={{raw}}\n \n       - name: Build and push Docker image\n-        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n+        uses: docker/build-push-action@v6\n         with:\n           file: schematic_api/Dockerfile\n           push: true\ndiff --git a/.github/workflows/pdoc.yml b/.github/workflows/pdoc.yml\nindex 51924c4b4..fdf0be220 100644\n--- a/.github/workflows/pdoc.yml\n+++ b/.github/workflows/pdoc.yml\n@@ -37,10 +37,10 @@ jobs:\n       #       check-out repo and set-up python     \n       #----------------------------------------------\n       - name: Check out repository\n-        uses: actions/checkout@v3\n+        uses: actions/checkout@v4\n \n       - name: Set up Python ${{ matrix.python-version }}\n-        uses: actions/setup-python@v3\n+        uses: actions/setup-python@v5\n         with:\n           python-version: ${{ matrix.python-version }}\n       \n@@ -59,10 +59,10 @@ jobs:\n       #----------------------------------------------\n       - name: Load cached venv\n         id: cached-poetry-dependencies\n-        uses: actions/cache@v3\n+        uses: actions/cache@v4\n         with:\n           path: .venv\n-          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}\n+          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}-${{ matrix.python-version }}\n \n       #----------------------------------------------\n       # install dependencies and root project\n@@ -76,7 +76,7 @@ jobs:\n       - run: poetry show pdoc\n       - run: poetry run pdoc --docformat google --mermaid -o docs/schematic schematic/manifest schematic/models schematic/schemas schematic/store schematic/utils schematic/visualization\n \n-      - uses: actions/upload-pages-artifact@v1\n+      - uses: actions/upload-pages-artifact@v3\n         with:\n           path: docs/schematic\n \n@@ -93,4 +93,4 @@ jobs:\n       url: ${{ steps.deployment.outputs.page_url }}\n     steps:\n       - id: deployment\n-        uses: actions/deploy-pages@v1\n+        uses: actions/deploy-pages@v4\ndiff --git a/.github/workflows/publish.yml b/.github/workflows/publish.yml\nindex eb2ebafcb..c3225661e 100644\n--- a/.github/workflows/publish.yml\n+++ b/.github/workflows/publish.yml\n@@ -16,10 +16,10 @@ jobs:\n       #       check-out repo and set-up python     \n       #----------------------------------------------\n       - name: Check out repository\n-        uses: actions/checkout@v2\n+        uses: actions/checkout@v4\n \n       - name: Set up Python ${{ matrix.python-version }}\n-        uses: actions/setup-python@v2\n+        uses: actions/setup-python@v5\n         with:\n           python-version: ${{ matrix.python-version }}\n \n@@ -38,10 +38,10 @@ jobs:\n       #----------------------------------------------\n       - name: Load cached venv\n         id: cached-poetry-dependencies\n-        uses: actions/cache@v2\n+        uses: actions/cache@v4\n         with:\n           path: .venv\n-          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}\n+          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}-${{ matrix.python-version }}\n \n       #----------------------------------------------\n       # install dependencies and root project\ndiff --git a/.github/workflows/scan_repo.yml b/.github/workflows/scan_repo.yml\nindex 6a93beee7..56c7ac35a 100644\n--- a/.github/workflows/scan_repo.yml\n+++ b/.github/workflows/scan_repo.yml\n@@ -14,7 +14,7 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n       - name: Checkout code\n-        uses: actions/checkout@v3\n+        uses: actions/checkout@v4\n \n       - name: Run Trivy vulnerability scanner in repo mode\n         uses: aquasecurity/trivy-action@master\n@@ -32,4 +32,4 @@ jobs:\n         uses: github/codeql-action/upload-sarif@v3\n         with:\n           sarif_file: 'trivy-results.sarif'\n-          category: Git Repository\n\\ No newline at end of file\n+          category: Git Repository\n", "instance_id": "Sage-Bionetworks__schematic-1502", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the intent and scope of the changes. It outlines the purpose of the PR as adding error handling for specific cases related to `entityId` in a manifest row and mentions the shift from an outer join to a left join to address ordering issues. It also references updates to functions like `generate_filename_error` and the addition of test cases. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the input/output formats for the validation rules or provide examples of the manifest rows and dataset structures. Additionally, the specific nature of the \"incorrect errors on manifest rows\" caused by the outer join is not detailed, which could lead to some uncertainty during implementation or review. Edge cases are mentioned (e.g., empty `entityId` or non-existent `entityId`), but the expected behavior or error messages for these cases are not specified. Overall, while the goal and general approach are understandable, these minor gaps in detail reduce the clarity score to \"Mostly Clear.\"", "difficulty_explanation": "The difficulty of this problem is rated as Easy (0.25) based on the provided factors. First, regarding the scope and depth of code changes, the provided diff shows updates to GitHub Actions workflows, which are unrelated to the core problem described (handling `entityId` validation and join logic). Assuming the relevant code changes for the validation logic and testing are not shown in the diff, the problem statement suggests modifications are likely localized to specific functions like `filename_validation` and `generate_filename_error`, with additional test cases. This implies a limited scope, likely within a single module or a few files, without significant architectural impact. Second, the number of technical concepts involved appears minimal\u2014basic database join operations (outer to left join), error handling, and testing in a Python-based system (given the context of GitHub Actions and Poetry). These concepts are straightforward for a developer with moderate experience. Third, while edge cases are mentioned (empty or non-existent `entityId`), they do not seem particularly complex to handle, likely requiring simple conditional checks and error message generation. Finally, the problem does not suggest deep architectural understanding or complex performance considerations. Overall, this task involves understanding some code logic and making relatively simple modifications, fitting within the Easy range (0.2-0.4), with a score of 0.25 reflecting the need for minor design consideration around join logic and error handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Documentation improvement\nThis updates 2 items of the PyAT documentation:\r\n\r\n1. Since PyAT is now distributed on [conda-forge](https://conda-forge.org), the installation instructions are updated mention it,\r\n2. @simoneliuzzo reported in #728 that the documentation of get_optics was incomplete. This is now improved.\n", "patch": "diff --git a/pyat/at.c b/pyat/at.c\nindex 735d82f54..af18959cc 100644\n--- a/pyat/at.c\n+++ b/pyat/at.c\n@@ -525,9 +525,11 @@ static PyObject *at_atpass(PyObject *self, PyObject *args, PyObject *kwargs) {\n             Py_DECREF(PyPassMethod);\n             if (!LibraryListPtr) return print_error(elem_index, rout);  /* No trackFunction for the given PassMethod: RuntimeError */\n             pylength = PyObject_GetAttrString(el, \"Length\");\n-            length = PyFloat_AsDouble(pylength);\n-            Py_XDECREF(pylength);\n-            if (PyErr_Occurred()) {\n+            if (pylength) {\n+                length = PyFloat_AsDouble(pylength);\n+                Py_XDECREF(pylength);\n+            }\n+            else {\n                 length = 0.0;\n                 PyErr_Clear();\n             }\ndiff --git a/pyat/at/tracking/atpass.pyi b/pyat/at/tracking/atpass.pyi\nindex 94b9e9ea0..f51cc2f92 100644\n--- a/pyat/at/tracking/atpass.pyi\n+++ b/pyat/at/tracking/atpass.pyi\n@@ -2,10 +2,10 @@\n \n import numpy as np\n from typing import List, Optional\n-from at.lattice import Element, Particle\n+from at.lattice import Element, Particle, Refpts\n \n def atpass(line: List[Element], r_in: np.ndarray, nturns: int,\n-           refpts: np.ndarray,\n+           refpts: Optional[Refpts] = [],\n            turn: Optional[int] = None,\n            energy: Optional[float] = None,\n            particle: Optional[Particle] = None,\n", "instance_id": "atcollab__at-798", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to improve documentation for the PyAT project. It specifies two distinct updates: adding information about distribution on conda-forge and addressing incomplete documentation for the `get_optics` function as reported in an issue. However, the statement lacks specific details about what exactly needs to be documented for `get_optics` or how the conda-forge installation instructions should be updated (e.g., exact wording or format). Additionally, the provided code changes do not directly align with the stated documentation goals, as they include modifications to C code and a Python type hint file, which seem unrelated to pure documentation updates. This introduces minor ambiguity about the full scope of the task or whether additional documentation files were updated but not shown. Despite these gaps, the overall goal is understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is very low, as the stated goal primarily involves documentation updates, which typically require minimal technical complexity. However, the provided code changes introduce slight complexity beyond pure documentation. The change in `pyat/at.c` modifies error handling for the `Length` attribute in a C-based Python extension, which requires basic understanding of Python C API and error handling (e.g., `PyFloat_AsDouble`, `PyErr_Clear`). The update in `atpass.pyi` adjusts type hints for the `atpass` function, requiring familiarity with Python type annotations and the project's type system. Despite these changes, the scope is limited to small, localized modifications in two files with no significant impact on the broader codebase or architecture. No complex algorithms, edge cases, or domain-specific knowledge beyond basic Python/C integration are required. The task aligns with a difficulty score of 0.15, falling in the \"Very Easy\" range (0.0-0.2), as it involves straightforward updates with minimal logic or system-level considerations, even if the code changes slightly exceed typical documentation tasks.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Project name conflicts with existing packages on PyPI and Crates.io\nPackages named \"degen\" have already been uploaded to [PyPI](https://pypi.org/project/degen/) and [Crates.io](https://crates.io/crates/degen).\r\n\r\nThis project should be renamed to something else that is actually available on both indexes.\r\n\r\nBlocks #3 #4\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex e6c036e..4dd1978 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -129,17 +129,6 @@ dependencies = [\n  \"rand_core\",\n ]\n \n-[[package]]\n-name = \"degen\"\n-version = \"0.1.0\"\n-dependencies = [\n- \"blake3\",\n- \"pyo3\",\n- \"rand_chacha\",\n- \"rsa\",\n- \"tiny-bip39\",\n-]\n-\n [[package]]\n name = \"der\"\n version = \"0.8.0-pre.0\"\n@@ -151,6 +140,17 @@ dependencies = [\n  \"zeroize\",\n ]\n \n+[[package]]\n+name = \"deterministic-keygen\"\n+version = \"0.1.0\"\n+dependencies = [\n+ \"blake3\",\n+ \"pyo3\",\n+ \"rand_chacha\",\n+ \"rsa\",\n+ \"tiny-bip39\",\n+]\n+\n [[package]]\n name = \"digest\"\n version = \"0.10.7\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex a8312d9..4ed372a 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,11 +1,11 @@\n [package]\n-name = \"degen\"\n+name = \"deterministic-keygen\"\n version = \"0.1.0\"\n edition = \"2021\"\n \n # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n [lib]\n-name = \"degen\"\n+name = \"deterministic_keygen\"\n crate-type = [\"cdylib\"]\n \n [dependencies]\ndiff --git a/README.md b/README.md\nindex 88a1350..c2f420c 100644\n--- a/README.md\n+++ b/README.md\n@@ -1,7 +1,7 @@\n-Degen - a deterministic key-generator\n-=====================================\n+Deterministic-keygen\n+====================\n \n-_Degen_ is an experimental Rust and Python library for generating cryptographic keys deterministically, i.e., repeatably deriving the same key-material (output) given the same initial bytes of entropy (input). This can be used by applications to regenerate or restore a given key from some user-supplied input (for example, a BIP-39 mnemonic phrase), potentially enabling more \"human-friendly\" forms of key backup and recovery.\n+_Deterministic-keygen_ is an experimental Rust and Python library for generating cryptographic keys deterministically, i.e., repeatably deriving the same key-material (output) given the same initial bytes of entropy (input). This can be used by applications to regenerate or restore a given key from some user-supplied input (for example, a BIP-39 mnemonic phrase), potentially enabling more \"human-friendly\" forms of key backup and recovery.\n \n Currently, only RSA keys are supported.\n \ndiff --git a/degen.pyi b/deterministic_keygen.pyi\nsimilarity index 100%\nrename from degen.pyi\nrename to deterministic_keygen.pyi\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 22eab0e..9bbbb53 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -3,7 +3,7 @@ requires = [\"maturin>=1.3,<2.0\"]\n build-backend = \"maturin\"\n \n [project]\n-name = \"degen\"\n+name = \"deterministic-keygen\"\n requires-python = \">=3.8\"\n classifiers = [\n     \"Programming Language :: Rust\",\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 388fce2..726b3ad 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -11,7 +11,7 @@ use std::str;\n // \"The context string should be hardcoded, globally unique, and application-specific.\n // A good default format for the context string is '[application] [commit timestamp]\n // [purpose]':\"\n-const RSA_CONTEXT: &str = \"degen Wed 07 Feb 2024 11:50:00 AM EST RSA v1\";\n+const RSA_CONTEXT: &str = \"deterministic-keygen Wed 07 Feb 2024 11:50:00 AM EST RSA v1\";\n \n /// Generate a new BIP-39 mnemonic phrase.\n #[pyfunction]\n@@ -74,7 +74,7 @@ pub fn derive_rsa_key_from_phrase(phrase: &str, bit_size: usize) -> PyResult<Str\n \n /// Deterministic key-generator.\n #[pymodule]\n-fn degen(_py: Python, m: &PyModule) -> PyResult<()> {\n+fn deterministic_keygen(_py: Python, m: &PyModule) -> PyResult<()> {\n     m.add_function(wrap_pyfunction!(generate_phrase, m)?)?;\n     m.add_function(wrap_pyfunction!(derive_rsa_key_from_phrase, m)?)?;\n     Ok(())\n", "instance_id": "crwood__deterministic-keygen-12", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: the project name \"degen\" conflicts with existing packages on PyPI and Crates.io, and it needs to be renamed to something available on both indexes. The goal is straightforward, and the code changes provided align with this goal by renaming the project to \"deterministic-keygen.\" However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the new name (\"deterministic-keygen\") has already been checked for availability on PyPI and Crates.io, nor does it provide guidance on how to choose a new name if the proposed one is also taken. Additionally, there are no explicit mentions of potential downstream impacts (e.g., documentation, CI/CD pipelines, or external dependencies) that might need to be updated due to the rename. Despite these minor gaps, the core issue and required action are clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is very low, as it primarily involves a straightforward renaming of the project across various files. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes:** The changes are limited to updating the project name in configuration files (Cargo.toml, pyproject.toml), documentation (README.md), and code (lib.rs, file renames). The modifications span multiple files but are superficial, involving simple string replacements with no impact on the system's architecture or logic. The overall amount of code change is minimal and does not require understanding complex interactions within the codebase.\n\n2. **Number of Technical Concepts:** The task requires only basic knowledge of project configuration in Rust (Cargo.toml) and Python (pyproject.toml), as well as familiarity with renaming files and updating strings in code. No advanced language features, algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Edge Cases and Error Handling:** There are no edge cases or error handling requirements mentioned in the problem statement or evident in the code changes. The task is purely mechanical and does not involve logic that could introduce errors or require special handling.\n\n4. **Overall Complexity:** This is a trivial task that can be completed quickly by anyone with basic familiarity with project setup in Rust and Python. The only potential challenge might be ensuring that the new name does not conflict with existing packages, but this is not addressed in the code changes and appears to be outside the scope of the provided diff.\n\nGiven these considerations, I assign a difficulty score of 0.1, placing it in the \"Very Easy\" category. It involves basic code modifications with no significant technical depth or complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ignore `.DS_Store` file by default\nWhen deployed from a macOS machine, .DS_Store` files often accidentally \"contaminate\" the staged assets, changing the asset hash unexpectedly and disabling the cache mechanism. \r\n\r\nIn most of use cases `.DS_Store` files should be unnecessary, and we may want to exclude the file by default.\n", "patch": "diff --git a/API.md b/API.md\nindex e0eac4e..cb7e2c3 100644\n--- a/API.md\n+++ b/API.md\n@@ -839,6 +839,7 @@ const nodejsBuildProps: NodejsBuildProps = { ... }\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.buildEnvironment\">buildEnvironment</a></code> | <code>{[ key: string ]: string}</code> | Environment variables injected to the build environment. |\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.destinationKeyPrefix\">destinationKeyPrefix</a></code> | <code>string</code> | Key prefix to deploy your build artifact. |\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.distribution\">distribution</a></code> | <code>aws-cdk-lib.aws_cloudfront.IDistribution</code> | The distribution you are using to publish you build artifact. |\n+| <code><a href=\"#deploy-time-build.NodejsBuildProps.property.excludeCommonFiles\">excludeCommonFiles</a></code> | <code>boolean</code> | If true, common unnecessary files/directories such as .DS_Store, .git, node_modules, etc are excluded from the assets by default. |\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.nodejsVersion\">nodejsVersion</a></code> | <code>number</code> | The version of Node.js to use in a build environment. Available versions: 12, 14, 16, 18, 20. |\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.outputEnvFile\">outputEnvFile</a></code> | <code>boolean</code> | If true, a .env file is uploaded to an S3 bucket with values of `buildEnvironment` property. You can copy it to your local machine by running the command in the stack output. |\n | <code><a href=\"#deploy-time-build.NodejsBuildProps.property.workingDirectory\">workingDirectory</a></code> | <code>string</code> | Relative path from the build directory to the directory where build commands run. |\n@@ -939,6 +940,19 @@ If any specified, the caches are invalidated on new artifact deployments.\n \n ---\n \n+##### `excludeCommonFiles`<sup>Optional</sup> <a name=\"excludeCommonFiles\" id=\"deploy-time-build.NodejsBuildProps.property.excludeCommonFiles\"></a>\n+\n+```typescript\n+public readonly excludeCommonFiles: boolean;\n+```\n+\n+- *Type:* boolean\n+- *Default:* true\n+\n+If true, common unnecessary files/directories such as .DS_Store, .git, node_modules, etc are excluded from the assets by default.\n+\n+---\n+\n ##### `nodejsVersion`<sup>Optional</sup> <a name=\"nodejsVersion\" id=\"deploy-time-build.NodejsBuildProps.property.nodejsVersion\"></a>\n \n ```typescript\ndiff --git a/src/nodejs-build.ts b/src/nodejs-build.ts\nindex a3b889c..18c0a46 100644\n--- a/src/nodejs-build.ts\n+++ b/src/nodejs-build.ts\n@@ -75,6 +75,12 @@ export interface NodejsBuildProps {\n    * @default false\n    */\n   readonly outputEnvFile?: boolean;\n+  /**\n+   * If true, common unnecessary files/directories such as .DS_Store, .git, node_modules, etc are excluded\n+   * from the assets by default.\n+   * @default true\n+   */\n+  readonly excludeCommonFiles?: boolean;\n }\n \n /**\n@@ -229,9 +235,11 @@ curl -v -i -X PUT -H 'Content-Type:' -d \"@payload.json\" \"$responseURL\"\n \n     this.grantPrincipal = project.grantPrincipal;\n \n+    const commonExclude = ['.DS_Store', '.git', 'node_modules'];\n     const assets = props.assets.map((assetProps) => {\n       const asset = new Asset(this, `Source-${assetProps.path.replace('/', '')}`, {\n         ...assetProps,\n+        ...(props.excludeCommonFiles ?? true ? { exclude: [...commonExclude, ...(assetProps.exclude ?? [])] } : {}),\n       });\n       asset.grantRead(project);\n       return asset;\n", "instance_id": "tmokmss__deploy-time-build-23", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to address an issue with `.DS_Store` files affecting asset hashes and cache mechanisms when deploying from macOS machines. It specifies the goal of excluding these files by default to prevent unintended changes. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define what constitutes \"common unnecessary files\" beyond `.DS_Store`, although the code changes provide some clarity by including `.git` and `node_modules`. Additionally, there are no examples or specific scenarios provided to illustrate the problem (e.g., how the asset hash changes or what kind of cache mechanism is impacted). Edge cases, such as user overrides for exclusion rules or potential conflicts with existing asset configurations, are not mentioned. Overall, while the goal is understandable, the lack of detailed context and edge case discussion prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to two files (`API.md` and `src/nodejs-build.ts`). The modifications involve adding a new optional property `excludeCommonFiles` to the `NodejsBuildProps` interface and updating the asset configuration logic to conditionally apply a default exclusion list. The amount of code change is minimal, with only a few lines added or modified. There is no significant impact on the system's architecture, as this is a straightforward enhancement to an existing feature.\n\n2. **Number of Technical Concepts**: The solution requires basic understanding of TypeScript (given the codebase context), object property spreading, and conditional logic. It also involves familiarity with the AWS CDK library for asset handling, but the usage here is simple (modifying the `exclude` property of an `Asset` object). No advanced algorithms, design patterns, or domain-specific knowledge beyond basic asset management in a deployment context are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce a default exclusion list that could potentially conflict with user-defined `exclude` rules. However, the implementation handles this by merging the default exclusions with user-provided ones using the spread operator, which is a simple and effective approach. No complex error handling is required, as the change is purely additive and does not introduce new failure modes beyond what might already exist in the asset configuration logic.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase and making a simple feature addition. It does not require deep knowledge of the broader system or intricate logic. A junior to mid-level developer with basic familiarity in TypeScript and AWS CDK could implement this change with minimal guidance.\n\nGiven these points, a difficulty score of 0.25 reflects the ease of the task. It requires slightly more than trivial changes (e.g., beyond fixing a typo) due to the need to understand the asset configuration logic and ensure the exclusion merging works as intended, but it remains a straightforward modification with low risk and complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "cells_to_wkb_points returning longitude for both coordinates\nHi! I've been migrating a project to `h3ronpy` but I've been running into an issue returning the centroids of hexagons. It seems that the coordinates returned have exactly the same value:\r\n\r\n```python\r\nimport h3ronpy\r\nfrom shapely.geometry import Point\r\nfrom h3ronpy.arrow.vector import geometry_to_cells, cells_to_wkb_points\r\nimport pyarrow as pa\r\nfrom shapely import wkb\r\n\r\n# Step 1: Create a point (latitude and longitude)\r\nlat, lon = 37.7749, -122.4194  # Example coordinates (San Francisco)\r\npoint = Point(lon, lat)  # shapely expects (longitude, latitude)\r\n\r\n# Step 2: Convert the point to an H3 cell (resolution 9 for example)\r\nresolution = 9\r\nh3_cells = geometry_to_cells(point, resolution)\r\n\r\n# Step 3: Convert the H3 cell back to WKB points\r\nwkb_points = cells_to_wkb_points(h3_cells)\r\n\r\n# Step 4: Decode the WKB point to a Shapely geometry\r\nfor wkb_point in wkb_points:\r\n    if isinstance(wkb_point, pa.Scalar):  # Ensure it's a pyarrow Scalar\r\n        shapely_point = wkb.loads(wkb_point.as_buffer().to_pybytes())\r\n        print(f\"Shapely Geometry: {shapely_point}\")\r\n    else:\r\n        print(f\"Unexpected Type: {type(wkb_point)}\")\r\n```\r\n\r\nReturns:\r\n```\r\nShapely Geometry: POINT (-122.41827103692466 -122.41827103692466)\r\n```\r\n\r\nI'd be happy to help with the fix! \n", "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex 878f731..ddc4b64 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -55,7 +55,7 @@ jobs:\n         run: |\n           set -e\n           pip install --force-reinstall dist/*.whl\n-          pytest -s h3ronpy/tests\n+          python -m pytest -s h3ronpy/tests\n \n   linux-aarch64:\n     runs-on: ubuntu-latest\n@@ -120,7 +120,7 @@ jobs:\n         run: |\n           set -e\n           pip install --force-reinstall dist/*.whl\n-          pytest h3ronpy/tests\n+          python -m pytest h3ronpy/tests\n \n   macos-x86_64:\n     needs:\n@@ -162,7 +162,7 @@ jobs:\n       #  run: |\n       #    set -e\n       #    pip install --force-reinstall --verbose dist/*.whl\n-      #    pytest h3ronpy/tests\n+      #    python -m pytest h3ronpy/tests\n \n   macos-aarch64:\n     runs-on: macos-latest\ndiff --git a/crates/h3arrow/src/array/to_geo.rs b/crates/h3arrow/src/array/to_geo.rs\nindex 0c9d3ca..d9149c5 100644\n--- a/crates/h3arrow/src/array/to_geo.rs\n+++ b/crates/h3arrow/src/array/to_geo.rs\n@@ -81,7 +81,7 @@ macro_rules! impl_iter_points {\n                                 let pt: Point = if use_degrees {\n                                     Coord {\n                                         x: ll.lng(),\n-                                        y: ll.lng(),\n+                                        y: ll.lat(),\n                                     }\n                                     .into()\n                                 } else {\ndiff --git a/h3ronpy/CHANGES.rst b/h3ronpy/CHANGES.rst\nindex 42a0896..3dc62d2 100644\n--- a/h3ronpy/CHANGES.rst\n+++ b/h3ronpy/CHANGES.rst\n@@ -12,6 +12,8 @@ Versioning <https://semver.org/spec/v2.0.0.html>`__.\n Unreleased\n ----------\n \n+- Fixed coordinate messup when converting H3 entities to points (issue #58).\n+\n 0.21.0 - 2024-07-01\n -------------------\n \ndiff --git a/h3ronpy/pyproject.toml b/h3ronpy/pyproject.toml\nindex 0fdaebb..f5643b9 100644\n--- a/h3ronpy/pyproject.toml\n+++ b/h3ronpy/pyproject.toml\n@@ -37,6 +37,7 @@ polars = [\n ]\n pandas = [\n     \"geopandas>=0.10\",\n+    \"fiona<1.10\"\n ]\n test = [\n     \"rasterio\",\n", "instance_id": "nmandery__h3ronpy-60", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `cells_to_wkb_points` function in the `h3ronpy` library returns incorrect coordinates, specifically the longitude value for both latitude and longitude. The provided Python code snippet effectively demonstrates the problem with a reproducible example, showing the expected input and the erroneous output. However, the statement lacks explicit mention of edge cases, constraints, or specific requirements for the fix beyond identifying the coordinate mix-up. Additionally, there is no discussion of potential impacts on other parts of the library or dependent code. While the goal is clear (fix the coordinate output), these missing minor details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to several factors. First, the scope of the code change is minimal and localized: the fix involves a single line in the Rust codebase (`crates/h3arrow/src/array/to_geo.rs`), correcting a simple error where `ll.lng()` was used for both coordinates instead of `ll.lat()` for the y-coordinate. This is a straightforward bug fix with no impact on the broader system architecture. Second, the technical concepts required are basic\u2014understanding coordinate systems (latitude/longitude) and familiarity with Rust syntax for struct field access. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic geospatial concepts are needed. Third, the problem does not explicitly mention edge cases or require additional error handling beyond the coordinate correction, and the provided diff does not introduce such complexity. Finally, while the codebase involves Rust (a systems language with some learning curve), the change itself is trivial for anyone with basic Rust experience. The minor updates to CI scripts and documentation are clerical and do not add to the technical difficulty. Therefore, a difficulty score of 0.25 reflects the simplicity of the fix and the limited depth of understanding required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Safe mode XSS\n**Describe the bug**\r\nIt's possible to bypass the safe mode (escape and replace) \r\n\r\n**To Reproduce**\r\nCreate a file called PoC.html:\r\n\r\n```html\r\n<img src=# onerror=\"alert()\"></p>\r\n```\r\n\r\nThen, execute\r\n```sh\r\nmarkdown2 --safe escape PoC.html > result.html\r\n```\r\nThe content of `result.html` will be:\r\n```html\r\n<p><img src=# onerror=\"alert()\">&lt;/p&gt;</p>\r\n```\r\n\r\nNote that the `<img>` tag has not been escaped.\r\nFinally, open `result.html` with a browser and you should see a pop up.\r\n\r\n**Expected behavior**\r\nAll dangerous characters should be escaped.\r\n\r\n**Debug info**\r\nVersion of library being used: 2.5.0\r\n\r\n**Additional context**\r\nno\n", "patch": "diff --git a/CHANGES.md b/CHANGES.md\nindex f07fb095..66a15743 100644\n--- a/CHANGES.md\n+++ b/CHANGES.md\n@@ -5,6 +5,7 @@\n - [pull #590] Fix underscores within bold text getting emphasized (#589)\n - [pull #591] Add Alerts extra\n - [pull #595] Fix img alt text being processed as markdown (#594)\n+- [pull #602] Fix XSS issue in safe mode (#601)\n - [pull #604] Fix XSS injection in image URLs (#603)\n \n \ndiff --git a/lib/markdown2.py b/lib/markdown2.py\nindex 26e5b075..5d8106fc 100755\n--- a/lib/markdown2.py\n+++ b/lib/markdown2.py\n@@ -1260,8 +1260,13 @@ def _run_span_gamut(self, text: str) -> str:\n             (?:\n                 # tag\n                 </?\n-                (?:\\w+)                                     # tag name\n-                (?:\\s+(?:[\\w-]+:)?[\\w-]+=(?:\".*?\"|'.*?'))*  # attributes\n+                (?:\\w+)         # tag name\n+                (?:             # attributes\n+                    \\s+                           # whitespace after tag\n+                    (?:[^\\t<>\"'=/]+:)?\n+                    [^<>\"'=/]+=                   # attr name\n+                    (?:\".*?\"|'.*?'|[^<>\"'=/\\s]+)  # value, quoted or unquoted. If unquoted, no spaces allowed\n+                )*\n                 \\s*/?>\n                 |\n                 # auto-link (e.g., <http://www.activestate.com/>)\n", "instance_id": "trentm__python-markdown2-602", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to XSS (Cross-Site Scripting) in safe mode for a markdown processing library. It provides a reproducible example with specific input (PoC.html), the command to execute, and the resulting output (result.html), which demonstrates the issue of unescaped HTML tags. The expected behavior is also mentioned (all dangerous characters should be escaped). However, there are minor ambiguities: the problem does not explicitly define what constitutes \"dangerous characters\" or provide a comprehensive list of tags/attributes that should be escaped. Additionally, edge cases or other potential XSS vectors are not discussed, which could leave room for interpretation. Overall, the statement is valid and clear but lacks some finer details about the scope of \"safe mode\" and potential edge cases.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single part of the codebase (the `_run_span_gamut` method in `markdown2.py`), specifically the regular expression for parsing HTML tags and attributes. The change involves tightening the regex to better handle attribute values and prevent XSS by restricting unquoted values and spaces, which requires a good understanding of regex intricacies. Second, the technical concepts involved include knowledge of regular expressions, HTML parsing, and XSS vulnerabilities, which are moderately complex but not overly advanced. Third, the problem impacts a security-critical area (safe mode), so care must be taken to avoid breaking legitimate use cases while fixing the vulnerability, adding a layer of complexity in testing and validation. However, the change does not appear to affect the broader architecture or multiple modules, and the amount of code modified is small. Edge cases, such as various malformed HTML inputs or different attribute formats, are implied but not explicitly detailed in the problem statement, requiring the developer to anticipate and handle them. Overall, this problem requires a moderate level of expertise in Python, regex, and web security, placing it at a difficulty of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Solve multi-modal models with a new concept of \"attachments\"\nPrevious work is in:\r\n- #331\r\n\r\nI'm going a different direction. Previously I had just been thinking about images, but Gemini accepts PDFs and videos and audio clips and the latest GPT-4o model supports audio clips too.\r\n\r\nThe `llm prompt` command isn't using `-a` for anything yet, so I'm going to have `-a filename` be the way an attachment (or multiple attachments) is added to a prompt.\r\n\r\n`-a` is short for `--attachment` - not for `--attach` because that already means something different for the `llm embed-multi` command (it attaches extra SQLite databases).\r\n\r\n## TODO\r\n\r\n- [x] Get `llm 'describe image' -a image.jpeg` working\r\n- [x] And `llm 'describe image' -a https://static.simonwillison.net/static/2024/imgcat.jpg`\r\n- [x] And `cat image.jpeg | llm 'describe image' -a -`\r\n- [x] Think about how async might work. Maybe the `Attachment` class should not have code for `httpx.get()` fetching of content, since an `asyncio` wrapper may want to do that a different way.\r\n- [x] Figure out database persistence, so continue conversation can work\r\n- [x] Implement OpenAI and Gemini plugins\r\n- [x] Docs for how to write plugins that accept attachments\r\n- [x] `llm logs` output for prompts with attachments\r\n- [x] `llm logs --json` output\r\n- [x] Finalize Python API\r\n- [x] Document Python API\r\n- [x] Document how to use attachments in CLI\r\n- [x] Ship an alpha\r\n- [x] Automated tests\r\n\r\n## Out of scope for this issue:\r\n\r\n- `llm chat` support for attachments via `!attachment path-or-url`\nSolve multi-modal models with a new concept of \"attachments\"\nPrevious work is in:\r\n- #331\r\n\r\nI'm going a different direction. Previously I had just been thinking about images, but Gemini accepts PDFs and videos and audio clips and the latest GPT-4o model supports audio clips too.\r\n\r\nThe `llm prompt` command isn't using `-a` for anything yet, so I'm going to have `-a filename` be the way an attachment (or multiple attachments) is added to a prompt.\r\n\r\n`-a` is short for `--attachment` - not for `--attach` because that already means something different for the `llm embed-multi` command (it attaches extra SQLite databases).\r\n\r\n## TODO\r\n\r\n- [x] Get `llm 'describe image' -a image.jpeg` working\r\n- [x] And `llm 'describe image' -a https://static.simonwillison.net/static/2024/imgcat.jpg`\r\n- [x] And `cat image.jpeg | llm 'describe image' -a -`\r\n- [x] Think about how async might work. Maybe the `Attachment` class should not have code for `httpx.get()` fetching of content, since an `asyncio` wrapper may want to do that a different way.\r\n- [x] Figure out database persistence, so continue conversation can work\r\n- [x] Implement OpenAI and Gemini plugins\r\n- [x] Docs for how to write plugins that accept attachments\r\n- [x] `llm logs` output for prompts with attachments\r\n- [x] `llm logs --json` output\r\n- [x] Finalize Python API\r\n- [x] Document Python API\r\n- [x] Document how to use attachments in CLI\r\n- [x] Ship an alpha\r\n- [x] Automated tests\r\n\r\n## Out of scope for this issue:\r\n\r\n- `llm chat` support for attachments via `!attachment path-or-url`\n", "patch": "diff --git a/docs/help.md b/docs/help.md\nindex 9cd2927c..0e28494a 100644\n--- a/docs/help.md\n+++ b/docs/help.md\n@@ -86,20 +86,37 @@ Usage: llm prompt [OPTIONS] [PROMPT]\n \n   Documentation: https://llm.datasette.io/en/stable/usage.html\n \n-Options:\n-  -s, --system TEXT            System prompt to use\n-  -m, --model TEXT             Model to use\n-  -o, --option <TEXT TEXT>...  key/value options for the model\n-  -t, --template TEXT          Template to use\n-  -p, --param <TEXT TEXT>...   Parameters for template\n-  --no-stream                  Do not stream output\n-  -n, --no-log                 Don't log to database\n-  --log                        Log prompt and response to the database\n-  -c, --continue               Continue the most recent conversation.\n-  --cid, --conversation TEXT   Continue the conversation with the given ID.\n-  --key TEXT                   API key to use\n-  --save TEXT                  Save prompt with this template name\n-  --help                       Show this message and exit.\n+  Examples:\n+\n+      llm 'Capital of France?'\n+      llm 'Capital of France?' -m gpt-4o\n+      llm 'Capital of France?' -s 'answer in Spanish'\n+\n+  Multi-modal models can be called with attachments like this:\n+\n+      llm 'Extract text from this image' -a image.jpg\n+      llm 'Describe' -a https://static.simonwillison.net/static/2024/pelicans.jpg\n+      cat image | llm 'describe image' -a -\n+      # With an explicit mimetype:\n+      cat image | llm 'describe image' --at - image/jpeg\n+\n+Options:\n+  -s, --system TEXT               System prompt to use\n+  -m, --model TEXT                Model to use\n+  -a, --attachment ATTACHMENT     Attachment path or URL or -\n+  --at, --attachment-type <TEXT TEXT>...\n+                                  Attachment with explicit mimetype\n+  -o, --option <TEXT TEXT>...     key/value options for the model\n+  -t, --template TEXT             Template to use\n+  -p, --param <TEXT TEXT>...      Parameters for template\n+  --no-stream                     Do not stream output\n+  -n, --no-log                    Don't log to database\n+  --log                           Log prompt and response to the database\n+  -c, --continue                  Continue the most recent conversation.\n+  --cid, --conversation TEXT      Continue the conversation with the given ID.\n+  --key TEXT                      API key to use\n+  --save TEXT                     Save prompt with this template name\n+  --help                          Show this message and exit.\n ```\n \n (help-chat)=\ndiff --git a/docs/index.md b/docs/index.md\nindex 6b580015..9b0ad47e 100644\n--- a/docs/index.md\n+++ b/docs/index.md\n@@ -46,10 +46,13 @@ If you have an [OpenAI API key](https://platform.openai.com/api-keys) key you ca\n # Paste your OpenAI API key into this\n llm keys set openai\n \n-# Run a prompt\n+# Run a prompt (with the default gpt-4o-mini model)\n llm \"Ten fun names for a pet pelican\"\n \n-# Run a system prompt against a file\n+# Extract text from an image\n+llm \"extract text\" -a scanned-document.jpg\n+\n+# Use a system prompt against a file\n cat myfile.py | llm -s \"Explain this code\"\n ```\n Or you can {ref}`install a plugin <installing-plugins>` and use models that can run on your local device:\n@@ -62,10 +65,10 @@ llm -m orca-mini-3b-gguf2-q4_0 'What is the capital of France?'\n ```\n To start {ref}`an interactive chat <usage-chat>` with a model, use `llm chat`:\n ```bash\n-llm chat -m gpt-4o-mini\n+llm chat -m gpt-4o\n ```\n ```\n-Chatting with gpt-4o-mini\n+Chatting with gpt-4o\n Type 'exit' or 'quit' to exit\n Type '!multi' to enter multiple lines, then '!end' to finish\n > Tell me a joke about a pelican\ndiff --git a/docs/plugins/advanced-model-plugins.md b/docs/plugins/advanced-model-plugins.md\nnew file mode 100644\nindex 00000000..fdbdc232\n--- /dev/null\n+++ b/docs/plugins/advanced-model-plugins.md\n@@ -0,0 +1,102 @@\n+(advanced-model-plugins)=\n+# Advanced model plugins\n+\n+The {ref}`model plugin tutorial <tutorial-model-plugin>` covers the basics of developing a plugin that adds support for a new model.\n+\n+This document covers more advanced topics.\n+\n+(advanced-model-plugins-attachments)=\n+## Attachments for multi-modal models\n+\n+Models such as GPT-4o, Claude 3.5 Sonnet and Google's Gemini 1.5 are multi-modal: they accept input in the form of images and maybe even audio, video and other formats.\n+\n+LLM calls these **attachments**. Models can specify the types of attachments they accept and then implement special code in the `.execute()` method to handle them.\n+\n+### Specifying attachment types\n+\n+A `Model` subclass can list the types of attachments it accepts by defining a `attachment_types` class attribute:\n+\n+```python\n+class NewModel(llm.Model):\n+    model_id = \"new-model\"\n+    attachment_types = {\n+        \"image/png\",\n+        \"image/jpeg\",\n+        \"image/webp\",\n+        \"image/gif\",\n+    }\n+```\n+These content types are detected when an attachment is passed to LLM using `llm -a filename`, or can be specified by the user using the `--attachment-type filename image/png` option.\n+\n+**Note:** *MP3 files will have their attachment type detected as `audio/mpeg`, not `audio/mp3`.\n+\n+LLM will use the `attachment_types` attribute to validate that provided attachments should be accepted before passing them to the model.\n+\n+### Handling attachments\n+\n+The `prompt` object passed to the `execute()` method will have an `attachments` attribute containing a list of `Attachment` objects provided by the user.\n+\n+An `Attachment` instance has the following properties:\n+\n+- `url (str)`: The URL of the attachment, if it was provided as a URL\n+- `path (str)`: The resolved file path of the attachment, if it was provided as a file\n+- `type (str)`: The content type of the attachment, if it was provided\n+- `content (bytes)`: The binary content of the attachment, if it was provided\n+\n+Generally only one of `url`, `path` or `content` will be set.\n+\n+You should usually access the type and the content through one of these methods:\n+\n+- `attachment.resolve_type() -> str`: Returns the `type` if it is available, otherwise attempts to guess the type by looking at the first few bytes of content\n+- `attachment.content_bytes() -> bytes`: Returns the binary content, which it may need to read from a file or fetch from a URL\n+- `attachment.base64_content() -> str`: Returns that content as a base64-encoded string\n+\n+A `id()` method returns a database ID for this content, which is either a SHA256 hash of the binary content or, in the case of attachments hosted at an external URL, a hash of `{\"url\": url}` instead. This is an implementation detail which you should not need to access directly.\n+\n+Here's how the OpenAI plugin handles attachments:\n+\n+```python\n+messages = []\n+if not prompt.attachments:\n+    messages.append({\"role\": \"user\", \"content\": prompt.prompt})\n+else:\n+    attachment_message = [{\"type\": \"text\", \"text\": prompt.prompt}]\n+    for attachment in prompt.attachments:\n+        url = attachment.url\n+        if not url:\n+            base64_image = attachment.base64_content()\n+            url = f\"data:{attachment.resolve_type()};base64,{base64_image}\"\n+        attachment_message.append(\n+            {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n+        )\n+    messages.append({\"role\": \"user\", \"content\": attachment_message})\n+```\n+As you can see, it uses `attachment.url` if that is available and otherwise falls back to using the `base64_content()` method to embed the image directly in the JSON sent to the API.\n+\n+### Attachments from previous conversations\n+\n+Models that implement the ability to continue a conversation can reconstruct the previous message JSON using the `response.attachments` attribute.\n+\n+Here's how the OpenAI plugin does that:\n+\n+```python\n+for prev_response in conversation.responses:\n+    if prev_response.attachments:\n+        attachment_message = [\n+            {\"type\": \"text\", \"text\": prev_response.prompt.prompt}\n+        ]\n+        for attachment in prev_response.attachments:\n+            url = attachment.url\n+            if not url:\n+                base64_image = attachment.base64_content()\n+                url = f\"data:{attachment.resolve_type()};base64,{base64_image}\"\n+            attachment_message.append(\n+                {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n+            )\n+        messages.append({\"role\": \"user\", \"content\": attachment_message})\n+    else:\n+        messages.append(\n+            {\"role\": \"user\", \"content\": prev_response.prompt.prompt}\n+        )\n+    messages.append({\"role\": \"assistant\", \"content\": prev_response.text()})\n+```\ndiff --git a/docs/plugins/index.md b/docs/plugins/index.md\nindex 96ae62fd..2a08844d 100644\n--- a/docs/plugins/index.md\n+++ b/docs/plugins/index.md\n@@ -17,5 +17,6 @@ installing-plugins\n directory\n plugin-hooks\n tutorial-model-plugin\n+advanced-model-plugins\n plugin-utilities\n ```\ndiff --git a/docs/plugins/tutorial-model-plugin.md b/docs/plugins/tutorial-model-plugin.md\nindex ff9c17fb..6f1bcbbc 100644\n--- a/docs/plugins/tutorial-model-plugin.md\n+++ b/docs/plugins/tutorial-model-plugin.md\n@@ -1,5 +1,5 @@\n (tutorial-model-plugin)=\n-# Writing a plugin to support a new model\n+# Model plugin tutorial\n \n This tutorial will walk you through developing a new plugin for LLM that adds support for a new Large Language Model.\n \ndiff --git a/docs/python-api.md b/docs/python-api.md\nindex dccf46bf..dd553101 100644\n--- a/docs/python-api.md\n+++ b/docs/python-api.md\n@@ -49,6 +49,24 @@ response = model.prompt(\n     system=\"Answer like GlaDOS\"\n )\n ```\n+### Attachments\n+\n+Model that accept multi-modal input (images, audio, video etc) can be passed attachments using the `attachments=` keyword argument. This accepts a list of `llm.Attachment()` instances.\n+\n+This example shows two attachments - one from a file path and one from a URL:\n+```python\n+import llm\n+\n+model = llm.get_model(\"gpt-4o-mini\")\n+response = model.prompt(\n+    \"Describe these images\",\n+    attachments=[\n+        llm.Attachment(path=\"pelican.jpg\"),\n+        llm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\"),\n+    ]\n+)\n+```\n+Use `llm.Attachment(content=b\"binary image content here\")` to pass binary content directly.\n \n ### Model options\n \n@@ -114,6 +132,16 @@ print(response2.text())\n ```\n You will get back five fun facts about skunks.\n \n+The `conversation.prompt()` method supports attachments as well:\n+```python\n+response = conversation.prompt(\n+    \"Describe these birds\",\n+    attachments=[\n+        llm.Attachment(url=\"https://static.simonwillison.net/static/2024/pelicans.jpg\")\n+    ]\n+)\n+```\n+\n Access `conversation.responses` for a list of all of the responses that have so far been returned during the conversation.\n \n ## Other functions\ndiff --git a/docs/usage.md b/docs/usage.md\nindex 005a1690..6b825245 100644\n--- a/docs/usage.md\n+++ b/docs/usage.md\n@@ -45,49 +45,30 @@ Some models support options. You can pass these using `-o/--option name value` -\n ```bash\n llm 'Ten names for cheesecakes' -o temperature 1.5\n ```\n+### Attachments\n \n-(usage-completion-prompts)=\n-## Completion prompts\n-\n-Some models are completion models - rather than being tuned to respond to chat style prompts, they are designed to complete a sentence or paragraph.\n-\n-An example of this is the `gpt-3.5-turbo-instruct` OpenAI model.\n+Some models are multi-modal, which means they can accept input in more than just text. GPT-4o and GPT-4o mini can accept images, and models such as Google Gemini 1.5 can accept audio and video as well.\n \n-You can prompt that model the same way as the chat models, but be aware that the prompt format that works best is likely to differ.\n+LLM calls these **attachments**. You can pass attachments using the `-a` option like this:\n \n ```bash\n-llm -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver:'\n+llm \"describe this image\" -a https://static.simonwillison.net/static/2024/pelicans.jpg\n ```\n-\n-(conversation)=\n-## Continuing a conversation\n-\n-By default, the tool will start a new conversation each time you run it.\n-\n-You can opt to continue the previous conversation by passing the `-c/--continue` option:\n+Attachments can be passed using URLs or file paths, and you can attach more than one attachment to a single prompt:\n ```bash\n-llm 'More names' -c\n+llm \"describe these images\" -a image1.jpg -a image2.jpg\n ```\n-This will re-send the prompts and responses for the previous conversation as part of the call to the language model. Note that this can add up quickly in terms of tokens, especially if you are using expensive models.\n-\n-`--continue` will automatically use the same model as the conversation that you are continuing, even if you omit the `-m/--model` option.\n-\n-To continue a conversation that is not the most recent one, use the `--cid/--conversation <id>` option:\n+You can also pipe an attachment to LLM by using `-` as the filename:\n ```bash\n-llm 'More names' --cid 01h53zma5txeby33t1kbe3xk8q\n+cat image.jpg | llm \"describe this image\" -a -\n ```\n-You can find these conversation IDs using the `llm logs` command.\n-\n-## Using with a shell\n-\n-To learn more about your computer's operating system based on the output of `uname -a`, run this:\n+LLM will attempt to automatically detect the content type of the image. If this doesn't work you can instead use the `--attachment-type` option (`--at` for short) which takes the URL/path plus an explicit content type:\n ```bash\n-llm \"Tell me about my operating system: $(uname -a)\"\n+cat myfile | llm \"describe this image\" --at - image/jpeg\n ```\n-This pattern of using `$(command)` inside a double quoted string is a useful way to quickly assemble prompts.\n \n (system-prompts)=\n-## System prompts\n+### System prompts\n \n You can use `-s/--system '...'` to set a system prompt.\n ```bash\n@@ -120,6 +101,46 @@ cat llm/utils.py | llm -t pytest\n ```\n See {ref}`prompt templates <prompt-templates>` for more.\n \n+(conversation)=\n+### Continuing a conversation\n+\n+By default, the tool will start a new conversation each time you run it.\n+\n+You can opt to continue the previous conversation by passing the `-c/--continue` option:\n+```bash\n+llm 'More names' -c\n+```\n+This will re-send the prompts and responses for the previous conversation as part of the call to the language model. Note that this can add up quickly in terms of tokens, especially if you are using expensive models.\n+\n+`--continue` will automatically use the same model as the conversation that you are continuing, even if you omit the `-m/--model` option.\n+\n+To continue a conversation that is not the most recent one, use the `--cid/--conversation <id>` option:\n+```bash\n+llm 'More names' --cid 01h53zma5txeby33t1kbe3xk8q\n+```\n+You can find these conversation IDs using the `llm logs` command.\n+\n+### Tips for using LLM with Bash or Zsh\n+\n+To learn more about your computer's operating system based on the output of `uname -a`, run this:\n+```bash\n+llm \"Tell me about my operating system: $(uname -a)\"\n+```\n+This pattern of using `$(command)` inside a double quoted string is a useful way to quickly assemble prompts.\n+\n+(usage-completion-prompts)=\n+### Completion prompts\n+\n+Some models are completion models - rather than being tuned to respond to chat style prompts, they are designed to complete a sentence or paragraph.\n+\n+An example of this is the `gpt-3.5-turbo-instruct` OpenAI model.\n+\n+You can prompt that model the same way as the chat models, but be aware that the prompt format that works best is likely to differ.\n+\n+```bash\n+llm -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver:'\n+```\n+\n (usage-chat)=\n \n ## Starting an interactive chat\ndiff --git a/llm/__init__.py b/llm/__init__.py\nindex 9e8afacb..0ea6c242 100644\n--- a/llm/__init__.py\n+++ b/llm/__init__.py\n@@ -4,6 +4,7 @@\n     NeedsKeyException,\n )\n from .models import (\n+    Attachment,\n     Conversation,\n     Model,\n     ModelWithAliases,\n@@ -28,6 +29,7 @@\n     \"get_model\",\n     \"get_key\",\n     \"user_dir\",\n+    \"Attachment\",\n     \"Collection\",\n     \"Conversation\",\n     \"Model\",\ndiff --git a/llm/cli.py b/llm/cli.py\nindex a1b14576..941831c5 100644\n--- a/llm/cli.py\n+++ b/llm/cli.py\n@@ -4,6 +4,7 @@\n import io\n import json\n from llm import (\n+    Attachment,\n     Collection,\n     Conversation,\n     Response,\n@@ -30,7 +31,9 @@\n from .migrations import migrate\n from .plugins import pm\n import base64\n+import httpx\n import pathlib\n+import puremagic\n import pydantic\n import readline\n from runpy import run_module\n@@ -48,6 +51,56 @@\n DEFAULT_TEMPLATE = \"prompt: \"\n \n \n+class AttachmentType(click.ParamType):\n+    name = \"attachment\"\n+\n+    def convert(self, value, param, ctx):\n+        if value == \"-\":\n+            content = sys.stdin.buffer.read()\n+            # Try to guess type\n+            try:\n+                mimetype = puremagic.from_string(content, mime=True)\n+            except puremagic.PureError:\n+                raise click.BadParameter(\"Could not determine mimetype of stdin\")\n+            return Attachment(type=mimetype, path=None, url=None, content=content)\n+        if \"://\" in value:\n+            # Confirm URL exists and try to guess type\n+            try:\n+                response = httpx.head(value)\n+                response.raise_for_status()\n+                mimetype = response.headers.get(\"content-type\")\n+            except httpx.HTTPError as ex:\n+                raise click.BadParameter(str(ex))\n+            return Attachment(mimetype, None, value, None)\n+        # Check that the file exists\n+        path = pathlib.Path(value)\n+        if not path.exists():\n+            self.fail(f\"File {value} does not exist\", param, ctx)\n+        path = path.resolve()\n+        # Try to guess type\n+        mimetype = puremagic.from_file(str(path), mime=True)\n+        return Attachment(type=mimetype, path=str(path), url=None, content=None)\n+\n+\n+def attachment_types_callback(ctx, param, values):\n+    collected = []\n+    for value, mimetype in values:\n+        if \"://\" in value:\n+            attachment = Attachment(mimetype, None, value, None)\n+        elif value == \"-\":\n+            content = sys.stdin.buffer.read()\n+            attachment = Attachment(mimetype, None, None, content)\n+        else:\n+            # Look for file\n+            path = pathlib.Path(value)\n+            if not path.exists():\n+                raise click.BadParameter(f\"File {value} does not exist\")\n+            path = path.resolve()\n+            attachment = Attachment(mimetype, str(path), None, None)\n+        collected.append(attachment)\n+    return collected\n+\n+\n def _validate_metadata_json(ctx, param, value):\n     if value is None:\n         return value\n@@ -88,6 +141,23 @@ def cli():\n @click.argument(\"prompt\", required=False)\n @click.option(\"-s\", \"--system\", help=\"System prompt to use\")\n @click.option(\"model_id\", \"-m\", \"--model\", help=\"Model to use\")\n+@click.option(\n+    \"attachments\",\n+    \"-a\",\n+    \"--attachment\",\n+    type=AttachmentType(),\n+    multiple=True,\n+    help=\"Attachment path or URL or -\",\n+)\n+@click.option(\n+    \"attachment_types\",\n+    \"--at\",\n+    \"--attachment-type\",\n+    type=(str, str),\n+    multiple=True,\n+    callback=attachment_types_callback,\n+    help=\"Attachment with explicit mimetype\",\n+)\n @click.option(\n     \"options\",\n     \"-o\",\n@@ -127,6 +197,8 @@ def prompt(\n     prompt,\n     system,\n     model_id,\n+    attachments,\n+    attachment_types,\n     options,\n     template,\n     param,\n@@ -142,6 +214,22 @@ def prompt(\n     Execute a prompt\n \n     Documentation: https://llm.datasette.io/en/stable/usage.html\n+\n+    Examples:\n+\n+    \\b\n+        llm 'Capital of France?'\n+        llm 'Capital of France?' -m gpt-4o\n+        llm 'Capital of France?' -s 'answer in Spanish'\n+\n+    Multi-modal models can be called with attachments like this:\n+\n+    \\b\n+        llm 'Extract text from this image' -a image.jpg\n+        llm 'Describe' -a https://static.simonwillison.net/static/2024/pelicans.jpg\n+        cat image | llm 'describe image' -a -\n+        # With an explicit mimetype:\n+        cat image | llm 'describe image' --at - image/jpeg\n     \"\"\"\n     if log and no_log:\n         raise click.ClickException(\"--log and --no-log are mutually exclusive\")\n@@ -262,6 +350,8 @@ def read_prompt():\n         except pydantic.ValidationError as ex:\n             raise click.ClickException(render_errors(ex.errors()))\n \n+    resolved_attachments = [*attachments, *attachment_types]\n+\n     should_stream = model.can_stream and not no_stream\n     if not should_stream:\n         validated_options[\"stream\"] = False\n@@ -273,7 +363,9 @@ def read_prompt():\n         prompt_method = conversation.prompt\n \n     try:\n-        response = prompt_method(prompt, system, **validated_options)\n+        response = prompt_method(\n+            prompt, attachments=resolved_attachments, system=system, **validated_options\n+        )\n         if should_stream:\n             for chunk in response:\n                 print(chunk, end=\"\")\n@@ -437,7 +529,7 @@ def chat(\n                 raise click.ClickException(str(ex))\n         if prompt.strip() in (\"exit\", \"quit\"):\n             break\n-        response = conversation.prompt(prompt, system, **validated_options)\n+        response = conversation.prompt(prompt, system=system, **validated_options)\n         # System prompt only sent for the first message:\n         system = None\n         for chunk in response:\n@@ -468,7 +560,7 @@ def load_conversation(conversation_id: Optional[str]) -> Optional[Conversation]:\n     for response in db[\"responses\"].rows_where(\n         \"conversation_id = ?\", [conversation_id]\n     ):\n-        conversation.responses.append(Response.from_row(response))\n+        conversation.responses.append(Response.from_row(db, response))\n     return conversation\n \n \n@@ -611,6 +703,21 @@ def logs_turn_off():\n order by responses_fts.rank desc{limit}\n \"\"\"\n \n+ATTACHMENTS_SQL = \"\"\"\n+select\n+    response_id,\n+    attachments.id,\n+    attachments.type,\n+    attachments.path,\n+    attachments.url,\n+    length(attachments.content) as content_length\n+from attachments\n+join prompt_attachments\n+    on attachments.id = prompt_attachments.attachment_id\n+where prompt_attachments.response_id in ({})\n+order by prompt_attachments.\"order\"\n+\"\"\"\n+\n \n @logs.command(name=\"list\")\n @click.option(\n@@ -732,6 +839,14 @@ def logs_list(\n     # ... except for searches where we don't do this\n     if not query:\n         rows.reverse()\n+\n+    # Fetch any attachments\n+    ids = [row[\"id\"] for row in rows]\n+    attachments = list(db.query(ATTACHMENTS_SQL.format(\",\".join(\"?\" * len(ids))), ids))\n+    attachments_by_id = {}\n+    for attachment in attachments:\n+        attachments_by_id.setdefault(attachment[\"response_id\"], []).append(attachment)\n+\n     for row in rows:\n         if truncate:\n             row[\"prompt\"] = _truncate_string(row[\"prompt\"])\n@@ -747,6 +862,11 @@ def logs_list(\n \n     if json_output:\n         # Output as JSON if requested\n+        for row in rows:\n+            row[\"attachments\"] = [\n+                {k: v for k, v in attachment.items() if k != \"response_id\"}\n+                for attachment in attachments_by_id.get(row[\"id\"], [])\n+            ]\n         click.echo(json.dumps(list(rows), indent=2))\n     elif response:\n         # Just output the last response\n@@ -780,6 +900,30 @@ def logs_list(\n                 if row[\"system\"] is not None:\n                     click.echo(\"\\n## System:\\n\\n{}\".format(row[\"system\"]))\n                 current_system = row[\"system\"]\n+            attachments = attachments_by_id.get(row[\"id\"])\n+            if attachments:\n+                click.echo(\"\\n### Attachments\\n\")\n+                for i, attachment in enumerate(attachments, 1):\n+                    if attachment[\"path\"]:\n+                        path = attachment[\"path\"]\n+                        click.echo(\n+                            \"{}. **{}**: `{}`\".format(i, attachment[\"type\"], path)\n+                        )\n+                    elif attachment[\"url\"]:\n+                        click.echo(\n+                            \"{}. **{}**: {}\".format(\n+                                i, attachment[\"type\"], attachment[\"url\"]\n+                            )\n+                        )\n+                    elif attachment[\"content_length\"]:\n+                        click.echo(\n+                            \"{}. **{}**: `<{} bytes>`\".format(\n+                                i,\n+                                attachment[\"type\"],\n+                                f\"{attachment['content_length']:,}\",\n+                            )\n+                        )\n+\n             click.echo(\"\\n## Response:\\n\\n{}\\n\".format(row[\"response\"]))\n \n \ndiff --git a/llm/default_plugins/openai_models.py b/llm/default_plugins/openai_models.py\nindex 657c0d20..bce1fb04 100644\n--- a/llm/default_plugins/openai_models.py\n+++ b/llm/default_plugins/openai_models.py\n@@ -33,8 +33,8 @@ def register_models(register):\n     register(Chat(\"gpt-4-turbo-2024-04-09\"))\n     register(Chat(\"gpt-4-turbo\"), aliases=(\"gpt-4-turbo-preview\", \"4-turbo\", \"4t\"))\n     # GPT-4o\n-    register(Chat(\"gpt-4o\"), aliases=(\"4o\",))\n-    register(Chat(\"gpt-4o-mini\"), aliases=(\"4o-mini\",))\n+    register(Chat(\"gpt-4o\", vision=True), aliases=(\"4o\",))\n+    register(Chat(\"gpt-4o-mini\", vision=True), aliases=(\"4o-mini\",))\n     # o1\n     register(Chat(\"o1-preview\", can_stream=False, allows_system_prompt=False))\n     register(Chat(\"o1-mini\", can_stream=False, allows_system_prompt=False))\n@@ -271,6 +271,7 @@ def __init__(\n         api_engine=None,\n         headers=None,\n         can_stream=True,\n+        vision=False,\n         allows_system_prompt=True,\n     ):\n         self.model_id = model_id\n@@ -282,8 +283,17 @@ def __init__(\n         self.api_engine = api_engine\n         self.headers = headers\n         self.can_stream = can_stream\n+        self.vision = vision\n         self.allows_system_prompt = allows_system_prompt\n \n+        if vision:\n+            self.attachment_types = {\n+                \"image/png\",\n+                \"image/jpeg\",\n+                \"image/webp\",\n+                \"image/gif\",\n+            }\n+\n     def __str__(self):\n         return \"OpenAI Chat: {}\".format(self.model_id)\n \n@@ -302,14 +312,40 @@ def execute(self, prompt, stream, response, conversation=None):\n                         {\"role\": \"system\", \"content\": prev_response.prompt.system}\n                     )\n                     current_system = prev_response.prompt.system\n-                messages.append(\n-                    {\"role\": \"user\", \"content\": prev_response.prompt.prompt}\n-                )\n+                if prev_response.attachments:\n+                    attachment_message = [\n+                        {\"type\": \"text\", \"text\": prev_response.prompt.prompt}\n+                    ]\n+                    for attachment in prev_response.attachments:\n+                        url = attachment.url\n+                        if not url:\n+                            base64_image = attachment.base64_content()\n+                            url = f\"data:{attachment.resolve_type()};base64,{base64_image}\"\n+                        attachment_message.append(\n+                            {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n+                        )\n+                    messages.append({\"role\": \"user\", \"content\": attachment_message})\n+                else:\n+                    messages.append(\n+                        {\"role\": \"user\", \"content\": prev_response.prompt.prompt}\n+                    )\n                 messages.append({\"role\": \"assistant\", \"content\": prev_response.text()})\n         if prompt.system and prompt.system != current_system:\n             messages.append({\"role\": \"system\", \"content\": prompt.system})\n-        messages.append({\"role\": \"user\", \"content\": prompt.prompt})\n-        response._prompt_json = {\"messages\": messages}\n+        if not prompt.attachments:\n+            messages.append({\"role\": \"user\", \"content\": prompt.prompt})\n+        else:\n+            attachment_message = [{\"type\": \"text\", \"text\": prompt.prompt}]\n+            for attachment in prompt.attachments:\n+                url = attachment.url\n+                if not url:\n+                    base64_image = attachment.base64_content()\n+                    url = f\"data:{attachment.resolve_type()};base64,{base64_image}\"\n+                attachment_message.append(\n+                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n+                )\n+            messages.append({\"role\": \"user\", \"content\": attachment_message})\n+\n         kwargs = self.build_kwargs(prompt)\n         client = self.get_client()\n         if stream:\n@@ -335,6 +371,7 @@ def execute(self, prompt, stream, response, conversation=None):\n             )\n             response.response_json = remove_dict_none_values(completion.model_dump())\n             yield completion.choices[0].message.content\n+        response._prompt_json = redact_data_urls({\"messages\": messages})\n \n     def get_client(self):\n         kwargs = {}\n@@ -394,7 +431,6 @@ def execute(self, prompt, stream, response, conversation=None):\n                 messages.append(prev_response.prompt.prompt)\n                 messages.append(prev_response.text())\n         messages.append(prompt.prompt)\n-        response._prompt_json = {\"messages\": messages}\n         kwargs = self.build_kwargs(prompt)\n         client = self.get_client()\n         if stream:\n@@ -422,6 +458,7 @@ def execute(self, prompt, stream, response, conversation=None):\n             )\n             response.response_json = remove_dict_none_values(completion.model_dump())\n             yield completion.choices[0].text\n+        response._prompt_json = redact_data_urls({\"messages\": messages})\n \n \n def not_nulls(data) -> dict:\n@@ -469,3 +506,25 @@ def combine_chunks(chunks: List) -> dict:\n             combined[key] = value\n \n     return combined\n+\n+\n+def redact_data_urls(input_dict):\n+    \"\"\"\n+    Recursively search through the input dictionary for any 'image_url' keys\n+    and modify the 'url' value to be just 'data:...'.\n+    \"\"\"\n+    if isinstance(input_dict, dict):\n+        for key, value in input_dict.items():\n+            if (\n+                key == \"image_url\"\n+                and isinstance(value, dict)\n+                and \"url\" in value\n+                and value[\"url\"].startswith(\"data:\")\n+            ):\n+                value[\"url\"] = \"data:...\"\n+            else:\n+                redact_data_urls(value)\n+    elif isinstance(input_dict, list):\n+        for item in input_dict:\n+            redact_data_urls(item)\n+    return input_dict\ndiff --git a/llm/migrations.py b/llm/migrations.py\nindex 008ae976..91da6429 100644\n--- a/llm/migrations.py\n+++ b/llm/migrations.py\n@@ -201,3 +201,29 @@ def m010_create_new_log_tables(db):\n @migration\n def m011_fts_for_responses(db):\n     db[\"responses\"].enable_fts([\"prompt\", \"response\"], create_triggers=True)\n+\n+\n+@migration\n+def m012_attachments_tables(db):\n+    db[\"attachments\"].create(\n+        {\n+            \"id\": str,\n+            \"type\": str,\n+            \"path\": str,\n+            \"url\": str,\n+            \"content\": bytes,\n+        },\n+        pk=\"id\",\n+    )\n+    db[\"prompt_attachments\"].create(\n+        {\n+            \"response_id\": str,\n+            \"attachment_id\": str,\n+            \"order\": int,\n+        },\n+        foreign_keys=(\n+            (\"response_id\", \"responses\", \"id\"),\n+            (\"attachment_id\", \"attachments\", \"id\"),\n+        ),\n+        pk=(\"response_id\", \"attachment_id\"),\n+    )\ndiff --git a/llm/models.py b/llm/models.py\nindex 0e47bb60..6a602f52 100644\n--- a/llm/models.py\n+++ b/llm/models.py\n@@ -1,7 +1,11 @@\n+import base64\n from dataclasses import dataclass, field\n import datetime\n from .errors import NeedsKeyException\n+import hashlib\n+import httpx\n from itertools import islice\n+import puremagic\n import re\n import time\n from typing import Any, Dict, Iterable, Iterator, List, Optional, Set, Union\n@@ -13,17 +17,88 @@\n CONVERSATION_NAME_LENGTH = 32\n \n \n+@dataclass\n+class Attachment:\n+    type: Optional[str] = None\n+    path: Optional[str] = None\n+    url: Optional[str] = None\n+    content: Optional[bytes] = None\n+    _id: Optional[str] = None\n+\n+    def id(self):\n+        # Hash of the binary content, or of '{\"url\": \"https://...\"}' for URL attachments\n+        if self._id is None:\n+            if self.content:\n+                self._id = hashlib.sha256(self.content).hexdigest()\n+            elif self.path:\n+                self._id = hashlib.sha256(open(self.path, \"rb\").read()).hexdigest()\n+            else:\n+                self._id = hashlib.sha256(\n+                    json.dumps({\"url\": self.url}).encode(\"utf-8\")\n+                ).hexdigest()\n+        return self._id\n+\n+    def resolve_type(self):\n+        if self.type:\n+            return self.type\n+        # Derive it from path or url or content\n+        if self.path:\n+            return puremagic.from_file(self.path, mime=True)\n+        if self.url:\n+            response = httpx.head(self.url)\n+            response.raise_for_status()\n+            return response.headers.get(\"content-type\")\n+        if self.content:\n+            return puremagic.from_string(self.content, mime=True)\n+        raise ValueError(\"Attachment has no type and no content to derive it from\")\n+\n+    def content_bytes(self):\n+        content = self.content\n+        if not content:\n+            if self.path:\n+                content = open(self.path, \"rb\").read()\n+            elif self.url:\n+                response = httpx.get(self.url)\n+                response.raise_for_status()\n+                content = response.content\n+        return content\n+\n+    def base64_content(self):\n+        return base64.b64encode(self.content_bytes()).decode(\"utf-8\")\n+\n+    @classmethod\n+    def from_row(cls, row):\n+        return cls(\n+            _id=row[\"id\"],\n+            type=row[\"type\"],\n+            path=row[\"path\"],\n+            url=row[\"url\"],\n+            content=row[\"content\"],\n+        )\n+\n+\n @dataclass\n class Prompt:\n     prompt: str\n     model: \"Model\"\n+    attachments: Optional[List[Attachment]]\n     system: Optional[str]\n     prompt_json: Optional[str]\n     options: \"Options\"\n \n-    def __init__(self, prompt, model, system=None, prompt_json=None, options=None):\n+    def __init__(\n+        self,\n+        prompt,\n+        model,\n+        *,\n+        attachments=None,\n+        system=None,\n+        prompt_json=None,\n+        options=None\n+    ):\n         self.prompt = prompt\n         self.model = model\n+        self.attachments = list(attachments or [])\n         self.system = system\n         self.prompt_json = prompt_json\n         self.options = options or {}\n@@ -39,6 +114,8 @@ class Conversation:\n     def prompt(\n         self,\n         prompt: Optional[str],\n+        *,\n+        attachments: Optional[List[Attachment]] = None,\n         system: Optional[str] = None,\n         stream: bool = True,\n         **options\n@@ -46,8 +123,9 @@ def prompt(\n         return Response(\n             Prompt(\n                 prompt,\n-                system=system,\n                 model=self.model,\n+                attachments=attachments,\n+                system=system,\n                 options=self.model.Options(**options),\n             ),\n             self.model,\n@@ -138,8 +216,9 @@ def log_to_db(self, db):\n             },\n             ignore=True,\n         )\n+        response_id = str(ULID()).lower()\n         response = {\n-            \"id\": str(ULID()).lower(),\n+            \"id\": response_id,\n             \"model\": self.model.model_id,\n             \"prompt\": self.prompt.prompt,\n             \"system\": self.prompt.system,\n@@ -156,16 +235,44 @@ def log_to_db(self, db):\n             \"datetime_utc\": self.datetime_utc(),\n         }\n         db[\"responses\"].insert(response)\n+        # Persist any attachments - loop through with index\n+        for index, attachment in enumerate(self.prompt.attachments):\n+            attachment_id = attachment.id()\n+            db[\"attachments\"].insert(\n+                {\n+                    \"id\": attachment_id,\n+                    \"type\": attachment.resolve_type(),\n+                    \"path\": attachment.path,\n+                    \"url\": attachment.url,\n+                    \"content\": attachment.content,\n+                },\n+                replace=True,\n+            )\n+            db[\"prompt_attachments\"].insert(\n+                {\n+                    \"response_id\": response_id,\n+                    \"attachment_id\": attachment_id,\n+                    \"order\": index,\n+                },\n+            )\n \n     @classmethod\n-    def fake(cls, model: \"Model\", prompt: str, system: str, response: str):\n+    def fake(\n+        cls,\n+        model: \"Model\",\n+        prompt: str,\n+        *attachments: List[Attachment],\n+        system: str,\n+        response: str\n+    ):\n         \"Utility method to help with writing tests\"\n         response_obj = cls(\n             model=model,\n             prompt=Prompt(\n                 prompt,\n-                system=system,\n                 model=model,\n+                attachments=attachments,\n+                system=system,\n             ),\n             stream=False,\n         )\n@@ -174,7 +281,7 @@ def fake(cls, model: \"Model\", prompt: str, system: str, response: str):\n         return response_obj\n \n     @classmethod\n-    def from_row(cls, row):\n+    def from_row(cls, db, row):\n         from llm import get_model\n \n         model = get_model(row[\"model\"])\n@@ -183,8 +290,9 @@ def from_row(cls, row):\n             model=model,\n             prompt=Prompt(\n                 prompt=row[\"prompt\"],\n-                system=row[\"system\"],\n                 model=model,\n+                attachments=[],\n+                system=row[\"system\"],\n                 options=model.Options(**json.loads(row[\"options_json\"])),\n             ),\n             stream=False,\n@@ -194,6 +302,19 @@ def from_row(cls, row):\n         response.response_json = json.loads(row[\"response_json\"] or \"null\")\n         response._done = True\n         response._chunks = [row[\"response\"]]\n+        # Attachments\n+        response.attachments = [\n+            Attachment.from_row(arow)\n+            for arow in db.query(\n+                \"\"\"\n+                select attachments.* from attachments\n+                join prompt_attachments on attachments.id = prompt_attachments.attachment_id\n+                where prompt_attachments.response_id = ?\n+                order by prompt_attachments.\"order\"\n+            \"\"\",\n+                [row[\"id\"]],\n+            )\n+        ]\n         return response\n \n     def __repr__(self):\n@@ -242,10 +363,15 @@ def get_key(self):\n \n class Model(ABC, _get_key_mixin):\n     model_id: str\n+\n+    # API key handling\n     key: Optional[str] = None\n     needs_key: Optional[str] = None\n     key_env_var: Optional[str] = None\n+\n+    # Model characteristics\n     can_stream: bool = False\n+    attachment_types: Set = set()\n \n     class Options(_Options):\n         pass\n@@ -269,13 +395,34 @@ def execute(\n \n     def prompt(\n         self,\n-        prompt: Optional[str],\n+        prompt: str,\n+        *,\n+        attachments: Optional[List[Attachment]] = None,\n         system: Optional[str] = None,\n         stream: bool = True,\n         **options\n     ):\n+        # Validate attachments\n+        if attachments and not self.attachment_types:\n+            raise ValueError(\n+                \"This model does not support attachments, but some were provided\"\n+            )\n+        for attachment in attachments or []:\n+            attachment_type = attachment.resolve_type()\n+            if attachment_type not in self.attachment_types:\n+                raise ValueError(\n+                    \"This model does not support attachments of type '{}', only {}\".format(\n+                        attachment_type, \", \".join(self.attachment_types)\n+                    )\n+                )\n         return self.response(\n-            Prompt(prompt, system=system, model=self, options=self.Options(**options)),\n+            Prompt(\n+                prompt,\n+                attachments=attachments,\n+                system=system,\n+                model=self,\n+                options=self.Options(**options),\n+            ),\n             stream=stream,\n         )\n \ndiff --git a/setup.py b/setup.py\nindex bc77c22b..4c77a9e3 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -48,12 +48,13 @@ def get_long_description():\n         \"setuptools\",\n         \"pip\",\n         \"pyreadline3; sys_platform == 'win32'\",\n+        \"puremagic\",\n     ],\n     extras_require={\n         \"test\": [\n             \"pytest\",\n             \"numpy\",\n-            \"pytest-httpx\",\n+            \"pytest-httpx>=0.33.0\",\n             \"cogapp\",\n             \"mypy>=1.10.0\",\n             \"black>=24.1.0\",\n", "instance_id": "simonw__llm-590", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to introduce a new \"attachments\" feature for multi-modal models in the `llm` tool, supporting various file types like images, PDFs, videos, and audio clips. It provides specific examples of command-line usage (e.g., `llm 'describe image' -a image.jpeg`) and outlines a TODO list that covers implementation aspects like CLI integration, database persistence, plugin support for OpenAI and Gemini, and documentation. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define how attachments should be processed or stored in detail (e.g., size limits, supported formats per model), nor does it address potential edge cases like invalid file types or network failures for URL-based attachments. Additionally, while the goal is clear, the expected behavior for async handling and plugin design is only vaguely mentioned (e.g., \"think about how async might work\"), leaving room for interpretation. Overall, the statement is valid and provides a good starting point but lacks some critical details on constraints and edge cases.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules, including CLI modifications (`cli.py`), model handling (`models.py`, `openai_models.py`), database schema updates (`migrations.py`), and extensive documentation updates. This requires a deep understanding of the existing codebase architecture to integrate the new \"attachments\" feature seamlessly. Second, the problem involves multiple technical concepts, such as handling binary data (e.g., reading file content, base64 encoding), MIME type detection (using `puremagic`), HTTP requests for URL-based attachments (using `httpx`), database persistence for conversation continuity, and plugin design to support multi-modal inputs for different AI models (OpenAI, Gemini). Third, the implementation must account for edge cases like invalid file paths, unsupported MIME types, network errors for URLs, and large file handling, which adds complexity to error handling and validation logic. Finally, the changes impact core functionality (e.g., prompt execution, logging) and require careful consideration of backward compatibility and performance (e.g., not fetching content unnecessarily in async contexts). While not at the extreme end of difficulty (e.g., no distributed systems or highly specialized domain knowledge required), this problem demands a solid grasp of Python, file I/O, HTTP handling, and plugin architectures, along with meticulous attention to detail across the codebase, justifying a score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "UTF-8 extended attributes are cut off\n## DESCRIPTION\r\n\r\nWhen using `getxattr` and returning an UTF-8 string, the length is improperly calculated: it uses the length of the string character-wise, but not byte-wise, which results in either incorrectly returned strings.\r\n\r\nThis happened to me when using Japanese characters in the extended attributes. Below I am appending an example of this behavior. For me it happens both on `python-fuse` version 1.0.7 and 1.0.8.\r\n\r\n## EXAMPLE\r\n\r\n```python\r\nimport fuse\r\nimport stat, errno\r\nfrom fuse import Fuse, Stat, Direntry\r\n\r\nfuse.fuse_python_api = (0, 2)\r\n\r\nBROKEN_FILE = '/utf8_attr'\r\nFATTR_NAME = 'user.xdg.comment'\r\nFATTR_VALUE = '\u3042\u3042\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u5207\u308a\u53d6\u3089\u308c\u3066\u3044\u306a\u3044'\r\n\r\nclass EmptyStat(Stat):\r\n  def __init__(self):\r\n    self.st_mode = 0\r\n    self.st_ino = 0\r\n    self.st_dev = 0\r\n    self.st_nlink = 0\r\n    self.st_uid = 0\r\n    self.st_gid = 0\r\n    self.st_size = 0\r\n    self.st_atime = 0\r\n    self.st_mtime = 0\r\n    self.st_ctime = 0\r\n\r\nclass GetAttrBug(Fuse):\r\n  def getattr(self, path):\r\n    ret_stat = EmptyStat()\r\n    if path == '/':\r\n      ret_stat.st_mode = stat.S_IFDIR | int(0e755)\r\n      return ret_stat\r\n\r\n    if path == BROKEN_FILE:\r\n      ret_stat.st_mode = stat.S_IFREG | int(0e000)\r\n      return ret_stat\r\n    \r\n    return -errno.ENOENT\r\n\r\n  def readdir(self, path, offset):\r\n    yield Direntry('.')\r\n    yield Direntry('..')\r\n    yield Direntry(BROKEN_FILE[1:])\r\n\r\n  def open(self, path, flags):\r\n    return -errno.EACCES\r\n\r\n  def read(self, path, size, offset):\r\n    return\r\n\r\n  def listxattr(self, path, size):\r\n    if size == 0: return 1\r\n    else: return [ FATTR_NAME ]\r\n\r\n  def getxattr(self, path, attr, size):\r\n    if size == 0: return len(FATTR_VALUE.encode('utf8'))\r\n    else: return FATTR_VALUE\r\n\r\nif __name__ == '__main__':\r\n  server = GetAttrBug(dash_s_do='setsingle')\r\n  server.parse(errex=1)\r\n  server.main()\r\n```\r\n\r\nIn this example, `FATTR_VALUE` is **18** characters long, but takes up **54** bytes.\r\n\r\nAfter running the example, and using `os.getxattr` to retrieve it back, we'll get a shorter string, **6 characters long** and **18** bytes long.\r\n\r\n```sh\r\n# start the bug example\r\n$ python3 test.py test\r\n\r\n# read the file within the fuse\r\n$ python3 -c 'import os; print(os.getxattr(\"./test/utf8_attr\", \"user.xdg.comment\").decode())'\r\n#   returns: [len=6,  bytelength=18] \"\u3042\u3042\u3001\u30e1\u30c3\u30bb\"\r\n#  expected: [len=18, bytelength=54] \"\u3042\u3042\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u5207\u308a\u53d6\u3089\u308c\u3066\u3044\u306a\u3044\"\r\n```\n", "patch": "diff --git a/example/xattr.py b/example/xattr.py\nnew file mode 100644\nindex 0000000..fe5ffab\n--- /dev/null\n+++ b/example/xattr.py\n@@ -0,0 +1,61 @@\n+# Example by github @vignedev from https://github.com/libfuse/python-fuse/issues/77\n+\n+import fuse\n+import stat, errno\n+from fuse import Fuse, Stat, Direntry\n+\n+fuse.fuse_python_api = (0, 2)\n+\n+BROKEN_FILE = '/utf8_attr'\n+FATTR_NAME = 'user.xdg.comment'\n+FATTR_VALUE = '\u3042\u3042\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u5207\u308a\u53d6\u3089\u308c\u3066\u3044\u306a\u3044'\n+\n+class EmptyStat(Stat):\n+  def __init__(self):\n+    self.st_mode = 0\n+    self.st_ino = 0\n+    self.st_dev = 0\n+    self.st_nlink = 0\n+    self.st_uid = 0\n+    self.st_gid = 0\n+    self.st_size = 0\n+    self.st_atime = 0\n+    self.st_mtime = 0\n+    self.st_ctime = 0\n+\n+class GetAttrBug(Fuse):\n+  def getattr(self, path):\n+    ret_stat = EmptyStat()\n+    if path == '/':\n+      ret_stat.st_mode = stat.S_IFDIR | int(0e755)\n+      return ret_stat\n+\n+    if path == BROKEN_FILE:\n+      ret_stat.st_mode = stat.S_IFREG | int(0e000)\n+      return ret_stat\n+    \n+    return -errno.ENOENT\n+\n+  def readdir(self, path, offset):\n+    yield Direntry('.')\n+    yield Direntry('..')\n+    yield Direntry(BROKEN_FILE[1:])\n+\n+  def open(self, path, flags):\n+    return -errno.EACCES\n+\n+  def read(self, path, size, offset):\n+    return\n+\n+  def listxattr(self, path, size):\n+    if size == 0: return 1\n+    else: return [ FATTR_NAME ]\n+\n+  def getxattr(self, path, attr, size):\n+    if size == 0: return len(FATTR_VALUE.encode('utf8'))\n+    else: return FATTR_VALUE\n+\n+if __name__ == '__main__':\n+  server = GetAttrBug(dash_s_do='setsingle')\n+  server.parse(errex=1)\n+  server.main()\ndiff --git a/example/xmp.py b/example/xmp.py\nindex 26bee73..d99b07e 100644\n--- a/example/xmp.py\n+++ b/example/xmp.py\n@@ -1,6 +1,6 @@\n #!/usr/bin/env python\n \n-#    Copyright (C) 2001  Jeff Epler  <jepler@unpythonic.dhs.org>\n+#    Copyright (C) 2001  Jeff Epler  <jepler@gmail.com>\n #    Copyright (C) 2006  Csaba Henk  <csaba.henk@creo.hu>\n #\n #    This program can be distributed under the terms of the GNU LGPL.\ndiff --git a/fuse.py b/fuse.py\nindex 68d5e50..2b80c09 100644\n--- a/fuse.py\n+++ b/fuse.py\n@@ -1,5 +1,5 @@\n #\n-#    Copyright (C) 2001  Jeff Epler  <jepler@unpythonic.dhs.org>\n+#    Copyright (C) 2001  Jeff Epler  <jepler@gmail.com>\n #    Copyright (C) 2006  Csaba Henk  <csaba.henk@creo.hu>\n #\n #    This program can be distributed under the terms of the GNU LGPL.\ndiff --git a/fuseparts/_fusemodule.c b/fuseparts/_fusemodule.c\nindex 25523af..691b286 100644\n--- a/fuseparts/_fusemodule.c\n+++ b/fuseparts/_fusemodule.c\n@@ -1,5 +1,5 @@\n /*\n-    Copyright (C) 2001  Jeff Epler  <jepler@unpythonic.dhs.org>\n+    Copyright (C) 2001  Jeff Epler  <jepler@gmail.com>\n \n     This program can be distributed under the terms of the GNU LGPL.\n     See the file COPYING.\n@@ -94,12 +94,16 @@\n         PyErr_SetString(PyExc_ValueError, \"non-decodable filename\");\n         return NULL;\n     }\n-\n+    static inline Py_ssize_t PyString_Size(PyObject *s) {\n+        Py_ssize_t result = -1;\n+        (void)PyUnicode_AsUTF8AndSize(s, &result);\n+        return result;\n+    }\n #else\n     #define PyString_AsString PyUnicode_AsUTF8\n+    #define PyString_Size PyUnicode_GET_LENGTH\n #endif\n     #define PyString_Check PyUnicode_Check\n-    #define PyString_Size PyUnicode_GET_LENGTH\n #endif\n \n #ifdef FIX_PATH_DECODING\n@@ -683,14 +687,14 @@ read_func(const char *path, char *buf, size_t s, off_t off)\n \tif(PyObject_CheckBuffer(v)) {\n \t\tPyObject_GetBuffer(v, &buffer, PyBUF_SIMPLE);\n \n-\t\tif(buffer.len <= s) {\n+\t\tif((size_t)buffer.len <= s) {\n \t\t\tmemcpy(buf, buffer.buf, buffer.len);\n \t\t\tret = buffer.len;\n \t\t}\n \n \t\tPyBuffer_Release(&buffer);\n \t} else if(PyBytes_Check(v)) {\n-\t\tif(PyBytes_Size(v) > s)\n+\t\tif((size_t)PyBytes_Size(v) > s)\n \t\t\tgoto OUT_DECREF;\n \t\tmemcpy(buf, PyBytes_AsString(v), PyBytes_Size(v));\n \t\tret = PyBytes_Size(v);\n@@ -944,7 +948,7 @@ getxattr_func(const char *path, const char *name, char *value, size_t size)\n         /* If the size of the value buffer is too small to hold the result,  errno\n          * is set to ERANGE.\n          */\n-\t\tif (PyString_Size(v) > size) {\n+\t\tif ((size_t)PyString_Size(v) > size) {\n             ret = -ERANGE;\n \t\t\tgoto OUT_DECREF;\n         }\n@@ -980,9 +984,9 @@ listxattr_func(const char *path, char *list, size_t size)\n \t}\n \n \tfor (;;) {\n-\t\tint ilen;\n+\t\tsize_t ilen;\n \n-\t        w = PyIter_Next(iter);\n+\t\tw = PyIter_Next(iter);\n \t\tif (!w) {\n \t\t\tret = lx - list;\n \t\t\tbreak;\n@@ -1302,7 +1306,9 @@ pyfuse_loop_mt(struct fuse *f)\n #ifdef WITH_THREAD\n \tPyThreadState *save;\n \n+#if PY_VERSION_HEX < 0x03070000\n \tPyEval_InitThreads();\n+#endif\n \tinterp = PyThreadState_Get()->interp;\n \tsave = PyEval_SaveThread();\n \terr = fuse_loop_mt(f);\ndiff --git a/setup.py b/setup.py\nindex 4916579..64ad5a8 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -103,7 +103,7 @@ def setup(**kwargs):\n        package_data={'': ['COPYING', 'AUTHORS', 'FAQ', 'INSTALL',\n                           'README.md', 'README.new_fusepy_api.rst',\n                           'README.package_maintainers.rst']},\n-       author = 'Jeff Epler <jepler@unpythonic.dhs.org>, Csaba Henk <csaba.henk@creo.hu>, Steven James, Miklos Szeredi <miklos@szeredi.hu>, S\u00e9bastien Delafond<sdelafond@gmail.com>',\n+       author = 'Csaba Henk <csaba.henk@creo.hu>, Steven James, Miklos Szeredi <miklos@szeredi.hu>, S\u00e9bastien Delafond<sdelafond@gmail.com>',\n        maintainer = 'S\u00e9bastien Delafond',\n        maintainer_email = 'sdelafond@gmail.com',\n        ext_modules = [fusemodule],\n", "instance_id": "libfuse__python-fuse-79", "clarity": 3, "difficulty": 0.55, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly describes the issue with UTF-8 extended attributes being cut off due to improper length calculation (character-wise instead of byte-wise) in the `getxattr` function. The goal is evident: fix the length calculation to handle UTF-8 strings correctly. The input, output, and constraints are implicitly clear through the provided example code and the expected versus actual output. The example script demonstrates the issue with Japanese characters, and the shell commands show how to reproduce the problem, making it easy to understand the context and impact. There are no significant ambiguities, and the inclusion of a detailed reproduction script adds to the clarity. All critical details are present, including the specific library versions affected (python-fuse 1.0.7 and 1.0.8). Therefore, this problem statement earns a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem falls into the medium range, and I assign it a score of 0.55. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes involve multiple files, including the core C module (`_fusemodule.c`), where the primary fix for UTF-8 string length calculation is implemented, and additional minor updates (e.g., copyright notices in other files). The main fix requires modifying how string sizes are handled in the `getxattr`, `read`, and `listxattr` functions to account for byte length rather than character count for UTF-8 strings. While the changes are not extensive in terms of lines of code, they impact a critical part of the library (the interface between Python and the FUSE C library), requiring careful handling to avoid breaking existing functionality. Additionally, a new example file (`xattr.py`) is added to demonstrate the issue, but this does not significantly contribute to the difficulty of the fix itself. The changes do not appear to impact the overall system architecture but do require understanding the interaction between Python's Unicode handling and C-level buffer management.\n\n2. **Number of Technical Concepts:** Solving this problem requires understanding several technical concepts, including UTF-8 encoding and byte-length versus character-length calculations, Python's Unicode handling (especially differences across Python versions, as seen in the conditional compilation for `PyString_Size`), and the FUSE library's interaction with Python through C bindings. Familiarity with Python's C API (e.g., `PyUnicode_AsUTF8AndSize`, `PyBytes_Size`) and buffer protocols is necessary. Additionally, knowledge of how extended attributes (`xattr`) work in filesystems and how FUSE exposes them is required. While these concepts are not extremely advanced, they do require a moderate level of expertise in systems programming and Python-C interoperability.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement highlights a specific edge case: UTF-8 strings with multi-byte characters (e.g., Japanese characters). The fix must ensure that such strings are handled correctly in terms of byte length. The code changes also address buffer size checks to prevent overflows or incorrect truncation, as seen in the casting of sizes to `size_t` and comparisons in `getxattr` and `read` functions. While the edge cases are not numerous, handling multi-byte UTF-8 strings correctly across different Python versions adds some complexity. Error handling logic (e.g., setting `ERANGE` when the buffer is too small) is already present and modified slightly, which is not overly challenging but requires attention to detail.\n\n4. **Overall Complexity:** The problem requires a moderate level of understanding of the codebase, particularly the C module interfacing with Python. The fix is not trivial, as it involves low-level string handling and ensuring compatibility with different Python versions. However, it does not require deep architectural changes or advanced algorithms, nor does it involve complex domain-specific knowledge beyond filesystem attributes and UTF-8 encoding. The impact of the change is significant for users dealing with non-ASCII extended attributes, but the implementation complexity is manageable for someone with experience in systems programming.\n\nGiven these factors, the difficulty score of 0.55 reflects a medium-level challenge. It is more complex than a simple bug fix (e.g., changing a constant or basic logic) due to the need to handle UTF-8 encoding correctly and modify C code, but it is not as hard as refactoring core components or implementing intricate algorithms. It sits slightly above the midpoint of the medium range due to the cross-language (Python-C) nature of the fix and the importance of getting string handling right to avoid subtle bugs.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Miss pk identify in some cases in GOpt\n**Describe the bug**\r\n\r\nCurrently, in the GOpt-based compilation, the following query (on LDBC graph with \"Person\"'s \"id\" as pk) missed pk optimization in the plan:\r\n```\r\ng.V().has(\"PERSON\", \"id\", 19791209300143)\r\n```\r\nThe plan looks like:\r\n```\r\n  \"plan\": [{\r\n    \"opr\": {\r\n      \"scan\": {\r\n        \"params\": {\r\n          \"tables\": [{\r\n            \"id\": 1\r\n          }],\r\n          \"predicate\": {\r\n            \"operators\": [{\r\n              \"var\": {\r\n                \"property\": {\r\n                  \"key\": {\r\n                    \"name\": \"id\"\r\n                  }\r\n                },\r\n                \"nodeType\": {\r\n                  \"dataType\": \"INT64\"\r\n                }\r\n              },\r\n              \"nodeType\": {\r\n                \"dataType\": \"INT64\"\r\n              }\r\n            }, {\r\n              \"logical\": \"EQ\",\r\n              \"nodeType\": {\r\n                \"dataType\": \"BOOLEAN\"\r\n              }\r\n            }, {\r\n              \"const\": {\r\n                \"i64\": \"19791209300143\"\r\n              },\r\n              \"nodeType\": {\r\n                \"dataType\": \"INT64\"\r\n              }\r\n            }]\r\n          },\r\n          \"sampleRatio\": 1.0\r\n        }\r\n      }\r\n    },\r\n...\r\n}\r\n```\r\n\r\nWhile the following query can correctly identify pk:\r\n```\r\ng.V().hasLabel(\"PERSON\").has(\"id\", 19791209300143)\r\n```\r\nThe plan looks like:\r\n```\r\n  \"plan\": [{\r\n    \"opr\": {\r\n      \"scan\": {\r\n        \"params\": {\r\n          \"tables\": [{\r\n            \"id\": 1\r\n          }],\r\n          \"sampleRatio\": 1.0\r\n        },\r\n        \"idxPredicate\": {\r\n          \"orPredicates\": [{\r\n            \"predicates\": [{\r\n              \"key\": {\r\n                \"key\": {\r\n                  \"name\": \"id\"\r\n                }\r\n              },\r\n              \"const\": {\r\n                \"i64\": \"19791209300143\"\r\n              }\r\n            }]\r\n          }]\r\n        }\r\n      }\r\n    },\r\n...\r\n}\r\n```\r\n\n", "patch": "diff --git a/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/ClassifiedFilter.java b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/ClassifiedFilter.java\nnew file mode 100644\nindex 000000000000..ea6dde0057fe\n--- /dev/null\n+++ b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/ClassifiedFilter.java\n@@ -0,0 +1,58 @@\n+/*\n+ *\n+ *  * Copyright 2020 Alibaba Group Holding Limited.\n+ *  *\n+ *  * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  * you may not use this file except in compliance with the License.\n+ *  * You may obtain a copy of the License at\n+ *  *\n+ *  * http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package com.alibaba.graphscope.common.ir.rex;\n+\n+import org.apache.calcite.rex.RexNode;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class ClassifiedFilter {\n+    private final List<RexNode> labelFilters;\n+    private final List<Comparable> labelValues;\n+    private final List<RexNode> uniqueKeyFilters;\n+    private final List<RexNode> extraFilters;\n+\n+    public ClassifiedFilter(\n+            List<RexNode> labelFilters,\n+            List<Comparable> labelValues,\n+            List<RexNode> uniqueKeyFilters,\n+            List<RexNode> extraFilters) {\n+        this.labelFilters = labelFilters;\n+        this.labelValues = labelValues;\n+        this.uniqueKeyFilters = uniqueKeyFilters;\n+        this.extraFilters = extraFilters;\n+    }\n+\n+    public List<RexNode> getLabelFilters() {\n+        return Collections.unmodifiableList(labelFilters);\n+    }\n+\n+    public List<Comparable> getLabelValues() {\n+        return Collections.unmodifiableList(labelValues);\n+    }\n+\n+    public List<RexNode> getUniqueKeyFilters() {\n+        return Collections.unmodifiableList(uniqueKeyFilters);\n+    }\n+\n+    public List<RexNode> getExtraFilters() {\n+        return Collections.unmodifiableList(extraFilters);\n+    }\n+}\ndiff --git a/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/RexFilterClassifier.java b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/RexFilterClassifier.java\nnew file mode 100644\nindex 000000000000..4c268ff1e490\n--- /dev/null\n+++ b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/rex/RexFilterClassifier.java\n@@ -0,0 +1,430 @@\n+/*\n+ *\n+ *  * Copyright 2020 Alibaba Group Holding Limited.\n+ *  *\n+ *  * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  * you may not use this file except in compliance with the License.\n+ *  * You may obtain a copy of the License at\n+ *  *\n+ *  * http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package com.alibaba.graphscope.common.ir.rex;\n+\n+import com.alibaba.graphscope.common.ir.rel.graph.AbstractBindableTableScan;\n+import com.alibaba.graphscope.common.ir.rel.graph.GraphLogicalSource;\n+import com.alibaba.graphscope.common.ir.rel.type.TableConfig;\n+import com.alibaba.graphscope.common.ir.tools.GraphBuilder;\n+import com.alibaba.graphscope.common.ir.tools.Utils;\n+import com.alibaba.graphscope.common.ir.type.GraphLabelType;\n+import com.alibaba.graphscope.common.ir.type.GraphNameOrId;\n+import com.alibaba.graphscope.common.ir.type.GraphProperty;\n+import com.alibaba.graphscope.common.ir.type.GraphSchemaType;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.*;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlOperator;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.apache.calcite.util.Sarg;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.javatuples.Pair;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * We use this class to decouple a complex condition that contains nested {@code OR} and {@code AND} operators into different filtering conditions.\n+ * There are mainly three types of filtering:\n+ * 1) filtering based on labels;\n+ * 2) based on the scan's unique key;\n+ * 3) querying other properties.\n+ *\n+ * For example: for the condition _.~label = 'person' AND _.~id = 1 AND _.name = 'marko',\n+ * _.~label = 'person', _.~id = 1, and _.name = 'marko' would be decoupled into three different {@code RexNode} structures.\n+ */\n+public class RexFilterClassifier extends RexVisitorImpl<RexFilterClassifier.Filter> {\n+    private final GraphBuilder builder;\n+    private final @Nullable AbstractBindableTableScan tableScan;\n+\n+    public RexFilterClassifier(\n+            GraphBuilder builder, @Nullable AbstractBindableTableScan tableScan) {\n+        super(true);\n+        this.builder = builder;\n+        this.tableScan = tableScan;\n+    }\n+\n+    public ClassifiedFilter classify(RexNode condition) {\n+        Filter filter = condition.accept(this);\n+        List<RexNode> labelFilters = Lists.newArrayList();\n+        List<RexNode> uniqueKeyFilters = Lists.newArrayList();\n+        filter.getSchemaFilters()\n+                .forEach(\n+                        k -> {\n+                            switch (k.getSchemaType()) {\n+                                case LABEL:\n+                                    labelFilters.add(k.getFilter());\n+                                    break;\n+                                case UNIQUE_KEY:\n+                                default:\n+                                    uniqueKeyFilters.add(k.getFilter());\n+                            }\n+                        });\n+        List<RexNode> extraFilters = Lists.newArrayList();\n+        if (filter.getOtherFilter() != null) {\n+            extraFilters.add(filter.getOtherFilter());\n+        }\n+        List<Comparable> labelValues = Lists.newArrayList();\n+        labelFilters.forEach(k -> labelValues.addAll(getLabelValues(k)));\n+        return new ClassifiedFilter(labelFilters, labelValues, uniqueKeyFilters, extraFilters);\n+    }\n+\n+    private List<Comparable> getLabelValues(RexNode labelFilter) {\n+        return labelFilter.accept(new LabelValueCollector());\n+    }\n+\n+    @Override\n+    public Filter visitCall(RexCall call) {\n+        SqlOperator operator = call.getOperator();\n+        List<RexNode> operands = call.getOperands();\n+        switch (operator.getKind()) {\n+            case AND:\n+                return conjunctions(call, visitList(operands));\n+            case OR:\n+                return disjunctions(call);\n+            case EQUALS:\n+            case SEARCH:\n+                RexVariableAliasCollector<Integer> aliasCollector =\n+                        new RexVariableAliasCollector<>(\n+                                true, (RexGraphVariable var) -> var.getAliasId());\n+                if (isLabelEqualFilter(call)) {\n+                    Integer tagId = call.accept(aliasCollector).get(0);\n+                    return new Filter(\n+                            ImmutableList.of(\n+                                    new Filter.SchemaFilter(tagId, call, Filter.SchemaType.LABEL)),\n+                            null);\n+                } else if (tableScan != null && isUniqueKeyEqualFilter(call)) {\n+                    Integer tagId = call.accept(aliasCollector).get(0);\n+                    return new Filter(\n+                            ImmutableList.of(\n+                                    new Filter.SchemaFilter(\n+                                            tagId, call, Filter.SchemaType.UNIQUE_KEY)),\n+                            null);\n+                }\n+            default:\n+                return new Filter(ImmutableList.of(), call);\n+        }\n+    }\n+\n+    private Filter conjunctions(RexNode original, List<Filter> filters) {\n+        Map<Pair<Integer, Filter.SchemaType>, RexNode> schemaFilterMap = Maps.newLinkedHashMap();\n+        List<RexNode> otherFilters = Lists.newArrayList();\n+        filters.forEach(\n+                k -> {\n+                    k.getSchemaFilters()\n+                            .forEach(\n+                                    v -> {\n+                                        Pair<Integer, Filter.SchemaType> key =\n+                                                Pair.with(v.getTagId(), v.getSchemaType());\n+                                        RexNode filtering = v.getFilter();\n+                                        RexNode existing = schemaFilterMap.get(key);\n+                                        if (existing != null) {\n+                                            filtering = builder.and(existing, v.getFilter());\n+                                        }\n+                                        if (!filtering.equals(existing)) {\n+                                            schemaFilterMap.put(key, filtering);\n+                                        }\n+                                    });\n+                    if (k.getOtherFilter() != null) {\n+                        otherFilters.add(k.getOtherFilter());\n+                    }\n+                });\n+        List<Filter.SchemaFilter> andSchemaFilters = Lists.newArrayList();\n+        schemaFilterMap.forEach(\n+                (k, v) -> {\n+                    andSchemaFilters.add(new Filter.SchemaFilter(k.getValue0(), v, k.getValue1()));\n+                });\n+        RexNode otherFilter =\n+                otherFilters.isEmpty()\n+                        ? null\n+                        : RexUtil.composeConjunction(builder.getRexBuilder(), otherFilters, false);\n+        return new Filter(andSchemaFilters, otherFilter);\n+    }\n+\n+    private Filter disjunctions(RexCall original) {\n+        SqlOperator operator = original.getOperator();\n+        switch (operator.getKind()) {\n+            case OR:\n+                List<RexNode> operands = original.getOperands();\n+                Filter.SchemaFilter schemaFilter = null;\n+                for (RexNode operand : operands) {\n+                    if (operand.getKind() != SqlKind.EQUALS\n+                            && operand.getKind() != SqlKind.SEARCH) {\n+                        return new Filter(ImmutableList.of(), original);\n+                    }\n+                    List<Filter.SchemaFilter> curFilters = operand.accept(this).getSchemaFilters();\n+                    if (curFilters.size() != 1) {\n+                        return new Filter(ImmutableList.of(), original);\n+                    }\n+                    Filter.SchemaFilter cur = curFilters.get(0);\n+                    if (schemaFilter == null) {\n+                        schemaFilter = cur;\n+                    } else {\n+                        if (schemaFilter.getTagId() == cur.getTagId()\n+                                && schemaFilter.getSchemaType() == cur.getSchemaType()) {\n+                            schemaFilter =\n+                                    new Filter.SchemaFilter(\n+                                            schemaFilter.getTagId(),\n+                                            builder.or(schemaFilter.getFilter(), cur.getFilter()),\n+                                            schemaFilter.getSchemaType());\n+                        } else {\n+                            return new Filter(ImmutableList.of(), original);\n+                        }\n+                    }\n+                }\n+            default:\n+                return new Filter(ImmutableList.of(), original);\n+        }\n+    }\n+\n+    // check the condition if it is the pattern of label equal filter, i.e. ~label = 'person' or\n+    // ~label within ['person', 'software']\n+    // if it is then return the literal containing label values, otherwise null\n+    private boolean isLabelEqualFilter(RexCall rexCall) {\n+        return isLabelEqualFilter0(rexCall) != null;\n+    }\n+\n+    // check the condition if it is the pattern of label equal filter, i.e. ~label = 'person' or\n+    // ~label within ['person', 'software']\n+    // if it is then return the literal containing label values, otherwise null\n+    private static @Nullable RexLiteral isLabelEqualFilter0(RexNode condition) {\n+        if (condition instanceof RexCall) {\n+            RexCall rexCall = (RexCall) condition;\n+            SqlOperator operator = rexCall.getOperator();\n+            switch (operator.getKind()) {\n+                case EQUALS:\n+                case SEARCH:\n+                    RexNode left = rexCall.getOperands().get(0);\n+                    RexNode right = rexCall.getOperands().get(1);\n+                    if (left.getType() instanceof GraphLabelType && right instanceof RexLiteral) {\n+                        Comparable value = ((RexLiteral) right).getValue();\n+                        // if Sarg is a continuous range then the filter is not the 'equal', i.e.\n+                        // ~label SEARCH [[1, 10]] which means ~label >= 1 and ~label <= 10\n+                        if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n+                            return null;\n+                        }\n+                        return (RexLiteral) right;\n+                    } else if (right.getType() instanceof GraphLabelType\n+                            && left instanceof RexLiteral) {\n+                        Comparable value = ((RexLiteral) left).getValue();\n+                        if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n+                            return null;\n+                        }\n+                        return (RexLiteral) left;\n+                    }\n+                default:\n+                    return null;\n+            }\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    private boolean isUniqueKeyEqualFilter(RexNode condition) {\n+        if (!(tableScan instanceof GraphLogicalSource)) return false;\n+        if (condition instanceof RexCall) {\n+            RexCall rexCall = (RexCall) condition;\n+            SqlOperator operator = rexCall.getOperator();\n+            switch (operator.getKind()) {\n+                case EQUALS:\n+                case SEARCH:\n+                    RexNode left = rexCall.getOperands().get(0);\n+                    RexNode right = rexCall.getOperands().get(1);\n+                    if (isUniqueKey(left, tableScan) && isLiteralOrDynamicParams(right)) {\n+                        if (right instanceof RexLiteral) {\n+                            Comparable value = ((RexLiteral) right).getValue();\n+                            // if Sarg is a continuous range then the filter is not the 'equal',\n+                            // i.e. ~id SEARCH [[1, 10]] which means ~id >= 1 and ~id <= 10\n+                            if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n+                                return false;\n+                            }\n+                        }\n+                        return true;\n+                    } else if (isUniqueKey(right, tableScan) && isLiteralOrDynamicParams(left)) {\n+                        if (left instanceof RexLiteral) {\n+                            Comparable value = ((RexLiteral) left).getValue();\n+                            if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n+                                return false;\n+                            }\n+                        }\n+                        return true;\n+                    }\n+                default:\n+                    return false;\n+            }\n+        } else {\n+            return false;\n+        }\n+    }\n+\n+    private boolean isUniqueKey(RexNode rexNode, RelNode tableScan) {\n+        if (rexNode instanceof RexGraphVariable) {\n+            return isUniqueKey((RexGraphVariable) rexNode, tableScan);\n+        }\n+        return false;\n+    }\n+\n+    private boolean isUniqueKey(RexGraphVariable var, RelNode tableScan) {\n+        if (var.getProperty() == null) return false;\n+        switch (var.getProperty().getOpt()) {\n+            case ID:\n+                return true;\n+            case KEY:\n+                GraphSchemaType schemaType =\n+                        (GraphSchemaType) tableScan.getRowType().getFieldList().get(0).getType();\n+                ImmutableBitSet propertyIds = getPropertyIds(var.getProperty(), schemaType);\n+                TableConfig tableConfig = ((AbstractBindableTableScan) tableScan).getTableConfig();\n+                if (!propertyIds.isEmpty()\n+                        && tableConfig.getTables().stream().allMatch(k -> k.isKey(propertyIds))) {\n+                    return true;\n+                }\n+            case LABEL:\n+            case ALL:\n+            case LEN:\n+            default:\n+                return false;\n+        }\n+    }\n+\n+    private ImmutableBitSet getPropertyIds(GraphProperty property, GraphSchemaType schemaType) {\n+        if (property.getOpt() != GraphProperty.Opt.KEY) return ImmutableBitSet.of();\n+        GraphNameOrId key = property.getKey();\n+        if (key.getOpt() == GraphNameOrId.Opt.ID) {\n+            return ImmutableBitSet.of(key.getId());\n+        }\n+        for (int i = 0; i < schemaType.getFieldList().size(); ++i) {\n+            RelDataTypeField field = schemaType.getFieldList().get(i);\n+            if (field.getName().equals(key.getName())) {\n+                return ImmutableBitSet.of(i);\n+            }\n+        }\n+        return ImmutableBitSet.of();\n+    }\n+\n+    private boolean isLiteralOrDynamicParams(RexNode node) {\n+        return node instanceof RexLiteral || node instanceof RexDynamicParam;\n+    }\n+\n+    private static class LabelValueCollector extends RexVisitorImpl<List<Comparable>> {\n+        public LabelValueCollector() {\n+            super(true);\n+        }\n+\n+        @Override\n+        public List<Comparable> visitCall(RexCall call) {\n+            SqlOperator operator = call.getOperator();\n+            switch (operator.getKind()) {\n+                case AND:\n+                    List<Comparable> andLabels = Lists.newArrayList();\n+                    call.getOperands()\n+                            .forEach(\n+                                    k -> {\n+                                        List<Comparable> cur = k.accept(this);\n+                                        if (andLabels.isEmpty()) {\n+                                            andLabels.addAll(cur);\n+                                        } else {\n+                                            andLabels.retainAll(cur);\n+                                        }\n+                                        if (andLabels.isEmpty()) {\n+                                            throw new IllegalArgumentException(\n+                                                    \"cannot find common labels between values=\"\n+                                                            + andLabels\n+                                                            + \" and values=\"\n+                                                            + cur);\n+                                        }\n+                                    });\n+                case OR:\n+                    List<Comparable> orLabels = Lists.newArrayList();\n+                    call.getOperands()\n+                            .forEach(\n+                                    k -> {\n+                                        orLabels.addAll(k.accept(this));\n+                                    });\n+                    return orLabels.stream().distinct().collect(Collectors.toList());\n+                case EQUALS:\n+                case SEARCH:\n+                    RexLiteral labelLiteral = isLabelEqualFilter0(call);\n+                    if (labelLiteral != null) {\n+                        return Utils.getValuesAsList(labelLiteral.getValueAs(Comparable.class));\n+                    }\n+                default:\n+                    return ImmutableList.of();\n+            }\n+        }\n+    }\n+\n+    // Here we further differentiate the filter conditions for different tags to prevent the\n+    // grouping of label conditions from various tags.\n+    // For example, _.~label == 'person' OR _.~label = 'software' can be organized into _.~label IN\n+    // ['person', 'software'],\n+    // but a.~label = 'person' OR b.~label = 'software' cannot be.\n+    public static class Filter {\n+        private final List<SchemaFilter> schemaFilters;\n+        private final @Nullable RexNode otherFilter;\n+\n+        public Filter(List<SchemaFilter> schemaFilters, RexNode otherFilter) {\n+            this.schemaFilters = schemaFilters;\n+            this.otherFilter = otherFilter;\n+        }\n+\n+        public List<SchemaFilter> getSchemaFilters() {\n+            return Collections.unmodifiableList(schemaFilters);\n+        }\n+\n+        public @Nullable RexNode getOtherFilter() {\n+            return otherFilter;\n+        }\n+\n+        public static class SchemaFilter {\n+            private final Integer tagId;\n+            private final RexNode filter;\n+            private final SchemaType schemaType;\n+\n+            public SchemaFilter(Integer tagId, RexNode filtering, SchemaType schemaType) {\n+                this.tagId = tagId;\n+                this.filter = filtering;\n+                this.schemaType = schemaType;\n+            }\n+\n+            public Integer getTagId() {\n+                return tagId;\n+            }\n+\n+            public RexNode getFilter() {\n+                return filter;\n+            }\n+\n+            public SchemaType getSchemaType() {\n+                return schemaType;\n+            }\n+        }\n+\n+        public enum SchemaType {\n+            LABEL,\n+            UNIQUE_KEY\n+        }\n+    }\n+}\ndiff --git a/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/tools/GraphBuilder.java b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/tools/GraphBuilder.java\nindex 8c83cb7adf47..56b68bacd9ce 100644\n--- a/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/tools/GraphBuilder.java\n+++ b/interactive_engine/compiler/src/main/java/com/alibaba/graphscope/common/ir/tools/GraphBuilder.java\n@@ -59,10 +59,8 @@\n import org.apache.calcite.sql.type.IntervalSqlType;\n import org.apache.calcite.sql.type.SqlTypeName;\n import org.apache.calcite.tools.RelBuilder;\n-import org.apache.calcite.util.ImmutableBitSet;\n import org.apache.calcite.util.Litmus;\n import org.apache.calcite.util.Pair;\n-import org.apache.calcite.util.Sarg;\n import org.apache.commons.lang3.ObjectUtils;\n import org.checkerframework.checker.nullness.qual.Nullable;\n \n@@ -932,10 +930,11 @@ public GraphBuilder filter(Iterable<? extends RexNode> conditions) {\n \n     private AbstractBindableTableScan fuseFilters(\n             AbstractBindableTableScan tableScan, RexNode condition, GraphBuilder builder) {\n-        List<Comparable> labelValues = Lists.newArrayList();\n-        List<RexNode> uniqueKeyFilters = Lists.newArrayList();\n-        List<RexNode> extraFilters = Lists.newArrayList();\n-        classifyFilters(tableScan, condition, labelValues, uniqueKeyFilters, extraFilters);\n+        RexFilterClassifier classifier = new RexFilterClassifier(builder, tableScan);\n+        ClassifiedFilter filterResult = classifier.classify(condition);\n+        List<Comparable> labelValues = filterResult.getLabelValues();\n+        List<RexNode> uniqueKeyFilters = Lists.newArrayList(filterResult.getUniqueKeyFilters());\n+        List<RexNode> extraFilters = Lists.newArrayList(filterResult.getExtraFilters());\n         if (!labelValues.isEmpty()) {\n             GraphLabelType labelType =\n                     ((GraphSchemaType) tableScan.getRowType().getFieldList().get(0).getType())\n@@ -983,6 +982,10 @@ private AbstractBindableTableScan fuseFilters(\n                             originalUniqueKeyFilters.accept(propertyChecker);\n                             builder.filter(originalUniqueKeyFilters);\n                         }\n+                        if (!uniqueKeyFilters.isEmpty()) {\n+                            builder.filter(uniqueKeyFilters);\n+                            uniqueKeyFilters.clear();\n+                        }\n                     }\n                     ImmutableList originalFilters = tableScan.getFilters();\n                     if (ObjectUtils.isNotEmpty(originalFilters)) {\n@@ -991,6 +994,8 @@ private AbstractBindableTableScan fuseFilters(\n                     }\n                     if (!extraFilters.isEmpty()) {\n                         extraFilters.forEach(k -> k.accept(propertyChecker));\n+                        builder.filter(extraFilters);\n+                        extraFilters.clear();\n                     }\n                     tableScan = (AbstractBindableTableScan) builder.build();\n                 }\n@@ -998,11 +1003,11 @@ private AbstractBindableTableScan fuseFilters(\n         }\n         if (tableScan instanceof GraphLogicalSource && !uniqueKeyFilters.isEmpty()) {\n             GraphLogicalSource source = (GraphLogicalSource) tableScan;\n-            Preconditions.checkArgument(\n-                    source.getUniqueKeyFilters() == null,\n-                    \"can not add unique key filters if original is not empty\");\n-            source.setUniqueKeyFilters(\n-                    RexUtil.composeDisjunction(this.getRexBuilder(), uniqueKeyFilters));\n+            if (source.getUniqueKeyFilters() != null || uniqueKeyFilters.size() > 1) {\n+                extraFilters.addAll(uniqueKeyFilters);\n+            } else {\n+                source.setUniqueKeyFilters(uniqueKeyFilters.get(0));\n+            }\n         }\n         if (!extraFilters.isEmpty()) {\n             ImmutableList originalFilters = tableScan.getFilters();\n@@ -1031,14 +1036,10 @@ private AbstractLogicalMatch fuseFilters(\n             RexNode condition,\n             List<RexNode> extraFilters,\n             GraphBuilder builder) {\n-        List<RexNode> labelFilters = Lists.newArrayList();\n-        for (RexNode conjunction : RelOptUtil.conjunctions(condition)) {\n-            if (isLabelEqualFilter(conjunction) != null) {\n-                labelFilters.add(conjunction);\n-            } else {\n-                extraFilters.add(conjunction);\n-            }\n-        }\n+        RexFilterClassifier classifier = new RexFilterClassifier(builder, null);\n+        ClassifiedFilter filter = classifier.classify(condition);\n+        List<RexNode> labelFilters = filter.getLabelFilters();\n+        extraFilters.addAll(filter.getExtraFilters());\n         for (RexNode labelFilter : labelFilters) {\n             PushFilterVisitor visitor = new PushFilterVisitor(builder, labelFilter);\n             match = (AbstractLogicalMatch) match.accept(visitor);\n@@ -1049,161 +1050,6 @@ private AbstractLogicalMatch fuseFilters(\n         return match;\n     }\n \n-    private void classifyFilters(\n-            AbstractBindableTableScan tableScan,\n-            RexNode condition,\n-            List<Comparable> labelValues,\n-            List<RexNode> uniqueKeyFilters, // unique key filters int the list are composed by 'OR'\n-            List<RexNode> filters) {\n-        List<RexNode> conjunctions = RelOptUtil.conjunctions(condition);\n-        List<RexNode> filtersToRemove = Lists.newArrayList();\n-        for (RexNode conjunction : conjunctions) {\n-            RexLiteral labelLiteral = isLabelEqualFilter(conjunction);\n-            if (labelLiteral != null) {\n-                filtersToRemove.add(conjunction);\n-                labelValues.addAll(\n-                        com.alibaba.graphscope.common.ir.tools.Utils.getValuesAsList(\n-                                labelLiteral.getValueAs(Comparable.class)));\n-                break;\n-            }\n-        }\n-        if (tableScan instanceof GraphLogicalSource\n-                && ((GraphLogicalSource) tableScan).getUniqueKeyFilters() == null) {\n-            // try to extract unique key filters from the original condition\n-            List<RexNode> disjunctions = RelOptUtil.disjunctions(condition);\n-            for (RexNode disjunction : disjunctions) {\n-                if (isUniqueKeyEqualFilter(disjunction, tableScan)) {\n-                    filtersToRemove.add(disjunction);\n-                    uniqueKeyFilters.add(disjunction);\n-                }\n-            }\n-        }\n-        if (!filtersToRemove.isEmpty()) {\n-            conjunctions.removeAll(filtersToRemove);\n-        }\n-        filters.addAll(conjunctions);\n-    }\n-\n-    // check the condition if it is the pattern of label equal filter, i.e. ~label = 'person' or\n-    // ~label within ['person', 'software']\n-    // if it is then return the literal containing label values, otherwise null\n-    private @Nullable RexLiteral isLabelEqualFilter(RexNode condition) {\n-        if (condition instanceof RexCall) {\n-            RexCall rexCall = (RexCall) condition;\n-            SqlOperator operator = rexCall.getOperator();\n-            switch (operator.getKind()) {\n-                case EQUALS:\n-                case SEARCH:\n-                    RexNode left = rexCall.getOperands().get(0);\n-                    RexNode right = rexCall.getOperands().get(1);\n-                    if (left.getType() instanceof GraphLabelType && right instanceof RexLiteral) {\n-                        Comparable value = ((RexLiteral) right).getValue();\n-                        // if Sarg is a continuous range then the filter is not the 'equal', i.e.\n-                        // ~label SEARCH [[1, 10]] which means ~label >= 1 and ~label <= 10\n-                        if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n-                            return null;\n-                        }\n-                        return (RexLiteral) right;\n-                    } else if (right.getType() instanceof GraphLabelType\n-                            && left instanceof RexLiteral) {\n-                        Comparable value = ((RexLiteral) left).getValue();\n-                        if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n-                            return null;\n-                        }\n-                        return (RexLiteral) left;\n-                    }\n-                default:\n-                    return null;\n-            }\n-        } else {\n-            return null;\n-        }\n-    }\n-\n-    // check the condition if it is the pattern of unique key equal filter, i.e. ~id = 1 or ~id\n-    // within [1, 2]\n-    private boolean isUniqueKeyEqualFilter(RexNode condition, RelNode tableScan) {\n-        if (condition instanceof RexCall) {\n-            RexCall rexCall = (RexCall) condition;\n-            SqlOperator operator = rexCall.getOperator();\n-            switch (operator.getKind()) {\n-                case EQUALS:\n-                case SEARCH:\n-                    RexNode left = rexCall.getOperands().get(0);\n-                    RexNode right = rexCall.getOperands().get(1);\n-                    if (isUniqueKey(left, tableScan) && isLiteralOrDynamicParams(right)) {\n-                        if (right instanceof RexLiteral) {\n-                            Comparable value = ((RexLiteral) right).getValue();\n-                            // if Sarg is a continuous range then the filter is not the 'equal',\n-                            // i.e. ~id SEARCH [[1, 10]] which means ~id >= 1 and ~id <= 10\n-                            if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n-                                return false;\n-                            }\n-                        }\n-                        return true;\n-                    } else if (isUniqueKey(right, tableScan) && isLiteralOrDynamicParams(left)) {\n-                        if (left instanceof RexLiteral) {\n-                            Comparable value = ((RexLiteral) left).getValue();\n-                            if (value instanceof Sarg && !((Sarg) value).isPoints()) {\n-                                return false;\n-                            }\n-                        }\n-                        return true;\n-                    }\n-                default:\n-                    return false;\n-            }\n-        } else {\n-            return false;\n-        }\n-    }\n-\n-    private boolean isUniqueKey(RexNode rexNode, RelNode tableScan) {\n-        if (rexNode instanceof RexGraphVariable) {\n-            return isUniqueKey((RexGraphVariable) rexNode, tableScan);\n-        }\n-        return false;\n-    }\n-\n-    private boolean isUniqueKey(RexGraphVariable var, RelNode tableScan) {\n-        if (var.getProperty() == null) return false;\n-        switch (var.getProperty().getOpt()) {\n-            case ID:\n-                return true;\n-            case KEY:\n-                GraphSchemaType schemaType =\n-                        (GraphSchemaType) tableScan.getRowType().getFieldList().get(0).getType();\n-                ImmutableBitSet propertyIds = getPropertyIds(var.getProperty(), schemaType);\n-                if (!propertyIds.isEmpty() && tableScan.getTable().isKey(propertyIds)) {\n-                    return true;\n-                }\n-            case LABEL:\n-            case ALL:\n-            case LEN:\n-            default:\n-                return false;\n-        }\n-    }\n-\n-    private ImmutableBitSet getPropertyIds(GraphProperty property, GraphSchemaType schemaType) {\n-        if (property.getOpt() != GraphProperty.Opt.KEY) return ImmutableBitSet.of();\n-        GraphNameOrId key = property.getKey();\n-        if (key.getOpt() == GraphNameOrId.Opt.ID) {\n-            return ImmutableBitSet.of(key.getId());\n-        }\n-        for (int i = 0; i < schemaType.getFieldList().size(); ++i) {\n-            RelDataTypeField field = schemaType.getFieldList().get(i);\n-            if (field.getName().equals(key.getName())) {\n-                return ImmutableBitSet.of(i);\n-            }\n-        }\n-        return ImmutableBitSet.of();\n-    }\n-\n-    private boolean isLiteralOrDynamicParams(RexNode node) {\n-        return node instanceof RexLiteral || node instanceof RexDynamicParam;\n-    }\n-\n     // return the top node if its type is Filter, otherwise null\n     private Filter topFilter() {\n         if (this.size() > 0 && this.peek() instanceof Filter) {\n", "instance_id": "alibaba__GraphScope-3830", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to missing primary key (pk) optimization in a specific query scenario using GOpt-based compilation. It provides concrete examples of queries and their resulting plans to illustrate the issue (one query missing the optimization and another correctly identifying the pk). The goal is evident: to ensure that the pk optimization is applied consistently across similar query patterns. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes a \"pk optimization\" or the expected behavior in the query plan beyond the provided examples. Additionally, edge cases or constraints (e.g., other query patterns that might be affected) are not mentioned, which could lead to incomplete understanding of the full scope of the issue. Overall, while the core issue is clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving the creation of two new files (`ClassifiedFilter.java` and `RexFilterClassifier.java`) with substantial logic (over 400 lines in `RexFilterClassifier.java` alone) and modifications to an existing file (`GraphBuilder.java`). These changes impact the query optimization logic, a core component of the system, requiring a deep understanding of the codebase's architecture, particularly how filters are classified and applied in the query planning process. Second, the technical concepts involved are complex, including knowledge of Apache Calcite's `RexNode` framework for query expressions, graph database schema handling (e.g., labels and unique keys), and query optimization techniques. The code also deals with intricate logic for parsing and classifying filter conditions (e.g., handling `AND`, `OR`, `EQUALS`, and `SEARCH` operators), which demands a strong grasp of relational algebra and query processing. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest the need to handle various query patterns and filter combinations, adding to the complexity of ensuring correctness and performance. Finally, the changes could have a broad impact on the system's query execution plans, necessitating careful consideration of performance implications. Given these factors, a difficulty score of 0.75 reflects the challenging nature of the problem, requiring advanced technical expertise and significant effort to implement and validate the solution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Run pre-commit run --all\nAs a preparation for the PR for #192, I ran `pre-commit run --all` and commit the changes to a separate branch now.\r\n\r\nThis is the result, mostly some changed JSON files and a little bit of black formatting.\r\n\r\n### All Submissions\r\n\r\n**Describe your environment**\r\n\r\n* [ ] OS: macOS, Sonoma 14.5, M1 <!-- (e. g. Linux, Ubuntu 18.04, 64bit) -->\r\n* [ ] pyDataverse: main branch <!-- (e. g. 0.2.1) -->\r\n* [ ] Python: 3.12 <!-- (e. g. 3.6.9) -->\r\n* [ ] Dataverse: 6.3 (see #195) <!-- (optional, e. g. 4.18.1) -->\r\n\r\n**Follow best practices**\r\n\r\n* [x] Have you checked to ensure there aren't other open [Pull Requests](https://github.com/gdcc/pyDataverse/pulls) for the same update/change?\r\n* [x] Have you followed the guidelines in our [Contribution Guide](https://pydataverse.readthedocs.io/en/master/contributing/contributing.html)?\r\n* [x] Have you read the [Code of Conduct](https://github.com/gdcc/pyDataverse/blob/master/CODE_OF_CONDUCT.md)?\r\n* [x] Do your changes in a seperate branch. Branches MUST have descriptive names.\r\n* [x] Have you merged the latest changes from upstream to your branch?\r\n\r\n**Describe the PR**\r\n\r\n* [x] What kind of change does this PR introduce?\r\n  * It formats some JSON files, a little bit of Python code and one missing EOL\r\n* [x] Why is this change required? What problem does it solve?\r\n  * It is not required, but it makes working with pre-commit a little easier to have a \"clean base\" \u2013\u00a0otherwise, future changes might suddenly reformat a while file for minor changes.\r\n* [ ] Screenshots (if appropriate)\r\n* [x] Put `Closes #ISSUE_NUMBER` to the end of this pull request <!-- (e. g. Closes #1234) -->\r\n\r\n**Testing**\r\n\r\n* [x] Have you used tox and/or pytest for testing the changes? pytest, run-tests.sh\r\n* [x] Did the local testing ran successfully? yes, if changing DV_VERSION to 6.3.\r\n* [ ] Did the Continous Integration testing (Travis-CI) ran successfully?\r\n\r\n**Commits**\r\n\r\n* [x] Have descriptive commit messages with a short title (first line).\r\n* [x] Use the [commit message template](https://github.com/gdcc/pyDataverse/blob/master/.github/.gitmessage.txt)\r\n* [x] Put `Closes #ISSUE_NUMBER` in your commit messages to auto-close the issue that it fixes (if such).\r\n\r\n**Others**\r\n\r\n* [ ] Is there anything you need from someone else? No.\r\n\r\n### Code contribution\r\n\r\n* [x] Have you used pre-commit? Yes, `pre-commit run --all` :)\r\n* [x] Have you formatted your code with black prior to submission (e. g. via pre-commit)? yes\r\n* [ ] Have you written new tests for your changes? No\r\n* [ ] Have you ran mypy on your changes successfully? Not attempted\r\n* [ ] Have you documented your update (Docstrings and/or Docs)? No\r\n* [ ] Do your changes require additional changes to the documentation? No\r\n\r\nCloses #194\r\n\nUpdate the Contributor Guide\n### Issue\r\n\r\nWhile preparing my PR for #192, I noticed that the Contributor guide is a little outdated.\r\n\r\n- One should now use poetry to install the dev-dependencies as specified in the pyproject.toml, in contrast to the [different requirements.txt files](https://pydataverse.readthedocs.io/en/master/contributing/contributing.html#creating-a-development-environment), however, poetry behaves nicely if one already has a .venv (as I learned \"the hard way\")\r\n- `tox -e docs` does not work to build the docs, it expects a Python 3.6 which I don't have at hand and which is [EOL](https://devguide.python.org/versions/) anyways. Ubuntu 20 is still on 3.8, I think, but even the default for the tests is 3.11, so this should probably be updated. I built the docs with `sphinx-build pyDataverse/docs/source build` (inspired by the .readthedocs.yml) , but it couldn't find some of the rst files in the repository root. I ignored that issue as it was not relevant for me, but I'll check the tox.ini and see what one should configure instead, or what should be the working directory.\r\n- The main branch is now `main` and no longer `master`.\r\n- The testing section also mentions tox -py36, which should probably become -py311 (however, the tox environments are only py36,py37,py38 -- so maybe tox is just not really used anymore?)\r\n\r\nI will update the points I mentioned here and create a PR.\r\n\nFix `jsonData` not passed correctly\n**Overview**\r\n\r\nThis PR addresses the issue reported at https://github.com/datalad/datalad-dataverse/issues/320, where metadata sent via NativeApi.replace_datafile wasn\u2019t updating the file metadata correctly. The problem arose because the jsonData payload needs to be sent as form-data, which HTTPX\u2019s json kwarg doesn\u2019t handle properly.\r\n\r\nThe fix involves sending the payload via the data argument. However, since other endpoints require data to be sent through the json argument, this PR introduces a private method in api.py to determine the correct way to send the data. This method checks for the presence of the jsonData key and adjusts the argument accordingly. If the key isn\u2019t found, the json argument is used by default.\r\n\r\nTo prevent similar issues in the future, the tests have been updated. The replace_datafile tests now ensure that the metadata is correctly updated.\r\n\r\n**TLDR**\r\n\r\n* If `jsonData` key is present sent through `data` kwarg\r\n* If not, JSON payload is sent through `json`\r\n* Extended tests to verify metadata is updated correctly\r\n\nUpdate DV_VERSION to 6.3\nVersion 6.3 is what the :unstable docker image currently provides.\r\n\r\n### All Submissions\r\n\r\nInside container:\r\n* [x] OS: I think the python images use ubuntu, but didn't check\r\n* [x] pyDataverse: this PR (i.e., main branch + patch)\r\n* [x] Python: 3.11\r\n* [x] Dataverse: 6.3 (!) -- this is what this PR does\r\n\r\n**Follow best practices**\r\n\r\n* [x] Have you checked to ensure there aren't other open [Pull Requests](https://github.com/gdcc/pyDataverse/pulls) for the same update/change? yes, basically all due to the API version change.\r\n* [x] Have you followed the guidelines in our [Contribution Guide](https://pydataverse.readthedocs.io/en/master/contributing/contributing.html)? Yes, but it seems a little outdated. Will create a follow-up issue for this one to update it.\r\n* [x] Have you read the [Code of Conduct](https://github.com/gdcc/pyDataverse/blob/master/CODE_OF_CONDUCT.md)? Yes.\r\n* [x] Do your changes in a separate branch. Branches MUST have descriptive names.\r\n* [x] Have you merged the latest changes from upstream to your branch? Yes\r\n**Describe the PR**\r\n\r\n* [x] What kind of change does this PR introduce?\r\n  * Updates DV_VERSION from 6.2 to 6.3\r\n* [x] Why is this change required? What problem does it solve?\r\n  * the :unstable docker image got updated \r\n* [x] Screenshots (if appropriate)\r\n   <img width=\"305\" alt=\"image\" src=\"https://github.com/user-attachments/assets/460bed41-0af6-403b-abe5-4b4f9c6744d0\">\r\n* [x] Put `Closes #ISSUE_NUMBER` to the end of this pull request <!-- (e. g. Closes #1234) -->\r\n\r\n**Testing**\r\n\r\n* [x] Have you used tox and/or pytest for testing the changes? pytest, sh run-tests.sh\r\n* [x] Did the local testing ran successfully? yes\r\n* [ ] Did the Continuous Integration testing (Travis-CI) ran successfully?\r\n\r\n**Commits**\r\n\r\n* [x] Have descriptive commit messages with a short title (first line).\r\n* [x] Use the [commit message template](https://github.com/gdcc/pyDataverse/blob/master/.github/.gitmessage.txt)\r\n* [x] Put `Closes #ISSUE_NUMBER` in your commit messages to auto-close the issue that it fixes (if such).\r\n\r\n**Others**\r\n\r\n* [ ] Is there anything you need from someone else?\r\n\r\n### Documentation contribution\r\n\r\n* [ ] Have you followed NumPy Docstring standard?\r\n\r\n### Code contribution\r\n\r\n* [x] Have you used pre-commit?\r\n* [x] Have you formatted your code with black prior to submission (e. g. via pre-commit)?\r\n* [ ] Have you written new tests for your changes?\r\n* [ ] Have you ran mypy on your changes successfully?\r\n* [ ] Have you documented your update (Docstrings and/or Docs)?\r\n* [ ] Do your changes require additional changes to the documentation?\r\n\r\n\r\nCloses #195 \r\n\nAPI Key and User-Agent should preferrably be passed as headers\n### Issue\r\n\r\nThe API key and User-Agent are currently passed as GET parameters, ideally they should be passed as headers. User-Agents are commonly passed as headers, and for the key this is [actually preferred](https://guides.dataverse.org/en/latest/api/auth.html).\r\nIt also is slightly more secure, as the dataverse API will return the full request URL on error, possibly exposing API keys in logs:\r\n\r\n```json\r\n{\"status\":\"ERROR\",\"code\":404,\"message\":\"API endpoint does not exist on this server. Please check your code for typos, or consult our API guide at http://guides.dataverse.org.\",\"requestUrl\":\"http://localhost:8080/api/v1/access/datafile/:persistentId/?persistentId=23&User-Agent=pydataverse&key=<the API token>\",\"requestMethod\":\"GET\"}\r\n```\r\n(taken from datalad/datalad-dataverse#310, where I found out about this; it also contains a little bit of context)\r\n\r\nI am not sure if this request is a bug or feature, so I decided to open a normal issue. The pyDataverse version I use is 0.3.3, and the code doing this is in pyDataverse/api.py:\r\n\r\nhttps://github.com/gdcc/pyDataverse/blob/3b6fe063dfc73d6fa3aa674cc02313504c40204f/pyDataverse/api.py#L103-L138\r\n\r\nThe same is done for post_request, etc.\r\n\r\nA possible change would be to use headers and pass them as headers:\r\n```python\r\n        headers = {}\r\n        headers[\"User-Agent\"] = \"pydataverse\"\r\n        if self.api_token:\r\n            headers[\"X-Dataverse-key\"] = str(self.api_token)\r\n\r\n        if self.client is None:\r\n            return self._sync_request(\r\n                method=httpx.get,\r\n                url=url,\r\n                headers=headers,\r\n            )\r\n```\r\n\r\nAdditionally, I noticed that neither the `params` nor the `auth` attributes are used.\r\n\r\nFor the params a fix could be, especially after moving the key and user-agent to the headers:\r\n\r\n```python\r\nself._sync_request(\r\n    method=httpx.get,\r\n    url=url,\r\n    headers=headers,\r\n    params=params or {},  # maybe httpx.get even handles params=None, in that case they could simply be passed through\r\n)\r\n```\r\n\r\nFor the auth parameter it's a bit more difficult, since it currently does not affect the behavior although it should.\r\nOne possible way would be to update the if statement before adding the token to the header:\r\n```\r\nif auth and self.api_token:\r\n```\r\nHowever, it might make sense to change the default value to True in this case, to keep the behavior stable. However, I am not 100% sure what is the best way to handle this. Of course, one option would also be to remove the auth argument completely.\r\n\r\nLastly, the docstring of the function mentions the requests.Response, while it's now a httpx.Response. While they do have a similar interface, they have some minor differences; e.g., requests has an .ok property and uses iter_content, while httpx does not have .ok and uses iter_bytes.\r\n\r\nI think I could try to \"fix\" this and draft a PR.\r\n\nAPI Key and User-Agent should preferrably be passed as headers\n### Issue\r\n\r\nThe API key and User-Agent are currently passed as GET parameters, ideally they should be passed as headers. User-Agents are commonly passed as headers, and for the key this is [actually preferred](https://guides.dataverse.org/en/latest/api/auth.html).\r\nIt also is slightly more secure, as the dataverse API will return the full request URL on error, possibly exposing API keys in logs:\r\n\r\n```json\r\n{\"status\":\"ERROR\",\"code\":404,\"message\":\"API endpoint does not exist on this server. Please check your code for typos, or consult our API guide at http://guides.dataverse.org.\",\"requestUrl\":\"http://localhost:8080/api/v1/access/datafile/:persistentId/?persistentId=23&User-Agent=pydataverse&key=<the API token>\",\"requestMethod\":\"GET\"}\r\n```\r\n(taken from datalad/datalad-dataverse#310, where I found out about this; it also contains a little bit of context)\r\n\r\nI am not sure if this request is a bug or feature, so I decided to open a normal issue. The pyDataverse version I use is 0.3.3, and the code doing this is in pyDataverse/api.py:\r\n\r\nhttps://github.com/gdcc/pyDataverse/blob/3b6fe063dfc73d6fa3aa674cc02313504c40204f/pyDataverse/api.py#L103-L138\r\n\r\nThe same is done for post_request, etc.\r\n\r\nA possible change would be to use headers and pass them as headers:\r\n```python\r\n        headers = {}\r\n        headers[\"User-Agent\"] = \"pydataverse\"\r\n        if self.api_token:\r\n            headers[\"X-Dataverse-key\"] = str(self.api_token)\r\n\r\n        if self.client is None:\r\n            return self._sync_request(\r\n                method=httpx.get,\r\n                url=url,\r\n                headers=headers,\r\n            )\r\n```\r\n\r\nAdditionally, I noticed that neither the `params` nor the `auth` attributes are used.\r\n\r\nFor the params a fix could be, especially after moving the key and user-agent to the headers:\r\n\r\n```python\r\nself._sync_request(\r\n    method=httpx.get,\r\n    url=url,\r\n    headers=headers,\r\n    params=params or {},  # maybe httpx.get even handles params=None, in that case they could simply be passed through\r\n)\r\n```\r\n\r\nFor the auth parameter it's a bit more difficult, since it currently does not affect the behavior although it should.\r\nOne possible way would be to update the if statement before adding the token to the header:\r\n```\r\nif auth and self.api_token:\r\n```\r\nHowever, it might make sense to change the default value to True in this case, to keep the behavior stable. However, I am not 100% sure what is the best way to handle this. Of course, one option would also be to remove the auth argument completely.\r\n\r\nLastly, the docstring of the function mentions the requests.Response, while it's now a httpx.Response. While they do have a similar interface, they have some minor differences; e.g., requests has an .ok property and uses iter_content, while httpx does not have .ok and uses iter_bytes.\r\n\r\nI think I could try to \"fix\" this and draft a PR.\r\n\nRework auth/api_token parameters\n**Describe your environment**\r\n\r\nOn host:\r\n* [x] OS: macOS, Sonoma 14.5, M1\r\n* [x] pyDataverse: this PR (i.e., main branch + patches)\r\n* [x] Python: 3.12\r\n* [x] Dataverse: 6.3 (local/container), 6.2 (demo.dataverse.org)\r\n\r\nInside container:\r\n* [x] OS: I think the python images use ubuntu, but didn't check\r\n* [x] pyDataverse: this PR (i.e., main branch + patches)\r\n* [x] Python: 3.11\r\n* [x] Dataverse: 6.3 (!) -- had to override DV_VERSION in docker-compose-test-all.yml to make tests pass, as three tests check for the version. See also #197 .\r\n\r\n\r\n**Follow best practices**\r\n\r\n* [x] Have you checked to ensure there aren't other open [Pull Requests](https://github.com/gdcc/pyDataverse/pulls) for the same update/change? #146 is partially affected by this as it changes the params argument of `put_request`.\r\n* [x] Have you followed the guidelines in our [Contribution Guide](https://pydataverse.readthedocs.io/en/master/contributing/contributing.html)? Yes, see #193 .\r\n* [x] Have you read the [Code of Conduct](https://github.com/gdcc/pyDataverse/blob/master/CODE_OF_CONDUCT.md)? Yes.\r\n* [x] Do your changes in a separate branch. Branches MUST have descriptive names.\r\n* [x] Have you merged the latest changes from upstream to your branch? Yes\r\n\r\n**Describe the PR**\r\n\r\n* [x] What kind of change does this PR introduce?\r\n  * This PR introduces custom authentication implementation to allow for API token and Bearer token-based authentication.\r\n  * Sword requires BasicAuth, which this PR also addresses (see https://guides.dataverse.org/en/latest/api/sword.html#sword-auth)\r\n  * An Auth implementation must be passed as a keyword-argument to the __init__ function. We can discuss this requirement, but it felt better to be explicit about it and make sure implementers actually pass it correctly rather than accidentally passing it as the api_token or something.\r\n  * I replaced some `isinstance(..., (\"\".__class__, \"\".__class__))` with the much more legible and equivalent (as far as I can tell) `isinstance(..., str)`.\r\n  * It also deprecates the use of the previously ignored `auth` on individual functions in favor of generating multiple API objects with different auth methods.\r\n    * For the deprecation, I introduced a DEPRECATION_GUARD to warn people who are trying to pass something to the functions. Initially I wanted to check for `False`, but there was one call defaulting to `True` (`get_access`) and this would not catch code explicitly setting `auth=False` on the callsite. So I decided to introduce a new default argument and show a warning if it is overwritten. Happy to discuss this decision \u2013 I have never done such an orderly deprecation before, so this is my first try :)\r\n  * I updated the documentation.\r\n  * Oh, and I smuggled a change to the `Api.__str__` methods into the PR. As I scrolled by, I saw that they essentially could be removed and just the `Api.__str__` method itself should use `self.__class__.__name__` to get a similar result. However, the DataAccessApi will not print `DataAccessApi: ...` instead of the previous `Data Access API: ...` \u2013\u00a0the same applies for the others. However, this felt to be more in spirit with the docstring (which I then also updated slightly). If that's not so nice, we can move that to its own PR or even drop that commit entirely.\r\n  * I had to adapt three unrelated things (conf.py, the `self.base_url = None` assignment in api.py, and the pyproject.toml) for mypy.\r\n  * This PR almost accidentally also allows to pass `params` properly due to the name change of params -> headers.\r\n* [x] Why is this change required? What problem does it solve?\r\n  * First and foremost, the issue it resolves is that tokens could potentially be leaked when sharing log messages, as the error response from dataverse includes the request URL, which, if the token is provided as ?key, contains the key.\r\n  * The PR also lays the groundwork to allow for different authentication schemas in the future and even to configure it from the outside, favoring composition over inheritance. This will hopefully make maintenance and the refactoring in #189 easier.\r\n  * By deprecating the auth on individual methods, it will eventually be possible to remove this unused argument and make the API a little bit leaner and reduce confusing behavior.\r\n* [ ] Screenshots (if appropriate)\r\n* [x] Put `Closes #ISSUE_NUMBER` to the end of this pull request\r\n\r\n**Testing**\r\n\r\n* [x] Have you used tox and/or pytest for testing the changes? pytest\r\n* [x] Did the local testing ran successfully? yes, if applying #197. For demo.dataverse.org, I could not run test_upload.py successfully, as I was unauthorized to perform those uploads (401). However, locally (outside the containers) I was able to run the asyncio tests with pytest-asyncio. The containers seem to miss this.\r\n* [ ] Did the Continuous Integration testing (Travis-CI) ran successfully?\r\n\r\n**Commits**\r\n\r\n* [x] Have descriptive commit messages with a short title (first line).\r\n* [x] Use the [commit message template](https://github.com/gdcc/pyDataverse/blob/master/.github/.gitmessage.txt)\r\n* [x] Put `Closes #ISSUE_NUMBER` in your commit messages to auto-close the issue that it fixes (if such).\r\n  * I have only done this for 3e73fbf, as that is arguably the \"essence\" of this PR.\r\n\r\n**Others**\r\n\r\n* [x] Is there anything you need from someone else? Take your time to review these changes. While some are quite repetitive and I already split some parts off into other PRs, this is a massive changeset due to the deprecation. In fact, maybe the deprecation should be its own PR? What do you think?\r\n\r\n### Documentation contribution\r\n\r\n* [?] Have you followed NumPy Docstring standard? I hope so\r\n\r\n### Code contribution\r\n\r\n* [x] Have you used pre-commit? Yes, see also #196.\r\n* [x] Have you formatted your code with black prior to submission (e. g. via pre-commit)? Yes\r\n* [x] Have you written new tests for your changes? Yes\r\n* [x] Have you ran mypy on your changes successfully? Yes, but I had to add types-jsonschema to the lint dependencies and in the conf.py. I haven't tried --strict.\r\n* [x] Have you documented your update (Docstrings and/or Docs)? yes\r\n* [x] Do your changes require additional changes to the documentation? No, but m\r\n\r\nNote that this PR depends on other PRs which should be reviewed and decided/acted upon first, I can also rebase/merge the changes into this branch afterwards.\r\nIn particular:\r\n- #196\r\n- #197\r\n\r\nCloses #192 .\r\n\nRework auth/api_token parameters\n**Describe your environment**\r\n\r\nOn host:\r\n* [x] OS: macOS, Sonoma 14.5, M1\r\n* [x] pyDataverse: this PR (i.e., main branch + patches)\r\n* [x] Python: 3.12\r\n* [x] Dataverse: 6.3 (local/container), 6.2 (demo.dataverse.org)\r\n\r\nInside container:\r\n* [x] OS: I think the python images use ubuntu, but didn't check\r\n* [x] pyDataverse: this PR (i.e., main branch + patches)\r\n* [x] Python: 3.11\r\n* [x] Dataverse: 6.3 (!) -- had to override DV_VERSION in docker-compose-test-all.yml to make tests pass, as three tests check for the version. See also #197 .\r\n\r\n\r\n**Follow best practices**\r\n\r\n* [x] Have you checked to ensure there aren't other open [Pull Requests](https://github.com/gdcc/pyDataverse/pulls) for the same update/change? #146 is partially affected by this as it changes the params argument of `put_request`.\r\n* [x] Have you followed the guidelines in our [Contribution Guide](https://pydataverse.readthedocs.io/en/master/contributing/contributing.html)? Yes, see #193 .\r\n* [x] Have you read the [Code of Conduct](https://github.com/gdcc/pyDataverse/blob/master/CODE_OF_CONDUCT.md)? Yes.\r\n* [x] Do your changes in a separate branch. Branches MUST have descriptive names.\r\n* [x] Have you merged the latest changes from upstream to your branch? Yes\r\n\r\n**Describe the PR**\r\n\r\n* [x] What kind of change does this PR introduce?\r\n  * This PR introduces custom authentication implementation to allow for API token and Bearer token-based authentication.\r\n  * Sword requires BasicAuth, which this PR also addresses (see https://guides.dataverse.org/en/latest/api/sword.html#sword-auth)\r\n  * An Auth implementation must be passed as a keyword-argument to the __init__ function. We can discuss this requirement, but it felt better to be explicit about it and make sure implementers actually pass it correctly rather than accidentally passing it as the api_token or something.\r\n  * I replaced some `isinstance(..., (\"\".__class__, \"\".__class__))` with the much more legible and equivalent (as far as I can tell) `isinstance(..., str)`.\r\n  * It also deprecates the use of the previously ignored `auth` on individual functions in favor of generating multiple API objects with different auth methods.\r\n    * For the deprecation, I introduced a DEPRECATION_GUARD to warn people who are trying to pass something to the functions. Initially I wanted to check for `False`, but there was one call defaulting to `True` (`get_access`) and this would not catch code explicitly setting `auth=False` on the callsite. So I decided to introduce a new default argument and show a warning if it is overwritten. Happy to discuss this decision \u2013 I have never done such an orderly deprecation before, so this is my first try :)\r\n  * I updated the documentation.\r\n  * Oh, and I smuggled a change to the `Api.__str__` methods into the PR. As I scrolled by, I saw that they essentially could be removed and just the `Api.__str__` method itself should use `self.__class__.__name__` to get a similar result. However, the DataAccessApi will not print `DataAccessApi: ...` instead of the previous `Data Access API: ...` \u2013\u00a0the same applies for the others. However, this felt to be more in spirit with the docstring (which I then also updated slightly). If that's not so nice, we can move that to its own PR or even drop that commit entirely.\r\n  * I had to adapt three unrelated things (conf.py, the `self.base_url = None` assignment in api.py, and the pyproject.toml) for mypy.\r\n  * This PR almost accidentally also allows to pass `params` properly due to the name change of params -> headers.\r\n* [x] Why is this change required? What problem does it solve?\r\n  * First and foremost, the issue it resolves is that tokens could potentially be leaked when sharing log messages, as the error response from dataverse includes the request URL, which, if the token is provided as ?key, contains the key.\r\n  * The PR also lays the groundwork to allow for different authentication schemas in the future and even to configure it from the outside, favoring composition over inheritance. This will hopefully make maintenance and the refactoring in #189 easier.\r\n  * By deprecating the auth on individual methods, it will eventually be possible to remove this unused argument and make the API a little bit leaner and reduce confusing behavior.\r\n* [ ] Screenshots (if appropriate)\r\n* [x] Put `Closes #ISSUE_NUMBER` to the end of this pull request\r\n\r\n**Testing**\r\n\r\n* [x] Have you used tox and/or pytest for testing the changes? pytest\r\n* [x] Did the local testing ran successfully? yes, if applying #197. For demo.dataverse.org, I could not run test_upload.py successfully, as I was unauthorized to perform those uploads (401). However, locally (outside the containers) I was able to run the asyncio tests with pytest-asyncio. The containers seem to miss this.\r\n* [ ] Did the Continuous Integration testing (Travis-CI) ran successfully?\r\n\r\n**Commits**\r\n\r\n* [x] Have descriptive commit messages with a short title (first line).\r\n* [x] Use the [commit message template](https://github.com/gdcc/pyDataverse/blob/master/.github/.gitmessage.txt)\r\n* [x] Put `Closes #ISSUE_NUMBER` in your commit messages to auto-close the issue that it fixes (if such).\r\n  * I have only done this for 3e73fbf, as that is arguably the \"essence\" of this PR.\r\n\r\n**Others**\r\n\r\n* [x] Is there anything you need from someone else? Take your time to review these changes. While some are quite repetitive and I already split some parts off into other PRs, this is a massive changeset due to the deprecation. In fact, maybe the deprecation should be its own PR? What do you think?\r\n\r\n### Documentation contribution\r\n\r\n* [?] Have you followed NumPy Docstring standard? I hope so\r\n\r\n### Code contribution\r\n\r\n* [x] Have you used pre-commit? Yes, see also #196.\r\n* [x] Have you formatted your code with black prior to submission (e. g. via pre-commit)? Yes\r\n* [x] Have you written new tests for your changes? Yes\r\n* [x] Have you ran mypy on your changes successfully? Yes, but I had to add types-jsonschema to the lint dependencies and in the conf.py. I haven't tried --strict.\r\n* [x] Have you documented your update (Docstrings and/or Docs)? yes\r\n* [x] Do your changes require additional changes to the documentation? No, but m\r\n\r\nNote that this PR depends on other PRs which should be reviewed and decided/acted upon first, I can also rebase/merge the changes into this branch afterwards.\r\nIn particular:\r\n- #196\r\n- #197\r\n\r\nCloses #192 .\r\n\n", "patch": "diff --git a/pyDataverse/api.py b/pyDataverse/api.py\nindex a15decc..b364847 100644\n--- a/pyDataverse/api.py\n+++ b/pyDataverse/api.py\n@@ -4,9 +4,11 @@\n from typing import Any, Dict, Optional\n import httpx\n import subprocess as sp\n+from warnings import warn\n \n from httpx import ConnectError, Response\n \n+from pyDataverse.auth import ApiTokenAuth\n from pyDataverse.exceptions import (\n     ApiAuthorizationError,\n     ApiUrlError,\n@@ -16,6 +18,8 @@\n     OperationFailedError,\n )\n \n+DEPRECATION_GUARD = object()\n+\n \n class Api:\n     \"\"\"Base class.\n@@ -41,6 +45,8 @@ def __init__(\n         base_url: str,\n         api_token: Optional[str] = None,\n         api_version: str = \"latest\",\n+        *,\n+        auth: Optional[httpx.Auth] = None,\n     ):\n         \"\"\"Init an Api() class.\n \n@@ -52,16 +58,54 @@ def __init__(\n         base_url : str\n             Base url for Dataverse api.\n         api_token : str | None\n-            Api token for Dataverse api.\n+            API token for Dataverse API. If you provide an :code:`api_token`, we\n+            assume it is an API token as retrieved via your Dataverse instance\n+            user profile.\n+            We recommend using the :code:`auth` argument instead.\n+            To retain the current behaviour with the :code:`auth` argument, change\n+\n+            .. code-block:: python\n+\n+                Api(\"https://demo.dataverse.org\", \"my_token\")\n+\n+            to\n+\n+            .. code-block:: python\n+\n+                from pyDataverse.auth import ApiTokenAuth\n \n+                Api(\"https://demo.dataverse.org\", auth=ApiTokenAuth(\"my_token\"))\n+\n+            If you are using an OIDC/OAuth 2.0 Bearer token, please use the :code:`auth`\n+            parameter with the :py:class:`.auth.BearerTokenAuth`.\n+        api_version : str\n+            The version string of the Dataverse API or :code:`latest`, e.g.,\n+            :code:`v1`. Defaults to :code:`latest`, which drops the version from\n+            the API urls.\n+        auth : httpx.Auth | None\n+            You can provide any authentication mechanism you like to connect to\n+            your Dataverse instance.  The most common mechanisms are implemented\n+            in :py:mod:`.auth`, but if one is missing, you can use your own\n+            `httpx.Auth`-compatible class. For more information, have a look at\n+            `httpx' Authentication docs\n+            <https://www.python-httpx.org/advanced/authentication/>`_.\n         Examples\n-        --------\n-        Create an Api connection::\n+        -------\n+        Create an API connection::\n+\n+        .. code-block::\n \n             >>> from pyDataverse.api import Api\n             >>> base_url = 'http://demo.dataverse.org'\n             >>> api = Api(base_url)\n \n+        .. code-block::\n+\n+            >>> from pyDataverse.api import Api\n+            >>> from pyDataverse.auth import ApiTokenAuth\n+            >>> base_url = 'http://demo.dataverse.org'\n+            >>> api = Api(base_url, ApiTokenAuth('my_api_token'))\n+\n         \"\"\"\n         if not isinstance(base_url, str):\n             raise ApiUrlError(\"base_url {0} is not a string.\".format(base_url))\n@@ -69,28 +113,33 @@ def __init__(\n         self.base_url = base_url\n         self.client = None\n \n-        if not isinstance(api_version, (\"\".__class__, \"\".__class__)):\n+        if not isinstance(api_version, str):\n             raise ApiUrlError(\"api_version {0} is not a string.\".format(api_version))\n         self.api_version = api_version\n \n-        if api_token:\n-            if not isinstance(api_token, (\"\".__class__, \"\".__class__)):\n-                raise ApiAuthorizationError(\"Api token passed is not a string.\")\n+        self.auth = auth\n         self.api_token = api_token\n-\n-        if self.base_url:\n-            if self.api_version == \"latest\":\n-                self.base_url_api = \"{0}/api\".format(self.base_url)\n+        if api_token is not None:\n+            if auth is None:\n+                self.auth = ApiTokenAuth(api_token)\n             else:\n-                self.base_url_api = \"{0}/api/{1}\".format(\n-                    self.base_url, self.api_version\n+                self.api_token = None\n+                warn(\n+                    UserWarning(\n+                        \"You provided both, an api_token and a custom auth \"\n+                        \"method. We will only use the auth method.\"\n+                    )\n                 )\n+\n+        if self.api_version == \"latest\":\n+            self.base_url_api = \"{0}/api\".format(self.base_url)\n         else:\n-            self.base_url_api = None\n+            self.base_url_api = \"{0}/api/{1}\".format(self.base_url, self.api_version)\n+\n         self.timeout = 500\n \n     def __str__(self):\n-        \"\"\"Return name of Api() class for users.\n+        \"\"\"Return the class name and URL of the used API class.\n \n         Returns\n         -------\n@@ -98,9 +147,9 @@ def __str__(self):\n             Naming of the API class.\n \n         \"\"\"\n-        return \"API: {0}\".format(self.base_url_api)\n+        return f\"{self.__class__.__name__}: {self.base_url_api}\"\n \n-    def get_request(self, url, params=None, auth=False):\n+    def get_request(self, url, params=None, auth=DEPRECATION_GUARD):\n         \"\"\"Make a GET request.\n \n         Parameters\n@@ -110,8 +159,18 @@ def get_request(self, url, params=None, auth=False):\n         params : dict\n             Dictionary of parameters to be passed with the request.\n             Defaults to `None`.\n-        auth : bool\n-            Should an api token be sent in the request. Defaults to `False`.\n+        auth : Any\n+            .. deprecated:: 0.3.4\n+                The auth parameter was ignored before version 0.3.4.\n+                Please pass your auth to the Api instance directly, as\n+                explained in :py:func:`Api.__init__`.\n+                If you need multiple auth methods, create multiple\n+                API instances:\n+\n+                .. code-block:: python\n+\n+                    api = Api(\"https://demo.dataverse.org\", auth=ApiTokenAuth(\"my_api_token\"))\n+                    api_oauth = Api(\"https://demo.dataverse.org\", auth=BearerTokenAuth(\"my_bearer_token\"))\n \n         Returns\n         -------\n@@ -119,25 +178,34 @@ def get_request(self, url, params=None, auth=False):\n             Response object of httpx library.\n \n         \"\"\"\n-        params = {}\n-        params[\"User-Agent\"] = \"pydataverse\"\n-        if self.api_token:\n-            params[\"key\"] = str(self.api_token)\n+        if auth is not DEPRECATION_GUARD:\n+            warn(\n+                DeprecationWarning(\n+                    \"The auth parameter is deprecated. Please pass your auth \"\n+                    \"arguments to the __init__ method instead.\"\n+                )\n+            )\n+        headers = {}\n+        headers[\"User-Agent\"] = \"pydataverse\"\n \n         if self.client is None:\n             return self._sync_request(\n                 method=httpx.get,\n                 url=url,\n+                headers=headers,\n                 params=params,\n             )\n         else:\n             return self._async_request(\n                 method=self.client.get,\n                 url=url,\n+                headers=headers,\n                 params=params,\n             )\n \n-    def post_request(self, url, data=None, auth=False, params=None, files=None):\n+    def post_request(\n+        self, url, data=None, auth=DEPRECATION_GUARD, params=None, files=None\n+    ):\n         \"\"\"Make a POST request.\n \n         params will be added as key-value pairs to the URL.\n@@ -148,8 +216,18 @@ def post_request(self, url, data=None, auth=False, params=None, files=None):\n             Full URL.\n         data : str\n             Metadata as a json-formatted string. Defaults to `None`.\n-        auth : bool\n-            Should an api token be sent in the request. Defaults to `False`.\n+        auth : Any\n+            .. deprecated:: 0.3.4\n+                The auth parameter was ignored before version 0.3.4.\n+                Please pass your auth to the Api instance directly, as\n+                explained in :py:func:`Api.__init__`.\n+                If you need multiple auth methods, create multiple\n+                API instances:\n+\n+                .. code-block:: python\n+\n+                    api = Api(\"https://demo.dataverse.org\", auth=ApiTokenAuth(\"my_api_token\"))\n+                    api_oauth = Api(\"https://demo.dataverse.org\", auth=BearerTokenAuth(\"my_bearer_token\"))\n         files : dict\n             e.g. :code:`files={'file': open('sample_file.txt','rb')}`\n         params : dict\n@@ -162,10 +240,15 @@ def post_request(self, url, data=None, auth=False, params=None, files=None):\n             Response object of httpx library.\n \n         \"\"\"\n-        params = {}\n-        params[\"User-Agent\"] = \"pydataverse\"\n-        if self.api_token:\n-            params[\"key\"] = self.api_token\n+        if auth is not DEPRECATION_GUARD:\n+            warn(\n+                DeprecationWarning(\n+                    \"The auth parameter is deprecated. Please pass your auth \"\n+                    \"arguments to the __init__ method instead.\"\n+                )\n+            )\n+        headers = {}\n+        headers[\"User-Agent\"] = \"pydataverse\"\n \n         if isinstance(data, str):\n             data = json.loads(data)\n@@ -175,18 +258,24 @@ def post_request(self, url, data=None, auth=False, params=None, files=None):\n \n         if self.client is None:\n             return self._sync_request(\n-                method=httpx.post, url=url, params=params, files=files, **request_params\n+                method=httpx.post,\n+                url=url,\n+                headers=headers,\n+                params=params,\n+                files=files,\n+                **request_params,\n             )\n         else:\n             return self._async_request(\n                 method=self.client.post,\n                 url=url,\n+                headers=headers,\n                 params=params,\n                 files=files,\n                 **request_params,\n             )\n \n-    def put_request(self, url, data=None, auth=False, params=None):\n+    def put_request(self, url, data=None, auth=DEPRECATION_GUARD, params=None):\n         \"\"\"Make a PUT request.\n \n         Parameters\n@@ -195,8 +284,18 @@ def put_request(self, url, data=None, auth=False, params=None):\n             Full URL.\n         data : str\n             Metadata as a json-formatted string. Defaults to `None`.\n-        auth : bool\n-            Should an api token be sent in the request. Defaults to `False`.\n+        auth : Any\n+            .. deprecated:: 0.3.4\n+                The auth parameter was ignored before version 0.3.4.\n+                Please pass your auth to the Api instance directly, as\n+                explained in :py:func:`Api.__init__`.\n+                If you need multiple auth methods, create multiple\n+                API instances:\n+\n+                .. code-block:: python\n+\n+                    api = Api(\"https://demo.dataverse.org\", auth=ApiTokenAuth(\"my_api_token\"))\n+                    api_oauth = Api(\"https://demo.dataverse.org\", auth=BearerTokenAuth(\"my_bearer_token\"))\n         params : dict\n             Dictionary of parameters to be passed with the request.\n             Defaults to `None`.\n@@ -207,10 +306,15 @@ def put_request(self, url, data=None, auth=False, params=None):\n             Response object of httpx library.\n \n         \"\"\"\n-        params = {}\n-        params[\"User-Agent\"] = \"pydataverse\"\n-        if self.api_token:\n-            params[\"key\"] = self.api_token\n+        if auth is not DEPRECATION_GUARD:\n+            warn(\n+                DeprecationWarning(\n+                    \"The auth parameter is deprecated. Please pass your auth \"\n+                    \"arguments to the __init__ method instead.\"\n+                )\n+            )\n+        headers = {}\n+        headers[\"User-Agent\"] = \"pydataverse\"\n \n         if isinstance(data, str):\n             data = json.loads(data)\n@@ -222,6 +326,8 @@ def put_request(self, url, data=None, auth=False, params=None):\n             return self._sync_request(\n                 method=httpx.put,\n                 url=url,\n+                json=data,\n+                headers=headers,\n                 params=params,\n                 **request_params,\n             )\n@@ -229,19 +335,31 @@ def put_request(self, url, data=None, auth=False, params=None):\n             return self._async_request(\n                 method=self.client.put,\n                 url=url,\n+                json=data,\n+                headers=headers,\n                 params=params,\n                 **request_params,\n             )\n \n-    def delete_request(self, url, auth=False, params=None):\n+    def delete_request(self, url, auth=DEPRECATION_GUARD, params=None):\n         \"\"\"Make a Delete request.\n \n         Parameters\n         ----------\n         url : str\n             Full URL.\n-        auth : bool\n-            Should an api token be sent in the request. Defaults to `False`.\n+        auth : Any\n+            .. deprecated:: 0.3.4\n+                The auth parameter was ignored before version 0.3.4.\n+                Please pass your auth to the Api instance directly, as\n+                explained in :py:func:`Api.__init__`.\n+                If you need multiple auth methods, create multiple\n+                API instances:\n+\n+                .. code-block:: python\n+\n+                    api = Api(\"https://demo.dataverse.org\", auth=ApiTokenAuth(\"my_api_token\"))\n+                    api_oauth = Api(\"https://demo.dataverse.org\", auth=BearerTokenAuth(\"my_bearer_token\"))\n         params : dict\n             Dictionary of parameters to be passed with the request.\n             Defaults to `None`.\n@@ -252,21 +370,28 @@ def delete_request(self, url, auth=False, params=None):\n             Response object of httpx library.\n \n         \"\"\"\n-        params = {}\n-        params[\"User-Agent\"] = \"pydataverse\"\n-        if self.api_token:\n-            params[\"key\"] = self.api_token\n+        if auth is not DEPRECATION_GUARD:\n+            warn(\n+                DeprecationWarning(\n+                    \"The auth parameter is deprecated. Please pass your auth \"\n+                    \"arguments to the __init__ method instead.\"\n+                )\n+            )\n+        headers = {}\n+        headers[\"User-Agent\"] = \"pydataverse\"\n \n         if self.client is None:\n             return self._sync_request(\n                 method=httpx.delete,\n                 url=url,\n+                headers=headers,\n                 params=params,\n             )\n         else:\n             return self._async_request(\n                 method=self.client.delete,\n                 url=url,\n+                headers=headers,\n                 params=params,\n             )\n \n@@ -321,9 +446,14 @@ def _sync_request(\n         kwargs = self._filter_kwargs(kwargs)\n \n         try:\n-            resp = method(**kwargs, follow_redirects=True, timeout=None)\n+            resp: httpx.Response = method(\n+                **kwargs, auth=self.auth, follow_redirects=True, timeout=None\n+            )\n             if resp.status_code == 401:\n-                error_msg = resp.json()[\"message\"]\n+                try:\n+                    error_msg = resp.json()[\"message\"]\n+                except json.JSONDecodeError:\n+                    error_msg = resp.reason_phrase\n                 raise ApiAuthorizationError(\n                     \"ERROR: HTTP 401 - Authorization error {0}. MSG: {1}\".format(\n                         kwargs[\"url\"], error_msg\n@@ -364,7 +494,7 @@ async def _async_request(\n         kwargs = self._filter_kwargs(kwargs)\n \n         try:\n-            resp = await method(**kwargs)\n+            resp = await method(**kwargs, auth=self.auth)\n \n             if resp.status_code == 401:\n                 error_msg = resp.json()[\"message\"]\n@@ -431,25 +561,14 @@ class DataAccessApi(Api):\n \n     \"\"\"\n \n-    def __init__(self, base_url, api_token=None):\n+    def __init__(self, base_url, api_token=None, *, auth=None):\n         \"\"\"Init an DataAccessApi() class.\"\"\"\n-        super().__init__(base_url, api_token)\n+        super().__init__(base_url, api_token, auth=auth)\n         if base_url:\n             self.base_url_api_data_access = \"{0}/access\".format(self.base_url_api)\n         else:\n             self.base_url_api_data_access = self.base_url_api\n \n-    def __str__(self):\n-        \"\"\"Return name of DataAccessApi() class for users.\n-\n-        Returns\n-        -------\n-        str\n-            Naming of the DataAccess API class.\n-\n-        \"\"\"\n-        return \"Data Access API: {0}\".format(self.base_url_api_data_access)\n-\n     def get_datafile(\n         self,\n         identifier,\n@@ -457,7 +576,7 @@ def get_datafile(\n         no_var_header=None,\n         image_thumb=None,\n         is_pid=True,\n-        auth=False,\n+        auth=DEPRECATION_GUARD,\n     ):\n         \"\"\"Download a datafile via the Dataverse Data Access API.\n \n@@ -510,7 +629,7 @@ def get_datafile(\n             url += \"imageThumb={0}\".format(image_thumb)\n         return self.get_request(url, auth=auth)\n \n-    def get_datafiles(self, identifier, data_format=None, auth=False):\n+    def get_datafiles(self, identifier, data_format=None, auth=DEPRECATION_GUARD):\n         \"\"\"Download a datafile via the Dataverse Data Access API.\n \n         Get by file id (HTTP Request).\n@@ -538,7 +657,9 @@ def get_datafiles(self, identifier, data_format=None, auth=False):\n             url += \"?format={0}\".format(data_format)\n         return self.get_request(url, auth=auth)\n \n-    def get_datafile_bundle(self, identifier, file_metadata_id=None, auth=False):\n+    def get_datafile_bundle(\n+        self, identifier, file_metadata_id=None, auth=DEPRECATION_GUARD\n+    ):\n         \"\"\"Download a datafile in all its formats.\n \n         HTTP Request:\n@@ -581,7 +702,7 @@ def get_datafile_bundle(self, identifier, file_metadata_id=None, auth=False):\n             url += \"?fileMetadataId={0}\".format(file_metadata_id)\n         return self.get_request(url, auth=auth)\n \n-    def request_access(self, identifier, auth=True, is_filepid=False):\n+    def request_access(self, identifier, auth=DEPRECATION_GUARD, is_filepid=False):\n         \"\"\"Request datafile access.\n \n         This method requests access to the datafile whose id is passed on the behalf of an authenticated user whose key is passed. Note that not all datasets allow access requests to restricted files.\n@@ -602,7 +723,9 @@ def request_access(self, identifier, auth=True, is_filepid=False):\n             )\n         return self.put_request(url, auth=auth)\n \n-    def allow_access_request(self, identifier, do_allow=True, auth=True, is_pid=True):\n+    def allow_access_request(\n+        self, identifier, do_allow=True, auth=DEPRECATION_GUARD, is_pid=True\n+    ):\n         \"\"\"Allow access request for datafiles.\n \n         https://guides.dataverse.org/en/latest/api/dataaccess.html#allow-access-requests\n@@ -625,7 +748,7 @@ def allow_access_request(self, identifier, do_allow=True, auth=True, is_pid=True\n             data = \"false\"\n         return self.put_request(url, data=data, auth=auth)\n \n-    def grant_file_access(self, identifier, user, auth=False):\n+    def grant_file_access(self, identifier, user, auth=DEPRECATION_GUARD):\n         \"\"\"Grant datafile access.\n \n         https://guides.dataverse.org/en/4.18.1/api/dataaccess.html#grant-file-access\n@@ -637,7 +760,7 @@ def grant_file_access(self, identifier, user, auth=False):\n         )\n         return self.put_request(url, auth=auth)\n \n-    def list_file_access_requests(self, identifier, auth=False):\n+    def list_file_access_requests(self, identifier, auth=DEPRECATION_GUARD):\n         \"\"\"Liste datafile access requests.\n \n         https://guides.dataverse.org/en/4.18.1/api/dataaccess.html#list-file-access-requests\n@@ -662,26 +785,15 @@ class MetricsApi(Api):\n \n     \"\"\"\n \n-    def __init__(self, base_url, api_token=None, api_version=\"latest\"):\n+    def __init__(self, base_url, api_token=None, api_version=\"latest\", *, auth=None):\n         \"\"\"Init an MetricsApi() class.\"\"\"\n-        super().__init__(base_url, api_token, api_version)\n+        super().__init__(base_url, api_token, api_version, auth=auth)\n         if base_url:\n             self.base_url_api_metrics = \"{0}/api/info/metrics\".format(self.base_url)\n         else:\n             self.base_url_api_metrics = None\n \n-    def __str__(self):\n-        \"\"\"Return name of MetricsApi() class for users.\n-\n-        Returns\n-        -------\n-        str\n-            Naming of the MetricsApi() class.\n-\n-        \"\"\"\n-        return \"Metrics API: {0}\".format(self.base_url_api_metrics)\n-\n-    def total(self, data_type, date_str=None, auth=False):\n+    def total(self, data_type, date_str=None, auth=DEPRECATION_GUARD):\n         \"\"\"\n         GET https://$SERVER/api/info/metrics/$type\n         GET https://$SERVER/api/info/metrics/$type/toMonth/$YYYY-DD\n@@ -694,7 +806,7 @@ def total(self, data_type, date_str=None, auth=False):\n             url += \"/toMonth/{0}\".format(date_str)\n         return self.get_request(url, auth=auth)\n \n-    def past_days(self, data_type, days_str, auth=False):\n+    def past_days(self, data_type, days_str, auth=DEPRECATION_GUARD):\n         \"\"\"\n \n         http://guides.dataverse.org/en/4.18.1/api/metrics.html\n@@ -708,7 +820,7 @@ def past_days(self, data_type, days_str, auth=False):\n         )\n         return self.get_request(url, auth=auth)\n \n-    def get_dataverses_by_subject(self, auth=False):\n+    def get_dataverses_by_subject(self, auth=DEPRECATION_GUARD):\n         \"\"\"\n         GET https://$SERVER/api/info/metrics/dataverses/bySubject\n \n@@ -718,7 +830,7 @@ def get_dataverses_by_subject(self, auth=False):\n         url = \"{0}/dataverses/bySubject\".format(self.base_url_api_metrics)\n         return self.get_request(url, auth=auth)\n \n-    def get_dataverses_by_category(self, auth=False):\n+    def get_dataverses_by_category(self, auth=DEPRECATION_GUARD):\n         \"\"\"\n         GET https://$SERVER/api/info/metrics/dataverses/byCategory\n \n@@ -728,7 +840,7 @@ def get_dataverses_by_category(self, auth=False):\n         url = \"{0}/dataverses/byCategory\".format(self.base_url_api_metrics)\n         return self.get_request(url, auth=auth)\n \n-    def get_datasets_by_subject(self, date_str=None, auth=False):\n+    def get_datasets_by_subject(self, date_str=None, auth=DEPRECATION_GUARD):\n         \"\"\"\n         GET https://$SERVER/api/info/metrics/datasets/bySubject\n \n@@ -740,7 +852,7 @@ def get_datasets_by_subject(self, date_str=None, auth=False):\n             url += \"/toMonth/{0}\".format(date_str)\n         return self.get_request(url, auth=auth)\n \n-    def get_datasets_by_data_location(self, data_location, auth=False):\n+    def get_datasets_by_data_location(self, data_location, auth=DEPRECATION_GUARD):\n         \"\"\"\n         GET https://$SERVER/api/info/metrics/datasets/bySubject\n \n@@ -774,7 +886,7 @@ class NativeApi(Api):\n \n     \"\"\"\n \n-    def __init__(self, base_url: str, api_token=None, api_version=\"v1\"):\n+    def __init__(self, base_url: str, api_token=None, api_version=\"v1\", *, auth=None):\n         \"\"\"Init an Api() class.\n \n         Scheme, host and path combined create the base-url for the api.\n@@ -783,24 +895,13 @@ def __init__(self, base_url: str, api_token=None, api_version=\"v1\"):\n         Parameters\n         ----------\n         native_api_version : str\n-            Api version of Dataverse native api. Default is `v1`.\n+            API version of Dataverse native API. Default is `v1`.\n \n         \"\"\"\n-        super().__init__(base_url, api_token, api_version)\n+        super().__init__(base_url, api_token, api_version, auth=auth)\n         self.base_url_api_native = self.base_url_api\n \n-    def __str__(self):\n-        \"\"\"Return name of NativeApi() class for users.\n-\n-        Returns\n-        -------\n-        str\n-            Naming of the NativeApi() class.\n-\n-        \"\"\"\n-        return \"Native API: {0}\".format(self.base_url_api_native)\n-\n-    def get_dataverse(self, identifier, auth=False):\n+    def get_dataverse(self, identifier, auth=DEPRECATION_GUARD):\n         \"\"\"Get dataverse metadata by alias or id.\n \n         View metadata about a dataverse.\n@@ -1041,7 +1142,7 @@ def get_dataverse_contents(self, identifier, auth=True):\n         url = \"{0}/dataverses/{1}/contents\".format(self.base_url_api_native, identifier)\n         return self.get_request(url, auth=auth)\n \n-    def get_dataverse_assignments(self, identifier, auth=False):\n+    def get_dataverse_assignments(self, identifier, auth=DEPRECATION_GUARD):\n         \"\"\"Get dataverse assignments by alias or id.\n \n         View assignments of a dataverse.\n@@ -1067,7 +1168,7 @@ def get_dataverse_assignments(self, identifier, auth=False):\n         )\n         return self.get_request(url, auth=auth)\n \n-    def get_dataverse_facets(self, identifier, auth=False):\n+    def get_dataverse_facets(self, identifier, auth=DEPRECATION_GUARD):\n         \"\"\"Get dataverse facets by alias or id.\n \n         View facets of a dataverse.\n@@ -1091,7 +1192,7 @@ def get_dataverse_facets(self, identifier, auth=False):\n         url = \"{0}/dataverses/{1}/facets\".format(self.base_url_api_native, identifier)\n         return self.get_request(url, auth=auth)\n \n-    def dataverse_id2alias(self, dataverse_id, auth=False):\n+    def dataverse_id2alias(self, dataverse_id, auth=DEPRECATION_GUARD):\n         \"\"\"Converts a Dataverse ID to an alias.\n \n         Parameters\n@@ -1240,7 +1341,7 @@ def get_dataset_version(self, identifier, version, auth=True, is_pid=True):\n             )\n         return self.get_request(url, auth=auth)\n \n-    def get_dataset_export(self, pid, export_format, auth=False):\n+    def get_dataset_export(self, pid, export_format, auth=DEPRECATION_GUARD):\n         \"\"\"Get metadata of dataset exported in different formats.\n \n         Export the metadata of the current published version of a dataset\n@@ -1950,7 +2051,7 @@ def replace_datafile(self, identifier, filename, json_str, is_filepid=True):\n             url += \"/files/{0}/replace\".format(identifier)\n         return self.post_request(url, data=data, files=files, auth=True)\n \n-    def get_info_version(self, auth=False):\n+    def get_info_version(self, auth=DEPRECATION_GUARD):\n         \"\"\"Get the Dataverse version and build number.\n \n         The response contains the version and build numbers. Requires no api\n@@ -1971,7 +2072,7 @@ def get_info_version(self, auth=False):\n         url = \"{0}/info/version\".format(self.base_url_api_native)\n         return self.get_request(url, auth=auth)\n \n-    def get_info_server(self, auth=False):\n+    def get_info_server(self, auth=DEPRECATION_GUARD):\n         \"\"\"Get dataverse server name.\n \n         This is useful when a Dataverse system is composed of multiple Java EE\n@@ -1992,7 +2093,7 @@ def get_info_server(self, auth=False):\n         url = \"{0}/info/server\".format(self.base_url_api_native)\n         return self.get_request(url, auth=auth)\n \n-    def get_info_api_terms_of_use(self, auth=False):\n+    def get_info_api_terms_of_use(self, auth=DEPRECATION_GUARD):\n         \"\"\"Get API Terms of Use url.\n \n         The response contains the text value inserted as API Terms of use which\n@@ -2013,7 +2114,7 @@ def get_info_api_terms_of_use(self, auth=False):\n         url = \"{0}/info/apiTermsOfUse\".format(self.base_url_api_native)\n         return self.get_request(url, auth=auth)\n \n-    def get_metadatablocks(self, auth=False):\n+    def get_metadatablocks(self, auth=DEPRECATION_GUARD):\n         \"\"\"Get info about all metadata blocks.\n \n         Lists brief info about all metadata blocks registered in the system.\n@@ -2033,7 +2134,7 @@ def get_metadatablocks(self, auth=False):\n         url = \"{0}/metadatablocks\".format(self.base_url_api_native)\n         return self.get_request(url, auth=auth)\n \n-    def get_metadatablock(self, identifier, auth=False):\n+    def get_metadatablock(self, identifier, auth=DEPRECATION_GUARD):\n         \"\"\"Get info about single metadata block.\n \n         Returns data about the block whose identifier is passed. identifier can\n@@ -2059,7 +2160,7 @@ def get_metadatablock(self, identifier, auth=False):\n         url = \"{0}/metadatablocks/{1}\".format(self.base_url_api_native, identifier)\n         return self.get_request(url, auth=auth)\n \n-    def get_user_api_token_expiration_date(self, auth=False):\n+    def get_user_api_token_expiration_date(self, auth=DEPRECATION_GUARD):\n         \"\"\"Get the expiration date of an Users's API token.\n \n         HTTP Request:\n@@ -2138,7 +2239,7 @@ def create_role(self, dataverse_id):\n         url = \"{0}/roles?dvo={1}\".format(self.base_url_api_native, dataverse_id)\n         return self.post_request(url)\n \n-    def show_role(self, role_id, auth=False):\n+    def show_role(self, role_id, auth=DEPRECATION_GUARD):\n         \"\"\"Show role.\n \n         `Docs <https://guides.dataverse.org/en/latest/api/native-api.html#show-role>`_\n@@ -2459,25 +2560,14 @@ class SearchApi(Api):\n \n     \"\"\"\n \n-    def __init__(self, base_url, api_token=None, api_version=\"latest\"):\n+    def __init__(self, base_url, api_token=None, api_version=\"latest\", *, auth=None):\n         \"\"\"Init an SearchApi() class.\"\"\"\n-        super().__init__(base_url, api_token, api_version)\n+        super().__init__(base_url, api_token, api_version, auth=auth)\n         if base_url:\n             self.base_url_api_search = \"{0}/search?q=\".format(self.base_url_api)\n         else:\n             self.base_url_api_search = self.base_url_api\n \n-    def __str__(self):\n-        \"\"\"Return name of SearchApi() class for users.\n-\n-        Returns\n-        -------\n-        str\n-            Naming of the Search API class.\n-\n-        \"\"\"\n-        return \"Search API: {0}\".format(self.base_url_api_search)\n-\n     def search(\n         self,\n         q_str,\n@@ -2492,7 +2582,7 @@ def search(\n         filter_query=None,\n         show_entity_ids=None,\n         query_entities=None,\n-        auth=False,\n+        auth=DEPRECATION_GUARD,\n     ):\n         \"\"\"Search.\n \n@@ -2547,7 +2637,13 @@ class SwordApi(Api):\n     \"\"\"\n \n     def __init__(\n-        self, base_url, api_version=\"v1.1\", api_token=None, sword_api_version=\"v1.1\"\n+        self,\n+        base_url,\n+        api_version=\"v1.1\",\n+        api_token=None,\n+        sword_api_version=\"v1.1\",\n+        *,\n+        auth=None,\n     ):\n         \"\"\"Init a :class:`SwordApi <pyDataverse.api.SwordApi>` instance.\n \n@@ -2555,9 +2651,27 @@ def __init__(\n         ----------\n         sword_api_version : str\n             Api version of Dataverse SWORD API.\n+        api_token : str | None\n+            An Api token as retrieved from your Dataverse instance.\n+        auth : httpx.Auth\n+            Note that the SWORD API uses a different authentication mechanism\n+            than the native API, in particular it uses `HTTP Basic\n+            Authentication\n+            <https://guides.dataverse.org/en/latest/api/sword.html#sword-auth>`_.\n+            Thus, if you pass an api_token, it will be used as the username in\n+            the HTTP Basic Authentication. If you pass a custom :py:class:`httpx.Auth`, use\n+            :py:class:`httpx.BasicAuth` with an empty password:\n+\n+            .. code-block:: python\n+\n+                sword_api = Api(\n+                    \"https://demo.dataverse.org\", auth=httpx.BasicAuth(username=\"my_token\", password=\"\")\n+                )\n \n         \"\"\"\n-        super().__init__(base_url, api_token, api_version)\n+        if auth is None and api_token is not None:\n+            auth = httpx.BasicAuth(api_token, \"\")\n+        super().__init__(base_url, api_token, api_version, auth=auth)\n         if not isinstance(sword_api_version, (\"\".__class__, \"\".__class__)):\n             raise ApiUrlError(\n                 \"sword_api_version {0} is not a string.\".format(sword_api_version)\n@@ -2572,17 +2686,6 @@ def __init__(\n         else:\n             self.base_url_api_sword = base_url\n \n-    def __str__(self):\n-        \"\"\"Return name of :class:Api() class for users.\n-\n-        Returns\n-        -------\n-        str\n-            Naming of the SWORD API class.\n-\n-        \"\"\"\n-        return \"SWORD API: {0}\".format(self.base_url_api_sword)\n-\n     def get_service_document(self):\n         url = \"{0}/swordv2/service-document\".format(self.base_url_api_sword)\n         return self.get_request(url, auth=True)\ndiff --git a/pyDataverse/auth.py b/pyDataverse/auth.py\nnew file mode 100644\nindex 0000000..3e8c62b\n--- /dev/null\n+++ b/pyDataverse/auth.py\n@@ -0,0 +1,103 @@\n+\"\"\"This module contains authentication handlers compatible with :class:`httpx.Auth`\"\"\"\n+\n+from typing import Generator\n+\n+from httpx import Auth, Request, Response\n+\n+from pyDataverse.exceptions import ApiAuthorizationError\n+\n+\n+class ApiTokenAuth(Auth):\n+    \"\"\"An authentication handler to add an API token as the X-Dataverse-key\n+    header.\n+\n+    For more information on how to retrieve an API token and how it is used,\n+    please refer to https://guides.dataverse.org/en/latest/api/auth.html.\n+    \"\"\"\n+\n+    def __init__(self, api_token: str):\n+        \"\"\"Initializes the auth handler with an API token.\n+\n+        Parameters\n+        ----------\n+        api_token : str\n+            The API token retrieved from your Dataverse instance user profile.\n+\n+        Examples\n+        --------\n+\n+            >>> import os\n+            >>> from pyDataverse.api import DataAccessApi\n+            >>> base_url = 'https://demo.dataverse.org'\n+            >>> api_token_auth = ApiTokenAuth(os.getenv('API_TOKEN'))\n+            >>> api = DataAccessApi(base_url, api_token_auth)\n+\n+        \"\"\"\n+        if not isinstance(api_token, str):\n+            raise ApiAuthorizationError(\"API token passed is not a string.\")\n+        self.api_token = api_token\n+\n+    def auth_flow(self, request: Request) -> Generator[Request, Response, None]:\n+        \"\"\"Adds the X-Dataverse-key header with the API token and yields the\n+        original :class:`httpx.Request`.\n+\n+        Parameters\n+        ----------\n+        request : httpx.Request\n+            The request object which requires authentication headers\n+\n+        Yields\n+        ------\n+        httpx.Request\n+            The original request with modified headers\n+        \"\"\"\n+        request.headers[\"X-Dataverse-key\"] = self.api_token\n+        yield request\n+\n+\n+class BearerTokenAuth(Auth):\n+    \"\"\"An authentication handler to add a Bearer token as defined in `RFC 6750\n+    <https://datatracker.ietf.org/doc/html/rfc6750>`_ to the request.\n+\n+    A bearer token could be obtained from an OIDC provider, for example,\n+    Keycloak.\n+    \"\"\"\n+\n+    def __init__(self, bearer_token: str):\n+        \"\"\"Initializes the auth handler with a bearer token.\n+\n+        Parameters\n+        ----------\n+        bearer_token : str\n+            The bearer token retrieved from your OIDC provider.\n+\n+        Examples\n+        --------\n+\n+            >>> import os\n+            >>> from pyDataverse.api import DataAccessApi\n+            >>> base_url = 'https://demo.dataverse.org'\n+            >>> bearer_token_auth = OAuthBearerTokenAuth(os.getenv('OAUTH_TOKEN'))\n+            >>> api = DataAccessApi(base_url, bearer_token_auth)\n+\n+        \"\"\"\n+        if not isinstance(bearer_token, str):\n+            raise ApiAuthorizationError(\"API token passed is not a string.\")\n+        self.bearer_token = bearer_token\n+\n+    def auth_flow(self, request: Request) -> Generator[Request, Response, None]:\n+        \"\"\"Adds the X-Dataverse-key header with the API token and yields the\n+        original :class:`httpx.Request`.\n+\n+        Parameters\n+        ----------\n+        request : httpx.Request\n+            The request object which requires authentication headers\n+\n+        Yields\n+        ------\n+        httpx.Request\n+            The original request with modified headers\n+        \"\"\"\n+        request.headers[\"Authorization\"] = f\"Bearer {self.bearer_token}\"\n+        yield request\ndiff --git a/pyDataverse/docs/source/reference.rst b/pyDataverse/docs/source/reference.rst\nindex d4787ef..5a76291 100644\n--- a/pyDataverse/docs/source/reference.rst\n+++ b/pyDataverse/docs/source/reference.rst\n@@ -17,6 +17,7 @@ Access all of Dataverse APIs.\n \n .. automodule:: pyDataverse.api\n    :members:\n+   :special-members:\n \n \n Models Interface\n@@ -38,6 +39,13 @@ Helper functions.\n   :members:\n \n \n+Auth Helpers\n+-----------------------------\n+\n+.. automodule:: pyDataverse.auth\n+  :members:\n+\n+\n Exceptions\n -----------------------------\n \ndiff --git a/pyproject.toml b/pyproject.toml\nindex 6c513fa..4e1e8de 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -44,6 +44,7 @@ optional = true\n [tool.poetry.group.tests.dependencies]\n pytest = \"^8.1.1\"\n pytest-cov = \"^5.0.0\"\n+pytest-asyncio = \"^0.23.7\"\n tox = \"^4.14.2\"\n selenium = \"^4.19.0\"\n \n@@ -64,6 +65,7 @@ optional = true\n black = \"^24.3.0\"\n radon = \"^6.0.1\"\n mypy = \"^1.9.0\"\n+types-jsonschema = \"^4.23.0\"\n autopep8 = \"^2.1.0\"\n ruff = \"^0.4.4\"\n \n", "instance_id": "gdcc__pyDataverse-201", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, covering multiple related issues and pull requests (PRs) in the pyDataverse repository. It describes the primary goal of refactoring the authentication mechanism to pass API keys and User-Agent as headers instead of GET parameters, addressing a security concern. Additionally, it introduces a new authentication framework with support for different auth methods (API token, Bearer token, etc.) and deprecates an unused `auth` parameter. The statement includes detailed descriptions of the changes, motivations (e.g., security), and references to specific issues and documentation. However, there are minor ambiguities: the problem statement combines multiple PRs and issues into a single narrative, which can be confusing without clear separation of concerns. Edge cases (e.g., backward compatibility with existing users of the `auth` parameter) are mentioned but not exhaustively detailed, and some context about the broader impact on dependent projects is missing. Overall, it is valid and clear but lacks some finer details for a fully comprehensive score.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, impacting a core component (authentication) across multiple API classes in `pyDataverse/api.py` and introducing a new module (`pyDataverse/auth.py`). It requires understanding and modifying interactions between different parts of the codebase, such as request handling with `httpx` and ensuring compatibility with synchronous and asynchronous modes. The changes involve a moderate amount of code, including refactoring method signatures, adding deprecation warnings, and creating new authentication handlers. Second, the technical concepts involved are moderately complex, requiring knowledge of HTTP authentication mechanisms (Basic Auth, Bearer tokens), `httpx` library internals for custom auth flows, Python type hints, and deprecation strategies. Third, the problem demands handling edge cases like backward compatibility (via deprecation warnings), ensuring no API token leakage in logs, and supporting multiple authentication methods without breaking existing functionality. While it does not drastically alter the system's architecture, it touches a critical area (security and API interaction), necessitating careful testing and validation. It falls short of \"Very Hard\" as it does not require advanced domain-specific knowledge or system-level redesign, but it still demands a deep understanding of the codebase and careful implementation, justifying a score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[REQ] Add x-field-extra-annotation for \"kotlin\" generator\n### Is your feature request related to a problem? Please describe.\r\n\r\nCurrently java generator (as well as some other generators) has x-field-extra-annotation feature, which allows to annotate Java fileds for any model class, generated from openapi spec. You can find description of x-field-extra-annotation property in openapi-generator docs https://openapi-generator.tech/docs/generators/java/\r\nCurrently kotlin generator doesn't support this feature as you can find in docs https://openapi-generator.tech/docs/generators/kotlin/\r\n\r\n## Describe the solution you'd like\r\n\r\nThe request is to add support for x-field-extra-annotation feature for \"kotlin\" generator. So kotlin models can be generated with additional field annotations. This is especially vital when you need to provide custom <span>@</span>JsonSerialize or JsonDeserialize annotation to deserialize or serialize model correctly. Currently I didn't found any workaround to provide custom JsonSerialize/JsonDeserialize annotations for model fields.\r\n\r\n## Describe alternatives you've considered\r\n\r\nThe only alternative solution I can see so far is to define customized template for kotlin model generator, to manually add support for x-field-extra-annotation attribute.\r\n\r\n## Additional context\r\n\r\nHere is an example of openapi spec and expected generated model\r\n\r\nOpenAPI spec:\r\n```yaml\r\nAccount:\r\n  type: \"object\"\r\n  required:\r\n    - \"name\"\r\n    - \"assets\"\r\n  properties:\r\n    name:\r\n      type: \"string\"\r\n    assets:\r\n      type: \"object\"\r\n      additionalProperties:\r\n        type: \"string\"\r\n      x-field-extra-annotation: \"@JsonDeserialize(converter = AssetConverter::class)\"\r\n```\r\n\r\nGenerated Model:\r\n```kotlin\r\ndata class Account (\r\n\r\n    @field:JsonProperty(\"name\")\r\n    val name: kotlin.String,\r\n\r\n    @field:JsonProperty(\"assets\")\r\n    @JsonDeserialize(converter = AssetConverter::class)\r\n    val assets: Map<String, String>\r\n)\r\n```\r\n\r\n\n", "patch": "diff --git a/docs/generators/kotlin.md b/docs/generators/kotlin.md\nindex 82011d0e54b6..7dbee25f2f7e 100644\n--- a/docs/generators/kotlin.md\n+++ b/docs/generators/kotlin.md\n@@ -51,6 +51,14 @@ These options may be applied as additional-properties (cli) or configOptions (pl\n |useSettingsGradle|Whether the project uses settings.gradle.| |false|\n |useSpringBoot3|Whether to use the Spring Boot 3 with the jvm-spring-webclient library.| |false|\n \n+## SUPPORTED VENDOR EXTENSIONS\n+\n+| Extension name | Description | Applicable for | Default value |\n+| -------------- | ----------- | -------------- | ------------- |\n+|x-class-extra-annotation|List of custom annotations to be added to model|MODEL|null\n+|x-field-extra-annotation|List of custom annotations to be added to property|FIELD, OPERATION_PARAMETER|null\n+\n+\n ## IMPORT MAPPING\n \n | Type/Alias | Imports |\ndiff --git a/modules/openapi-generator/src/main/java/org/openapitools/codegen/languages/KotlinClientCodegen.java b/modules/openapi-generator/src/main/java/org/openapitools/codegen/languages/KotlinClientCodegen.java\nindex 1310e2861912..69258bdf092b 100644\n--- a/modules/openapi-generator/src/main/java/org/openapitools/codegen/languages/KotlinClientCodegen.java\n+++ b/modules/openapi-generator/src/main/java/org/openapitools/codegen/languages/KotlinClientCodegen.java\n@@ -38,6 +38,7 @@\n import org.openapitools.codegen.CodegenProperty;\n import org.openapitools.codegen.CodegenType;\n import org.openapitools.codegen.SupportingFile;\n+import org.openapitools.codegen.VendorExtension;\n import org.openapitools.codegen.meta.features.ClientModificationFeature;\n import org.openapitools.codegen.meta.features.DocumentationFeature;\n import org.openapitools.codegen.meta.features.GlobalFeature;\n@@ -1049,4 +1050,12 @@ public void postProcess() {\n         System.out.println(\"# Please support his work directly via https://patreon.com/jimschubert \\uD83D\\uDE4F      #\");\n         System.out.println(\"################################################################################\");\n     }\n+\n+    @Override\n+    public List<VendorExtension> getSupportedVendorExtensions() {\n+        var extensions = super.getSupportedVendorExtensions();\n+        extensions.add(VendorExtension.X_CLASS_EXTRA_ANNOTATION);\n+        extensions.add(VendorExtension.X_FIELD_EXTRA_ANNOTATION);\n+        return extensions;\n+    }\n }\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/data_class.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/data_class.mustache\nindex b6d2b11522f9..394af616a128 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/data_class.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/data_class.mustache\n@@ -74,6 +74,9 @@ import {{packageName}}.infrastructure.ITransformForStorage\n @Deprecated(message = \"This schema is deprecated.\")\n {{/isDeprecated}}\n {{>additionalModelTypeAnnotations}}\n+{{#vendorExtensions.x-class-extra-annotation}}\n+{{{vendorExtensions.x-class-extra-annotation}}}\n+{{/vendorExtensions.x-class-extra-annotation}}\n {{#nonPublicApi}}internal {{/nonPublicApi}}{{#discriminator}}interface{{/discriminator}}{{^discriminator}}{{#hasVars}}data {{/hasVars}}class{{/discriminator}} {{classname}}{{^discriminator}} (\n \n {{#allVars}}\n@@ -194,7 +197,7 @@ import {{packageName}}.infrastructure.ITransformForStorage\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             {{#allVars}}\n             {{#-first}}\n@@ -210,7 +213,7 @@ import {{packageName}}.infrastructure.ITransformForStorage\n             openapiRequiredFields.add(\"{{baseName}}\")\n             {{/requiredVars}}\n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\n@@ -227,7 +230,7 @@ import {{packageName}}.infrastructure.ITransformForStorage\n             {{^hasChildren}}\n             {{#requiredVars}}\n             {{#-first}}\n-      \n+\n             // check to make sure all required properties/fields are present in the JSON string\n             for (requiredField in openapiRequiredFields) {\n               requireNotNull(jsonElement!!.getAsJsonObject()[requiredField]) {\n@@ -249,7 +252,7 @@ import {{packageName}}.infrastructure.ITransformForStorage\n             if (!jsonObj.get(\"{{{baseName}}}\").isJsonArray) {\n               throw IllegalArgumentException(String.format(\"Expected the field `{{{baseName}}}` to be an array in the JSON string but got `%s`\", jsonObj[\"{{{baseName}}}\"].toString()))\n             }\n-      \n+\n             // validate the required field `{{{baseName}}}` (array)\n             for (i in 0 until jsonObj.getAsJsonArray(\"{{{baseName}}}\").size()) {\n               {{{items.dataType}}}.validateJsonElement(jsonObj.getAsJsonArray(\"{{{baseName}}}\").get(i))\n@@ -262,7 +265,7 @@ import {{packageName}}.infrastructure.ITransformForStorage\n                 require(jsonObj[\"{{{baseName}}}\"].isJsonArray) {\n                   String.format(\"Expected the field `{{{baseName}}}` to be an array in the JSON string but got `%s`\", jsonObj[\"{{{baseName}}}\"].toString())\n                 }\n-      \n+\n                 // validate the optional field `{{{baseName}}}` (array)\n                 for (i in 0 until jsonObj.getAsJsonArray(\"{{{baseName}}}\").size()) {\n                   {{{items.dataType}}}.validateJsonElement(jsonObj.getAsJsonArray(\"{{{baseName}}}\").get(i))\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/data_class_opt_var.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/data_class_opt_var.mustache\nindex 1609fa8656e4..897b18f9e599 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/data_class_opt_var.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/data_class_opt_var.mustache\n@@ -15,6 +15,9 @@\n     {{^isEnum}}{{^isArray}}{{^isPrimitiveType}}{{^isModel}}@Contextual {{/isModel}}{{/isPrimitiveType}}{{/isArray}}{{/isEnum}}@SerialName(value = \"{{{vendorExtensions.x-base-name-literal}}}\")\n     {{/kotlinx_serialization}}\n     {{/multiplatform}}\n+    {{#vendorExtensions.x-field-extra-annotation}}\n+    {{{vendorExtensions.x-field-extra-annotation}}}\n+    {{/vendorExtensions.x-field-extra-annotation}}\n     {{#deprecated}}\n     @Deprecated(message = \"This property is deprecated.\")\n     {{/deprecated}}\ndiff --git a/modules/openapi-generator/src/main/resources/kotlin-client/data_class_req_var.mustache b/modules/openapi-generator/src/main/resources/kotlin-client/data_class_req_var.mustache\nindex 3c9387d10151..811867dfc51f 100644\n--- a/modules/openapi-generator/src/main/resources/kotlin-client/data_class_req_var.mustache\n+++ b/modules/openapi-generator/src/main/resources/kotlin-client/data_class_req_var.mustache\n@@ -15,6 +15,9 @@\n     {{^isEnum}}{{^isArray}}{{^isPrimitiveType}}{{^isModel}}@Contextual {{/isModel}}{{/isPrimitiveType}}{{/isArray}}{{/isEnum}}@SerialName(value = \"{{{vendorExtensions.x-base-name-literal}}}\")\n     {{/kotlinx_serialization}}\n     {{/multiplatform}}\n+    {{#vendorExtensions.x-field-extra-annotation}}\n+    {{{vendorExtensions.x-field-extra-annotation}}}\n+    {{/vendorExtensions.x-field-extra-annotation}}\n     {{#deprecated}}\n     @Deprecated(message = \"This property is deprecated.\")\n     {{/deprecated}}\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiAnnotation.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiAnnotation.kt\nindex bd33ca92746f..457d2586a3dc 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiAnnotation.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiAnnotation.kt\n@@ -71,13 +71,13 @@ data class ApiAnnotation (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"id\")\n \n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiApiResponse.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiApiResponse.kt\nindex 018bb458a120..b51d977e5208 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiApiResponse.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiApiResponse.kt\n@@ -79,7 +79,7 @@ data class ApiApiResponse (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"code\")\n@@ -87,7 +87,7 @@ data class ApiApiResponse (\n             openapiFields.add(\"message\")\n \n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiCategory.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiCategory.kt\nindex 4d2ab888bb38..38d92229ede2 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiCategory.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiCategory.kt\n@@ -75,14 +75,14 @@ data class ApiCategory (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"id\")\n             openapiFields.add(\"name\")\n \n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiOrder.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiOrder.kt\nindex af39fc31cfc6..e5bb80c0639e 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiOrder.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiOrder.kt\n@@ -102,7 +102,7 @@ data class ApiOrder (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"id\")\n@@ -113,7 +113,7 @@ data class ApiOrder (\n             openapiFields.add(\"complete\")\n \n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiPet.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiPet.kt\nindex 35871da8730d..f47705a67d6e 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiPet.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiPet.kt\n@@ -105,7 +105,7 @@ data class ApiPet (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"name\")\n@@ -119,7 +119,7 @@ data class ApiPet (\n             openapiRequiredFields.add(\"name\")\n             openapiRequiredFields.add(\"photoUrls\")\n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\n@@ -133,7 +133,7 @@ data class ApiPet (\n                 String.format(\"The required field(s) %s in ApiPet is not found in the empty JSON string\", ApiPet.openapiRequiredFields.toString())\n               }\n             }\n-      \n+\n             // check to make sure all required properties/fields are present in the JSON string\n             for (requiredField in openapiRequiredFields) {\n               requireNotNull(jsonElement!!.getAsJsonObject()[requiredField]) {\n@@ -161,7 +161,7 @@ data class ApiPet (\n                 require(jsonObj[\"tags\"].isJsonArray) {\n                   String.format(\"Expected the field `tags` to be an array in the JSON string but got `%s`\", jsonObj[\"tags\"].toString())\n                 }\n-      \n+\n                 // validate the optional field `tags` (array)\n                 for (i in 0 until jsonObj.getAsJsonArray(\"tags\").size()) {\n                   ApiTag.validateJsonElement(jsonObj.getAsJsonArray(\"tags\").get(i))\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiTag.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiTag.kt\nindex 8f4b05dcd09d..8418ea6512b7 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiTag.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiTag.kt\n@@ -75,14 +75,14 @@ data class ApiTag (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"id\")\n             openapiFields.add(\"name\")\n \n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\ndiff --git a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiUser.kt b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiUser.kt\nindex d47f6636a72a..dc5ff32bd7cd 100644\n--- a/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiUser.kt\n+++ b/samples/client/petstore/kotlin-model-prefix-type-mappings/src/main/kotlin/org/openapitools/client/models/ApiUser.kt\n@@ -100,7 +100,7 @@ data class ApiUser (\n     companion object {\n         var openapiFields = HashSet<String>()\n         var openapiRequiredFields = HashSet<String>()\n-      \n+\n         init {\n             // a set of all properties/fields (JSON key names)\n             openapiFields.add(\"username\")\n@@ -115,7 +115,7 @@ data class ApiUser (\n             // a set of required properties/fields (JSON key names)\n             openapiRequiredFields.add(\"username\")\n         }\n-      \n+\n        /**\n         * Validates the JSON Element and throws an exception if issues found\n         *\n@@ -129,7 +129,7 @@ data class ApiUser (\n                 String.format(\"The required field(s) %s in ApiUser is not found in the empty JSON string\", ApiUser.openapiRequiredFields.toString())\n               }\n             }\n-      \n+\n             // check to make sure all required properties/fields are present in the JSON string\n             for (requiredField in openapiRequiredFields) {\n               requireNotNull(jsonElement!!.getAsJsonObject()[requiredField]) {\n", "instance_id": "OpenAPITools__openapi-generator-19899", "clarity": 3, "difficulty": 0.45, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly describes the feature request to add support for the `x-field-extra-annotation` in the Kotlin generator of the OpenAPI Generator tool, mirroring functionality already present in the Java generator. The goal is explicit: to enable custom field annotations (e.g., for JSON serialization/deserialization) in generated Kotlin models. The input (OpenAPI spec with vendor extensions) and expected output (generated Kotlin model with annotations) are provided with concrete examples, making the requirements unambiguous. Additionally, the problem statement includes context about alternatives considered and links to relevant documentation, further clarifying the scope and intent. There are no significant ambiguities or missing critical details, as the desired behavior and use case are well-articulated.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes involves multiple files, including modifications to the Kotlin generator's Java codebase (`KotlinClientCodegen.java`), Mustache templates for code generation (`data_class.mustache`, `data_class_opt_var.mustache`, `data_class_req_var.mustache`), and documentation updates (`kotlin.md`). This requires understanding the interaction between the generator's logic and its templating system. Second, the technical concepts involved include familiarity with OpenAPI Generator's vendor extension mechanism, Java-based code generation for Kotlin, and Mustache templating, which are moderately complex but not overly advanced. Third, the changes do not significantly impact the system's architecture; they extend existing functionality in a straightforward manner by adding support for vendor extensions already implemented for other generators. Finally, edge cases and error handling are not explicitly mentioned in the problem statement, and the code changes do not introduce complex error handling logic beyond what is already present in the templates. Overall, this task requires a moderate level of understanding of the codebase and involves changes across several components, but it does not demand deep architectural redesign or advanced domain-specific knowledge, placing it at the lower end of the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "LMDB on Windows creates unusable libraries when using invalid characters\n### Describe the bug\n\nWhen using any of the [unallowed windows directory characters](https://stackoverflow.com/questions/1976007/what-characters-are-forbidden-in-windows-and-linux-directory-names) in a library name on Windows LMDB, the library is written but is unusable.\r\n\r\nCreating the library appropriately raises an exception but when listing the libraries it wrongly appears in the list and it's unusable.\n\n### Steps/Code to Reproduce\n\n```\r\n>>> from arcticdb import Arctic\r\n>>> ac = Arctic(\"lmdb://test\")\r\n>>> ac.create_library(\"lib|1\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\iddil\\source\\venvs\\310\\Lib\\site-packages\\arcticdb\\arctic.py\", line 290, in create_library\r\n    return self.get_library(name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\iddil\\source\\venvs\\310\\Lib\\site-packages\\arcticdb\\arctic.py\", line 239, in get_library\r\n    lib = self[name]\r\n          ~~~~^^^^^^\r\n  File \"C:\\Users\\iddil\\source\\venvs\\310\\Lib\\site-packages\\arcticdb\\arctic.py\", line 190, in __getitem__\r\n    self._library_manager.get_library(name, storage_override),\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\narcticdb_ext.exceptions.InternalException: (create_directories: The filename, directory name, or volume label syntax is incorrect.: \"C:\\Users\\iddil\\test\\test\\lib|1\")\r\n>>> # Listing the libraries contains the library which failed on creation\r\n>>> ac.list_libraries()\r\n['lib|1']\r\n>>> # We can't access the library\r\n>>> lib = ac['lib|1']\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\iddil\\source\\venvs\\310\\Lib\\site-packages\\arcticdb\\arctic.py\", line 190, in __getitem__\r\n    self._library_manager.get_library(name, storage_override),\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\narcticdb_ext.exceptions.InternalException: (create_directories: The filename, directory name, or volume label syntax is incorrect.: \"C:\\Users\\iddil\\test\\test\\lib|1\")\r\n```\n\n### Expected Results\n\nA better exception is raised when using an invalid character on Windows LMDB and list_libraries should not contain the failed library.\n\n### OS, Python Version and ArcticDB Version\n\nWindows, Python 3.11.7\n\n### Backend storage used\n\nLMDB\n\n### Additional Context\n\n_No response_\n", "patch": "diff --git a/cpp/arcticdb/async/async_store.hpp b/cpp/arcticdb/async/async_store.hpp\nindex 3c105e1adc..6f2d6c9413 100644\n--- a/cpp/arcticdb/async/async_store.hpp\n+++ b/cpp/arcticdb/async/async_store.hpp\n@@ -147,6 +147,10 @@ entity::VariantKey write_sync(\n     return WriteSegmentTask{library_}(std::move(encoded));\n }\n \n+bool is_path_valid(const std::string_view path) const override {\n+    return library_->is_path_valid(path);\n+}\n+\n folly::Future<folly::Unit> write_compressed(storage::KeySegmentPair &&ks) override {\n     return async::submit_io_task(WriteCompressedTask{std::move(ks), library_});\n }\ndiff --git a/cpp/arcticdb/storage/library.hpp b/cpp/arcticdb/storage/library.hpp\nindex bc02a2f653..3e5122b636 100644\n--- a/cpp/arcticdb/storage/library.hpp\n+++ b/cpp/arcticdb/storage/library.hpp\n@@ -130,6 +130,10 @@ class Library {\n         return storages_->key_exists(key);\n     }\n \n+    bool is_path_valid(const std::string_view path) const {\n+        return storages_->is_path_valid(path);\n+    }\n+\n     KeySegmentPair read(VariantKey key, ReadKeyOpts opts = ReadKeyOpts{}) {\n         KeySegmentPair res{VariantKey{key}};\n         util::check(!std::holds_alternative<StringId>(variant_key_id(key)) || !std::get<StringId>(variant_key_id(key)).empty(), \"Unexpected empty id\");\ndiff --git a/cpp/arcticdb/storage/library_manager.cpp b/cpp/arcticdb/storage/library_manager.cpp\nindex 8072f413bb..098f48c300 100644\n--- a/cpp/arcticdb/storage/library_manager.cpp\n+++ b/cpp/arcticdb/storage/library_manager.cpp\n@@ -103,7 +103,7 @@ void LibraryManager::write_library_config(const py::object& lib_cfg, const Libra\n     segment.set_metadata(std::move(output));\n \n     auto library_name = path.to_delim_path();\n-    verify_library_path_on_write(library_name);\n+    verify_library_path_on_write(store_.get(), library_name);\n \n     store_->write_sync(\n             entity::KeyType::LIBRARY_CONFIG,\ndiff --git a/cpp/arcticdb/storage/lmdb/lmdb_storage.cpp b/cpp/arcticdb/storage/lmdb/lmdb_storage.cpp\nindex 0559d1f60d..db1879cd2a 100644\n--- a/cpp/arcticdb/storage/lmdb/lmdb_storage.cpp\n+++ b/cpp/arcticdb/storage/lmdb/lmdb_storage.cpp\n@@ -259,6 +259,23 @@ void LmdbStorage::do_iterate_type(KeyType key_type, const IterateTypeVisitor& vi\n     }\n }\n \n+bool LmdbStorage::do_is_path_valid(const std::string_view pathString) const {\n+#ifdef _WIN32\n+    std::string_view invalid_win32_chars = \"<>:\\\"|?*\";\n+    auto found = pathString.find_first_of(invalid_win32_chars);\n+    if (found != std::string::npos) {\n+        return false;\n+    }\n+\n+    if (!pathString.empty() && (pathString.back() == '.' || std::isspace(pathString.back()))) {\n+        return false;\n+    }\n+#else\n+    (void) pathString; // suppress -Werror=unused-parameter\n+#endif\n+    return true;\n+}\n+\n void remove_db_files(const fs::path& lib_path) {\n     std::vector<std::string> files = {\"lock.mdb\", \"data.mdb\"};\n \ndiff --git a/cpp/arcticdb/storage/lmdb/lmdb_storage.hpp b/cpp/arcticdb/storage/lmdb/lmdb_storage.hpp\nindex 704472db71..5629738084 100644\n--- a/cpp/arcticdb/storage/lmdb/lmdb_storage.hpp\n+++ b/cpp/arcticdb/storage/lmdb/lmdb_storage.hpp\n@@ -52,6 +52,8 @@ class LmdbStorage final : public Storage {\n \n     bool do_key_exists(const VariantKey & key) final;\n \n+    bool do_is_path_valid(const std::string_view path) const final;\n+\n     ::lmdb::env& env() {\n         if (!env_) {\n             raise<ErrorCode::E_UNEXPECTED_LMDB_ERROR>(\"Unexpected LMDB Error: Invalid operation: LMDB environment has been removed. \"\ndiff --git a/cpp/arcticdb/storage/storage.hpp b/cpp/arcticdb/storage/storage.hpp\nindex c0da8795c0..9825d4415c 100644\n--- a/cpp/arcticdb/storage/storage.hpp\n+++ b/cpp/arcticdb/storage/storage.hpp\n@@ -166,6 +166,10 @@ class Storage {\n         return do_key_path(key);\n     }\n \n+    bool is_path_valid(const std::string_view path) const {\n+        return do_is_path_valid(path);\n+    }\n+\n     [[nodiscard]] const LibraryPath &library_path() const { return lib_path_; }\n     [[nodiscard]] OpenMode open_mode() const { return mode_; }\n \n@@ -188,6 +192,8 @@ class Storage {\n \n     virtual std::string do_key_path(const VariantKey& key) const = 0;\n \n+    virtual bool do_is_path_valid(const std::string_view) const { return true; }\n+\n     LibraryPath lib_path_;\n     OpenMode mode_;\n };\ndiff --git a/cpp/arcticdb/storage/storages.hpp b/cpp/arcticdb/storage/storages.hpp\nindex 8199a3746e..20e447f2ba 100644\n--- a/cpp/arcticdb/storage/storages.hpp\n+++ b/cpp/arcticdb/storage/storages.hpp\n@@ -67,6 +67,10 @@ class Storages {\n         return primary().key_exists(key);\n     }\n \n+    bool is_path_valid(const std::string_view path) const {\n+        return primary().is_path_valid(path);\n+    }\n+\n     auto read(Composite<VariantKey>&& ks, const ReadVisitor& visitor, ReadKeyOpts opts, bool primary_only=true) {\n         ARCTICDB_RUNTIME_SAMPLE(StoragesRead, 0)\n         if(primary_only)\ndiff --git a/cpp/arcticdb/stream/stream_sink.hpp b/cpp/arcticdb/stream/stream_sink.hpp\nindex abbf5363b4..5b252e85a9 100644\n--- a/cpp/arcticdb/stream/stream_sink.hpp\n+++ b/cpp/arcticdb/stream/stream_sink.hpp\n@@ -112,6 +112,8 @@ struct StreamSink {\n         folly::Future<std::tuple<PartialKey, SegmentInMemory, pipelines::FrameSlice>> &&input_fut,\n         const std::shared_ptr<DeDupMap> &de_dup_map) = 0;\n \n+    virtual bool is_path_valid(const std::string_view path) const = 0;\n+\n     [[nodiscard]] virtual folly::Future<folly::Unit> batch_write_compressed(\n         std::vector<storage::KeySegmentPair> kvs) = 0;\n \ndiff --git a/cpp/arcticdb/util/name_validation.cpp b/cpp/arcticdb/util/name_validation.cpp\nindex fbf0ac3849..b066b97595 100644\n--- a/cpp/arcticdb/util/name_validation.cpp\n+++ b/cpp/arcticdb/util/name_validation.cpp\n@@ -11,6 +11,7 @@\n #include <arcticdb/entity/types.hpp>\n #include <arcticdb/util/configs_map.hpp>\n #include <arcticdb/stream/index.hpp>\n+#include <arcticdb/storage/store.hpp>\n \n namespace arcticdb {\n \n@@ -118,8 +119,14 @@ void verify_library_path_part(const std::string& library_part, char delim) {\n     }\n }\n \n-void verify_library_path_on_write(const StringId& library_path) {\n+void verify_library_path_on_write(const Store* store, const StringId& library_path) {\n     verify_name(\"library name\", library_path, true, UNSUPPORTED_S3_CHARS);\n+    if(!store->is_path_valid(library_path)) {\n+        user_input::raise<ErrorCode::E_INVALID_CHAR_IN_NAME>(\n+                \"The library name contains unsupported chars. Library Name: {}\",\n+                library_path\n+        );\n+    }\n }\n \n }\n\\ No newline at end of file\ndiff --git a/cpp/arcticdb/util/name_validation.hpp b/cpp/arcticdb/util/name_validation.hpp\nindex 95673e24ef..ad0587e433 100644\n--- a/cpp/arcticdb/util/name_validation.hpp\n+++ b/cpp/arcticdb/util/name_validation.hpp\n@@ -11,6 +11,7 @@\n #include <arcticdb/entity/types.hpp>\n #include <arcticdb/util/configs_map.hpp>\n #include <arcticdb/stream/index.hpp>\n+#include <arcticdb/pipeline/index_segment_reader.hpp>\n \n namespace arcticdb {\n \n@@ -21,7 +22,7 @@ void verify_symbol_key(const StreamId &symbol_key);\n // Does strict checks on library names and raises UserInputException if it encounters an error.\n // Should be checked only when writing new libraries to allow for backwards compatibility\n // with old invalid libraries.\n-void verify_library_path_on_write(const StringId& library_path);\n+void verify_library_path_on_write(const Store* store, const StringId& library_path);\n \n // These two do relaxed checks which should always be run on each library operation (including\n // already existing libraries). These raise friendly error messages instead of segfaulting or\n", "instance_id": "man-group__ArcticDB-1481", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to LMDB on Windows when using invalid characters in library names. It provides a detailed reproduction script, expected results, and context about the environment (Windows, Python version, and backend storage). The goal is evident: prevent the creation of unusable libraries with invalid characters and ensure they do not appear in the library list. However, there are minor ambiguities, such as the lack of explicit mention of all edge cases (e.g., specific invalid characters beyond the general reference to Windows restrictions) and no detailed specification on how the exception message should be formatted or what exact behavior should occur when invalid characters are detected. Additionally, the problem statement does not clarify if there are any performance or compatibility considerations for the fix. Overall, it is clear enough to understand the issue and intent but misses some finer details that could aid in a comprehensive solution.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (e.g., storage, library, and utility modules) in a C++ codebase, indicating a need to understand interactions between different components like the storage backend (LMDB) and library management. The changes involve adding a new validation mechanism for paths, which requires integrating a new method (`is_path_valid`) across the storage hierarchy and updating the library path verification logic to use this method. This suggests a moderate level of architectural understanding is needed, though the changes do not appear to fundamentally alter the system's design.\n\nSecond, the technical concepts involved include platform-specific behavior (Windows path restrictions), string manipulation, and error handling in C++. While these are not overly complex for an experienced developer, they do require attention to detail, especially in ensuring that the validation logic correctly identifies invalid characters and trailing spaces/dots on Windows without breaking compatibility on other platforms (as seen in the conditional compilation with `#ifdef _WIN32`).\n\nThird, the problem touches on edge cases related to invalid characters and path formats, but these are relatively straightforward to handle with basic string checks. The error handling logic needs to be updated to raise a user-friendly exception, which adds a small layer of complexity but is not particularly challenging.\n\nOverall, this task requires a moderate understanding of the codebase and careful implementation across multiple files, but it does not involve advanced algorithms, deep system-level changes, or complex domain-specific knowledge. A score of 0.50 reflects this balance of moderate complexity and scope, suitable for a developer with intermediate experience in C++ and familiarity with platform-specific constraints.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update python matrix (add 3.10, 3.11, 3.12 - drop 3.7)\n### Description\r\n\r\n- Removes python 3.7\r\n    - Because `conda` no longer supports 3.7\r\n- Adds python 3.10, 3.11, and 3.12\r\n    - All Supported by `conda` now\r\n\r\n### Checklist - did you ...\r\n\r\n- [x] Add a file to the `news` directory ([using the template](../blob/main/news/TEMPLATE)) for the next release's release notes?\r\n- [ ] Add / update necessary tests?\r\n- [ ] Add / update outdated documentation?\r\n    - I don't see any documentation that shows the python version range\r\n\r\n### Extra Note\r\n\r\nIn `python 3.12` this now raises the following warning:\r\n\r\n> `DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.`\r\n\r\nThat relates to https://docs.python.org/3/library/tarfile.html#extraction-filters and specifically in 3.8->3.12 an non-specified filter will default to `fully_trusted` which may not be secure.  I went with *not* altering the behavior in this PR, but a followup issue should be filed/acted on to decide how we want to proceed.\r\n\r\nThis is a warning that is also present on `3.12` CI for `conda` itself.\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 7d2d35c9..a8b39733 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -19,7 +19,7 @@ repos:\n     rev: v3.15.0\n     hooks:\n       - id: pyupgrade\n-        args: [\"--py37-plus\"]\n+        args: [\"--py38-plus\"]\n   - repo: https://github.com/PyCQA/isort\n     rev: 5.12.0\n     hooks:\ndiff --git a/news/python-update b/news/python-update\nnew file mode 100644\nindex 00000000..c9cf0d4d\n--- /dev/null\n+++ b/news/python-update\n@@ -0,0 +1,19 @@\n+### Enhancements\n+\n+* Added formal support for Python 3.10, 3.11, and 3.12. (#231)\n+\n+### Bug fixes\n+\n+* <news item>\n+\n+### Deprecations\n+\n+* Removed formal support for Python 3.7. (#231)\n+\n+### Docs\n+\n+* <news item>\n+\n+### Other\n+\n+* <news item>\ndiff --git a/setup.py b/setup.py\nindex f6ae5832..8d1f0309 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -24,7 +24,7 @@\n     entry_points={\"console_scripts\": [\"cph=conda_package_handling.cli:main\"]},\n     keywords=\"conda-package-handling\",\n     classifiers=[\"Programming Language :: Python :: 3\"],\n-    python_requires=\">=3.7\",\n+    python_requires=\">=3.8\",\n     install_requires=[\"conda-package-streaming >= 0.9.0\"],\n     extras_require={\n         \"docs\": [\"furo\", \"sphinx\", \"sphinx-argparse\", \"myst-parser\", \"mdit-py-plugins>=0.3.0\"],\n", "instance_id": "conda__conda-package-handling-231", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update the supported Python versions by removing Python 3.7 and adding support for Python 3.10, 3.11, and 3.12. The reasoning (e.g., conda no longer supporting 3.7 and newer versions being supported) is provided, which adds context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention which files or configurations need to be updated (though this is implied in the code changes). Additionally, while a deprecation warning for Python 3.12 is mentioned, there is no clear guidance on whether immediate action is required or how it should influence the current changes. The checklist provided also raises questions about documentation and testing, as it is unclear whether these are relevant to the task or simply part of a template. Overall, the goal is understandable, but minor details and potential follow-up actions are not fully specified.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The code changes are minimal and straightforward, involving updates to a few configuration files (e.g., changing the Python version requirement in `setup.py` from 3.7 to 3.8, updating `pyupgrade` arguments in `.pre-commit-config.yaml`, and adding a news entry). The scope is limited to a small number of files with no significant impact on the broader codebase or system architecture. The technical concepts required are basic\u2014understanding Python versioning and configuration files. There are no complex algorithms, design patterns, or domain-specific knowledge needed. While a deprecation warning for Python 3.12 is mentioned, it does not require immediate action or complex error handling in this PR, as noted by the author. Edge cases are not a significant concern here, as the task is primarily about version compatibility updates rather than functional changes. Overall, this is a very easy task that requires only basic modifications and minimal understanding of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Introduces the new IcebergSink based on the new V2 Flink Sink Abstraction\nCo-authored-by: Liwei Li <hililiwei@gmail.com>\r\nCo-authored-by: Kyle Bendickson <kjbendickson@gmail.com>\r\nCo-authored-by: Peter Vary <peter.vary.apache@gmail.com>\r\n\r\n*Summary*\r\n\r\nThe Flink community created a new Sink specification in [FLIP-143](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API) with the explicit goal to guarantee the unified handling of the bounded and unbounded data streams. Later it was enhanced in [FLIP-191](https://cwiki.apache.org/confluence/display/FLINK/FLIP-191%3A+Extend+unified+Sink+interface+to+support+small+file+compaction) so there is a well defined place to execute small files compaction. The deprecation of the old SinkFunction is postponed to somewhere around Flink 2.0 based on the discussion on the dev mailing list , so the migration is not extremely urgent, but having the possibility to use the PostCommitTopology to execute the compaction of the small files could provide immediate benefits for the users of the Iceberg-Flink integration.\r\n\r\n*Previous work*\r\n\r\n1. There is an existing Iceberg PR https://github.com/apache/iceberg/pull/4904 for the Sink migration by Liwei Li (https://github.com/hililiwei) and Kyle Bendickson (https://github.com/kbendick) with the [related documentation](https://docs.google.com/document/d/1G4O6JidAoKgbIdy8Ts73OfG_KBEMpsW-LkXIb89I5k8/edit#heading=h.qqlw5ghn3vp7) which is authored by the same team. The discussion there is stuck, and the PR has been out of date for almost a year now. The current proposal builds heavily on their work and wants to keep them as the co-authors for the proposed change.\r\n\r\n2. @pvary opened https://github.com/apache/iceberg/pull/8653 which this code is based on. On his PR, lots of code is refactored so that the V1 Sink (FlinkSink) and the new V2 (IcebergSink) can reuse and share many code paths and components. \r\n\r\n*New Implementation*\r\n\r\nThis PR introduces a brand new `IcebergSink`, but it doesn't change the existing `FlinkSink` \r\n\r\nWith Flink 1.19 released, we now have access to the new Hooks like `SupportsPreWriteTopology`, `SupportsCommitter`, `SupportsPreCommitTopology` and `SupportsPostCommitTopology`. The code in this PR takes advantage of those interfaces to fully implement a brand new `IcebergSink` using the new V2 Sink Model. \r\n\r\nThe new `IcebergSink` implemented here, is needed for the `Flink Table maintenance` effort led by @pvary and it's described in: https://docs.google.com/document/d/16g3vR18mVBy8jbFaLjf2JwAANuYOmIwr15yDDxovdnA/edit?pli=1#heading=h.jl0dmuup12gz\r\n\r\nParticularly, the new `IcebergSink` implements \r\n```java\r\npublic void addPostCommitTopology(DataStream<CommittableMessage<SinkCommittable>> committables) {\r\n```\r\nwhich can be used to perform cleaning/compacting as described in the above doc.\r\n\r\n\r\n\n", "patch": "diff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java\nindex c7e8a2dea7cb..9571efdc5268 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java\n@@ -33,9 +33,14 @@\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n class FlinkManifestUtil {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(FlinkManifestUtil.class);\n   private static final int FORMAT_V2 = 2;\n   private static final Long DUMMY_SNAPSHOT_ID = 0L;\n \n@@ -129,4 +134,26 @@ static WriteResult readCompletedFiles(\n \n     return builder.addReferencedDataFiles(deltaManifests.referencedDataFiles()).build();\n   }\n+\n+  static void deleteCommittedManifests(\n+      Table table, List<ManifestFile> manifests, String newFlinkJobId, long checkpointId) {\n+    for (ManifestFile manifest : manifests) {\n+      try {\n+        table.io().deleteFile(manifest.path());\n+      } catch (Exception e) {\n+        // The flink manifests cleaning failure shouldn't abort the completed checkpoint.\n+        String details =\n+            MoreObjects.toStringHelper(FlinkManifestUtil.class)\n+                .add(\"tableName\", table.name())\n+                .add(\"flinkJobId\", newFlinkJobId)\n+                .add(\"checkpointId\", checkpointId)\n+                .add(\"manifestPath\", manifest.path())\n+                .toString();\n+        LOG.warn(\n+            \"The iceberg transaction has been committed, but we failed to clean the temporary flink manifests: {}\",\n+            details,\n+            e);\n+      }\n+    }\n+  }\n }\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 2256d1e874ce..be2a8db03097 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -406,7 +406,8 @@ private <T> DataStreamSink<T> chainIcebergOperators() {\n       flinkWriteConf = new FlinkWriteConf(table, writeOptions, readableConfig);\n \n       // Find out the equality field id list based on the user-provided equality field column names.\n-      List<Integer> equalityFieldIds = checkAndGetEqualityFieldIds();\n+      List<Integer> equalityFieldIds =\n+          SinkUtil.checkAndGetEqualityFieldIds(table, equalityFieldColumns);\n \n       RowType flinkRowType = toFlinkRowType(table.schema(), tableSchema);\n       int writerParallelism =\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittable.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittable.java\nnew file mode 100644\nindex 000000000000..408c3e9a9d5f\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittable.java\n@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+/**\n+ * The aggregated results of a single checkpoint which should be committed. Containing the\n+ * serialized {@link org.apache.iceberg.flink.sink.DeltaManifests} file - which contains the commit\n+ * data, and the jobId, operatorId, checkpointId triplet which helps identifying the specific commit\n+ *\n+ * <p>{@link IcebergCommittableSerializer} is used for serializing the objects between the Writer\n+ * and the Aggregator operator and between the Aggregator and the Committer as well.\n+ */\n+class IcebergCommittable implements Serializable {\n+  private final byte[] manifest;\n+  private final String jobId;\n+  private final String operatorId;\n+  private final long checkpointId;\n+\n+  IcebergCommittable(byte[] manifest, String jobId, String operatorId, long checkpointId) {\n+    this.manifest = manifest;\n+    this.jobId = jobId;\n+    this.operatorId = operatorId;\n+    this.checkpointId = checkpointId;\n+  }\n+\n+  byte[] manifest() {\n+    return manifest;\n+  }\n+\n+  String jobId() {\n+    return jobId;\n+  }\n+\n+  String operatorId() {\n+    return operatorId;\n+  }\n+\n+  Long checkpointId() {\n+    return checkpointId;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"jobId\", jobId)\n+        .add(\"checkpointId\", checkpointId)\n+        .add(\"operatorId\", operatorId)\n+        .toString();\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    IcebergCommittable that = (IcebergCommittable) o;\n+    return checkpointId == that.checkpointId\n+        && Arrays.equals(manifest, that.manifest)\n+        && Objects.equals(jobId, that.jobId)\n+        && Objects.equals(operatorId, that.operatorId);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    int result = Objects.hash(jobId, operatorId, checkpointId);\n+    result = 31 * result + Arrays.hashCode(manifest);\n+    return result;\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittableSerializer.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittableSerializer.java\nnew file mode 100644\nindex 000000000000..e2b388a83c75\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommittableSerializer.java\n@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+\n+/**\n+ * This serializer is used for serializing the {@link IcebergCommittable} objects between the Writer\n+ * and the Aggregator operator and between the Aggregator and the Committer as well.\n+ *\n+ * <p>In both cases only the respective part is serialized.\n+ */\n+class IcebergCommittableSerializer implements SimpleVersionedSerializer<IcebergCommittable> {\n+  private static final int VERSION = 1;\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION;\n+  }\n+\n+  @Override\n+  public byte[] serialize(IcebergCommittable committable) throws IOException {\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    DataOutputViewStreamWrapper view = new DataOutputViewStreamWrapper(out);\n+    view.writeUTF(committable.jobId());\n+    view.writeUTF(committable.operatorId());\n+    view.writeLong(committable.checkpointId());\n+    view.writeInt(committable.manifest().length);\n+    view.write(committable.manifest());\n+    return out.toByteArray();\n+  }\n+\n+  @Override\n+  public IcebergCommittable deserialize(int version, byte[] serialized) throws IOException {\n+    if (version == 1) {\n+      DataInputDeserializer view = new DataInputDeserializer(serialized);\n+      String jobId = view.readUTF();\n+      String operatorId = view.readUTF();\n+      long checkpointId = view.readLong();\n+      int manifestLen = view.readInt();\n+      byte[] manifestBuf;\n+      manifestBuf = new byte[manifestLen];\n+      view.read(manifestBuf);\n+      return new IcebergCommittable(manifestBuf, jobId, operatorId, checkpointId);\n+    }\n+    throw new IOException(\"Unrecognized version or corrupt state: \" + version);\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommitter.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommitter.java\nnew file mode 100644\nindex 000000000000..2245b36f15a7\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommitter.java\n@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.flink.api.connector.sink2.Committer;\n+import org.apache.flink.core.io.SimpleVersionedSerialization;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ReplacePartitions;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.SnapshotUpdate;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class implements the Flink SinkV2 {@link Committer} interface to implement the Iceberg\n+ * commits. The implementation builds on the following assumptions:\n+ *\n+ * <ul>\n+ *   <li>There is a single {@link IcebergCommittable} for every checkpoint\n+ *   <li>There is no late checkpoint - if checkpoint 'x' has received in one call, then after a\n+ *       successful run only checkpoints &gt; x will arrive\n+ *   <li>There is no other writer which would generate another commit to the same branch with the\n+ *       same jobId-operatorId-checkpointId triplet\n+ * </ul>\n+ */\n+class IcebergCommitter implements Committer<IcebergCommittable> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergCommitter.class);\n+  private static final byte[] EMPTY_MANIFEST_DATA = new byte[0];\n+  public static final WriteResult EMPTY_WRITE_RESULT =\n+      WriteResult.builder()\n+          .addDataFiles(Lists.newArrayList())\n+          .addDeleteFiles(Lists.newArrayList())\n+          .build();\n+\n+  @VisibleForTesting\n+  static final String MAX_CONTINUOUS_EMPTY_COMMITS = \"flink.max-continuous-empty-commits\";\n+\n+  private final String branch;\n+  private final Map<String, String> snapshotProperties;\n+  private final boolean replacePartitions;\n+  private IcebergFilesCommitterMetrics committerMetrics;\n+  private Table table;\n+  private final TableLoader tableLoader;\n+  private int maxContinuousEmptyCommits;\n+  private ExecutorService workerPool;\n+  private int continuousEmptyCheckpoints = 0;\n+\n+  IcebergCommitter(\n+      TableLoader tableLoader,\n+      String branch,\n+      Map<String, String> snapshotProperties,\n+      boolean replacePartitions,\n+      int workerPoolSize,\n+      String sinkId,\n+      IcebergFilesCommitterMetrics committerMetrics) {\n+    this.branch = branch;\n+    this.snapshotProperties = snapshotProperties;\n+    this.replacePartitions = replacePartitions;\n+    this.committerMetrics = committerMetrics;\n+    this.tableLoader = tableLoader;\n+    if (!tableLoader.isOpen()) {\n+      tableLoader.open();\n+    }\n+\n+    this.table = tableLoader.loadTable();\n+    this.maxContinuousEmptyCommits =\n+        PropertyUtil.propertyAsInt(table.properties(), MAX_CONTINUOUS_EMPTY_COMMITS, 10);\n+    Preconditions.checkArgument(\n+        maxContinuousEmptyCommits > 0, MAX_CONTINUOUS_EMPTY_COMMITS + \" must be positive\");\n+    this.workerPool =\n+        ThreadPools.newWorkerPool(\n+            \"iceberg-committer-pool-\" + table.name() + \"-\" + sinkId, workerPoolSize);\n+    this.continuousEmptyCheckpoints = 0;\n+  }\n+\n+  @Override\n+  public void commit(Collection<CommitRequest<IcebergCommittable>> commitRequests)\n+      throws IOException, InterruptedException {\n+    if (commitRequests.isEmpty()) {\n+      return;\n+    }\n+\n+    NavigableMap<Long, CommitRequest<IcebergCommittable>> commitRequestMap = Maps.newTreeMap();\n+    for (CommitRequest<IcebergCommittable> request : commitRequests) {\n+      commitRequestMap.put(request.getCommittable().checkpointId(), request);\n+    }\n+\n+    IcebergCommittable last = commitRequestMap.lastEntry().getValue().getCommittable();\n+    long maxCommittedCheckpointId =\n+        SinkUtil.getMaxCommittedCheckpointId(table, last.jobId(), last.operatorId(), branch);\n+    // Mark the already committed FilesCommittable(s) as finished\n+    commitRequestMap\n+        .headMap(maxCommittedCheckpointId, true)\n+        .values()\n+        .forEach(CommitRequest::signalAlreadyCommitted);\n+    NavigableMap<Long, CommitRequest<IcebergCommittable>> uncommitted =\n+        commitRequestMap.tailMap(maxCommittedCheckpointId, false);\n+    if (!uncommitted.isEmpty()) {\n+      commitPendingRequests(uncommitted, last.jobId(), last.operatorId());\n+    }\n+  }\n+\n+  /**\n+   * Commits the data to the Iceberg table by reading the file data from the {@link\n+   * org.apache.iceberg.flink.sink.DeltaManifests} ordered by the checkpointId, and writing the new\n+   * snapshot to the Iceberg table. The {@link org.apache.iceberg.SnapshotSummary} will contain the\n+   * jobId, snapshotId, checkpointId so in case of job restart we can identify which changes are\n+   * committed, and which are still waiting for the commit.\n+   *\n+   * @param commitRequestMap The checkpointId to {@link CommitRequest} map of the changes to commit\n+   * @param newFlinkJobId The jobId to store in the {@link org.apache.iceberg.SnapshotSummary}\n+   * @param operatorId The operatorId to store in the {@link org.apache.iceberg.SnapshotSummary}\n+   * @throws IOException On commit failure\n+   */\n+  private void commitPendingRequests(\n+      NavigableMap<Long, CommitRequest<IcebergCommittable>> commitRequestMap,\n+      String newFlinkJobId,\n+      String operatorId)\n+      throws IOException {\n+    long checkpointId = commitRequestMap.lastKey();\n+    List<ManifestFile> manifests = Lists.newArrayList();\n+    NavigableMap<Long, WriteResult> pendingResults = Maps.newTreeMap();\n+    for (Map.Entry<Long, CommitRequest<IcebergCommittable>> e : commitRequestMap.entrySet()) {\n+      if (Arrays.equals(EMPTY_MANIFEST_DATA, e.getValue().getCommittable().manifest())) {\n+        pendingResults.put(e.getKey(), EMPTY_WRITE_RESULT);\n+      } else {\n+        DeltaManifests deltaManifests =\n+            SimpleVersionedSerialization.readVersionAndDeSerialize(\n+                DeltaManifestsSerializer.INSTANCE, e.getValue().getCommittable().manifest());\n+        pendingResults.put(\n+            e.getKey(),\n+            FlinkManifestUtil.readCompletedFiles(deltaManifests, table.io(), table.specs()));\n+        manifests.addAll(deltaManifests.manifests());\n+      }\n+    }\n+\n+    CommitSummary summary = new CommitSummary(pendingResults);\n+    commitPendingResult(pendingResults, summary, newFlinkJobId, operatorId);\n+    if (committerMetrics != null) {\n+      committerMetrics.updateCommitSummary(summary);\n+    }\n+\n+    FlinkManifestUtil.deleteCommittedManifests(table, manifests, newFlinkJobId, checkpointId);\n+  }\n+\n+  private void logCommitSummary(CommitSummary summary, String description) {\n+    LOG.info(\n+        \"Preparing for commit: {} on table: {} branch: {} with summary: {}.\",\n+        description,\n+        table,\n+        branch,\n+        summary);\n+  }\n+\n+  private void commitPendingResult(\n+      NavigableMap<Long, WriteResult> pendingResults,\n+      CommitSummary summary,\n+      String newFlinkJobId,\n+      String operatorId) {\n+    long totalFiles = summary.dataFilesCount() + summary.deleteFilesCount();\n+    continuousEmptyCheckpoints = totalFiles == 0 ? continuousEmptyCheckpoints + 1 : 0;\n+    if (totalFiles != 0 || continuousEmptyCheckpoints % maxContinuousEmptyCommits == 0) {\n+      if (replacePartitions) {\n+        replacePartitions(pendingResults, summary, newFlinkJobId, operatorId);\n+      } else {\n+        commitDeltaTxn(pendingResults, summary, newFlinkJobId, operatorId);\n+      }\n+      continuousEmptyCheckpoints = 0;\n+    } else {\n+      long checkpointId = pendingResults.lastKey();\n+      LOG.info(\"Skip commit for checkpoint {} due to no data files or delete files.\", checkpointId);\n+    }\n+  }\n+\n+  private void replacePartitions(\n+      NavigableMap<Long, WriteResult> pendingResults,\n+      CommitSummary summary,\n+      String newFlinkJobId,\n+      String operatorId) {\n+    long checkpointId = pendingResults.lastKey();\n+    Preconditions.checkState(\n+        summary.deleteFilesCount() == 0, \"Cannot overwrite partitions with delete files.\");\n+    // Commit the overwrite transaction.\n+    ReplacePartitions dynamicOverwrite = table.newReplacePartitions().scanManifestsWith(workerPool);\n+    for (WriteResult result : pendingResults.values()) {\n+      Preconditions.checkState(\n+          result.referencedDataFiles().length == 0, \"Should have no referenced data files.\");\n+      Arrays.stream(result.dataFiles()).forEach(dynamicOverwrite::addFile);\n+    }\n+    String description = \"dynamic partition overwrite\";\n+\n+    logCommitSummary(summary, description);\n+    commitOperation(dynamicOverwrite, description, newFlinkJobId, operatorId, checkpointId);\n+  }\n+\n+  private void commitDeltaTxn(\n+      NavigableMap<Long, WriteResult> pendingResults,\n+      CommitSummary summary,\n+      String newFlinkJobId,\n+      String operatorId) {\n+    long checkpointId = pendingResults.lastKey();\n+    if (summary.deleteFilesCount() == 0) {\n+      // To be compatible with iceberg format V1.\n+      AppendFiles appendFiles = table.newAppend().scanManifestsWith(workerPool);\n+      for (WriteResult result : pendingResults.values()) {\n+        Preconditions.checkState(\n+            result.referencedDataFiles().length == 0,\n+            \"Should have no referenced data files for append.\");\n+        Arrays.stream(result.dataFiles()).forEach(appendFiles::appendFile);\n+      }\n+      String description = \"append\";\n+      logCommitSummary(summary, description);\n+      // fail all commits as really its only one\n+      commitOperation(appendFiles, description, newFlinkJobId, operatorId, checkpointId);\n+    } else {\n+      // To be compatible with iceberg format V2.\n+      for (Map.Entry<Long, WriteResult> e : pendingResults.entrySet()) {\n+        // We don't commit the merged result into a single transaction because for the sequential\n+        // transaction txn1 and txn2, the equality-delete files of txn2 are required to be applied\n+        // to data files from txn1. Committing the merged one will lead to the incorrect delete\n+        // semantic.\n+        WriteResult result = e.getValue();\n+\n+        // Row delta validations are not needed for streaming changes that write equality deletes.\n+        // Equality deletes are applied to data in all previous sequence numbers, so retries may\n+        // push deletes further in the future, but do not affect correctness. Position deletes\n+        // committed to the table in this path are used only to delete rows from data files that are\n+        // being added in this commit. There is no way for data files added along with the delete\n+        // files to be concurrently removed, so there is no need to validate the files referenced by\n+        // the position delete files that are being committed.\n+        RowDelta rowDelta = table.newRowDelta().scanManifestsWith(workerPool);\n+\n+        Arrays.stream(result.dataFiles()).forEach(rowDelta::addRows);\n+        Arrays.stream(result.deleteFiles()).forEach(rowDelta::addDeletes);\n+\n+        String description = \"rowDelta\";\n+        logCommitSummary(summary, description);\n+        commitOperation(rowDelta, description, newFlinkJobId, operatorId, e.getKey());\n+      }\n+    }\n+  }\n+\n+  private void commitOperation(\n+      SnapshotUpdate<?> operation,\n+      String description,\n+      String newFlinkJobId,\n+      String operatorId,\n+      long checkpointId) {\n+\n+    snapshotProperties.forEach(operation::set);\n+    // custom snapshot metadata properties will be overridden if they conflict with internal ones\n+    // used by the sink.\n+    operation.set(SinkUtil.MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    operation.set(SinkUtil.FLINK_JOB_ID, newFlinkJobId);\n+    operation.set(SinkUtil.OPERATOR_ID, operatorId);\n+    operation.toBranch(branch);\n+\n+    long startNano = System.nanoTime();\n+    operation.commit(); // abort is automatically called if this fails.\n+    long durationMs = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startNano);\n+    LOG.info(\n+        \"Committed {} to table: {}, branch: {}, checkpointId {} in {} ms\",\n+        description,\n+        table.name(),\n+        branch,\n+        checkpointId,\n+        durationMs);\n+    if (committerMetrics != null) {\n+      committerMetrics.commitDuration(durationMs);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    tableLoader.close();\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\nindex b9bceaa9311d..622daa808897 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n@@ -44,13 +44,11 @@\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ReplacePartitions;\n import org.apache.iceberg.RowDelta;\n-import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotUpdate;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.flink.TableLoader;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n-import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -187,7 +185,7 @@ public void initializeState(StateInitializationContext context) throws Exception\n       // it's safe to assign the max committed checkpoint id from restored flink job to the current\n       // flink job.\n       this.maxCommittedCheckpointId =\n-          getMaxCommittedCheckpointId(table, restoredFlinkJobId, operatorUniqueId, branch);\n+          SinkUtil.getMaxCommittedCheckpointId(table, restoredFlinkJobId, operatorUniqueId, branch);\n \n       NavigableMap<Long, byte[]> uncommittedDataFiles =\n           Maps.newTreeMap(checkpointsState.get().iterator().next())\n@@ -280,7 +278,7 @@ private void commitUpToCheckpoint(\n     commitPendingResult(pendingResults, summary, newFlinkJobId, operatorId, checkpointId);\n     committerMetrics.updateCommitSummary(summary);\n     pendingMap.clear();\n-    deleteCommittedManifests(manifests, newFlinkJobId, checkpointId);\n+    FlinkManifestUtil.deleteCommittedManifests(table, manifests, newFlinkJobId, checkpointId);\n   }\n \n   private void commitPendingResult(\n@@ -303,27 +301,6 @@ private void commitPendingResult(\n     }\n   }\n \n-  private void deleteCommittedManifests(\n-      List<ManifestFile> manifests, String newFlinkJobId, long checkpointId) {\n-    for (ManifestFile manifest : manifests) {\n-      try {\n-        table.io().deleteFile(manifest.path());\n-      } catch (Exception e) {\n-        // The flink manifests cleaning failure shouldn't abort the completed checkpoint.\n-        String details =\n-            MoreObjects.toStringHelper(this)\n-                .add(\"flinkJobId\", newFlinkJobId)\n-                .add(\"checkpointId\", checkpointId)\n-                .add(\"manifestPath\", manifest.path())\n-                .toString();\n-        LOG.warn(\n-            \"The iceberg transaction has been committed, but we failed to clean the temporary flink manifests: {}\",\n-            details,\n-            e);\n-      }\n-    }\n-  }\n-\n   private void replacePartitions(\n       NavigableMap<Long, WriteResult> pendingResults,\n       CommitSummary summary,\n@@ -489,28 +466,4 @@ static ListStateDescriptor<SortedMap<Long, byte[]>> buildStateDescriptor() {\n             longComparator);\n     return new ListStateDescriptor<>(\"iceberg-files-committer-state\", sortedMapTypeInfo);\n   }\n-\n-  static long getMaxCommittedCheckpointId(\n-      Table table, String flinkJobId, String operatorId, String branch) {\n-    Snapshot snapshot = table.snapshot(branch);\n-    long lastCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n-\n-    while (snapshot != null) {\n-      Map<String, String> summary = snapshot.summary();\n-      String snapshotFlinkJobId = summary.get(FLINK_JOB_ID);\n-      String snapshotOperatorId = summary.get(OPERATOR_ID);\n-      if (flinkJobId.equals(snapshotFlinkJobId)\n-          && (snapshotOperatorId == null || snapshotOperatorId.equals(operatorId))) {\n-        String value = summary.get(MAX_COMMITTED_CHECKPOINT_ID);\n-        if (value != null) {\n-          lastCommittedCheckpointId = Long.parseLong(value);\n-          break;\n-        }\n-      }\n-      Long parentSnapshotId = snapshot.parentId();\n-      snapshot = parentSnapshotId != null ? table.snapshot(parentSnapshotId) : null;\n-    }\n-\n-    return lastCommittedCheckpointId;\n-  }\n }\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java\nnew file mode 100644\nindex 000000000000..d080169544cd\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java\n@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import static org.apache.iceberg.TableProperties.AVRO_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.AVRO_COMPRESSION_LEVEL;\n+import static org.apache.iceberg.TableProperties.ORC_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.ORC_COMPRESSION_STRATEGY;\n+import static org.apache.iceberg.TableProperties.PARQUET_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.PARQUET_COMPRESSION_LEVEL;\n+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.connector.sink2.Committer;\n+import org.apache.flink.api.connector.sink2.CommitterInitContext;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+import org.apache.flink.api.connector.sink2.SupportsCommitter;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.streaming.api.connector.sink2.CommittableMessage;\n+import org.apache.flink.streaming.api.connector.sink2.CommittableMessageTypeInfo;\n+import org.apache.flink.streaming.api.connector.sink2.SupportsPostCommitTopology;\n+import org.apache.flink.streaming.api.connector.sink2.SupportsPreCommitTopology;\n+import org.apache.flink.streaming.api.connector.sink2.SupportsPreWriteTopology;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DistributionMode;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SerializableTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.FlinkWriteConf;\n+import org.apache.iceberg.flink.FlinkWriteOptions;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.flink.util.FlinkCompatibilityUtil;\n+import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Flink v2 sink offer different hooks to insert custom topologies into the sink. We will use the\n+ * following:\n+ *\n+ * <ul>\n+ *   <li>{@link SupportsPreWriteTopology} which redistributes the data to the writers based on the\n+ *       {@link DistributionMode}\n+ *   <li>{@link org.apache.flink.api.connector.sink2.SinkWriter} which writes data/delete files, and\n+ *       generates the {@link org.apache.iceberg.io.WriteResult} objects for the files\n+ *   <li>{@link SupportsPreCommitTopology} which we use to place the {@link\n+ *       org.apache.iceberg.flink.sink.IcebergWriteAggregator} which merges the individual {@link\n+ *       org.apache.flink.api.connector.sink2.SinkWriter}'s {@link\n+ *       org.apache.iceberg.io.WriteResult}s to a single {@link\n+ *       org.apache.iceberg.flink.sink.IcebergCommittable}\n+ *   <li>{@link org.apache.iceberg.flink.sink.IcebergCommitter} which commits the incoming{@link\n+ *       org.apache.iceberg.flink.sink.IcebergCommittable}s to the Iceberg table\n+ *   <li>{@link SupportsPostCommitTopology} we could use for incremental compaction later. This is\n+ *       not implemented yet.\n+ * </ul>\n+ *\n+ * The job graph looks like below:\n+ *\n+ * <pre>{@code\n+ *                            Flink sink\n+ *               +-----------------------------------------------------------------------------------+\n+ *               |                                                                                   |\n+ * +-------+     | +----------+                               +-------------+      +---------------+ |\n+ * | Map 1 | ==> | | writer 1 |                               | committer 1 | ---> | post commit 1 | |\n+ * +-------+     | +----------+                               +-------------+      +---------------+ |\n+ *               |             \\                             /                \\                      |\n+ *               |              \\                           /                  \\                     |\n+ *               |               \\                         /                    \\                    |\n+ * +-------+     | +----------+   \\ +-------------------+ /   +-------------+    \\ +---------------+ |\n+ * | Map 2 | ==> | | writer 2 | --->| commit aggregator |     | committer 2 |      | post commit 2 | |\n+ * +-------+     | +----------+     +-------------------+     +-------------+      +---------------+ |\n+ *               |                                             Commit only on                        |\n+ *               |                                             committer 1                           |\n+ *               +-----------------------------------------------------------------------------------+\n+ * }</pre>\n+ */\n+@Experimental\n+public class IcebergSink\n+    implements Sink<RowData>,\n+        SupportsPreWriteTopology<RowData>,\n+        SupportsCommitter<IcebergCommittable>,\n+        SupportsPreCommitTopology<WriteResult, IcebergCommittable>,\n+        SupportsPostCommitTopology<IcebergCommittable> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergSink.class);\n+  private final TableLoader tableLoader;\n+  private final Map<String, String> snapshotProperties;\n+  private final String uidSuffix;\n+  private final String sinkId;\n+  private final Map<String, String> writeProperties;\n+  private final RowType flinkRowType;\n+  private final SerializableSupplier<Table> tableSupplier;\n+  private final transient FlinkWriteConf flinkWriteConf;\n+  private final List<Integer> equalityFieldIds;\n+  private final boolean upsertMode;\n+  private final FileFormat dataFileFormat;\n+  private final long targetDataFileSize;\n+  private final String branch;\n+  private final boolean overwriteMode;\n+  private final int workerPoolSize;\n+\n+  private final Table table;\n+  private final List<String> equalityFieldColumns = null;\n+\n+  private IcebergSink(\n+      TableLoader tableLoader,\n+      Table table,\n+      Map<String, String> snapshotProperties,\n+      String uidSuffix,\n+      Map<String, String> writeProperties,\n+      RowType flinkRowType,\n+      SerializableSupplier<Table> tableSupplier,\n+      FlinkWriteConf flinkWriteConf,\n+      List<Integer> equalityFieldIds,\n+      String branch,\n+      boolean overwriteMode) {\n+    this.tableLoader = tableLoader;\n+    this.snapshotProperties = snapshotProperties;\n+    this.uidSuffix = uidSuffix;\n+    this.writeProperties = writeProperties;\n+    this.flinkRowType = flinkRowType;\n+    this.tableSupplier = tableSupplier;\n+    this.flinkWriteConf = flinkWriteConf;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.branch = branch;\n+    this.overwriteMode = overwriteMode;\n+    this.table = table;\n+    this.upsertMode = flinkWriteConf.upsertMode();\n+    this.dataFileFormat = flinkWriteConf.dataFileFormat();\n+    this.targetDataFileSize = flinkWriteConf.targetDataFileSize();\n+    this.workerPoolSize = flinkWriteConf.workerPoolSize();\n+    // We generate a random UUID every time when a sink is created.\n+    // This is used to separate files generated by different sinks writing the same table.\n+    // Also used to generate the aggregator operator name\n+    this.sinkId = UUID.randomUUID().toString();\n+  }\n+\n+  @Override\n+  public SinkWriter<RowData> createWriter(InitContext context) {\n+    RowDataTaskWriterFactory taskWriterFactory =\n+        new RowDataTaskWriterFactory(\n+            tableSupplier,\n+            flinkRowType,\n+            targetDataFileSize,\n+            dataFileFormat,\n+            writeProperties,\n+            equalityFieldIds,\n+            upsertMode);\n+    IcebergStreamWriterMetrics metrics =\n+        new IcebergStreamWriterMetrics(context.metricGroup(), table.name());\n+    return new IcebergSinkWriter(\n+        tableSupplier.get().name(),\n+        taskWriterFactory,\n+        metrics,\n+        context.getSubtaskId(),\n+        context.getAttemptNumber());\n+  }\n+\n+  @Override\n+  public Committer<IcebergCommittable> createCommitter(CommitterInitContext context) {\n+    IcebergFilesCommitterMetrics metrics =\n+        new IcebergFilesCommitterMetrics(context.metricGroup(), table.name());\n+    return new IcebergCommitter(\n+        tableLoader, branch, snapshotProperties, overwriteMode, workerPoolSize, sinkId, metrics);\n+  }\n+\n+  @Override\n+  public SimpleVersionedSerializer<IcebergCommittable> getCommittableSerializer() {\n+    return new IcebergCommittableSerializer();\n+  }\n+\n+  @Override\n+  public void addPostCommitTopology(\n+      DataStream<CommittableMessage<IcebergCommittable>> committables) {\n+    // TODO Support small file compaction\n+  }\n+\n+  @Override\n+  public DataStream<RowData> addPreWriteTopology(DataStream<RowData> inputDataStream) {\n+    return distributeDataStream(inputDataStream);\n+  }\n+\n+  @Override\n+  public DataStream<CommittableMessage<IcebergCommittable>> addPreCommitTopology(\n+      DataStream<CommittableMessage<WriteResult>> writeResults) {\n+    TypeInformation<CommittableMessage<IcebergCommittable>> typeInformation =\n+        CommittableMessageTypeInfo.of(this::getCommittableSerializer);\n+\n+    String suffix = defaultSuffix(uidSuffix, table.name());\n+    String preCommitAggregatorUid = String.format(\"Sink pre-commit aggregator: %s\", suffix);\n+\n+    // global forces all output records send to subtask 0 of the downstream committer operator.\n+    // This is to ensure commit only happen in one committer subtask.\n+    // Once upstream Flink provides the capability of setting committer operator\n+    // parallelism to 1, this can be removed.\n+    return writeResults\n+        .global()\n+        .transform(preCommitAggregatorUid, typeInformation, new IcebergWriteAggregator(tableLoader))\n+        .uid(preCommitAggregatorUid)\n+        .setParallelism(1)\n+        .setMaxParallelism(1)\n+        // global forces all output records send to subtask 0 of the downstream committer operator.\n+        // This is to ensure commit only happen in one committer subtask.\n+        // Once upstream Flink provides the capability of setting committer operator\n+        // parallelism to 1, this can be removed.\n+        .global();\n+  }\n+\n+  @Override\n+  public SimpleVersionedSerializer<WriteResult> getWriteResultSerializer() {\n+    return new WriteResultSerializer();\n+  }\n+\n+  public static class Builder {\n+    private TableLoader tableLoader;\n+    private String uidSuffix = \"\";\n+    private Function<String, DataStream<RowData>> inputCreator = null;\n+    private TableSchema tableSchema;\n+    private SerializableTable table;\n+    private final Map<String, String> writeOptions = Maps.newHashMap();\n+    private final Map<String, String> snapshotSummary = Maps.newHashMap();\n+    private ReadableConfig readableConfig = new Configuration();\n+    private List<String> equalityFieldColumns = null;\n+\n+    private Builder() {}\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.inputCreator = ignored -> newRowDataInput;\n+      return this;\n+    }\n+\n+    private Builder forRow(DataStream<Row> input, TableSchema inputTableSchema) {\n+      RowType rowType = (RowType) inputTableSchema.toRowDataType().getLogicalType();\n+      DataType[] fieldDataTypes = inputTableSchema.getFieldDataTypes();\n+\n+      DataFormatConverters.RowConverter rowConverter =\n+          new DataFormatConverters.RowConverter(fieldDataTypes);\n+      return forMapperOutputType(\n+              input, rowConverter::toInternal, FlinkCompatibilityUtil.toTypeInfo(rowType))\n+          .tableSchema(inputTableSchema);\n+    }\n+\n+    private <T> Builder forMapperOutputType(\n+        DataStream<T> input, MapFunction<T, RowData> mapper, TypeInformation<RowData> outputType) {\n+      this.inputCreator =\n+          newUidSuffix -> {\n+            // Input stream order is crucial for some situation(e.g. in cdc case). Therefore, we\n+            // need to set the parallelism of map operator same as its input to keep map operator\n+            // chaining its input, and avoid rebalanced by default.\n+            SingleOutputStreamOperator<RowData> inputStream =\n+                input.map(mapper, outputType).setParallelism(input.getParallelism());\n+            if (newUidSuffix != null) {\n+              String uid = String.format(\"Sink pre-writer mapper: %s\", newUidSuffix);\n+              inputStream.name(uid).uid(uid);\n+            }\n+            return inputStream;\n+          };\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link SerializableTable} instance is used for initializing {@link\n+     * IcebergStreamWriter} which will write all the records into {@link DataFile}s and emit them to\n+     * downstream operator. Providing a table would avoid so many table loading from each separate\n+     * task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link IcebergSink.Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = (SerializableTable) SerializableTable.copyOf(newTable);\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link\n+     * org.apache.iceberg.flink.sink.IcebergCommitter} lazily, we need this loader because {@link\n+     * Table} is not serializable and could not just use the loaded table from Builder#table in the\n+     * remote task manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    TableLoader tableLoader() {\n+      return tableLoader;\n+    }\n+\n+    /**\n+     * Set the write properties for IcebergSink. View the supported properties in {@link\n+     * FlinkWriteOptions}\n+     */\n+    public Builder set(String property, String value) {\n+      writeOptions.put(property, value);\n+      return this;\n+    }\n+\n+    /**\n+     * Set the write properties for IcebergSink. View the supported properties in {@link\n+     * FlinkWriteOptions}\n+     */\n+    public Builder setAll(Map<String, String> properties) {\n+      writeOptions.putAll(properties);\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    public Builder overwrite(boolean newOverwrite) {\n+      writeOptions.put(FlinkWriteOptions.OVERWRITE_MODE.key(), Boolean.toString(newOverwrite));\n+      return this;\n+    }\n+\n+    public Builder flinkConf(ReadableConfig config) {\n+      this.readableConfig = config;\n+      return this;\n+    }\n+\n+    /**\n+     * Configure the write {@link DistributionMode} that the IcebergSink will use. Currently, flink\n+     * support {@link DistributionMode#NONE} and {@link DistributionMode#HASH}.\n+     *\n+     * @param mode to specify the write distribution mode.\n+     * @return {@link IcebergSink.Builder} to connect the iceberg table.\n+     */\n+    public Builder distributionMode(DistributionMode mode) {\n+      Preconditions.checkArgument(\n+          !DistributionMode.RANGE.equals(mode),\n+          \"Flink does not support 'range' write distribution mode now.\");\n+      if (mode != null) {\n+        writeOptions.put(FlinkWriteOptions.DISTRIBUTION_MODE.key(), mode.modeName());\n+      }\n+      return this;\n+    }\n+\n+    /**\n+     * Configuring the write parallel number for iceberg stream writer.\n+     *\n+     * @param newWriteParallelism the number of parallel iceberg stream writer.\n+     * @return {@link IcebergSink.Builder} to connect the iceberg table.\n+     */\n+    public Builder writeParallelism(int newWriteParallelism) {\n+      writeOptions.put(\n+          FlinkWriteOptions.WRITE_PARALLELISM.key(), Integer.toString(newWriteParallelism));\n+      return this;\n+    }\n+\n+    /**\n+     * All INSERT/UPDATE_AFTER events from input stream will be transformed to UPSERT events, which\n+     * means it will DELETE the old records and then INSERT the new records. In partitioned table,\n+     * the partition fields should be a subset of equality fields, otherwise the old row that\n+     * located in partition-A could not be deleted by the new row that located in partition-B.\n+     *\n+     * @param enabled indicate whether it should transform all INSERT/UPDATE_AFTER events to UPSERT.\n+     * @return {@link IcebergSink.Builder} to connect the iceberg table.\n+     */\n+    public Builder upsert(boolean enabled) {\n+      writeOptions.put(FlinkWriteOptions.WRITE_UPSERT_ENABLED.key(), Boolean.toString(enabled));\n+      return this;\n+    }\n+\n+    /**\n+     * Configuring the equality field columns for iceberg table that accept CDC or UPSERT events.\n+     *\n+     * @param columns defines the iceberg table's key.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder equalityFieldColumns(List<String> columns) {\n+      this.equalityFieldColumns = columns;\n+      return this;\n+    }\n+\n+    /**\n+     * Set the uid suffix for IcebergSink operators. Note that IcebergSink internally consists of\n+     * multiple operators (like writer, committer, aggregator). Actual operator uid will be appended\n+     * with a suffix like \"Sink Committer: $uidSuffix\".\n+     *\n+     * <p>Flink auto generates operator uid if not set explicitly. It is a recommended <a\n+     * href=\"https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/production_ready/\">\n+     * best-practice to set uid for all operators</a> before deploying to production. Flink has an\n+     * option to {@code pipeline.auto-generate-uid=false} to disable auto-generation and force\n+     * explicit setting of all operator uid.\n+     *\n+     * <p>Be careful with setting this for an existing job, because now we are changing the operator\n+     * uid from an auto-generated one to this new value. When deploying the change with a\n+     * checkpoint, Flink won't be able to restore the previous IcebergSink operator state (more\n+     * specifically the committer operator state). You need to use {@code --allowNonRestoredState}\n+     * to ignore the previous sink state. During restore IcebergSink state is used to check if last\n+     * commit was actually successful or not. {@code --allowNonRestoredState} can lead to data loss\n+     * if the Iceberg commit failed in the last completed checkpoint.\n+     *\n+     * @param newSuffix suffix for Flink sink operator uid and name\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder uidSuffix(String newSuffix) {\n+      this.uidSuffix = newSuffix;\n+      return this;\n+    }\n+\n+    public Builder snapshotProperties(Map<String, String> properties) {\n+      snapshotSummary.putAll(properties);\n+      return this;\n+    }\n+\n+    public Builder setSnapshotProperty(String property, String value) {\n+      snapshotSummary.put(property, value);\n+      return this;\n+    }\n+\n+    public Builder toBranch(String branch) {\n+      writeOptions.put(FlinkWriteOptions.BRANCH.key(), branch);\n+      return this;\n+    }\n+\n+    IcebergSink build() {\n+\n+      Preconditions.checkArgument(\n+          inputCreator != null,\n+          \"Please use forRowData() or forMapperOutputType() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(tableLoader(), \"Table loader shouldn't be null\");\n+\n+      // Set the table if it is not yet set in the builder, so we can do the equalityId checks\n+      SerializableTable serializableTable = checkAndGetTable(tableLoader(), table);\n+      this.table = serializableTable;\n+      // Init the `flinkWriteConf` here, so we can do the checks\n+      FlinkWriteConf flinkWriteConf = new FlinkWriteConf(table, writeOptions, readableConfig);\n+\n+      Duration tableRefreshInterval = flinkWriteConf.tableRefreshInterval();\n+      SerializableSupplier<Table> tableSupplier;\n+      if (tableRefreshInterval != null) {\n+        tableSupplier = new CachingTableSupplier(table, tableLoader(), tableRefreshInterval);\n+      } else {\n+        tableSupplier = () -> serializableTable;\n+      }\n+\n+      boolean overwriteMode = flinkWriteConf.overwriteMode();\n+\n+      // Validate the equality fields and partition fields if we enable the upsert mode.\n+      List<Integer> equalityFieldIds =\n+          SinkUtil.checkAndGetEqualityFieldIds(table, equalityFieldColumns);\n+\n+      if (flinkWriteConf.upsertMode()) {\n+        Preconditions.checkState(\n+            !overwriteMode,\n+            \"OVERWRITE mode shouldn't be enable when configuring to use UPSERT data stream.\");\n+        Preconditions.checkState(\n+            !equalityFieldIds.isEmpty(),\n+            \"Equality field columns shouldn't be empty when configuring to use UPSERT data stream.\");\n+        if (!table.spec().isUnpartitioned()) {\n+          for (PartitionField partitionField : table.spec().fields()) {\n+            Preconditions.checkState(\n+                equalityFieldIds.contains(partitionField.sourceId()),\n+                \"In UPSERT mode, partition field '%s' should be included in equality fields: '%s'\",\n+                partitionField,\n+                equalityFieldColumns);\n+          }\n+        }\n+      }\n+\n+      return new IcebergSink(\n+          tableLoader,\n+          table,\n+          snapshotSummary,\n+          uidSuffix,\n+          writeProperties(table, flinkWriteConf.dataFileFormat(), flinkWriteConf),\n+          toFlinkRowType(table.schema(), tableSchema),\n+          tableSupplier,\n+          flinkWriteConf,\n+          equalityFieldIds,\n+          flinkWriteConf.branch(),\n+          overwriteMode);\n+    }\n+\n+    /**\n+     * Append the iceberg sink operators to write records to iceberg table.\n+     *\n+     * @return {@link DataStreamSink} for sink.\n+     */\n+    public DataStreamSink<RowData> append() {\n+      IcebergSink sink = build();\n+      String suffix = defaultSuffix(uidSuffix, table.name());\n+      DataStream<RowData> rowDataInput = inputCreator.apply(suffix);\n+      // Please note that V2 sink framework will apply the uid here to the framework created\n+      // operators like writer,\n+      // committer. E.g. \"Sink writer: <uidSuffix>\n+      DataStreamSink<RowData> rowDataDataStreamSink =\n+          rowDataInput.sinkTo(sink).uid(suffix).name(suffix);\n+\n+      // Note that IcebergSink internally consists o multiple operators (like writer, committer,\n+      // aggregator).\n+      // The following parallelism will be propagated to all of the above operators.\n+      if (sink.flinkWriteConf.writeParallelism() != null) {\n+        rowDataDataStreamSink.setParallelism(sink.flinkWriteConf.writeParallelism());\n+      }\n+      return rowDataDataStreamSink;\n+    }\n+  }\n+\n+  private static String defaultSuffix(String uidSuffix, String defaultSuffix) {\n+    if (uidSuffix == null || uidSuffix.isEmpty()) {\n+      return defaultSuffix;\n+    }\n+    return uidSuffix;\n+  }\n+\n+  private static SerializableTable checkAndGetTable(TableLoader tableLoader, Table table) {\n+    if (table == null) {\n+      if (!tableLoader.isOpen()) {\n+        tableLoader.open();\n+      }\n+\n+      try (TableLoader loader = tableLoader) {\n+        return (SerializableTable) SerializableTable.copyOf(loader.loadTable());\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(\n+            \"Failed to load iceberg table from table loader: \" + tableLoader, e);\n+      }\n+    }\n+\n+    return (SerializableTable) SerializableTable.copyOf(table);\n+  }\n+\n+  private static RowType toFlinkRowType(Schema schema, TableSchema requestedSchema) {\n+    if (requestedSchema != null) {\n+      // Convert the flink schema to iceberg schema firstly, then reassign ids to match the existing\n+      // iceberg schema.\n+      Schema writeSchema = TypeUtil.reassignIds(FlinkSchemaUtil.convert(requestedSchema), schema);\n+      TypeUtil.validateWriteSchema(schema, writeSchema, true, true);\n+\n+      // We use this flink schema to read values from RowData. The flink's TINYINT and SMALLINT will\n+      // be promoted to iceberg INTEGER, that means if we use iceberg's table schema to read TINYINT\n+      // (backend by 1 'byte'), we will read 4 bytes rather than 1 byte, it will mess up the byte\n+      // array in BinaryRowData. So here we must use flink schema.\n+      return (RowType) requestedSchema.toRowDataType().getLogicalType();\n+    } else {\n+      return FlinkSchemaUtil.convert(schema);\n+    }\n+  }\n+\n+  /**\n+   * Based on the {@link FileFormat} overwrites the table level compression properties for the table\n+   * write.\n+   *\n+   * @param table The table to get the table level settings\n+   * @param format The FileFormat to use\n+   * @param conf The write configuration\n+   * @return The properties to use for writing\n+   */\n+  private static Map<String, String> writeProperties(\n+      Table table, FileFormat format, FlinkWriteConf conf) {\n+    Map<String, String> writeProperties = Maps.newHashMap(table.properties());\n+\n+    switch (format) {\n+      case PARQUET:\n+        writeProperties.put(PARQUET_COMPRESSION, conf.parquetCompressionCodec());\n+        String parquetCompressionLevel = conf.parquetCompressionLevel();\n+        if (parquetCompressionLevel != null) {\n+          writeProperties.put(PARQUET_COMPRESSION_LEVEL, parquetCompressionLevel);\n+        }\n+\n+        break;\n+      case AVRO:\n+        writeProperties.put(AVRO_COMPRESSION, conf.avroCompressionCodec());\n+        String avroCompressionLevel = conf.avroCompressionLevel();\n+        if (avroCompressionLevel != null) {\n+          writeProperties.put(AVRO_COMPRESSION_LEVEL, conf.avroCompressionLevel());\n+        }\n+\n+        break;\n+      case ORC:\n+        writeProperties.put(ORC_COMPRESSION, conf.orcCompressionCodec());\n+        writeProperties.put(ORC_COMPRESSION_STRATEGY, conf.orcCompressionStrategy());\n+        break;\n+      default:\n+        throw new IllegalArgumentException(String.format(\"Unknown file format %s\", format));\n+    }\n+\n+    return writeProperties;\n+  }\n+\n+  private DataStream<RowData> distributeDataStream(DataStream<RowData> input) {\n+    DistributionMode mode = flinkWriteConf.distributionMode();\n+    Schema schema = table.schema();\n+    PartitionSpec spec = table.spec();\n+    LOG.info(\"Write distribution mode is '{}'\", mode.modeName());\n+    switch (mode) {\n+      case NONE:\n+        if (equalityFieldIds.isEmpty()) {\n+          return input;\n+        } else {\n+          LOG.info(\"Distribute rows by equality fields, because there are equality fields set\");\n+          return input.keyBy(new EqualityFieldKeySelector(schema, flinkRowType, equalityFieldIds));\n+        }\n+\n+      case HASH:\n+        if (equalityFieldIds.isEmpty()) {\n+          if (table.spec().isUnpartitioned()) {\n+            LOG.warn(\n+                \"Fallback to use 'none' distribution mode, because there are no equality fields set \"\n+                    + \"and table is unpartitioned\");\n+            return input;\n+          } else {\n+            if (BucketPartitionerUtil.hasOneBucketField(spec)) {\n+              return input.partitionCustom(\n+                  new BucketPartitioner(spec),\n+                  new BucketPartitionKeySelector(spec, schema, flinkRowType));\n+            } else {\n+              return input.keyBy(new PartitionKeySelector(spec, schema, flinkRowType));\n+            }\n+          }\n+        } else {\n+          if (spec.isUnpartitioned()) {\n+            LOG.info(\n+                \"Distribute rows by equality fields, because there are equality fields set \"\n+                    + \"and table is unpartitioned\");\n+            return input.keyBy(\n+                new EqualityFieldKeySelector(schema, flinkRowType, equalityFieldIds));\n+          } else {\n+            for (PartitionField partitionField : spec.fields()) {\n+              Preconditions.checkState(\n+                  equalityFieldIds.contains(partitionField.sourceId()),\n+                  \"In 'hash' distribution mode with equality fields set, partition field '%s' \"\n+                      + \"should be included in equality fields: '%s'\",\n+                  partitionField,\n+                  equalityFieldColumns);\n+            }\n+            return input.keyBy(new PartitionKeySelector(spec, schema, flinkRowType));\n+          }\n+        }\n+\n+      case RANGE:\n+        if (equalityFieldIds.isEmpty()) {\n+          LOG.warn(\n+              \"Fallback to use 'none' distribution mode, because there are no equality fields set \"\n+                  + \"and {}=range is not supported yet in flink\",\n+              WRITE_DISTRIBUTION_MODE);\n+          return input;\n+        } else {\n+          LOG.info(\n+              \"Distribute rows by equality fields, because there are equality fields set \"\n+                  + \"and{}=range is not supported yet in flink\",\n+              WRITE_DISTRIBUTION_MODE);\n+          return input.keyBy(new EqualityFieldKeySelector(schema, flinkRowType, equalityFieldIds));\n+        }\n+\n+      default:\n+        throw new RuntimeException(\"Unrecognized \" + WRITE_DISTRIBUTION_MODE + \": \" + mode);\n+    }\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg\n+   * table. We use {@link RowData} inside the sink connector, so users need to provide a mapper\n+   * function and a {@link TypeInformation} to convert those generic records to a RowData\n+   * DataStream.\n+   *\n+   * @param input the generic source input data stream.\n+   * @param mapper function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T> the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(\n+      DataStream<T> input, MapFunction<T, RowData> mapper, TypeInformation<RowData> outputType) {\n+    return new Builder().forMapperOutputType(input, mapper, outputType);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into\n+   * iceberg table. We use {@link RowData} inside the sink connector, so users need to provide a\n+   * {@link TableSchema} for builder to convert those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    return new Builder().forRow(input, tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s\n+   * into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkWriter.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkWriter.java\nnew file mode 100644\nindex 000000000000..7234cf74020e\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkWriter.java\n@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.flink.api.connector.sink2.CommittingSinkWriter;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Iceberg writer implementation for the {@link SinkWriter} interface. Used by the {@link\n+ * org.apache.iceberg.flink.sink.IcebergSink} (SinkV2). Writes out the data to the final place, and\n+ * emits a single {@link WriteResult} at every checkpoint for every data/delete file created by this\n+ * writer.\n+ */\n+class IcebergSinkWriter implements CommittingSinkWriter<RowData, WriteResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergSinkWriter.class);\n+\n+  private final String fullTableName;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+  private final IcebergStreamWriterMetrics metrics;\n+  private TaskWriter<RowData> writer;\n+  private final int subTaskId;\n+  private final int attemptId;\n+\n+  IcebergSinkWriter(\n+      String fullTableName,\n+      TaskWriterFactory<RowData> taskWriterFactory,\n+      IcebergStreamWriterMetrics metrics,\n+      int subTaskId,\n+      int attemptId) {\n+    this.fullTableName = fullTableName;\n+    this.taskWriterFactory = taskWriterFactory;\n+    // Initialize the task writer factory.\n+    taskWriterFactory.initialize(subTaskId, attemptId);\n+    // Initialize the task writer.\n+    this.writer = taskWriterFactory.create();\n+    this.metrics = metrics;\n+    this.subTaskId = subTaskId;\n+    this.attemptId = attemptId;\n+    LOG.debug(\n+        \"Created Stream Writer for table {} subtask {} attemptId {}\",\n+        fullTableName,\n+        subTaskId,\n+        attemptId);\n+  }\n+\n+  @Override\n+  public void write(RowData element, Context context) throws IOException, InterruptedException {\n+    writer.write(element);\n+  }\n+\n+  @Override\n+  public void flush(boolean endOfInput) {\n+    // flush is used to handle flush/endOfInput, so no action is taken here.\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"tableName\", fullTableName)\n+        .add(\"subTaskId\", subTaskId)\n+        .add(\"attemptId\", attemptId)\n+        .toString();\n+  }\n+\n+  @Override\n+  public Collection<WriteResult> prepareCommit() throws IOException {\n+    long startNano = System.nanoTime();\n+    WriteResult result = writer.complete();\n+    this.writer = taskWriterFactory.create();\n+    metrics.updateFlushResult(result);\n+    metrics.flushDuration(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startNano));\n+    LOG.debug(\n+        \"Iceberg writer subtask {} attempt {} flushed {} data files and {} delete files\",\n+        subTaskId,\n+        attemptId,\n+        result.dataFiles().length,\n+        result.deleteFiles().length);\n+    return Lists.newArrayList(result);\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\nindex 9ea0349fb057..7d86baa14fc2 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\n@@ -95,9 +95,9 @@ public void endInput() throws IOException {\n   @Override\n   public String toString() {\n     return MoreObjects.toStringHelper(this)\n-        .add(\"table_name\", fullTableName)\n-        .add(\"subtask_id\", subTaskId)\n-        .add(\"attempt_id\", attemptId)\n+        .add(\"tableName\", fullTableName)\n+        .add(\"subTaskId\", subTaskId)\n+        .add(\"attemptId\", attemptId)\n         .toString();\n   }\n \ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergWriteAggregator.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergWriteAggregator.java\nnew file mode 100644\nindex 000000000000..794ade577976\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergWriteAggregator.java\n@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import org.apache.flink.core.io.SimpleVersionedSerialization;\n+import org.apache.flink.streaming.api.connector.sink2.CommittableMessage;\n+import org.apache.flink.streaming.api.connector.sink2.CommittableSummary;\n+import org.apache.flink.streaming.api.connector.sink2.CommittableWithLineage;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Operator which aggregates the individual {@link WriteResult} objects) to a single {@link\n+ * IcebergCommittable} per checkpoint (storing the serialized {@link\n+ * org.apache.iceberg.flink.sink.DeltaManifests}, jobId, operatorId, checkpointId)\n+ */\n+class IcebergWriteAggregator extends AbstractStreamOperator<CommittableMessage<IcebergCommittable>>\n+    implements OneInputStreamOperator<\n+        CommittableMessage<WriteResult>, CommittableMessage<IcebergCommittable>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergWriteAggregator.class);\n+  private static final byte[] EMPTY_MANIFEST_DATA = new byte[0];\n+  private final Collection<WriteResult> results;\n+  private transient ManifestOutputFileFactory icebergManifestOutputFileFactory;\n+  private transient Table table;\n+  private final TableLoader tableLoader;\n+\n+  IcebergWriteAggregator(TableLoader tableLoader) {\n+    this.results = Sets.newHashSet();\n+    this.tableLoader = tableLoader;\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    if (!tableLoader.isOpen()) {\n+      tableLoader.open();\n+    }\n+\n+    String flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+    String operatorId = getOperatorID().toString();\n+    int subTaskId = getRuntimeContext().getTaskInfo().getIndexOfThisSubtask();\n+    Preconditions.checkArgument(\n+        subTaskId == 0, \"The subTaskId must be zero in the IcebergWriteAggregator\");\n+    int attemptId = getRuntimeContext().getTaskInfo().getAttemptNumber();\n+    this.table = tableLoader.loadTable();\n+\n+    this.icebergManifestOutputFileFactory =\n+        FlinkManifestUtil.createOutputFileFactory(\n+            () -> table, table.properties(), flinkJobId, operatorId, subTaskId, attemptId);\n+  }\n+\n+  @Override\n+  public void finish() throws IOException {\n+    prepareSnapshotPreBarrier(Long.MAX_VALUE);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws IOException {\n+    IcebergCommittable committable =\n+        new IcebergCommittable(\n+            writeToManifest(results, checkpointId),\n+            getContainingTask().getEnvironment().getJobID().toString(),\n+            getRuntimeContext().getOperatorUniqueID(),\n+            checkpointId);\n+    CommittableMessage<IcebergCommittable> summary =\n+        new CommittableSummary<>(0, 1, checkpointId, 1, 1, 0);\n+    output.collect(new StreamRecord<>(summary));\n+    CommittableMessage<IcebergCommittable> message =\n+        new CommittableWithLineage<>(committable, checkpointId, 0);\n+    output.collect(new StreamRecord<>(message));\n+    LOG.info(\"Emitted commit message to downstream committer operator\");\n+    results.clear();\n+  }\n+\n+  /**\n+   * Write all the completed data files to a newly created manifest file and return the manifest's\n+   * avro serialized bytes.\n+   */\n+  public byte[] writeToManifest(Collection<WriteResult> writeResults, long checkpointId)\n+      throws IOException {\n+    if (writeResults.isEmpty()) {\n+      return EMPTY_MANIFEST_DATA;\n+    }\n+\n+    WriteResult result = WriteResult.builder().addAll(writeResults).build();\n+    DeltaManifests deltaManifests =\n+        FlinkManifestUtil.writeCompletedFiles(\n+            result, () -> icebergManifestOutputFileFactory.create(checkpointId), table.spec());\n+\n+    return SimpleVersionedSerialization.writeVersionAndSerialize(\n+        DeltaManifestsSerializer.INSTANCE, deltaManifests);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<CommittableMessage<WriteResult>> element)\n+      throws Exception {\n+\n+    if (element.isRecord() && element.getValue() instanceof CommittableWithLineage) {\n+      results.add(((CommittableWithLineage<WriteResult>) element.getValue()).getCommittable());\n+    }\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java\nindex da5e6e7627ae..1cb7f4dea1e8 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java\n@@ -26,13 +26,13 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n \n class ManifestOutputFileFactory {\n   // Users could define their own flink manifests directory by setting this value in table\n   // properties.\n-  static final String FLINK_MANIFEST_LOCATION = \"flink.manifests.location\";\n-\n+  @VisibleForTesting static final String FLINK_MANIFEST_LOCATION = \"flink.manifests.location\";\n   private final Supplier<Table> tableSupplier;\n   private final Map<String, String> props;\n   private final String flinkJobId;\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/SinkUtil.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/SinkUtil.java\nnew file mode 100644\nindex 000000000000..7f28a50ecaa8\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/SinkUtil.java\n@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class SinkUtil {\n+\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  public static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  public static final String OPERATOR_ID = \"flink.operator-id\";\n+  public static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  private SinkUtil() {}\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(SinkUtil.class);\n+\n+  static List<Integer> checkAndGetEqualityFieldIds(Table table, List<String> equalityFieldColumns) {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().identifierFieldIds());\n+    if (equalityFieldColumns != null && !equalityFieldColumns.isEmpty()) {\n+      Set<Integer> equalityFieldSet = Sets.newHashSetWithExpectedSize(equalityFieldColumns.size());\n+      for (String column : equalityFieldColumns) {\n+        org.apache.iceberg.types.Types.NestedField field = table.schema().findField(column);\n+        Preconditions.checkNotNull(\n+            field,\n+            \"Missing required equality field column '%s' in table schema %s\",\n+            column,\n+            table.schema());\n+        equalityFieldSet.add(field.fieldId());\n+      }\n+\n+      if (!equalityFieldSet.equals(table.schema().identifierFieldIds())) {\n+        LOG.warn(\n+            \"The configured equality field column IDs {} are not matched with the schema identifier field IDs\"\n+                + \" {}, use job specified equality field columns as the equality fields by default.\",\n+            equalityFieldSet,\n+            table.schema().identifierFieldIds());\n+      }\n+      equalityFieldIds = Lists.newArrayList(equalityFieldSet);\n+    }\n+    return equalityFieldIds;\n+  }\n+\n+  static long getMaxCommittedCheckpointId(\n+      Table table, String flinkJobId, String operatorId, String branch) {\n+    Snapshot snapshot = table.snapshot(branch);\n+    long lastCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    while (snapshot != null) {\n+      Map<String, String> summary = snapshot.summary();\n+      String snapshotFlinkJobId = summary.get(FLINK_JOB_ID);\n+      String snapshotOperatorId = summary.get(OPERATOR_ID);\n+      if (flinkJobId.equals(snapshotFlinkJobId)\n+          && (snapshotOperatorId == null || snapshotOperatorId.equals(operatorId))) {\n+        String value = summary.get(MAX_COMMITTED_CHECKPOINT_ID);\n+        if (value != null) {\n+          lastCommittedCheckpointId = Long.parseLong(value);\n+          break;\n+        }\n+      }\n+      Long parentSnapshotId = snapshot.parentId();\n+      snapshot = parentSnapshotId != null ? table.snapshot(parentSnapshotId) : null;\n+    }\n+\n+    return lastCommittedCheckpointId;\n+  }\n+}\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/WriteResultSerializer.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/WriteResultSerializer.java\nnew file mode 100644\nindex 000000000000..5a44373cccaa\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/WriteResultSerializer.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.util.InstantiationUtil;\n+import org.apache.iceberg.io.WriteResult;\n+\n+class WriteResultSerializer implements SimpleVersionedSerializer<WriteResult> {\n+  private static final int VERSION = 1;\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION;\n+  }\n+\n+  @Override\n+  public byte[] serialize(WriteResult writeResult) throws IOException {\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    DataOutputViewStreamWrapper view = new DataOutputViewStreamWrapper(out);\n+    byte[] result = InstantiationUtil.serializeObject(writeResult);\n+    view.write(result);\n+    return out.toByteArray();\n+  }\n+\n+  @Override\n+  public WriteResult deserialize(int version, byte[] serialized) throws IOException {\n+    if (version == 1) {\n+      DataInputDeserializer view = new DataInputDeserializer(serialized);\n+      byte[] resultBuf = new byte[serialized.length];\n+      view.read(resultBuf);\n+      try {\n+        return InstantiationUtil.deserializeObject(\n+            resultBuf, IcebergCommittableSerializer.class.getClassLoader());\n+      } catch (ClassNotFoundException cnc) {\n+        throw new IOException(\"Could not deserialize the WriteResult object\", cnc);\n+      }\n+    }\n+    throw new IOException(\"Unrecognized version or corrupt state: \" + version);\n+  }\n+}\n", "instance_id": "apache__iceberg-11011", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of introducing a new `IcebergSink` based on the Flink V2 Sink Abstraction. It provides context about the motivation (unified handling of bounded and unbounded data streams, small file compaction), references to prior work, and links to relevant documentation and discussions (e.g., FLIP-143, FLIP-191). The summary and new implementation sections outline the purpose and scope of the change, including the use of new Flink hooks like `SupportsPostCommitTopology`. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the `IcebergSink`, nor does it detail specific constraints or performance requirements. Additionally, while edge cases like handling empty commits are mentioned in the code, they are not discussed in the problem statement. Overall, the statement is valid and clear but lacks some specifics that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.75) due to several factors. First, the scope of code changes is significant, involving multiple new files and modifications across the existing codebase (e.g., `IcebergSink.java`, `IcebergCommitter.java`, `IcebergWriteAggregator.java`, and updates to utility classes like `FlinkManifestUtil.java`). This requires understanding interactions between various components of the Apache Iceberg and Flink integration, such as writers, committers, and aggregators, as well as the broader system architecture of Flink's Sink API. Second, the number of technical concepts involved is substantial, including Flink's V2 Sink Abstraction, Iceberg\u2019s table and snapshot management, serialization/deserialization mechanisms, and distributed data processing patterns. Familiarity with specific Flink hooks (`SupportsPreWriteTopology`, `SupportsCommitter`, etc.) and Iceberg\u2019s commit and manifest handling is necessary. Third, the problem demands handling complex edge cases and error conditions, such as managing continuous empty commits, cleaning up temporary manifests with error logging, and ensuring commit consistency across checkpoints in a distributed environment. While it does not reach the \"very hard\" level (0.8-1.0) due to the absence of novel algorithm design or system-level distributed consensus challenges, it still requires deep knowledge of the codebase and careful implementation to avoid data loss or inconsistencies. The impact on the system is notable as it introduces a new sink implementation that could affect user workflows and performance in production environments.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add a public function to generate test data\n**Feature request**\r\n\r\nIt would be useful to create a `datasets` module with some simple datasets built-in. We may start with a really simple function to generate a simple test time-domain catalog, like we have in `tape`'s tests.\r\n\r\n**Before submitting**\r\nPlease check the following:\r\n\r\n- [x] I have described the purpose of the suggested change, specifying what I need the enhancement to accomplish, i.e. what problem it solves.\r\n- [x] I have included any relevant links, screenshots, environment information, and data relevant to implementing the requested feature, as well as pseudocode for how I want to access the new functionality.\r\n- [x] If I have ideas for how the new feature could be implemented, I have provided explanations and/or pseudocode and/or task lists for the steps.\r\n\n", "patch": "diff --git a/src/nested_pandas/datasets/__init__.py b/src/nested_pandas/datasets/__init__.py\nnew file mode 100644\nindex 0000000..4b8e827\n--- /dev/null\n+++ b/src/nested_pandas/datasets/__init__.py\n@@ -0,0 +1,1 @@\n+from .generation import *  # noqa\ndiff --git a/src/nested_pandas/datasets/generation.py b/src/nested_pandas/datasets/generation.py\nnew file mode 100644\nindex 0000000..f50bfe1\n--- /dev/null\n+++ b/src/nested_pandas/datasets/generation.py\n@@ -0,0 +1,55 @@\n+import numpy as np\n+\n+from nested_pandas import NestedFrame\n+\n+\n+def generate_data(n_base, n_layer, seed=None) -> NestedFrame:\n+    \"\"\"Generates a toy dataset.\n+\n+    Parameters\n+    ----------\n+    n_base : int\n+        The number of rows to generate for the base layer\n+    n_layer : int, or dict\n+        The number of rows per n_base row to generate for a nested layer.\n+        Alternatively, a dictionary of layer label, layer_size pairs may be\n+        specified to created multiple nested columns with custom sizing.\n+    seed : int\n+        A seed to use for random generation of data\n+\n+    Returns\n+    -------\n+    NestedFrame\n+        The constructed NestedFrame.\n+\n+    Examples\n+    --------\n+    >>> nested_pandas.datasets.generate_data(10,100)\n+    >>> nested_pandas.datasets.generate_data(10, {\"nested_a\": 100, \"nested_b\": 200})\n+    \"\"\"\n+    # use provided seed, \"None\" acts as if no seed is provided\n+    randomstate = np.random.RandomState(seed=seed)\n+\n+    # Generate base data\n+    base_data = {\"a\": randomstate.random(n_base), \"b\": randomstate.random(n_base) * 2}\n+    base_nf = NestedFrame(data=base_data)\n+\n+    # In case of int, create a single nested layer called \"nested\"\n+    if isinstance(n_layer, int):\n+        n_layer = {\"nested\": n_layer}\n+\n+    # It should be a dictionary\n+    if isinstance(n_layer, dict):\n+        for key in n_layer:\n+            layer_size = n_layer[key]\n+            layer_data = {\n+                \"t\": randomstate.random(layer_size * n_base) * 20,\n+                \"flux\": randomstate.random(layer_size * n_base) * 100,\n+                \"band\": randomstate.choice([\"r\", \"g\"], size=layer_size * n_base),\n+                \"index\": np.arange(layer_size * n_base) % n_base,\n+            }\n+            layer_nf = NestedFrame(data=layer_data).set_index(\"index\")\n+            base_nf = base_nf.add_nested(layer_nf, key)\n+        return base_nf\n+    else:\n+        raise TypeError(\"Input to n_layer is not an int or dict.\")\n", "instance_id": "lincc-frameworks__nested-pandas-55", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add a public function for generating test data within a new `datasets` module. It specifies the goal of creating a simple dataset similar to one used in tests and provides a basic idea of the desired functionality. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected structure or content of the dataset (e.g., what fields or data types should be included), nor does it mention specific constraints or edge cases to handle. While the code changes provide clarity on the implementation, the problem statement itself lacks comprehensive examples or detailed requirements for the function's behavior. Thus, it falls into the \"Mostly Clear\" category with minor details missing.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is limited to creating a new module with two files (`__init__.py` and `generation.py`) and implementing a single function `generate_data`. The changes do not impact the broader codebase architecture or require modifications across multiple existing modules. Second, the technical concepts involved are relatively straightforward: basic Python programming, usage of `numpy` for random data generation, and familiarity with a custom `NestedFrame` class (presumably part of the project's library). The logic for generating nested data structures is not overly complex, though it requires understanding how to handle nested layers via a dictionary or integer input. Third, the problem does not explicitly mention complex edge cases or performance considerations, though the code does include basic type checking and error handling (raising a `TypeError` for invalid input). Overall, the task requires understanding some code logic and making simple function implementations, fitting a difficulty score of 0.35. It does not rise to a medium difficulty level as it lacks significant cross-module interactions or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "AWS 2-4: Fix Installing GWLF-E on MMW\nCurrently we get the following error:\r\n\r\n```\r\nRuntimeError: module compiled against API version 0xf but this version of numpy is 0xe\r\n```\r\n\r\nThis is because `numba==0.55.*` uses `numpy<1.23` which translates to `numpy==1.22.*`, but in this project we specify `numpy==1.21.*`, which causes such conflicts.\n", "patch": "diff --git a/Pipfile b/Pipfile\nindex a825a6d..62ec2b5 100644\n--- a/Pipfile\n+++ b/Pipfile\n@@ -8,7 +8,7 @@ certifi = \">=2021\"\n funcsigs = \"==1.0.2\"\n llvmlite = \"==0.38.*\"\n pynose = \"==1.5.*\"\n-numpy = \"==1.21.*\"\n+numpy = \"==1.22.*\"\n numba = \"==0.55.*\"\n \n [dev-packages]\ndiff --git a/Pipfile.lock b/Pipfile.lock\nindex 0447b85..2ed64c9 100644\n--- a/Pipfile.lock\n+++ b/Pipfile.lock\n@@ -1,7 +1,7 @@\n {\n     \"_meta\": {\n         \"hash\": {\n-            \"sha256\": \"db50ee70395401a3ee4232a96283055f794d4572c2a9c270f960eaae2546dedb\"\n+            \"sha256\": \"d0423256532094b11070be115346721a961b7095f561e612fd7c60866822a56d\"\n         },\n         \"pipfile-spec\": 6,\n         \"requires\": {\n@@ -18,12 +18,12 @@\n     \"default\": {\n         \"certifi\": {\n             \"hashes\": [\n-                \"sha256:3cd43f1c6fa7dedc5899d69d3ad0398fd018ad1a17fba83ddaf78aa46c747516\",\n-                \"sha256:ddc6c8ce995e6987e7faf5e3f1b02b302836a0e5d98ece18392cb1a36c72ad56\"\n+                \"sha256:5a1e7645bc0ec61a09e26c36f6106dd4cf40c6db3a1fb6352b0244e7fb057c7b\",\n+                \"sha256:c198e21b1289c2ab85ee4e67bb4b4ef3ead0892059901a8d5b622f24a1101e90\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.6'\",\n-            \"version\": \"==2024.6.2\"\n+            \"version\": \"==2024.7.4\"\n         },\n         \"funcsigs\": {\n             \"hashes\": [\n@@ -105,41 +105,32 @@\n         },\n         \"numpy\": {\n             \"hashes\": [\n-                \"sha256:1dbe1c91269f880e364526649a52eff93ac30035507ae980d2fed33aaee633ac\",\n-                \"sha256:357768c2e4451ac241465157a3e929b265dfac85d9214074985b1786244f2ef3\",\n-                \"sha256:3820724272f9913b597ccd13a467cc492a0da6b05df26ea09e78b171a0bb9da6\",\n-                \"sha256:4391bd07606be175aafd267ef9bea87cf1b8210c787666ce82073b05f202add1\",\n-                \"sha256:4aa48afdce4660b0076a00d80afa54e8a97cd49f457d68a4342d188a09451c1a\",\n-                \"sha256:58459d3bad03343ac4b1b42ed14d571b8743dc80ccbf27444f266729df1d6f5b\",\n-                \"sha256:5c3c8def4230e1b959671eb959083661b4a0d2e9af93ee339c7dada6759a9470\",\n-                \"sha256:5f30427731561ce75d7048ac254dbe47a2ba576229250fb60f0fb74db96501a1\",\n-                \"sha256:643843bcc1c50526b3a71cd2ee561cf0d8773f062c8cbaf9ffac9fdf573f83ab\",\n-                \"sha256:67c261d6c0a9981820c3a149d255a76918278a6b03b6a036800359aba1256d46\",\n-                \"sha256:67f21981ba2f9d7ba9ade60c9e8cbaa8cf8e9ae51673934480e45cf55e953673\",\n-                \"sha256:6aaf96c7f8cebc220cdfc03f1d5a31952f027dda050e5a703a0d1c396075e3e7\",\n-                \"sha256:7c4068a8c44014b2d55f3c3f574c376b2494ca9cc73d2f1bd692382b6dffe3db\",\n-                \"sha256:7c7e5fa88d9ff656e067876e4736379cc962d185d5cd808014a8a928d529ef4e\",\n-                \"sha256:7f5ae4f304257569ef3b948810816bc87c9146e8c446053539947eedeaa32786\",\n-                \"sha256:82691fda7c3f77c90e62da69ae60b5ac08e87e775b09813559f8901a88266552\",\n-                \"sha256:8737609c3bbdd48e380d463134a35ffad3b22dc56295eff6f79fd85bd0eeeb25\",\n-                \"sha256:9f411b2c3f3d76bba0865b35a425157c5dcf54937f82bbeb3d3c180789dd66a6\",\n-                \"sha256:a6be4cb0ef3b8c9250c19cc122267263093eee7edd4e3fa75395dfda8c17a8e2\",\n-                \"sha256:bcb238c9c96c00d3085b264e5c1a1207672577b93fa666c3b14a45240b14123a\",\n-                \"sha256:bf2ec4b75d0e9356edea834d1de42b31fe11f726a81dfb2c2112bc1eaa508fcf\",\n-                \"sha256:d136337ae3cc69aa5e447e78d8e1514be8c3ec9b54264e680cf0b4bd9011574f\",\n-                \"sha256:d4bf4d43077db55589ffc9009c0ba0a94fa4908b9586d6ccce2e0b164c86303c\",\n-                \"sha256:d6a96eef20f639e6a97d23e57dd0c1b1069a7b4fd7027482a4c5c451cd7732f4\",\n-                \"sha256:d9caa9d5e682102453d96a0ee10c7241b72859b01a941a397fd965f23b3e016b\",\n-                \"sha256:dd1c8f6bd65d07d3810b90d02eba7997e32abbdf1277a481d698969e921a3be0\",\n-                \"sha256:e31f0bb5928b793169b87e3d1e070f2342b22d5245c755e2b81caa29756246c3\",\n-                \"sha256:ecb55251139706669fdec2ff073c98ef8e9a84473e51e716211b41aa0f18e656\",\n-                \"sha256:ee5ec40fdd06d62fe5d4084bef4fd50fd4bb6bfd2bf519365f569dc470163ab0\",\n-                \"sha256:f17e562de9edf691a42ddb1eb4a5541c20dd3f9e65b09ded2beb0799c0cf29bb\",\n-                \"sha256:fdffbfb6832cd0b300995a2b08b8f6fa9f6e856d562800fea9182316d99c4e8e\"\n+                \"sha256:0791fbd1e43bf74b3502133207e378901272f3c156c4df4954cad833b1380207\",\n+                \"sha256:1ce7ab2053e36c0a71e7a13a7475bd3b1f54750b4b433adc96313e127b870887\",\n+                \"sha256:2d487e06ecbf1dc2f18e7efce82ded4f705f4bd0cd02677ffccfb39e5c284c7e\",\n+                \"sha256:37431a77ceb9307c28382c9773da9f306435135fae6b80b62a11c53cfedd8802\",\n+                \"sha256:3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\",\n+                \"sha256:425b390e4619f58d8526b3dcf656dde069133ae5c240229821f01b5f44ea07af\",\n+                \"sha256:43a8ca7391b626b4c4fe20aefe79fec683279e31e7c79716863b4b25021e0e74\",\n+                \"sha256:4c6036521f11a731ce0648f10c18ae66d7143865f19f7299943c985cdc95afb5\",\n+                \"sha256:59d55e634968b8f77d3fd674a3cf0b96e85147cd6556ec64ade018f27e9479e1\",\n+                \"sha256:64f56fc53a2d18b1924abd15745e30d82a5782b2cab3429aceecc6875bd5add0\",\n+                \"sha256:7228ad13744f63575b3a972d7ee4fd61815b2879998e70930d4ccf9ec721dce0\",\n+                \"sha256:9ce7df0abeabe7fbd8ccbf343dc0db72f68549856b863ae3dd580255d009648e\",\n+                \"sha256:a911e317e8c826ea632205e63ed8507e0dc877dcdc49744584dfc363df9ca08c\",\n+                \"sha256:b89bf9b94b3d624e7bb480344e91f68c1c6c75f026ed6755955117de00917a7c\",\n+                \"sha256:ba9ead61dfb5d971d77b6c131a9dbee62294a932bf6a356e48c75ae684e635b3\",\n+                \"sha256:c1d937820db6e43bec43e8d016b9b3165dcb42892ea9f106c70fb13d430ffe72\",\n+                \"sha256:cc7f00008eb7d3f2489fca6f334ec19ca63e31371be28fd5dad955b16ec285bd\",\n+                \"sha256:d4c5d5eb2ec8da0b4f50c9a843393971f31f1d60be87e0fb0917a49133d257d6\",\n+                \"sha256:e96d7f3096a36c8754207ab89d4b3282ba7b49ea140e4973591852c77d09eb76\",\n+                \"sha256:f0725df166cf4785c0bc4cbfb320203182b1ecd30fee6e541c8752a92df6aa32\",\n+                \"sha256:f3eb268dbd5cfaffd9448113539e44e2dd1c5ca9ce25576f7c04a5453edc26fa\",\n+                \"sha256:fb7a980c81dd932381f8228a426df8aeb70d59bbcda2af075b627bbc50207cba\"\n             ],\n             \"index\": \"pypi\",\n-            \"markers\": \"python_version < '3.11' and python_version >= '3.7'\",\n-            \"version\": \"==1.21.6\"\n+            \"markers\": \"python_version >= '3.8'\",\n+            \"version\": \"==1.22.4\"\n         },\n         \"pynose\": {\n             \"hashes\": [\n@@ -195,12 +186,12 @@\n         },\n         \"certifi\": {\n             \"hashes\": [\n-                \"sha256:3cd43f1c6fa7dedc5899d69d3ad0398fd018ad1a17fba83ddaf78aa46c747516\",\n-                \"sha256:ddc6c8ce995e6987e7faf5e3f1b02b302836a0e5d98ece18392cb1a36c72ad56\"\n+                \"sha256:5a1e7645bc0ec61a09e26c36f6106dd4cf40c6db3a1fb6352b0244e7fb057c7b\",\n+                \"sha256:c198e21b1289c2ab85ee4e67bb4b4ef3ead0892059901a8d5b622f24a1101e90\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.6'\",\n-            \"version\": \"==2024.6.2\"\n+            \"version\": \"==2024.7.4\"\n         },\n         \"cffi\": {\n             \"hashes\": [\ndiff --git a/README.md b/README.md\nindex 75220c7..75b56fd 100644\n--- a/README.md\n+++ b/README.md\n@@ -23,7 +23,8 @@ $ pip install --no-build-isolation gwlf-e\n Ensure you have Python 3.10 and [pipenv](https://pipenv.pypa.io/en/latest/) available. Then run:\n \n ```bash\n-$ pipenv sync\n+$ pipenv sync --dev\n+$ pipenv run python setup.py build\n ```\n \n ### Running Locally\ndiff --git a/requirements.txt b/requirements.txt\nindex b7c75be..e309cdc 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -2,5 +2,5 @@ certifi>=2021\n funcsigs==1.0.2\n llvmlite==0.38.*\n pynose==1.5.*\n-numpy==1.21.*\n+numpy==1.22.*\n numba==0.55.*\ndiff --git a/setup.py b/setup.py\nindex c0cd6eb..77ab42e 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -56,7 +56,7 @@\n         'funcsigs==1.0.2',\n         'llvmlite==0.38.*',\n         'pynose==1.5.*',\n-        'numpy==1.21.*',\n+        'numpy==1.22.*',\n         'numba==0.55.*',\n     ],\n     extras_require={\n", "instance_id": "WikiWatershed__gwlf-e-101", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a version conflict between `numpy` and `numba` causing a runtime error due to API version mismatch. The goal of resolving this conflict by updating the `numpy` version is implied and supported by the provided error message. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly confirm whether updating `numpy` to `1.22.*` fully resolves the issue or if there are other dependencies or side effects to consider. Additionally, there is no mention of potential compatibility issues with other parts of the codebase or specific testing requirements post-change. The context of \"Installing GWLF-E on MMW\" is also not elaborated upon, which might be critical for someone unfamiliar with the project. Despite these minor gaps, the problem is valid and the intent is reasonably clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The issue is a straightforward dependency version conflict, and the solution involves a simple update of the `numpy` version from `1.21.*` to `1.22.*` across a few configuration files (`Pipfile`, `requirements.txt`, `setup.py`). The code changes are minimal, localized to specific lines in dependency files, and do not require deep understanding of the codebase or complex logic modifications. The scope is limited to updating version strings and regenerating the `Pipfile.lock`, with no impact on the system's architecture or interactions between modules. The technical concepts involved are basic\u2014understanding Python dependency management with tools like `pipenv`\u2014and do not require advanced knowledge or algorithms. While there is a slight risk of unmentioned compatibility issues with other dependencies or parts of the codebase, no specific edge cases or error handling requirements are mentioned or appear necessary beyond the version update. Overall, this is a routine task for anyone with basic experience in Python development and dependency management, justifying a difficulty score of 0.15.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Node for parenthesized expressions\nHello,\r\n\r\nmaintainer of [HiPhish/rainbow-delimiters.nvim](https://github.com/hiphish/rainbow-delimiters.nvim) here; I have a problem matching and properly highlighting parenthesized expressions. The problem is that all the parentheses of an expression are on the same level as their siblings. Consider the following query:\r\n\r\n```sql\r\nSELECT ((((1))));\r\n```\r\n\r\nIt's a weird query, but at least in SQLite it is valid. The tree looks like this:\r\n\r\n```\r\n  (select\r\n    (keyword_select)\r\n    (select_expression\r\n      (term\r\n        value: \"(\"\r\n        value: \"(\"\r\n        value: \"(\"\r\n        value: \"(\"\r\n        value: (literal)\r\n        value: \")\"\r\n        value: \")\"\r\n        value: \")\"\r\n        value: \")\")))\r\n```\r\n\r\nAll the parentheses are on the same level, it is impossible to build up a tree of \"containers\" to determine the proper nesting. I propose a sort of `parenthesized_expression` node (or whatever name you consider more appropriate). With that node the `term` would have only one `value`:\r\n\r\n```\r\n  (select\r\n    (keyword_select)\r\n    (select_expression\r\n      (term\r\n    \tvalue: (parenthesized_expression\r\n    \t  \"(\"\r\n    \t  (parenthesized_expression\r\n    \t    \"(\"\r\n    \t    (parenthesized_expression\r\n    \t      \"(\"\r\n    \t      (parenthesized_expression\r\n    \t        \"(\"\r\n                (literal)\r\n    \t        \")\")\r\n    \t      \")\")\r\n    \t    \")\")\r\n    \t  \")\"))))\r\n```\r\n\r\nLet's take another example:\r\n\r\n```sql\r\nSELECT 1 + (2);\r\n```\r\n\r\nThe original tree:\r\n\r\n```\r\n  (select\r\n    (keyword_select)\r\n    (select_expression\r\n      (term\r\n        value: (binary_expression\r\n          left: (literal)\r\n          operator: \"+\"\r\n          right: \"(\"\r\n          right: (literal)\r\n          right: \")\"))))\r\n```\r\n\r\nHow many `right` nodes can a binary expression reasonably have? Here is what it would look like with `parenthesized_expression`:\r\n\r\n```\r\n(select\r\n  (keyword_select)\r\n  (select_expression\r\n    (term\r\n      value: (binary_expression\r\n        left: (literal)\r\n        operator: \"+\"\r\n        right: (parenthesized_expression\r\n          \"(\"\r\n          (literal)\r\n          \")\")))))\r\n```\r\n\n", "patch": "diff --git a/grammar.js b/grammar.js\nindex 99f57229..422940f3 100644\n--- a/grammar.js\n+++ b/grammar.js\n@@ -3254,10 +3254,14 @@ module.exports = grammar({\n         $.array,\n         $.interval,\n         $.between_expression,\n-        wrapped_in_parenthesis($._expression),\n+        $.parenthesized_expression,\n       )\n     ),\n \n+    parenthesized_expression: $ => prec(2,\n+      wrapped_in_parenthesis($._expression)\n+    ),\n+\n     subscript: $ => prec.left('binary_is',\n       seq(\n         field('expression', $._expression),\n", "instance_id": "DerekStride__tree-sitter-sql-275", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the current parsing of parenthesized expressions in SQL queries and the desired outcome of introducing a `parenthesized_expression` node to represent nested parentheses hierarchically. The examples provided (SQL queries and corresponding tree structures) effectively illustrate the problem and the proposed solution, which aids in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., malformed or unbalanced parentheses) or constraints on the nesting depth. Additionally, it lacks clarity on how this change might interact with other parts of the grammar or parsing logic beyond the provided examples. While the intent is clear, these omissions prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is relatively small and localized, as shown in the diff, which modifies a single file (`grammar.js`) and introduces a new rule for `parenthesized_expression` with minimal alterations to existing logic. The change does not appear to impact the broader architecture of the system or require extensive modifications across multiple modules. Second, the technical concepts involved are straightforward: it requires understanding of grammar definitions in a parser generator (likely Tree-sitter, given the context) and basic precedence rules (`prec(2, ...)`). These concepts are not particularly complex for someone familiar with parser development. Third, while the problem statement does not explicitly mention edge cases, the nature of parsing nested parentheses implies potential issues like unbalanced parentheses or deeply nested structures, but the provided code change does not address these, suggesting minimal complexity in error handling at this stage. Overall, solving this problem requires understanding some code logic and making a simple modification to the grammar, aligning with an \"Easy\" difficulty level of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\"Information loss on integer cast\" for pre-1970 timestamp_ns\n### What happens?\r\n\r\nThe query \"select '1969-01-01T23:59:59.9999999'::timestamp_ns;\" gives the following error:\r\n\r\n```\r\nINTERNAL Error: Information loss on integer cast: value -99952 outside of target range [-128, 127]\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun the query\r\n```sql\r\nselect '1969-01-01T23:59:59.9999999'::timestamp_ns;\r\n```\r\non DuckDB v1.1.3 19864453f7. The error will appear.\r\n\r\n### OS:\r\n\r\nMacOS ARM (Darwin 23.6.0 Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8122 arm64)\r\n\r\n### DuckDB Version:\r\n\r\nv1.1.3 19864453f7\r\n\r\n### DuckDB Client:\r\n\r\nCommand-line \"duckdb\" on MacOS\r\n\r\n### Hardware:\r\n\r\nMacBook Air Apple M3\r\n\r\n### Full Name:\r\n\r\nEirik Bakke\r\n\r\n### Affiliation:\r\n\r\nUltorg Inc.\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/common/types/timestamp.cpp b/src/common/types/timestamp.cpp\nindex dbb1202d002e..f23ee2146c40 100644\n--- a/src/common/types/timestamp.cpp\n+++ b/src/common/types/timestamp.cpp\n@@ -17,6 +17,15 @@ namespace duckdb {\n \n static_assert(sizeof(timestamp_t) == sizeof(int64_t), \"timestamp_t was padded\");\n \n+// Temporal values need to round down when changing precision,\n+// but C/C++ rounds towrds 0 when you simply divide.\n+// This piece of bit banging solves that problem.\n+template <typename T>\n+static inline T TemporalRound(T value, T scale) {\n+\tconst auto negative = int(value < 0);\n+\treturn UnsafeNumericCast<T>((value + negative) / scale - negative);\n+}\n+\n // timestamp/datetime uses 64 bits, high 32 bits for date and low 32 bits for time\n // string format is YYYY-MM-DDThh:mm:ssZ\n // T may be a space\n@@ -342,7 +351,7 @@ void Timestamp::Convert(timestamp_t timestamp, date_t &out_date, dtime_t &out_ti\n }\n \n void Timestamp::Convert(timestamp_ns_t input, date_t &out_date, dtime_t &out_time, int32_t &out_nanos) {\n-\ttimestamp_t ms(input.value / Interval::NANOS_PER_MICRO);\n+\ttimestamp_t ms(TemporalRound(input.value, Interval::NANOS_PER_MICRO));\n \tout_date = Timestamp::GetDate(ms);\n \tint64_t days_nanos;\n \tif (!TryMultiplyOperator::Operation<int64_t, int64_t, int64_t>(out_date.days, Interval::NANOS_PER_DAY,\n", "instance_id": "duckdb__duckdb-15289", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an error occurs when casting a pre-1970 timestamp_ns value in DuckDB, with a specific query provided to reproduce the issue. The goal is implicitly understood as fixing the error related to integer cast information loss. The input (a specific timestamp string), output (expected to be a valid timestamp without error), and the error message are provided, along with relevant environment details (OS, DuckDB version, hardware). However, there are minor ambiguities: the problem statement does not explicitly state the expected behavior or output for the given query, nor does it mention specific constraints or edge cases beyond the provided example. Additionally, it lacks clarity on whether the fix should handle only this specific case or generalize to other timestamp values. Despite these minor gaps, the issue is reproducible and the intent is reasonably clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is localized to a single file (`timestamp.cpp`) and involves a small modification (replacing a direct division with a custom rounding function). It does not impact multiple modules or the broader system architecture, and the amount of code change is minimal (adding a helper function and updating one line). Understanding the interaction between components is limited to the timestamp conversion logic within this file.\n\n2. **Number of Technical Concepts:** Solving this requires understanding C++ integer arithmetic, specifically the behavior of division and rounding with negative numbers, as well as domain-specific knowledge of timestamp handling in DuckDB (e.g., how nanosecond precision is converted to milliseconds). The concept of \"rounding down\" for temporal values is introduced, which is not overly complex but requires attention to detail. No advanced algorithms, design patterns, or external libraries are involved.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement highlights a specific edge case (pre-1970 timestamps with nanosecond precision), and the code change addresses it by adjusting rounding behavior for negative values. However, the statement does not discuss other potential edge cases (e.g., other negative timestamps, boundary values near zero, or overflow scenarios), and the code change does not introduce new error handling logic beyond the rounding fix. The complexity of edge cases appears moderate and localized to negative value handling.\n\n4. **Overall Assessment:** This problem requires understanding a specific bug in timestamp conversion and applying a straightforward fix via a custom rounding function. It does not demand deep knowledge of the DuckDB codebase beyond the timestamp module, nor does it involve complex system-wide changes or performance optimizations. The main challenge lies in correctly handling negative values during division, which is a relatively simple arithmetic issue. Therefore, a difficulty score of 0.35 is assigned, reflecting an Easy problem that requires some logic understanding and a targeted code modification.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "FATAL Error: 'Operation requires a flat vector but a non-flat vector was encountered' on CASE conditional expression\n### What happens?\r\n\r\nThe latest versions of the DuckDB (latest main: v1.1.4-dev2317 a4963a5aea and released version: v1.1.3 19864453f7) crash when running the following SQL statement:\r\n\r\n```sql\r\nSELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\r\n```\r\n\r\nHere is the stack frame that triggers the Internal Error: \r\n\r\n```\r\n#0  duckdb::InternalException::InternalException (this=0xaaaaea5738f0, msg=\"Operation requires a flat vector but a non-flat vector was encountered\")\r\n    at /home/duckdb/duckdb/src/common/exception.cpp:333\r\n#1  0x0000aaaadb9cd190 in duckdb::FlatVector::VerifyFlatVector (vector=...) at /usr/include/c++/9/ext/new_allocator.h:80\r\n#2  0x0000aaaadc1792b4 in duckdb::FlatVector::Validity (vector=...) at ../../src/include/duckdb/common/types/vector.hpp:361\r\n#3  duckdb::VectorStringToMap::StringToNestedTypeCastLoop (source_data=source_data@entry=0xaaaaea550280, source_mask=..., result=..., result_mask=..., count=count@entry=1, \r\n    parameters=..., sel=sel@entry=0x0) at /home/duckdb/duckdb/src/function/cast/string_cast.cpp:345\r\n#4  0x0000aaaadc216dec in duckdb::StringToNestedTypeCast<duckdb::VectorStringToMap> (count=<optimized out>, parameters=..., result=..., source=...)\r\n    at ../../src/include/duckdb/common/types/vector.hpp:362\r\n#5  duckdb::StringToNestedTypeCast<duckdb::VectorStringToMap> (source=..., result=..., count=<optimized out>, parameters=...)\r\n    at /home/duckdb/duckdb/src/function/cast/string_cast.cpp:440\r\n#6  0x0000aaaadbb54350 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state=state@entry=0xaaaaea54b980, sel=sel@entry=0x0, \r\n    count=count@entry=1, result=...) at /home/duckdb/duckdb/src/execution/expression_executor/execute_cast.cpp:42\r\n#7  0x0000aaaadbc83f48 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state=0xaaaaea54b980, sel=sel@entry=0x0, count=count@entry=1, \r\n    result=...) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:205\r\n#8  0x0000aaaadbb567f4 in duckdb::ExpressionExecutor::Select (this=0xffffe54a2a60, expr=..., state=<optimized out>, sel=0x0, count=1, true_sel=0xaaaaea530380, \r\n    false_sel=0xaaaaea530398) at /usr/include/c++/9/bits/unique_ptr.h:360\r\n#9  0x0000aaaadbc849c0 in duckdb::ExpressionExecutor::Select (false_sel=0xaaaaea530380, true_sel=0xaaaaea530380, count=1, sel=0x0, state=<optimized out>, expr=..., \r\n    this=0xffffe54a2a60) at ../../src/include/duckdb/parser/base_expression.hpp:98\r\n#10 duckdb::ExpressionExecutor::Select (this=this@entry=0xffffe54a2a60, expr=..., state=<optimized out>, sel=sel@entry=0x0, count=count@entry=1, \r\n    true_sel=true_sel@entry=0xaaaaea530380, false_sel=false_sel@entry=0xaaaaea530398) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:231\r\n#11 0x0000aaaadbb57224 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state_p=state_p@entry=0xaaaaea5302d0, sel=sel@entry=0x0, \r\n    count=count@entry=1, result=...) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#12 0x0000aaaadbc83fa8 in duckdb::ExpressionExecutor::Execute (this=0xffffe54a2a60, expr=..., state=0xaaaaea5302d0, sel=sel@entry=0x0, count=1, result=...)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:202\r\n#13 0x0000aaaadbc89518 in duckdb::ExpressionExecutor::ExecuteExpression (this=this@entry=0xffffe54a2a60, expr_idx=expr_idx@entry=0, result=...)\r\n    at ../../src/include/duckdb/common/types/data_chunk.hpp:54\r\n#14 0x0000aaaadbc896bc in duckdb::ExpressionExecutor::ExecuteExpression (this=this@entry=0xffffe54a2a60, result=...)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:96\r\n#15 0x0000aaaadbc89750 in duckdb::ExpressionExecutor::EvaluateScalar (context=..., expr=..., allow_unfoldable=allow_unfoldable@entry=false)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:112\r\n#16 0x0000aaaadbc899c8 in duckdb::ExpressionExecutor::TryEvaluateScalar (context=..., expr=..., result=...) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:122\r\n#17 0x0000aaaadbf5f2fc in duckdb::ConstantFoldingRule::Apply (this=0xaaaaea2f66c0, op=..., bindings=..., changes_made=<optimized out>, is_root=<optimized out>)\r\n    at /home/duckdb/duckdb/src/optimizer/rule/constant_folding.cpp:36\r\n#18 0x0000aaaadbf6ec40 in duckdb::ExpressionRewriter::ApplyRules (op=..., rules=..., expr=..., changes_made=@0xffffe54a2da7: false, is_root=is_root@entry=true)\r\n    at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:20\r\n#19 0x0000aaaadbf6f0a8 in duckdb::ExpressionRewriter::VisitExpression (this=0xffffe54a32f8, expression=0xaaaaea52eec0) at /usr/include/c++/9/bits/move.h:74\r\n#20 0x0000aaaadc0657b8 in std::function<void (duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*)>::operator()(duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*) const (__args#0=<optimized out>, this=0xffffe54a2ed8) at /usr/include/c++/9/bits/std_function.h:683\r\n#21 duckdb::LogicalOperatorVisitor::EnumerateExpressions(duckdb::LogicalOperator&, std::function<void (duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*)> const&) (op=..., callback=...) at /home/duckdb/duckdb/src/planner/logical_operator_visitor.cpp:175\r\n#22 0x0000aaaadc065b7c in duckdb::LogicalOperatorVisitor::VisitOperatorExpressions (this=this@entry=0xffffe54a32f8, op=...) at /usr/include/c++/9/new:174\r\n#23 0x0000aaaadbf6b8c8 in duckdb::ExpressionRewriter::VisitOperator (this=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:68\r\n#24 0x0000aaaadc07341c in duckdb::LogicalOperatorVisitor::VisitOperatorChildren (op=..., this=0xffffe54a32f8) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#25 duckdb::LogicalOperatorVisitor::VisitOperatorChildren (this=this@entry=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/planner/logical_operator_visitor.cpp:14\r\n#26 0x0000aaaadbf6b788 in duckdb::ExpressionRewriter::VisitOperator (this=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:60\r\n#27 0x0000aaaadbf6ba34 in duckdb::Optimizer::<lambda()>::operator() (__closure=<optimized out>) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#28 std::_Function_handler<void(), duckdb::Optimizer::RunBuiltInOptimizers()::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)\r\n    at /usr/include/c++/9/bits/std_function.h:300\r\n#29 0x0000aaaadbf6c624 in std::function<void ()>::operator()() const (this=0xffffe54a31a8) at /usr/include/c++/9/bits/std_function.h:683\r\n#30 duckdb::Optimizer::RunOptimizer(duckdb::OptimizerType, std::function<void ()> const&) (callback=..., type=duckdb::OptimizerType::EXPRESSION_REWRITER, \r\n    this=0xffffe54a32e8) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:80\r\n#31 duckdb::Optimizer::RunOptimizer(duckdb::OptimizerType, std::function<void ()> const&) (this=0xffffe54a32e8, type=duckdb::OptimizerType::EXPRESSION_REWRITER, \r\n    callback=...) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:73\r\n#32 0x0000aaaadbf6dc78 in duckdb::Optimizer::RunBuiltInOptimizers (this=this@entry=0xffffe54a32e8) at /usr/include/c++/9/new:174\r\n#33 0x0000aaaadbf6e6b0 in duckdb::Optimizer::Optimize (this=this@entry=0xffffe54a32e8, plan_p=...) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:253\r\n#34 0x0000aaaadbd75a50 in duckdb::ClientContext::CreatePreparedStatementInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., values=values@entry=...) at /usr/include/c++/9/bits/move.h:74\r\n#35 0x0000aaaadbd76108 in duckdb::ClientContext::CreatePreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., values=..., \r\n    mode=mode@entry=duckdb::PreparedStatementMode::PREPARE_ONLY) at /usr/include/c++/9/bits/move.h:74\r\n#36 0x0000aaaadbd76550 in duckdb::ClientContext::RebindPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...) at /usr/include/c++/9/bits/unique_ptr.h:360\r\n#37 0x0000aaaadbd777b8 in duckdb::ClientContext::PendingPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...)\r\n    at /home/duckdb/duckdb/src/main/client_context.cpp:544\r\n#38 0x0000aaaadbd7802c in duckdb::ClientContext::PendingStatementOrPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., prepared=..., parameters=...)\r\n    at /usr/include/c++/9/ext/atomicity.h:96\r\n#39 0x0000aaaadbd79400 in duckdb::ClientContext::PendingStatementOrPreparedStatementInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., prepared=..., parameters=...)\r\n    at /usr/include/c++/9/bits/move.h:74\r\n#40 0x0000aaaadbd79724 in duckdb::ClientContext::PendingQueryPreparedInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...) at /usr/include/c++/9/tuple:918\r\n#41 0x0000aaaadbd79864 in duckdb::ClientContext::PendingQuery (this=this@entry=0xaaaaea3954e0, \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...)\r\n    at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#42 0x0000aaaadbd79a80 in duckdb::PreparedStatement::PendingQuery (this=this@entry=0xaaaaea379bf0, named_values=std::unordered_map with 1 element = {...}, \r\n    allow_stream_result=allow_stream_result@entry=false) at ../../src/include/duckdb/common/shared_ptr_ipp.hpp:204\r\n#43 0x0000aaaadbd79fa4 in duckdb::PreparedStatement::PendingQuery (this=0xaaaaea379bf0, values=..., allow_stream_result=allow_stream_result@entry=false)\r\n    at /home/duckdb/duckdb/src/main/prepared_statement.cpp:94\r\n#44 0x0000aaaadbd7a210 in duckdb::PreparedStatement::Execute (this=<optimized out>, values=..., allow_stream_result=allow_stream_result@entry=false)\r\n    at /home/duckdb/duckdb/src/main/prepared_statement.cpp:81\r\n#45 0x0000aaaadb8a55fc in duckdb_shell_sqlite3_print_duckbox (pStmt=0xaaaaea52ac70, max_rows=40, max_width=0, null_value=0xffffe54a4948 \"NULL\", columnar=0, \r\n    thousand_separator=0 '\\000', decimal_separator=0 '\\000', result_renderer=0xffffe54a4530) at ../../src/include/duckdb/common/unique_ptr.hpp:39\r\n#46 0x0000aaaadb895dfc in duckdb_shell::ShellState::ExecutePreparedStatement (this=0xffffe54a4840, pStmt=0xaaaaea52ac70) at /usr/include/c++/9/bits/basic_string.h:2304\r\n#47 0x0000aaaadb896384 in duckdb_shell::ShellState::ExecuteSQL (this=0xffffe54a4840, \r\n    zSql=0xaaaaea2f2f10 \"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", pzErrMsg=0xffffe54a4660) at ../../tools/shell/shell.cpp:1788\r\n#48 0x0000aaaadb896c74 in duckdb_shell::ShellState::RunOneSqlLine (this=0xffffe54a4840, \r\n    zSql=0xaaaaea2f2f10 \"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\") at ../../tools/shell/shell.cpp:4318\r\n#49 0x0000aaaadb897298 in duckdb_shell::ShellState::ProcessInput (this=0xffffe54a4840) at ../../tools/shell/shell.cpp:4433\r\n#50 0x0000aaaadb87bd9c in main (argc=<optimized out>, argv=0xffffe54a4b28) at ../../tools/shell/shell.cpp:5032\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Clone the DuckDB Git from the official repo.\r\n2. Checkout to the latest main (v1.1.4-dev2317 a4963a5aea).\r\n3. Compile the DuckDB binary by using `CORE_EXTENSIONS='autocomplete;httpfs;icu;parquet;json' GEN=ninja make debug`.\r\n4. Run the compiled DuckDB and input the following SQL:\r\n\r\n```sql\r\nSELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 24.04.01 LTS (x86-64 & aarch64)\r\n\r\n### DuckDB Version:\r\n\r\nv1.1.3 19864453f7 and v1.1.4-dev2317 a4963a5aea\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nYu Liang\r\n\r\n### Affiliation:\r\n\r\nThe Pennsylvania State University\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nFATAL Error: 'Operation requires a flat vector but a non-flat vector was encountered' on CASE conditional expression\n### What happens?\r\n\r\nThe latest versions of the DuckDB (latest main: v1.1.4-dev2317 a4963a5aea and released version: v1.1.3 19864453f7) crash when running the following SQL statement:\r\n\r\n```sql\r\nSELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\r\n```\r\n\r\nHere is the stack frame that triggers the Internal Error: \r\n\r\n```\r\n#0  duckdb::InternalException::InternalException (this=0xaaaaea5738f0, msg=\"Operation requires a flat vector but a non-flat vector was encountered\")\r\n    at /home/duckdb/duckdb/src/common/exception.cpp:333\r\n#1  0x0000aaaadb9cd190 in duckdb::FlatVector::VerifyFlatVector (vector=...) at /usr/include/c++/9/ext/new_allocator.h:80\r\n#2  0x0000aaaadc1792b4 in duckdb::FlatVector::Validity (vector=...) at ../../src/include/duckdb/common/types/vector.hpp:361\r\n#3  duckdb::VectorStringToMap::StringToNestedTypeCastLoop (source_data=source_data@entry=0xaaaaea550280, source_mask=..., result=..., result_mask=..., count=count@entry=1, \r\n    parameters=..., sel=sel@entry=0x0) at /home/duckdb/duckdb/src/function/cast/string_cast.cpp:345\r\n#4  0x0000aaaadc216dec in duckdb::StringToNestedTypeCast<duckdb::VectorStringToMap> (count=<optimized out>, parameters=..., result=..., source=...)\r\n    at ../../src/include/duckdb/common/types/vector.hpp:362\r\n#5  duckdb::StringToNestedTypeCast<duckdb::VectorStringToMap> (source=..., result=..., count=<optimized out>, parameters=...)\r\n    at /home/duckdb/duckdb/src/function/cast/string_cast.cpp:440\r\n#6  0x0000aaaadbb54350 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state=state@entry=0xaaaaea54b980, sel=sel@entry=0x0, \r\n    count=count@entry=1, result=...) at /home/duckdb/duckdb/src/execution/expression_executor/execute_cast.cpp:42\r\n#7  0x0000aaaadbc83f48 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state=0xaaaaea54b980, sel=sel@entry=0x0, count=count@entry=1, \r\n    result=...) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:205\r\n#8  0x0000aaaadbb567f4 in duckdb::ExpressionExecutor::Select (this=0xffffe54a2a60, expr=..., state=<optimized out>, sel=0x0, count=1, true_sel=0xaaaaea530380, \r\n    false_sel=0xaaaaea530398) at /usr/include/c++/9/bits/unique_ptr.h:360\r\n#9  0x0000aaaadbc849c0 in duckdb::ExpressionExecutor::Select (false_sel=0xaaaaea530380, true_sel=0xaaaaea530380, count=1, sel=0x0, state=<optimized out>, expr=..., \r\n    this=0xffffe54a2a60) at ../../src/include/duckdb/parser/base_expression.hpp:98\r\n#10 duckdb::ExpressionExecutor::Select (this=this@entry=0xffffe54a2a60, expr=..., state=<optimized out>, sel=sel@entry=0x0, count=count@entry=1, \r\n    true_sel=true_sel@entry=0xaaaaea530380, false_sel=false_sel@entry=0xaaaaea530398) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:231\r\n#11 0x0000aaaadbb57224 in duckdb::ExpressionExecutor::Execute (this=this@entry=0xffffe54a2a60, expr=..., state_p=state_p@entry=0xaaaaea5302d0, sel=sel@entry=0x0, \r\n    count=count@entry=1, result=...) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#12 0x0000aaaadbc83fa8 in duckdb::ExpressionExecutor::Execute (this=0xffffe54a2a60, expr=..., state=0xaaaaea5302d0, sel=sel@entry=0x0, count=1, result=...)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:202\r\n#13 0x0000aaaadbc89518 in duckdb::ExpressionExecutor::ExecuteExpression (this=this@entry=0xffffe54a2a60, expr_idx=expr_idx@entry=0, result=...)\r\n    at ../../src/include/duckdb/common/types/data_chunk.hpp:54\r\n#14 0x0000aaaadbc896bc in duckdb::ExpressionExecutor::ExecuteExpression (this=this@entry=0xffffe54a2a60, result=...)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:96\r\n#15 0x0000aaaadbc89750 in duckdb::ExpressionExecutor::EvaluateScalar (context=..., expr=..., allow_unfoldable=allow_unfoldable@entry=false)\r\n    at /home/duckdb/duckdb/src/execution/expression_executor.cpp:112\r\n#16 0x0000aaaadbc899c8 in duckdb::ExpressionExecutor::TryEvaluateScalar (context=..., expr=..., result=...) at /home/duckdb/duckdb/src/execution/expression_executor.cpp:122\r\n#17 0x0000aaaadbf5f2fc in duckdb::ConstantFoldingRule::Apply (this=0xaaaaea2f66c0, op=..., bindings=..., changes_made=<optimized out>, is_root=<optimized out>)\r\n    at /home/duckdb/duckdb/src/optimizer/rule/constant_folding.cpp:36\r\n#18 0x0000aaaadbf6ec40 in duckdb::ExpressionRewriter::ApplyRules (op=..., rules=..., expr=..., changes_made=@0xffffe54a2da7: false, is_root=is_root@entry=true)\r\n    at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:20\r\n#19 0x0000aaaadbf6f0a8 in duckdb::ExpressionRewriter::VisitExpression (this=0xffffe54a32f8, expression=0xaaaaea52eec0) at /usr/include/c++/9/bits/move.h:74\r\n#20 0x0000aaaadc0657b8 in std::function<void (duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*)>::operator()(duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*) const (__args#0=<optimized out>, this=0xffffe54a2ed8) at /usr/include/c++/9/bits/std_function.h:683\r\n#21 duckdb::LogicalOperatorVisitor::EnumerateExpressions(duckdb::LogicalOperator&, std::function<void (duckdb::unique_ptr<duckdb::Expression, std::default_delete<duckdb::Expression>, true>*)> const&) (op=..., callback=...) at /home/duckdb/duckdb/src/planner/logical_operator_visitor.cpp:175\r\n#22 0x0000aaaadc065b7c in duckdb::LogicalOperatorVisitor::VisitOperatorExpressions (this=this@entry=0xffffe54a32f8, op=...) at /usr/include/c++/9/new:174\r\n#23 0x0000aaaadbf6b8c8 in duckdb::ExpressionRewriter::VisitOperator (this=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:68\r\n#24 0x0000aaaadc07341c in duckdb::LogicalOperatorVisitor::VisitOperatorChildren (op=..., this=0xffffe54a32f8) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#25 duckdb::LogicalOperatorVisitor::VisitOperatorChildren (this=this@entry=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/planner/logical_operator_visitor.cpp:14\r\n#26 0x0000aaaadbf6b788 in duckdb::ExpressionRewriter::VisitOperator (this=0xffffe54a32f8, op=...) at /home/duckdb/duckdb/src/optimizer/expression_rewriter.cpp:60\r\n#27 0x0000aaaadbf6ba34 in duckdb::Optimizer::<lambda()>::operator() (__closure=<optimized out>) at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#28 std::_Function_handler<void(), duckdb::Optimizer::RunBuiltInOptimizers()::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)\r\n    at /usr/include/c++/9/bits/std_function.h:300\r\n#29 0x0000aaaadbf6c624 in std::function<void ()>::operator()() const (this=0xffffe54a31a8) at /usr/include/c++/9/bits/std_function.h:683\r\n#30 duckdb::Optimizer::RunOptimizer(duckdb::OptimizerType, std::function<void ()> const&) (callback=..., type=duckdb::OptimizerType::EXPRESSION_REWRITER, \r\n    this=0xffffe54a32e8) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:80\r\n#31 duckdb::Optimizer::RunOptimizer(duckdb::OptimizerType, std::function<void ()> const&) (this=0xffffe54a32e8, type=duckdb::OptimizerType::EXPRESSION_REWRITER, \r\n    callback=...) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:73\r\n#32 0x0000aaaadbf6dc78 in duckdb::Optimizer::RunBuiltInOptimizers (this=this@entry=0xffffe54a32e8) at /usr/include/c++/9/new:174\r\n#33 0x0000aaaadbf6e6b0 in duckdb::Optimizer::Optimize (this=this@entry=0xffffe54a32e8, plan_p=...) at /home/duckdb/duckdb/src/optimizer/optimizer.cpp:253\r\n#34 0x0000aaaadbd75a50 in duckdb::ClientContext::CreatePreparedStatementInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., values=values@entry=...) at /usr/include/c++/9/bits/move.h:74\r\n#35 0x0000aaaadbd76108 in duckdb::ClientContext::CreatePreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., values=..., \r\n    mode=mode@entry=duckdb::PreparedStatementMode::PREPARE_ONLY) at /usr/include/c++/9/bits/move.h:74\r\n#36 0x0000aaaadbd76550 in duckdb::ClientContext::RebindPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...) at /usr/include/c++/9/bits/unique_ptr.h:360\r\n#37 0x0000aaaadbd777b8 in duckdb::ClientContext::PendingPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...)\r\n    at /home/duckdb/duckdb/src/main/client_context.cpp:544\r\n#38 0x0000aaaadbd7802c in duckdb::ClientContext::PendingStatementOrPreparedStatement (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., prepared=..., parameters=...)\r\n    at /usr/include/c++/9/ext/atomicity.h:96\r\n#39 0x0000aaaadbd79400 in duckdb::ClientContext::PendingStatementOrPreparedStatementInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", statement=..., prepared=..., parameters=...)\r\n    at /usr/include/c++/9/bits/move.h:74\r\n#40 0x0000aaaadbd79724 in duckdb::ClientContext::PendingQueryPreparedInternal (this=this@entry=0xaaaaea3954e0, lock=..., \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...) at /usr/include/c++/9/tuple:918\r\n#41 0x0000aaaadbd79864 in duckdb::ClientContext::PendingQuery (this=this@entry=0xaaaaea3954e0, \r\n    query=\"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", prepared=..., parameters=...)\r\n    at ../../src/include/duckdb/common/unique_ptr.hpp:20\r\n#42 0x0000aaaadbd79a80 in duckdb::PreparedStatement::PendingQuery (this=this@entry=0xaaaaea379bf0, named_values=std::unordered_map with 1 element = {...}, \r\n    allow_stream_result=allow_stream_result@entry=false) at ../../src/include/duckdb/common/shared_ptr_ipp.hpp:204\r\n#43 0x0000aaaadbd79fa4 in duckdb::PreparedStatement::PendingQuery (this=0xaaaaea379bf0, values=..., allow_stream_result=allow_stream_result@entry=false)\r\n    at /home/duckdb/duckdb/src/main/prepared_statement.cpp:94\r\n#44 0x0000aaaadbd7a210 in duckdb::PreparedStatement::Execute (this=<optimized out>, values=..., allow_stream_result=allow_stream_result@entry=false)\r\n    at /home/duckdb/duckdb/src/main/prepared_statement.cpp:81\r\n#45 0x0000aaaadb8a55fc in duckdb_shell_sqlite3_print_duckbox (pStmt=0xaaaaea52ac70, max_rows=40, max_width=0, null_value=0xffffe54a4948 \"NULL\", columnar=0, \r\n    thousand_separator=0 '\\000', decimal_separator=0 '\\000', result_renderer=0xffffe54a4530) at ../../src/include/duckdb/common/unique_ptr.hpp:39\r\n#46 0x0000aaaadb895dfc in duckdb_shell::ShellState::ExecutePreparedStatement (this=0xffffe54a4840, pStmt=0xaaaaea52ac70) at /usr/include/c++/9/bits/basic_string.h:2304\r\n#47 0x0000aaaadb896384 in duckdb_shell::ShellState::ExecuteSQL (this=0xffffe54a4840, \r\n    zSql=0xaaaaea2f2f10 \"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\", pzErrMsg=0xffffe54a4660) at ../../tools/shell/shell.cpp:1788\r\n#48 0x0000aaaadb896c74 in duckdb_shell::ShellState::RunOneSqlLine (this=0xffffe54a4840, \r\n    zSql=0xaaaaea2f2f10 \"SELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\") at ../../tools/shell/shell.cpp:4318\r\n#49 0x0000aaaadb897298 in duckdb_shell::ShellState::ProcessInput (this=0xffffe54a4840) at ../../tools/shell/shell.cpp:4433\r\n#50 0x0000aaaadb87bd9c in main (argc=<optimized out>, argv=0xffffe54a4b28) at ../../tools/shell/shell.cpp:5032\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Clone the DuckDB Git from the official repo.\r\n2. Checkout to the latest main (v1.1.4-dev2317 a4963a5aea).\r\n3. Compile the DuckDB binary by using `CORE_EXTENSIONS='autocomplete;httpfs;icu;parquet;json' GEN=ninja make debug`.\r\n4. Run the compiled DuckDB and input the following SQL:\r\n\r\n```sql\r\nSELECT TRUE WHERE CASE MAP { } WHEN 'abc' [ 'any_string' IN ? : ] THEN TRUE END ;\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 24.04.01 LTS (x86-64 & aarch64)\r\n\r\n### DuckDB Version:\r\n\r\nv1.1.3 19864453f7 and v1.1.4-dev2317 a4963a5aea\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nYu Liang\r\n\r\n### Affiliation:\r\n\r\nThe Pennsylvania State University\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/function/cast/string_cast.cpp b/src/function/cast/string_cast.cpp\nindex 9f8b5ee29811..f3a19c4273bf 100644\n--- a/src/function/cast/string_cast.cpp\n+++ b/src/function/cast/string_cast.cpp\n@@ -342,8 +342,8 @@ bool VectorStringToMap::StringToNestedTypeCastLoop(const string_t *source_data,\n \t\tvector_cast_data.all_converted = false;\n \t}\n \n-\tauto &key_validity = FlatVector::Validity(result_key_child);\n \tif (!vector_cast_data.all_converted) {\n+\t\tauto &key_validity = FlatVector::Validity(result_key_child);\n \t\tfor (idx_t row_idx = 0; row_idx < count; row_idx++) {\n \t\t\tif (!result_mask.RowIsValid(row_idx)) {\n \t\t\t\tcontinue;\n", "instance_id": "duckdb__duckdb-15046", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a fatal error occurs in DuckDB when executing a specific SQL query involving a CASE expression with a MAP type. The statement provides detailed reproduction steps, including the exact SQL query, DuckDB versions affected, OS details, and a comprehensive stack trace. This makes it easy to understand the context and replicate the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or output of the SQL query (i.e., what should happen when the query is executed correctly). Additionally, while the error message and stack trace point to a specific issue with flat vectors, the root cause or intended fix is not discussed in the problem description, leaving some interpretation to the reader. Edge cases or constraints beyond the provided query are also not mentioned. Overall, the statement is valid and clear but lacks some minor details that would make it fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of the code change appears to be relatively small, as shown in the provided diff, which modifies a single line in the `string_cast.cpp` file to address the issue with vector validity checks. This suggests that the fix itself is not extensive. However, understanding the root cause requires a moderate level of expertise in DuckDB's internals, specifically in how vectors (flat vs. non-flat) are handled during type casting operations, as well as familiarity with the expression execution and optimization pipeline, as evidenced by the stack trace. The problem involves technical concepts such as vectorized query processing, type casting, and validity mask handling in a database engine, which are moderately complex for someone unfamiliar with database internals or DuckDB's architecture. The code change does not seem to impact the broader system architecture significantly, and the provided diff suggests a targeted fix rather than a large refactoring. Edge cases and error handling requirements are not extensively detailed in the problem statement, but the nature of the error (a crash on a specific query) implies that careful validation of vector states is necessary, which adds some complexity. Overall, this problem requires understanding multiple concepts and making a precise modification, but it does not appear to involve deep architectural changes or extremely intricate logic, placing it in the medium difficulty range at 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Appender C API does not support generated columns\n### What happens?\n\nInserting using the appender C API does not work:\r\n\r\n* Inserting a NULL or default value does not work, raises errors\r\n* Not inserting anything does not work either - errors with number of column mismatch \n\n### To Reproduce\n\n```cpp\r\n#include \"duckdb.h\"\r\n\r\nint main() {\r\n  duckdb_database db;\r\n  duckdb_connection con;\r\n\r\n  if (duckdb_open(\"db\", &db) == DuckDBError)\r\n    throw;\r\n  if (duckdb_connect(db, &con) == DuckDBError)\r\n    throw;\r\n\r\n  // works\r\n  // if (duckdb_query(con, \"create or replace table tbl (a varchar)\", NULL) == DuckDBError)\r\n  // does not work\r\n  if (duckdb_query(con, \"create or replace table tbl (a varchar, b varchar generated always as (a))\", NULL) == DuckDBError)\r\n    throw;\r\n\r\n  duckdb_appender appender;\r\n  if (duckdb_appender_create(con, NULL, \"tbl\", &appender) == DuckDBError)\r\n    throw;\r\n\r\n  // append row\r\n  duckdb_append_varchar(appender, \"a\");\r\n  // not appending anything at column b does not work\r\n  // duckdb_append_default(appender); // does not work either\r\n  // duckdb_append_varchar(appender, \"b\"); // does not work either\r\n  if (duckdb_appender_end_row(appender) == DuckDBError)\r\n    throw;\r\n\r\n  duckdb_appender_destroy(&appender);\r\n  duckdb_disconnect(&con);\r\n  duckdb_close(&db);\r\n\r\n  return 0;\r\n}\r\n```\n\n### OS:\n\nUbuntu 22.04.4 LTS (x86_64, WSL)\n\n### DuckDB Version:\n\nv1.1.1 af39bd0dcf\n\n### DuckDB Client:\n\nv1.1.1 af39bd0dcf\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nGerman Gambon\n\n### Affiliation:\n\nN/A\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/include/duckdb/main/table_description.hpp b/src/include/duckdb/main/table_description.hpp\nindex 2a35b4f16691..151592f49355 100644\n--- a/src/include/duckdb/main/table_description.hpp\n+++ b/src/include/duckdb/main/table_description.hpp\n@@ -13,12 +13,26 @@\n namespace duckdb {\n \n struct TableDescription {\n+public:\n \t//! The schema of the table\n \tstring schema;\n \t//! The table name of the table\n \tstring table;\n \t//! The columns of the table\n \tvector<ColumnDefinition> columns;\n+\n+public:\n+\tidx_t PhysicalColumnCount() const {\n+\t\tidx_t count = 0;\n+\t\tfor (auto &column : columns) {\n+\t\t\tif (column.Generated()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tcount++;\n+\t\t}\n+\t\tD_ASSERT(count != 0);\n+\t\treturn count;\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/main/appender.cpp b/src/main/appender.cpp\nindex ae3608524149..33ef17bf444c 100644\n--- a/src/main/appender.cpp\n+++ b/src/main/appender.cpp\n@@ -62,6 +62,9 @@ Appender::Appender(Connection &con, const string &schema_name, const string &tab\n \t}\n \tvector<optional_ptr<const ParsedExpression>> defaults;\n \tfor (auto &column : description->columns) {\n+\t\tif (column.Generated()) {\n+\t\t\tcontinue;\n+\t\t}\n \t\ttypes.push_back(column.Type());\n \t\tdefaults.push_back(column.HasDefaultValue() ? &column.DefaultValue() : nullptr);\n \t}\ndiff --git a/src/main/client_context.cpp b/src/main/client_context.cpp\nindex 467f28dacde1..90c029ab498b 100644\n--- a/src/main/client_context.cpp\n+++ b/src/main/client_context.cpp\n@@ -1132,13 +1132,19 @@ void ClientContext::Append(TableDescription &description, ColumnDataCollection &\n \t\tauto &table_entry =\n \t\t    Catalog::GetEntry<TableCatalogEntry>(*this, INVALID_CATALOG, description.schema, description.table);\n \t\t// verify that the table columns and types match up\n-\t\tif (description.columns.size() != table_entry.GetColumns().PhysicalColumnCount()) {\n+\t\tif (description.PhysicalColumnCount() != table_entry.GetColumns().PhysicalColumnCount()) {\n \t\t\tthrow InvalidInputException(\"Failed to append: table entry has different number of columns!\");\n \t\t}\n+\t\tidx_t table_entry_col_idx = 0;\n \t\tfor (idx_t i = 0; i < description.columns.size(); i++) {\n-\t\t\tif (description.columns[i].Type() != table_entry.GetColumns().GetColumn(PhysicalIndex(i)).Type()) {\n+\t\t\tauto &column = description.columns[i];\n+\t\t\tif (column.Generated()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tif (column.Type() != table_entry.GetColumns().GetColumn(PhysicalIndex(table_entry_col_idx)).Type()) {\n \t\t\t\tthrow InvalidInputException(\"Failed to append: table entry has different number of columns!\");\n \t\t\t}\n+\t\t\ttable_entry_col_idx++;\n \t\t}\n \t\tauto binder = Binder::CreateBinder(*this);\n \t\tauto bound_constraints = binder->BindConstraints(table_entry);\n", "instance_id": "duckdb__duckdb-14346", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the DuckDB appender C API not supporting generated columns. It provides a reproducible code snippet that demonstrates the problem, including specific behaviors (e.g., errors when inserting NULL or default values, column mismatch errors). The OS, DuckDB version, and other relevant details are also included, which aids in understanding the context. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior for generated columns (e.g., should they be automatically handled by the appender, or should there be a specific API to handle them?). Additionally, edge cases or specific constraints around generated columns are not mentioned, which could lead to some uncertainty during implementation. Overall, the statement is valid and clear but lacks minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves modifications across multiple files (table_description.hpp, appender.cpp, client_context.cpp) in the DuckDB codebase, requiring an understanding of how these components interact. The changes are not trivial; they involve adding logic to handle generated columns by filtering them out during column counting and type checking, which requires a grasp of the internal representation of table schemas and the appender's behavior. Second, the technical concepts involved include familiarity with C++ (specifically, working with structs and member functions), understanding of database schema management (e.g., distinguishing between physical and generated columns), and the DuckDB-specific appender API. While these concepts are not overly complex for an experienced developer, they do require domain-specific knowledge of database internals. Third, the problem does not explicitly mention edge cases, but the code changes suggest potential issues like ensuring correct column indexing when skipping generated columns, which adds moderate complexity to error handling. Finally, the impact on the system's architecture is minimal, as the changes are localized to specific components without altering core functionality. Given these considerations\u2014a moderate scope of changes, a few technical concepts, and some implicit edge case handling\u2014I assign a difficulty score of 0.45, placing it on the lower end of the medium range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add tests for sha1 function\nThis is a followup to #13020 that adds some basic tests for the new `sha1` function, modeled after the existing tests for `sha512`.\nCatalog Errors only offer one suggestion when multiple suggestions are returned with the same score\n### What happens?\n\nConsider the case where there are multiple schemas with the same matching score for a Catalog error.\r\n\r\n```sql\r\ncreate schema a;\r\ncreate schema b;\r\ncreate table a.foo(name text);\r\ncreate table b.foo(name text);\r\nselect * from foo;\r\n```\r\n\r\n```\r\nCatalog Error: Table with name foo does not exist!\r\nDid you mean \"b.foo\"?\r\nLINE 1: select * from foo;\r\n```\r\n\r\nThere is an equally qualified match in schema `a`, but Catalog Error just presents the last match.\r\n\r\nHere is the relevant snippet of code.\r\n\r\nhttps://github.com/duckdb/duckdb/blob/45787e5f9f8bdb9dce97890c1ac7a7eb2dc3a49f/src/catalog/catalog.cpp#L639-L654\r\n\r\nI'd suggest that the code should be changed to present all suggestions that have the same score so that the a user that has tables in multiple schemas with the same name can make the appropriate choice.\r\n\r\nThanks,\r\n\r\nRusty\n\n### To Reproduce\n\n```sql\r\ncreate schema a;\r\ncreate schema b;\r\ncreate table a.foo(name text);\r\ncreate table b.foo(name text);\r\nselect * from foo;\r\n```\r\n\n\n### OS:\n\nMac OS X\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nRusty Conover\n\n### Affiliation:\n\nself\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/catalog/catalog.cpp b/src/catalog/catalog.cpp\nindex 9bbcb1f4825f..1adb05c5953a 100644\n--- a/src/catalog/catalog.cpp\n+++ b/src/catalog/catalog.cpp\n@@ -367,9 +367,10 @@ SchemaCatalogEntry &Catalog::GetSchema(CatalogTransaction transaction, const str\n //===--------------------------------------------------------------------===//\n // Lookup\n //===--------------------------------------------------------------------===//\n-SimilarCatalogEntry Catalog::SimilarEntryInSchemas(ClientContext &context, const string &entry_name, CatalogType type,\n-                                                   const reference_set_t<SchemaCatalogEntry> &schemas) {\n-\tSimilarCatalogEntry result;\n+vector<SimilarCatalogEntry> Catalog::SimilarEntriesInSchemas(ClientContext &context, const string &entry_name,\n+                                                             CatalogType type,\n+                                                             const reference_set_t<SchemaCatalogEntry> &schemas) {\n+\tvector<SimilarCatalogEntry> results;\n \tfor (auto schema_ref : schemas) {\n \t\tauto &schema = schema_ref.get();\n \t\tauto transaction = schema.catalog.GetCatalogTransaction(context);\n@@ -378,12 +379,16 @@ SimilarCatalogEntry Catalog::SimilarEntryInSchemas(ClientContext &context, const\n \t\t\t// no similar entry found\n \t\t\tcontinue;\n \t\t}\n-\t\tif (!result.Found() || result.score < entry.score) {\n-\t\t\tresult = entry;\n-\t\t\tresult.schema = &schema;\n+\t\tif (results.empty() || results[0].score <= entry.score) {\n+\t\t\tif (!results.empty() && results[0].score < entry.score) {\n+\t\t\t\tresults.clear();\n+\t\t\t}\n+\n+\t\t\tresults.push_back(entry);\n+\t\t\tresults.back().schema = &schema;\n \t\t}\n \t}\n-\treturn result;\n+\treturn results;\n }\n \n vector<CatalogSearchEntry> GetCatalogEntries(CatalogEntryRetriever &retriever, const string &catalog,\n@@ -588,7 +593,7 @@ CatalogException Catalog::CreateMissingEntryException(CatalogEntryRetriever &ret\n                                                       const reference_set_t<SchemaCatalogEntry> &schemas,\n                                                       QueryErrorContext error_context) {\n \tauto &context = retriever.GetContext();\n-\tauto entry = SimilarEntryInSchemas(context, entry_name, type, schemas);\n+\tauto entries = SimilarEntriesInSchemas(context, entry_name, type, schemas);\n \n \treference_set_t<SchemaCatalogEntry> unseen_schemas;\n \tauto &db_manager = DatabaseManager::Get(context);\n@@ -668,20 +673,36 @@ CatalogException Catalog::CreateMissingEntryException(CatalogEntryRetriever &ret\n \t// entries in other schemas get a penalty\n \t// however, if there is an exact match in another schema, we will always show it\n \tstatic constexpr const double UNSEEN_PENALTY = 0.2;\n-\tauto unseen_entry = SimilarEntryInSchemas(context, entry_name, type, unseen_schemas);\n-\tstring did_you_mean;\n-\tif (unseen_entry.Found() && (unseen_entry.score == 1.0 || unseen_entry.score - UNSEEN_PENALTY > entry.score)) {\n+\tauto unseen_entries = SimilarEntriesInSchemas(context, entry_name, type, unseen_schemas);\n+\tvector<string> suggestions;\n+\tif (!unseen_entries.empty() && (unseen_entries[0].score == 1.0 || unseen_entries[0].score - UNSEEN_PENALTY >\n+\t                                                                      (entries.empty() ? 0.0 : entries[0].score))) {\n \t\t// the closest matching entry requires qualification as it is not in the default search path\n \t\t// check how to minimally qualify this entry\n-\t\tauto catalog_name = unseen_entry.schema->catalog.GetName();\n-\t\tauto schema_name = unseen_entry.schema->name;\n-\t\tbool qualify_database;\n-\t\tbool qualify_schema;\n-\t\tFindMinimalQualification(retriever, catalog_name, schema_name, qualify_database, qualify_schema);\n-\t\tdid_you_mean = unseen_entry.GetQualifiedName(qualify_database, qualify_schema);\n-\t} else if (entry.Found()) {\n-\t\tdid_you_mean = entry.name;\n+\t\tfor (auto &unseen_entry : unseen_entries) {\n+\t\t\tauto catalog_name = unseen_entry.schema->catalog.GetName();\n+\t\t\tauto schema_name = unseen_entry.schema->name;\n+\t\t\tbool qualify_database;\n+\t\t\tbool qualify_schema;\n+\t\t\tFindMinimalQualification(retriever, catalog_name, schema_name, qualify_database, qualify_schema);\n+\t\t\tsuggestions.push_back(unseen_entry.GetQualifiedName(qualify_database, qualify_schema));\n+\t\t}\n+\t} else if (!entries.empty()) {\n+\t\tfor (auto &entry : entries) {\n+\t\t\tsuggestions.push_back(entry.name);\n+\t\t}\n+\t}\n+\n+\tstring did_you_mean;\n+\tstd::sort(suggestions.begin(), suggestions.end());\n+\tif (suggestions.size() > 2) {\n+\t\tauto last = suggestions.back();\n+\t\tsuggestions.pop_back();\n+\t\tdid_you_mean = StringUtil::Join(suggestions, \", \") + \", or \" + last;\n+\t} else {\n+\t\tdid_you_mean = StringUtil::Join(suggestions, \" or \");\n \t}\n+\n \treturn CatalogException::MissingEntry(type, entry_name, did_you_mean, error_context);\n }\n \ndiff --git a/src/include/duckdb/catalog/catalog.hpp b/src/include/duckdb/catalog/catalog.hpp\nindex 2bf7fad6e3c2..5460199c07c1 100644\n--- a/src/include/duckdb/catalog/catalog.hpp\n+++ b/src/include/duckdb/catalog/catalog.hpp\n@@ -370,8 +370,9 @@ class Catalog {\n \t                                                    QueryErrorContext error_context);\n \n \t//! Return the close entry name, the distance and the belonging schema.\n-\tstatic SimilarCatalogEntry SimilarEntryInSchemas(ClientContext &context, const string &entry_name, CatalogType type,\n-\t                                                 const reference_set_t<SchemaCatalogEntry> &schemas);\n+\tstatic vector<SimilarCatalogEntry> SimilarEntriesInSchemas(ClientContext &context, const string &entry_name,\n+\t                                                           CatalogType type,\n+\t                                                           const reference_set_t<SchemaCatalogEntry> &schemas);\n \n \tvirtual void DropSchema(ClientContext &context, DropInfo &info) = 0;\n \n", "instance_id": "duckdb__duckdb-14048", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue and the desired outcome. It identifies a specific problem with the DuckDB catalog error reporting, where only one suggestion is shown despite multiple schemas having equally qualified matches for a table name. The statement includes a reproducible SQL example, relevant code snippet, and a suggestion for improvement (showing all suggestions with the same score). However, there are minor ambiguities: the problem statement does not explicitly define the expected format or behavior for displaying multiple suggestions (e.g., how many suggestions to show, how to format them), and it lacks discussion of potential edge cases (e.g., what if there are dozens of matching suggestions?). Additionally, the mention of \"adding tests for sha1 function\" in the title seems unrelated to the main content, causing minor confusion. Overall, the statement is valid and clear but misses some finer details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is moderate, involving modifications to a few key functions in the catalog system (specifically in `catalog.cpp` and `catalog.hpp`). The changes affect how similar entries are collected and reported, requiring updates to function signatures (from returning a single `SimilarCatalogEntry` to a vector of entries) and logic for handling multiple suggestions. Second, the technical concepts involved include understanding C++ data structures (e.g., vectors, sorting), string manipulation, and the DuckDB catalog system\u2019s internal logic for schema resolution and error reporting, which requires some domain-specific knowledge of database systems. Third, the problem introduces moderate complexity in handling edge cases, such as formatting multiple suggestions (the code handles this by sorting and joining strings with commas or \"or\") and deciding when to show suggestions from unseen schemas, though these are not extensively complex. Finally, the changes do not significantly impact the broader system architecture but do require a good understanding of the local codebase context (e.g., how `SimilarCatalogEntry` and schema resolution work). Given these factors\u2014a moderate scope of changes, a few technical concepts, and manageable edge case handling\u2014I assign a difficulty score of 0.45, placing it on the lower end of the medium range as it does not involve deep architectural changes or highly intricate logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Random generator gives repeated results \n### What happens?\r\n\r\nrandom() gives repeated results even when run only 100k times and single-threaded.\r\n\r\nIn comparison, numpy gives unique results even when asked for 100M random numbers.\r\n\r\n### To Reproduce\r\n\r\n```sql\r\nSET THREADS to 1;\r\nSELECT count() FROM (SELECT DISTINCT random() AS a FROM range(100_000));\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        99998 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n### OS:\r\n\r\nUbuntu22.04\r\n\r\n### DuckDB Version:\r\n\r\n1.1.0\r\n\r\n### DuckDB Client:\r\n\r\nPython3.9\r\n\r\n### Hardware:\r\n\r\ni5, x64\r\n\r\n### Full Name:\r\n\r\nSoeren Wolfers\r\n\r\n### Affiliation:\r\n\r\nG-Research\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nNot applicable - the reproduction does not require a data set\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nRandom generator gives repeated results \n### What happens?\r\n\r\nrandom() gives repeated results even when run only 100k times and single-threaded.\r\n\r\nIn comparison, numpy gives unique results even when asked for 100M random numbers.\r\n\r\n### To Reproduce\r\n\r\n```sql\r\nSET THREADS to 1;\r\nSELECT count() FROM (SELECT DISTINCT random() AS a FROM range(100_000));\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        99998 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n### OS:\r\n\r\nUbuntu22.04\r\n\r\n### DuckDB Version:\r\n\r\n1.1.0\r\n\r\n### DuckDB Client:\r\n\r\nPython3.9\r\n\r\n### Hardware:\r\n\r\ni5, x64\r\n\r\n### Full Name:\r\n\r\nSoeren Wolfers\r\n\r\n### Affiliation:\r\n\r\nG-Research\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nNot applicable - the reproduction does not require a data set\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/scripts/apply_extension_patches.py b/scripts/apply_extension_patches.py\nindex c76b900ae20f..8f4c7c263eb1 100644\n--- a/scripts/apply_extension_patches.py\n+++ b/scripts/apply_extension_patches.py\n@@ -13,7 +13,7 @@\n \n \n def raise_error(error_msg):\n-    sys.stderr.write(error_message + '\\n')\n+    sys.stderr.write(error_msg + '\\n')\n     sys.exit(1)\n \n \ndiff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp\nindex e51f71001024..2dc0df1894d0 100644\n--- a/src/common/random_engine.cpp\n+++ b/src/common/random_engine.cpp\n@@ -29,12 +29,18 @@ double RandomEngine::NextRandom(double min, double max) {\n }\n \n double RandomEngine::NextRandom() {\n-\treturn std::ldexp(random_state->pcg(), -32);\n+\tauto uint64 = NextRandomInteger64();\n+\treturn std::ldexp(uint64, -64);\n }\n+\n uint32_t RandomEngine::NextRandomInteger() {\n \treturn random_state->pcg();\n }\n \n+uint64_t RandomEngine::NextRandomInteger64() {\n+\treturn (static_cast<uint64_t>(NextRandomInteger()) << UINT64_C(32)) | static_cast<uint64_t>(NextRandomInteger());\n+}\n+\n uint32_t RandomEngine::NextRandomInteger(uint32_t min, uint32_t max) {\n \treturn min + static_cast<uint32_t>(NextRandom() * double(max - min));\n }\ndiff --git a/src/include/duckdb/common/random_engine.hpp b/src/include/duckdb/common/random_engine.hpp\nindex 224a5a20322d..ab91f3684a04 100644\n--- a/src/include/duckdb/common/random_engine.hpp\n+++ b/src/include/duckdb/common/random_engine.hpp\n@@ -30,6 +30,7 @@ struct RandomEngine {\n \tdouble NextRandom();\n \tuint32_t NextRandomInteger();\n \tuint32_t NextRandomInteger(uint32_t min, uint32_t max);\n+\tuint64_t NextRandomInteger64();\n \n \tvoid SetSeed(uint32_t seed);\n \n", "instance_id": "duckdb__duckdb-13920", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `random()` function in DuckDB produces repeated results when called 100,000 times in a single-threaded environment, which is undesirable compared to numpy's behavior with much larger sample sizes. The reproduction steps are provided with a clear SQL query and output, demonstrating the issue effectively. Additionally, relevant environment details (OS, DuckDB version, hardware, etc.) are included, which aids in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior (e.g., should `random()` guarantee uniqueness for a certain number of calls, or just improve randomness distribution?). It also lacks mention of specific constraints or edge cases to consider (e.g., behavior under multi-threading beyond the provided setting, or performance expectations). These omissions prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively localized, primarily affecting the `RandomEngine` class in DuckDB. Modifications are made to two files (`random_engine.cpp` and `random_engine.hpp`) to improve the randomness by generating a 64-bit random number instead of a 32-bit one for the `NextRandom()` function. The changes are straightforward, involving the addition of a new method (`NextRandomInteger64()`) and updating the random number generation logic. There is no indication of widespread impact on the system's architecture or interactions with other modules, keeping the scope limited.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of random number generation, specifically the use of a PCG (Permuted Congruential Generator) algorithm as implemented in DuckDB. Familiarity with bit manipulation (e.g., shifting and combining 32-bit integers into a 64-bit integer) and the `std::ldexp` function for converting integers to floating-point numbers is necessary. These concepts are not overly complex for a developer with moderate experience in C++ and numerical programming, though they do require some domain knowledge of random number generation quality.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the single-threaded scenario with 100,000 calls. The code changes do not introduce new error handling logic, focusing solely on improving randomness by increasing the bit width of the generated numbers. However, implicit considerations might include ensuring the new 64-bit approach does not degrade performance or introduce other statistical biases in randomness, though these are not addressed in the provided diff.\n\n4. **Overall Complexity**: The task involves a simple bug fix by enhancing the random number generation mechanism. It does not require deep architectural changes, advanced algorithms, or extensive refactoring. The primary challenge lies in understanding the existing random number generator's limitations and applying a straightforward fix, which aligns with an \"Easy\" difficulty level.\n\nGiven these points, a score of 0.35 reflects a problem that is slightly more involved than the simplest fixes (e.g., changing a constant) due to the need to understand random number generation and bit manipulation, but it remains within the realm of easy modifications with limited scope and complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ungraceful Handling of Apache Arrow `ArrowNotImplementedError` with `timestamp` Datatype\n### What happens?\r\n\r\nExecuting a query against an Apache Arrow table from LanceDB that includes a column of datatype `timestamp[ns]`:\r\n- FAILS if the query contains a `WHERE` clause;\r\n- SUCCEEDS if the query does _not_ contain a `WHERE` clause.\r\n\r\nSpecifically, this behavior seems to be present when creating a LanceDB table from a `pandas.DataFrame`.\r\n\r\n### To Reproduce\r\n\r\n## Minimum Reproducible Example\r\n\r\n```python\r\nimport lancedb\r\nimport duckdb\r\nimport pandas as pd\r\n\r\n# Connect to a fresh database\r\ndb = lancedb.connect(\"./test_db\")\r\n\r\n# Create a LanceDB table (uses Apache Arrow)\r\ndata = pd.DataFrame({\r\n    \"corpus\": [\"day\", \"night\"],\r\n    \"date\": [pd.Timestamp(1708951076175900000, unit=\"ns\")] * 2\r\n})\r\ntable = db.create_table('test-table', data=data, exist_ok=True, mode=\"overwrite\")\r\n\r\nprint(\"\u2728 Table Schema:\\n\", table.schema, \"\\n\\n\")\r\n\r\n# Convert the \"Table\" to a \"dataset\" which is an Apache Arrow\r\ndataset = table.to_lance() # type: ignore (it works :shrug:)\r\n\r\n# Succeeds\r\nprint(\"\u2705 \", duckdb.execute('SELECT * FROM dataset').arrow(), \"\\n\\n\")\r\n\r\nargs = {\"corpus\": \"day\"}\r\n\r\n# Fails\r\nprint(duckdb.execute('SELECT * FROM dataset WHERE corpus LIKE $corpus', args).arrow())\r\n```\r\n\r\n## Output:\r\n```bash\r\n\u2728 Table Schema:\r\ncorpus: string\r\ndate: timestamp[ns] \r\n\r\n\r\n\u2705  pyarrow.Table\r\ncorpus: string\r\ndate: timestamp[ns]\r\n----\r\ncorpus: [[\"day\",\"night\"]]\r\ndate: [[2024-02-26 12:37:56.175900000,2024-02-26 12:37:56.175900000]] \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<redacted>\", line 39, in <module>\r\n    test()\r\n  File \"<redacted>\", line 34, in test\r\n    print(duckdb.execute('SELECT * FROM dataset WHERE corpus LIKE $corpus', args).arrow())\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nduckdb.duckdb.InvalidInputException: Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\nError: Invalid Error: ArrowNotImplementedError: conversion to substrait::Type from timestamp[ns]\r\n\r\nAt:\r\n  pyarrow/error.pxi(91): pyarrow.lib.check_status\r\n  ```\r\n\r\n> [!Important]\r\n> It turns out that if you replace the `data` variable with a list of dicts instead of a DataFrame, there are no errors. Curious!\r\n> ```python\r\n>data = [\r\n>    {\"corpus\": \"day\", \"date\": pd.Timestamp(1708951076175900000, unit=\"ns\")},\r\n>    {\"corpus\": \"night\", \"date\": pd.Timestamp(1708951076175900000, unit=\"ns\")},\r\n>]\r\n>```\r\n\r\n### OS:\r\n\r\nLinux (Ubuntu x64)\r\n\r\n### DuckDB Version:\r\n\r\n0.10.1\r\n\r\n### DuckDB Client:\r\n\r\nPython (Python 3.11)\r\n\r\n### Full Name:\r\n\r\nJohn\r\n\r\n### Affiliation:\r\n\r\nN/A\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nQuery on Arrow views fail with specific order of filters\n### What happens?\n\nWhen executing a valid SQL query against an arrow registered table, a weird error pops up: \r\n\r\n```Invalid Input Error: Attempting to execute an unsuccessful or closed pending query result\r\nError: Not implemented Error: DatetimeType not recognized in ConvertTimestampUnit: 4```\r\n\r\nChanging the order of the filters or query the arrow table as a variable directly does not result in an error - given the GH format limitation we uploaded a reproducible code with all the options and the original arrow file from the taxi dataset we use.\r\n\r\nPlease note that in our system we rely on explicit registering the python objects as views so this bug is actually blocking us.\n\n### To Reproduce\n\n* download the arrow file [here](https://drive.google.com/file/d/1svr_up221yTLWTWzjWyOkwDuBbfZWJ8G/view?usp=sharing)\r\n* install a virtual env with the\r\n[requirements.txt](https://github.com/user-attachments/files/15568772/requirements.txt)\r\n* run the following code\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.ipc as ipc\r\nimport duckdb\r\nimport pyarrow.compute as pc\r\nfrom datetime import datetime\r\n\r\n\r\n# Read the table from the Arrow file\r\n\r\ndef read_arrow_file(file_path):\r\n    with pa.memory_map(file_path, 'r') as source:\r\n        reader = ipc.RecordBatchFileReader(source)\r\n        return reader.read_all()\r\n\r\ntaxi_fhvhv_arrow = read_arrow_file('arrow_table')\r\nprint(\"Arrow table loaded\")\r\nprint(taxi_fhvhv_arrow.schema, taxi_fhvhv_arrow.num_rows)\r\n\r\nprint('\\n---------- Pyarrow version  ----------\\n')\r\n\r\n# query with pyarrow directly to have an arrow only baseline as a gold standard\r\nexpr = (pc.field(\"pickup_datetime\") >= pa.scalar(datetime(2023, 1, 1, 5, 0, 0), type=pa.timestamp('us', tz='America/New_York'))) & \\\r\n    (pc.field(\"pickup_datetime\") < pa.scalar(datetime(2023, 1, 1, 15, 0, 0), type=pa.timestamp('us', tz='America/New_York'))) & \\\r\n    (pc.field(\"PULocationID\") == 244)\r\ntable = taxi_fhvhv_arrow.filter(expr)\r\nprint(table.schema, table.num_rows)\r\n\r\n# double check the number of rows\r\n# table = taxi_fhvhv_arrow.filter(pc.field(\"PULocationID\") == 244)\r\n# print(table.schema, table.num_rows)\r\n\r\nprint('\\n----------\\n')\r\n\r\n# instantiate a DuckDB connection\r\ncon = duckdb.connect(database=':memory:')\r\ncon.execute(\"SET TimeZone='UTC';\")\r\n\r\ntry:\r\n    # query with replacement scan\r\n    rows = con.execute(\"SELECT PULocationID, pickup_datetime FROM taxi_fhvhv_arrow WHERE pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-01-01T10:00:00-05:00' AND PULocationID = 244\").arrow()\r\n    print(rows.schema, rows.num_rows)\r\nexcept Exception as e:\r\n    print('!!!! Query error !!!!')\r\n    print(e)\r\n\r\nprint('\\n----------\\n')\r\n\r\ntry:\r\n    # query with register view\r\n    con.register('taxi_fhvhv', taxi_fhvhv_arrow)\r\n    rows = con.execute(\"SELECT PULocationID, pickup_datetime FROM taxi_fhvhv WHERE PULocationID = 244 AND pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-01-01T10:00:00-05:00'\").arrow()\r\n    print(rows.schema, rows.num_rows)\r\nexcept Exception as e:\r\n    print('!!!! Query error !!!!')\r\n    print(e)\r\n\r\nprint('\\n----------\\n')\r\n    \r\ntry:\r\n    # query with register view but different filters\r\n    con.register('taxi_fhvhv', taxi_fhvhv_arrow)\r\n    rows = con.execute(\"SELECT PULocationID, pickup_datetime FROM taxi_fhvhv WHERE pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-01-01T10:00:00-05:00' AND PULocationID = 244\").arrow()\r\n    print(rows.schema, rows.num_rows)\r\nexcept Exception as e:\r\n    print('!!!! Query error !!!!')\r\n    print(e)\r\n    \r\nprint('\\n----------\\n')\r\n    \r\ntry:\r\n    # query with register view but only the equality filter\r\n    con.register('taxi_fhvhv', taxi_fhvhv_arrow)\r\n    rows = con.execute(\"SELECT PULocationID, pickup_datetime FROM taxi_fhvhv WHERE PULocationID = 244\").arrow()\r\n    print(rows.schema, rows.num_rows)\r\nexcept Exception as e:\r\n    print('!!!! Query error !!!!')\r\n    print(e)\r\n    \r\nprint('\\n----------\\n')\r\n    \r\ntry:\r\n    # query with register view but only the datetime filter\r\n    con.register('taxi_fhvhv', taxi_fhvhv_arrow)\r\n    rows = con.execute(\"SELECT PULocationID, pickup_datetime FROM taxi_fhvhv WHERE pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-01-01T10:00:00-05:00'\").arrow()\r\n    print(rows.schema, rows.num_rows)\r\nexcept Exception as e:\r\n    print('!!!! Query error !!!!')\r\n    print(e)\r\n```\r\n\r\nYou will notice that the order in which filters are used in the query will produce (or not) an error - we also have a pure arrow implementation to begin with to make sure there is no corruption etc. In particular, this `pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-01-01T10:00:00-05:00' AND PULocationID = 244` generates an error but  moving the last condition first won't.\r\n\n\n### OS:\n\nMac\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nJacopo Tagliabue\n\n### Affiliation:\n\nBauplan\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nindex 7becadea3205..8a78927aeea2 100644\n--- a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -391,16 +391,17 @@ py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &f\n \tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\n-\tauto &arrow_type = *arrow_table.GetColumns().at(filter_to_col.at(it->first));\n+\tauto arrow_type = &arrow_table.GetColumns().at(filter_to_col.at(it->first));\n \n \tvector<string> column_ref;\n \tcolumn_ref.push_back(columns[it->first]);\n-\tpy::object expression = TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, arrow_type);\n+\tpy::object expression = TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, **arrow_type);\n \twhile (it != filters_map->end()) {\n+\t\tarrow_type = &arrow_table.GetColumns().at(filter_to_col.at(it->first));\n \t\tcolumn_ref.clear();\n \t\tcolumn_ref.push_back(columns[it->first]);\n \t\tpy::object child_expression =\n-\t\t    TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, arrow_type);\n+\t\t    TransformFilterRecursive(it->second.get(), column_ref, config.time_zone, **arrow_type);\n \t\texpression = expression.attr(\"__and__\")(child_expression);\n \t\tit++;\n \t}\n", "instance_id": "duckdb__duckdb-13593", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear and provides detailed reproduction steps, including code examples, environment details, and specific error messages for two related issues involving Apache Arrow and DuckDB with timestamp data types. The issues are well-documented with minimal reproducible examples, which helps in understanding the problem context. The first issue describes a failure when querying a LanceDB table with a `WHERE` clause on a `timestamp[ns]` column, and the second issue highlights a failure in DuckDB when querying an Arrow table with specific filter orders. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or output for the failing cases beyond \"it should work,\" and edge cases or constraints (e.g., specific timestamp formats or timezone handling) are not fully specified. Additionally, while the reproduction steps are thorough, the root cause or hypothesis for the issue (e.g., specific Arrow or DuckDB limitations) is not discussed, which could aid in clarity for developers tackling the fix.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.75, placing it in the \"Hard\" category due to several factors:\n\n1. **Clarity and Complexity of Problem Description**: While the problem is mostly clear, the underlying issue involves subtle interactions between DuckDB, Apache Arrow, and timestamp data types, which are not immediately obvious. Understanding why the query fails with specific filter orders or with a `WHERE` clause requires deep knowledge of how DuckDB translates SQL queries to Arrow operations and how Arrow handles timestamp types.\n\n2. **Scope and Depth of Code Changes**: The provided code change in `arrow_array_stream.cpp` is minimal (a few lines adjusting pointer dereferencing), but this is likely only a partial fix or a specific instance of the problem. The actual resolution may require broader changes across the DuckDB codebase, particularly in how filters are transformed into Arrow expressions. It may involve multiple modules (e.g., query parsing, Arrow integration) and understanding the interaction between DuckDB's internal representation and Arrow's type system. The change could impact the system's query execution architecture if it requires altering how timestamp types are handled globally.\n\n3. **Number of Technical Concepts**: Solving this issue demands familiarity with several advanced concepts: (a) Apache Arrow's type system and its timestamp handling (including timezone and unit conversions), (b) DuckDB's query execution and filter transformation logic, (c) C++ memory management and pointer handling (as seen in the code diff), and (d) integration between Python and native code via pybind11. Additionally, domain-specific knowledge of database query optimization and data type compatibility is necessary. These concepts are moderately to highly complex, especially for developers not already familiar with DuckDB or Arrow internals.\n\n4. **Edge Cases and Error Handling**: The problem statement highlights specific failing scenarios (e.g., filter order, presence of `WHERE` clause), which are edge cases tied to timestamp data types and query structure. The fix must handle these cases without breaking other query patterns or data types, which adds complexity. Error handling logic in DuckDB may need to be updated to gracefully manage unsupported Arrow type conversions or to provide better diagnostics, further increasing the difficulty.\n\nOverall, this problem requires a deep understanding of the DuckDB and Arrow ecosystems, careful handling of data type conversions, and potentially significant modifications to ensure robustness across various query patterns. While not at the extreme end of difficulty (e.g., redesigning a core system), it is challenging enough to warrant a score of 0.75 due to the specialized knowledge and potential for subtle bugs in the fix.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Query unnesting with grouping sets results in wrong output\n### What happens?\n\nThe following two queries should return the same result:\r\n```sql\r\nselect x, a, b\r\nfrom\r\n(values (1), (2)) t2(x),\r\nlateral (select count(*), count(a) from (select 1, 2 where 1 = x) t(a, b) group by grouping sets ((), (b), (a, b))) t3(a, b);\r\n\r\nselect x, a, b\r\nfrom\r\n(\r\nselect 1, count(*), count(a) from (select 1, 2 where 1 = 1) t(a, b) group by grouping sets ((), (b), (a, b))\r\nunion all\r\nselect 2, count(*), count(a) from (select 1, 2 where 1 = 2) t(a, b) group by grouping sets ((), (b), (a, b))\r\n) t3(x, a, b);\r\n```\r\n\r\nThe correct result for both queries (also returned by Postgres) is:\r\n```\r\nx,a,b\r\n1,1,1\r\n1,1,1\r\n1,1,1\r\n2,0,0\r\n```\r\n\r\nHowever, for the first query, duckdb returns:\r\n```\r\nx,a,b\r\n1,1,1\r\n1,1,1\r\n1,1,1\r\n```\n\n### To Reproduce\n\nThe queries require no database, thus DuckDB CLI can be used to reproduce.\r\nI can also reproduce on DuckDB WASM.\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.0\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nAltan Birler\n\n### Affiliation:\n\nTUM\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\nQuery unnesting with grouping sets results in wrong output\n### What happens?\n\nThe following two queries should return the same result:\r\n```sql\r\nselect x, a, b\r\nfrom\r\n(values (1), (2)) t2(x),\r\nlateral (select count(*), count(a) from (select 1, 2 where 1 = x) t(a, b) group by grouping sets ((), (b), (a, b))) t3(a, b);\r\n\r\nselect x, a, b\r\nfrom\r\n(\r\nselect 1, count(*), count(a) from (select 1, 2 where 1 = 1) t(a, b) group by grouping sets ((), (b), (a, b))\r\nunion all\r\nselect 2, count(*), count(a) from (select 1, 2 where 1 = 2) t(a, b) group by grouping sets ((), (b), (a, b))\r\n) t3(x, a, b);\r\n```\r\n\r\nThe correct result for both queries (also returned by Postgres) is:\r\n```\r\nx,a,b\r\n1,1,1\r\n1,1,1\r\n1,1,1\r\n2,0,0\r\n```\r\n\r\nHowever, for the first query, duckdb returns:\r\n```\r\nx,a,b\r\n1,1,1\r\n1,1,1\r\n1,1,1\r\n```\n\n### To Reproduce\n\nThe queries require no database, thus DuckDB CLI can be used to reproduce.\r\nI can also reproduce on DuckDB WASM.\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.0\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nAltan Birler\n\n### Affiliation:\n\nTUM\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/include/duckdb/planner/subquery/flatten_dependent_join.hpp b/src/include/duckdb/planner/subquery/flatten_dependent_join.hpp\nindex 991e084c42ab..9b7ad035389e 100644\n--- a/src/include/duckdb/planner/subquery/flatten_dependent_join.hpp\n+++ b/src/include/duckdb/planner/subquery/flatten_dependent_join.hpp\n@@ -29,7 +29,8 @@ struct FlattenDependentJoins {\n \tbool MarkSubtreeCorrelated(LogicalOperator &op);\n \n \t//! Push the dependent join down a LogicalOperator\n-\tunique_ptr<LogicalOperator> PushDownDependentJoin(unique_ptr<LogicalOperator> plan);\n+\tunique_ptr<LogicalOperator> PushDownDependentJoin(unique_ptr<LogicalOperator> plan,\n+\t                                                  bool propagates_null_values = true);\n \n \tBinder &binder;\n \tColumnBinding base_binding;\ndiff --git a/src/planner/binder/query_node/plan_subquery.cpp b/src/planner/binder/query_node/plan_subquery.cpp\nindex af70d7e033e0..860b17914aca 100644\n--- a/src/planner/binder/query_node/plan_subquery.cpp\n+++ b/src/planner/binder/query_node/plan_subquery.cpp\n@@ -429,7 +429,7 @@ unique_ptr<LogicalOperator> Binder::PlanLateralJoin(unique_ptr<LogicalOperator>\n \t// first we check which logical operators have correlated expressions in the first place\n \tflatten.DetectCorrelatedExpressions(*right, true);\n \t// now we push the dependent join down\n-\tauto dependent_join = flatten.PushDownDependentJoin(std::move(right));\n+\tauto dependent_join = flatten.PushDownDependentJoin(std::move(right), join_type != JoinType::INNER);\n \n \t// now the dependent join is fully eliminated\n \t// we only need to create the join conditions between the LHS and the RHS\ndiff --git a/src/planner/subquery/flatten_dependent_join.cpp b/src/planner/subquery/flatten_dependent_join.cpp\nindex 4c60d299d779..ac36e9eaef4c 100644\n--- a/src/planner/subquery/flatten_dependent_join.cpp\n+++ b/src/planner/subquery/flatten_dependent_join.cpp\n@@ -87,8 +87,8 @@ bool FlattenDependentJoins::MarkSubtreeCorrelated(LogicalOperator &op) {\n \treturn has_correlation;\n }\n \n-unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoin(unique_ptr<LogicalOperator> plan) {\n-\tbool propagate_null_values = true;\n+unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoin(unique_ptr<LogicalOperator> plan,\n+                                                                         bool propagate_null_values) {\n \tauto result = PushDownDependentJoinInternal(std::move(plan), propagate_null_values, 0);\n \tif (!replacement_map.empty()) {\n \t\t// check if we have to replace any COUNT aggregates into \"CASE WHEN X IS NULL THEN 0 ELSE COUNT END\"\n@@ -255,18 +255,32 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal\n \t\t\tdelim_column_offset = aggr.groups.size() - correlated_columns.size();\n \t\t\tdelim_data_offset = aggr.groups.size();\n \t\t}\n-\t\tif (aggr.groups.size() == new_group_count) {\n-\t\t\t// we have to perform a LEFT OUTER JOIN between the result of this aggregate and the delim scan\n-\t\t\t// FIXME: this does not always have to be a LEFT OUTER JOIN, depending on whether aggr.expressions return\n+\t\tbool ungrouped_join = false;\n+\t\tif (aggr.grouping_sets.empty()) {\n+\t\t\tungrouped_join = aggr.groups.size() == new_group_count;\n+\t\t} else {\n+\t\t\tfor (auto &grouping_set : aggr.grouping_sets) {\n+\t\t\t\tif (grouping_set.size() == new_group_count) {\n+\t\t\t\t\tungrouped_join = true;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif (ungrouped_join) {\n+\t\t\t// we have to perform an INNER or LEFT OUTER JOIN between the result of this aggregate and the delim scan\n+\t\t\t// this does not always have to be a LEFT OUTER JOIN, depending on whether aggr.expressions return\n \t\t\t// NULL or a value\n-\t\t\tunique_ptr<LogicalComparisonJoin> join = make_uniq<LogicalComparisonJoin>(JoinType::INNER);\n+\t\t\tJoinType join_type = JoinType::INNER;\n+\t\t\tif (any_join || !parent_propagate_null_values) {\n+\t\t\t\tjoin_type = JoinType::LEFT;\n+\t\t\t}\n \t\t\tfor (auto &aggr_exp : aggr.expressions) {\n \t\t\t\tauto &b_aggr_exp = aggr_exp->Cast<BoundAggregateExpression>();\n-\t\t\t\tif (!b_aggr_exp.PropagatesNullValues() || any_join || !parent_propagate_null_values) {\n-\t\t\t\t\tjoin = make_uniq<LogicalComparisonJoin>(JoinType::LEFT);\n+\t\t\t\tif (!b_aggr_exp.PropagatesNullValues()) {\n+\t\t\t\t\tjoin_type = JoinType::LEFT;\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t}\n+\t\t\tunique_ptr<LogicalComparisonJoin> join = make_uniq<LogicalComparisonJoin>(join_type);\n \t\t\tauto left_index = binder.GenerateTableIndex();\n \t\t\tdelim_scan = make_uniq<LogicalDelimGet>(left_index, delim_types);\n \t\t\tjoin->children.push_back(std::move(delim_scan));\n", "instance_id": "duckdb__duckdb-13291", "clarity": 3, "difficulty": 0.75, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue with query unnesting involving grouping sets in DuckDB, provides specific SQL queries to demonstrate the discrepancy between expected and actual output, and compares the behavior with PostgreSQL as a reference. The expected and actual results are explicitly listed, leaving no ambiguity about the goal. Additionally, the reproduction steps are detailed, including the environment (DuckDB CLI, WASM), version (1.0), and OS (Linux). The problem statement also includes all necessary context, such as the lack of need for a database to reproduce the issue. There are no significant ambiguities or missing critical details, and the inclusion of specific queries and outputs makes the problem very clear. Therefore, I assign a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem is assessed based on the provided factors and falls into the \"Hard\" category (0.6-0.8). Here's the breakdown of the evaluation:\n\n1. **Clarity and Complexity of the Problem Description**: While the problem statement is clear, the underlying logic of SQL query optimization, particularly with dependent joins, lateral joins, and grouping sets, is inherently complex. Understanding the discrepancy between the two queries requires knowledge of how DuckDB processes subqueries and joins compared to PostgreSQL.\n\n2. **Scope and Depth of Code Changes**: The code changes span multiple files (`flatten_dependent_join.hpp`, `plan_subquery.cpp`, and `flatten_dependent_join.cpp`) in the DuckDB planner module, which is a critical part of the query execution engine. The modifications involve altering the logic for pushing down dependent joins and handling null propagation in joins with grouping sets. While the changes are not extensive in terms of lines of code, they impact core functionality related to query optimization and execution. This requires a deep understanding of the planner's architecture and how different components interact, increasing the difficulty.\n\n3. **Number of Technical Concepts to Understand**: Solving this problem requires familiarity with several advanced concepts, including:\n   - SQL query planning and optimization (specifically, dependent joins and lateral joins).\n   - DuckDB's internal representation of queries and subquery flattening.\n   - Handling of grouping sets and aggregate functions in SQL.\n   - Join types (INNER vs. LEFT OUTER) and their implications on null propagation.\n   - C++ programming, as DuckDB is implemented in C++, with a focus on memory management and unique pointers.\n   These concepts are moderately to highly complex, especially for someone not already familiar with database internals.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention additional edge cases beyond the provided queries, but the code changes suggest handling specific scenarios like null propagation and ungrouped joins with grouping sets. The modifications introduce logic to decide between INNER and LEFT OUTER joins based on null propagation behavior, which indicates attention to edge cases in query results. This adds to the complexity, as incorrect handling could introduce new bugs in other query scenarios.\n\nOverall, this problem requires a deep understanding of DuckDB's query planner and optimization logic, as well as expertise in database internals and SQL semantics. The changes, while focused, have a significant impact on query correctness, a critical aspect of the system. It is not at the extreme end of difficulty (e.g., redesigning the entire planner), but it is challenging enough to warrant a score of 0.75, placing it in the \"Hard\" range. This reflects the need for specialized knowledge and careful consideration of the system's behavior across various query patterns.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "chore: Switch to Ruff\n**Type:  Task**\r\n\r\n## Description\r\nUse Ruff to replace a variety of linters and formatters.  Also turn on many of the linters Ruff adds over our prior configuration, and address any findings.\r\n\r\n## Related Issues/PRs\r\nCloses #56.\r\n\r\n## Commits\r\n1. **chore: Switch to Ruff** (6112943ca3f2a74c0419d8d2452884dd144c8309)\r\n\r\n   Use Ruff to replace a variety of linters/formatters.\r\n1. **docs: Add docstrings to test/example files** (f0b7eb96cff1cd6a6360b1e9eba0b66c614c6ac1)\r\n1. **chore: Ignore security findings in tests/examples** (f18ece4a69291b96b84a80bd8eea0a5f51697026)\r\n1. **refactor: Don't override Python builtins** (9006adbeefdbd08237325d91fbdf7ecf952b386d)\r\n1. **test: Use UTC timezone with datetime** (25ff30ff9c19378108db1cc31f367e0a98e189d9)\r\n\r\n   Running `flake8-datetimez` via Ruff yielded the following findings:\r\n\r\n       example/test_examples.py:88:25: DTZ005 `datetime.datetime.now()` called without a `tz` argument\r\n       example/test_examples.py:89:25: DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z\r\n       example/test_examples.py:125:13: DTZ005 `datetime.datetime.now()` called without a `tz` argument\r\n       example/test_examples.py:126:25: DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z\r\n       Found 4 errors.\r\n\r\n   Specifying timezone info resolves the issues.\r\n1. **refactor: Assign exception messages to variables** (844c71ab25ce20ebb22b111524740b2c991f3f29)\r\n\r\n   Running `flake8-errmsg` via Ruff yielded the following findings:\r\n\r\n       reverse_argparse/reverse_argparse.py:143:17: EM102 Exception must not use an f-string literal, assign to variable first\r\n       reverse_argparse/reverse_argparse.py:461:17: EM102 Exception must not use an f-string literal, assign to variable first\r\n       Found 2 errors.\r\n\r\n   This change resolves the issues.\r\n1. **refactor: Remove unnecessary shebang lines** (a7ae1f6813797f48f3889d6b362217c55a853079)\r\n1. **test: Use tuples for parametrize variables** (25d9be8d014f0159c4328594af84fc723fdad6f9)\r\n1. **refactor: Ignore particular type error** (9d412ddc83a6b0ad5ad3acfb5a520eb0df20086b)\r\n1. **refactor: Address Pylint findings** (862bc62bee7a61659cb69e668f4a1f1d1e9c0c1f)\r\n1. **refactor: Address Ruff-specific lint findings** (076e19951c8fb2241c85d4da063d166292054f25)\r\n1. **patch: Force `prefer_short` to be keyword-only** (f02c7f2eee4861904d3505fd7a6d2cc0fbc5cfc8)\r\n\r\n   Turning on `flake8-boolean-trap` linting via Ruff resulted in the following findings:\r\n\r\n       reverse_argparse/reverse_argparse.py:242:31: FBT002 Boolean default positional argument in function definition\r\n       reverse_argparse/reverse_argparse.py:242:31: FBT001 Boolean-typed positional argument in function definition\r\n       Found 2 errors.\r\n\r\n   Switching `prefer_short` from a positional to a keyword-only argument addresses the problem.\r\n\r\n   Note that this is technically a breaking change, but only for a \"private\" method, not in the package's public API.  Therefore the change is not registered as a breaking change via Conventional Commit syntax, and no major version update will be created.  Instead, this commit will force the creation of a patch release.  If users were relying on the prior behavior of this internal method, they can simply switch to the keyword syntax when calling it.\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex eee3c0f..75fe78b 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -3,10 +3,6 @@ requires = [\"poetry-core\"]\n build-backend = \"poetry.core.masonry.api\"\n \n \n-[tool.bandit.assert_used]\n-skips = [\"**/test_*.py\"]\n-\n-\n [tool.commitizen]\n name = \"cz_customize\"\n \n@@ -103,6 +99,10 @@ ignore = [\n ]\n \n \n+[tool.ruff.lint.per-file-ignores]\n+\"**/test_*.py\" = [\"S101\"]\n+\n+\n [tool.ruff.lint.pydocstyle]\n convention = \"google\"\n \n", "instance_id": "sandialabs__reverse_argparse-117", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to switch to Ruff as a replacement for various linters and formatters, and to address findings from enabling additional linting rules. It provides a high-level description of the task and lists related commits with brief explanations of the changes made (e.g., addressing datetime timezone issues, refactoring exception messages). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the full scope of linters/formatters being replaced, nor does it provide detailed expectations for \"addressing findings\" (e.g., whether all findings must be resolved or if some can be ignored). Additionally, while the commits provide context, the problem statement lacks specific examples of input/output or constraints for the changes. Edge cases or potential challenges in adopting Ruff are also not mentioned. Overall, the statement is valid and clear in its general goal but misses some finer details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this task falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes shown (e.g., updates to `pyproject.toml`) are relatively minor and localized, primarily involving configuration adjustments for Ruff and removal of obsolete settings (e.g., Bandit). However, the commit list suggests broader changes across multiple files, such as refactoring code to address linting findings (e.g., datetime timezone issues, exception message handling, and function argument styles). While these changes span multiple files, they appear to be straightforward fixes rather than deep architectural modifications. The impact on the system's architecture is minimal, as this is primarily a tooling and code quality improvement task.\n\n2. **Number of Technical Concepts:** The task requires familiarity with Python tooling, specifically linters and formatters like Ruff, flake8, and Pylint, as well as understanding linting rules (e.g., `DTZ005`, `FBT002`). Basic knowledge of Python best practices (e.g., handling datetime with timezones, avoiding f-strings in exceptions) is also necessary. These concepts are not particularly complex for a developer with moderate Python experience, though familiarity with Ruff's configuration and rule sets might require some learning for those new to the tool.\n\n3. **Edge Cases and Error Handling:** The problem statement and commits do not explicitly highlight complex edge cases or error handling requirements beyond addressing specific linting findings. Some changes, like ignoring security findings in test files or making a breaking change to a private method, suggest minor considerations for compatibility or user impact, but these are not deeply challenging. The task focuses more on compliance with linting rules than on handling intricate runtime errors or edge cases.\n\n4. **Overall Complexity:** The task involves understanding and applying linting rules across the codebase, which requires attention to detail and some refactoring. However, the changes are mostly mechanical (e.g., adding timezone info, refactoring argument styles) and do not involve complex algorithms, design patterns, or deep system-level modifications. The primary challenge lies in ensuring consistency across the codebase and understanding Ruff's configuration, which is manageable for a developer with basic to intermediate Python experience.\n\nGiven these factors, a difficulty score of 0.35 reflects an \"Easy\" task that requires understanding some code logic and making simple modifications across multiple files, with minimal impact on the overall architecture and no significant edge case complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Drop Python 2.7 support\nStatistics as of today. Python 2.7 represents 2.25% of total downloads.\r\n\r\n```\r\n$ pypinfo pyftpdlib pyversion\r\nServed from cache: False\r\nData processed: 2.17 GiB\r\nData billed: 2.17 GiB\r\nEstimated cost: $0.02\r\n\r\n| python_version | download_count | percentage |\r\n| -------------- | -------------- | ---------- |\r\n| 3.10           | 160,511        | 60.91%     |\r\n| 3.8            | 32,816         | 12.46%     |\r\n| 3.12           | 18,976         | 7.20%      |\r\n| 3.9            | 18,598         | 7.06%      |\r\n| 3.11           | 14,820         | 5.62%      |\r\n| 3.7            | 8,173          | 3.10%      |\r\n| 2.7            | 5,927          | 2.25%      |\r\n| 3.6            | 3,615          | 1.37%      |\r\n| 3.13           | 52             | 0.02%      |\r\n| 3.5            | 36             | 0.01%      |\r\n| Total          | 263,524        | 100.00%    |\r\n```\n", "patch": "diff --git a/HISTORY.rst b/HISTORY.rst\nindex 7d9fb0d3..fdca9019 100644\n--- a/HISTORY.rst\n+++ b/HISTORY.rst\n@@ -1,5 +1,18 @@\n Bug tracker at https://github.com/giampaolo/pyftpdlib/issues\n \n+Version: 2.0.0 - (IN DEVELOPMENT)\n+=================================\n+\n+**Enhancements**\n+\n+* #629: removed Python 2.7 support.\n+\n+**Notes about backward compatibility**\n+\n+* #629: Python 2.7 is no longer supported.\n+* #629: pysendfile module is no longer a required dependency, because we ceased\n+  support for Python 2.\n+\n Version: 1.5.10 - 2024-06-23\n ============================\n \ndiff --git a/MANIFEST.in b/MANIFEST.in\nindex ec2fbf0f..5c579014 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -36,7 +36,6 @@ include pyftpdlib/__init__.py\n include pyftpdlib/__main__.py\n include pyftpdlib/_asynchat.py\n include pyftpdlib/_asyncore.py\n-include pyftpdlib/_compat.py\n include pyftpdlib/authorizers.py\n include pyftpdlib/filesystems.py\n include pyftpdlib/handlers.py\ndiff --git a/Makefile b/Makefile\nindex 65d3cdff..0b2d5bd0 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -28,17 +28,6 @@ ifndef GITHUB_ACTIONS\n \t\ttoml-sort \\\n \t\ttwine\n endif\n-# python 2 deps\n-ifeq ($(shell $(PYTHON) -c \"import sys; print(sys.version_info[0])\"), 2)\n-\tPYDEPS = \\\n-\t\tipaddress \\\n-\t\tmock \\\n-\t\tpsutil \\\n-\t\tpytest \\\n-\t\tpyopenssl \\\n-\t\tpysendfile \\\n-\t\tsetuptools\n-endif\n \n # In not in a virtualenv, add --user options for install commands.\n INSTALL_OPTS = `$(PYTHON) -c \"import sys; print('' if hasattr(sys, 'real_prefix') else '--user')\"`\n@@ -94,12 +83,11 @@ uninstall:  ## Uninstall this package.\n install-pip:  ## Install pip (no-op if already installed).\n \t@$(PYTHON) -c \\\n \t\t\"import sys, ssl, os, pkgutil, tempfile, atexit; \\\n+\t\tfrom urllib.request import urlopen; \\\n \t\tsys.exit(0) if pkgutil.find_loader('pip') else None; \\\n-\t\tPY3 = sys.version_info[0] == 3; \\\n-\t\tpyexc = 'from urllib.request import urlopen' if PY3 else 'from urllib2 import urlopen'; \\\n \t\texec(pyexc); \\\n \t\tctx = ssl._create_unverified_context() if hasattr(ssl, '_create_unverified_context') else None; \\\n-\t\turl = 'https://bootstrap.pypa.io/pip/2.7/get-pip.py' if not PY3 else 'https://bootstrap.pypa.io/get-pip.py'; \\\n+\t\turl = 'https://bootstrap.pypa.io/get-pip.py'; \\\n \t\tkw = dict(context=ctx) if ctx else {}; \\\n \t\treq = urlopen(url, **kw); \\\n \t\tdata = req.read(); \\\ndiff --git a/README.rst b/README.rst\nindex b7fb9a0b..ac898d86 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -1,6 +1,6 @@\n |  |downloads| |stars| |forks| |contributors|\n |  |version| |packages| |license|\n-|  |github-actions| |appveyor| |doc| |twitter|\n+|  |github-actions| |doc| |twitter|\n \n .. |downloads| image:: https://img.shields.io/pypi/dm/pyftpdlib.svg\n     :target: https://pepy.tech/project/pyftpdlib\n@@ -22,10 +22,6 @@\n     :target: https://github.com/giampaolo/pyftpdlib/actions\n     :alt: GH actions\n \n-.. |appveyor| image:: https://img.shields.io/appveyor/build/giampaolo/pyftpdlib/master.svg?maxAge=3600&label=Windows%20(py2)\n-    :target: https://ci.appveyor.com/project/giampaolo/pyftpdlib\n-    :alt: Windows (Py2, Windows)\n-\n .. |doc| image:: https://readthedocs.org/projects/pyftpdlib/badge/?version=latest\n     :target: https://pyftpdlib.readthedocs.io/en/latest/\n     :alt: Documentation Status\n@@ -80,8 +76,7 @@ Features\n - ...But can optionally skip to a\n   `multiple thread / process <http://pyftpdlib.readthedocs.io/en/latest/tutorial.html#changing-the-concurrency-model>`__\n   model (as in: you'll be free to block or use slow filesystems).\n-- Portable: entirely written in pure Python; works with Python **2.7** and\n-  **3.X** using a single code base.\n+- Portable: entirely written in pure Python.\n - Supports **FTPS** (`RFC-4217 <http://tools.ietf.org/html/rfc4217>`__),\n   **IPv6** (`RFC-2428 <ftp://ftp.rfc-editor.org/in-notes/rfc2428.txt>`__),\n   **Unicode** file names (`RFC-2640 <http://tools.ietf.org/html/rfc2640>`__),\ndiff --git a/appveyor.yml b/appveyor.yml\ndeleted file mode 100644\nindex 04a97c5f..00000000\n--- a/appveyor.yml\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-# Build: 3 (bump this up by 1 to force an appveyor run)\n-\n-os: Visual Studio 2015\n-# avoid 2 builds when pushing on PRs\n-skip_branch_with_pr: true\n-# avoid build on new GIT tag\n-skip_tags: true\n-matrix:\n-  # stop build on first failure\n-  fast_finish: true\n-environment:\n-  global:\n-    # SDK v7.0 MSVC Express 2008's SetEnv.cmd script will fail if the\n-    # /E:ON and /V:ON options are not enabled in the batch script interpreter\n-    # See: http://stackoverflow.com/a/13751649/163740\n-    # WITH_COMPILER: \"cmd /E:ON /V:ON /C .\\\\scripts\\\\internal\\\\appveyor_run_with_compiler.cmd\"\n-    PYTHONWARNINGS: always\n-    PYTHONUNBUFFERED: 1\n-    PSUTIL_DEBUG: 1\n-  matrix:\n-    - PYTHON: \"C:\\\\Python27-x64\"\n-      PYTHON_VERSION: \"2.7.x\"\n-      PYTHON_ARCH: \"64\"\n-\n-init:\n-  - \"ECHO %PYTHON% %PYTHON_VERSION% %PYTHON_ARCH%\"\n-\n-install:\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe -m pip --version\"\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe -m pip install --upgrade --user setuptools pip\"\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe scripts/internal/winmake.py setup-dev-env\"\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe -m pip freeze\"\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe scripts/internal/winmake.py install\"\n-\n-build: off\n-\n-test_script:\n-  - \"%WITH_COMPILER% %PYTHON%/python.exe scripts/internal/winmake.py test\"\n-\n-cache:\n-  - '%LOCALAPPDATA%\\pip\\Cache'\n-\n-# on_success:\n-#   - might want to upload the content of dist/*.whl to a public wheelhouse\n-\n-skip_commits:\n-  message: skip-appveyor\ndiff --git a/demo/unix_daemon.py b/demo/unix_daemon.py\nindex 1f3a58cf..ca88722a 100755\n--- a/demo/unix_daemon.py\n+++ b/demo/unix_daemon.py\n@@ -32,7 +32,6 @@\n \n import argparse\n import atexit\n-import errno\n import os\n import signal\n import sys\n@@ -57,8 +56,9 @@ def pid_exists(pid):\n     \"\"\"Return True if a process with the given PID is currently running.\"\"\"\n     try:\n         os.kill(pid, 0)\n-    except OSError as err:\n-        return err.errno == errno.EPERM\n+    except PermissionError:\n+        # EPERM clearly means there's a process to deny access to\n+        return True\n     else:\n         return True\n \n@@ -68,9 +68,8 @@ def get_pid():\n     try:\n         with open(PID_FILE) as f:\n             return int(f.read().strip())\n-    except IOError as err:\n-        if err.errno != errno.ENOENT:\n-            raise\n+    except FileNotFoundError:\n+        pass\n \n \n def stop():\n@@ -87,12 +86,8 @@ def stop():\n         sys.stdout.flush()\n         try:\n             os.kill(pid, sig)\n-        except OSError as err:\n-            if err.errno == errno.ESRCH:\n-                print(\"\\nstopped (pid %s)\" % pid)\n-                return\n-            else:\n-                raise\n+        except ProcessLookupError:\n+            print(\"\\nstopped (pid %s)\" % pid)\n         i += 1\n         if i == 25:\n             sig = signal.SIGKILL\ndiff --git a/docs/api.rst b/docs/api.rst\nindex 87722935..0ea05b8b 100644\n--- a/docs/api.rst\n+++ b/docs/api.rst\n@@ -527,8 +527,7 @@ Filesystem\n \n     Wrapper around\n     `os.listdir <http://docs.python.org/library/os.html#os.listdir>`_.\n-    It is expected to return a list of unicode strings or a generator yielding\n-    unicode strings.\n+    It is expected to return a list of strings or a generator yielding strings.\n \n     .. versionchanged:: 1.6.0 can also return a generator.\n \ndiff --git a/docs/faqs.rst b/docs/faqs.rst\nindex 00dbef04..a10ed8d1 100644\n--- a/docs/faqs.rst\n+++ b/docs/faqs.rst\n@@ -84,14 +84,23 @@ If you are not new to Python you probably don't need that, otherwise follow the\n Which Python versions are compatible?\n -------------------------------------\n \n-*2.7* and *3.X*.\n+Python *3.X*. Anything above 3.8 should be good to go. Pypy should also work.\n+\n+What about Python 2.7?\n+----------------------\n+\n+Latest pyftpdlib version supporting Python 2.7 is 1.5.10. You can install it\n+with:\n+\n+.. code-block:: sh\n+\n+    python3 -m pip install pyftpdlib==1.5.10\n \n On which platforms can pyftpdlib be used?\n -----------------------------------------\n \n pyftpdlib should work on any platform where **select()**, **poll()**,\n-**epoll()** or **kqueue()** system calls are available and on any Python\n-implementation which refers to *cPython 2.7* or superior.\n+**epoll()** or **kqueue()** system calls are available.\n The development team has mainly tested it under various *Linux*, *Windows*,\n *OSX* and *FreeBSD* systems.\n For FreeBSD is also available a\n@@ -140,8 +149,8 @@ avoid race conditions, dead locks etc.\n Another possibility is to\n `change the default concurrency model <tutorial.html#changing-the-concurrency-model>`__.\n \n-Why do I get socket.error \"Permission denied\" error on ftpd starting?\n----------------------------------------------------------------------\n+Why do I get \"Permission denied\" error on startup?\n+--------------------------------------------------\n \n Probably because you're on a Unix system and you're trying to start ftpd as an\n unprivileged user. FTP servers bind on port 21 by default and only super-user\n@@ -244,10 +253,8 @@ Implementation\n sendfile()\n ----------\n \n-Starting from version 0.7.0 if\n-`pysendfile <https://github.com/giampaolo/pysendfile/>`__ module is installed\n-sendfile(2) system call be used when uploading files (from server to client)\n-via RETR command.\n+Starting from version 0.7.0, sendfile(2) system call be used when uploading\n+files (from server to client) via RETR command.\n Using sendfile(2) usually results in transfer rates from 2x to 3x faster\n and less CPU usage.\n Note: use of sendfile() might introduce some unexpected issues with \"non\ndiff --git a/docs/install.rst b/docs/install.rst\nindex 1da7c59f..9ec32f0b 100644\n--- a/docs/install.rst\n+++ b/docs/install.rst\n@@ -32,12 +32,3 @@ Additional dependencies\n .. code-block:: sh\n \n     $ pip install PyOpenSSL\n-\n-`pysendfile <https://github.com/giampaolo/pysendfile>`__, if you're on UNIX,\n-in order to\n-`speedup uploads <http://pyftpdlib.readthedocs.io/faqs.html#sendfile>`__\n-(from server to client):\n-\n-.. code-block:: sh\n-\n-    $ pip install pysendfile\ndiff --git a/make.bat b/make.bat\nindex 263fadda..e9c9a9e1 100644\n--- a/make.bat\n+++ b/make.bat\n@@ -6,12 +6,6 @@ rem It is primarly intended as a shortcut for compiling / installing\n rem psutil (\"make.bat build\", \"make.bat install\") and running tests\n rem (\"make.bat test\").\n rem\n-rem This script is modeled after my Windows installation which uses:\n-rem - Visual studio 2008 for Python 2.7\n-rem - Visual studio 2010 for Python 3.4+\n-rem ...therefore it might not work on your Windows installation.\n-rem\n-rem By default C:\\Python27\\python.exe is used.\n rem To compile for a specific Python version run:\n rem     set PYTHON=C:\\Python34\\python.exe & make.bat build\n rem\ndiff --git a/pyftpdlib/__init__.py b/pyftpdlib/__init__.py\nindex f14d6274..f9d2d743 100644\n--- a/pyftpdlib/__init__.py\n+++ b/pyftpdlib/__init__.py\n@@ -69,6 +69,6 @@ class used to interact with the file system, providing a high level,\n \"\"\"\n \n \n-__ver__ = '1.5.10'\n+__ver__ = '2.0.0'\n __author__ = \"Giampaolo Rodola' <g.rodola@gmail.com>\"\n __web__ = 'https://github.com/giampaolo/pyftpdlib/'\ndiff --git a/pyftpdlib/__main__.py b/pyftpdlib/__main__.py\nindex 8f2077b3..6167207c 100644\n--- a/pyftpdlib/__main__.py\n+++ b/pyftpdlib/__main__.py\n@@ -14,7 +14,6 @@\n import sys\n \n from . import __ver__\n-from ._compat import getcwdu\n from .authorizers import DummyAuthorizer\n from .handlers import FTPHandler\n from .log import config_logging\n@@ -53,7 +52,7 @@ def main(args=None):\n     parser.add_argument(\n         '-d',\n         '--directory',\n-        default=getcwdu(),\n+        default=os.getcwd(),\n         metavar=\"FOLDER\",\n         help=\"specify the directory to share (default current directory)\",\n     )\ndiff --git a/pyftpdlib/_compat.py b/pyftpdlib/_compat.py\ndeleted file mode 100644\nindex 410fd65d..00000000\n--- a/pyftpdlib/_compat.py\n+++ /dev/null\n@@ -1,182 +0,0 @@\n-# Copyright (C) 2007 Giampaolo Rodola' <g.rodola@gmail.com>.\n-# Use of this source code is governed by MIT license that can be\n-# found in the LICENSE file.\n-\n-\"\"\"\n-Compatibility module similar to six lib, which helps maintaining\n-a single code base working with both python 2.7 and 3.x.\n-\"\"\"\n-\n-import errno\n-import os\n-import sys\n-import types\n-\n-\n-PY3 = sys.version_info[0] >= 3\n-_SENTINEL = object()\n-\n-if PY3:\n-\n-    def u(s):\n-        return s\n-\n-    def b(s):\n-        return s.encode(\"latin-1\")\n-\n-    getcwdu = os.getcwd\n-    unicode = str\n-    xrange = range\n-    long = int\n-else:\n-\n-    def u(s):\n-        return unicode(s)\n-\n-    def b(s):\n-        return s\n-\n-    getcwdu = os.getcwdu\n-    unicode = unicode\n-    xrange = xrange\n-    long = long\n-\n-\n-# removed in 3.0, reintroduced in 3.2\n-try:\n-    callable = callable\n-except Exception:\n-\n-    def callable(obj):\n-        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n-\n-\n-# --- exceptions\n-\n-\n-if PY3:\n-    FileNotFoundError = FileNotFoundError  # NOQA\n-    FileExistsError = FileExistsError  # NOQA\n-    InterruptedError = InterruptedError  # NOQA\n-    PermissionError = PermissionError  # NOQA\n-else:\n-    # https://github.com/PythonCharmers/python-future/blob/exceptions/\n-    #     src/future/types/exceptions/pep3151.py\n-    import platform\n-\n-    def _instance_checking_exception(base_exception=Exception):\n-        def wrapped(instance_checker):\n-            class TemporaryClass(base_exception):\n-\n-                def __init__(self, *args, **kwargs):\n-                    if len(args) == 1 and isinstance(args[0], TemporaryClass):\n-                        unwrap_me = args[0]\n-                        for attr in dir(unwrap_me):\n-                            if not attr.startswith('__'):\n-                                setattr(self, attr, getattr(unwrap_me, attr))\n-                    else:\n-                        super(TemporaryClass, self).__init__(  # noqa\n-                            *args, **kwargs\n-                        )\n-\n-                class __metaclass__(type):\n-                    def __instancecheck__(cls, inst):\n-                        return instance_checker(inst)\n-\n-                    def __subclasscheck__(cls, classinfo):\n-                        value = sys.exc_info()[1]\n-                        return isinstance(value, cls)\n-\n-            TemporaryClass.__name__ = instance_checker.__name__\n-            TemporaryClass.__doc__ = instance_checker.__doc__\n-            return TemporaryClass\n-\n-        return wrapped\n-\n-    @_instance_checking_exception(EnvironmentError)\n-    def FileNotFoundError(inst):\n-        return getattr(inst, 'errno', _SENTINEL) == errno.ENOENT\n-\n-    @_instance_checking_exception(EnvironmentError)\n-    def InterruptedError(inst):\n-        return getattr(inst, 'errno', _SENTINEL) == errno.EINTR\n-\n-    @_instance_checking_exception(EnvironmentError)\n-    def PermissionError(inst):\n-        return getattr(inst, 'errno', _SENTINEL) in (errno.EACCES, errno.EPERM)\n-\n-    @_instance_checking_exception(EnvironmentError)\n-    def FileExistsError(inst):\n-        return getattr(inst, 'errno', _SENTINEL) == errno.EEXIST\n-\n-    if platform.python_implementation() != \"CPython\":\n-        try:\n-            raise OSError(errno.EEXIST, \"perm\")\n-        except FileExistsError:\n-            pass\n-        except OSError:\n-            raise RuntimeError(\n-                \"broken or incompatible Python implementation, see: \"\n-                \"https://github.com/giampaolo/psutil/issues/1659\"\n-            )\n-\n-\n-# Python 3 super().\n-# Taken from \"future\" package.\n-# Credit: Ryan Kelly\n-if PY3:\n-    super = super\n-else:\n-    _builtin_super = super\n-\n-    def super(type_=_SENTINEL, type_or_obj=_SENTINEL, framedepth=1):\n-        \"\"\"Like Python 3 builtin super(). If called without any arguments\n-        it attempts to infer them at runtime.\n-        \"\"\"\n-        if type_ is _SENTINEL:\n-            f = sys._getframe(framedepth)\n-            try:\n-                # Get the function's first positional argument.\n-                type_or_obj = f.f_locals[f.f_code.co_varnames[0]]\n-            except (IndexError, KeyError):\n-                raise RuntimeError('super() used in a function with no args')\n-            try:\n-                # Get the MRO so we can crawl it.\n-                mro = type_or_obj.__mro__\n-            except (AttributeError, RuntimeError):\n-                try:\n-                    mro = type_or_obj.__class__.__mro__\n-                except AttributeError:\n-                    raise RuntimeError('super() used in a non-newstyle class')\n-            for type_ in mro:\n-                #  Find the class that owns the currently-executing method.\n-                for meth in type_.__dict__.values():\n-                    # Drill down through any wrappers to the underlying func.\n-                    # This handles e.g. classmethod() and staticmethod().\n-                    try:\n-                        while not isinstance(meth, types.FunctionType):\n-                            if isinstance(meth, property):\n-                                # Calling __get__ on the property will invoke\n-                                # user code which might throw exceptions or\n-                                # have side effects\n-                                meth = meth.fget\n-                            else:\n-                                try:\n-                                    meth = meth.__func__\n-                                except AttributeError:\n-                                    meth = meth.__get__(type_or_obj, type_)\n-                    except (AttributeError, TypeError):\n-                        continue\n-                    if meth.func_code is f.f_code:\n-                        break  # found\n-                else:\n-                    # Not found. Move onto the next class in MRO.\n-                    continue\n-                break  # found\n-            else:\n-                raise RuntimeError('super() called outside a method')\n-\n-        # Dispatch to builtin super().\n-        if type_or_obj is not _SENTINEL:\n-            return _builtin_super(type_, type_or_obj)\n-        return _builtin_super(type_)\ndiff --git a/pyftpdlib/authorizers.py b/pyftpdlib/authorizers.py\nindex c2d63d77..6efd542d 100644\n--- a/pyftpdlib/authorizers.py\n+++ b/pyftpdlib/authorizers.py\n@@ -18,14 +18,9 @@ class for:\n \"\"\"\n \n \n-import errno\n import os\n import warnings\n \n-from ._compat import PY3\n-from ._compat import getcwdu\n-from ._compat import unicode\n-\n \n __all__ = [\n     'DummyAuthorizer',\n@@ -110,8 +105,6 @@ def add_user(\n         \"\"\"\n         if self.has_user(username):\n             raise ValueError('user %r already exists' % username)\n-        if not isinstance(homedir, unicode):\n-            homedir = homedir.decode('utf8')\n         if not os.path.isdir(homedir):\n             raise ValueError('no such directory: %r' % homedir)\n         homedir = os.path.realpath(homedir)\n@@ -353,8 +346,6 @@ def override_user(\n             raise AuthorizerError(\"can't assign password to anonymous user\")\n         if not self.has_user(username):\n             raise AuthorizerError('no such user %s' % username)\n-        if homedir is not None and not isinstance(homedir, unicode):\n-            homedir = homedir.decode('utf8')\n \n         if username in self._dummy_authorizer.user_table:\n             # re-set parameters\n@@ -362,7 +353,7 @@ def override_user(\n         self._dummy_authorizer.add_user(\n             username,\n             password or \"\",\n-            homedir or getcwdu(),\n+            homedir or os.getcwd(),\n             perm or \"\",\n             msg_login or \"\",\n             msg_quit or \"\",\n@@ -484,19 +475,13 @@ def has_user(self, username):\n         def get_home_dir(self, username):\n             \"\"\"Return user home directory.\"\"\"\n             try:\n-                home = pwd.getpwnam(username).pw_dir\n+                return pwd.getpwnam(username).pw_dir\n             except KeyError:\n                 raise AuthorizerError(self.msg_no_such_user)\n-            else:\n-                if not PY3:\n-                    home = home.decode('utf8')\n-                return home\n \n         @staticmethod\n         def _get_system_users():\n             \"\"\"Return all users defined on the UNIX system.\"\"\"\n-            # there should be no need to convert usernames to unicode\n-            # as UNIX does not allow chars outside of ASCII set\n             return [entry.pw_name for entry in pwd.getpwall()]\n \n         def get_msg_login(self, username):\n@@ -664,10 +649,8 @@ def _has_valid_shell(username):\n             \"\"\"\n             try:\n                 file = open('/etc/shells')\n-            except IOError as err:\n-                if err.errno == errno.ENOENT:\n-                    return True\n-                raise\n+            except FileNotFoundError:\n+                return True\n             else:\n                 with file:\n                     try:\n@@ -697,10 +680,7 @@ def _has_valid_shell(username):\n except ImportError:\n     pass\n else:  # pragma: no cover\n-    if PY3:\n-        import winreg\n-    else:\n-        import _winreg as winreg\n+    import winreg\n \n     __all__.extend(['BaseWindowsAuthorizer', 'WindowsAuthorizer'])\n \n@@ -773,14 +753,12 @@ def get_home_dir(self, username):\n             path += r\"\\CurrentVersion\\ProfileList\" + \"\\\\\" + sid\n             try:\n                 key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, path)\n-            except WindowsError:\n+            except OSError:\n                 raise AuthorizerError(\n                     \"No profile directory defined for user %s\" % username\n                 )\n             value = winreg.QueryValueEx(key, \"ProfileImagePath\")[0]\n             home = win32api.ExpandEnvironmentStrings(value)\n-            if not PY3 and not isinstance(home, unicode):\n-                home = home.decode('utf8')\n             return home\n \n         @classmethod\n@@ -949,6 +927,4 @@ def get_home_dir(self, username):\n                 home = overridden_home\n             else:\n                 home = BaseWindowsAuthorizer.get_home_dir(self, username)\n-            if not PY3 and not isinstance(home, unicode):\n-                home = home.decode('utf8')\n             return home\ndiff --git a/pyftpdlib/filesystems.py b/pyftpdlib/filesystems.py\nindex 9e39ff1f..9a9d7ac4 100644\n--- a/pyftpdlib/filesystems.py\n+++ b/pyftpdlib/filesystems.py\n@@ -8,20 +8,12 @@\n import time\n \n \n-try:\n-    from stat import filemode as _filemode  # PY 3.3\n-except ImportError:\n-    from tarfile import filemode as _filemode\n try:\n     import grp\n     import pwd\n except ImportError:\n     pwd = grp = None\n \n-from ._compat import PY3\n-from ._compat import u\n-from ._compat import unicode\n-\n \n __all__ = ['FilesystemError', 'AbstractedFS']\n \n@@ -100,13 +92,12 @@ def __init__(self, root, cmd_channel):\n         - (str) root: the user \"real\" home directory (e.g. '/home/user')\n         - (instance) cmd_channel: the FTPHandler class instance.\n         \"\"\"\n-        assert isinstance(root, unicode)\n         # Set initial current working directory.\n         # By default initial cwd is set to \"/\" to emulate a chroot jail.\n         # If a different behavior is desired (e.g. initial cwd = root,\n         # to reflect the real filesystem) users overriding this class\n         # are responsible to set _cwd attribute as necessary.\n-        self._cwd = u('/')\n+        self._cwd = '/'\n         self._root = root\n         self.cmd_channel = cmd_channel\n \n@@ -122,12 +113,10 @@ def cwd(self):\n \n     @root.setter\n     def root(self, path):\n-        assert isinstance(path, unicode), path\n         self._root = path\n \n     @cwd.setter\n     def cwd(self, path):\n-        assert isinstance(path, unicode), path\n         self._cwd = path\n \n     # --- Pathname / conversion utilities\n@@ -143,7 +132,6 @@ def ftpnorm(self, ftppath):\n         Note: directory separators are system independent (\"/\").\n         Pathname returned is always absolutized.\n         \"\"\"\n-        assert isinstance(ftppath, unicode), ftppath\n         if os.path.isabs(ftppath):\n             p = os.path.normpath(ftppath)\n         else:\n@@ -161,7 +149,7 @@ def ftpnorm(self, ftppath):\n         # that self.cwd is not absolute, return \"/\" as a safety measure.\n         # This is for extra protection, maybe not really necessary.\n         if not os.path.isabs(p):\n-            p = u(\"/\")\n+            p = \"/\"\n         return p\n \n     def ftp2fs(self, ftppath):\n@@ -175,7 +163,6 @@ def ftp2fs(self, ftppath):\n \n         Note: directory separators are system dependent.\n         \"\"\"\n-        assert isinstance(ftppath, unicode), ftppath\n         # as far as I know, it should always be path traversal safe...\n         if os.path.normpath(self.root) == os.sep:\n             return os.path.normpath(self.ftpnorm(ftppath))\n@@ -198,13 +185,12 @@ def fs2ftp(self, fspath):\n         On invalid pathnames escaping from user's root directory\n         (e.g. \"/home\" when root is \"/home/user\") always return \"/\".\n         \"\"\"\n-        assert isinstance(fspath, unicode), fspath\n         if os.path.isabs(fspath):\n             p = os.path.normpath(fspath)\n         else:\n             p = os.path.normpath(os.path.join(self.root, fspath))\n         if not self.validpath(p):\n-            return u('/')\n+            return '/'\n         p = p.replace(os.sep, \"/\")\n         p = p[len(self.root) :]\n         if not p.startswith('/'):\n@@ -221,7 +207,6 @@ def validpath(self, path):\n         Pathnames escaping from user's root directory are considered\n         not valid.\n         \"\"\"\n-        assert isinstance(path, unicode), path\n         root = self.realpath(self.root)\n         path = self.realpath(path)\n         if not root.endswith(os.sep):\n@@ -234,7 +219,6 @@ def validpath(self, path):\n \n     def open(self, filename, mode):\n         \"\"\"Open a file returning its handler.\"\"\"\n-        assert isinstance(filename, unicode), filename\n         return open(filename, mode)\n \n     def mkstemp(self, suffix='', prefix='', dir=None, mode='wb'):\n@@ -266,52 +250,41 @@ def chdir(self, path):\n         it is vital that `cwd` attribute gets set.\n         \"\"\"\n         # note: process cwd will be reset by the caller\n-        assert isinstance(path, unicode), path\n         os.chdir(path)\n         self.cwd = self.fs2ftp(path)\n \n     def mkdir(self, path):\n         \"\"\"Create the specified directory.\"\"\"\n-        assert isinstance(path, unicode), path\n         os.mkdir(path)\n \n     def listdir(self, path):\n         \"\"\"List the content of a directory.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.listdir(path)\n \n     def listdirinfo(self, path):\n         \"\"\"List the content of a directory.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.listdir(path)\n \n     def rmdir(self, path):\n         \"\"\"Remove the specified directory.\"\"\"\n-        assert isinstance(path, unicode), path\n         os.rmdir(path)\n \n     def remove(self, path):\n         \"\"\"Remove the specified file.\"\"\"\n-        assert isinstance(path, unicode), path\n         os.remove(path)\n \n     def rename(self, src, dst):\n         \"\"\"Rename the specified src file to the dst filename.\"\"\"\n-        assert isinstance(src, unicode), src\n-        assert isinstance(dst, unicode), dst\n         os.rename(src, dst)\n \n     def chmod(self, path, mode):\n         \"\"\"Change file/directory mode.\"\"\"\n-        assert isinstance(path, unicode), path\n         if not hasattr(os, 'chmod'):\n             raise NotImplementedError\n         os.chmod(path, mode)\n \n     def stat(self, path):\n         \"\"\"Perform a stat() system call on the given path.\"\"\"\n-        # on python 2 we might also get bytes from os.lisdir()\n-        # assert isinstance(path, unicode), path\n         return os.stat(path)\n \n     def utime(self, path, timeval):\n@@ -324,8 +297,6 @@ def utime(self, path, timeval):\n \n         def lstat(self, path):\n             \"\"\"Like stat but does not follow symbolic links.\"\"\"\n-            # on python 2 we might also get bytes from os.lisdir()\n-            # assert isinstance(path, unicode), path\n             return os.lstat(path)\n \n     else:\n@@ -337,35 +308,29 @@ def readlink(self, path):\n             \"\"\"Return a string representing the path to which a\n             symbolic link points.\n             \"\"\"\n-            assert isinstance(path, unicode), path\n             return os.readlink(path)\n \n     # --- Wrapper methods around os.path.* calls\n \n     def isfile(self, path):\n         \"\"\"Return True if path is a file.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.isfile(path)\n \n     def islink(self, path):\n         \"\"\"Return True if path is a symbolic link.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.islink(path)\n \n     def isdir(self, path):\n         \"\"\"Return True if path is a directory.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.isdir(path)\n \n     def getsize(self, path):\n         \"\"\"Return the size of the specified file in bytes.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.getsize(path)\n \n     def getmtime(self, path):\n         \"\"\"Return the last modified time as a number of seconds since\n         the epoch.\"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.getmtime(path)\n \n     def realpath(self, path):\n@@ -373,14 +338,12 @@ def realpath(self, path):\n         symbolic links encountered in the path (if they are\n         supported by the operating system).\n         \"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.realpath(path)\n \n     def lexists(self, path):\n         \"\"\"Return True if path refers to an existing path, including\n         a broken or circular symbolic link.\n         \"\"\"\n-        assert isinstance(path, unicode), path\n         return os.path.lexists(path)\n \n     if pwd is not None:\n@@ -449,7 +412,6 @@ def get_user_by_uid(uid):\n         def get_group_by_gid(gid):\n             return self.get_group_by_gid(gid)\n \n-        assert isinstance(basedir, unicode), basedir\n         if self.cmd_channel.use_gmt_times:\n             timefunc = time.gmtime\n         else:\n@@ -458,20 +420,7 @@ def get_group_by_gid(gid):\n         readlink = getattr(self, 'readlink', None)\n         now = time.time()\n         for basename in listing:\n-            if not PY3:\n-                try:\n-                    file = os.path.join(basedir, basename)\n-                except UnicodeDecodeError:\n-                    # (Python 2 only) might happen on filesystem not\n-                    # supporting UTF8 meaning os.listdir() returned a list\n-                    # of mixed bytes and unicode strings:\n-                    # http://goo.gl/6DLHD\n-                    # http://bugs.python.org/issue683592\n-                    file = os.path.join(bytes(basedir), bytes(basename))\n-                    if not isinstance(basename, unicode):\n-                        basename = unicode(basename, 'utf8', 'ignore')\n-            else:\n-                file = os.path.join(basedir, basename)\n+            file = os.path.join(basedir, basename)\n             try:\n                 st = self.lstat(file)\n             except (OSError, FilesystemError):\n@@ -479,7 +428,7 @@ def get_group_by_gid(gid):\n                     continue\n                 raise\n \n-            perms = _filemode(st.st_mode)  # permissions\n+            perms = stat.filemode(st.st_mode)  # permissions\n             nlinks = st.st_nlink  # number of links to inode\n             if not nlinks:  # non-posix system, let's use a bogus value\n                 nlinks = 1\n@@ -555,7 +504,6 @@ def format_mlsx(self, basedir, listing, perms, facts, ignore_err=True):\n         type=dir;size=0;perm=el;modify=20071127230206;unique=801e33; ebooks\n         type=file;size=211;perm=r;modify=20071103093626;unique=192; module.py\n         \"\"\"\n-        assert isinstance(basedir, unicode), basedir\n         if self.cmd_channel.use_gmt_times:\n             timefunc = time.gmtime\n         else:\n@@ -577,20 +525,7 @@ def format_mlsx(self, basedir, listing, perms, facts, ignore_err=True):\n         show_unique = 'unique' in facts\n         for basename in listing:\n             retfacts = {}\n-            if not PY3:\n-                try:\n-                    file = os.path.join(basedir, basename)\n-                except UnicodeDecodeError:\n-                    # (Python 2 only) might happen on filesystem not\n-                    # supporting UTF8 meaning os.listdir() returned a list\n-                    # of mixed bytes and unicode strings:\n-                    # http://goo.gl/6DLHD\n-                    # http://bugs.python.org/issue683592\n-                    file = os.path.join(bytes(basedir), bytes(basename))\n-                    if not isinstance(basename, unicode):\n-                        basename = unicode(basename, 'utf8', 'ignore')\n-            else:\n-                file = os.path.join(basedir, basename)\n+            file = os.path.join(basedir, basename)\n             # in order to properly implement 'unique' fact (RFC-3659,\n             # chapter 7.5.2) we are supposed to follow symlinks, hence\n             # use os.stat() instead of os.lstat()\ndiff --git a/pyftpdlib/handlers.py b/pyftpdlib/handlers.py\nindex afd78742..a0040dc7 100644\n--- a/pyftpdlib/handlers.py\n+++ b/pyftpdlib/handlers.py\n@@ -12,7 +12,6 @@\n import sys\n import time\n import traceback\n-import warnings\n from datetime import datetime\n \n \n@@ -27,20 +26,8 @@\n except ImportError:\n     SSL = None\n \n-try:\n-    from collections import OrderedDict  # python >= 2.7\n-except ImportError:\n-    OrderedDict = dict\n-\n from . import __ver__\n-from ._compat import PY3\n-from ._compat import PermissionError\n-from ._compat import b\n-from ._compat import getcwdu\n-from ._compat import super\n-from ._compat import u\n-from ._compat import unicode\n-from ._compat import xrange\n+from . import _asynchat as asynchat\n from .authorizers import AuthenticationFailed\n from .authorizers import AuthorizerError\n from .authorizers import DummyAuthorizer\n@@ -57,38 +44,9 @@\n from .log import logger\n \n \n-if PY3:\n-    from . import _asynchat as asynchat\n-else:\n-    import asynchat\n-\n-\n CR_BYTE = ord('\\r')\n \n \n-def _import_sendfile():\n-    # By default attempt to use os.sendfile introduced in Python 3.3:\n-    # http://bugs.python.org/issue10882\n-    # ...otherwise fallback on using third-party pysendfile module:\n-    # https://github.com/giampaolo/pysendfile/\n-    if os.name == 'posix':\n-        try:\n-            return os.sendfile  # py >= 3.3\n-        except AttributeError:\n-            try:\n-                import sendfile as sf\n-\n-                # dirty hack to detect whether old 1.2.4 version is installed\n-                if hasattr(sf, 'has_sf_hdtr'):\n-                    raise ImportError\n-                return sf.sendfile\n-            except ImportError:\n-                pass\n-    return None\n-\n-\n-sendfile = _import_sendfile()\n-\n proto_cmds = {\n     'ABOR': dict(\n         perm=None, auth=True, arg=False, help='Syntax: ABOR (abort transfer).'\n@@ -403,7 +361,7 @@ def _support_hybrid_ipv6():\n             return False\n         with contextlib.closing(socket.socket(socket.AF_INET6)) as sock:\n             return not sock.getsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY)\n-    except (socket.error, AttributeError):\n+    except (OSError, AttributeError):\n         return False\n \n \n@@ -483,7 +441,7 @@ def __init__(self, cmd_channel, extmode=False):\n                         \"ignoring EPERM when bind()ing port %s\" % port,\n                         logfun=logger.debug,\n                     )\n-                except socket.error as err:\n+                except OSError as err:\n                     if err.errno == errno.EADDRINUSE:  # port already in use\n                         if ports:\n                             continue\n@@ -548,7 +506,7 @@ def handle_accepted(self, sock, addr):\n             if not self.cmd_channel.permit_foreign_addresses:\n                 try:\n                     sock.close()\n-                except socket.error:\n+                except OSError:\n                     pass\n                 msg = (\n                     '425 Rejected data connection from foreign address '\n@@ -637,7 +595,7 @@ def __init__(self, ip, port, cmd_channel):\n         # dual stack IPv4/IPv6 support\n         try:\n             self.connect_af_unspecified((ip, port), (source_ip, 0))\n-        except (socket.gaierror, socket.error):\n+        except (socket.gaierror, OSError):\n             self.handle_close()\n \n     def readable(self):\n@@ -650,17 +608,13 @@ def handle_connect(self):\n             self._idler.cancel()\n         if not self.cmd_channel.connected:\n             return self.close()\n-        # fix for asyncore on python < 2.6, meaning we aren't\n-        # actually connected.\n         # test_active_conn_error tests this condition\n         err = self.socket.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n         if err != 0:\n-            raise socket.error(err)\n-        #\n+            raise OSError(err)\n         msg = 'Active data connection established.'\n         self.cmd_channel.respond('200 ' + msg)\n         self.cmd_channel.log_cmd(self._cmd, self._normalized_addr, 200, msg)\n-        #\n         if not self.cmd_channel.connected:\n             return self.close()\n         # delegate such connection to DTP handler\n@@ -694,7 +648,7 @@ def handle_error(self):\n         \"\"\"Called to handle any uncaught exceptions.\"\"\"\n         try:\n             raise  # noqa: PLE0704\n-        except (socket.gaierror, socket.error):\n+        except (socket.gaierror, OSError):\n             pass\n         except Exception:\n             self.log_exception(self)\n@@ -759,7 +713,7 @@ def __init__(self, sock, cmd_channel):\n         self._initialized = False\n         try:\n             AsyncChat.__init__(self, sock, ioloop=cmd_channel.ioloop)\n-        except socket.error as err:\n+        except OSError as err:\n             # if we get an exception here we want the dispatcher\n             # instance to set socket attribute before closing, see:\n             # https://github.com/giampaolo/pyftpdlib/issues/188\n@@ -842,7 +796,7 @@ def initiate_send(self):\n     def initiate_sendfile(self):\n         \"\"\"A wrapper around sendfile.\"\"\"\n         try:\n-            sent = sendfile(\n+            sent = os.sendfile(\n                 self._fileno,\n                 self._filefd,\n                 self._offset,\n@@ -886,7 +840,7 @@ def _posix_ascii_data_wrapper(self, chunk):\n         else:\n             self._had_cr = False\n \n-        return chunk.replace(b'\\r\\n', b(os.linesep))\n+        return chunk.replace(b'\\r\\n', bytes(os.linesep, \"ascii\"))\n \n     def enable_receiving(self, type, cmd):\n         \"\"\"Enable receiving of data over the channel. Depending on the\n@@ -964,7 +918,7 @@ def handle_read(self):\n             chunk = self.recv(self.ac_in_buffer_size)\n         except RetryError:\n             pass\n-        except socket.error:\n+        except OSError:\n             self.handle_error()\n         else:\n             self.tot_bytes_received += len(chunk)\n@@ -1096,22 +1050,7 @@ def close(self):\n             self.cmd_channel._on_dtp_close()\n \n \n-# dirty hack in order to turn AsyncChat into a new style class in\n-# python 2.x so that we can use super()\n-if PY3:\n-\n-    class _AsyncChatNewStyle(AsyncChat):\n-        pass\n-\n-else:\n-\n-    class _AsyncChatNewStyle(object, AsyncChat):  # noqa\n-\n-        def __init__(self, *args, **kwargs):\n-            super(object, self).__init__(*args, **kwargs)  # bypass object\n-\n-\n-class ThrottledDTPHandler(_AsyncChatNewStyle, DTPHandler):\n+class ThrottledDTPHandler(DTPHandler):\n     \"\"\"A DTPHandler subclass which wraps sending and receiving in a data\n     counter and temporarily \"sleeps\" the channel so that you burst to no\n     more than x Kb/sec average.\n@@ -1270,7 +1209,7 @@ def more(self):\n         its next() method different times.\n         \"\"\"\n         buffer = []\n-        for _ in xrange(self.loops):\n+        for _ in range(self.loops):\n             try:\n                 buffer.append(next(self.iterator))\n             except StopIteration:\n@@ -1392,7 +1331,7 @@ class FTPHandler(AsyncChat):\n     masquerade_address_map = {}\n     passive_ports = None\n     use_gmt_times = True\n-    use_sendfile = sendfile is not None\n+    use_sendfile = hasattr(os, \"sendfile\")  # added in python 3.3\n     tcp_no_delay = hasattr(socket, \"TCP_NODELAY\")\n     unicode_errors = 'replace'\n     log_prefix = '%(remote_ip)s:%(remote_port)s-[%(username)s]'\n@@ -1447,7 +1386,7 @@ def __init__(self, conn, server, ioloop=None):\n \n         try:\n             AsyncChat.__init__(self, conn, ioloop=ioloop)\n-        except socket.error as err:\n+        except OSError as err:\n             # if we get an exception here we want the dispatcher\n             # instance to set socket attribute before closing, see:\n             # https://github.com/giampaolo/pyftpdlib/issues/188\n@@ -1464,7 +1403,7 @@ def __init__(self, conn, server, ioloop=None):\n         # connection properties\n         try:\n             self.remote_ip, self.remote_port = self.socket.getpeername()[:2]\n-        except socket.error as err:\n+        except OSError as err:\n             debug(\n                 \"call: FTPHandler.__init__, err on getpeername() %r\" % err,\n                 self,\n@@ -1484,7 +1423,7 @@ def __init__(self, conn, server, ioloop=None):\n         # try to handle urgent data inline\n         try:\n             self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_OOBINLINE, 1)\n-        except socket.error as err:\n+        except OSError as err:\n             debug(\n                 \"call: FTPHandler.__init__, err on SO_OOBINLINE %r\" % err, self\n             )\n@@ -1494,7 +1433,7 @@ def __init__(self, conn, server, ioloop=None):\n         if self.tcp_no_delay:\n             try:\n                 self.socket.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)\n-            except socket.error as err:\n+            except OSError as err:\n                 debug(\n                     \"call: FTPHandler.__init__, err on TCP_NODELAY %r\" % err,\n                     self,\n@@ -1513,7 +1452,7 @@ def __init__(self, conn, server, ioloop=None):\n     def get_repr_info(self, as_str=False, extra_info=None):\n         if extra_info is None:\n             extra_info = {}\n-        info = OrderedDict()\n+        info = {}\n         info['id'] = id(self)\n         info['addr'] = \"%s:%s\" % (self.remote_ip, self.remote_port)\n         if _is_ssl_sock(self.socket):\n@@ -1687,16 +1626,16 @@ def pre_process_command(self, line, cmd, arg):\n                 return\n         else:\n             if (cmd == 'STAT') and not arg:\n-                self.ftp_STAT(u(''))\n+                self.ftp_STAT('')\n                 return\n \n             # for file-system related commands check whether real path\n             # destination is valid\n             if self.proto_cmds[cmd]['perm'] and (cmd != 'STOU'):\n                 if cmd in ('CWD', 'XCWD'):\n-                    arg = self.fs.ftp2fs(arg or u('/'))\n+                    arg = self.fs.ftp2fs(arg or '/')\n                 elif cmd in ('CDUP', 'XCUP'):\n-                    arg = self.fs.ftp2fs(u('..'))\n+                    arg = self.fs.ftp2fs('..')\n                 elif cmd == 'LIST':\n                     if arg.lower() in ('-a', '-l', '-al', '-la'):\n                         arg = self.fs.ftp2fs(self.fs.cwd)\n@@ -2376,17 +2315,8 @@ def ftp_LIST(self, path):\n             isdir = self.fs.isdir(path)\n             if isdir:\n                 listing = self.run_as_current_user(self.fs.listdir, path)\n-                if isinstance(listing, list):\n-                    try:\n-                        # RFC 959 recommends the listing to be sorted.\n-                        listing.sort()\n-                    except UnicodeDecodeError:\n-                        # (Python 2 only) might happen on filesystem not\n-                        # supporting UTF8 meaning os.listdir() returned a list\n-                        # of mixed bytes and unicode strings:\n-                        # http://goo.gl/6DLHD\n-                        # http://bugs.python.org/issue683592\n-                        pass\n+                # RFC 959 recommends the listing to be sorted.\n+                listing.sort()\n                 iterator = self.fs.format_list(path, listing)\n             else:\n                 basedir, filename = os.path.split(path)\n@@ -2417,20 +2347,8 @@ def ftp_NLST(self, path):\n         else:\n             data = ''\n             if listing:\n-                try:\n-                    listing.sort()\n-                except UnicodeDecodeError:\n-                    # (Python 2 only) might happen on filesystem not\n-                    # supporting UTF8 meaning os.listdir() returned a list\n-                    # of mixed bytes and unicode strings:\n-                    # http://goo.gl/6DLHD\n-                    # http://bugs.python.org/issue683592\n-                    ls = []\n-                    for x in listing:\n-                        if not isinstance(x, unicode):\n-                            x = unicode(x, 'utf8')\n-                        ls.append(x)\n-                    listing = sorted(ls)\n+                # RFC 959 recommends the listing to be sorted.\n+                listing.sort()\n                 data = '\\r\\n'.join(listing) + '\\r\\n'\n             data = data.encode('utf8', self.unicode_errors)\n             self.push_dtp_data(data, cmd=\"NLST\")\n@@ -2506,7 +2424,7 @@ def ftp_RETR(self, file):\n         self._restart_position = 0\n         try:\n             fd = self.run_as_current_user(self.fs.open, file, 'rb')\n-        except (EnvironmentError, FilesystemError) as err:\n+        except (OSError, FilesystemError) as err:\n             why = _strerror(err)\n             self.respond('550 %s.' % why)\n             return\n@@ -2530,7 +2448,7 @@ def ftp_RETR(self, file):\n                         rest_pos,\n                         fsize,\n                     )\n-                except (EnvironmentError, FilesystemError) as err:\n+                except (OSError, FilesystemError) as err:\n                     why = _strerror(err)\n                 if not ok:\n                     fd.close()\n@@ -2559,7 +2477,7 @@ def ftp_STOR(self, file, mode='w'):\n             mode = 'r+'\n         try:\n             fd = self.run_as_current_user(self.fs.open, file, mode + 'b')\n-        except (EnvironmentError, FilesystemError) as err:\n+        except (OSError, FilesystemError) as err:\n             why = _strerror(err)\n             self.respond('550 %s.' % why)\n             return\n@@ -2583,7 +2501,7 @@ def ftp_STOR(self, file, mode='w'):\n                         rest_pos,\n                         fsize,\n                     )\n-                except (EnvironmentError, FilesystemError) as err:\n+                except (OSError, FilesystemError) as err:\n                     why = _strerror(err)\n                 if not ok:\n                     fd.close()\n@@ -2633,7 +2551,7 @@ def ftp_STOU(self, line):\n             fd = self.run_as_current_user(\n                 self.fs.mkstemp, prefix=prefix, dir=basedir\n             )\n-        except (EnvironmentError, FilesystemError) as err:\n+        except (OSError, FilesystemError) as err:\n             # likely, we hit the max number of retries to find out a\n             # file with a unique name\n             if getattr(err, \"errno\", -1) == errno.EEXIST:\n@@ -2790,19 +2708,6 @@ def callback(username, password, msg):\n         self.username = \"\"\n \n     def handle_auth_success(self, home, password, msg_login):\n-        if not isinstance(home, unicode):\n-            if PY3:\n-                raise TypeError('type(home) != text')\n-            else:\n-                warnings.warn(\n-                    '%s.get_home_dir returned a non-unicode string; now '\n-                    'casting to unicode'\n-                    % (self.authorizer.__class__.__name__),\n-                    RuntimeWarning,\n-                    stacklevel=2,\n-                )\n-                home = home.decode('utf8')\n-\n         if len(msg_login) <= 75:\n             self.respond('230 %s' % msg_login)\n         else:\n@@ -2856,7 +2761,6 @@ def ftp_PWD(self, line):\n         # name and in case it contains embedded double-quotes\n         # they must be doubled (see RFC-959, chapter 7, appendix 2).\n         cwd = self.fs.cwd\n-        assert isinstance(cwd, unicode), cwd\n         self.respond(\n             '257 \"%s\" is the current directory.' % cwd.replace('\"', '\"\"')\n         )\n@@ -2872,7 +2776,7 @@ def ftp_CWD(self, path):\n         # the process is started we'll get into troubles (os.getcwd()\n         # will fail with ENOENT) but we can't do anything about that\n         # except logging an error.\n-        init_cwd = getcwdu()\n+        init_cwd = os.getcwd()\n         try:\n             self.run_as_current_user(self.fs.chdir, path)\n         except (OSError, FilesystemError) as err:\n@@ -2880,9 +2784,8 @@ def ftp_CWD(self, path):\n             self.respond('550 %s.' % why)\n         else:\n             cwd = self.fs.cwd\n-            assert isinstance(cwd, unicode), cwd\n             self.respond('250 \"%s\" is the current directory.' % cwd)\n-            if getcwdu() != init_cwd:\n+            if os.getcwd() != init_cwd:\n                 os.chdir(init_cwd)\n             return path\n \n@@ -3174,16 +3077,8 @@ def ftp_STAT(self, path):\n                 if isdir:\n                     listing = self.run_as_current_user(self.fs.listdir, path)\n                     if isinstance(listing, list):\n-                        try:\n-                            # RFC 959 recommends the listing to be sorted.\n-                            listing.sort()\n-                        except UnicodeDecodeError:\n-                            # (Python 2 only) might happen on filesystem not\n-                            # supporting UTF8 meaning os.listdir() returned a\n-                            # list of mixed bytes and unicode strings:\n-                            # http://goo.gl/6DLHD\n-                            # http://bugs.python.org/issue683592\n-                            pass\n+                        # RFC 959 recommends the listing to be sorted.\n+                        listing.sort()\n                     iterator = self.fs.format_list(path, listing)\n                 else:\n                     basedir, filename = os.path.split(path)\n@@ -3200,7 +3095,7 @@ def ftp_STAT(self, path):\n \n     def ftp_FEAT(self, line):\n         \"\"\"List all new features supported as defined in RFC-2398.\"\"\"\n-        features = set(['UTF8', 'TVFS'])\n+        features = {'UTF8', 'TVFS'}\n         features.update(\n             [\n                 feat\n@@ -3374,7 +3269,7 @@ def ftp_XRMD(self, line):\n \n if SSL is not None:\n \n-    class SSLConnection(_AsyncChatNewStyle):\n+    class SSLConnection:\n         \"\"\"An AsyncChat subclass supporting TLS/SSL.\"\"\"\n \n         _ssl_accepting = False\n@@ -3406,7 +3301,7 @@ def secure_connection(self, ssl_context):\n             self._ssl_requested = True\n             try:\n                 self.socket = SSL.Connection(ssl_context, self.socket)\n-            except socket.error as err:\n+            except OSError as err:\n                 # may happen in case the client connects/disconnects\n                 # very quickly\n                 debug(\n@@ -3613,7 +3508,7 @@ def _do_ssl_shutdown(self):\n                 # connection has gone away\n                 try:\n                     os.write(self.socket.fileno(), b'')\n-                except (OSError, socket.error) as err:\n+                except OSError as err:\n                     debug(\n                         \"call: _do_ssl_shutdown() -> os.write, err: %r\" % err,\n                         inst=self,\n@@ -3688,7 +3583,7 @@ def _do_ssl_shutdown(self):\n                     pass\n                 else:\n                     raise\n-            except socket.error as err:\n+            except OSError as err:\n                 debug(\n                     \"call: _do_ssl_shutdown() -> shutdown(), err: %r\" % err,\n                     inst=self,\ndiff --git a/pyftpdlib/ioloop.py b/pyftpdlib/ioloop.py\nindex 8ac317f8..f0fa46b5 100644\n--- a/pyftpdlib/ioloop.py\n+++ b/pyftpdlib/ioloop.py\n@@ -62,49 +62,32 @@ def handle_accepted(self, sock, addr):\n import select\n import socket\n import sys\n+import threading\n import time\n import traceback\n \n-from ._compat import PY3\n-from ._compat import InterruptedError\n-\n-\n-try:\n-    import threading\n-except ImportError:\n-    import dummy_threading as threading\n-\n-from ._compat import callable\n+from . import _asynchat as asynchat\n+from . import _asyncore as asyncore\n from .log import config_logging\n from .log import debug\n from .log import is_logging_configured\n from .log import logger\n \n \n-if PY3:\n-    from . import _asynchat as asynchat\n-    from . import _asyncore as asyncore\n-else:\n-    import asynchat\n-    import asyncore\n-\n-\n timer = getattr(time, 'monotonic', time.time)\n _read = asyncore.read\n _write = asyncore.write\n \n # These errnos indicate that a connection has been abruptly terminated.\n-_ERRNOS_DISCONNECTED = set(\n-    (\n-        errno.ECONNRESET,\n-        errno.ENOTCONN,\n-        errno.ESHUTDOWN,\n-        errno.ECONNABORTED,\n-        errno.EPIPE,\n-        errno.EBADF,\n-        errno.ETIMEDOUT,\n-    )\n-)\n+_ERRNOS_DISCONNECTED = {\n+    errno.ECONNRESET,\n+    errno.ENOTCONN,\n+    errno.ESHUTDOWN,\n+    errno.ECONNABORTED,\n+    errno.EPIPE,\n+    errno.EBADF,\n+    errno.ETIMEDOUT,\n+}\n if hasattr(errno, \"WSAECONNRESET\"):\n     _ERRNOS_DISCONNECTED.add(errno.WSAECONNRESET)\n if hasattr(errno, \"WSAECONNABORTED\"):\n@@ -112,7 +95,7 @@ def handle_accepted(self, sock, addr):\n \n # These errnos indicate that a non-blocking operation must be retried\n # at a later time.\n-_ERRNOS_RETRY = set((errno.EAGAIN, errno.EWOULDBLOCK))\n+_ERRNOS_RETRY = {errno.EAGAIN, errno.EWOULDBLOCK}\n if hasattr(errno, \"WSAEWOULDBLOCK\"):\n     _ERRNOS_RETRY.add(errno.WSAEWOULDBLOCK)\n \n@@ -521,11 +504,8 @@ def __init__(self):\n     def register(self, fd, instance, events):\n         try:\n             self._poller.register(fd, events)\n-        except EnvironmentError as err:\n-            if err.errno == errno.EEXIST:\n-                debug(\"call: register(); poller raised EEXIST; ignored\", self)\n-            else:\n-                raise\n+        except FileExistsError:\n+            debug(\"call: register(); poller raised EEXIST; ignored\", self)\n         self.socket_map[fd] = instance\n \n     def unregister(self, fd):\n@@ -536,7 +516,7 @@ def unregister(self, fd):\n         else:\n             try:\n                 self._poller.unregister(fd)\n-            except EnvironmentError as err:\n+            except OSError as err:\n                 if err.errno in (errno.ENOENT, errno.EBADF):\n                     debug(\n                         \"call: unregister(); poller returned %r; ignoring it\"\n@@ -549,8 +529,8 @@ def unregister(self, fd):\n     def modify(self, fd, events):\n         try:\n             self._poller.modify(fd, events)\n-        except OSError as err:\n-            if err.errno == errno.ENOENT and fd in self.socket_map:\n+        except FileNotFoundError:\n+            if fd in self.socket_map:\n                 # XXX - see:\n                 # https://github.com/giampaolo/pyftpdlib/issues/329\n                 instance = self.socket_map[fd]\n@@ -695,13 +675,8 @@ def register(self, fd, instance, events):\n             self.socket_map[fd] = instance\n             try:\n                 self._control(fd, events, select.KQ_EV_ADD)\n-            except EnvironmentError as err:\n-                if err.errno == errno.EEXIST:\n-                    debug(\n-                        \"call: register(); poller raised EEXIST; ignored\", self\n-                    )\n-                else:\n-                    raise\n+            except FileExistsError:\n+                debug(\"call: register(); poller raised EEXIST; ignored\", self)\n             self._active[fd] = events\n \n         def unregister(self, fd):\n@@ -713,7 +688,7 @@ def unregister(self, fd):\n             else:\n                 try:\n                     self._control(fd, events, select.KQ_EV_DELETE)\n-                except EnvironmentError as err:\n+                except OSError as err:\n                     if err.errno in (errno.ENOENT, errno.EBADF):\n                         debug(\n                             \"call: unregister(); poller returned %r; \"\n@@ -931,7 +906,7 @@ def connect_af_unspecified(self, addr, source_address=None):\n                         )\n                     self.bind(source_address)\n                 self.connect((host, port))\n-            except socket.error as _:\n+            except OSError as _:\n                 err = _\n                 if self.socket is not None:\n                     self.socket.close()\n@@ -941,7 +916,7 @@ def connect_af_unspecified(self, addr, source_address=None):\n             break\n         if self.socket is None:\n             self.del_channel()\n-            raise socket.error(err)\n+            raise OSError(err)\n         return af\n \n     # send() and recv() overridden as a fix around various bugs:\n@@ -952,7 +927,7 @@ def connect_af_unspecified(self, addr, source_address=None):\n     def send(self, data):\n         try:\n             return self.socket.send(data)\n-        except socket.error as err:\n+        except OSError as err:\n             debug(\"call: send(), err: %s\" % err, inst=self)\n             if err.errno in _ERRNOS_RETRY:\n                 return 0\n@@ -965,7 +940,7 @@ def send(self, data):\n     def recv(self, buffer_size):\n         try:\n             data = self.socket.recv(buffer_size)\n-        except socket.error as err:\n+        except OSError as err:\n             debug(\"call: recv(), err: %s\" % err, inst=self)\n             if err.errno in _ERRNOS_DISCONNECTED:\n                 self.handle_close()\n@@ -1080,7 +1055,7 @@ def bind_af_unspecified(self, addr):\n                 self.create_socket(af, socktype)\n                 self.set_reuse_addr()\n                 self.bind(sa)\n-            except socket.error as _:\n+            except OSError as _:\n                 err = _\n                 if self.socket is not None:\n                     self.socket.close()\n@@ -1090,7 +1065,7 @@ def bind_af_unspecified(self, addr):\n             break\n         if self.socket is None:\n             self.del_channel()\n-            raise socket.error(err)\n+            raise OSError(err)\n         return af\n \n     def listen(self, num):\n@@ -1111,7 +1086,7 @@ def handle_accept(self):\n             # https://github.com/giampaolo/pyftpdlib/issues/91\n             debug(\"call: handle_accept(); accept() returned None\", self)\n             return\n-        except socket.error as err:\n+        except OSError as err:\n             # ECONNABORTED might be thrown on *BSD, see:\n             # https://github.com/giampaolo/pyftpdlib/issues/105\n             if err.errno != errno.ECONNABORTED:\ndiff --git a/pyftpdlib/log.py b/pyftpdlib/log.py\nindex cdaed866..905172a9 100644\n--- a/pyftpdlib/log.py\n+++ b/pyftpdlib/log.py\n@@ -21,9 +21,6 @@\n except ImportError:\n     curses = None\n \n-from ._compat import PY3\n-from ._compat import unicode\n-\n \n # default logger\n logger = logging.getLogger('pyftpdlib')\n@@ -71,24 +68,22 @@ def __init__(self, *args, **kwargs):\n             # bytes, but only accept strings. In addition, we want to\n             # output these strings with the logging module, which\n             # works with unicode strings. The explicit calls to\n-            # unicode() below are harmless in python2 but will do the\n+            # str() below are harmless in python2 but will do the\n             # right conversion in python 3.\n             fg_color = (\n                 curses.tigetstr(\"setaf\") or curses.tigetstr(\"setf\") or \"\"\n             )\n-            if not PY3:\n-                fg_color = unicode(fg_color, \"ascii\")\n             self._colors = {\n                 # blues\n-                logging.DEBUG: unicode(curses.tparm(fg_color, 4), \"ascii\"),\n+                logging.DEBUG: str(curses.tparm(fg_color, 4), \"ascii\"),\n                 # green\n-                logging.INFO: unicode(curses.tparm(fg_color, 2), \"ascii\"),\n+                logging.INFO: str(curses.tparm(fg_color, 2), \"ascii\"),\n                 # yellow\n-                logging.WARNING: unicode(curses.tparm(fg_color, 3), \"ascii\"),\n+                logging.WARNING: str(curses.tparm(fg_color, 3), \"ascii\"),\n                 # red\n-                logging.ERROR: unicode(curses.tparm(fg_color, 1), \"ascii\"),\n+                logging.ERROR: str(curses.tparm(fg_color, 1), \"ascii\"),\n             }\n-            self._normal = unicode(curses.tigetstr(\"sgr0\"), \"ascii\")\n+            self._normal = str(curses.tigetstr(\"sgr0\"), \"ascii\")\n \n     def format(self, record):\n         try:\n@@ -124,7 +119,7 @@ def format(self, record):\n         # result are so useless (and tornado is fond of using utf8-encoded\n         # byte strings wherever possible).\n         try:\n-            message = unicode(record.message)\n+            message = str(record.message)\n         except UnicodeDecodeError:\n             message = repr(record.message)\n \ndiff --git a/pyftpdlib/prefork.py b/pyftpdlib/prefork.py\nindex e6700a28..de97dee1 100644\n--- a/pyftpdlib/prefork.py\n+++ b/pyftpdlib/prefork.py\n@@ -5,6 +5,7 @@\n \"\"\"Process utils.\"\"\"\n \n import os\n+import random\n import sys\n import time\n from binascii import hexlify\n@@ -15,8 +16,6 @@\n except ImportError:\n     multiprocessing = None\n \n-from ._compat import InterruptedError\n-from ._compat import long\n from .log import logger\n \n \n@@ -39,15 +38,12 @@ def cpu_count():\n \n \n def _reseed_random():\n-    if 'random' not in sys.modules:\n-        return\n-    import random\n \n     # If os.urandom is available, this method does the same thing as\n     # random.seed.  If os.urandom is not available, we mix in the pid in\n     # addition to a timestamp.\n     try:\n-        seed = long(hexlify(os.urandom(16)), 16)\n+        seed = int(hexlify(os.urandom(16)), 16)\n     except NotImplementedError:\n         seed = int(time.time() * 1000) ^ os.getpid()\n     random.seed(seed)\ndiff --git a/pyftpdlib/servers.py b/pyftpdlib/servers.py\nindex 21101d8c..1a9015f8 100644\n--- a/pyftpdlib/servers.py\n+++ b/pyftpdlib/servers.py\n@@ -229,7 +229,6 @@ def serve_forever(\n         \"\"\"\n         log = handle_exit and blocking\n \n-        #\n         if worker_processes != 1 and os.name == 'posix':\n             if not blocking:\n                 raise ValueError(\n@@ -242,14 +241,12 @@ def serve_forever(\n             if log:\n                 self._log_start()\n \n-        #\n         proto = \"FTP+SSL\" if hasattr(self.handler, 'ssl_protocol') else \"FTP\"\n         logger.info(\n             \">>> starting %s server on %s:%s, pid=%i <<<\"\n             % (proto, self.address[0], self.address[1], os.getpid())\n         )\n \n-        #\n         if handle_exit:\n             try:\n                 self.ioloop.loop(timeout, blocking)\n@@ -397,7 +394,7 @@ def _loop(self, handler):\n             handler.ioloop = ioloop\n             try:\n                 handler.add_channel()\n-            except EnvironmentError as err:\n+            except OSError as err:\n                 if err.errno == errno.EBADF:\n                     # we might get here in case the other end quickly\n                     # disconnected (see test_quick_connect())\n@@ -449,7 +446,7 @@ def _loop(self, handler):\n                         for fd in list(ioloop.socket_map.keys()):\n                             try:\n                                 select.select([fd], [], [], 0)\n-                            except select.error:\n+                            except OSError:\n                                 try:\n                                     logger.info(\n                                         \"discarding broken socket %r\",\n@@ -524,9 +521,8 @@ def _terminate_task(self, t):\n                     # as the process hangs on kqueue.control() or\n                     # select.select(). Use SIGKILL instead.\n                     os.kill(t.pid, signal.SIGKILL)\n-            except OSError as err:\n-                if err.errno != errno.ESRCH:\n-                    raise\n+            except ProcessLookupError:\n+                pass\n \n     def _join_task(self, t):\n         logger.debug(\"join()ing task %r\" % t)\ndiff --git a/pyproject.toml b/pyproject.toml\nindex d4c7df57..e516d0fa 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -15,13 +15,13 @@ line-length = 79\n preview = true\n select = [\n     \"ALL\",  # to get a list of all values: `python3 -m ruff linter`\n+    \"PLR2044\",  # [*] Line with empty comment\n ]\n ignore = [\n     \"A\",  # flake8-builtins\n     \"ANN\",  # flake8-annotations\n     \"ARG\",  # flake8-unused-arguments\n     \"B007\",  # Loop control variable `x` not used within loop body\n-    \"B904\",  # Within an `except` clause, raise exceptions with `raise ... from err` (PYTHON2.7 COMPAT)\n     \"B904\",  # Use `raise from` to specify exception cause (PYTHON2.7 COMPAT)\n     \"BLE001\",  # Do not catch blind exception: `Exception`\n     \"C4\",  # flake8-comprehensions (PYTHON2.7 COMPAT)\n@@ -49,7 +49,6 @@ ignore = [\n     \"N818\",  # Exception name `FooBar` should be named with an Error suffix\n     \"PERF\",  # Perflint\n     \"PGH004\",  # Use specific rule codes when using `noqa`\n-    \"PLC0415\",  # `import` should be at the top-level of a file\n     \"PLC2701\",  # Private name import `_winreg`\n     \"PLR\",  # pylint\n     \"PLW\",  # pylint\n@@ -69,8 +68,6 @@ ignore = [\n     \"TRY003\",  # Avoid specifying long messages outside the exception class\n     \"TRY300\",  # Consider moving this statement to an `else` block\n     \"TRY301\",  # Abstract `raise` to an inner function\n-    \"UP010\",  # [*] Unnecessary `__future__` import `print_function` for target Python version (PYTHON2.7 COMPAT)\n-    \"UP024\",  # [*] Replace aliased errors with `OSError` (PYTHON2.7 COMPAT)\n     \"UP031\",  # [*] Use format specifiers instead of percent format\n ]\n \n@@ -94,7 +91,6 @@ omit = [\n ]\n exclude_lines = [\n     \"except ImportError:\",\n-    \"if PY3:\",\n     \"if __name__ == .__main__.:\",\n     \"if hasattr(select, 'devpoll'):\",\n     \"if hasattr(select, 'epoll'):\",\ndiff --git a/scripts/ftpbench b/scripts/ftpbench\nindex 36911841..ed6e4a90 100755\n--- a/scripts/ftpbench\n+++ b/scripts/ftpbench\n@@ -63,9 +63,6 @@ Example usages:\n #   300 concurrent clients (STOR 10.0M file)               9.74 secs    140.9M\n #   300 concurrent clients (QUIT)                          0.00 secs\n \n-from __future__ import division\n-from __future__ import print_function\n-\n import argparse\n import asynchat\n import asyncore\n@@ -99,15 +96,8 @@ SERVER_PROC = None\n TIMEOUT = None\n FILE_SIZE = \"10M\"\n SSL = False\n-PY3 = sys.version_info >= (3, 0)\n \n server_memory = []\n-# python >= 2.7.9\n-SSLWantReadError = getattr(ssl, \"SSLWantReadError\", object())\n-SSLWantWriteError = getattr(ssl, \"SSLWantWriteError\", object())\n-# python <= 2.7.8\n-SSL_ERROR_WANT_READ = getattr(ssl, \"SSL_ERROR_WANT_READ\", object())\n-SSL_ERROR_WANT_WRITE = getattr(ssl, \"SSL_ERROR_WANT_WRITE\", object())\n \n \n if not sys.stdout.isatty() or os.name != 'posix':\n@@ -412,15 +402,9 @@ def bench_multi(howmany):\n def handle_ssl_want_rw_errs():\n     try:\n         yield\n-    except (SSLWantReadError, SSLWantWriteError) as err:\n+    except (ssl.SSLWantReadError, ssl.SSLWantWriteError) as err:\n         if DEBUG:\n             print(err)\n-    except ssl.SSLError as err:\n-        if err.args[0] in (SSL_ERROR_WANT_READ, SSL_ERROR_WANT_WRITE):\n-            if DEBUG:\n-                print(err)\n-        else:\n-            raise\n \n \n class AsyncReader(asyncore.dispatcher):\n@@ -442,7 +426,7 @@ class AsyncReader(asyncore.dispatcher):\n         self.close()\n \n     def handle_error(self):\n-        raise\n+        raise  # noqa\n \n \n class AsyncWriter(asyncore.dispatcher):\n@@ -464,7 +448,7 @@ class AsyncWriter(asyncore.dispatcher):\n             self.handle_close()\n \n     def handle_error(self):\n-        raise\n+        raise  # noqa\n \n \n class AsyncQuit(asynchat.async_chat):\ndiff --git a/scripts/internal/generate_manifest.py b/scripts/internal/generate_manifest.py\nindex 88249a37..f2613c7c 100755\n--- a/scripts/internal/generate_manifest.py\n+++ b/scripts/internal/generate_manifest.py\n@@ -13,7 +13,7 @@\n \n \n SKIP_EXTS = ('.png', '.jpg', '.jpeg', '.svg')\n-SKIP_FILES = 'appveyor.yml'\n+SKIP_FILES = tuple()\n SKIP_PREFIXES = ('.ci/', '.github/')\n \n \ndiff --git a/scripts/internal/git_pre_commit.py b/scripts/internal/git_pre_commit.py\nindex 30331668..a7f3b1ec 100755\n--- a/scripts/internal/git_pre_commit.py\n+++ b/scripts/internal/git_pre_commit.py\n@@ -11,7 +11,6 @@\n install-git-hooks\".\n \"\"\"\n \n-from __future__ import print_function\n \n import os\n import shlex\n@@ -20,13 +19,12 @@\n \n \n PYTHON = sys.executable\n-PY3 = sys.version_info[0] >= 3\n THIS_SCRIPT = os.path.realpath(__file__)\n \n \n def term_supports_colors():\n     try:\n-        import curses\n+        import curses  # noqa: PLC0415\n \n         assert sys.stderr.isatty()\n         curses.setupterm()\n@@ -77,8 +75,7 @@ def sh(cmd):\n \n \n def open_text(path):\n-    kw = {\"encoding\": \"utf8\"} if PY3 else {}\n-    return open(path, **kw)\n+    return open(path, encoding=\"utf8\")\n \n \n def git_committed_files():\ndiff --git a/scripts/internal/winmake.py b/scripts/internal/winmake.py\nindex ee78907a..f8290155 100755\n--- a/scripts/internal/winmake.py\n+++ b/scripts/internal/winmake.py\n@@ -11,7 +11,6 @@\n that they should be deemed illegal!\n \"\"\"\n \n-from __future__ import print_function\n \n import argparse\n import atexit\n@@ -25,15 +24,12 @@\n import subprocess\n import sys\n import tempfile\n+from urllib.request import urlopen\n \n \n-APPVEYOR = bool(os.environ.get('APPVEYOR'))\n-PYTHON = sys.executable if APPVEYOR else os.getenv('PYTHON', sys.executable)\n+PYTHON = os.getenv('PYTHON', sys.executable)\n GET_PIP_URL = \"https://bootstrap.pypa.io/get-pip.py\"\n-PY3 = sys.version_info[0] >= 3\n-PYTEST_ARGS = \"-v --tb=native \"\n-if PY3:\n-    PYTEST_ARGS += \"-o \"\n+PYTEST_ARGS = \"-v --tb=native -o\"\n HERE = os.path.abspath(os.path.dirname(__file__))\n ROOT_DIR = os.path.realpath(os.path.join(HERE, \"..\", \"..\"))\n PYPY = '__pypy__' in sys.builtin_module_names\n@@ -47,17 +43,7 @@\n     \"wmi\",\n ]\n \n-if sys.version_info[:2] == (2, 7):\n-    DEPS.extend(\n-        [\n-            \"ipaddress\",\n-            \"mock\",\n-        ]\n-    )\n-\n _cmds = {}\n-if PY3:\n-    basestring = str\n \n GREEN = 2\n LIGHTBLUE = 3\n@@ -75,9 +61,8 @@ def safe_print(text, file=sys.stdout):\n     \"\"\"Prints a (unicode) string to the console, encoded depending on\n     the stdout/file encoding (eg. cp437 on Windows). This is to avoid\n     encoding errors in case of funky path names.\n-    Works with Python 2 and 3.\n     \"\"\"\n-    if not isinstance(text, basestring):\n+    if not isinstance(text, str):\n         return print(text, file=file)\n     try:\n         file.write(text)\n@@ -123,15 +108,6 @@ def sh(cmd, nolog=False):\n def rm(pattern, directory=False):\n     \"\"\"Recursively remove a file or dir by pattern.\"\"\"\n \n-    def safe_remove(path):\n-        try:\n-            os.remove(path)\n-        except OSError as err:\n-            if err.errno != errno.ENOENT:\n-                raise\n-        else:\n-            safe_print(\"rm %s\" % path)\n-\n     def safe_rmtree(path):\n         def onerror(fun, path, excinfo):\n             exc = excinfo[1]\n@@ -168,9 +144,8 @@ def onerror(fun, path, excinfo):\n def safe_remove(path):\n     try:\n         os.remove(path)\n-    except OSError as err:\n-        if err.errno != errno.ENOENT:\n-            raise\n+    except FileNotFoundError:\n+        pass\n     else:\n         safe_print(\"rm %s\" % path)\n \n@@ -216,11 +191,11 @@ def build():\n \n     cmd = [PYTHON, \"setup.py\", \"build\"]\n     # Print coloured warnings in real time.\n-    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n+    p = subprocess.Popen(\n+        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n+    )\n     try:\n-        for line in iter(p.stdout.readline, b''):\n-            if PY3:\n-                line = line.decode()\n+        for line in iter(p.stdout.readline, ''):\n             line = line.strip()\n             if 'warning' in line:\n                 win_colorprint(line, YELLOW)\n@@ -259,11 +234,6 @@ def install_pip():\n     try:\n         sh('%s -c \"import pip\"' % PYTHON)\n     except SystemExit:\n-        if PY3:\n-            from urllib.request import urlopen\n-        else:\n-            from urllib2 import urlopen\n-\n         if hasattr(ssl, '_create_unverified_context'):\n             ctx = ssl._create_unverified_context()\n         else:\n@@ -366,8 +336,7 @@ def setup_dev_env():\n def lint():\n     \"\"\"Run flake8 against all py files.\"\"\"\n     py_files = subprocess.check_output(\"git ls-files\")\n-    if PY3:\n-        py_files = py_files.decode()\n+    py_files = py_files.decode()\n     py_files = [x for x in py_files.split() if x.endswith('.py')]\n     py_files = ' '.join(py_files)\n     sh(\"%s -m flake8 %s\" % (PYTHON, py_files), nolog=True)\ndiff --git a/setup.py b/setup.py\nindex dee1f04b..09cce803 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -7,7 +7,6 @@\n $ python setup.py install\n \"\"\"\n \n-from __future__ import print_function\n \n import ast\n import os\n@@ -38,7 +37,7 @@ def get_version():\n \n def term_supports_colors():\n     try:\n-        import curses\n+        import curses  # noqa: PLC0415\n \n         assert sys.stderr.isatty()\n         curses.setupterm()\n@@ -66,14 +65,13 @@ def hilite(s, ok=True, bold=False):\n         return '\\x1b[%sm%s\\x1b[0m' % (';'.join(attr), s)\n \n \n-if sys.version_info < (2, 7):  # noqa\n-    sys.exit('python version not supported (< 2.7)')\n-\n-require_pysendfile = os.name == 'posix' and sys.version_info < (3, 3)\n+if sys.version_info[0] < 3:\n+    sys.exit(\n+        'Python 2 is no longer supported. Latest version is 1.5.10; use:\\n'\n+        'python3 -m pip install pyftpdlib==1.5.10'\n+    )\n \n extras_require = {'ssl': [\"PyOpenSSL\"]}\n-if require_pysendfile:\n-    extras_require.update({'sendfile': ['pysendfile']})\n \n VERSION = get_version()\n \n@@ -119,31 +117,10 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Topic :: System :: Filesystems',\n             'Programming Language :: Python',\n-            'Programming Language :: Python :: 2',\n             'Programming Language :: Python :: 3',\n         ],\n     )\n \n-    # suggest to install pysendfile\n-    if require_pysendfile:\n-        try:\n-            # os.sendfile() appeared in python 3.3\n-            # http://bugs.python.org/issue10882\n-            if not hasattr(os, 'sendfile'):\n-                # fallback on using third-party pysendfile module\n-                # https://github.com/giampaolo/pysendfile/\n-                import sendfile\n-\n-                if hasattr(sendfile, 'has_sf_hdtr'):  # old 1.2.4 version\n-                    raise ImportError\n-        except ImportError:\n-            msg = textwrap.dedent(\"\"\"\n-                'pysendfile' third-party module is not installed. This is not\n-                essential but it considerably speeds up file transfers.\n-                You can install it with 'pip install pysendfile'.\n-                More at: https://github.com/giampaolo/pysendfile\"\"\")\n-            print(hilite(msg, ok=False), file=sys.stderr)\n-\n     try:\n         from OpenSSL import SSL  # NOQA\n     except ImportError:\n", "instance_id": "giampaolo__pyftpdlib-635", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to drop Python 2.7 support from the `pyftpdlib` library, as it explicitly states the goal and provides download statistics to justify the decision (Python 2.7 represents only 2.25% of downloads). However, it lacks critical details about the specific requirements or constraints for dropping support, such as compatibility goals for Python 3 versions, potential backward compatibility concerns, or specific areas of the codebase that need attention (e.g., handling of Unicode, deprecated modules). There are no examples or test cases provided to validate the changes, and edge cases or migration challenges for users are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is significant, spanning multiple files (e.g., `setup.py`, `pyftpdlib/_compat.py`, `Makefile`, documentation files like `README.rst`, and various implementation files). It involves not just removing compatibility code but also updating build scripts, CI configurations (e.g., deleting `appveyor.yml`), and documentation, requiring a broad understanding of the codebase. Second, the technical concepts involved include Python 2/3 compatibility (e.g., handling Unicode vs. bytes, exception handling differences, and deprecated modules like `asyncore`), as well as build system modifications and dependency management (e.g., removing `pysendfile` dependency). While these concepts are not overly complex for an experienced developer, they require careful attention to ensure no regressions are introduced. Third, the changes impact the system's architecture to a moderate extent by simplifying the codebase (removing dual-version support) but also potentially affecting users still on Python 2.7, though no explicit error handling or edge cases are specified in the problem statement. The task does not involve advanced algorithms or system-level challenges, but the breadth of changes and the need to validate compatibility with various Python 3 versions add to the complexity. A score of 0.55 reflects a medium difficulty task that requires understanding multiple concepts and making coordinated changes across the codebase without being overly intricate or domain-specific.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove distutils\n**Describe the bug**\r\n`distutils` is deprecated and slated for removal in Python 3.12\r\n\r\nhttps://peps.python.org/pep-0632//#migration-advice\r\n\r\n**To Reproduce**\r\nRun with Python 3.12\r\n\r\n```bash\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.12.2, pytest-7.4.4, pluggy-1.4.0\r\nrootdir: /build/source\r\ncollected 63 items / 10 errors / 1 deselected / 62 selected                    \r\n\r\n==================================== ERRORS ====================================\r\n____________________ ERROR collecting test/test_backend.py _____________________\r\nImportError while importing test module '/build/source/test/test_backend.py'.\r\nHint: make sure your test modules/packages have valid Python names.\r\nTraceback:\r\n/nix/store/n3jf1lkdfakxskzsm4nlhss8mdgmcqhc-python3-3.12.2/lib/python3.12/importlib/__init__.py:90: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\ntest/test_backend.py:7: in <module>\r\n    from stone.ir import (\r\nstone/ir/__init__.py:1: in <module>\r\n    from .api import *  # noqa: F401,F403\r\nstone/ir/api.py:5: in <module>\r\n    from distutils.version import StrictVersion\r\nE   ModuleNotFoundError: No module named 'distutils'\r\n___________________ ERROR collecting test/test_js_client.py ____________________\r\n[...]\r\n```\r\n\n", "patch": "diff --git a/ez_setup.py b/ez_setup.py\nindex eb9b690e..722496a6 100644\n--- a/ez_setup.py\n+++ b/ez_setup.py\n@@ -23,7 +23,7 @@\n import tempfile\n import textwrap\n import zipfile\n-from distutils import log\n+from setuptools import log\n \n try:\n     from site import USER_SITE\ndiff --git a/requirements.txt b/requirements.txt\nindex 657f7a07..07605c18 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,2 +1,3 @@\n ply>= 3.4\n-six>= 1.12.0\n\\ No newline at end of file\n+six>= 1.12.0\n+packaging>=21.0\ndiff --git a/stone/ir/api.py b/stone/ir/api.py\nindex 654e004a..8cb74846 100644\n--- a/stone/ir/api.py\n+++ b/stone/ir/api.py\n@@ -1,6 +1,6 @@\n from collections import OrderedDict\n # See <https://github.com/PyCQA/pylint/issues/73>\n-from distutils.version import StrictVersion  # pylint: disable=deprecated-module\n+from packaging.version import Version\n \n from .data_types import (\n     doc_unwrap,\n@@ -34,7 +34,7 @@ class Api:\n     \"\"\"\n     def __init__(self, version):\n         # type: (str) -> None\n-        self.version = StrictVersion(version)\n+        self.version = Version(version)\n         self.namespaces = OrderedDict()  # type: NamespaceDict\n         self.route_schema = None  # type: typing.Optional[Struct]\n \n", "instance_id": "dropbox__stone-334", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the core issue: the deprecation of `distutils` in Python 3.12, which causes import errors in the codebase when running tests. It provides a reference to the relevant PEP (PEP 632) for migration advice and includes a reproducible error log showing the specific failure due to the missing `distutils` module. However, the statement lacks explicit details on the expected solution or migration path (e.g., which alternative library or module to use), and it does not mention any specific edge cases or compatibility concerns with other Python versions. While the goal is implied (replace `distutils` with a supported alternative), the absence of detailed requirements or constraints prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting a few files (`ez_setup.py`, `requirements.txt`, and `stone/ir/api.py`). The modifications involve straightforward replacements, such as swapping `distutils.version.StrictVersion` with `packaging.version.Version` and updating dependencies. There is no indication of deep architectural impact or complex interactions across the codebase.\n2. **Technical Concepts Required:** The problem requires basic familiarity with Python's packaging ecosystem, specifically understanding the deprecation of `distutils` and the use of alternatives like `packaging` for version handling. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce new error handling logic. However, there is a minor implicit concern about ensuring compatibility with different Python versions when using the `packaging` library, though this does not significantly increase complexity.\n4. **Overall Complexity:** The task is primarily a migration effort involving minimal logic changes and a small amount of code modification. It requires understanding the purpose of the replaced module and ensuring the alternative behaves similarly, which is a routine task for a developer familiar with Python.\n\nOverall, this is a straightforward bug fix with limited scope and complexity, suitable for a developer with basic to intermediate Python experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Drop support for Python 3.8\n### Summary:\r\nAs the Python community has moved towards supporting newer Python versions, it is becoming increasingly difficult to maintain compatibility with older versions like Python 3.8. \r\n\r\nGiven that Python 3.8 has reached its end of life (EOL) and the FlexGet project relies on dependencies and features that are better supported in later Python versions, it would be beneficial for the project to drop support for Python 3.8.\r\n\r\n### Rationale:\r\n**Security**: Python 3.8 is no longer receiving security updates, which could expose the project to potential vulnerabilities.\r\n\r\n**Maintenance**: Maintaining support for deprecated versions can be resource-intensive, and focusing on newer versions will allow us to take advantage of improvements in performance, security, and new features.\r\n\r\n**Future-proofing**: By supporting only actively maintained versions of Python, we ensure that FlexGet remains compatible with the latest advancements in the Python ecosystem.\n", "patch": "File: Flexget__Flexget-4078\nType: block\nName: \nLines: 522-546\nCode:\nif files_only and dirs_only:\n         if files_only and dirs_only:\n             logger.warning(\n             logger.warning(\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n \n \n        entries: List[Entry] = sftp.list_directories(\n        entries: list[Entry] = sftp.list_directories(\n             directories, recursive, get_size, files_only, dirs_only\n             directories, recursive, get_size, files_only, dirs_only\n         )\n         )\n         sftp.close()\n         sftp.close()\nindex 1ae5b9d0f1..13adfe3195 100644\nindex 1ae5b9d0f1..13adfe3195 100644\n from functools import partial\n from functools import partial\n from pathlib import Path, PurePath, PurePosixPath\n from pathlib import Path, PurePath, PurePosixPath\n from stat import S_ISLNK\n\nFile: Flexget__Flexget-4078\nType: block\nName: \nLines: 523-546\nCode:\nif files_only and dirs_only:\n             logger.warning(\n             logger.warning(\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n \n \n        entries: List[Entry] = sftp.list_directories(\n        entries: list[Entry] = sftp.list_directories(\n             directories, recursive, get_size, files_only, dirs_only\n             directories, recursive, get_size, files_only, dirs_only\n         )\n         )\n         sftp.close()\n         sftp.close()\nindex 1ae5b9d0f1..13adfe3195 100644\nindex 1ae5b9d0f1..13adfe3195 100644\n from functools import partial\n from functools import partial\n from pathlib import Path, PurePath, PurePosixPath\n from pathlib import Path, PurePath, PurePosixPath\n from stat import S_ISLNK\n\nFile: Flexget__Flexget-4078\nType: block\nName: on_task_input\nLines: 504-546\nCode:\ndef on_task_input(cls, task: Task, config: dict) -> List[Entry]:\n    def on_task_input(cls, task: Task, config: dict) -> list[Entry]:\n         \"\"\"\n         \"\"\"\n         Input task handler\n         Input task handler\n         \"\"\"\n         \"\"\"\n         get_size: bool = config['get_size']\n         get_size: bool = config['get_size']\n         socket_timeout_sec: int = config['socket_timeout_sec']\n         socket_timeout_sec: int = config['socket_timeout_sec']\n         connection_tries: int = config['connection_tries']\n         connection_tries: int = config['connection_tries']\n        directories: List[str] = []\n        directories: list[str] = []\n \n \n         if files_only and dirs_only:\n         if files_only and dirs_only:\n             logger.warning(\n             logger.warning(\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp_config: SftpConfig = task_config_to_sftp_config(config)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n         sftp: SftpClient = sftp_connect(sftp_config, socket_timeout_sec, connection_tries)\n \n \n        entries: List[Entry] = sftp.list_directories(\n        entries: list[Entry] = sftp.list_directories(\n             directories, recursive, get_size, files_only, dirs_only\n             directories, recursive, get_size, files_only, dirs_only\n         )\n         )\n         sftp.close()\n         sftp.close()\nindex 1ae5b9d0f1..13adfe3195 100644\nindex 1ae5b9d0f1..13adfe3195 100644\n from functools import partial\n from functools import partial\n from pathlib import Path, PurePath, PurePosixPath\n from pathlib import Path, PurePath, PurePosixPath\n from stat import S_ISLNK\n", "instance_id": "Flexget__Flexget-4078", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to drop support for Python 3.8 due to its end-of-life status, security concerns, and maintenance overhead. The rationale provided is logical and aligns with common practices in software development for phasing out deprecated dependencies. However, the statement lacks specific details about the expected changes in the codebase, such as which Python 3.8-specific features or compatibility code need to be removed or updated. Additionally, there are no explicit mentions of target Python versions to support after dropping 3.8, nor any specific constraints or edge cases to consider during the transition. The provided code changes seem unrelated to the problem statement (focused on SFTP functionality and type hints), which introduces some ambiguity about whether they are relevant to the task of dropping Python 3.8 support. Overall, while the goal is clear, the lack of actionable details and potential mismatch with the code changes prevents a perfect clarity score.", "difficulty_explanation": "The difficulty of dropping support for Python 3.8 is assessed as easy, falling in the 0.2-0.4 range. This task typically involves updating project metadata (e.g., setup.py or pyproject.toml) to reflect the minimum supported Python version, removing compatibility shims or workarounds specific to Python 3.8, and potentially updating CI/CD configurations to exclude testing on Python 3.8. The scope of code changes is generally limited to configuration files and documentation, with minimal impact on the core codebase unless there are significant dependencies or features tied exclusively to Python 3.8, which is not indicated in the problem statement. The technical concepts involved are straightforward, requiring basic knowledge of Python versioning, dependency management, and project configuration. However, there might be a need to understand the project's dependency tree to ensure no critical libraries are incompatible with newer Python versions, which adds a slight layer of complexity. Edge cases, such as handling users still on Python 3.8 or ensuring backward compatibility for certain features, are not explicitly mentioned but could require minor consideration. The provided code changes appear unrelated to the stated problem (focused on type hints and SFTP logic), so they do not factor into this difficulty assessment. Overall, this task is relatively simple for a developer with moderate experience in Python project maintenance.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Python 3.13\n### Summary\n\nUpdate Python versions support\n\n### Motivation\n\nWe need to reflect the current status of Python versions.\n\n### Proposed changes\n\nWe need to reflect the current status of Python versions:\r\n- Add compatibility with 3.13\r\n- Drop compatibility with 3.8\n", "patch": "File: vortico__flama-153\nType: block\nName: \nLines: 68-76\nCode:\nif sys.version_info < (3, 10):  # PORT: Remove when stop supporting 3.9 # pragma: no cover\n     from typing_extensions import ParamSpec, TypeGuard\n     from typing_extensions import ParamSpec, TypeGuard\n \n \n     while isinstance(obj, functools.partial):\n     while isinstance(obj, functools.partial):\n         obj = obj.func\n         obj = obj.func\n\nFile: vortico__flama-153\nType: block\nName: predict\nLines: 144-153\nCode:\ndef predict(self, x: t.List[t.List[t.Any]]) -> t.Any:\n        assert torch is not None, \"`torch` must be installed to use PyTorchModel.\"\n        if torch is None:  # noqa\n            raise exceptions.FrameworkNotInstalled(\"pytorch\")\n \n \n         try:\n         try:\n             return self.model(torch.Tensor(x)).tolist()\n             return self.model(torch.Tensor(x)).tolist()\n\nFile: vortico__flama-153\nType: block\nName: \nLines: 57-65\nCode:\nif sys.version_info < (3, 9):  # PORT: Remove when stop supporting 3.8 # pragma: no cover\n    import contextvars\n\n    async def to_thread(func, /, *args, **kwargs):\n        return await asyncio.get_running_loop().run_in_executor(\n            None, functools.partial(contextvars.copy_context().run, func, *args, **kwargs)\n        )\n\n    asyncio.to_thread = to_thread  # pyright: ignore\n", "instance_id": "vortico__flama-153", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to update Python version support by adding compatibility for Python 3.13 and dropping support for Python 3.8. The motivation and proposed changes are straightforward. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify how compatibility should be added for Python 3.13 (e.g., are there specific features or libraries to support?) or what specific issues arise from dropping Python 3.8 support beyond the code changes shown. Additionally, there are no examples or test cases provided to validate the changes, and edge cases or potential compatibility issues are not mentioned. Despite these gaps, the overall goal is understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The task involves updating Python version compatibility, which primarily requires modifying conditional checks for version numbers and removing or adjusting code blocks specific to older versions (e.g., Python 3.8 and 3.9). The code changes provided are limited to a few files and specific blocks, focusing on version-specific imports and utility functions like `asyncio.to_thread`. The scope of changes appears localized and does not suggest significant architectural impact or interaction with multiple modules beyond what is shown. \n\nFrom a technical concepts perspective, the problem requires basic knowledge of Python's `sys.version_info` for version checking, familiarity with conditional imports, and understanding of compatibility shims (e.g., `typing_extensions` and `contextvars`). These are relatively simple concepts for a developer familiar with Python. The code changes do not indicate complex algorithms, design patterns, or domain-specific knowledge.\n\nRegarding edge cases and error handling, the problem statement and code changes do not explicitly mention specific edge cases or new error conditions to handle. The modifications appear to be straightforward updates to version checks without introducing new error handling logic. However, there is a minor risk of unforeseen compatibility issues when adding support for a new Python version (3.13), but this is not reflected in the provided changes or statement as a significant concern.\n\nOverall, the task requires understanding some code logic and making simple modifications, aligning with a difficulty score of 0.25. It does not involve deep codebase understanding, complex refactoring, or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "SatInstance::to_dimacs_path slow (unbuffered)\nSome benchmarks on macOS (using a 23MB uncompressed CNF file):\r\n\r\nThis took \u2248 30 seconds:\r\n```\r\ninstance.to_dimacs_path(filename)?;\r\n```\r\n\r\nThis also took \u2248 30 seconds:\r\n```\r\nlet mut f = File::create(filename)?;\r\ninstance.to_dimacs(&mut f)?;\r\n```\r\n\r\nBut this was only \u2248 0.2 seconds:\r\n```\r\nlet f = File::create(filename)?;\r\nlet mut f = BufWriter::new(f);\r\ninstance.to_dimacs(&mut f)?;\r\n```\r\n\r\nIt looks like `to_dimacs_path` should wrap things in `BufWriter`; now it is apparently doing just `File::create` + `to_dimacs`, which can be very slow.\nrustsat_minisat::simp segmentation fault\nSample code:\r\n\r\n```\r\nuse rustsat::instances::SatInstance;\r\nuse rustsat::solvers::Solve;\r\nuse std::error::Error;\r\nuse std::result::Result;\r\n\r\nfn main() -> Result<(), Box<dyn Error>> {\r\n    let instance: SatInstance = SatInstance::from_dimacs_path(\"example.txt\")?;\r\n    let mut solver = rustsat_minisat::simp::Minisat::default();\r\n    solver.add_cnf(instance.as_cnf().0)?;\r\n    println!(\"solving...\");\r\n    let res = solver.solve()?;\r\n    println!(\"{res}\");\r\n    Ok(())\r\n}\r\n```\r\n\r\nInput file: [example.txt](https://github.com/chrjabs/rustsat/files/14796073/example.txt)\r\n\r\nThis seems to crash every time with a \"Segmentation fault\" right after it prints \"solving...\". Tried these systems, with the same result:\r\n- Intel iMac with macOS 12.7.4\r\n- Apple M3 MacBook Pro with macOS 14.4\r\n- some random Intel x86 Linux \r\n\r\nIt works fine if I change `rustsat_minisat::simp` to e.g. `rustsat_minisat::core` (or any other solver).\r\n\r\nVersions:\r\n- Rust 1.77.0\r\n- rustsat 0.4.3\r\n- rustsat-minisat 0.2.4\n", "patch": "diff --git a/data/minisat-segfault.cnf b/data/minisat-segfault.cnf\nnew file mode 100755\nindex 00000000..d68bd433\n--- /dev/null\n+++ b/data/minisat-segfault.cnf\n@@ -0,0 +1,465 @@\n+c this instance caused a segfault in minisat simp\n+c https://github.com/chrjabs/rustsat/issues/74\n+p cnf 156 462\n+2 0\n+3 0\n+4 0\n+5 0\n+7 0\n+8 0\n+9 0\n+10 0\n+12 0\n+13 0\n+14 0\n+15 0\n+21 22 23 24 0\n+-1 -21 17 0\n+-1 -22 18 0\n+-1 -23 19 0\n+-1 -24 20 0\n+25 26 27 28 0\n+-2 -25 17 0\n+-2 -26 18 0\n+-2 -27 19 0\n+-2 -28 20 0\n+29 30 31 32 0\n+-3 -29 17 0\n+-3 -30 18 0\n+-3 -31 19 0\n+-3 -32 20 0\n+33 34 35 36 0\n+-4 -33 17 0\n+-4 -34 18 0\n+-4 -35 19 0\n+-4 -36 20 0\n+41 42 43 44 0\n+-5 -41 37 0\n+-5 -42 38 0\n+-5 -43 39 0\n+-5 -44 40 0\n+45 46 47 48 0\n+-6 -45 37 0\n+-6 -46 38 0\n+-6 -47 39 0\n+-6 -48 40 0\n+49 50 51 52 0\n+-7 -49 37 0\n+-7 -50 38 0\n+-7 -51 39 0\n+-7 -52 40 0\n+53 54 55 56 0\n+-8 -53 37 0\n+-8 -54 38 0\n+-8 -55 39 0\n+-8 -56 40 0\n+61 62 63 64 0\n+-9 -61 57 0\n+-9 -62 58 0\n+-9 -63 59 0\n+-9 -64 60 0\n+65 66 67 68 0\n+-10 -65 57 0\n+-10 -66 58 0\n+-10 -67 59 0\n+-10 -68 60 0\n+69 70 71 72 0\n+-11 -69 57 0\n+-11 -70 58 0\n+-11 -71 59 0\n+-11 -72 60 0\n+73 74 75 76 0\n+-12 -73 57 0\n+-12 -74 58 0\n+-12 -75 59 0\n+-12 -76 60 0\n+81 82 83 84 0\n+-13 -81 77 0\n+-13 -82 78 0\n+-13 -83 79 0\n+-13 -84 80 0\n+85 86 87 88 0\n+-14 -85 77 0\n+-14 -86 78 0\n+-14 -87 79 0\n+-14 -88 80 0\n+89 90 91 92 0\n+-15 -89 77 0\n+-15 -90 78 0\n+-15 -91 79 0\n+-15 -92 80 0\n+93 94 95 96 0\n+-16 -93 77 0\n+-16 -94 78 0\n+-16 -95 79 0\n+-16 -96 80 0\n+-1 -21 -17 97 0\n+-1 -21 -18 98 0\n+-1 -21 -19 99 0\n+-1 -21 -20 100 0\n+-1 -22 -17 101 0\n+-1 -22 -18 102 0\n+-1 -22 -19 103 0\n+-1 -22 -20 104 0\n+-1 -23 -17 105 0\n+-1 -23 -18 106 0\n+-1 -23 -19 107 0\n+-1 -23 -20 108 0\n+-1 -24 -17 109 0\n+-1 -24 -18 110 0\n+-1 -24 -19 111 0\n+-1 -24 -20 112 0\n+-2 -25 -37 97 0\n+-2 -25 -38 98 0\n+-2 -25 -39 99 0\n+-2 -25 -40 100 0\n+-2 -26 -37 101 0\n+-2 -26 -38 102 0\n+-2 -26 -39 103 0\n+-2 -26 -40 104 0\n+-2 -27 -37 105 0\n+-2 -27 -38 106 0\n+-2 -27 -39 107 0\n+-2 -27 -40 108 0\n+-2 -28 -37 109 0\n+-2 -28 -38 110 0\n+-2 -28 -39 111 0\n+-2 -28 -40 112 0\n+-3 -29 -57 97 0\n+-3 -29 -58 98 0\n+-3 -29 -59 99 0\n+-3 -29 -60 100 0\n+-3 -30 -57 101 0\n+-3 -30 -58 102 0\n+-3 -30 -59 103 0\n+-3 -30 -60 104 0\n+-3 -31 -57 105 0\n+-3 -31 -58 106 0\n+-3 -31 -59 107 0\n+-3 -31 -60 108 0\n+-3 -32 -57 109 0\n+-3 -32 -58 110 0\n+-3 -32 -59 111 0\n+-3 -32 -60 112 0\n+-4 -33 -77 97 0\n+-4 -33 -78 98 0\n+-4 -33 -79 99 0\n+-4 -33 -80 100 0\n+-4 -34 -77 101 0\n+-4 -34 -78 102 0\n+-4 -34 -79 103 0\n+-4 -34 -80 104 0\n+-4 -35 -77 105 0\n+-4 -35 -78 106 0\n+-4 -35 -79 107 0\n+-4 -35 -80 108 0\n+-4 -36 -77 109 0\n+-4 -36 -78 110 0\n+-4 -36 -79 111 0\n+-4 -36 -80 112 0\n+-5 -41 -17 97 0\n+-5 -41 -18 98 0\n+-5 -41 -19 99 0\n+-5 -41 -20 100 0\n+-5 -42 -17 101 0\n+-5 -42 -18 102 0\n+-5 -42 -19 103 0\n+-5 -42 -20 104 0\n+-5 -43 -17 105 0\n+-5 -43 -18 106 0\n+-5 -43 -19 107 0\n+-5 -43 -20 108 0\n+-5 -44 -17 109 0\n+-5 -44 -18 110 0\n+-5 -44 -19 111 0\n+-5 -44 -20 112 0\n+-6 -45 -37 97 0\n+-6 -45 -38 98 0\n+-6 -45 -39 99 0\n+-6 -45 -40 100 0\n+-6 -46 -37 101 0\n+-6 -46 -38 102 0\n+-6 -46 -39 103 0\n+-6 -46 -40 104 0\n+-6 -47 -37 105 0\n+-6 -47 -38 106 0\n+-6 -47 -39 107 0\n+-6 -47 -40 108 0\n+-6 -48 -37 109 0\n+-6 -48 -38 110 0\n+-6 -48 -39 111 0\n+-6 -48 -40 112 0\n+-7 -49 -57 97 0\n+-7 -49 -58 98 0\n+-7 -49 -59 99 0\n+-7 -49 -60 100 0\n+-7 -50 -57 101 0\n+-7 -50 -58 102 0\n+-7 -50 -59 103 0\n+-7 -50 -60 104 0\n+-7 -51 -57 105 0\n+-7 -51 -58 106 0\n+-7 -51 -59 107 0\n+-7 -51 -60 108 0\n+-7 -52 -57 109 0\n+-7 -52 -58 110 0\n+-7 -52 -59 111 0\n+-7 -52 -60 112 0\n+-8 -53 -77 97 0\n+-8 -53 -78 98 0\n+-8 -53 -79 99 0\n+-8 -53 -80 100 0\n+-8 -54 -77 101 0\n+-8 -54 -78 102 0\n+-8 -54 -79 103 0\n+-8 -54 -80 104 0\n+-8 -55 -77 105 0\n+-8 -55 -78 106 0\n+-8 -55 -79 107 0\n+-8 -55 -80 108 0\n+-8 -56 -77 109 0\n+-8 -56 -78 110 0\n+-8 -56 -79 111 0\n+-8 -56 -80 112 0\n+-9 -61 -17 97 0\n+-9 -61 -18 98 0\n+-9 -61 -19 99 0\n+-9 -61 -20 100 0\n+-9 -62 -17 101 0\n+-9 -62 -18 102 0\n+-9 -62 -19 103 0\n+-9 -62 -20 104 0\n+-9 -63 -17 105 0\n+-9 -63 -18 106 0\n+-9 -63 -19 107 0\n+-9 -63 -20 108 0\n+-9 -64 -17 109 0\n+-9 -64 -18 110 0\n+-9 -64 -19 111 0\n+-9 -64 -20 112 0\n+-10 -65 -37 97 0\n+-10 -65 -38 98 0\n+-10 -65 -39 99 0\n+-10 -65 -40 100 0\n+-10 -66 -37 101 0\n+-10 -66 -38 102 0\n+-10 -66 -39 103 0\n+-10 -66 -40 104 0\n+-10 -67 -37 105 0\n+-10 -67 -38 106 0\n+-10 -67 -39 107 0\n+-10 -67 -40 108 0\n+-10 -68 -37 109 0\n+-10 -68 -38 110 0\n+-10 -68 -39 111 0\n+-10 -68 -40 112 0\n+-11 -69 -57 97 0\n+-11 -69 -58 98 0\n+-11 -69 -59 99 0\n+-11 -69 -60 100 0\n+-11 -70 -57 101 0\n+-11 -70 -58 102 0\n+-11 -70 -59 103 0\n+-11 -70 -60 104 0\n+-11 -71 -57 105 0\n+-11 -71 -58 106 0\n+-11 -71 -59 107 0\n+-11 -71 -60 108 0\n+-11 -72 -57 109 0\n+-11 -72 -58 110 0\n+-11 -72 -59 111 0\n+-11 -72 -60 112 0\n+-12 -73 -77 97 0\n+-12 -73 -78 98 0\n+-12 -73 -79 99 0\n+-12 -73 -80 100 0\n+-12 -74 -77 101 0\n+-12 -74 -78 102 0\n+-12 -74 -79 103 0\n+-12 -74 -80 104 0\n+-12 -75 -77 105 0\n+-12 -75 -78 106 0\n+-12 -75 -79 107 0\n+-12 -75 -80 108 0\n+-12 -76 -77 109 0\n+-12 -76 -78 110 0\n+-12 -76 -79 111 0\n+-12 -76 -80 112 0\n+-13 -81 -17 97 0\n+-13 -81 -18 98 0\n+-13 -81 -19 99 0\n+-13 -81 -20 100 0\n+-13 -82 -17 101 0\n+-13 -82 -18 102 0\n+-13 -82 -19 103 0\n+-13 -82 -20 104 0\n+-13 -83 -17 105 0\n+-13 -83 -18 106 0\n+-13 -83 -19 107 0\n+-13 -83 -20 108 0\n+-13 -84 -17 109 0\n+-13 -84 -18 110 0\n+-13 -84 -19 111 0\n+-13 -84 -20 112 0\n+-14 -85 -37 97 0\n+-14 -85 -38 98 0\n+-14 -85 -39 99 0\n+-14 -85 -40 100 0\n+-14 -86 -37 101 0\n+-14 -86 -38 102 0\n+-14 -86 -39 103 0\n+-14 -86 -40 104 0\n+-14 -87 -37 105 0\n+-14 -87 -38 106 0\n+-14 -87 -39 107 0\n+-14 -87 -40 108 0\n+-14 -88 -37 109 0\n+-14 -88 -38 110 0\n+-14 -88 -39 111 0\n+-14 -88 -40 112 0\n+-15 -89 -57 97 0\n+-15 -89 -58 98 0\n+-15 -89 -59 99 0\n+-15 -89 -60 100 0\n+-15 -90 -57 101 0\n+-15 -90 -58 102 0\n+-15 -90 -59 103 0\n+-15 -90 -60 104 0\n+-15 -91 -57 105 0\n+-15 -91 -58 106 0\n+-15 -91 -59 107 0\n+-15 -91 -60 108 0\n+-15 -92 -57 109 0\n+-15 -92 -58 110 0\n+-15 -92 -59 111 0\n+-15 -92 -60 112 0\n+-16 -93 -77 97 0\n+-16 -93 -78 98 0\n+-16 -93 -79 99 0\n+-16 -93 -80 100 0\n+-16 -94 -77 101 0\n+-16 -94 -78 102 0\n+-16 -94 -79 103 0\n+-16 -94 -80 104 0\n+-16 -95 -77 105 0\n+-16 -95 -78 106 0\n+-16 -95 -79 107 0\n+-16 -95 -80 108 0\n+-16 -96 -77 109 0\n+-16 -96 -78 110 0\n+-16 -96 -79 111 0\n+-16 -96 -80 112 0\n+115 116 0\n+-97 -115 113 0\n+-97 -116 114 0\n+117 118 0\n+-98 -117 113 0\n+-98 -118 114 0\n+119 120 0\n+-99 -119 113 0\n+-99 -120 114 0\n+121 122 0\n+-100 -121 113 0\n+-100 -122 114 0\n+125 126 0\n+-101 -125 123 0\n+-101 -126 124 0\n+127 128 0\n+-102 -127 123 0\n+-102 -128 124 0\n+129 130 0\n+-103 -129 123 0\n+-103 -130 124 0\n+131 132 0\n+-104 -131 123 0\n+-104 -132 124 0\n+135 136 0\n+-105 -135 133 0\n+-105 -136 134 0\n+137 138 0\n+-106 -137 133 0\n+-106 -138 134 0\n+139 140 0\n+-107 -139 133 0\n+-107 -140 134 0\n+141 142 0\n+-108 -141 133 0\n+-108 -142 134 0\n+145 146 0\n+-109 -145 143 0\n+-109 -146 144 0\n+147 148 0\n+-110 -147 143 0\n+-110 -148 144 0\n+149 150 0\n+-111 -149 143 0\n+-111 -150 144 0\n+151 152 0\n+-112 -151 143 0\n+-112 -152 144 0\n+-97 -115 -113 153 0\n+-97 -115 -114 154 0\n+-97 -116 -113 155 0\n+-97 -116 -114 156 0\n+-98 -117 -123 153 0\n+-98 -117 -124 154 0\n+-98 -118 -123 155 0\n+-98 -118 -124 156 0\n+-99 -119 -133 153 0\n+-99 -119 -134 154 0\n+-99 -120 -133 155 0\n+-99 -120 -134 156 0\n+-100 -121 -143 153 0\n+-100 -121 -144 154 0\n+-100 -122 -143 155 0\n+-100 -122 -144 156 0\n+-101 -125 -113 153 0\n+-101 -125 -114 154 0\n+-101 -126 -113 155 0\n+-101 -126 -114 156 0\n+-102 -127 -123 153 0\n+-102 -127 -124 154 0\n+-102 -128 -123 155 0\n+-102 -128 -124 156 0\n+-103 -129 -133 153 0\n+-103 -129 -134 154 0\n+-103 -130 -133 155 0\n+-103 -130 -134 156 0\n+-104 -131 -143 153 0\n+-104 -131 -144 154 0\n+-104 -132 -143 155 0\n+-104 -132 -144 156 0\n+-105 -135 -113 153 0\n+-105 -135 -114 154 0\n+-105 -136 -113 155 0\n+-105 -136 -114 156 0\n+-106 -137 -123 153 0\n+-106 -137 -124 154 0\n+-106 -138 -123 155 0\n+-106 -138 -124 156 0\n+-107 -139 -133 153 0\n+-107 -139 -134 154 0\n+-107 -140 -133 155 0\n+-107 -140 -134 156 0\n+-108 -141 -143 153 0\n+-108 -141 -144 154 0\n+-108 -142 -143 155 0\n+-108 -142 -144 156 0\n+-109 -145 -113 153 0\n+-109 -145 -114 154 0\n+-109 -146 -113 155 0\n+-109 -146 -114 156 0\n+-110 -147 -123 153 0\n+-110 -147 -124 154 0\n+-110 -148 -123 155 0\n+-110 -148 -124 156 0\n+-111 -149 -133 153 0\n+-111 -149 -134 154 0\n+-111 -150 -133 155 0\n+-111 -150 -134 156 0\n+-112 -151 -143 153 0\n+-112 -151 -144 154 0\n+-112 -152 -143 155 0\n+-112 -152 -144 156 0\n+-153 0\n+-156 0\ndiff --git a/ipasir/src/lib.rs b/ipasir/src/lib.rs\nindex ebe6d5a6..1970d20f 100644\n--- a/ipasir/src/lib.rs\n+++ b/ipasir/src/lib.rs\n@@ -98,7 +98,7 @@ impl Default for IpasirSolver<'_, '_> {\n \n impl IpasirSolver<'_, '_> {\n     fn get_core_assumps(&self, assumps: &[Lit]) -> Result<Vec<Lit>, InvalidApiReturn> {\n-        let mut core = Vec::new();\n+        let mut core = Vec::with_capacity(assumps.len());\n         core.reserve(assumps.len());\n         for a in assumps {\n             match unsafe { ffi::ipasir_failed(self.handle, a.to_ipasir()) } {\ndiff --git a/minisat/build.rs b/minisat/build.rs\nindex 8c3850d0..1d90a38f 100644\n--- a/minisat/build.rs\n+++ b/minisat/build.rs\n@@ -11,7 +11,7 @@ fn main() {\n     build(\n         \"https://github.com/chrjabs/minisat.git\",\n         \"master\",\n-        \"f91df3ab8f9eeaebe351f4a1890cf92831b8b636\",\n+        \"e168f6e72600f4b04769b0f3bbb7f89b1a200a67\",\n     );\n \n     let out_dir = env::var(\"OUT_DIR\").unwrap();\ndiff --git a/rustsat/src/bench.rs b/rustsat/src/bench.rs\nindex 478d5297..7bb79c5a 100644\n--- a/rustsat/src/bench.rs\n+++ b/rustsat/src/bench.rs\n@@ -154,3 +154,22 @@ mod pb_enc {\n         b.iter(|| build_full_ub::<DynamicPolyWatchdog>(&lits!()));\n     }\n }\n+\n+mod fio {\n+    extern crate test;\n+\n+    use test::Bencher;\n+\n+    use crate::instances::SatInstance;\n+\n+    fn read_write_dimacs() {\n+        let inst: SatInstance =\n+            SatInstance::from_dimacs_path(\"./data/minisat-segfault.cnf\").unwrap();\n+        inst.to_dimacs_path(\"/tmp/rustsat-test.cnf\").unwrap();\n+    }\n+\n+    #[bench]\n+    fn read_write(b: &mut Bencher) {\n+        b.iter(|| read_write_dimacs());\n+    }\n+}\ndiff --git a/rustsat/src/instances/fio.rs b/rustsat/src/instances/fio.rs\nindex 1fa11360..3ee2cc9f 100644\n--- a/rustsat/src/instances/fio.rs\n+++ b/rustsat/src/instances/fio.rs\n@@ -43,24 +43,26 @@ pub(crate) fn open_compressed_uncompressed_write<P: AsRef<Path>>(\n     path: P,\n ) -> Result<Box<dyn io::Write>, io::Error> {\n     let path = path.as_ref();\n-    let raw_reader = File::create(path)?;\n+    let raw_writer = File::create(path)?;\n     #[cfg(feature = \"compression\")]\n     if let Some(ext) = path.extension() {\n         if ext.eq_ignore_ascii_case(std::ffi::OsStr::new(\"bz2\")) {\n-            return Ok(Box::new(bzip2::write::BzEncoder::new(\n-                raw_reader,\n+            return Ok(Box::new(io::BufWriter::new(bzip2::write::BzEncoder::new(\n+                raw_writer,\n                 bzip2::Compression::fast(),\n-            )));\n+            ))));\n         }\n         if ext.eq_ignore_ascii_case(std::ffi::OsStr::new(\"gz\")) {\n-            return Ok(Box::new(flate2::write::GzEncoder::new(\n-                raw_reader,\n+            return Ok(Box::new(io::BufWriter::new(flate2::write::GzEncoder::new(\n+                raw_writer,\n                 flate2::Compression::fast(),\n-            )));\n+            ))));\n         }\n         if ext.eq_ignore_ascii_case(std::ffi::OsStr::new(\"xz\")) {\n-            return Ok(Box::new(xz2::write::XzEncoder::new(raw_reader, 1)));\n+            return Ok(Box::new(io::BufWriter::new(xz2::write::XzEncoder::new(\n+                raw_writer, 1,\n+            ))));\n         }\n     }\n-    Ok(Box::new(raw_reader))\n+    Ok(Box::new(io::BufWriter::new(raw_writer)))\n }\ndiff --git a/rustsat/src/instances/multiopt.rs b/rustsat/src/instances/multiopt.rs\nindex 41ca4a00..53ed0aa3 100644\n--- a/rustsat/src/instances/multiopt.rs\n+++ b/rustsat/src/instances/multiopt.rs\n@@ -178,6 +178,10 @@ impl<VM: ManageVars> MultiOptInstance<VM> {\n     }\n \n     /// Writes the instance to a DIMACS MCNF file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_dimacs_path<P: AsRef<Path>>(self, path: P) -> Result<(), io::Error> {\n         let mut writer = fio::open_compressed_uncompressed_write(path)?;\n         self.to_dimacs(&mut writer)\n@@ -211,6 +215,10 @@ impl<VM: ManageVars> MultiOptInstance<VM> {\n     }\n \n     /// Writes the instance to an OPB file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_opb_path<P: AsRef<Path>>(\n         self,\n         path: P,\ndiff --git a/rustsat/src/instances/opt.rs b/rustsat/src/instances/opt.rs\nindex 6bfb2608..84bbd64c 100644\n--- a/rustsat/src/instances/opt.rs\n+++ b/rustsat/src/instances/opt.rs\n@@ -963,6 +963,10 @@ impl<VM: ManageVars> OptInstance<VM> {\n     }\n \n     /// Writes the instance to a DIMACS WCNF file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_dimacs_path<P: AsRef<Path>>(self, path: P) -> Result<(), io::Error> {\n         let mut writer = fio::open_compressed_uncompressed_write(path)?;\n         self.to_dimacs(&mut writer)\n@@ -996,6 +1000,10 @@ impl<VM: ManageVars> OptInstance<VM> {\n     }\n \n     /// Writes the instance to an OPB file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_opb_path<P: AsRef<Path>>(\n         self,\n         path: P,\ndiff --git a/rustsat/src/instances/sat.rs b/rustsat/src/instances/sat.rs\nindex 244930aa..3a475205 100644\n--- a/rustsat/src/instances/sat.rs\n+++ b/rustsat/src/instances/sat.rs\n@@ -710,6 +710,10 @@ impl<VM: ManageVars> SatInstance<VM> {\n     }\n \n     /// Writes the instance to a DIMACS CNF file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_dimacs_path<P: AsRef<Path>>(self, path: P) -> Result<(), io::Error> {\n         let mut writer = fio::open_compressed_uncompressed_write(path)?;\n         self.to_dimacs(&mut writer)\n@@ -742,6 +746,10 @@ impl<VM: ManageVars> SatInstance<VM> {\n     }\n \n     /// Writes the instance to an OPB file at a path\n+    ///\n+    /// # Performance\n+    ///\n+    /// For performance, consider using a [`std::io::BufWriter`] instance.\n     pub fn to_opb_path<P: AsRef<Path>>(\n         self,\n         path: P,\n", "instance_id": "chrjabs__rustsat-75", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear and addresses two distinct issues: a performance problem with `SatInstance::to_dimacs_path` due to unbuffered I/O operations and a segmentation fault in `rustsat_minisat::simp` with a specific input file. The performance issue is well-documented with benchmark results and a clear suggestion for improvement (using `BufWriter`). The segmentation fault issue includes a reproducible example, sample code, input file, and details about the environments where the issue occurs. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or constraints for the performance fix (e.g., acceptable performance thresholds), and for the segmentation fault, it lacks details on the root cause or specific expectations for the fix (e.g., whether it\u2019s a library bug or a usage issue). Additionally, edge cases or constraints for the input file causing the crash are not fully specified. Overall, the statement is valid and clear but misses some minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty score of 0.55 places this problem in the medium range, reflecting a mix of straightforward and moderately complex tasks across the two issues described. For the performance issue with `SatInstance::to_dimacs_path`, the fix is relatively simple\u2014wrapping file writes with `BufWriter` as shown in the code changes. This requires basic understanding of Rust\u2019s I/O system (e.g., `std::io::BufWriter`) and involves minimal code changes in a single module (`fio.rs`), with no significant architectural impact. This part is easy (0.2-0.4). However, the segmentation fault in `rustsat_minisat::simp` elevates the difficulty. Diagnosing and fixing a segfault requires deeper knowledge of Rust\u2019s FFI (Foreign Function Interface) with C libraries (since MiniSat is likely a C/C++ library wrapped by Rust), memory safety, and debugging across multiple platforms (macOS and Linux). The code changes include updating the MiniSat commit hash, suggesting a potential upstream fix or workaround, and minor capacity optimization in `ipasir/src/lib.rs`, but the root cause analysis and ensuring stability across platforms add complexity. This involves understanding interactions between Rust and C code, potentially multiple files, and domain-specific knowledge of SAT solvers. Additionally, handling the segfault requires considering edge cases in input files (as provided in the CNF file) and ensuring robust error handling, though the problem statement does not explicitly demand extensive edge case coverage beyond the provided example. The combined scope of changes is moderate, impacting a few files without major architectural redesign. Balancing the simplicity of the I/O fix with the complexity of the segfault, a score of 0.55 reflects a medium difficulty task that requires understanding multiple concepts (I/O buffering, FFI, SAT solver internals) and making targeted but non-trivial modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Migrate from `tox` to `nox`\nIt is more of an opinionated choice because both of them are quite robust tools. Some arguments in favour of `nox` and how to configure it are listed the Scientific Python guides: https://learn.scientific-python.org/development/guides/tasks/#task-runners\r\n\r\nWe don't need to follow them fully, but in general, I have found them to be pretty good and reasonable guidelines :)\r\n\r\nNote that `nox` provides a `tox-to-nox` CLI as well to ease the migration, but it isn't too difficult to do so by hand either, and it shouldn't take too long.\n", "patch": "diff --git a/.github/workflows/check.yml b/.github/workflows/check.yml\nindex bdb5aa1c..dab30d1d 100644\n--- a/.github/workflows/check.yml\n+++ b/.github/workflows/check.yml\n@@ -1,4 +1,4 @@\n-name: Checks\n+name: Style and package checks\n \n on:\n   pull_request:\n@@ -7,27 +7,31 @@ on:\n   push:\n     branches:\n     - master\n+  workflow_dispatch:\n \n env:\n-  PIP_DISABLE_PIP_VERSION_CHECK: '1'\n-  PY_COLORS: '1'\n+  PIP_DISABLE_PIP_VERSION_CHECK: \"1\"\n+  FORCE_COLOR: \"3\"\n+\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n+  cancel-in-progress: true\n \n jobs:\n   check:\n+    name: ${{ matrix.env }}\n     runs-on: ubuntu-latest\n     strategy:\n       fail-fast: false\n       matrix:\n-        env:\n-        #- ruff\n-        - package\n+        session:\n+      # - lint\n+        - validate-package\n     steps:\n     - uses: actions/checkout@v4\n     - uses: actions/setup-python@v5.1.1\n-      with:\n-        python-version: '3.10'\n-    - name: Install build tools\n-      run: |\n-        python -m pip install tox wheel\n+\n+    - uses: yezz123/setup-uv@v4\n+\n     - name: Run ${{ matrix.env }}\n-      run: python -m tox -e ${{ matrix.env }}\n+      run: uvx nox -s ${{ matrix.env }}\ndiff --git a/.gitignore b/.gitignore\nindex cb7301dc..47a5ac31 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -28,6 +28,7 @@ pip-delete-this-directory.txt\n # Unit test / coverage reports\n htmlcov/\n .tox/\n+.nox/\n .coverage\n .coverage.*\n .cache\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex ded36ae3..7b87093c 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -1,38 +1,34 @@\n # Contributing\n \n-Use Tox to run tests and linting, e.g.\n+Use [Nox](https://nox.thea.codes/en/stable/) to run tests and linting, e.g.,\n \n ```shell\n-pip install tox\n+pip install nox\n ```\n \n+`nox` will run all checks in an isolated virtual environment with Autograd and its dependencies, including its optional dependencies, installed.\n+\n ## Run tests, linting, packaging checks\n \n-```shell\n-tox list                # list all Tox environments\n-tox run -e ruff         # run code style checks\n-tox run -e py           # run tests with your default Python\n-tox run -e package      # verify packaging\n-tox                     # run all Tox environments\n-```\n+| Command                   | Description                                                                                                                                                     |\n+| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n+| `nox --list`              | Lists all available Nox sessions, including selected ones                                                                                                       |\n+| `nox -s lint`             | Runs code style checks with pre-commit and pre-commit hooks as listed in `.pre-commit-config.yaml`. Accepts posargs to pass additional arguments to the linter. |\n+| `nox -s tests`            | Runs tests with your default Python interpreter. Accepts posargs to pass additional arguments and configuration to `pytest`.                                    |\n+| `nox -s nightly-tests`    | Similar to `nox -s tests`, except that it runs tests with nightly versions of dependencies (NumPy, SciPy, etc.).                                                |\n+| `nox -s validate-package` | Builds a source distribution and a wheel using `pypa/build` and checks the package with `twine` in strict mode.                                                 |\n+| `nox`                     | Runs all selected sessions, as listed in `nox.options.sessions` in `noxfile.py`.                                                                                |\n+\n+Additionally, `nox` supports tags to run specific sessions, e.g., `nox --tags tests` runs all sessions tagged with `tests`.\n \n Make sure all tests pass before you push your changes to GitHub.\n GH Actions will run the tests across all supported Python versions.\n \n-## Using arguments (reformat, upload package, help)\n-\n-You can use additional arguments for the tools called by Tox by\n-separating them from the Tox arguments by a double-dash `--`, e.g.\n-\n-```shell\n-tox run -e ruff -- autograd/core.py --show-source\n-tox run -e ruff -- autograd/core.py --fix\n-```\n+## Using positional arguments (reformat, upload package, help)\n \n-```shell\n-tox run -e package -- upload\n-```\n+You can use additional arguments for the tools (`pytest`, `pre-commit`, etc.) called by Nox by\n+separating them from the Nox arguments by a double-hyphen `--`, e.g.,\n \n-```shell\n-tox run -e py -- --help\n-```\n+- `nox -s tests -- --tests/test_tuple.py` runs just the tests listed `tests/test_tuple.py`.\n+- `nox -s lint -- --fix` runs the linter with the `--fix` flag.\n+- and so on.\ndiff --git a/examples/tanh.py b/examples/tanh.py\nindex 3d984994..fb5a5757 100644\n--- a/examples/tanh.py\n+++ b/examples/tanh.py\n@@ -26,13 +26,13 @@ def tanh(x):\n ### Plotting\n plt.figure(figsize=(12, 8))\n x = np.linspace(-7, 7, 700)\n-plt.plot(x, tanh(x), label='tanh(x)')\n+plt.plot(x, tanh(x), label=\"tanh(x)\")\n plt.plot(x, egrad(tanh)(x), label=\"1st derivative\")\n plt.plot(x, egrad(egrad(tanh))(x), label=\"2nd derivative\")\n plt.plot(x, egrad(egrad(egrad(tanh)))(x), label=\"3rd derivative\")\n plt.plot(x, egrad(egrad(egrad(egrad(tanh))))(x), label=\"4th derivative\")\n-plt.xlabel('x')\n-plt.ylabel('y')\n+plt.xlabel(\"x\")\n+plt.ylabel(\"y\")\n plt.ylim(-5, 5)\n plt.yticks(np.arange(-5, 6, 1))\n plt.legend()\ndiff --git a/noxfile.py b/noxfile.py\nnew file mode 100644\nindex 00000000..e19f9d80\n--- /dev/null\n+++ b/noxfile.py\n@@ -0,0 +1,56 @@\n+import platform\n+\n+import nox\n+\n+NIGHTLY_INDEX_URL = \"https://pypi.anaconda.org/scientific-python-nightly-wheels/simple\"\n+UV_NIGHTLY_ENV_VARS = {\n+    \"UV_INDEX_URL\": NIGHTLY_INDEX_URL,\n+    \"UV_PRERELEASE\": \"allow\",\n+    \"UV_INDEX_STRATEGY\": \"first-index\",\n+    \"UV_NO_CACHE\": \"true\",\n+}\n+\n+nox.needs_version = \">=2024.4.15\"\n+nox.options.default_venv_backend = \"uv|virtualenv\"\n+nox.options.reuse_existing_virtualenvs = False\n+nox.options.error_on_external_run = True\n+# nox.options.sessions = [\"lint\", \"validate-package\", \"tests\"]\n+nox.options.sessions = [\"tests\"]\n+\n+\n+@nox.session(name=\"validate-package\")\n+def check(session):\n+    \"\"\"Build source distribution, wheel, and check their metadata\"\"\"\n+    session.install(\"build\", \"twine\", silent=False)\n+    session.run(\"python\", \"-m\", \"build\")\n+    session.run(\"twine\", \"check\", \"--strict\", \"dist/*\")\n+\n+\n+@nox.session(name=\"tests\", tags=[\"tests\"])\n+def run_tests(session):\n+    \"\"\"Run unit tests and generate a coverage report\"\"\"\n+    # SciPy doesn't have wheels on PyPy\n+    if platform.python_implementation() == \"PyPy\":\n+        session.install(\"-e\", \".[test]\", silent=False)\n+    else:\n+        session.install(\"-e\", \".[test,scipy]\", silent=False)\n+    session.run(\"pytest\", \"--cov=autograd\", \"--cov-report=xml\", \"--cov-append\", *session.posargs)\n+\n+\n+@nox.session(name=\"lint\", reuse_venv=True)\n+def ruff(session):\n+    \"\"\"Lightning-fast linting for Python\"\"\"\n+    session.install(\"pre-commit\", silent=False)\n+    session.run(\"pre-commit\", \"run\", \"--all-files\", \"--show-diff-on-failure\")\n+\n+\n+@nox.session(name=\"nightly-tests\", tags=[\"tests\"])\n+def run_nightly_tests(session):\n+    \"\"\"Run tests against nightly versions of dependencies\"\"\"\n+    session.install(\"-e\", \".[test]\", silent=False)\n+    # SciPy doesn't have wheels on PyPy\n+    if platform.python_implementation() == \"PyPy\":\n+        session.install(\"numpy\", \"--upgrade\", silent=False, env=UV_NIGHTLY_ENV_VARS)\n+    else:\n+        session.install(\"numpy\", \"scipy\", \"--upgrade\", silent=False, env=UV_NIGHTLY_ENV_VARS)\n+    session.run(\"pytest\", \"--cov=autograd\", \"--cov-report=xml\", \"--cov-append\", *session.posargs)\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 34739047..9108f56f 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -55,6 +55,11 @@ Source = \"https://github.com/HIPS/autograd\"\n scipy = [\n   \"scipy\",\n ]\n+test = [\n+  \"pytest\",\n+  \"pytest-cov\",\n+  \"pytest-xdist\",\n+]\n \n [tool.coverage.run]\n source = [\"autograd\"]\n@@ -63,7 +68,9 @@ source = [\"autograd\"]\n show_missing = true\n \n [tool.pytest.ini_options]\n-addopts = \"--color=yes --junitxml=junit-report.xml\"\n+required_plugins = [\"pytest-cov\", \"pytest-xdist\"]\n+# TODO: generate HTML report, upload to CodeCov\n+addopts = \"--color=yes -sra -n auto --cov=autograd --cov-report=xml --cov-report=term\"\n \n [tool.ruff]\n extend-exclude = []\ndiff --git a/tox.ini b/tox.ini\ndeleted file mode 100644\nindex b2043666..00000000\n--- a/tox.ini\n+++ /dev/null\n@@ -1,60 +0,0 @@\n-# Tox (https://tox.wiki/) - run tests in isolation using virtualenv.\n-# Also contains config settings for tools that don't look into pyproject.toml.\n-\n-# TODO: Migrate to tool.hatch.run or noxfile.py\n-\n-[tox]\n-envlist =\n-    ruff\n-    py2{7}{,-scipy}\n-    pypy2{7}{,-scipy}\n-    py3{5,6,7,8,9,10,11}{,-scipy}\n-    pypy3{8,9,10}{,-scipy}\n-    package\n-    clean\n-requires = virtualenv<20.22.0\n-\n-[testenv]\n-description = Unit tests and test coverage\n-deps =\n-    py27: mock\n-    pypy27: mock\n-    coverage[toml]\n-    pytest\n-extras =\n-    scipy: scipy\n-commands =\n-    coverage run -m pytest {posargs}\n-    coverage xml\n-    coverage report\n-\n-[testenv:clean]\n-description = Clean up bytecode and build artifacts\n-skip_install = true\n-deps = pyclean\n-commands = pyclean {posargs:. --debris --erase junit-report.xml --yes}\n-\n-[testenv:ruff]\n-description = Lightning-fast linting for Python\n-skip_install = true\n-deps = ruff\n-commands = ruff check {posargs:.}  # TODO: Fix style failures\n-\n-[testenv:package]\n-description = Build package and check metadata (or upload package)\n-skip_install = true\n-deps =\n-    build\n-    twine\n-commands =\n-    python -m build\n-    twine {posargs:check --strict} dist/*\n-passenv =\n-    TWINE_USERNAME\n-    TWINE_PASSWORD\n-    TWINE_REPOSITORY_URL\n-\n-[pytest]\n-addopts =\n-    --color=yes\n-    --junitxml=junit-report.xml\n", "instance_id": "HIPS__autograd-632", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to migrate from `tox` to `nox` as a task runner for a Python project. It provides a rationale for the switch by referencing the Scientific Python guides and mentions a helpful tool (`tox-to-nox` CLI) for easing the migration. However, it lacks specific details about the expected configuration or behavior of `nox` in the context of the project. For instance, it does not explicitly define which tasks or environments need to be migrated, nor does it specify any constraints or requirements for compatibility with existing workflows. Additionally, there are no examples or detailed steps provided for the migration process, which could lead to minor ambiguities for someone unfamiliar with `nox` or the project's setup. Despite these gaps, the overall goal is understandable, and the provided code changes offer context for the intended outcome, making it \"Mostly Clear.\"", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes involve multiple files (e.g., `.github/workflows/check.yml`, `CONTRIBUTING.md`, `pyproject.toml`, `noxfile.py`, and deletion of `tox.ini`), but the modifications are relatively straightforward. They primarily consist of replacing `tox` commands and configurations with `nox` equivalents, updating documentation, and setting up a new `noxfile.py` for task definitions. The changes do not significantly impact the system's architecture or require deep understanding of complex interactions within the codebase. The overall amount of code change is moderate, focused on configuration and tooling rather than core logic.\n\n2. **Number of Technical Concepts:** The task requires familiarity with Python task runners (`tox` and `nox`), basic understanding of CI/CD workflows (GitHub Actions), and configuration file formats (e.g., TOML, YAML). Additionally, knowledge of Python packaging tools (`build`, `twine`) and testing frameworks (`pytest`) is necessary. However, these concepts are not particularly complex for a developer with moderate experience in Python development and DevOps practices. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error conditions to handle during the migration. The code changes also do not introduce significant error handling logic beyond what is standard in task runner configurations. The focus is on replacing one tool with another while maintaining existing functionality, so edge cases are minimal and likely related to ensuring compatibility with different Python versions or environments, which are already addressed in the updated configurations.\n\n4. **Overall Complexity:** This task is relatively straightforward for someone with experience in Python tooling and configuration management. It involves understanding the differences between `tox` and `nox`, translating existing environments and tasks to the new tool, and updating related documentation and CI scripts. While it requires attention to detail to ensure no functionality is lost during migration, it does not pose significant technical challenges or require deep architectural changes.\n\nGiven these considerations, a difficulty score of 0.35 reflects an \"Easy\" task that requires some understanding of tooling and configuration logic but does not demand advanced skills or extensive modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: only anonymous permissions checked properly\n**Bug**\r\n\r\nThe default AWS credentials file `~/.aws/credentials` is not being read, thus only checks for anonymous permissions are being made. `AuthUsers` in the output will likely always be `[]` - meaning \"no permissions\". This can be a false negative.\r\n\r\n**Reproduction**\r\n\r\n- Create a bucket in AWS S3 with no READ permissions (except implicit permission to the owner)\r\n- Configure credentials with `aws configure`\r\n- Run `s3scanner -bucket your-bucket-here`\r\n- Observe the output `INFO exists    | your-bucket-here | us-east-1 | AuthUsers: [] | AllUsers: []`\r\n\r\n**Expected output**\r\n\r\n`INFO exists    | s3scanner-private | us-east-1 | AuthUsers: [READ, READACP] | AllUsers: []`\r\n\r\nThank you to Twitter user `@thaivd98` for reporting this.\nbug: only anonymous permissions checked properly\n**Bug**\r\n\r\nThe default AWS credentials file `~/.aws/credentials` is not being read, thus only checks for anonymous permissions are being made. `AuthUsers` in the output will likely always be `[]` - meaning \"no permissions\". This can be a false negative.\r\n\r\n**Reproduction**\r\n\r\n- Create a bucket in AWS S3 with no READ permissions (except implicit permission to the owner)\r\n- Configure credentials with `aws configure`\r\n- Run `s3scanner -bucket your-bucket-here`\r\n- Observe the output `INFO exists    | your-bucket-here | us-east-1 | AuthUsers: [] | AllUsers: []`\r\n\r\n**Expected output**\r\n\r\n`INFO exists    | s3scanner-private | us-east-1 | AuthUsers: [READ, READACP] | AllUsers: []`\r\n\r\nThank you to Twitter user `@thaivd98` for reporting this.\n", "patch": "diff --git a/.dev/docker-compose.yml b/.dev/docker-compose.yml\nindex d4b019c..3047061 100644\n--- a/.dev/docker-compose.yml\n+++ b/.dev/docker-compose.yml\n@@ -23,6 +23,7 @@ services:\n     container_name: app_dev\n     volumes:\n       - $PWD:/app\n+      - ~/.aws:/root/.aws\n     entrypoint: [\"tail\", \"-f\", \"/dev/null\"]\n     profiles: [\"default\"]\n \ndiff --git a/bucket/bucket.go b/bucket/bucket.go\nindex a529e86..26e07a0 100644\n--- a/bucket/bucket.go\n+++ b/bucket/bucket.go\n@@ -24,15 +24,8 @@ var PermissionAllowed = uint8(1)\n var PermissionDenied = uint8(0)\n var PermissionUnknown = uint8(2)\n \n-// var bucketReIP = regexp.MustCompile(`^[0-9]{1-3}\\.[0-9]{1-3}\\.[0-9]{1-3}\\.[0-9]{1-3}$`)\n var bucketRe = regexp.MustCompile(`[^.\\-a-z0-9]`)\n \n-// Pattern from https://blogs.easydynamics.com/2016/10/24/aws-s3-bucket-name-validation-regex/\n-// Missing:\n-// No xn-- prefix\n-// No -s3alias suffix\n-// https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html\n-\n type Bucket struct {\n \t//gorm.Model\n \tID                uint           `gorm:\"primarykey\" json:\",omitempty\"`\n@@ -192,9 +185,12 @@ func (bucket *Bucket) ParseAclOutputv2(aclOutput *s3.GetBucketAclOutput) error {\n \tif aclOutput.Owner.DisplayName != nil {\n \t\tbucket.OwnerDisplayName = *aclOutput.Owner.DisplayName\n \t}\n+\t// Since we can read the permissions, there should be no unknowns. Set all to denied, then read each grant and\n+\t// set the corresponding permission to allowed.\n+\tbucket.DenyAll()\n \n \tfor _, b := range aclOutput.Grants {\n-\t\tif b.Grantee == groups.AllUsersv2 {\n+\t\tif b.Grantee != nil && b.Grantee.Type == \"Group\" && *b.Grantee.URI == groups.AllUsersGroup {\n \t\t\tswitch b.Permission {\n \t\t\tcase types.PermissionRead:\n \t\t\t\tbucket.PermAllUsersRead = PermissionAllowed\n@@ -210,7 +206,7 @@ func (bucket *Bucket) ParseAclOutputv2(aclOutput *s3.GetBucketAclOutput) error {\n \t\t\t\tbreak\n \t\t\t}\n \t\t}\n-\t\tif b.Grantee == groups.AuthenticatedUsersv2 {\n+\t\tif b.Grantee != nil && b.Grantee.Type == \"Group\" && *b.Grantee.URI == groups.AuthUsersGroup {\n \t\t\tswitch b.Permission {\n \t\t\tcase types.PermissionRead:\n \t\t\t\tbucket.PermAuthUsersRead = PermissionAllowed\n@@ -277,3 +273,16 @@ func IsValidS3BucketName(bucketName string) bool {\n \n \treturn true\n }\n+\n+func (b *Bucket) DenyAll() {\n+\tb.PermAllUsersRead = PermissionDenied\n+\tb.PermAllUsersWrite = PermissionDenied\n+\tb.PermAllUsersReadACL = PermissionDenied\n+\tb.PermAllUsersWriteACL = PermissionDenied\n+\tb.PermAllUsersFullControl = PermissionDenied\n+\tb.PermAuthUsersRead = PermissionDenied\n+\tb.PermAuthUsersWrite = PermissionDenied\n+\tb.PermAuthUsersReadACL = PermissionDenied\n+\tb.PermAuthUsersWriteACL = PermissionDenied\n+\tb.PermAuthUsersFullControl = PermissionDenied\n+}\ndiff --git a/groups/groups.go b/groups/groups.go\nindex efce294..1606431 100644\n--- a/groups/groups.go\n+++ b/groups/groups.go\n@@ -5,13 +5,16 @@ import (\n \t\"github.com/aws/aws-sdk-go-v2/service/s3/types\"\n )\n \n+const AuthUsersGroup = \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\"\n+const AllUsersGroup = \"http://acs.amazonaws.com/groups/global/AllUsers\"\n+\n var AllUsersv2 = &types.Grantee{\n \tType: types.TypeGroup,\n-\tURI:  aws.String(\"http://acs.amazonaws.com/groups/global/AllUsers\")}\n+\tURI:  aws.String(AllUsersGroup)}\n \n var AuthenticatedUsersv2 = &types.Grantee{\n \tType: types.TypeGroup,\n-\tURI:  aws.String(\"http://acs.amazonaws.com/groups/global/AllUsers\")}\n+\tURI:  aws.String(AuthUsersGroup)}\n \n-const ALL_USERS_URI = \"uri=http://acs.amazonaws.com/groups/global/AllUsers\"\n-const AUTH_USERS_URI = \"uri=http://acs.amazonaws.com/groups/global/AuthenticatedUsers\"\n+const AllUsersUri = \"uri=\" + AllUsersGroup\n+const AuthUsersUri = \"uri=\" + AuthUsersGroup\ndiff --git a/permission/permission.go b/permission/permission.go\nindex 4af319b..a210ca1 100644\n--- a/permission/permission.go\n+++ b/permission/permission.go\n@@ -46,35 +46,35 @@ func CheckPermWriteAcl(svc *s3.Client, b *Bucket) (bool, error) {\n \n \tgrants := map[string][]string{}\n \tif b.PermAuthUsersFullControl == PermissionAllowed {\n-\t\tgrants[\"FULL_CONTROL\"] = append(grants[\"FULL_CONTROL\"], AUTH_USERS_URI)\n+\t\tgrants[\"FULL_CONTROL\"] = append(grants[\"FULL_CONTROL\"], AuthUsersUri)\n \t}\n \tif b.PermAuthUsersWriteACL == PermissionAllowed {\n-\t\tgrants[\"WRITE_ACP\"] = append(grants[\"WRITE_ACP\"], AUTH_USERS_URI)\n+\t\tgrants[\"WRITE_ACP\"] = append(grants[\"WRITE_ACP\"], AuthUsersUri)\n \t}\n \tif b.PermAuthUsersWrite == PermissionAllowed {\n-\t\tgrants[\"WRITE\"] = append(grants[\"WRITE\"], AUTH_USERS_URI)\n+\t\tgrants[\"WRITE\"] = append(grants[\"WRITE\"], AuthUsersUri)\n \t}\n \tif b.PermAuthUsersReadACL == PermissionAllowed {\n-\t\tgrants[\"READ_ACP\"] = append(grants[\"READ_ACP\"], AUTH_USERS_URI)\n+\t\tgrants[\"READ_ACP\"] = append(grants[\"READ_ACP\"], AuthUsersUri)\n \t}\n \tif b.PermAuthUsersRead == PermissionAllowed {\n-\t\tgrants[\"READ\"] = append(grants[\"READ\"], AUTH_USERS_URI)\n+\t\tgrants[\"READ\"] = append(grants[\"READ\"], AuthUsersUri)\n \t}\n \n \tif b.PermAllUsersFullControl == PermissionAllowed {\n-\t\tgrants[\"FULL_CONTROL\"] = append(grants[\"FULL_CONTROL\"], ALL_USERS_URI)\n+\t\tgrants[\"FULL_CONTROL\"] = append(grants[\"FULL_CONTROL\"], AllUsersUri)\n \t}\n \tif b.PermAllUsersWriteACL == PermissionAllowed {\n-\t\tgrants[\"WRITE_ACP\"] = append(grants[\"WRITE_ACP\"], ALL_USERS_URI)\n+\t\tgrants[\"WRITE_ACP\"] = append(grants[\"WRITE_ACP\"], AllUsersUri)\n \t}\n \tif b.PermAllUsersWrite == PermissionAllowed {\n-\t\tgrants[\"WRITE\"] = append(grants[\"WRITE\"], ALL_USERS_URI)\n+\t\tgrants[\"WRITE\"] = append(grants[\"WRITE\"], AllUsersUri)\n \t}\n \tif b.PermAllUsersReadACL == PermissionAllowed {\n-\t\tgrants[\"READ_ACP\"] = append(grants[\"READ_ACP\"], ALL_USERS_URI)\n+\t\tgrants[\"READ_ACP\"] = append(grants[\"READ_ACP\"], AllUsersUri)\n \t}\n \tif b.PermAllUsersRead == PermissionAllowed {\n-\t\tgrants[\"READ\"] = append(grants[\"READ\"], ALL_USERS_URI)\n+\t\tgrants[\"READ\"] = append(grants[\"READ\"], AllUsersUri)\n \t}\n \n \t_, err := svc.PutBucketAcl(context.TODO(), &s3.PutBucketAclInput{\ndiff --git a/provider/aws.go b/provider/aws.go\nindex 9d10009..2cd9877 100644\n--- a/provider/aws.go\n+++ b/provider/aws.go\n@@ -3,9 +3,13 @@ package provider\n import (\n \t\"context\"\n \t\"errors\"\n+\t\"fmt\"\n+\t\"github.com/aws/aws-sdk-go-v2/aws\"\n+\t\"github.com/aws/aws-sdk-go-v2/feature/ec2/imds\"\n+\t\"github.com/sa7mon/s3scanner/permission\"\n \t\"net/http\"\n+\t\"time\"\n \n-\t\"github.com/aws/aws-sdk-go-v2/aws\"\n \tawshttp \"github.com/aws/aws-sdk-go-v2/aws/transport/http\"\n \t\"github.com/aws/aws-sdk-go-v2/config\"\n \t\"github.com/aws/aws-sdk-go-v2/feature/s3/manager\"\n@@ -16,8 +20,9 @@ import (\n )\n \n type providerAWS struct {\n-\texistsClient *s3.Client\n-\tclients      *clientmap.ClientMap\n+\texistsClient   *s3.Client\n+\tclients        *clientmap.ClientMap\n+\thasCredentials bool\n }\n \n func (a *providerAWS) BucketExists(b *bucket.Bucket) (*bucket.Bucket, error) {\n@@ -53,16 +58,27 @@ func (a *providerAWS) BucketExists(b *bucket.Bucket) (*bucket.Bucket, error) {\n }\n \n func (a *providerAWS) Scan(b *bucket.Bucket, doDestructiveChecks bool) error {\n-\tclient, err := a.getRegionClient(b.Region)\n-\tif err != nil {\n-\t\treturn err\n+\tanonClient, anonErr := a.getRegionClient(b.Region, false)\n+\tif anonErr != nil {\n+\t\treturn anonErr\n \t}\n \n-\treturn checkPermissions(client, b, doDestructiveChecks)\n+\tif a.hasCredentials {\n+\t\tauthClient, authClientErr := a.getRegionClient(b.Region, true)\n+\t\tif authClientErr != nil {\n+\t\t\treturn authClientErr\n+\t\t}\n+\t\treturn checkPermissionsWithAuth(anonClient, authClient, b, doDestructiveChecks)\n+\t}\n+\treturn checkPermissionsWithAuth(anonClient, nil, b, doDestructiveChecks)\n }\n \n func (a *providerAWS) Enumerate(b *bucket.Bucket) error {\n-\tclient, err := a.getRegionClient(b.Region)\n+\tuseCreds := false\n+\tif b.PermAuthUsersRead == bucket.PermissionAllowed {\n+\t\tuseCreds = true\n+\t}\n+\tclient, err := a.getRegionClient(b.Region, useCreds)\n \tif err != nil {\n \t\treturn err\n \t}\n@@ -76,19 +92,23 @@ func (a *providerAWS) Enumerate(b *bucket.Bucket) error {\n \n func NewProviderAWS() (*providerAWS, error) {\n \tpa := new(providerAWS)\n-\tclient, err := pa.newAnonClientNoRegion()\n+\tclient, err := pa.newAnonClient(\"us-east-1\")\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \tpa.existsClient = client\n \n \t// Seed the clients map with a common region\n-\tusEastClient, usErr := pa.newClient(\"us-east-1\")\n+\tusEastClient, usErr := pa.newClient(\"us-east-1\", nil)\n \tif usErr != nil {\n \t\treturn nil, usErr\n \t}\n+\n+\t// check if the user has properly configured credentials for scanning\n+\tclientHasCreds := ClientHasCredentials(usEastClient)\n+\tpa.hasCredentials = clientHasCreds\n \tpa.clients = clientmap.New()\n-\tpa.clients.Set(\"us-east-1\", usEastClient)\n+\tpa.clients.Set(\"us-east-1\", clientHasCreds, usEastClient)\n \treturn pa, nil\n }\n \n@@ -105,56 +125,141 @@ func (*providerAWS) Name() string {\n \treturn \"aws\"\n }\n \n-func (*providerAWS) newAnonClientNoRegion() (*s3.Client, error) {\n+func (*providerAWS) newAnonClient(region string) (*s3.Client, error) {\n \tcfg, err := config.LoadDefaultConfig(\n \t\tcontext.TODO(),\n-\t\tconfig.WithDefaultRegion(\"us-west-2\"),\n-\t\tconfig.WithCredentialsProvider(aws.AnonymousCredentials{}),\n \t\tconfig.WithHTTPClient(&http.Client{Transport: &http.Transport{\n \t\t\tProxy: http.ProxyFromEnvironment,\n \t\t}}),\n+\t\tconfig.WithRegion(region),\n+\t\tconfig.WithCredentialsProvider(aws.AnonymousCredentials{}),\n \t)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\tcfg.Credentials = nil\n-\ts3ClientNoRegion := s3.NewFromConfig(cfg, func(o *s3.Options) {\n-\t\to.UsePathStyle = false\n-\t})\n-\n-\treturn s3ClientNoRegion, nil\n+\treturn s3.NewFromConfig(cfg), nil\n }\n \n-func (a *providerAWS) newClient(region string) (*s3.Client, error) {\n-\tcfg, err := config.LoadDefaultConfig(\n-\t\tcontext.TODO(),\n-\t\tconfig.WithRegion(region),\n-\t\tconfig.WithCredentialsProvider(aws.AnonymousCredentials{}),\n+func (a *providerAWS) newClient(region string, profile *string) (*s3.Client, error) {\n+\tlogFields := log.Fields{\"profile\": profile, \"method\": \"aws.newClient\", \"region\": region}\n+\n+\tconfigOpts := []func(*config.LoadOptions) error{\n \t\tconfig.WithHTTPClient(&http.Client{Transport: &http.Transport{\n \t\t\tProxy: http.ProxyFromEnvironment,\n-\t\t}}))\n+\t\t}}),\n+\t\tconfig.WithRegion(region),\n+\t\tconfig.WithEC2IMDSClientEnableState(imds.ClientDisabled), // Otherwise we wait 4 seconds to IMDSv2 to timeout\n+\t}\n+\tif profile != nil {\n+\t\tconfigOpts = append(configOpts, config.WithSharedConfigProfile(*profile))\n+\t}\n \n+\tcfg, err := config.LoadDefaultConfig(\n+\t\tcontext.TODO(),\n+\t\tconfigOpts...,\n+\t)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\tcfg.Credentials = nil\n+\thasCreds, accessKeyID := HasCredentials(cfg)\n+\tif hasCreds {\n+\t\tlog.WithFields(logFields).Debugf(\"using access key ID: %s\", accessKeyID)\n+\t} else {\n+\t\tlog.WithFields(logFields).Debugf(\"no credentials found\")\n+\t}\n+\n \treturn s3.NewFromConfig(cfg), nil\n }\n \n // TODO: This method is copied from providerLinode\n-func (a *providerAWS) getRegionClient(region string) (*s3.Client, error) {\n-\tc := a.clients.Get(region)\n+func (a *providerAWS) getRegionClient(region string, useCreds bool) (*s3.Client, error) {\n+\tc := a.clients.Get(region, useCreds)\n \tif c != nil {\n \t\treturn c, nil\n \t}\n \n \t// No client for this region yet - create one\n-\tc, err := a.newClient(region)\n+\tvar newClient *s3.Client\n+\tvar newClientErr error\n+\tif useCreds {\n+\t\tnewClient, newClientErr = a.newClient(region, nil)\n+\t} else {\n+\t\tnewClient, newClientErr = a.newAnonClient(region)\n+\t}\n+\tif newClientErr != nil {\n+\t\treturn nil, newClientErr\n+\t}\n+\ta.clients.Set(region, useCreds, newClient)\n+\treturn newClient, nil\n+}\n+\n+func checkPermissionsWithAuth(anonClient *s3.Client, authClient *s3.Client, b *bucket.Bucket, doDestructiveChecks bool) error {\n+\t/*\n+\t\t// 1. Check if b exists\n+\t\t// 2. Check for READ_ACP\n+\t\t// 3. If FullControl is allowed for either AllUsers or AuthorizedUsers, skip the remainder of those tests\n+\t\t// 4. Check for READ\n+\t\t// 5. If doing destructive checks:\n+\t\t// 5a. Check for Write\n+\t\t// 5b. Check for WriteACP\n+\t*/\n+\n+\tb.DateScanned = time.Now()\n+\n+\t// Check for anon READ_ACP permission. If allowed, exit\n+\tanonReadACL, err := permission.CheckPermReadACL(anonClient, b)\n \tif err != nil {\n-\t\treturn nil, err\n+\t\treturn fmt.Errorf(\"error occurred while checking for anon ReadACL: %v\", err.Error())\n+\t}\n+\tb.PermAllUsersReadACL = bucket.Permission(anonReadACL)\n+\tif b.PermAllUsersReadACL == bucket.PermissionAllowed {\n+\t\treturn nil\n \t}\n-\ta.clients.Set(region, c)\n-\treturn c, nil\n+\n+\t// Check for auth READ_ACP permission. If allowed, exit\n+\tif authClient != nil {\n+\t\tauthReadACL, authAclErr := permission.CheckPermReadACL(authClient, b)\n+\t\tif authAclErr != nil {\n+\t\t\treturn fmt.Errorf(\"error occured while checking for auth ReadACL: %v\", authAclErr.Error())\n+\t\t}\n+\t\tb.PermAuthUsersReadACL = bucket.Permission(authReadACL)\n+\t\tif b.PermAuthUsersReadACL == bucket.PermissionAllowed {\n+\t\t\treturn nil\n+\t\t}\n+\t}\n+\n+\t// Check for anon READ\n+\tcanRead, err := permission.CheckPermRead(anonClient, b)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"error occured while checking for anon READ: %v\", err.Error())\n+\t}\n+\tb.PermAllUsersRead = bucket.Permission(canRead)\n+\n+\t// Check for auth READ\n+\tif authClient != nil {\n+\t\tauthCanRead, authReadErr := permission.CheckPermRead(authClient, b)\n+\t\tif authReadErr != nil {\n+\t\t\treturn fmt.Errorf(\"error occured while checking for auth READ: %v\", authReadErr.Error())\n+\t\t}\n+\t\tb.PermAuthUsersRead = bucket.Permission(authCanRead)\n+\t}\n+\n+\tif doDestructiveChecks {\n+\t\t// Check for WRITE permission\n+\t\tpermWrite, writeErr := permission.CheckPermWrite(anonClient, b)\n+\t\tif writeErr != nil {\n+\t\t\treturn fmt.Errorf(\"%v | error occured while checking for WRITE: %v\", b.Name, writeErr.Error())\n+\t\t}\n+\t\tb.PermAllUsersWrite = bucket.Permission(permWrite)\n+\n+\t\t// Check for WRITE_ACP permission\n+\t\tpermWriteAcl, writeAclErr := permission.CheckPermWriteAcl(anonClient, b)\n+\t\tif writeAclErr != nil {\n+\t\t\treturn fmt.Errorf(\"error occured while checking for WriteACL: %v\", writeAclErr.Error())\n+\t\t}\n+\t\tb.PermAllUsersWriteACL = bucket.Permission(permWriteAcl)\n+\t}\n+\treturn nil\n }\ndiff --git a/provider/clientmap/clientmap.go b/provider/clientmap/clientmap.go\nindex dced224..d7afdd5 100644\n--- a/provider/clientmap/clientmap.go\n+++ b/provider/clientmap/clientmap.go\n@@ -5,37 +5,42 @@ import (\n \t\"sync\"\n )\n \n+type ClientKey struct {\n+\tRegion      string\n+\tCredentials bool\n+}\n+\n type ClientMap struct {\n \tsync.Mutex\n-\tinner map[string]*s3.Client\n+\tinner map[ClientKey]*s3.Client\n }\n \n func New() *ClientMap {\n \treturn &ClientMap{\n \t\tMutex: sync.Mutex{},\n-\t\tinner: make(map[string]*s3.Client),\n+\t\tinner: make(map[ClientKey]*s3.Client),\n \t}\n }\n \n func WithCapacity(cap int) *ClientMap {\n \treturn &ClientMap{\n \t\tMutex: sync.Mutex{},\n-\t\tinner: make(map[string]*s3.Client, cap),\n+\t\tinner: make(map[ClientKey]*s3.Client, cap),\n \t}\n }\n \n-func (m *ClientMap) Get(key string) *s3.Client {\n+func (m *ClientMap) Get(region string, credentials bool) *s3.Client {\n \tm.Lock()\n \tdefer m.Unlock()\n-\tif v, ok := m.inner[key]; ok {\n+\tif v, ok := m.inner[ClientKey{Region: region, Credentials: credentials}]; ok {\n \t\treturn v\n \t}\n \treturn nil\n }\n \n-func (m *ClientMap) Set(key string, value *s3.Client) {\n+func (m *ClientMap) Set(region string, credentials bool, value *s3.Client) {\n \tm.Lock()\n-\tm.inner[key] = value\n+\tm.inner[ClientKey{Region: region, Credentials: credentials}] = value\n \tm.Unlock()\n }\n \n@@ -45,10 +50,10 @@ func (m *ClientMap) Len() int {\n \treturn len(m.inner)\n }\n \n-func (m *ClientMap) Each(fn func(region string, client *s3.Client)) {\n+func (m *ClientMap) Each(fn func(region string, credentials bool, client *s3.Client)) {\n \tm.Lock()\n-\tfor region, client := range m.inner {\n-\t\tfn(region, client)\n+\tfor key, client := range m.inner {\n+\t\tfn(key.Region, key.Credentials, client)\n \t}\n \tm.Unlock()\n }\ndiff --git a/provider/credentials.go b/provider/credentials.go\nnew file mode 100644\nindex 0000000..abe1468\n--- /dev/null\n+++ b/provider/credentials.go\n@@ -0,0 +1,42 @@\n+package provider\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"github.com/aws/aws-sdk-go-v2/aws\"\n+\t\"github.com/aws/aws-sdk-go-v2/service/s3\"\n+\t\"github.com/aws/smithy-go\"\n+\tlog \"github.com/sirupsen/logrus\"\n+)\n+\n+// TODO: If user explicitly set profile name and we don't find creds, probably blow up\n+// returns:\n+//   - bool    - if there are credentials configured\n+//   - string - AccessKeyID if credentials loaded\n+func HasCredentials(cfg aws.Config) (bool, string) {\n+\tcredentials, credsErr := cfg.Credentials.Retrieve(context.TODO())\n+\tif credsErr != nil {\n+\t\tvar oe *smithy.OperationError\n+\t\tif errors.As(credsErr, &oe) {\n+\t\t\tif !(oe.ServiceID == \"ec2imds\" && oe.OperationName == \"GetMetadata\") {\n+\t\t\t\tlog.WithFields(log.Fields{\"method\": \"provider.HasCredentials\"}).Error(oe.Error())\n+\t\t\t}\n+\t\t\treturn false, \"\"\n+\t\t}\n+\t}\n+\treturn true, credentials.AccessKeyID\n+}\n+\n+func ClientHasCredentials(client *s3.Client) bool {\n+\t_, credsErr := client.Options().Credentials.Retrieve(context.TODO())\n+\tif credsErr != nil {\n+\t\tvar oe *smithy.OperationError\n+\t\tif errors.As(credsErr, &oe) {\n+\t\t\tif !(oe.ServiceID == \"ec2imds\" && oe.OperationName == \"GetMetadata\") {\n+\t\t\t\tlog.WithFields(log.Fields{\"method\": \"provider.ClientHasCredentials\"}).Error(oe.Error())\n+\t\t\t}\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\treturn true\n+}\ndiff --git a/provider/custom.go b/provider/custom.go\nindex fd1ef22..0f7b5fe 100644\n--- a/provider/custom.go\n+++ b/provider/custom.go\n@@ -68,7 +68,7 @@ func (cp CustomProvider) Enumerate(b *bucket.Bucket) error {\n }\n \n func (cp *CustomProvider) getRegionClient(region string) *s3.Client {\n-\treturn cp.clients.Get(region)\n+\treturn cp.clients.Get(region, false)\n }\n \n /*\n@@ -104,7 +104,7 @@ func (cp *CustomProvider) newClients() (*clientmap.ClientMap, error) {\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n-\t\tclients.Set(r, client)\n+\t\tclients.Set(r, false, client)\n \t}\n \n \treturn clients, nil\ndiff --git a/provider/digitalocean.go b/provider/digitalocean.go\nindex 7260102..15d6e4f 100644\n--- a/provider/digitalocean.go\n+++ b/provider/digitalocean.go\n@@ -66,14 +66,14 @@ func (pdo *providerDO) newClients() (*clientmap.ClientMap, error) {\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n-\t\tclients.Set(r, client)\n+\t\tclients.Set(r, false, client)\n \t}\n \n \treturn clients, nil\n }\n \n func (pdo *providerDO) getRegionClient(region string) *s3.Client {\n-\treturn pdo.clients.Get(region)\n+\treturn pdo.clients.Get(region, false)\n }\n \n func NewProviderDO() (*providerDO, error) {\ndiff --git a/provider/dreamhost.go b/provider/dreamhost.go\nindex 2cea22c..d5d71a4 100644\n--- a/provider/dreamhost.go\n+++ b/provider/dreamhost.go\n@@ -58,7 +58,7 @@ func (p ProviderDreamhost) Scan(bucket *bucket.Bucket, doDestructiveChecks bool)\n }\n \n func (p ProviderDreamhost) getRegionClient(region string) *s3.Client {\n-\treturn p.clients.Get(region)\n+\treturn p.clients.Get(region, false)\n }\n \n func (p ProviderDreamhost) Enumerate(b *bucket.Bucket) error {\n@@ -81,7 +81,7 @@ func (p *ProviderDreamhost) newClients() (*clientmap.ClientMap, error) {\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n-\t\tclients.Set(r, client)\n+\t\tclients.Set(r, false, client)\n \t}\n \n \treturn clients, nil\ndiff --git a/provider/gcp.go b/provider/gcp.go\nindex 2a89a51..2fb6961 100644\n--- a/provider/gcp.go\n+++ b/provider/gcp.go\n@@ -33,7 +33,7 @@ func (g GCP) BucketExists(b *bucket.Bucket) (*bucket.Bucket, error) {\n \t\treturn nil, errors.New(\"invalid bucket name\")\n \t}\n \tclients := clientmap.New()\n-\tclients.Set(\"default\", g.client)\n+\tclients.Set(\"default\", false, g.client)\n \texists, region, err := bucketExists(clients, b)\n \tif err != nil {\n \t\treturn b, err\ndiff --git a/provider/linode.go b/provider/linode.go\nindex 7cb4ad8..0232427 100644\n--- a/provider/linode.go\n+++ b/provider/linode.go\n@@ -25,7 +25,7 @@ func NewProviderLinode() (*providerLinode, error) {\n }\n \n func (pl *providerLinode) getRegionClient(region string) *s3.Client {\n-\treturn pl.clients.Get(region)\n+\treturn pl.clients.Get(region, false)\n }\n \n func (pl *providerLinode) BucketExists(b *bucket.Bucket) (*bucket.Bucket, error) {\n@@ -64,7 +64,7 @@ func (pl *providerLinode) newClients() (*clientmap.ClientMap, error) {\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n-\t\tclients.Set(r, client)\n+\t\tclients.Set(r, false, client)\n \t}\n \n \treturn clients, nil\ndiff --git a/provider/providers.go b/provider/providers.go\nindex 329fc2c..a2d4cb5 100644\n--- a/provider/providers.go\n+++ b/provider/providers.go\n@@ -212,7 +212,7 @@ func bucketExists(clients *clientmap.ClientMap, b *bucket.Bucket) (bool, string,\n \tresults := make(chan bucketCheckResult, clients.Len())\n \te := make(chan error, 1)\n \n-\tclients.Each(func(region string, client *s3.Client) {\n+\tclients.Each(func(region string, credentials bool, client *s3.Client) {\n \t\tgo func(bucketName string, client *s3.Client, region string) {\n \t\t\tlogFields := log.Fields{\n \t\t\t\t\"bucket_name\": b.Name,\ndiff --git a/provider/scaleway.go b/provider/scaleway.go\nindex d077f69..4c3c944 100644\n--- a/provider/scaleway.go\n+++ b/provider/scaleway.go\n@@ -29,14 +29,14 @@ func (sc *providerScaleway) newClients() (*clientmap.ClientMap, error) {\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n-\t\tclients.Set(r, client)\n+\t\tclients.Set(r, false, client)\n \t}\n \n \treturn clients, nil\n }\n \n func (sc *providerScaleway) Scan(b *bucket.Bucket, doDestructiveChecks bool) error {\n-\tclient := sc.clients.Get(b.Region)\n+\tclient := sc.clients.Get(b.Region, false)\n \treturn checkPermissions(client, b, doDestructiveChecks)\n }\n \n@@ -73,7 +73,7 @@ func (sc *providerScaleway) Enumerate(b *bucket.Bucket) error {\n \t\treturn errors.New(\"bucket might not exist\")\n \t}\n \n-\tclient := sc.clients.Get(b.Region)\n+\tclient := sc.clients.Get(b.Region, false)\n \tenumErr := enumerateListObjectsV2(client, b)\n \tif enumErr != nil {\n \t\treturn enumErr\n", "instance_id": "sa7mon__S3Scanner-337", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, earning a score of 2. The goal is well-defined: the bug involves the application failing to read AWS credentials from the default file (`~/.aws/credentials`), resulting in only anonymous permissions being checked, which leads to false negatives in permission reporting. The reproduction steps are explicit, providing a clear path to replicate the issue, and the expected output is provided for comparison. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the issue is limited to a specific AWS SDK version or configuration, nor does it mention potential edge cases like missing or malformed credential files. Additionally, while the reproduction steps are helpful, there is no mention of specific constraints or prerequisites beyond basic AWS setup. These minor gaps prevent it from being comprehensive, but the statement is still actionable and clear enough for a developer to understand the core issue.", "difficulty_explanation": "I assign a difficulty score of 0.65, placing this problem in the \"Hard\" range (0.6-0.8), due to the combination of technical depth, codebase impact, and required domain knowledge. Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of Problem Description**: While the problem is mostly clear, the underlying issue (failure to read AWS credentials) hints at deeper complexities in AWS SDK configuration and credential handling, which adds to the difficulty of diagnosing and resolving it correctly.\n\n2. **Scope and Depth of Code Changes**: The code changes are extensive, spanning multiple files (`bucket.go`, `aws.go`, `clientmap.go`, `credentials.go`, etc.) and affecting core functionality related to permission checking and client initialization. The modifications involve both structural changes (e.g., updating the `ClientMap` to handle credentials) and logical changes (e.g., distinguishing between anonymous and authenticated clients). These changes impact the system's architecture by introducing a dual-client approach (anonymous vs. authenticated), requiring a solid understanding of the existing codebase interactions. The volume of code changes is significant, with new files added and existing logic heavily modified.\n\n3. **Number of Technical Concepts**: Solving this requires familiarity with several concepts, including AWS SDK for Go (v2), credential provider mechanisms, S3 permission models, and concurrent client management. Developers need to understand how AWS credentials are loaded (via `config.LoadDefaultConfig`), how to handle anonymous vs. authenticated requests, and how to map these to permission checks. Additionally, knowledge of Go-specific features like structs, interfaces, and concurrency (via `sync.Mutex`) is necessary. The AWS domain knowledge is particularly critical, as misconfigurations or incorrect assumptions about credential precedence could lead to further bugs.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes reveal several implicit ones that must be handled, such as missing credentials, invalid profiles, or AWS API errors during credential retrieval (handled in `credentials.go`). The introduction of error wrapping in `checkPermissionsWithAuth` shows attention to robust error handling, but the complexity of managing dual clients (anonymous and authenticated) increases the risk of unhandled edge cases, such as fallback behavior when credentials are partially loaded or when regions differ in behavior. This adds to the difficulty.\n\nOverall, this problem requires a deep understanding of AWS interactions, significant modifications to the codebase, and careful handling of potential errors and edge cases. It falls short of \"Very Hard\" (0.8-1.0) because it does not involve system-level redesign or highly intricate algorithms, but it is still challenging enough to warrant a score of 0.65 due to the need for domain expertise and cross-module changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Error in creating meta for isin with PyArrow strings\n**Describe the issue**: Fails with error message `ArrowTypeError: Array type doesn't match type of values set: string vs int64` when I try to call .isin() on a Dask Series with a PyArrow string type.\r\n\r\nFull traceback:\r\n\r\n<details>\r\n\r\n```python-traceback\r\n\r\n---------------------------------------------------------------------------\r\nArrowTypeError                            Traceback (most recent call last)\r\nCell In[18], line 11\r\n      7 import pyarrow as pa\r\n      9 sample_df = dd.from_pandas(pd.DataFrame({'foo': [\"1\", \"2\", \"3\"]}), npartitions=1).astype(pd.ArrowDtype(pa.string()))\r\n---> 11 sample_df.foo.isin([\"1\", \"2\"])\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/dask_expr/_collection.py:771, in FrameBase.isin(self, values)\r\n    768             pass\r\n    769 from dask_expr.io._delayed import _DelayedExpr\r\n--> 771 return new_collection(\r\n    772     expr.Isin(\r\n    773         self,\r\n    774         values=_DelayedExpr(\r\n    775             delayed(values, name=\"delayed-\" + _tokenize_deterministic(values))\r\n    776         ),\r\n    777     )\r\n    778 )\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/dask_expr/_collection.py:4730, in new_collection(expr)\r\n   4728 def new_collection(expr):\r\n   4729     \"\"\"Create new collection from an expr\"\"\"\r\n-> 4730     meta = expr._meta\r\n   4731     expr._name  # Ensure backend is imported\r\n   4732     return get_collection_type(meta)(expr)\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/functools.py:981, in cached_property.__get__(self, instance, owner)\r\n    979 val = cache.get(self.attrname, _NOT_FOUND)\r\n    980 if val is _NOT_FOUND:\r\n--> 981     val = self.func(instance)\r\n    982     try:\r\n    983         cache[self.attrname] = val\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/dask_expr/_expr.py:1363, in Isin._meta(self)\r\n   1361 @functools.cached_property\r\n   1362 def _meta(self):\r\n-> 1363     return make_meta(meta_nonempty(self.frame._meta).isin([1]))\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pandas/core/series.py:5559, in Series.isin(self, values)\r\n   5486 def isin(self, values) -> Series:\r\n   5487     \"\"\"\r\n   5488     Whether elements in Series are contained in `values`.\r\n   5489 \r\n   (...)\r\n   5557     dtype: bool\r\n   5558     \"\"\"\r\n-> 5559     result = algorithms.isin(self._values, values)\r\n   5560     return self._constructor(result, index=self.index, copy=False).__finalize__(\r\n   5561         self, method=\"isin\"\r\n   5562     )\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pandas/core/algorithms.py:505, in isin(comps, values)\r\n    502 comps_array = extract_array(comps_array, extract_numpy=True)\r\n    503 if not isinstance(comps_array, np.ndarray):\r\n    504     # i.e. Extension Array\r\n--> 505     return comps_array.isin(values)\r\n    507 elif needs_i8_conversion(comps_array.dtype):\r\n    508     # Dispatch to DatetimeLikeArrayMixin.isin\r\n    509     return pd_array(comps_array).isin(values)\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py:1113, in ArrowExtensionArray.isin(self, values)\r\n   1110 if not len(values):\r\n   1111     return np.zeros(len(self), dtype=bool)\r\n-> 1113 result = pc.is_in(self._pa_array, value_set=pa.array(values, from_pandas=True))\r\n   1114 # pyarrow 2.0.0 returned nulls, so we explicitly specify dtype to convert nulls\r\n   1115 # to False\r\n   1116 return np.array(result, dtype=np.bool_)\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pyarrow/compute.py:263, in _make_generic_wrapper.<locals>.wrapper(memory_pool, options, *args, **kwargs)\r\n    261 if args and isinstance(args[0], Expression):\r\n    262     return Expression._call(func_name, list(args), options)\r\n--> 263 return func.call(args, options, memory_pool)\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pyarrow/_compute.pyx:385, in pyarrow._compute.Function.call()\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pyarrow/error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~/mambaforge/envs/person_linkage_case_study_20240423/lib/python3.10/site-packages/pyarrow/error.pxi:91, in pyarrow.lib.check_status()\r\n\r\nArrowTypeError: Array type doesn't match type of values set: string vs int64\r\n```\r\n\r\n</details>\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask\r\n# If you uncomment this, it works\r\n# dask.config.set({\"dataframe.query-planning\": False})\r\n\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nsample_df = dd.from_pandas(pd.DataFrame({'foo': [\"1\", \"2\", \"3\"]}), npartitions=1).astype(pd.ArrowDtype(pa.string()))\r\n\r\nsample_df.foo.isin([\"1\", \"2\"])\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n\r\n- Dask version: 2024.5.0\r\n- Python version: 3.10.14\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): pip\r\n\n", "patch": "diff --git a/dask_expr/_expr.py b/dask_expr/_expr.py\nindex a33f8a80..6b669d9b 100644\n--- a/dask_expr/_expr.py\n+++ b/dask_expr/_expr.py\n@@ -1361,7 +1361,11 @@ class Isin(Elemwise):\n \n     @functools.cached_property\n     def _meta(self):\n-        return make_meta(meta_nonempty(self.frame._meta).isin([1]))\n+        return make_meta(\n+            meta_nonempty(self.frame._meta).isin(\n+                meta_nonempty(self.frame._meta).iloc[[0]]\n+            )\n+        )\n \n     def _broadcast_dep(self, dep: Expr):\n         return dep.npartitions == 1\n", "instance_id": "dask__dask-expr-1067", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the issue, a full traceback, and a minimal reproducible example. The goal is evident: to fix an error when calling `.isin()` on a Dask Series with PyArrow string type. The environment details and the specific error message (`ArrowTypeError`) are helpful in understanding the context. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior or output beyond fixing the error, and no discussion of potential edge cases or constraints (e.g., performance implications or compatibility with other data types). Additionally, the problem statement does not clarify whether the fix should handle only PyArrow string types or generalize to other types. Despite these minor gaps, the statement is actionable and provides sufficient information to start addressing the issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively small, confined to a single file (`dask_expr/_expr.py`) and a specific function (`Isin._meta`). The change itself is minimal, modifying how metadata is computed for the `.isin()` operation by using a representative value from the frame's metadata instead of a hardcoded integer list. However, understanding the fix requires a moderate depth of knowledge in multiple technical concepts: familiarity with Dask's expression system, PyArrow's type system, and how metadata is handled in Dask for operations like `.isin()`. Additionally, the developer must grasp the interaction between Dask, Pandas, and PyArrow, which adds complexity. While the problem does not explicitly mention edge cases, the nature of type mismatches suggests potential issues with other data types or empty datasets, though the provided fix does not address these. The change does not impact the broader system architecture but requires careful consideration to avoid introducing new type-related errors. Overall, this problem requires understanding multiple concepts and making a targeted but non-trivial modification, justifying a difficulty score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Consider flagging ListenOptions.Listen(IPAddress.Any)\n## Background and Motivation\r\n\r\nOn a machine that supports IPv6, listening to `Any`, rather than `IPv6Any` will either not work or be slower than necessary.  For HTTP/1.x and 2/0, a name like `localhost` will resolve to `[::1]`, which won't be accepted by the server, forcing a retry with `127.0.0.1` (i.e. failed attempt before each connection).\r\n\r\nFor HTTP/3.0, `[::1]` will fail and there won't be a retry, so the connection will fail.\r\n\r\nRequested [here](https://github.com/dotnet/runtime/issues/108259#issuecomment-2377432722).\r\n\r\nSee also #58094, #58171, https://github.com/dotnet/runtime/issues/108259\r\n\r\n<!--\r\nWe welcome new analyzers and codefixers in the ASP.NET repo!\r\n\r\nWe use the same process to review both new analyzer/codefixer submissions and API proposals. There is an overview of our process [here](https://github.com/dotnet/aspnetcore/blob/main/docs/APIReviewProcess.md). This template will help us gather the information we need to start the review process.\r\n\r\nUnder this heading, describe the problem that your analyzer is trying to solve. Examples of great motivating scenarios include helping users avoid\r\nperformance issues, potentially insecure code, or recommending better APIs for a scenario.\r\n-->\r\n\r\n\r\n\r\n## Proposed Analyzer\r\n\r\n### Analyzer Behavior and Message\r\n\r\nWe could probably get away with flagging every usage and letting users disable it on IPv4-only machines.  Having said that, a nicer solution might be to have the analyzer detect IPv6 support.\r\n\r\n<!--\r\nProvide a description of when the analyzer will trigger and the associated analyzer message.\r\n-->\r\n\r\n<!--\r\nAnalyzer categories are derived from the categories documented in https://learn.microsoft.com/dotnet/fundamentals/code-analysis/categories\r\nTo select a category, review each category's description and select the best category based on the functionality of your analyzer.\r\n\r\nAnalyzer severity levels are documented in https://learn.microsoft.com/visualstudio/code-quality/use-roslyn-analyzers#configure-severity-levels\r\nReview the description to observe how the level set on the analyzer will affect build-time and editor behavior and select the best\r\nlevel for the task.\r\n-->\r\n\r\n### Category\r\n\r\n- [ ] Design\r\n- [ ] Documentation\r\n- [ ] Globalization\r\n- [ ] Interoperability\r\n- [ ] Maintainability\r\n- [ ] Naming\r\n- [x] Performance\r\n- [ ] Reliability\r\n- [ ] Security\r\n- [ ] Style\r\n- [x] Usage\r\n\r\n### Severity Level\r\n\r\n- [ ] Error\r\n- [ ] Warning\r\n- [x] Info\r\n- [ ] Hidden\r\n\r\n## Usage Scenarios\r\n\r\n```csharp\r\noptions.Listen(IPAddress.Any, 5001, listenOptions =>\r\n{\r\n    listenOptions.UseHttps();\r\n    listenOptions.Protocols = HttpProtocols.Http1AndHttp2AndHttp3;\r\n});\r\n```\r\n<!--\r\nProvide code examples that would trigger your analyzer to warn. Identify the spans of code that the analyzer\r\nwill be triggered on. When applicable, describe the result of the code fix associated with the change.\r\n-->\r\n\r\n## Risks\r\n\r\nIf a server specifically wants to reject IPv6 requests, such an analyzer would just be an irritation.  It could, however, be disabled.\r\n<!--\r\nPlease mention any risks that to your knowledge the API proposal might entail, such as breaking changes, performance regressions, etc.\r\n-->\r\n\n", "patch": "diff --git a/docs/list-of-diagnostics.md b/docs/list-of-diagnostics.md\nindex 64bbc9f53655..58874a8de585 100644\n--- a/docs/list-of-diagnostics.md\n+++ b/docs/list-of-diagnostics.md\n@@ -30,6 +30,10 @@\n |  __`ASP0022`__ | Route conflict detected between route handlers |\n |  __`ASP0023`__ | Route conflict detected between controller actions |\n |  __`ASP0024`__ | Route handler has multiple parameters with the [FromBody] attribute |\n+|  __`ASP0025`__ | Use AddAuthorizationBuilder |\n+|  __`ASP0026`__ | [Authorize] overridden by [AllowAnonymous] from farther away |\n+|  __`ASP0027`__ | Unnecessary public Program class declaration |\n+|  __`ASP0028`__ | Consider using ListenAnyIP() instead of Listen(IPAddress.Any) |\n \n ### API (`API1000-API1003`)\n \ndiff --git a/src/Framework/AspNetCoreAnalyzers/src/Analyzers/DiagnosticDescriptors.cs b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/DiagnosticDescriptors.cs\nindex 82fa924407df..a9ec1e603486 100644\n--- a/src/Framework/AspNetCoreAnalyzers/src/Analyzers/DiagnosticDescriptors.cs\n+++ b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/DiagnosticDescriptors.cs\n@@ -233,4 +233,13 @@ internal static class DiagnosticDescriptors\n         isEnabledByDefault: true,\n         helpLinkUri: \"https://aka.ms/aspnet/analyzers\",\n         customTags: WellKnownDiagnosticTags.Unnecessary);\n+\n+    internal static readonly DiagnosticDescriptor KestrelShouldListenOnIPv6AnyInsteadOfIpAny = new(\n+        \"ASP0028\",\n+        new LocalizableResourceString(nameof(Resources.Analyzer_KestrelShouldListenOnIPv6AnyInsteadOfIpAny_Title), Resources.ResourceManager, typeof(Resources)),\n+        new LocalizableResourceString(nameof(Resources.Analyzer_KestrelShouldListenOnIPv6AnyInsteadOfIpAny_Message), Resources.ResourceManager, typeof(Resources)),\n+        \"Usage\",\n+        DiagnosticSeverity.Info,\n+        isEnabledByDefault: true,\n+        helpLinkUri: \"https://aka.ms/aspnet/analyzers\");\n }\ndiff --git a/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Kestrel/ListenOnIPv6AnyAnalyzer.cs b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Kestrel/ListenOnIPv6AnyAnalyzer.cs\nnew file mode 100644\nindex 000000000000..63fe4134e9b4\n--- /dev/null\n+++ b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Kestrel/ListenOnIPv6AnyAnalyzer.cs\n@@ -0,0 +1,143 @@\n+// Licensed to the .NET Foundation under one or more agreements.\n+// The .NET Foundation licenses this file to you under the MIT license.\n+\n+using Microsoft.CodeAnalysis.Diagnostics;\n+using Microsoft.CodeAnalysis;\n+using System.Collections.Immutable;\n+using Microsoft.CodeAnalysis.CSharp;\n+using Microsoft.CodeAnalysis.Operations;\n+using Microsoft.CodeAnalysis.CSharp.Syntax;\n+using System.Linq;\n+\n+namespace Microsoft.AspNetCore.Analyzers.Kestrel;\n+\n+[DiagnosticAnalyzer(LanguageNames.CSharp)]\n+public class ListenOnIPv6AnyAnalyzer : DiagnosticAnalyzer\n+{\n+    public override ImmutableArray<DiagnosticDescriptor> SupportedDiagnostics => [ DiagnosticDescriptors.KestrelShouldListenOnIPv6AnyInsteadOfIpAny ];\n+\n+    public override void Initialize(AnalysisContext context)\n+    {\n+        context.ConfigureGeneratedCodeAnalysis(GeneratedCodeAnalysisFlags.None);\n+        context.EnableConcurrentExecution();\n+\n+        context.RegisterSyntaxNodeAction(KestrelServerOptionsListenInvocation, SyntaxKind.InvocationExpression);\n+    }\n+\n+    private void KestrelServerOptionsListenInvocation(SyntaxNodeAnalysisContext context)\n+    {\n+        // fail fast before accessing SemanticModel\n+        if (context.Node is not InvocationExpressionSyntax\n+            {\n+                Expression: MemberAccessExpressionSyntax\n+                {\n+                    Name: IdentifierNameSyntax { Identifier.ValueText: \"Listen\" }\n+                }\n+            } kestrelOptionsListenExpressionSyntax)\n+        {\n+            return;\n+        }\n+\n+        var nodeOperation = context.SemanticModel.GetOperation(context.Node, context.CancellationToken);\n+        if (!IsKestrelServerOptionsType(nodeOperation, out var kestrelOptionsListenInvocation))\n+        {\n+            return;\n+        }\n+\n+        var addressArgument = kestrelOptionsListenInvocation?.Arguments.FirstOrDefault();\n+        if (!IsIPAddressType(addressArgument?.Parameter))\n+        {\n+            return;\n+        }\n+\n+        var args = kestrelOptionsListenExpressionSyntax.ArgumentList;\n+        var ipAddressArgumentSyntax = args.Arguments.FirstOrDefault();\n+        if (ipAddressArgumentSyntax is null)\n+        {\n+            return;\n+        }\n+\n+        // explicit usage like `options.Listen(IPAddress.Any, ...)`\n+        if (ipAddressArgumentSyntax is ArgumentSyntax\n+        {\n+            Expression: MemberAccessExpressionSyntax\n+            {\n+                Name: IdentifierNameSyntax { Identifier.ValueText: \"Any\" }\n+            }\n+        })\n+        {\n+            context.ReportDiagnostic(Diagnostic.Create(DiagnosticDescriptors.KestrelShouldListenOnIPv6AnyInsteadOfIpAny, ipAddressArgumentSyntax.GetLocation()));\n+        }\n+\n+        // usage via local variable like\n+        // ```\n+        // var myIp = IPAddress.Any;\n+        // options.Listen(myIp, ...);\n+        // ```\n+        if (addressArgument!.Value is ILocalReferenceOperation localReferenceOperation)\n+        {\n+            var localVariableDeclaration = localReferenceOperation.Local.DeclaringSyntaxReferences.FirstOrDefault();\n+            if (localVariableDeclaration is null)\n+            {\n+                return;\n+            }\n+\n+            var localVarSyntax = localVariableDeclaration.GetSyntax(context.CancellationToken);\n+            if (localVarSyntax is VariableDeclaratorSyntax\n+            {\n+                Initializer.Value: MemberAccessExpressionSyntax\n+                {\n+                    Name.Identifier.ValueText: \"Any\"\n+                }\n+            })\n+            {\n+                context.ReportDiagnostic(Diagnostic.Create(DiagnosticDescriptors.KestrelShouldListenOnIPv6AnyInsteadOfIpAny, ipAddressArgumentSyntax.GetLocation()));\n+            }\n+        }\n+    }\n+\n+    private static bool IsIPAddressType(IParameterSymbol? parameter) => parameter is \n+    {\n+        Type: // searching type `System.Net.IPAddress`\n+        {\n+            Name: \"IPAddress\",\n+            ContainingNamespace: { Name: \"Net\", ContainingNamespace: { Name: \"System\", ContainingNamespace.IsGlobalNamespace: true } }\n+        }\n+    };\n+\n+    private static bool IsKestrelServerOptionsType(IOperation? operation, out IInvocationOperation? kestrelOptionsListenInvocation)\n+    {\n+        var result = operation is IInvocationOperation // searching type `Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServerOptions`\n+        {\n+            TargetMethod: { Name: \"Listen\" },\n+            Instance.Type:\n+            {\n+                Name: \"KestrelServerOptions\",\n+                ContainingNamespace:\n+                {\n+                    Name: \"Core\",\n+                    ContainingNamespace:\n+                    {\n+                        Name: \"Kestrel\",\n+                        ContainingNamespace:\n+                        {\n+                            Name: \"Server\",\n+                            ContainingNamespace:\n+                            {\n+                                Name: \"AspNetCore\",\n+                                ContainingNamespace:\n+                                {\n+                                    Name: \"Microsoft\",\n+                                    ContainingNamespace.IsGlobalNamespace: true\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        };\n+\n+        kestrelOptionsListenInvocation = result ? (IInvocationOperation)operation! : null;\n+        return result;\n+    }\n+}\ndiff --git a/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Resources.resx b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Resources.resx\nindex 075bcb700452..8c9397f5be64 100644\n--- a/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Resources.resx\n+++ b/src/Framework/AspNetCoreAnalyzers/src/Analyzers/Resources.resx\n@@ -327,4 +327,10 @@\n   <data name=\"Analyzer_PublicPartialProgramClass_Title\" xml:space=\"preserve\">\n     <value>Unnecessary public Program class declaration</value>\n   </data>\n+  <data name=\"Analyzer_KestrelShouldListenOnIPv6AnyInsteadOfIpAny_Title\" xml:space=\"preserve\">\n+    <value>Consider using ListenAnyIP() instead of Listen(IPAddress.Any)</value>\n+  </data>\n+  <data name=\"Analyzer_KestrelShouldListenOnIPv6AnyInsteadOfIpAny_Message\" xml:space=\"preserve\">\n+    <value>If the server does not specifically reject IPv6, IPAddress.IPv6Any is preferred over IPAddress.Any usage for safety and performance reasons. See https://aka.ms/aspnetcore-warnings/ASP0028 for more details.</value>\n+  </data>\n </root>\ndiff --git a/src/Framework/AspNetCoreAnalyzers/src/CodeFixes/Kestrel/ListenOnIPv6AnyFixer.cs b/src/Framework/AspNetCoreAnalyzers/src/CodeFixes/Kestrel/ListenOnIPv6AnyFixer.cs\nnew file mode 100644\nindex 000000000000..207d80e9c6e0\n--- /dev/null\n+++ b/src/Framework/AspNetCoreAnalyzers/src/CodeFixes/Kestrel/ListenOnIPv6AnyFixer.cs\n@@ -0,0 +1,79 @@\n+// Licensed to the .NET Foundation under one or more agreements.\n+// The .NET Foundation licenses this file to you under the MIT license.\n+\n+using System.Composition;\n+using Microsoft.CodeAnalysis.CodeFixes;\n+using Microsoft.CodeAnalysis;\n+using System.Collections.Immutable;\n+using System.Threading.Tasks;\n+using Microsoft.AspNetCore.Analyzers;\n+using Microsoft.CodeAnalysis.CodeActions;\n+using Microsoft.CodeAnalysis.Editing;\n+using Microsoft.CodeAnalysis.CSharp.Syntax;\n+using Microsoft.CodeAnalysis.CSharp;\n+\n+namespace Microsoft.AspNetCore.Fixers.Kestrel;\n+\n+[ExportCodeFixProvider(LanguageNames.CSharp), Shared]\n+public class ListenOnIPv6AnyFixer : CodeFixProvider\n+{\n+    public override ImmutableArray<string> FixableDiagnosticIds => [ DiagnosticDescriptors.KestrelShouldListenOnIPv6AnyInsteadOfIpAny.Id ];\n+\n+    public sealed override FixAllProvider GetFixAllProvider() => WellKnownFixAllProviders.BatchFixer;\n+\n+    public override Task RegisterCodeFixesAsync(CodeFixContext context)\n+    {\n+        foreach (var diagnostic in context.Diagnostics)\n+        {\n+            context.RegisterCodeFix(\n+                CodeAction.Create(\n+                    \"Consider using IPAddress.IPv6Any instead of IPAddress.Any\",\n+                    async cancellationToken =>\n+                    {\n+                        var editor = await DocumentEditor.CreateAsync(context.Document, cancellationToken).ConfigureAwait(false);\n+                        var root = await context.Document.GetSyntaxRootAsync(cancellationToken);\n+                        if (root is null)\n+                        {\n+                            return context.Document;\n+                        }\n+\n+                        var argumentSyntax = root.FindNode(diagnostic.Location.SourceSpan).FirstAncestorOrSelf<ArgumentSyntax>();\n+                        if (argumentSyntax is null)\n+                        {\n+                            return context.Document;\n+                        }\n+\n+                        // get to the `Listen(IPAddress.Any, ...)` invocation\n+                        if (argumentSyntax.Parent?.Parent is not InvocationExpressionSyntax { ArgumentList.Arguments.Count: > 1 } invocationExpressionSyntax)\n+                        {\n+                            return context.Document;\n+                        }\n+                        if (invocationExpressionSyntax.Expression is not MemberAccessExpressionSyntax memberAccessExpressionSyntax)\n+                        {\n+                            return context.Document;\n+                        }\n+\n+                        var instanceVariableInvoked = memberAccessExpressionSyntax.Expression;\n+                        var adjustedArgumentList = invocationExpressionSyntax.ArgumentList.RemoveNode(invocationExpressionSyntax.ArgumentList.Arguments.First(), SyntaxRemoveOptions.KeepLeadingTrivia);\n+                        if (adjustedArgumentList is null || adjustedArgumentList.Arguments.Count == 0)\n+                        {\n+                            return context.Document;\n+                        }\n+\n+                        // changing invocation from `<variable>.Listen(IPAddress.Any, ...)` to `<variable>.ListenAnyIP(...)`\n+                        editor.ReplaceNode(\n+                            invocationExpressionSyntax,\n+                            invocationExpressionSyntax\n+                                .WithExpression(SyntaxFactory.ParseExpression($\"{instanceVariableInvoked.ToString()}.ListenAnyIP\"))\n+                                .WithArgumentList(adjustedArgumentList!)\n+                                .WithLeadingTrivia(invocationExpressionSyntax.GetLeadingTrivia())\n+                        );\n+                        return editor.GetChangedDocument();\n+                    },\n+                    equivalenceKey: DiagnosticDescriptors.KestrelShouldListenOnIPv6AnyInsteadOfIpAny.Id),\n+                diagnostic);\n+        }\n+\n+        return Task.CompletedTask;\n+    }\n+}\n", "instance_id": "dotnet__aspnetcore-58872", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue with using `IPAddress.Any` instead of `IPAddress.IPv6Any` or `ListenAnyIP()` in the context of Kestrel server configurations for ASP.NET Core. It provides background on why this is a performance and compatibility issue, especially with IPv6-enabled systems, and includes a usage scenario with code examples. The motivation and risks are also outlined, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the analyzer should behave in edge cases (e.g., on systems where IPv6 is explicitly disabled) beyond suggesting that users can disable the warning. Additionally, while it mentions detecting IPv6 support as a \"nicer solution,\" it lacks specifics on how this detection should be implemented or whether it is a requirement. These gaps prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves creating a new analyzer and code fixer within the ASP.NET Core analyzers project, which requires adding new files (`ListenOnIPv6AnyAnalyzer.cs` and `ListenOnIPv6AnyFixer.cs`) and updating existing ones (e.g., diagnostic descriptors and documentation). This is not a trivial change but is localized to the analyzer framework. Second, the technical concepts involved include understanding Roslyn APIs for syntax and semantic analysis, which are moderately complex and require familiarity with compiler-level programming in C#. The analyzer must detect specific patterns in code (e.g., `Listen(IPAddress.Any, ...)` calls) and handle cases like direct usage and variable references, adding to the complexity. Third, while the problem does not deeply impact the broader system architecture, it requires careful consideration of edge cases, such as different ways `IPAddress.Any` might be passed to the `Listen` method (e.g., via variables or expressions), though these are handled in the provided code. Finally, the error handling and risk mitigation (e.g., allowing users to disable the warning) are straightforward but still need to be considered. Given the need to understand Roslyn, implement pattern matching in code analysis, and handle moderate edge cases across a few files, I assign a difficulty score of 0.55, placing it in the medium category (0.4-0.6), leaning slightly towards the higher end due to the specialized knowledge of Roslyn required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Change error handling strategy and add verbose errors\n## Summary of the changes / Why this is an improvement\r\nIssue #2\r\n\r\nThis is one of the first pieces required to finish the feature and its only implemented in Python for early feedback/validation. \r\n\r\nMain changes of PR:\r\n- Throwing antlr4 parse errors as exceptions is now only enabled by `raise_exception` API influenced by [DRF Validators](https://www.django-rest-framework.org/api-guide/exceptions/#validationerror)\r\n\r\n- Default behaviour is not to raise an exception, the reason is that it forces clients to try/catch errors, when you already work with CrateDB in the HTTP protocol you are used to check the json response, rather than catching exceptions, this change of behaviour aims to make integrating with this library as similar as possible as with CrateDB.\r\n\r\n- Implements an `ExceptionCollectorListener` that collects all errors that are caught, (remember that we use `parser.statements`, to support running several queries on the frontends, so several errors can happen simultaneously)\r\n\r\n- Adds more verbose errors \r\n\r\n**_The main issue I would like help with the review:_** <-------------------------------------\r\n- Whenever we collect the errors I just add the errors to a list for further processing:\r\n```python\r\nclass ExceptionCollectorListener(ErrorListener):\r\n    \"\"\"\r\n    Error listener that collects all errors into errors for further processing.\r\n    Based partially on https://github.com/antlr/antlr4/issues/396\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        self.errors = []\r\n\r\n    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):\r\n        error = ParsingException(\r\n            msg=msg,\r\n            offending_token=offendingSymbol,\r\n            e=e,\r\n            query=e.ctx.parser.getTokenStream().getText(e.ctx.start, e.offendingToken.tokenIndex),\r\n        )\r\n\r\n        self.errors.append(error)\r\n\r\n```\r\n\r\n\r\nwe lose the information on which statement the error belongs to, I tried with different ErrorStrategies and Error listeners but to no avail, also didn't find anything online.\r\n\r\nMy best effort is to try to match the error's offendingToken query, to any of the parsed statements:\r\n\r\n```python\r\n# Shortened for clarity purposes.\r\ndef find_suitable_error(statement, errors):\r\n    for error in errors[:]:\r\n        if error.query == statement.query:\r\n            statement.exception = error\r\n            errors.pop(errors.index(error))\r\n```\r\nThere are a couple of edge cases that I covered but in short, it works reasonable well, even though I'm not a fan of it, maybe you folks way more experience in `antlr4 ` have an idea?\r\n\r\n## Checklist\r\n\r\n - [x] Link to issue this PR refers to (if applicable): \r\n - [x] [CLA](https://crate.io/community/contribute/cla/) is signed\r\n\nAdd `with_properties` and `interpolated_properties`\n## Summary of the changes / Why this is an improvement\r\n\r\nThis PR allows for extracting properties defined by `[ WITH ( parameter [= value] [, ... ] ) ]` statements.\r\n\r\nFor example:\r\n\r\n```python\r\nfrom cratedb_sqlparse import sqlparse\r\n\r\nstmt = sqlparse(\"\"\"\r\n    CREATE TABLE doc.tbl12 (A TEXT) WITH (\r\n      \"allocation.max_retries\" = 5,\r\n      \"blocks.metadata\" = false\r\n    );\r\n\"\"\")[0]\r\n\r\nprint(stmt.metadata)\r\n# Metadata(schema='doc', table_name='tbl12', interpolated_properties={}, with_properties={'allocation.max_retries': '5', 'blocks.metadata': 'false'})\r\n```\r\n\r\nIt also handles the special case of what I call, `interpolated values` and #31 if you have a better name recommendation, please go ahead :P\r\n\r\n## Checklist\r\n\r\n - [x] Link to issue this PR refers to (if applicable): \r\n - [x] [CLA](https://crate.io/community/contribute/cla/) is signed\r\n\nAdd verbose options to both python and javascript target to include query and approximate position of the error.\nFrom https://github.com/crate/crate/issues/15826\r\n\r\nSteps:\r\n- Implement a new ErrorListener that displays the query with the offending token marked both in javascript and python targets.\r\n- Add the Error to every parsed statement. An error could be an object with the following properties:\r\n```\r\n{\r\n   \"error\": \"ParseError\",\r\n   \"original_query\": '...\",\r\n   \"original_query_with_offending_token\": \"...\",\r\n   \"offending_token\": \"...\",\r\n   \"error_message\": \"...\",\r\n   \"col\": 1,\r\n   \"line\": 1\r\n}\r\n```\r\n\r\nWe should be able to disable this by:\r\n\r\n```python\r\nsqlparse('SELECT 1;', raise_exception=True)\r\n```\r\n\r\nMissing: Javascript target\nMigrate cratedb_sqlparse from javascript to typescript\nRight now `cratedb_sqlparse` does not support typescript, so every typescript project that wants to use this library needs to define their own types, leading to possible type error(s). We should migrate it to typescript (since it is also compatible with plain javascript).\n", "patch": "diff --git a/cratedb_sqlparse_js/README.md b/cratedb_sqlparse_js/README.md\nindex 6019bd6..36e7f43 100644\n--- a/cratedb_sqlparse_js/README.md\n+++ b/cratedb_sqlparse_js/README.md\n@@ -9,12 +9,12 @@\n ![NPM Unpacked Size](https://img.shields.io/npm/unpacked-size/@cratedb/cratedb-sqlparse)\n ![NPM Type Definitions](https://img.shields.io/npm/types/@cratedb/cratedb-sqlparse)\n \n-\n CrateDB SQL Parser for JavaScript, compiled from antlr4 JavaScript compile target.\n \n ### Simple usage\n+\n ```javascript\n-import { sqlparse } from \"@cratedb/cratedb-sqlparse\";\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n \n const query = `\n SELECT * FROM SYS.SHARDS;\n@@ -37,27 +37,197 @@ console.log(queries[0].original_query)\n ```\n \n ### CrateDB version\n+\n You can programmatically check the CrateDB version the package was compiled for in `index.js`\n \n ```javascript\n-import { __cratedb_version__ } from \"@cratedb/cratedb-sqlparse\";\n+import {__cratedb_version__} from \"@cratedb/cratedb-sqlparse\";\n \n console.log(__cratedb_version__)\n-// 5.6.4\n+// 5.7.2\n ```\n \n ### Features\n-Currently, we support the same features as CrateDB java's parser:\n+\n+Currently, the parser supports a subset of the features of CrateDB's Java/ANTLR parser:\n+\n - First class CrateDB SQL dialect support.\n - Input is case-insensitive.\n-- Native errors as exceptions.\n+- Native errors as exceptions or as objects.\n - Dollar strings.\n+- Tables\n+- Properties and parametrized properties.\n+\n+### Exceptions and errors.\n+\n+By default, exceptions are stored in `statement.exception`.\n \n-Optional features:\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n \n-### Errors\n-Errors are thrown as 'ParseError' e.g.\n+const query = `\n+SELECT COUNT(*) FROM doc.tbl f HERE f.id = 1;\n \n-```text\n-ParseError: line2:9 mismatched input 'ROM' expecting {<EOF>, ';'}\n+INSERT INTO doc.tbl VALUES (1, 23, 4);\n+`\n+const statements = sqlparse(query)\n+const stmt = statements[0]\n+\n+if (stmt.exception) {\n+    console.log(stmt.exception.errorMessage)\n+    // [line 2:43 mismatched input 'HERE' expecting {<EOF>, ';'}]\n+\n+    console.log(stmt.exception.errorMessageVerbose)\n+    //      SELECT COUNT(*) FROM doc.tbl f HERE f.id = 1;\n+    //                                     ^^^^\n+    //      INSERT INTO doc.tbl VALUES (1, 23, 4);\n+}\n+\n+console.log(stmt.exception)\n+\n+// ParseError: mismatched input 'HERE' expecting {<EOF>, ';'}\n+//     at ExceptionCollectorListener.syntaxError (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/cratedb_sqlparse/parser.js:115:23)\n+//     at file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/node_modules/antlr4/dist/antlr4.node.mjs:1:42125\n+//     at Array.map (<anonymous>)\n+//     at wt.syntaxError (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/node_modules/antlr4/dist/antlr4.node.mjs:1:42115)\n+//     at SqlBaseParser.notifyErrorListeners (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/node_modules/antlr4/dist/antlr4.node.mjs:1:102085)\n+//     at Ce.reportInputMismatch (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/node_modules/antlr4/dist/antlr4.node.mjs:1:90577)\n+//     at Ce.reportError (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/node_modules/antlr4/dist/antlr4.node.mjs:1:88813)\n+//     at SqlBaseParser.statements (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/cratedb_sqlparse/generated_parser/SqlBaseParser.js:1345:28)\n+//     at sqlparse (file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/cratedb_sqlparse/parser.js:207:25)\n+//     at file:///home/surister/PycharmProjects/cratedb-sqlparse/cratedb_sqlparse_js/t.js:4:14 {\n+//   query: 'SELECT COUNT(*) FROM doc.tbl f HERE',\n+//   msg: \"mismatched input 'HERE' expecting {<EOF>, ';'}\",\n+//   offendingToken: bt {\n+//     source: [ [SqlBaseLexer], [CaseInsensitiveStream] ],\n+//     type: 322,\n+//     channel: 0,\n+//     start: 32,\n+//     stop: 35,\n+//     tokenIndex: 16,\n+//     line: 2,\n+//     column: 31,\n+//     _text: null\n+//   },\n+//   line: 2,\n+//   column: 31,\n+//   errorMessage: \"[line 2:31 mismatched input 'HERE' expecting {<EOF>, ';'}]\",\n+//   errorMessageVerbose: '\\n' +\n+//     'SELECT COUNT(*) FROM doc.tbl f HERE f.id = 1;\\n' +\n+//     '                               ^^^^\\n' +\n+//     '\\n' +\n+//     'INSERT INTO doc.tbl VALUES (1, 23, 4);\\n'\n+// }\n ```\n+\n+In some situations, you might want sqlparse to throw an error.\n+\n+You can set `raise_exception` to `true`\n+\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n+\n+let stmt = sqlparse('SELECT COUNT(*) FROM doc.tbl f WHERE .id = 1;', true);\n+\n+//         throw new ParseError(\n+//            ^\n+//\n+// ParseError: no viable alternative at input 'SELECT COUNT(*) FROM doc.tbl f WHERE .'\n+```\n+\n+Catch the exception:\n+\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n+\n+try {\n+    sqlparse('SELECT COUNT(*) FROM doc.tbl f WHERE .id = 1;', true)\n+} catch (e) {\n+    console.log(e)\n+}\n+```\n+\n+> [!NOTE]\n+> It will only raise the first exception it finds, even if you pass in several statements.\n+\n+### Query metadata\n+\n+Query metadata can be read with `statement.metadata`\n+\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n+\n+const stmt = sqlparse(\"SELECT A, B FROM doc.tbl12\")[0]\n+\n+console.log(stmt.metadata);\n+\n+// Metadata {\n+//   tables: [ Table { name: 'tbl12', schema: 'doc' } ],\n+//   parameterizedProperties: {},\n+//   withProperties: {}\n+// }\n+\n+```\n+\n+#### Query properties\n+\n+Properties defined within a `WITH` statement, `statement.metadata.withProperties:`.\n+\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n+\n+\n+const stmt = sqlparse(`\n+    CREATE TABLE doc.tbl12 (A TEXT) WITH (\n+      \"allocation.max_retries\" = 5,\n+      \"blocks.metadata\" = false\n+    );\n+`)[0]\n+\n+console.log(stmt.metadata);\n+\n+// Metadata {\n+//   tables: [ Table { name: 'tbl12', schema: 'doc' } ],\n+//   parameterizedProperties: {},\n+//   withProperties: { 'allocation.max_retries': '5', 'blocks.metadata': 'false' }\n+// }\n+```\n+\n+#### Table name\n+```javascript\n+console.log(stmt.metadata.tables)\n+// [ Table { name: 'tbl12', schema: 'doc' } ]\n+\n+table = stmt.metadata.tables[0]\n+\n+console.log(table.schema, table.name, table.fqn)\n+// doc tbl12 \"doc\".\"tbl12\"\n+```\n+\n+#### Parameterized properties\n+\n+Parameterized properties are properties without a real defined value, marked with a dollar string,  `metadata.parameterized_properties`\n+\n+```javascript\n+import {sqlparse} from \"@cratedb/cratedb-sqlparse\";\n+\n+const stmt = sqlparse(`\n+    CREATE TABLE doc.tbl12 (A TEXT) WITH (\n+    \"allocation.max_retries\" = 5,\n+    \"blocks.metadata\" = $1\n+);\n+`)[0]\n+\n+console.log(stmt.metadata)\n+\n+// Metadata {\n+//   tables: [ Table { name: 'tbl12', schema: 'doc', fqn: '\"doc\".\"tbl12\"' } ],\n+//   parameterizedProperties: { 'blocks.metadata': '$1' },\n+//   withProperties: { 'allocation.max_retries': '5', 'blocks.metadata': '$1' }\n+// }\n+```\n+\n+In this case, `blocks.metadata` will be in `with_properties` and `parameterized_properties` as well.\n+\n+For values to be picked up they need to start with a dollar `'$'` and be preceded by integers, e.g. `'$1'` or `'$123'`.\n+`'$123abc'` would not be valid.\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/cratedb_sqlparse/AstBuilder.js b/cratedb_sqlparse_js/cratedb_sqlparse/AstBuilder.js\nnew file mode 100644\nindex 0000000..151b7f1\n--- /dev/null\n+++ b/cratedb_sqlparse_js/cratedb_sqlparse/AstBuilder.js\n@@ -0,0 +1,98 @@\n+import SqlBaseParserVisitor from \"./generated_parser/SqlBaseParserVisitor.js\";\n+import SqlBaseParser from \"./generated_parser/SqlBaseParser.js\";\n+import {Statement} from \"./parser.js\"\n+import {Table} from \"./models.js\"\n+\n+\n+/**\n+ *\n+ * @param {string} text\n+ * @returns {Boolean}\n+ */\n+function isDigit(text) {\n+    return text.split('').every(char => char >= '0' && char <= '9');\n+}\n+\n+\n+export class AstBuilder extends SqlBaseParserVisitor {\n+//     The class implements the antlr4 visitor pattern similar to how we do it in CrateDB\n+//     https://github.com/crate/crate/blob/master/libs/sql-parser/src/main/java/io/crate/sql/parser/AstBuilder.java\n+//\n+//     The biggest difference is that in CrateDB, `AstBuilder`, visitor methods\n+//     return a specialized Statement visitor.\n+//\n+//     Sqlparse just extracts whatever data it needs from the context and injects it to the current\n+//     visited statement, enriching its metadata.\n+\n+    /**\n+     *\n+     * @param {Object} node\n+     * @returns {(string|null)}\n+     */\n+    getText(node) {\n+        if (node) {\n+            return node.getText().replaceAll(\"'\", \"\").replaceAll('\"', \"\")\n+        }\n+        return null\n+    }\n+\n+    /**\n+     *\n+     * @param {Statement} stmt\n+     */\n+    enrich(stmt) {\n+        this.stmt = stmt\n+        this.visit(this.stmt.ctx)\n+    }\n+\n+    /**\n+     *\n+     * @param {SqlBaseParser.TableNameContext} ctx\n+     */\n+    visitTableName(ctx) {\n+        const fqn = ctx.qname()\n+        const parts = this.getText(fqn).split(\".\")\n+\n+        let schema = null\n+        let name = null;\n+        if (parts.length === 1) {\n+            name = parts[0]\n+        } else {\n+            schema = parts[0]\n+            name = parts[1]\n+        }\n+\n+        this.stmt.metadata.tables.push(\n+            new Table(name, schema)\n+        )\n+    }\n+\n+    /**\n+     *\n+     * @param {SqlBaseParser.GenericPropertiesContext} ctx\n+     */\n+    visitGenericProperties(ctx) {\n+        const nodeProperties = ctx.genericProperty()\n+        const properties = {}\n+        const parameterizedProperties = {}\n+\n+        for (const property of nodeProperties) {\n+            let key = this.getText(property.ident())\n+            let value = this.getText(property.expr())\n+\n+            properties[key] = value\n+\n+            if (value && value[0] === '$') {\n+                // It might be a parameterized value, e.g. '$1'\n+                if (isDigit(value.slice(1))) {\n+                    parameterizedProperties[key] = value\n+                }\n+            }\n+\n+            this.stmt.metadata.withProperties = properties\n+            this.stmt.metadata.parameterizedProperties = parameterizedProperties\n+\n+        }\n+    }\n+\n+}\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/cratedb_sqlparse/generated_parser/SqlBaseParserVisitor.js b/cratedb_sqlparse_js/cratedb_sqlparse/generated_parser/SqlBaseParserVisitor.js\nnew file mode 100644\nindex 0000000..3c050de\n--- /dev/null\n+++ b/cratedb_sqlparse_js/cratedb_sqlparse/generated_parser/SqlBaseParserVisitor.js\n@@ -0,0 +1,1702 @@\n+// Generated from SqlBaseParser.g4 by ANTLR 4.13.1\n+// jshint ignore: start\n+import antlr4 from 'antlr4';\n+\n+// This class defines a complete generic visitor for a parse tree produced by SqlBaseParser.\n+\n+export default class SqlBaseParserVisitor extends antlr4.tree.ParseTreeVisitor {\n+\n+\t// Visit a parse tree produced by SqlBaseParser#statements.\n+\tvisitStatements(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#singleStatement.\n+\tvisitSingleStatement(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#singleExpression.\n+\tvisitSingleExpression(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#default.\n+\tvisitDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#begin.\n+\tvisitBegin(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#startTransaction.\n+\tvisitStartTransaction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#commit.\n+\tvisitCommit(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#explain.\n+\tvisitExplain(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#optimize.\n+\tvisitOptimize(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#refreshTable.\n+\tvisitRefreshTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#update.\n+\tvisitUpdate(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#delete.\n+\tvisitDelete(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showTransaction.\n+\tvisitShowTransaction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showCreateTable.\n+\tvisitShowCreateTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showTables.\n+\tvisitShowTables(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showSchemas.\n+\tvisitShowSchemas(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showColumns.\n+\tvisitShowColumns(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#showSessionParameter.\n+\tvisitShowSessionParameter(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alter.\n+\tvisitAlter(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#resetGlobal.\n+\tvisitResetGlobal(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setTransaction.\n+\tvisitSetTransaction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setSessionAuthorization.\n+\tvisitSetSessionAuthorization(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#resetSessionAuthorization.\n+\tvisitResetSessionAuthorization(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#set.\n+\tvisitSet(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setGlobal.\n+\tvisitSetGlobal(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setTimeZone.\n+\tvisitSetTimeZone(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#kill.\n+\tvisitKill(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#insert.\n+\tvisitInsert(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#restore.\n+\tvisitRestore(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#copyFrom.\n+\tvisitCopyFrom(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#copyTo.\n+\tvisitCopyTo(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#drop.\n+\tvisitDrop(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#grantPrivilege.\n+\tvisitGrantPrivilege(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#denyPrivilege.\n+\tvisitDenyPrivilege(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#revokePrivilege.\n+\tvisitRevokePrivilege(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#create.\n+\tvisitCreate(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#deallocate.\n+\tvisitDeallocate(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#analyze.\n+\tvisitAnalyze(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#discard.\n+\tvisitDiscard(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#declare.\n+\tvisitDeclare(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#fetch.\n+\tvisitFetch(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#close.\n+\tvisitClose(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropBlobTable.\n+\tvisitDropBlobTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropTable.\n+\tvisitDropTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropAlias.\n+\tvisitDropAlias(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropRepository.\n+\tvisitDropRepository(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropSnapshot.\n+\tvisitDropSnapshot(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropFunction.\n+\tvisitDropFunction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropRole.\n+\tvisitDropRole(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropView.\n+\tvisitDropView(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropAnalyzer.\n+\tvisitDropAnalyzer(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropPublication.\n+\tvisitDropPublication(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropSubscription.\n+\tvisitDropSubscription(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropServer.\n+\tvisitDropServer(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropForeignTable.\n+\tvisitDropForeignTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropUserMapping.\n+\tvisitDropUserMapping(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#addColumn.\n+\tvisitAddColumn(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropColumn.\n+\tvisitDropColumn(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropCheckConstraint.\n+\tvisitDropCheckConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterTableProperties.\n+\tvisitAlterTableProperties(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterBlobTableProperties.\n+\tvisitAlterBlobTableProperties(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterTableOpenClose.\n+\tvisitAlterTableOpenClose(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterTableRenameTable.\n+\tvisitAlterTableRenameTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterTableRenameColumn.\n+\tvisitAlterTableRenameColumn(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterTableReroute.\n+\tvisitAlterTableReroute(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterClusterRerouteRetryFailed.\n+\tvisitAlterClusterRerouteRetryFailed(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterClusterSwapTable.\n+\tvisitAlterClusterSwapTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterClusterDecommissionNode.\n+\tvisitAlterClusterDecommissionNode(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterClusterGCDanglingArtifacts.\n+\tvisitAlterClusterGCDanglingArtifacts(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterRole.\n+\tvisitAlterRole(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterPublication.\n+\tvisitAlterPublication(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterSubscription.\n+\tvisitAlterSubscription(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#queryOptParens.\n+\tvisitQueryOptParens(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#query.\n+\tvisitQuery(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#queryNoWith.\n+\tvisitQueryNoWith(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#limitClause.\n+\tvisitLimitClause(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#offsetClause.\n+\tvisitOffsetClause(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#queryTermDefault.\n+\tvisitQueryTermDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setOperation.\n+\tvisitSetOperation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setQuant.\n+\tvisitSetQuant(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#sortItem.\n+\tvisitSortItem(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#defaultQuerySpec.\n+\tvisitDefaultQuerySpec(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#valuesRelation.\n+\tvisitValuesRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#selectSingle.\n+\tvisitSelectSingle(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#selectAll.\n+\tvisitSelectAll(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#where.\n+\tvisitWhere(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#returning.\n+\tvisitReturning(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#filter.\n+\tvisitFilter(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#relationDefault.\n+\tvisitRelationDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#joinRelation.\n+\tvisitJoinRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#joinType.\n+\tvisitJoinType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#joinCriteria.\n+\tvisitJoinCriteria(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#aliasedRelation.\n+\tvisitAliasedRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableRelation.\n+\tvisitTableRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#subqueryRelation.\n+\tvisitSubqueryRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parenthesizedRelation.\n+\tvisitParenthesizedRelation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableWithPartition.\n+\tvisitTableWithPartition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableName.\n+\tvisitTableName(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableFunction.\n+\tvisitTableFunction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#aliasedColumns.\n+\tvisitAliasedColumns(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#with.\n+\tvisitWith(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#namedQuery.\n+\tvisitNamedQuery(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#expr.\n+\tvisitExpr(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#logicalNot.\n+\tvisitLogicalNot(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#booleanDefault.\n+\tvisitBooleanDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#match.\n+\tvisitMatch(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#logicalBinary.\n+\tvisitLogicalBinary(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#predicated.\n+\tvisitPredicated(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#comparison.\n+\tvisitComparison(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#quantifiedComparison.\n+\tvisitQuantifiedComparison(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#between.\n+\tvisitBetween(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#inList.\n+\tvisitInList(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#inSubquery.\n+\tvisitInSubquery(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#like.\n+\tvisitLike(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arrayLike.\n+\tvisitArrayLike(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#nullPredicate.\n+\tvisitNullPredicate(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#distinctFrom.\n+\tvisitDistinctFrom(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#bitwiseBinary.\n+\tvisitBitwiseBinary(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#valueExpressionDefault.\n+\tvisitValueExpressionDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#concatenation.\n+\tvisitConcatenation(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#fromStringLiteralCast.\n+\tvisitFromStringLiteralCast(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arithmeticBinary.\n+\tvisitArithmeticBinary(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arithmeticUnary.\n+\tvisitArithmeticUnary(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#subqueryExpressionDefault.\n+\tvisitSubqueryExpressionDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dereference.\n+\tvisitDereference(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnReference.\n+\tvisitColumnReference(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#atTimezone.\n+\tvisitAtTimezone(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#subscript.\n+\tvisitSubscript(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#recordSubscript.\n+\tvisitRecordSubscript(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#explicitFunctionDefault.\n+\tvisitExplicitFunctionDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#doubleColonCast.\n+\tvisitDoubleColonCast(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#defaultParamOrLiteral.\n+\tvisitDefaultParamOrLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#functionCall.\n+\tvisitFunctionCall(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#nestedExpression.\n+\tvisitNestedExpression(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arraySlice.\n+\tvisitArraySlice(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#exists.\n+\tvisitExists(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#emptyArray.\n+\tvisitEmptyArray(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#specialDateTimeFunction.\n+\tvisitSpecialDateTimeFunction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#currentSchema.\n+\tvisitCurrentSchema(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#currentUser.\n+\tvisitCurrentUser(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#sessionUser.\n+\tvisitSessionUser(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#left.\n+\tvisitLeft(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#right.\n+\tvisitRight(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#substring.\n+\tvisitSubstring(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#trim.\n+\tvisitTrim(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#extract.\n+\tvisitExtract(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#cast.\n+\tvisitCast(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#simpleCase.\n+\tvisitSimpleCase(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#searchedCase.\n+\tvisitSearchedCase(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#ifCase.\n+\tvisitIfCase(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arraySubquery.\n+\tvisitArraySubquery(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#subqueryExpression.\n+\tvisitSubqueryExpression(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#simpleLiteral.\n+\tvisitSimpleLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arrayLiteral.\n+\tvisitArrayLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#objectLiteral.\n+\tvisitObjectLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parameterOrSimpleLiteral.\n+\tvisitParameterOrSimpleLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parameterExpression.\n+\tvisitParameterExpression(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#intAsLiteral.\n+\tvisitIntAsLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#nullAsLiteral.\n+\tvisitNullAsLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#integerParamOrLiteralDoubleColonCast.\n+\tvisitIntegerParamOrLiteralDoubleColonCast(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#integerParamOrLiteralCast.\n+\tvisitIntegerParamOrLiteralCast(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parameterOrIdent.\n+\tvisitParameterOrIdent(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parameterOrString.\n+\tvisitParameterOrString(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#positionalParameter.\n+\tvisitPositionalParameter(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#parameterPlaceholder.\n+\tvisitParameterPlaceholder(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#nullLiteral.\n+\tvisitNullLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#escapedCharsStringLiteral.\n+\tvisitEscapedCharsStringLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dollarQuotedStringLiteral.\n+\tvisitDollarQuotedStringLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#stringLiteral.\n+\tvisitStringLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#bitString.\n+\tvisitBitString(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#subscriptSafe.\n+\tvisitSubscriptSafe(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#cmpOp.\n+\tvisitCmpOp(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setCmpQuantifier.\n+\tvisitSetCmpQuantifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#whenClause.\n+\tvisitWhenClause(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#namedWindow.\n+\tvisitNamedWindow(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#over.\n+\tvisitOver(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#windowDefinition.\n+\tvisitWindowDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#windowFrame.\n+\tvisitWindowFrame(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#unboundedFrame.\n+\tvisitUnboundedFrame(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#currentRowBound.\n+\tvisitCurrentRowBound(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#boundedFrame.\n+\tvisitBoundedFrame(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#qnames.\n+\tvisitQnames(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#qname.\n+\tvisitQname(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#spaceSeparatedIdents.\n+\tvisitSpaceSeparatedIdents(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#identWithOrWithoutValue.\n+\tvisitIdentWithOrWithoutValue(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#idents.\n+\tvisitIdents(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#ident.\n+\tvisitIdent(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#unquotedIdentifier.\n+\tvisitUnquotedIdentifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#digitIdentifier.\n+\tvisitDigitIdentifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#quotedIdentifier.\n+\tvisitQuotedIdentifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#backQuotedIdentifier.\n+\tvisitBackQuotedIdentifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#stringLiteralOrIdentifier.\n+\tvisitStringLiteralOrIdentifier(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#stringLiteralOrIdentifierOrQname.\n+\tvisitStringLiteralOrIdentifierOrQname(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#numericLiteral.\n+\tvisitNumericLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#intervalLiteral.\n+\tvisitIntervalLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#intervalField.\n+\tvisitIntervalField(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#booleanLiteral.\n+\tvisitBooleanLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#decimalLiteral.\n+\tvisitDecimalLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#integerLiteral.\n+\tvisitIntegerLiteral(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#objectKeyValue.\n+\tvisitObjectKeyValue(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#insertSource.\n+\tvisitInsertSource(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#onConflict.\n+\tvisitOnConflict(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#conflictTarget.\n+\tvisitConflictTarget(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#values.\n+\tvisitValues(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columns.\n+\tvisitColumns(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#assignment.\n+\tvisitAssignment(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createTable.\n+\tvisitCreateTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createTableAs.\n+\tvisitCreateTableAs(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createForeignTable.\n+\tvisitCreateForeignTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createBlobTable.\n+\tvisitCreateBlobTable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createRepository.\n+\tvisitCreateRepository(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createSnapshot.\n+\tvisitCreateSnapshot(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createAnalyzer.\n+\tvisitCreateAnalyzer(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createFunction.\n+\tvisitCreateFunction(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createUserMapping.\n+\tvisitCreateUserMapping(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createRole.\n+\tvisitCreateRole(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createView.\n+\tvisitCreateView(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createPublication.\n+\tvisitCreatePublication(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createSubscription.\n+\tvisitCreateSubscription(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#createServer.\n+\tvisitCreateServer(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#mappedUser.\n+\tvisitMappedUser(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#kvOptions.\n+\tvisitKvOptions(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#kvOption.\n+\tvisitKvOption(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#functionArgument.\n+\tvisitFunctionArgument(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableOnly.\n+\tvisitTableOnly(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableWithPartitionDefault.\n+\tvisitTableWithPartitionDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#alterSubscriptionMode.\n+\tvisitAlterSubscriptionMode(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#partitionedByOrClusteredInto.\n+\tvisitPartitionedByOrClusteredInto(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#partitionedBy.\n+\tvisitPartitionedBy(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#clusteredBy.\n+\tvisitClusteredBy(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#blobClusteredInto.\n+\tvisitBlobClusteredInto(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnDefinitionDefault.\n+\tvisitColumnDefinitionDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#primaryKeyConstraintTableLevel.\n+\tvisitPrimaryKeyConstraintTableLevel(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#indexDefinition.\n+\tvisitIndexDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableCheckConstraint.\n+\tvisitTableCheckConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnDefinition.\n+\tvisitColumnDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#addColumnDefinition.\n+\tvisitAddColumnDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#dropColumnDefinition.\n+\tvisitDropColumnDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#rerouteMoveShard.\n+\tvisitRerouteMoveShard(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#rerouteAllocateReplicaShard.\n+\tvisitRerouteAllocateReplicaShard(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#reroutePromoteReplica.\n+\tvisitReroutePromoteReplica(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#rerouteCancelShard.\n+\tvisitRerouteCancelShard(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#objectDataType.\n+\tvisitObjectDataType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#maybeParametrizedDataType.\n+\tvisitMaybeParametrizedDataType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#arrayDataType.\n+\tvisitArrayDataType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#definedDataTypeDefault.\n+\tvisitDefinedDataTypeDefault(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#identDataType.\n+\tvisitIdentDataType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#definedDataType.\n+\tvisitDefinedDataType(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#objectTypeDefinition.\n+\tvisitObjectTypeDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnConstraintPrimaryKey.\n+\tvisitColumnConstraintPrimaryKey(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnConstraintNotNull.\n+\tvisitColumnConstraintNotNull(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnConstraintNull.\n+\tvisitColumnConstraintNull(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnIndexConstraint.\n+\tvisitColumnIndexConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnIndexOff.\n+\tvisitColumnIndexOff(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnStorageDefinition.\n+\tvisitColumnStorageDefinition(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnDefaultConstraint.\n+\tvisitColumnDefaultConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnGeneratedConstraint.\n+\tvisitColumnGeneratedConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#columnCheckConstraint.\n+\tvisitColumnCheckConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#primaryKeyContraint.\n+\tvisitPrimaryKeyContraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#checkConstraint.\n+\tvisitCheckConstraint(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#withGenericProperties.\n+\tvisitWithGenericProperties(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#genericProperties.\n+\tvisitGenericProperties(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#genericProperty.\n+\tvisitGenericProperty(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#explainOptions.\n+\tvisitExplainOptions(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#explainOption.\n+\tvisitExplainOption(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#matchPredicateIdents.\n+\tvisitMatchPredicateIdents(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#matchPredicateIdent.\n+\tvisitMatchPredicateIdent(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#analyzerElement.\n+\tvisitAnalyzerElement(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tokenizer.\n+\tvisitTokenizer(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tokenFilters.\n+\tvisitTokenFilters(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#charFilters.\n+\tvisitCharFilters(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#namedProperties.\n+\tvisitNamedProperties(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#tableWithPartitions.\n+\tvisitTableWithPartitions(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setGlobalAssignment.\n+\tvisitSetGlobalAssignment(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#setExpr.\n+\tvisitSetExpr(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#on.\n+\tvisitOn(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#securable.\n+\tvisitSecurable(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#transactionMode.\n+\tvisitTransactionMode(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#isolationLevel.\n+\tvisitIsolationLevel(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#direction.\n+\tvisitDirection(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#declareCursorParams.\n+\tvisitDeclareCursorParams(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\t// Visit a parse tree produced by SqlBaseParser#nonReserved.\n+\tvisitNonReserved(ctx) {\n+\t  return this.visitChildren(ctx);\n+\t}\n+\n+\n+\n+}\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/cratedb_sqlparse/index.js b/cratedb_sqlparse_js/cratedb_sqlparse/index.js\nindex 692a1f4..b7b308b 100644\n--- a/cratedb_sqlparse_js/cratedb_sqlparse/index.js\n+++ b/cratedb_sqlparse_js/cratedb_sqlparse/index.js\n@@ -1,3 +1,3 @@\n import {sqlparse} from \"./parser.js\";\n export {sqlparse};\n-export const __cratedb_version__ = \"5.6.4\"\n+export const __cratedb_version__ = \"5.7.2\"\ndiff --git a/cratedb_sqlparse_js/cratedb_sqlparse/models.js b/cratedb_sqlparse_js/cratedb_sqlparse/models.js\nnew file mode 100644\nindex 0000000..bd3042d\n--- /dev/null\n+++ b/cratedb_sqlparse_js/cratedb_sqlparse/models.js\n@@ -0,0 +1,48 @@\n+/**\n+ * Represents the metadata of the query, the actual interesting parts of the query such as:\n+ * table, schema, columns, options...\n+ */\n+export class Metadata {\n+\n+    /**\n+     * @param {Table[]} tables - The referenced tables in the query.\n+     * @param {object} parameterizedProperties - The properties whose value can be used to inject parameters, they start with '$', example: `CREATE TABLE a (b text) WITH (\"allocation.max_retries\" = $1)`\n+     * @param {object} withProperties - SQL properties, defined after a `WITH` statement. Example: `CREATE TABLE a (b text) WITH (\"allocation.max_retries\" = 5)`\n+     */\n+    constructor(tables, parameterizedProperties, withProperties) {\n+        this.tables = tables || []\n+        this.parameterizedProperties = parameterizedProperties || {}\n+        this.withProperties = withProperties || {}\n+    }\n+}\n+\n+/**\n+ *\n+ * @param {string} text\n+ * @param {string} quoted_with\n+ * @return {string}\n+ */\n+function quoted(text, quoted_with = '\"') {\n+    return quoted_with + text + quoted_with\n+}\n+\n+export class Table {\n+    /**\n+     *\n+     * @param {string} name\n+     * @param {string} schema\n+     * @property {string} fqn - Full qualified name, example: \"sys\".\"shards\"\n+     */\n+    constructor(name, schema) {\n+        this.name = name\n+        this.schema = schema\n+        this.fqn = this._getFqn()\n+    }\n+\n+    /**\n+     * @return {string} - The full qualified name, quoted.\n+     */\n+    _getFqn() {\n+        return (this.schema ? quoted(this.schema) + \".\" : \"\") + quoted(this.name)\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/cratedb_sqlparse/parser.js b/cratedb_sqlparse_js/cratedb_sqlparse/parser.js\nindex 90c6ef0..072f5f8 100644\n--- a/cratedb_sqlparse_js/cratedb_sqlparse/parser.js\n+++ b/cratedb_sqlparse_js/cratedb_sqlparse/parser.js\n@@ -1,14 +1,14 @@\n import SqlBaseLexer from \"./generated_parser/SqlBaseLexer.js\";\n import SqlBaseParser from \"./generated_parser/SqlBaseParser.js\";\n import {CommonTokenStream, ErrorListener, InputStream, Interval, Token} from \"antlr4\";\n-\n+import {AstBuilder} from \"./AstBuilder.js\";\n+import {Metadata} from \"./models.js\"\n function BEGIN_DOLLAR_QUOTED_STRING_action(localctx, actionIndex) {\n     if (actionIndex === 0) {\n         this.tags.push(this.text);\n     }\n }\n \n-\n function END_DOLLAR_QUOTED_STRING_action(localctx, actionIndex) {\n     if (actionIndex === 1) {\n         this.tags.pop();\n@@ -28,6 +28,67 @@ SqlBaseLexer.prototype.END_DOLLAR_QUOTED_STRING_sempred = END_DOLLAR_QUOTED_STRI\n \n export class ParseError extends Error {\n     name = 'ParseError'\n+\n+    /**\n+     *\n+     * @param {string} query\n+     * @param {string} msg\n+     * @param {object} offending_token\n+     * @param {object} e\n+     * @member {string} errorMessage\n+     * @member {string} errorMessageVerbose\n+     */\n+    constructor(query, msg, offending_token, e) {\n+        super(msg);\n+        this.query = query;\n+        this.msg = msg;\n+        this.offendingToken = offending_token;\n+        this.line = this.getLine();\n+        this.column = this.getColumn();\n+        this.errorMessage = this._getErrorMessage();\n+        this.errorMessageVerbose = this.getOriginalQueryWithErrorMarked()\n+    }\n+\n+    _getErrorMessage() {\n+        return `[line ${this.line}:${this.column} ${this.message}]`\n+    }\n+\n+    /**\n+     *\n+     * @returns {Number}\n+     */\n+    getColumn() {\n+        return this.offendingToken.column\n+    }\n+\n+    /**\n+     *\n+     * @returns {Number}\n+     */\n+    getLine() {\n+        return this.offendingToken.line\n+    }\n+\n+    /**\n+     *\n+     * @returns {string}\n+     */\n+    getOriginalQueryWithErrorMarked() {\n+        const query = this.offendingToken.source[1].strdata\n+        const offendingTokenText = query.substring(this.offendingToken.start, this.offendingToken.stop + 1)\n+        const queryLines = query.split(\"\\n\")\n+        const offendingLine = queryLines[this.getLine() - 1]\n+        const newLineOffset = offendingLine.indexOf(offendingTokenText)\n+\n+        const newline = (\n+            offendingLine\n+            + \"\\n\"\n+            + (\" \".repeat(newLineOffset) + \"^\".repeat(this.offendingToken.stop - this.offendingToken.start + 1))\n+        )\n+        queryLines[this.line - 1] = newline\n+\n+        return queryLines.join(\"\\n\")\n+    }\n }\n \n class CaseInsensitiveStream extends InputStream {\n@@ -41,27 +102,108 @@ class CaseInsensitiveStream extends InputStream {\n }\n \n class ExceptionErrorListener extends ErrorListener {\n+    errors = []\n     syntaxError(recognizer, offendingSymbol, line, column, msg, e) {\n-        throw new ParseError(`line${line}:${column} ${msg}`);\n+        throw new ParseError(\n+            e.ctx.parser.getTokenStream().getText(new Interval(\n+                e.ctx.start,\n+                e.offendingToken.tokenIndex)\n+            ),\n+            msg,\n+            offendingSymbol,\n+            e\n+        )\n     }\n }\n \n-class Statement {\n-    constructor(ctx) {\n+class ExceptionCollectorListener extends ErrorListener {\n+    constructor() {\n+        super();\n+        this.errors = [];\n+    }\n+\n+    syntaxError(recognizer, offendingSymbol, line, column, msg, e) {\n+        super.syntaxError(recognizer, offendingSymbol, line, column, msg, e);\n+        const error = new ParseError(\n+            e.ctx.parser.getTokenStream().getText(new Interval(\n+                e.ctx.start,\n+                e.offendingToken.tokenIndex)\n+            ),\n+            msg,\n+            offendingSymbol,\n+            e\n+        )\n+        this.errors.push(error)\n+    }\n+}\n+\n+\n+/*\n+* Represents a CrateDB SQL statement.\n+* */\n+export class Statement {\n+\n+    /**\n+     *\n+     * @member {Statement} query\n+     * @member {string} originalQuery\n+     * @member {Metadata} metadata\n+     * @member {string} type - The type of query, example: 'SELECT'\n+     * @member {string} tree\n+     * @param {object} ctx\n+     * @param {ParseError} exception\n+     */\n+    constructor(ctx, exception) {\n         this.query = ctx.parser.getTokenStream().getText(\n             new Interval(\n                 ctx.start.tokenIndex,\n                 ctx.stop.tokenIndex,\n             )\n         )\n-        this.original_query = ctx.parser.getTokenStream().getText()\n-        this.tree = ctx.toStringTree(null, ctx.parser)\n-        this.type = ctx.start.text\n-        this.ctx = ctx\n+        this.originalQuery = ctx.parser.getTokenStream().getText();\n+        this.tree = ctx.toStringTree(null, ctx.parser);\n+        this.type = ctx.start.text;\n+        this.ctx = ctx;\n+        this.exception = exception || null;\n+        this.metadata = new Metadata();\n+    }\n+}\n+\n+/**\n+ *\n+ * @param {string} string\n+ * @returns {string}\n+ */\n+function trim(string) {\n+    return string.replace(/^\\s+|\\s+$/gm, '');\n+}\n+\n+\n+function findSuitableError(statement, errors) {\n+    for (const error of errors) {\n+        let errorQuery = error.query;\n+\n+        if (errorQuery.endsWith(\";\")) {\n+            errorQuery = errorQuery.substring(0, errorQuery.length - 1);\n+        }\n+\n+        errorQuery = trim(errorQuery);\n+\n+        // If a good match error_query contains statement.query\n+        if (errorQuery.includes(statement.query)) {\n+            statement.exception = error;\n+            errors.splice(errors.indexOf(error), 1);\n+        }\n     }\n }\n \n-export function sqlparse(query) {\n+/**\n+ *\n+ * @param {string} query\n+ * @param {Boolean} raise_exception\n+ * @returns {Statement[]}\n+ */\n+export function sqlparse(query, raise_exception = false) {\n     const input = new CaseInsensitiveStream(query);\n     const lexer = new SqlBaseLexer(input);\n     lexer.removeErrorListeners();\n@@ -69,9 +211,45 @@ export function sqlparse(query) {\n \n     const parser = new SqlBaseParser(stream);\n     parser.removeErrorListeners();\n-    parser.addErrorListener(new ExceptionErrorListener());\n+\n+    const errorListener = raise_exception ? new ExceptionErrorListener() : new ExceptionCollectorListener()\n+\n+    parser.addErrorListener(errorListener);\n \n     const tree = parser.statements();\n \n-    return tree.children.filter((children) => children instanceof SqlBaseParser.StatementContext).map((children) => new Statement(children))\n-}\n+    const statementsContext = tree.children.filter((children) => children instanceof SqlBaseParser.StatementContext)\n+\n+    let statements = []\n+    for (const statementContext of statementsContext) {\n+        let stmt = new Statement(statementContext)\n+        findSuitableError(stmt, errorListener.errors)\n+        statements.push(stmt)\n+    }\n+\n+    if (errorListener.errors.length === 1) {\n+        // Fixme, what if there are two unassigned errors ?\n+        // can that even be possible?\n+        let error = errorListener.errors[0]\n+\n+        for (const stmt of statements) {\n+            if (stmt.exception === null && stmt.query.includes(error.query)) {\n+                stmt.exception = error\n+                break;\n+            }\n+        }\n+    }\n+\n+    if (errorListener.errors.length > 1) {\n+        console.error(\"Could not match errors to queries, too much ambiguity, please report it opening an issue with the query.\")\n+    }\n+\n+\n+    const stmtEnricher = new AstBuilder()\n+\n+    for (const stmt of statements) {\n+        stmtEnricher.enrich(stmt)\n+    }\n+\n+    return statements\n+}\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/package.json b/cratedb_sqlparse_js/package.json\nindex cd9b086..ece1f0c 100644\n--- a/cratedb_sqlparse_js/package.json\n+++ b/cratedb_sqlparse_js/package.json\n@@ -32,6 +32,7 @@\n   ],\n   \"main\": \"./dist/sqlparse.umd.cjs\",\n   \"module\": \"./dist/sqlparse.js\",\n+  \"types\": \"./dist/index.d.ts\",\n   \"exports\": {\n     \".\": {\n       \"import\": \"./dist/sqlparse.js\",\n@@ -42,11 +43,14 @@\n     \"antlr4\": \"^4.13.1-patch-1\"\n   },\n   \"devDependencies\": {\n+    \"jsdoc\": \"^4.0.3\",\n+    \"terser\": \"^5.31.1\",\n     \"vite\": \"^5.3.2\",\n     \"vitest\": \"^1.6.0\"\n   },\n   \"scripts\": {\n     \"test\": \"vitest run\",\n-    \"build\": \"vite build\"\n+    \"build\": \"vite build && tsc\",\n+    \"docs\": \"jsdoc *\"\n   }\n }\ndiff --git a/cratedb_sqlparse_js/tsconfig.json b/cratedb_sqlparse_js/tsconfig.json\nnew file mode 100644\nindex 0000000..c5f90d2\n--- /dev/null\n+++ b/cratedb_sqlparse_js/tsconfig.json\n@@ -0,0 +1,10 @@\n+{\n+  \"include\": [\"cratedb_sqlparse/**/*\"],\n+  \"compilerOptions\": {\n+    \"allowJs\": true,\n+    \"declaration\": true,\n+    \"emitDeclarationOnly\": true,\n+    \"outDir\": \"./dist\",\n+    \"declarationMap\": true,\n+  }\n+}\n\\ No newline at end of file\ndiff --git a/cratedb_sqlparse_js/vite.config.js b/cratedb_sqlparse_js/vite.config.js\nindex 248b0ff..15d0572 100644\n--- a/cratedb_sqlparse_js/vite.config.js\n+++ b/cratedb_sqlparse_js/vite.config.js\n@@ -9,6 +9,7 @@ import packageJson from './package.json';\n \n export default defineConfig({\n     build: {\n+        minify: \"terser\",\n         lib: {\n             // Could also be a dictionary or array of multiple entry points\n             entry: resolve(__dirname, 'cratedb_sqlparse/index.js'),\n@@ -17,7 +18,8 @@ export default defineConfig({\n         }, rollupOptions: {\n             // make sure to externalize deps that shouldn't be bundled\n             // into your library\n-            external: [], output: {},\n+            external: [],\n+            output: {},\n         },\n     },\n })\n\\ No newline at end of file\n", "instance_id": "crate__cratedb-sqlparse-38", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear with a detailed summary of the changes and the motivation behind them. It outlines the primary goals, such as changing the error handling strategy, adding verbose errors, extracting properties from SQL statements, and migrating to TypeScript for better compatibility. The inclusion of examples (e.g., Python and JavaScript code snippets) and references to specific issues (e.g., GitHub issue links) helps in understanding the context and desired outcomes. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not fully specify the expected behavior for edge cases in error matching (e.g., handling multiple unassigned errors), and the TypeScript migration lacks details on specific typing challenges or compatibility requirements. Additionally, the request for help with associating errors to specific statements is somewhat vague, lacking a clear definition of the desired solution or constraints. Overall, while the intent and scope are understandable, these minor gaps in detail slightly reduce the clarity.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.65, placing it in the \"Hard\" category due to the combination of technical challenges and the depth of understanding required. Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of Problem Description**: While mostly clear, the problem involves multiple distinct tasks (error handling strategy, verbose error reporting, property extraction, and TypeScript migration), each with its own complexity. Understanding and integrating these changes requires a solid grasp of the existing codebase and the ANTLR4 parsing framework, which adds to the difficulty.\n\n2. **Scope and Depth of Code Changes**: The code changes span multiple files and modules, affecting both Python and JavaScript implementations. Key modifications include the introduction of new classes (e.g., `AstBuilder`, `ExceptionCollectorListener`), significant updates to error handling logic in `parser.js`, and structural changes for TypeScript compatibility (e.g., adding type definitions in `tsconfig.json`). These changes impact core functionality like parsing and error reporting, requiring an understanding of interactions between lexer, parser, and custom listeners. While the changes do not fundamentally alter the system's architecture, they are substantial and involve a moderate amount of code (several hundred lines across multiple files).\n\n3. **Number of Technical Concepts**: Solving this problem requires familiarity with several technical concepts, including:\n   - **ANTLR4**: Deep knowledge of parser generation, error listeners, and visitor patterns to handle parsing errors and extract metadata (e.g., table names, properties).\n   - **JavaScript/TypeScript**: Understanding of ES modules, TypeScript configuration, and type declaration generation for library compatibility.\n   - **Error Handling**: Designing a robust strategy to collect and associate errors with specific statements, including verbose error formatting.\n   - **SQL Parsing**: Domain-specific knowledge of SQL syntax and CrateDB-specific dialects to handle features like `WITH` properties and parameterized values.\n   These concepts are moderately complex, especially for someone not already familiar with ANTLR4 or SQL parsing libraries.\n\n4. **Edge Cases and Error Handling**: The problem explicitly mentions challenges with associating errors to specific statements, and the provided solution (matching based on query text) is acknowledged as imperfect. Edge cases like multiple simultaneous errors, ambiguous error matching, or handling malformed queries are partially addressed but require further refinement. The code changes introduce new error handling logic (e.g., `ExceptionCollectorListener`), which adds complexity in ensuring robustness across various input scenarios. While not extremely intricate, these aspects demand careful consideration and testing.\n\nOverall, this problem requires a deep understanding of the parsing logic and moderate refactoring across multiple components. It involves handling non-trivial edge cases and integrating several technical concepts, justifying a difficulty score of 0.65. It falls short of \"Very Hard\" as it does not require advanced system-level changes or highly specialized domain knowledge beyond SQL parsing and ANTLR4 expertise.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use Ruff as linter and formatter\n## \ud83d\ude80 Feature Request\r\n\r\nCurrently, this project uses mypy for type checks and black plus isort for code formatting. It would be great to add [ruff](https://github.com/astral-sh/ruff) as a linter and use its formatter to replace black + isort.\r\n\r\n## \ud83d\udd08 Motivation\r\n\r\nRuff is very fast and we can use one tool instead for linting and formatting.\r\n\r\n## \ud83d\udef0 Alternatives\r\n\r\nJust add flake8. But ruff also implements those rules.\r\n\r\n## \ud83d\udcce Additional context\r\n\r\nn/a\r\n\n", "patch": "diff --git a/.github/release-drafter.yml b/.github/release-drafter.yml\nindex e85cb0f..41a1ab0 100644\n--- a/.github/release-drafter.yml\n+++ b/.github/release-drafter.yml\n@@ -8,12 +8,12 @@ sort-direction: ascending\n categories:\n   - title: \":rocket: Features\"\n     labels: [enhancement, feature]\n+  - title: \":boom: Breaking Changes\"\n+    labels: [breaking]\n   - title: \":wrench: Fixes & Refactoring\"\n     labels: [bug, refactoring, bugfix, fix]\n   - title: \":package: Build System & CI/CD\"\n     labels: [build, ci, testing]\n-  - title: \":boom: Breaking Changes\"\n-    labels: [breaking]\n   - title: \":pencil: Documentation\"\n     labels: [documentation]\n   - title: \":arrow_up: Dependencies updates\"\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 40eb5b6..ff224fd 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -1,5 +1,5 @@\n default_language_version:\n-  python: python3.11\n+  python: python3.12\n \n default_stages: [commit, push]\n \n@@ -13,26 +13,12 @@ repos:\n         exclude: LICENSE\n       - id: trailing-whitespace\n \n-  - repo: local\n+  - repo: https://github.com/astral-sh/ruff-pre-commit\n+    # Ruff version.\n+    rev: v0.3.4\n     hooks:\n-      - id: pyupgrade\n-        name: pyupgrade\n-        entry: poetry run pyupgrade --py38-plus\n-        types: [python]\n-        language: system\n-\n-  - repo: local\n-    hooks:\n-      - id: isort\n-        name: isort\n-        entry: poetry run isort --settings-path pyproject.toml\n-        types: [python]\n-        language: system\n-\n-  - repo: local\n-    hooks:\n-      - id: black\n-        name: black\n-        entry: poetry run black --config pyproject.toml\n-        types: [python]\n-        language: system\n+      # Run the linter.\n+      - id: ruff\n+        args: [ --fix ]\n+      # Run the formatter.\n+      - id: ruff-format\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex e76ff6f..fab2df7 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -24,12 +24,12 @@ make codestyle\n \n ### Checks\n \n-Many checks are configured for this project. Command `make check-codestyle` will check black and isort.\n-The `make check-typing` command will run mypy to check for typing issues. \n+Many checks are configured for this project. The command `make check-codestyle` will run ruff (lint and format) but won't edit files.\n+The `make check-typing` command will run mypy to check for typing issues.\n \n-Comand `make lint` applies both checks above.\n+The command `make lint` applies both checks above.\n \n-The `make check-safety` command will look at the security of your code. \n+The `make check-safety` command will look at the security of your code.\n \n ### Before submitting\n \n@@ -94,7 +94,7 @@ make pre-commit-install\n <summary>3. Codestyle</summary>\n <p>\n \n-Automatic formatting uses `pyupgrade`, `isort` and `black`.\n+Automatic formatting uses `ruff`.\n \n ```bash\n make codestyle\n@@ -109,7 +109,7 @@ Codestyle checks only, without rewriting files:\n make check-codestyle\n ```\n \n-> Note: `check-codestyle` uses `isort` and `black` library\n+> Note: `check-codestyle` uses `ruff`\n \n <details>\n <summary>4. Code security</summary>\ndiff --git a/Makefile b/Makefile\nindex d4c3327..1907696 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -36,9 +36,8 @@ pre-commit-install:\n #* Formatters\n .PHONY: codestyle\n codestyle:\n-\tpoetry run pyupgrade --exit-zero-even-if-changed --py38-plus **/*.py\n-\tpoetry run isort --settings-path pyproject.toml ./\n-\tpoetry run black --config pyproject.toml ./\n+\tpoetry run ruff check --fix\n+\tpoetry run ruff format\n \n .PHONY: formatting\n formatting: codestyle\n@@ -54,10 +53,16 @@ test-examples:\n \t  PYTHONPATH=$(PYTHONPATH) poetry run python $$file; \\\n \tdone\n \n+\tfor file in ./docs/sync_async/*.py; do \\\n+\t  sed -i 's/NUM_REQUESTS = 20/NUM_REQUESTS = 2/' $$file; \\\n+\t  PYTHONPATH=$(PYTHONPATH) poetry run python $$file; \\\n+\t  sed -i 's/NUM_REQUESTS = 2/NUM_REQUESTS = 20/' $$file; \\\n+\tdone\n+\n .PHONY: check-codestyle\n check-codestyle:\n-\tpoetry run isort --diff --check-only --settings-path pyproject.toml ./\n-\tpoetry run black --diff --check --config pyproject.toml ./\n+\tpoetry run ruff check\n+\tpoetry run ruff format --check\n \n .PHONY: check-typing\n check-typing:\ndiff --git a/README.md b/README.md\nindex 178e12f..4883389 100644\n--- a/README.md\n+++ b/README.md\n@@ -12,7 +12,7 @@ Create async HTTP requests with Python in no time.\n [![Package Version](https://img.shields.io/pypi/v/unparallel.svg)](https://pypi.org/project/unparallel/)\n [![Python Version](https://img.shields.io/pypi/pyversions/unparallel.svg)](https://pypi.org/project/unparallel/)\n <br>\n-[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n+[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n [![Security: bandit](https://img.shields.io/badge/security-bandit-green.svg)](https://github.com/PyCQA/bandit)\n [![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/RafaelWO/unparallel/blob/main/.pre-commit-config.yaml)\n [![License](https://img.shields.io/github/license/RafaelWO/unparallel)](https://github.com/RafaelWO/unparallel/blob/main/LICENSE)\n@@ -86,56 +86,17 @@ For more details on the usage and examples, check out the [docs][docs-usage].\n \n ## Why unparallel? Why async?\n Async is a really powerful feature - especially when you have to wait for I/O.\n+When we create HTTP requests synchronously we have to wait for every response before we can start with the next request.\n+If we utilize asynchronous programming, our runtime thread can do other work (other requests) during those periods of waiting.\n+\n Here is an example of making 20 web requests synchronously vs. asynchronously via `unparallel`.\n \n ![Sync-vs-Async][sync-async-gif]\n \n-As you can see, the async version finishes in less than a second.\n-\n-<details><summary>Code for sync</summary>\n-\n-```python\n-import httpx\n-from tqdm import tqdm\n-\n-\n-def main():\n-    url = \"https://httpbin.org\"\n-    paths = [f\"/get?i={i}\" for i in range(20)]\n-    results = [\n-        httpx.get(f\"{url}{path}\") for path in tqdm(paths, desc=\"Making sync requests\")\n-    ]\n-    assert len(results) == 20\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n-```\n-\n-</details>\n-\n-<details><summary>Code for async</summary>\n-\n-```python\n-import asyncio\n-\n-from unparallel import up\n-\n-\n-async def main():\n-    url = \"https://httpbin.org\"\n-    paths = [f\"/get?i={i}\" for i in range(20)]\n-\n-    results = await up(paths, base_url=url)\n-\n-    assert len(results) == 20\n-\n-\n-if __name__ == \"__main__\":\n-    asyncio.run(main())\n-```\n+As you can see, the async version finishes in less than a second while the sync code runs for around 10 seconds.\n+The difference gets even more drastic if you create much more requests.\n \n-</details>\n+You can find the sync/async code in the [docs folder](https://github.com/RafaelWO/unparallel/blob/main/docs/sync_async/).\n \n ## Contributing\n As this project is still in early development, I'm happy for any feedback and contributions!\ndiff --git a/docs/async.py b/docs/async.py\ndeleted file mode 100644\nindex 307313f..0000000\n--- a/docs/async.py\n+++ /dev/null\n@@ -1,16 +0,0 @@\n-import asyncio\n-\n-from unparallel import up\n-\n-\n-async def main():\n-    url = \"https://httpbin.org\"\n-    paths = [f\"/get?i={i}\" for i in range(20)]\n-\n-    results = await up(url, paths)\n-\n-    assert len(results) == 20\n-\n-\n-if __name__ == \"__main__\":\n-    asyncio.run(main())\ndiff --git a/docs/examples/wordpress.py b/docs/examples/wordpress.py\nindex 9838261..977c9d3 100644\n--- a/docs/examples/wordpress.py\n+++ b/docs/examples/wordpress.py\n@@ -21,7 +21,6 @@ async def main():\n     pagination_url = f\"/posts?per_page={page_size}\"\n \n     # Get page count\n-    page_size = 20\n     response = httpx.head(base_url + pagination_url)\n     total_pages = int(response.headers[\"X-WP-TotalPages\"])\n     print(f\"Website '{base_url}' has {total_pages} pages (page size = {page_size})\")\n@@ -30,7 +29,7 @@ async def main():\n     # the settings for this to work without errors. For me, it worked using\n     # max_connections=800 and timeout=60.\n \n-    total_pages = min(total_pages, 500)\n+    total_pages = min(total_pages, 100)\n \n     # Get all pages and flatten the result\n     paths = [f\"{pagination_url}&page={i}\" for i in range(1, total_pages + 1)]\ndiff --git a/docs/sync_async/async.py b/docs/sync_async/async.py\nnew file mode 100644\nindex 0000000..5a8f8ae\n--- /dev/null\n+++ b/docs/sync_async/async.py\n@@ -0,0 +1,18 @@\n+import asyncio\n+\n+from unparallel import up\n+\n+NUM_REQUESTS = 20\n+\n+\n+async def main():\n+    url = \"https://httpbin.org\"\n+    paths = [f\"/get?i={i}\" for i in range(NUM_REQUESTS)]\n+\n+    results = await up(paths, base_url=url)\n+\n+    assert len(results) == NUM_REQUESTS\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\ndiff --git a/docs/sync.py b/docs/sync_async/sync.py\nsimilarity index 66%\nrename from docs/sync.py\nrename to docs/sync_async/sync.py\nindex 53c9993..56bff2b 100644\n--- a/docs/sync.py\n+++ b/docs/sync_async/sync.py\n@@ -1,14 +1,16 @@\n import httpx\n from tqdm import tqdm\n \n+NUM_REQUESTS = 20\n+\n \n def main():\n     url = \"https://httpbin.org\"\n-    paths = [f\"/get?i={i}\" for i in range(20)]\n+    paths = [f\"/get?i={i}\" for i in range(NUM_REQUESTS)]\n     results = [\n         httpx.get(f\"{url}{path}\") for path in tqdm(paths, desc=\"Making sync requests\")\n     ]\n-    assert len(results) == 20\n+    assert len(results) == NUM_REQUESTS\n \n \n if __name__ == \"__main__\":\ndiff --git a/poetry.lock b/poetry.lock\nindex 7d80281..b8b42f0 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1,4 +1,4 @@\n-# This file is automatically @generated by Poetry 1.7.1 and should not be changed by hand.\n+# This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.\n \n [[package]]\n name = \"anyio\"\n@@ -22,20 +22,6 @@ doc = [\"Sphinx (>=7)\", \"packaging\", \"sphinx-autodoc-typehints (>=1.2.0)\", \"sphin\n test = [\"anyio[trio]\", \"coverage[toml] (>=7)\", \"exceptiongroup (>=1.2.0)\", \"hypothesis (>=4.0)\", \"psutil (>=5.9)\", \"pytest (>=7.0)\", \"pytest-mock (>=3.6.1)\", \"trustme\", \"uvloop (>=0.17)\"]\n trio = [\"trio (>=0.23)\"]\n \n-[[package]]\n-name = \"astroid\"\n-version = \"3.0.3\"\n-description = \"An abstract syntax tree for Python with inference support.\"\n-optional = false\n-python-versions = \">=3.8.0\"\n-files = [\n-    {file = \"astroid-3.0.3-py3-none-any.whl\", hash = \"sha256:92fcf218b89f449cdf9f7b39a269f8d5d617b27be68434912e11e79203963a17\"},\n-    {file = \"astroid-3.0.3.tar.gz\", hash = \"sha256:4148645659b08b70d72460ed1921158027a9e53ae8b7234149b1400eddacbb93\"},\n-]\n-\n-[package.dependencies]\n-typing-extensions = {version = \">=4.0.0\", markers = \"python_version < \\\"3.11\\\"\"}\n-\n [[package]]\n name = \"authlib\"\n version = \"1.3.0\"\n@@ -93,33 +79,33 @@ yaml = [\"PyYAML\"]\n \n [[package]]\n name = \"black\"\n-version = \"23.12.1\"\n+version = \"24.3.0\"\n description = \"The uncompromising code formatter.\"\n optional = false\n python-versions = \">=3.8\"\n files = [\n-    {file = \"black-23.12.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:e0aaf6041986767a5e0ce663c7a2f0e9eaf21e6ff87a5f95cbf3675bfd4c41d2\"},\n-    {file = \"black-23.12.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:c88b3711d12905b74206227109272673edce0cb29f27e1385f33b0163c414bba\"},\n-    {file = \"black-23.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a920b569dc6b3472513ba6ddea21f440d4b4c699494d2e972a1753cdc25df7b0\"},\n-    {file = \"black-23.12.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:3fa4be75ef2a6b96ea8d92b1587dd8cb3a35c7e3d51f0738ced0781c3aa3a5a3\"},\n-    {file = \"black-23.12.1-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:8d4df77958a622f9b5a4c96edb4b8c0034f8434032ab11077ec6c56ae9f384ba\"},\n-    {file = \"black-23.12.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:602cfb1196dc692424c70b6507593a2b29aac0547c1be9a1d1365f0d964c353b\"},\n-    {file = \"black-23.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9c4352800f14be5b4864016882cdba10755bd50805c95f728011bcb47a4afd59\"},\n-    {file = \"black-23.12.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:0808494f2b2df923ffc5723ed3c7b096bd76341f6213989759287611e9837d50\"},\n-    {file = \"black-23.12.1-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:25e57fd232a6d6ff3f4478a6fd0580838e47c93c83eaf1ccc92d4faf27112c4e\"},\n-    {file = \"black-23.12.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:2d9e13db441c509a3763a7a3d9a49ccc1b4e974a47be4e08ade2a228876500ec\"},\n-    {file = \"black-23.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6d1bd9c210f8b109b1762ec9fd36592fdd528485aadb3f5849b2740ef17e674e\"},\n-    {file = \"black-23.12.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:ae76c22bde5cbb6bfd211ec343ded2163bba7883c7bc77f6b756a1049436fbb9\"},\n-    {file = \"black-23.12.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1fa88a0f74e50e4487477bc0bb900c6781dbddfdfa32691e780bf854c3b4a47f\"},\n-    {file = \"black-23.12.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:a4d6a9668e45ad99d2f8ec70d5c8c04ef4f32f648ef39048d010b0689832ec6d\"},\n-    {file = \"black-23.12.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b18fb2ae6c4bb63eebe5be6bd869ba2f14fd0259bda7d18a46b764d8fb86298a\"},\n-    {file = \"black-23.12.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:c04b6d9d20e9c13f43eee8ea87d44156b8505ca8a3c878773f68b4e4812a421e\"},\n-    {file = \"black-23.12.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:3e1b38b3135fd4c025c28c55ddfc236b05af657828a8a6abe5deec419a0b7055\"},\n-    {file = \"black-23.12.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:4f0031eaa7b921db76decd73636ef3a12c942ed367d8c3841a0739412b260a54\"},\n-    {file = \"black-23.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:97e56155c6b737854e60a9ab1c598ff2533d57e7506d97af5481141671abf3ea\"},\n-    {file = \"black-23.12.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:dd15245c8b68fe2b6bd0f32c1556509d11bb33aec9b5d0866dd8e2ed3dba09c2\"},\n-    {file = \"black-23.12.1-py3-none-any.whl\", hash = \"sha256:78baad24af0f033958cad29731e27363183e140962595def56423e626f4bee3e\"},\n-    {file = \"black-23.12.1.tar.gz\", hash = \"sha256:4ce3ef14ebe8d9509188014d96af1c456a910d5b5cbf434a09fef7e024b3d0d5\"},\n+    {file = \"black-24.3.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:7d5e026f8da0322b5662fa7a8e752b3fa2dac1c1cbc213c3d7ff9bdd0ab12395\"},\n+    {file = \"black-24.3.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9f50ea1132e2189d8dff0115ab75b65590a3e97de1e143795adb4ce317934995\"},\n+    {file = \"black-24.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e2af80566f43c85f5797365077fb64a393861a3730bd110971ab7a0c94e873e7\"},\n+    {file = \"black-24.3.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:4be5bb28e090456adfc1255e03967fb67ca846a03be7aadf6249096100ee32d0\"},\n+    {file = \"black-24.3.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:4f1373a7808a8f135b774039f61d59e4be7eb56b2513d3d2f02a8b9365b8a8a9\"},\n+    {file = \"black-24.3.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:aadf7a02d947936ee418777e0247ea114f78aff0d0959461057cae8a04f20597\"},\n+    {file = \"black-24.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:65c02e4ea2ae09d16314d30912a58ada9a5c4fdfedf9512d23326128ac08ac3d\"},\n+    {file = \"black-24.3.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:bf21b7b230718a5f08bd32d5e4f1db7fc8788345c8aea1d155fc17852b3410f5\"},\n+    {file = \"black-24.3.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:2818cf72dfd5d289e48f37ccfa08b460bf469e67fb7c4abb07edc2e9f16fb63f\"},\n+    {file = \"black-24.3.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:4acf672def7eb1725f41f38bf6bf425c8237248bb0804faa3965c036f7672d11\"},\n+    {file = \"black-24.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c7ed6668cbbfcd231fa0dc1b137d3e40c04c7f786e626b405c62bcd5db5857e4\"},\n+    {file = \"black-24.3.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:56f52cfbd3dabe2798d76dbdd299faa046a901041faf2cf33288bc4e6dae57b5\"},\n+    {file = \"black-24.3.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:79dcf34b33e38ed1b17434693763301d7ccbd1c5860674a8f871bd15139e7837\"},\n+    {file = \"black-24.3.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:e19cb1c6365fd6dc38a6eae2dcb691d7d83935c10215aef8e6c38edee3f77abd\"},\n+    {file = \"black-24.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:65b76c275e4c1c5ce6e9870911384bff5ca31ab63d19c76811cb1fb162678213\"},\n+    {file = \"black-24.3.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:b5991d523eee14756f3c8d5df5231550ae8993e2286b8014e2fdea7156ed0959\"},\n+    {file = \"black-24.3.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:c45f8dff244b3c431b36e3224b6be4a127c6aca780853574c00faf99258041eb\"},\n+    {file = \"black-24.3.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:6905238a754ceb7788a73f02b45637d820b2f5478b20fec82ea865e4f5d4d9f7\"},\n+    {file = \"black-24.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d7de8d330763c66663661a1ffd432274a2f92f07feeddd89ffd085b5744f85e7\"},\n+    {file = \"black-24.3.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:7bb041dca0d784697af4646d3b62ba4a6b028276ae878e53f6b4f74ddd6db99f\"},\n+    {file = \"black-24.3.0-py3-none-any.whl\", hash = \"sha256:41622020d7120e01d377f74249e677039d20e6344ff5851de8a10f11f513bf93\"},\n+    {file = \"black-24.3.0.tar.gz\", hash = \"sha256:a0c9c4a0771afc6919578cec71ce82a3e31e054904e7197deacbc9382671c41f\"},\n ]\n \n [package.dependencies]\n@@ -493,21 +479,6 @@ ssh = [\"bcrypt (>=3.1.5)\"]\n test = [\"certifi\", \"pretend\", \"pytest (>=6.2.0)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-xdist\"]\n test-randomorder = [\"pytest-randomly\"]\n \n-[[package]]\n-name = \"dill\"\n-version = \"0.3.8\"\n-description = \"serialize all of Python\"\n-optional = false\n-python-versions = \">=3.8\"\n-files = [\n-    {file = \"dill-0.3.8-py3-none-any.whl\", hash = \"sha256:c36ca9ffb54365bdd2f8eb3eff7d2a21237f8452b57ace88b1ac615b7e815bd7\"},\n-    {file = \"dill-0.3.8.tar.gz\", hash = \"sha256:3ebe3c479ad625c4553aca177444d89b486b1d84982eeacded644afc0cf797ca\"},\n-]\n-\n-[package.extras]\n-graph = [\"objgraph (>=1.7.2)\"]\n-profile = [\"gprof2dot (>=2022.7.29)\"]\n-\n [[package]]\n name = \"distlib\"\n version = \"0.3.8\"\n@@ -730,23 +701,6 @@ files = [\n     {file = \"iniconfig-2.0.0.tar.gz\", hash = \"sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3\"},\n ]\n \n-[[package]]\n-name = \"isort\"\n-version = \"5.13.2\"\n-description = \"A Python utility / library to sort Python imports.\"\n-optional = false\n-python-versions = \">=3.8.0\"\n-files = [\n-    {file = \"isort-5.13.2-py3-none-any.whl\", hash = \"sha256:8ca5e72a8d85860d5a3fa69b8745237f2939afe12dbf656afbcb47fe72d947a6\"},\n-    {file = \"isort-5.13.2.tar.gz\", hash = \"sha256:48fdfcb9face5d58a4f6dde2e72a1fb8dcaf8ab26f95ab49fab84c2ddefb0109\"},\n-]\n-\n-[package.dependencies]\n-colorama = {version = \">=0.4.6\", optional = true, markers = \"extra == \\\"colors\\\"\"}\n-\n-[package.extras]\n-colors = [\"colorama (>=0.4.6)\"]\n-\n [[package]]\n name = \"jinja2\"\n version = \"3.1.3\"\n@@ -895,17 +849,6 @@ docs = [\"alabaster (==0.7.15)\", \"autodocsumm (==0.2.12)\", \"sphinx (==7.2.6)\", \"s\n lint = [\"pre-commit (>=2.4,<4.0)\"]\n tests = [\"pytest\", \"pytz\", \"simplejson\"]\n \n-[[package]]\n-name = \"mccabe\"\n-version = \"0.7.0\"\n-description = \"McCabe checker, plugin for flake8\"\n-optional = false\n-python-versions = \">=3.6\"\n-files = [\n-    {file = \"mccabe-0.7.0-py2.py3-none-any.whl\", hash = \"sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e\"},\n-    {file = \"mccabe-0.7.0.tar.gz\", hash = \"sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325\"},\n-]\n-\n [[package]]\n name = \"mdurl\"\n version = \"0.1.2\"\n@@ -1340,36 +1283,6 @@ files = [\n plugins = [\"importlib-metadata\"]\n windows-terminal = [\"colorama (>=0.4.6)\"]\n \n-[[package]]\n-name = \"pylint\"\n-version = \"3.0.4\"\n-description = \"python code static checker\"\n-optional = false\n-python-versions = \">=3.8.0\"\n-files = [\n-    {file = \"pylint-3.0.4-py3-none-any.whl\", hash = \"sha256:59ab3532506f32affefeb50d5057a221bb351f5a1383fa36c424c2c6c05e7005\"},\n-    {file = \"pylint-3.0.4.tar.gz\", hash = \"sha256:d73b70b3fff8f3fbdcb49a209b9c7d71d8090c138d61d576d1895e152cb392b3\"},\n-]\n-\n-[package.dependencies]\n-astroid = \">=3.0.1,<=3.1.0-dev0\"\n-colorama = {version = \">=0.4.5\", markers = \"sys_platform == \\\"win32\\\"\"}\n-dill = [\n-    {version = \">=0.2\", markers = \"python_version < \\\"3.11\\\"\"},\n-    {version = \">=0.3.7\", markers = \"python_version >= \\\"3.12\\\"\"},\n-    {version = \">=0.3.6\", markers = \"python_version >= \\\"3.11\\\" and python_version < \\\"3.12\\\"\"},\n-]\n-isort = \">=4.2.5,<5.13.0 || >5.13.0,<6\"\n-mccabe = \">=0.6,<0.8\"\n-platformdirs = \">=2.2.0\"\n-tomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n-tomlkit = \">=0.10.1\"\n-typing-extensions = {version = \">=3.10.0\", markers = \"python_version < \\\"3.10\\\"\"}\n-\n-[package.extras]\n-spelling = [\"pyenchant (>=3.2,<4.0)\"]\n-testutils = [\"gitpython (>3)\"]\n-\n [[package]]\n name = \"pymdown-extensions\"\n version = \"10.7\"\n@@ -1516,20 +1429,6 @@ files = [\n     {file = \"pytz-2024.1.tar.gz\", hash = \"sha256:2a29735ea9c18baf14b448846bde5a48030ed267578472d8955cd0e7443a9812\"},\n ]\n \n-[[package]]\n-name = \"pyupgrade\"\n-version = \"3.8.0\"\n-description = \"A tool to automatically upgrade syntax for newer versions.\"\n-optional = false\n-python-versions = \">=3.8\"\n-files = [\n-    {file = \"pyupgrade-3.8.0-py2.py3-none-any.whl\", hash = \"sha256:08d0e6129f5e9da7e7a581bdbea689e0d49c3c93eeaf156a07ae2fd794f52660\"},\n-    {file = \"pyupgrade-3.8.0.tar.gz\", hash = \"sha256:1facb0b8407cca468dfcc1d13717e3a85aa37b9e6e7338664ad5bfe5ef50c867\"},\n-]\n-\n-[package.dependencies]\n-tokenize-rt = \">=3.2.0\"\n-\n [[package]]\n name = \"pyyaml\"\n version = \"6.0.1\"\n@@ -1555,7 +1454,6 @@ files = [\n     {file = \"PyYAML-6.0.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9\"},\n-    {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-win32.whl\", hash = \"sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54\"},\n@@ -1787,24 +1685,24 @@ python-versions = \">=3.6\"\n files = [\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:b42169467c42b692c19cf539c38d4602069d8c1505e97b86387fcf7afb766e1d\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-macosx_13_0_arm64.whl\", hash = \"sha256:07238db9cbdf8fc1e9de2489a4f68474e70dffcb32232db7c08fa61ca0c7c462\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux2014_aarch64.whl\", hash = \"sha256:d92f81886165cb14d7b067ef37e142256f1c6a90a65cd156b063a43da1708cfd\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\", hash = \"sha256:fff3573c2db359f091e1589c3d7c5fc2f86f5bdb6f24252c2d8e539d4e45f412\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_24_aarch64.whl\", hash = \"sha256:aa2267c6a303eb483de8d02db2871afb5c5fc15618d894300b88958f729ad74f\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:840f0c7f194986a63d2c2465ca63af8ccbbc90ab1c6001b1978f05119b5e7334\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:024cfe1fc7c7f4e1aff4a81e718109e13409767e4f871443cbff3dba3578203d\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-win32.whl\", hash = \"sha256:c69212f63169ec1cfc9bb44723bf2917cbbd8f6191a00ef3410f5a7fe300722d\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-win_amd64.whl\", hash = \"sha256:cabddb8d8ead485e255fe80429f833172b4cadf99274db39abc080e068cbcc31\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:bef08cd86169d9eafb3ccb0a39edb11d8e25f3dae2b28f5c52fd997521133069\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl\", hash = \"sha256:b16420e621d26fdfa949a8b4b47ade8810c56002f5389970db4ddda51dbff248\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux2014_aarch64.whl\", hash = \"sha256:b5edda50e5e9e15e54a6a8a0070302b00c518a9d32accc2346ad6c984aacd279\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\", hash = \"sha256:25c515e350e5b739842fc3228d662413ef28f295791af5e5110b543cf0b57d9b\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_24_aarch64.whl\", hash = \"sha256:1707814f0d9791df063f8c19bb51b0d1278b8e9a2353abbb676c2f685dee6afe\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:46d378daaac94f454b3a0e3d8d78cafd78a026b1d71443f4966c696b48a6d899\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:09b055c05697b38ecacb7ac50bdab2240bfca1a0c4872b0fd309bb07dc9aa3a9\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-win32.whl\", hash = \"sha256:53a300ed9cea38cf5a2a9b069058137c2ca1ce658a874b79baceb8f892f915a7\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl\", hash = \"sha256:c2a72e9109ea74e511e29032f3b670835f8a59bbdc9ce692c5b4ed91ccf1eedb\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:ebc06178e8821efc9692ea7544aa5644217358490145629914d8020042c24aa1\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-macosx_13_0_arm64.whl\", hash = \"sha256:edaef1c1200c4b4cb914583150dcaa3bc30e592e907c01117c08b13a07255ec2\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-manylinux2014_aarch64.whl\", hash = \"sha256:7048c338b6c86627afb27faecf418768acb6331fc24cfa56c93e8c9780f815fa\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:d176b57452ab5b7028ac47e7b3cf644bcfdc8cacfecf7e71759f7f51a59e5c92\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-manylinux_2_24_aarch64.whl\", hash = \"sha256:1dc67314e7e1086c9fdf2680b7b6c2be1c0d8e3a8279f2e993ca2a7545fecf62\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:3213ece08ea033eb159ac52ae052a4899b56ecc124bb80020d9bbceeb50258e9\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:aab7fd643f71d7946f2ee58cc88c9b7bfc97debd71dcc93e03e2d174628e7e2d\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-win32.whl\", hash = \"sha256:5c365d91c88390c8d0a8545df0b5857172824b1c604e867161e6b3d59a827eaa\"},\n@@ -1812,7 +1710,7 @@ files = [\n     {file = \"ruamel.yaml.clib-0.2.8-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:a5aa27bad2bb83670b71683aae140a1f52b0857a2deff56ad3f6c13a017a26ed\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:c58ecd827313af6864893e7af0a3bb85fd529f862b6adbefe14643947cfe2942\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-macosx_12_0_arm64.whl\", hash = \"sha256:f481f16baec5290e45aebdc2a5168ebc6d35189ae6fea7a58787613a25f6e875\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-manylinux_2_24_aarch64.whl\", hash = \"sha256:77159f5d5b5c14f7c34073862a6b7d34944075d9f93e681638f6d753606c6ce6\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-manylinux2014_aarch64.whl\", hash = \"sha256:3fcc54cb0c8b811ff66082de1680b4b14cf8a81dce0d4fbf665c2265a81e07a1\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:7f67a1ee819dc4562d444bbafb135832b0b909f81cc90f7aa00260968c9ca1b3\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:4ecbf9c3e19f9562c7fdd462e8d18dd902a47ca046a2e64dba80699f0b6c09b7\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:87ea5ff66d8064301a154b3933ae406b0863402a799b16e4a1d24d9fbbcbe0d3\"},\n@@ -1820,7 +1718,7 @@ files = [\n     {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-win_amd64.whl\", hash = \"sha256:3f215c5daf6a9d7bbed4a0a4f760f3113b10e82ff4c5c44bec20a68c8014f675\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1b617618914cb00bf5c34d4357c37aa15183fa229b24767259657746c9077615\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-macosx_12_0_arm64.whl\", hash = \"sha256:a6a9ffd280b71ad062eae53ac1659ad86a17f59a0fdc7699fd9be40525153337\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux_2_24_aarch64.whl\", hash = \"sha256:305889baa4043a09e5b76f8e2a51d4ffba44259f6b4c72dec8ca56207d9c6fe1\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux2014_aarch64.whl\", hash = \"sha256:665f58bfd29b167039f714c6998178d27ccd83984084c286110ef26b230f259f\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:700e4ebb569e59e16a976857c8798aee258dceac7c7d6b50cab63e080058df91\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:e2b4c44b60eadec492926a7270abb100ef9f72798e18743939bdbf037aab8c28\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:e79e5db08739731b0ce4850bed599235d601701d5694c36570a99a0c5ca41a9d\"},\n@@ -1828,7 +1726,7 @@ files = [\n     {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-win_amd64.whl\", hash = \"sha256:56f4252222c067b4ce51ae12cbac231bce32aee1d33fbfc9d17e5b8d6966c312\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:03d1162b6d1df1caa3a4bd27aa51ce17c9afc2046c31b0ad60a0a96ec22f8001\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:bba64af9fa9cebe325a62fa398760f5c7206b215201b0ec825005f1b18b9bccf\"},\n-    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux_2_24_aarch64.whl\", hash = \"sha256:a1a45e0bb052edf6a1d3a93baef85319733a888363938e1fc9924cb00c8df24c\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux2014_aarch64.whl\", hash = \"sha256:9eb5dee2772b0f704ca2e45b1713e4e5198c18f515b52743576d196348f374d3\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:da09ad1c359a728e112d60116f626cc9f29730ff3e0e7db72b9a2dbc2e4beed5\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:184565012b60405d93838167f425713180b949e9d8dd0bbc7b49f074407c5a8b\"},\n     {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:a75879bacf2c987c003368cf14bed0ffe99e8e85acfa6c0bfffc21a090f16880\"},\n@@ -1839,28 +1737,28 @@ files = [\n \n [[package]]\n name = \"ruff\"\n-version = \"0.3.0\"\n+version = \"0.3.4\"\n description = \"An extremely fast Python linter and code formatter, written in Rust.\"\n optional = false\n python-versions = \">=3.7\"\n files = [\n-    {file = \"ruff-0.3.0-py3-none-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl\", hash = \"sha256:7deb528029bacf845bdbb3dbb2927d8ef9b4356a5e731b10eef171e3f0a85944\"},\n-    {file = \"ruff-0.3.0-py3-none-macosx_10_12_x86_64.whl\", hash = \"sha256:e1e0d4381ca88fb2b73ea0766008e703f33f460295de658f5467f6f229658c19\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2f7dbba46e2827dfcb0f0cc55fba8e96ba7c8700e0a866eb8cef7d1d66c25dcb\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:23dbb808e2f1d68eeadd5f655485e235c102ac6f12ad31505804edced2a5ae77\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:3ef655c51f41d5fa879f98e40c90072b567c666a7114fa2d9fe004dffba00932\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl\", hash = \"sha256:d0d3d7ef3d4f06433d592e5f7d813314a34601e6c5be8481cccb7fa760aa243e\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b08b356d06a792e49a12074b62222f9d4ea2a11dca9da9f68163b28c71bf1dd4\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9343690f95710f8cf251bee1013bf43030072b9f8d012fbed6ad702ef70d360a\"},\n-    {file = \"ruff-0.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a1f3ed501a42f60f4dedb7805fa8d4534e78b4e196f536bac926f805f0743d49\"},\n-    {file = \"ruff-0.3.0-py3-none-musllinux_1_2_aarch64.whl\", hash = \"sha256:cc30a9053ff2f1ffb505a585797c23434d5f6c838bacfe206c0e6cf38c921a1e\"},\n-    {file = \"ruff-0.3.0-py3-none-musllinux_1_2_armv7l.whl\", hash = \"sha256:5da894a29ec018a8293d3d17c797e73b374773943e8369cfc50495573d396933\"},\n-    {file = \"ruff-0.3.0-py3-none-musllinux_1_2_i686.whl\", hash = \"sha256:755c22536d7f1889be25f2baf6fedd019d0c51d079e8417d4441159f3bcd30c2\"},\n-    {file = \"ruff-0.3.0-py3-none-musllinux_1_2_x86_64.whl\", hash = \"sha256:dd73fe7f4c28d317855da6a7bc4aa29a1500320818dd8f27df95f70a01b8171f\"},\n-    {file = \"ruff-0.3.0-py3-none-win32.whl\", hash = \"sha256:19eacceb4c9406f6c41af806418a26fdb23120dfe53583df76d1401c92b7c14b\"},\n-    {file = \"ruff-0.3.0-py3-none-win_amd64.whl\", hash = \"sha256:128265876c1d703e5f5e5a4543bd8be47c73a9ba223fd3989d4aa87dd06f312f\"},\n-    {file = \"ruff-0.3.0-py3-none-win_arm64.whl\", hash = \"sha256:e3a4a6d46aef0a84b74fcd201a4401ea9a6cd85614f6a9435f2d33dd8cefbf83\"},\n-    {file = \"ruff-0.3.0.tar.gz\", hash = \"sha256:0886184ba2618d815067cf43e005388967b67ab9c80df52b32ec1152ab49f53a\"},\n+    {file = \"ruff-0.3.4-py3-none-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl\", hash = \"sha256:60c870a7d46efcbc8385d27ec07fe534ac32f3b251e4fc44b3cbfd9e09609ef4\"},\n+    {file = \"ruff-0.3.4-py3-none-macosx_10_12_x86_64.whl\", hash = \"sha256:6fc14fa742e1d8f24910e1fff0bd5e26d395b0e0e04cc1b15c7c5e5fe5b4af91\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d3ee7880f653cc03749a3bfea720cf2a192e4f884925b0cf7eecce82f0ce5854\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:cf133dd744f2470b347f602452a88e70dadfbe0fcfb5fd46e093d55da65f82f7\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:3f3860057590e810c7ffea75669bdc6927bfd91e29b4baa9258fd48b540a4365\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl\", hash = \"sha256:986f2377f7cf12efac1f515fc1a5b753c000ed1e0a6de96747cdf2da20a1b369\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c4fd98e85869603e65f554fdc5cddf0712e352fe6e61d29d5a6fe087ec82b76c\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:64abeed785dad51801b423fa51840b1764b35d6c461ea8caef9cf9e5e5ab34d9\"},\n+    {file = \"ruff-0.3.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:df52972138318bc7546d92348a1ee58449bc3f9eaf0db278906eb511889c4b50\"},\n+    {file = \"ruff-0.3.4-py3-none-musllinux_1_2_aarch64.whl\", hash = \"sha256:98e98300056445ba2cc27d0b325fd044dc17fcc38e4e4d2c7711585bd0a958ed\"},\n+    {file = \"ruff-0.3.4-py3-none-musllinux_1_2_armv7l.whl\", hash = \"sha256:519cf6a0ebed244dce1dc8aecd3dc99add7a2ee15bb68cf19588bb5bf58e0488\"},\n+    {file = \"ruff-0.3.4-py3-none-musllinux_1_2_i686.whl\", hash = \"sha256:bb0acfb921030d00070539c038cd24bb1df73a2981e9f55942514af8b17be94e\"},\n+    {file = \"ruff-0.3.4-py3-none-musllinux_1_2_x86_64.whl\", hash = \"sha256:cf187a7e7098233d0d0c71175375c5162f880126c4c716fa28a8ac418dcf3378\"},\n+    {file = \"ruff-0.3.4-py3-none-win32.whl\", hash = \"sha256:af27ac187c0a331e8ef91d84bf1c3c6a5dea97e912a7560ac0cef25c526a4102\"},\n+    {file = \"ruff-0.3.4-py3-none-win_amd64.whl\", hash = \"sha256:de0d5069b165e5a32b3c6ffbb81c350b1e3d3483347196ffdf86dc0ef9e37dd6\"},\n+    {file = \"ruff-0.3.4-py3-none-win_arm64.whl\", hash = \"sha256:6810563cc08ad0096b57c717bd78aeac888a1bfd38654d9113cb3dc4d3f74232\"},\n+    {file = \"ruff-0.3.4.tar.gz\", hash = \"sha256:f0f4484c6541a99862b693e13a151435a279b271cff20e37101116a21e2a1ad1\"},\n ]\n \n [[package]]\n@@ -1977,17 +1875,6 @@ files = [\n [package.dependencies]\n pbr = \">=2.0.0,<2.1.0 || >2.1.0\"\n \n-[[package]]\n-name = \"tokenize-rt\"\n-version = \"5.2.0\"\n-description = \"A wrapper around the stdlib `tokenize` which roundtrips.\"\n-optional = false\n-python-versions = \">=3.8\"\n-files = [\n-    {file = \"tokenize_rt-5.2.0-py2.py3-none-any.whl\", hash = \"sha256:b79d41a65cfec71285433511b50271b05da3584a1da144a0752e9c621a285289\"},\n-    {file = \"tokenize_rt-5.2.0.tar.gz\", hash = \"sha256:9fe80f8a5c1edad2d3ede0f37481cc0cc1538a2f442c9c2f9e4feacd2792d054\"},\n-]\n-\n [[package]]\n name = \"tomli\"\n version = \"2.0.1\"\n@@ -1999,17 +1886,6 @@ files = [\n     {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\n ]\n \n-[[package]]\n-name = \"tomlkit\"\n-version = \"0.12.3\"\n-description = \"Style preserving TOML library\"\n-optional = false\n-python-versions = \">=3.7\"\n-files = [\n-    {file = \"tomlkit-0.12.3-py3-none-any.whl\", hash = \"sha256:b0a645a9156dc7cb5d3a1f0d4bab66db287fcb8e0430bdd4664a095ea16414ba\"},\n-    {file = \"tomlkit-0.12.3.tar.gz\", hash = \"sha256:75baf5012d06501f07bee5bf8e801b9f343e7aac5a92581f20f80ce632e6b5a4\"},\n-]\n-\n [[package]]\n name = \"tqdm\"\n version = \"4.66.2\"\n@@ -2051,6 +1927,17 @@ dev = [\"autoflake (>=1.3.1,<2.0.0)\", \"flake8 (>=3.8.3,<4.0.0)\", \"pre-commit (>=2\n doc = [\"cairosvg (>=2.5.2,<3.0.0)\", \"mdx-include (>=1.4.1,<2.0.0)\", \"mkdocs (>=1.1.2,<2.0.0)\", \"mkdocs-material (>=8.1.4,<9.0.0)\", \"pillow (>=9.3.0,<10.0.0)\"]\n test = [\"black (>=22.3.0,<23.0.0)\", \"coverage (>=6.2,<7.0)\", \"isort (>=5.0.6,<6.0.0)\", \"mypy (==0.910)\", \"pytest (>=4.4.0,<8.0.0)\", \"pytest-cov (>=2.10.0,<5.0.0)\", \"pytest-sugar (>=0.9.4,<0.10.0)\", \"pytest-xdist (>=1.32.0,<4.0.0)\", \"rich (>=10.11.0,<14.0.0)\", \"shellingham (>=1.3.0,<2.0.0)\"]\n \n+[[package]]\n+name = \"types-tqdm\"\n+version = \"4.66.0.20240106\"\n+description = \"Typing stubs for tqdm\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"types-tqdm-4.66.0.20240106.tar.gz\", hash = \"sha256:7acf4aade5bad3ded76eb829783f9961b1c2187948eaa6dd1ae8644dff95a938\"},\n+    {file = \"types_tqdm-4.66.0.20240106-py3-none-any.whl\", hash = \"sha256:7459b0f441b969735685645a5d8480f7912b10d05ab45f99a2db8a8e45cb550b\"},\n+]\n+\n [[package]]\n name = \"typing-extensions\"\n version = \"4.9.0\"\n@@ -2172,4 +2059,4 @@ testing = [\"big-O\", \"jaraco.functools\", \"jaraco.itertools\", \"more-itertools\", \"p\n [metadata]\n lock-version = \"2.0\"\n python-versions = \">=3.8,<4.0\"\n-content-hash = \"d56208951bfe1f3be689be845d5b48d92ce7dabec3f27aa666d6bf3611131c85\"\n+content-hash = \"286a1eff40cff4ec0a44780afc25eb067b32b5ef99833a743cd36f6ffc8d2ae0\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 87c0d56..f33e108 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -17,8 +17,8 @@ homepage = \"https://github.com/RafaelWO/unparallel\"\n keywords = [\"async\", \"http\", \"requests\", \"network\"]\n \n # Pypi classifiers: https://pypi.org/classifiers/\n-classifiers = [  #! Update me\n-  \"Development Status :: 3 - Alpha\",\n+classifiers = [\n+  \"Development Status :: 4 - Beta\",\n   \"Intended Audience :: Developers\",\n   \"Operating System :: OS Independent\",\n   \"Topic :: Software Development :: Libraries :: Python Modules\",\n@@ -31,8 +31,6 @@ classifiers = [  #! Update me\n   \"Programming Language :: Python :: 3.12\",\n ]\n \n-\n-\n [tool.poetry.dependencies]\n python = \">=3.8,<4.0\"\n httpx = \">=0.21.0\"\n@@ -40,15 +38,11 @@ tqdm = \">=4.61.2\"\n \n [tool.poetry.group.dev.dependencies]\n bandit = \"^1.7.1\"\n-black = \"^23.11.0\"\n-isort = {extras = [\"colors\"], version = \"^5.10.1\"}\n mypy = \"^1.7\"\n mypy-extensions = \"^1.0.0\"\n pre-commit = \"^3.5.0\"\n pydocstyle = \"^6.1.1\"\n-pylint = \"^3.0.2\"\n pytest = \">=7.4.3,<9.0.0\"\n-pyupgrade = \"^3.8.0\"\n safety = \">=2.3.5,<4.0.0\"\n coverage = \"^7.3.2\"\n coverage-badge = \"^1.1.0\"\n@@ -58,70 +52,31 @@ respx = \"^0.20.2\"\n bump2version = \"^1.0.1\"\n pytest-markdown-docs = \"^0.5.0\"\n pytest-examples = \"^0.0.10\"\n+ruff = \"^0.3.4\"\n+types-tqdm = \"^4.66.0.20240106\"\n \n [tool.poetry.group.docs.dependencies]\n mkdocs-material = \"^9.5.1\"\n mkdocstrings-python = \"^1.7.5\"\n mike = \"^2.0.0\"\n \n-[tool.black]\n-# https://github.com/psf/black\n-target-version = [\"py38\"]\n-line-length = 88\n-color = true\n-\n-exclude = '''\n-/(\n-    \\.git\n-    | \\.hg\n-    | \\.mypy_cache\n-    | \\.tox\n-    | \\.venv\n-    | _build\n-    | buck-out\n-    | build\n-    | dist\n-    | env\n-    | venv\n-)/\n-'''\n-\n-[tool.isort]\n-# https://github.com/timothycrosley/isort/\n-py_version = 38\n-line_length = 88\n-\n-include_trailing_comma = true\n-profile = \"black\"\n-multi_line_output = 3\n-indent = 4\n-color_output = true\n+##########################################\n+\n+[tool.ruff.lint]\n+select = [\n+    \"E\",  # pycodestyle errors\n+    \"W\",  # pycodestyle warnings\n+    \"F\",  # pyflakes\n+    \"I\",  # isort\n+    \"B\",  # flake8-bugbear\n+    \"C4\",  # flake8-comprehensions\n+    \"UP\",  # pyupgrade\n+]\n \n [tool.mypy]\n # https://mypy.readthedocs.io/en/latest/config_file.html#using-a-pyproject-toml-file\n python_version = \"3.8\"\n-pretty = true\n-show_traceback = true\n-color_output = true\n-\n-allow_redefinition = false\n-check_untyped_defs = true\n-disallow_any_generics = true\n-disallow_incomplete_defs = true\n-ignore_missing_imports = true\n-implicit_reexport = false\n-no_implicit_optional = true\n-show_column_numbers = true\n-show_error_codes = true\n-show_error_context = true\n-strict_equality = true\n-strict_optional = true\n-warn_no_return = true\n-warn_redundant_casts = true\n-warn_return_any = true\n-warn_unreachable = true\n-warn_unused_configs = true\n-warn_unused_ignores = true\n+strict = true\n \n \n [tool.pytest.ini_options]\ndiff --git a/unparallel/utils.py b/unparallel/utils.py\nindex 4208ddd..8b28a66 100644\n--- a/unparallel/utils.py\n+++ b/unparallel/utils.py\n@@ -1,9 +1,15 @@\n+from types import TracebackType\n+from typing import Any, Type\n+\n+\n class AsyncNullContext:\n     \"\"\"A nullcontext including __aenter__ and __aexit__ to support Python versions\n     below 3.10.\"\"\"\n \n-    async def __aenter__(self):\n+    async def __aenter__(self) -> None:\n         return None\n \n-    async def __aexit__(self, *excinfo):\n+    async def __aexit__(\n+        self, exc_type: Type[Exception], exc_value: Any, traceback: TracebackType\n+    ) -> None:\n         pass\n", "instance_id": "RafaelWO__unparallel-165", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the current linting and formatting tools (mypy, black, and isort) with Ruff, motivated by Ruff's speed and ability to consolidate linting and formatting into a single tool. The goal is explicitly stated, and the motivation provides a reasonable justification. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not specify how Ruff should be configured (e.g., which rules to enable or disable), whether there are specific compatibility concerns with the existing codebase, or if there are any edge cases or migration challenges to consider when replacing black and isort. Additionally, there are no examples or detailed requirements for integration into the project's workflows beyond a general mention of using Ruff as a linter and formatter. These omissions leave room for interpretation, which could lead to implementation inconsistencies.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to the relatively straightforward nature of the changes required, though it involves understanding and modifying multiple configuration files and scripts across the codebase. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes impact multiple files, including configuration files (e.g., `.pre-commit-config.yaml`, `pyproject.toml`), documentation (e.g., `README.md`, `CONTRIBUTING.md`), and build scripts (e.g., `Makefile`). However, the modifications are mostly confined to replacing references and configurations for black, isort, and pyupgrade with Ruff. The overall amount of code change is moderate, and it does not significantly impact the system's architecture since it primarily affects development tools rather than the core functionality of the project.\n\n2. **Number of Technical Concepts**: The task requires basic familiarity with Python development tools and workflows, specifically linting and formatting tools like Ruff, black, and isort. It also involves understanding pre-commit hooks, configuration file syntax (e.g., TOML, YAML), and Makefile syntax. These concepts are relatively simple for a developer with moderate experience in Python projects. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce significant error handling requirements. However, there is a minor implicit concern regarding compatibility\u2014ensuring that Ruff's rules and formatting align with the project's existing style and that no existing code is inadvertently broken by stricter linting rules. This is a low-complexity concern that can be addressed through testing and configuration adjustments.\n\n4. **Overall Assessment**: The task requires understanding the project's development environment setup and making consistent changes across several files, but it does not involve deep architectural changes or complex logic. It is more involved than a trivial fix (e.g., changing a constant) due to the need to coordinate changes across multiple files and ensure consistency in documentation and workflows. Therefore, a score of 0.35 reflects an \"Easy\" task with slightly more complexity than the lowest end of the range due to the breadth of files affected and the need for basic tool configuration knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Pretty install on 'manage.py shell'\n### Description\n\nRich has \"pretty\" mode that can be installed in the plain python repl, and in ipython:\r\n\r\nhttps://rich.readthedocs.io/en/latest/introduction.html?highlight=install#rich-in-the-repl\r\n\r\nWe could extend `manage.py shell` to use that automatically.\n", "patch": "diff --git a/CHANGELOG.rst b/CHANGELOG.rst\nindex ceed0cf..b58878c 100644\n--- a/CHANGELOG.rst\n+++ b/CHANGELOG.rst\n@@ -2,6 +2,11 @@\n Changelog\n =========\n \n+* Add a ``shell`` command extension that enables Rich\u2019s pretty-printing.\n+  Activate this feature by adding ``django_rich`` to your ``INSTALLED_APPS`` setting.\n+\n+  Thanks to q0w in `PR #78 <https://github.com/adamchainz/django-rich/pull/78>`__.\n+\n 1.9.0 (2024-06-19)\n ------------------\n \ndiff --git a/README.rst b/README.rst\nindex 28489f7..c03fcc5 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -49,6 +49,25 @@ Follow the documentation below to use them.\n Reference\n ---------\n \n+``shell`` command integration\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+django-rich has an extended version of Django\u2019s built-in |shell command|__ that enables `Rich\u2019s pretty-printing <https://rich.readthedocs.io/en/stable/introduction.html?highlight=install#rich-in-the-repl>`__.\n+To activate this feature, add ``django_rich`` to your ``INSTALLED_APPS`` setting:\n+\n+.. |shell command| replace:: ``shell`` command\n+__ https://docs.djangoproject.com/en/stable/ref/django-admin/#shell\n+\n+   .. code-block:: python\n+\n+       INSTALLED_APPS = [\n+           ...,\n+           \"django_rich\",\n+           ...,\n+       ]\n+\n+This feature only affects the Python and bypthon interpreters, not IPython.\n+\n ``django_rich.management.RichCommand``\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \ndiff --git a/src/django_rich/management.py b/src/django_rich/management/__init__.py\nsimilarity index 100%\nrename from src/django_rich/management.py\nrename to src/django_rich/management/__init__.py\ndiff --git a/src/django_rich/management/commands/__init__.py b/src/django_rich/management/commands/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/src/django_rich/management/commands/shell.py b/src/django_rich/management/commands/shell.py\nnew file mode 100644\nindex 0000000..7595a18\n--- /dev/null\n+++ b/src/django_rich/management/commands/shell.py\n@@ -0,0 +1,12 @@\n+from __future__ import annotations\n+\n+from typing import Any\n+\n+from django.core.management.commands.shell import Command as BaseCommand\n+from rich import pretty\n+\n+\n+class Command(BaseCommand):\n+    def handle(self, **options: Any) -> None:\n+        pretty.install()\n+        return super().handle(**options)\n", "instance_id": "adamchainz__django-rich-78", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to extend Django's `manage.py shell` command to automatically enable Rich's pretty-printing feature in the Python REPL. It provides a reference to the relevant documentation for Rich's REPL integration, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, it does not explicitly mention whether this feature should work with all supported shell interpreters (e.g., IPython, bpython) or just the plain Python REPL, though the code changes and README updates clarify this to some extent. Additionally, there are no examples of expected behavior or output, and potential constraints or edge cases (e.g., compatibility issues with certain environments or interpreters) are not addressed. Overall, while the goal is understandable, the lack of comprehensive details and examples prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are minimal and localized. The primary modification is the addition of a new `shell.py` file that extends Django's built-in `shell` command to install Rich's pretty-printing feature. This involves overriding a single method (`handle`) with a one-line addition (`pretty.install()`). Additionally, there are documentation updates in `CHANGELOG.rst` and `README.rst`, and minor structural changes (e.g., renaming a file to create a package). The changes do not impact the broader system architecture or require understanding complex interactions across the codebase. The overall amount of code change is small.\n\n2. **Number of Technical Concepts:** The solution requires basic familiarity with Django's management command system, specifically how to extend built-in commands by subclassing. It also involves minimal interaction with the Rich library, using a straightforward API (`pretty.install()`). No advanced language features, algorithms, design patterns, or domain-specific knowledge are needed. The concepts involved are simple and accessible to developers with basic to intermediate experience in Python and Django.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention any specific edge cases or error conditions, and the code changes do not introduce new error handling logic. While there could be implicit edge cases (e.g., ensuring Rich's pretty-printing does not conflict with certain shell environments or user configurations), these are not addressed in the problem or solution. The simplicity of the change suggests that handling such cases, if needed, would not significantly increase complexity.\n\n4. **Overall Assessment:** This task involves a straightforward feature addition with minimal code changes and low conceptual overhead. It requires understanding a small part of the Django framework and applying a simple library feature. The lack of complex logic, architectural impact, or significant edge case handling keeps the difficulty low. A score of 0.25 reflects that this is an easy task, suitable for developers with basic experience in Python and Django, requiring only a small amount of effort to implement and test.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug][Compiler-v2] Types not inferred properly when using receiver function style calls\n# \ud83d\udc1b Bug\r\n\r\nInstances where types were inferred are failing when replaced with receiver style functions.\r\n\r\n## To reproduce\r\n\r\n**Code snippet to reproduce**\r\n\r\n#### Original code\r\nCan be found within `/move-examples/post_mint_reveal_nft/sources/big_vector.move`, [here](https://github.com/aptos-labs/aptos-core/blob/7228496c1a7b0e7b465a4c8d9bf20972bfc59fff/aptos-move/move-examples/post_mint_reveal_nft/sources/big_vector.move#L268).\r\n\r\n#### Updated to receiver function style\r\n```rust\r\n    #[test]\r\n    fun big_vector_append_edge_case_test() {\r\n        let v1 = empty(5);\r\n        let v2 = singleton(1u64, 7);\r\n        let v3 = empty(6);\r\n        let v4 = empty(8);\r\n        v3.append(v4);\r\n        assert!(v3.length() == 0, 0);\r\n        v2.append(v3);\r\n        assert!(v2.length() == 1, 0);\r\n        v1.append(v2);\r\n        assert!(v1.length() == 1, 0);\r\n        v1.destroy();\r\n    }\r\n```\r\n\r\n**Stack trace/error message**\r\n```bash\r\nbug: unexpected type: _\r\n    \u250c\u2500 /Users/alexzanderstone/Documents/aptos/aptos-core/aptos-move/move-examples/post_mint_reveal_nft/sources/big_vector.move:268:9\r\n    \u2502\r\n268 \u2502     fun big_vector_append_edge_case_test() {\r\n    \u2502         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\n## Expected Behavior\r\n\r\nType should be properly inferred, like original implementation.\r\n\r\n## System information\r\n\r\n**Please complete the following information:**\r\n- Aptos CLI Version\r\n  - aptos 3.5.0\r\n- Rust Version\r\n  - rustc 1.79.0\r\n- Computer OS\r\n  - MacOS\r\n\r\n\n", "patch": "diff --git a/third_party/move/move-model/src/builder/exp_builder.rs b/third_party/move/move-model/src/builder/exp_builder.rs\nindex 41a1dfea61295..8f5df47065031 100644\n--- a/third_party/move/move-model/src/builder/exp_builder.rs\n+++ b/third_party/move/move-model/src/builder/exp_builder.rs\n@@ -2044,7 +2044,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         receiver_arg_ty: &Type,\n         inst: ReceiverFunctionInstance,\n     ) -> RewriteResult {\n-        let receiver_param_type = inst.arg_types.first().expect(\"argument\").clone();\n+        let mut receiver_param_type = inst.arg_types.first().expect(\"argument\").clone();\n         // Determine whether an automatic borrow needs to be inserted\n         // and it's kind.\n         let borrow_kind_opt = inst.receiver_needs_borrow(receiver_arg_ty);\n@@ -2099,6 +2099,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n             // to avoid follow-up errors, only do if unification\n             // succeeded\n             if ok {\n+                receiver_param_type = subs.specialize(&receiver_param_type);\n                 self.subs = subs;\n                 let inst = inst\n                     .type_inst\n", "instance_id": "aptos-labs__aptos-core-14475", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: type inference fails when using receiver-style function calls in a specific test case within the provided codebase. It includes a code snippet to reproduce the issue, a reference to the original code, and the expected behavior (proper type inference). Additionally, it provides a stack trace/error message that points to the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly describe the root cause of the type inference failure or provide detailed constraints on what constitutes a valid fix. Edge cases or specific scenarios beyond the provided test case are not mentioned, which could leave room for interpretation. Overall, while the goal and context are clear, the lack of exhaustive details about the expected fix or broader implications prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code changes, while seemingly localized to a single file (`exp_builder.rs`), involves a critical part of the compiler infrastructure (type inference and expression translation in the Move language compiler). The changes impact how receiver-style function calls are handled, which is a core mechanism, potentially affecting the entire codebase's behavior. Understanding the fix requires deep knowledge of the Move language's type system, Rust's advanced type manipulation (e.g., specialization and substitution), and the specific implementation of the expression translator in the compiler. The code change itself is small but highly technical, involving the specialization of types during unification, which is a complex concept. Additionally, while the problem statement does not explicitly mention edge cases, the nature of type inference in a compiler suggests that numerous edge cases (e.g., different type combinations, nested calls, or borrow semantics) must be considered to ensure the fix does not introduce regressions. This requires a thorough understanding of the compiler's architecture and careful testing. The difficulty is not at the extreme end (e.g., 0.9-1.0) because the provided diff offers a starting point for the solution, but it still demands significant expertise and caution due to the potential impact on the system. Hence, a score of 0.75 reflects the challenging nature of the problem, balancing the localized change with the high technical complexity and risk.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "The create schedule command doesn't add double quotes around the --config {path} inside \"Add arguments (optional):\" field\nHi,\r\n\r\nI have noticed that after performing `resticprofile.exe -c \"C:\\foo bar\\baz boo\\config.toml\" --name my_task schedule`, the task is successfully created. However, after verification in taskschd.msc, the path located in the \"Actions\" tab, specifically inside \"Add arguments (optional):,\" results in` --no-ansi --config C:\\foo bar\\baz boo\\config.toml --name obsidian --no-lock my_task `. This leads to improper execution of the scheduled task, since spaces in the path aren\u2019t recognized. Adding the double quotes manually fixes the issue.\r\n\r\nIf I\u2019m overlooking something to avoid that behavior, please let me know.\r\n\r\nMany thanks for this great and useful wrapper. I really appreciate the work!\r\n\n", "patch": "diff --git a/config/mocks/NamedPropertySet.go b/config/mocks/NamedPropertySet.go\nindex c606d31f..48b38516 100644\n--- a/config/mocks/NamedPropertySet.go\n+++ b/config/mocks/NamedPropertySet.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/config/mocks/ProfileInfo.go b/config/mocks/ProfileInfo.go\nindex 8bafba01..55438596 100644\n--- a/config/mocks/ProfileInfo.go\n+++ b/config/mocks/ProfileInfo.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/config/mocks/PropertyInfo.go b/config/mocks/PropertyInfo.go\nindex 98e43991..b6a3b4c3 100644\n--- a/config/mocks/PropertyInfo.go\n+++ b/config/mocks/PropertyInfo.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/config/mocks/SectionInfo.go b/config/mocks/SectionInfo.go\nindex 695def83..929bfb56 100644\n--- a/config/mocks/SectionInfo.go\n+++ b/config/mocks/SectionInfo.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/monitor/mocks/OutputAnalysis.go b/monitor/mocks/OutputAnalysis.go\nindex a65cd599..1aa6f1cd 100644\n--- a/monitor/mocks/OutputAnalysis.go\n+++ b/monitor/mocks/OutputAnalysis.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/schedule/command_arguments.go b/schedule/command_arguments.go\nnew file mode 100644\nindex 00000000..9ad618f8\n--- /dev/null\n+++ b/schedule/command_arguments.go\n@@ -0,0 +1,50 @@\n+package schedule\n+\n+import \"strings\"\n+\n+type CommandArguments struct {\n+\targs []string\n+}\n+\n+func NewCommandArguments(args []string) CommandArguments {\n+\treturn CommandArguments{\n+\t\targs: args,\n+\t}\n+}\n+\n+func (ca CommandArguments) RawArgs() []string {\n+\tresult := make([]string, len(ca.args))\n+\tcopy(result, ca.args)\n+\treturn result\n+}\n+\n+// String returns the arguments as a string, with quotes around arguments that contain spaces\n+func (ca CommandArguments) String() string {\n+\tif len(ca.args) == 0 {\n+\t\treturn \"\"\n+\t}\n+\n+\tvar n int\n+\tfor _, elem := range ca.args {\n+\t\tn += len(elem) + 3 // add 2 if quotes are needed, plus 1 for the space\n+\t}\n+\n+\tb := new(strings.Builder)\n+\tb.Grow(n)\n+\tca.writeString(b, ca.args[0])\n+\tfor _, s := range ca.args[1:] {\n+\t\tb.WriteString(\" \")\n+\t\tca.writeString(b, s)\n+\t}\n+\treturn b.String()\n+}\n+\n+func (ca CommandArguments) writeString(b *strings.Builder, str string) {\n+\tif strings.Contains(str, \" \") {\n+\t\tb.WriteString(`\"`)\n+\t\tb.WriteString(str)\n+\t\tb.WriteString(`\"`)\n+\t} else {\n+\t\tb.WriteString(str)\n+\t}\n+}\ndiff --git a/schedule/config.go b/schedule/config.go\nindex d1526506..434d6fc6 100644\n--- a/schedule/config.go\n+++ b/schedule/config.go\n@@ -14,7 +14,7 @@ type Config struct {\n \tPermission              string\n \tWorkingDirectory        string\n \tCommand                 string // path to resticprofile executable\n-\tArguments               []string\n+\tArguments               CommandArguments\n \tEnvironment             []string\n \tJobDescription          string\n \tTimerDescription        string\n@@ -37,10 +37,12 @@ func NewRemoveOnlyConfig(profileName, commandName string) *Config {\n \t}\n }\n \n+// SetCommand sets the command details for scheduling. Arguments are automatically\n+// processed to ensure proper handling of paths with spaces and special characters.\n func (s *Config) SetCommand(wd, command string, args []string) {\n \ts.WorkingDirectory = wd\n \ts.Command = command\n-\ts.Arguments = args\n+\ts.Arguments = NewCommandArguments(args)\n }\n \n // Priority is either \"background\" or \"standard\"\ndiff --git a/schedule/handler_crond.go b/schedule/handler_crond.go\nindex cf1f77bc..2d69f103 100644\n--- a/schedule/handler_crond.go\n+++ b/schedule/handler_crond.go\n@@ -1,8 +1,6 @@\n package schedule\n \n import (\n-\t\"strings\"\n-\n \t\"github.com/creativeprojects/resticprofile/calendar\"\n \t\"github.com/creativeprojects/resticprofile/constants\"\n \t\"github.com/creativeprojects/resticprofile/crond\"\n@@ -68,7 +66,7 @@ func (h *HandlerCrond) CreateJob(job *Config, schedules []*calendar.Event, permi\n \t\t\tjob.ConfigFile,\n \t\t\tjob.ProfileName,\n \t\t\tjob.CommandName,\n-\t\t\tjob.Command+\" \"+strings.Join(job.Arguments, \" \"),\n+\t\t\tjob.Command+\" \"+job.Arguments.String(),\n \t\t\tjob.WorkingDirectory,\n \t\t)\n \t\tif h.config.Username != \"\" {\n@@ -92,7 +90,7 @@ func (h *HandlerCrond) RemoveJob(job *Config, permission string) error {\n \t\t\tjob.ConfigFile,\n \t\t\tjob.ProfileName,\n \t\t\tjob.CommandName,\n-\t\t\tjob.Command+\" \"+strings.Join(job.Arguments, \" \"),\n+\t\t\tjob.Command+\" \"+job.Arguments.String(),\n \t\t\tjob.WorkingDirectory,\n \t\t),\n \t}\ndiff --git a/schedule/handler_darwin.go b/schedule/handler_darwin.go\nindex 9aa49e1e..d5604d0e 100644\n--- a/schedule/handler_darwin.go\n+++ b/schedule/handler_darwin.go\n@@ -144,7 +144,6 @@ Do you want to start it now?`\n \n func (h *HandlerLaunchd) getLaunchdJob(job *Config, schedules []*calendar.Event) *LaunchdJob {\n \tname := getJobName(job.ProfileName, job.CommandName)\n-\targs := job.Arguments\n \t// we always set the log file in the job settings as a default\n \t// if changed in the configuration via schedule-log the standard output will be empty anyway\n \tlogfile := name + \".log\"\n@@ -165,7 +164,7 @@ func (h *HandlerLaunchd) getLaunchdJob(job *Config, schedules []*calendar.Event)\n \tlaunchdJob := &LaunchdJob{\n \t\tLabel:                 name,\n \t\tProgram:               job.Command,\n-\t\tProgramArguments:      append([]string{job.Command, \"--no-prio\"}, args...),\n+\t\tProgramArguments:      append([]string{job.Command, \"--no-prio\"}, job.Arguments.RawArgs()...),\n \t\tStandardOutPath:       logfile,\n \t\tStandardErrorPath:     logfile,\n \t\tWorkingDirectory:      job.WorkingDirectory,\ndiff --git a/schedule/handler_systemd.go b/schedule/handler_systemd.go\nindex 55023407..bc6b1eb9 100644\n--- a/schedule/handler_systemd.go\n+++ b/schedule/handler_systemd.go\n@@ -107,7 +107,7 @@ func (h *HandlerSystemd) CreateJob(job *Config, schedules []*calendar.Event, per\n \t}\n \n \terr := systemd.Generate(systemd.Config{\n-\t\tCommandLine:          job.Command + \" \" + strings.Join(append([]string{\"--no-prio\"}, job.Arguments...), \" \"),\n+\t\tCommandLine:          job.Command + \" \" + strings.Join(append([]string{\"--no-prio\"}, job.Arguments.RawArgs()...), \" \"),\n \t\tEnvironment:          job.Environment,\n \t\tWorkingDirectory:     job.WorkingDirectory,\n \t\tTitle:                job.ProfileName,\ndiff --git a/schedule/handler_windows.go b/schedule/handler_windows.go\nindex cc98c8ce..cc3af507 100644\n--- a/schedule/handler_windows.go\n+++ b/schedule/handler_windows.go\n@@ -58,7 +58,7 @@ func (h *HandlerWindows) CreateJob(job *Config, schedules []*calendar.Event, per\n \t\tProfileName:      job.ProfileName,\n \t\tCommandName:      job.CommandName,\n \t\tCommand:          job.Command,\n-\t\tArguments:        job.Arguments,\n+\t\tArguments:        job.Arguments.String(),\n \t\tWorkingDirectory: job.WorkingDirectory,\n \t\tJobDescription:   job.JobDescription,\n \t}\ndiff --git a/schedule/mocks/Handler.go b/schedule/mocks/Handler.go\nindex 75c4ba37..37af71ec 100644\n--- a/schedule/mocks/Handler.go\n+++ b/schedule/mocks/Handler.go\n@@ -1,4 +1,4 @@\n-// Code generated by mockery v2.46.2. DO NOT EDIT.\n+// Code generated by mockery v2.46.3. DO NOT EDIT.\n \n package mocks\n \ndiff --git a/schedule_jobs.go b/schedule_jobs.go\nindex 892e5104..2ca0440a 100644\n--- a/schedule_jobs.go\n+++ b/schedule_jobs.go\n@@ -33,7 +33,7 @@ func scheduleJobs(handler schedule.Handler, profileName string, configs []*confi\n \t\targs := []string{\n \t\t\t\"--no-ansi\",\n \t\t\t\"--config\",\n-\t\t\t`\"` + scheduleConfig.ConfigFile + `\"`,\n+\t\t\tscheduleConfig.ConfigFile,\n \t\t\t\"run-schedule\",\n \t\t\tscheduleName,\n \t\t}\n@@ -141,7 +141,7 @@ func scheduleToConfig(sched *config.Schedule) *schedule.Config {\n \t\tPermission:              sched.Permission,\n \t\tWorkingDirectory:        \"\",\n \t\tCommand:                 \"\",\n-\t\tArguments:               []string{},\n+\t\tArguments:               schedule.NewCommandArguments(nil),\n \t\tEnvironment:             sched.Environment,\n \t\tJobDescription:          \"\",\n \t\tTimerDescription:        \"\",\ndiff --git a/schtasks/config.go b/schtasks/config.go\nindex eebb417c..5356016e 100644\n--- a/schtasks/config.go\n+++ b/schtasks/config.go\n@@ -4,7 +4,7 @@ type Config struct {\n \tProfileName      string\n \tCommandName      string\n \tCommand          string\n-\tArguments        []string\n+\tArguments        string\n \tWorkingDirectory string\n \tJobDescription   string\n }\ndiff --git a/schtasks/taskscheduler.go b/schtasks/taskscheduler.go\nindex 71415133..5de4dfb0 100644\n--- a/schtasks/taskscheduler.go\n+++ b/schtasks/taskscheduler.go\n@@ -114,7 +114,7 @@ func createUserTask(config *Config, schedules []*calendar.Event) error {\n \ttask := taskService.NewTaskDefinition()\n \ttask.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Principal.LogonType = taskmaster.TASK_LOGON_PASSWORD\n@@ -154,7 +154,7 @@ func updateUserTask(task taskmaster.RegisteredTask, config *Config, schedules []\n \ttask.Definition.Actions = make([]taskmaster.Action, 0, 1)\n \ttask.Definition.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Definition.Principal.LogonType = taskmaster.TASK_LOGON_PASSWORD\n@@ -210,7 +210,7 @@ func createUserLoggedOnTask(config *Config, schedules []*calendar.Event) error {\n \ttask := taskService.NewTaskDefinition()\n \ttask.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Principal.LogonType = taskmaster.TASK_LOGON_INTERACTIVE_TOKEN\n@@ -244,7 +244,7 @@ func updateUserLoggedOnTask(task taskmaster.RegisteredTask, config *Config, sche\n \ttask.Definition.Actions = make([]taskmaster.Action, 0, 1)\n \ttask.Definition.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Definition.Principal.LogonType = taskmaster.TASK_LOGON_INTERACTIVE_TOKEN\n@@ -278,7 +278,7 @@ func createSystemTask(config *Config, schedules []*calendar.Event) error {\n \ttask := taskService.NewTaskDefinition()\n \ttask.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Principal.LogonType = taskmaster.TASK_LOGON_SERVICE_ACCOUNT\n@@ -307,7 +307,7 @@ func updateSystemTask(task taskmaster.RegisteredTask, config *Config, schedules\n \ttask.Definition.Actions = make([]taskmaster.Action, 0, 1)\n \ttask.Definition.AddAction(taskmaster.ExecAction{\n \t\tPath:       config.Command,\n-\t\tArgs:       strings.Join(config.Arguments, \" \"),\n+\t\tArgs:       config.Arguments,\n \t\tWorkingDir: config.WorkingDirectory,\n \t})\n \ttask.Definition.Principal.LogonType = taskmaster.TASK_LOGON_SERVICE_ACCOUNT\n", "instance_id": "creativeprojects__resticprofile-423", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `resticprofile` command for scheduling tasks on Windows does not add double quotes around paths with spaces in the `--config` argument, leading to improper execution in the Windows Task Scheduler. The goal (fixing the argument formatting) and the context (scheduled task creation) are evident. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly mention whether this issue affects other platforms (e.g., Linux or macOS) or if there are specific constraints on how quotes should be applied (e.g., only for paths with spaces or all arguments). Additionally, edge cases such as paths with special characters or nested quotes are not addressed. Despite these minor gaps, the problem is valid and understandable with the provided context and user feedback.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes span multiple files (e.g., `schedule/command_arguments.go`, `schedule/config.go`, `schtasks/taskscheduler.go`, etc.), but the modifications are relatively straightforward. The core change involves creating a new `CommandArguments` type to handle string formatting with quotes for arguments containing spaces and updating various handlers (Windows, systemd, crond, etc.) to use this new type. The impact is localized to the scheduling logic and does not affect the broader system architecture. The amount of code change is moderate, with a new file for argument handling and small updates across related modules.\n\n2. **Number of Technical Concepts:** Solving this requires basic knowledge of Go (string manipulation, struct creation, and method implementation) and an understanding of how command-line arguments are passed to scheduled tasks on different platforms (especially Windows Task Scheduler). No advanced algorithms, design patterns, or domain-specific knowledge are needed. The concept of quoting arguments with spaces is a common issue in command-line tools and is not inherently complex.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond spaces in file paths, but the code changes handle this specific case by adding quotes around arguments with spaces. There is no indication of additional error handling for more complex scenarios (e.g., arguments already containing quotes or special characters). The complexity of edge cases appears minimal at this stage, though a more thorough solution might consider such scenarios.\n\n4. **Overall Complexity:** The task requires understanding the scheduling logic across multiple platforms (Windows, Linux, macOS) but does not demand deep architectural changes or advanced technical expertise. It involves simple modifications to ensure proper string formatting and integration with existing scheduling handlers.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires moderate code changes and basic logic adjustments across several files, with minimal complexity in concepts or edge cases.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Changes in cargo metadata PackageId format leading to test failures\nIt appears that somewhat recently `cargo metadata` changed the format of the `package_id` field in its JSON output (presumably https://github.com/rust-lang/cargo/pull/12914). Significantly, this changed the way that packages are sorted in internal data structures, such that while previously `crate 1.1.1` and `crate 1.1.1@git:XXX` would have sorted adjacent to one-another, they now sort further apart.\r\n\r\nhttps://github.com/mozilla/cargo-vet/blob/abb7411fbb669577e6d6c8d289c3a903f0ad2109/src/resolver.rs#L452-L454\r\n\r\nThe new format for this field is a `PackageIdSpec`, which is supposedly going to be a more stable format going forwards. This format change also will have broken one case where we did inspect the internal format of the `PackageId` (despite it being documented as opaque). \r\n\r\nhttps://github.com/mozilla/cargo-vet/blob/abb7411fbb669577e6d6c8d289c3a903f0ad2109/src/resolver.rs#L214-L217\r\n\r\nThe file URI in this case now looks more like `path+file:///path/to/example#0.1.0` rather than `example 0.1.0 (path+file:///path/to/example)`, so will no longer match the `contains` check.\r\n\r\nIn order to keep tests passing with both older and newer versions of rustc, we'll likely need to tweak how we sort packages to avoid using `package_id` for sorting when possible. In addition, there are some commands where the output contains the package id, specifically the dump-graph test, which will likely need to be updated in some way - likely by removing the unstable PackageId check, and instead never serializing package IDs.\n", "patch": "diff --git a/src/resolver.rs b/src/resolver.rs\nindex b9e55e34..6fa2bf03 100644\n--- a/src/resolver.rs\n+++ b/src/resolver.rs\n@@ -180,9 +180,12 @@ pub type PackageIdx = usize;\n \n #[derive(Debug, Clone, Serialize)]\n pub struct PackageNode<'a> {\n-    #[serde(skip_serializing_if = \"pkgid_unstable\")]\n+    #[serde(skip)]\n     /// The PackageId that cargo uses to uniquely identify this package\n     ///\n+    /// This ID is not guaranteed to be stable across cargo versions, so is not\n+    /// serialized into graph JSON.\n+    ///\n     /// Prefer using a [`DepGraph`] and its memoized [`PackageIdx`]'s.\n     pub package_id: &'a PackageId,\n     /// The name of the package\n@@ -211,11 +214,6 @@ pub struct PackageNode<'a> {\n     pub is_dev_only: bool,\n }\n \n-/// Don't serialize path package ids, not stable across systems\n-fn pkgid_unstable(pkgid: &PackageId) -> bool {\n-    pkgid.repr.contains(\"(path+file:/\")\n-}\n-\n /// The dependency graph in a form we can use more easily.\n #[derive(Debug, Clone)]\n pub struct DepGraph<'a> {\n@@ -449,9 +447,13 @@ impl<'a> DepGraph<'a> {\n             });\n         }\n \n-        // Sort the nodes by package_id to make the graph more stable and to make\n-        // anything sorted by package_idx to also be approximately sorted by name and version.\n-        nodes.sort_by_key(|k| k.package_id);\n+        // Sort the nodes by package name and version to make the graph as\n+        // stable as possible.  We avoid sorting by the package_id if possible,\n+        // as for some packages it may not be stable (e.g. file:///), and the\n+        // package_id format can also vary between cargo versions.\n+        nodes.sort_by(|a, b| {\n+            (a.name, &a.version, &a.package_id).cmp(&(b.name, &b.version, &b.package_id))\n+        });\n \n         // Populate the interners based on the new ordering\n         for (idx, node) in nodes.iter_mut().enumerate() {\n", "instance_id": "mozilla__cargo-vet-603", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a change in the `cargo metadata` output format for `PackageId` has caused test failures due to sorting and string matching issues in the codebase. It provides specific references to the affected code lines and explains the impact of the format change. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior for backward compatibility with older `cargo` versions beyond a general suggestion to avoid using `package_id` for sorting. Additionally, while it mentions the need to update the `dump-graph` test, it lacks specific guidance or examples on how to handle or replace the unstable `PackageId` check in the output. Edge cases, such as varying `cargo` versions or different package ID formats, are implied but not exhaustively detailed. Overall, the statement is valid and clear but could benefit from more precise requirements or examples for handling compatibility and test updates.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively limited, primarily affecting a single file (`resolver.rs`) with modifications to sorting logic and serialization behavior. The changes involve removing a function (`pkgid_unstable`) and updating the sorting mechanism to prioritize package name and version over `package_id`, which is a straightforward adjustment. However, it requires understanding specific Rust and Cargo concepts, such as the `PackageId` structure, its instability across versions, and how it impacts serialization and sorting in a dependency graph. The problem also demands awareness of backward compatibility with different `cargo` versions, which adds a layer of complexity in ensuring tests pass across environments. Edge cases, such as handling various `PackageId` formats (e.g., file URIs) and ensuring stable graph output, are present but not overly intricate. There is no significant architectural impact or need for extensive refactoring beyond the localized changes. Overall, this problem requires moderate technical knowledge of Rust/Cargo internals and careful handling of compatibility, but it does not involve deep system-level changes or highly complex logic, placing it at a difficulty of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support to `hexagonal_lattice_graph` for periodcity and position coordinates\n### What is the expected enhancement?\r\nnetworkx's [`hexagonal_lattice_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.lattice.hexagonal_lattice_graph.html) function provides support for periodicity and setting position coordinates, and it would be great if rustworkx's `hexagonal_lattice_graph` did the same.\r\n\r\nDefinition of done:\r\n- a `periodic` bool argument added to `hexagonal_lattice_graph`\r\n- a `with_positions` bool argument added to `hexagonal_lattice_graph`\r\n\r\ncc @mrossinek\r\n\r\n\n", "patch": "diff --git a/rustworkx-core/src/generators/hexagonal_lattice_graph.rs b/rustworkx-core/src/generators/hexagonal_lattice_graph.rs\nindex 7adbba201..0f6571e38 100644\n--- a/rustworkx-core/src/generators/hexagonal_lattice_graph.rs\n+++ b/rustworkx-core/src/generators/hexagonal_lattice_graph.rs\n@@ -17,6 +17,184 @@ use petgraph::visit::{Data, NodeIndexable};\n \n use super::InvalidInputError;\n \n+mod utils {\n+    use super::*;\n+\n+    pub struct HexagonalLatticeBuilder {\n+        rowlen: usize,    // Number of nodes in each vertical chain\n+        collen: usize,    // Number of vertical chains\n+        num_nodes: usize, // Total number of nodes\n+        bidirectional: bool,\n+        periodic: bool,\n+    }\n+\n+    impl HexagonalLatticeBuilder {\n+        pub fn new(\n+            rows: usize,\n+            cols: usize,\n+            bidirectional: bool,\n+            periodic: bool,\n+        ) -> Result<HexagonalLatticeBuilder, InvalidInputError> {\n+            if periodic && (cols % 2 == 1 || rows < 2 || cols < 2) {\n+                return Err(InvalidInputError {});\n+            }\n+\n+            let (rowlen, collen, num_nodes) = if periodic {\n+                let r_len = 2 * rows;\n+                (r_len, cols, r_len * cols)\n+            } else {\n+                let r_len = 2 * rows + 2;\n+                (r_len, cols + 1, r_len * (cols + 1) - 2)\n+            };\n+\n+            Ok(HexagonalLatticeBuilder {\n+                rowlen,\n+                collen,\n+                num_nodes,\n+                bidirectional,\n+                periodic,\n+            })\n+        }\n+\n+        pub fn build_with_default_node_weight<G, T, F, H, M>(\n+            self,\n+            mut default_node_weight: F,\n+            default_edge_weight: H,\n+        ) -> G\n+        where\n+            G: Build + Create + Data<NodeWeight = T, EdgeWeight = M> + NodeIndexable,\n+            F: FnMut() -> T,\n+            H: FnMut() -> M,\n+            G::NodeId: Eq + Hash,\n+        {\n+            // ToDo: be more precise about the number of edges\n+            let mut graph = G::with_capacity(self.num_nodes, self.num_nodes);\n+            let nodes: Vec<G::NodeId> = (0..self.num_nodes)\n+                .map(|_| graph.add_node(default_node_weight()))\n+                .collect();\n+            self.add_edges(&mut graph, nodes, default_edge_weight);\n+\n+            graph\n+        }\n+\n+        pub fn build_with_position_dependent_node_weight<G, T, F, H, M>(\n+            self,\n+            mut node_weight: F,\n+            default_edge_weight: H,\n+        ) -> G\n+        where\n+            G: Build + Create + Data<NodeWeight = T, EdgeWeight = M> + NodeIndexable,\n+            F: FnMut(usize, usize) -> T,\n+            H: FnMut() -> M,\n+            G::NodeId: Eq + Hash,\n+        {\n+            // ToDo: be more precise about the number of edges\n+            let mut graph = G::with_capacity(self.num_nodes, self.num_nodes);\n+\n+            let lattice_position = |n| -> (usize, usize) {\n+                if self.periodic {\n+                    (n / self.rowlen, n % self.rowlen)\n+                } else {\n+                    // In the non-periodic case the first and last vertical\n+                    // chains have rowlen - 1 = 2 * rows + 1 nodes. All others\n+                    // have rowlen = 2 * rows + 2 nodes.\n+                    if n < self.rowlen - 1 {\n+                        (0, n)\n+                    } else {\n+                        let k = n - (self.rowlen - 1);\n+                        let u = k / self.rowlen + 1;\n+                        let v = k % self.rowlen;\n+                        if u == self.collen - 1 {\n+                            (u, v + 1)\n+                        } else {\n+                            (u, v)\n+                        }\n+                    }\n+                }\n+            };\n+\n+            let nodes: Vec<G::NodeId> = (0..self.num_nodes)\n+                .map(lattice_position)\n+                .map(|(u, v)| graph.add_node(node_weight(u, v)))\n+                .collect();\n+            self.add_edges(&mut graph, nodes, default_edge_weight);\n+\n+            graph\n+        }\n+\n+        fn add_edges<G, H, M>(\n+            &self,\n+            graph: &mut G,\n+            nodes: Vec<G::NodeId>,\n+            mut default_edge_weight: H,\n+        ) where\n+            G: Build + NodeIndexable + Data<EdgeWeight = M>,\n+            H: FnMut() -> M,\n+        {\n+            let mut add_edge = |u, v| {\n+                graph.add_edge(nodes[u], nodes[v], default_edge_weight());\n+                if self.bidirectional {\n+                    graph.add_edge(nodes[v], nodes[u], default_edge_weight());\n+                }\n+            };\n+\n+            if self.periodic {\n+                // Add column edges\n+                for i in 0..self.collen {\n+                    let col_start = i * self.rowlen;\n+                    for j in col_start..(col_start + self.rowlen - 1) {\n+                        add_edge(j, j + 1);\n+                    }\n+                    add_edge(col_start + self.rowlen - 1, col_start);\n+                }\n+                // Add row edges\n+                for i in 0..self.collen {\n+                    let col_start = i * self.rowlen + i % 2;\n+                    for j in (col_start..(col_start + self.rowlen)).step_by(2) {\n+                        add_edge(j, (j + self.rowlen) % self.num_nodes);\n+                    }\n+                }\n+            } else {\n+                // Add column edges\n+                for j in 0..(self.rowlen - 2) {\n+                    add_edge(j, j + 1);\n+                }\n+                for i in 1..(self.collen - 1) {\n+                    for j in 0..(self.rowlen - 1) {\n+                        add_edge(i * self.rowlen + j - 1, i * self.rowlen + j);\n+                    }\n+                }\n+                for j in 0..(self.rowlen - 2) {\n+                    add_edge(\n+                        (self.collen - 1) * self.rowlen + j - 1,\n+                        (self.collen - 1) * self.rowlen + j,\n+                    );\n+                }\n+\n+                // Add row edges\n+                for j in (0..(self.rowlen - 1)).step_by(2) {\n+                    add_edge(j, j + self.rowlen - 1);\n+                }\n+                for i in 1..(self.collen - 2) {\n+                    for j in 0..self.rowlen {\n+                        if i % 2 == j % 2 {\n+                            add_edge(i * self.rowlen + j - 1, (i + 1) * self.rowlen + j - 1);\n+                        }\n+                    }\n+                }\n+                if self.collen > 2 {\n+                    for j in ((self.collen % 2)..self.rowlen).step_by(2) {\n+                        add_edge(\n+                            (self.collen - 2) * self.rowlen + j - 1,\n+                            (self.collen - 1) * self.rowlen + j - 1 - (self.collen % 2),\n+                        );\n+                    }\n+                }\n+            }\n+        }\n+    }\n+}\n+\n /// Generate a hexagonal lattice graph\n ///\n /// Arguments:\n@@ -24,12 +202,15 @@ use super::InvalidInputError;\n /// * `rows` - The number of rows to generate the graph with.\n /// * `cols` - The number of columns to generate the graph with.\n /// * `default_node_weight` - A callable that will return the weight to use\n-///     for newly created nodes. This is ignored if `weights` is specified.\n+///     for newly created nodes.\n /// * `default_edge_weight` - A callable that will return the weight object\n ///     to use for newly created edges.\n /// * `bidirectional` - Whether edges are added bidirectionally. If set to\n ///     `true` then for any edge `(u, v)` an edge `(v, u)` will also be added.\n ///     If the graph is undirected this will result in a parallel edge.\n+/// * `periodic` - If set to `true`, the boundaries of the lattice will be\n+///     joined to form a periodic grid. Requires `cols` to be even,\n+///     `rows > 1`, and `cols > 1`.\n ///\n /// # Example\n /// ```rust\n@@ -42,6 +223,7 @@ use super::InvalidInputError;\n ///     2,\n ///     || {()},\n ///     || {()},\n+///     false,\n ///     false\n /// ).unwrap();\n /// let expected_edges = vec![\n@@ -75,9 +257,10 @@ use super::InvalidInputError;\n pub fn hexagonal_lattice_graph<G, T, F, H, M>(\n     rows: usize,\n     cols: usize,\n-    mut default_node_weight: F,\n-    mut default_edge_weight: H,\n+    default_node_weight: F,\n+    default_edge_weight: H,\n     bidirectional: bool,\n+    periodic: bool,\n ) -> Result<G, InvalidInputError>\n where\n     G: Build + Create + Data<NodeWeight = T, EdgeWeight = M> + NodeIndexable,\n@@ -88,107 +271,149 @@ where\n     if rows == 0 || cols == 0 {\n         return Ok(G::with_capacity(0, 0));\n     }\n-    let mut rowlen = rows;\n-    let mut collen = cols;\n \n-    // Needs two times the number of nodes vertically\n-    rowlen = 2 * rowlen + 2;\n-    collen += 1;\n-    let num_nodes = rowlen * collen - 2;\n+    let builder = utils::HexagonalLatticeBuilder::new(rows, cols, bidirectional, periodic)?;\n \n-    let mut graph = G::with_capacity(num_nodes, num_nodes);\n+    let graph = builder\n+        .build_with_default_node_weight::<G, T, F, H, M>(default_node_weight, default_edge_weight);\n \n-    let nodes: Vec<G::NodeId> = (0..num_nodes)\n-        .map(|_| graph.add_node(default_node_weight()))\n-        .collect();\n+    Ok(graph)\n+}\n \n-    // Add column edges\n-    for j in 0..(rowlen - 2) {\n-        graph.add_edge(nodes[j], nodes[j + 1], default_edge_weight());\n-        if bidirectional {\n-            graph.add_edge(nodes[j + 1], nodes[j], default_edge_weight());\n-        }\n-    }\n-    for i in 1..(collen - 1) {\n-        for j in 0..(rowlen - 1) {\n-            graph.add_edge(\n-                nodes[i * rowlen + j - 1],\n-                nodes[i * rowlen + j],\n-                default_edge_weight(),\n-            );\n-            if bidirectional {\n-                graph.add_edge(\n-                    nodes[i * rowlen + j],\n-                    nodes[i * rowlen + j - 1],\n-                    default_edge_weight(),\n-                );\n-            }\n-        }\n-    }\n-    for j in 0..(rowlen - 2) {\n-        graph.add_edge(\n-            nodes[(collen - 1) * rowlen + j - 1],\n-            nodes[(collen - 1) * rowlen + j],\n-            default_edge_weight(),\n-        );\n-        if bidirectional {\n-            graph.add_edge(\n-                nodes[(collen - 1) * rowlen + j],\n-                nodes[(collen - 1) * rowlen + j - 1],\n-                default_edge_weight(),\n-            );\n-        }\n+/// Generate a hexagonal lattice graph where each node is assigned a weight\n+/// depending on its position in the lattice.\n+///\n+/// Arguments:\n+///\n+/// * `rows` - The number of rows to generate the graph with.\n+/// * `cols` - The number of columns to generate the graph with.\n+/// * `node_weight` - A callable that will return the weight to use\n+///     for newly created nodes. Must take two arguments `i` and `j` of\n+///     type `usize`, where `(i, j)` gives the position of the node\n+///     in the lattice.\n+/// * `default_edge_weight` - A callable that will return the weight object\n+///     to use for newly created edges.\n+/// * `bidirectional` - Whether edges are added bidirectionally. If set to\n+///     `true` then for any edge `(u, v)` an edge `(v, u)` will also be added.\n+///     If the graph is undirected this will result in a parallel edge.\n+/// * `periodic` - If set to `true`, the boundaries of the lattice will be\n+///     joined to form a periodic grid. Requires `cols` to be even,\n+///     `rows > 1`, and `cols > 1`.\n+///\n+/// # Example\n+/// ```rust\n+/// use rustworkx_core::petgraph;\n+/// use rustworkx_core::generators::hexagonal_lattice_graph_weighted;\n+/// use rustworkx_core::petgraph::visit::{IntoNodeReferences, NodeRef};\n+///\n+/// let g: petgraph::graph::UnGraph<(usize, usize), ()> = hexagonal_lattice_graph_weighted(\n+///     2,\n+///     2,\n+///     |u, v| {(u, v)},\n+///     || {()},\n+///     false,\n+///     false\n+/// ).unwrap();\n+/// let expected_node_weights = vec![\n+///     (0, 0),\n+///     (0, 1),\n+///     (0, 2),\n+///     (0, 3),\n+///     (0, 4),\n+///     (1, 0),\n+///     (1, 1),\n+///     (1, 2),\n+///     (1, 3),\n+///     (1, 4),\n+///     (1, 5),\n+///     (2, 1),\n+///     (2, 2),\n+///     (2, 3),\n+///     (2, 4),\n+///     (2, 5),\n+/// ];\n+/// assert_eq!(\n+///     expected_node_weights,\n+///     g.node_references()\n+///         .map(|node| *node.weight())\n+///         .collect::<Vec<(usize, usize)>>(),\n+/// )\n+/// ```\n+pub fn hexagonal_lattice_graph_weighted<G, T, F, H, M>(\n+    rows: usize,\n+    cols: usize,\n+    node_weight: F,\n+    default_edge_weight: H,\n+    bidirectional: bool,\n+    periodic: bool,\n+) -> Result<G, InvalidInputError>\n+where\n+    G: Build + Create + Data<NodeWeight = T, EdgeWeight = M> + NodeIndexable,\n+    F: FnMut(usize, usize) -> T,\n+    H: FnMut() -> M,\n+    G::NodeId: Eq + Hash,\n+{\n+    if rows == 0 || cols == 0 {\n+        return Ok(G::with_capacity(0, 0));\n     }\n \n-    // Add row edges\n-    for j in (0..(rowlen - 1)).step_by(2) {\n-        graph.add_edge(nodes[j], nodes[j + rowlen - 1], default_edge_weight());\n-        if bidirectional {\n-            graph.add_edge(nodes[j + rowlen - 1], nodes[j], default_edge_weight());\n-        }\n-    }\n-    for i in 1..(collen - 2) {\n-        for j in 0..rowlen {\n-            if i % 2 == j % 2 {\n-                graph.add_edge(\n-                    nodes[i * rowlen + j - 1],\n-                    nodes[(i + 1) * rowlen + j - 1],\n-                    default_edge_weight(),\n-                );\n-                if bidirectional {\n-                    graph.add_edge(\n-                        nodes[(i + 1) * rowlen + j - 1],\n-                        nodes[i * rowlen + j - 1],\n-                        default_edge_weight(),\n-                    );\n-                }\n-            }\n-        }\n-    }\n-    if collen > 2 {\n-        for j in ((collen % 2)..rowlen).step_by(2) {\n-            graph.add_edge(\n-                nodes[(collen - 2) * rowlen + j - 1],\n-                nodes[(collen - 1) * rowlen + j - 1 - (collen % 2)],\n-                default_edge_weight(),\n-            );\n-            if bidirectional {\n-                graph.add_edge(\n-                    nodes[(collen - 1) * rowlen + j - 1 - (collen % 2)],\n-                    nodes[(collen - 2) * rowlen + j - 1],\n-                    default_edge_weight(),\n-                );\n-            }\n-        }\n-    }\n+    let builder = utils::HexagonalLatticeBuilder::new(rows, cols, bidirectional, periodic)?;\n+\n+    let graph = builder.build_with_position_dependent_node_weight::<G, T, F, H, M>(\n+        node_weight,\n+        default_edge_weight,\n+    );\n+\n     Ok(graph)\n }\n \n #[cfg(test)]\n mod tests {\n-    use crate::generators::hexagonal_lattice_graph;\n+    use crate::generators::{hexagonal_lattice_graph, hexagonal_lattice_graph_weighted};\n     use crate::petgraph;\n-    use crate::petgraph::visit::EdgeRef;\n+    use crate::petgraph::visit::{EdgeRef, IntoNodeReferences};\n+    use std::collections::HashSet;\n+\n+    fn check_expected_edges_directed<T>(\n+        graph: &petgraph::graph::DiGraph<T, ()>,\n+        expected_edges: &Vec<(usize, usize)>,\n+    ) {\n+        assert_eq!(graph.edge_count(), expected_edges.len());\n+\n+        let edge_set: HashSet<(usize, usize)> = graph\n+            .edge_references()\n+            .map(|edge| (edge.source().index(), edge.target().index()))\n+            .collect();\n+        let expected_set: HashSet<(usize, usize)> = expected_edges.iter().map(|&e| e).collect();\n+        assert_eq!(edge_set, expected_set);\n+    }\n+\n+    fn check_expected_edges_undirected(\n+        graph: &petgraph::graph::UnGraph<(), ()>,\n+        expected_edges: &Vec<(usize, usize)>,\n+    ) {\n+        assert_eq!(graph.edge_count(), expected_edges.len());\n+\n+        let sorted_pair = |(a, b)| {\n+            if a > b {\n+                (b, a)\n+            } else {\n+                (a, b)\n+            }\n+        };\n+\n+        let edge_set: HashSet<(usize, usize)> = graph\n+            .edge_references()\n+            .map(|edge| (edge.source().index(), edge.target().index()))\n+            .map(|e| sorted_pair(e))\n+            .collect();\n+        let expected_set: HashSet<(usize, usize)> = expected_edges\n+            .iter()\n+            .map(|&e| e)\n+            .map(|e| sorted_pair(e))\n+            .collect();\n+        assert_eq!(edge_set, expected_set);\n+    }\n \n     #[test]\n     fn test_hexagonal_lattice_graph() {\n@@ -214,15 +439,9 @@ mod tests {\n             (10, 15),\n         ];\n         let g: petgraph::graph::UnGraph<(), ()> =\n-            hexagonal_lattice_graph(2, 2, || (), || (), false).unwrap();\n+            hexagonal_lattice_graph(2, 2, || (), || (), false, false).unwrap();\n         assert_eq!(g.node_count(), 16);\n-        assert_eq!(g.edge_count(), expected_edges.len());\n-        assert_eq!(\n-            expected_edges,\n-            g.edge_references()\n-                .map(|edge| (edge.source().index(), edge.target().index()))\n-                .collect::<Vec<(usize, usize)>>(),\n-        );\n+        check_expected_edges_undirected(&g, &expected_edges);\n     }\n \n     #[test]\n@@ -249,15 +468,14 @@ mod tests {\n             (10, 15),\n         ];\n         let g: petgraph::graph::DiGraph<(), ()> =\n-            hexagonal_lattice_graph(2, 2, || (), || (), false).unwrap();\n+            hexagonal_lattice_graph(2, 2, || (), || (), false, false).unwrap();\n         assert_eq!(g.node_count(), 16);\n-        assert_eq!(g.edge_count(), expected_edges.len());\n-        assert_eq!(\n-            expected_edges,\n-            g.edge_references()\n-                .map(|edge| (edge.source().index(), edge.target().index()))\n-                .collect::<Vec<(usize, usize)>>(),\n-        );\n+        check_expected_edges_directed(&g, &expected_edges);\n+\n+        let g_weighted: petgraph::graph::DiGraph<(usize, usize), ()> =\n+            hexagonal_lattice_graph_weighted(2, 2, |u, v| (u, v), || (), false, false).unwrap();\n+        assert_eq!(g_weighted.node_count(), 16);\n+        check_expected_edges_directed(&g_weighted, &expected_edges);\n     }\n \n     #[test]\n@@ -303,22 +521,141 @@ mod tests {\n             (15, 10),\n         ];\n         let g: petgraph::graph::DiGraph<(), ()> =\n-            hexagonal_lattice_graph(2, 2, || (), || (), true).unwrap();\n+            hexagonal_lattice_graph(2, 2, || (), || (), true, false).unwrap();\n         assert_eq!(g.node_count(), 16);\n-        assert_eq!(g.edge_count(), expected_edges.len());\n-        assert_eq!(\n-            expected_edges,\n-            g.edge_references()\n-                .map(|edge| (edge.source().index(), edge.target().index()))\n-                .collect::<Vec<(usize, usize)>>(),\n-        );\n+        check_expected_edges_directed(&g, &expected_edges);\n     }\n \n     #[test]\n     fn test_hexagonal_lattice_error() {\n         let g: petgraph::graph::UnGraph<(), ()> =\n-            hexagonal_lattice_graph(0, 0, || (), || (), false).unwrap();\n+            hexagonal_lattice_graph(0, 0, || (), || (), false, false).unwrap();\n         assert_eq!(g.node_count(), 0);\n         assert_eq!(g.edge_count(), 0);\n     }\n+\n+    #[test]\n+    fn test_hexagonal_lattice_periodic_error() {\n+        match hexagonal_lattice_graph::<petgraph::graph::UnGraph<(), ()>, (), _, _, ()>(\n+            5,\n+            3,\n+            || (),\n+            || (),\n+            false,\n+            true,\n+        ) {\n+            Ok(_) => panic!(\"Returned a non-error\"),\n+            Err(e) => assert_eq!(e, crate::generators::InvalidInputError),\n+        }\n+    }\n+\n+    #[test]\n+    fn test_hexagonal_lattice_graph_periodic() {\n+        let expected_edges = vec![\n+            (0, 1),\n+            (1, 2),\n+            (2, 3),\n+            (3, 0),\n+            (4, 5),\n+            (5, 6),\n+            (6, 7),\n+            (7, 4),\n+            (0, 4),\n+            (2, 6),\n+            (5, 1),\n+            (7, 3),\n+        ];\n+        let g: petgraph::graph::UnGraph<(), ()> =\n+            hexagonal_lattice_graph(2, 2, || (), || (), false, true).unwrap();\n+        assert_eq!(g.node_count(), 8);\n+        check_expected_edges_undirected(&g, &expected_edges);\n+    }\n+\n+    #[test]\n+    fn test_directed_hexagonal_lattice_graph_periodic() {\n+        let expected_edges = vec![\n+            (0, 1),\n+            (1, 2),\n+            (2, 3),\n+            (3, 0),\n+            (4, 5),\n+            (5, 6),\n+            (6, 7),\n+            (7, 4),\n+            (0, 4),\n+            (2, 6),\n+            (5, 1),\n+            (7, 3),\n+        ];\n+        let g: petgraph::graph::DiGraph<(), ()> =\n+            hexagonal_lattice_graph(2, 2, || (), || (), false, true).unwrap();\n+        assert_eq!(g.node_count(), 8);\n+        check_expected_edges_directed(&g, &expected_edges);\n+\n+        let g_weighted: petgraph::graph::DiGraph<(usize, usize), ()> =\n+            hexagonal_lattice_graph_weighted(2, 2, |u, v| (u, v), || (), false, true).unwrap();\n+        assert_eq!(g_weighted.node_count(), 8);\n+        check_expected_edges_directed(&g_weighted, &expected_edges);\n+\n+        let g: petgraph::graph::UnGraph<(usize, usize), ()> =\n+            hexagonal_lattice_graph_weighted(2, 2, |u, v| (u, v), || (), false, false).unwrap();\n+        let expected_node_weights = vec![\n+            (0, 0),\n+            (0, 1),\n+            (0, 2),\n+            (0, 3),\n+            (0, 4),\n+            (1, 0),\n+            (1, 1),\n+            (1, 2),\n+            (1, 3),\n+            (1, 4),\n+            (1, 5),\n+            (2, 1),\n+            (2, 2),\n+            (2, 3),\n+            (2, 4),\n+            (2, 5),\n+        ];\n+        assert_eq!(\n+            expected_node_weights,\n+            g.node_references()\n+                .map(|node| *node.1)\n+                .collect::<Vec<(usize, usize)>>(),\n+        )\n+    }\n+\n+    #[test]\n+    fn test_directed_hexagonal_lattice_graph_bidirectional_periodic() {\n+        let expected_edges = vec![\n+            (0, 1),\n+            (1, 0),\n+            (1, 2),\n+            (2, 1),\n+            (2, 3),\n+            (3, 2),\n+            (3, 0),\n+            (0, 3),\n+            (4, 5),\n+            (5, 4),\n+            (5, 6),\n+            (6, 5),\n+            (6, 7),\n+            (7, 6),\n+            (7, 4),\n+            (4, 7),\n+            (0, 4),\n+            (4, 0),\n+            (2, 6),\n+            (6, 2),\n+            (5, 1),\n+            (1, 5),\n+            (7, 3),\n+            (3, 7),\n+        ];\n+        let g: petgraph::graph::DiGraph<(), ()> =\n+            hexagonal_lattice_graph(2, 2, || (), || (), true, true).unwrap();\n+        assert_eq!(g.node_count(), 8);\n+        check_expected_edges_directed(&g, &expected_edges);\n+    }\n }\ndiff --git a/rustworkx-core/src/generators/mod.rs b/rustworkx-core/src/generators/mod.rs\nindex bde826cbf..6c0af1ace 100644\n--- a/rustworkx-core/src/generators/mod.rs\n+++ b/rustworkx-core/src/generators/mod.rs\n@@ -54,7 +54,7 @@ pub use full_rary_tree_graph::full_rary_tree_graph;\n pub use grid_graph::grid_graph;\n pub use heavy_hex_graph::heavy_hex_graph;\n pub use heavy_square_graph::heavy_square_graph;\n-pub use hexagonal_lattice_graph::hexagonal_lattice_graph;\n+pub use hexagonal_lattice_graph::{hexagonal_lattice_graph, hexagonal_lattice_graph_weighted};\n pub use lollipop_graph::lollipop_graph;\n pub use path_graph::path_graph;\n pub use petersen_graph::petersen_graph;\ndiff --git a/rustworkx/generators/__init__.pyi b/rustworkx/generators/__init__.pyi\nindex 03b626940..99946bed8 100644\n--- a/rustworkx/generators/__init__.pyi\n+++ b/rustworkx/generators/__init__.pyi\n@@ -90,12 +90,16 @@ def full_rary_tree(\n     weights: Sequence[Any] | None = ...,\n     multigraph: bool = ...,\n ) -> PyGraph: ...\n-def hexagonal_lattice_graph(rows: int, cols: int, multigraph: bool = ...) -> PyGraph: ...\n+def hexagonal_lattice_graph(\n+    rows: int, cols: int, multigraph: bool = ..., periodic: bool = ..., with_positions: bool = ...\n+) -> PyGraph: ...\n def directed_hexagonal_lattice_graph(\n     rows: int,\n     cols: int,\n     bidirectional: bool = ...,\n     multigraph: bool = ...,\n+    periodic: bool = ...,\n+    with_positions: bool = ...,\n ) -> PyDiGraph: ...\n def lollipop_graph(\n     num_mesh_nodes: int | None = ...,\ndiff --git a/src/generators.rs b/src/generators.rs\nindex df4074d37..7d41a10b4 100644\n--- a/src/generators.rs\n+++ b/src/generators.rs\n@@ -14,7 +14,7 @@ use petgraph::algo;\n use petgraph::prelude::*;\n use petgraph::Undirected;\n \n-use pyo3::exceptions::{PyIndexError, PyOverflowError};\n+use pyo3::exceptions::{PyIndexError, PyOverflowError, PyValueError};\n use pyo3::prelude::*;\n use pyo3::wrap_pyfunction;\n use pyo3::Python;\n@@ -1103,6 +1103,12 @@ pub fn full_rary_tree(\n     })\n }\n \n+fn _hexagonal_lattice_node_position(u: usize, v: usize) -> (f64, f64) {\n+    let [i, j, a, b, c] = [u, v, u / 2, v % 2, u % 2].map(|val| val as f64);\n+    const SQRT3: f64 = 1.732_050_807_568_877_2_f64;\n+    (0.5 + i + a + b * (c - 0.5), SQRT3 * j)\n+}\n+\n /// Generate an undirected hexagonal lattice graph.\n ///\n /// :param int rows: The number of rows to generate the graph with.\n@@ -1111,6 +1117,12 @@ pub fn full_rary_tree(\n ///     :class:`~rustworkx.PyGraph` object will not be not be a multigraph and\n ///     won't allow parallel edges to be added. Instead\n ///     calls which would create a parallel edge will update the existing edge.\n+/// :param bool periodic: When set to ``True`` the boundaries of the lattice\n+///     will be joined to form a periodic grid. Requires ``cols`` to be even,\n+///     ``rows > 1``, and ``cols > 1``.\n+/// :param bool with_positions: When set to ``True`` each node will be assigned\n+///     a pair of coordinates ``(x, y)`` as a weight. This embeds the nodes in\n+///     the plane so that each hexagon is regular (with side length 2).\n ///\n /// :returns: The generated hexagonal lattice graph.\n ///\n@@ -1128,20 +1140,39 @@ pub fn full_rary_tree(\n ///\n #[pyfunction]\n #[pyo3(\n-    signature=(rows, cols, multigraph=true),\n+    signature=(rows, cols, multigraph=true, periodic=false, with_positions=false),\n )]\n pub fn hexagonal_lattice_graph(\n     py: Python,\n     rows: usize,\n     cols: usize,\n     multigraph: bool,\n+    periodic: bool,\n+    with_positions: bool,\n ) -> PyResult<graph::PyGraph> {\n     let default_fn = || py.None();\n-    let graph: StablePyGraph<Undirected> =\n-        match core_generators::hexagonal_lattice_graph(rows, cols, default_fn, default_fn, false) {\n+    let graph: StablePyGraph<Undirected> = if with_positions {\n+        let node_position_fn =\n+            |u: usize, v: usize| _hexagonal_lattice_node_position(u, v).to_object(py);\n+        match core_generators::hexagonal_lattice_graph_weighted(\n+            rows,\n+            cols,\n+            node_position_fn,\n+            default_fn,\n+            false,\n+            periodic,\n+        ) {\n             Ok(graph) => graph,\n-            Err(_) => return Err(PyIndexError::new_err(\"rows and cols not specified\")),\n-        };\n+            Err(_) => return Err(PyValueError::new_err(\"Invalid arguments\")),\n+        }\n+    } else {\n+        match core_generators::hexagonal_lattice_graph(\n+            rows, cols, default_fn, default_fn, false, periodic,\n+        ) {\n+            Ok(graph) => graph,\n+            Err(_) => return Err(PyValueError::new_err(\"Invalid arguments\")),\n+        }\n+    };\n     Ok(graph::PyGraph {\n         graph,\n         node_removed: false,\n@@ -1162,6 +1193,12 @@ pub fn hexagonal_lattice_graph(\n ///     :class:`~rustworkx.PyDiGraph` object will not be not be a multigraph and\n ///     won't allow parallel edges to be added. Instead\n ///     calls which would create a parallel edge will update the existing edge.\n+/// :param bool periodic: When set to ``True`` the boundaries of the lattice\n+///     will be joined to form a periodic grid. Requires ``cols`` to be even,\n+///     ``rows > 1``, and ``cols > 1``.\n+/// :param bool with_positions: When set to ``True`` each node will be assigned\n+///     a pair of coordinates ``(x, y)`` as a payload. This embeds the nodes in\n+///     the plane so that each hexagon is regular (with side length 2).\n ///\n /// :returns: The generated directed hexagonal lattice graph.\n ///\n@@ -1179,7 +1216,7 @@ pub fn hexagonal_lattice_graph(\n ///\n #[pyfunction]\n #[pyo3(\n-    signature=(rows, cols, bidirectional=false, multigraph=true),\n+    signature=(rows, cols, bidirectional=false, multigraph=true, periodic=false, with_positions=false),\n )]\n pub fn directed_hexagonal_lattice_graph(\n     py: Python,\n@@ -1187,17 +1224,36 @@ pub fn directed_hexagonal_lattice_graph(\n     cols: usize,\n     bidirectional: bool,\n     multigraph: bool,\n+    periodic: bool,\n+    with_positions: bool,\n ) -> PyResult<digraph::PyDiGraph> {\n     let default_fn = || py.None();\n-    let graph: StablePyGraph<Directed> = match core_generators::hexagonal_lattice_graph(\n-        rows,\n-        cols,\n-        default_fn,\n-        default_fn,\n-        bidirectional,\n-    ) {\n-        Ok(graph) => graph,\n-        Err(_) => return Err(PyIndexError::new_err(\"rows and cols not specified\")),\n+    let graph: StablePyGraph<Directed> = if with_positions {\n+        let node_position_fn =\n+            |u: usize, v: usize| _hexagonal_lattice_node_position(u, v).to_object(py);\n+        match core_generators::hexagonal_lattice_graph_weighted(\n+            rows,\n+            cols,\n+            node_position_fn,\n+            default_fn,\n+            bidirectional,\n+            periodic,\n+        ) {\n+            Ok(graph) => graph,\n+            Err(_) => return Err(PyValueError::new_err(\"Invalid arguments\")),\n+        }\n+    } else {\n+        match core_generators::hexagonal_lattice_graph(\n+            rows,\n+            cols,\n+            default_fn,\n+            default_fn,\n+            bidirectional,\n+            periodic,\n+        ) {\n+            Ok(graph) => graph,\n+            Err(_) => return Err(PyValueError::new_err(\"Invalid arguments\")),\n+        }\n     };\n     Ok(digraph::PyDiGraph {\n         graph,\n", "instance_id": "Qiskit__rustworkx-1213", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to add support for periodicity and position coordinates to the `hexagonal_lattice_graph` function in the rustworkx library, mirroring functionality from NetworkX. It specifies the expected enhancements through the addition of `periodic` and `with_positions` boolean arguments, which provides a clear goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not elaborate on how periodicity should behave (e.g., how boundaries are connected) or what the position coordinates should represent (e.g., the coordinate system or scaling). Additionally, edge cases such as invalid input handling for periodicity constraints are mentioned in the code but not in the problem statement. While the \"Definition of Done\" is provided, it lacks detailed examples or expected behavior descriptions, which would have made the requirements comprehensive. Thus, I rate the clarity as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, involving modifications to a core graph generation function in Rust, with additions of new logic for periodicity and position coordinates across multiple parts of the file (e.g., new builder struct, edge connection logic for periodic boundaries, and position-dependent node weights). The changes span a single primary file but also affect Python bindings and interface definitions, requiring understanding of both Rust and Python integration via PyO3. Second, the technical concepts involved include graph theory (understanding hexagonal lattice structures and periodicity), Rust generics and traits (for graph construction with `petgraph`), and coordinate geometry for position assignments. These concepts are moderately complex, especially for someone unfamiliar with graph generation algorithms. Third, the problem requires handling edge cases, such as input validation for periodic graphs (e.g., even column count, minimum row/column constraints), which are implemented in the code changes. While the changes do not impact the broader system architecture significantly, the logic for periodic boundary connections and position calculations adds non-trivial complexity. Overall, I assess the difficulty as 0.55, placing it in the medium category (0.4-0.6), as it requires a solid understanding of multiple concepts and careful implementation across related components, but does not reach the level of hard or very hard due to the localized impact and lack of deep architectural changes or advanced algorithmic challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`script_element.process()` function not working\n### Checklist\n\n- [X] I added a descriptive title\n- [X] I searched for other issues and couldn't find a solution or duplication\n- [X] I already searched in Google and didn't find any good information or help\n\n### What happened?\n\nI'm trying to programmatically execute code in a `<py-editor>` using [the documentation for `editor.process(code)`](https://docs.pyscript.net/2024.8.2/user-guide/editor/#read-write-execute).\r\n\r\nI get the following error when executing `editor.process(editor.code)`:\r\n```\r\npyodide.asm.js:10\r\nUnable to use `window` or `document` -> https://docs.pyscript.net/latest/faq/#sharedarraybuffer\r\n```\r\n### mre.html\r\n```html\r\n<!doctype html>\r\n<html>\r\n\t<head>\r\n\t\t<link\r\n\t\t\trel=\"stylesheet\"\r\n\t\t\thref=\"https://pyscript.net/releases/2024.8.2/core.css\"\r\n\t\t/>\r\n\t\t<script\r\n\t\t\ttype=\"module\"\r\n\t\t\tsrc=\"https://pyscript.net/releases/2024.8.2/core.js\"\r\n\t\t></script>\r\n\t</head>\r\n\t<body>\r\n\t\t<script type=\"py-editor\" id=\"editor\">\r\n\t\t\timport sys\r\n\t\t\tprint(sys.version)\r\n\t\t</script>\r\n\t\t<script type=\"py\" src=\"mre.py\"></script>\r\n\t\t<button py-click=\"submit\">Submit</button>\r\n\t</body>\r\n</html>\r\n```\r\n### mre.py\r\n```python\r\nfrom pyscript import document\r\n\r\n\r\ndef submit(event):\r\n    editor = document.querySelector('#editor')\r\n    editor.process(editor.code)\r\n```\r\nI've tried making the `submit` function `async` and `await`ing `editor.process(editor.code)`. I've also tried using micropython instead of pyodide. I've even tried `process`ing the editor's code from JavaScript according to #2053 :\r\n```javascript\r\n<script>\r\naddEventListener('mpy-editor', async (event) => {\r\n    // the script is the target of this event\r\n    const { target: script } = event\r\n    // it can process any Python code\r\n    await script.process('print(\"hello\")')\r\n})\r\n</script>\r\n```\r\nEverything results in the same error message above.\n\n### What browsers are you seeing the problem on? (if applicable)\n\nChrome\n\n### Console info\n\n```shell\npyodide.asm.js:10\r\nUnable to use `window` or `document` -> https://docs.pyscript.net/latest/faq/#sharedarraybuffer\n```\n\n\n### Additional Context\n\n_No response_\n", "patch": "diff --git a/pyscript.core/package-lock.json b/pyscript.core/package-lock.json\nindex 1eae2889472..f70fafe25da 100644\n--- a/pyscript.core/package-lock.json\n+++ b/pyscript.core/package-lock.json\n@@ -19,9 +19,9 @@\n                 \"type-checked-collections\": \"^0.1.7\"\n             },\n             \"devDependencies\": {\n-                \"@codemirror/commands\": \"^6.6.1\",\n+                \"@codemirror/commands\": \"^6.6.2\",\n                 \"@codemirror/lang-python\": \"^6.1.6\",\n-                \"@codemirror/language\": \"^6.10.2\",\n+                \"@codemirror/language\": \"^6.10.3\",\n                 \"@codemirror/state\": \"^6.4.1\",\n                 \"@codemirror/view\": \"^6.33.0\",\n                 \"@playwright/test\": \"1.45.3\",\n@@ -31,13 +31,13 @@\n                 \"@webreflection/toml-j0.4\": \"^1.1.3\",\n                 \"@xterm/addon-fit\": \"^0.10.0\",\n                 \"@xterm/addon-web-links\": \"^0.11.0\",\n-                \"bun\": \"^1.1.27\",\n+                \"bun\": \"^1.1.29\",\n                 \"chokidar\": \"^4.0.0\",\n                 \"codedent\": \"^0.1.2\",\n                 \"codemirror\": \"^6.0.1\",\n                 \"eslint\": \"^9.10.0\",\n                 \"flatted\": \"^3.3.1\",\n-                \"rollup\": \"^4.21.3\",\n+                \"rollup\": \"^4.22.2\",\n                 \"rollup-plugin-postcss\": \"^4.0.2\",\n                 \"rollup-plugin-string\": \"^3.0.0\",\n                 \"static-handler\": \"^0.5.3\",\n@@ -66,9 +66,9 @@\n             }\n         },\n         \"node_modules/@codemirror/commands\": {\n-            \"version\": \"6.6.1\",\n-            \"resolved\": \"https://registry.npmjs.org/@codemirror/commands/-/commands-6.6.1.tgz\",\n-            \"integrity\": \"sha512-iBfKbyIoXS1FGdsKcZmnrxmbc8VcbMrSgD7AVrsnX+WyAYjmUDWvE93dt5D874qS4CCVu4O1JpbagHdXbbLiOw==\",\n+            \"version\": \"6.6.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@codemirror/commands/-/commands-6.6.2.tgz\",\n+            \"integrity\": \"sha512-Fq7eWOl1Rcbrfn6jD8FPCj9Auaxdm5nIK5RYOeW7ughnd/rY5AmPg6b+CfsG39ZHdwiwe8lde3q8uR7CF5S0yQ==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n@@ -93,9 +93,9 @@\n             }\n         },\n         \"node_modules/@codemirror/language\": {\n-            \"version\": \"6.10.2\",\n-            \"resolved\": \"https://registry.npmjs.org/@codemirror/language/-/language-6.10.2.tgz\",\n-            \"integrity\": \"sha512-kgbTYTo0Au6dCSc/TFy7fK3fpJmgHDv1sG1KNQKJXVi+xBTEeBPY/M30YXiU6mMXeH+YIDLsbrT4ZwNRdtF+SA==\",\n+            \"version\": \"6.10.3\",\n+            \"resolved\": \"https://registry.npmjs.org/@codemirror/language/-/language-6.10.3.tgz\",\n+            \"integrity\": \"sha512-kDqEU5sCP55Oabl6E7m5N+vZRoc0iWqgDVhEKifcHzPzjqCegcO4amfrYVL9PmPZpl4G0yjkpTpUO/Ui8CzO8A==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n@@ -478,9 +478,9 @@\n             }\n         },\n         \"node_modules/@oven/bun-darwin-aarch64\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-aarch64/-/bun-darwin-aarch64-1.1.27.tgz\",\n-            \"integrity\": \"sha512-h5Mf2+AESEtHHGRdSDQgDU7TY2vMPMeAuwkf/XPMDpaS+EoIUFayj1CXShB25gT16uw/ww/Pl3LaN0sN0tcWhQ==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-aarch64/-/bun-darwin-aarch64-1.1.29.tgz\",\n+            \"integrity\": \"sha512-Z8WnrXoAXg6ENU6z+Mil94oipMBMliJIj4XWWqjNC33/ZGPqMdPT5MbFQjbdHjxe5bRBW/aHaMU3G35fGx9Seg==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -492,9 +492,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-darwin-x64\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64/-/bun-darwin-x64-1.1.27.tgz\",\n-            \"integrity\": \"sha512-l+uAcE0kh4+c2VUwKvSpsHFQbzR2eLxGKhONa4AJBNVW4UXIpSdH12Qj9fG4Xqgq7s26zs32jPsl2C2NQsmRag==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64/-/bun-darwin-x64-1.1.29.tgz\",\n+            \"integrity\": \"sha512-tDHDR+OWLCKEP0xE8IGCWRxSsxO5tr3rvyrSzy6CXYkoWPz66kODbtbGfWl8HSwunrQxJKnE2L9LWX2Eiz3Cpw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -506,9 +506,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-darwin-x64-baseline\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64-baseline/-/bun-darwin-x64-baseline-1.1.27.tgz\",\n-            \"integrity\": \"sha512-Ld+FBW3VUzGOI9QhsQKnaiYinGcOQg83NpagoeyMASozFJEMyDEr/jxA5KxoSEJRChQrGZowq/7IfoPIcJZIUA==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-darwin-x64-baseline/-/bun-darwin-x64-baseline-1.1.29.tgz\",\n+            \"integrity\": \"sha512-B+zKlJR+3ANPDU+vNjvXaqgB63hYcCk16/k45hiInvIsDXfSdDn4+LMHwZTFdfUWTCGbNMFg9u4bZCtK0nhMDw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -520,9 +520,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-aarch64\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-aarch64/-/bun-linux-aarch64-1.1.27.tgz\",\n-            \"integrity\": \"sha512-4oaNlx07SFxhO26Lm2cfRjozEQX/qRoWFhnz+EFLlGPUPLB2cNmXCVMtmXJep0u4Jub5pGK9yISwNT1voZ6KMg==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-aarch64/-/bun-linux-aarch64-1.1.29.tgz\",\n+            \"integrity\": \"sha512-BuV2Gni+FaMo1r7+3vLjefQYtQcWE01zJJ4zCjyzikP1L4r6PUaLeJyKXbqF6sIR0cLjG6LWj66EGskakDyvWA==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -534,9 +534,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-x64\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64/-/bun-linux-x64-1.1.27.tgz\",\n-            \"integrity\": \"sha512-ktplW245+ke0GIKdBEzZisZ4mwAqdPYmW3RzhDPGB7193jVmaAE1YVUscrWkTAdfl6YMGb99IXThrhcXSddeAw==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64/-/bun-linux-x64-1.1.29.tgz\",\n+            \"integrity\": \"sha512-JVLiTafybuOFIeC80mZEzHdkOCCzlOtOV5zCbvYAIb7D3SM64fNBwrR0dvDkJTQtsjbwt8YeVFN2YRSI/wlFkw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -548,9 +548,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-linux-x64-baseline\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64-baseline/-/bun-linux-x64-baseline-1.1.27.tgz\",\n-            \"integrity\": \"sha512-495DOwgzpr/0ta/AaSRYb7EJeAEAf38cBOUO6oJzCxT5NdWRbZeAo5bmqcrBke1AexnKPA7RUN9Z5+t1TvqjtQ==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-linux-x64-baseline/-/bun-linux-x64-baseline-1.1.29.tgz\",\n+            \"integrity\": \"sha512-tQhm1SDyymBvc2L3zIN7FI+sI3g9gxavDYuvL/Lz17nEk1oqP5DFE2lF9QWJMDdQL3XQKH/drI27yNpfPHgAPw==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -562,9 +562,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-windows-x64\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64/-/bun-windows-x64-1.1.27.tgz\",\n-            \"integrity\": \"sha512-LIZRqlyLpMEkmbsA+d/GmQnEdJzMGlFRlVtdQnyjTskW+uMv+lBHmWu7CkdcCCVPtf6wRDYgdkjpQOAw1wle5g==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64/-/bun-windows-x64-1.1.29.tgz\",\n+            \"integrity\": \"sha512-d930+pSkNVHPJJBryIh1gsAKk7lp1HzcktFe0bfxwmpHwjj4BiVfMjU1YaCnsdWKNs2Nv1Y1PLVDNyUCQVfo0Q==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -576,9 +576,9 @@\n             ]\n         },\n         \"node_modules/@oven/bun-windows-x64-baseline\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64-baseline/-/bun-windows-x64-baseline-1.1.27.tgz\",\n-            \"integrity\": \"sha512-Qfn80kb9eDZaecCVVVxFpNfuHvo7h4yQlh278u9Vs0vSrHTYG4dxoH4SWkZSzHa44ZwKRpa1TmMfDVhX/HE+mA==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/@oven/bun-windows-x64-baseline/-/bun-windows-x64-baseline-1.1.29.tgz\",\n+            \"integrity\": \"sha512-+QWT8Kp+3Sl54QUa6uBsDzQlX11thMMVAw+yUwSRU4Y5FVSN8RPuxhN8ijJ1QGm1KOYA2sCI6V+3lyNMNmRvAA==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -715,9 +715,9 @@\n             }\n         },\n         \"node_modules/@rollup/rollup-android-arm-eabi\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.21.3.tgz\",\n-            \"integrity\": \"sha512-MmKSfaB9GX+zXl6E8z4koOr/xU63AMVleLEa64v7R0QF/ZloMs5vcD1sHgM64GXXS1csaJutG+ddtzcueI/BLg==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.22.2.tgz\",\n+            \"integrity\": \"sha512-8Ao+EDmTPjZ1ZBABc1ohN7Ylx7UIYcjReZinigedTOnGFhIctyGPxY2II+hJ6gD2/vkDKZTyQ0e7++kwv6wDrw==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -729,9 +729,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-android-arm64\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.21.3.tgz\",\n-            \"integrity\": \"sha512-zrt8ecH07PE3sB4jPOggweBjJMzI1JG5xI2DIsUbkA+7K+Gkjys6eV7i9pOenNSDJH3eOr/jLb/PzqtmdwDq5g==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.22.2.tgz\",\n+            \"integrity\": \"sha512-I+B1v0a4iqdS9DvYt1RJZ3W+Oh9EVWjbY6gp79aAYipIbxSLEoQtFQlZEnUuwhDXCqMxJ3hluxKAdPD+GiluFQ==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -743,9 +743,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-darwin-arm64\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.21.3.tgz\",\n-            \"integrity\": \"sha512-P0UxIOrKNBFTQaXTxOH4RxuEBVCgEA5UTNV6Yz7z9QHnUJ7eLX9reOd/NYMO3+XZO2cco19mXTxDMXxit4R/eQ==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.22.2.tgz\",\n+            \"integrity\": \"sha512-BTHO7rR+LC67OP7I8N8GvdvnQqzFujJYWo7qCQ8fGdQcb8Gn6EQY+K1P+daQLnDCuWKbZ+gHAQZuKiQkXkqIYg==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -757,9 +757,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-darwin-x64\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.21.3.tgz\",\n-            \"integrity\": \"sha512-L1M0vKGO5ASKntqtsFEjTq/fD91vAqnzeaF6sfNAy55aD+Hi2pBI5DKwCO+UNDQHWsDViJLqshxOahXyLSh3EA==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.22.2.tgz\",\n+            \"integrity\": \"sha512-1esGwDNFe2lov4I6GsEeYaAMHwkqk0IbuGH7gXGdBmd/EP9QddJJvTtTF/jv+7R8ZTYPqwcdLpMTxK8ytP6k6Q==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -771,9 +771,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm-gnueabihf\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.21.3.tgz\",\n-            \"integrity\": \"sha512-btVgIsCjuYFKUjopPoWiDqmoUXQDiW2A4C3Mtmp5vACm7/GnyuprqIDPNczeyR5W8rTXEbkmrJux7cJmD99D2g==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.22.2.tgz\",\n+            \"integrity\": \"sha512-GBHuY07x96OTEM3OQLNaUSUwrOhdMea/LDmlFHi/HMonrgF6jcFrrFFwJhhe84XtA1oK/Qh4yFS+VMREf6dobg==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -785,9 +785,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm-musleabihf\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.21.3.tgz\",\n-            \"integrity\": \"sha512-zmjbSphplZlau6ZTkxd3+NMtE4UKVy7U4aVFMmHcgO5CUbw17ZP6QCgyxhzGaU/wFFdTfiojjbLG3/0p9HhAqA==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.22.2.tgz\",\n+            \"integrity\": \"sha512-Dbfa9Sc1G1lWxop0gNguXOfGhaXQWAGhZUcqA0Vs6CnJq8JW/YOw/KvyGtQFmz4yDr0H4v9X248SM7bizYj4yQ==\",\n             \"cpu\": [\n                 \"arm\"\n             ],\n@@ -799,9 +799,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm64-gnu\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.21.3.tgz\",\n-            \"integrity\": \"sha512-nSZfcZtAnQPRZmUkUQwZq2OjQciR6tEoJaZVFvLHsj0MF6QhNMg0fQ6mUOsiCUpTqxTx0/O6gX0V/nYc7LrgPw==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.22.2.tgz\",\n+            \"integrity\": \"sha512-Z1YpgBvFYhZIyBW5BoopwSg+t7yqEhs5HCei4JbsaXnhz/eZehT18DaXl957aaE9QK7TRGFryCAtStZywcQe1A==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -813,9 +813,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-arm64-musl\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.21.3.tgz\",\n-            \"integrity\": \"sha512-MnvSPGO8KJXIMGlQDYfvYS3IosFN2rKsvxRpPO2l2cum+Z3exiExLwVU+GExL96pn8IP+GdH8Tz70EpBhO0sIQ==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.22.2.tgz\",\n+            \"integrity\": \"sha512-66Zszr7i/JaQ0u/lefcfaAw16wh3oT72vSqubIMQqWzOg85bGCPhoeykG/cC5uvMzH80DQa2L539IqKht6twVA==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -827,9 +827,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-powerpc64le-gnu\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-powerpc64le-gnu/-/rollup-linux-powerpc64le-gnu-4.21.3.tgz\",\n-            \"integrity\": \"sha512-+W+p/9QNDr2vE2AXU0qIy0qQE75E8RTwTwgqS2G5CRQ11vzq0tbnfBd6brWhS9bCRjAjepJe2fvvkvS3dno+iw==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-powerpc64le-gnu/-/rollup-linux-powerpc64le-gnu-4.22.2.tgz\",\n+            \"integrity\": \"sha512-HpJCMnlMTfEhwo19bajvdraQMcAq3FX08QDx3OfQgb+414xZhKNf3jNvLFYKbbDSGBBrQh5yNwWZrdK0g0pokg==\",\n             \"cpu\": [\n                 \"ppc64\"\n             ],\n@@ -841,9 +841,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-riscv64-gnu\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.21.3.tgz\",\n-            \"integrity\": \"sha512-yXH6K6KfqGXaxHrtr+Uoy+JpNlUlI46BKVyonGiaD74ravdnF9BUNC+vV+SIuB96hUMGShhKV693rF9QDfO6nQ==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.22.2.tgz\",\n+            \"integrity\": \"sha512-/egzQzbOSRef2vYCINKITGrlwkzP7uXRnL+xU2j75kDVp3iPdcF0TIlfwTRF8woBZllhk3QaxNOEj2Ogh3t9hg==\",\n             \"cpu\": [\n                 \"riscv64\"\n             ],\n@@ -855,9 +855,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-s390x-gnu\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.21.3.tgz\",\n-            \"integrity\": \"sha512-R8cwY9wcnApN/KDYWTH4gV/ypvy9yZUHlbJvfaiXSB48JO3KpwSpjOGqO4jnGkLDSk1hgjYkTbTt6Q7uvPf8eg==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.22.2.tgz\",\n+            \"integrity\": \"sha512-qgYbOEbrPfEkH/OnUJd1/q4s89FvNJQIUldx8X2F/UM5sEbtkqZpf2s0yly2jSCKr1zUUOY1hnTP2J1WOzMAdA==\",\n             \"cpu\": [\n                 \"s390x\"\n             ],\n@@ -869,9 +869,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-x64-gnu\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.21.3.tgz\",\n-            \"integrity\": \"sha512-kZPbX/NOPh0vhS5sI+dR8L1bU2cSO9FgxwM8r7wHzGydzfSjLRCFAT87GR5U9scj2rhzN3JPYVC7NoBbl4FZ0g==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.22.2.tgz\",\n+            \"integrity\": \"sha512-a0lkvNhFLhf+w7A95XeBqGQaG0KfS3hPFJnz1uraSdUe/XImkp/Psq0Ca0/UdD5IEAGoENVmnYrzSC9Y2a2uKQ==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -883,9 +883,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-linux-x64-musl\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.21.3.tgz\",\n-            \"integrity\": \"sha512-S0Yq+xA1VEH66uiMNhijsWAafffydd2X5b77eLHfRmfLsRSpbiAWiRHV6DEpz6aOToPsgid7TI9rGd6zB1rhbg==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.22.2.tgz\",\n+            \"integrity\": \"sha512-sSWBVZgzwtsuG9Dxi9kjYOUu/wKW+jrbzj4Cclabqnfkot8Z3VEHcIgyenA3lLn/Fu11uDviWjhctulkhEO60g==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -897,9 +897,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-arm64-msvc\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.21.3.tgz\",\n-            \"integrity\": \"sha512-9isNzeL34yquCPyerog+IMCNxKR8XYmGd0tHSV+OVx0TmE0aJOo9uw4fZfUuk2qxobP5sug6vNdZR6u7Mw7Q+Q==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.22.2.tgz\",\n+            \"integrity\": \"sha512-t/YgCbZ638R/r7IKb9yCM6nAek1RUvyNdfU0SHMDLOf6GFe/VG1wdiUAsxTWHKqjyzkRGg897ZfCpdo1bsCSsA==\",\n             \"cpu\": [\n                 \"arm64\"\n             ],\n@@ -911,9 +911,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-ia32-msvc\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.21.3.tgz\",\n-            \"integrity\": \"sha512-nMIdKnfZfzn1Vsk+RuOvl43ONTZXoAPUUxgcU0tXooqg4YrAqzfKzVenqqk2g5efWh46/D28cKFrOzDSW28gTA==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.22.2.tgz\",\n+            \"integrity\": \"sha512-kTmX5uGs3WYOA+gYDgI6ITkZng9SP71FEMoHNkn+cnmb9Zuyyay8pf0oO5twtTwSjNGy1jlaWooTIr+Dw4tIbw==\",\n             \"cpu\": [\n                 \"ia32\"\n             ],\n@@ -925,9 +925,9 @@\n             ]\n         },\n         \"node_modules/@rollup/rollup-win32-x64-msvc\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.21.3.tgz\",\n-            \"integrity\": \"sha512-fOvu7PCQjAj4eWDEuD8Xz5gpzFqXzGlxHZozHP4b9Jxv9APtdxL6STqztDzMLuRXEc4UpXGGhx029Xgm91QBeA==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.22.2.tgz\",\n+            \"integrity\": \"sha512-Yy8So+SoRz8I3NS4Bjh91BICPOSVgdompTIPYTByUqU66AXSIOgmW3Lv1ke3NORPqxdF+RdrZET+8vYai6f4aA==\",\n             \"cpu\": [\n                 \"x64\"\n             ],\n@@ -1178,9 +1178,9 @@\n             }\n         },\n         \"node_modules/bun\": {\n-            \"version\": \"1.1.27\",\n-            \"resolved\": \"https://registry.npmjs.org/bun/-/bun-1.1.27.tgz\",\n-            \"integrity\": \"sha512-FLnMaOuVVuhjPIfmlRtAE42B2x+oSWO6jW8MZ/Ss179qokN//nzIhuj/07bdA016U6d11HyLvvGb6eaH0HKhBQ==\",\n+            \"version\": \"1.1.29\",\n+            \"resolved\": \"https://registry.npmjs.org/bun/-/bun-1.1.29.tgz\",\n+            \"integrity\": \"sha512-SKhpyKNZtgxrVel9ec9xon3LDv8mgpiuFhARgcJo1YIbggY2PBrKHRNiwQ6Qlb+x3ivmRurfuwWgwGexjpgBRg==\",\n             \"cpu\": [\n                 \"arm64\",\n                 \"x64\"\n@@ -1198,14 +1198,14 @@\n                 \"bunx\": \"bin/bun.exe\"\n             },\n             \"optionalDependencies\": {\n-                \"@oven/bun-darwin-aarch64\": \"1.1.27\",\n-                \"@oven/bun-darwin-x64\": \"1.1.27\",\n-                \"@oven/bun-darwin-x64-baseline\": \"1.1.27\",\n-                \"@oven/bun-linux-aarch64\": \"1.1.27\",\n-                \"@oven/bun-linux-x64\": \"1.1.27\",\n-                \"@oven/bun-linux-x64-baseline\": \"1.1.27\",\n-                \"@oven/bun-windows-x64\": \"1.1.27\",\n-                \"@oven/bun-windows-x64-baseline\": \"1.1.27\"\n+                \"@oven/bun-darwin-aarch64\": \"1.1.29\",\n+                \"@oven/bun-darwin-x64\": \"1.1.29\",\n+                \"@oven/bun-darwin-x64-baseline\": \"1.1.29\",\n+                \"@oven/bun-linux-aarch64\": \"1.1.29\",\n+                \"@oven/bun-linux-x64\": \"1.1.29\",\n+                \"@oven/bun-linux-x64-baseline\": \"1.1.29\",\n+                \"@oven/bun-windows-x64\": \"1.1.29\",\n+                \"@oven/bun-windows-x64-baseline\": \"1.1.29\"\n             }\n         },\n         \"node_modules/callsites\": {\n@@ -3546,9 +3546,9 @@\n             }\n         },\n         \"node_modules/rollup\": {\n-            \"version\": \"4.21.3\",\n-            \"resolved\": \"https://registry.npmjs.org/rollup/-/rollup-4.21.3.tgz\",\n-            \"integrity\": \"sha512-7sqRtBNnEbcBtMeRVc6VRsJMmpI+JU1z9VTvW8D4gXIYQFz0aLcsE6rRkyghZkLfEgUZgVvOG7A5CVz/VW5GIA==\",\n+            \"version\": \"4.22.2\",\n+            \"resolved\": \"https://registry.npmjs.org/rollup/-/rollup-4.22.2.tgz\",\n+            \"integrity\": \"sha512-JWWpTrZmqQGQWt16xvNn6KVIUz16VtZwl984TKw0dfqqRpFwtLJYYk1/4BTgplndMQKWUk/yB4uOShYmMzA2Vg==\",\n             \"dev\": true,\n             \"license\": \"MIT\",\n             \"dependencies\": {\n@@ -3562,22 +3562,22 @@\n                 \"npm\": \">=8.0.0\"\n             },\n             \"optionalDependencies\": {\n-                \"@rollup/rollup-android-arm-eabi\": \"4.21.3\",\n-                \"@rollup/rollup-android-arm64\": \"4.21.3\",\n-                \"@rollup/rollup-darwin-arm64\": \"4.21.3\",\n-                \"@rollup/rollup-darwin-x64\": \"4.21.3\",\n-                \"@rollup/rollup-linux-arm-gnueabihf\": \"4.21.3\",\n-                \"@rollup/rollup-linux-arm-musleabihf\": \"4.21.3\",\n-                \"@rollup/rollup-linux-arm64-gnu\": \"4.21.3\",\n-                \"@rollup/rollup-linux-arm64-musl\": \"4.21.3\",\n-                \"@rollup/rollup-linux-powerpc64le-gnu\": \"4.21.3\",\n-                \"@rollup/rollup-linux-riscv64-gnu\": \"4.21.3\",\n-                \"@rollup/rollup-linux-s390x-gnu\": \"4.21.3\",\n-                \"@rollup/rollup-linux-x64-gnu\": \"4.21.3\",\n-                \"@rollup/rollup-linux-x64-musl\": \"4.21.3\",\n-                \"@rollup/rollup-win32-arm64-msvc\": \"4.21.3\",\n-                \"@rollup/rollup-win32-ia32-msvc\": \"4.21.3\",\n-                \"@rollup/rollup-win32-x64-msvc\": \"4.21.3\",\n+                \"@rollup/rollup-android-arm-eabi\": \"4.22.2\",\n+                \"@rollup/rollup-android-arm64\": \"4.22.2\",\n+                \"@rollup/rollup-darwin-arm64\": \"4.22.2\",\n+                \"@rollup/rollup-darwin-x64\": \"4.22.2\",\n+                \"@rollup/rollup-linux-arm-gnueabihf\": \"4.22.2\",\n+                \"@rollup/rollup-linux-arm-musleabihf\": \"4.22.2\",\n+                \"@rollup/rollup-linux-arm64-gnu\": \"4.22.2\",\n+                \"@rollup/rollup-linux-arm64-musl\": \"4.22.2\",\n+                \"@rollup/rollup-linux-powerpc64le-gnu\": \"4.22.2\",\n+                \"@rollup/rollup-linux-riscv64-gnu\": \"4.22.2\",\n+                \"@rollup/rollup-linux-s390x-gnu\": \"4.22.2\",\n+                \"@rollup/rollup-linux-x64-gnu\": \"4.22.2\",\n+                \"@rollup/rollup-linux-x64-musl\": \"4.22.2\",\n+                \"@rollup/rollup-win32-arm64-msvc\": \"4.22.2\",\n+                \"@rollup/rollup-win32-ia32-msvc\": \"4.22.2\",\n+                \"@rollup/rollup-win32-x64-msvc\": \"4.22.2\",\n                 \"fsevents\": \"~2.3.2\"\n             }\n         },\ndiff --git a/pyscript.core/package.json b/pyscript.core/package.json\nindex 2d9c7e128d6..b6865ffef57 100644\n--- a/pyscript.core/package.json\n+++ b/pyscript.core/package.json\n@@ -65,9 +65,9 @@\n         \"type-checked-collections\": \"^0.1.7\"\n     },\n     \"devDependencies\": {\n-        \"@codemirror/commands\": \"^6.6.1\",\n+        \"@codemirror/commands\": \"^6.6.2\",\n         \"@codemirror/lang-python\": \"^6.1.6\",\n-        \"@codemirror/language\": \"^6.10.2\",\n+        \"@codemirror/language\": \"^6.10.3\",\n         \"@codemirror/state\": \"^6.4.1\",\n         \"@codemirror/view\": \"^6.33.0\",\n         \"@playwright/test\": \"1.45.3\",\n@@ -77,13 +77,13 @@\n         \"@webreflection/toml-j0.4\": \"^1.1.3\",\n         \"@xterm/addon-fit\": \"^0.10.0\",\n         \"@xterm/addon-web-links\": \"^0.11.0\",\n-        \"bun\": \"^1.1.27\",\n+        \"bun\": \"^1.1.29\",\n         \"chokidar\": \"^4.0.0\",\n         \"codedent\": \"^0.1.2\",\n         \"codemirror\": \"^6.0.1\",\n         \"eslint\": \"^9.10.0\",\n         \"flatted\": \"^3.3.1\",\n-        \"rollup\": \"^4.21.3\",\n+        \"rollup\": \"^4.22.2\",\n         \"rollup-plugin-postcss\": \"^4.0.2\",\n         \"rollup-plugin-string\": \"^3.0.0\",\n         \"static-handler\": \"^0.5.3\",\ndiff --git a/pyscript.core/src/plugins/py-editor.js b/pyscript.core/src/plugins/py-editor.js\nindex ba4fdcb60b0..01293ae4d7f 100644\n--- a/pyscript.core/src/plugins/py-editor.js\n+++ b/pyscript.core/src/plugins/py-editor.js\n@@ -290,9 +290,11 @@ const init = async (script, type, interpreter) => {\n             /**\n              * Simulate a setup node overriding the source to evaluate.\n              * @param {string} code the Python code to evaluate.\n+             * @param {boolean} asRunButtonAction invoke the `Run` button handler.\n              * @returns {Promise<...>} fulfill once code has been evaluated.\n              */\n-            value(code) {\n+            value(code, asRunButtonAction = false) {\n+                if (asRunButtonAction) return listener();\n                 const wasSetup = isSetup;\n                 const wasSource = source;\n                 isSetup = true;\n", "instance_id": "pyscript__pyscript-2177", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `script_element.process()` function in the PyScript library fails with a specific error related to `window` or `document` usage and SharedArrayBuffer restrictions. The user provides a minimal reproducible example (MRE) in HTML and Python, along with the exact error message and context about the browser (Chrome). They also mention attempts to resolve the issue (e.g., using async/await, switching to MicroPython, and trying JavaScript alternatives). However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior of `editor.process()` beyond a general reference to the documentation, nor does it clarify the environment setup (e.g., whether the browser is configured to support SharedArrayBuffer as per the linked FAQ). Additionally, edge cases or specific constraints (e.g., browser compatibility beyond Chrome) are not mentioned. Overall, the statement is valid and clear but lacks some finer details that could aid in a complete understanding of the issue.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes appears limited, primarily affecting a single file (`py-editor.js`) with a small modification to the `value()` method to handle a specific use case (adding a parameter to optionally invoke a run button handler). This suggests a focused fix rather than a broad architectural change. However, understanding the root cause requires knowledge of multiple technical concepts: familiarity with PyScript's internals, browser security features like SharedArrayBuffer and Cross-Origin-Opener-Policy (COOP)/Cross-Origin-Embedder-Policy (COEP) headers, and how these interact with Pyodide's runtime environment. Additionally, the error handling and edge cases related to browser configuration (e.g., enabling specific flags or headers) add moderate complexity, as the solution may need to account for environments where such configurations are not feasible. The code changes in the diff (updating dependencies and modifying `py-editor.js`) indicate a straightforward implementation once the issue is understood, but diagnosing the problem and ensuring compatibility across different setups elevate the difficulty slightly. A score of 0.45 reflects a medium-level challenge that requires understanding specific web and runtime constraints but does not demand extensive refactoring or advanced algorithmic work.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add API coverage for LTI resource links\n**What resource needs additional coverage?**\r\nCourses `lti_resource links`, which is a new API which is in beta and will be released soon. \r\n\r\n**What endpoints need to be covered?**\r\nAdd support for these https://canvas.instructure.com/doc/api/lti_resource_links.html \r\n```\r\nGET /api/v1/courses/:course_id/lti_resource_links\r\nGET /api/v1/courses/:course_id/lti_resource_links/:id\r\nPUT /api/v1/courses/:course_id/lti_resource_links/:id\r\n```\r\n\n", "patch": "diff --git a/AUTHORS.md b/AUTHORS.md\nindex d2ac0bfa..67b1d405 100644\n--- a/AUTHORS.md\n+++ b/AUTHORS.md\n@@ -53,6 +53,7 @@\n - Ian Altgilbers [@altgilbers](https://github.com/altgilbers)\n - Ian Turgeon [@iturgeon](https://github.com/iturgeon)\n - [@jackrsteiner](https://github.com/jackrsteiner)\n+- Jasmine Hou [@jsmnhou](https://github.com/jsmnhou)\n - John Raible [@rebelaide](https://github.com/rebelaide)\n - Joon Ro [@joonro](https://github.com/joonro)\n - Jonah Majumder [@jonahmajumder](https://github.com/jonahmajumder)\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex c0a7326f..9a2b16d7 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,10 @@\n \n ## [Unreleased]\n \n+### New Endpoint Coverage\n+\n+- LTI Resource Links (Thanks, [@jsmnhou](https://github.com/jsmnhou))\n+\n ### Backstage\n \n - Updated deploy Action to use more modern processes.\ndiff --git a/canvasapi/course.py b/canvasapi/course.py\nindex 523a2713..2b28f810 100644\n--- a/canvasapi/course.py\n+++ b/canvasapi/course.py\n@@ -23,6 +23,7 @@\n from canvasapi.grading_period import GradingPeriod\n from canvasapi.grading_standard import GradingStandard\n from canvasapi.license import License\n+from canvasapi.lti_resource_link import LTIResourceLink\n from canvasapi.module import Module\n from canvasapi.new_quiz import NewQuiz\n from canvasapi.outcome_import import OutcomeImport\n@@ -438,6 +439,39 @@ def create_late_policy(self, **kwargs):\n \n         return LatePolicy(self._requester, late_policy_json[\"late_policy\"])\n \n+    def create_lti_resource_link(self, url, title=None, custom=None, **kwargs):\n+        \"\"\"\n+        Create a new LTI resource link.\n+\n+        :calls: `POST /api/v1/courses/:course_id/lti_resource_links \\\n+        <https://canvas.instructure.com/doc/api/lti_resource_links.html#method.lti/resource_links.create>`_\n+\n+        :param url: The launch URL for the resource link.\n+        :type url: `str`\n+        :param title: The title of the resource link.\n+        :type title: `str`, optional\n+        :param custom: Custom parameters to send to the tool.\n+        :type custom: `dict`, optional\n+\n+        :rtype: :class:`canvasapi.lti_resource_link.LTIResourceLink`\n+        \"\"\"\n+\n+        if not url:\n+            raise RequiredFieldMissing(\"url is required as a str.\")\n+\n+        kwargs[\"url\"] = url\n+        if title:\n+            kwargs[\"title\"] = title\n+        if custom:\n+            kwargs[\"custom\"] = custom\n+\n+        response = self._requester.request(\n+            \"POST\",\n+            f\"courses/{self.id}/lti_resource_links\",\n+            _kwargs=combine_kwargs(**kwargs),\n+        )\n+        return LTIResourceLink(self._requester, response.json())\n+\n     def create_module(self, module, **kwargs):\n         \"\"\"\n         Create a new module.\n@@ -1645,6 +1679,49 @@ def get_licenses(self, **kwargs):\n             _kwargs=combine_kwargs(**kwargs),\n         )\n \n+    def get_lti_resource_link(self, lti_resource_link, **kwargs):\n+        \"\"\"\n+        Return details about the specified resource link.\n+\n+        :calls: `GET /api/v1/courses/:course_id/lti_resource_links/:id \\\n+        <https://canvas.instructure.com/doc/api/lti_resource_links.html#method.lti/resource_links.show>`_\n+\n+        :param lti_resource_link: The object or ID of the LTI resource link.\n+        :type lti_resource_link: :class:`canvasapi.lti_resource_link.LTIResourceLink` or int\n+\n+        :rtype: :class:`canvasapi.lti_resource_link.LTIResourceLink`\n+        \"\"\"\n+\n+        lti_resource_link_id = obj_or_id(\n+            lti_resource_link, \"lti_resource_link\", (LTIResourceLink,)\n+        )\n+\n+        response = self._requester.request(\n+            \"GET\",\n+            f\"courses/{self.id}/lti_resource_links/{lti_resource_link_id}\",\n+            _kwargs=combine_kwargs(**kwargs),\n+        )\n+        return LTIResourceLink(self._requester, response.json())\n+\n+    def get_lti_resource_links(self, **kwargs):\n+        \"\"\"\n+        Returns all LTI resource links for this course as a PaginatedList.\n+\n+        :calls: `GET /api/v1/courses/:course_id/lti_resource_links \\\n+        <https://canvas.instructure.com/doc/api/lti_resource_links.html#method.lti/resource_links.index>`_\n+\n+        :rtype: :class:`canvasapi.paginated_list.PaginatedList` of\n+            :class:`canvasapi.lti_resource_link.LTIResourceLink`\n+        \"\"\"\n+\n+        return PaginatedList(\n+            LTIResourceLink,\n+            self._requester,\n+            \"GET\",\n+            f\"courses/{self.id}/lti_resource_links\",\n+            kwargs=combine_kwargs(**kwargs),\n+        )\n+\n     def get_migration_systems(self, **kwargs):\n         \"\"\"\n         Return a list of migration systems.\ndiff --git a/canvasapi/lti_resource_link.py b/canvasapi/lti_resource_link.py\nnew file mode 100644\nindex 00000000..d951569e\n--- /dev/null\n+++ b/canvasapi/lti_resource_link.py\n@@ -0,0 +1,6 @@\n+from canvasapi.canvas_object import CanvasObject\n+\n+\n+class LTIResourceLink(CanvasObject):\n+    def __str__(self):\n+        return \"{} ({})\".format(self.url, self.title)\ndiff --git a/docs/class-reference.rst b/docs/class-reference.rst\nindex 7420abff..e73ade2c 100644\n--- a/docs/class-reference.rst\n+++ b/docs/class-reference.rst\n@@ -41,6 +41,7 @@ Class Reference\n     jwt-ref\n     login-ref\n     license-ref\n+    lti-resource-link-ref\n     module-ref\n     outcome-ref\n     outcome-import-ref\ndiff --git a/docs/lti-resource-link-ref.rst b/docs/lti-resource-link-ref.rst\nnew file mode 100644\nindex 00000000..b14ff90e\n--- /dev/null\n+++ b/docs/lti-resource-link-ref.rst\n@@ -0,0 +1,6 @@\n+===============\n+LTIResourceLink\n+===============\n+\n+.. autoclass:: canvasapi.lti_resource_link.LTIResourceLink\n+    :members:\n", "instance_id": "ucfopen__canvasapi-673", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in terms of the goal, which is to add API coverage for LTI resource links in a Canvas API client library. It specifies the resource (Courses LTI resource links) and lists the exact endpoints to be covered (GET and PUT methods for specific URLs). The reference to the official documentation is helpful for understanding the expected behavior of these endpoints. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly mention the need for a `create` endpoint, though the code changes include it. Additionally, there are no details about expected input/output formats beyond the linked documentation, nor are there mentions of specific edge cases, constraints, or error conditions to handle. While the intent is clear, a developer might need to infer some requirements or consult external resources for full clarity.", "difficulty_explanation": "The difficulty of this task falls in the easy range (0.2-0.4) due to the straightforward nature of the changes required. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modifications are localized to a few files, primarily `course.py` for adding new methods and `lti_resource_link.py` for defining a new class. The changes involve adding new API endpoint methods to an existing course class, which is a common pattern in API client libraries. The amount of code added is moderate (around 100 lines), and it does not impact the broader system architecture. The updates to documentation and changelog are trivial.\n\n2. **Number of Technical Concepts:** The task requires basic familiarity with Python and the structure of the Canvas API client library. Key concepts include making HTTP requests (using a `requester` object), handling pagination (via `PaginatedList`), and defining new classes for API resources. These are relatively simple concepts for a developer familiar with Python and API client libraries. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic API interactions are needed.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases or specific error conditions to handle. The code changes include minimal error handling (e.g., checking for a required `url` field in `create_lti_resource_link`), which suggests that the developer must infer additional error conditions from the documentation or existing codebase patterns. However, the complexity of edge cases appears low, as the API interactions are straightforward CRUD operations.\n\n4. **Overall Complexity:** The task involves understanding the existing codebase structure to add new methods in a consistent style, but it does not require deep architectural changes or complex logic. It is a typical feature addition to an API client, following established patterns in the codebase (e.g., similar methods for other resources).\n\nGiven these factors, a score of 0.35 reflects an easy task that requires some understanding of the codebase and API client patterns but does not pose significant technical challenges or require handling complex edge cases. A junior to mid-level developer with Python experience could likely complete this with minimal guidance, provided they have access to the referenced documentation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Re-Licensing under Apache v2\nUp until now, TeNPy is licensed under the [GPL v3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.\r\nIt is one of the major open source licenses, e.g. the Linux kernel is released with it, and it is the reason why all linux distros are open source. However, the license is quite strict in enforcing what's called \"copyleft\": anybody who want's to modify and extend the code has to publish his code as open source under the very same GPL license. This is a red flag for companies like IBM developing quiskit or quantum computing start-ups and hinders IBM to use TeNPy as a backend for qiskit.\r\n\r\n@bartandrews hence suggested re-license TeNPy under the [Apache v2](http://www.apache.org/licenses/LICENSE-2.0) license instead. Although I was the one originally choosing the GPLv3 license, I fully support the change to Apache v2. I put many hours into TeNpy in the hope that it will be useful for the scientific community, and blocking the further development of it due to licensing issues seems counter-productive.\r\nApache v2 is also a popular license with many other scientific python projects, including [quiskit](https://github.com/Qiskit/qiskit/blob/main/LICENSE.txt),  [tensorflow](https://github.com/tensorflow/tensorflow/blob/master/LICENSE) and the [cytnx](https://github.com/Cytnx-dev/Cytnx/blob/master/LICENSE) library that we're planning to collaborate with.\r\n\r\nTo compare the licenses, you might want to read the full texts linked above or check out the bullet point summaries on\r\nchoosealicense.com, [Apache v2](https://choosealicense.com/licenses/apache-2.0/) vs [GPL v3](https://choosealicense.com/licenses/gpl-3.0/). By changing to Apache v2, we're essentially dropping the copyleft, i.e. become more permissive towards future developers to choose their own license for their contributions. In theory, someone could even do that closed source (which the copyleft of GPL would avoid), but I think the risk for that is just a very negligible epsilon > 0.\r\n\r\nWith this issue, I'm hence asking the contributors of TeNPy for their consent or possible objections to this re-licensing. \r\nFor the legal documentation, it would be great if everyone of the current contributors (in particular the authors of our [TeNPy v1 paper](https://arxiv.org/abs/2408.02010)) could give a short comment like \"changing the license is fine with me\" - or raise objections and concerns, should there be any. Thank you!\r\n\n", "patch": "diff --git a/LICENSE b/LICENSE\nindex f288702d2..d64569567 100644\n--- a/LICENSE\n+++ b/LICENSE\n@@ -1,674 +1,202 @@\n-                    GNU GENERAL PUBLIC LICENSE\n-                       Version 3, 29 June 2007\n-\n- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n- Everyone is permitted to copy and distribute verbatim copies\n- of this license document, but changing it is not allowed.\n-\n-                            Preamble\n-\n-  The GNU General Public License is a free, copyleft license for\n-software and other kinds of works.\n-\n-  The licenses for most software and other practical works are designed\n-to take away your freedom to share and change the works.  By contrast,\n-the GNU General Public License is intended to guarantee your freedom to\n-share and change all versions of a program--to make sure it remains free\n-software for all its users.  We, the Free Software Foundation, use the\n-GNU General Public License for most of our software; it applies also to\n-any other work released this way by its authors.  You can apply it to\n-your programs, too.\n-\n-  When we speak of free software, we are referring to freedom, not\n-price.  Our General Public Licenses are designed to make sure that you\n-have the freedom to distribute copies of free software (and charge for\n-them if you wish), that you receive source code or can get it if you\n-want it, that you can change the software or use pieces of it in new\n-free programs, and that you know you can do these things.\n-\n-  To protect your rights, we need to prevent others from denying you\n-these rights or asking you to surrender the rights.  Therefore, you have\n-certain responsibilities if you distribute copies of the software, or if\n-you modify it: responsibilities to respect the freedom of others.\n-\n-  For example, if you distribute copies of such a program, whether\n-gratis or for a fee, you must pass on to the recipients the same\n-freedoms that you received.  You must make sure that they, too, receive\n-or can get the source code.  And you must show them these terms so they\n-know their rights.\n-\n-  Developers that use the GNU GPL protect your rights with two steps:\n-(1) assert copyright on the software, and (2) offer you this License\n-giving you legal permission to copy, distribute and/or modify it.\n-\n-  For the developers' and authors' protection, the GPL clearly explains\n-that there is no warranty for this free software.  For both users' and\n-authors' sake, the GPL requires that modified versions be marked as\n-changed, so that their problems will not be attributed erroneously to\n-authors of previous versions.\n-\n-  Some devices are designed to deny users access to install or run\n-modified versions of the software inside them, although the manufacturer\n-can do so.  This is fundamentally incompatible with the aim of\n-protecting users' freedom to change the software.  The systematic\n-pattern of such abuse occurs in the area of products for individuals to\n-use, which is precisely where it is most unacceptable.  Therefore, we\n-have designed this version of the GPL to prohibit the practice for those\n-products.  If such problems arise substantially in other domains, we\n-stand ready to extend this provision to those domains in future versions\n-of the GPL, as needed to protect the freedom of users.\n-\n-  Finally, every program is threatened constantly by software patents.\n-States should not allow patents to restrict development and use of\n-software on general-purpose computers, but in those that do, we wish to\n-avoid the special danger that patents applied to a free program could\n-make it effectively proprietary.  To prevent this, the GPL assures that\n-patents cannot be used to render the program non-free.\n-\n-  The precise terms and conditions for copying, distribution and\n-modification follow.\n-\n-                       TERMS AND CONDITIONS\n-\n-  0. Definitions.\n-\n-  \"This License\" refers to version 3 of the GNU General Public License.\n-\n-  \"Copyright\" also means copyright-like laws that apply to other kinds of\n-works, such as semiconductor masks.\n-\n-  \"The Program\" refers to any copyrightable work licensed under this\n-License.  Each licensee is addressed as \"you\".  \"Licensees\" and\n-\"recipients\" may be individuals or organizations.\n-\n-  To \"modify\" a work means to copy from or adapt all or part of the work\n-in a fashion requiring copyright permission, other than the making of an\n-exact copy.  The resulting work is called a \"modified version\" of the\n-earlier work or a work \"based on\" the earlier work.\n-\n-  A \"covered work\" means either the unmodified Program or a work based\n-on the Program.\n-\n-  To \"propagate\" a work means to do anything with it that, without\n-permission, would make you directly or secondarily liable for\n-infringement under applicable copyright law, except executing it on a\n-computer or modifying a private copy.  Propagation includes copying,\n-distribution (with or without modification), making available to the\n-public, and in some countries other activities as well.\n-\n-  To \"convey\" a work means any kind of propagation that enables other\n-parties to make or receive copies.  Mere interaction with a user through\n-a computer network, with no transfer of a copy, is not conveying.\n-\n-  An interactive user interface displays \"Appropriate Legal Notices\"\n-to the extent that it includes a convenient and prominently visible\n-feature that (1) displays an appropriate copyright notice, and (2)\n-tells the user that there is no warranty for the work (except to the\n-extent that warranties are provided), that licensees may convey the\n-work under this License, and how to view a copy of this License.  If\n-the interface presents a list of user commands or options, such as a\n-menu, a prominent item in the list meets this criterion.\n-\n-  1. Source Code.\n-\n-  The \"source code\" for a work means the preferred form of the work\n-for making modifications to it.  \"Object code\" means any non-source\n-form of a work.\n-\n-  A \"Standard Interface\" means an interface that either is an official\n-standard defined by a recognized standards body, or, in the case of\n-interfaces specified for a particular programming language, one that\n-is widely used among developers working in that language.\n-\n-  The \"System Libraries\" of an executable work include anything, other\n-than the work as a whole, that (a) is included in the normal form of\n-packaging a Major Component, but which is not part of that Major\n-Component, and (b) serves only to enable use of the work with that\n-Major Component, or to implement a Standard Interface for which an\n-implementation is available to the public in source code form.  A\n-\"Major Component\", in this context, means a major essential component\n-(kernel, window system, and so on) of the specific operating system\n-(if any) on which the executable work runs, or a compiler used to\n-produce the work, or an object code interpreter used to run it.\n-\n-  The \"Corresponding Source\" for a work in object code form means all\n-the source code needed to generate, install, and (for an executable\n-work) run the object code and to modify the work, including scripts to\n-control those activities.  However, it does not include the work's\n-System Libraries, or general-purpose tools or generally available free\n-programs which are used unmodified in performing those activities but\n-which are not part of the work.  For example, Corresponding Source\n-includes interface definition files associated with source files for\n-the work, and the source code for shared libraries and dynamically\n-linked subprograms that the work is specifically designed to require,\n-such as by intimate data communication or control flow between those\n-subprograms and other parts of the work.\n-\n-  The Corresponding Source need not include anything that users\n-can regenerate automatically from other parts of the Corresponding\n-Source.\n-\n-  The Corresponding Source for a work in source code form is that\n-same work.\n-\n-  2. Basic Permissions.\n-\n-  All rights granted under this License are granted for the term of\n-copyright on the Program, and are irrevocable provided the stated\n-conditions are met.  This License explicitly affirms your unlimited\n-permission to run the unmodified Program.  The output from running a\n-covered work is covered by this License only if the output, given its\n-content, constitutes a covered work.  This License acknowledges your\n-rights of fair use or other equivalent, as provided by copyright law.\n-\n-  You may make, run and propagate covered works that you do not\n-convey, without conditions so long as your license otherwise remains\n-in force.  You may convey covered works to others for the sole purpose\n-of having them make modifications exclusively for you, or provide you\n-with facilities for running those works, provided that you comply with\n-the terms of this License in conveying all material for which you do\n-not control copyright.  Those thus making or running the covered works\n-for you must do so exclusively on your behalf, under your direction\n-and control, on terms that prohibit them from making any copies of\n-your copyrighted material outside their relationship with you.\n-\n-  Conveying under any other circumstances is permitted solely under\n-the conditions stated below.  Sublicensing is not allowed; section 10\n-makes it unnecessary.\n-\n-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n-\n-  No covered work shall be deemed part of an effective technological\n-measure under any applicable law fulfilling obligations under article\n-11 of the WIPO copyright treaty adopted on 20 December 1996, or\n-similar laws prohibiting or restricting circumvention of such\n-measures.\n-\n-  When you convey a covered work, you waive any legal power to forbid\n-circumvention of technological measures to the extent such circumvention\n-is effected by exercising rights under this License with respect to\n-the covered work, and you disclaim any intention to limit operation or\n-modification of the work as a means of enforcing, against the work's\n-users, your or third parties' legal rights to forbid circumvention of\n-technological measures.\n-\n-  4. Conveying Verbatim Copies.\n-\n-  You may convey verbatim copies of the Program's source code as you\n-receive it, in any medium, provided that you conspicuously and\n-appropriately publish on each copy an appropriate copyright notice;\n-keep intact all notices stating that this License and any\n-non-permissive terms added in accord with section 7 apply to the code;\n-keep intact all notices of the absence of any warranty; and give all\n-recipients a copy of this License along with the Program.\n-\n-  You may charge any price or no price for each copy that you convey,\n-and you may offer support or warranty protection for a fee.\n-\n-  5. Conveying Modified Source Versions.\n-\n-  You may convey a work based on the Program, or the modifications to\n-produce it from the Program, in the form of source code under the\n-terms of section 4, provided that you also meet all of these conditions:\n-\n-    a) The work must carry prominent notices stating that you modified\n-    it, and giving a relevant date.\n-\n-    b) The work must carry prominent notices stating that it is\n-    released under this License and any conditions added under section\n-    7.  This requirement modifies the requirement in section 4 to\n-    \"keep intact all notices\".\n-\n-    c) You must license the entire work, as a whole, under this\n-    License to anyone who comes into possession of a copy.  This\n-    License will therefore apply, along with any applicable section 7\n-    additional terms, to the whole of the work, and all its parts,\n-    regardless of how they are packaged.  This License gives no\n-    permission to license the work in any other way, but it does not\n-    invalidate such permission if you have separately received it.\n-\n-    d) If the work has interactive user interfaces, each must display\n-    Appropriate Legal Notices; however, if the Program has interactive\n-    interfaces that do not display Appropriate Legal Notices, your\n-    work need not make them do so.\n-\n-  A compilation of a covered work with other separate and independent\n-works, which are not by their nature extensions of the covered work,\n-and which are not combined with it such as to form a larger program,\n-in or on a volume of a storage or distribution medium, is called an\n-\"aggregate\" if the compilation and its resulting copyright are not\n-used to limit the access or legal rights of the compilation's users\n-beyond what the individual works permit.  Inclusion of a covered work\n-in an aggregate does not cause this License to apply to the other\n-parts of the aggregate.\n-\n-  6. Conveying Non-Source Forms.\n-\n-  You may convey a covered work in object code form under the terms\n-of sections 4 and 5, provided that you also convey the\n-machine-readable Corresponding Source under the terms of this License,\n-in one of these ways:\n-\n-    a) Convey the object code in, or embodied in, a physical product\n-    (including a physical distribution medium), accompanied by the\n-    Corresponding Source fixed on a durable physical medium\n-    customarily used for software interchange.\n-\n-    b) Convey the object code in, or embodied in, a physical product\n-    (including a physical distribution medium), accompanied by a\n-    written offer, valid for at least three years and valid for as\n-    long as you offer spare parts or customer support for that product\n-    model, to give anyone who possesses the object code either (1) a\n-    copy of the Corresponding Source for all the software in the\n-    product that is covered by this License, on a durable physical\n-    medium customarily used for software interchange, for a price no\n-    more than your reasonable cost of physically performing this\n-    conveying of source, or (2) access to copy the\n-    Corresponding Source from a network server at no charge.\n-\n-    c) Convey individual copies of the object code with a copy of the\n-    written offer to provide the Corresponding Source.  This\n-    alternative is allowed only occasionally and noncommercially, and\n-    only if you received the object code with such an offer, in accord\n-    with subsection 6b.\n-\n-    d) Convey the object code by offering access from a designated\n-    place (gratis or for a charge), and offer equivalent access to the\n-    Corresponding Source in the same way through the same place at no\n-    further charge.  You need not require recipients to copy the\n-    Corresponding Source along with the object code.  If the place to\n-    copy the object code is a network server, the Corresponding Source\n-    may be on a different server (operated by you or a third party)\n-    that supports equivalent copying facilities, provided you maintain\n-    clear directions next to the object code saying where to find the\n-    Corresponding Source.  Regardless of what server hosts the\n-    Corresponding Source, you remain obligated to ensure that it is\n-    available for as long as needed to satisfy these requirements.\n-\n-    e) Convey the object code using peer-to-peer transmission, provided\n-    you inform other peers where the object code and Corresponding\n-    Source of the work are being offered to the general public at no\n-    charge under subsection 6d.\n-\n-  A separable portion of the object code, whose source code is excluded\n-from the Corresponding Source as a System Library, need not be\n-included in conveying the object code work.\n-\n-  A \"User Product\" is either (1) a \"consumer product\", which means any\n-tangible personal property which is normally used for personal, family,\n-or household purposes, or (2) anything designed or sold for incorporation\n-into a dwelling.  In determining whether a product is a consumer product,\n-doubtful cases shall be resolved in favor of coverage.  For a particular\n-product received by a particular user, \"normally used\" refers to a\n-typical or common use of that class of product, regardless of the status\n-of the particular user or of the way in which the particular user\n-actually uses, or expects or is expected to use, the product.  A product\n-is a consumer product regardless of whether the product has substantial\n-commercial, industrial or non-consumer uses, unless such uses represent\n-the only significant mode of use of the product.\n-\n-  \"Installation Information\" for a User Product means any methods,\n-procedures, authorization keys, or other information required to install\n-and execute modified versions of a covered work in that User Product from\n-a modified version of its Corresponding Source.  The information must\n-suffice to ensure that the continued functioning of the modified object\n-code is in no case prevented or interfered with solely because\n-modification has been made.\n-\n-  If you convey an object code work under this section in, or with, or\n-specifically for use in, a User Product, and the conveying occurs as\n-part of a transaction in which the right of possession and use of the\n-User Product is transferred to the recipient in perpetuity or for a\n-fixed term (regardless of how the transaction is characterized), the\n-Corresponding Source conveyed under this section must be accompanied\n-by the Installation Information.  But this requirement does not apply\n-if neither you nor any third party retains the ability to install\n-modified object code on the User Product (for example, the work has\n-been installed in ROM).\n-\n-  The requirement to provide Installation Information does not include a\n-requirement to continue to provide support service, warranty, or updates\n-for a work that has been modified or installed by the recipient, or for\n-the User Product in which it has been modified or installed.  Access to a\n-network may be denied when the modification itself materially and\n-adversely affects the operation of the network or violates the rules and\n-protocols for communication across the network.\n-\n-  Corresponding Source conveyed, and Installation Information provided,\n-in accord with this section must be in a format that is publicly\n-documented (and with an implementation available to the public in\n-source code form), and must require no special password or key for\n-unpacking, reading or copying.\n-\n-  7. Additional Terms.\n-\n-  \"Additional permissions\" are terms that supplement the terms of this\n-License by making exceptions from one or more of its conditions.\n-Additional permissions that are applicable to the entire Program shall\n-be treated as though they were included in this License, to the extent\n-that they are valid under applicable law.  If additional permissions\n-apply only to part of the Program, that part may be used separately\n-under those permissions, but the entire Program remains governed by\n-this License without regard to the additional permissions.\n-\n-  When you convey a copy of a covered work, you may at your option\n-remove any additional permissions from that copy, or from any part of\n-it.  (Additional permissions may be written to require their own\n-removal in certain cases when you modify the work.)  You may place\n-additional permissions on material, added by you to a covered work,\n-for which you have or can give appropriate copyright permission.\n-\n-  Notwithstanding any other provision of this License, for material you\n-add to a covered work, you may (if authorized by the copyright holders of\n-that material) supplement the terms of this License with terms:\n-\n-    a) Disclaiming warranty or limiting liability differently from the\n-    terms of sections 15 and 16 of this License; or\n-\n-    b) Requiring preservation of specified reasonable legal notices or\n-    author attributions in that material or in the Appropriate Legal\n-    Notices displayed by works containing it; or\n-\n-    c) Prohibiting misrepresentation of the origin of that material, or\n-    requiring that modified versions of such material be marked in\n-    reasonable ways as different from the original version; or\n-\n-    d) Limiting the use for publicity purposes of names of licensors or\n-    authors of the material; or\n-\n-    e) Declining to grant rights under trademark law for use of some\n-    trade names, trademarks, or service marks; or\n-\n-    f) Requiring indemnification of licensors and authors of that\n-    material by anyone who conveys the material (or modified versions of\n-    it) with contractual assumptions of liability to the recipient, for\n-    any liability that these contractual assumptions directly impose on\n-    those licensors and authors.\n-\n-  All other non-permissive additional terms are considered \"further\n-restrictions\" within the meaning of section 10.  If the Program as you\n-received it, or any part of it, contains a notice stating that it is\n-governed by this License along with a term that is a further\n-restriction, you may remove that term.  If a license document contains\n-a further restriction but permits relicensing or conveying under this\n-License, you may add to a covered work material governed by the terms\n-of that license document, provided that the further restriction does\n-not survive such relicensing or conveying.\n-\n-  If you add terms to a covered work in accord with this section, you\n-must place, in the relevant source files, a statement of the\n-additional terms that apply to those files, or a notice indicating\n-where to find the applicable terms.\n-\n-  Additional terms, permissive or non-permissive, may be stated in the\n-form of a separately written license, or stated as exceptions;\n-the above requirements apply either way.\n-\n-  8. Termination.\n-\n-  You may not propagate or modify a covered work except as expressly\n-provided under this License.  Any attempt otherwise to propagate or\n-modify it is void, and will automatically terminate your rights under\n-this License (including any patent licenses granted under the third\n-paragraph of section 11).\n-\n-  However, if you cease all violation of this License, then your\n-license from a particular copyright holder is reinstated (a)\n-provisionally, unless and until the copyright holder explicitly and\n-finally terminates your license, and (b) permanently, if the copyright\n-holder fails to notify you of the violation by some reasonable means\n-prior to 60 days after the cessation.\n-\n-  Moreover, your license from a particular copyright holder is\n-reinstated permanently if the copyright holder notifies you of the\n-violation by some reasonable means, this is the first time you have\n-received notice of violation of this License (for any work) from that\n-copyright holder, and you cure the violation prior to 30 days after\n-your receipt of the notice.\n-\n-  Termination of your rights under this section does not terminate the\n-licenses of parties who have received copies or rights from you under\n-this License.  If your rights have been terminated and not permanently\n-reinstated, you do not qualify to receive new licenses for the same\n-material under section 10.\n-\n-  9. Acceptance Not Required for Having Copies.\n-\n-  You are not required to accept this License in order to receive or\n-run a copy of the Program.  Ancillary propagation of a covered work\n-occurring solely as a consequence of using peer-to-peer transmission\n-to receive a copy likewise does not require acceptance.  However,\n-nothing other than this License grants you permission to propagate or\n-modify any covered work.  These actions infringe copyright if you do\n-not accept this License.  Therefore, by modifying or propagating a\n-covered work, you indicate your acceptance of this License to do so.\n-\n-  10. Automatic Licensing of Downstream Recipients.\n-\n-  Each time you convey a covered work, the recipient automatically\n-receives a license from the original licensors, to run, modify and\n-propagate that work, subject to this License.  You are not responsible\n-for enforcing compliance by third parties with this License.\n-\n-  An \"entity transaction\" is a transaction transferring control of an\n-organization, or substantially all assets of one, or subdividing an\n-organization, or merging organizations.  If propagation of a covered\n-work results from an entity transaction, each party to that\n-transaction who receives a copy of the work also receives whatever\n-licenses to the work the party's predecessor in interest had or could\n-give under the previous paragraph, plus a right to possession of the\n-Corresponding Source of the work from the predecessor in interest, if\n-the predecessor has it or can get it with reasonable efforts.\n-\n-  You may not impose any further restrictions on the exercise of the\n-rights granted or affirmed under this License.  For example, you may\n-not impose a license fee, royalty, or other charge for exercise of\n-rights granted under this License, and you may not initiate litigation\n-(including a cross-claim or counterclaim in a lawsuit) alleging that\n-any patent claim is infringed by making, using, selling, offering for\n-sale, or importing the Program or any portion of it.\n-\n-  11. Patents.\n-\n-  A \"contributor\" is a copyright holder who authorizes use under this\n-License of the Program or a work on which the Program is based.  The\n-work thus licensed is called the contributor's \"contributor version\".\n-\n-  A contributor's \"essential patent claims\" are all patent claims\n-owned or controlled by the contributor, whether already acquired or\n-hereafter acquired, that would be infringed by some manner, permitted\n-by this License, of making, using, or selling its contributor version,\n-but do not include claims that would be infringed only as a\n-consequence of further modification of the contributor version.  For\n-purposes of this definition, \"control\" includes the right to grant\n-patent sublicenses in a manner consistent with the requirements of\n-this License.\n-\n-  Each contributor grants you a non-exclusive, worldwide, royalty-free\n-patent license under the contributor's essential patent claims, to\n-make, use, sell, offer for sale, import and otherwise run, modify and\n-propagate the contents of its contributor version.\n-\n-  In the following three paragraphs, a \"patent license\" is any express\n-agreement or commitment, however denominated, not to enforce a patent\n-(such as an express permission to practice a patent or covenant not to\n-sue for patent infringement).  To \"grant\" such a patent license to a\n-party means to make such an agreement or commitment not to enforce a\n-patent against the party.\n-\n-  If you convey a covered work, knowingly relying on a patent license,\n-and the Corresponding Source of the work is not available for anyone\n-to copy, free of charge and under the terms of this License, through a\n-publicly available network server or other readily accessible means,\n-then you must either (1) cause the Corresponding Source to be so\n-available, or (2) arrange to deprive yourself of the benefit of the\n-patent license for this particular work, or (3) arrange, in a manner\n-consistent with the requirements of this License, to extend the patent\n-license to downstream recipients.  \"Knowingly relying\" means you have\n-actual knowledge that, but for the patent license, your conveying the\n-covered work in a country, or your recipient's use of the covered work\n-in a country, would infringe one or more identifiable patents in that\n-country that you have reason to believe are valid.\n-\n-  If, pursuant to or in connection with a single transaction or\n-arrangement, you convey, or propagate by procuring conveyance of, a\n-covered work, and grant a patent license to some of the parties\n-receiving the covered work authorizing them to use, propagate, modify\n-or convey a specific copy of the covered work, then the patent license\n-you grant is automatically extended to all recipients of the covered\n-work and works based on it.\n-\n-  A patent license is \"discriminatory\" if it does not include within\n-the scope of its coverage, prohibits the exercise of, or is\n-conditioned on the non-exercise of one or more of the rights that are\n-specifically granted under this License.  You may not convey a covered\n-work if you are a party to an arrangement with a third party that is\n-in the business of distributing software, under which you make payment\n-to the third party based on the extent of your activity of conveying\n-the work, and under which the third party grants, to any of the\n-parties who would receive the covered work from you, a discriminatory\n-patent license (a) in connection with copies of the covered work\n-conveyed by you (or copies made from those copies), or (b) primarily\n-for and in connection with specific products or compilations that\n-contain the covered work, unless you entered into that arrangement,\n-or that patent license was granted, prior to 28 March 2007.\n-\n-  Nothing in this License shall be construed as excluding or limiting\n-any implied license or other defenses to infringement that may\n-otherwise be available to you under applicable patent law.\n-\n-  12. No Surrender of Others' Freedom.\n-\n-  If conditions are imposed on you (whether by court order, agreement or\n-otherwise) that contradict the conditions of this License, they do not\n-excuse you from the conditions of this License.  If you cannot convey a\n-covered work so as to satisfy simultaneously your obligations under this\n-License and any other pertinent obligations, then as a consequence you may\n-not convey it at all.  For example, if you agree to terms that obligate you\n-to collect a royalty for further conveying from those to whom you convey\n-the Program, the only way you could satisfy both those terms and this\n-License would be to refrain entirely from conveying the Program.\n-\n-  13. Use with the GNU Affero General Public License.\n-\n-  Notwithstanding any other provision of this License, you have\n-permission to link or combine any covered work with a work licensed\n-under version 3 of the GNU Affero General Public License into a single\n-combined work, and to convey the resulting work.  The terms of this\n-License will continue to apply to the part which is the covered work,\n-but the special requirements of the GNU Affero General Public License,\n-section 13, concerning interaction through a network will apply to the\n-combination as such.\n-\n-  14. Revised Versions of this License.\n-\n-  The Free Software Foundation may publish revised and/or new versions of\n-the GNU General Public License from time to time.  Such new versions will\n-be similar in spirit to the present version, but may differ in detail to\n-address new problems or concerns.\n-\n-  Each version is given a distinguishing version number.  If the\n-Program specifies that a certain numbered version of the GNU General\n-Public License \"or any later version\" applies to it, you have the\n-option of following the terms and conditions either of that numbered\n-version or of any later version published by the Free Software\n-Foundation.  If the Program does not specify a version number of the\n-GNU General Public License, you may choose any version ever published\n-by the Free Software Foundation.\n-\n-  If the Program specifies that a proxy can decide which future\n-versions of the GNU General Public License can be used, that proxy's\n-public statement of acceptance of a version permanently authorizes you\n-to choose that version for the Program.\n-\n-  Later license versions may give you additional or different\n-permissions.  However, no additional obligations are imposed on any\n-author or copyright holder as a result of your choosing to follow a\n-later version.\n-\n-  15. Disclaimer of Warranty.\n-\n-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\n-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\n-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\n-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\n-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\n-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\n-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n-\n-  16. Limitation of Liability.\n-\n-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\n-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\n-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\n-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\n-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\n-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\n-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\n-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\n-SUCH DAMAGES.\n-\n-  17. Interpretation of Sections 15 and 16.\n-\n-  If the disclaimer of warranty and limitation of liability provided\n-above cannot be given local legal effect according to their terms,\n-reviewing courts shall apply local law that most closely approximates\n-an absolute waiver of all civil liability in connection with the\n-Program, unless a warranty or assumption of liability accompanies a\n-copy of the Program in return for a fee.\n-\n-                     END OF TERMS AND CONDITIONS\n-\n-            How to Apply These Terms to Your New Programs\n-\n-  If you develop a new program, and you want it to be of the greatest\n-possible use to the public, the best way to achieve this is to make it\n-free software which everyone can redistribute and change under these terms.\n-\n-  To do so, attach the following notices to the program.  It is safest\n-to attach them to the start of each source file to most effectively\n-state the exclusion of warranty; and each file should have at least\n-the \"copyright\" line and a pointer to where the full notice is found.\n-\n-    <one line to give the program's name and a brief idea of what it does.>\n-    Copyright (C) <year>  <name of author>\n-\n-    This program is free software: you can redistribute it and/or modify\n-    it under the terms of the GNU General Public License as published by\n-    the Free Software Foundation, either version 3 of the License, or\n-    (at your option) any later version.\n-\n-    This program is distributed in the hope that it will be useful,\n-    but WITHOUT ANY WARRANTY; without even the implied warranty of\n-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-    GNU General Public License for more details.\n-\n-    You should have received a copy of the GNU General Public License\n-    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n-\n-Also add information on how to contact you by electronic and paper mail.\n-\n-  If the program does terminal interaction, make it output a short\n-notice like this when it starts in an interactive mode:\n-\n-    <program>  Copyright (C) <year>  <name of author>\n-    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n-    This is free software, and you are welcome to redistribute it\n-    under certain conditions; type `show c' for details.\n-\n-The hypothetical commands `show w' and `show c' should show the appropriate\n-parts of the General Public License.  Of course, your program's commands\n-might be different; for a GUI interface, you would use an \"about box\".\n-\n-  You should also get your employer (if you work as a programmer) or school,\n-if any, to sign a \"copyright disclaimer\" for the program, if necessary.\n-For more information on this, and how to apply and follow the GNU GPL, see\n-<https://www.gnu.org/licenses/>.\n-\n-  The GNU General Public License does not permit incorporating your program\n-into proprietary programs.  If your program is a subroutine library, you\n-may consider it more useful to permit linking proprietary applications with\n-the library.  If this is what you want to do, use the GNU Lesser General\n-Public License instead of this License.  But first, please read\n-<https://www.gnu.org/licenses/why-not-lgpl.html>.\n+\n+                                 Apache License\n+                           Version 2.0, January 2004\n+                        http://www.apache.org/licenses/\n+\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+   1. Definitions.\n+\n+      \"License\" shall mean the terms and conditions for use, reproduction,\n+      and distribution as defined by Sections 1 through 9 of this document.\n+\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\n+      the copyright owner that is granting the License.\n+\n+      \"Legal Entity\" shall mean the union of the acting entity and all\n+      other entities that control, are controlled by, or are under common\n+      control with that entity. For the purposes of this definition,\n+      \"control\" means (i) the power, direct or indirect, to cause the\n+      direction or management of such entity, whether by contract or\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+      outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+      exercising permissions granted by this License.\n+\n+      \"Source\" form shall mean the preferred form for making modifications,\n+      including but not limited to software source code, documentation\n+      source, and configuration files.\n+\n+      \"Object\" form shall mean any form resulting from mechanical\n+      transformation or translation of a Source form, including but\n+      not limited to compiled object code, generated documentation,\n+      and conversions to other media types.\n+\n+      \"Work\" shall mean the work of authorship, whether in Source or\n+      Object form, made available under the License, as indicated by a\n+      copyright notice that is included in or attached to the work\n+      (an example is provided in the Appendix below).\n+\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\n+      form, that is based on (or derived from) the Work and for which the\n+      editorial revisions, annotations, elaborations, or other modifications\n+      represent, as a whole, an original work of authorship. For the purposes\n+      of this License, Derivative Works shall not include works that remain\n+      separable from, or merely link (or bind by name) to the interfaces of,\n+      the Work and Derivative Works thereof.\n+\n+      \"Contribution\" shall mean any work of authorship, including\n+      the original version of the Work and any modifications or additions\n+      to that Work or Derivative Works thereof, that is intentionally\n+      submitted to Licensor for inclusion in the Work by the copyright owner\n+      or by an individual or Legal Entity authorized to submit on behalf of\n+      the copyright owner. For the purposes of this definition, \"submitted\"\n+      means any form of electronic, verbal, or written communication sent\n+      to the Licensor or its representatives, including but not limited to\n+      communication on electronic mailing lists, source code control systems,\n+      and issue tracking systems that are managed by, or on behalf of, the\n+      Licensor for the purpose of discussing and improving the Work, but\n+      excluding communication that is conspicuously marked or otherwise\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+      on behalf of whom a Contribution has been received by Licensor and\n+      subsequently incorporated within the Work.\n+\n+   2. Grant of Copyright License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      copyright license to reproduce, prepare Derivative Works of,\n+      publicly display, publicly perform, sublicense, and distribute the\n+      Work and such Derivative Works in Source or Object form.\n+\n+   3. Grant of Patent License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      (except as stated in this section) patent license to make, have made,\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\n+      where such license applies only to those patent claims licensable\n+      by such Contributor that are necessarily infringed by their\n+      Contribution(s) alone or by combination of their Contribution(s)\n+      with the Work to which such Contribution(s) was submitted. If You\n+      institute patent litigation against any entity (including a\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n+      or a Contribution incorporated within the Work constitutes direct\n+      or contributory patent infringement, then any patent licenses\n+      granted to You under this License for that Work shall terminate\n+      as of the date such litigation is filed.\n+\n+   4. Redistribution. You may reproduce and distribute copies of the\n+      Work or Derivative Works thereof in any medium, with or without\n+      modifications, and in Source or Object form, provided that You\n+      meet the following conditions:\n+\n+      (a) You must give any other recipients of the Work or\n+          Derivative Works a copy of this License; and\n+\n+      (b) You must cause any modified files to carry prominent notices\n+          stating that You changed the files; and\n+\n+      (c) You must retain, in the Source form of any Derivative Works\n+          that You distribute, all copyright, patent, trademark, and\n+          attribution notices from the Source form of the Work,\n+          excluding those notices that do not pertain to any part of\n+          the Derivative Works; and\n+\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\n+          distribution, then any Derivative Works that You distribute must\n+          include a readable copy of the attribution notices contained\n+          within such NOTICE file, excluding those notices that do not\n+          pertain to any part of the Derivative Works, in at least one\n+          of the following places: within a NOTICE text file distributed\n+          as part of the Derivative Works; within the Source form or\n+          documentation, if provided along with the Derivative Works; or,\n+          within a display generated by the Derivative Works, if and\n+          wherever such third-party notices normally appear. The contents\n+          of the NOTICE file are for informational purposes only and\n+          do not modify the License. You may add Your own attribution\n+          notices within Derivative Works that You distribute, alongside\n+          or as an addendum to the NOTICE text from the Work, provided\n+          that such additional attribution notices cannot be construed\n+          as modifying the License.\n+\n+      You may add Your own copyright statement to Your modifications and\n+      may provide additional or different license terms and conditions\n+      for use, reproduction, or distribution of Your modifications, or\n+      for any such Derivative Works as a whole, provided Your use,\n+      reproduction, and distribution of the Work otherwise complies with\n+      the conditions stated in this License.\n+\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\n+      any Contribution intentionally submitted for inclusion in the Work\n+      by You to the Licensor shall be under the terms and conditions of\n+      this License, without any additional terms or conditions.\n+      Notwithstanding the above, nothing herein shall supersede or modify\n+      the terms of any separate license agreement you may have executed\n+      with Licensor regarding such Contributions.\n+\n+   6. Trademarks. This License does not grant permission to use the trade\n+      names, trademarks, service marks, or product names of the Licensor,\n+      except as required for reasonable and customary use in describing the\n+      origin of the Work and reproducing the content of the NOTICE file.\n+\n+   7. Disclaimer of Warranty. Unless required by applicable law or\n+      agreed to in writing, Licensor provides the Work (and each\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+      implied, including, without limitation, any warranties or conditions\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\n+      appropriateness of using or redistributing the Work and assume any\n+      risks associated with Your exercise of permissions under this License.\n+\n+   8. Limitation of Liability. In no event and under no legal theory,\n+      whether in tort (including negligence), contract, or otherwise,\n+      unless required by applicable law (such as deliberate and grossly\n+      negligent acts) or agreed to in writing, shall any Contributor be\n+      liable to You for damages, including any direct, indirect, special,\n+      incidental, or consequential damages of any character arising as a\n+      result of this License or out of the use or inability to use the\n+      Work (including but not limited to damages for loss of goodwill,\n+      work stoppage, computer failure or malfunction, or any and all\n+      other commercial damages or losses), even if such Contributor\n+      has been advised of the possibility of such damages.\n+\n+   9. Accepting Warranty or Additional Liability. While redistributing\n+      the Work or Derivative Works thereof, You may choose to offer,\n+      and charge a fee for, acceptance of support, warranty, indemnity,\n+      or other liability obligations and/or rights consistent with this\n+      License. However, in accepting such obligations, You may act only\n+      on Your own behalf and on Your sole responsibility, not on behalf\n+      of any other Contributor, and only if You agree to indemnify,\n+      defend, and hold each Contributor harmless for any liability\n+      incurred by, or claims asserted against, such Contributor by reason\n+      of your accepting any such warranty or additional liability.\n+\n+   END OF TERMS AND CONDITIONS\n+\n+   APPENDIX: How to apply the Apache License to your work.\n+\n+      To apply the Apache License to your work, attach the following\n+      boilerplate notice, with the fields enclosed by brackets \"[]\"\n+      replaced with your own identifying information. (Don't include\n+      the brackets!)  The text should be enclosed in the appropriate\n+      comment syntax for the file format. We also recommend that a\n+      file or class name and description of purpose be included on the\n+      same \"printed page\" as the copyright notice for easier\n+      identification within third-party archives.\n+\n+   Copyright [yyyy] [name of copyright owner]\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\ndiff --git a/README.rst b/README.rst\nindex 77a3ddeaf..9babee5ba 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -97,5 +97,5 @@ This work was funded by the U.S. Department of Energy, Office of Science, Office\n \n License\n -------\n-The code is licensed under GPL-v3.0 given in the file ``LICENSE`` of the repository, \n+Sinc TeNPy version 1.0.4, the code is licensed under Apache v2 given in the file ``LICENSE`` of the repository,\n in the online documentation readable at https://tenpy.readthedocs.io/en/latest/install/license.html.\ndiff --git a/doc/conf.py b/doc/conf.py\nindex f4d075a42..53ad6bfb2 100644\n--- a/doc/conf.py\n+++ b/doc/conf.py\n@@ -1,6 +1,6 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n #\n import sys\n import os\ndiff --git a/doc/install/license.rst b/doc/install/license.rst\nindex 8d5d3073d..709cd2a9e 100644\n--- a/doc/install/license.rst\n+++ b/doc/install/license.rst\n@@ -7,7 +7,8 @@ TeNPy developer team\n License\n =======\n \n-The source code documented here is published under a GPL v3 license, which we include below.\n+The source code documented here is published under the Apache v2 license, which we include below.\n+Up to version 1.0.3, TeNPy was distributed under GPL v3; see :issue:`462` for the discussion around the license change.\n \n .. include:: /../LICENSE\n     :literal:\ndiff --git a/doc/notebooks b/doc/notebooks\nindex 39300beb9..b6bd1ebfc 160000\n--- a/doc/notebooks\n+++ b/doc/notebooks\n@@ -1,1 +1,1 @@\n-Subproject commit 39300beb98484bcb38088bfee39ea09d560b9838\n+Subproject commit b6bd1ebfc6dda73ed0013ffd96a6c4eb3425d228\ndiff --git a/doc/toycodes b/doc/toycodes\nindex cb9511cbd..cc3937ff1 160000\n--- a/doc/toycodes\n+++ b/doc/toycodes\n@@ -1,1 +1,1 @@\n-Subproject commit cb9511cbd08211d010398453fe96713ce1494040\n+Subproject commit cc3937ff1a1b05dd19971ebad38a90d69853aa96\ndiff --git a/examples/a_np_conserved.py b/examples/a_np_conserved.py\nindex 59bad6c87..4676aba5a 100644\n--- a/examples/a_np_conserved.py\n+++ b/examples/a_np_conserved.py\n@@ -13,7 +13,7 @@\n Compare it to the example `b_mps.py`,\n which does the same steps using a few predefined classes like MPS and MPO.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import tenpy.linalg.np_conserved as npc\n import numpy as np\ndiff --git a/examples/advanced/central_charge_ising.py b/examples/advanced/central_charge_ising.py\nindex bb7677e44..f7bda9db5 100644\n--- a/examples/advanced/central_charge_ising.py\n+++ b/examples/advanced/central_charge_ising.py\n@@ -6,7 +6,7 @@\n \n For the theoretical background why :math:`S = c/6 log(xi)`, see :cite:`pollmann2009`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import tenpy\ndiff --git a/examples/advanced/mpo_exponential_decay.py b/examples/advanced/mpo_exponential_decay.py\nindex 474250771..9271c1c00 100644\n--- a/examples/advanced/mpo_exponential_decay.py\n+++ b/examples/advanced/mpo_exponential_decay.py\n@@ -10,7 +10,7 @@\n We run the iDMRG algorithm to find the ground state and energy density of the\n system in the thermodynamic limit.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/advanced/tfi_phase_transition.py b/examples/advanced/tfi_phase_transition.py\nindex 44f263a1c..482f1903b 100644\n--- a/examples/advanced/tfi_phase_transition.py\n+++ b/examples/advanced/tfi_phase_transition.py\n@@ -4,7 +4,7 @@\n through the phase transition by changing the field `g`.\n It plots a few observables in the end.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/advanced/tfi_segment.py b/examples/advanced/tfi_segment.py\nindex 7519a4b53..d270c192b 100644\n--- a/examples/advanced/tfi_segment.py\n+++ b/examples/advanced/tfi_segment.py\n@@ -8,7 +8,7 @@\n :class:`~tenpy.simulations.GroundStateSearch.OrthogonalExcitations` class,\n but this example might be helpful to see the general idea.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/advanced/vumps_and_plane_wave.py b/examples/advanced/vumps_and_plane_wave.py\nindex d8363cdc9..368b22d1c 100644\n--- a/examples/advanced/vumps_and_plane_wave.py\n+++ b/examples/advanced/vumps_and_plane_wave.py\n@@ -2,7 +2,7 @@\n \n This example uses VUMPS to find the ground state of the transverse field Ising model and uses the plane wave excitation ansatz to compute the first excited states.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from tenpy.models.tf_ising import TFIChain\n from tenpy.algorithms import vumps, plane_wave_excitation\ndiff --git a/examples/advanced/xxz_corr_length.py b/examples/advanced/xxz_corr_length.py\nindex 6eb250628..1ba8fafa4 100644\n--- a/examples/advanced/xxz_corr_length.py\n+++ b/examples/advanced/xxz_corr_length.py\n@@ -5,7 +5,7 @@\n :meth:`~tenpy.networks.mps.MPS.correlation_length` to extract the correlation length of the ground\n state, and plots it vs. hz in the end.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/b_mps.py b/examples/b_mps.py\nindex dad771bf7..26afa54c6 100644\n--- a/examples/b_mps.py\n+++ b/examples/b_mps.py\n@@ -12,7 +12,7 @@\n Note that this example performs the same steps as `a_np_conserved.py`,\n but makes use of other predefined classes except npc.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import tenpy.linalg.np_conserved as npc\n import numpy as np\ndiff --git a/examples/c_tebd.py b/examples/c_tebd.py\nindex 10f04974d..2d547a926 100644\n--- a/examples/c_tebd.py\n+++ b/examples/c_tebd.py\n@@ -3,7 +3,7 @@\n The example functions in this class do the same as the ones in `toycodes/c_tebd.py`, but make use\n of the classes defined in tenpy.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/chern_insulators/chiral_pi_flux.py b/examples/chern_insulators/chiral_pi_flux.py\nindex c8f75bffe..c6abb4edd 100644\n--- a/examples/chern_insulators/chiral_pi_flux.py\n+++ b/examples/chern_insulators/chiral_pi_flux.py\n@@ -3,7 +3,7 @@\n Based on the model in [Neupert2011]_\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/chern_insulators/haldane.py b/examples/chern_insulators/haldane.py\nindex d917e5804..660ba1551 100644\n--- a/examples/chern_insulators/haldane.py\n+++ b/examples/chern_insulators/haldane.py\n@@ -3,7 +3,7 @@\n Reproduces Fig. 2.a,b) in [Grushin2015]_\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/chern_insulators/haldane_C3.py b/examples/chern_insulators/haldane_C3.py\nindex 3fe1495d7..3b9083a77 100644\n--- a/examples/chern_insulators/haldane_C3.py\n+++ b/examples/chern_insulators/haldane_C3.py\n@@ -3,7 +3,7 @@\n Based on the model in [Yang2012]_\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/chern_insulators/haldane_FCI.py b/examples/chern_insulators/haldane_FCI.py\nindex 2aa9c3e33..1a540c94a 100644\n--- a/examples/chern_insulators/haldane_FCI.py\n+++ b/examples/chern_insulators/haldane_FCI.py\n@@ -6,7 +6,7 @@\n - 1/2 filling of the lowest band (i.e. 1/4 total filling)\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/examples/d_dmrg.py b/examples/d_dmrg.py\nindex a8c48e4a3..11f40f93a 100644\n--- a/examples/d_dmrg.py\n+++ b/examples/d_dmrg.py\n@@ -3,7 +3,7 @@\n The example functions in this class do the same as the ones in `toycodes/d_dmrg.py`,\n but make use of the classes defined in tenpy.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/examples/e_tdvp.py b/examples/e_tdvp.py\nindex 979b3eae5..2dbe33144 100644\n--- a/examples/e_tdvp.py\n+++ b/examples/e_tdvp.py\n@@ -4,7 +4,7 @@\n difference is that we can run one-site TDVP or two-site TDVP. In the former, the bond dimension can\n not grow; the latter allows to grow the bond dimension and hence requires a truncation.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n import numpy as np\n import tenpy\n from tenpy.algorithms import tdvp\ndiff --git a/examples/tfi_exact.py b/examples/tfi_exact.py\nindex 643cc3caf..dd2fdd20b 100644\n--- a/examples/tfi_exact.py\n+++ b/examples/tfi_exact.py\n@@ -4,7 +4,7 @@\n .. math ::\n     H = - J \\\\sum_{i} \\\\sigma^x_i \\\\sigma^x_{i+1} - g \\\\sum_{i} \\\\sigma^z_i\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import scipy.sparse as sparse\ndiff --git a/examples/v1_publication/heisenberg_tebd.py b/examples/v1_publication/heisenberg_tebd.py\nindex 9563379d5..41f64eda4 100644\n--- a/examples/v1_publication/heisenberg_tebd.py\n+++ b/examples/v1_publication/heisenberg_tebd.py\n@@ -1,5 +1,5 @@\n \"\"\"An example simulating the dynamics of the Neel state under Heisenberg evolution, using TEBD.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n import argparse\n import matplotlib.pyplot as plt\n import matplotlib as mpl\ndiff --git a/examples/v1_publication/tfi_cylinder.py b/examples/v1_publication/tfi_cylinder.py\nindex 93f0d011b..8156b002d 100644\n--- a/examples/v1_publication/tfi_cylinder.py\n+++ b/examples/v1_publication/tfi_cylinder.py\n@@ -1,5 +1,5 @@\n \"\"\"An example determining the phase diagram of the 2D transverse field Ising model.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n import pickle\n import numpy as np\n import os\ndiff --git a/examples/z_exact_diag.py b/examples/z_exact_diag.py\nindex 68eeb487c..163838114 100644\n--- a/examples/z_exact_diag.py\n+++ b/examples/z_exact_diag.py\n@@ -2,7 +2,7 @@\n \n Sorry that this is not well documented! ED is meant to be used for debugging only ;)\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import tenpy.linalg.np_conserved as npc\n from tenpy.models.xxz_chain import XXZChain\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 6a4bf757c..41e5c13ae 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -8,7 +8,6 @@ dynamic = [\"version\"]\n description = \"Simulation of quantum many-body systems with tensor networks in Python\"\n readme = {file = \"README.rst\", content-type = \"text/x-rst\"}\n requires-python = \">=3.8\"\n-license = {text = \"GPLv3\"}\n authors = [\n     {name = \"TeNPy Developer Team\"},\n ]\n@@ -21,7 +20,7 @@ classifiers = [\n     \"Development Status :: 5 - Production/Stable\",\n     \"Intended Audience :: Science/Research\",\n     \"Intended Audience :: Developers\",\n-    \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n+    \"License :: OSI Approved :: Apache Software License\",\n     \"Natural Language :: English\",\n     \"Programming Language :: C\",\n     \"Programming Language :: Python\",\ndiff --git a/setup.py b/setup.py\nindex 638d31f3b..92ba55e63 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -1,4 +1,4 @@\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n from setuptools import setup, Extension\n try:\n     from Cython.Build import cythonize\ndiff --git a/tenpy/__init__.py b/tenpy/__init__.py\nindex 674ff7f89..99f4c4e07 100644\n--- a/tenpy/__init__.py\n+++ b/tenpy/__init__.py\n@@ -6,7 +6,7 @@\n The code is intended to be accessible for newcomers\n and yet powerful enough for day-to-day research.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n # This file marks this directory as a python package.\n \n # Note: all external packages that are imported should be `del`-ed at the end of the file!\ndiff --git a/tenpy/__main__.py b/tenpy/__main__.py\nindex cf51defe0..e82754a23 100644\n--- a/tenpy/__main__.py\n+++ b/tenpy/__main__.py\n@@ -1,5 +1,5 @@\n \"\"\"The tenpy entry point, called by ``python -m tenpy``.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import tenpy\n \ndiff --git a/tenpy/algorithms/__init__.py b/tenpy/algorithms/__init__.py\nindex a153ff892..f9df99683 100644\n--- a/tenpy/algorithms/__init__.py\n+++ b/tenpy/algorithms/__init__.py\n@@ -19,7 +19,7 @@\n     exact_diag\n     disentangler\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import algorithm, dmrg, dmrg_parallel, disentangler, mps_common, tebd, tdvp, \\\n     exact_diag, purification, network_contractor, mpo_evolution, vumps, plane_wave_excitation\ndiff --git a/tenpy/algorithms/algorithm.py b/tenpy/algorithms/algorithm.py\nindex 16f9e77d8..01ce61bf3 100644\n--- a/tenpy/algorithms/algorithm.py\n+++ b/tenpy/algorithms/algorithm.py\n@@ -1,5 +1,5 @@\n \"\"\"This module contains some base classes for algorithms.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import time\n import numpy as np\ndiff --git a/tenpy/algorithms/disentangler.py b/tenpy/algorithms/disentangler.py\nindex 955022df3..a34dccfc6 100644\n--- a/tenpy/algorithms/disentangler.py\n+++ b/tenpy/algorithms/disentangler.py\n@@ -7,7 +7,7 @@\n \n .. autodata:: disentanglers_atom_parse_dict\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import logging\ndiff --git a/tenpy/algorithms/dmrg.py b/tenpy/algorithms/dmrg.py\nindex 692ce36b8..630d1edfc 100644\n--- a/tenpy/algorithms/dmrg.py\n+++ b/tenpy/algorithms/dmrg.py\n@@ -30,7 +30,7 @@\n A generic protocol for approaching a physics question using DMRG is given in\n :doc:`/intro/dmrg-protocol`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import time\ndiff --git a/tenpy/algorithms/dmrg_parallel.py b/tenpy/algorithms/dmrg_parallel.py\nindex 05db7a678..bfbd28ece 100644\n--- a/tenpy/algorithms/dmrg_parallel.py\n+++ b/tenpy/algorithms/dmrg_parallel.py\n@@ -3,7 +3,7 @@\n .. warning ::\n     This module is still under active development. Use with care!\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from ..tools.thread import Worker\n \ndiff --git a/tenpy/algorithms/exact_diag.py b/tenpy/algorithms/exact_diag.py\nindex 43145ebd0..9c76cd5b6 100644\n--- a/tenpy/algorithms/exact_diag.py\n+++ b/tenpy/algorithms/exact_diag.py\n@@ -13,7 +13,7 @@\n     but just the ability to diagonalize the defined models for small system sizes\n     without additional extra work.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/algorithms/mpo_evolution.py b/tenpy/algorithms/mpo_evolution.py\nindex 36fcb63d7..1f0a3d89e 100644\n--- a/tenpy/algorithms/mpo_evolution.py\n+++ b/tenpy/algorithms/mpo_evolution.py\n@@ -1,6 +1,6 @@\n \"\"\"Time evolution using the WI or WII approximation of the time evolution operator.\"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import logging\n \ndiff --git a/tenpy/algorithms/mps_common.py b/tenpy/algorithms/mps_common.py\nindex 7b697426a..d17aedeeb 100644\n--- a/tenpy/algorithms/mps_common.py\n+++ b/tenpy/algorithms/mps_common.py\n@@ -18,7 +18,7 @@\n The :class:`VariationalCompression` and :class:`VariationalApplyMPO`\n implemented here also directly use the :class:`Sweep` class.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from ..linalg import np_conserved as npc\n from .algorithm import Algorithm\ndiff --git a/tenpy/algorithms/network_contractor.py b/tenpy/algorithms/network_contractor.py\nindex 3c0f4cb10..45c69be23 100644\n--- a/tenpy/algorithms/network_contractor.py\n+++ b/tenpy/algorithms/network_contractor.py\n@@ -9,7 +9,7 @@\n     - implement or wrap netcon.m, a function to find optimal contraction sequences\n         (:arxiv:`1304.6112`)\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from ..linalg import np_conserved as npc\ndiff --git a/tenpy/algorithms/plane_wave_excitation.py b/tenpy/algorithms/plane_wave_excitation.py\nindex b7e0dab4d..e21018110 100644\n--- a/tenpy/algorithms/plane_wave_excitation.py\n+++ b/tenpy/algorithms/plane_wave_excitation.py\n@@ -22,7 +22,7 @@\n in :class:`MultiSitePlaneWaveExcitationEngine`. Note that with the current implementation, the\n numerical costs scale exponentially with the number of exciting sites.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import logging\ndiff --git a/tenpy/algorithms/purification.py b/tenpy/algorithms/purification.py\nindex fa5beb005..99e822c1f 100644\n--- a/tenpy/algorithms/purification.py\n+++ b/tenpy/algorithms/purification.py\n@@ -1,6 +1,6 @@\n \"\"\"Algorithms for using Purification.\"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import logging\ndiff --git a/tenpy/algorithms/tdvp.py b/tenpy/algorithms/tdvp.py\nindex d92c2f506..47bb3e24f 100644\n--- a/tenpy/algorithms/tdvp.py\n+++ b/tenpy/algorithms/tdvp.py\n@@ -30,7 +30,7 @@\n .. todo ::\n     allow for increasing bond dimension in SingleSiteTDVPEngine, similar to DMRG Mixer\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from ..linalg.krylov_based import LanczosEvolution\n from ..linalg.truncation import svd_theta, TruncationError\ndiff --git a/tenpy/algorithms/tebd.py b/tenpy/algorithms/tebd.py\nindex 46cb0aa00..ade8e3bde 100644\n--- a/tenpy/algorithms/tebd.py\n+++ b/tenpy/algorithms/tebd.py\n@@ -37,7 +37,7 @@\n     Yet, imaginary TEBD might be useful for cross-checks and testing.\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import time\ndiff --git a/tenpy/algorithms/truncation.py b/tenpy/algorithms/truncation.py\nindex 063beffbf..10a57aab3 100644\n--- a/tenpy/algorithms/truncation.py\n+++ b/tenpy/algorithms/truncation.py\n@@ -4,7 +4,7 @@\n Here we just import all the names again for backwards compatibility,\n to support loading pickle and HDF5 data files with `TruncationError` instances in them.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n # just provide namespace from ..linalg.truncation\n \ndiff --git a/tenpy/algorithms/vumps.py b/tenpy/algorithms/vumps.py\nindex df560108e..15837e9e4 100644\n--- a/tenpy/algorithms/vumps.py\n+++ b/tenpy/algorithms/vumps.py\n@@ -31,7 +31,7 @@\n the single-site algorithm, which is the more principled algorithm.\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import time\ndiff --git a/tenpy/linalg/__init__.py b/tenpy/linalg/__init__.py\nindex 91a2ec4f6..2d64de758 100644\n--- a/tenpy/linalg/__init__.py\n+++ b/tenpy/linalg/__init__.py\n@@ -22,7 +22,7 @@\n     truncation\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import charges, np_conserved, krylov_based, random_matrix, sparse, svd_robust, truncation\n from .charges import *\ndiff --git a/tenpy/linalg/_npc_helper.pyx b/tenpy/linalg/_npc_helper.pyx\nindex 46423ecf2..54ed1a3d8 100644\n--- a/tenpy/linalg/_npc_helper.pyx\n+++ b/tenpy/linalg/_npc_helper.pyx\n@@ -8,7 +8,7 @@ functions/classes defined here to overwrite those written in pure Python wheneve\n decorator ``@use_cython`` is used in other python files of tenpy.\n If this module was not compiled and could not be imported, a warning is issued.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n DEF DEBUG_PRINT = 0  # set this to 1 for debug output (e.g. benchmark timings within the functions)\n DEF USE_MKL_GEMM_BATCH = 1 # whether to use ?gemm_batch function of MKL\ndiff --git a/tenpy/linalg/charges.py b/tenpy/linalg/charges.py\nindex d4a2d8254..f409f3617 100644\n--- a/tenpy/linalg/charges.py\n+++ b/tenpy/linalg/charges.py\n@@ -19,7 +19,7 @@\n \n .. autodata:: QTYPE\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import bisect\ndiff --git a/tenpy/linalg/krylov_based.py b/tenpy/linalg/krylov_based.py\nindex 28eec7ea7..dc3abb1b6 100644\n--- a/tenpy/linalg/krylov_based.py\n+++ b/tenpy/linalg/krylov_based.py\n@@ -1,5 +1,5 @@\n \"\"\"Lanczos algorithm for np_conserved arrays.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from .sparse import FlatHermitianOperator, OrthogonalNpcLinearOperator, ShiftNpcLinearOperator\ndiff --git a/tenpy/linalg/np_conserved.py b/tenpy/linalg/np_conserved.py\nindex 0b09b487e..ec6ab5797 100644\n--- a/tenpy/linalg/np_conserved.py\n+++ b/tenpy/linalg/np_conserved.py\n@@ -81,7 +81,7 @@\n     speigs\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import scipy.linalg\ndiff --git a/tenpy/linalg/random_matrix.py b/tenpy/linalg/random_matrix.py\nindex 6f5dcc7da..a0a95d343 100644\n--- a/tenpy/linalg/random_matrix.py\n+++ b/tenpy/linalg/random_matrix.py\n@@ -29,7 +29,7 @@\n     npc.Array.from_func_square(GOE, [leg, leg.conj()])\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/linalg/sparse.py b/tenpy/linalg/sparse.py\nindex ec8f742b1..07dd97c87 100644\n--- a/tenpy/linalg/sparse.py\n+++ b/tenpy/linalg/sparse.py\n@@ -7,7 +7,7 @@\n :class:`FlatLinearOperator` allows to use all the scipy sparse methods by providing functionality\n to convert flat numpy arrays to and from np_conserved arrays.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from . import np_conserved as npc\ndiff --git a/tenpy/linalg/svd_robust.py b/tenpy/linalg/svd_robust.py\nindex 177819e7c..75a079783 100644\n--- a/tenpy/linalg/svd_robust.py\n+++ b/tenpy/linalg/svd_robust.py\n@@ -21,7 +21,7 @@\n >>> from tenpy.linalg.svd_robust import svd\n >>> U, S, VT = svd([[1., 1.], [0., 1.]])\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import scipy\ndiff --git a/tenpy/linalg/truncation.py b/tenpy/linalg/truncation.py\nindex 9124428be..a0589a1c9 100644\n--- a/tenpy/linalg/truncation.py\n+++ b/tenpy/linalg/truncation.py\n@@ -41,7 +41,7 @@\n     There might be other sources of error as well, for example TEBD has also an discretization\n     error depending on the chosen time step.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from ..linalg import np_conserved as npc\ndiff --git a/tenpy/models/__init__.py b/tenpy/models/__init__.py\nindex 5fefe6497..56aef1c74 100644\n--- a/tenpy/models/__init__.py\n+++ b/tenpy/models/__init__.py\n@@ -33,7 +33,7 @@\n     mixed_xk\n     clock\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import lattice, model\n from . import tf_ising, xxz_chain, spins, spins_nnn\ndiff --git a/tenpy/models/aklt.py b/tenpy/models/aklt.py\nindex 1d5613d4e..fcf0badc9 100644\n--- a/tenpy/models/aklt.py\n+++ b/tenpy/models/aklt.py\n@@ -4,7 +4,7 @@\n Writing down the Hamiltonian is easiest done in terms of bond couplings.\n This class thus serves as an example how this can be done.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/clock.py b/tenpy/models/clock.py\nindex a3600c9e1..c5073f5cb 100644\n--- a/tenpy/models/clock.py\n+++ b/tenpy/models/clock.py\n@@ -2,7 +2,7 @@\n \n Generalization of transverse field Ising model to higher dimensional on-site Hilbert space.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from .model import CouplingMPOModel, NearestNeighborModel\ndiff --git a/tenpy/models/fermions_spinless.py b/tenpy/models/fermions_spinless.py\nindex 89dea3e14..3ba3708a5 100644\n--- a/tenpy/models/fermions_spinless.py\n+++ b/tenpy/models/fermions_spinless.py\n@@ -2,7 +2,7 @@\n \n .. todo ::     add further terms (e.g. c^dagger c^dagger + h.c.) to the Hamiltonian.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n \n from .model import CouplingMPOModel, NearestNeighborModel\ndiff --git a/tenpy/models/haldane.py b/tenpy/models/haldane.py\nindex 19e8b4404..44d8d558e 100644\n--- a/tenpy/models/haldane.py\n+++ b/tenpy/models/haldane.py\n@@ -1,5 +1,5 @@\n \"\"\"Bosonic and fermionic Haldane models.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/hofstadter.py b/tenpy/models/hofstadter.py\nindex 0eb65e028..e93551356 100644\n--- a/tenpy/models/hofstadter.py\n+++ b/tenpy/models/hofstadter.py\n@@ -4,7 +4,7 @@\n     Long term: implement different lattices.\n     Long term: implement variable hopping strengths Jx, Jy.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/hubbard.py b/tenpy/models/hubbard.py\nindex d64c08dcb..80f669678 100644\n--- a/tenpy/models/hubbard.py\n+++ b/tenpy/models/hubbard.py\n@@ -1,5 +1,5 @@\n \"\"\"Bosonic and fermionic Hubbard models.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/lattice.py b/tenpy/models/lattice.py\nindex e55f99580..69f4d2028 100644\n--- a/tenpy/models/lattice.py\n+++ b/tenpy/models/lattice.py\n@@ -16,7 +16,7 @@\n :doc:`/notebooks/90_overview_predefined_lattices`\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from scipy.spatial import ConvexHull, Voronoi\ndiff --git a/tenpy/models/mixed_xk.py b/tenpy/models/mixed_xk.py\nindex 934abd241..2fa84a8eb 100644\n--- a/tenpy/models/mixed_xk.py\n+++ b/tenpy/models/mixed_xk.py\n@@ -62,7 +62,7 @@\n     exclusion principle implies a possibly large occupation on single k modes, i.e., hard-core\n     bosons in x-y-space don't map to hard-core bosons in x-k-space!\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import itertools as it\ndiff --git a/tenpy/models/model.py b/tenpy/models/model.py\nindex 971ff8821..3207bed78 100644\n--- a/tenpy/models/model.py\n+++ b/tenpy/models/model.py\n@@ -27,7 +27,7 @@\n \n See also the introduction in :doc:`/intro/model`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/models/spins.py b/tenpy/models/spins.py\nindex 6869772ff..552142151 100644\n--- a/tenpy/models/spins.py\n+++ b/tenpy/models/spins.py\n@@ -2,7 +2,7 @@\n \n Uniform lattice of spin-S sites, coupled by nearest-neighbor interactions.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from ..networks.site import SpinSite\n from .model import CouplingMPOModel, NearestNeighborModel\ndiff --git a/tenpy/models/spins_nnn.py b/tenpy/models/spins_nnn.py\nindex ef53f1e72..cc870b9d1 100644\n--- a/tenpy/models/spins_nnn.py\n+++ b/tenpy/models/spins_nnn.py\n@@ -14,7 +14,7 @@\n :meth:`~tenpy.models.model.NearestNeighborModel.from_MPOModel`.\n An example for such a case is given in the file ``examples/c_tebd.py``.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from .lattice import Chain\n from ..networks.site import SpinSite, GroupedSite\ndiff --git a/tenpy/models/tf_ising.py b/tenpy/models/tf_ising.py\nindex b7680f8ad..f9ff3a8a3 100644\n--- a/tenpy/models/tf_ising.py\n+++ b/tenpy/models/tf_ising.py\n@@ -6,7 +6,7 @@\n \n We choose the field along z to allow to conserve the parity, if desired.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/tj_model.py b/tenpy/models/tj_model.py\nindex aa0e5e2cf..19b75cc9d 100644\n--- a/tenpy/models/tj_model.py\n+++ b/tenpy/models/tj_model.py\n@@ -1,5 +1,5 @@\n \"\"\"tJ model\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from .model import CouplingMPOModel, NearestNeighborModel\n from .lattice import Chain\ndiff --git a/tenpy/models/toric_code.py b/tenpy/models/toric_code.py\nindex 83d17560b..ca9943b85 100644\n--- a/tenpy/models/toric_code.py\n+++ b/tenpy/models/toric_code.py\n@@ -3,7 +3,7 @@\n As we put the model on a cylinder, the name \"toric code\" is a bit misleading, but it is the\n established name for this model...\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/models/xxz_chain.py b/tenpy/models/xxz_chain.py\nindex d68deedb8..755a6142f 100644\n--- a/tenpy/models/xxz_chain.py\n+++ b/tenpy/models/xxz_chain.py\n@@ -3,7 +3,7 @@\n The XXZ chain is contained in the more general :class:`~tenpy.models.spins.SpinChain`; the idea of\n this module is more to serve as a pedagogical example for a model.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from .lattice import Site, Chain\n from .model import CouplingModel, NearestNeighborModel, MPOModel, CouplingMPOModel\ndiff --git a/tenpy/networks/__init__.py b/tenpy/networks/__init__.py\nindex e5bddf1c6..2a1213793 100644\n--- a/tenpy/networks/__init__.py\n+++ b/tenpy/networks/__init__.py\n@@ -16,7 +16,7 @@\n     uniform_mps\n     momentum_mps\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import site, mps, mpo, purification_mps, momentum_mps, uniform_mps\n \ndiff --git a/tenpy/networks/momentum_mps.py b/tenpy/networks/momentum_mps.py\nindex 11dc0d460..4490845ca 100644\n--- a/tenpy/networks/momentum_mps.py\n+++ b/tenpy/networks/momentum_mps.py\n@@ -20,7 +20,7 @@\n always orthogonal to the initial uniform MPS. `X` parametrizes the excited states.\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import logging\ndiff --git a/tenpy/networks/mpo.py b/tenpy/networks/mpo.py\nindex 20e518b7c..00391a757 100644\n--- a/tenpy/networks/mpo.py\n+++ b/tenpy/networks/mpo.py\n@@ -34,7 +34,7 @@\n Similar as for the MPS, a bond index ``i`` is *left* of site `i`,\n i.e. between sites ``i-1`` and ``i``.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from scipy.linalg import expm\ndiff --git a/tenpy/networks/mps.py b/tenpy/networks/mps.py\nindex 6552416a6..13cc9de56 100644\n--- a/tenpy/networks/mps.py\n+++ b/tenpy/networks/mps.py\n@@ -143,7 +143,7 @@\n iMPS, so this is the form *assumed* when calling MPO :meth:`~tenpy.networks.mpo.MPO.apply` on an\n MPS.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from abc import ABCMeta, abstractmethod\n import numpy as np\ndiff --git a/tenpy/networks/purification_mps.py b/tenpy/networks/purification_mps.py\nindex 32a216338..577d88cbe 100644\n--- a/tenpy/networks/purification_mps.py\n+++ b/tenpy/networks/purification_mps.py\n@@ -116,7 +116,7 @@\n     Moreover, we don't split the physical and auxiliary space into separate sites, which makes\n     TEBD as costly as :math:`O(d^6 \\chi^3)`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import copy\n import numpy as np\ndiff --git a/tenpy/networks/site.py b/tenpy/networks/site.py\nindex 0419ca2db..1a9b247c2 100644\n--- a/tenpy/networks/site.py\n+++ b/tenpy/networks/site.py\n@@ -3,7 +3,7 @@\n The :class:`Site` is the prototype, read it's docstring.\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import itertools\ndiff --git a/tenpy/networks/terms.py b/tenpy/networks/terms.py\nindex d4dcd361f..8dbe03fb0 100644\n--- a/tenpy/networks/terms.py\n+++ b/tenpy/networks/terms.py\n@@ -5,7 +5,7 @@\n sites it acts on. Moreover, we associate a `strength` to each term, which corresponds to the\n prefactor when specifying e.g. a Hamiltonian.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/networks/uniform_mps.py b/tenpy/networks/uniform_mps.py\nindex 492183539..92259101c 100644\n--- a/tenpy/networks/uniform_mps.py\n+++ b/tenpy/networks/uniform_mps.py\n@@ -20,7 +20,7 @@\n account for the additional type of tensor structure.\n \n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import logging\ndiff --git a/tenpy/simulations/__init__.py b/tenpy/simulations/__init__.py\nindex d606f7a0f..422f57f69 100644\n--- a/tenpy/simulations/__init__.py\n+++ b/tenpy/simulations/__init__.py\n@@ -13,7 +13,7 @@\n     time_evolution\n     post_processing\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import measurement, simulation, ground_state_search, time_evolution\n from .measurement import *\ndiff --git a/tenpy/simulations/ground_state_search.py b/tenpy/simulations/ground_state_search.py\nindex 12629b4fe..35f285432 100644\n--- a/tenpy/simulations/ground_state_search.py\n+++ b/tenpy/simulations/ground_state_search.py\n@@ -1,5 +1,5 @@\n \"\"\"Simulations for ground state searches.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from pathlib import Path\ndiff --git a/tenpy/simulations/measurement.py b/tenpy/simulations/measurement.py\nindex d31fe8574..fc38a8223 100644\n--- a/tenpy/simulations/measurement.py\n+++ b/tenpy/simulations/measurement.py\n@@ -10,7 +10,7 @@\n \n Full description and details in :doc:`/intro/measurements`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/simulations/post_processing.py b/tenpy/simulations/post_processing.py\nindex a11c32f0e..5aafd5a3c 100644\n--- a/tenpy/simulations/post_processing.py\n+++ b/tenpy/simulations/post_processing.py\n@@ -9,7 +9,7 @@\n the simulation class in a post-processing step. They follow the syntax\n ``def pp_function(DL, *, kwarg1, kwarg_2=default_2):``.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n \n import os\ndiff --git a/tenpy/simulations/simulation.py b/tenpy/simulations/simulation.py\nindex 9d4e9aa6d..12b1a5513 100644\n--- a/tenpy/simulations/simulation.py\n+++ b/tenpy/simulations/simulation.py\n@@ -7,7 +7,7 @@\n See :doc:`/intro/simulations` for an overview and\n :doc:`/examples` for a list of example parameter yaml files.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import os\n import sys\ndiff --git a/tenpy/simulations/time_evolution.py b/tenpy/simulations/time_evolution.py\nindex 12b1ab29f..941dcd775 100644\n--- a/tenpy/simulations/time_evolution.py\n+++ b/tenpy/simulations/time_evolution.py\n@@ -1,6 +1,6 @@\n \"\"\"Simulations for (real) time evolution, time dependent correlation\n functions and spectral functions.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/tools/__init__.py b/tenpy/tools/__init__.py\nindex c3514c9ec..d3279e84f 100644\n--- a/tenpy/tools/__init__.py\n+++ b/tenpy/tools/__init__.py\n@@ -23,7 +23,7 @@\n     spectral_function_tools\n     prediction\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from . import (events, fit, hdf5_io, math, misc, params, process, string, optimization, cache,\n                thread)\ndiff --git a/tenpy/tools/cache.py b/tenpy/tools/cache.py\nindex e65c6bfb4..6de99ec6c 100644\n--- a/tenpy/tools/cache.py\n+++ b/tenpy/tools/cache.py\n@@ -7,7 +7,7 @@\n this is easiest done through a ``with`` statement, see the example in :class:`DictCache`.\n \"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import pickle\n import numpy as np\ndiff --git a/tenpy/tools/docs.py b/tenpy/tools/docs.py\nindex f40257d6a..f4e8469a6 100644\n--- a/tenpy/tools/docs.py\n+++ b/tenpy/tools/docs.py\n@@ -1,5 +1,5 @@\n \"\"\"Tools for handling docstrings.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n __all__ = ['amend_parent_docstring']\n \ndiff --git a/tenpy/tools/events.py b/tenpy/tools/events.py\nindex 585e68336..a26b4d677 100644\n--- a/tenpy/tools/events.py\n+++ b/tenpy/tools/events.py\n@@ -4,7 +4,7 @@\n which can get called once a certain \"event\" happens.\n Examples are given in the class doc-string.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from collections import namedtuple\n import warnings\ndiff --git a/tenpy/tools/fit.py b/tenpy/tools/fit.py\nindex 3a49a9bc1..b9ea90225 100644\n--- a/tenpy/tools/fit.py\n+++ b/tenpy/tools/fit.py\n@@ -1,5 +1,5 @@\n \"\"\"tools to fit to an algebraic decay.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import scipy.optimize as optimize\ndiff --git a/tenpy/tools/hdf5_io.py b/tenpy/tools/hdf5_io.py\nindex cadd4dab8..297bdd9cd 100644\n--- a/tenpy/tools/hdf5_io.py\n+++ b/tenpy/tools/hdf5_io.py\n@@ -65,7 +65,7 @@\n \n .. autodata:: TYPES_FOR_HDF5_DATASETS\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import pickle\n import gzip\ndiff --git a/tenpy/tools/math.py b/tenpy/tools/math.py\nindex 44a93bca6..b61f17c33 100644\n--- a/tenpy/tools/math.py\n+++ b/tenpy/tools/math.py\n@@ -2,7 +2,7 @@\n \n .. autodata:: LeviCivita3\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n import warnings\ndiff --git a/tenpy/tools/misc.py b/tenpy/tools/misc.py\nindex 0ce9c9c33..b06ed28d5 100644\n--- a/tenpy/tools/misc.py\n+++ b/tenpy/tools/misc.py\n@@ -1,5 +1,5 @@\n \"\"\"Miscellaneous tools, somewhat random mix yet often helpful.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import operator\n import numpy as np\ndiff --git a/tenpy/tools/optimization.py b/tenpy/tools/optimization.py\nindex 8706fa520..39239058e 100644\n--- a/tenpy/tools/optimization.py\n+++ b/tenpy/tools/optimization.py\n@@ -76,7 +76,7 @@\n .. autodata:: have_cython_functions\n .. autodata:: compiled_with_MKL\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from enum import IntEnum\n import warnings\ndiff --git a/tenpy/tools/params.py b/tenpy/tools/params.py\nindex a21517810..57a2a080a 100644\n--- a/tenpy/tools/params.py\n+++ b/tenpy/tools/params.py\n@@ -2,7 +2,7 @@\n \n See the doc-string of :class:`Config` for details.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import warnings\n import numpy\ndiff --git a/tenpy/tools/prediction.py b/tenpy/tools/prediction.py\nindex 727c1ad33..1298dc1df 100644\n--- a/tenpy/tools/prediction.py\n+++ b/tenpy/tools/prediction.py\n@@ -1,5 +1,5 @@\n \"\"\"This module contains functions for linear prediction.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n from scipy.linalg import solve_toeplitz\ndiff --git a/tenpy/tools/process.py b/tenpy/tools/process.py\nindex b42a5204d..8acb3076f 100644\n--- a/tenpy/tools/process.py\n+++ b/tenpy/tools/process.py\n@@ -13,7 +13,7 @@\n while still being failsafe if the shared OpenMP library is not found.  In the latter case,\n you might also try the equivalent :func:`mkl_get_nthreads` and :func:`mkl_set_nthreads`.\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import warnings\n import ctypes\ndiff --git a/tenpy/tools/spectral_function_tools.py b/tenpy/tools/spectral_function_tools.py\nindex 45f5e40d5..2c18a80cd 100644\n--- a/tenpy/tools/spectral_function_tools.py\n+++ b/tenpy/tools/spectral_function_tools.py\n@@ -8,7 +8,7 @@\n However, they can also be used in a standalone way on available results (i.e. in an interactive ipython or\n jupyter notebook session).\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import numpy as np\n \ndiff --git a/tenpy/tools/string.py b/tenpy/tools/string.py\nindex 82793462a..c62c9db98 100644\n--- a/tenpy/tools/string.py\n+++ b/tenpy/tools/string.py\n@@ -1,5 +1,5 @@\n \"\"\"Tools for handling strings.\"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n from typing import Sequence\n import numpy as np\ndiff --git a/tenpy/tools/thread.py b/tenpy/tools/thread.py\nindex c6cb30dac..85ab0c860 100644\n--- a/tenpy/tools/thread.py\n+++ b/tenpy/tools/thread.py\n@@ -1,6 +1,6 @@\n \"\"\"Tools for thread-based parallelization.\"\"\"\n \n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import threading\n import queue\ndiff --git a/tenpy/version.py b/tenpy/version.py\nindex 85e638d1d..47ecaf689 100644\n--- a/tenpy/version.py\n+++ b/tenpy/version.py\n@@ -10,7 +10,7 @@\n .. autodata :: full_version\n .. autodata :: version_summary\n \"\"\"\n-# Copyright (C) TeNPy Developers, GNU GPLv3\n+# Copyright (C) TeNPy Developers, Apache license\n \n import sys\n import subprocess\n", "instance_id": "tenpy__tenpy-465", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly explains the goal of re-licensing the TeNPy project from GPL v3 to Apache v2, providing detailed reasoning behind the change, including the implications of the copyleft clause in GPL v3 and how Apache v2 aligns better with the project's goals and collaborations (e.g., with IBM's Qiskit). The statement includes references to external resources for license comparison, mentions the need for contributor consent, and outlines the legal documentation process. There are no significant ambiguities regarding the intent or the process, and the code changes provided align directly with the stated goal. The problem description is thorough, with no critical details missing, and it even addresses potential concerns about the risk of closed-source contributions under Apache v2. Thus, it merits a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range (Very Easy). The problem involves re-licensing the project, which primarily requires updating license files and copyright notices across the codebase. The code changes are straightforward, consisting of replacing the GPL v3 license text with Apache v2 in the LICENSE file, updating references to the license in documentation and source files, and modifying classifiers in configuration files like pyproject.toml. The scope of changes is broad but shallow, affecting multiple files with minimal logic or technical complexity\u2014mostly textual replacements. No deep understanding of the codebase architecture, algorithms, or domain-specific knowledge is required beyond basic familiarity with licensing and file editing. There are no edge cases or error handling considerations, as the task is purely administrative and does not impact the system's functionality or performance. The primary challenge lies in ensuring all contributors consent to the re-licensing, which is a legal rather than technical issue and outside the scope of coding difficulty. Therefore, I assign a difficulty score of 0.15, reflecting a very easy task with minimal technical demand.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ValueError: '.ipynb' is not a valid Extension\nI have tried to use Hyperstyle as a linter in a real python project. I ran Hyperstyle on a directory with `.ipynb` files besides `.py`, but failed with exceptions`ValueError: '.ipynb' is not a valid Extension`. Same was with  `.md` files in diectoy: `ValueError: '.md' is not a valid Extension`. The full traceback was following:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/run_tool.py\", line 169, in main\r\n    n_issues = perform_and_print_review(args.path, OutputFormat(args.format), config)\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/reviewers/perform_review.py\", line 57, in perform_and_print_review\r\n    review_result = perform_review(path, config)\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/reviewers/perform_review.py\", line 85, in perform_review\r\n    metadata = explore_project(path)\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/reviewers/utils/metadata_exploration.py\", line 75, in explore_project\r\n    inner_files.append(explore_file(file_path))\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/reviewers/utils/metadata_exploration.py\", line 65, in explore_file\r\n    language = guess_file_language(path)\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/common/language.py\", line 54, in guess_file_language\r\n    return EXTENSION_TO_LANGUAGE.get(Extension.get_extension_from_file(file_path), Language.UNKNOWN)\r\n  File \"/Users/tiginamaria1999/PycharmProjects/hyperstyle/hyperstyle/src/python/review/common/file_system.py\", line 40, in get_extension_from_file\r\n    return Extension(os.path.splitext(file)[1])\r\n  File \"/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/enum.py\", line 384, in __call__\r\n    return cls.__new__(cls, value)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/enum.py\", line 702, in __new__\r\n    raise ve_exc\r\nValueError: '.ipynb' is not a valid Extension\r\n```\r\n\r\n It seems that Hyperstyle fails when meets file with extension out of enum class `hyperstyle.src.python.review.common.file_system.Extension`. But I expected that Hyperstyle will skip such files and give feedback only on `.py` files it meets. \r\n\r\nMaybe you can do something with it?\n", "patch": "diff --git a/.dockerignore b/.dockerignore\nindex 3f78c748..cb26562a 100644\n--- a/.dockerignore\n+++ b/.dockerignore\n@@ -5,3 +5,9 @@\n .github\n .idea\n node_modules\n+venv\n+.venv\n+build\n+**/*.egg-info\n+**/model_pb2*\n+linters\ndiff --git a/.github/workflows/build.yml b/.github/workflows/build.yml\nindex a3763b5d..3bd27396 100644\n--- a/.github/workflows/build.yml\n+++ b/.github/workflows/build.yml\n@@ -1,79 +1,97 @@\n name: Python build\n \n-on: [ push, pull_request ]\n+on:\n+  push:\n+    branches:\n+      - main\n+      - develop\n+  pull_request:\n \n-jobs:\n-\n-  build:\n-    runs-on: ubuntu-latest\n-    # Consistent with base image in Dockerfile\n-    # container: stepik/hyperstyle-base:py3.8.11-java11.0.11-node14.17.3-go1.18.5\n-    container: nastyabirillo/hyperstyle:1.4.4\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.ref }}\n+  cancel-in-progress: true\n \n+jobs:\n+  build_image:\n+    name: Build Image\n+    runs-on: [ self-hosted, small ]\n     steps:\n-      - name: Install git\n-        run: |\n-            apt-get update\n-            apt-get -y install git\n+      - uses: actions/checkout@v4\n \n-      - name: Check env variables\n-        run: |\n-            echo $DETEKT_DIRECTORY && echo $DETEKT_VERSION\n-            echo $CHECKSTYLE_DIRECTORY && echo CHECKSTYLE_VERSION\n-            echo $PMD_DIRECTORY && echo PMD_VERSION\n+      - uses: docker/login-action@v3\n+        with:\n+          registry: hyperskill.azurecr.io\n+          username: ${{ secrets.REGISTRY_USER }}\n+          password: ${{ secrets.REGISTRY_PASSWORD }}\n \n-      - name: Checkout\n-        uses: actions/checkout@v3\n+      - uses: docker/setup-buildx-action@v3\n \n-      - name: Install requirements\n+      - name: Build and push server image\n+        uses: docker/build-push-action@v6\n+        with:\n+          context: .\n+          pull: true\n+          push: true\n+          tags: hyperskill.azurecr.io/hyperstyle:${{ github.sha }}\n+          cache-from: |\n+            type=gha\n+            type=gha,scope=main\n+          cache-to: type=gha,mode=max\n+\n+  tests:\n+    name: Tests\n+    needs:\n+      - build_image\n+    runs-on: [ self-hosted, small ]\n+    container:\n+      image: hyperskill.azurecr.io/hyperstyle:${{ github.sha }}\n+      credentials:\n+        username: ${{ secrets.REGISTRY_USER }}\n+        password: ${{ secrets.REGISTRY_PASSWORD }}\n+    steps:\n+      - name: Run tests\n         run: |\n-          pip install --no-cache-dir -r requirements-test.txt -r requirements.txt\n+          cd /review\n+          /hyperstyle/bin/pytest\n \n-      - name: Set up git rights\n+  build:\n+    runs-on: [ self-hosted, small ]\n+    needs:\n+      - build_image\n+    container:\n+      image: hyperskill.azurecr.io/hyperstyle:${{ github.sha }}\n+      credentials:\n+        username: ${{ secrets.REGISTRY_USER }}\n+        password: ${{ secrets.REGISTRY_PASSWORD }}\n+    steps:\n+      - name: Check env variables\n         run: |\n-          git config --global --add safe.directory '*'\n+          echo $DETEKT_DIRECTORY && echo $DETEKT_VERSION\n+          echo $CHECKSTYLE_DIRECTORY && echo $CHECKSTYLE_VERSION\n+          echo $PMD_DIRECTORY && echo $PMD_VERSION\n+          echo $GOLANG_LINT_DIRECTORY && echo $GOLANG_LINT_VERSION\n \n       - name: Lint with flake8\n         run: |\n+          cd /review\n           # stop the build if there are Python syntax errors or undefined names\n-          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude=.git,__pycache__,docs/source/conf.py,old,build,dist,venv,test/resources,.eggs,review.egg-info,.pytest_cache,node_modules\n+          /hyperstyle/bin/flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude=.git,__pycache__,docs/source/conf.py,old,build,dist,venv,test/resources,.eggs,review.egg-info,.pytest_cache,node_modules,hyperstyle/src/python/review/inspectors/common/inspector/proto\n+\n           # TODO: change max-complexity into 10 after refactoring\n           # TODO: remove R504, A003, E800, E402, WPS1, WPS2, WPS3, WPS4, WPS5, WPS6, H601\n-          flake8 . --count --max-complexity=11 --max-line-length=120 --max-doc-length=120 --ignore=R504,A003,E800,E402,W503,WPS,H601,N400 --statistics --exclude=.git,__pycache__,docs/source/conf.py,old,build,dist,venv,test/resources,.eggs,review.egg-info,.pytest_cache,node_modules\n-\n-      - name: Sort whitelists\n-        run: |\n-            for file in \"whitelist.txt\" \"hyperstyle/src/python/review/inspectors/flake8/whitelist.txt\"\n-            do\n-            LC_ALL=C sort $file -o $file\n-            done\n-      - name: Commit sorted whitelists\n-        uses: EndBug/add-and-commit@v7.2.1\n-        with:\n-          add: \"['whitelist.txt', 'hyperstyle/src/python/review/inspectors/flake8/whitelist.txt']\"\n-          message: 'Sort whitelists (Github Actions)'\n-\n-      - name: Set up Eslint\n-        run: |\n-          # Consistent with eslint version in Dockerfile\n-          npm install eslint@7.5.0 -g && eslint --init\n-\n-      - name: Test with pytest\n-        run: |\n-          pytest -vv\n-\n-      - name: Install review module\n-        run: |\n-          pip install .\n+          /hyperstyle/bin/flake8 . --count --max-complexity=11 --max-line-length=120 --max-doc-length=120 --ignore=R504,A003,E800,E402,W503,WPS,H601,N400,I100,I201,I202, --statistics --exclude=.git,__pycache__,docs/source/conf.py,old,build,dist,venv,test/resources,.eggs,review.egg-info,.pytest_cache,node_modules,hyperstyle/src/python/review/inspectors/common/inspector/proto\n \n       - name: Check installed module can run python linters\n         run: |\n-            review setup.py\n+          cd /review\n+          /hyperstyle/bin/python -m hyperstyle.src.python.review.run_tool test/resources/inspectors/python/case39_no_issues.py\n \n       - name: Check installed module can run java linters\n         run: |\n-            review test/resources/inspectors/java/test_algorithm_with_scanner.java\n+          cd /review\n+          /hyperstyle/bin/python -m hyperstyle.src.python.review.run_tool test/resources/inspectors/java/test_algorithm_with_scanner.java\n \n       - name: Check installed module can run js linters\n         run: |\n-          review test/resources/inspectors/js/case0_no_issues.js\n+          cd /review\n+          /hyperstyle/bin/python -m hyperstyle.src.python.review.run_tool test/resources/inspectors/js/case0_no_issues.js\ndiff --git a/.github/workflows/build_base_image.yml b/.github/workflows/build_base_image.yml\nindex 51ae5774..7f66dab4 100644\n--- a/.github/workflows/build_base_image.yml\n+++ b/.github/workflows/build_base_image.yml\n@@ -7,7 +7,7 @@ on:\n         type: string\n         description: 'Image tag'\n         required: true\n-        default: 'py3.12.5-java11.0.11-node14.17.3-go1.18.5'\n+        default: 'py3.10.14-java11.0.11-node14.17.3-go1.18.5'\n \n concurrency:\n   group: ${{ github.workflow }}-${{ github.ref }}\ndiff --git a/.github/workflows/build_image.yml b/.github/workflows/build_image.yml\nnew file mode 100644\nindex 00000000..80f61805\n--- /dev/null\n+++ b/.github/workflows/build_image.yml\n@@ -0,0 +1,43 @@\n+name: Build Image after Release\n+\n+on:\n+  workflow_dispatch:\n+    inputs:\n+      image_tag:\n+        description: 'Image tag'\n+        required: true\n+        default: 'latest'\n+  release:\n+    types:\n+      - released\n+\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.ref }}\n+  cancel-in-progress: false\n+\n+jobs:\n+  build_image:\n+    name: Build Image\n+    runs-on: [ self-hosted, small ]\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - uses: docker/login-action@v3\n+        with:\n+          registry: hyperskill.azurecr.io\n+          username: ${{ secrets.REGISTRY_USER }}\n+          password: ${{ secrets.REGISTRY_PASSWORD }}\n+\n+      - uses: docker/setup-buildx-action@v3\n+\n+      - name: Build and push server image\n+        uses: docker/build-push-action@v6\n+        with:\n+          context: .\n+          pull: true\n+          push: true\n+          tags: hyperskill.azurecr.io/hyperstyle:${{ inputs.image_tag || github.event.release.tag_name }}\n+          cache-from: |\n+            type=gha\n+            type=gha,scope=main\n+          cache-to: type=gha,mode=max\ndiff --git a/.github/workflows/publish.yml b/.github/workflows/publish.yml\nindex 47819798..c1cbfafb 100644\n--- a/.github/workflows/publish.yml\n+++ b/.github/workflows/publish.yml\n@@ -1,19 +1,23 @@\n-# Publish to PyPI in case of releasing\n+name: Publish to PyPI in case of releasing\n \n on:\n   push:\n \n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.ref }}\n+  cancel-in-progress: false\n+\n jobs:\n   build-n-publish:\n     name: Build and publish Python \ud83d\udc0d distributions \ud83d\udce6 to PyPI\n-    runs-on: ubuntu-latest\n+    runs-on: [ self-hosted, small ]\n \n     steps:\n       - uses: actions/checkout@v2\n-      - name: Set up Python 3.8\n-        uses: actions/setup-python@v2\n+      - name: Set up Python\n+        uses: actions/setup-python@v4\n         with:\n-          python-version: 3.8\n+          python-version-file: .python-version\n       - name: Install pypa/build\n         run: |\n           python -m pip install build --user\n@@ -24,4 +28,4 @@ jobs:\n         if: startsWith(github.ref, 'refs/tags')\n         uses: pypa/gh-action-pypi-publish@master\n         with:\n-          password: ${{ secrets.SECRETS_TEST_PYPI_API_TOKEN }}\n\\ No newline at end of file\n+          password: ${{ secrets.SECRETS_TEST_PYPI_API_TOKEN }}\ndiff --git a/.github/workflows/sort_whitespaces.yml b/.github/workflows/sort_whitespaces.yml\nnew file mode 100644\nindex 00000000..ecbc7a64\n--- /dev/null\n+++ b/.github/workflows/sort_whitespaces.yml\n@@ -0,0 +1,39 @@\n+name: Sort whitespaces\n+\n+on:\n+  push:\n+    branches:\n+      - main\n+      - develop\n+    paths:\n+      - 'whitelist.txt'\n+      - 'hyperstyle/src/python/review/inspectors/flake8/whitelist.txt'\n+  pull_request:\n+    paths:\n+      - 'whitelist.txt'\n+      - 'hyperstyle/src/python/review/inspectors/flake8/whitelist.txt'\n+\n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.ref }}\n+  cancel-in-progress: true\n+\n+jobs:\n+  sort_whitespaces:\n+    name: Sort whitespaces\n+    runs-on: [ self-hosted, small ]\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Sort whitelists\n+        run: |\n+          for file in \"whitelist.txt\" \"hyperstyle/src/python/review/inspectors/flake8/whitelist.txt\"\n+          do\n+          LC_ALL=C sort $file -o $file\n+          done\n+\n+      - name: Commit sorted whitelists\n+        uses: EndBug/add-and-commit@v7.2.1\n+        with:\n+          add: \"['whitelist.txt', 'hyperstyle/src/python/review/inspectors/flake8/whitelist.txt']\"\n+          message: 'Sort whitelists (Github Actions)'\ndiff --git a/.gitignore b/.gitignore\nindex 63d25e2b..981f14ba 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -77,3 +77,11 @@ __pycache__/\n /.intellij_inspector/out/\n \n lightweight.Dockerfile\n+linters\n+\n+node_modules\n+.eslintrc.js\n+package-lock.json\n+\n+hyperstyle/src/python/review/inspectors/common/inspector/proto/model_pb2*\n+.env\ndiff --git a/.python-version b/.python-version\nnew file mode 100644\nindex 00000000..1445aee8\n--- /dev/null\n+++ b/.python-version\n@@ -0,0 +1,1 @@\n+3.10.14\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex ce90bf0e..341e2ddb 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -10,9 +10,9 @@ In this document you will find a detailed description of how to add new function\n ## Adding a new inspector for an existing language\n \n If you want to add a new inspector for an existing language, you need to:\n-1. Inherit from the [`BaseInspector`](hyperstyle/src/python/review/inspectors/base_inspector.py#L9) class.\n-2. Define the [`inspector_type`](hyperstyle/src/python/review/inspectors/base_inspector.py#L29) property. To do this, you need to update the [`InspectorType`](hyperstyle/src/python/review/inspectors/inspector_type.py#L6) class by adding there a name of the inspector.\n-3. Implement the [`inspect`](hyperstyle/src/python/review/inspectors/base_inspector.py#L33) function, which should run an analysis of the language and return a list of found issues;\n+1. Inherit from the [`BaseInspector`](hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py#L9) class.\n+2. Define the [`inspector_type`](hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py#L29) property. To do this, you need to update the [`InspectorType`](hyperstyle/src/python/review/inspectors/common/inspector/inspector_type.py#L6) class by adding there a name of the inspector.\n+3. Implement the [`inspect`](hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py#L33) function, which should run an analysis of the language and return a list of found issues;\n \n Usually the inspector runs a third-party linter through a command line and parses its result. In this case it will be useful to implement several auxiliary functions:\n 1. `create_command` \u2013 creates a command to run the linter via subprocess.\n@@ -31,9 +31,9 @@ If you are implementing the inspector that uses the third-party linter, you must\n \n ### Implementation of the `parse` function\n \n-Usually, the `parse` function parses the result of the third-party linter line-by-line, then creates a base issue using the [`BaseIssue`](hyperstyle/src/python/review/inspectors/issue.py#L199) dataclass, which is later converted to either [`CodeIssue`](hyperstyle/src/python/review/inspectors/issue.py#L217) or one of the measurable issues using the [`convert_base_issue`](hyperstyle/src/python/review/inspectors/common/base_issue_converter.py#L17) function and an instance of the [`IssueConigsHandler`](hyperstyle/src/python/review/inspectors/issue_configs.py#L117) class. The resulting issue is added to the general list of found issues and this list is returned from the function after the parsing is finished.\n+Usually, the `parse` function parses the result of the third-party linter line-by-line, then creates a base issue using the [`BaseIssue`](hyperstyle/src/python/review/inspectors/common/issue/issue.py#L199) dataclass, which is later converted to either [`CodeIssue`](hyperstyle/src/python/review/inspectors/common/issue/issue.py#L217) or one of the measurable issues using the [`convert_base_issue`](hyperstyle/src/python/review/inspectors/common/issue/base_issue_converter.py#L17) function and an instance of the [`IssueConigsHandler`](hyperstyle/src/python/review/inspectors/common/issue/issue_configs.py#L117) class. The resulting issue is added to the general list of found issues and this list is returned from the function after the parsing is finished.\n \n-The `IssueConfigHandler` class handles custom issue descriptions and also parses metrics from their descriptions (examples of metrics are: line or function length, cyclomatic complexity, maintainability index).  It receives instances of [`IssueConfig`](hyperstyle/src/python/review/inspectors/issue_configs.py#L46) or [`MeasurableIssueConfig`](hyperstyle/src/python/review/inspectors/issue_configs.py#L85) classes as input, which should be stored in the `ISSUE_CONFIGS` list next to the inspector.\n+The `IssueConfigHandler` class handles custom issue descriptions and also parses metrics from their descriptions (examples of metrics are: line or function length, cyclomatic complexity, maintainability index).  It receives instances of [`IssueConfig`](hyperstyle/src/python/review/inspectors/common/issue/issue_configs.py#L46) or [`MeasurableIssueConfig`](hyperstyle/src/python/review/inspectors/common/issue/issue_configs.py#L85) classes as input, which should be stored in the `ISSUE_CONFIGS` list next to the inspector.\n \n Also, if the third-party linter supports output in \"Checkstyle\" format, you can use the [`parse_xml_file_result`](hyperstyle/src/python/review/inspectors/common/xml_parser.py#L47) function to parse the output file.\n \n@@ -42,7 +42,7 @@ Also, if the third-party linter supports output in \"Checkstyle\" format, you can\n A sample checklist for adding a new inspector looks like this:\n - [ ] I've inherited from the `BaseInspector` class.\n - [ ] I've implemented the `inspect` function and _if needed_ I've added a check for the existence of third-party linter environment variables using the [`check_set_up_env_variable`](hyperstyle/src/python/review/common/file_system.py#L124) function.\n-- [ ] I've added a new inspector type to the `InspectorType` class, updated the [`available_values`](hyperstyle/src/python/review/inspectors/inspector_type.py#L27) function and defined the inspector type.\n+- [ ] I've added a new inspector type to the `InspectorType` class, updated the [`available_values`](hyperstyle/src/python/review/inspectors/common/inspector/inspector_type.py#L27) function and defined the inspector type.\n - [ ] _If needed_. I've added a config to run the third-party linter.\n - [ ] _If needed_. I've implemented the `create_command` function.\n - [ ] _if needed_. I've implemented the `parse` function and defined the `ISSUES_CONFIGS` list in a separate file.\ndiff --git a/Dockerfile b/Dockerfile\nindex 4ba0e5e6..9cedaea3 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -1,64 +1,79 @@\n-FROM hyperskill.azurecr.io/hyperstyle-base:py3.8.11-java11.0.11-node14.17.3-go1.18.5\n+FROM hyperskill.azurecr.io/hyperstyle-base:py3.10.14-java11.0.11-node14.17.3-go1.18.5\n \n-RUN npm install eslint@7.5.0 -g \\\n-    && eslint --init\n+ENV LINTERS_DIRECTORY=/opt/linters\n \n-COPY . review\n-RUN pip install --no-cache-dir \\\n-    -r review/requirements-test.txt \\\n-    -r review/requirements.txt \\\n-    ./review\n+ENV ESLINT_VERSION=7.5.0\n \n-ENV LINTERS_DIRECTORY      /opt/linters\n+ENV CHECKSTYLE_VERSION=8.44\n+ENV CHECKSTYLE_DIRECTORY=${LINTERS_DIRECTORY}/checkstyle\n \n-ENV CHECKSTYLE_VERSION  8.44\n-ENV CHECKSTYLE_DIRECTORY  ${LINTERS_DIRECTORY}/checkstyle\n+ENV DETEKT_VERSION=1.14.2\n+ENV DETEKT_DIRECTORY=${LINTERS_DIRECTORY}/detekt\n \n-ENV DETEKT_VERSION  1.14.2\n-ENV DETEKT_DIRECTORY  ${LINTERS_DIRECTORY}/detekt\n+ENV PMD_VERSION=6.37.0\n+ENV PMD_DIRECTORY=${LINTERS_DIRECTORY}/pmd\n \n-ENV PMD_VERSION  6.37.0\n-ENV PMD_DIRECTORY  ${LINTERS_DIRECTORY}/pmd\n+ENV GOLANG_LINT_VERSION=1.49.0\n+ENV GOLANG_LINT_DIRECTORY=${LINTERS_DIRECTORY}/golangci-lint\n \n-ENV GOLANG_LINT_VERSION  1.49.0\n-ENV GOLANG_LINT_DIRECTORY  ${LINTERS_DIRECTORY}/golangci-lint\n-\n-RUN mkdir -p ${CHECKSTYLE_DIRECTORY} &&  \\\n-    mkdir -p ${DETEKT_DIRECTORY} &&  \\\n-    mkdir -p ${PMD_DIRECTORY} &&  \\\n-    mkdir -p ${GOLANG_LINT_DIRECTORY}\n+RUN mkdir -p ${CHECKSTYLE_DIRECTORY} \\\n+  && mkdir -p ${DETEKT_DIRECTORY} \\\n+  && mkdir -p ${PMD_DIRECTORY} \\\n+  && mkdir -p ${GOLANG_LINT_DIRECTORY}\n \n # Install Curl and Unzip\n-RUN apt -y update &&  \\\n-    apt -y upgrade && \\\n-    apt -y install curl unzip\n+RUN apt-get update \\\n+  && apt-get install -y --no-install-recommends curl unzip ca-certificates \\\n+  && rm -rf /var/lib/apt/lists/*\n+\n+# Install eslint\n+RUN npm install eslint@${ESLINT_VERSION} -g \\\n+  && eslint --init\n \n # Install Detekt and Detekt-formatting\n RUN curl -sSLO https://github.com/detekt/detekt/releases/download/v${DETEKT_VERSION}/detekt-cli-${DETEKT_VERSION}.zip \\\n-    && unzip detekt-cli-${DETEKT_VERSION}.zip -d ${DETEKT_DIRECTORY} \\\n-    &&  curl -H \"Accept: application/zip\" https://repo.maven.apache.org/maven2/io/gitlab/arturbosch/detekt/detekt-formatting/${DETEKT_VERSION}/detekt-formatting-${DETEKT_VERSION}.jar -o ${DETEKT_DIRECTORY}/detekt-formatting-${DETEKT_VERSION}.jar\n+  && unzip detekt-cli-${DETEKT_VERSION}.zip -d ${DETEKT_DIRECTORY} \\\n+  && rm detekt-cli-${DETEKT_VERSION}.zip \\\n+  && curl -H \"Accept: application/zip\" https://repo.maven.apache.org/maven2/io/gitlab/arturbosch/detekt/detekt-formatting/${DETEKT_VERSION}/detekt-formatting-${DETEKT_VERSION}.jar -o ${DETEKT_DIRECTORY}/detekt-formatting-${DETEKT_VERSION}.jar\n \n # Install Checkstyle\n RUN curl -L https://github.com/checkstyle/checkstyle/releases/download/checkstyle-${CHECKSTYLE_VERSION}/checkstyle-${CHECKSTYLE_VERSION}-all.jar > ${CHECKSTYLE_DIRECTORY}/checkstyle-${CHECKSTYLE_VERSION}-all.jar\n \n # Install PMD\n RUN curl -sSLO https://github.com/pmd/pmd/releases/download/pmd_releases/${PMD_VERSION}/pmd-bin-${PMD_VERSION}.zip \\\n-    && unzip pmd-bin-${PMD_VERSION}.zip -d ${PMD_DIRECTORY}\n+  && unzip pmd-bin-${PMD_VERSION}.zip -d ${PMD_DIRECTORY} \\\n+  && rm pmd-bin-${PMD_VERSION}.zip\n \n # Install golangci-lint\n-RUN curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh  \\\n-    | sh -s -- -b ${GOLANG_LINT_DIRECTORY} v${GOLANG_LINT_VERSION}\n+RUN curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh \\\n+  | sh -s -- -b ${GOLANG_LINT_DIRECTORY} v${GOLANG_LINT_VERSION}\n \n # Install third party golang libraries and pre-cache them by compiling main.go\n # Taken from: https://github.com/StepicOrg/epicbox-images/blob/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/Dockerfile\n-RUN curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/go.mod && \\\n-    curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/go.sum && \\\n-    curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/main.go && \\\n-    go mod download && \\\n-    go mod verify && \\\n-    go mod tidy && \\\n-    go run main.go &&  \\\n-    rm main.go &&  \\\n-    chmod ugo-w go.mod go.sum\n-\n-CMD [\"/bin/bash\"]\n+RUN curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/go.mod \\\n+  && curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/go.sum \\\n+  && curl -sSfLO https://raw.githubusercontent.com/StepicOrg/epicbox-images/a5eadb5211909fc7ef99724ee0b8bf3a758ae1b7/epicbox-go/main.go \\\n+  && go mod download \\\n+  && go mod verify \\\n+  && go mod tidy \\\n+  && go run main.go \\\n+  && rm main.go \\\n+  && chmod ugo-w go.mod go.sum\n+\n+ARG POETRY_VERSION=1.8.3\n+RUN pip install poetry==${POETRY_VERSION} \\\n+  && poetry config virtualenvs.create false \\\n+  && python -m venv /hyperstyle\n+\n+WORKDIR /review\n+\n+COPY pyproject.toml poetry.lock ./\n+RUN . /hyperstyle/bin/activate \\\n+  && poetry install --no-interaction --no-ansi --no-cache --no-root\n+\n+COPY . .\n+\n+RUN PROTO_PATH=\"hyperstyle/src/python/review/inspectors/common/inspector/proto\" \\\n+  && /hyperstyle/bin/python -m grpc_tools.protoc --proto_path=. --python_out=. --pyi_out=. --grpc_python_out=. ${PROTO_PATH}/model.proto\n+\n+CMD [\"/hyperstyle/bin/pytest\"]\ndiff --git a/MANIFEST.in b/MANIFEST.in\ndeleted file mode 100644\nindex bdf4942c..00000000\n--- a/MANIFEST.in\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-include VERSION.md\n-include requirements.txt\n-recursive-exclude __pycache__ *.pyc *.pyo *.orig\n\\ No newline at end of file\ndiff --git a/README.md b/README.md\nindex 22797696..b6f29a35 100644\n--- a/README.md\n+++ b/README.md\n@@ -6,7 +6,7 @@ A tool for running a set of pre-configured linters and evaluating code quality.\n It is used on the [Hyperskill](https://hyperskill.org/) platform \n to check the quality of learners' code.\n \n-[Read more detail about the project at Hyperskill Help Center](https://support.hyperskill.org/hc/en-us/articles/360049582712-Code-style-Code-quality)\n+[Read more details about the project at Hyperskill Help Center](https://support.hyperskill.org/hc/en-us/articles/360049582712-Code-style-Code-quality)\n \n [The dockerized version](https://hub.docker.com/r/stepik/hyperstyle/tags)\n \n@@ -24,13 +24,14 @@ The source code of **hyperstyle** is distributed under the Apache 2.0 License.\n \n The 3rd party software we use in this project has its own licenses.\n \n+\n Python language (all versions can be found in the [requirements.txt](requirements.txt) file):\n \n-- [x]  flake8 [MIT]\n+- [x] flake8 [MIT]\n     * [Site and docs](https://flake8.pycqa.org/en/latest/)\n     * [Repository](https://github.com/PyCQA/flake8)\n   \n-- [x]  Pylint [GNU LGPL v2]\n+- [x] Pylint [GNU LGPL v2]\n     * [Site and docs](https://www.pylint.org/)\n     * [Repository](https://github.com/PyCQA/pylint)\n    \n@@ -38,13 +39,19 @@ Python language (all versions can be found in the [requirements.txt](requirement\n     * [Site and docs](https://radon.readthedocs.io/en/latest/)\n     * [Repository](https://github.com/rubik/radon)\n \n+- [x] Python IJ Inspections [MIT]\n+    * [Site and docs](https://www.jetbrains.com/help/pycharm/disabling-and-enabling-inspections.html)\n+    * [Repository](https://github.com/JetBrains-Research/code-quality-ij-server/tree/master)\n+\n+\n+\n Java language:\n \n-- [x]  PMD [BSD] (Version: 6.37.0)\n+- [x] PMD [BSD] (Version: 6.37.0)\n     * [Site and docs](https://pmd.github.io/)\n     * [Repository](https://github.com/pmd/pmd)\n   \n-- [x]  Checkstyle [GNU LGPL v2.1] (Version: 8.44)\n+- [x] Checkstyle [GNU LGPL v2.1] (Version: 8.44)\n     * [Site and docs](https://checkstyle.sourceforge.io/)\n     * [Repository](https://github.com/checkstyle/checkstyle)\n \n@@ -52,15 +59,18 @@ Java language:\n \n Kotlin language:\n \n-- [x]  Detekt [Apache 2.0] (Version: 1.14.2)\n+- [x] Detekt [Apache 2.0] (Version: 1.14.2)\n     * [Site and docs](https://detekt.github.io/detekt/)\n     * [Repository](https://github.com/detekt/detekt)\n \n+- [x] Kotlin IJ inspections [MIT]\n+    * [Site and docs](https://www.jetbrains.com/help/idea/code-inspection.html)\n+    * [Repository](https://github.com/JetBrains-Research/code-quality-ij-server/tree/master)\n \n \n JavaScript language:\n \n-- [x]  ESlint [MIT] (Version: 7.5.0)\n+- [x] ESlint [MIT] (Version: 7.5.0)\n     * [Site and docs](https://eslint.org/)\n     * [Repository](https://github.com/eslint/eslint)\n   \n@@ -75,7 +85,9 @@ Go language:\n \n ## Installation\n \n-You have to create set of environment variables:\n+### Pre-requirements\n+\n+You have to create a set of environment variables in order to be able to use several linters:\n - `CHECKSTYLE_VERSION` (the value of the variable must be the same with its value in [Dockerfile](Dockerfile))\n - `CHECKSTYLE_DIRECTORY` (the directory with `CHECKSTYLE` linter sources)\n - `DETEKT_VERSION` (the value of the variable must be the same with its value in [Dockerfile](Dockerfile))\n@@ -85,52 +97,97 @@ You have to create set of environment variables:\n - `GOLANG_LINT_VERSION` (the value of the variable must be the same with its value in [Dockerfile](Dockerfile))\n - `GOLANG_LINT_DIRECTORY` (the directory with `GOLANG_LINT` linter sources)\n \n-### Using script\n+### Using pip\n \n-Just run the following command:\n-```bash\n-./setup_environment.sh\n-```\n-and install everything the script suggests. \n+Just run the following commands to install everything you need to run the tool:\n+\n+1. Install the latest version of hyperstyle from [PyPI](https://pypi.org/project/hyperstyle/):\n+   ```bash\n+   pip install hyperstyle\n+   ```\n+   \n+   You could also install a specific version:\n+   ```bash\n+   pip install hyperstyle==<VERSION>\n+   ```\n+   where `<VERSION>` is your versions.\n+   The list of all available versions you could find [here](https://pypi.org/project/hyperstyle/#history).\n+\n+2. Install (or update) linters specified in the environment variables above:\n+   ```bash\n+   curl -sSL https://github.com/hyperskill/hyperstyle/blob/main/setup_environment.sh | bash -\n+   ```\n+   This is necessary because the package does not distribute several third-party linters.\n+\n+   For now the script proposes to install development requirements, you should skip this step.\n+\n+   You can also install linters manually. To do this, please refer to [this](#linter-manual-installation) section.\n+\n+### Using docker\n+\n+Alternatively, you can build a docker image by [Dockerfile](Dockerfile) and run the tool inside this image.\n+Or use the public docker image, that we use in the [build.yml](.github/workflows/build.yml) file.\n \n-**Note**: You can also use this script to update linters. To do this, just update the corresponding \n-linter version variables, run the script, and reinstall only the necessary linters. \n+### Manually (for development purposes)\n \n-### Manually\n+To set up a development environment, you need to run the following commands:\n \n-If you don't want to use the script, you can install the environment manually.\n+1. Download the repository:\n+   ```bash\n+   git clone https://github.com/hyperskill/hyperstyle.git && cd hyperstyle\n+   ```\n \n-Simply clone the repository and run the following commands:\n+2. Install a virtual environment:\n+   ```bash\n+   python3 -m venv venv && source venv/bin/activate\n+   ```\n \n-1. `pip install -r requirements.txt`\n-2. `pip install -r requirements-test.txt` for tests\n-3. `npm install eslint@7.5.0 -g && eslint --init`\n+3. Install development requirements and install (or update) linters specified in the environment variables above:\n+   ```bash\n+   ./setup_environment.sh\n+   ```\n+\n+   It will install all dependencies from the [requirements-dev.txt](requirements-dev.txt), \n+   which contains runtime, test and build dependencies.   \n+\n+   You can also install linters manually. To do this, please refer to [this](#linter-manual-installation) section.\n+\n+### Linter manual installation\n+\n+You can download all linters' sources by the following commands:\n+\n+- `ESLINT`:\n+  ```bash\n+  npm install eslint@7.5.0 -g && eslint --init\n+  ```\n \n-You can download all linters sources manually or by the following commands:\n - `CHECKSTYLE`: \n-```bash\n-curl -L https://github.com/checkstyle/checkstyle/releases/download/checkstyle-${CHECKSTYLE_VERSION}/checkstyle-${CHECKSTYLE_VERSION}-all.jar > ${CHECKSTYLE_DIRECTORY}/checkstyle-${CHECKSTYLE_VERSION}-all.jar\n-```\n+  ```bash\n+  curl -L https://github.com/checkstyle/checkstyle/releases/download/checkstyle-${CHECKSTYLE_VERSION}/checkstyle-${CHECKSTYLE_VERSION}-all.jar > ${CHECKSTYLE_DIRECTORY}/checkstyle-${CHECKSTYLE_VERSION}-all.jar\n+  ```\n+\n - `DETEKT`: \n-```bash\n-curl -sSLO https://github.com/detekt/detekt/releases/download/v${DETEKT_VERSION}/detekt-cli-${DETEKT_VERSION}.zip \\\n-&& unzip detekt-cli-${DETEKT_VERSION}.zip -d ${DETEKT_DIRECTORY} \\\n-&&  curl -H \"Accept: application/zip\" https://repo.maven.apache.org/maven2/io/gitlab/arturbosch/detekt/detekt-formatting/${DETEKT_VERSION}/detekt-formatting-${DETEKT_VERSION}.jar -o ${DETEKT_DIRECTORY}/detekt-formatting-${DETEKT_VERSION}.jar\n-```\n+  ```bash\n+  curl -sSLO https://github.com/detekt/detekt/releases/download/v${DETEKT_VERSION}/detekt-cli-${DETEKT_VERSION}.zip \\\n+  && unzip detekt-cli-${DETEKT_VERSION}.zip -d ${DETEKT_DIRECTORY} \\\n+  &&  curl -H \"Accept: application/zip\" https://repo.maven.apache.org/maven2/io/gitlab/arturbosch/detekt/detekt-formatting/${DETEKT_VERSION}/detekt-formatting-${DETEKT_VERSION}.jar -o ${DETEKT_DIRECTORY}/detekt-formatting-${DETEKT_VERSION}.jar\n+  ```\n+\n - `PMD`: \n-```bash\n-curl -sSLO https://github.com/pmd/pmd/releases/download/pmd_releases/${PMD_VERSION}/pmd-bin-${PMD_VERSION}.zip \\\n-&& unzip pmd-bin-${PMD_VERSION}.zip -d ${PMD_DIRECTORY}\n-```\n-- `GOLANG_LINT`:\n-```bash\n-curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b ${GOLANG_LINT_DIRECTORY} v${GOLANG_LINT_VERSION}\n-```\n+  ```bash\n+  curl -sSLO https://github.com/pmd/pmd/releases/download/pmd_releases/${PMD_VERSION}/pmd-bin-${PMD_VERSION}.zip \\\n+  && unzip pmd-bin-${PMD_VERSION}.zip -d ${PMD_DIRECTORY}\n+  ```\n \n-### Using docker\n+- `GOLANG_LINT`:\n+  ```bash\n+  curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b ${GOLANG_LINT_DIRECTORY} v${GOLANG_LINT_VERSION}\n+  ```\n \n-Alternatively, you can build a docker image by [Dockerfile](Dockerfile) and run the tool inside this image.\n-Or use the public docker image, that we use in the [build.yml](.github/workflows/build.yml) file.\n+- IJ-based linters:\n+  ```bash\n+  python3 -m grpc_tools.protoc --proto_path=. --python_out=. --pyi_out=. --grpc_python_out=. hyperstyle/src/python/review/inspectors/common/inspector/proto/model.proto\n+  ```\n \n ## Usage\n \n@@ -143,22 +200,23 @@ A simple configuration: `python run_tool.py <path>`.\n \n Optional arguments:\n \n-Argument | Description\n---- | ---\n-**&#8209;h**, **&#8209;&#8209;help**      |  show the help message and exit.\n-**&#8209;v**, **&#8209;&#8209;verbosity** |  choose logging level according [this](https://docs.python.org/3/library/logging.html#levels) list: `1` - **ERROR**; `2` - **INFO**; `3` - **DEBUG**; `0` - disable logging (**CRITICAL** value); default value is `0` (**CRITICAL**).\n-**&#8209;d**, **&#8209;&#8209;disable**   |  disable inspectors. Available values: for **Python** language: `pylint` for [Pylint](https://github.com/PyCQA/pylint), `flake8` for [flake8](https://flake8.pycqa.org/en/latest/), `radon` for [Radon](https://radon.readthedocs.io/en/latest/), `python_ast` to check different measures providing by AST; for **Java** language: `checkstyle` for the [Checkstyle](https://checkstyle.sourceforge.io/), `pmd` for [PMD](https://pmd.github.io/); for `Kotlin` language: detekt for [Detekt](https://detekt.github.io/detekt/); for **JavaScript** language: `eslint` for [ESlint](https://eslint.org/); for **Go** language: `golang_lint` for [golangci-lint](https://golangci-lint.run/). Example: `-d pylint,flake8`.\n-**&#8209;&#8209;allow-duplicates**        |  allow duplicate issues found by different linters. By default, duplicates are skipped.\n-**&#8209;&#8209;language-version**, **&#8209;&#8209;language_version**  |  specify the language version for JAVA inspectors. Available values: `java7`, `java8`, `java9`, `java11`, `java15`, `java17`. **Note**: **&#8209;&#8209;language_version** is deprecated and will be deleted in the future.\n-**&#8209;&#8209;n-cpu**, **&#8209;&#8209;n_cpu**  |  specify number of _cpu_ that can be used to run inspectors. **Note**: **&#8209;&#8209;n_cpu** is deprecated. Will be deleted in the future.\n-**&#8209;f**, **&#8209;&#8209;format**    |  the output format. Available values: `json`, `text`. Default value is `json`.\n-**&#8209;s**, **&#8209;&#8209;start-line**|  the first line to be analyzed. By default it starts from `1`.\n-**&#8209;e**, **&#8209;&#8209;end-line**  |  the end line to be analyzed. The default value is `None`, which meant to handle file by the end.\n-**&#8209;&#8209;new-format**              |  the argument determines whether the tool should use the _new format_. _New format_ means separating the result by the files to allow getting quality and observed issues for each file separately. The default value is `False`.\n-**&#8209;&#8209;history**                 |  JSON string with a list of issues for each language. For each issue its class and quantity are specified. Example: `--history \"{\\\"python\\\": [{\\\"origin_class\\\": \\\"SC200\\\", \\\"number\\\": 20}, {\\\"origin_class\\\": \\\"WPS314\\\", \\\"number\\\": 3}]}\"`\n-**&#8209;&#8209;with&#8209;all&#8209;categories** | Without this flag, all issues will be categorized into 5 main categories: `CODE_STYLE`, `BEST_PRACTICES`, `ERROR_PRONE`, `COMPLEXITY`, `INFO`.\n-**&#8209;&#8209;group&#8209;by&#8209;difficulty** | With this flag, the final grade and influence on penalty will be grouped by the issue difficulty.\n-**&#8209;&#8209;language** | Specify the language to inspect. The tool will check all languages by default. The default value is `None`.\n+| Argument                                                               | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n+|------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| **&#8209;h**, **&#8209;&#8209;help**                                   | show the help message and exit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n+| **&#8209;v**, **&#8209;&#8209;verbosity**                              | choose logging level according [this](https://docs.python.org/3/library/logging.html#levels) list: `1` - **ERROR**; `2` - **INFO**; `3` - **DEBUG**; `0` - disable logging (**CRITICAL** value); default value is `0` (**CRITICAL**).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n+| **&#8209;d**, **&#8209;&#8209;disable**                                | disable inspectors. Available values: for **Python** language: `pylint` for [Pylint](https://github.com/PyCQA/pylint), `flake8` for [flake8](https://flake8.pycqa.org/en/latest/), `radon` for [Radon](https://radon.readthedocs.io/en/latest/), `python_ast` to check different measures providing by AST, `ij-python` for IJ inspections; for **Java** language: `checkstyle` for the [Checkstyle](https://checkstyle.sourceforge.io/), `pmd` for [PMD](https://pmd.github.io/); for **Kotlin** language: `detekt` for [Detekt](https://detekt.github.io/detekt/), `ij-kotlin` for IJ inspections; for **JavaScript** language: `eslint` for [ESlint](https://eslint.org/); for **Go** language: `golang_lint` for [golangci-lint](https://golangci-lint.run/). Example: `-d pylint,flake8`. |\n+| **&#8209;&#8209;allow-duplicates**                                     | allow duplicate issues found by different linters. By default, duplicates are skipped.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n+| **&#8209;&#8209;language-version**, **&#8209;&#8209;language_version** | specify the language version for JAVA inspectors. Available values: `java7`, `java8`, `java9`, `java11`, `java15`, `java17`. **Note**: **&#8209;&#8209;language_version** is deprecated and will be deleted in the future.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n+| **&#8209;&#8209;n-cpu**, **&#8209;&#8209;n_cpu**                       | specify number of _cpu_ that can be used to run inspectors. **Note**: **&#8209;&#8209;n_cpu** is deprecated. Will be deleted in the future.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n+| **&#8209;f**, **&#8209;&#8209;format**                                 | the output format. Available values: `json`, `text`. Default value is `json`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+| **&#8209;s**, **&#8209;&#8209;start-line**                             | the first line to be analyzed. By default it starts from `1`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+| **&#8209;e**, **&#8209;&#8209;end-line**                               | the end line to be analyzed. The default value is `None`, which meant to handle file by the end.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n+| **&#8209;&#8209;new-format**                                           | the argument determines whether the tool should use the _new format_. _New format_ means separating the result by the files to allow getting quality and observed issues for each file separately. The default value is `False`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n+| **&#8209;&#8209;history**                                              | JSON string with a list of issues for each language. For each issue its class and quantity are specified. Example: `--history \"{\\\"python\\\": [{\\\"origin_class\\\": \\\"SC200\\\", \\\"number\\\": 20}, {\\\"origin_class\\\": \\\"WPS314\\\", \\\"number\\\": 3}]}\"`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+| **&#8209;&#8209;with&#8209;all&#8209;categories**                      | Without this flag, all issues will be categorized into 5 main categories: `CODE_STYLE`, `BEST_PRACTICES`, `ERROR_PRONE`, `COMPLEXITY`, `INFO`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n+| **&#8209;&#8209;group&#8209;by&#8209;difficulty**                      | With this flag, the final grade and influence on penalty will be grouped by the issue difficulty.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n+| **&#8209;&#8209;language**                                             | Specify the language to inspect. The tool will check all languages by default. The default value is `None`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n+| **&#8209;&#8209;ij&#8209;config**                                      | JSON string containing information for setting up a connection to the IJ server for each language to be analyzed with the IJ inspector. Example: `--ij-config \"{\\\"python\\\": {\\\"host\\\": \\\"localhost\\\", \\\"port\\\": 8080}, \\\"kotlin\\\": {\\\"host\\\": \\\"localhost\\\", \\\"port\\\": 8081}}\"`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n \n The output examples:\n \ndiff --git a/VERSION.md b/VERSION.md\ndeleted file mode 100644\nindex 1c99cf0e..00000000\n--- a/VERSION.md\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-1.4.4\ndiff --git a/base.Dockerfile b/base.Dockerfile\nindex a7d6c6bb..a9d7c75b 100644\n--- a/base.Dockerfile\n+++ b/base.Dockerfile\n@@ -1,4 +1,4 @@\n-FROM python:3.12.5-slim\n+FROM python:3.10.14-slim\n \n #########\n # Taken from https://github.com/docker-library/openjdk/blob/608f26c5ea63ca34070b439c904cb94a30f6b0c1/11/jdk/slim-buster/Dockerfile\ndiff --git a/hyperstyle/src/python/common/tool_arguments.py b/hyperstyle/src/python/common/tool_arguments.py\nindex f258bb11..e4455bb3 100644\n--- a/hyperstyle/src/python/common/tool_arguments.py\n+++ b/hyperstyle/src/python/common/tool_arguments.py\n@@ -4,7 +4,7 @@\n \n from hyperstyle.src.python.review.common.language import Language\n from hyperstyle.src.python.review.common.language_version import LanguageVersion\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n \n \n @unique\n@@ -80,7 +80,7 @@ class RunToolArgument(Enum):\n                                'should use the new format')\n \n     HISTORY = ArgumentsInfo(None, '--history',\n-                            'Json string, which contains lists of issues in the previous submissions '\n+                            'JSON string, which contains lists of issues in the previous submissions '\n                             'for other tasks for one user.')\n \n     WITH_ALL_CATEGORIES = ArgumentsInfo(None, '--with-all-categories',\n@@ -89,3 +89,11 @@ class RunToolArgument(Enum):\n \n     GROUP_BY_DIFFICULTY = ArgumentsInfo(None, '--group-by-difficulty',\n                                         'With this flag, the final grade will be grouped by the issue difficulty.')\n+\n+    IJ_CONFIG = ArgumentsInfo(\n+        None,\n+        '--ij-config',\n+        'JSON string containing information for setting up a connection to the IJ server '\n+        'for each language to be analyzed with the IJ inspector. '\n+        'It should be a dictionary of dictionaries where for each language host and port are specified.',\n+    )\ndiff --git a/hyperstyle/src/python/review/application_config.py b/hyperstyle/src/python/review/application_config.py\nindex 79514c3f..f8315a10 100644\n--- a/hyperstyle/src/python/review/application_config.py\n+++ b/hyperstyle/src/python/review/application_config.py\n@@ -2,7 +2,7 @@\n from typing import Optional, Set\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n \n \n @dataclass\n@@ -18,6 +18,7 @@ class ApplicationConfig:\n     new_format: bool = False\n     history: Optional[str] = None\n     group_by_difficulty: bool = False\n+    ij_config: Optional[str] = None\n \n     @staticmethod\n     def get_default_config() -> 'ApplicationConfig':\ndiff --git a/hyperstyle/src/python/review/common/parallel_runner.py b/hyperstyle/src/python/review/common/parallel_runner.py\nindex 7b6a7e45..e0033eb3 100644\n--- a/hyperstyle/src/python/review/common/parallel_runner.py\n+++ b/hyperstyle/src/python/review/common/parallel_runner.py\n@@ -6,8 +6,8 @@\n from typing import Any, Callable, Dict, List\n \n from hyperstyle.src.python.review.application_config import ApplicationConfig\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue\n \n logger = logging.getLogger(__name__)\n \n@@ -39,11 +39,11 @@ def inspect_in_parallel(inspector_runner: Callable[[Any, ApplicationConfig, Base\n                         data: Any,\n                         config: ApplicationConfig,\n                         inspectors: List[BaseInspector]) -> List[BaseIssue]:\n-    inspectors = filter(lambda i: i.inspector_type not in config.disabled_inspectors, inspectors)\n+    inspectors_to_run = filter(lambda i: i.inspector_type not in config.disabled_inspectors, inspectors)\n \n     if config.n_cpu == 1:\n         issues = []\n-        for inspector in inspectors:\n+        for inspector in inspectors_to_run:\n             inspector_issues = inspector_runner(data, config, inspector)\n             issues.extend(inspector_issues)\n         return issues\n@@ -51,7 +51,7 @@ def inspect_in_parallel(inspector_runner: Callable[[Any, ApplicationConfig, Base\n     with multiprocessing.Pool(config.n_cpu) as pool:\n         issues = pool.map(\n             functools.partial(inspector_runner, data, config),\n-            inspectors,\n+            inspectors_to_run,\n         )\n \n     return list(itertools.chain(*issues))\ndiff --git a/hyperstyle/src/python/review/inspectors/base_inspector.py b/hyperstyle/src/python/review/inspectors/base_inspector.py\ndeleted file mode 100644\nindex 401ef6a4..00000000\n--- a/hyperstyle/src/python/review/inspectors/base_inspector.py\n+++ /dev/null\n@@ -1,41 +0,0 @@\n-import abc\n-from pathlib import Path\n-from typing import Any, Dict, List\n-\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue\n-\n-\n-class BaseInspector(abc.ABC):\n-    \"\"\"\n-    Each external inspector contains a dictionary in which the IssueType corresponds to the original linter classes.\n-    The dictionary helps to categorize errors during parsing the linters' output.\n-\n-    To add a new inspector, you need:\n-     - to create a class that inherits from the BaseInspector class,\n-     - define the type of inspector (the type filed) by adding a new option in the InspectorType,\n-     - implement the <inspect >function.\n-\n-    Typically, the <inspect> function launches a linter and parses its output (XML or JSON) to get a list of BaseIssue.\n-\n-    Also, if you need to launch inspectors in memory (without using/creating a file with code, you need to implement\n-    <inspect_in_memory> function.\n-\n-    Some inspectors (internal) do not require creating a dictionary with IssueType.\n-    This is connected to the fact that they do not launch an additional analysis tool and work with the code directly,\n-    for example, the python AST inspector.\n-    \"\"\"\n-\n-    # Type of inspection for analyzing, e.g. pylint, detekt and etc\n-    @property\n-    @abc.abstractmethod\n-    def inspector_type(self) -> InspectorType:\n-        raise NotImplementedError('inspector_type property not implemented yet')\n-\n-    @abc.abstractmethod\n-    def inspect(self, path: Path, config: Dict[str, Any]) -> List[BaseIssue]:\n-        raise NotImplementedError('inspect method not implemented yet')\n-\n-    @abc.abstractmethod\n-    def inspect_in_memory(self, code: str, config: Dict[str, Any]) -> List[BaseIssue]:\n-        raise NotImplementedError('inspect in memory method not implemented yet')\ndiff --git a/hyperstyle/src/python/review/inspectors/checkstyle/checkstyle.py b/hyperstyle/src/python/review/inspectors/checkstyle/checkstyle.py\nindex e2d726c8..1c83ab45 100644\n--- a/hyperstyle/src/python/review/inspectors/checkstyle/checkstyle.py\n+++ b/hyperstyle/src/python/review/inspectors/checkstyle/checkstyle.py\n@@ -5,13 +5,13 @@\n \n from hyperstyle.src.python.review.common.file_system import check_set_up_env_variable, new_temp_dir\n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n from hyperstyle.src.python.review.inspectors.checkstyle.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.checkstyle.issue_types import CHECK_CLASS_NAME_TO_ISSUE_TYPE\n from hyperstyle.src.python.review.inspectors.common.xml_parser import parse_xml_file_result\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n logger = logging.getLogger(__name__)\n \ndiff --git a/hyperstyle/src/python/review/inspectors/checkstyle/files/config.xml b/hyperstyle/src/python/review/inspectors/checkstyle/files/config.xml\nindex f49b5267..2018949b 100644\n--- a/hyperstyle/src/python/review/inspectors/checkstyle/files/config.xml\n+++ b/hyperstyle/src/python/review/inspectors/checkstyle/files/config.xml\n@@ -6,8 +6,9 @@\n <module name=\"Checker\">\n     <property name=\"fileExtensions\" value=\"java\"/>\n \n-    <!-- This config was created using the following table: https://bit.ly/3BbxmD5 -->\n-\n+    <!-- This config was created using the following table: -->\n+    <!-- https://docs.google.com/spreadsheets/d/1G3NOpo9PzZcL0IsnSlxIHIWcSZWIX0km -->\n+        \n     <!-- Size Violations -->\n     <module name=\"LineLength\">\n         <property name=\"max\" value=\"120\"/>\ndiff --git a/hyperstyle/src/python/review/inspectors/checkstyle/issue_configs.py b/hyperstyle/src/python/review/inspectors/checkstyle/issue_configs.py\nindex ddbc1262..d76f4590 100644\n--- a/hyperstyle/src/python/review/inspectors/checkstyle/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/checkstyle/issue_configs.py\n@@ -1,13 +1,13 @@\n import re\n \n-from hyperstyle.src.python.review.inspectors.common.tips import (\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import (\n     get_bool_expr_len_tip,\n     get_cyclomatic_complexity_tip,\n     get_func_len_tip,\n     get_line_len_tip,\n     get_magic_number_tip,\n )\n-from hyperstyle.src.python.review.inspectors.issue_configs import (\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import (\n     IssueConfig,\n     IssueDescriptionParser,\n     MeasurableIssueConfig,\ndiff --git a/hyperstyle/src/python/review/inspectors/checkstyle/issue_types.py b/hyperstyle/src/python/review/inspectors/checkstyle/issue_types.py\nindex 604d8a2d..61941556 100644\n--- a/hyperstyle/src/python/review/inspectors/checkstyle/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/checkstyle/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n CHECK_CLASS_NAME_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # ---- Annotations ----\ndiff --git a/hyperstyle/src/python/review/inspectors/common/inspector/__init__.py b/hyperstyle/src/python/review/inspectors/common/inspector/__init__.py\nnew file mode 100644\nindex 00000000..e69de29b\ndiff --git a/hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py b/hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py\nnew file mode 100644\nindex 00000000..fd669327\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/common/inspector/base_inspector.py\n@@ -0,0 +1,154 @@\n+import logging\n+from abc import ABC, abstractmethod\n+from pathlib import Path\n+from typing import Any, Dict, List\n+\n+from hyperstyle.src.python.review.common.file_system import get_content_from_file\n+from hyperstyle.src.python.review.inspectors.common.inspector.ij_client import IJClient\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.proto import model_pb2\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfig, IssueConfigsHandler\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BaseInspector(ABC):\n+    \"\"\"\n+    Each external inspector contains a dictionary in which the IssueType corresponds to the original linter classes.\n+    The dictionary helps to categorize errors during parsing the linters' output.\n+\n+    To add a new inspector, you need:\n+     - to create a class that inherits from the BaseInspector class,\n+     - define the type of inspector (the type filed) by adding a new option in the InspectorType,\n+     - implement the <inspect >function.\n+\n+    Typically, the <inspect> function launches a linter and parses its output (XML or JSON) to get a list of BaseIssue.\n+\n+    Also, if you need to launch inspectors in memory (without using/creating a file with code, you need to implement\n+    <inspect_in_memory> function.\n+\n+    Some inspectors (internal) do not require creating a dictionary with IssueType.\n+    This is connected to the fact that they do not launch an additional analysis tool and work with the code directly,\n+    for example, the python AST inspector.\n+    \"\"\"\n+\n+    # Type of inspection for analyzing, e.g. pylint, detekt and etc\n+    @property\n+    @abstractmethod\n+    def inspector_type(self) -> InspectorType:\n+        raise NotImplementedError('inspector_type property not implemented yet')\n+\n+    @abstractmethod\n+    def inspect(self, path: Path, config: Dict[str, Any]) -> List[BaseIssue]:\n+        raise NotImplementedError('inspect method not implemented yet')\n+\n+    @abstractmethod\n+    def inspect_in_memory(self, code: str, config: Dict[str, Any]) -> List[BaseIssue]:\n+        raise NotImplementedError('inspect in memory method not implemented yet')\n+\n+\n+class BaseIJInspector(BaseInspector):\n+    \"\"\"\n+    Base class for every IJ-based inspector.\n+\n+    It inherits from the `BaseInspector` class, so see its documentation for more information.\n+\n+    To implement this kind of inspector, you should additionally specify `language_id` from the `model.proto` file,\n+    specify issue configs and implement the `choose_issue_type` function.\n+\n+    Before running the inspector, you should set up connection parameters\n+    using the `setup_connection_parameters` function.\n+    \"\"\"\n+    host: str\n+    port: int\n+\n+    @property\n+    @abstractmethod\n+    def language_id(self) -> model_pb2.LanguageId:\n+        raise NotImplementedError('language_id property is not implemented yet')\n+\n+    @property\n+    @abstractmethod\n+    def issue_configs(self) -> List[IssueConfig]:\n+        raise NotImplementedError('issue_configs property is not implemented yet')\n+\n+    @property\n+    @abstractmethod\n+    def ij_inspection_to_issue_type(self) -> Dict[str, IssueType]:\n+        raise NotImplementedError('ij_inspection_to_issue_type property is not implemented yet')\n+\n+    @property\n+    @abstractmethod\n+    def ij_message_to_issue_type(self) -> Dict[str, Dict[str, IssueType]]:\n+        raise NotImplementedError('ij_message_to_issue_type property is not implemented yet')\n+\n+    def setup_connection_parameters(self, host: str, port: int):\n+        self.host = host\n+        self.port = port\n+\n+    def inspect(self, path: Path, config: Dict[str, Any]) -> List[BaseIssue]:\n+        code = get_content_from_file(path)\n+        return self._get_inspection_result(code, path)\n+\n+    def inspect_in_memory(self, code: str, config: Dict[str, Any]) -> List[BaseIssue]:\n+        return self._get_inspection_result(code, Path(\"\"))\n+\n+    def convert_to_base_issues(self, inspection_result: model_pb2.InspectionResult, file_path: Path) -> List[BaseIssue]:\n+        base_issues = []\n+        issue_configs_handler = IssueConfigsHandler(*self.issue_configs)\n+        for problem in inspection_result.problems:\n+            issue_type = self.choose_issue_type(problem)\n+            base_issue = BaseIssue(\n+                origin_class=problem.inspector,\n+                type=issue_type,\n+                description=problem.name,\n+                file_path=file_path,\n+                line_no=problem.lineNumber,\n+                column_no=problem.offset,\n+                inspector_type=self.inspector_type,\n+                difficulty=IssueDifficulty.get_by_issue_type(issue_type),\n+            )\n+\n+            issue = convert_base_issue(base_issue, issue_configs_handler)\n+            if issue is None:\n+                logger.error(f'{self.inspector_type.value}: an error occurred during converting a base issue.')\n+                continue\n+\n+            base_issues.append(base_issue)\n+\n+        return base_issues\n+\n+    def _get_inspection_result(self, code_text: str, file_path: Path) -> List[BaseIssue]:\n+        if self.host is None or self.port is None:\n+            raise Exception('Connection parameters is not set up.')\n+\n+        try:\n+            client = IJClient(self.host, self.port)\n+\n+            code = model_pb2.Code()\n+            code.languageId = self.language_id\n+            code.text = code_text\n+\n+            inspection_result = client.inspect(code)\n+\n+            return self.convert_to_base_issues(inspection_result, file_path)\n+\n+        except Exception as e:\n+            # TODO: replace with error when add mock server into tests\n+            logger.info('Inspector failed to connect to code server.', e)\n+            return []\n+\n+    def choose_issue_type(self, problem: model_pb2.Problem) -> IssueType:\n+        if problem.inspector in self.ij_message_to_issue_type:\n+            for key, value in self.ij_message_to_issue_type[problem.inspector].items():\n+                if problem.name in key:\n+                    return value\n+\n+        if problem.inspector in self.ij_inspection_to_issue_type:\n+            return self.ij_inspection_to_issue_type[problem.inspector]\n+\n+        # PEP-8 inspection\n+        return IssueType.CODE_STYLE\ndiff --git a/hyperstyle/src/python/review/inspectors/common/inspector/ij_client.py b/hyperstyle/src/python/review/inspectors/common/inspector/ij_client.py\nnew file mode 100644\nindex 00000000..ff7a0d20\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/common/inspector/ij_client.py\n@@ -0,0 +1,28 @@\n+import grpc\n+\n+from hyperstyle.src.python.review.inspectors.common.inspector.proto import model_pb2, model_pb2_grpc\n+\n+TIMEOUT = 1\n+\n+\n+class IJClient(object):\n+    def __init__(self, host: str = 'localhost', port: int = 8080):\n+        self.host = host\n+        self.port = port\n+\n+        # instantiate a channel\n+        self.channel = grpc.insecure_channel(f'{self.host}:{self.port}')\n+\n+        # bind the client and the server\n+        try:\n+            grpc.channel_ready_future(self.channel).result(timeout=TIMEOUT)\n+        except grpc.FutureTimeoutError:\n+            raise Exception(\"Failed to connect to ij code server\")\n+        else:\n+            self.stub = model_pb2_grpc.CodeInspectionServiceStub(self.channel)\n+\n+    def inspect(self, code: model_pb2.Code) -> model_pb2.InspectionResult:\n+        return self.stub.inspect(code, timeout=TIMEOUT)\n+\n+    def init(self, service: model_pb2.Service) -> model_pb2.InitResult:\n+        return self.stub.init(service, timeout=TIMEOUT)\ndiff --git a/hyperstyle/src/python/review/inspectors/inspector_type.py b/hyperstyle/src/python/review/inspectors/common/inspector/inspector_type.py\nsimilarity index 78%\nrename from hyperstyle/src/python/review/inspectors/inspector_type.py\nrename to hyperstyle/src/python/review/inspectors/common/inspector/inspector_type.py\nindex 5ce892d6..04c1ceee 100644\n--- a/hyperstyle/src/python/review/inspectors/inspector_type.py\n+++ b/hyperstyle/src/python/review/inspectors/common/inspector/inspector_type.py\n@@ -9,6 +9,7 @@ class InspectorType(Enum):\n     PYTHON_AST = 'PYTHON_AST'\n     FLAKE8 = 'FLAKE8'\n     RADON = 'RADON'\n+    IJ_PYTHON = 'IJ_PYTHON'\n \n     # Java language\n     PMD = 'PMD'\n@@ -16,6 +17,7 @@ class InspectorType(Enum):\n \n     # Kotlin language\n     DETEKT = 'DETEKT'\n+    IJ_KOTLIN = 'IJ_KOTLIN'\n \n     # JavaScript language\n     ESLINT = 'ESLINT'\n@@ -25,6 +27,8 @@ class InspectorType(Enum):\n \n     UNDEFINED = 'UNDEFINED'\n     QODANA = 'QODANA'\n+    # TODO: it is used on production for java inspections, remove in the future releases\n+    IJ_OLD = 'INTELLIJ'\n \n     @classmethod\n     def available_values(cls) -> List[str]:\n@@ -34,6 +38,7 @@ def available_values(cls) -> List[str]:\n             cls.FLAKE8.value,\n             cls.PYTHON_AST.value,\n             cls.RADON.value,\n+            cls.IJ_PYTHON.value,\n \n             # Java language\n             cls.PMD.value,\n@@ -41,10 +46,13 @@ def available_values(cls) -> List[str]:\n \n             # Kotlin language\n             cls.DETEKT.value,\n+            cls.IJ_KOTLIN.value,\n \n             # JavaScript language\n             cls.ESLINT.value,\n \n             # Go language\n             cls.GOLANG_LINT.value,\n+\n+            cls.IJ_OLD.value,\n         ]\ndiff --git a/hyperstyle/src/python/review/inspectors/common/inspector/proto/__init__.py b/hyperstyle/src/python/review/inspectors/common/inspector/proto/__init__.py\nnew file mode 100644\nindex 00000000..e69de29b\ndiff --git a/hyperstyle/src/python/review/inspectors/common/inspector/proto/model.proto b/hyperstyle/src/python/review/inspectors/common/inspector/proto/model.proto\nnew file mode 100644\nindex 00000000..4dfb60da\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/common/inspector/proto/model.proto\n@@ -0,0 +1,41 @@\n+syntax = \"proto3\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.jetbrains.research.ij.headless.server\";\n+option java_outer_classname = \"CodeServerProto\";\n+\n+service CodeInspectionService {\n+    rpc inspect (Code) returns (InspectionResult) {};\n+}\n+\n+enum LanguageId {\n+    Python = 0;\n+    kotlin = 1;\n+    Java = 2;\n+}\n+\n+message Code {\n+    string text = 1;\n+    LanguageId languageId = 2;\n+}\n+\n+message Problem {\n+    string name = 1;\n+    string inspector = 2;\n+    uint64 lineNumber = 3;\n+    uint64 offset = 4;\n+    uint64 length = 5;\n+}\n+\n+message InspectionResult {\n+    repeated Problem problems = 1;\n+}\n+\n+message Service {\n+    string name = 1;\n+    LanguageId languageId = 2;\n+}\n+\n+message InitResult {\n+    uint64 status = 1;\n+}\ndiff --git a/hyperstyle/src/python/review/inspectors/common/issue/__init__.py b/hyperstyle/src/python/review/inspectors/common/issue/__init__.py\nnew file mode 100644\nindex 00000000..e69de29b\ndiff --git a/hyperstyle/src/python/review/inspectors/common/base_issue_converter.py b/hyperstyle/src/python/review/inspectors/common/issue/base_issue_converter.py\nsimilarity index 90%\nrename from hyperstyle/src/python/review/inspectors/common/base_issue_converter.py\nrename to hyperstyle/src/python/review/inspectors/common/issue/base_issue_converter.py\nindex d2a3b9cf..d8ceaa62 100644\n--- a/hyperstyle/src/python/review/inspectors/common/base_issue_converter.py\n+++ b/hyperstyle/src/python/review/inspectors/common/issue/base_issue_converter.py\n@@ -1,14 +1,14 @@\n import logging\n from typing import Optional\n \n-from hyperstyle.src.python.review.inspectors.issue import (\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import (\n     BaseIssue,\n     get_issue_class_by_issue_type,\n     get_measure_name_by_measurable_issue_type,\n     IssueData,\n     Measurable,\n )\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n \n logger = logging.getLogger(__name__)\ndiff --git a/hyperstyle/src/python/review/inspectors/issue.py b/hyperstyle/src/python/review/inspectors/common/issue/issue.py\nsimilarity index 99%\nrename from hyperstyle/src/python/review/inspectors/issue.py\nrename to hyperstyle/src/python/review/inspectors/common/issue/issue.py\nindex a8f03bb1..06c509d9 100644\n--- a/hyperstyle/src/python/review/inspectors/issue.py\n+++ b/hyperstyle/src/python/review/inspectors/common/issue/issue.py\n@@ -6,7 +6,7 @@\n from pathlib import Path\n from typing import Any, Dict, List, Type, Union\n \n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n \n logger = logging.getLogger(__name__)\n \ndiff --git a/hyperstyle/src/python/review/inspectors/issue_configs.py b/hyperstyle/src/python/review/inspectors/common/issue/issue_configs.py\nsimilarity index 100%\nrename from hyperstyle/src/python/review/inspectors/issue_configs.py\nrename to hyperstyle/src/python/review/inspectors/common/issue/issue_configs.py\ndiff --git a/hyperstyle/src/python/review/inspectors/common/tips.py b/hyperstyle/src/python/review/inspectors/common/issue/tips.py\nsimilarity index 100%\nrename from hyperstyle/src/python/review/inspectors/common/tips.py\nrename to hyperstyle/src/python/review/inspectors/common/issue/tips.py\ndiff --git a/hyperstyle/src/python/review/inspectors/common/utils.py b/hyperstyle/src/python/review/inspectors/common/utils.py\nindex de633472..981d8e1f 100644\n--- a/hyperstyle/src/python/review/inspectors/common/utils.py\n+++ b/hyperstyle/src/python/review/inspectors/common/utils.py\n@@ -5,7 +5,7 @@\n from typing import List\n \n from hyperstyle.src.python.review.common.file_system import get_content_from_file\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n \n logger = logging.getLogger(__name__)\n \n@@ -37,17 +37,6 @@ def convert_percentage_of_value_to_lack_of_value(percentage_of_value: float) ->\n     return floor(100 - percentage_of_value)\n \n \n-# TODO: When upgrading to python 3.9+, replace it with removeprefix.\n-# See: https://docs.python.org/3.9/library/stdtypes.html#str.removeprefix\n-def remove_prefix(text: str, prefix: str) -> str:\n-    \"\"\"\n-    Removes the prefix if it is present, otherwise returns the original string.\n-    \"\"\"\n-    if text.startswith(prefix):\n-        return text[len(prefix):]\n-    return text\n-\n-\n def _get_format_fields(input_string: str) -> List[str]:\n     \"\"\"\n     Get all format fields from the input string.\ndiff --git a/hyperstyle/src/python/review/inspectors/common/xml_parser.py b/hyperstyle/src/python/review/inspectors/common/xml_parser.py\nindex 7e4dde18..ff4b3fa3 100644\n--- a/hyperstyle/src/python/review/inspectors/common/xml_parser.py\n+++ b/hyperstyle/src/python/review/inspectors/common/xml_parser.py\n@@ -3,11 +3,11 @@\n from typing import Callable, List\n from xml.etree import ElementTree\n \n-from hyperstyle.src.python.review.inspectors.common.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n from hyperstyle.src.python.review.inspectors.common.utils import is_result_file_correct\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n logger = logging.getLogger(__name__)\n \ndiff --git a/hyperstyle/src/python/review/inspectors/detekt/detekt.py b/hyperstyle/src/python/review/inspectors/detekt/detekt.py\nindex c0bcf1a9..3bba971c 100644\n--- a/hyperstyle/src/python/review/inspectors/detekt/detekt.py\n+++ b/hyperstyle/src/python/review/inspectors/detekt/detekt.py\n@@ -5,13 +5,13 @@\n \n from hyperstyle.src.python.review.common.file_system import check_set_up_env_variable, new_temp_dir\n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n from hyperstyle.src.python.review.inspectors.common.xml_parser import parse_xml_file_result\n from hyperstyle.src.python.review.inspectors.detekt.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.detekt.issue_types import DETEKT_CLASS_NAME_TO_ISSUE_TYPE\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n logger = logging.getLogger(__name__)\n \ndiff --git a/hyperstyle/src/python/review/inspectors/detekt/files/detekt-config.yml b/hyperstyle/src/python/review/inspectors/detekt/files/detekt-config.yml\nindex 950bc7b4..fe49c429 100644\n--- a/hyperstyle/src/python/review/inspectors/detekt/files/detekt-config.yml\n+++ b/hyperstyle/src/python/review/inspectors/detekt/files/detekt-config.yml\n@@ -1,5 +1,5 @@\n-# This config was created based on the default config for Detekt 1.18.0\n-# using the following table: https://bit.ly/2V2kCyi\n+# This config was created based on the default config for Detekt 1.18.0 using the following table:\n+# https://docs.google.com/spreadsheets/d/1N6CMqXKW-FnrlevwwD8iO75M1gTjmLO_\n \n # Some inspections have been commented out due to problems with upgrading to version 1.18.0.\n # The current config contains only those inspections that work on version 1.14.2.\ndiff --git a/hyperstyle/src/python/review/inspectors/detekt/issue_configs.py b/hyperstyle/src/python/review/inspectors/detekt/issue_configs.py\nindex 426944ba..813c3a27 100644\n--- a/hyperstyle/src/python/review/inspectors/detekt/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/detekt/issue_configs.py\n@@ -1,12 +1,12 @@\n import re\n \n-from hyperstyle.src.python.review.inspectors.common.tips import (\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import (\n     get_bool_expr_len_tip,\n     get_cyclomatic_complexity_tip,\n     get_func_len_tip,\n     get_magic_number_tip,\n )\n-from hyperstyle.src.python.review.inspectors.issue_configs import (\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import (\n     IssueConfig,\n     IssueDescriptionParser,\n     MeasurableIssueConfig,\ndiff --git a/hyperstyle/src/python/review/inspectors/detekt/issue_types.py b/hyperstyle/src/python/review/inspectors/detekt/issue_types.py\nindex 19b4da80..35b9d07d 100644\n--- a/hyperstyle/src/python/review/inspectors/detekt/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/detekt/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n DETEKT_CLASS_NAME_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # Comments\ndiff --git a/hyperstyle/src/python/review/inspectors/eslint/eslint.py b/hyperstyle/src/python/review/inspectors/eslint/eslint.py\nindex 20e35092..d0557fb3 100644\n--- a/hyperstyle/src/python/review/inspectors/eslint/eslint.py\n+++ b/hyperstyle/src/python/review/inspectors/eslint/eslint.py\n@@ -3,13 +3,13 @@\n \n from hyperstyle.src.python.review.common.file_system import new_temp_dir\n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n from hyperstyle.src.python.review.inspectors.common.xml_parser import parse_xml_file_result\n from hyperstyle.src.python.review.inspectors.eslint.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.eslint.issue_types import ESLINT_CLASS_NAME_TO_ISSUE_TYPE\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n PATH_ESLINT_CONFIG = Path(__file__).parent / '.eslintrc'\n \ndiff --git a/hyperstyle/src/python/review/inspectors/eslint/issue_configs.py b/hyperstyle/src/python/review/inspectors/eslint/issue_configs.py\nindex fe15f326..7ae584a6 100644\n--- a/hyperstyle/src/python/review/inspectors/eslint/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/eslint/issue_configs.py\n@@ -1,7 +1,10 @@\n import re\n \n-from hyperstyle.src.python.review.inspectors.common.tips import get_cyclomatic_complexity_tip\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueDescriptionParser, MeasurableIssueConfig\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import get_cyclomatic_complexity_tip\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import (\n+    IssueDescriptionParser,\n+    MeasurableIssueConfig,\n+)\n \n ISSUE_CONFIGS = [\n     MeasurableIssueConfig(\ndiff --git a/hyperstyle/src/python/review/inspectors/eslint/issue_types.py b/hyperstyle/src/python/review/inspectors/eslint/issue_types.py\nindex db8b082b..1ab776b1 100644\n--- a/hyperstyle/src/python/review/inspectors/eslint/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/eslint/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n ESLINT_CLASS_NAME_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # Possible errors (according to Eslint doc)\ndiff --git a/hyperstyle/src/python/review/inspectors/flake8/flake8.py b/hyperstyle/src/python/review/inspectors/flake8/flake8.py\nindex f63fbe9a..63d5f21d 100644\n--- a/hyperstyle/src/python/review/inspectors/flake8/flake8.py\n+++ b/hyperstyle/src/python/review/inspectors/flake8/flake8.py\n@@ -1,16 +1,17 @@\n import logging\n import re\n+import sys\n from pathlib import Path\n from typing import Any, Dict, List\n \n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n from hyperstyle.src.python.review.inspectors.flake8.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.flake8.issue_types import CODE_PREFIX_TO_ISSUE_TYPE, CODE_TO_ISSUE_TYPE\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n logger = logging.getLogger(__name__)\n \n@@ -22,6 +23,7 @@\n FORMAT = '%(path)s:%(row)d:%(col)d:%(code)s:%(text)s'\n INSPECTOR_NAME = 'flake8'\n BASE_COMMAND = [\n+    sys.executable, '-m',\n     'flake8',\n     f'--format={FORMAT}',\n     f'--config={PATH_FLAKE8_CONFIG}',\ndiff --git a/hyperstyle/src/python/review/inspectors/flake8/issue_configs.py b/hyperstyle/src/python/review/inspectors/flake8/issue_configs.py\nindex ee721f76..9117e44c 100644\n--- a/hyperstyle/src/python/review/inspectors/flake8/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/flake8/issue_configs.py\n@@ -1,6 +1,6 @@\n import re\n \n-from hyperstyle.src.python.review.inspectors.common.tips import (\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import (\n     get_augmented_assign_pattern_tip,\n     get_cohesion_tip,\n     get_cyclomatic_complexity_tip,\n@@ -8,7 +8,7 @@\n     get_magic_number_tip,\n )\n from hyperstyle.src.python.review.inspectors.common.utils import convert_percentage_of_value_to_lack_of_value\n-from hyperstyle.src.python.review.inspectors.issue_configs import (\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import (\n     IssueConfig,\n     IssueDescriptionParser,\n     MeasurableIssueConfig,\ndiff --git a/hyperstyle/src/python/review/inspectors/flake8/issue_types.py b/hyperstyle/src/python/review/inspectors/flake8/issue_types.py\nindex d3e549d3..f3cf4a48 100644\n--- a/hyperstyle/src/python/review/inspectors/flake8/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/flake8/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n CODE_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # Line length\n@@ -37,7 +37,8 @@\n     # Cohesion\n     'H601': IssueType.COHESION,\n \n-    # The categorization for WPS was created using the following document: https://bit.ly/3yms06n\n+    # The categorization for WPS was created using the following document:\n+    # https://docs.google.com/document/d/1Y_DkYuglPvYlYmAJoyvkXEERbNDS3UhS\n \n     # WPS: Naming\n     'WPS117': IssueType.CODE_STYLE,  # Forbid naming variables self, cls, or mcs.\ndiff --git a/hyperstyle/src/python/review/inspectors/golang_lint/config.yml b/hyperstyle/src/python/review/inspectors/golang_lint/config.yml\nindex fe00bdf1..02a1e8e1 100644\n--- a/hyperstyle/src/python/review/inspectors/golang_lint/config.yml\n+++ b/hyperstyle/src/python/review/inspectors/golang_lint/config.yml\n@@ -1,4 +1,5 @@\n-# The configuration was made using the following table: https://bit.ly/3Kvz0nK\n+# The configuration was made using the following table:\n+# https://docs.google.com/spreadsheets/d/1zJrORtjZ3UyoLJV5qaLw9Y_cnK0sNlDW\n \n run:\n   tests: false\ndiff --git a/hyperstyle/src/python/review/inspectors/golang_lint/golang_lint.py b/hyperstyle/src/python/review/inspectors/golang_lint/golang_lint.py\nindex 087228e4..9f6c6674 100644\n--- a/hyperstyle/src/python/review/inspectors/golang_lint/golang_lint.py\n+++ b/hyperstyle/src/python/review/inspectors/golang_lint/golang_lint.py\n@@ -7,17 +7,17 @@\n \n from hyperstyle.src.python.review.common.file_system import check_set_up_env_variable, new_temp_dir\n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n from hyperstyle.src.python.review.inspectors.common.utils import is_result_file_correct\n from hyperstyle.src.python.review.inspectors.golang_lint.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.golang_lint.issue_types import (\n     CODE_PREFIX_TO_ISSUE_TYPE,\n     CODE_TO_ISSUE_TYPE,\n )\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n \n GOLANG_LINT_DIRECTORY_ENV = 'GOLANG_LINT_DIRECTORY'\n GOLANG_LINT_CONFIG_PATH = Path(__file__).parent / 'config.yml'\ndiff --git a/hyperstyle/src/python/review/inspectors/golang_lint/issue_configs.py b/hyperstyle/src/python/review/inspectors/golang_lint/issue_configs.py\nindex 18e7233e..a348abbd 100644\n--- a/hyperstyle/src/python/review/inspectors/golang_lint/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/golang_lint/issue_configs.py\n@@ -1,6 +1,6 @@\n import re\n \n-from hyperstyle.src.python.review.inspectors.common.tips import (\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import (\n     get_cyclomatic_complexity_tip,\n     get_func_len_tip,\n     get_line_len_tip,\n@@ -8,7 +8,7 @@\n     get_maintainability_index_tip,\n )\n from hyperstyle.src.python.review.inspectors.common.utils import convert_percentage_of_value_to_lack_of_value\n-from hyperstyle.src.python.review.inspectors.issue_configs import (\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import (\n     IssueConfig,\n     IssueDescriptionParser,\n     MeasurableIssueConfig,\ndiff --git a/hyperstyle/src/python/review/inspectors/golang_lint/issue_types.py b/hyperstyle/src/python/review/inspectors/golang_lint/issue_types.py\nindex 8addba6d..4ee25e90 100644\n--- a/hyperstyle/src/python/review/inspectors/golang_lint/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/golang_lint/issue_types.py\n@@ -2,9 +2,10 @@\n \n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n-# The configuration was made using the following table: https://bit.ly/3Kvz0nK\n+# The configuration was made using the following table:\n+# https://docs.google.com/spreadsheets/d/1zJrORtjZ3UyoLJV5qaLw9Y_cnK0sNlDW\n CODE_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     'errcheck': IssueType.ERROR_PRONE,\n     'ineffassign': IssueType.ERROR_PRONE,\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_kotlin/__init__.py b/hyperstyle/src/python/review/inspectors/ij_kotlin/__init__.py\nnew file mode 100644\nindex 00000000..e69de29b\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_kotlin/ij_kotlin.py b/hyperstyle/src/python/review/inspectors/ij_kotlin/ij_kotlin.py\nnew file mode 100644\nindex 00000000..6871a1fa\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_kotlin/ij_kotlin.py\n@@ -0,0 +1,16 @@\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseIJInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.proto import model_pb2\n+from hyperstyle.src.python.review.inspectors.ij_kotlin.issue_configs import ISSUE_CONFIGS\n+from hyperstyle.src.python.review.inspectors.ij_kotlin.issue_types import (\n+    IJ_INSPECTION_TO_ISSUE_TYPE,\n+    IJ_MESSAGE_TO_ISSUE_TYPE,\n+)\n+\n+\n+class KotlinIJInspector(BaseIJInspector):\n+    inspector_type = InspectorType.IJ_KOTLIN\n+    language_id = model_pb2.LanguageId.kotlin\n+    issue_configs = ISSUE_CONFIGS\n+    ij_inspection_to_issue_type = IJ_INSPECTION_TO_ISSUE_TYPE\n+    ij_message_to_issue_type = IJ_MESSAGE_TO_ISSUE_TYPE\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_configs.py b/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_configs.py\nnew file mode 100644\nindex 00000000..5b01ae4b\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_configs.py\n@@ -0,0 +1,1 @@\n+ISSUE_CONFIGS = []\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_types.py b/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_types.py\nnew file mode 100644\nindex 00000000..5f37738f\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_kotlin/issue_types.py\n@@ -0,0 +1,3 @@\n+IJ_INSPECTION_TO_ISSUE_TYPE = {}\n+\n+IJ_MESSAGE_TO_ISSUE_TYPE = {}\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_python/__init__.py b/hyperstyle/src/python/review/inspectors/ij_python/__init__.py\nnew file mode 100644\nindex 00000000..e69de29b\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_python/ij_python.py b/hyperstyle/src/python/review/inspectors/ij_python/ij_python.py\nnew file mode 100644\nindex 00000000..04f9b40b\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_python/ij_python.py\n@@ -0,0 +1,16 @@\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseIJInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.proto import model_pb2\n+from hyperstyle.src.python.review.inspectors.ij_python.issue_configs import ISSUE_CONFIGS\n+from hyperstyle.src.python.review.inspectors.ij_python.issue_types import (\n+    IJ_INSPECTION_TO_ISSUE_TYPE,\n+    IJ_MESSAGE_TO_ISSUE_TYPE,\n+)\n+\n+\n+class PythonIJInspector(BaseIJInspector):\n+    inspector_type = InspectorType.IJ_PYTHON\n+    language_id = model_pb2.LanguageId.Python\n+    issue_configs = ISSUE_CONFIGS\n+    ij_inspection_to_issue_type = IJ_INSPECTION_TO_ISSUE_TYPE\n+    ij_message_to_issue_type = IJ_MESSAGE_TO_ISSUE_TYPE\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_python/issue_configs.py b/hyperstyle/src/python/review/inspectors/ij_python/issue_configs.py\nnew file mode 100644\nindex 00000000..827e5f76\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_python/issue_configs.py\n@@ -0,0 +1,8 @@\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfig\n+\n+ISSUE_CONFIGS = [\n+    IssueConfig(\n+        origin_class='E128',\n+        new_description='Incorrect indent',\n+    ),\n+]\ndiff --git a/hyperstyle/src/python/review/inspectors/ij_python/issue_types.py b/hyperstyle/src/python/review/inspectors/ij_python/issue_types.py\nnew file mode 100644\nindex 00000000..6bff3d44\n--- /dev/null\n+++ b/hyperstyle/src/python/review/inspectors/ij_python/issue_types.py\n@@ -0,0 +1,77 @@\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n+\n+# Synchronized with https://github.com/JetBrains-Research/code-quality-ij-server/tree/master/docs/inspections/python\n+IJ_INSPECTION_TO_ISSUE_TYPE = {\n+    # BEST_PRACTICES\n+    'PyUnusedLocalInspection': IssueType.BEST_PRACTICES,\n+    'PySimplifyBooleanCheckInspection': IssueType.BEST_PRACTICES,\n+    'PyArgumentEqualDefaultInspection': IssueType.BEST_PRACTICES,\n+    'PyAttributeOutsideInitInspection': IssueType.BEST_PRACTICES,\n+    'PyBroadExceptionInspection': IssueType.BEST_PRACTICES,\n+    'PyDictCreationInspection': IssueType.BEST_PRACTICES,\n+    'PyFromFutureImportInspection': IssueType.BEST_PRACTICES,\n+    'PyReturnFromInitInspection': IssueType.BEST_PRACTICES,\n+    'PyListCreationInspection': IssueType.BEST_PRACTICES,\n+    'PySetFunctionToLiteralInspection': IssueType.BEST_PRACTICES,\n+    'PyProtectedMemberInspection': IssueType.BEST_PRACTICES,\n+    'PyMethodMayBeStaticInspection': IssueType.BEST_PRACTICES,\n+    'PyChainedComparisonsInspection': IssueType.BEST_PRACTICES,\n+    'PyGlobalUndefinedInspection': IssueType.BEST_PRACTICES,\n+    'PyComparisonWithNoneInspection': IssueType.BEST_PRACTICES,\n+    'PyShadowingNamesInspection': IssueType.BEST_PRACTICES,\n+\n+    # CODE_STYLE\n+    'PyRedundantParenthesesInspection': IssueType.CODE_STYLE,\n+    'PyAugmentAssignmentInspection': IssueType.CODE_STYLE,\n+    'PyMethodParametersInspection': IssueType.CODE_STYLE,\n+    'PyUnreachableCodeInspection': IssueType.CODE_STYLE,\n+    'PyTrailingSemicolonInspection': IssueType.CODE_STYLE,\n+    'PyStatementEffectInspection': IssueType.CODE_STYLE,\n+    'PyPep8NamingInspection': IssueType.CODE_STYLE,\n+    'PyPep8Inspection': IssueType.CODE_STYLE,\n+\n+    # ERROR_PRONE\n+    'PyDataclassInspection': IssueType.ERROR_PRONE,\n+    'PyDefaultArgumentInspection': IssueType.ERROR_PRONE,\n+    'PyAssignmentToLoopOrWithParameterInspection': IssueType.ERROR_PRONE,\n+    'PyAsyncCallInspection': IssueType.ERROR_PRONE,\n+    'PyByteLiteralInspection': IssueType.ERROR_PRONE,\n+    'PyCallingNonCallableInspection': IssueType.ERROR_PRONE,\n+    'PyDictDuplicateKeysInspection': IssueType.ERROR_PRONE,\n+    'PyExceptClausesOrderInspection': IssueType.ERROR_PRONE,\n+    'PyFinalInspection': IssueType.ERROR_PRONE,\n+    'PyArgumentListInspection': IssueType.ERROR_PRONE,\n+    'PyMethodFirstArgAssignmentInspection': IssueType.ERROR_PRONE,\n+    'PyStringFormatInspection': IssueType.ERROR_PRONE,\n+    'PyMethodOverridingInspection': IssueType.ERROR_PRONE,\n+    'PyTupleAssignmentBalanceInspection': IssueType.ERROR_PRONE,\n+    'PyExceptionInheritInspection': IssueType.ERROR_PRONE,\n+    'PyUnboundLocalVariableInspection': IssueType.ERROR_PRONE,\n+    'PySuperArgumentsInspection': IssueType.ERROR_PRONE,\n+    'PyTupleItemAssignmentInspection': IssueType.ERROR_PRONE,\n+    'PyPropertyAccessInspection': IssueType.ERROR_PRONE,\n+    'PyNestedDecoratorsInspection': IssueType.ERROR_PRONE,\n+    'PyMissingConstructorInspection': IssueType.ERROR_PRONE,\n+    'PyDecoratorInspection': IssueType.ERROR_PRONE,\n+    'PyTypeCheckerInspection': IssueType.ERROR_PRONE,\n+    'PyNoneFunctionAssignmentInspection': IssueType.ERROR_PRONE,\n+    'PyAbstractClassInspection': IssueType.ERROR_PRONE,\n+    'PyOverloadsInspection': IssueType.ERROR_PRONE,\n+    'PyTypeHintsInspection': IssueType.ERROR_PRONE,\n+    'PyTypedDictInspection': IssueType.ERROR_PRONE,\n+    'PyShadowingBuiltinsInspection': IssueType.ERROR_PRONE,\n+    'PyUnresolvedReferencesInspection': IssueType.ERROR_PRONE,\n+}\n+\n+IJ_MESSAGE_TO_ISSUE_TYPE = {\n+    'PyDataclassInspection': {\n+        'is useless until ''__post_init__'' is declared': IssueType.BEST_PRACTICES,\n+        'should take all init-only variables (incl. inherited) in the same order as they are defined':\n+            IssueType.BEST_PRACTICES,\n+        'would not be called until \\'init\\' parameter is set to True \\'__attrs_post_init__\\' '\n+        'should not take any parameters except \\'self\\'': IssueType.BEST_PRACTICES,\n+    },\n+    'PyFinalInspection': {\n+        'No need to mark method in \\'Final\\' class as \\'@final\\'': IssueType.BEST_PRACTICES,\n+    },\n+}\ndiff --git a/hyperstyle/src/python/review/inspectors/pmd/files/config.xml b/hyperstyle/src/python/review/inspectors/pmd/files/config.xml\nindex abf91677..4961705f 100644\n--- a/hyperstyle/src/python/review/inspectors/pmd/files/config.xml\n+++ b/hyperstyle/src/python/review/inspectors/pmd/files/config.xml\n@@ -9,7 +9,8 @@\n         The Basic ruleset contains a collection of good practices which should be followed.\n     </description>\n \n-    <!-- This config was created using the following table: https://bit.ly/2VC6E61 -->\n+    <!-- This config was created using the following table: -->\n+    <!-- https://docs.google.com/spreadsheets/d/1HS4LrgAV3a59D0zhjfb6OATkgxWkyz2G -->\n \n     <!-- Best Practices -->\n     <rule ref=\"category/java/bestpractices.xml/AbstractClassWithoutAbstractMethod\"/>\ndiff --git a/hyperstyle/src/python/review/inspectors/pmd/issue_types.py b/hyperstyle/src/python/review/inspectors/pmd/issue_types.py\nindex e505acab..856150ae 100644\n--- a/hyperstyle/src/python/review/inspectors/pmd/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/pmd/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n PMD_RULE_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # ---- Best Practices ----\ndiff --git a/hyperstyle/src/python/review/inspectors/pmd/pmd.py b/hyperstyle/src/python/review/inspectors/pmd/pmd.py\nindex 3c1b6d72..104d4aeb 100644\n--- a/hyperstyle/src/python/review/inspectors/pmd/pmd.py\n+++ b/hyperstyle/src/python/review/inspectors/pmd/pmd.py\n@@ -7,12 +7,11 @@\n from hyperstyle.src.python.review.common.file_system import check_set_up_env_variable, new_temp_dir\n from hyperstyle.src.python.review.common.language_version import LanguageVersion\n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.base_issue_converter import convert_base_issue\n-from hyperstyle.src.python.review.inspectors.common.utils import remove_prefix\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n from hyperstyle.src.python.review.inspectors.pmd.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.pmd.issue_types import PMD_RULE_TO_ISSUE_TYPE\n \n@@ -138,4 +137,4 @@ def _get_java_version(language_version: LanguageVersion) -> str:\n             )\n             java_version = DEFAULT_JAVA_VERSION.value\n \n-        return remove_prefix(java_version, \"java\")\n+        return java_version.removeprefix('java')\ndiff --git a/hyperstyle/src/python/review/inspectors/pyast/python_ast.py b/hyperstyle/src/python/review/inspectors/pyast/python_ast.py\nindex 5f7d5790..1a473d05 100644\n--- a/hyperstyle/src/python/review/inspectors/pyast/python_ast.py\n+++ b/hyperstyle/src/python/review/inspectors/pyast/python_ast.py\n@@ -6,10 +6,10 @@\n from hyperstyle.src.python.review.common import language\n from hyperstyle.src.python.review.common.file_system import get_all_file_system_items\n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.tips import get_bool_expr_len_tip, get_func_len_tip\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import (\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import get_bool_expr_len_tip, get_func_len_tip\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import (\n     BaseIssue, BoolExprLenIssue, FuncLenIssue, IssueDifficulty, IssueType,\n )\n \ndiff --git a/hyperstyle/src/python/review/inspectors/pylint/issue_configs.py b/hyperstyle/src/python/review/inspectors/pylint/issue_configs.py\nindex 58604ae6..e95a5e8b 100644\n--- a/hyperstyle/src/python/review/inspectors/pylint/issue_configs.py\n+++ b/hyperstyle/src/python/review/inspectors/pylint/issue_configs.py\n@@ -1,4 +1,4 @@\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfig\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfig\n \n ISSUE_CONFIGS = [\n     IssueConfig(\ndiff --git a/hyperstyle/src/python/review/inspectors/pylint/issue_types.py b/hyperstyle/src/python/review/inspectors/pylint/issue_types.py\nindex 5375532d..540359e8 100644\n--- a/hyperstyle/src/python/review/inspectors/pylint/issue_types.py\n+++ b/hyperstyle/src/python/review/inspectors/pylint/issue_types.py\n@@ -1,6 +1,6 @@\n from typing import Dict\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n CODE_TO_ISSUE_TYPE: Dict[str, IssueType] = {\n     # Basic checker\ndiff --git a/hyperstyle/src/python/review/inspectors/pylint/pylint.py b/hyperstyle/src/python/review/inspectors/pylint/pylint.py\nindex 3c09cdec..476cef1e 100644\n--- a/hyperstyle/src/python/review/inspectors/pylint/pylint.py\n+++ b/hyperstyle/src/python/review/inspectors/pylint/pylint.py\n@@ -1,14 +1,15 @@\n import logging\n import re\n+import sys\n from pathlib import Path\n from typing import Any, Dict, List, Optional\n \n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.base_issue_converter import convert_base_issue\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n-from hyperstyle.src.python.review.inspectors.issue_configs import IssueConfigsHandler\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.base_issue_converter import convert_base_issue\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue_configs import IssueConfigsHandler\n from hyperstyle.src.python.review.inspectors.pylint.issue_configs import ISSUE_CONFIGS\n from hyperstyle.src.python.review.inspectors.pylint.issue_types import CATEGORY_TO_ISSUE_TYPE, CODE_TO_ISSUE_TYPE\n \n@@ -21,6 +22,7 @@\n INFO_CATEGORY = 'I'\n \n BASE_COMMAND = [\n+    sys.executable, '-m',\n     'pylint',\n     '--load-plugins', 'pylint_django',\n     # TODO: ask about django settings via an cli argument?\ndiff --git a/hyperstyle/src/python/review/inspectors/radon/radon.py b/hyperstyle/src/python/review/inspectors/radon/radon.py\nindex c7b010c5..aef101ab 100644\n--- a/hyperstyle/src/python/review/inspectors/radon/radon.py\n+++ b/hyperstyle/src/python/review/inspectors/radon/radon.py\n@@ -1,13 +1,14 @@\n import re\n+import sys\n from pathlib import Path\n from typing import Any, Dict, List\n \n from hyperstyle.src.python.review.common.subprocess_runner import run_in_subprocess\n-from hyperstyle.src.python.review.inspectors.base_inspector import BaseInspector\n-from hyperstyle.src.python.review.inspectors.common.tips import get_maintainability_index_tip\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.tips import get_maintainability_index_tip\n from hyperstyle.src.python.review.inspectors.common.utils import convert_percentage_of_value_to_lack_of_value\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import (\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import (\n     BaseIssue,\n     IssueData,\n     IssueDifficulty,\n@@ -30,6 +31,7 @@ def inspect_in_memory(cls, code: str, config: Dict[str, Any]) -> List[BaseIssue]\n     @classmethod\n     def inspect(cls, path: Path, config: Dict[str, Any]) -> List[BaseIssue]:\n         mi_command = [\n+            sys.executable, '-m',\n             'radon', 'mi',  # compute the Maintainability Index score\n             '--max', 'F',  # set the maximum MI rank to display\n             '--show',  # actual MI value is shown in results, alongside the rank\ndiff --git a/hyperstyle/src/python/review/quality/evaluate_quality.py b/hyperstyle/src/python/review/quality/evaluate_quality.py\nindex 100a92dc..1b5e609b 100644\n--- a/hyperstyle/src/python/review/quality/evaluate_quality.py\n+++ b/hyperstyle/src/python/review/quality/evaluate_quality.py\n@@ -1,7 +1,7 @@\n from typing import List\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import Quality, Rule\n from hyperstyle.src.python.review.quality.rules.best_practices_scoring import (\n     BestPracticesRule,\ndiff --git a/hyperstyle/src/python/review/quality/model.py b/hyperstyle/src/python/review/quality/model.py\nindex ca0f40b1..639cbbb4 100644\n--- a/hyperstyle/src/python/review/quality/model.py\n+++ b/hyperstyle/src/python/review/quality/model.py\n@@ -4,7 +4,7 @@\n from functools import total_ordering\n from typing import List\n \n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n \n \n @total_ordering\ndiff --git a/hyperstyle/src/python/review/quality/penalty.py b/hyperstyle/src/python/review/quality/penalty.py\nindex 2add2917..eea08063 100644\n--- a/hyperstyle/src/python/review/quality/penalty.py\n+++ b/hyperstyle/src/python/review/quality/penalty.py\n@@ -3,7 +3,7 @@\n from typing import Dict, List, Optional, Set\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueType\n from hyperstyle.src.python.review.quality.model import QualityType\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/best_practices_scoring.py b/hyperstyle/src/python/review/quality/rules/best_practices_scoring.py\nindex 1d3ce256..81bed025 100644\n--- a/hyperstyle/src/python/review/quality/rules/best_practices_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/best_practices_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/boolean_length_scoring.py b/hyperstyle/src/python/review/quality/rules/boolean_length_scoring.py\nindex cf685962..c8d49a55 100644\n--- a/hyperstyle/src/python/review/quality/rules/boolean_length_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/boolean_length_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/class_response_scoring.py b/hyperstyle/src/python/review/quality/rules/class_response_scoring.py\nindex a33d60c2..0956886d 100644\n--- a/hyperstyle/src/python/review/quality/rules/class_response_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/class_response_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/code_style_scoring.py b/hyperstyle/src/python/review/quality/rules/code_style_scoring.py\nindex be450212..20d92b88 100644\n--- a/hyperstyle/src/python/review/quality/rules/code_style_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/code_style_scoring.py\n@@ -1,7 +1,7 @@\n from dataclasses import dataclass\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/cohesion_scoring.py b/hyperstyle/src/python/review/quality/rules/cohesion_scoring.py\nindex 541ac226..2e64fa31 100644\n--- a/hyperstyle/src/python/review/quality/rules/cohesion_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/cohesion_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/complexity_scoring.py b/hyperstyle/src/python/review/quality/rules/complexity_scoring.py\nindex a8a6aa6d..95080a7e 100644\n--- a/hyperstyle/src/python/review/quality/rules/complexity_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/complexity_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/coupling_scoring.py b/hyperstyle/src/python/review/quality/rules/coupling_scoring.py\nindex 552dc1c6..77771a0f 100644\n--- a/hyperstyle/src/python/review/quality/rules/coupling_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/coupling_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/cyclomatic_complexity_scoring.py b/hyperstyle/src/python/review/quality/rules/cyclomatic_complexity_scoring.py\nindex b3dc8d75..be939d85 100644\n--- a/hyperstyle/src/python/review/quality/rules/cyclomatic_complexity_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/cyclomatic_complexity_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/error_prone_scoring.py b/hyperstyle/src/python/review/quality/rules/error_prone_scoring.py\nindex d7549633..b7034cd4 100644\n--- a/hyperstyle/src/python/review/quality/rules/error_prone_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/error_prone_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/function_length_scoring.py b/hyperstyle/src/python/review/quality/rules/function_length_scoring.py\nindex e303f7ba..cff285e7 100644\n--- a/hyperstyle/src/python/review/quality/rules/function_length_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/function_length_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/inheritance_depth_scoring.py b/hyperstyle/src/python/review/quality/rules/inheritance_depth_scoring.py\nindex 4c52daf7..3da80633 100644\n--- a/hyperstyle/src/python/review/quality/rules/inheritance_depth_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/inheritance_depth_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/line_len_scoring.py b/hyperstyle/src/python/review/quality/rules/line_len_scoring.py\nindex fee4e186..8aa30362 100644\n--- a/hyperstyle/src/python/review/quality/rules/line_len_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/line_len_scoring.py\n@@ -1,7 +1,7 @@\n from dataclasses import dataclass\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/maintainability_scoring.py b/hyperstyle/src/python/review/quality/rules/maintainability_scoring.py\nindex 7ec8e1ef..16fd1cb1 100644\n--- a/hyperstyle/src/python/review/quality/rules/maintainability_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/maintainability_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/method_number_scoring.py b/hyperstyle/src/python/review/quality/rules/method_number_scoring.py\nindex e0da86ec..669f1bb9 100644\n--- a/hyperstyle/src/python/review/quality/rules/method_number_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/method_number_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/quality/rules/weighted_methods_scoring.py b/hyperstyle/src/python/review/quality/rules/weighted_methods_scoring.py\nindex 34ae03d9..1d01d623 100644\n--- a/hyperstyle/src/python/review/quality/rules/weighted_methods_scoring.py\n+++ b/hyperstyle/src/python/review/quality/rules/weighted_methods_scoring.py\n@@ -2,7 +2,7 @@\n from typing import Optional\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.quality.model import QualityType, Rule\n \n \ndiff --git a/hyperstyle/src/python/review/reviewers/common.py b/hyperstyle/src/python/review/reviewers/common.py\nindex fbe616fa..a2a28a52 100644\n--- a/hyperstyle/src/python/review/reviewers/common.py\n+++ b/hyperstyle/src/python/review/reviewers/common.py\n@@ -1,3 +1,5 @@\n+import json\n+import logging\n from collections import defaultdict\n from typing import List, Optional\n \n@@ -9,11 +11,14 @@\n     run_inspector_in_memory,\n )\n from hyperstyle.src.python.review.inspectors.checkstyle.checkstyle import CheckstyleInspector\n+from hyperstyle.src.python.review.inspectors.common.inspector.base_inspector import BaseIJInspector\n from hyperstyle.src.python.review.inspectors.detekt.detekt import DetektInspector\n from hyperstyle.src.python.review.inspectors.eslint.eslint import ESLintInspector\n from hyperstyle.src.python.review.inspectors.flake8.flake8 import Flake8Inspector\n from hyperstyle.src.python.review.inspectors.golang_lint.golang_lint import GolangLintInspector\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue\n+from hyperstyle.src.python.review.inspectors.ij_kotlin.ij_kotlin import KotlinIJInspector\n+from hyperstyle.src.python.review.inspectors.ij_python.ij_python import PythonIJInspector\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue\n from hyperstyle.src.python.review.inspectors.pmd.pmd import PMDInspector\n from hyperstyle.src.python.review.inspectors.pyast.python_ast import PythonAstInspector\n from hyperstyle.src.python.review.inspectors.pylint.pylint import PylintInspector\n@@ -41,6 +46,7 @@\n         Flake8Inspector(),\n         PythonAstInspector(),\n         RadonInspector(),\n+        PythonIJInspector(),\n     ],\n     Language.JAVA: [\n         CheckstyleInspector(),\n@@ -48,6 +54,7 @@\n     ],\n     Language.KOTLIN: [\n         DetektInspector(),\n+        KotlinIJInspector(),\n     ],\n     Language.JS: [\n         ESLintInspector(),\n@@ -60,8 +67,31 @@\n \n def _inspect_code(metadata: Metadata, config: ApplicationConfig, language: Language) -> Optional[List[BaseIssue]]:\n     inspectors = LANGUAGE_TO_INSPECTORS[language]\n+    ij_inspectors = list(\n+        filter(\n+            lambda inspector: isinstance(inspector, BaseIJInspector)\n+            and inspector.inspector_type not in config.disabled_inspectors,\n+            inspectors,\n+        ),\n+    )\n+\n+    connection_parameters = (\n+        None if config.ij_config is None else json.loads(config.ij_config).get(language.value.lower())\n+    )\n+    if ij_inspectors and connection_parameters is None:\n+        logging.warning(\n+            f'IJ inspectors for the {language.value} will be disabled '\n+            f'as the IJ config for this language was not specified.',\n+        )\n+\n+        config.disabled_inspectors.update(map(lambda inspector: inspector.inspector_type, ij_inspectors))\n+    else:\n+        for inspector in ij_inspectors:\n+            inspector.setup_connection_parameters(connection_parameters['host'], connection_parameters['port'])\n+\n     if isinstance(metadata, InMemoryMetadata):\n         return inspect_in_parallel(run_inspector_in_memory, metadata.code, config, inspectors)\n+\n     return inspect_in_parallel(run_inspector, metadata.path, config, inspectors)\n \n \n@@ -100,9 +130,7 @@ def perform_language_review(metadata: Metadata, config: ApplicationConfig, langu\n         difficulty: Punisher(issues, previous_issues) for difficulty, issues in issues_by_difficulty.items()\n     }\n \n-    general_quality_by_difficulty = {\n-        difficulty: Quality([]) for difficulty in issues_by_difficulty.keys()\n-    }\n+    general_quality_by_difficulty = {difficulty: Quality([]) for difficulty in issues_by_difficulty.keys()}\n \n     file_review_results = []\n     for file in current_files:\n@@ -115,8 +143,10 @@ def perform_language_review(metadata: Metadata, config: ApplicationConfig, langu\n         }\n \n         for code_statistics in code_statistics_by_difficulty.values():\n-            code_statistics.total_lines = min(code_statistics.total_lines,\n-                                              get_range_lines(config.start_line, config.end_line))\n+            code_statistics.total_lines = min(\n+                code_statistics.total_lines,\n+                get_range_lines(config.start_line, config.end_line),\n+            )\n \n         punisher_by_difficulty = {\n             difficulty: Punisher(file_issues, previous_issues)\n@@ -143,9 +173,11 @@ def perform_language_review(metadata: Metadata, config: ApplicationConfig, langu\n     )\n \n \n-def filter_out_of_range_issues(issues: List[BaseIssue],\n-                               start_line: int = 1,\n-                               end_line: Optional[int] = None) -> List[BaseIssue]:\n+def filter_out_of_range_issues(\n+    issues: List[BaseIssue],\n+    start_line: int = 1,\n+    end_line: Optional[int] = None,\n+) -> List[BaseIssue]:\n     if end_line is None:\n         end_line = 100_000_000\n \ndiff --git a/hyperstyle/src/python/review/reviewers/perform_review.py b/hyperstyle/src/python/review/reviewers/perform_review.py\nindex 025ca409..cb2ccf91 100644\n--- a/hyperstyle/src/python/review/reviewers/perform_review.py\n+++ b/hyperstyle/src/python/review/reviewers/perform_review.py\n@@ -6,7 +6,7 @@\n \n from hyperstyle.src.python.review.application_config import ApplicationConfig\n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import IssueType\n from hyperstyle.src.python.review.reviewers.common import perform_language_review\n from hyperstyle.src.python.review.reviewers.go import perform_go_review\n from hyperstyle.src.python.review.reviewers.python import perform_python_review\ndiff --git a/hyperstyle/src/python/review/reviewers/review_result.py b/hyperstyle/src/python/review/reviewers/review_result.py\nindex 8c934008..eac03aaa 100644\n--- a/hyperstyle/src/python/review/reviewers/review_result.py\n+++ b/hyperstyle/src/python/review/reviewers/review_result.py\n@@ -2,7 +2,7 @@\n from pathlib import Path\n from typing import Dict, List\n \n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty\n from hyperstyle.src.python.review.quality.model import Quality\n from hyperstyle.src.python.review.quality.penalty import Punisher\n \ndiff --git a/hyperstyle/src/python/review/reviewers/utils/code_statistics.py b/hyperstyle/src/python/review/reviewers/utils/code_statistics.py\nindex 7e9c5a8c..074027b5 100644\n--- a/hyperstyle/src/python/review/reviewers/utils/code_statistics.py\n+++ b/hyperstyle/src/python/review/reviewers/utils/code_statistics.py\n@@ -4,7 +4,7 @@\n from typing import Dict, List\n \n from hyperstyle.src.python.review.common.file_system import get_total_code_lines_from_file\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueType\n \n \n @dataclass\ndiff --git a/hyperstyle/src/python/review/reviewers/utils/issues_filter.py b/hyperstyle/src/python/review/reviewers/utils/issues_filter.py\nindex 5acd4517..1e235ff8 100644\n--- a/hyperstyle/src/python/review/reviewers/utils/issues_filter.py\n+++ b/hyperstyle/src/python/review/reviewers/utils/issues_filter.py\n@@ -2,7 +2,7 @@\n from typing import Dict, List, Tuple\n \n from hyperstyle.src.python.review.common.language import Language\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType, Measurable\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType, Measurable\n from hyperstyle.src.python.review.quality.rules.boolean_length_scoring import LANGUAGE_TO_BOOLEAN_EXPRESSION_RULE_CONFIG\n from hyperstyle.src.python.review.quality.rules.class_response_scoring import LANGUAGE_TO_RESPONSE_RULE_CONFIG\n from hyperstyle.src.python.review.quality.rules.cohesion_scoring import LANGUAGE_TO_COHESION_RULE_CONFIG\ndiff --git a/hyperstyle/src/python/review/reviewers/utils/print_review.py b/hyperstyle/src/python/review/reviewers/utils/print_review.py\nindex b0776163..bfffbf8f 100644\n--- a/hyperstyle/src/python/review/reviewers/utils/print_review.py\n+++ b/hyperstyle/src/python/review/reviewers/utils/print_review.py\n@@ -6,8 +6,8 @@\n \n from hyperstyle.src.python.review.application_config import ApplicationConfig\n from hyperstyle.src.python.review.common.file_system import get_file_line\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n-from hyperstyle.src.python.review.inspectors.issue import BaseIssue, IssueDifficulty, IssueType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.issue.issue import BaseIssue, IssueDifficulty, IssueType\n from hyperstyle.src.python.review.quality.model import QualityType\n from hyperstyle.src.python.review.quality.penalty import PenaltyIssue\n from hyperstyle.src.python.review.reviewers.review_result import FileReviewResult, GeneralReviewResult, ReviewResult\ndiff --git a/hyperstyle/src/python/review/run_tool.py b/hyperstyle/src/python/review/run_tool.py\nindex 613ed1b0..ce7408e0 100644\n--- a/hyperstyle/src/python/review/run_tool.py\n+++ b/hyperstyle/src/python/review/run_tool.py\n@@ -14,7 +14,7 @@\n from hyperstyle.src.python.review.application_config import ApplicationConfig\n from hyperstyle.src.python.review.common.language import Language\n from hyperstyle.src.python.review.common.language_version import LanguageVersion\n-from hyperstyle.src.python.review.inspectors.inspector_type import InspectorType\n+from hyperstyle.src.python.review.inspectors.common.inspector.inspector_type import InspectorType\n from hyperstyle.src.python.review.logging_config import logging_config\n from hyperstyle.src.python.review.reviewers.perform_review import (\n     OutputFormat,\n@@ -28,10 +28,6 @@\n \n def parse_disabled_inspectors(value: str) -> Set[InspectorType]:\n     passed_names = value.upper().split(',')\n-    # TODO: delete it after updating the run configuration in production\n-    intellij_key_word = 'intellij'.upper()\n-    if intellij_key_word in passed_names:\n-        passed_names.remove(intellij_key_word)\n     allowed_names = {inspector.value for inspector in InspectorType}\n     if not all(name in allowed_names for name in passed_names):\n         raise argparse.ArgumentError('disable', 'Incorrect inspectors\\' names')\n@@ -55,7 +51,7 @@ def configure_arguments(parser: argparse.ArgumentParser) -> None:\n                         choices=VerbosityLevel.values(),\n                         type=int)\n \n-    # Usage example: -d Flake8,Intelli\n+    # Usage example: -d Flake8,IntelliJ\n     parser.add_argument(RunToolArgument.DISABLE.value.short_name,\n                         RunToolArgument.DISABLE.value.long_name,\n                         help=RunToolArgument.DISABLE.value.description,\n@@ -126,6 +122,10 @@ def configure_arguments(parser: argparse.ArgumentParser) -> None:\n                         help=RunToolArgument.GROUP_BY_DIFFICULTY.value.description,\n                         action='store_true')\n \n+    parser.add_argument(RunToolArgument.IJ_CONFIG.value.long_name,\n+                        help=RunToolArgument.IJ_CONFIG.value.description,\n+                        type=str)\n+\n \n def configure_logging(verbosity: VerbosityLevel) -> None:\n     if verbosity is VerbosityLevel.ERROR:\n@@ -173,6 +173,7 @@ def main() -> int:\n             history=args.history,\n             with_all_categories=args.with_all_categories,\n             group_by_difficulty=args.group_by_difficulty,\n+            ij_config=args.ij_config,\n         )\n \n         n_issues = perform_and_print_review(args.path, OutputFormat(args.format), config)\ndiff --git a/poetry.lock b/poetry.lock\nnew file mode 100644\nindex 00000000..4f59c863\n--- /dev/null\n+++ b/poetry.lock\n@@ -0,0 +1,1393 @@\n+# This file is automatically @generated by Poetry 1.8.3 and should not be changed by hand.\n+\n+[[package]]\n+name = \"argparse\"\n+version = \"1.4.0\"\n+description = \"Python command-line parsing library\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"argparse-1.4.0-py2.py3-none-any.whl\", hash = \"sha256:c31647edb69fd3d465a847ea3157d37bed1f95f19760b11a47aa91c04b666314\"},\n+    {file = \"argparse-1.4.0.tar.gz\", hash = \"sha256:62b089a55be1d8949cd2bc7e0df0bddb9e028faefc8c32038cc84862aefdd6e4\"},\n+]\n+\n+[[package]]\n+name = \"asgiref\"\n+version = \"3.6.0\"\n+description = \"ASGI specs, helper code, and adapters\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"asgiref-3.6.0-py3-none-any.whl\", hash = \"sha256:71e68008da809b957b7ee4b43dbccff33d1b23519fb8344e33f049897077afac\"},\n+    {file = \"asgiref-3.6.0.tar.gz\", hash = \"sha256:9567dfe7bd8d3c8c892227827c41cce860b368104c3431da67a0c5a65a949506\"},\n+]\n+\n+[package.extras]\n+tests = [\"mypy (>=0.800)\", \"pytest\", \"pytest-asyncio\"]\n+\n+[[package]]\n+name = \"astor\"\n+version = \"0.8.1\"\n+description = \"Read/rewrite/write Python ASTs\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,>=2.7\"\n+files = [\n+    {file = \"astor-0.8.1-py2.py3-none-any.whl\", hash = \"sha256:070a54e890cefb5b3739d19f30f5a5ec840ffc9c50ffa7d23cc9fc1a38ebbfc5\"},\n+    {file = \"astor-0.8.1.tar.gz\", hash = \"sha256:6a6effda93f4e1ce9f618779b2dd1d9d84f1e32812c23a29b3fff6fd7f63fa5e\"},\n+]\n+\n+[[package]]\n+name = \"astroid\"\n+version = \"2.11.7\"\n+description = \"An abstract syntax tree for Python with inference support.\"\n+optional = false\n+python-versions = \">=3.6.2\"\n+files = [\n+    {file = \"astroid-2.11.7-py3-none-any.whl\", hash = \"sha256:86b0a340a512c65abf4368b80252754cda17c02cdbbd3f587dddf98112233e7b\"},\n+    {file = \"astroid-2.11.7.tar.gz\", hash = \"sha256:bb24615c77f4837c707669d16907331374ae8a964650a66999da3f5ca68dc946\"},\n+]\n+\n+[package.dependencies]\n+lazy-object-proxy = \">=1.4.0\"\n+setuptools = \">=20.0\"\n+wrapt = \">=1.11,<2\"\n+\n+[[package]]\n+name = \"attrs\"\n+version = \"24.2.0\"\n+description = \"Classes Without Boilerplate\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"attrs-24.2.0-py3-none-any.whl\", hash = \"sha256:81921eb96de3191c8258c199618104dd27ac608d9366f5e35d011eae1867ede2\"},\n+    {file = \"attrs-24.2.0.tar.gz\", hash = \"sha256:5cfb1b9148b5b086569baec03f20d7b6bf3bcacc9a42bebf87ffaaca362f6346\"},\n+]\n+\n+[package.extras]\n+benchmark = [\"cloudpickle\", \"hypothesis\", \"mypy (>=1.11.1)\", \"pympler\", \"pytest (>=4.3.0)\", \"pytest-codspeed\", \"pytest-mypy-plugins\", \"pytest-xdist[psutil]\"]\n+cov = [\"cloudpickle\", \"coverage[toml] (>=5.3)\", \"hypothesis\", \"mypy (>=1.11.1)\", \"pympler\", \"pytest (>=4.3.0)\", \"pytest-mypy-plugins\", \"pytest-xdist[psutil]\"]\n+dev = [\"cloudpickle\", \"hypothesis\", \"mypy (>=1.11.1)\", \"pre-commit\", \"pympler\", \"pytest (>=4.3.0)\", \"pytest-mypy-plugins\", \"pytest-xdist[psutil]\"]\n+docs = [\"cogapp\", \"furo\", \"myst-parser\", \"sphinx\", \"sphinx-notfound-page\", \"sphinxcontrib-towncrier\", \"towncrier (<24.7)\"]\n+tests = [\"cloudpickle\", \"hypothesis\", \"mypy (>=1.11.1)\", \"pympler\", \"pytest (>=4.3.0)\", \"pytest-mypy-plugins\", \"pytest-xdist[psutil]\"]\n+tests-mypy = [\"mypy (>=1.11.1)\", \"pytest-mypy-plugins\"]\n+\n+[[package]]\n+name = \"certifi\"\n+version = \"2024.8.30\"\n+description = \"Python package for providing Mozilla's CA Bundle.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"certifi-2024.8.30-py3-none-any.whl\", hash = \"sha256:922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8\"},\n+    {file = \"certifi-2024.8.30.tar.gz\", hash = \"sha256:bec941d2aa8195e248a60b31ff9f0558284cf01a52591ceda73ea9afffd69fd9\"},\n+]\n+\n+[[package]]\n+name = \"charset-normalizer\"\n+version = \"3.3.2\"\n+description = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"\n+optional = false\n+python-versions = \">=3.7.0\"\n+files = [\n+    {file = \"charset-normalizer-3.3.2.tar.gz\", hash = \"sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-win32.whl\", hash = \"sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-win32.whl\", hash = \"sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-win32.whl\", hash = \"sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win32.whl\", hash = \"sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-win32.whl\", hash = \"sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-win32.whl\", hash = \"sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d\"},\n+    {file = \"charset_normalizer-3.3.2-py3-none-any.whl\", hash = \"sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc\"},\n+]\n+\n+[[package]]\n+name = \"cohesion\"\n+version = \"1.0.0\"\n+description = \"A tool for measuring Python class cohesion.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"cohesion-1.0.0-py2.py3-none-any.whl\", hash = \"sha256:a33a526eb8922c7dbfef19257b3532d16e6246c4ece3ab3b492b290d9d3f2d4b\"},\n+]\n+\n+[[package]]\n+name = \"colorama\"\n+version = \"0.4.6\"\n+description = \"Cross-platform colored terminal text.\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"\n+files = [\n+    {file = \"colorama-0.4.6-py2.py3-none-any.whl\", hash = \"sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\"},\n+    {file = \"colorama-0.4.6.tar.gz\", hash = \"sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44\"},\n+]\n+\n+[[package]]\n+name = \"dataclasses-json\"\n+version = \"0.5.7\"\n+description = \"Easily serialize dataclasses to and from JSON\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"dataclasses-json-0.5.7.tar.gz\", hash = \"sha256:c2c11bc8214fbf709ffc369d11446ff6945254a7f09128154a7620613d8fda90\"},\n+    {file = \"dataclasses_json-0.5.7-py3-none-any.whl\", hash = \"sha256:bc285b5f892094c3a53d558858a88553dd6a61a11ab1a8128a0e554385dcc5dd\"},\n+]\n+\n+[package.dependencies]\n+marshmallow = \">=3.3.0,<4.0.0\"\n+marshmallow-enum = \">=1.5.1,<2.0.0\"\n+typing-inspect = \">=0.4.0\"\n+\n+[package.extras]\n+dev = [\"flake8\", \"hypothesis\", \"ipython\", \"mypy (>=0.710)\", \"portray\", \"pytest (>=6.2.3)\", \"simplejson\", \"types-dataclasses\"]\n+\n+[[package]]\n+name = \"dill\"\n+version = \"0.3.8\"\n+description = \"serialize all of Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"dill-0.3.8-py3-none-any.whl\", hash = \"sha256:c36ca9ffb54365bdd2f8eb3eff7d2a21237f8452b57ace88b1ac615b7e815bd7\"},\n+    {file = \"dill-0.3.8.tar.gz\", hash = \"sha256:3ebe3c479ad625c4553aca177444d89b486b1d84982eeacded644afc0cf797ca\"},\n+]\n+\n+[package.extras]\n+graph = [\"objgraph (>=1.7.2)\"]\n+profile = [\"gprof2dot (>=2022.7.29)\"]\n+\n+[[package]]\n+name = \"django\"\n+version = \"4.2.5\"\n+description = \"A high-level Python web framework that encourages rapid development and clean, pragmatic design.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"Django-4.2.5-py3-none-any.whl\", hash = \"sha256:b6b2b5cae821077f137dc4dade696a1c2aa292f892eca28fa8d7bfdf2608ddd4\"},\n+    {file = \"Django-4.2.5.tar.gz\", hash = \"sha256:5e5c1c9548ffb7796b4a8a4782e9a2e5a3df3615259fc1bfd3ebc73b646146c1\"},\n+]\n+\n+[package.dependencies]\n+asgiref = \">=3.6.0,<4\"\n+sqlparse = \">=0.3.1\"\n+tzdata = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+\n+[package.extras]\n+argon2 = [\"argon2-cffi (>=19.1.0)\"]\n+bcrypt = [\"bcrypt\"]\n+\n+[[package]]\n+name = \"eradicate\"\n+version = \"2.3.0\"\n+description = \"Removes commented-out code.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"eradicate-2.3.0-py3-none-any.whl\", hash = \"sha256:2b29b3dd27171f209e4ddd8204b70c02f0682ae95eecb353f10e8d72b149c63e\"},\n+    {file = \"eradicate-2.3.0.tar.gz\", hash = \"sha256:06df115be3b87d0fc1c483db22a2ebb12bcf40585722810d809cc770f5031c37\"},\n+]\n+\n+[[package]]\n+name = \"exceptiongroup\"\n+version = \"1.2.2\"\n+description = \"Backport of PEP 654 (exception groups)\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"exceptiongroup-1.2.2-py3-none-any.whl\", hash = \"sha256:3111b9d131c238bec2f8f516e123e14ba243563fb135d3fe885990585aa7795b\"},\n+    {file = \"exceptiongroup-1.2.2.tar.gz\", hash = \"sha256:47c2edf7c6738fafb49fd34290706d1a1a2f4d1c6df275526b62cbb4aa5393cc\"},\n+]\n+\n+[package.extras]\n+test = [\"pytest (>=6)\"]\n+\n+[[package]]\n+name = \"flake8\"\n+version = \"3.9.0\"\n+description = \"the modular source code checker: pep8 pyflakes and co\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,>=2.7\"\n+files = [\n+    {file = \"flake8-3.9.0-py2.py3-none-any.whl\", hash = \"sha256:12d05ab02614b6aee8df7c36b97d1a3b2372761222b19b58621355e82acddcff\"},\n+    {file = \"flake8-3.9.0.tar.gz\", hash = \"sha256:78873e372b12b093da7b5e5ed302e8ad9e988b38b063b61ad937f26ca58fc5f0\"},\n+]\n+\n+[package.dependencies]\n+mccabe = \">=0.6.0,<0.7.0\"\n+pycodestyle = \">=2.7.0,<2.8.0\"\n+pyflakes = \">=2.3.0,<2.4.0\"\n+\n+[[package]]\n+name = \"flake8-broken-line\"\n+version = \"0.3.0\"\n+description = \"Flake8 plugin to forbid backslashes for line breaks\"\n+optional = false\n+python-versions = \">=3.6,<4.0\"\n+files = [\n+    {file = \"flake8-broken-line-0.3.0.tar.gz\", hash = \"sha256:f74e052833324a9e5f0055032f7ccc54b23faabafe5a26241c2f977e70b10b50\"},\n+    {file = \"flake8_broken_line-0.3.0-py3-none-any.whl\", hash = \"sha256:611f79c7f27118e7e5d3dc098ef7681c40aeadf23783700c5dbee840d2baf3af\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \">=3.5,<4.0\"\n+\n+[[package]]\n+name = \"flake8-bugbear\"\n+version = \"21.4.3\"\n+description = \"A plugin for flake8 finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"flake8-bugbear-21.4.3.tar.gz\", hash = \"sha256:2346c81f889955b39e4a368eb7d508de723d9de05716c287dc860a4073dc57e7\"},\n+    {file = \"flake8_bugbear-21.4.3-py36.py37.py38-none-any.whl\", hash = \"sha256:4f305dca96be62bf732a218fe6f1825472a621d3452c5b994d8f89dae21dbafa\"},\n+]\n+\n+[package.dependencies]\n+attrs = \">=19.2.0\"\n+flake8 = \">=3.0.0\"\n+\n+[package.extras]\n+dev = [\"black\", \"coverage\", \"hypothesis\", \"hypothesmith\"]\n+\n+[[package]]\n+name = \"flake8-builtins\"\n+version = \"1.5.3\"\n+description = \"Check for python builtins being used as variables or parameters.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-builtins-1.5.3.tar.gz\", hash = \"sha256:09998853b2405e98e61d2ff3027c47033adbdc17f9fe44ca58443d876eb00f3b\"},\n+    {file = \"flake8_builtins-1.5.3-py2.py3-none-any.whl\", hash = \"sha256:7706babee43879320376861897e5d1468e396a40b8918ed7bccf70e5f90b8687\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \"*\"\n+\n+[package.extras]\n+test = [\"coverage\", \"coveralls\", \"mock\", \"pytest\", \"pytest-cov\"]\n+\n+[[package]]\n+name = \"flake8-commas\"\n+version = \"2.0.0\"\n+description = \"Flake8 lint for trailing commas.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-commas-2.0.0.tar.gz\", hash = \"sha256:d3005899466f51380387df7151fb59afec666a0f4f4a2c6a8995b975de0f44b7\"},\n+    {file = \"flake8_commas-2.0.0-py2.py3-none-any.whl\", hash = \"sha256:ee2141a3495ef9789a3894ed8802d03eff1eaaf98ce6d8653a7c573ef101935e\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \">=2,<4.0.0\"\n+\n+[[package]]\n+name = \"flake8-comprehensions\"\n+version = \"3.4.0\"\n+description = \"A flake8 plugin to help you write better list/set/dict comprehensions.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"flake8-comprehensions-3.4.0.tar.gz\", hash = \"sha256:c00039be9f3959a26a98da3024f0fe809859bf1753ccb90e228cc40f3ac31ca7\"},\n+    {file = \"flake8_comprehensions-3.4.0-py3-none-any.whl\", hash = \"sha256:7258a28e229fb9a8d16370b9c47a7d66396ba0201abb06c9d11df41b18ed64c4\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \">=3.0,<3.2.0 || >3.2.0,<4\"\n+\n+[[package]]\n+name = \"flake8-eradicate\"\n+version = \"1.0.0\"\n+description = \"Flake8 plugin to find commented out code\"\n+optional = false\n+python-versions = \">=3.6,<4.0\"\n+files = [\n+    {file = \"flake8-eradicate-1.0.0.tar.gz\", hash = \"sha256:fe7167226676823d50cf540532302a6f576c5a398c5260692571a05ef72c5f5b\"},\n+    {file = \"flake8_eradicate-1.0.0-py3-none-any.whl\", hash = \"sha256:0fc4ab858a18c7ed630621b5345254c8f55be6060ea5c44a25e384d613618d1f\"},\n+]\n+\n+[package.dependencies]\n+attrs = \"*\"\n+eradicate = \">=2.0,<3.0\"\n+flake8 = \">=3.5,<4.0\"\n+\n+[[package]]\n+name = \"flake8-import-order\"\n+version = \"0.18.1\"\n+description = \"Flake8 and pylama plugin that checks the ordering of import statements.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-import-order-0.18.1.tar.gz\", hash = \"sha256:a28dc39545ea4606c1ac3c24e9d05c849c6e5444a50fb7e9cdd430fc94de6e92\"},\n+    {file = \"flake8_import_order-0.18.1-py2.py3-none-any.whl\", hash = \"sha256:90a80e46886259b9c396b578d75c749801a41ee969a235e163cfe1be7afd2543\"},\n+]\n+\n+[package.dependencies]\n+pycodestyle = \"*\"\n+setuptools = \"*\"\n+\n+[[package]]\n+name = \"flake8-plugin-utils\"\n+version = \"1.3.2\"\n+description = \"The package provides base classes and utils for flake8 plugin writing\"\n+optional = false\n+python-versions = \">=3.6,<4.0\"\n+files = [\n+    {file = \"flake8-plugin-utils-1.3.2.tar.gz\", hash = \"sha256:20fa2a8ca2decac50116edb42e6af0a1253ef639ad79941249b840531889c65a\"},\n+    {file = \"flake8_plugin_utils-1.3.2-py3-none-any.whl\", hash = \"sha256:1fe43e3e9acf3a7c0f6b88f5338cad37044d2f156c43cb6b080b5f9da8a76f06\"},\n+]\n+\n+[[package]]\n+name = \"flake8-polyfill\"\n+version = \"1.0.2\"\n+description = \"Polyfill package for Flake8 plugins\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-polyfill-1.0.2.tar.gz\", hash = \"sha256:e44b087597f6da52ec6393a709e7108b2905317d0c0b744cdca6208e670d8eda\"},\n+    {file = \"flake8_polyfill-1.0.2-py2.py3-none-any.whl\", hash = \"sha256:12be6a34ee3ab795b19ca73505e7b55826d5f6ad7230d31b18e106400169b9e9\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \"*\"\n+\n+[[package]]\n+name = \"flake8-return\"\n+version = \"1.1.2\"\n+description = \"Flake8 plugin that checks return values\"\n+optional = false\n+python-versions = \">=3.6,<4.0\"\n+files = [\n+    {file = \"flake8-return-1.1.2.tar.gz\", hash = \"sha256:d646d3b010a9736ddc23c24f98ad3282999f575da45d6eb9cefe4adddb44062d\"},\n+    {file = \"flake8_return-1.1.2-py3-none-any.whl\", hash = \"sha256:183d0ad2f8553cb2c63c0cf288eb799d967577a74639599525adcd3860f6bb12\"},\n+]\n+\n+[package.dependencies]\n+flake8-plugin-utils = \">=1.0,<2.0\"\n+\n+[[package]]\n+name = \"flake8-spellcheck\"\n+version = \"0.24.0\"\n+description = \"Spellcheck variables, comments and docstrings\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-spellcheck-0.24.0.tar.gz\", hash = \"sha256:833c92222158b5dea74f858ccfb9f980b7c865fbd89cabc18447dadb07ec62e6\"},\n+    {file = \"flake8_spellcheck-0.24.0-py2.py3-none-any.whl\", hash = \"sha256:33d1727e4edb2b0cda3a78dee467d7bde53ff5a2a90056bc7d1764e0b060f331\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \">3.0.0\"\n+\n+[[package]]\n+name = \"flake8-string-format\"\n+version = \"0.3.0\"\n+description = \"string format checker, plugin for flake8\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"flake8-string-format-0.3.0.tar.gz\", hash = \"sha256:65f3da786a1461ef77fca3780b314edb2853c377f2e35069723348c8917deaa2\"},\n+    {file = \"flake8_string_format-0.3.0-py2.py3-none-any.whl\", hash = \"sha256:812ff431f10576a74c89be4e85b8e075a705be39bc40c4b4278b5b13e2afa9af\"},\n+]\n+\n+[package.dependencies]\n+flake8 = \"*\"\n+\n+[[package]]\n+name = \"future\"\n+version = \"1.0.0\"\n+description = \"Clean single-source support for Python 3 and 2\"\n+optional = false\n+python-versions = \">=2.6, !=3.0.*, !=3.1.*, !=3.2.*\"\n+files = [\n+    {file = \"future-1.0.0-py3-none-any.whl\", hash = \"sha256:929292d34f5872e70396626ef385ec22355a1fae8ad29e1a734c3e43f9fbc216\"},\n+    {file = \"future-1.0.0.tar.gz\", hash = \"sha256:bd2968309307861edae1458a4f8a4f3598c03be43b97521076aebf5d94c07b05\"},\n+]\n+\n+[[package]]\n+name = \"grpcio\"\n+version = \"1.66.1\"\n+description = \"HTTP/2-based RPC framework\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"grpcio-1.66.1-cp310-cp310-linux_armv7l.whl\", hash = \"sha256:4877ba180591acdf127afe21ec1c7ff8a5ecf0fe2600f0d3c50e8c4a1cbc6492\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-macosx_12_0_universal2.whl\", hash = \"sha256:3750c5a00bd644c75f4507f77a804d0189d97a107eb1481945a0cf3af3e7a5ac\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-manylinux_2_17_aarch64.whl\", hash = \"sha256:a013c5fbb12bfb5f927444b477a26f1080755a931d5d362e6a9a720ca7dbae60\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b1b24c23d51a1e8790b25514157d43f0a4dce1ac12b3f0b8e9f66a5e2c4c132f\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b7ffb8ea674d68de4cac6f57d2498fef477cef582f1fa849e9f844863af50083\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:307b1d538140f19ccbd3aed7a93d8f71103c5d525f3c96f8616111614b14bf2a\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:1c17ebcec157cfb8dd445890a03e20caf6209a5bd4ac5b040ae9dbc59eef091d\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-win32.whl\", hash = \"sha256:ef82d361ed5849d34cf09105d00b94b6728d289d6b9235513cb2fcc79f7c432c\"},\n+    {file = \"grpcio-1.66.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:292a846b92cdcd40ecca46e694997dd6b9be6c4c01a94a0dfb3fcb75d20da858\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-linux_armv7l.whl\", hash = \"sha256:c30aeceeaff11cd5ddbc348f37c58bcb96da8d5aa93fed78ab329de5f37a0d7a\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:8a1e224ce6f740dbb6b24c58f885422deebd7eb724aff0671a847f8951857c26\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-manylinux_2_17_aarch64.whl\", hash = \"sha256:a66fe4dc35d2330c185cfbb42959f57ad36f257e0cc4557d11d9f0a3f14311df\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:e3ba04659e4fce609de2658fe4dbf7d6ed21987a94460f5f92df7579fd5d0e22\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4573608e23f7e091acfbe3e84ac2045680b69751d8d67685ffa193a4429fedb1\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:7e06aa1f764ec8265b19d8f00140b8c4b6ca179a6dc67aa9413867c47e1fb04e\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:3885f037eb11f1cacc41f207b705f38a44b69478086f40608959bf5ad85826dd\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-win32.whl\", hash = \"sha256:97ae7edd3f3f91480e48ede5d3e7d431ad6005bfdbd65c1b56913799ec79e791\"},\n+    {file = \"grpcio-1.66.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:cfd349de4158d797db2bd82d2020554a121674e98fbe6b15328456b3bf2495bb\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-linux_armv7l.whl\", hash = \"sha256:a92c4f58c01c77205df6ff999faa008540475c39b835277fb8883b11cada127a\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:fdb14bad0835914f325349ed34a51940bc2ad965142eb3090081593c6e347be9\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-manylinux_2_17_aarch64.whl\", hash = \"sha256:f03a5884c56256e08fd9e262e11b5cfacf1af96e2ce78dc095d2c41ccae2c80d\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2ca2559692d8e7e245d456877a85ee41525f3ed425aa97eb7a70fc9a79df91a0\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:84ca1be089fb4446490dd1135828bd42a7c7f8421e74fa581611f7afdf7ab761\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:d639c939ad7c440c7b2819a28d559179a4508783f7e5b991166f8d7a34b52815\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:b9feb4e5ec8dc2d15709f4d5fc367794d69277f5d680baf1910fc9915c633524\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-win32.whl\", hash = \"sha256:7101db1bd4cd9b880294dec41a93fcdce465bdbb602cd8dc5bd2d6362b618759\"},\n+    {file = \"grpcio-1.66.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:b0aa03d240b5539648d996cc60438f128c7f46050989e35b25f5c18286c86734\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-linux_armv7l.whl\", hash = \"sha256:ecfe735e7a59e5a98208447293ff8580e9db1e890e232b8b292dc8bd15afc0d2\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:4825a3aa5648010842e1c9d35a082187746aa0cdbf1b7a2a930595a94fb10fce\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-manylinux_2_17_aarch64.whl\", hash = \"sha256:f517fd7259fe823ef3bd21e508b653d5492e706e9f0ef82c16ce3347a8a5620c\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f1fe60d0772831d96d263b53d83fb9a3d050a94b0e94b6d004a5ad111faa5b5b\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:31a049daa428f928f21090403e5d18ea02670e3d5d172581670be006100db9ef\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:6f914386e52cbdeb5d2a7ce3bf1fdfacbe9d818dd81b6099a05b741aaf3848bb\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:bff2096bdba686019fb32d2dde45b95981f0d1490e054400f70fc9a8af34b49d\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-win32.whl\", hash = \"sha256:aa8ba945c96e73de29d25331b26f3e416e0c0f621e984a3ebdb2d0d0b596a3b3\"},\n+    {file = \"grpcio-1.66.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:161d5c535c2bdf61b95080e7f0f017a1dfcb812bf54093e71e5562b16225b4ce\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-linux_armv7l.whl\", hash = \"sha256:d0cd7050397b3609ea51727b1811e663ffda8bda39c6a5bb69525ef12414b503\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:0e6c9b42ded5d02b6b1fea3a25f036a2236eeb75d0579bfd43c0018c88bf0a3e\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-manylinux_2_17_aarch64.whl\", hash = \"sha256:c9f80f9fad93a8cf71c7f161778ba47fd730d13a343a46258065c4deb4b550c0\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:5dd67ed9da78e5121efc5c510f0122a972216808d6de70953a740560c572eb44\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:48b0d92d45ce3be2084b92fb5bae2f64c208fea8ceed7fccf6a7b524d3c4942e\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:4d813316d1a752be6f5c4360c49f55b06d4fe212d7df03253dfdae90c8a402bb\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:9c9bebc6627873ec27a70fc800f6083a13c70b23a5564788754b9ee52c5aef6c\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-win32.whl\", hash = \"sha256:30a1c2cf9390c894c90bbc70147f2372130ad189cffef161f0432d0157973f45\"},\n+    {file = \"grpcio-1.66.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:17663598aadbedc3cacd7bbde432f541c8e07d2496564e22b214b22c7523dac8\"},\n+    {file = \"grpcio-1.66.1.tar.gz\", hash = \"sha256:35334f9c9745add3e357e3372756fd32d925bd52c41da97f4dfdafbde0bf0ee2\"},\n+]\n+\n+[package.extras]\n+protobuf = [\"grpcio-tools (>=1.66.1)\"]\n+\n+[[package]]\n+name = \"grpcio-tools\"\n+version = \"1.66.1\"\n+description = \"Protobuf code generator for gRPC\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-linux_armv7l.whl\", hash = \"sha256:e0c71405399ef59782600b1f0bdebc69ba12d7c9527cd268162a86273971d294\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-macosx_12_0_universal2.whl\", hash = \"sha256:df1a174a6f9d3b4c380f005f33352d2e95464f33f021fb08084735a2eb6e23b1\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_aarch64.whl\", hash = \"sha256:7d789bfe53fce9e87aa80c3694a366258ce4c41b706258e9228ed4994832b780\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:95c44a265ff01fd05166edae9350bc2e7d1d9a95e8f53b8cd04d2ae0a588c583\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b962a8767c3c0f9afe92e0dd6bb0b2305d35195a1053f84d4d31f585b87557ed\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:d8616773126ec3cdf747b06a12e957b43ac15c34e4728def91fa67249a7c689a\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:0067e79b6001560ac6acc78cca11fd3504fa27f8af46e3cdbac2f4998505e597\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-win32.whl\", hash = \"sha256:fa4f95a79a34afc3b5464895d091cd1911227fc3ab0441b9a37cd1817cf7db86\"},\n+    {file = \"grpcio_tools-1.66.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:3acce426f5e643de63019311171f4d31131da8149de518716a95c29a2c12dd38\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-linux_armv7l.whl\", hash = \"sha256:9a07e24feb7472419cf70ebbb38dd4299aea696f91f191b62a99b3ee9ff03f89\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:097a069e7c640043921ecaf3e88d7af78ccd40c25dbddc91db2a4a2adbd0393d\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-manylinux_2_17_aarch64.whl\", hash = \"sha256:016fa273dc696c9d8045091ac50e000bce766183a6b150801f51c2946e33dbe3\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1ec9f4f964f8e8ed5e9cc13deb678c83d5597074c256805373220627833bc5ad\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3198815814cdd12bdb69b7580d7770a4ad4c8b2093e0bd6b987bc817618e3eec\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:796620fc41d3fbb566d9614ef22bc55df67fac1f1e19c1e0fb6ec48bc9b6a44b\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:222d8dc218560698e1abf652fb47e4015994ec7a265ef46e012fd9c9e77a4d6b\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-win32.whl\", hash = \"sha256:56e17a11f34df252b4c6fb8aa8cd7b44d162dba9f3333be87ddf7c8bf496622a\"},\n+    {file = \"grpcio_tools-1.66.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:edd52d667f2aa3c73233be0a821596937f24536647c12d96bfc54aa4cb04747d\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-linux_armv7l.whl\", hash = \"sha256:869b6960d5daffda0dac1a474b44144f0dace0d4336394e499c4f400c5e2f8d9\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:68d9390bf9ba863ac147fc722d6548caa587235e887cac1bc2438212e89d1de7\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-manylinux_2_17_aarch64.whl\", hash = \"sha256:b8660401beca7e3af28722439e07b0bcdca80b4a68f5a5a1138ae7b7780a6abf\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:eb67b9aa9cd69468bceb933e8e0f89fd13695746c018c4d2e6b3b84e73f3ad97\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5daceb9716e31edc0e1ba0f93303785211438c43502edddad7a919fc4cb3d664\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:0a86398a4cd0665bc7f09fa90b89bac592c959d2c895bf3cf5d47a98c0f2d24c\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:1b4acb53338072ab3023e418a5c7059cb15686abd1607516fa1453406dd5f69d\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-win32.whl\", hash = \"sha256:88e04b7546101bc79c868c941777efd5088063a9e4f03b4d7263dde796fbabf7\"},\n+    {file = \"grpcio_tools-1.66.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:5b4fc56abeafae74140f5da29af1093e88ce64811d77f1a81c3146e9e996fb6a\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-linux_armv7l.whl\", hash = \"sha256:d4dd2ff982c1aa328ef47ce34f07af82f1f13599912fb1618ebc5fe1e14dddb8\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:066648543f786cb74b1fef5652359952455dbba37e832642026fd9fd8a219b5f\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-manylinux_2_17_aarch64.whl\", hash = \"sha256:d19d47744c30e6bafa76b3113740e71f382d75ebb2918c1efd62ebe6ba7e20f9\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:739c53571130b359b738ac7d6d0a1f772e15779b66df7e6764bee4071cd38689\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2226ff8d3ecba83b7622946df19d6e8e15cb52f761b8d9e2f807b228db5f1b1e\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:2f4b1498cb8b422fbae32a491c9154e8d47650caf5852fbe6b3b34253e824343\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:93d2d9e14e81affdc63d67c42eb16a8da1b6fecc16442a703ca60eb0e7591691\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-win32.whl\", hash = \"sha256:d761dfd97a10e4aae73628b5120c64e56f0cded88651d0003d2d80e678c3e7c9\"},\n+    {file = \"grpcio_tools-1.66.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:e1c2ac0955f5fb87b8444316e475242d194c3f3cd0b7b6e54b889a7b6f05156f\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-linux_armv7l.whl\", hash = \"sha256:5f1f04578b72c281e39274348a61d240c48d5321ba8d7a8838e194099ecbc322\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:da9b0c08dbbf07535ee1b75a22d0acc5675a808a3a3df9f9b21e0e73ddfbb3a9\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-manylinux_2_17_aarch64.whl\", hash = \"sha256:e302b4e1fa856d74ff65c65888b3a37153287ce6ad5bad80b2fdf95130accec2\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7fc3f62494f238774755ff90f0e66a93ac7972ea1eb7180c45acf4fd53b25cca\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:23cad65ff22459aa387f543d293f54834c9aac8f76fb7416a7046556df75b567\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:3d17a27c567a5e4d18f487368215cb51b43e2499059fd6113b92f7ae1fee48be\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4df167e67b083f96bc277032a526f6186e98662aaa49baea1dfb8ecfe26ce117\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-win32.whl\", hash = \"sha256:f94d5193b2f2a9595795b83e7978b2bee1c0399da66f2f24d179c388f81fb99c\"},\n+    {file = \"grpcio_tools-1.66.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:66f527a1e3f063065e29cf6f3e55892434d13a5a51e3b22402e09da9521e98a3\"},\n+    {file = \"grpcio_tools-1.66.1.tar.gz\", hash = \"sha256:5055ffe840ea8f505c30378be02afb4dbecb33480e554debe10b63d6b2f641c3\"},\n+]\n+\n+[package.dependencies]\n+grpcio = \">=1.66.1\"\n+protobuf = \">=5.26.1,<6.0dev\"\n+setuptools = \"*\"\n+\n+[[package]]\n+name = \"idna\"\n+version = \"3.10\"\n+description = \"Internationalized Domain Names in Applications (IDNA)\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"idna-3.10-py3-none-any.whl\", hash = \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\"},\n+    {file = \"idna-3.10.tar.gz\", hash = \"sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9\"},\n+]\n+\n+[package.extras]\n+all = [\"flake8 (>=7.1.1)\", \"mypy (>=1.11.2)\", \"pytest (>=8.3.2)\", \"ruff (>=0.6.2)\"]\n+\n+[[package]]\n+name = \"iniconfig\"\n+version = \"2.0.0\"\n+description = \"brain-dead simple config-ini parsing\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"iniconfig-2.0.0-py3-none-any.whl\", hash = \"sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\"},\n+    {file = \"iniconfig-2.0.0.tar.gz\", hash = \"sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3\"},\n+]\n+\n+[[package]]\n+name = \"isort\"\n+version = \"5.13.2\"\n+description = \"A Python utility / library to sort Python imports.\"\n+optional = false\n+python-versions = \">=3.8.0\"\n+files = [\n+    {file = \"isort-5.13.2-py3-none-any.whl\", hash = \"sha256:8ca5e72a8d85860d5a3fa69b8745237f2939afe12dbf656afbcb47fe72d947a6\"},\n+    {file = \"isort-5.13.2.tar.gz\", hash = \"sha256:48fdfcb9face5d58a4f6dde2e72a1fb8dcaf8ab26f95ab49fab84c2ddefb0109\"},\n+]\n+\n+[package.extras]\n+colors = [\"colorama (>=0.4.6)\"]\n+\n+[[package]]\n+name = \"jsonschema\"\n+version = \"4.23.0\"\n+description = \"An implementation of JSON Schema validation for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"jsonschema-4.23.0-py3-none-any.whl\", hash = \"sha256:fbadb6f8b144a8f8cf9f0b89ba94501d143e50411a1278633f56a7acf7fd5566\"},\n+    {file = \"jsonschema-4.23.0.tar.gz\", hash = \"sha256:d71497fef26351a33265337fa77ffeb82423f3ea21283cd9467bb03999266bc4\"},\n+]\n+\n+[package.dependencies]\n+attrs = \">=22.2.0\"\n+jsonschema-specifications = \">=2023.03.6\"\n+referencing = \">=0.28.4\"\n+rpds-py = \">=0.7.1\"\n+\n+[package.extras]\n+format = [\"fqdn\", \"idna\", \"isoduration\", \"jsonpointer (>1.13)\", \"rfc3339-validator\", \"rfc3987\", \"uri-template\", \"webcolors (>=1.11)\"]\n+format-nongpl = [\"fqdn\", \"idna\", \"isoduration\", \"jsonpointer (>1.13)\", \"rfc3339-validator\", \"rfc3986-validator (>0.1.0)\", \"uri-template\", \"webcolors (>=24.6.0)\"]\n+\n+[[package]]\n+name = \"jsonschema-specifications\"\n+version = \"2023.12.1\"\n+description = \"The JSON Schema meta-schemas and vocabularies, exposed as a Registry\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"jsonschema_specifications-2023.12.1-py3-none-any.whl\", hash = \"sha256:87e4fdf3a94858b8a2ba2778d9ba57d8a9cafca7c7489c46ba0d30a8bc6a9c3c\"},\n+    {file = \"jsonschema_specifications-2023.12.1.tar.gz\", hash = \"sha256:48a76787b3e70f5ed53f1160d2b81f586e4ca6d1548c5de7085d1682674764cc\"},\n+]\n+\n+[package.dependencies]\n+referencing = \">=0.31.0\"\n+\n+[[package]]\n+name = \"lazy-object-proxy\"\n+version = \"1.10.0\"\n+description = \"A fast and thorough lazy object proxy.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"lazy-object-proxy-1.10.0.tar.gz\", hash = \"sha256:78247b6d45f43a52ef35c25b5581459e85117225408a4128a3daf8bf9648ac69\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:855e068b0358ab916454464a884779c7ffa312b8925c6f7401e952dcf3b89977\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:7ab7004cf2e59f7c2e4345604a3e6ea0d92ac44e1c2375527d56492014e690c3\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:dc0d2fc424e54c70c4bc06787e4072c4f3b1aa2f897dfdc34ce1013cf3ceef05\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:e2adb09778797da09d2b5ebdbceebf7dd32e2c96f79da9052b2e87b6ea495895\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:b1f711e2c6dcd4edd372cf5dec5c5a30d23bba06ee012093267b3376c079ec83\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-win32.whl\", hash = \"sha256:76a095cfe6045c7d0ca77db9934e8f7b71b14645f0094ffcd842349ada5c5fb9\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:b4f87d4ed9064b2628da63830986c3d2dca7501e6018347798313fcf028e2fd4\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:fec03caabbc6b59ea4a638bee5fce7117be8e99a4103d9d5ad77f15d6f81020c\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:02c83f957782cbbe8136bee26416686a6ae998c7b6191711a04da776dc9e47d4\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:009e6bb1f1935a62889ddc8541514b6a9e1fcf302667dcb049a0be5c8f613e56\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:75fc59fc450050b1b3c203c35020bc41bd2695ed692a392924c6ce180c6f1dc9\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:782e2c9b2aab1708ffb07d4bf377d12901d7a1d99e5e410d648d892f8967ab1f\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-win32.whl\", hash = \"sha256:edb45bb8278574710e68a6b021599a10ce730d156e5b254941754a9cc0b17d03\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:e271058822765ad5e3bca7f05f2ace0de58a3f4e62045a8c90a0dfd2f8ad8cc6\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:e98c8af98d5707dcdecc9ab0863c0ea6e88545d42ca7c3feffb6b4d1e370c7ba\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:952c81d415b9b80ea261d2372d2a4a2332a3890c2b83e0535f263ddfe43f0d43\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:80b39d3a151309efc8cc48675918891b865bdf742a8616a337cb0090791a0de9\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:e221060b701e2aa2ea991542900dd13907a5c90fa80e199dbf5a03359019e7a3\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:92f09ff65ecff3108e56526f9e2481b8116c0b9e1425325e13245abfd79bdb1b\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-win32.whl\", hash = \"sha256:3ad54b9ddbe20ae9f7c1b29e52f123120772b06dbb18ec6be9101369d63a4074\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:127a789c75151db6af398b8972178afe6bda7d6f68730c057fbbc2e96b08d282\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:9e4ed0518a14dd26092614412936920ad081a424bdcb54cc13349a8e2c6d106a\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5ad9e6ed739285919aa9661a5bbed0aaf410aa60231373c5579c6b4801bd883c\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2fc0a92c02fa1ca1e84fc60fa258458e5bf89d90a1ddaeb8ed9cc3147f417255\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:0aefc7591920bbd360d57ea03c995cebc204b424524a5bd78406f6e1b8b2a5d8\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:5faf03a7d8942bb4476e3b62fd0f4cf94eaf4618e304a19865abf89a35c0bbee\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-win32.whl\", hash = \"sha256:e333e2324307a7b5d86adfa835bb500ee70bfcd1447384a822e96495796b0ca4\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:cb73507defd385b7705c599a94474b1d5222a508e502553ef94114a143ec6696\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:366c32fe5355ef5fc8a232c5436f4cc66e9d3e8967c01fb2e6302fd6627e3d94\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2297f08f08a2bb0d32a4265e98a006643cd7233fb7983032bd61ac7a02956b3b\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:18dd842b49456aaa9a7cf535b04ca4571a302ff72ed8740d06b5adcd41fe0757\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:217138197c170a2a74ca0e05bddcd5f1796c735c37d0eee33e43259b192aa424\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:9a3a87cf1e133e5b1994144c12ca4aa3d9698517fe1e2ca82977781b16955658\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-win32.whl\", hash = \"sha256:30b339b2a743c5288405aa79a69e706a06e02958eab31859f7f3c04980853b70\"},\n+    {file = \"lazy_object_proxy-1.10.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:a899b10e17743683b293a729d3a11f2f399e8a90c73b089e29f5d0fe3509f0dd\"},\n+    {file = \"lazy_object_proxy-1.10.0-pp310.pp311.pp312.pp38.pp39-none-any.whl\", hash = \"sha256:80fa48bd89c8f2f456fc0765c11c23bf5af827febacd2f523ca5bc1893fcc09d\"},\n+]\n+\n+[[package]]\n+name = \"mando\"\n+version = \"0.6.4\"\n+description = \"Create Python CLI apps with little to no effort at all!\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"mando-0.6.4-py2.py3-none-any.whl\", hash = \"sha256:4ce09faec7e5192ffc3c57830e26acba0fd6cd11e1ee81af0d4df0657463bd1c\"},\n+    {file = \"mando-0.6.4.tar.gz\", hash = \"sha256:79feb19dc0f097daa64a1243db578e7674909b75f88ac2220f1c065c10a0d960\"},\n+]\n+\n+[package.dependencies]\n+six = \"*\"\n+\n+[package.extras]\n+restructuredtext = [\"rst2ansi\"]\n+\n+[[package]]\n+name = \"marshmallow\"\n+version = \"3.22.0\"\n+description = \"A lightweight library for converting complex datatypes to and from native Python datatypes.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"marshmallow-3.22.0-py3-none-any.whl\", hash = \"sha256:71a2dce49ef901c3f97ed296ae5051135fd3febd2bf43afe0ae9a82143a494d9\"},\n+    {file = \"marshmallow-3.22.0.tar.gz\", hash = \"sha256:4972f529104a220bb8637d595aa4c9762afbe7f7a77d82dc58c1615d70c5823e\"},\n+]\n+\n+[package.dependencies]\n+packaging = \">=17.0\"\n+\n+[package.extras]\n+dev = [\"marshmallow[tests]\", \"pre-commit (>=3.5,<4.0)\", \"tox\"]\n+docs = [\"alabaster (==1.0.0)\", \"autodocsumm (==0.2.13)\", \"sphinx (==8.0.2)\", \"sphinx-issues (==4.1.0)\", \"sphinx-version-warning (==1.1.2)\"]\n+tests = [\"pytest\", \"pytz\", \"simplejson\"]\n+\n+[[package]]\n+name = \"marshmallow-enum\"\n+version = \"1.5.1\"\n+description = \"Enum field for Marshmallow\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"marshmallow-enum-1.5.1.tar.gz\", hash = \"sha256:38e697e11f45a8e64b4a1e664000897c659b60aa57bfa18d44e226a9920b6e58\"},\n+    {file = \"marshmallow_enum-1.5.1-py2.py3-none-any.whl\", hash = \"sha256:57161ab3dbfde4f57adeb12090f39592e992b9c86d206d02f6bd03ebec60f072\"},\n+]\n+\n+[package.dependencies]\n+marshmallow = \">=2.0.0\"\n+\n+[[package]]\n+name = \"mccabe\"\n+version = \"0.6.1\"\n+description = \"McCabe checker, plugin for flake8\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"mccabe-0.6.1-py2.py3-none-any.whl\", hash = \"sha256:ab8a6258860da4b6677da4bd2fe5dc2c659cff31b3ee4f7f5d64e79735b80d42\"},\n+    {file = \"mccabe-0.6.1.tar.gz\", hash = \"sha256:dd8d182285a0fe56bace7f45b5e7d1a6ebcbf524e8f3bd87eb0f125271b8831f\"},\n+]\n+\n+[[package]]\n+name = \"mypy-extensions\"\n+version = \"1.0.0\"\n+description = \"Type system extensions for programs checked with the mypy type checker.\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"mypy_extensions-1.0.0-py3-none-any.whl\", hash = \"sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d\"},\n+    {file = \"mypy_extensions-1.0.0.tar.gz\", hash = \"sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782\"},\n+]\n+\n+[[package]]\n+name = \"packaging\"\n+version = \"24.1\"\n+description = \"Core utilities for Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"packaging-24.1-py3-none-any.whl\", hash = \"sha256:5b8f2217dbdbd2f7f384c41c628544e6d52f2d0f53c6d0c3ea61aa5d1d7ff124\"},\n+    {file = \"packaging-24.1.tar.gz\", hash = \"sha256:026ed72c8ed3fcce5bf8950572258698927fd1dbda10a5e981cdf0ac37f4f002\"},\n+]\n+\n+[[package]]\n+name = \"pep8-naming\"\n+version = \"0.11.1\"\n+description = \"Check PEP-8 naming conventions, plugin for flake8\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"pep8-naming-0.11.1.tar.gz\", hash = \"sha256:a1dd47dd243adfe8a83616e27cf03164960b507530f155db94e10b36a6cd6724\"},\n+    {file = \"pep8_naming-0.11.1-py2.py3-none-any.whl\", hash = \"sha256:f43bfe3eea7e0d73e8b5d07d6407ab47f2476ccaeff6937c84275cd30b016738\"},\n+]\n+\n+[package.dependencies]\n+flake8-polyfill = \">=1.0.2,<2\"\n+\n+[[package]]\n+name = \"platformdirs\"\n+version = \"4.3.6\"\n+description = \"A small Python package for determining appropriate platform-specific dirs, e.g. a `user data dir`.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"platformdirs-4.3.6-py3-none-any.whl\", hash = \"sha256:73e575e1408ab8103900836b97580d5307456908a03e92031bab39e4554cc3fb\"},\n+    {file = \"platformdirs-4.3.6.tar.gz\", hash = \"sha256:357fb2acbc885b0419afd3ce3ed34564c13c9b95c89360cd9563f73aa5e2b907\"},\n+]\n+\n+[package.extras]\n+docs = [\"furo (>=2024.8.6)\", \"proselint (>=0.14)\", \"sphinx (>=8.0.2)\", \"sphinx-autodoc-typehints (>=2.4)\"]\n+test = [\"appdirs (==1.4.4)\", \"covdefaults (>=2.3)\", \"pytest (>=8.3.2)\", \"pytest-cov (>=5)\", \"pytest-mock (>=3.14)\"]\n+type = [\"mypy (>=1.11.2)\"]\n+\n+[[package]]\n+name = \"pluggy\"\n+version = \"1.5.0\"\n+description = \"plugin and hook calling mechanisms for python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pluggy-1.5.0-py3-none-any.whl\", hash = \"sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669\"},\n+    {file = \"pluggy-1.5.0.tar.gz\", hash = \"sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1\"},\n+]\n+\n+[package.extras]\n+dev = [\"pre-commit\", \"tox\"]\n+testing = [\"pytest\", \"pytest-benchmark\"]\n+\n+[[package]]\n+name = \"protobuf\"\n+version = \"5.27.2\"\n+description = \"\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"protobuf-5.27.2-cp310-abi3-win32.whl\", hash = \"sha256:354d84fac2b0d76062e9b3221f4abbbacdfd2a4d8af36bab0474f3a0bb30ab38\"},\n+    {file = \"protobuf-5.27.2-cp310-abi3-win_amd64.whl\", hash = \"sha256:0e341109c609749d501986b835f667c6e1e24531096cff9d34ae411595e26505\"},\n+    {file = \"protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl\", hash = \"sha256:a109916aaac42bff84702fb5187f3edadbc7c97fc2c99c5ff81dd15dcce0d1e5\"},\n+    {file = \"protobuf-5.27.2-cp38-abi3-manylinux2014_aarch64.whl\", hash = \"sha256:176c12b1f1c880bf7a76d9f7c75822b6a2bc3db2d28baa4d300e8ce4cde7409b\"},\n+    {file = \"protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl\", hash = \"sha256:b848dbe1d57ed7c191dfc4ea64b8b004a3f9ece4bf4d0d80a367b76df20bf36e\"},\n+    {file = \"protobuf-5.27.2-cp38-cp38-win32.whl\", hash = \"sha256:4fadd8d83e1992eed0248bc50a4a6361dc31bcccc84388c54c86e530b7f58863\"},\n+    {file = \"protobuf-5.27.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:610e700f02469c4a997e58e328cac6f305f649826853813177e6290416e846c6\"},\n+    {file = \"protobuf-5.27.2-cp39-cp39-win32.whl\", hash = \"sha256:9e8f199bf7f97bd7ecebffcae45ebf9527603549b2b562df0fbc6d4d688f14ca\"},\n+    {file = \"protobuf-5.27.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:7fc3add9e6003e026da5fc9e59b131b8f22b428b991ccd53e2af8071687b4fce\"},\n+    {file = \"protobuf-5.27.2-py3-none-any.whl\", hash = \"sha256:54330f07e4949d09614707c48b06d1a22f8ffb5763c159efd5c0928326a91470\"},\n+    {file = \"protobuf-5.27.2.tar.gz\", hash = \"sha256:f3ecdef226b9af856075f28227ff2c90ce3a594d092c39bee5513573f25e2714\"},\n+]\n+\n+[[package]]\n+name = \"pycodestyle\"\n+version = \"2.7.0\"\n+description = \"Python style guide checker\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"pycodestyle-2.7.0-py2.py3-none-any.whl\", hash = \"sha256:514f76d918fcc0b55c6680472f0a37970994e07bbb80725808c17089be302068\"},\n+    {file = \"pycodestyle-2.7.0.tar.gz\", hash = \"sha256:c389c1d06bf7904078ca03399a4816f974a1d590090fecea0c63ec26ebaf1cef\"},\n+]\n+\n+[[package]]\n+name = \"pyflakes\"\n+version = \"2.3.1\"\n+description = \"passive checker of Python programs\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"pyflakes-2.3.1-py2.py3-none-any.whl\", hash = \"sha256:7893783d01b8a89811dd72d7dfd4d84ff098e5eed95cfa8905b22bbffe52efc3\"},\n+    {file = \"pyflakes-2.3.1.tar.gz\", hash = \"sha256:f5bc8ecabc05bb9d291eb5203d6810b49040f6ff446a756326104746cc00c1db\"},\n+]\n+\n+[[package]]\n+name = \"pygments\"\n+version = \"2.18.0\"\n+description = \"Pygments is a syntax highlighting package written in Python.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pygments-2.18.0-py3-none-any.whl\", hash = \"sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\"},\n+    {file = \"pygments-2.18.0.tar.gz\", hash = \"sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199\"},\n+]\n+\n+[package.extras]\n+windows-terminal = [\"colorama (>=0.4.6)\"]\n+\n+[[package]]\n+name = \"pylint\"\n+version = \"2.13.9\"\n+description = \"python code static checker\"\n+optional = false\n+python-versions = \">=3.6.2\"\n+files = [\n+    {file = \"pylint-2.13.9-py3-none-any.whl\", hash = \"sha256:705c620d388035bdd9ff8b44c5bcdd235bfb49d276d488dd2c8ff1736aa42526\"},\n+    {file = \"pylint-2.13.9.tar.gz\", hash = \"sha256:095567c96e19e6f57b5b907e67d265ff535e588fe26b12b5ebe1fc5645b2c731\"},\n+]\n+\n+[package.dependencies]\n+astroid = \">=2.11.5,<=2.12.0-dev0\"\n+colorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+dill = \">=0.2\"\n+isort = \">=4.2.5,<6\"\n+mccabe = \">=0.6,<0.8\"\n+platformdirs = \">=2.2.0\"\n+tomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+testutil = [\"gitpython (>3)\"]\n+\n+[[package]]\n+name = \"pylint-django\"\n+version = \"2.5.3\"\n+description = \"A Pylint plugin to help Pylint understand the Django web framework\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"pylint-django-2.5.3.tar.gz\", hash = \"sha256:0ac090d106c62fe33782a1d01bda1610b761bb1c9bf5035ced9d5f23a13d8591\"},\n+    {file = \"pylint_django-2.5.3-py3-none-any.whl\", hash = \"sha256:56b12b6adf56d548412445bd35483034394a1a94901c3f8571980a13882299d5\"},\n+]\n+\n+[package.dependencies]\n+pylint = \">=2.0,<3\"\n+pylint-plugin-utils = \">=0.7\"\n+\n+[package.extras]\n+for-tests = [\"coverage\", \"django-tables2\", \"django-tastypie\", \"factory-boy\", \"pylint (>=2.13)\", \"pytest\", \"wheel\"]\n+with-django = [\"Django\"]\n+\n+[[package]]\n+name = \"pylint-plugin-utils\"\n+version = \"0.8.2\"\n+description = \"Utilities and helpers for writing Pylint plugins\"\n+optional = false\n+python-versions = \">=3.7,<4.0\"\n+files = [\n+    {file = \"pylint_plugin_utils-0.8.2-py3-none-any.whl\", hash = \"sha256:ae11664737aa2effbf26f973a9e0b6779ab7106ec0adc5fe104b0907ca04e507\"},\n+    {file = \"pylint_plugin_utils-0.8.2.tar.gz\", hash = \"sha256:d3cebf68a38ba3fba23a873809155562571386d4c1b03e5b4c4cc26c3eee93e4\"},\n+]\n+\n+[package.dependencies]\n+pylint = \">=1.7\"\n+\n+[[package]]\n+name = \"pytest\"\n+version = \"8.3.3\"\n+description = \"pytest: simple powerful testing with Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pytest-8.3.3-py3-none-any.whl\", hash = \"sha256:a6853c7375b2663155079443d2e45de913a911a11d669df02a50814944db57b2\"},\n+    {file = \"pytest-8.3.3.tar.gz\", hash = \"sha256:70b98107bd648308a7952b06e6ca9a50bc660be218d53c257cc1fc94fda10181\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+exceptiongroup = {version = \">=1.0.0rc8\", markers = \"python_version < \\\"3.11\\\"\"}\n+iniconfig = \"*\"\n+packaging = \"*\"\n+pluggy = \">=1.5,<2\"\n+tomli = {version = \">=1\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+dev = [\"argcomplete\", \"attrs (>=19.2)\", \"hypothesis (>=3.56)\", \"mock\", \"pygments (>=2.7.2)\", \"requests\", \"setuptools\", \"xmlschema\"]\n+\n+[[package]]\n+name = \"pytest-runner\"\n+version = \"6.0.1\"\n+description = \"Invoke py.test as distutils command with dependency resolution\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"pytest-runner-6.0.1.tar.gz\", hash = \"sha256:70d4739585a7008f37bf4933c013fdb327b8878a5a69fcbb3316c88882f0f49b\"},\n+    {file = \"pytest_runner-6.0.1-py3-none-any.whl\", hash = \"sha256:ea326ed6f6613992746062362efab70212089a4209c08d67177b3df1c52cd9f2\"},\n+]\n+\n+[package.extras]\n+docs = [\"jaraco.packaging (>=9)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx\"]\n+testing = [\"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"pytest-flake8\", \"pytest-mypy (>=0.9.1)\", \"pytest-virtualenv\", \"types-setuptools\"]\n+\n+[[package]]\n+name = \"pytest-subtests\"\n+version = \"0.13.1\"\n+description = \"unittest subTest() support and subtests fixture\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"pytest_subtests-0.13.1-py3-none-any.whl\", hash = \"sha256:ab616a22f64cd17c1aee65f18af94dbc30c444f8683de2b30895c3778265e3bd\"},\n+    {file = \"pytest_subtests-0.13.1.tar.gz\", hash = \"sha256:989e38f0f1c01bc7c6b2e04db7d9fd859db35d77c2c1a430c831a70cbf3fde2d\"},\n+]\n+\n+[package.dependencies]\n+attrs = \">=19.2.0\"\n+pytest = \">=7.0\"\n+\n+[[package]]\n+name = \"radon\"\n+version = \"4.5.0\"\n+description = \"Code Metrics in Python\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"radon-4.5.0-py2.py3-none-any.whl\", hash = \"sha256:ad4c1b7a228cb4d33f7500e89326fd98d287938b96820e748ccdbd1a9bf91f0b\"},\n+    {file = \"radon-4.5.0.tar.gz\", hash = \"sha256:7afa65db14d759616ab68033e0e1caf1f624c97308dd256afa47518ecebddf6e\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \">=0.4.1\", markers = \"python_version > \\\"3.4\\\"\"}\n+future = \"*\"\n+mando = \">=0.6,<0.7\"\n+\n+[package.extras]\n+flake8 = [\"flake8-polyfill\"]\n+\n+[[package]]\n+name = \"referencing\"\n+version = \"0.35.1\"\n+description = \"JSON Referencing + Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"referencing-0.35.1-py3-none-any.whl\", hash = \"sha256:eda6d3234d62814d1c64e305c1331c9a3a6132da475ab6382eaa997b21ee75de\"},\n+    {file = \"referencing-0.35.1.tar.gz\", hash = \"sha256:25b42124a6c8b632a425174f24087783efb348a6f1e0008e63cd4466fedf703c\"},\n+]\n+\n+[package.dependencies]\n+attrs = \">=22.2.0\"\n+rpds-py = \">=0.7.0\"\n+\n+[[package]]\n+name = \"requests\"\n+version = \"2.32.3\"\n+description = \"Python HTTP for Humans.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"requests-2.32.3-py3-none-any.whl\", hash = \"sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6\"},\n+    {file = \"requests-2.32.3.tar.gz\", hash = \"sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760\"},\n+]\n+\n+[package.dependencies]\n+certifi = \">=2017.4.17\"\n+charset-normalizer = \">=2,<4\"\n+idna = \">=2.5,<4\"\n+urllib3 = \">=1.21.1,<3\"\n+\n+[package.extras]\n+socks = [\"PySocks (>=1.5.6,!=1.5.7)\"]\n+use-chardet-on-py3 = [\"chardet (>=3.0.2,<6)\"]\n+\n+[[package]]\n+name = \"rpds-py\"\n+version = \"0.20.0\"\n+description = \"Python bindings to Rust's persistent data structures (rpds)\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"rpds_py-0.20.0-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:3ad0fda1635f8439cde85c700f964b23ed5fc2d28016b32b9ee5fe30da5c84e2\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9bb4a0d90fdb03437c109a17eade42dfbf6190408f29b2744114d11586611d6f\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c6377e647bbfd0a0b159fe557f2c6c602c159fc752fa316572f012fc0bf67150\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:eb851b7df9dda52dc1415ebee12362047ce771fc36914586b2e9fcbd7d293b3e\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1e0f80b739e5a8f54837be5d5c924483996b603d5502bfff79bf33da06164ee2\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5a8c94dad2e45324fc74dce25e1645d4d14df9a4e54a30fa0ae8bad9a63928e3\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f8e604fe73ba048c06085beaf51147eaec7df856824bfe7b98657cf436623daf\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:df3de6b7726b52966edf29663e57306b23ef775faf0ac01a3e9f4012a24a4140\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:cf258ede5bc22a45c8e726b29835b9303c285ab46fc7c3a4cc770736b5304c9f\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:55fea87029cded5df854ca7e192ec7bdb7ecd1d9a3f63d5c4eb09148acf4a7ce\"},\n+    {file = \"rpds_py-0.20.0-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:ae94bd0b2f02c28e199e9bc51485d0c5601f58780636185660f86bf80c89af94\"},\n+    {file = \"rpds_py-0.20.0-cp310-none-win32.whl\", hash = \"sha256:28527c685f237c05445efec62426d285e47a58fb05ba0090a4340b73ecda6dee\"},\n+    {file = \"rpds_py-0.20.0-cp310-none-win_amd64.whl\", hash = \"sha256:238a2d5b1cad28cdc6ed15faf93a998336eb041c4e440dd7f902528b8891b399\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:ac2f4f7a98934c2ed6505aead07b979e6f999389f16b714448fb39bbaa86a489\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:220002c1b846db9afd83371d08d239fdc865e8f8c5795bbaec20916a76db3318\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8d7919548df3f25374a1f5d01fbcd38dacab338ef5f33e044744b5c36729c8db\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:758406267907b3781beee0f0edfe4a179fbd97c0be2e9b1154d7f0a1279cf8e5\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3d61339e9f84a3f0767b1995adfb171a0d00a1185192718a17af6e124728e0f5\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1259c7b3705ac0a0bd38197565a5d603218591d3f6cee6e614e380b6ba61c6f6\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5c1dc0f53856b9cc9a0ccca0a7cc61d3d20a7088201c0937f3f4048c1718a209\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:7e60cb630f674a31f0368ed32b2a6b4331b8350d67de53c0359992444b116dd3\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:dbe982f38565bb50cb7fb061ebf762c2f254ca3d8c20d4006878766e84266272\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:514b3293b64187172bc77c8fb0cdae26981618021053b30d8371c3a902d4d5ad\"},\n+    {file = \"rpds_py-0.20.0-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:d0a26ffe9d4dd35e4dfdd1e71f46401cff0181c75ac174711ccff0459135fa58\"},\n+    {file = \"rpds_py-0.20.0-cp311-none-win32.whl\", hash = \"sha256:89c19a494bf3ad08c1da49445cc5d13d8fefc265f48ee7e7556839acdacf69d0\"},\n+    {file = \"rpds_py-0.20.0-cp311-none-win_amd64.whl\", hash = \"sha256:c638144ce971df84650d3ed0096e2ae7af8e62ecbbb7b201c8935c370df00a2c\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:a84ab91cbe7aab97f7446652d0ed37d35b68a465aeef8fc41932a9d7eee2c1a6\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:56e27147a5a4c2c21633ff8475d185734c0e4befd1c989b5b95a5d0db699b21b\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2580b0c34583b85efec8c5c5ec9edf2dfe817330cc882ee972ae650e7b5ef739\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:b80d4a7900cf6b66bb9cee5c352b2d708e29e5a37fe9bf784fa97fc11504bf6c\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:50eccbf054e62a7b2209b28dc7a22d6254860209d6753e6b78cfaeb0075d7bee\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:49a8063ea4296b3a7e81a5dfb8f7b2d73f0b1c20c2af401fb0cdf22e14711a96\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ea438162a9fcbee3ecf36c23e6c68237479f89f962f82dae83dc15feeceb37e4\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:18d7585c463087bddcfa74c2ba267339f14f2515158ac4db30b1f9cbdb62c8ef\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:d4c7d1a051eeb39f5c9547e82ea27cbcc28338482242e3e0b7768033cb083821\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:e4df1e3b3bec320790f699890d41c59d250f6beda159ea3c44c3f5bac1976940\"},\n+    {file = \"rpds_py-0.20.0-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:2cf126d33a91ee6eedc7f3197b53e87a2acdac63602c0f03a02dd69e4b138174\"},\n+    {file = \"rpds_py-0.20.0-cp312-none-win32.whl\", hash = \"sha256:8bc7690f7caee50b04a79bf017a8d020c1f48c2a1077ffe172abec59870f1139\"},\n+    {file = \"rpds_py-0.20.0-cp312-none-win_amd64.whl\", hash = \"sha256:0e13e6952ef264c40587d510ad676a988df19adea20444c2b295e536457bc585\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:aa9a0521aeca7d4941499a73ad7d4f8ffa3d1affc50b9ea11d992cd7eff18a29\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:4a1f1d51eccb7e6c32ae89243cb352389228ea62f89cd80823ea7dd1b98e0b91\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8a86a9b96070674fc88b6f9f71a97d2c1d3e5165574615d1f9168ecba4cecb24\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:6c8ef2ebf76df43f5750b46851ed1cdf8f109d7787ca40035fe19fbdc1acc5a7\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b74b25f024b421d5859d156750ea9a65651793d51b76a2e9238c05c9d5f203a9\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:57eb94a8c16ab08fef6404301c38318e2c5a32216bf5de453e2714c964c125c8\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e1940dae14e715e2e02dfd5b0f64a52e8374a517a1e531ad9412319dc3ac7879\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:d20277fd62e1b992a50c43f13fbe13277a31f8c9f70d59759c88f644d66c619f\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:06db23d43f26478303e954c34c75182356ca9aa7797d22c5345b16871ab9c45c\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:b2a5db5397d82fa847e4c624b0c98fe59d2d9b7cf0ce6de09e4d2e80f8f5b3f2\"},\n+    {file = \"rpds_py-0.20.0-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:5a35df9f5548fd79cb2f52d27182108c3e6641a4feb0f39067911bf2adaa3e57\"},\n+    {file = \"rpds_py-0.20.0-cp313-none-win32.whl\", hash = \"sha256:fd2d84f40633bc475ef2d5490b9c19543fbf18596dcb1b291e3a12ea5d722f7a\"},\n+    {file = \"rpds_py-0.20.0-cp313-none-win_amd64.whl\", hash = \"sha256:9bc2d153989e3216b0559251b0c260cfd168ec78b1fac33dd485750a228db5a2\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:f2fbf7db2012d4876fb0d66b5b9ba6591197b0f165db8d99371d976546472a24\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:1e5f3cd7397c8f86c8cc72d5a791071431c108edd79872cdd96e00abd8497d29\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ce9845054c13696f7af7f2b353e6b4f676dab1b4b215d7fe5e05c6f8bb06f965\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c3e130fd0ec56cb76eb49ef52faead8ff09d13f4527e9b0c400307ff72b408e1\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4b16aa0107ecb512b568244ef461f27697164d9a68d8b35090e9b0c1c8b27752\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:aa7f429242aae2947246587d2964fad750b79e8c233a2367f71b554e9447949c\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:af0fc424a5842a11e28956e69395fbbeab2c97c42253169d87e90aac2886d751\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:b8c00a3b1e70c1d3891f0db1b05292747f0dbcfb49c43f9244d04c70fbc40eb8\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:40ce74fc86ee4645d0a225498d091d8bc61f39b709ebef8204cb8b5a464d3c0e\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:4fe84294c7019456e56d93e8ababdad5a329cd25975be749c3f5f558abb48253\"},\n+    {file = \"rpds_py-0.20.0-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:338ca4539aad4ce70a656e5187a3a31c5204f261aef9f6ab50e50bcdffaf050a\"},\n+    {file = \"rpds_py-0.20.0-cp38-none-win32.whl\", hash = \"sha256:54b43a2b07db18314669092bb2de584524d1ef414588780261e31e85846c26a5\"},\n+    {file = \"rpds_py-0.20.0-cp38-none-win_amd64.whl\", hash = \"sha256:a1862d2d7ce1674cffa6d186d53ca95c6e17ed2b06b3f4c476173565c862d232\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:3fde368e9140312b6e8b6c09fb9f8c8c2f00999d1823403ae90cc00480221b22\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:9824fb430c9cf9af743cf7aaf6707bf14323fb51ee74425c380f4c846ea70789\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:11ef6ce74616342888b69878d45e9f779b95d4bd48b382a229fe624a409b72c5\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c52d3f2f82b763a24ef52f5d24358553e8403ce05f893b5347098014f2d9eff2\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9d35cef91e59ebbeaa45214861874bc6f19eb35de96db73e467a8358d701a96c\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d72278a30111e5b5525c1dd96120d9e958464316f55adb030433ea905866f4de\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b4c29cbbba378759ac5786730d1c3cb4ec6f8ababf5c42a9ce303dc4b3d08cda\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:6632f2d04f15d1bd6fe0eedd3b86d9061b836ddca4c03d5cf5c7e9e6b7c14580\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:d0b67d87bb45ed1cd020e8fbf2307d449b68abc45402fe1a4ac9e46c3c8b192b\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:ec31a99ca63bf3cd7f1a5ac9fe95c5e2d060d3c768a09bc1d16e235840861420\"},\n+    {file = \"rpds_py-0.20.0-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:22e6c9976e38f4d8c4a63bd8a8edac5307dffd3ee7e6026d97f3cc3a2dc02a0b\"},\n+    {file = \"rpds_py-0.20.0-cp39-none-win32.whl\", hash = \"sha256:569b3ea770c2717b730b61998b6c54996adee3cef69fc28d444f3e7920313cf7\"},\n+    {file = \"rpds_py-0.20.0-cp39-none-win_amd64.whl\", hash = \"sha256:e6900ecdd50ce0facf703f7a00df12374b74bbc8ad9fe0f6559947fb20f82364\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:617c7357272c67696fd052811e352ac54ed1d9b49ab370261a80d3b6ce385045\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:9426133526f69fcaba6e42146b4e12d6bc6c839b8b555097020e2b78ce908dcc\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:deb62214c42a261cb3eb04d474f7155279c1a8a8c30ac89b7dcb1721d92c3c02\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:fcaeb7b57f1a1e071ebd748984359fef83ecb026325b9d4ca847c95bc7311c92\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d454b8749b4bd70dd0a79f428731ee263fa6995f83ccb8bada706e8d1d3ff89d\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d807dc2051abe041b6649681dce568f8e10668e3c1c6543ebae58f2d7e617855\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c3c20f0ddeb6e29126d45f89206b8291352b8c5b44384e78a6499d68b52ae511\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:b7f19250ceef892adf27f0399b9e5afad019288e9be756d6919cb58892129f51\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-musllinux_1_2_aarch64.whl\", hash = \"sha256:4f1ed4749a08379555cebf4650453f14452eaa9c43d0a95c49db50c18b7da075\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-musllinux_1_2_i686.whl\", hash = \"sha256:dcedf0b42bcb4cfff4101d7771a10532415a6106062f005ab97d1d0ab5681c60\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-musllinux_1_2_x86_64.whl\", hash = \"sha256:39ed0d010457a78f54090fafb5d108501b5aa5604cc22408fc1c0c77eac14344\"},\n+    {file = \"rpds_py-0.20.0-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:bb273176be34a746bdac0b0d7e4e2c467323d13640b736c4c477881a3220a989\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:f918a1a130a6dfe1d7fe0f105064141342e7dd1611f2e6a21cd2f5c8cb1cfb3e\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:f60012a73aa396be721558caa3a6fd49b3dd0033d1675c6d59c4502e870fcf0c\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3d2b1ad682a3dfda2a4e8ad8572f3100f95fad98cb99faf37ff0ddfe9cbf9d03\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:614fdafe9f5f19c63ea02817fa4861c606a59a604a77c8cdef5aa01d28b97921\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:fa518bcd7600c584bf42e6617ee8132869e877db2f76bcdc281ec6a4113a53ab\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f0475242f447cc6cb8a9dd486d68b2ef7fbee84427124c232bff5f63b1fe11e5\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f90a4cd061914a60bd51c68bcb4357086991bd0bb93d8aa66a6da7701370708f\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:def7400461c3a3f26e49078302e1c1b38f6752342c77e3cf72ce91ca69fb1bc1\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-musllinux_1_2_aarch64.whl\", hash = \"sha256:65794e4048ee837494aea3c21a28ad5fc080994dfba5b036cf84de37f7ad5074\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-musllinux_1_2_i686.whl\", hash = \"sha256:faefcc78f53a88f3076b7f8be0a8f8d35133a3ecf7f3770895c25f8813460f08\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-musllinux_1_2_x86_64.whl\", hash = \"sha256:5b4f105deeffa28bbcdff6c49b34e74903139afa690e35d2d9e3c2c2fba18cec\"},\n+    {file = \"rpds_py-0.20.0-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:fdfc3a892927458d98f3d55428ae46b921d1f7543b89382fdb483f5640daaec8\"},\n+    {file = \"rpds_py-0.20.0.tar.gz\", hash = \"sha256:d72a210824facfdaf8768cf2d7ca25a042c30320b3020de2fa04640920d4e121\"},\n+]\n+\n+[[package]]\n+name = \"setuptools\"\n+version = \"56.0.0\"\n+description = \"Easily download, build, install, upgrade, and uninstall Python packages\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"setuptools-56.0.0-py3-none-any.whl\", hash = \"sha256:7430499900e443375ba9449a9cc5d78506b801e929fef4a186496012f93683b5\"},\n+    {file = \"setuptools-56.0.0.tar.gz\", hash = \"sha256:08a1c0f99455307c48690f00d5c2ac2c1ccfab04df00454fef854ec145b81302\"},\n+]\n+\n+[package.extras]\n+certs = [\"certifi (==2016.9.26)\"]\n+docs = [\"jaraco.packaging (>=8.2)\", \"pygments-github-lexers (==0.0.5)\", \"rst.linker (>=1.9)\", \"sphinx\", \"sphinx-inline-tabs\"]\n+ssl = [\"wincertstore (==0.2)\"]\n+testing = [\"flake8-2020\", \"jaraco.envs\", \"jaraco.path (>=3.2.0)\", \"mock\", \"paver\", \"pip (>=19.1)\", \"pytest (>=4.6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"pytest-flake8\", \"pytest-mypy\", \"pytest-virtualenv (>=1.2.7)\", \"pytest-xdist\", \"sphinx\", \"virtualenv (>=13.0.0)\", \"wheel\"]\n+\n+[[package]]\n+name = \"six\"\n+version = \"1.16.0\"\n+description = \"Python 2 and 3 compatibility utilities\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*\"\n+files = [\n+    {file = \"six-1.16.0-py2.py3-none-any.whl\", hash = \"sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\"},\n+    {file = \"six-1.16.0.tar.gz\", hash = \"sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926\"},\n+]\n+\n+[[package]]\n+name = \"sqlparse\"\n+version = \"0.5.1\"\n+description = \"A non-validating SQL parser.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"sqlparse-0.5.1-py3-none-any.whl\", hash = \"sha256:773dcbf9a5ab44a090f3441e2180efe2560220203dc2f8c0b0fa141e18b505e4\"},\n+    {file = \"sqlparse-0.5.1.tar.gz\", hash = \"sha256:bb6b4df465655ef332548e24f08e205afc81b9ab86cb1c45657a7ff173a3a00e\"},\n+]\n+\n+[package.extras]\n+dev = [\"build\", \"hatch\"]\n+doc = [\"sphinx\"]\n+\n+[[package]]\n+name = \"tomli\"\n+version = \"2.0.1\"\n+description = \"A lil' TOML parser\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"tomli-2.0.1-py3-none-any.whl\", hash = \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\"},\n+    {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\n+]\n+\n+[[package]]\n+name = \"typing-extensions\"\n+version = \"3.10.0.2\"\n+description = \"Backported and Experimental Type Hints for Python 3.5+\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"typing_extensions-3.10.0.2-py2-none-any.whl\", hash = \"sha256:d8226d10bc02a29bcc81df19a26e56a9647f8b0a6d4a83924139f4a8b01f17b7\"},\n+    {file = \"typing_extensions-3.10.0.2-py3-none-any.whl\", hash = \"sha256:f1d25edafde516b146ecd0613dabcc61409817af4766fbbcfb8d1ad4ec441a34\"},\n+    {file = \"typing_extensions-3.10.0.2.tar.gz\", hash = \"sha256:49f75d16ff11f1cd258e1b988ccff82a3ca5570217d7ad8c5f48205dd99a677e\"},\n+]\n+\n+[[package]]\n+name = \"typing-inspect\"\n+version = \"0.9.0\"\n+description = \"Runtime inspection utilities for typing module.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"typing_inspect-0.9.0-py3-none-any.whl\", hash = \"sha256:9ee6fc59062311ef8547596ab6b955e1b8aa46242d854bfc78f4f6b0eff35f9f\"},\n+    {file = \"typing_inspect-0.9.0.tar.gz\", hash = \"sha256:b23fc42ff6f6ef6954e4852c1fb512cdd18dbea03134f91f856a95ccc9461f78\"},\n+]\n+\n+[package.dependencies]\n+mypy-extensions = \">=0.3.0\"\n+typing-extensions = \">=3.7.4\"\n+\n+[[package]]\n+name = \"tzdata\"\n+version = \"2024.1\"\n+description = \"Provider of IANA time zone data\"\n+optional = false\n+python-versions = \">=2\"\n+files = [\n+    {file = \"tzdata-2024.1-py2.py3-none-any.whl\", hash = \"sha256:9068bc196136463f5245e51efda838afa15aaeca9903f49050dfa2679db4d252\"},\n+    {file = \"tzdata-2024.1.tar.gz\", hash = \"sha256:2674120f8d891909751c38abcdfd386ac0a5a1127954fbc332af6b5ceae07efd\"},\n+]\n+\n+[[package]]\n+name = \"urllib3\"\n+version = \"2.2.3\"\n+description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"urllib3-2.2.3-py3-none-any.whl\", hash = \"sha256:ca899ca043dcb1bafa3e262d73aa25c465bfb49e0bd9dd5d59f1d0acba2f8fac\"},\n+    {file = \"urllib3-2.2.3.tar.gz\", hash = \"sha256:e7d814a81dad81e6caf2ec9fdedb284ecc9c73076b62654547cc64ccdcae26e9\"},\n+]\n+\n+[package.extras]\n+brotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\n+h2 = [\"h2 (>=4,<5)\"]\n+socks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\n+zstd = [\"zstandard (>=0.18.0)\"]\n+\n+[[package]]\n+name = \"wps-light\"\n+version = \"0.15.3\"\n+description = \"The strictest and most opinionated python linter ever (lighter fork).\"\n+optional = false\n+python-versions = \">=3.6,<4.0\"\n+files = [\n+    {file = \"wps-light-0.15.3.tar.gz\", hash = \"sha256:e3040611340796427c346006298954c859ab12471cf31ec60d6254af95d70c37\"},\n+    {file = \"wps_light-0.15.3-py3-none-any.whl\", hash = \"sha256:1193bad23fd2bfd74c0cb2558f3dd66eb0619cfaee895a1c113252d1965d555e\"},\n+]\n+\n+[package.dependencies]\n+astor = \">=0.8,<0.9\"\n+attrs = \"*\"\n+flake8 = \">=3.7,<4.0\"\n+flake8-polyfill = \">=1.0.2,<2.0.0\"\n+pygments = \">=2.4,<3.0\"\n+typing_extensions = \">=3.6,<4.0\"\n+\n+[[package]]\n+name = \"wrapt\"\n+version = \"1.16.0\"\n+description = \"Module for decorators, wrappers and monkey patching.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"wrapt-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:ffa565331890b90056c01db69c0fe634a776f8019c143a5ae265f9c6bc4bd6d4\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:e4fdb9275308292e880dcbeb12546df7f3e0f96c6b41197e0cf37d2826359020\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bb2dee3874a500de01c93d5c71415fcaef1d858370d405824783e7a8ef5db440\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2a88e6010048489cda82b1326889ec075a8c856c2e6a256072b28eaee3ccf487\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ac83a914ebaf589b69f7d0a1277602ff494e21f4c2f743313414378f8f50a4cf\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:73aa7d98215d39b8455f103de64391cb79dfcad601701a3aa0dddacf74911d72\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:807cc8543a477ab7422f1120a217054f958a66ef7314f76dd9e77d3f02cdccd0\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:bf5703fdeb350e36885f2875d853ce13172ae281c56e509f4e6eca049bdfb136\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-win32.whl\", hash = \"sha256:f6b2d0c6703c988d334f297aa5df18c45e97b0af3679bb75059e0e0bd8b1069d\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:decbfa2f618fa8ed81c95ee18a387ff973143c656ef800c9f24fb7e9c16054e2\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:1a5db485fe2de4403f13fafdc231b0dbae5eca4359232d2efc79025527375b09\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:75ea7d0ee2a15733684badb16de6794894ed9c55aa5e9903260922f0482e687d\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a452f9ca3e3267cd4d0fcf2edd0d035b1934ac2bd7e0e57ac91ad6b95c0c6389\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:43aa59eadec7890d9958748db829df269f0368521ba6dc68cc172d5d03ed8060\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:72554a23c78a8e7aa02abbd699d129eead8b147a23c56e08d08dfc29cfdddca1\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:d2efee35b4b0a347e0d99d28e884dfd82797852d62fcd7ebdeee26f3ceb72cf3\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:6dcfcffe73710be01d90cae08c3e548d90932d37b39ef83969ae135d36ef3956\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:eb6e651000a19c96f452c85132811d25e9264d836951022d6e81df2fff38337d\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-win32.whl\", hash = \"sha256:66027d667efe95cc4fa945af59f92c5a02c6f5bb6012bff9e60542c74c75c362\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:aefbc4cb0a54f91af643660a0a150ce2c090d3652cf4052a5397fb2de549cd89\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:5eb404d89131ec9b4f748fa5cfb5346802e5ee8836f57d516576e61f304f3b7b\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:9090c9e676d5236a6948330e83cb89969f433b1943a558968f659ead07cb3b36\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:94265b00870aa407bd0cbcfd536f17ecde43b94fb8d228560a1e9d3041462d73\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f2058f813d4f2b5e3a9eb2eb3faf8f1d99b81c3e51aeda4b168406443e8ba809\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:98b5e1f498a8ca1858a1cdbffb023bfd954da4e3fa2c0cb5853d40014557248b\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:14d7dc606219cdd7405133c713f2c218d4252f2a469003f8c46bb92d5d095d81\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:49aac49dc4782cb04f58986e81ea0b4768e4ff197b57324dcbd7699c5dfb40b9\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:418abb18146475c310d7a6dc71143d6f7adec5b004ac9ce08dc7a34e2babdc5c\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-win32.whl\", hash = \"sha256:685f568fa5e627e93f3b52fda002c7ed2fa1800b50ce51f6ed1d572d8ab3e7fc\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:dcdba5c86e368442528f7060039eda390cc4091bfd1dca41e8046af7c910dda8\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:d462f28826f4657968ae51d2181a074dfe03c200d6131690b7d65d55b0f360f8\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a33a747400b94b6d6b8a165e4480264a64a78c8a4c734b62136062e9a248dd39\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b3646eefa23daeba62643a58aac816945cadc0afaf21800a1421eeba5f6cfb9c\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3ebf019be5c09d400cf7b024aa52b1f3aeebeff51550d007e92c3c1c4afc2a40\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:0d2691979e93d06a95a26257adb7bfd0c93818e89b1406f5a28f36e0d8c1e1fc\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:1acd723ee2a8826f3d53910255643e33673e1d11db84ce5880675954183ec47e\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:bc57efac2da352a51cc4658878a68d2b1b67dbe9d33c36cb826ca449d80a8465\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-win32.whl\", hash = \"sha256:da4813f751142436b075ed7aa012a8778aa43a99f7b36afe9b742d3ed8bdc95e\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-win_amd64.whl\", hash = \"sha256:6f6eac2360f2d543cc875a0e5efd413b6cbd483cb3ad7ebf888884a6e0d2e966\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:a0ea261ce52b5952bf669684a251a66df239ec6d441ccb59ec7afa882265d593\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:7bd2d7ff69a2cac767fbf7a2b206add2e9a210e57947dd7ce03e25d03d2de292\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9159485323798c8dc530a224bd3ffcf76659319ccc7bbd52e01e73bd0241a0c5\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a86373cf37cd7764f2201b76496aba58a52e76dedfaa698ef9e9688bfd9e41cf\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:73870c364c11f03ed072dda68ff7aea6d2a3a5c3fe250d917a429c7432e15228\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:b935ae30c6e7400022b50f8d359c03ed233d45b725cfdd299462f41ee5ffba6f\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:db98ad84a55eb09b3c32a96c576476777e87c520a34e2519d3e59c44710c002c\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-win32.whl\", hash = \"sha256:9153ed35fc5e4fa3b2fe97bddaa7cbec0ed22412b85bcdaf54aeba92ea37428c\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:66dfbaa7cfa3eb707bbfcd46dab2bc6207b005cbc9caa2199bcbc81d95071a00\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1dd50a2696ff89f57bd8847647a1c363b687d3d796dc30d4dd4a9d1689a706f0\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:44a2754372e32ab315734c6c73b24351d06e77ffff6ae27d2ecf14cf3d229202\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8e9723528b9f787dc59168369e42ae1c3b0d3fadb2f1a71de14531d321ee05b0\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:dbed418ba5c3dce92619656802cc5355cb679e58d0d89b50f116e4a9d5a9603e\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:941988b89b4fd6b41c3f0bfb20e92bd23746579736b7343283297c4c8cbae68f\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:6a42cd0cfa8ffc1915aef79cb4284f6383d8a3e9dcca70c445dcfdd639d51267\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:1ca9b6085e4f866bd584fb135a041bfc32cab916e69f714a7d1d397f8c4891ca\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:d5e49454f19ef621089e204f862388d29e6e8d8b162efce05208913dde5b9ad6\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-win32.whl\", hash = \"sha256:c31f72b1b6624c9d863fc095da460802f43a7c6868c5dda140f51da24fd47d7b\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:490b0ee15c1a55be9c1bd8609b8cecd60e325f0575fc98f50058eae366e01f41\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:9b201ae332c3637a42f02d1045e1d0cccfdc41f1f2f801dafbaa7e9b4797bfc2\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:2076fad65c6736184e77d7d4729b63a6d1ae0b70da4868adeec40989858eb3fb\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c5cd603b575ebceca7da5a3a251e69561bec509e0b46e4993e1cac402b7247b8\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b47cfad9e9bbbed2339081f4e346c93ecd7ab504299403320bf85f7f85c7d46c\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f8212564d49c50eb4565e502814f694e240c55551a5f1bc841d4fcaabb0a9b8a\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:5f15814a33e42b04e3de432e573aa557f9f0f56458745c2074952f564c50e664\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:db2e408d983b0e61e238cf579c09ef7020560441906ca990fe8412153e3b291f\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:edfad1d29c73f9b863ebe7082ae9321374ccb10879eeabc84ba3b69f2579d537\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-win32.whl\", hash = \"sha256:ed867c42c268f876097248e05b6117a65bcd1e63b779e916fe2e33cd6fd0d3c3\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:eb1b046be06b0fce7249f1d025cd359b4b80fc1c3e24ad9eca33e0dcdb2e4a35\"},\n+    {file = \"wrapt-1.16.0-py3-none-any.whl\", hash = \"sha256:6906c4100a8fcbf2fa735f6059214bb13b97f75b1a61777fcf6432121ef12ef1\"},\n+    {file = \"wrapt-1.16.0.tar.gz\", hash = \"sha256:5f370f952971e7d17c7d1ead40e49f32345a7f7a5373571ef44d800d06b1899d\"},\n+]\n+\n+[metadata]\n+lock-version = \"2.0\"\n+python-versions = \"~3.10\"\n+content-hash = \"313305578f918bbeb47a11a96f72df48fbabca013ad8ad2222f5ceb9518bd357\"\ndiff --git a/pyproject.toml b/pyproject.toml\nnew file mode 100644\nindex 00000000..55fabe23\n--- /dev/null\n+++ b/pyproject.toml\n@@ -0,0 +1,70 @@\n+[tool.poetry]\n+name = \"hyperstyle\"\n+version = \"1.5.0\"\n+description = \"A tool for running a set of pre-configured linters and evaluating code quality.\"\n+authors = [\"Hyperskill Team\"]\n+readme = \"README.md\"\n+keywords = [\"code review\"]\n+classifiers = [\n+    \"Development Status :: 5 - Production/Stable\",\n+    \"Environment :: Console\",\n+    \"Intended Audience :: Developers\",\n+    \"Intended Audience :: Education\",\n+    \"Intended Audience :: Science/Research\",\n+    \"License :: OSI Approved :: Apache Software License\",\n+    \"Operating System :: OS Independent\",\n+    \"Programming Language :: Python :: 3\",\n+    \"Topic :: Education\",\n+]\n+\n+[project.urls]\n+Homepage = \"https://support.hyperskill.org/hc/en-us/articles/360049582712-Code-style-Code-quality\"\n+Repository = \"https://github.com/hyperskill/hyperstyle\"\n+\n+[project.scripts]\n+review = \"hyperstyle.src.python.review.run_tool:main\"\n+\n+[tool.poetry.dependencies]\n+python = \"~3.10\"\n+setuptools = \"56.0.0\"\n+# python code analysis tools\n+pylint = \"2.13.9\"\n+pylint-django = \"2.5.3\"\n+flake8 = \"3.9.0\"\n+dataclasses-json=\"0.5.7\"\n+# flake8 plugins\n+flake8-plugin-utils = \"1.3.2\"\n+flake8-bugbear = \"21.4.3\"\n+flake8-builtins = \"1.5.3\"\n+flake8-comprehensions = \"3.4.0\"\n+flake8-eradicate = \"1.0.0\"\n+flake8-import-order = \"0.18.1\"\n+flake8-polyfill = \"1.0.2\"\n+flake8-return = \"1.1.2\"\n+flake8-spellcheck = \"0.24.0\"\n+mccabe = \"0.6.1\"\n+pep8-naming = \"0.11.1\"\n+wps-light = \"0.15.3\"\n+flake8-broken-line = \"0.3.0\"\n+flake8-string-format = \"0.3.0\"\n+flake8-commas = \"2.0.0\"\n+cohesion = \"1.0.0\"\n+radon = \"4.5.0\"\n+# extra libraries and frameworks\n+argparse = \"1.4.0\"\n+django = \"4.2.5\"\n+requests = \"2.32.3\"\n+# grpc\n+grpcio-tools = \"1.66.1\"\n+protobuf = \"5.27.2\"\n+\n+\n+[tool.poetry.group.dev.dependencies]\n+pytest = \"8.3.3\"\n+pytest-runner = \"6.0.1\"\n+pytest-subtests = \"0.13.1\"\n+jsonschema = \"4.23.0\"\n+\n+[build-system]\n+requires = [\"poetry-core\"]\n+build-backend = \"poetry.core.masonry.api\"\ndiff --git a/requirements.txt b/requirements.txt\ndeleted file mode 100644\nindex fd49f0a6..00000000\n--- a/requirements.txt\n+++ /dev/null\n@@ -1,30 +0,0 @@\n-setuptools==56.0.0\n-\n-# python code analysis tools\n-pylint==2.13.0\n-pylint-django==2.5.3\n-flake8==3.9.0\n-\n-# flake8 plugins\n-flake8-plugin-utils==1.3.2\n-flake8-bugbear==21.4.3\n-flake8-builtins==1.5.3\n-flake8-comprehensions==3.4.0\n-flake8-eradicate==1.0.0\n-flake8-import-order==0.18.1\n-flake8-polyfill==1.0.2\n-flake8-return==1.1.2\n-flake8-spellcheck==0.24.0\n-mccabe==0.6.1\n-pep8-naming==0.11.1\n-wps-light==0.15.2\n-flake8-broken-line==0.3.0\n-flake8-string-format==0.3.0\n-flake8-commas==2.0.0\n-cohesion==1.0.0\n-radon==4.5.0\n-\n-# extra libraries and frameworks\n-django==4.2.5\n-requests==2.25.1\n-argparse==1.4.0\n\\ No newline at end of file\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 33ebde4e..00000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,76 +0,0 @@\n-import os\n-from pathlib import Path\n-from typing import List\n-\n-from setuptools import find_packages, setup\n-\n-current_dir = Path(__file__).parent.absolute()\n-\n-\n-def get_long_description() -> str:\n-    with open(current_dir / 'README.md', encoding='utf-8') as f:\n-        return f.read()\n-\n-\n-def get_version() -> str:\n-    with open(current_dir / 'VERSION.md') as version_file:\n-        return version_file.read().replace('\\n', '')\n-\n-\n-def get_inspectors_additional_files() -> List[str]:\n-    inspectors_path = current_dir / 'hyperstyle' / 'src' / 'python' / 'review' / 'inspectors'\n-    configs = ['xml', 'yml', 'eslintrc', 'flake8', 'txt', 'pylintrc']\n-    result = []\n-    for root, _, files in os.walk(inspectors_path):\n-        for file in files:\n-            if not file.endswith('.py') and file.split('.')[-1] in configs:\n-                result.append(str(Path(root) / file))\n-    return result\n-\n-\n-def get_requires() -> List[str]:\n-    with open(current_dir / 'requirements.txt') as requirements_file:\n-        return requirements_file.read().split('\\n')\n-\n-\n-setup(\n-    name='hyperstyle',\n-    version=get_version(),\n-    description='A tool for running a set of pre-configured linters and evaluating code quality.',\n-    long_description=get_long_description(),\n-    long_description_content_type='text/markdown',\n-    url='https://github.com/hyperskill/hyperstyle',\n-    author='Stepik.org',\n-    author_email='ivan.magda@stepik.org',\n-    classifiers=[\n-        'Development Status :: 3 - Alpha',\n-        'Intended Audience :: Developers',\n-        'Topic :: Software Development :: Build Tools',\n-        'License :: OSI Approved :: MIT License',\n-        'Programming Language :: Python :: 3',\n-        'Operating System :: OS Independent',\n-    ],\n-    keywords='code review',\n-    python_requires='>=3.8, <4',\n-    install_requires=get_requires(),\n-    include_package_data=True,\n-    packages=find_packages(exclude=[\n-        '*.unit_tests',\n-        '*.unit_tests.*',\n-        'unit_tests.*',\n-        'unit_tests',\n-        '*.functional_tests',\n-        '*.functional_tests.*',\n-        'functional_tests.*',\n-        'functional_tests',\n-    ]),\n-    zip_safe=False,\n-    package_data={\n-        '': get_inspectors_additional_files(),\n-    },\n-    entry_points={\n-        'console_scripts': [\n-            'review=hyperstyle.src.python.review.run_tool:main',\n-        ],\n-    },\n-)\ndiff --git a/setup_environment.sh b/setup_environment.sh\nindex cf2bbe85..a782d594 100755\n--- a/setup_environment.sh\n+++ b/setup_environment.sh\n@@ -73,24 +73,13 @@ echo \"The variables are defined.\"\n \n echo\n \n-read -p \"Do you want to install Python requirements for the Hyperstyle project? (Y/n): \" -r\n+read -p \"Do you want to install Python development requirements for the Hyperstyle project? (Y/n): \" -r\n if [[ $REPLY =~ ^[Yy]$ ]]; then\n-  echo \"Installing Python requirements...\"\n-  pip install --no-cache-dir -r requirements.txt\n-  check_return_code $? \"Python requirements installed.\" \"Python requirements installation failed.\"\n+  echo \"Installing Python development requirements...\"\n+  pip install --no-cache-dir -r requirements-dev.txt\n+  check_return_code $? \"Python development requirements installed.\" \"Python development requirements installation failed.\"\n else\n-  echo \"Python requirements installation skipped.\"\n-fi\n-\n-echo\n-\n-read -p \"Do you want to install Python test requirements for the Hyperstyle project? (Y/n): \" -r\n-if [[ $REPLY =~ ^[Yy]$ ]]; then\n-  echo \"Installing Python test requirements...\"\n-  pip install --no-cache-dir -r requirements-test.txt\n-  check_return_code $? \"Python test requirements installed.\" \"Python test requirements installation failed.\"\n-else\n-  echo \"Python test requirements installation skipped.\"\n+  echo \"Python development requirements installation skipped.\"\n fi\n \n echo\n@@ -147,3 +136,7 @@ if need_to_install_linter \"golangci-lint\" \"${GOLANG_LINT_DIRECTORY}\"; then\n else\n   echo \"Golangci-lint ${GOLANG_LINT_VERSION} installation skipped.\"\n fi\n+\n+echo \"Generating proto files  ...\"\n+   export PROTO_PATH=\"hyperstyle/src/python/review/inspectors/common/inspector/proto\"\n+   python3 -m grpc_tools.protoc --proto_path=. --python_out=. --pyi_out=. --grpc_python_out=. ${PROTO_PATH}/model.proto\ndiff --git a/whitelist.txt b/whitelist.txt\nindex 50298483..ad4d1ecc 100644\n--- a/whitelist.txt\n+++ b/whitelist.txt\n@@ -9,6 +9,7 @@ ECMA\n EXPR\n G10\n IGNORECASE\n+IJ\n INTELLIJ\n Intelli\n JS\n@@ -33,6 +34,7 @@ astype\n atclause\n barmode\n bce\n+bdist\n bgcolor\n binarizer\n capsys\n@@ -42,6 +44,7 @@ cbo\n changelog\n checkstyle\n cloneable\n+cmdclass\n concat\n config\n configs\n@@ -91,6 +94,7 @@ exprs\n f1\n file\n filemode\n+filepath\n fillna\n formatter\n fs\n@@ -106,12 +110,14 @@ getuid\n gradle\n groupby\n groupdict\n+grpc\n hashtable\n hline\n hyperstyle\n idiv\n idx\n ignorecase\n+ij\n iloc\n inerop\n initializer\n@@ -182,6 +188,7 @@ parametrize\n params\n parsers\n pathlib\n+pb2\n pickler\n plotly\n pmd\n@@ -192,6 +199,8 @@ preprocess\n preprocessing\n pretrained\n println\n+proto\n+protoc\n punisher\n puppycrawl\n pyast\n@@ -207,6 +216,7 @@ removeprefix\n rfind\n rmdir\n runtime\n+sdist\n setdefault\n setslice\n showline\n", "instance_id": "hyperskill__hyperstyle-191", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Hyperstyle fails with a `ValueError` when encountering files with extensions like `.ipynb` and `.md` that are not in the defined `Extension` enum, instead of skipping them as expected. The goal is implied\u2014to modify Hyperstyle to ignore unsupported file extensions rather than crashing. The input (a directory with mixed file types) and desired output (Hyperstyle should process only supported files like `.py` and skip others) are indirectly specified through the error traceback and user expectation. However, there are minor ambiguities: the statement lacks explicit mention of edge cases (e.g., nested directories, symbolic links, or empty files) and does not provide detailed requirements or examples of expected behavior beyond the general idea of \"skipping\" unsupported files. Additionally, the problem statement does not clarify whether this behavior should be configurable or hardcoded. Despite these minor gaps, the issue is valid and understandable with the provided context and traceback, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The issue likely requires a targeted modification in the file extension handling logic, probably within the `file_system.py` or `language.py` modules as indicated by the traceback. The change would involve adding a check to skip files with unsupported extensions before attempting to process them. This is a localized fix, likely affecting a single function or a small set of related functions, without significant impact on the broader system architecture. However, the provided code changes in the diff are unrelated to the core issue (they focus on build configurations, Dockerfiles, and CI workflows), so I am assessing based on the problem statement alone, assuming the actual fix is not shown.\n\n2. **Technical Concepts Involved**: Solving this requires basic understanding of Python file handling, string manipulation (for extension extraction), and enum usage. The developer needs to comprehend how Hyperstyle's file exploration logic works (e.g., `guess_file_language` and `get_extension_from_file`) but does not need advanced knowledge of algorithms, design patterns, or domain-specific concepts beyond standard Python practices.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but a competent solution would need to consider scenarios like files without extensions, case sensitivity in extensions, or invalid file paths. These are relatively straightforward to handle with basic conditional checks, adding minimal complexity to the solution.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase and making a simple modification to filter out unsupported extensions. It does not require deep architectural changes, performance optimization, or complex debugging across multiple modules. The primary challenge is identifying the exact location in the code to implement the skip logic, which is manageable with the provided traceback.\n\nGiven these considerations, I assign a difficulty score of 0.30, reflecting an Easy problem that requires moderate code logic understanding and a straightforward fix, with minimal impact on the overall system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Mambaforge is deprecated\nOG-Core and its country calibrations currently use the Mambaforge installer as the method in our GitHub Actions of installing a small-footprint version of Python to run in the GH Actions. However, Mambaforge is being deprecated and Miniforge is taking its place (see this blog post \"[Sunsetting Mambaforge](https://conda-forge.org/news/2024/07/29/sunsetting-mambaforge/)). Key points are that Mambaforge is going away and Miniforge is taking its place. The sunsetting schedule listed in the link tells us that we will begin to experience more and more incovenient delays in our GH Actions until we make this switch. And Mambaforge will be completely gone by January 2025.\r\n\r\nI made [these changes](https://github.com/EAPD-DRB/UN-OG-Training/pull/4/files) in a PR in the UN-OG-Training repository as suggested by the [setup-miniconda README.md](https://github.com/conda-incubator/setup-miniconda), and they seem to work great. We need to make the same changes in OG-Core and its related country calibration repositories.\r\n\r\ncc: @jdebacker \n", "patch": "diff --git a/.github/workflows/deploy_docs.yml b/.github/workflows/deploy_docs.yml\nindex aa35afc26..dfa60e56e 100644\n--- a/.github/workflows/deploy_docs.yml\n+++ b/.github/workflows/deploy_docs.yml\n@@ -24,7 +24,7 @@ jobs:\n       - name: Setup Miniconda\n         uses: conda-incubator/setup-miniconda@v3\n         with:\n-          miniforge-variant: Mambaforge\n+          miniconda-version: \"latest\"\n           activate-environment: ogcore-dev\n           environment-file: environment.yml\n           python-version: \"3.12\"\ndiff --git a/.github/workflows/docs_check.yml b/.github/workflows/docs_check.yml\nindex 35714b60b..e82946d93 100644\n--- a/.github/workflows/docs_check.yml\n+++ b/.github/workflows/docs_check.yml\n@@ -22,7 +22,7 @@ jobs:\n       - name: Setup Miniconda\n         uses: conda-incubator/setup-miniconda@v3\n         with:\n-          miniforge-variant: Mambaforge\n+          miniconda-version: \"latest\"\n           activate-environment: ogcore-dev\n           environment-file: environment.yml\n           python-version: \"3.12\"\n", "instance_id": "PSLmodels__OG-Core-993", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the need to transition from Mambaforge to Miniforge due to the deprecation of Mambaforge, as outlined in the provided blog post. The goal of updating the GitHub Actions configuration to avoid delays and ensure compatibility is evident. The reference to a successful pull request in another repository and the specific code changes provided further clarify the intended solution. However, there are minor ambiguities: the statement does not explicitly mention potential risks or side effects of the transition (e.g., compatibility issues with existing dependencies or environment configurations). Additionally, while the blog post and PR provide context, the problem statement lacks detailed guidance on testing or validating the change across different repositories. Overall, it is clear enough to act upon but misses some minor details that could ensure a smoother implementation.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward modification to GitHub Actions workflow files. The scope of the code changes is minimal, limited to updating a single parameter in two YAML files (changing 'miniforge-variant: Mambaforge' to 'miniconda-version: \"latest\"'). This does not require deep understanding of the codebase, complex logic, or interactions between modules, as it is a configuration change rather than a functional or architectural modification. The technical concepts involved are basic\u2014familiarity with GitHub Actions and the `setup-miniconda` action is sufficient, and no advanced programming language features, algorithms, or domain-specific knowledge are required. There are no explicit edge cases or error handling considerations mentioned in the problem statement or evident in the code changes, and the risk of unintended consequences appears low given the simplicity of the update and the precedent set by the referenced pull request. Overall, this is a very easy task that can be completed quickly by someone with basic familiarity with GitHub Actions.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Changes to `groupby.size()` don't work for all possible groupby \n**Describe the bug**\r\n```python\r\nimport pandas as pd\r\nddf = pd.DataFrame({\"a\": [1, 1, 2], \"b\": [2, 3, 2]})\r\nreveal_type(ddf.groupby([\"a\", \"b\"]).size())\r\n\r\n```\r\npyright reports\r\n```text\r\n - information: Type of \"ddf.groupby([\"a\", \"b\"]).size()\" is \"Unknown\"\r\n  c:\\Code\\pandas-stubs\\play\\issue1044.py:3:37 - error: Cannot access attribute \"size\" for class \"DataFrameGroupBy[tuple[Unknown, ...], bool]\"\r\n  \u00a0\u00a0Could not bind method \"size\" because \"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to parameter \"self\"\r\n  \u00a0\u00a0\u00a0\u00a0\"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to \"DataFrameGroupBy[tuple[Unknown, ...], Literal[True]]\"\r\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type parameter \"_TT@DataFrameGroupBy\" is invariant, but \"bool\" is not the same as \"Literal[True]\"\r\n  \u00a0\u00a0Could not bind method \"size\" because \"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to parameter \"self\"\r\n  \u00a0\u00a0\u00a0\u00a0\"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to \"DataFrameGroupBy[tuple[Unknown, ...], Literal[False]]\"\r\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type parameter \"_TT@DataFrameGroupBy\" is invariant, but \"bool\" is not the same as \"Literal[False]\"\r\n  \u00a0\u00a0Could not bind method \"size\" because \"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to parameter \"self\"\r\n  \u00a0\u00a0\u00a0\u00a0\"DataFrameGroupBy[tuple[Unknown, ...], bool]\" is not assignable to \"DataFrameGroupBy[Timestamp, Literal[True]]\" (reportAttributeAccessIssue)\r\n```\r\nmypy reports\r\n```python\r\nnote: Revealed type is \"Any\"\r\n```\r\n\r\nThe fix here is that any of the methods in `frame.pyi` for `def groupby()` that return `DataFrameGroupBy[SOMETYPE, bool]` need to be split into 2 where there is one overload that returns `DataFrameGroupBy[SOMETYPE, True]` and another that returns `DataFrameGroupBy[SOMETYPE, False]` like is done with `DataFrameGroupBy[Scalar, True]` and `DataFrameGroupBy[Scalar, False]` .\r\n\r\n**Please complete the following information:**\r\n - OS:  Windows 11\r\n - python version   3.10\r\n - version of type checker   pyright 1.1.389, mypy 1.13.0\r\n - version of installed `pandas-stubs`:   Dev version\r\n\r\n\r\n**Additional context**\r\n\r\nThis was missed in PR #1014 by @loicdiridollou .  Hopefully he can fix soon, since the prerelease of VS Code pylance has this bug.\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 295aad8b..b574fb21 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -1112,7 +1112,7 @@ class DataFrame(NDFrame, OpsMixin):\n         dropna: _bool = ...,\n     ) -> DataFrameGroupBy[Timestamp, Literal[True]]: ...\n     @overload\n-    def groupby(\n+    def groupby(  # pyright: ignore reportOverlappingOverload\n         self,\n         by: DatetimeIndex,\n         axis: AxisIndex | NoDefault = ...,\n@@ -1124,77 +1124,149 @@ class DataFrame(NDFrame, OpsMixin):\n         dropna: _bool = ...,\n     ) -> DataFrameGroupBy[Timestamp, Literal[False]]: ...\n     @overload\n-    def groupby(\n+    def groupby(  # pyright: ignore reportOverlappingOverload\n         self,\n         by: TimedeltaIndex,\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[True] = True,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[Timedelta, bool]: ...\n+    ) -> DataFrameGroupBy[Timedelta, Literal[True]]: ...\n     @overload\n     def groupby(\n+        self,\n+        by: TimedeltaIndex,\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[False] = ...,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[Timedelta, Literal[False]]: ...\n+    @overload\n+    def groupby(  # pyright: ignore reportOverlappingOverload\n         self,\n         by: PeriodIndex,\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[True] = True,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[Period, bool]: ...\n+    ) -> DataFrameGroupBy[Period, Literal[True]]: ...\n     @overload\n     def groupby(\n+        self,\n+        by: PeriodIndex,\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[False] = ...,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[Period, Literal[False]]: ...\n+    @overload\n+    def groupby(  # pyright: ignore reportOverlappingOverload\n         self,\n         by: IntervalIndex[IntervalT],\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[True] = True,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[IntervalT, bool]: ...\n+    ) -> DataFrameGroupBy[IntervalT, Literal[True]]: ...\n     @overload\n     def groupby(\n+        self,\n+        by: IntervalIndex[IntervalT],\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[False] = ...,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[IntervalT, Literal[False]]: ...\n+    @overload\n+    def groupby(  # type: ignore[overload-overlap] # pyright: ignore reportOverlappingOverload\n         self,\n         by: MultiIndex | GroupByObjectNonScalar | None = ...,\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[True] = True,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[tuple, bool]: ...\n+    ) -> DataFrameGroupBy[tuple, Literal[True]]: ...\n+    @overload\n+    def groupby(  # type: ignore[overload-overlap]\n+        self,\n+        by: MultiIndex | GroupByObjectNonScalar | None = ...,\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[False] = ...,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[tuple, Literal[False]]: ...\n+    @overload\n+    def groupby(  # pyright: ignore reportOverlappingOverload\n+        self,\n+        by: Series[SeriesByT],\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[True] = True,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[SeriesByT, Literal[True]]: ...\n     @overload\n     def groupby(\n         self,\n         by: Series[SeriesByT],\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[False] = ...,\n+        sort: _bool = ...,\n+        group_keys: _bool = ...,\n+        observed: _bool | NoDefault = ...,\n+        dropna: _bool = ...,\n+    ) -> DataFrameGroupBy[SeriesByT, Literal[False]]: ...\n+    @overload\n+    def groupby(\n+        self,\n+        by: CategoricalIndex | Index | Series,\n+        axis: AxisIndex | NoDefault = ...,\n+        level: IndexLabel | None = ...,\n+        as_index: Literal[True] = True,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[SeriesByT, bool]: ...\n+    ) -> DataFrameGroupBy[Any, Literal[True]]: ...\n     @overload\n     def groupby(\n         self,\n         by: CategoricalIndex | Index | Series,\n         axis: AxisIndex | NoDefault = ...,\n         level: IndexLabel | None = ...,\n-        as_index: _bool = ...,\n+        as_index: Literal[False] = ...,\n         sort: _bool = ...,\n         group_keys: _bool = ...,\n         observed: _bool | NoDefault = ...,\n         dropna: _bool = ...,\n-    ) -> DataFrameGroupBy[Any, bool]: ...\n+    ) -> DataFrameGroupBy[Any, Literal[False]]: ...\n     def pivot(\n         self,\n         *,\n", "instance_id": "pandas-dev__pandas-stubs-1046", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to type checking with `groupby.size()` in the pandas-stubs library. It provides a reproducible code snippet, error messages from both pyright and mypy, and a clear explanation of the root cause (type mismatch in `DataFrameGroupBy` return types). The suggested fix\u2014splitting overloads for `as_index` into `True` and `False`\u2014is also outlined. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases or constraints (e.g., compatibility with different pandas versions or type checker configurations), and it lacks detailed examples beyond the provided snippet. Additionally, the impact on other parts of the library or downstream users is not addressed. Overall, it is clear enough for someone familiar with type hints and pandas to understand the issue, but it misses some comprehensive details.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes are confined to a single file (`frame.pyi`) and involve modifying type annotations for the `groupby` method. The diff shows a moderate amount of code change, primarily repetitive updates to overload signatures, splitting `bool` into `Literal[True]` and `Literal[False]` for various input types. There is no impact on the system's architecture, as this is purely a type hint adjustment with no runtime behavior changes.\n\n2. **Technical Concepts Required:** Solving this requires a basic understanding of Python type hints, specifically overloads using `@overload`, and familiarity with pandas' type stub structure (`DataFrameGroupBy` and its type parameters). Knowledge of type checkers like pyright and mypy is necessary to interpret the error messages and ensure compatibility. These concepts are not overly complex for someone with intermediate Python experience, especially in the context of type annotations.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not involve runtime logic or error handling modifications. The focus is solely on static type checking, so edge case complexity is minimal. However, one might need to consider if these changes affect other type checker behaviors or pandas versions, though this is not highlighted as a concern.\n\n4. **Overall Complexity:** The task involves straightforward, mechanical updates to type signatures with a clear pattern (replacing `bool` with explicit `Literal[True]` and `Literal[False]`). It requires some understanding of the pandas-stubs codebase and type system intricacies, but it does not demand deep architectural changes or complex debugging. The primary challenge lies in ensuring all relevant overloads are updated consistently, which is more tedious than difficult.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires understanding type hints and making systematic, localized changes without significant complexity or broader impact.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "DataFrame.pct_change typing for None missing\n**Describe the bug**\r\nA clear and concise description of what the bug is.\r\nFor the `pd.DataFrame.pct_change` the documentation allows the `None` method but the typing only supports `str`.\r\nThis results in an error from pyright to be raised.\r\n\r\n\r\n**To Reproduce**\r\n1. Provide a minimal runnable `pandas` example that is not properly checked by the stubs.\r\n2. Indicate which type checker you are using (`mypy` or  `pyright`).\r\n3. Show the error message received from that type checker while checking your example.\r\n\r\n```python\r\ndf = pd.DataFrame({\"x\": [1, 2, 2, 3, 3], \"y\": [10, 20, 30, 40, 50]})\r\ndf.pct_change(fill_method=None)\r\n```\r\n\r\nThis example will yield the error:\r\n```\r\nerror: Argument \"fill_method\" to \"pct_change\" of \"DataFrame\" has incompatible type \"None\"; expected \"str\"  [arg-type]\r\n```\r\n\r\nWhile the documentation allows for None:\r\n<img width=\"745\" alt=\"Screenshot 2024-09-03 at 6 04 46\u202fPM\" src=\"https://github.com/user-attachments/assets/c75f9152-f354-4e08-8440-153d55b767a2\">\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: MacOs\r\n - OS Version Sonoma 14\r\n - python version 3.12.4\r\n - version of type checker 1.1.378\r\n - version of installed `pandas-stubs` v2.2.2.240807\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 0e151c4a..3102e02f 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -1874,7 +1874,7 @@ class DataFrame(NDFrame, OpsMixin):\n     def pct_change(\n         self,\n         periods: int = ...,\n-        fill_method: _str = ...,\n+        fill_method: None = ...,\n         limit: int | None = ...,\n         freq=...,\n         **kwargs,\n", "instance_id": "pandas-dev__pandas-stubs-993", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear and provides a valid description of the bug related to the typing of the `fill_method` parameter in `pd.DataFrame.pct_change`. It includes a minimal reproducible example, specifies the type checker used (pyright), and shows the exact error message, which helps in understanding the issue. Additionally, it references the documentation allowing `None` as a valid input, supported by a screenshot. However, there are minor details missing, such as explicit mention of expected behavior when `fill_method=None` (though implied by the documentation) and any potential edge cases or side effects of changing the typing. Overall, the statement is clear enough to understand the goal but lacks some depth in discussing implications or constraints.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward fix in the type annotation of a single parameter in a type stub file (`pandas-stubs/core/frame.pyi`). The code change is minimal, affecting only one line, and does not require deep understanding of the codebase or complex logic. It is a simple typing adjustment from `fill_method: _str` to `fill_method: None`, with no impact on the system's architecture or interactions between modules. The technical concepts involved are basic\u2014understanding Python type hints and how type checkers like pyright interpret them. There are no significant edge cases or error handling requirements mentioned or implied in the problem statement or code change. This task falls into the \"very easy\" category, requiring only a basic modification to resolve the issue.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Series apply frozenset raises error, frozenset not in enumerated Callable output types\n**Describe the bug**\r\nSeries.apply has two overloads, the first one enumerates the possible return types of a Callable, this is missing frozenset (probably others as well).\r\n\r\nhttps://github.com/pandas-dev/pandas-stubs/blob/23b35e48fb6b159d9ddab7f007ede26ae8a05564/pandas-stubs/core/series.pyi#L930-L937\r\n\r\n**To Reproduce**\r\n\r\n1. Provide a minimal runnable `pandas` example that is not properly checked by the stubs.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns = pd.Series([\"A\", \"B\", \"AB\"])\r\n\r\ns.apply(tuple)\r\ns.apply(list)\r\ns.apply(set)\r\ns.apply(frozenset)\r\n```\r\n\r\n2. Indicate which type checker you are using (`mypy` or  `pyright`).\r\n\r\n`mypy`\r\n\r\n3. Show the error message received from that type checker while checking your example.\r\n\r\n```none\r\nseries-apply.py:8: error: Argument 1 to \"apply\" of \"Series\" has incompatible type \"type[frozenset[Any]]\"; expected \"Callable[..., str | bytes | date | datetime | timedelta | datetime64 | timedelta64 | bool | int | float | Timestamp | Timedelta | complex | Sequence[Any] | set[Any] | Mapping[Any, Any] | NAType | None]\"  [arg-type]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: MacOS\r\n - OS Version 14.1.2 (23B92)\r\n - python version 3.11.4 (pyenv managed)\r\n - version of type checker `mypy==1.8.0`\r\n - version of installed `pandas-stubs==2.1.4.231227`\r\n\n", "patch": "diff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex d38b54da..58e09d6f 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -939,7 +939,9 @@ class Series(IndexOpsMixin[S1], NDFrame):\n     @overload\n     def apply(\n         self,\n-        func: Callable[..., Scalar | Sequence | set | Mapping | NAType | None],\n+        func: Callable[\n+            ..., Scalar | Sequence | set | Mapping | NAType | frozenset | None\n+        ],\n         convertDType: _bool = ...,\n         args: tuple = ...,\n         **kwds,\n", "instance_id": "pandas-dev__pandas-stubs-903", "clarity": 3, "difficulty": 0.1, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the `Series.apply` method of the pandas-stubs library, specifically pointing out that the `frozenset` type is missing from the enumerated return types of a Callable in the type hints. The statement includes a minimal reproducible example, specifies the type checker used (`mypy`), and provides the exact error message received. Additionally, it references the specific lines of code in the GitHub repository where the issue exists. There are no significant ambiguities, and the goal (adding `frozenset` to the type hints) is explicitly defined with supporting details such as OS, Python version, and library versions. The only minor omission is the lack of discussion on potential edge cases or other missing types beyond `frozenset`, but this does not detract from the overall clarity of the core issue.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to the type hints in a single file (`series.pyi`). The code change is minimal, requiring only the addition of `frozenset` to the list of possible return types in the `apply` method's type signature. This does not impact the broader codebase architecture or require understanding complex interactions between modules. The technical concepts involved are basic\u2014familiarity with Python type hints and the `pandas-stubs` library's purpose is sufficient, and no advanced algorithms, design patterns, or domain-specific knowledge are needed. There are no edge cases or error handling requirements mentioned in the problem statement, and the provided code change does not introduce any. Overall, this is a very easy task that can be completed quickly by someone with basic knowledge of Python type annotations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "depend on torchdata 0.8.0 instead of nightly\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #513\r\n\r\ntorchdata 0.8.0 is officially released (https://github.com/pytorch/data/releases), which contains the `StatefulDataLoader` we use.\r\nWe can remove the temporary nightly install.\n", "patch": "diff --git a/.ci/docker/requirements.txt b/.ci/docker/requirements.txt\nindex 71669e74..b9460590 100644\n--- a/.ci/docker/requirements.txt\n+++ b/.ci/docker/requirements.txt\n@@ -1,4 +1,5 @@\n torch >= 2.3.0\n+torchdata >= 0.8.0\n datasets >= 2.19.0\n tomli >= 1.1.0 ; python_version < \"3.11\"\n tensorboard\ndiff --git a/README.md b/README.md\nindex a02a7cc4..f659e9e9 100644\n--- a/README.md\n+++ b/README.md\n@@ -64,7 +64,6 @@ git clone https://github.com/pytorch/torchtitan\n cd torchtitan\n pip install -r requirements.txt\n pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121 # or cu118\n-pip3 install --pre torchdata --index-url https://download.pytorch.org/whl/nightly\n ```\n \n ### Downloading a tokenizer\n", "instance_id": "pytorch__torchtitan-513", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to update the dependency from a nightly version of `torchdata` to the officially released version 0.8.0, as it now includes the required `StatefulDataLoader` feature. The goal is straightforward, and the context (release of `torchdata 0.8.0`) is provided with a reference link. However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly mention whether there are any compatibility issues or additional considerations when switching from a nightly build to a stable release. It also lacks information on potential impacts to the codebase or testing requirements to validate the change. While the intent is clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a simple update to dependency versions in a requirements file and a corresponding update to the installation instructions in the README. The scope of code changes is minimal, affecting only two files (`.ci/docker/requirements.txt` and `README.md`) with straightforward modifications\u2014adding a line for `torchdata >= 0.8.0` and removing a line for the nightly installation. There is no complex logic, deep understanding of the codebase, or architectural impact involved. No specific technical concepts, algorithms, or domain knowledge beyond basic dependency management are required. Additionally, there are no edge cases or error handling considerations mentioned or implied in the problem or code changes. This task falls into the \"very easy\" category, requiring only basic modifications to configuration and documentation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Cursewords flag\nI'm generating adjective-noun pairs as job names for my projects.\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nI would like *not* to serve my users random words like 'ugly fuck' or 'nasty shit'.\r\n\r\n**Describe the solution you'd like**\r\nI would like to pass a flag to the generator, specifying whether I want curse words sprinkled into my random words. This flag would default to False (no curse words).\r\n\r\n**Describe alternatives you've considered**\r\nI considered just leaving it as-is and laughing when somebody complains about their random words being something like 'massive cock'.\n", "patch": "diff --git a/.github/workflows/python-package.yml b/.github/workflows/python-package.yml\nindex 5ccca83..66e425d 100644\n--- a/.github/workflows/python-package.yml\n+++ b/.github/workflows/python-package.yml\n@@ -5,9 +5,9 @@ name: Python package\n \n on:\n   push:\n-    branches: [ master ]\n+    branches: [ master, \"v*\" ]\n   pull_request:\n-    branches: [ master ]\n+    branches: [ master, \"v*\" ]\n \n jobs:\n   build:\n@@ -15,26 +15,32 @@ jobs:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n-        python-version: [3.6, 3.7, 3.8, 3.9]\n+        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n \n     steps:\n-    - uses: actions/checkout@v2\n+    - uses: actions/checkout@v4\n     - name: Set up Python ${{ matrix.python-version }}\n-      uses: actions/setup-python@v2\n+      uses: actions/setup-python@v5\n       with:\n         python-version: ${{ matrix.python-version }}\n     - name: Install dependencies\n       run: |\n         python -m pip install --upgrade pip\n-        python -m pip install flake8 pytest\n-        # if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n-        python -m pip install .\n+        python -m pip install flake8 pytest mypy\n+        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n+        \n+        python -m pip install build\n+        python -m build\n+        python -m pip install dist/*.whl\n     - name: Lint with flake8\n       run: |\n         # stop the build if there are Python syntax errors or undefined names\n-        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n+        python -m flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n         # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n-        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n+        python -m flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n+    - name: Type check with MyPy\n+      run: |\n+        python -m mypy wonderwords\n     - name: Test with pytest\n       run: |\n-        pytest\n+        python -m pytest\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 21b55ad..f707abc 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -5,7 +5,33 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html)\n since version 2.\n \n-## [Unreleased]\n+## [3.0.0] - Unreleased\n+\n+### Breaking Changes\n+\n+- This version officially supports Python 3.8+\n+- The command-line interface has changed. Shell scripts relying on\n+  Wonderwords need to be updated\n+\n+### Added\n+\n+- Speed improvements when using `starts_with` and `ends_with`\n+- More efficient batch operations and regular expression compilation\n+- Idiomatic CLI commands\n+- Profanity filtering\n+- Support for custom ``Random`` objects and determinism when running\n+  with a specific seed.\n+\n+### Fixed\n+\n+- Fixes and updates to word lists\n+- Fixed bug where ``return_less_if_necessary`` would produce result\n+  in incorrect filtering\n+\n+### Removed\n+\n+- Dropped support for Python 3.6 and 3.7\n+- No more dependency on ``importlib_resources``\n \n ## [2.2.0] - 2021-02-17\n \ndiff --git a/LICENSE b/LICENSE\nindex 0ffd785..582adc8 100644\n--- a/LICENSE\n+++ b/LICENSE\n@@ -1,6 +1,6 @@\n MIT License\n \n-Copyright (c) 2020 mrmaxguns\n+Copyright (c) 2020 Maxim Rebguns\n \n Permission is hereby granted, free of charge, to any person obtaining a copy\n of this software and associated documentation files (the \"Software\"), to deal\ndiff --git a/README.md b/README.md\nindex cb7e4b3..2586a76 100644\n--- a/README.md\n+++ b/README.md\n@@ -1,26 +1,18 @@\n-<p align=\"center\">\n-  <img width=\"460\" height=\"300\" src=\"assets/main.gif\">\n-</p>\n-\n-<h1 align=\"center\">Wonderwords</h1>\n-<p align=\"center\">Generate random words and sentences with ease in python</p>\n-<p align=\"center\">\n-  <img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/wonderwords?style=for-the-badge\">\n-  <img alt=\"Libraries.io SourceRank\" src=\"https://img.shields.io/librariesio/sourcerank/pypi/wonderwords?style=for-the-badge\">\n-  <img alt=\"PyPI - License\" src=\"https://img.shields.io/pypi/l/wonderwords?style=for-the-badge\">\n-</p>\n-<p align=\"center\">\n-  <img alt=\"Python package build\" src=\"https://github.com/mrmaxguns/wonderwordsmodule/workflows/Python%20package/badge.svg\">\n-</p>\n-<p align=\"center\">\n-  <a href=\"https://github.com/mrmaxguns/wonderwordsmodule\">GitHub repository</a>\n-  | <a href=\"https://pypi.org/project/wonderwords\">PyPI page</a>\n-  | <a href=\"https://wonderwords.readthedocs.io\">Official Documentation</a>\n-</p>\n+# Wonderwords\n+\n+*Generate random words and sentences with ease in python*\n+\n+<img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/wonderwords?style=for-the-badge\">\n+<img alt=\"Libraries.io SourceRank\" src=\"https://img.shields.io/librariesio/sourcerank/pypi/wonderwords?style=for-the-badge\">\n+<img alt=\"PyPI - License\" src=\"https://img.shields.io/pypi/l/wonderwords?style=for-the-badge\">\n+\n+[GitHub Repository](https://github.com/mrmaxguns/wonderwordsmodule) |\n+[PyPI](https://pypi.org/project/wonderwords) |\n+[Documentation](https://wonderwords.readthedocs.io)\n \n ***\n \n-Wonderwords is a python package useful for generating random words and\n+Wonderwords is a Python package useful for generating random words and\n structured random sentences. It also comes with a colorful command line\n interface for quickly generating random words. The latest version is available\n [on GitHub](https://github.com/mrmaxguns/wonderwordsmodule) while the stable\n@@ -42,10 +34,12 @@ version is available [on PyPI](https://pypi.org/project/wonderwords).\n \n Here's what Wonderwords is capable of:\n \n-- Random word generation\n+- Random word generation in English\n - Specify word length, what it starts and ends with, category, and even custom\n-  regular expressions!\n+  regular expressions\n - Use custom word lists and define custom categories of words\n+- Generate structured random sentences\n+- Basic profanity filtering\n - Beautiful command line interface\n - Easy-to-use interface and comprehensive documentation\n - Open source!\n@@ -103,9 +97,13 @@ r.word(include_parts_of_speech=[\"nouns\", \"adjectives\"])\n # generate a random word between the length of 3 and 8 characters\n r.word(word_min_length=3, word_max_length=8)\n \n-# generate a random word with a custom regular expression\n+# generate a random word with a custom Python regular expression\n r.word(regex=\".*a\")\n \n+# some of the words in the default word lists have spaces, such as 'contact lens'\n+# this option disables them\n+r.word(exclude_with_spaces=True)\n+\n # you can combine multiple filtering options\n r.word(starts_with=\"ru\", word_max_length=10, include_parts_of_speech=[\"verbs\"])\n ```\n@@ -164,44 +162,96 @@ s.bare_bone_with_adjective()\n s.sentence()\n ```\n \n+Words are organized in categories, such as \"nouns\", \"verbs\", and \"adjectives\".\n+What if you had your own categories of words? You can specify your custom\n+categories when instantiating the `RandomWord` class:\n+\n+```python\n+from wonderwords import RandomWord\n+\n+cars = [\"Chevrolet\", \"Subaru\", \"Tesla\"]\n+airplanes = [\"Boeing\", \"Airbus\", \"Cessna\"]\n+w = RandomWord(cars=cars, airplanes=airplanes)\n+\n+# Will return a random car or airplane\n+w.word()\n+\n+# Will return a random car\n+w.word(include_categories=[\"cars\"])\n+\n+# You can also mix and match custom categories with defaults\n+from wonderwords import Defaults\n+proper_nouns = [\"Austin\", \"Seattle\", \"New York\"]\n+w2 = RandomWord(proper_nouns=proper_nouns, common_nouns=Defaults.NOUNS)\n+\n+# Will return either Seattle or seat\n+w.word(regex=\"[Ss]eat.*\")\n+```\n+\n+Finally, starting with version 2.3, Wonderwords has explicit support for filtering\n+out profanities from lists of words. At the moment, this is rudimentary:\n+\n+```python\n+from wonderwords import is_profanity, filter_profanity\n+\n+# Test against words that could possibly be offensive. Good of user-facing apps.\n+is_profanity(\"apple\") # False\n+\n+# Can be done with a list\n+words = [ ... ]\n+# The function returns a generator, so we convert it to a list\n+words_clean = list(filter_profanity(words))\n+```\n+\n More advanced usage (and a tutorial!) is found in the documentation, such as\n adding custom categories of words. The full documentation with all information\n can be found at: https://wonderwords.readthedocs.io\n \n ## The Wonderwords CLI\n \n+**NOTE**: Before using the command-line interface (CLI), ensure that you installed\n+all required dependencies for the CLI using `pip install wonderwords[cli]`.\n+Wonderwords normally requires no dependencies, but uses Rich for colorized\n+output in the command line.\n+\n Wonderwords provides a command line interface, too, which can be used with the\n `wonderwords` command. Usage:\n \n ```\n-usage: wonderwords [-h] [-w] [-f] [-l LIST] [-s {bb,ss,bba,s}] [-v] [-sw STARTS_WITH] [-ew ENDS_WITH]\n-                   [-p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]] [-min WORD_MIN_LENGTH]\n-                   [-max WORD_MAX_LENGTH] [-r REGEX] [-d DELIMITER]\n+usage: wonderwords [-h] [-w] [-f] [-l LIST] [-s {bb,ss,bba,s}] [-v] [-S STARTS_WITH] [-e ENDS_WITH]\n+                   [-p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]]\n+                   [-m WORD_MIN_LENGTH] [-M WORD_MAX_LENGTH] [-r REGEX] [-x] [-d DELIMITER] [-E]\n \n-optional arguments:\n+Generate random words and sentences from the command line. Here is a full list of available commands. To learn more\n+about each command, go to the documentation at https://wonderwords.readthedocs.io\n+\n+options:\n   -h, --help            show this help message and exit\n   -w, --word, --random-word\n                         generate a random word\n-  -f, --filter          filter a list of words matching the criteria specified\n-  -l LIST, --list LIST  return a list of words of a certain length\n+  -f, --filter          get a list of all known words matching the criteria specified\n+  -l LIST, --list LIST  return a list of a certain length of random words\n   -s {bb,ss,bba,s}, --sentence {bb,ss,bba,s}\n                         return a sentence based on the structure chosen\n-  -v, --version         Print the version number and exit\n-  -sw STARTS_WITH, --starts-with STARTS_WITH\n-                        specify what string the random word(s) should start with\n-  -ew ENDS_WITH, --ends-with ENDS_WITH\n-                        specify what string the random word(s) should end with\n+  -v, --version         print the version number and exit\n+  -S STARTS_WITH, --starts-with STARTS_WITH\n+                        strings the random word(s) should start with\n+  -e ENDS_WITH, --ends-with ENDS_WITH\n+                        strings the random word(s) should end with\n   -p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...], --parts-of-speech {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]\n-                        specify to only include certain parts of speech (by default all parts of speech are included)\n-  -min WORD_MIN_LENGTH, --word-min-length WORD_MIN_LENGTH\n-                        specify the minimum length of the word(s)\n-  -max WORD_MAX_LENGTH, --word-max-length WORD_MAX_LENGTH\n-                        specify the maximum length of the word(s)\n+                        only include certain parts of speech (by default all parts of speech are included)\n+  -m WORD_MIN_LENGTH, --word-min-length WORD_MIN_LENGTH\n+                        minimum length of the word(s)\n+  -M WORD_MAX_LENGTH, --word-max-length WORD_MAX_LENGTH\n+                        maximum length of the word(s)\n   -r REGEX, --regex REGEX, --re REGEX, --regular-expression REGEX\n-                        specify a python-style regular expression that every word must match\n+                        a python-style regular expression for the word(s) to match\n+  -x, --exclude-with-spaces\n+                        exclude open compounds, such as 'contact lens'\n   -d DELIMITER, --delimiter DELIMITER\n-                        Specify the delimiter to put between a list of words, default is ', '\n-\n+                        specify the delimiter to put between a list of words, default is ', '\n+  -E, --suppress-error-on-less\n+                        suppress errors when less words are returned in a list then wanted\n ```\n \n The basic commands are:\n@@ -229,7 +279,7 @@ for more details.\n \n # Contributing\n \n-All contributions are welcome and we hope Wonderwords will continue growing.\n+All contributions are welcome, and we hope Wonderwords will continue growing.\n Start out by reading `CONTRIBUTING.md` for contributing guidelines and how to\n get started.\n \ndiff --git a/assets/config.yml b/assets/config.yml\ndeleted file mode 100644\nindex 82ac3c9..0000000\n--- a/assets/config.yml\n+++ /dev/null\n@@ -1,107 +0,0 @@\n-# Specify a command to be executed\n-# like `/bin/bash -l`, `ls`, or any other commands\n-# the default is bash for Linux\n-# or powershell.exe for Windows\n-command: null\n-\n-# Specify the current working directory path\n-# the default is the current working directory path\n-cwd: /home/maxim/\n-\n-# Export additional ENV variables\n-env:\n-  recording: true\n-\n-# Explicitly set the number of columns\n-# or use `auto` to take the current\n-# number of columns of your shell\n-cols: auto\n-\n-# Explicitly set the number of rows\n-# or use `auto` to take the current\n-# number of rows of your shell\n-rows: auto\n-\n-# Amount of times to repeat GIF\n-# If value is -1, play once\n-# If value is 0, loop indefinitely\n-# If value is a positive number, loop n times\n-repeat: 0\n-\n-# Quality\n-# 1 - 100\n-quality: 100\n-\n-# Delay between frames in ms\n-# If the value is `auto` use the actual recording delays\n-frameDelay: auto\n-\n-# Maximum delay between frames in ms\n-# Ignored if the `frameDelay` isn't set to `auto`\n-# Set to `auto` to prevent limiting the max idle time\n-maxIdleTime: 2000\n-\n-# The surrounding frame box\n-# The `type` can be null, window, floating, or solid`\n-# To hide the title use the value null\n-# Don't forget to add a backgroundColor style with a null as type\n-frameBox:\n-  type: floating\n-  title: Wonderwords\n-  style:\n-    border: 0px black solid\n-    # boxShadow: none\n-    # margin: 0px\n-\n-# Add a watermark image to the rendered gif\n-# You need to specify an absolute path for\n-# the image on your machine or a URL, and you can also\n-# add your own CSS styles\n-watermark:\n-  imagePath: null\n-  style:\n-    position: absolute\n-    right: 15px\n-    bottom: 15px\n-    width: 100px\n-    opacity: 0.9\n-\n-# Cursor style can be one of\n-# `block`, `underline`, or `bar`\n-cursorStyle: block\n-\n-# Font family\n-# You can use any font that is installed on your machine\n-# in CSS-like syntax\n-fontFamily: \"Monaco, Lucida Console, Ubuntu Mono, Monospace\"\n-\n-# The size of the font\n-fontSize: 12\n-\n-# The height of lines\n-lineHeight: 1\n-\n-# The spacing between letters\n-letterSpacing: 0\n-\n-# Theme\n-theme:\n-  background: \"transparent\"\n-  foreground: \"#afafaf\"\n-  cursor: \"#c7c7c7\"\n-  black: \"#232628\"\n-  red: \"#fc4384\"\n-  green: \"#b3e33b\"\n-  yellow: \"#ffa727\"\n-  blue: \"#75dff2\"\n-  magenta: \"#ae89fe\"\n-  cyan: \"#708387\"\n-  white: \"#d5d5d0\"\n-  brightBlack: \"#626566\"\n-  brightRed: \"#ff7fac\"\n-  brightGreen: \"#c8ed71\"\n-  brightYellow: \"#ebdf86\"\n-  brightBlue: \"#75dff2\"\n-  brightMagenta: \"#ae89fe\"\n-  brightCyan: \"#b1c6ca\"\n-  brightWhite: \"#f9f9f4\"\ndiff --git a/assets/main.gif b/assets/main.gif\ndeleted file mode 100644\nindex bf94300..0000000\nBinary files a/assets/main.gif and /dev/null differ\ndiff --git a/assets/main.yml b/assets/main.yml\ndeleted file mode 100644\nindex 1a357c6..0000000\n--- a/assets/main.yml\n+++ /dev/null\n@@ -1,607 +0,0 @@\n-# The configurations that used for the recording, feel free to edit them\n-config:\n-\n-  # Specify a command to be executed\n-  # like `/bin/bash -l`, `ls`, or any other commands\n-  # the default is bash for Linux\n-  # or powershell.exe for Windows\n-  command: bash -l\n-\n-  # Specify the current working directory path\n-  # the default is the current working directory path\n-  cwd: /home/maxim\n-\n-  # Export additional ENV variables\n-  env:\n-    recording: true\n-\n-  # Explicitly set the number of columns\n-  # or use `auto` to take the current\n-  # number of columns of your shell\n-  cols: 100\n-\n-  # Explicitly set the number of rows\n-  # or use `auto` to take the current\n-  # number of rows of your shell\n-  rows: 24\n-\n-  # Amount of times to repeat GIF\n-  # If value is -1, play once\n-  # If value is 0, loop indefinitely\n-  # If value is a positive number, loop n times\n-  repeat: 0\n-\n-  # Quality\n-  # 1 - 100\n-  quality: 100\n-\n-  # Delay between frames in ms\n-  # If the value is `auto` use the actual recording delays\n-  frameDelay: auto\n-\n-  # Maximum delay between frames in ms\n-  # Ignored if the `frameDelay` isn't set to `auto`\n-  # Set to `auto` to prevent limiting the max idle time\n-  maxIdleTime: 2000\n-\n-  # The surrounding frame box\n-  # The `type` can be null, window, floating, or solid`\n-  # To hide the title use the value null\n-  # Don't forget to add a backgroundColor style with a null as type\n-  frameBox:\n-    type: floating\n-    title: Wonderwords\n-    style:\n-      border: 0px black solid\n-      # boxShadow: none\n-      # margin: 0px\n-\n-  # Add a watermark image to the rendered gif\n-  # You need to specify an absolute path for\n-  # the image on your machine or a URL, and you can also\n-  # add your own CSS styles\n-  watermark:\n-    imagePath: null\n-    style:\n-      position: absolute\n-      right: 15px\n-      bottom: 15px\n-      width: 100px\n-      opacity: 0.9\n-\n-  # Cursor style can be one of\n-  # `block`, `underline`, or `bar`\n-  cursorStyle: block\n-\n-  # Font family\n-  # You can use any font that is installed on your machine\n-  # in CSS-like syntax\n-  fontFamily: \"Monaco, Lucida Console, Ubuntu Mono, Monospace\"\n-\n-  # The size of the font\n-  fontSize: 12\n-\n-  # The height of lines\n-  lineHeight: 1\n-\n-  # The spacing between letters\n-  letterSpacing: 0\n-\n-  # Theme\n-  theme:\n-    background: \"transparent\"\n-    foreground: \"#afafaf\"\n-    cursor: \"#c7c7c7\"\n-    black: \"#232628\"\n-    red: \"#fc4384\"\n-    green: \"#b3e33b\"\n-    yellow: \"#ffa727\"\n-    blue: \"#75dff2\"\n-    magenta: \"#ae89fe\"\n-    cyan: \"#708387\"\n-    white: \"#d5d5d0\"\n-    brightBlack: \"#626566\"\n-    brightRed: \"#ff7fac\"\n-    brightGreen: \"#c8ed71\"\n-    brightYellow: \"#ebdf86\"\n-    brightBlue: \"#75dff2\"\n-    brightMagenta: \"#ae89fe\"\n-    brightCyan: \"#b1c6ca\"\n-    brightWhite: \"#f9f9f4\"\n-\n-# Records, feel free to edit them\n-records:\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: '#'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: W\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: '!'\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 200\n-    content: \"\\e[1;38;5;17;47m\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\\e[0m\\r\\n\\e[1;38;5;17;47m\u2502\\e[0m\\e[1;38;5;17;47m                                                                                                  \\e[0m\\e[1;38;5;17;47m\u2502\\e[0m\\r\\n\\e[1;38;5;17;47m\u2502\\e[0m\\e[1;38;5;17;47m \\e[0m\\e[1;38;5;17;47m                                      WONDERWORDS 2.0.0a1                                       \\e[0m\\e[1;38;5;17;47m \\e[0m\\e[1;38;5;17;47m\u2502\\e[0m\\r\\n\\e[1;38;5;17;47m\u2502\\e[0m\\e[1;38;5;17;47m                                                                                                  \\e[0m\\e[1;38;5;17;47m\u2502\\e[0m\\r\\n\\e[1;38;5;17;47m\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\\e[0m\\r\\n                                                                                                    \\r\\n \\e[1m                                       No commands given \\U0001F61E                                       \\e[0m \\r\\n                                                                                                    \\r\\n\"\n-  - delay: 140\n-    content: \"\\r\\n                                         \\e[1;4mAvailable Commands\\e[0m                                         \\r\\n\\r\\n\\e[1;33m \u2022 \\e[0m\\e[97;40mwonderwords -w\\e[0m - generate a random word                                                          \\r\\n\\e[1;33m \u2022 \\e[0m\\e[97;40mwonderwords -f\\e[0m - get all words matching a certain criteria                                       \\r\\n\\e[1;33m \u2022 \\e[0m\\e[97;40mwonderwords -l AMOUNT\\e[0m - get a list of \\e[97;40mAMOUNT\\e[0m random words                                        \\r\\n\\e[1;33m \u2022 \\e[0m\\e[97;40mwonderwords -s SENT_TYPE\\e[0m - generate a random sentence of a certain type                          \\r\\n\\r\\nFor a list of all options, type \\e[97;40mwonderwords -h\\e[0m. To see a detailed and comprehensive explanation of  \\r\\nthe commands, visit \\e]8;id=1599667951.0427635-512407;https://wonderwords.readthedocs.io\\e\\\\\\e[94mthe documentation\\e[0m\\e]8;;\\e\\\\                                                               \\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 1000\n-    content: '#'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: l\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: ''''\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: g\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: m\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '-'\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 140\n-    content: \"\\e[1;37;48;5;17mplastic\\e[0m\\r\\n\"\n-  - delay: 140\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 1000\n-    content: '#'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: h\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: b\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: u\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '5'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: '?'\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '-'\n-  - delay: 140\n-    content: l\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '5'\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 140\n-    content: \"\\e[1;37;48;5;17mspark, hashtag, citron, doctrine, promote\\e[0m\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 1000\n-    content: '#'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: l\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: u\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: g\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: m\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: c\n-  - delay: 140\n-    content: e\n-  - delay: 100\n-    content: \"\\r\\n\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '-'\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 100\n-    content: \"\\e[1;37;48;5;22mThe zany casino unites backpack.\\e[0m\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 1000\n-    content: '#'\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: c\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: l\n-  - delay: 140\n-    content: ','\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: h\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: b\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: u\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: h\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: i\n-  - delay: 140\n-    content: t\n-  - delay: 140\n-    content: h\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: '?'\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 100\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 300\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: 'n'\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: e\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: o\n-  - delay: 140\n-    content: r\n-  - delay: 140\n-    content: d\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '-'\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: '-'\n-  - delay: 140\n-    content: s\n-  - delay: 140\n-    content: w\n-  - delay: 140\n-    content: ' '\n-  - delay: 140\n-    content: a\n-  - delay: 140\n-    content: \"\\r\\n\"\n-  - delay: 140\n-    content: \"\\e[1;37;48;5;17mappreciate\\e[0m\\r\\n\"\n-  - delay: 140\n-    content: \"\\e]0;maxim@maxim-laptop:~\\a\\e]7;file://maxim-laptop/home/maxim\\a\\e]0;maxim@maxim-laptop: ~\\a\\e[01;32mmaxim@maxim-laptop\\e[00m:\\e[01;34m~\\e[00m$ \"\n-  - delay: 600\n-    content: ' '\ndiff --git a/docs/advanced.rst b/docs/advanced.rst\nindex 5b18f02..79481a8 100644\n--- a/docs/advanced.rst\n+++ b/docs/advanced.rst\n@@ -26,20 +26,19 @@ of words in those categories.\n \n Here's an example::\n \n-  from wonderwords import RandomWord\n+    from wonderwords import RandomWord\n \n-  fruits = [\"apple\", \"orange\", \"banana\", \"strawberry\"]\n-  generator = RandomWord(fruit=fruits)\n+    fruits = [\"apple\", \"orange\", \"banana\", \"strawberry\"]\n+    generator = RandomWord(fruits=fruits)\n \n-  print(generator.word()) # ex: apple\n+    print(generator.word()) # ex: apple\n \n Let's break this down:\n \n 1. First, we import the ``RandomWord`` class from ``wonderwords``.\n 2. We then define a list of fruits.\n 3. After that, we create a new instance of ``RandomWord``, where we create a\n-   category called ``fruit``. Normally, we define category names in the singular\n-   form, so ``fruit`` instead of ``fruits``.\n+   category called ``fruits``.\n 4. We print a random word from all the available categories.\n \n All of the arguments we had when generating random words from the default word\n@@ -49,37 +48,37 @@ custom categories.\n \n We can add as many word lists as we want::\n \n-  animals = [\"cow\", \"cat\", \"dog\", \"elephant\"]\n-  plants = [\"tree\", \"grass\", \"sunflower\"]\n-  generator2 = RandomWord(animal=animals, plant=plants)\n+    animals = [\"cow\", \"cat\", \"dog\", \"elephant\"]\n+    plants = [\"tree\", \"grass\", \"sunflower\"]\n+    generator2 = RandomWord(animals=animals, plants=plants)\n \n-  print(generator2.word()) # ex: grass (all categories are enabled by default)\n-  print(generator2.word()) # ex: cat\n-  print(generator2.word(include_categories=[\"animal\"])) # ex: dog\n+    print(generator2.word()) # ex: grass (all categories are enabled by default)\n+    print(generator2.word()) # ex: cat\n+    print(generator2.word(include_categories=[\"animals\"])) # ex: dog\n \n As illustrated in the example above, we can include only specific categories\n with ``include_categories``. We have already seen this argument before, when\n-specifying parts of speech, such as \"noun\" and \"verb\". But now, we can no longer\n+specifying parts of speech, such as \"nouns\" and \"verbs\". But now, we can no longer\n generate random nouns, verbs, and adjectives. The following code won't work::\n \n-  generator2.word(include_categories=[\"noun\"])\n-  # ValueError: 'noun' is an invalid category\n-  # :(\n+    generator2.word(include_categories=[\"nouns\"])\n+    # ValueError: 'noun' is an invalid category\n+    # :(\n \n This is because when we specify custom categories, the default configuration is\n overwritten. What if we want both a custom category, and one of the default\n categories as well? This can be done with ``Defaults``::\n \n-  from wonderwords import RandomWord, Defaults\n+    from wonderwords import RandomWord, Defaults\n \n-  writing_utensils = [\"graphite pencil\", \"pen\", \"marker\", \"colored pencil\"]\n-  generator = RandomWord(\n-      utensil=writing_utensils,\n-      adjective=Defaults.ADJECTIVES\n-  )\n-  print(generator.word()) # ex: angry\n-  print(generator.word(include_categories=[\"utensil\"])) # ex: marker\n-  print(generator.word(include_categories=[\"adjective\"])) # ex: sparkling\n+    writing_utensils = [\"graphite pencil\", \"pen\", \"marker\", \"colored pencil\"]\n+    generator = RandomWord(\n+        utensils=writing_utensils,\n+        adjectives=Defaults.ADJECTIVES\n+    )\n+    print(generator.word()) # ex: angry\n+    print(generator.word(include_categories=[\"utensils\"])) # ex: marker\n+    print(generator.word(include_categories=[\"adjectives\"])) # ex: sparkling\n \n ``Defaults`` is a python object that has a number of constants representing\n various default categories. We can specify one of these categories instead of\n@@ -100,24 +99,24 @@ With all of that, let's get back to our random name generator. First, we'll do\n some initial setup. Put the following lines at the top of\n ``random_name_generator.py``::\n \n-  from wonderwords import RandomWord\n+    from wonderwords import RandomWord\n \n-  # Note: here, we put the names in a list, but when you're writing code with\n-  # large lists, you typically put them in a file, and read from there.\n-  FIRST_NAMES = [\"Jane\", \"Bob\", \"Anne\", \"Max\", \"Jake\"]\n-  LAST_NAMES = [\"Jacobson\", \"Johnson\", \"Miller\", \"Rodriguez\", \"Davis\"]\n+    # Note: here, we put the names in a list, but when you're writing code with\n+    # large lists, you typically put them in a file, and read from there.\n+    FIRST_NAMES = [\"Jane\", \"Bob\", \"Anne\", \"Max\", \"Jake\"]\n+    LAST_NAMES = [\"Jacobson\", \"Johnson\", \"Miller\", \"Rodriguez\", \"Davis\"]\n \n Here we import ``RandomWord`` and create a list of first names, and a list of\n surnames. Now, let's create an instance of the ``RandomWord`` class::\n \n-  generator = RandomWord(name=FIRST_NAMES, surname=LAST_NAMES)\n+    generator = RandomWord(names=FIRST_NAMES, surnames=LAST_NAMES)\n \n Here we create a random word generator with two categories: ``name`` and\n ``surname``. We pass the lists we defined earlier to the categories. Now it's\n time to write our ``main`` function, where the bulk of our code will reside::\n \n-  def main():\n-      while True:\n+    def main():\n+        while True:\n           # We put this here, so that the user can chose to generate another\n           # word or quit.\n           action = input(\"Generate (enter) or quit (q) \").strip()\n@@ -125,8 +124,8 @@ time to write our ``main`` function, where the bulk of our code will reside::\n           if action.lower() == \"q\":\n               break\n \n-          first_name = generator.word(include_categories=[\"name\"])\n-          last_name = generator.word(include_categories=[\"surname\"])\n+          first_name = generator.word(include_categories=[\"names\"])\n+          last_name = generator.word(include_categories=[\"surnames\"])\n           print(first_name, last_name)\n       print(\"Thanks for using the generator!\")\n \n@@ -141,11 +140,32 @@ specify the categories used. Finally, we print the generated full name.\n \n The only thing left is to call our main function::\n \n-  if __name__ == \"__main__\":\n-      main()\n+    if __name__ == \"__main__\":\n+        main()\n \n In the code above, we call the ``main`` function as long as we run the code\n directly. If someone imports our code, the ``main`` function won't run.\n \n+Filtering Profanity\n+^^^^^^^^^^^^^^^^^^^\n+\n+Wonderwords supports a basic way to filter words that may be unsafe or profane\n+for applications using the word list represented by ``Defaults.PROFANITIES``. There\n+are two main functions to deal with profanity detection and removal.\n+\n+The first function, ``is_profanity`` returns ``True`` if the word was found in\n+the Wonderwords profanity list. The second, ``filter_profanity``, takes an iterator\n+of words and returns a generator with the same words, but with any possible curse\n+words filtered out::\n+\n+    from wonderwords import is_profanity, filter_profanity\n+\n+    is_profanity(\"apple\") # False\n+    is_profanity(\"piss\") # True\n+\n+    words = [\"apple\", \"PiSS \", \" orange\"]\n+    # Convert the generator into a list\n+    list(filter_profanity(words)) # [\"apple\", \" orange\"]\n+\n That's it! If you've read this far, you have completely mastered Wonderwords.\n Go on, and put your newly learned skills to practice.\ndiff --git a/docs/api_docs/api.rst b/docs/api_docs/api.rst\nnew file mode 100644\nindex 0000000..7d99cf2\n--- /dev/null\n+++ b/docs/api_docs/api.rst\n@@ -0,0 +1,21 @@\n+\n+API Documentation\n+=================\n+\n+.. autoexception:: wonderwords.NoWordsToChooseFrom\n+\n+.. autoclass:: wonderwords.Defaults\n+    :members:\n+    :undoc-members:\n+\n+.. autoclass:: wonderwords.RandomWord\n+    :members:\n+    :undoc-members:\n+\n+.. autoclass:: wonderwords.RandomSentence\n+    :members:\n+    :undoc-members:\n+\n+.. autofunction:: wonderwords.is_profanity\n+\n+.. autofunction:: wonderwords.filter_profanity\ndiff --git a/docs/api_docs/cli.rst b/docs/api_docs/cli.rst\nindex 7b94b91..b82cfd8 100644\n--- a/docs/api_docs/cli.rst\n+++ b/docs/api_docs/cli.rst\n@@ -5,83 +5,56 @@ Command Line Interface\n ======================\n \n The Wonderwords command line interface can be accessed with the ``wonderwords``\n-command. Usage::\n-\n-  usage: wonderwords [-h] [-w] [-f] [-l LIST] [-s {bb,ss,bba,s}] [-v] [-sw STARTS_WITH] [-ew ENDS_WITH]\n-                     [-p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]] [-min WORD_MIN_LENGTH]\n-                     [-max WORD_MAX_LENGTH] [-r REGEX] [-d DELIMITER]\n-\n-  optional arguments:\n-    -h, --help            show this help message and exit\n-    -w, --word, --random-word\n-                          generate a random word\n-    -f, --filter          filter a list of words matching the criteria specified\n-    -l LIST, --list LIST  return a list of words of a certain length\n-    -s {bb,ss,bba,s}, --sentence {bb,ss,bba,s}\n-                          return a sentence based on the structure chosen\n-    -v, --version         Print the version number and exit\n-    -sw STARTS_WITH, --starts-with STARTS_WITH\n-                          specify what string the random word(s) should start with\n-    -ew ENDS_WITH, --ends-with ENDS_WITH\n-                          specify what string the random word(s) should end with\n-    -p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...], --parts-of-speech {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]\n-                          specify to only include certain parts of speech (by default all parts of speech are included)\n-    -min WORD_MIN_LENGTH, --word-min-length WORD_MIN_LENGTH\n-                          specify the minimum length of the word(s)\n-    -max WORD_MAX_LENGTH, --word-max-length WORD_MAX_LENGTH\n-                          specify the maximum length of the word(s)\n-    -r REGEX, --regex REGEX, --re REGEX, --regular-expression REGEX\n-                          specify a python-style regular expression that every word must match\n-    -d DELIMITER, --delimiter DELIMITER\n-                          Specify the delimiter to put between a list of words, default is ', '\n-\n-Core commands\n--------------\n-\n-There are a number of core commands that provide basic functionality:\n-\n-* ``-w`` or ``--random-word``: generate a random word\n-* ``-f`` or ``--filter``: return a list of words based on specified criteria\n-* ``-l LIST`` or ``--list LIST``: much like filter, except you need to specify\n-  an integer of the amount of words you want to get\n-* ``-s {bb,ss,bba,s}`` or ``--sentence {bb,ss,bba,s}`` generate a sentence. You\n-  must specify the sentence type from the following types:\n-\n-    * ``bb``: bare bone sentence\n-    * ``ss``: simple sentence (bare bone sentence with a direct object)\n-    * ``bba``: bare bone sentence with an adjective\n-    * ``s``: simple sentence with an adjective\n-\n-* ``-v`` or ``--version``: return the version and exit\n-* ``-h`` or ``--help``: show a list of commands\n-\n-Other commands\n---------------\n-\n-The following commands apply only to ``random-word``, ``filter`` and ``list``:\n-\n-* ``-sw`` or ``--starts-with``: the string the word(s) must start with\n-* ``-ew`` or ``--ends-with``:  the string the word(s) must end with\n-* ``-p`` or ``--parts-of-speech``: only include certain parts of speech, choose\n-  one or more from nouns, verbs and adjectives\n-* ``-min`` or ``--word-min-length``: the minimum length of each word\n-* ``-max`` or ``--word-max-length``: the maximum length of each word\n-* ``-r`` or ``--regex`` or ``--re`` or ``--regular-expression``: use a custom\n-  python regular expression in order to filter a word. All words that do not\n-  fully match the expression will be removed.\n-\n-The following commands apply only to ``filter`` and ``list``:\n-\n-* ``-d DELIMITER`` or ``--delimiter DELIMITER``: the delimiter used to separate\n-  words. Is \", \" by default.\n+command. Usage\n+\n+.. code-block:: text\n+\n+    usage: wonderwords [-h] [-w] [-f] [-l LIST] [-s {bb,ss,bba,s}] [-v] [-S STARTS_WITH]\n+                       [-e ENDS_WITH]\n+                       [-p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]]\n+                       [-m WORD_MIN_LENGTH] [-M WORD_MAX_LENGTH] [-r REGEX] [-x] [-d DELIMITER]\n+                       [-E]\n+\n+    Generate random words and sentences from the command line. Here is a full list of available\n+    commands. To learn more about each command, go to the documentation at\n+    https://wonderwords.readthedocs.io\n+\n+    options:\n+      -h, --help            show this help message and exit\n+      -w, --word, --random-word\n+                            generate a random word\n+      -f, --filter          get a list of all known words matching the criteria specified\n+      -l LIST, --list LIST  return a list of a certain length of random words\n+      -s {bb,ss,bba,s}, --sentence {bb,ss,bba,s}\n+                            return a sentence based on the structure chosen\n+      -v, --version         print the version number and exit\n+      -S STARTS_WITH, --starts-with STARTS_WITH\n+                            strings the random word(s) should start with\n+      -e ENDS_WITH, --ends-with ENDS_WITH\n+                            strings the random word(s) should end with\n+      -p {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...], --parts-of-speech {noun,verb,adjective,nouns,verbs,adjectives} [{noun,verb,adjective,nouns,verbs,adjectives} ...]\n+                            only include certain parts of speech (by default all parts of speech\n+                            are included)\n+      -m WORD_MIN_LENGTH, --word-min-length WORD_MIN_LENGTH\n+                            minimum length of the word(s)\n+      -M WORD_MAX_LENGTH, --word-max-length WORD_MAX_LENGTH\n+                            maximum length of the word(s)\n+      -r REGEX, --regex REGEX, --re REGEX, --regular-expression REGEX\n+                            a python-style regular expression for the word(s) to match\n+      -x, --exclude-with-spaces\n+                            exclude open compounds, such as 'contact lens'\n+      -d DELIMITER, --delimiter DELIMITER\n+                            specify the delimiter to put between a list of words, default is ', '\n+      -E, --suppress-error-on-less\n+                            suppress errors when less words are returned in a list then wanted\n \n Examples\n --------\n \n ::\n \n-  $ wonderwords -w -sw ma -max 5 # choose a random word which starts with \"ma\" and is a max length of 5\n-  $ wonderwords -f -sw a -ew k -p nouns # choose all nouns that start with a, and ends with k\n-  $ wonderwords -l 5 -sw n -d \" & \" # random a list of 5 nouns separated by \" & \" that start with \"n\"\n+  $ wonderwords -w -S ma -M 5 # choose a random word which starts with \"ma\" and is a max length of 5\n+  $ wonderwords -f -S a -e k -p nouns # choose all nouns that start with a, and ends with k\n+  $ wonderwords -l 5 -S n -d \" & \" # random a list of 5 nouns separated by \" & \" that start with \"n\"\n   $ wonderwords -s bb # generate a random bare-bone sentence\n   $ wonderwords -v # print the version\ndiff --git a/docs/api_docs/index.rst b/docs/api_docs/index.rst\nindex 83aa57b..f9e69e9 100644\n--- a/docs/api_docs/index.rst\n+++ b/docs/api_docs/index.rst\n@@ -11,6 +11,5 @@ start with :ref:`the quickstart <quickstart>`.\n .. toctree::\n   :maxdepth: 2\n \n-  random_word\n-  random_sentence\n+  api\n   cli\ndiff --git a/docs/api_docs/random_sentence.rst b/docs/api_docs/random_sentence.rst\ndeleted file mode 100644\nindex b1db6aa..0000000\n--- a/docs/api_docs/random_sentence.rst\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-\n-wonderwords.random_sentence\n-===========================\n-\n-.. automodule:: wonderwords.random_sentence\n-    :members:\n-    :show-inheritance:\n-    :exclude-members:\ndiff --git a/docs/api_docs/random_word.rst b/docs/api_docs/random_word.rst\ndeleted file mode 100644\nindex 9734586..0000000\n--- a/docs/api_docs/random_word.rst\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-\n-wonderwords.random_word\n-=======================\n-\n-.. automodule:: wonderwords.random_word\n-    :members:\n-    :show-inheritance:\n-    :exclude-members: read_words\ndiff --git a/docs/conf.py b/docs/conf.py\nindex dc0017e..c8abf05 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -18,8 +18,8 @@\n # -- Project information -----------------------------------------------------\n \n project = \"Wonderwords\"\n-copyright = \"2020, Maxim R (mrmaxguns)\"\n-author = \"Maxim R (mrmaxguns)\"\n+copyright = \"2020, Maxim Rebguns (mrmaxguns)\"\n+author = \"Maxim Rebguns (mrmaxguns)\"\n \n \n # -- General configuration ---------------------------------------------------\ndiff --git a/docs/index.rst b/docs/index.rst\nindex f917567..36526ae 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -4,29 +4,30 @@ Wonderwords Official Documentation\n ==================================\n \n Wonderwords is a python package with a command line interface with the purpose\n-of generating random words and sentences. Wonderwords is **free** and **open\n-source**, meaning anyone can contribute to its repository.\n+of generating random words and sentences. Wonderwords is free software under\n+the MIT license. Contributions to the library are welcome.\n+\n+While Wonderwords supports English by default, custom word lists can be\n+used to generate and filter words in other languages as well.\n \n Here is a simple example::\n \n-  >>> from wonderwords import RandomWord\n-  >>>\n-  >>> generator = RandomWord()\n-  >>> generator.word()\n-  'stomach'\n+    >>> from wonderwords import RandomWord\n+    >>> generator = RandomWord()\n+    >>> generator.word()\n+    'stomach'\n \n The random words can also be further customized::\n \n-  >>> generator.word(starts_with=\"ap\", include_categories=[\"adjective\"])\n-  'apathetic'\n+    >>> generator.word(starts_with=\"ap\", include_categories=[\"adjectives\"])\n+    'apathetic'\n \n-With Wonderwords you can also generate lists of words and sentences, though\n-the sentence feature is in its early development. To install, head over to the\n-:ref:`install <install_uninstall_upgrade>` page. The :ref:`quickstart <quickstart>` is a great\n-place to get started with Wonderwords.\n+With Wonderwords you can also generate lists of words and sentences.\n+To install, head over to the :ref:`install <install_uninstall_upgrade>` page.\n+The :ref:`quickstart <quickstart>` is a great place to get started with Wonderwords.\n \n-A full reference of all modules (the API documentation) can be found\n-:ref:`here <docs>`.\n+A full reference of all modules can be found at the :ref:`API documentation <docs>`\n+page.\n \n Documentation\n -------------\ndiff --git a/docs/install.rst b/docs/install.rst\nindex 6c960bd..c3d6a0b 100644\n--- a/docs/install.rst\n+++ b/docs/install.rst\n@@ -9,7 +9,7 @@ Prerequisites\n \n Before proceeding, make sure you have the following items installed:\n \n-* `Python <https://python.org>`_ version 3.6 or greater\n+* `Python <https://python.org>`_ version 3.8 or greater\n * A python package manager, such as `pip <https://pip.pypa.io/en/stable/installing/>`_\n \n Install\n@@ -20,29 +20,37 @@ install the package ``wonderwords``:\n \n .. code-block:: bash\n \n-  $ pip install wonderwords\n+    $ pip install wonderwords\n \n Once installed, verify that it was correctly installed by going to a python\n console and typing::\n \n-  >>> import wonderwords\n+    >>> import wonderwords\n \n If you get a ``ModuleNotFoundError``, python cannot find Wonderwords. If you\n have trouble with installation, create a new issue at the Wonderwords\n `GitHub page <https://github.com/mrmaxguns/wonderwordsmodule>`_.\n \n+To install the command-line interface, additional dependencies are required\n+by Wonderwords. To do so, install the ``cli`` dependency category. With pip,\n+you can do so with:\n+\n+.. code-block:: bash\n+\n+    $ pip install wonderwords[cli]\n+\n The command line interface can be accessed with the ``wonderwords`` command.\n Verify the command line interface by typing:\n \n .. code-block:: bash\n \n-  $ wonderwords -v\n+    $ wonderwords -v\n \n You should get an output similar to the following:\n \n .. raw:: html\n \n-  <pre><span style=\"background-color:#D3D7CF\"><font color=\"#00005F\">Running wonderwords version </font></span><span style=\"background-color:#D3D7CF\"><font color=\"#729FCF\"><b>2.0</b></font></span><span style=\"background-color:#D3D7CF\"><font color=\"#00005F\">.0a1</font></span></pre>\n+    <pre>Wonderwords v3.0.0</pre>\n \n Upgrade\n -------\n@@ -51,7 +59,7 @@ To upgrade Wonderwords to the latest stable version, use:\n \n .. code-block:: bash\n \n-  $ pip install --upgrade wonderwords\n+    $ pip install --upgrade wonderwords\n \n Uninstall\n ---------\n@@ -60,4 +68,4 @@ To uninstall with pip, use:\n \n .. code-block:: bash\n \n-  $ pip uninstall wonderwords\n+    $ pip uninstall wonderwords\ndiff --git a/docs/quickstart.rst b/docs/quickstart.rst\nindex 27eb5e1..9bca0d6 100644\n--- a/docs/quickstart.rst\n+++ b/docs/quickstart.rst\n@@ -4,36 +4,38 @@\n Quickstart\n ==========\n \n-Wonderwords is a lightweight python tool that can be used to\n+Wonderwords is a lightweight Python tool that can be used to\n generate random words and sentences. In this tutorial you will learn the basics\n of Wonderwords and the command line interface. This tutorial is meant for those\n who have never used Wonderwords or those who want to learn more about it.\n-For a full reference of all commands, visit the API documentation.\n+For a full reference of all commands, visit the :ref:`API documentation <docs>`.\n \n .. note::\n \n-  This tutorial assumes you have already installed Wonderwords. If this is not\n-  the case, head over to the :ref:`install_uninstall_upgrade` page before proceeding.\n+    This tutorial assumes you have already installed Wonderwords. If this is not\n+    the case, head over to the :ref:`install_uninstall_upgrade` page before\n+    proceeding.\n \n The ``RandomWord`` class\n ------------------------\n \n One of the core Wonderwords classes is the ``RandomWord`` class. This class\n-encapsulates operations dealing with individual words. One method of this\n-class is the ``word`` method, which can be used to generate individual random\n-words::\n+deals with generating individual random words. The ``word`` method can be used\n+to generate individual random words::\n \n-  >>> from wonderwords import RandomWord\n-  >>>\n-  >>> w = RandomWord()\n-  >>> w.word()\n-  'sordid'\n+    >>> from wonderwords import RandomWord\n+    >>> w = RandomWord()\n+    >>> w.word()\n+    'sordid'\n+\n+Calling the word method returned a string containing a random word from\n+Wonderwords' default word lists. When using Wonderwords, it is helpful to create\n+an instance of the ``RandomWord`` class and reuse it to generate words.\n \n-Calling the word class returned a string containing a word. When using\n-Wonderwords, it is helpful to create an instance of the ``RandomWord`` class\n-in the top-level module of your project and import it when necessary.\n-Wonderwords loads word lists only once. If you create a second `RandomWord`\n-instance, the word lists won't be loaded twice.\n+Wonderwords loads default word lists only once. If you create a second\n+``RandomWord`` instance, the word lists won't be loaded twice. There is no\n+performance or memory cost associated with creating multiple ``RandomWord``\n+instances.\n \n The ``word`` Method\n ^^^^^^^^^^^^^^^^^^^\n@@ -43,63 +45,69 @@ Here is where the ``starts_with`` and ``ends_with`` arguments come into play.\n For example, to retrieve a word that starts with ``\"n\"`` and ends with\n ``\"es\"``, we can do the following::\n \n-  >>> w.word(starts_with=\"n\", ends_with=\"es\")\n-  'noodles'\n+    >>> w.word(starts_with=\"n\", ends_with=\"es\")\n+    'noodles'\n \n You don't have to use both arguments. You can specify either one individually\n like so::\n \n-  >>> w.word(starts_with=\"can\")\n-  'cannon'\n+    >>> w.word(starts_with=\"can\")\n+    'cannon'\n \n Sometimes, however, we may try to look for a pattern that doesn't exist. In that\n case a ``NoWordsToChoseFrom`` exception is raised::\n \n-  >>> w.word(starts_with=\"ja\", ends_with=\"ka\")\n-  NoWordsToChoseFrom: There aren't enough words to choose from. Cannot generate 1 word(s)\n+    >>> w.word(starts_with=\"ja\", ends_with=\"ka\")\n+    NoWordsToChoseFrom: There aren't enough words to choose from. Cannot generate 1 word(s)\n \n We can also narrow down a word by part of speech. By default, nouns, verbs and\n-adjectives are all enabled. If you want to generate a word by only a certain\n+adjectives are all enabled. If you want to generate a of a certain\n part of speech, you can use the ``include_categories`` parameter::\n \n-  >>> w.word(include_categories=[\"adjective\"])\n-  'tough'\n-  >>> w.word(include_categories=[\"noun\", \"verb\"])\n-  'cinder'\n+    >>> w.word(include_categories=[\"adjectives\"])\n+    'tough'\n+    >>> w.word(include_categories=[\"nouns\", \"verbs\"])\n+    'cinder'\n \n We can also filter words by length using the ``word_min_length`` and\n ``word_max_length`` parameters::\n \n-  >>> w.word(word_min_length=5)\n-  'documentary'\n-  >>> w.word(word_max_length=3)\n-  'dog'\n-  >>> w.word(word_min_length=9, word_max_length=10)\n-  'velodrome'\n+    >>> w.word(word_min_length=5)\n+    'documentary'\n+    >>> w.word(word_max_length=3)\n+    'dog'\n+    >>> w.word(word_min_length=9, word_max_length=10)\n+    'velodrome'\n \n-Finally, we can filter words by a custom python\n+We can additionally filter words by a custom python\n `regular expression <https://docs.python.org/3/library/re.html>`_::\n \n-  >>> w.word(regex=\"..\")\n-  'TV'\n-  >>> w.word(regex=\".*a\")\n-  'terracotta'\n+    >>> w.word(regex=\"..\")\n+    'TV'\n+    >>> w.word(regex=\".*a\")\n+    'terracotta'\n+\n+Finally, we can choose to ignore open compounds (words with spaces in between,\n+such as 'dump truck') if this feature is required:\n \n-Remember that we can combine multiple filters together, like so::\n+    >>> w.word(exclude_with_spaces=True)\n+    'partrimony'\n \n-  >>> w.word(\n-  ...     word_min_length=4,\n-  ...     starts_with=\"k\",\n-  ...     include_categories=[\"verb\"]\n-  ... )\n-  'keep'\n+Filters are not exclusive and can be combined, like so::\n+\n+    >>> w.word(\n+    ...     word_min_length=4,\n+    ...     starts_with=\"k\",\n+    ...     include_categories=[\"verb\"]\n+    ... )\n+    'keep'\n \n The ``filter`` method\n ^^^^^^^^^^^^^^^^^^^^^\n \n As you saw above, the word class allows the filtering of many words. What if we\n want to get a list of all words that match a certain filter? The ``filter``\n-method allows us to get all words matching a certain criteria::\n+method allows us to get all words matching the given criteria::\n \n   >>> w.filter(word_min_length=4, starts_with=\"loc\")\n   ['locality',\n@@ -122,29 +130,29 @@ The ``random_words`` method\n The ``random_words`` method acts just like the ``filter`` method, except with\n two differences:\n \n-  * You can limit the amount of words fitting the criteria\n-  * If there aren't enough words to reach the limit, a ``NoWordsToChoseFrom``\n-    exception is raised **unless** ``return_less_if_necessary`` is set to\n-    ``True``.\n+* You can limit the amount of words fitting the criteria\n+* The returned words are randomly chosen and ordered\n+* If there aren't enough words to reach the limit, a ``NoWordsToChooseFrom``\n+  exception is raised **unless** ``return_less_if_necessary`` is set to\n+  ``True``.\n \n-This method is useful if you want to get a list of words::\n+This method is useful if you want to get a random list of words::\n \n-  >>> w.random_words(3)\n-  ['prince', 'handover', 'cell']\n-  >>> w.random_words(4, word_min_length=5, starts_with=\"a\")\n-  ['abrogation', 'animal', 'appropriation', 'angry']\n-  >>> w.random_words(3, word_min_length=5, starts_with=\"alg\") # The exception is\n-  ...                                                         # raised as 3 words cannot be generated\n-  NoWordsToChoseFrom: There aren't enough words to choose from. Cannot generate 3 word(s)\n-  >>> w.random_words(3, word_min_length=5, starts_with=\"alg\", return_less_if_necessary=True)\n-  ['algebra', 'algorithm']\n+    >>> w.random_words(3)\n+    ['prince', 'handover', 'cell']\n+    >>> w.random_words(4, word_min_length=5, starts_with=\"a\")\n+    ['abrogation', 'animal', 'appropriation', 'angry']\n+    >>> w.random_words(3, word_min_length=5, starts_with=\"alg\") # The exception is\n+    ...                                                         # raised as 3 words cannot be generated\n+    NoWordsToChooseFrom: There aren't enough words to choose from. Cannot generate 3 word(s)\n+    >>> w.random_words(3, word_min_length=5, starts_with=\"alg\", return_less_if_necessary=True)\n+    ['algebra', 'algorithm']\n \n The ``RandomSentence`` class\n ----------------------------\n \n-Wonderwords makes generation of structured sentences made of random words easy.\n-The ``RandomSentence`` class houses many of these features. You should keep\n-an instance of this class at the top-level of your project just like the\n+Wonderwords makes generation of structured sentences made of random with the\n+``RandomSentence`` class. It is a generator class similar to the\n ``RandomWord`` class::\n \n   >>> from wonderwords import RandomSentence\n@@ -157,7 +165,7 @@ Creating sentences with the RandomSentence class\n The RandomSentence class provides multiple methods to generate random sentences,\n for example::\n \n-  >>> s.bare_bone_sentence() # generate a bare-bone sentence (The [subject] [predicate])\n+  >>> s.bare_bone_sentence() # generate a bare-bone sentence ([article] [subject] [predicate])\n   'The hut frames.'\n   >>> s.simple_sentence() # generate a simple sentence\n   'A reprocessing formulates enrollment.'\n@@ -182,57 +190,59 @@ the command ``wonderwords``::\n \n .. raw:: html\n \n-  <pre><span style=\"background-color:#D3D7CF\"><font color=\"#00005F\"><b>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e</b></font></span>\n-  <span style=\"background-color:#D3D7CF\"><font color=\"#00005F\"><b>\u2502                                                                             \u2502</b></font></span>\n-  <span style=\"background-color:#D3D7CF\"><font color=\"#00005F\"><b>\u2502                             WONDERWORDS 2.0.0a1                             \u2502</b></font></span>\n-  <span style=\"background-color:#D3D7CF\"><font color=\"#00005F\"><b>\u2502                                                                             \u2502</b></font></span>\n-  <span style=\"background-color:#D3D7CF\"><font color=\"#00005F\"><b>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f</b></font></span>\n-\n-   <b>                            No commands given \ud83d\ude1e                             </b>\n+    <pre>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n+    \u2551                  <b>Wonderwords  [your version]</b>                  \u2551\n+    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n \n \n-                                <u style=\"text-decoration-style:single\"><b>Available Commands</b></u>\n+                           <u style=\"text-decoration-style:single\"><b>Available Commands</b></u>\n \n-  <font color=\"#FCE94F\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -w</font></span> - generate a random word\n-  <font color=\"#FCE94F\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -f</font></span> - get all words matching a certain criteria\n-  <font color=\"#FCE94F\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -l AMOUNT</font></span> - get a list of <span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">AMOUNT</font></span> random words\n-  <font color=\"#FCE94F\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -s SENT_TYPE</font></span> - generate a random sentence of a certain type\n+    <font color=\"#C4A000\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -w</font></span> - generate a random word\n+    <font color=\"#C4A000\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -f</font></span> - get all words matching certain criteria\n+    <font color=\"#C4A000\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -l AMOUNT</font></span> - get a list of <span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">AMOUNT</font></span> random words\n+    <font color=\"#C4A000\"><b> \u2022 </b></font><span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -s SENT_TYPE</font></span> - generate a random sentence of a\n+    <font color=\"#C4A000\"><b>   </b></font>certain type\n \n-  For a list of all options, type <span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">wonderwords -h</font></span>. To see a detailed and\n-  comprehensive explanation of the commands, visit <font color=\"#729FCF\">the documentation</font>             </pre>\n+    For a list of all options, use the <span style=\"background-color:#2E3436\"><font color=\"#EEEEEC\">--help</font></span> option. To see a\n+    detailed and comprehensive explanation of the commands, visit <font color=\"#729FCF\">the</font>\n+    <font color=\"#729FCF\">documentation</font>.\n+    </pre>\n \n-When typing the ``wonderwords`` command, you are greeted with a main page with\n-basic information, such as basic commands and the ``wonderwords`` version.\n+When typing the ``wonderwords`` command, you are greeted with a welcome message that\n+lists basic commands and the ``wonderwords`` version.\n To get a full list of commands, type ``wonderwords -h`` or\n ``wonderwords --help``.\n \n+Depending on how you installed it, you may need to run wonderwords as\n+``python -m wonderwords`` instead of just ``wonderwords``.\n+\n Generating random words\n ^^^^^^^^^^^^^^^^^^^^^^^\n \n To generate a random word, use the ``-w`` or ``--word`` flag. A random word will\n be printed to the console::\n \n-  $ wonderwords -w\n+    $ wonderwords -w\n \n .. raw:: html\n \n-  <pre><span style=\"background-color:#00005F\"><font color=\"#EEEEEC\"><b>poison</b></font></span></pre>\n+    <pre><font color=\"#06989A\">participate</font></pre>\n \n All of the filters that you have learned above have their own commands, too::\n \n-  $ wonderwords -w -sw a -ew e # -sw: starts with, -ew ends with; word that starts with a and ends with e\n+  $ wonderwords -w -S a -e e # -S: starts with, -e ends with; word that starts with a and ends with e\n   $ wonderwords -w -p nouns verbs # -p: parts of speech; select only nouns and verbs\n-  $ wonderwords -w -min 3 -max 5 # -min: minimum length, -max maximum length; minimum length 3 and maximum length 5\n+  $ wonderwords -w -m 3 -M 5 # -m: minimum length, -M maximum length; minimum length 3 and maximum length 5\n \n Generating filters and lists\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n You can also generate filters with the ``-f`` flag and lists with the ``-l``\n-flag. All modifiers such as ``-sw`` and ``-min`` can also be used. Additionally,\n+flag. All modifiers such as ``-S`` and ``-m`` can also be used. Additionally,\n the ``-d`` flag can set a delimiter between words::\n \n-  $ wonderwords -f -min 3 # get all words with a minimum length of 3\n-  $ wonderwords -l 5 -sw ap # get 5 words that start with \"ap\"\n+  $ wonderwords -f -m 3 # get all words with a minimum length of 3\n+  $ wonderwords -l 5 -S ap # get 5 random words that start with \"ap\"\n   $ wonderwords -l 3 -d \" | \" # get 3 random words separated with \" | \"\n \n Generating random sentences\n@@ -241,10 +251,10 @@ Generating random sentences\n The ``-s`` flag followed by a sentence type can generate a random sentence.\n The options of type are:\n \n-  * ``bb``: bare-bone sentence\n-  * ``ss``: simple sentence\n-  * ``bba``: bare-bone sentence with adjective\n-  * ``s``: a simple sentence plus and adjective\n+* ``bb``: bare-bone sentence\n+* ``ss``: simple sentence\n+* ``bba``: bare-bone sentence with adjective\n+* ``s``: a simple sentence plus and adjective\n \n For example::\n \n@@ -258,9 +268,9 @@ The quickstart tutorial has come to an end. In this tutorial, you learned the\n basics of Wonderwords. More specifically, you learned about:\n \n * The ``RandomWord`` class\n-    * The ``word`` method\n-    * The ``filter`` method\n-    * The ``random_words`` method\n+  * The ``word`` method\n+  * The ``filter`` method\n+  * The ``random_words`` method\n * The ``RandomSentence`` class and some of its methods\n * How to use the Wonderwords command line interface\n \n@@ -268,8 +278,8 @@ What's next?\n ^^^^^^^^^^^^\n \n After you have gotten comfortable using wonderwords, check out the\n-:ref:`advanced` tutorial. You can use the API reference for help on specific\n-classes, and functions. If you want to contribute, please read the contribution\n-guidelines. If you have any problems, bugs, or feature requests, please open up\n-an issue on the\n+:ref:`advanced` tutorial. You can use the :ref:`API documentation <docs>` for help\n+on specific classes, and functions. If you want to contribute, please read the\n+contribution guidelines. If you have any problems, bugs, or feature requests,\n+please open up an issue on the\n `Wonderwords GitHub page <https://github.com/mrmaxguns/wonderwordsmodule/>`_.\ndiff --git a/docs/requirements.txt b/docs/requirements.txt\nindex 787317a..aae9e6b 100644\n--- a/docs/requirements.txt\n+++ b/docs/requirements.txt\n@@ -1,1 +1,38 @@\n-sphinx-copybutton\n+alabaster==0.7.16\n+Babel==2.14.0\n+black==24.3.0\n+certifi==2024.2.2\n+charset-normalizer==3.3.2\n+click==8.1.7\n+docutils==0.20.1\n+exceptiongroup==1.2.0\n+idna==3.7\n+imagesize==1.4.1\n+iniconfig==2.0.0\n+Jinja2==3.1.3\n+markdown-it-py==3.0.0\n+MarkupSafe==2.1.5\n+mdurl==0.1.2\n+mypy==1.9.0\n+mypy-extensions==1.0.0\n+packaging==24.0\n+pathspec==0.12.1\n+platformdirs==4.2.0\n+pluggy==1.4.0\n+Pygments==2.17.2\n+pytest==8.1.1\n+requests==2.31.0\n+rich==13.7.1\n+setuptools-scm==8.0.4\n+snowballstemmer==2.2.0\n+Sphinx==7.2.6\n+sphinx-copybutton==0.5.2\n+sphinxcontrib-applehelp==1.0.8\n+sphinxcontrib-devhelp==1.0.6\n+sphinxcontrib-htmlhelp==2.0.5\n+sphinxcontrib-jsmath==1.0.1\n+sphinxcontrib-qthelp==1.0.7\n+sphinxcontrib-serializinghtml==1.1.10\n+tomli==2.0.1\n+typing_extensions==4.11.0\n+urllib3==2.2.1\ndiff --git a/pyproject.toml b/pyproject.toml\nnew file mode 100644\nindex 0000000..6ba2caa\n--- /dev/null\n+++ b/pyproject.toml\n@@ -0,0 +1,61 @@\n+[build-system]\n+requires = [\"setuptools>=64\", \"setuptools_scm>=8\"]\n+build-backend = \"setuptools.build_meta\"\n+\n+[tool.setuptools.packages.find]\n+where = [\"wonderwords\"]\n+\n+[tool.setuptools_scm]\n+write_to = \"wonderwords/_version.py\"\n+\n+[project]\n+name = \"wonderwords\"\n+#version = \"2.3.0\"\n+requires-python = \">=3.8\"\n+authors = [\n+    { name=\"Maxim Rebguns\", email=\"mrmaxguns@gmail.com\" },\n+]\n+license = {file = \"LICENSE\"}\n+keywords = [\"random\", \"English\"]\n+description = \"Generate random english words and phrases.\"\n+readme = \"PYPI.md\"\n+dependencies = []\n+classifiers = [\n+    \"Development Status :: 4 - Stable\",\n+    \"License :: OSI Approved :: MIT License\",\n+    \"Operating System :: OS Independent\",\n+\n+    \"Natural Language :: English\",\n+    \"Topic :: Text Processing :: Linguistic\",\n+\n+    \"Programming Language :: Python :: 3\",\n+    \"Programming Language :: Python :: 3.8\",\n+    \"Programming Language :: Python :: 3.9\",\n+    \"Programming Language :: Python :: 3.10\",\n+    \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n+]\n+dynamic = [\"version\"]\n+\n+[project.optional-dependencies]\n+dev = [\n+    \"pytest\",\n+    \"black\",\n+    \"mypy\",\n+    \"flake8\",\n+    \"setuptools\",\n+    \"setuptools-scm\",\n+    \"sphinx\",\n+    \"sphinx-copybutton\",\n+]\n+cli = [\n+    \"rich\",\n+]\n+\n+[project.scripts]\n+wonderwords = \"wonderwords.cmdline_parser:main\"\n+\n+[project.urls]\n+Homepage = \"https://github.com/mrmaxguns/wonderwordsmodule\"\n+Issues = \"https://github.com/mrmaxguns/wonderwordsmodule/issues\"\n+Documentation = \"https://wonderwords.readthedocs.io\"\ndiff --git a/requirements.txt b/requirements.txt\nindex 769d33e..4a5ba7b 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,6 +1,18 @@\n-colorama==0.4.4\n-commonmark==0.9.1\n-importlib-resources==5.1.0\n-Pygments==2.7.4\n-rich==9.10.0\n-typing-extensions==3.7.4.3\n+black==24.3.0\n+click==8.1.7\n+exceptiongroup==1.2.0\n+iniconfig==2.0.0\n+markdown-it-py==3.0.0\n+mdurl==0.1.2\n+mypy==1.9.0\n+mypy-extensions==1.0.0\n+packaging==24.0\n+pathspec==0.12.1\n+platformdirs==4.2.0\n+pluggy==1.4.0\n+Pygments==2.17.2\n+pytest==8.1.1\n+rich==13.7.1\n+setuptools-scm==8.0.4\n+tomli==2.0.1\n+typing_extensions==4.11.0\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 43211c3..0000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-import setuptools\n-\n-with open(\"PYPI.md\", \"r\") as fh:\n-    long_description = fh.read()\n-\n-setuptools.setup(\n-    name=\"wonderwords\",\n-    version=\"2.2.0\",\n-    author=\"Maxim Rebguns\",\n-    author_email=\"mrmaxguns@gmail.com\",\n-    include_package_data=True,\n-    description=(\n-        \"A python package for random words and sentences in the english\"\n-        \" language\"\n-    ),\n-    long_description=long_description,\n-    long_description_content_type=\"text/markdown\",\n-    url=\"https://github.com/mrmaxguns/wonderwordsmodule\",\n-    packages=setuptools.find_packages(),\n-    install_requires=[\n-        'importlib_resources == 5.1.0; python_version < \"3.7\"',\n-    ],\n-    extras_require={\"cli\": [\"rich == 9.10.0\"]},\n-    package_data={\"wonderwords\": [\"assets/*.txt\"]},\n-    classifiers=[\n-        \"Programming Language :: Python :: 3\",\n-        \"License :: OSI Approved :: MIT License\",\n-        \"Operating System :: OS Independent\",\n-    ],\n-    python_requires=\">=3.6\",\n-    entry_points={\n-        \"console_scripts\": [\"wonderwords = wonderwords.cmdline_parser:main\"]\n-    },\n-)\ndiff --git a/wonderwords/__init__.py b/wonderwords/__init__.py\nindex 2620169..4ec0e2b 100644\n--- a/wonderwords/__init__.py\n+++ b/wonderwords/__init__.py\n@@ -1,12 +1,28 @@\n # flake8: noqa\n-from .random_word import RandomWord, NoWordsToChoseFrom, Defaults\n+from .random_word import (\n+    WordList,\n+    NoWordsToChooseFrom,\n+    Defaults,\n+    RandomWord,\n+    is_profanity,\n+    filter_profanity,\n+)\n from .random_sentence import RandomSentence\n \n-__author__ = \"Maxim R.\"\n-__copyright__ = \"Copyright 2020, Wonderwords\"\n-__credits__ = [\"Maxim R.\"]\n+# Misspelling (kept for backwards compatibility)\n+NoWordsToChoseFrom = NoWordsToChooseFrom\n+\n+__author__ = \"Maxim Rebguns\"\n+__copyright__ = \"Copyright 2024, Maxim Rebguns\"\n+__credits__ = [\"Maxim Rebguns\"]\n __license__ = \"MIT\"\n-__version__ = \"2.2.0\"\n-__maintainer__ = \"Maxim R.\"\n+__maintainer__ = \"Maxim Rebguns\"\n __email__ = \"mrmaxguns@gmail.com\"\n __status__ = \"Production\"\n+\n+try:\n+    from ._version import version as __version__  # type: ignore\n+    from ._version import version_tuple  # type: ignore\n+except ImportError:\n+    __version__ = \"unknown version\"\n+    version_tuple = (0, 0, \"unknown version\")\ndiff --git a/wonderwords/_trie.py b/wonderwords/_trie.py\nnew file mode 100644\nindex 0000000..6904d97\n--- /dev/null\n+++ b/wonderwords/_trie.py\n@@ -0,0 +1,74 @@\n+\"\"\"An incomplete trie implementation used\n+to efficiently find words with a given prefix\n+or suffix.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from typing import Dict, Set, Optional, Iterable\n+\n+\n+@dataclass\n+class TrieNode:\n+    end_of_word: bool\n+    children: Dict[str, TrieNode]\n+\n+\n+class Trie:\n+    def __init__(self, words: Optional[Iterable[str]] = None):\n+        self.root: TrieNode = self._new_node()\n+\n+        if words is not None:\n+            for word in words:\n+                self.insert(word)\n+\n+    def insert(self, word: str) -> None:\n+        \"\"\"Insert a word into the trie.\"\"\"\n+        current_node = self.root\n+        for letter in word:\n+            if letter in current_node.children:\n+                current_node = current_node.children[letter]\n+            else:\n+                current_node.children[letter] = self._new_node()\n+                current_node = current_node.children[letter]\n+        current_node.end_of_word = True\n+\n+    def query_characters(self, characters: str) -> Optional[TrieNode]:\n+        \"\"\"Find the trie node matching as many ``characters`` as possible.\"\"\"\n+        current_node = self.root\n+        for character in characters:\n+            if character in current_node.children:\n+                current_node = current_node.children[character]\n+            else:\n+                return None\n+        return current_node\n+\n+    def get_words_that_start_with(self, characters: str) -> Set[str]:\n+        \"\"\"Get all words in the trie that start with ``characters``.\"\"\"\n+        trunk = self.query_characters(characters)\n+        if trunk is None:\n+            return set()\n+        return self.get_words_from_branch(trunk, characters)\n+\n+    def get_words_from_branch(self, branch: TrieNode, word_fragment: str) -> Set[str]:\n+        \"\"\"Get all words that start with ``word_fragment`` starting from the node ``branch``.\"\"\"\n+        words = set()\n+\n+        if branch.end_of_word:\n+            words.add(word_fragment)\n+\n+        for character, data in branch.children.items():\n+            if data.end_of_word:\n+                words.add(word_fragment + character)\n+\n+            if data.children:\n+                words = words | self.get_words_from_branch(\n+                    data, word_fragment + character\n+                )\n+\n+        return words\n+\n+    def _new_node(self) -> TrieNode:\n+        \"\"\"Create a new empty trie node.\"\"\"\n+        return TrieNode(end_of_word=False, children={})\ndiff --git a/wonderwords/assets/nounlist.txt b/wonderwords/assets/nounlist.txt\nindex 7f19e7a..09c8f3e 100644\n--- a/wonderwords/assets/nounlist.txt\n+++ b/wonderwords/assets/nounlist.txt\n@@ -730,7 +730,6 @@ broad\n broadcast\n broccoli\n brochure\n-brocolli\n broiler\n broker\n bronchitis\n@@ -4644,7 +4643,6 @@ practice\n practitioner\n prairie\n praise\n-pray\n prayer\n precedence\n precedent\ndiff --git a/wonderwords/assets/profanitylist.txt b/wonderwords/assets/profanitylist.txt\nindex 5a0241e..e2e9c22 100644\n--- a/wonderwords/assets/profanitylist.txt\n+++ b/wonderwords/assets/profanitylist.txt\n@@ -1,86 +1,200 @@\n+2 girls 1 cup\n+2g1c\n 4r5e\n 5h1t\n 5hit\n a55\n-abortion\n+a_s_s\n+acrotomophilia\n+alabama hot pocket\n+alaskan pipeline\n anal\n+anilingus\n anus\n+apeshit\n ar5e\n arrse\n arse\n+arsehole\n ass\n ass-fucker\n+ass-hat\n+ass-pirate\n+assbag\n+assbandit\n+assbanger\n+assbite\n+assclown\n+asscock\n+asscracker\n asses\n+assface\n assfucker\n assfukka\n+assgoblin\n+asshat\n+asshead\n asshole\n assholes\n+asshopper\n+assjacker\n+asslick\n+asslicker\n+assmonkey\n+assmunch\n+assmuncher\n+asspirate\n+assshole\n+asssucker\n+asswad\n asswhole\n-a_s_s\n+asswipe\n+auto erotic\n+autoerotic\n b!tch\n b00bs\n b17ch\n b1tch\n+babeland\n+baby batter\n+baby juice\n+ball gag\n+ball gravy\n+ball kicking\n+ball licking\n+ball sack\n+ball sucking\n ballbag\n balls\n ballsack\n+bampot\n+bangbros\n+bareback\n+barely legal\n+barenaked\n bastard\n+bastardo\n+bastinado\n+bbw\n+bdsm\n+beaner\n+beaners\n beastial\n beastiality\n+beastility\n+beaver cleaver\n+beaver lips\n bellend\n bestial\n bestiality\n bi+ch\n biatch\n+big black\n+big breasts\n+big knockers\n+big tits\n+bimbos\n+birdlock\n bitch\n bitcher\n bitchers\n bitches\n bitchin\n bitching\n+black cock\n+blonde action\n+blonde on blonde action\n bloody\n blow job\n+blow your load\n blowjob\n blowjobs\n+blue waffle\n+blumpkin\n boiolas\n bollock\n+bollocks\n bollok\n+bollox\n+bondage\n boner\n boob\n+boobie\n boobs\n booobs\n boooobs\n booooobs\n booooooobs\n+booty call\n breasts\n+brown showers\n+brunette action\n buceta\n bugger\n+bukkake\n+bulldyke\n+bullet vibe\n+bullshit\n bum\n+bung hole\n+bunghole\n bunny fucker\n+busty\n butt\n+butt-pirate\n+buttcheeks\n butthole\n buttmunch\n buttplug\n c0ck\n c0cksucker\n+camel toe\n+camgirl\n+camslut\n+camwhore\n carpet muncher\n+carpetmuncher\n cawk\n+chinc\n chink\n+choad\n+chocolate rosebuds\n+chode\n cipa\n+circlejerk\n cl1t\n+cleveland steamer\n clit\n+clitface\n clitoris\n clits\n+clover clamps\n+clusterfuck\n cnut\n cock\n cock-sucker\n+cockbite\n+cockburger\n cockface\n cockhead\n+cockjockey\n+cockknoker\n+cockmaster\n+cockmongler\n+cockmongruel\n+cockmonkey\n cockmunch\n cockmuncher\n+cocknose\n+cocknugget\n cocks\n+cockshit\n+cocksmith\n+cocksmoker\n+cocksuck\n cocksuck\n cocksucked\n+cocksucked\n cocksucker\n cocksucking\n cocksucks\n@@ -89,21 +203,42 @@ cocksukka\n cok\n cokmuncher\n coksucka\n+coochie\n+coochy\n coon\n+coons\n+cooter\n+coprolagnia\n+coprophilia\n+cornhole\n cox\n crap\n+creampie\n cum\n+cumbubble\n+cumdumpster\n+cumguzzler\n+cumjockey\n cummer\n cumming\n cums\n cumshot\n+cumslut\n+cumtart\n cunilingus\n cunillingus\n+cunnie\n cunnilingus\n cunt\n+cuntface\n+cunthole\n+cuntlick\n cuntlick\n cuntlicker\n+cuntlicker\n+cuntlicking\n cuntlicking\n+cuntrag\n cunts\n cyalis\n cyberfuc\n@@ -113,22 +248,71 @@ cyberfucker\n cyberfuckers\n cyberfucking\n d1ck\n+dammit\n damn\n+darkie\n+date rape\n+daterape\n+deep throat\n+deepthroat\n+dendrophilia\n dick\n+dickbag\n+dickbeater\n+dickface\n dickhead\n+dickhole\n+dickjuice\n+dickmilk\n+dickmonger\n+dickslap\n+dicksucker\n+dickwad\n+dickweasel\n+dickweed\n+dickwod\n+dike\n dildo\n dildos\n+dingleberries\n+dingleberry\n dink\n dinks\n+dipshit\n dirsa\n+dirty pillows\n+dirty sanchez\n dlck\n+dog style\n dog-fucker\n+doggie style\n+doggiestyle\n doggin\n dogging\n+doggy style\n+doggystyle\n+dolcett\n+domination\n+dominatrix\n+dommes\n+donkey punch\n donkeyribber\n+doochbag\n+dookie\n doosh\n+double dong\n+double penetration\n+douche\n+douchebag\n+dp action\n+dry hump\n duche\n+dumbshit\n+dumshit\n+dvda\n dyke\n+eat my ass\n+ecchi\n ejaculate\n ejaculated\n ejaculates\n@@ -136,36 +320,58 @@ ejaculating\n ejaculatings\n ejaculation\n ejakulate\n+erotic\n+erotism\n+escort\n+eunuch\n f u c k\n f u c k e r\n f4nny\n+f_u_c_k\n fag\n+fagbag\n+fagg\n fagging\n+faggit\n faggitt\n faggot\n faggs\n fagot\n fagots\n fags\n+fagtard\n fanny\n fannyflaps\n fannyfucker\n fanyy\n+fart\n+farted\n+farting\n+farty\n fatass\n fcuk\n fcuker\n fcuking\n+fecal\n feck\n fecker\n+felatio\n+felch\n felching\n fellate\n fellatio\n+feltch\n+female squirting\n+femdom\n+figging\n+fingerbang\n fingerfuck\n fingerfucked\n fingerfucker\n fingerfuckers\n fingerfucking\n fingerfucks\n+fingering\n fistfuck\n fistfucked\n fistfucker\n@@ -173,10 +379,16 @@ fistfuckers\n fistfucking\n fistfuckings\n fistfucks\n+fisting\n+flamer\n flange\n fook\n fooker\n+foot fetish\n+footjob\n+frotting\n fuck\n+fuck buttons\n fucka\n fucked\n fucker\n@@ -189,6 +401,7 @@ fuckings\n fuckingshitmotherfucker\n fuckme\n fucks\n+fucktards\n fuckwhit\n fuckwit\n fudge packer\n@@ -200,42 +413,106 @@ fukkin\n fuks\n fukwhit\n fukwit\n+futanari\n fux\n fux0r\n-f_u_c_k\n+g-spot\n+gang bang\n gangbang\n gangbanged\n+gangbanged\n gangbangs\n+gay sex\n+gayass\n+gaybob\n+gaydo\n gaylord\n gaysex\n+gaytard\n+gaywad\n+genitals\n+giant cock\n+girl on\n+girl on top\n+girls gone wild\n+goatcx\n goatse\n-God\n+god damn\n god-dam\n god-damned\n goddamn\n goddamned\n+gokkun\n+golden shower\n+goo girl\n+gooch\n+goodpoop\n+gook\n+goregasm\n+gringo\n+grope\n+group sex\n+guido\n+guro\n+hand job\n+handjob\n+hard core\n+hardcore\n hardcoresex\n+heeb\n hell\n+hentai\n heshe\n+ho\n hoar\n hoare\n+hoe\n hoer\n homo\n+homoerotic\n+honkey\n+honky\n+hooker\n hore\n horniest\n horny\n+hot carl\n+hot chick\n hotsex\n-idiot\n+how to kill\n+how to murder\n+huge fat\n+humping\n+incest\n+intercourse\n+jack off\n jack-off\n+jackass\n jackoff\n+jail bait\n+jailbait\n jap\n+jelly donut\n+jerk off\n jerk-off\n+jigaboo\n+jiggaboo\n+jiggerboo\n jism\n jiz\n+jiz\n+jizm\n jizm\n jizz\n+juggs\n kawk\n+kike\n+kinbaku\n+kinkster\n+kinky\n+kiunt\n knob\n+knobbing\n knobead\n knobed\n knobend\n@@ -245,15 +522,27 @@ knobjokey\n kock\n kondum\n kondums\n+kooch\n+kootch\n kum\n+kumer\n kummer\n kumming\n kums\n kunilingus\n+kunt\n+kyke\n l3i+ch\n l3itch\n labia\n+leather restraint\n+leather straight jacket\n+lemon party\n+lesbo\n+lezzie\n lmfao\n+lolita\n+lovemaking\n lust\n lusting\n m0f0\n@@ -261,6 +550,8 @@ m0fo\n m45terbate\n ma5terb8\n ma5terbate\n+make me come\n+male squirting\n masochist\n master-bate\n masterb8\n@@ -270,6 +561,10 @@ masterbate\n masterbation\n masterbations\n masturbate\n+menage a trois\n+milf\n+minge\n+missionary position\n mo-fo\n mof0\n mofo\n@@ -294,7 +589,12 @@ motherfucking\n motherfuckings\n motherfuckka\n motherfucks\n+mound of venus\n+mr hands\n muff\n+muff diver\n+muffdiver\n+muffdiving\n mutha\n muthafecker\n muthafuckker\n@@ -302,7 +602,12 @@ muther\n mutherfucker\n n1gga\n n1gger\n+nambla\n+nawashi\n nazi\n+negro\n+neonazi\n+nig nog\n nigg3r\n nigg4h\n nigga\n@@ -311,22 +616,47 @@ niggas\n niggaz\n nigger\n niggers\n+niglet\n+nimphomania\n+nipple\n+nipples\n nob\n nob jokey\n nobhead\n nobjocky\n nobjokey\n+nsfw images\n+nude\n+nudity\n numbnuts\n nutsack\n+nympho\n+nymphomania\n+octopussy\n+omorashi\n+one cup two girls\n+one guy one jar\n+orgasim\n orgasim\n orgasims\n orgasm\n orgasms\n+orgy\n p0rn\n+paedophile\n+paki\n+panooch\n+panties\n+panty\n pawn\n pecker\n+peckerhead\n+pedobear\n+pedophile\n+pegging\n penis\n penisfucker\n+phone sex\n phonesex\n phuck\n phuk\n@@ -336,45 +666,105 @@ phukked\n phukking\n phuks\n phuq\n+piece of shit\n pigfucker\n pimpis\n+pis\n+pises\n+pisin\n+pising\n+pisof\n piss\n+piss pig\n pissed\n pisser\n pissers\n pisses\n+pissflap\n pissflaps\n pissin\n+pissin\n pissing\n pissoff\n+pissoff\n+pisspig\n+playboy\n+pleasure chest\n+pole smoker\n+polesmoker\n+pollock\n+ponyplay\n+poo\n+poof\n+poon\n+poonani\n+poonany\n+poontang\n poop\n+poop chute\n+poopchute\n porn\n porno\n pornography\n pornos\n prick\n pricks\n+prince albert piercing\n pron\n+pthc\n pube\n+pubes\n+punanny\n+punany\n+punta\n+pusies\n pusse\n pussi\n pussies\n pussy\n+pussylicking\n pussys\n+pusy\n+puto\n+queaf\n+queef\n+queerbait\n+queerhole\n+quim\n+raghead\n+raging boner\n+rape\n+raping\n+rapist\n rectum\n+renob\n retard\n+reverse cowgirl\n rimjaw\n+rimjob\n rimming\n+rosy palm\n+rosy palm and her 5 sisters\n+ruski\n+rusty trombone\n s hit\n+s&m\n s.o.b.\n+s_h_i_t\n+sadism\n sadist\n+santorum\n+scat\n schlong\n+scissoring\n screwing\n scroat\n scrote\n scrotum\n semen\n sex\n+sexo\n+sexy\n sh!+\n sh!t\n sh1t\n@@ -382,41 +772,120 @@ shag\n shagger\n shaggin\n shagging\n+shaved beaver\n+shaved pussy\n shemale\n shi+\n+shibari\n shit\n+shit-ass\n+shit-bag\n+shit-bagger\n+shit-brain\n+shit-breath\n+shit-cunt\n+shit-dick\n+shit-eating\n+shit-face\n+shit-faced\n+shit-fit\n+shit-head\n+shit-heel\n+shit-hole\n+shit-house\n+shit-load\n+shit-pot\n+shit-spitter\n+shit-stain\n+shitass\n+shitbag\n+shitbagger\n+shitblimp\n+shitbrain\n+shitbreath\n+shitcunt\n shitdick\n shite\n+shiteating\n shited\n shitey\n+shitface\n+shitfaced\n+shitfit\n shitfuck\n shitfull\n shithead\n+shitheel\n+shithole\n+shithouse\n shiting\n shitings\n+shitload\n+shitpot\n shits\n+shitspitter\n+shitstain\n shitted\n shitter\n shitters\n+shittiest\n shitting\n shittings\n shitty\n+shitty\n+shity\n+shiz\n+shiznit\n+shota\n+shrimping\n skank\n+skeet\n+slanteye\n slut\n+slutbag\n sluts\n+smeg\n smegma\n smut\n snatch\n+snowballing\n+sodomize\n+sodomy\n son-of-a-bitch\n spac\n+spic\n+spick\n+splooge\n+splooge moose\n+spooge\n+spread legs\n spunk\n-s_h_i_t\n+strap on\n+strapon\n+strappado\n+strip club\n+style doggy\n+suck\n+sucks\n+suicide girls\n+sultry women\n+swastika\n+swinger\n t1tt1e5\n t1tties\n+tainted love\n+tard\n+taste my\n+tea bagging\n teets\n teez\n testical\n testicle\n+threesome\n+throating\n+thundercunt\n+tied up\n+tight white\n tit\n titfuck\n tits\n@@ -424,30 +893,66 @@ titt\n tittie5\n tittiefucker\n titties\n+titty\n tittyfuck\n tittywank\n titwank\n+tongue in a\n+topless\n tosser\n+towelhead\n+tranny\n+tribadism\n+tub girl\n+tubgirl\n turd\n+tushy\n tw4t\n twat\n twathead\n+twatlips\n twatty\n+twink\n+twinkie\n+two girls one cup\n twunt\n twunter\n+undressing\n+upskirt\n+urethra play\n+urophilia\n v14gra\n v1gra\n+va-j-j\n+vag\n vagina\n+venus mound\n viagra\n+vibrator\n+violet wand\n+vjayjay\n+vorarephilia\n+voyeur\n vulva\n w00se\n wang\n wank\n wanker\n wanky\n+wet dream\n+wetback\n+white power\n whoar\n whore\n willies\n willy\n+wrapping men\n+wrinkled starfish\n xrated\n+xx\n xxx\n+yaoi\n+yellow showers\n+yiffy\n+zoophilia\n+\ud83d\udd95\n\\ No newline at end of file\ndiff --git a/wonderwords/cmdline.py b/wonderwords/cmdline.py\nindex 1820a2b..fa110ef 100644\n--- a/wonderwords/cmdline.py\n+++ b/wonderwords/cmdline.py\n@@ -1,82 +1,51 @@\n-from rich.console import Console\n-from rich.panel import Panel\n-from rich.text import Text\n-from rich.emoji import Emoji\n-from rich.padding import Padding\n+from rich import print\n from rich.markdown import Markdown\n \n from . import __version__\n \n \n-console = Console()\n-\n-AVAILABLE_COMMANDS = \"\"\"## Available Commands\n+WELCOME_MESSAGE = f\"\"\"# Wonderwords {__version__}\n+## Available Commands\n \n * `wonderwords -w` - generate a random word\n-* `wonderwords -f` - get all words matching a certain criteria\n+* `wonderwords -f` - get all words matching certain criteria\n * `wonderwords -l AMOUNT` - get a list of `AMOUNT` random words\n * `wonderwords -s SENT_TYPE` - generate a random sentence of a certain type\n \n-For a list of all options, type `wonderwords -h`. To see a detailed and\n+For a list of all options, use the `--help` option. To see a detailed and\n comprehensive explanation of the commands, visit\n-[the documentation](https://wonderwords.readthedocs.io)\n+[the documentation](https://wonderwords.readthedocs.io).\n \"\"\"\n \n \n-class WonderwordsCommandLine:\n-    def print_title(self):\n-        title = Panel(\n-            Text(f\"WONDERWORDS {__version__}\", justify=\"center\"),\n-            padding=1,\n-            style=\"bold navy_blue on white\",\n-        )\n-        console.print(title)\n-\n-    def print_commands(self):\n-        commands = Markdown(AVAILABLE_COMMANDS)\n-        console.print(commands)\n-\n-    def version(self):\n-        console.print(\n-            f\"Running wonderwords version {__version__}\", style=\"navy_blue on\"\n-            \"white\"\n-        )\n-\n-    def intro(self):\n-        self.print_title()\n-        info_text = Text(\n-            f\"No commands given {Emoji('disappointed_face')}\",\n-            justify=\"center\",\n-            style=\"bold\",\n-        )\n-        console.print(Padding(info_text, pad=1))\n-        self.print_commands()\n-\n-    def word(self, word):\n-        word_text = Text(word, style=\"bold white on navy_blue\")\n-        console.print(word_text)\n-\n-    def words(self, words, delimiter):\n-        word_text = Text(\n-            delimiter.join(words), style=\"bold white on navy_blue\"\n-        )\n-        console.print(word_text)\n-\n-    def sentence(self, sent):\n-        sent_text = Text(sent, style=\"bold white on dark_green\")\n-        console.print(sent_text)\n-\n-    def no_word(self):\n-        console.print(\n-            \"A word with the parameters specified does not exist! :anguished:\",\n-            style=\"white on red\",\n-        )\n-\n-    def no_words(self):\n-        console.print(\n-            (\n-                \"There weren't enough words that matched your request. All\"\n-                \" words available are listed below :anguished: \"\n-            ),\n-            style=\"white on red\",\n-        )\n+def display_version() -> None:\n+    print(f\"Wonderwords {__version__}\")\n+\n+\n+def display_word(word: str) -> None:\n+    print(f\"[cyan]{word}[/cyan]\")\n+\n+\n+def display_list(words: list, delimiter=\",\") -> None:\n+    delimiter_colorized = f\"[grey50]{delimiter}[/grey50]\"\n+    print(delimiter_colorized.join([f\"[cyan]{word}[/cyan]\" for word in words]))\n+\n+\n+def display_word_not_found(one_word=True) -> None:\n+    print(\n+        f\"[red]:exclamation: No word{'' if one_word else 's'} matching the criteria specified could be found.[/red]\"\n+    )\n+\n+\n+def display_not_enough_words() -> None:\n+    print(\n+        f\"[red]:exclamation: Couldn't find enough words matching the criteria specified.[/red]\"\n+    )\n+\n+\n+def display_sentence(sentence: str) -> None:\n+    print(f\"[cyan]{sentence}[/cyan]\")\n+\n+\n+def display_hello() -> None:\n+    print(Markdown(WELCOME_MESSAGE))\ndiff --git a/wonderwords/cmdline_parser.py b/wonderwords/cmdline_parser.py\nindex 9b023b2..bf19406 100644\n--- a/wonderwords/cmdline_parser.py\n+++ b/wonderwords/cmdline_parser.py\n@@ -1,8 +1,9 @@\n import argparse\n+import sys\n \n-from .random_word import RandomWord, NoWordsToChoseFrom\n+from .random_word import RandomWord, NoWordsToChooseFrom\n from .random_sentence import RandomSentence\n-from .cmdline import WonderwordsCommandLine\n+from . import cmdline\n \n \n def main():\n@@ -20,6 +21,8 @@ def main():\n     #\n     # Base commands\n     #\n+\n+    # Will be changed to a single positional MODE argument in version 3\n     parser.add_argument(\n         \"-w\",\n         \"--word\",\n@@ -32,7 +35,7 @@ def main():\n         \"-f\",\n         \"--filter\",\n         action=\"store_true\",\n-        help=\"filter a list of words matching the criteria specified\",\n+        help=\"get a list of all known words matching the criteria specified\",\n     )\n \n     parser.add_argument(\n@@ -40,7 +43,7 @@ def main():\n         \"--list\",\n         action=\"store\",\n         type=int,\n-        help=\"return a list of words of a certain length\",\n+        help=\"return a list of a certain length of random words\",\n     )\n \n     parser.add_argument(\n@@ -58,31 +61,28 @@ def main():\n     )\n \n     parser.add_argument(\n-        \"-v\",\n-        \"--version\",\n-        action=\"store_true\",\n-        help=\"Print the version number and exit\"\n+        \"-v\", \"--version\", action=\"store_true\", help=\"print the version number and exit\"\n     )\n \n     #\n     # Settings and modifiers\n     #\n     parser.add_argument(\n-        \"-sw\",\n+        \"-S\",\n         \"--starts-with\",\n         action=\"store\",\n         default=\"\",\n         type=str,\n-        help=\"specify what string the random word(s) should start with\",\n+        help=\"strings the random word(s) should start with\",\n     )\n \n     parser.add_argument(\n-        \"-ew\",\n+        \"-e\",\n         \"--ends-with\",\n         action=\"store\",\n         default=\"\",\n         type=str,\n-        help=\"specify what string the random word(s) should end with\",\n+        help=\"strings the random word(s) should end with\",\n     )\n \n     parser.add_argument(\n@@ -94,25 +94,25 @@ def main():\n         # The plural forms will be removed in version 3\n         choices=[\"noun\", \"verb\", \"adjective\", \"nouns\", \"verbs\", \"adjectives\"],\n         help=(\n-            \"specify to only include certain parts of speech (by default all\"\n+            \"only include certain parts of speech (by default all\"\n             \" parts of speech are included)\"\n         ),\n     )\n \n     parser.add_argument(\n-        \"-min\",\n+        \"-m\",\n         \"--word-min-length\",\n         action=\"store\",\n         type=int,\n-        help=\"specify the minimum length of the word(s)\",\n+        help=\"minimum length of the word(s)\",\n     )\n \n     parser.add_argument(\n-        \"-max\",\n+        \"-M\",\n         \"--word-max-length\",\n         action=\"store\",\n         type=int,\n-        help=\"specify the maximum length of the word(s)\",\n+        help=\"maximum length of the word(s)\",\n     )\n \n     parser.add_argument(\n@@ -122,10 +122,15 @@ def main():\n         \"--regular-expression\",\n         action=\"store\",\n         type=str,\n-        help=(\n-            \"specify a python-style regular expression that every word must\"\n-            \" match\"\n-        ),\n+        help=(\"a python-style regular expression for the word(s) to match\"),\n+    )\n+\n+    parser.add_argument(\n+        \"-x\",\n+        \"--exclude-with-spaces\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"exclude open compounds, such as 'contact lens'\",\n     )\n \n     parser.add_argument(\n@@ -133,110 +138,89 @@ def main():\n         \"--delimiter\",\n         default=\", \",\n         type=str,\n-        help=(\n-            \"Specify the delimiter to put between a list of words, default is\"\n-            \" ', '\"\n-        ),\n+        help=\"specify the delimiter to put between a list of words, default is ', '\",\n+    )\n+\n+    parser.add_argument(\n+        \"-E\",\n+        \"--suppress-error-on-less\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"suppress errors when less words are returned in a list then wanted\",\n     )\n \n     args = parser.parse_args()\n     mode = get_mode(args)\n-    handle_mode(mode, args)\n+    sys.exit(run_wonderwords(mode, args))\n \n \n def get_mode(arguments):\n     if arguments.version:\n-        MODE = \"version\"\n+        return \"version\"\n     elif arguments.word:\n-        MODE = \"word\"\n+        return \"word\"\n     elif arguments.filter:\n-        MODE = \"filter\"\n+        return \"filter\"\n     elif arguments.list is not None:\n-        MODE = \"list\"\n+        return \"list\"\n     elif arguments.sentence is not None:\n-        MODE = \"sentence\"\n+        return \"sentence\"\n     else:\n-        MODE = None\n-\n-    return MODE\n-\n+        return None\n \n-def handle_mode(mode, arguments):  # noqa: C901\n-    command_line = WonderwordsCommandLine()\n \n+def run_wonderwords(mode, arguments):  # noqa: C901\n     if mode == \"version\":\n-        command_line.version()\n-        quit()\n-\n-    if mode == \"word\" or mode == \"filter\" or mode == \"list\":\n-        word_parser = RandomWord()\n-\n-    if mode == \"sentence\":\n-        sent_parser = RandomSentence()\n+        cmdline.display_version()\n+        return 0\n+\n+    kwargs = {\n+        \"starts_with\": arguments.starts_with,\n+        \"ends_with\": arguments.ends_with,\n+        \"include_categories\": arguments.parts_of_speech,\n+        \"word_min_length\": arguments.word_min_length,\n+        \"word_max_length\": arguments.word_max_length,\n+        \"regex\": arguments.regex,\n+        \"exclude_with_spaces\": arguments.exclude_with_spaces,\n+    }\n \n     if mode == \"word\":\n         try:\n-            word = word_parser.word(\n-                starts_with=arguments.starts_with,\n-                ends_with=arguments.ends_with,\n-                include_categories=arguments.parts_of_speech,\n-                word_min_length=arguments.word_min_length,\n-                word_max_length=arguments.word_max_length,\n-                regex=arguments.regex,\n-            )\n-        except NoWordsToChoseFrom:\n-            command_line.no_word()\n-            quit()\n-\n-        command_line.word(word)\n+            cmdline.display_word(RandomWord().word(**kwargs))\n+        except NoWordsToChooseFrom:\n+            cmdline.display_word_not_found(one_word=True)\n+            return 1\n     elif mode == \"filter\":\n-        words = word_parser.filter(\n-            starts_with=arguments.starts_with,\n-            ends_with=arguments.ends_with,\n-            include_categories=arguments.parts_of_speech,\n-            word_min_length=arguments.word_min_length,\n-            word_max_length=arguments.word_max_length,\n-            regex=arguments.regex,\n-        )\n-\n-        command_line.words(words, delimiter=arguments.delimiter)\n-\n-    elif mode == \"list\":\n-        try:\n-            words = word_parser.random_words(\n-                amount=arguments.list,\n-                starts_with=arguments.starts_with,\n-                ends_with=arguments.ends_with,\n-                include_categories=arguments.parts_of_speech,\n-                word_min_length=arguments.word_min_length,\n-                word_max_length=arguments.word_max_length,\n-                regex=arguments.regex,\n-            )\n-        except NoWordsToChoseFrom:\n-            command_line.no_words()\n-            words = word_parser.random_words(\n-                amount=arguments.list,\n-                starts_with=arguments.starts_with,\n-                ends_with=arguments.ends_with,\n-                include_categories=arguments.parts_of_speech,\n-                word_min_length=arguments.word_min_length,\n-                word_max_length=arguments.word_max_length,\n-                return_less_if_necessary=True,\n-                regex=arguments.regex,\n+        words = RandomWord().filter(**kwargs)\n+        if words:\n+            cmdline.display_list(\n+                RandomWord().filter(**kwargs), delimiter=arguments.delimiter\n             )\n-\n-        command_line.words(words, delimiter=arguments.delimiter)\n+        else:\n+            cmdline.display_word_not_found(one_word=False)\n+    elif mode == \"list\":\n+        words = RandomWord().random_words(\n+            amount=arguments.list, return_less_if_necessary=True, **kwargs\n+        )\n+        if words is not None:\n+            cmdline.display_list(words, delimiter=arguments.delimiter)\n+        if not arguments.suppress_error_on_less and len(words) < arguments.list:\n+            cmdline.display_not_enough_words()\n+            return 1\n     elif mode == \"sentence\":\n+        generator = RandomSentence()\n         if arguments.sentence == \"bb\":\n-            command_line.sentence(sent_parser.bare_bone_sentence())\n+            cmdline.display_sentence(generator.bare_bone_sentence())\n         elif arguments.sentence == \"ss\":\n-            command_line.sentence(sent_parser.simple_sentence())\n+            cmdline.display_sentence(generator.simple_sentence())\n         elif arguments.sentence == \"bba\":\n-            command_line.sentence(sent_parser.bare_bone_with_adjective())\n+            cmdline.display_sentence(generator.bare_bone_with_adjective())\n         else:\n-            command_line.sentence(sent_parser.sentence())\n+            cmdline.display_sentence(generator.sentence())\n     else:\n-        command_line.intro()\n+        cmdline.display_hello()\n+\n+    return 0\n \n \n if __name__ == \"__main__\":\ndiff --git a/wonderwords/random_sentence.py b/wonderwords/random_sentence.py\nindex bbc65bf..5590c5f 100644\n--- a/wonderwords/random_sentence.py\n+++ b/wonderwords/random_sentence.py\n@@ -11,14 +11,13 @@\n VOWELS = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n \n \n-def _present_tense(verb):\n-    \"\"\"Convert a verb in the form of \"to ____\" to the present tense 3rd person\n+def _present_tense(verb: str) -> str:\n+    \"\"\"Convert a verb from the infinitive to the present tense 3rd person\n     form\"\"\"\n-    verb = verb.strip()\n-    lowercase_verb = verb.lower()\n-    if lowercase_verb.endswith((\"ss\", \"ch\", \"x\", \"tch\", \"sh\", \"zz\")):\n+    verb = verb.strip().lower()\n+    if verb.endswith((\"ss\", \"ch\", \"x\", \"tch\", \"sh\", \"zz\")):\n         verb = verb + \"es\"\n-    elif lowercase_verb.endswith(\"y\") and not verb.endswith(\n+    elif verb.endswith(\"y\") and not verb.endswith(\n         tuple([vowel + \"y\" for vowel in VOWELS])\n     ):\n         verb = verb[:-1] + \"ies\"\n@@ -27,8 +26,8 @@ def _present_tense(verb):\n     return verb\n \n \n-def _with_article(word):\n-    article, = random.choices([\"the\", \"a\", \"\"], weights=[5, 3, 2])\n+def _with_article(word: str) -> str:\n+    (article,) = random.choices([\"the\", \"a\", \"\"], weights=[5, 3, 2])\n     if article == \"a\" and word[0] in VOWELS:\n         article = \"an\"\n     if article:\n@@ -69,7 +68,7 @@ def __init__(\n         self.gen = RandomWord(noun=noun, verb=verb, adjective=adjective)\n \n     # Randomly generate bare bone sentences\n-    def bare_bone_sentence(self):\n+    def bare_bone_sentence(self) -> str:\n         \"\"\"Generate a bare-bone sentence in the form of\n         ``[(article)] [subject (noun)] [predicate (verb)].``. For example:\n         ``The cat runs.``.\n@@ -87,7 +86,7 @@ def bare_bone_sentence(self):\n \n         return f\"{the_noun.capitalize()} {the_verb}.\"\n \n-    def simple_sentence(self):\n+    def simple_sentence(self) -> str:\n         \"\"\"Generate a simple sentence in the form of\n         ``[(article)] [subject (noun)] [predicate (verb)] [direct object (noun)].``.\n         For example: ``The cake plays golf``.\n@@ -105,7 +104,7 @@ def simple_sentence(self):\n \n         return f\"{the_bare_bone_sentence} {the_direct_object}.\"\n \n-    def bare_bone_with_adjective(self):\n+    def bare_bone_with_adjective(self) -> str:\n         \"\"\"Generate a bare-bone sentence with an adjective in the form of:\n         ``[(article)] [(adjective)] [subject (noun)] [predicate (verb)].``. For\n         example: ``The skinny cat reads.``\n@@ -124,7 +123,7 @@ def bare_bone_with_adjective(self):\n \n         return f\"{the_adjective.capitalize()} {the_noun} {the_verb}.\"\n \n-    def sentence(self):\n+    def sentence(self) -> str:\n         \"\"\"Generate a simple sentence with an adjective in the form of:\n         ``[(article)] [(adjective)] [subject (noun)] [predicate (verb)]\n         [direct object (noun)].``. For example:\ndiff --git a/wonderwords/random_word.py b/wonderwords/random_word.py\nindex 52884fe..6f99e9f 100644\n--- a/wonderwords/random_word.py\n+++ b/wonderwords/random_word.py\n@@ -3,22 +3,34 @@\n generation of single random words.\n \"\"\"\n \n-import random\n+from random import Random\n import re\n import enum\n-from typing import Optional, List\n+from types import ModuleType\n+from typing import (\n+    Union,\n+    Optional,\n+    List,\n+    Dict,\n+    Any,\n+    Type,\n+    TextIO,\n+    Tuple,\n+    IO,\n+    Iterable,\n+    Set,\n+    Iterator,\n+)\n \n from . import assets\n+from . import _trie\n \n-try:\n-    import importlib.resources as pkg_resources\n-except ImportError:\n-    # Try backported to PY<37 `importlib_resources`.\n-    import importlib_resources as pkg_resources\n \n+WordList = List[str]\n \n-class NoWordsToChoseFrom(Exception):\n-    \"\"\"NoWordsToChoseFrom is raised when there is an attempt to access more\n+\n+class NoWordsToChooseFrom(Exception):\n+    \"\"\"NoWordsToChooseFrom is raised when there is an attempt to access more\n     words than exist. This exception may be raised if the amount of random\n     words to generate is larger than the amount of words that exist.\n     \"\"\"\n@@ -51,7 +63,34 @@ class Defaults(enum.Enum):\n     PROFANITIES = \"profanitylist.txt\"\n \n \n-def _load_default_categories(default_categories):\n+def _obtain_resource(\n+    package: Union[str, ModuleType], resource: str\n+) -> Union[IO[str], TextIO]:\n+    \"\"\"Load a file object from a package, ensuring compatibility with supported Python versions.\"\"\"\n+    try:\n+        # Introduced in Python 3.9\n+        from importlib.resources import files  # type: ignore\n+\n+        return files(package).joinpath(resource).open(\"r\")\n+    except ImportError:\n+        # Required for Python 3.8, but emits a DeprecationWarning on Python 3.11\n+        from importlib.resources import open_text\n+\n+        return open_text(package, resource)\n+\n+\n+def _get_words_from_text_file(word_file: str) -> WordList:\n+    \"\"\"Read a file found in static/ where each line has a word, and return\n+    all words as a list\n+    \"\"\"\n+    with _obtain_resource(assets, word_file) as f:\n+        words = f.readlines()\n+    return [word.rstrip() for word in words]\n+\n+\n+def _load_default_categories(\n+    default_categories: Type[Defaults],\n+) -> Dict[Defaults, WordList]:\n     \"\"\"Load all the default word lists\"\"\"\n     out = {}\n     for category in default_categories:\n@@ -59,15 +98,37 @@ def _load_default_categories(default_categories):\n     return out\n \n \n-def _get_words_from_text_file(word_file):\n-    \"\"\"Read a file found in static/ where each line has a word, and return\n-    all words as a list\n+# A dictionary where each key representing a category like 'nouns' corresponds to a list of words.\n+_DEFAULT_CATEGORIES: Dict[Defaults, WordList] = _load_default_categories(Defaults)\n+\n+\n+def is_profanity(word: str) -> bool:\n+    \"\"\"See if a word matches one of the words in the profanity list.\n+\n+    :param word: The word to check\n+    :return: Whether the word was found in the profanity list\n     \"\"\"\n-    words = pkg_resources.open_text(assets, word_file).readlines()\n-    return [word.rstrip() for word in words]\n+    return word.lower().strip() in _DEFAULT_CATEGORIES[Defaults.PROFANITIES]\n+\n+\n+def filter_profanity(words: Iterable[str]) -> Iterator[str]:\n+    \"\"\"Attempt to filter out profane words from a list. This should be done in all user-facing applications if random\n+    words are generated to avoid anything that could possibly be offensive. Curse word filtering is currently not done\n+    by default on the :py:class:`RandomWord` class.\n \n+    Example::\n+\n+        >>> from wonderwords import filter_profanity\n+        >>> list(filter_profanity([\"hello\", \"aSS\", \"world\"]))\n+        [\"hello\", \"world\"]\n \n-_default_categories = _load_default_categories(Defaults)\n+    :param words: Iterable of words to filter\n+    :return: An iterable of the filtered result\n+    \"\"\"\n+    return filter(\n+        lambda w: not is_profanity(w),\n+        words,\n+    )\n \n \n class RandomWord:\n@@ -92,18 +153,38 @@ class RandomWord:\n        version ``3``. Please note that the ``parts_of_speech`` attribute will\n        soon be deprecated, along with other method-specific features.\n \n-    :param \\*\\*kwargs: keyword arguments where each key is a category of words\n+    :param enhanced_prefixes: whether to internally use a trie data\n+        structure to speed up ``starts_with`` and ``ends_with``. If enabled,\n+        the class takes longer to instantiate, but calls to the generation\n+        functions will be significantly (up to 4x) faster when using the\n+        ``starts_with`` and ``ends_with`` arguments. Defaults to True.\n+    :type enhanced_prefixes: bool, optional\n+    :param rng: an instance of a ``random.Random`` used for randomization\n+    :type rng: random.Random, optional\n+    :param kwargs: keyword arguments where each key is a category of words\n         and value is a list of words in that category. You can also use a\n-        default list of words by using the `Default` enum instead.\n-    :type nouns: list, optional\n+        default list of words by using a value from the `Default` enum instead.\n+    :type kwargs: list, optional\n \n     \"\"\"\n \n-    def __init__(self, **kwargs):\n+    def __init__(\n+        self, enhanced_prefixes: bool = True, rng=None, **kwargs: Union[WordList, Defaults]\n+    ):\n+        # A dictionary where lists of words organized into named categories\n+        self._categories: Dict[str, WordList]\n+        # If enhanced prefixes are enabled, these tries represent all the words known to the generator in forward and\n+        # reverse. If disabled, this is just None.\n+        self._tries: Union[Tuple[_trie.Trie, _trie.Trie], None]\n+        # Random number generator.\n+        self._generator: Random = rng or Random()\n+        # Kept for backwards compatibility. Same as self._categories\n+        self.parts_of_speech: Dict[str, WordList]\n+\n         if kwargs:\n-            self._categories = self._custom_categories(kwargs)\n+            self._categories = self._get_word_lists_by_category(kwargs)\n         else:\n-            self._categories = self._custom_categories(\n+            self._categories = self._get_word_lists_by_category(\n                 {\n                     \"noun\": Defaults.NOUNS,\n                     \"verb\": Defaults.VERBS,\n@@ -116,21 +197,35 @@ def __init__(self, **kwargs):\n                     \"adjectives\": Defaults.ADJECTIVES,\n                 }\n             )\n-        # Kept for backwards compatibility\n+\n+        if enhanced_prefixes:\n+            # Two tries. One trie data structure is generated from\n+            # the words, while the second one is generated from the\n+            # words in reverse to deal with starts_with and ends_with\n+            # respectively.\n+            self._tries = (_trie.Trie(), _trie.Trie())\n+            for _, category in self._categories.items():\n+                for word in category:\n+                    self._tries[0].insert(word)\n+                    self._tries[1].insert(word[::-1])\n+        else:\n+            self._tries = None\n+\n         self.parts_of_speech = self._categories\n \n     def filter(  # noqa: C901\n         self,\n         starts_with: str = \"\",\n         ends_with: str = \"\",\n-        include_categories: Optional[List[str]] = None,\n-        include_parts_of_speech: Optional[List[str]] = None,\n+        include_categories: Optional[Iterable[str]] = None,\n+        include_parts_of_speech: Optional[Iterable[str]] = None,\n         word_min_length: Optional[int] = None,\n         word_max_length: Optional[int] = None,\n         regex: Optional[str] = None,\n-    ):\n-        \"\"\"Return all existing words that match the criteria specified by the\n-        arguments.\n+        exclude_with_spaces: bool = False,\n+    ) -> WordList:\n+        \"\"\"Return a sorted list of all existing words that match the criteria\n+        specified by the arguments.\n \n         Example::\n \n@@ -163,6 +258,8 @@ def filter(  # noqa: C901\n         :param regex: a custom regular expression which each word must fully\n             match (re.fullmatch). Defaults to None.\n         :type regex: str, optional\n+        :param exclude_with_spaces: exclude words that may have spaces in them\n+        :type exclude_with_spaces: bool, optional\n \n         :return: a list of unique words that match each of the criteria\n             specified\n@@ -177,57 +274,83 @@ def filter(  # noqa: C901\n             if include_parts_of_speech:\n                 include_categories = include_parts_of_speech\n             else:\n-                include_categories = self._categories.keys()\n+                include_categories = list(self._categories.keys())\n+\n+        # Filter by part of speech and length. Both of these things\n+        # are done at once since categories are specifically ordered\n+        # in order to make filtering by length an efficient process.\n+        # See issue #14 for details.\n+        words = set()\n \n-        # filter parts of speech\n-        words = []\n         for category in include_categories:\n             try:\n-                words.extend(self._categories[category])\n+                words_in_category = self._categories[category]\n             except KeyError:\n-                raise ValueError(\n-                    f\"'{category}' is an invalid category\"\n-                ) from None\n-\n-        # starts/ends\n-        if starts_with != \"\" or ends_with != \"\":\n-            for word in words[:]:\n-                if not word.startswith(starts_with):\n-                    words.remove(word)\n-                elif not word.endswith(ends_with):\n-                    words.remove(word)\n-\n-        # length\n-        if word_min_length is not None or word_max_length is not None:\n-            for word in words[:]:\n-                if word_min_length is not None and len(word) < word_min_length:\n-                    words.remove(word)\n-                elif (\n-                    word_max_length is not None\n-                    and len(word) > word_max_length\n-                ):\n-                    words.remove(word)\n-\n-        # regex\n+                raise ValueError(f\"'{category}' is an invalid category\") from None\n+\n+            words_to_add = self._get_words_of_length(\n+                words_in_category, word_min_length, word_max_length\n+            )\n+            words.update(words_to_add)\n+\n+        if self._tries is not None:\n+            if starts_with:\n+                words = words & self._tries[0].get_words_that_start_with(starts_with)\n+            if ends_with:\n+                # Since the ends_with trie is in reverse, the\n+                # ends_with variable must also be reversed.\n+                # Example (apple):\n+                # - Backwards: elppa\n+                # - ends_with: el\n+                # Currently this is very clunky, since all words\n+                # that match then need to be reversed to their\n+                # original forms. Currently, I have no idea how\n+                # to improve this. But, even with the extra overhead\n+                # of iteration, this system still significantly\n+                # shortens the amount of time to filter the words.\n+                ends_with = ends_with[::-1]\n+                words = words & set(\n+                    [\n+                        i[::-1]\n+                        for i in self._tries[1].get_words_that_start_with(ends_with)\n+                    ]\n+                )\n+\n+        # Long operations that require looping over every word\n+        # (O(n)). Since they are so time-consuming, the arguments\n+        # passed to the function are first checked if the user\n+        # actually specified any time-consuming arguments. If they\n+        # are, all long filters are checked at once for every word.\n+        long_operations: Dict[str, Any] = {}\n+\n         if regex is not None:\n-            words = [\n-                word for word in words if re.fullmatch(regex, word) is not None\n-            ]\n+            long_operations[\"regex\"] = re.compile(regex)\n+        if exclude_with_spaces:\n+            long_operations[\"exclude_with_spaces\"] = None\n+        if self._tries is None:\n+            if starts_with:\n+                long_operations[\"starts_with\"] = starts_with\n+            if ends_with:\n+                long_operations[\"ends_with\"] = ends_with\n+\n+        if long_operations:\n+            words -= self._perform_long_operations(words, long_operations)\n \n-        return list(set(words))\n+        return sorted(list(words))\n \n     def random_words(\n         self,\n         amount: int = 1,\n         starts_with: str = \"\",\n         ends_with: str = \"\",\n-        include_categories: Optional[List[str]] = None,\n-        include_parts_of_speech: Optional[List[str]] = None,\n+        include_categories: Optional[Iterable[str]] = None,\n+        include_parts_of_speech: Optional[Iterable[str]] = None,\n         word_min_length: Optional[int] = None,\n         word_max_length: Optional[int] = None,\n         regex: Optional[str] = None,\n         return_less_if_necessary: bool = False,\n-    ):\n+        exclude_with_spaces: bool = False,\n+    ) -> WordList:\n         \"\"\"Generate a list of n random words specified by the ``amount``\n         parameter and fit the criteria specified.\n \n@@ -272,8 +395,10 @@ def random_words(\n             NoWordsToChoseFrom exception, return all words that did statisfy\n             the original query.\n         :type return_less_if_necessary: bool, optional\n+        :param exclude_with_spaces: exclude words that may have spaces in them\n+        :type exclude_with_spaces: bool, optional\n \n-        :raises NoWordsToChoseFrom: if there are less words to choose from than\n+        :raises NoWordsToChoseFrom: if there are fewer words to choose from than\n             the amount that was requested, a NoWordsToChoseFrom exception is\n             raised, **unless** return_less_if_necessary is set to True.\n \n@@ -288,20 +413,22 @@ def random_words(\n             word_min_length=word_min_length,\n             word_max_length=word_max_length,\n             regex=regex,\n+            exclude_with_spaces=exclude_with_spaces,\n         )\n \n-        if not return_less_if_necessary and len(choose_from) < amount:\n-            raise NoWordsToChoseFrom(\n-                \"There aren't enough words to choose from. Cannot generate \"\n-                f\"{str(amount)} word(s)\"\n-            )\n-        elif return_less_if_necessary:\n-            random.shuffle(choose_from)\n-            return choose_from\n+        if len(choose_from) < amount:\n+            if return_less_if_necessary:\n+                self._generator.shuffle(choose_from)\n+                return choose_from\n+            else:\n+                raise NoWordsToChooseFrom(\n+                    \"There aren't enough words to choose from. Cannot generate \"\n+                    f\"{str(amount)} word(s)\"\n+                )\n \n         words = []\n         for _ in range(amount):\n-            new_word = random.choice(choose_from)\n+            new_word = self._generator.choice(choose_from)\n             choose_from.remove(new_word)\n             words.append(new_word)\n \n@@ -311,12 +438,13 @@ def word(\n         self,\n         starts_with: str = \"\",\n         ends_with: str = \"\",\n-        include_categories: Optional[List[str]] = None,\n-        include_parts_of_speech: Optional[List[str]] = None,\n+        include_categories: Optional[Iterable[str]] = None,\n+        include_parts_of_speech: Optional[Iterable[str]] = None,\n         word_min_length: Optional[int] = None,\n         word_max_length: Optional[int] = None,\n         regex: Optional[str] = None,\n-    ):\n+        exclude_with_spaces: bool = False,\n+    ) -> str:\n         \"\"\"Returns a random word that fits the criteria specified by the\n         arguments.\n \n@@ -347,6 +475,8 @@ def word(\n         :param regex: a custom regular expression which each word must fully\n             match (re.fullmatch). Defaults to None.\n         :type regex: str, optional\n+        :param exclude_with_spaces: exclude words that may have spaces in them\n+        :type exclude_with_spaces: bool, optional\n \n         :raises NoWordsToChoseFrom: if a word fitting the criteria doesn't\n             exist\n@@ -363,19 +493,21 @@ def word(\n             word_min_length=word_min_length,\n             word_max_length=word_max_length,\n             regex=regex,\n+            exclude_with_spaces=exclude_with_spaces,\n         )[0]\n \n     @staticmethod\n-    def read_words(word_file):\n+    def read_words(word_file: str) -> WordList:\n         \"\"\"Will soon be deprecated. This method isn't meant to be public, but\n         will remain for backwards compatibility. Developers: use\n         _get_words_from_text_file internally instead.\n         \"\"\"\n         return _get_words_from_text_file(word_file)\n \n-    def _validate_lengths(self, word_min_length, word_max_length):\n-        \"\"\"Validate the values and types of word_min_length and word_max_length\n-        \"\"\"\n+    def _validate_lengths(\n+        self, word_min_length: Any, word_max_length: Any\n+    ) -> Tuple[Union[int, None], Union[int, None]]:\n+        \"\"\"Validate the values and types of word_min_length and word_max_length\"\"\"\n         if not isinstance(word_min_length, (int, type(None))):\n             raise TypeError(\"word_min_length must be type int or None\")\n \n@@ -383,7 +515,7 @@ def _validate_lengths(self, word_min_length, word_max_length):\n             raise TypeError(\"word_max_length must be type int or None\")\n \n         if word_min_length is not None and word_max_length is not None:\n-            if word_min_length > word_max_length and word_max_length != 0:\n+            if word_min_length > word_max_length != 0:\n                 raise ValueError(\n                     \"word_min_length cannot be greater than word_max_length\"\n                 )\n@@ -394,14 +526,83 @@ def _validate_lengths(self, word_min_length, word_max_length):\n         if word_max_length is not None and word_max_length < 0:\n             word_max_length = None\n \n-        return (word_min_length, word_max_length)\n+        return word_min_length, word_max_length\n \n-    def _custom_categories(self, custom_categories):\n-        \"\"\"Add custom categries of words\"\"\"\n+    def _get_word_lists_by_category(\n+        self, custom_categories: Dict[str, Any]\n+    ) -> Dict[str, WordList]:\n+        \"\"\"Add custom categories of words\"\"\"\n         out = {}\n         for name, words in custom_categories.items():\n             if isinstance(words, Defaults):\n-                out[name] = _default_categories[words]\n+                word_list = _DEFAULT_CATEGORIES[words]\n             else:\n-                out[name] = words\n+                word_list = words\n+\n+            # All the words in each category are sorted. This is so\n+            # that they can be bisected by length later on for more\n+            # efficient word length retrieval. See issue #14 for\n+            # details.\n+            word_list.sort(key=len)\n+            out[name] = word_list\n+\n         return out\n+\n+    def _get_words_of_length(\n+        self,\n+        word_list: WordList,\n+        min_length: Optional[int] = None,\n+        max_length: Optional[int] = None,\n+    ) -> WordList:\n+        \"\"\"Given ``word_list``, get all words that are at least\n+        ``min_length`` long and at most ``max_length`` long.\n+        \"\"\"\n+\n+        if min_length is None:\n+            left_index = 0\n+        else:\n+            left_index = self._bisect_by_length(word_list, min_length)\n+\n+        if max_length is None:\n+            right_index = None\n+        else:\n+            right_index = self._bisect_by_length(word_list, max_length + 1)\n+\n+        return word_list[left_index:right_index]\n+\n+    def _bisect_by_length(self, words: WordList, target_length: int) -> int:\n+        \"\"\"Given a list of sorted words by length, get the index of the\n+        first word that's of the ``target_length``.\n+        \"\"\"\n+\n+        left = 0\n+        right = len(words) - 1\n+\n+        while left <= right:\n+            middle = left + (right - left) // 2\n+            if len(words[middle]) < target_length:\n+                left = middle + 1\n+            else:\n+                right = middle - 1\n+\n+        return left\n+\n+    def _perform_long_operations(\n+        self, words: Set[str], long_operations: Dict[str, Any]\n+    ) -> Set[str]:\n+        \"\"\"Return a set of words that do not meet the criteria specified by the long operations.\"\"\"\n+        remove_words = set()\n+        for word in words:\n+            if \"regex\" in long_operations:\n+                if not long_operations[\"regex\"].fullmatch(word):\n+                    remove_words.add(word)\n+            if \"exclude_with_spaces\" in long_operations:\n+                if \" \" in word:\n+                    remove_words.add(word)\n+            if \"starts_with\" in long_operations:\n+                if not word.startswith(long_operations[\"starts_with\"]):\n+                    remove_words.add(word)\n+            if \"ends_with\" in long_operations:\n+                if not word.endswith(long_operations[\"ends_with\"]):\n+                    remove_words.add(word)\n+        return remove_words\n", "instance_id": "mrmaxguns__wonderwordsmodule-27", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in expressing the intent to add a feature for filtering curse words in a word generator with a flag that defaults to False. The goal is straightforward: prevent inappropriate adjective-noun pairs from being generated unless explicitly allowed. However, there are minor ambiguities and missing details. For instance, the problem does not specify how the curse word list should be sourced or maintained, nor does it define what constitutes a curse word (e.g., cultural or contextual considerations). Additionally, there are no explicit examples of input/output formats or constraints on how the flag should be implemented (e.g., as a command-line argument, API parameter, or both). Edge cases, such as handling partial matches or case sensitivity, are also not addressed. Despite these minor gaps, the overall intent and desired solution are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of implementing the curse words flag feature falls in the Easy range (0.2-0.4). The task requires understanding the existing word generation logic in the `RandomWord` class and adding a simple filtering mechanism based on a flag, which defaults to False. The code changes provided show updates to documentation, API, and CLI, indicating the feature has been partially or fully implemented with additions like `is_profanity` and `filter_profanity` functions, as well as updates to word lists. This suggests the core logic involves basic modifications, such as extending the filtering criteria with a profanity check, which is already supported by the existing framework (e.g., regex and category filtering). \n\nThe scope of changes is moderate, affecting multiple files (e.g., `random_word.py`, `cmdline_parser.py`, documentation), but the impact is localized to the word filtering logic without altering the system's architecture. Technical concepts involved are straightforward: basic string matching, list filtering, and parameter handling in Python, with no complex algorithms or domain-specific knowledge required. Edge cases, such as handling case sensitivity or partial word matches, are not explicitly mentioned in the problem but can be addressed with simple logic (e.g., normalizing input). Error handling requirements are minimal, likely limited to ensuring the flag behaves correctly when toggled.\n\nOverall, this task requires understanding some code logic and making simple function modifications, aligning with a difficulty score of 0.35. It is slightly above the lower end of the Easy range due to the need to integrate the feature across API and CLI interfaces, but it remains a relatively simple enhancement to an existing system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add DocumentNDCGEvaluator component\nSome datasets contain document relevance scores or at least a sorted order of ground truth relevant documents, for example HotpotQA. In order to evaluate rankers and retrievers regarding their ability to not only distinguish relevant from non-relevant documents but also to sort relevant documents by their relevance, we need a new component DocumentNDCGEvaluator. The currently available DocumentMRREvaluator considers relevancy of documents to be binary.\n", "patch": "diff --git a/docs/pydoc/config/evaluators_api.yml b/docs/pydoc/config/evaluators_api.yml\nindex 8c6ffe1556..9a8460b94a 100644\n--- a/docs/pydoc/config/evaluators_api.yml\n+++ b/docs/pydoc/config/evaluators_api.yml\n@@ -7,7 +7,7 @@ loaders:\n         \"context_relevance\",\n         \"document_map\",\n         \"document_mrr\",\n-        \"document_recall\",\n+        \"document_ndcg\",\n         \"document_recall\",\n         \"faithfulness\",\n         \"llm_evaluator\",\ndiff --git a/haystack/components/evaluators/__init__.py b/haystack/components/evaluators/__init__.py\nindex 69983a1108..d57da5c1f2 100644\n--- a/haystack/components/evaluators/__init__.py\n+++ b/haystack/components/evaluators/__init__.py\n@@ -6,6 +6,7 @@\n from .context_relevance import ContextRelevanceEvaluator\n from .document_map import DocumentMAPEvaluator\n from .document_mrr import DocumentMRREvaluator\n+from .document_ndcg import DocumentNDCGEvaluator\n from .document_recall import DocumentRecallEvaluator\n from .faithfulness import FaithfulnessEvaluator\n from .llm_evaluator import LLMEvaluator\n@@ -16,6 +17,7 @@\n     \"ContextRelevanceEvaluator\",\n     \"DocumentMAPEvaluator\",\n     \"DocumentMRREvaluator\",\n+    \"DocumentNDCGEvaluator\",\n     \"DocumentRecallEvaluator\",\n     \"FaithfulnessEvaluator\",\n     \"LLMEvaluator\",\ndiff --git a/haystack/components/evaluators/document_ndcg.py b/haystack/components/evaluators/document_ndcg.py\nnew file mode 100644\nindex 0000000000..e3430f1db7\n--- /dev/null\n+++ b/haystack/components/evaluators/document_ndcg.py\n@@ -0,0 +1,133 @@\n+# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>\n+#\n+# SPDX-License-Identifier: Apache-2.0\n+\n+from math import log2\n+from typing import Any, Dict, List\n+\n+from haystack import Document, component\n+\n+\n+@component\n+class DocumentNDCGEvaluator:\n+    \"\"\"\n+    Evaluator that calculates the normalized discounted cumulative gain (NDCG) of retrieved documents.\n+\n+    Each question can have multiple ground truth documents and multiple retrieved documents.\n+    If the ground truth documents have relevance scores, the NDCG calculation uses these scores.\n+    Otherwise, it assumes binary relevance of all ground truth documents.\n+\n+    Usage example:\n+    ```python\n+    from haystack import Document\n+    from haystack.components.evaluators import DocumentNDCGEvaluator\n+\n+    evaluator = DocumentNDCGEvaluator()\n+    result = evaluator.run(\n+        ground_truth_documents=[[Document(content=\"France\", score=1.0), Document(content=\"Paris\", score=0.5)]],\n+        retrieved_documents=[[Document(content=\"France\"), Document(content=\"Germany\"), Document(content=\"Paris\")]],\n+    )\n+    print(result[\"individual_scores\"])\n+    # [0.8869]\n+    print(result[\"score\"])\n+    # 0.8869\n+    ```\n+    \"\"\"\n+\n+    @component.output_types(score=float, individual_scores=List[float])\n+    def run(\n+        self, ground_truth_documents: List[List[Document]], retrieved_documents: List[List[Document]]\n+    ) -> Dict[str, Any]:\n+        \"\"\"\n+        Run the DocumentNDCGEvaluator on the given inputs.\n+\n+        `ground_truth_documents` and `retrieved_documents` must have the same length.\n+        The list items within `ground_truth_documents` and `retrieved_documents` can differ in length.\n+\n+        :param ground_truth_documents:\n+            Lists of expected documents, one list per question. Binary relevance is used if documents have no scores.\n+        :param retrieved_documents:\n+            Lists of retrieved documents, one list per question.\n+        :returns:\n+            A dictionary with the following outputs:\n+            - `score` - The average of calculated scores.\n+            - `individual_scores` - A list of numbers from 0.0 to 1.0 that represents the NDCG for each question.\n+        \"\"\"\n+        self.validate_inputs(ground_truth_documents, retrieved_documents)\n+\n+        individual_scores = []\n+\n+        for gt_docs, ret_docs in zip(ground_truth_documents, retrieved_documents):\n+            dcg = self.calculate_dcg(gt_docs, ret_docs)\n+            idcg = self.calculate_idcg(gt_docs)\n+            ndcg = dcg / idcg if idcg > 0 else 0\n+            individual_scores.append(ndcg)\n+\n+        score = sum(individual_scores) / len(ground_truth_documents)\n+\n+        return {\"score\": score, \"individual_scores\": individual_scores}\n+\n+    @staticmethod\n+    def validate_inputs(gt_docs: List[List[Document]], ret_docs: List[List[Document]]):\n+        \"\"\"\n+        Validate the input parameters.\n+\n+        :param gt_docs:\n+            The ground_truth_documents to validate.\n+        :param ret_docs:\n+            The retrieved_documents to validate.\n+\n+        :raises ValueError:\n+            If the ground_truth_documents or the retrieved_documents are an empty a list.\n+            If the length of ground_truth_documents and retrieved_documents differs.\n+            If any list of documents in ground_truth_documents contains a mix of documents with and without a score.\n+        \"\"\"\n+        if len(gt_docs) == 0 or len(ret_docs) == 0:\n+            msg = \"ground_truth_documents and retrieved_documents must be provided.\"\n+            raise ValueError(msg)\n+\n+        if len(gt_docs) != len(ret_docs):\n+            msg = \"The length of ground_truth_documents and retrieved_documents must be the same.\"\n+            raise ValueError(msg)\n+\n+        for docs in gt_docs:\n+            if any(doc.score is not None for doc in docs) and any(doc.score is None for doc in docs):\n+                msg = \"Either none or all documents in each list of ground_truth_documents must have a score.\"\n+                raise ValueError(msg)\n+\n+    @staticmethod\n+    def calculate_dcg(gt_docs: List[Document], ret_docs: List[Document]) -> float:\n+        \"\"\"\n+        Calculate the discounted cumulative gain (DCG) of the retrieved documents.\n+\n+        :param gt_docs:\n+            The ground truth documents.\n+        :param ret_docs:\n+            The retrieved documents.\n+        :returns:\n+            The discounted cumulative gain (DCG) of the retrieved\n+            documents based on the ground truth documents.\n+        \"\"\"\n+        dcg = 0.0\n+        relevant_id_to_score = {doc.id: doc.score if doc.score is not None else 1 for doc in gt_docs}\n+        for i, doc in enumerate(ret_docs):\n+            if doc.id in relevant_id_to_score:  # TODO Related to https://github.com/deepset-ai/haystack/issues/8412\n+                dcg += relevant_id_to_score[doc.id] / log2(i + 2)  # i + 2 because i is 0-indexed\n+        return dcg\n+\n+    @staticmethod\n+    def calculate_idcg(gt_docs: List[Document]) -> float:\n+        \"\"\"\n+        Calculate the ideal discounted cumulative gain (IDCG) of the ground truth documents.\n+\n+        :param gt_docs:\n+            The ground truth documents.\n+        :returns:\n+            The ideal discounted cumulative gain (IDCG) of the ground truth documents.\n+        \"\"\"\n+        idcg = 0.0\n+        for i, doc in enumerate(sorted(gt_docs, key=lambda x: x.score if x.score is not None else 1, reverse=True)):\n+            # If the document has a score, use it; otherwise, use 1 for binary relevance.\n+            relevance = doc.score if doc.score is not None else 1\n+            idcg += relevance / log2(i + 2)  # i + 2 because i is 0-indexed\n+        return idcg\ndiff --git a/releasenotes/notes/document-ndcg-evaluator-d579f51dd76ae76a.yaml b/releasenotes/notes/document-ndcg-evaluator-d579f51dd76ae76a.yaml\nnew file mode 100644\nindex 0000000000..eae5d32079\n--- /dev/null\n+++ b/releasenotes/notes/document-ndcg-evaluator-d579f51dd76ae76a.yaml\n@@ -0,0 +1,4 @@\n+---\n+features:\n+  - |\n+    Added a new component DocumentNDCGEvaluator, which is similar to DocumentMRREvaluator and useful for retrieval evaluation. It calculates the normalized discounted cumulative gain, an evaluation metric useful when there are multiple ground truth relevant documents and the order in which they are retrieved is important.\n", "instance_id": "deepset-ai__haystack-8419", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a new `DocumentNDCGEvaluator` component to evaluate rankers and retrievers based on their ability to sort relevant documents by relevance using normalized discounted cumulative gain (NDCG). It provides context by comparing it to the existing `DocumentMRREvaluator`, which uses binary relevance. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the component, nor does it mention specific edge cases or constraints (e.g., handling of documents without relevance scores, or performance considerations for large datasets). While the code changes and usage example in the implementation partially address these gaps, the problem statement itself lacks comprehensive detail, such as explicit mention of how relevance scores should be handled or what happens when ground truth and retrieved document lists differ significantly in size or content. Thus, it is rated as \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of this problem is rated as medium (0.50) based on the following analysis of the factors:\n\n1. **Scope and Depth of Code Changes**: The code changes involve creating a new file (`document_ndcg.py`) with a significant amount of new logic (133 lines), as well as minor updates to existing files for integration (e.g., updating imports and configuration in `__init__.py` and `evaluators_api.yml`). The changes are mostly isolated to adding a new component without altering the existing architecture or requiring deep modifications to other parts of the codebase. However, integrating this component requires understanding how evaluators are structured and used within the Haystack framework.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including the NDCG metric (a ranking evaluation metric involving discounted cumulative gain calculations), Python programming, and the Haystack framework's component-based architecture (e.g., using the `@component` decorator and defining input/output types). Additionally, the implementation involves basic mathematical computations (e.g., logarithmic discounting) and data structure handling (e.g., lists of `Document` objects). While NDCG is a moderately complex concept for those unfamiliar with information retrieval metrics, the other concepts are relatively standard for a mid-level developer.\n\n3. **Edge Cases and Error Handling**: The code changes include validation logic to handle specific edge cases, such as ensuring non-empty input lists, matching lengths of ground truth and retrieved document lists, and consistent presence of relevance scores within a list of documents. These validations are straightforward but necessary. The problem statement itself does not explicitly mention edge cases, but the implementation addresses them adequately. There are no particularly complex edge cases (e.g., performance issues with large datasets or intricate relevance score normalization) evident in the provided changes.\n\n4. **Overall Complexity**: The task requires implementing a new evaluation metric, which involves understanding a specific algorithm (NDCG) and integrating it into an existing framework. It is not a trivial task, as it demands both domain knowledge (information retrieval metrics) and framework familiarity. However, it does not involve deep architectural changes, advanced optimizations, or highly complex logic, which would push it into the \"hard\" category. The problem is self-contained and does not appear to have significant downstream impacts on other components.\n\nGiven these considerations, a difficulty score of 0.50 reflects a medium-level challenge that requires understanding multiple concepts and implementing a moderately complex feature across a few files, with basic error handling. It is suitable for a developer with intermediate experience in Python and some exposure to ranking metrics or framework development.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Suppress logs for inputs and outputs on stdout\n - [x] I have read and agree to the [contributing guidelines](https://github.com/griptape-ai/griptape#contributing).\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nI dont want always see the inputs and outputs of all tasks that are running. If I am doing webscraping, for example, the entire webscraped output being printed multiple times on stdout is not useful.\r\n\r\n**Describe the solution you'd like**\r\ntoggle the input and output logging for tasks. we currently allow overriding of a log level or the entire logger, but its not entirely clear how I would leverage that for this case. \r\n\r\nprobably not implemented this way but this is what i am currently doing:\r\n```\r\nclass BaseTask(ABC):\r\n    .....\r\n    log_input: bool = field(default=True, kw_only=True)\r\n    log_output: bool = field(default=True, kw_only=True)\r\n    \r\n```\r\nand in the various `Task`s that\u00a0 the inputs/outputs are logged:\r\n```\r\n    def before_run(self) -> None:\r\n        super().before_run()\r\n\r\n        self.structure.logger.info(\r\n            f\"{self.__class__.__name__} {self.id}\\nInput: {self.input.to_text() if self.log_input else self.input.name}\"\r\n        )\r\n\r\n    def after_run(self) -> None:\r\n        super().after_run()\r\n\r\n        self.structure.logger.info(\r\n            f\"{self.__class__.__name__} {self.id}\\nOutput: {self.output.to_text() if self.log_output else self.output.name}\"\r\n        )\r\n```\r\n\r\n\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 8b69e7f32..ca7c663ec 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -25,7 +25,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n - **BREAKING**: Removed `Structure.embedding_driver`, set this via `griptape.config.config.drivers.embedding` instead.\n - **BREAKING**: Removed `Structure.custom_logger` and `Structure.logger_level`, set these via `griptape.config.config.logger` instead. \n - **BREAKING**: Removed `BaseStructureConfig.merge_config`.\n-- **BREAKING**: Renamed `StructureConfig` to `DriverConfig`, and renamed fields accordingly.\n+- **BREAKING**: Renamed `StructureConfig` to `DriverConfig`, moved to `griptape.config.drivers` and renamed fields accordingly.\n - **BREAKING**: `RagContext.output` was changed to `RagContext.outputs` to support multiple outputs. All relevant RAG modules were adjusted accordingly.\n - **BREAKING**: Removed before and after response modules from `ResponseRagStage`.\n - **BREAKING**: Moved ruleset and metadata ingestion from standalone modules to `PromptResponseRagModule`.\ndiff --git a/docs/examples/src/multiple_agent_shared_memory_1.py b/docs/examples/src/multiple_agent_shared_memory_1.py\nindex e156e531a..11590df39 100644\n--- a/docs/examples/src/multiple_agent_shared_memory_1.py\n+++ b/docs/examples/src/multiple_agent_shared_memory_1.py\n@@ -1,6 +1,7 @@\n import os\n \n-from griptape.config import AzureOpenAiDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import AzureOpenAiDriverConfig\n from griptape.drivers import AzureMongoDbVectorStoreDriver, AzureOpenAiEmbeddingDriver\n from griptape.structures import Agent\n from griptape.tools import TaskMemoryTool, WebScraperTool\ndiff --git a/docs/examples/src/talk_to_a_video_1.py b/docs/examples/src/talk_to_a_video_1.py\nindex 3538c9071..377e177a6 100644\n--- a/docs/examples/src/talk_to_a_video_1.py\n+++ b/docs/examples/src/talk_to_a_video_1.py\n@@ -3,7 +3,8 @@\n import google.generativeai as genai\n \n from griptape.artifacts import GenericArtifact, TextArtifact\n-from griptape.config import GoogleDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import GoogleDriverConfig\n from griptape.structures import Agent\n \n config.drivers = GoogleDriverConfig()\ndiff --git a/docs/griptape-framework/drivers/src/embedding_drivers_10.py b/docs/griptape-framework/drivers/src/embedding_drivers_10.py\nindex 4f7560c99..a27e60298 100644\n--- a/docs/griptape-framework/drivers/src/embedding_drivers_10.py\n+++ b/docs/griptape-framework/drivers/src/embedding_drivers_10.py\n@@ -1,4 +1,5 @@\n-from griptape.config import DriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     OpenAiChatPromptDriver,\n     VoyageAiEmbeddingDriver,\ndiff --git a/docs/griptape-framework/drivers/src/event_listener_drivers_4.py b/docs/griptape-framework/drivers/src/event_listener_drivers_4.py\nindex a70e05d79..7a0957e63 100644\n--- a/docs/griptape-framework/drivers/src/event_listener_drivers_4.py\n+++ b/docs/griptape-framework/drivers/src/event_listener_drivers_4.py\n@@ -1,6 +1,7 @@\n import os\n \n-from griptape.config import DriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import AwsIotCoreEventListenerDriver, OpenAiChatPromptDriver\n from griptape.events import EventListener, FinishStructureRunEvent, event_bus\n from griptape.rules import Rule\ndiff --git a/docs/griptape-framework/structures/config.md b/docs/griptape-framework/structures/config.md\nindex 89399f60c..67721ebb1 100644\n--- a/docs/griptape-framework/structures/config.md\n+++ b/docs/griptape-framework/structures/config.md\n@@ -5,15 +5,17 @@ search:\n \n ## Overview\n \n-The [DriverConfig](../../reference/griptape/config/driver_config.md) class allows for the customization of Structures within Griptape, enabling specific settings such as Drivers to be defined for Tasks. \n+Griptape exposes global configuration options to easily customize different parts of the framework.\n \n-### Premade Configs\n+### Driver Configs\n \n-Griptape provides predefined [DriverConfig](../../reference/griptape/config/driver_config.md)'s for widely used services that provide APIs for most Driver types Griptape offers.\n+The [DriverConfig](../../reference/griptape/config/drivers/driver_config.md) class allows for the customization of Structures within Griptape, enabling specific settings such as Drivers to be defined for Tasks. \n+\n+Griptape provides predefined [DriverConfig](../../reference/griptape/config/drivers/driver_config.md)'s for widely used services that provide APIs for most Driver types Griptape offers.\n \n #### OpenAI\n \n-The [OpenAI Driver config](../../reference/griptape/config/openai_driver_config.md) provides default Drivers for OpenAI's APIs. This is the default config for all Structures.\n+The [OpenAI Driver config](../../reference/griptape/config/drivers/openai_driver_config.md) provides default Drivers for OpenAI's APIs. This is the default config for all Structures.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_1.py\"\n@@ -21,21 +23,21 @@ The [OpenAI Driver config](../../reference/griptape/config/openai_driver_config.\n \n #### Azure OpenAI\n \n-The [Azure OpenAI Driver config](../../reference/griptape/config/azure_openai_driver_config.md) provides default Drivers for Azure's OpenAI APIs.\n+The [Azure OpenAI Driver config](../../reference/griptape/config/drivers/azure_openai_driver_config.md) provides default Drivers for Azure's OpenAI APIs.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_2.py\"\n ```\n \n #### Amazon Bedrock\n-The [Amazon Bedrock Driver config](../../reference/griptape/config/amazon_bedrock_driver_config.md) provides default Drivers for Amazon Bedrock's APIs.\n+The [Amazon Bedrock Driver config](../../reference/griptape/config/drivers/amazon_bedrock_driver_config.md) provides default Drivers for Amazon Bedrock's APIs.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_3.py\"\n ```\n \n #### Google\n-The [Google Driver config](../../reference/griptape/config/google_driver_config.md) provides default Drivers for Google's Gemini APIs.\n+The [Google Driver config](../../reference/griptape/config/drivers/google_driver_config.md) provides default Drivers for Google's Gemini APIs.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_4.py\"\n@@ -43,7 +45,7 @@ The [Google Driver config](../../reference/griptape/config/google_driver_config.\n \n #### Anthropic\n \n-The [Anthropic Driver config](../../reference/griptape/config/anthropic_driver_config.md) provides default Drivers for Anthropic's APIs.\n+The [Anthropic Driver config](../../reference/griptape/config/drivers/anthropic_driver_config.md) provides default Drivers for Anthropic's APIs.\n \n !!! info\n     Anthropic does not provide an embeddings API which means you will need to use another service for embeddings.\n@@ -56,22 +58,30 @@ The [Anthropic Driver config](../../reference/griptape/config/anthropic_driver_c\n \n #### Cohere\n \n-The [Cohere Driver config](../../reference/griptape/config/cohere_driver_config.md) provides default Drivers for Cohere's APIs.\n+The [Cohere Driver config](../../reference/griptape/config/drivers/cohere_driver_config.md) provides default Drivers for Cohere's APIs.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_6.py\"\n ```\n \n-### Custom Configs\n+#### Custom\n \n-You can create your own [DriverConfig](../../reference/griptape/config/driver_config.md) by overriding relevant Drivers.\n-The [DriverConfig](../../reference/griptape/config/driver_config.md) class includes \"Dummy\" Drivers for all types, which throw a [DummyError](../../reference/griptape/exceptions/dummy_exception.md) if invoked without being overridden. \n+You can create your own [DriverConfig](../../reference/griptape/config/drivers/driver_config.md) by overriding relevant Drivers.\n+The [DriverConfig](../../reference/griptape/config/drivers/driver_config.md) class includes \"Dummy\" Drivers for all types, which throw a [DummyError](../../reference/griptape/exceptions/dummy_exception.md) if invoked without being overridden. \n This approach ensures that you are informed through clear error messages if you attempt to use Structures without proper Driver configurations.\n \n ```python\n --8<-- \"docs/griptape-framework/structures/src/config_7.py\"\n ```\n \n+### Logging Config\n+\n+Griptape provides a predefined [LoggingConfig](../../reference/griptape/config/logging/logging_config.md)'s for easily customizing the logging events that the framework emits. In order to customize the logger, the logger can be fetched by using the `config.logging.logger_name`.\n+\n+```python\n+--8<-- \"docs/griptape-framework/structures/src/config_logging.py\"\n+```\n+\n ### Loading/Saving Configs\n \n ```python\ndiff --git a/docs/griptape-framework/structures/src/config_1.py b/docs/griptape-framework/structures/src/config_1.py\nindex 0c7a5ed9e..df75488dc 100644\n--- a/docs/griptape-framework/structures/src/config_1.py\n+++ b/docs/griptape-framework/structures/src/config_1.py\n@@ -1,4 +1,5 @@\n-from griptape.config import OpenAiDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import OpenAiDriverConfig\n from griptape.structures import Agent\n \n config.drivers = OpenAiDriverConfig()\ndiff --git a/docs/griptape-framework/structures/src/config_2.py b/docs/griptape-framework/structures/src/config_2.py\nindex a5f8efbbe..6fcdedbc8 100644\n--- a/docs/griptape-framework/structures/src/config_2.py\n+++ b/docs/griptape-framework/structures/src/config_2.py\n@@ -1,6 +1,7 @@\n import os\n \n-from griptape.config import AzureOpenAiDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import AzureOpenAiDriverConfig\n from griptape.structures import Agent\n \n config.drivers = AzureOpenAiDriverConfig(\ndiff --git a/docs/griptape-framework/structures/src/config_3.py b/docs/griptape-framework/structures/src/config_3.py\nindex 6b3f51a76..e4e33e379 100644\n--- a/docs/griptape-framework/structures/src/config_3.py\n+++ b/docs/griptape-framework/structures/src/config_3.py\n@@ -2,7 +2,8 @@\n \n import boto3\n \n-from griptape.config import AmazonBedrockDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import AmazonBedrockDriverConfig\n from griptape.structures import Agent\n \n config.drivers = AmazonBedrockDriverConfig(\ndiff --git a/docs/griptape-framework/structures/src/config_4.py b/docs/griptape-framework/structures/src/config_4.py\nindex 5362b8c6b..7ab5eee70 100644\n--- a/docs/griptape-framework/structures/src/config_4.py\n+++ b/docs/griptape-framework/structures/src/config_4.py\n@@ -1,4 +1,5 @@\n-from griptape.config import GoogleDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import GoogleDriverConfig\n from griptape.structures import Agent\n \n config.drivers = GoogleDriverConfig()\ndiff --git a/docs/griptape-framework/structures/src/config_5.py b/docs/griptape-framework/structures/src/config_5.py\nindex 4f787a922..bee5050c2 100644\n--- a/docs/griptape-framework/structures/src/config_5.py\n+++ b/docs/griptape-framework/structures/src/config_5.py\n@@ -1,4 +1,5 @@\n-from griptape.config import AnthropicDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import AnthropicDriverConfig\n from griptape.structures import Agent\n \n config.drivers = AnthropicDriverConfig()\ndiff --git a/docs/griptape-framework/structures/src/config_6.py b/docs/griptape-framework/structures/src/config_6.py\nindex c26502401..569000180 100644\n--- a/docs/griptape-framework/structures/src/config_6.py\n+++ b/docs/griptape-framework/structures/src/config_6.py\n@@ -1,6 +1,7 @@\n import os\n \n-from griptape.config import CohereDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import CohereDriverConfig\n from griptape.structures import Agent\n \n config.drivers = CohereDriverConfig(api_key=os.environ[\"COHERE_API_KEY\"])\ndiff --git a/docs/griptape-framework/structures/src/config_7.py b/docs/griptape-framework/structures/src/config_7.py\nindex 6b285f0e5..9f464b167 100644\n--- a/docs/griptape-framework/structures/src/config_7.py\n+++ b/docs/griptape-framework/structures/src/config_7.py\n@@ -1,6 +1,7 @@\n import os\n \n-from griptape.config import DriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import AnthropicPromptDriver\n from griptape.structures import Agent\n \ndiff --git a/docs/griptape-framework/structures/src/config_8.py b/docs/griptape-framework/structures/src/config_8.py\nindex 4f23e3eaa..6bc87998c 100644\n--- a/docs/griptape-framework/structures/src/config_8.py\n+++ b/docs/griptape-framework/structures/src/config_8.py\n@@ -1,4 +1,5 @@\n-from griptape.config import AmazonBedrockDriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import AmazonBedrockDriverConfig\n from griptape.structures import Agent\n \n custom_config = AmazonBedrockDriverConfig()\ndiff --git a/docs/griptape-framework/structures/src/config_logging.py b/docs/griptape-framework/structures/src/config_logging.py\nnew file mode 100644\nindex 000000000..81645d5e2\n--- /dev/null\n+++ b/docs/griptape-framework/structures/src/config_logging.py\n@@ -0,0 +1,14 @@\n+import logging\n+\n+from griptape.config import config\n+from griptape.config.drivers import OpenAiDriverConfig\n+from griptape.config.logging import TruncateLoggingFilter\n+from griptape.structures import Agent\n+\n+config.drivers = OpenAiDriverConfig()\n+\n+logger = logging.getLogger(config.logging.logger_name)\n+logger.setLevel(logging.ERROR)\n+logger.addFilter(TruncateLoggingFilter(max_log_length=100))\n+\n+agent = Agent()\ndiff --git a/docs/griptape-framework/structures/src/task_memory_6.py b/docs/griptape-framework/structures/src/task_memory_6.py\nindex 3f4d14b0a..8d39f0286 100644\n--- a/docs/griptape-framework/structures/src/task_memory_6.py\n+++ b/docs/griptape-framework/structures/src/task_memory_6.py\n@@ -1,8 +1,6 @@\n from griptape.artifacts import TextArtifact\n-from griptape.config import (\n-    OpenAiDriverConfig,\n-    config,\n-)\n+from griptape.config import config\n+from griptape.config.drivers import OpenAiDriverConfig\n from griptape.drivers import (\n     LocalVectorStoreDriver,\n     OpenAiChatPromptDriver,\ndiff --git a/docs/griptape-tools/official-tools/src/rest_api_tool_1.py b/docs/griptape-tools/official-tools/src/rest_api_tool_1.py\nindex 2093163b7..4ef73dd9d 100644\n--- a/docs/griptape-tools/official-tools/src/rest_api_tool_1.py\n+++ b/docs/griptape-tools/official-tools/src/rest_api_tool_1.py\n@@ -1,6 +1,7 @@\n from json import dumps\n \n-from griptape.config import DriverConfig, config\n+from griptape.config import config\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import OpenAiChatPromptDriver\n from griptape.memory.structure import ConversationMemory\n from griptape.structures import Pipeline\ndiff --git a/griptape/config/__init__.py b/griptape/config/__init__.py\nindex 4a87c2d01..043d152ba 100644\n--- a/griptape/config/__init__.py\n+++ b/griptape/config/__init__.py\n@@ -1,28 +1,8 @@\n from .base_config import BaseConfig\n-\n-from .base_driver_config import BaseDriverConfig\n-\n-from .driver_config import DriverConfig\n-from .openai_driver_config import OpenAiDriverConfig\n-from .azure_openai_driver_config import AzureOpenAiDriverConfig\n-from .amazon_bedrock_driver_config import AmazonBedrockDriverConfig\n-from .anthropic_driver_config import AnthropicDriverConfig\n-from .google_driver_config import GoogleDriverConfig\n-from .cohere_driver_config import CohereDriverConfig\n-from .logging_config import LoggingConfig\n from .config import config\n \n \n __all__ = [\n     \"BaseConfig\",\n-    \"BaseDriverConfig\",\n-    \"DriverConfig\",\n-    \"OpenAiDriverConfig\",\n-    \"AzureOpenAiDriverConfig\",\n-    \"AmazonBedrockDriverConfig\",\n-    \"AnthropicDriverConfig\",\n-    \"GoogleDriverConfig\",\n-    \"CohereDriverConfig\",\n-    \"LoggingConfig\",\n     \"config\",\n ]\ndiff --git a/griptape/config/base_config.py b/griptape/config/base_config.py\nindex ef62a4e9b..7ed00e445 100644\n--- a/griptape/config/base_config.py\n+++ b/griptape/config/base_config.py\n@@ -8,8 +8,8 @@\n from griptape.mixins.serializable_mixin import SerializableMixin\n \n if TYPE_CHECKING:\n-    from .base_driver_config import BaseDriverConfig\n-    from .logging_config import LoggingConfig\n+    from .drivers.base_driver_config import BaseDriverConfig\n+    from .logging.logging_config import LoggingConfig\n \n \n @define(kw_only=True)\ndiff --git a/griptape/config/config.py b/griptape/config/config.py\nindex 7b70df409..11c2f9585 100644\n--- a/griptape/config/config.py\n+++ b/griptape/config/config.py\n@@ -5,11 +5,11 @@\n from attrs import define, field\n \n from .base_config import BaseConfig\n-from .logging_config import LoggingConfig\n-from .openai_driver_config import OpenAiDriverConfig\n+from .drivers.openai_driver_config import OpenAiDriverConfig\n+from .logging.logging_config import LoggingConfig\n \n if TYPE_CHECKING:\n-    from .base_driver_config import BaseDriverConfig\n+    from .drivers.base_driver_config import BaseDriverConfig\n \n \n @define(kw_only=True)\ndiff --git a/griptape/config/drivers/__init__.py b/griptape/config/drivers/__init__.py\nnew file mode 100644\nindex 000000000..9d5f2f510\n--- /dev/null\n+++ b/griptape/config/drivers/__init__.py\n@@ -0,0 +1,20 @@\n+from .base_driver_config import BaseDriverConfig\n+from .driver_config import DriverConfig\n+\n+from .openai_driver_config import OpenAiDriverConfig\n+from .azure_openai_driver_config import AzureOpenAiDriverConfig\n+from .amazon_bedrock_driver_config import AmazonBedrockDriverConfig\n+from .anthropic_driver_config import AnthropicDriverConfig\n+from .google_driver_config import GoogleDriverConfig\n+from .cohere_driver_config import CohereDriverConfig\n+\n+__all__ = [\n+    \"BaseDriverConfig\",\n+    \"DriverConfig\",\n+    \"OpenAiDriverConfig\",\n+    \"AzureOpenAiDriverConfig\",\n+    \"AmazonBedrockDriverConfig\",\n+    \"AnthropicDriverConfig\",\n+    \"GoogleDriverConfig\",\n+    \"CohereDriverConfig\",\n+]\ndiff --git a/griptape/config/amazon_bedrock_driver_config.py b/griptape/config/drivers/amazon_bedrock_driver_config.py\nsimilarity index 98%\nrename from griptape/config/amazon_bedrock_driver_config.py\nrename to griptape/config/drivers/amazon_bedrock_driver_config.py\nindex a07300638..ea540b391 100644\n--- a/griptape/config/amazon_bedrock_driver_config.py\n+++ b/griptape/config/drivers/amazon_bedrock_driver_config.py\n@@ -4,7 +4,7 @@\n \n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     AmazonBedrockImageGenerationDriver,\n     AmazonBedrockImageQueryDriver,\ndiff --git a/griptape/config/anthropic_driver_config.py b/griptape/config/drivers/anthropic_driver_config.py\nsimilarity index 96%\nrename from griptape/config/anthropic_driver_config.py\nrename to griptape/config/drivers/anthropic_driver_config.py\nindex 642a3fced..0c4524159 100644\n--- a/griptape/config/anthropic_driver_config.py\n+++ b/griptape/config/drivers/anthropic_driver_config.py\n@@ -1,6 +1,6 @@\n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     AnthropicImageQueryDriver,\n     AnthropicPromptDriver,\ndiff --git a/griptape/config/azure_openai_driver_config.py b/griptape/config/drivers/azure_openai_driver_config.py\nsimilarity index 98%\nrename from griptape/config/azure_openai_driver_config.py\nrename to griptape/config/drivers/azure_openai_driver_config.py\nindex c987a31b5..bcb173fbd 100644\n--- a/griptape/config/azure_openai_driver_config.py\n+++ b/griptape/config/drivers/azure_openai_driver_config.py\n@@ -4,7 +4,7 @@\n \n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     AzureOpenAiChatPromptDriver,\n     AzureOpenAiEmbeddingDriver,\ndiff --git a/griptape/config/base_driver_config.py b/griptape/config/drivers/base_driver_config.py\nsimilarity index 100%\nrename from griptape/config/base_driver_config.py\nrename to griptape/config/drivers/base_driver_config.py\ndiff --git a/griptape/config/cohere_driver_config.py b/griptape/config/drivers/cohere_driver_config.py\nsimilarity index 95%\nrename from griptape/config/cohere_driver_config.py\nrename to griptape/config/drivers/cohere_driver_config.py\nindex 7195f550f..eb8a55ce4 100644\n--- a/griptape/config/cohere_driver_config.py\n+++ b/griptape/config/drivers/cohere_driver_config.py\n@@ -1,6 +1,6 @@\n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     BaseEmbeddingDriver,\n     BasePromptDriver,\ndiff --git a/griptape/config/driver_config.py b/griptape/config/drivers/driver_config.py\nsimilarity index 97%\nrename from griptape/config/driver_config.py\nrename to griptape/config/drivers/driver_config.py\nindex 325591258..d342ecb0a 100644\n--- a/griptape/config/driver_config.py\n+++ b/griptape/config/drivers/driver_config.py\n@@ -4,7 +4,7 @@\n \n from attrs import Factory, define, field\n \n-from griptape.config import BaseDriverConfig\n+from griptape.config.drivers import BaseDriverConfig\n from griptape.drivers import (\n     DummyAudioTranscriptionDriver,\n     DummyEmbeddingDriver,\ndiff --git a/griptape/config/google_driver_config.py b/griptape/config/drivers/google_driver_config.py\nsimilarity index 94%\nrename from griptape/config/google_driver_config.py\nrename to griptape/config/drivers/google_driver_config.py\nindex a1089f0ee..6f4243f01 100644\n--- a/griptape/config/google_driver_config.py\n+++ b/griptape/config/drivers/google_driver_config.py\n@@ -1,6 +1,6 @@\n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     BaseEmbeddingDriver,\n     BasePromptDriver,\ndiff --git a/griptape/config/openai_driver_config.py b/griptape/config/drivers/openai_driver_config.py\nsimilarity index 97%\nrename from griptape/config/openai_driver_config.py\nrename to griptape/config/drivers/openai_driver_config.py\nindex 35ccde43d..0b05d1636 100644\n--- a/griptape/config/openai_driver_config.py\n+++ b/griptape/config/drivers/openai_driver_config.py\n@@ -1,6 +1,6 @@\n from attrs import Factory, define, field\n \n-from griptape.config import DriverConfig\n+from griptape.config.drivers import DriverConfig\n from griptape.drivers import (\n     BaseAudioTranscriptionDriver,\n     BaseEmbeddingDriver,\ndiff --git a/griptape/config/logging/__init__.py b/griptape/config/logging/__init__.py\nnew file mode 100644\nindex 000000000..de7726060\n--- /dev/null\n+++ b/griptape/config/logging/__init__.py\n@@ -0,0 +1,5 @@\n+from .logging_config import LoggingConfig\n+from .truncate_logging_filter import TruncateLoggingFilter\n+from .newline_logging_filter import NewlineLoggingFilter\n+\n+__all__ = [\"LoggingConfig\", \"TruncateLoggingFilter\", \"NewlineLoggingFilter\"]\ndiff --git a/griptape/config/logging_config.py b/griptape/config/logging/logging_config.py\nsimilarity index 53%\nrename from griptape/config/logging_config.py\nrename to griptape/config/logging/logging_config.py\nindex 0c0fcc020..80497d7c8 100644\n--- a/griptape/config/logging_config.py\n+++ b/griptape/config/logging/logging_config.py\n@@ -9,16 +9,9 @@\n @define\n class LoggingConfig:\n     logger_name: str = field(default=\"griptape\", kw_only=True)\n-    logger_level: int = field(\n-        default=logging.INFO,\n-        kw_only=True,\n-        on_setattr=lambda self, _, value: logging.getLogger(self.logger_name).setLevel(value),\n-    )\n \n     def __attrs_post_init__(self) -> None:\n         logger = logging.getLogger(self.logger_name)\n-\n+        logger.setLevel(logging.INFO)\n         logger.propagate = False\n-        logger.setLevel(self.logger_level)\n-\n-        logger.handlers = [RichHandler(show_time=True, show_path=False)]\n+        logger.addHandler(RichHandler(show_time=True, show_path=False))\ndiff --git a/griptape/config/logging/newline_logging_filter.py b/griptape/config/logging/newline_logging_filter.py\nnew file mode 100644\nindex 000000000..bae08265f\n--- /dev/null\n+++ b/griptape/config/logging/newline_logging_filter.py\n@@ -0,0 +1,13 @@\n+import logging\n+from typing import Any\n+\n+from attrs import define, field\n+\n+\n+@define\n+class NewlineLoggingFilter(logging.Filter):\n+    replace_str: str = field(default=\" \", kw_only=True)\n+\n+    def filter(self, record: Any) -> bool:\n+        record.msg = record.msg.replace(\"\\n\", self.replace_str)\n+        return True\ndiff --git a/griptape/config/logging/truncate_logging_filter.py b/griptape/config/logging/truncate_logging_filter.py\nnew file mode 100644\nindex 000000000..9888fc169\n--- /dev/null\n+++ b/griptape/config/logging/truncate_logging_filter.py\n@@ -0,0 +1,17 @@\n+import logging\n+from typing import Any\n+\n+from attrs import define, field\n+\n+\n+@define\n+class TruncateLoggingFilter(logging.Filter):\n+    max_log_length: int = field(default=1000, kw_only=True)\n+\n+    def filter(self, record: Any) -> bool:\n+        message = record.getMessage()\n+\n+        if len(message) > self.max_log_length:\n+            record.msg = f\"{message[:self.max_log_length]}... [{len(message) - self.max_log_length} more characters]\"\n+            record.args = ()\n+        return True\n", "instance_id": "griptape-ai__griptape-1059", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in expressing the goal: to suppress or toggle logging of inputs and outputs for tasks on stdout, particularly in scenarios like web scraping where large outputs clutter the console. The user provides a specific use case and even suggests a potential solution by modifying the `BaseTask` class to include `log_input` and `log_output` fields, along with conditional logging in `before_run` and `after_run` methods. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the toggle should be global, per-task, or configurable at runtime, nor does it address potential edge cases like logging behavior in nested tasks or error conditions. Additionally, while the intent is clear, the statement lacks formal input/output formats or constraints (e.g., how the toggle should be exposed to users\u2014via configuration, API, or environment variables). Thus, it falls short of being comprehensive but is still mostly clear with actionable intent.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the relatively straightforward nature of the required changes. Based on the problem statement and the suggested solution, implementing a toggle for input/output logging involves modifying a single class (`BaseTask`) or a small set of related classes to add boolean flags and conditional logic in logging methods. This requires basic understanding of the codebase's logging mechanism and task execution flow, likely confined to a few files. The provided code changes in the diff, however, appear unrelated to the problem statement, as they focus on restructuring configuration modules and updating documentation, which do not directly address the logging toggle feature. Assuming the actual implementation follows the user's suggestion, the scope of code changes is minimal, with no significant impact on the system's architecture or interactions between modules. The technical concepts involved are basic\u2014class inheritance, conditional statements, and logging configuration\u2014requiring no advanced algorithms or domain-specific knowledge. Edge cases, such as handling large inputs/outputs or nested tasks, are not explicitly mentioned and would likely be simple to address if needed. Overall, this is an easy task for a developer familiar with the codebase, requiring only simple modifications and minimal risk of introducing complex bugs.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "JSON Parse error when using dash_ag_grid and dash pages\n```\r\ndash                           2.16.1\r\ndash_ag_grid                   31.0.1\r\ndash-bootstrap-components      1.5.0\r\ndash-core-components           2.0.0\r\ndash-html-components           2.0.0\r\ndash-mantine-components        0.12.1\r\ndash-table                     5.0.0\r\ndash-testing-stub              0.0.2\r\n\r\n```\r\n\r\n**Description**\r\n\r\nSet up two dash pages. One should contain `dash_ag_grid`.\r\nRun the app, open the page with `dash_ag_grid` and quickly (in 1 second) switch to different page.\r\nNext error occurs in browser console:\r\n```\r\nSyntaxError: JSON Parse error: Unexpected identifier \"undefined\"\r\n```\r\n\r\n**Expected behavior**\r\n\r\nNo console error occurred\r\n\r\n**Screenshots**\r\n\r\n<img width=\"492\" alt=\"image\" src=\"https://github.com/plotly/dash/assets/35569332/926522cc-ce5e-4f0c-a82e-fa13bb9e31ce\">\r\n\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex aa819ac..00e79d4 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -6,7 +6,8 @@ Links \"DE#nnn\" prior to version 2.0 point to the Dash Enterprise closed-source D\n \n ## unreleased\n ### Fixed\n- - [#325](https://github.com/plotly/dash-ag-grid/pull/325).  Fixes issue #324 where `pivotComparator` functions were not sorting columns correctly because they were only being passed `params`.\n+ - [#300](https://github.com/plotly/dash-ag-grid/pull/300) Fixes issue [#299](https://github.com/plotly/dash-ag-grid/pull/299) where grid was unmounted and trying to update the `columnState`.\n+ - [#325](https://github.com/plotly/dash-ag-grid/pull/325)  Fixes issue [#324](https://github.com/plotly/dash-ag-grid/pull/324) where `pivotComparator` functions were not sorting columns correctly because they were only being passed `params`.\n  - [#314](https://github.com/plotly/dash-ag-grid/pull/314)\n    - locking selenium for tests that were failing due to missing import\n  - [#313](https://github.com/plotly/dash-ag-grid/pull/313)\ndiff --git a/src/lib/fragments/AgGrid.react.js b/src/lib/fragments/AgGrid.react.js\nindex 52a2b92..57353de 100644\n--- a/src/lib/fragments/AgGrid.react.js\n+++ b/src/lib/fragments/AgGrid.react.js\n@@ -554,9 +554,11 @@ export default class DashAgGrid extends Component {\n         if (rowModelType === 'clientSide') {\n             propsToSet.virtualRowData = this.virtualRowData();\n         }\n-        propsToSet.columnState = JSON.parse(\n-            JSON.stringify(this.state.gridApi.getColumnState())\n-        );\n+        if (!this.state.gridApi.isDestroyed()) {\n+            propsToSet.columnState = JSON.parse(\n+                JSON.stringify(this.state.gridApi.getColumnState())\n+            );\n+        }\n         setProps(propsToSet);\n     }\n \n@@ -570,6 +572,7 @@ export default class DashAgGrid extends Component {\n \n     componentWillUnmount() {\n         this.setState({mounted: false, gridApi: null});\n+        this.props.setProps = () => {};\n         if (this.props.id) {\n             delete agGridRefs[this.props.id];\n             eventBus.remove(this.props.id);\n@@ -1308,15 +1311,20 @@ export default class DashAgGrid extends Component {\n         if (!this.state.gridApi || !this.state.mounted) {\n             return;\n         }\n+        if (!this.state.gridApi.isDestroyed()) {\n+            var columnState = JSON.parse(\n+                JSON.stringify(this.state.gridApi.getColumnState())\n+            );\n \n-        var columnState = JSON.parse(\n-            JSON.stringify(this.state.gridApi.getColumnState())\n-        );\n-\n-        this.props.setProps({\n-            columnState,\n-            updateColumnState: false,\n-        });\n+            this.props.setProps({\n+                columnState,\n+                updateColumnState: false,\n+            });\n+        } else {\n+            this.props.setProps({\n+                updateColumnState: false,\n+            });\n+        }\n     }\n \n     buildArray(arr1, arr2) {\n", "instance_id": "plotly__dash-ag-grid-300", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a JSON parse error occurs in the browser console when quickly switching between Dash pages, one of which contains a `dash_ag_grid` component. The goal (no console error) and the steps to reproduce the issue are provided, along with a screenshot for context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the root cause of the error (e.g., whether it's related to component unmounting or state persistence), nor does it specify any constraints or edge cases beyond the rapid page switch. Additionally, there are no examples of expected input/output data or detailed logs of the error beyond the message. While the issue is reproducible based on the description, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of the code changes is relatively narrow, primarily affecting a single file (`AgGrid.react.js`) with modifications to a few specific functions. The changes involve adding conditional checks to prevent accessing or updating state after the grid API is destroyed, which indicates a bug fix related to component lifecycle management in React. This requires understanding React component lifecycle methods (e.g., `componentWillUnmount`), state management, and the specific behavior of the `dash_ag_grid` library's grid API. The technical concepts involved are moderately complex, as they include handling component destruction and ensuring state updates are safe, but they do not require advanced algorithms or system-level changes. The code changes are small in volume (a few lines added or modified) and do not impact the broader architecture of the system. Edge case handling is implicitly addressed by checking if the grid API is destroyed, but no additional complex error handling or performance considerations are evident. Overall, this problem requires a moderate level of expertise in React and the specific library, along with careful attention to lifecycle-related bugs, but it does not pose a significant challenge for an experienced developer familiar with frontend frameworks.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "STYLE: Replace `Fill(0)` with `{}` initializer for local variables in tests\nReplaced code of the form\r\n\r\n    T var;\r\n    var.Fill(0);\r\n\r\nwith `T var{};`\r\n\r\nUsing Notepad++, Replace in Files, doing:\r\n\r\n    Find what: ^( [ ]+[^ ].* )(\\w+);[\\r\\n]+ [ ]+\\2\\.Fill\\(0\\);\r\n    Replace with: $1$2{};\r\n    Filters: itk*Test*.cxx\r\n    [v] Match case\r\n    (*) Regular expression\r\n\r\nThe empty initializer list `{}` effectively zero-fills those variables.\r\n\r\n----\r\n\r\nWhen reviewing this pull request, it may be convenient to ignore whitespace changes: https://github.com/InsightSoftwareConsortium/ITK/pull/4881/files?w=1\n", "patch": "diff --git a/Modules/Core/Common/include/itkExtractImageFilter.hxx b/Modules/Core/Common/include/itkExtractImageFilter.hxx\nindex 2b03120c2e2..110c4fa0677 100644\n--- a/Modules/Core/Common/include/itkExtractImageFilter.hxx\n+++ b/Modules/Core/Common/include/itkExtractImageFilter.hxx\n@@ -62,11 +62,9 @@ ExtractImageFilter<TInputImage, TOutputImage>::SetExtractionRegion(InputImageReg\n                 \"InputImageDimension must be greater than OutputImageDimension\");\n   m_ExtractionRegion = extractRegion;\n \n-  InputImageSizeType  inputSize = extractRegion.GetSize();\n-  OutputImageSizeType outputSize;\n-  outputSize.Fill(0);\n-  OutputImageIndexType outputIndex;\n-  outputIndex.Fill(0);\n+  InputImageSizeType   inputSize = extractRegion.GetSize();\n+  OutputImageSizeType  outputSize{};\n+  OutputImageIndexType outputIndex{};\n \n   /**\n    * check to see if the number of non-zero entries in the extraction region\n@@ -131,8 +129,7 @@ ExtractImageFilter<TInputImage, TOutputImage>::GenerateOutputInformation()\n \n     typename OutputImageType::SpacingType   outputSpacing;\n     typename OutputImageType::DirectionType outputDirection;\n-    typename OutputImageType::PointType     outputOrigin;\n-    outputOrigin.Fill(0.0);\n+    typename OutputImageType::PointType     outputOrigin{};\n \n     if (static_cast<unsigned int>(OutputImageDimension) > static_cast<unsigned int>(InputImageDimension))\n     {\ndiff --git a/Modules/Core/Common/include/itkImageAlgorithm.hxx b/Modules/Core/Common/include/itkImageAlgorithm.hxx\nindex 8cead5f2ff6..1859906f069 100644\n--- a/Modules/Core/Common/include/itkImageAlgorithm.hxx\n+++ b/Modules/Core/Common/include/itkImageAlgorithm.hxx\n@@ -216,9 +216,8 @@ ImageAlgorithm::EnlargeRegionOverBox(const typename InputImageType::RegionType &\n \n   for (unsigned int count = 0; count < numberOfInputCorners; ++count)\n   {\n-    ContinuousInputIndexType currentInputCornerIndex;\n-    currentInputCornerIndex.Fill(0);\n-    unsigned int localCount = count;\n+    ContinuousInputIndexType currentInputCornerIndex{};\n+    unsigned int             localCount = count;\n \n     // For each dimension, set the current index to either\n     // the highest or lowest index along this dimension.\ndiff --git a/Modules/Core/Common/include/itkNeighborhoodIteratorTestCommon.hxx b/Modules/Core/Common/include/itkNeighborhoodIteratorTestCommon.hxx\nindex ec1ff550488..1ea2a2947a5 100644\n--- a/Modules/Core/Common/include/itkNeighborhoodIteratorTestCommon.hxx\n+++ b/Modules/Core/Common/include/itkNeighborhoodIteratorTestCommon.hxx\n@@ -68,9 +68,8 @@ FillImage(itk::Image<itk::Index<VDimension>, VDimension> * img)\n   using ImageType = itk::Image<IndexType, VDimension>;\n   const itk::Size<VDimension> size = img->GetRequestedRegion().GetSize();\n \n-  unsigned int i;\n-  IndexType    loop;\n-  loop.Fill(0);\n+  unsigned int                        i;\n+  IndexType                           loop{};\n   itk::ImageRegionIterator<ImageType> it(img, img->GetRequestedRegion());\n \n   while (!it.IsAtEnd())\ndiff --git a/Modules/Core/Common/include/itkTriangleCell.hxx b/Modules/Core/Common/include/itkTriangleCell.hxx\nindex 2f4a8fd0a4e..9df171acaf5 100644\n--- a/Modules/Core/Common/include/itkTriangleCell.hxx\n+++ b/Modules/Core/Common/include/itkTriangleCell.hxx\n@@ -350,8 +350,7 @@ TriangleCell<TCellInterface>::ComputeCircumCenter(PointsContainer * iPoints) ->\n \n   if (sum_weights != 0.)\n   {\n-    PointType oP;\n-    oP.Fill(0.);\n+    PointType oP{};\n \n     for (i = 0; i < 3; ++i)\n     {\ndiff --git a/Modules/Core/Common/include/itkTriangleHelper.hxx b/Modules/Core/Common/include/itkTriangleHelper.hxx\nindex 42bdd5de2ea..6a02d4f2627 100644\n--- a/Modules/Core/Common/include/itkTriangleHelper.hxx\n+++ b/Modules/Core/Common/include/itkTriangleHelper.hxx\n@@ -170,9 +170,7 @@ auto\n TriangleHelper<TPoint>::ComputeCircumCenter(const PointType & iP1, const PointType & iP2, const PointType & iP3)\n   -> PointType\n {\n-  PointType oPt;\n-\n-  oPt.Fill(0.0);\n+  PointType oPt{};\n \n   CoordRepType a = iP2.SquaredEuclideanDistanceTo(iP3);\n   CoordRepType b = iP1.SquaredEuclideanDistanceTo(iP3);\ndiff --git a/Modules/Core/Common/src/itkAnatomicalOrientation.cxx b/Modules/Core/Common/src/itkAnatomicalOrientation.cxx\nindex ab13b29c870..23c31135f95 100644\n--- a/Modules/Core/Common/src/itkAnatomicalOrientation.cxx\n+++ b/Modules/Core/Common/src/itkAnatomicalOrientation.cxx\n@@ -253,8 +253,7 @@ AnatomicalOrientation::ConvertPositiveEnumToDirection(PositiveEnum orientationEn\n   const AnatomicalOrientation o(orientationEnum);\n \n   CoordinateEnum terms[Dimension] = { o.GetPrimaryTerm(), o.GetSecondaryTerm(), o.GetTertiaryTerm() };\n-  DirectionType  direction;\n-  direction.Fill(0.0);\n+  DirectionType  direction{};\n \n   for (unsigned int i = 0; i < Dimension; ++i)\n   {\ndiff --git a/Modules/Core/Common/src/itkSpatialOrientationAdapter.cxx b/Modules/Core/Common/src/itkSpatialOrientationAdapter.cxx\nindex e7e19898d68..dd5466e761d 100644\n--- a/Modules/Core/Common/src/itkSpatialOrientationAdapter.cxx\n+++ b/Modules/Core/Common/src/itkSpatialOrientationAdapter.cxx\n@@ -132,8 +132,7 @@ SpatialOrientationAdapter::ToDirectionCosines(const OrientationType & Or)\n     (static_cast<uint32_t>(Or) >>\n      static_cast<uint32_t>(SpatialOrientationEnums::CoordinateMajornessTerms::ITK_COORDINATE_TertiaryMinor)) &\n     0xff);\n-  DirectionType direction;\n-  direction.Fill(0.0);\n+  DirectionType direction{};\n   for (unsigned int i = 0; i < DirectionType::ColumnDimensions; ++i)\n   {\n     switch (terms[i])\ndiff --git a/Modules/Core/FiniteDifference/include/itkFiniteDifferenceFunction.hxx b/Modules/Core/FiniteDifference/include/itkFiniteDifferenceFunction.hxx\nindex d60c7a01f81..835a9535a36 100644\n--- a/Modules/Core/FiniteDifference/include/itkFiniteDifferenceFunction.hxx\n+++ b/Modules/Core/FiniteDifference/include/itkFiniteDifferenceFunction.hxx\n@@ -79,9 +79,7 @@ template <typename TImageType>\n auto\n FiniteDifferenceFunction<TImageType>::ComputeNeighborhoodScales() const -> const NeighborhoodScalesType\n {\n-  NeighborhoodScalesType neighborhoodScales;\n-\n-  neighborhoodScales.Fill(0.0);\n+  NeighborhoodScalesType neighborhoodScales{};\n   for (unsigned int i = 0; i < ImageDimension; ++i)\n   {\n     if (this->m_Radius[i] > 0)\ndiff --git a/Modules/Core/ImageFunction/include/itkGaussianBlurImageFunction.hxx b/Modules/Core/ImageFunction/include/itkGaussianBlurImageFunction.hxx\nindex 47be2796168..f7ff001d6af 100644\n--- a/Modules/Core/ImageFunction/include/itkGaussianBlurImageFunction.hxx\n+++ b/Modules/Core/ImageFunction/include/itkGaussianBlurImageFunction.hxx\n@@ -351,8 +351,7 @@ GaussianBlurImageFunction<TInputImage, TOutput>::RecomputeContinuousGaussianKern\n {\n   for (unsigned int direction = 0; direction < Self::ImageDimension; ++direction)\n   {\n-    typename NeighborhoodType::SizeType size;\n-    size.Fill(0);\n+    typename NeighborhoodType::SizeType size{};\n     size[direction] = static_cast<SizeValueType>(m_Sigma[direction] * m_Extent[direction]);\n \n     NeighborhoodType gaussianNeighborhood;\ndiff --git a/Modules/Core/ImageFunction/include/itkGaussianDerivativeImageFunction.hxx b/Modules/Core/ImageFunction/include/itkGaussianDerivativeImageFunction.hxx\nindex f6880953a0f..3747bd9aaf1 100644\n--- a/Modules/Core/ImageFunction/include/itkGaussianDerivativeImageFunction.hxx\n+++ b/Modules/Core/ImageFunction/include/itkGaussianDerivativeImageFunction.hxx\n@@ -159,8 +159,7 @@ GaussianDerivativeImageFunction<TInputImage, TOutput>::RecomputeGaussianKernel()\n       // Set the derivative of the Gaussian first\n       OperatorNeighborhoodType                                  dogNeighborhood;\n       typename GaussianDerivativeSpatialFunctionType::InputType pt;\n-      typename NeighborhoodType::SizeType                       size;\n-      size.Fill(0);\n+      typename NeighborhoodType::SizeType                       size{};\n       size[direction] = static_cast<SizeValueType>(m_Sigma[direction] * m_Extent[direction]);\n       dogNeighborhood.SetRadius(size);\n       m_ImageNeighborhoodOffsets[direction] = GenerateRectangularImageNeighborhoodOffsets(size);\ndiff --git a/Modules/Core/ImageFunction/include/itkVectorLinearInterpolateImageFunction.hxx b/Modules/Core/ImageFunction/include/itkVectorLinearInterpolateImageFunction.hxx\nindex 2902c66b4a3..17dd41ae82c 100644\n--- a/Modules/Core/ImageFunction/include/itkVectorLinearInterpolateImageFunction.hxx\n+++ b/Modules/Core/ImageFunction/include/itkVectorLinearInterpolateImageFunction.hxx\n@@ -53,8 +53,7 @@ VectorLinearInterpolateImageFunction<TInputImage, TCoordRep>::EvaluateAtContinuo\n    * neighbors. The weight for each neighbor is the fraction overlap\n    * of the neighbor pixel with respect to a pixel centered on point.\n    */\n-  OutputType output;\n-  output.Fill(0.0);\n+  OutputType output{};\n \n   using ScalarRealType = typename NumericTraits<PixelType>::ScalarRealType;\n   ScalarRealType totalOverlap{};\ndiff --git a/Modules/Core/Mesh/include/itkBinaryMask3DMeshSource.hxx b/Modules/Core/Mesh/include/itkBinaryMask3DMeshSource.hxx\nindex fb5b5d95ee6..17bd8e891bd 100644\n--- a/Modules/Core/Mesh/include/itkBinaryMask3DMeshSource.hxx\n+++ b/Modules/Core/Mesh/include/itkBinaryMask3DMeshSource.hxx\n@@ -33,8 +33,7 @@ BinaryMask3DMeshSource<TInputImage, TOutputMesh>::BinaryMask3DMeshSource()\n   // Modify superclass default values, can be overridden by subclasses\n   this->SetNumberOfRequiredInputs(1);\n \n-  SizeType size;\n-  size.Fill(0);\n+  SizeType size{};\n   m_RegionOfInterest.SetSize(size);\n \n   this->GetOutput()->GetPoints()->Reserve(m_NodeLimit);\ndiff --git a/Modules/Core/Mesh/include/itkSimplexMesh.hxx b/Modules/Core/Mesh/include/itkSimplexMesh.hxx\nindex 0fe9221c08b..111914e5ead 100644\n--- a/Modules/Core/Mesh/include/itkSimplexMesh.hxx\n+++ b/Modules/Core/Mesh/include/itkSimplexMesh.hxx\n@@ -406,8 +406,7 @@ SimplexMesh<TPixelType, VDimension, TMeshTraits>::ComputeNormal(PointIdentifier\n   this->GetPoint(neighbors[2], &n3);\n \n   // compute normals\n-  CovariantVectorType normal;\n-  normal.Fill(0.0);\n+  CovariantVectorType normal{};\n   CovariantVectorType z;\n   z.SetVnlVector(vnl_cross_3d((n2 - n1).GetVnlVector(), (n3 - n1).GetVnlVector()));\n   z.Normalize();\ndiff --git a/Modules/Core/Mesh/include/itkSimplexMeshToTriangleMeshFilter.h b/Modules/Core/Mesh/include/itkSimplexMeshToTriangleMeshFilter.h\nindex fc14c875848..1aa0cfd69da 100644\n--- a/Modules/Core/Mesh/include/itkSimplexMeshToTriangleMeshFilter.h\n+++ b/Modules/Core/Mesh/include/itkSimplexMeshToTriangleMeshFilter.h\n@@ -112,11 +112,9 @@ class ITK_TEMPLATE_EXPORT SimplexMeshToTriangleMeshFilter : public MeshToMeshFil\n     {\n       using PointIdIterator = typename SimplexPolygonType::PointIdIterator;\n       PointIdIterator it = poly->PointIdsBegin();\n-      InputPointType  center;\n-      center.Fill(0);\n+      InputPointType  center{};\n \n-      InputPointType p;\n-      p.Fill(0);\n+      InputPointType p{};\n \n       while (it != poly->PointIdsEnd())\n       {\ndiff --git a/Modules/Core/Mesh/src/itkSimplexMeshGeometry.cxx b/Modules/Core/Mesh/src/itkSimplexMeshGeometry.cxx\nindex 75bf63c22b8..285be1ad768 100644\n--- a/Modules/Core/Mesh/src/itkSimplexMeshGeometry.cxx\n+++ b/Modules/Core/Mesh/src/itkSimplexMeshGeometry.cxx\n@@ -26,9 +26,7 @@ namespace itk\n SimplexMeshGeometry::SimplexMeshGeometry()\n {\n   double    c = 1.0 / 3.0;\n-  PointType p;\n-\n-  p.Fill(0.0);\n+  PointType p{};\n \n   pos.Fill(0);\n   oldPos.Fill(0);\ndiff --git a/Modules/Core/QuadEdgeMesh/include/itkQuadEdgeMeshEulerOperatorCreateCenterVertexFunction.hxx b/Modules/Core/QuadEdgeMesh/include/itkQuadEdgeMeshEulerOperatorCreateCenterVertexFunction.hxx\nindex fc4adf7c967..01d65e7d30a 100644\n--- a/Modules/Core/QuadEdgeMesh/include/itkQuadEdgeMeshEulerOperatorCreateCenterVertexFunction.hxx\n+++ b/Modules/Core/QuadEdgeMesh/include/itkQuadEdgeMeshEulerOperatorCreateCenterVertexFunction.hxx\n@@ -51,9 +51,8 @@ QuadEdgeMeshEulerOperatorCreateCenterVertexFunction<TMesh, TQEType>::Evaluate(QE\n   this->m_Mesh->DeleteFace(e->GetLeft());\n \n   // create new point geometry\n-  unsigned int sum = 0;\n-  VectorType   vec;\n-  vec.Fill(0);\n+  unsigned int    sum = 0;\n+  VectorType      vec{};\n   PointIdentifier pid = this->m_Mesh->FindFirstUnusedPointIndex();\n   using AssociatedBarycenters = std::map<QEType *, PointIdentifier>;\n   AssociatedBarycenters m_AssocBary;\ndiff --git a/Modules/Core/SpatialObjects/include/itkLineSpatialObjectPoint.hxx b/Modules/Core/SpatialObjects/include/itkLineSpatialObjectPoint.hxx\nindex f54d80a0a60..323517b4b65 100644\n--- a/Modules/Core/SpatialObjects/include/itkLineSpatialObjectPoint.hxx\n+++ b/Modules/Core/SpatialObjects/include/itkLineSpatialObjectPoint.hxx\n@@ -26,8 +26,7 @@ template <unsigned int TPointDimension>\n LineSpatialObjectPoint<TPointDimension>::LineSpatialObjectPoint()\n {\n   unsigned int        ii = 0;\n-  CovariantVectorType normal;\n-  normal.Fill(0);\n+  CovariantVectorType normal{};\n   while (ii < TPointDimension - 1)\n   {\n     this->m_NormalArrayInObjectSpace[ii] = normal;\ndiff --git a/Modules/Core/SpatialObjects/include/itkMetaDTITubeConverter.hxx b/Modules/Core/SpatialObjects/include/itkMetaDTITubeConverter.hxx\nindex 1672ee8f6b5..73f1d190fae 100644\n--- a/Modules/Core/SpatialObjects/include/itkMetaDTITubeConverter.hxx\n+++ b/Modules/Core/SpatialObjects/include/itkMetaDTITubeConverter.hxx\n@@ -56,10 +56,8 @@ MetaDTITubeConverter<VDimension>::MetaObjectToSpatialObject(const MetaObjectType\n \n   auto it2 = tube->GetPoints().begin();\n \n-  itk::CovariantVector<double, VDimension> v;\n-  v.Fill(0.0);\n-  itk::Vector<double, VDimension> t;\n-  t.Fill(0.0);\n+  itk::CovariantVector<double, VDimension> v{};\n+  itk::Vector<double, VDimension>          t{};\n \n   for (unsigned int identifier = 0; identifier < tube->GetPoints().size(); ++identifier)\n   {\ndiff --git a/Modules/Core/SpatialObjects/include/itkTubeSpatialObject.hxx b/Modules/Core/SpatialObjects/include/itkTubeSpatialObject.hxx\nindex 5030099b7be..093cb4f86a2 100644\n--- a/Modules/Core/SpatialObjects/include/itkTubeSpatialObject.hxx\n+++ b/Modules/Core/SpatialObjects/include/itkTubeSpatialObject.hxx\n@@ -405,11 +405,9 @@ TubeSpatialObject<TDimension, TTubePointType>::ComputeTangentsAndNormals()\n     ((TubePointType *)(this->GetPoint(it1)))->SetTangentInObjectSpace(tb);\n   }\n \n-  CovariantVectorType prevN1;\n-  prevN1.Fill(0);\n+  CovariantVectorType prevN1{};\n   prevN1[TDimension - 1] = 1;\n-  CovariantVectorType prevN2;\n-  prevN2.Fill(0);\n+  CovariantVectorType prevN2{};\n   prevN2[TDimension - 2] = 1;\n \n   it1 = 0;\ndiff --git a/Modules/Core/TestKernel/include/itkTestingExtractSliceImageFilter.hxx b/Modules/Core/TestKernel/include/itkTestingExtractSliceImageFilter.hxx\nindex 1ed56310a79..0d732d086eb 100644\n--- a/Modules/Core/TestKernel/include/itkTestingExtractSliceImageFilter.hxx\n+++ b/Modules/Core/TestKernel/include/itkTestingExtractSliceImageFilter.hxx\n@@ -67,12 +67,10 @@ ExtractSliceImageFilter<TInputImage, TOutputImage>::SetExtractionRegion(InputIma\n                 \"InputImageDimension must be greater than OutputImageDimension\");\n   m_ExtractionRegion = extractRegion;\n \n-  unsigned int        nonzeroSizeCount = 0;\n-  InputImageSizeType  inputSize = extractRegion.GetSize();\n-  OutputImageSizeType outputSize;\n-  outputSize.Fill(0);\n-  OutputImageIndexType outputIndex;\n-  outputIndex.Fill(0);\n+  unsigned int         nonzeroSizeCount = 0;\n+  InputImageSizeType   inputSize = extractRegion.GetSize();\n+  OutputImageSizeType  outputSize{};\n+  OutputImageIndexType outputIndex{};\n \n   /**\n    * check to see if the number of non-zero entries in the extraction region\n@@ -144,8 +142,7 @@ ExtractSliceImageFilter<TInputImage, TOutputImage>::GenerateOutputInformation()\n \n   typename OutputImageType::SpacingType   outputSpacing;\n   typename OutputImageType::DirectionType outputDirection;\n-  typename OutputImageType::PointType     outputOrigin;\n-  outputOrigin.Fill(0.0);\n+  typename OutputImageType::PointType     outputOrigin{};\n \n   if (static_cast<unsigned int>(OutputImageDimension) > static_cast<unsigned int>(InputImageDimension))\n   {\ndiff --git a/Modules/Core/Transform/include/itkBSplineTransform.hxx b/Modules/Core/Transform/include/itkBSplineTransform.hxx\nindex 8196af60fca..58f109def7e 100644\n--- a/Modules/Core/Transform/include/itkBSplineTransform.hxx\n+++ b/Modules/Core/Transform/include/itkBSplineTransform.hxx\n@@ -47,8 +47,7 @@ BSplineTransform<TParametersValueType, VDimension, VSplineOrder>::BSplineTransfo\n   // dir[0][2],dir[1][2],dir[2][2]]\n \n \n-  OriginType meshOrigin;\n-  meshOrigin.Fill(0.0);\n+  OriginType             meshOrigin{};\n   PhysicalDimensionsType meshPhysical;\n   meshPhysical.Fill(1.0);\n \n@@ -305,8 +304,7 @@ BSplineTransform<TParametersValueType, VDimension, VSplineOrder>::SetFixedParame\n \n   // Set the origin parameters\n   using PointType = typename ImageType::PointType;\n-  PointType origin;\n-  origin.Fill(0.0);\n+  PointType origin{};\n   for (unsigned int i = 0; i < VDimension; ++i)\n   {\n     ScalarType gridSpacing = meshPhysical[i] / static_cast<ScalarType>(meshSize[i]);\ndiff --git a/Modules/Core/Transform/include/itkBSplineTransformInitializer.hxx b/Modules/Core/Transform/include/itkBSplineTransformInitializer.hxx\nindex 286b1986e62..0ddbcef2ac1 100644\n--- a/Modules/Core/Transform/include/itkBSplineTransformInitializer.hxx\n+++ b/Modules/Core/Transform/include/itkBSplineTransformInitializer.hxx\n@@ -142,8 +142,7 @@ BSplineTransformInitializer<TTransform, TImage>::InitializeTransform() const\n \n   for (unsigned int d = 0; d < cornerPoints->GetNumberOfPoints(); ++d)\n   {\n-    PointType corner;\n-    corner.Fill(0.0);\n+    PointType corner{};\n     cornerPoints->GetPoint(d, &corner);\n \n     RealType distance = corner.SquaredEuclideanDistanceTo(bbox->GetMinimum());\n@@ -179,8 +178,7 @@ BSplineTransformInitializer<TTransform, TImage>::InitializeTransform() const\n       PointIdentifier oppositeCornerId =\n         (static_cast<PointIdentifier>(1) << static_cast<PointIdentifier>(i)) ^ transformDomainOriginId;\n \n-      PointType corner;\n-      corner.Fill(0.0);\n+      PointType corner{};\n       cornerPoints->GetPoint(oppositeCornerId, &corner);\n \n       VectorType vector = corner - transformDomainOrigin;\n@@ -216,8 +214,7 @@ BSplineTransformInitializer<TTransform, TImage>::InitializeTransform() const\n \n   for (unsigned int d = 0; d < SpaceDimension; ++d)\n   {\n-    PointType corner;\n-    corner.Fill(0.0);\n+    PointType corner{};\n     cornerPoints->GetPoint(minCornerId[d], &corner);\n \n     VectorType vector = corner - transformDomainOrigin;\ndiff --git a/Modules/Core/Transform/include/itkTransform.hxx b/Modules/Core/Transform/include/itkTransform.hxx\nindex 9251f0b5b6a..9471f75f60e 100644\n--- a/Modules/Core/Transform/include/itkTransform.hxx\n+++ b/Modules/Core/Transform/include/itkTransform.hxx\n@@ -299,9 +299,7 @@ Transform<TParametersValueType, VInputDimension, VOutputDimension>::\n   PreservationOfPrincipalDirectionDiffusionTensor3DReorientation(const InputDiffusionTensor3DType &  inputTensor,\n                                                                  const InverseJacobianPositionType & jacobian) const\n {\n-  Matrix<TParametersValueType, 3, 3> matrix;\n-\n-  matrix.Fill(0.0);\n+  Matrix<TParametersValueType, 3, 3> matrix{};\n   for (unsigned int i = 0; i < 3; ++i)\n   {\n     matrix(i, i) = 1.0;\ndiff --git a/Modules/Filtering/Denoising/include/itkPatchBasedDenoisingImageFilter.hxx b/Modules/Filtering/Denoising/include/itkPatchBasedDenoisingImageFilter.hxx\nindex bcbaf71934f..4cfff830ff7 100644\n--- a/Modules/Filtering/Denoising/include/itkPatchBasedDenoisingImageFilter.hxx\n+++ b/Modules/Filtering/Denoising/include/itkPatchBasedDenoisingImageFilter.hxx\n@@ -217,8 +217,7 @@ template <typename TInputImage, typename TOutputImage>\n void\n PatchBasedDenoisingImageFilter<TInputImage, TOutputImage>::Initialize()\n {\n-  typename InputImageType::IndexType requiredIndex;\n-  requiredIndex.Fill(0);\n+  typename InputImageType::IndexType        requiredIndex{};\n   const typename InputImageType::RegionType largestRegion = this->GetInput()->GetLargestPossibleRegion();\n   const PatchRadiusType                     radius = this->GetPatchRadiusInVoxels();\n   PatchRadiusType                           two;\ndiff --git a/Modules/Filtering/DisplacementField/include/itkTimeVaryingBSplineVelocityFieldTransform.hxx b/Modules/Filtering/DisplacementField/include/itkTimeVaryingBSplineVelocityFieldTransform.hxx\nindex ca805672757..72f048c21ac 100644\n--- a/Modules/Filtering/DisplacementField/include/itkTimeVaryingBSplineVelocityFieldTransform.hxx\n+++ b/Modules/Filtering/DisplacementField/include/itkTimeVaryingBSplineVelocityFieldTransform.hxx\n@@ -52,8 +52,7 @@ TimeVaryingBSplineVelocityFieldTransform<TParametersValueType, VDimension>::Inte\n \n   using BSplineFilterType = BSplineControlPointImageFilter<VelocityFieldType, VelocityFieldType>;\n \n-  typename BSplineFilterType::ArrayType closeDimensions;\n-  closeDimensions.Fill(0);\n+  typename BSplineFilterType::ArrayType closeDimensions{};\n   if (this->m_TemporalPeriodicity)\n   {\n     closeDimensions[VDimension] = 1;\ndiff --git a/Modules/Filtering/DisplacementField/include/itkTimeVaryingVelocityFieldIntegrationImageFilter.hxx b/Modules/Filtering/DisplacementField/include/itkTimeVaryingVelocityFieldIntegrationImageFilter.hxx\nindex ac6ba384fcf..27123ee3b60 100644\n--- a/Modules/Filtering/DisplacementField/include/itkTimeVaryingVelocityFieldIntegrationImageFilter.hxx\n+++ b/Modules/Filtering/DisplacementField/include/itkTimeVaryingVelocityFieldIntegrationImageFilter.hxx\n@@ -163,8 +163,7 @@ TimeVaryingVelocityFieldIntegrationImageFilter<TTimeVaryingVelocityField, TDispl\n   // Solve the initial value problem using fourth-order Runge-Kutta\n   //    y' = f(t, y), y(t_0) = y_0\n \n-  VectorType zeroVector;\n-  zeroVector.Fill(0.0);\n+  VectorType zeroVector{};\n \n   // Initial conditions\n \ndiff --git a/Modules/Filtering/DistanceMap/include/itkDanielssonDistanceMapImageFilter.hxx b/Modules/Filtering/DistanceMap/include/itkDanielssonDistanceMapImageFilter.hxx\nindex ea4615aa69f..fc2e3cdb868 100644\n--- a/Modules/Filtering/DistanceMap/include/itkDanielssonDistanceMapImageFilter.hxx\n+++ b/Modules/Filtering/DistanceMap/include/itkDanielssonDistanceMapImageFilter.hxx\n@@ -343,8 +343,7 @@ DanielssonDistanceMapImageFilter<TInputImage, TOutputImage, TVoronoiImage>::Gene\n \n   // Process image.\n \n-  OffsetType offset;\n-  offset.Fill(0);\n+  OffsetType offset{};\n \n   SizeValueType i = 0;\n \ndiff --git a/Modules/Filtering/DistanceMap/include/itkSignedMaurerDistanceMapImageFilter.hxx b/Modules/Filtering/DistanceMap/include/itkSignedMaurerDistanceMapImageFilter.hxx\nindex 7eb30664d00..d8104c05c1f 100644\n--- a/Modules/Filtering/DistanceMap/include/itkSignedMaurerDistanceMapImageFilter.hxx\n+++ b/Modules/Filtering/DistanceMap/include/itkSignedMaurerDistanceMapImageFilter.hxx\n@@ -223,11 +223,9 @@ SignedMaurerDistanceMapImageFilter<TInputImage, TOutputImage>::ThreadedGenerateD\n   // The result of this division is the offsetIndex, which is the index offset relative to the region of this thread.\n   // The true pixel location (idx) is provided by the sum of the offsetIndex and the startIndex.\n   InputSizeValueType index;\n-  OutputIndexType    offsetIndex;\n-  offsetIndex.Fill(0);\n+  OutputIndexType    offsetIndex{};\n   InputSizeValueType tempRow = NumberOfRows[m_CurrentDimension];\n-  OutputIndexType    idx;\n-  idx.Fill(0);\n+  OutputIndexType    idx{};\n \n   for (InputSizeValueType n = 0; n < tempRow; ++n)\n   {\ndiff --git a/Modules/Filtering/FastMarching/include/itkFastMarchingImageFilterBase.hxx b/Modules/Filtering/FastMarching/include/itkFastMarchingImageFilterBase.hxx\nindex c6791564c14..017c535c687 100644\n--- a/Modules/Filtering/FastMarching/include/itkFastMarchingImageFilterBase.hxx\n+++ b/Modules/Filtering/FastMarching/include/itkFastMarchingImageFilterBase.hxx\n@@ -56,8 +56,7 @@ FastMarchingImageFilterBase<TInput, TOutput>::FastMarchingImageFilterBase()\n   OutputSizeType outputSize;\n   outputSize.Fill(16);\n \n-  NodeType outputIndex;\n-  outputIndex.Fill(0);\n+  NodeType outputIndex{};\n \n   m_OutputRegion.SetSize(outputSize);\n   m_OutputRegion.SetIndex(outputIndex);\ndiff --git a/Modules/Filtering/ImageFeature/include/itkMaskFeaturePointSelectionFilter.hxx b/Modules/Filtering/ImageFeature/include/itkMaskFeaturePointSelectionFilter.hxx\nindex 1f2e440e6d1..c2c88049f1a 100644\n--- a/Modules/Filtering/ImageFeature/include/itkMaskFeaturePointSelectionFilter.hxx\n+++ b/Modules/Filtering/ImageFeature/include/itkMaskFeaturePointSelectionFilter.hxx\n@@ -227,8 +227,7 @@ MaskFeaturePointSelectionFilter<TImage, TMask, TFeatures>::GenerateData()\n       // compute and add structure tensor into pointData\n       if (m_ComputeStructureTensors)\n       {\n-        StructureTensorType tensor;\n-        tensor.Fill(0);\n+        StructureTensorType tensor{};\n \n         Matrix<SpacePrecisionType, ImageDimension, 1> gradI; // vector declared as column matrix\n \ndiff --git a/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilter.hxx b/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilter.hxx\nindex 7529f51e22e..091a89b5ee7 100644\n--- a/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilter.hxx\n+++ b/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilter.hxx\n@@ -69,8 +69,7 @@ MovingHistogramImageFilter<TInputImage, TOutputImage, TKernel, THistogram>::Dyna\n   FixedArray<short, ImageDimension> direction;\n   direction.Fill(1);\n   int        axis = ImageDimension - 1;\n-  OffsetType offset;\n-  offset.Fill(0);\n+  OffsetType offset{};\n   RegionType stRegion;\n   stRegion.SetSize(this->m_Kernel.GetSize());\n   stRegion.PadByRadius(1); // must pad the region by one because of the translation\ndiff --git a/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilterBase.hxx b/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilterBase.hxx\nindex 85056607cd7..ef679d04c16 100644\n--- a/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilterBase.hxx\n+++ b/Modules/Filtering/ImageFilterBase/include/itkMovingHistogramImageFilterBase.hxx\n@@ -108,13 +108,11 @@ MovingHistogramImageFilterBase<TInputImage, TOutputImage, TKernel>::SetKernel(co\n   // store the kernel offset list\n   m_KernelOffsets = kernelOffsets;\n \n-  FixedArray<SizeValueType, ImageDimension> axisCount;\n-  axisCount.Fill(0);\n+  FixedArray<SizeValueType, ImageDimension> axisCount{};\n \n   for (unsigned int axis = 0; axis < ImageDimension; ++axis)\n   {\n-    OffsetType refOffset;\n-    refOffset.Fill(0);\n+    OffsetType refOffset{};\n     for (int direction = -1; direction <= 1; direction += 2)\n     {\n       refOffset[axis] = direction;\ndiff --git a/Modules/Filtering/ImageFrequency/include/itkFrequencyFFTLayoutImageRegionConstIteratorWithIndex.h b/Modules/Filtering/ImageFrequency/include/itkFrequencyFFTLayoutImageRegionConstIteratorWithIndex.h\nindex 150466deb9d..2075784a919 100644\n--- a/Modules/Filtering/ImageFrequency/include/itkFrequencyFFTLayoutImageRegionConstIteratorWithIndex.h\n+++ b/Modules/Filtering/ImageFrequency/include/itkFrequencyFFTLayoutImageRegionConstIteratorWithIndex.h\n@@ -169,9 +169,7 @@ class ITK_TEMPLATE_EXPORT FrequencyFFTLayoutImageRegionConstIteratorWithIndex\n   IndexType\n   GetFrequencyBin() const\n   {\n-    IndexType freqInd;\n-\n-    freqInd.Fill(0);\n+    IndexType freqInd{};\n     for (unsigned int dim = 0; dim < TImage::ImageDimension; ++dim)\n     {\n       if (this->m_PositionIndex[dim] <= m_LargestPositiveFrequencyIndex[dim])\ndiff --git a/Modules/Filtering/ImageFrequency/include/itkFrequencyHalfHermitianFFTLayoutImageRegionConstIteratorWithIndex.h b/Modules/Filtering/ImageFrequency/include/itkFrequencyHalfHermitianFFTLayoutImageRegionConstIteratorWithIndex.h\nindex 332c3b0b38d..ae3f32c03df 100644\n--- a/Modules/Filtering/ImageFrequency/include/itkFrequencyHalfHermitianFFTLayoutImageRegionConstIteratorWithIndex.h\n+++ b/Modules/Filtering/ImageFrequency/include/itkFrequencyHalfHermitianFFTLayoutImageRegionConstIteratorWithIndex.h\n@@ -177,8 +177,7 @@ class FrequencyHalfHermitianFFTLayoutImageRegionConstIteratorWithIndex\n   IndexType\n   GetFrequencyBin() const\n   {\n-    IndexType freqInd;\n-    freqInd.Fill(0);\n+    IndexType freqInd{};\n     for (unsigned int dim = 0; dim < TImage::ImageDimension; ++dim)\n     {\n       if (this->m_PositionIndex[dim] <= m_LargestPositiveFrequencyIndex[dim])\ndiff --git a/Modules/Filtering/ImageFrequency/include/itkFrequencyShiftedFFTLayoutImageRegionConstIteratorWithIndex.h b/Modules/Filtering/ImageFrequency/include/itkFrequencyShiftedFFTLayoutImageRegionConstIteratorWithIndex.h\nindex f3034fbf3a2..aa9528617fb 100644\n--- a/Modules/Filtering/ImageFrequency/include/itkFrequencyShiftedFFTLayoutImageRegionConstIteratorWithIndex.h\n+++ b/Modules/Filtering/ImageFrequency/include/itkFrequencyShiftedFFTLayoutImageRegionConstIteratorWithIndex.h\n@@ -169,9 +169,7 @@ class ITK_TEMPLATE_EXPORT FrequencyShiftedFFTLayoutImageRegionConstIteratorWithI\n   IndexType\n   GetFrequencyBin() const\n   {\n-    IndexType freqInd;\n-\n-    freqInd.Fill(0);\n+    IndexType freqInd{};\n     for (unsigned int dim = 0; dim < TImage::ImageDimension; ++dim)\n     {\n       freqInd[dim] = this->m_PositionIndex[dim] - this->m_ZeroFrequencyIndex[dim];\ndiff --git a/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFilter.hxx b/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFilter.hxx\nindex 0f1010adb3c..467bdcbc150 100644\n--- a/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFilter.hxx\n+++ b/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFilter.hxx\n@@ -281,8 +281,7 @@ BSplineControlPointImageFilter<TInputImage, TOutputImage>::CollapsePhiLattice(Po\n        !It.IsAtEnd();\n        ++It)\n   {\n-    PointDataType data;\n-    data.Fill(0.0);\n+    PointDataType                          data{};\n     typename PointDataImageType::IndexType idx = It.GetIndex();\n     for (unsigned int i = 0; i < this->m_SplineOrder[dimension] + 1; ++i)\n     {\n@@ -423,8 +422,7 @@ BSplineControlPointImageFilter<TInputPointImage, TOutputImage>::RefineControlPoi\n     auto refinedLattice = ControlPointLatticeType::New();\n     refinedLattice->SetRegions(size);\n     refinedLattice->Allocate();\n-    PixelType data;\n-    data.Fill(0.0);\n+    PixelType data{};\n     refinedLattice->FillBuffer(data);\n \n     typename ControlPointLatticeType::IndexType            idx;\ndiff --git a/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFunction.hxx b/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFunction.hxx\nindex 417959ef0d2..8245e23fd06 100644\n--- a/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFunction.hxx\n+++ b/Modules/Filtering/ImageGrid/include/itkBSplineControlPointImageFunction.hxx\n@@ -235,8 +235,7 @@ BSplineControlPointImageFunction<TInputImage, TCoordRep>::Evaluate(const PointTy\n     }\n   }\n \n-  OutputType data;\n-  data.Fill(0.0);\n+  OutputType data{};\n \n   for (ImageRegionIteratorWithIndex<RealImageType> ItW(this->m_NeighborhoodWeightImage,\n                                                        this->m_NeighborhoodWeightImage->GetLargestPossibleRegion());\ndiff --git a/Modules/Filtering/ImageGrid/include/itkBSplineScatteredDataPointSetToImageFilter.hxx b/Modules/Filtering/ImageGrid/include/itkBSplineScatteredDataPointSetToImageFilter.hxx\nindex a503a96d68e..f5d985df76f 100644\n--- a/Modules/Filtering/ImageGrid/include/itkBSplineScatteredDataPointSetToImageFilter.hxx\n+++ b/Modules/Filtering/ImageGrid/include/itkBSplineScatteredDataPointSetToImageFilter.hxx\n@@ -453,8 +453,7 @@ BSplineScatteredDataPointSetToImageFilter<TInputPointSet, TOutputImage>::Threade\n \n   for (unsigned int n = start; n < end; ++n)\n   {\n-    PointType point;\n-    point.Fill(0.0);\n+    PointType point{};\n \n     input->GetPoint(n, &point);\n \n@@ -707,8 +706,7 @@ BSplineScatteredDataPointSetToImageFilter<TInputPointSet, TOutputImage>::AfterTh\n \n     for (ItP.GoToBegin(), ItO.GoToBegin(), ItD.GoToBegin(); !ItP.IsAtEnd(); ++ItP, ++ItO, ++ItD)\n     {\n-      PointDataType P;\n-      P.Fill(0);\n+      PointDataType P{};\n       if (Math::NotAlmostEquals(ItO.Get(), typename PointDataType::ValueType{}))\n       {\n         P = ItD.Get() / ItO.Get();\n@@ -755,8 +753,7 @@ BSplineScatteredDataPointSetToImageFilter<TInputPointSet, TOutputImage>::RefineC\n   refinedLattice->SetRegions(size);\n   refinedLattice->Allocate();\n \n-  PointDataType data;\n-  data.Fill(0.0);\n+  PointDataType data{};\n   refinedLattice->FillBuffer(data);\n \n   typename PointDataImageType::IndexType            idx;\n@@ -945,8 +942,7 @@ BSplineScatteredDataPointSetToImageFilter<TInputPointSet, TOutputImage>::Threade\n \n   for (unsigned int n = start; n < end; ++n)\n   {\n-    PointType point;\n-    point.Fill(0.0);\n+    PointType point{};\n \n     input->GetPoint(n, &point);\n \n@@ -1001,8 +997,7 @@ BSplineScatteredDataPointSetToImageFilter<TInputPointSet, TOutputImage>::Collaps\n        !It.IsAtEnd();\n        ++It)\n   {\n-    PointDataType data;\n-    data.Fill(0.0);\n+    PointDataType                          data{};\n     typename PointDataImageType::IndexType idx = It.GetIndex();\n     for (unsigned int i = 0; i < this->m_SplineOrder[dimension] + 1; ++i)\n     {\ndiff --git a/Modules/Filtering/ImageGrid/include/itkSliceBySliceImageFilter.hxx b/Modules/Filtering/ImageGrid/include/itkSliceBySliceImageFilter.hxx\nindex a1d3942cafa..546395ecaa6 100644\n--- a/Modules/Filtering/ImageGrid/include/itkSliceBySliceImageFilter.hxx\n+++ b/Modules/Filtering/ImageGrid/include/itkSliceBySliceImageFilter.hxx\n@@ -290,8 +290,7 @@ SliceBySliceImageFilter<TInputImage,\n       // not supported to avoid dealing with singularities, but we still account\n       // for the direction matrix when collapsing the origin to an N-1\n       // point.\n-      typename InputImageType::IndexType originIndex;\n-      originIndex.Fill(0);\n+      typename InputImageType::IndexType originIndex{};\n       originIndex[m_Dimension] = slice;\n \n       typename InputImageType::PointType inputOrigin;\ndiff --git a/Modules/Filtering/ImageGrid/include/itkSliceImageFilter.hxx b/Modules/Filtering/ImageGrid/include/itkSliceImageFilter.hxx\nindex 8a1e9dd1df9..e11d72cb2f9 100644\n--- a/Modules/Filtering/ImageGrid/include/itkSliceImageFilter.hxx\n+++ b/Modules/Filtering/ImageGrid/include/itkSliceImageFilter.hxx\n@@ -182,8 +182,7 @@ SliceImageFilter<TInputImage, TOutputImage>::GenerateInputRequestedRegion()\n   }\n \n \n-  typename TInputImage::SizeType inputRequestedRegionSize;\n-  inputRequestedRegionSize.Fill(0);\n+  typename TInputImage::SizeType inputRequestedRegionSize{};\n   for (unsigned int i = 0; i < TInputImage::ImageDimension; ++i)\n   {\n     if (outputRequestedRegionSize[i] > 0)\ndiff --git a/Modules/Filtering/ImageIntensity/include/itkPolylineMask2DImageFilter.hxx b/Modules/Filtering/ImageIntensity/include/itkPolylineMask2DImageFilter.hxx\nindex f2526e5687e..4175881bfea 100644\n--- a/Modules/Filtering/ImageIntensity/include/itkPolylineMask2DImageFilter.hxx\n+++ b/Modules/Filtering/ImageIntensity/include/itkPolylineMask2DImageFilter.hxx\n@@ -121,8 +121,7 @@ PolylineMask2DImageFilter<TInputImage, TPolyline, TOutputImage>::GenerateData()\n   tmpVertex = pstartVertex;\n   ++piter;\n \n-  ImageIndexType tmpImageIndex;\n-  tmpImageIndex.Fill(0);\n+  ImageIndexType tmpImageIndex{};\n \n   ImageLineIteratorType imit(outputImagePtr, outputImagePtr->GetLargestPossibleRegion());\n   imit.SetDirection(0);\ndiff --git a/Modules/Filtering/ImageIntensity/include/itkPolylineMaskImageFilter.hxx b/Modules/Filtering/ImageIntensity/include/itkPolylineMaskImageFilter.hxx\nindex 8005739eadc..ad4afbb0ce9 100644\n--- a/Modules/Filtering/ImageIntensity/include/itkPolylineMaskImageFilter.hxx\n+++ b/Modules/Filtering/ImageIntensity/include/itkPolylineMaskImageFilter.hxx\n@@ -167,8 +167,7 @@ PolylineMaskImageFilter<TInputImage, TPolyline, TVector, TOutputImage>::Generate\n   typename TPolyline::ConstPointer   polylinePtr(dynamic_cast<const TPolyline *>(this->ProcessObject::GetInput(1)));\n   typename TOutputImage::Pointer     outputImagePtr(dynamic_cast<TOutputImage *>(this->ProcessObject::GetOutput(0)));\n \n-  OriginType originInput;\n-  originInput.Fill(0.0);\n+  OriginType originInput{};\n   // outputImagePtr->SetOrigin(inputImagePtr->GetOrigin());\n   outputImagePtr->SetOrigin(originInput);\n   outputImagePtr->SetSpacing(inputImagePtr->GetSpacing());\ndiff --git a/Modules/Filtering/LabelMap/include/itkObjectByObjectLabelMapFilter.hxx b/Modules/Filtering/LabelMap/include/itkObjectByObjectLabelMapFilter.hxx\nindex 2a20fdcde71..0645fbf4c40 100644\n--- a/Modules/Filtering/LabelMap/include/itkObjectByObjectLabelMapFilter.hxx\n+++ b/Modules/Filtering/LabelMap/include/itkObjectByObjectLabelMapFilter.hxx\n@@ -194,14 +194,12 @@ ObjectByObjectLabelMapFilter<TInputImage,\n   if (m_ConstrainPaddingToImage)\n   {\n     m_Crop->SetCropBorder(m_PadSize);\n-    SizeType zero;\n-    zero.Fill(0);\n+    SizeType zero{};\n     m_Pad->SetPadSize(zero);\n   }\n   else\n   {\n-    SizeType zero;\n-    zero.Fill(0);\n+    SizeType zero{};\n     m_Crop->SetCropBorder(zero);\n     m_Pad->SetPadSize(m_PadSize);\n   }\ndiff --git a/Modules/Filtering/LabelMap/include/itkShapeLabelMapFilter.hxx b/Modules/Filtering/LabelMap/include/itkShapeLabelMapFilter.hxx\nindex 698da9f1ff7..33e31c20231 100644\n--- a/Modules/Filtering/LabelMap/include/itkShapeLabelMapFilter.hxx\n+++ b/Modules/Filtering/LabelMap/include/itkShapeLabelMapFilter.hxx\n@@ -95,16 +95,14 @@ ShapeLabelMapFilter<TImage, TLabelImage>::ThreadedProcessLabelObject(LabelObject\n \n   // Init the vars\n   SizeValueType                           nbOfPixels = 0;\n-  ContinuousIndex<double, ImageDimension> centroid;\n-  centroid.Fill(0);\n-  IndexType mins;\n+  ContinuousIndex<double, ImageDimension> centroid{};\n+  IndexType                               mins;\n   mins.Fill(NumericTraits<IndexValueType>::max());\n   IndexType maxs;\n   maxs.Fill(NumericTraits<IndexValueType>::NonpositiveMin());\n   SizeValueType nbOfPixelsOnBorder = 0;\n   double        perimeterOnBorder = 0;\n-  MatrixType    centralMoments;\n-  centralMoments.Fill(0);\n+  MatrixType    centralMoments{};\n \n   using LengthType = typename LabelObjectType::LengthType;\n \n@@ -551,8 +549,7 @@ ShapeLabelMapFilter<TImage, TLabelImage>::ComputePerimeter(LabelObjectType * lab\n     const VectorLineType & ls = lIt.GetCenterPixel();\n \n     // there are two intercepts on the 0 axis for each line\n-    OffsetType no;\n-    no.Fill(0);\n+    OffsetType no{};\n     no[0] = 1;\n     // std::cout << no << \"-> \" << 2 * ls.size() << std::endl;\n     intercepts[no] += 2 * static_cast<SizeValueType>(ls.size());\n@@ -677,8 +674,7 @@ ShapeLabelMapFilter<TImage, TLabelImage>::PerimeterFromInterceptCount(TMapInterc\n \n   for (int i = 0; i < dim; ++i)\n   {\n-    OffsetType no;\n-    no.Fill(0);\n+    OffsetType no{};\n     no[i] = 1;\n     // std::cout << no << \": \" << intercepts[no] << std::endl;\n     perimeter += pixelSize / spacing[i] * intercepts[no] / 2.0;\ndiff --git a/Modules/Filtering/LabelMap/include/itkStatisticsLabelMapFilter.hxx b/Modules/Filtering/LabelMap/include/itkStatisticsLabelMapFilter.hxx\nindex a9f61b4e947..e12a3171fbd 100644\n--- a/Modules/Filtering/LabelMap/include/itkStatisticsLabelMapFilter.hxx\n+++ b/Modules/Filtering/LabelMap/include/itkStatisticsLabelMapFilter.hxx\n@@ -96,18 +96,12 @@ StatisticsLabelMapFilter<TImage, TFeatureImage>::ThreadedProcessLabelObject(Labe\n   double                sum2 = 0;\n   double                sum3 = 0;\n   double                sum4 = 0;\n-  IndexType             minIdx;\n-  minIdx.Fill(0);\n-  IndexType maxIdx;\n-  maxIdx.Fill(0);\n-  PointType centerOfGravity;\n-  centerOfGravity.Fill(0);\n-  MatrixType centralMoments;\n-  centralMoments.Fill(0);\n-  MatrixType principalAxes;\n-  principalAxes.Fill(0);\n-  VectorType principalMoments;\n-  principalMoments.Fill(0);\n+  IndexType             minIdx{};\n+  IndexType             maxIdx{};\n+  PointType             centerOfGravity{};\n+  MatrixType            centralMoments{};\n+  MatrixType            principalAxes{};\n+  VectorType            principalMoments{};\n \n \n   // iterate over all the indexes\ndiff --git a/Modules/Filtering/MathematicalMorphology/include/itkFlatStructuringElement.hxx b/Modules/Filtering/MathematicalMorphology/include/itkFlatStructuringElement.hxx\nindex 11405393e53..2c0330d2994 100644\n--- a/Modules/Filtering/MathematicalMorphology/include/itkFlatStructuringElement.hxx\n+++ b/Modules/Filtering/MathematicalMorphology/include/itkFlatStructuringElement.hxx\n@@ -862,8 +862,7 @@ FlatStructuringElement<VDimension>::Box(RadiusType radius)\n   {\n     if (radius[i] != 0)\n     {\n-      LType L;\n-      L.Fill(0);\n+      LType L{};\n       L[i] = radius[i] * 2 + 1;\n       res.AddLine(L);\n     }\n@@ -896,8 +895,7 @@ FlatStructuringElement<VDimension>::Cross(RadiusType radius)\n   }\n   for (int d = 0; d < static_cast<int>(VDimension); ++d)\n   {\n-    OffsetType o;\n-    o.Fill(0);\n+    OffsetType o{};\n     for (int i = -static_cast<int>(radius[d]); i <= static_cast<int>(radius[d]); ++i)\n     {\n       o[d] = i;\ndiff --git a/Modules/Filtering/MathematicalMorphology/include/itkMaskedMovingHistogramImageFilter.hxx b/Modules/Filtering/MathematicalMorphology/include/itkMaskedMovingHistogramImageFilter.hxx\nindex 74f8c316fae..212ca439ab1 100644\n--- a/Modules/Filtering/MathematicalMorphology/include/itkMaskedMovingHistogramImageFilter.hxx\n+++ b/Modules/Filtering/MathematicalMorphology/include/itkMaskedMovingHistogramImageFilter.hxx\n@@ -161,8 +161,7 @@ MaskedMovingHistogramImageFilter<TInputImage, TMaskImage, TOutputImage, TKernel,\n   FixedArray<short, ImageDimension> direction;\n   direction.Fill(1);\n   int        axis = ImageDimension - 1;\n-  OffsetType offset;\n-  offset.Fill(0);\n+  OffsetType offset{};\n   RegionType stRegion;\n   stRegion.SetSize(this->m_Kernel.GetSize());\n   stRegion.PadByRadius(1); // must pad the region by one because of the\ndiff --git a/Modules/Filtering/Path/include/itkHilbertPath.hxx b/Modules/Filtering/Path/include/itkHilbertPath.hxx\nindex ad823f8cc37..ed4e54b943a 100644\n--- a/Modules/Filtering/Path/include/itkHilbertPath.hxx\n+++ b/Modules/Filtering/Path/include/itkHilbertPath.hxx\n@@ -53,8 +53,7 @@ HilbertPath<TIndexValue, VDimension>::TransformPathIndexToMultiDimensionalIndex(\n   PathIndexType d = 0;\n   PathIndexType e = 0;\n \n-  IndexType index;\n-  index.Fill(0);\n+  IndexType index{};\n \n   for (PathIndexType i = 0; i < this->m_HilbertOrder; ++i)\n   {\ndiff --git a/Modules/Filtering/Path/include/itkPathToChainCodePathFilter.hxx b/Modules/Filtering/Path/include/itkPathToChainCodePathFilter.hxx\nindex 6200444719e..9bfe7641095 100644\n--- a/Modules/Filtering/Path/include/itkPathToChainCodePathFilter.hxx\n+++ b/Modules/Filtering/Path/include/itkPathToChainCodePathFilter.hxx\n@@ -35,9 +35,7 @@ PathToChainCodePathFilter<TInputPath, TOutputChainCodePath>::GenerateData()\n {\n   OffsetType offset;\n   OffsetType tempOffset;\n-  OffsetType zeroOffset;\n-\n-  zeroOffset.Fill(0);\n+  OffsetType zeroOffset{};\n \n   InputPathInputType inputPathInput;\n \ndiff --git a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkBorderQuadEdgeMeshFilter.hxx b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkBorderQuadEdgeMeshFilter.hxx\nindex 17b6cc4b8bf..fe94f46a690 100644\n--- a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkBorderQuadEdgeMeshFilter.hxx\n+++ b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkBorderQuadEdgeMeshFilter.hxx\n@@ -285,9 +285,7 @@ BorderQuadEdgeMeshFilter<TInputMesh, TOutputMesh>::GetMeshBarycentre() -> InputP\n {\n   InputMeshConstPointer input = this->GetInput();\n \n-  InputPointType oCenter;\n-\n-  oCenter.Fill(0.0);\n+  InputPointType oCenter{};\n \n   const InputPointsContainer * points = input->GetPoints();\n \ndiff --git a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscreteMeanCurvatureQuadEdgeMeshFilter.h b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscreteMeanCurvatureQuadEdgeMeshFilter.h\nindex b30f685ac88..c5df552217b 100644\n--- a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscreteMeanCurvatureQuadEdgeMeshFilter.h\n+++ b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscreteMeanCurvatureQuadEdgeMeshFilter.h\n@@ -89,13 +89,10 @@ class ITK_TEMPLATE_EXPORT DiscreteMeanCurvatureQuadEdgeMeshFilter\n \n     OutputCurvatureType oH(0.);\n \n-    OutputVectorType Laplace;\n-\n-    Laplace.Fill(0.);\n+    OutputVectorType Laplace{};\n \n     OutputCurvatureType area(0.);\n-    OutputVectorType    normal;\n-    normal.Fill(0.);\n+    OutputVectorType    normal{};\n \n     if (qe != nullptr)\n     {\ndiff --git a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscretePrincipalCurvaturesQuadEdgeMeshFilter.h b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscretePrincipalCurvaturesQuadEdgeMeshFilter.h\nindex adae5e108f6..ea7d6a583de 100644\n--- a/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscretePrincipalCurvaturesQuadEdgeMeshFilter.h\n+++ b/Modules/Filtering/QuadEdgeMeshFiltering/include/itkDiscretePrincipalCurvaturesQuadEdgeMeshFilter.h\n@@ -92,8 +92,7 @@ class DiscretePrincipalCurvaturesQuadEdgeMeshFilter\n \n     if (qe != nullptr)\n     {\n-      OutputVectorType Laplace;\n-      Laplace.Fill(0.);\n+      OutputVectorType Laplace{};\n \n       OutputQEType * qe_it = qe;\n \n@@ -107,8 +106,7 @@ class DiscretePrincipalCurvaturesQuadEdgeMeshFilter\n         OutputPointType  q0, q1;\n         OutputVectorType face_normal;\n \n-        OutputVectorType normal;\n-        normal.Fill(0.);\n+        OutputVectorType normal{};\n \n         OutputCurvatureType temp_area;\n         OutputCoordType     temp_coeff;\ndiff --git a/Modules/Filtering/Smoothing/include/itkFFTDiscreteGaussianImageFilter.hxx b/Modules/Filtering/Smoothing/include/itkFFTDiscreteGaussianImageFilter.hxx\nindex 37399004984..be949741aef 100644\n--- a/Modules/Filtering/Smoothing/include/itkFFTDiscreteGaussianImageFilter.hxx\n+++ b/Modules/Filtering/Smoothing/include/itkFFTDiscreteGaussianImageFilter.hxx\n@@ -51,8 +51,7 @@ FFTDiscreteGaussianImageFilter<TInputImage, TOutputImage>::GenerateInputRequeste\n   inputRequestedRegion = inputPtr->GetRequestedRegion();\n \n   // pad the input requested region by the operator radius\n-  RadiusType radius;\n-  radius.Fill(0);\n+  RadiusType radius{};\n   for (size_t dim = 0; dim < ImageDimension; ++dim)\n   {\n     radius[dim] = this->GetKernelRadius(dim);\n@@ -91,8 +90,7 @@ FFTDiscreteGaussianImageFilter<TInputImage, TOutputImage>::GenerateKernelImage()\n     }\n \n     // Set up kernel image\n-    typename RealImageType::IndexType index;\n-    index.Fill(0);\n+    typename RealImageType::IndexType  index{};\n     typename RealImageType::RegionType region;\n     region.SetSize(kernelSize);\n     region.SetIndex(index);\ndiff --git a/Modules/IO/MINC/src/itkMINCImageIO.cxx b/Modules/IO/MINC/src/itkMINCImageIO.cxx\nindex f99afcf75a6..a304251a46d 100644\n--- a/Modules/IO/MINC/src/itkMINCImageIO.cxx\n+++ b/Modules/IO/MINC/src/itkMINCImageIO.cxx\n@@ -455,8 +455,7 @@ MINCImageIO::ReadImageInformation()\n   int numberOfComponents = 1;\n   int usable_dimensions = 0;\n \n-  Matrix<double, 3, 3> dir_cos;\n-  dir_cos.Fill(0.0);\n+  Matrix<double, 3, 3> dir_cos{};\n   dir_cos.SetIdentity();\n \n   Vector<double, 3> origin, sep;\ndiff --git a/Modules/Nonunit/Review/include/itkConformalFlatteningMeshFilter.hxx b/Modules/Nonunit/Review/include/itkConformalFlatteningMeshFilter.hxx\nindex 55ea0e9f498..9a87bbd082d 100644\n--- a/Modules/Nonunit/Review/include/itkConformalFlatteningMeshFilter.hxx\n+++ b/Modules/Nonunit/Review/include/itkConformalFlatteningMeshFilter.hxx\n@@ -137,12 +137,9 @@ ConformalFlatteningMeshFilter<TInputMesh, TOutputMesh>::GenerateData()\n   ++pointIditer;\n   unsigned int boundaryId2 = *pointIditer;\n \n-  InputPointType ptA;\n-  ptA.Fill(0.0);\n-  InputPointType ptB;\n-  ptB.Fill(0.0);\n-  InputPointType ptC;\n-  ptC.Fill(0.0);\n+  InputPointType ptA{};\n+  InputPointType ptB{};\n+  InputPointType ptC{};\n \n   inputMesh->GetPoint(boundaryId0, &ptA);\n   inputMesh->GetPoint(boundaryId1, &ptB);\ndiff --git a/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.h b/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.h\nindex c5ea4d18fdc..bea39414724 100644\n--- a/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.h\n+++ b/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.h\n@@ -193,9 +193,8 @@ class ITK_TEMPLATE_EXPORT\n       m_Eccentricity = 1;\n       m_Elongation = 1;\n       m_Orientation = 0;\n-      LabelPointType emptyPoint;\n-      emptyPoint.Fill(0);\n-      unsigned int numberOfVertices = 1 << ImageDimension;\n+      LabelPointType emptyPoint{};\n+      unsigned int   numberOfVertices = 1 << ImageDimension;\n       m_OrientedBoundingBoxVertices.resize(numberOfVertices, emptyPoint);\n       m_OrientedBoundingBoxVolume = 0;\n       m_OrientedBoundingBoxSize.Fill(0);\ndiff --git a/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.hxx b/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.hxx\nindex c806c60f7ba..c72a36ce7a1 100644\n--- a/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.hxx\n+++ b/Modules/Nonunit/Review/include/itkLabelGeometryImageFilter.hxx\n@@ -111,8 +111,7 @@ CalculateOrientedImage(const vnl_symmetric_eigensystem<double> &\n     center[i] = labelGeometry.m_Centroid[i] * inputImage->GetSpacing()[i];\n     origin[i] = labelGeometry.m_OrientedBoundingBoxOrigin[i] * inputImage->GetSpacing()[i];\n   }\n-  typename TransformType::OutputVectorType translation;\n-  translation.Fill(0);\n+  typename TransformType::OutputVectorType translation{};\n   transform->SetCenter(center);\n   transform->SetTranslation(translation);\n   transform->SetMatrix(rotationMatrix);\n@@ -861,8 +860,7 @@ LabelGeometryImageFilter<TLabelImage, TIntensityImage>::GetOrientedBoundingBoxVe\n   if (mapIt == m_LabelGeometryMapper.end())\n   {\n     // label does not exist, return a default value\n-    LabelPointType emptyPoint;\n-    emptyPoint.Fill(0);\n+    LabelPointType          emptyPoint{};\n     BoundingBoxVerticesType emptyVertices;\n     emptyVertices.resize(numberOfVertices, emptyPoint);\n     return emptyVertices;\ndiff --git a/Modules/Nonunit/Review/include/itkMiniPipelineSeparableImageFilter.hxx b/Modules/Nonunit/Review/include/itkMiniPipelineSeparableImageFilter.hxx\nindex 03ebcb08e59..5f9b1878901 100644\n--- a/Modules/Nonunit/Review/include/itkMiniPipelineSeparableImageFilter.hxx\n+++ b/Modules/Nonunit/Review/include/itkMiniPipelineSeparableImageFilter.hxx\n@@ -75,8 +75,7 @@ MiniPipelineSeparableImageFilter<TInputImage, TOutputImage, TFilter>::SetRadius(\n   // set up the kernels\n   for (unsigned int i = 0; i < ImageDimension; ++i)\n   {\n-    RadiusType rad;\n-    rad.Fill(0);\n+    RadiusType rad{};\n     rad[i] = radius[i];\n     m_Filters[i]->SetRadius(rad);\n   }\ndiff --git a/Modules/Numerics/Eigen/include/itkEigenAnalysis2DImageFilter.hxx b/Modules/Numerics/Eigen/include/itkEigenAnalysis2DImageFilter.hxx\nindex d24b1978914..6fad1637845 100644\n--- a/Modules/Numerics/Eigen/include/itkEigenAnalysis2DImageFilter.hxx\n+++ b/Modules/Numerics/Eigen/include/itkEigenAnalysis2DImageFilter.hxx\n@@ -146,8 +146,7 @@ EigenAnalysis2DImageFilter<TInputImage, TEigenValueImage, TEigenVectorImage>::Ge\n   ImageRegionIteratorWithIndex<EigenValueImageType>  outputIt2(outputPtr2, region);\n   ImageRegionIteratorWithIndex<EigenVectorImageType> outputIt3(outputPtr3, region);\n \n-  EigenVectorType nullVector;\n-  nullVector.Fill(0.0);\n+  EigenVectorType nullVector{};\n \n   // support progress methods/callbacks\n   ProgressReporter progress(this, 0, region.GetNumberOfPixels());\ndiff --git a/Modules/Numerics/FEM/include/itkFEMSolver.h b/Modules/Numerics/FEM/include/itkFEMSolver.h\nindex 4e93051bd46..5ffa36415a7 100644\n--- a/Modules/Numerics/FEM/include/itkFEMSolver.h\n+++ b/Modules/Numerics/FEM/include/itkFEMSolver.h\n@@ -228,9 +228,7 @@ class ITK_TEMPLATE_EXPORT Solver : public ProcessObject\n   void\n   InitializeInterpolationGrid(const InterpolationGridSizeType & size)\n   {\n-    InterpolationGridPointType bb1;\n-\n-    bb1.Fill(0.0);\n+    InterpolationGridPointType bb1{};\n \n     InterpolationGridPointType bb2;\n     for (unsigned int i = 0; i < FEMDimension; ++i)\ndiff --git a/Modules/Numerics/FEM/include/itkFEMSolver.hxx b/Modules/Numerics/FEM/include/itkFEMSolver.hxx\nindex a96783f1527..23fc4b41652 100644\n--- a/Modules/Numerics/FEM/include/itkFEMSolver.hxx\n+++ b/Modules/Numerics/FEM/include/itkFEMSolver.hxx\n@@ -731,8 +731,7 @@ Solver<VDimension>::InitializeInterpolationGrid(const InterpolationGridSizeType\n     image_size[i] = size[i];\n   }\n \n-  InterpolationGridPointType image_origin;\n-  image_origin.Fill(0.0);\n+  InterpolationGridPointType image_origin{};\n   for (unsigned int i = 0; i < FEMDimension; ++i)\n   {\n     image_origin[i] = bb1[i];\ndiff --git a/Modules/Numerics/Statistics/include/itkScalarImageToCooccurrenceListSampleFilter.hxx b/Modules/Numerics/Statistics/include/itkScalarImageToCooccurrenceListSampleFilter.hxx\nindex 91098760350..85afde38bc2 100644\n--- a/Modules/Numerics/Statistics/include/itkScalarImageToCooccurrenceListSampleFilter.hxx\n+++ b/Modules/Numerics/Statistics/include/itkScalarImageToCooccurrenceListSampleFilter.hxx\n@@ -98,8 +98,7 @@ ScalarImageToCooccurrenceListSampleFilter<TImage>::GenerateData()\n \n   typename FaceCalculatorType::FaceListType faceList = faceCalculator(input, input->GetRequestedRegion(), radius);\n \n-  OffsetType center_offset;\n-  center_offset.Fill(0);\n+  OffsetType center_offset{};\n \n   bool isInside;\n \ndiff --git a/Modules/Registration/Common/include/itkBSplineExponentialDiffeomorphicTransformParametersAdaptor.hxx b/Modules/Registration/Common/include/itkBSplineExponentialDiffeomorphicTransformParametersAdaptor.hxx\nindex c4e74024fc4..c4e2e177ba6 100644\n--- a/Modules/Registration/Common/include/itkBSplineExponentialDiffeomorphicTransformParametersAdaptor.hxx\n+++ b/Modules/Registration/Common/include/itkBSplineExponentialDiffeomorphicTransformParametersAdaptor.hxx\n@@ -40,8 +40,7 @@ void\n BSplineExponentialDiffeomorphicTransformParametersAdaptor<TTransform>::SetMeshSizeForTheConstantVelocityField(\n   const ArrayType & meshSize)\n {\n-  ArrayType numberOfControlPoints;\n-  numberOfControlPoints.Fill(0);\n+  ArrayType numberOfControlPoints{};\n   for (unsigned int d = 0; d < SpaceDimension; ++d)\n   {\n     if (meshSize[d] > 0)\n@@ -60,8 +59,7 @@ void\n BSplineExponentialDiffeomorphicTransformParametersAdaptor<TTransform>::SetMeshSizeForTheUpdateField(\n   const ArrayType & meshSize)\n {\n-  ArrayType numberOfControlPoints;\n-  numberOfControlPoints.Fill(0);\n+  ArrayType numberOfControlPoints{};\n   for (unsigned int d = 0; d < SpaceDimension; ++d)\n   {\n     if (meshSize[d] > 0)\ndiff --git a/Modules/Registration/Common/include/itkBSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor.hxx b/Modules/Registration/Common/include/itkBSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor.hxx\nindex abd3dd454ea..ce9baf7efae 100644\n--- a/Modules/Registration/Common/include/itkBSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor.hxx\n+++ b/Modules/Registration/Common/include/itkBSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor.hxx\n@@ -40,8 +40,7 @@ void\n BSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor<TTransform>::SetMeshSizeForTheUpdateField(\n   const ArrayType & meshSize)\n {\n-  ArrayType numberOfControlPoints;\n-  numberOfControlPoints.Fill(0);\n+  ArrayType numberOfControlPoints{};\n   for (unsigned int d = 0; d < SpaceDimension; ++d)\n   {\n     if (meshSize[d] > 0)\n@@ -60,8 +59,7 @@ void\n BSplineSmoothingOnUpdateDisplacementFieldTransformParametersAdaptor<TTransform>::SetMeshSizeForTheTotalField(\n   const ArrayType & meshSize)\n {\n-  ArrayType numberOfControlPoints;\n-  numberOfControlPoints.Fill(0);\n+  ArrayType numberOfControlPoints{};\n   for (unsigned int d = 0; d < SpaceDimension; ++d)\n   {\n     if (meshSize[d] > 0)\ndiff --git a/Modules/Registration/Common/include/itkLandmarkBasedTransformInitializer.hxx b/Modules/Registration/Common/include/itkLandmarkBasedTransformInitializer.hxx\nindex e5cbcd6af27..73d04040bac 100644\n--- a/Modules/Registration/Common/include/itkLandmarkBasedTransformInitializer.hxx\n+++ b/Modules/Registration/Common/include/itkLandmarkBasedTransformInitializer.hxx\n@@ -158,8 +158,7 @@ LandmarkBasedTransformInitializer<TTransform, TFixedImage, TMovingImage>::Intern\n \n   filter->SetNumberOfLevels(3);\n \n-  typename FilterType::ArrayType close;\n-  close.Fill(0);\n+  typename FilterType::ArrayType close{};\n   filter->SetCloseDimension(close);\n \n   filter->Update();\n@@ -645,8 +644,7 @@ LandmarkBasedTransformInitializer<TTransform, TFixedImage, TMovingImage>::Intern\n   transform->SetIdentity();\n \n   // Compute the centroids.\n-  PointType fixedCentroid;\n-  fixedCentroid.Fill(0.0);\n+  PointType                    fixedCentroid{};\n   PointsContainerConstIterator fixedItr = this->m_FixedLandmarks.begin();\n   while (fixedItr != this->m_FixedLandmarks.end())\n   {\n@@ -659,8 +657,7 @@ LandmarkBasedTransformInitializer<TTransform, TFixedImage, TMovingImage>::Intern\n   fixedCentroid[1] /= this->m_FixedLandmarks.size();\n \n   PointsContainerConstIterator movingItr = this->m_MovingLandmarks.begin();\n-  PointType                    movingCentroid;\n-  movingCentroid.Fill(0.0);\n+  PointType                    movingCentroid{};\n   while (movingItr != this->m_MovingLandmarks.end())\n   {\n     movingCentroid[0] += (*movingItr)[0];\n@@ -771,8 +768,7 @@ LandmarkBasedTransformInitializer<TTransform, TFixedImage, TMovingImage>::Comput\n   const LandmarkPointContainer inputLandmarks) -> PointType3D\n {\n   // Compute the centroids.\n-  PointType3D centroid;\n-  centroid.Fill(0.0);\n+  PointType3D                  centroid{};\n   PointsContainerConstIterator fixedItr = inputLandmarks.begin();\n   while (fixedItr != inputLandmarks.end())\n   {\ndiff --git a/Modules/Registration/FEM/include/itkFEMRegistrationFilter.hxx b/Modules/Registration/FEM/include/itkFEMRegistrationFilter.hxx\nindex 7bba4df6074..a5abe92f3cd 100644\n--- a/Modules/Registration/FEM/include/itkFEMRegistrationFilter.hxx\n+++ b/Modules/Registration/FEM/include/itkFEMRegistrationFilter.hxx\n@@ -857,8 +857,7 @@ FEMRegistrationFilter<TMovingImage, TFixedImage, TFemObject>::EnforceDiffeomorph\n     m_TotalField->SetLargestPossibleRegion(m_FieldRegion);\n     m_TotalField->Allocate();\n \n-    VectorType disp;\n-    disp.Fill(0.0);\n+    VectorType disp{};\n \n     FieldIterator fieldIter(m_TotalField, m_FieldRegion);\n \n@@ -930,8 +929,7 @@ FEMRegistrationFilter<TMovingImage, TFixedImage, TFemObject>::EnforceDiffeomorph\n     fieldIter.GoToBegin();\n     while (!fieldIter.IsAtEnd())\n     {\n-      VectorType disp;\n-      disp.Fill(0.0);\n+      VectorType disp{};\n       fieldIter.Set(disp);\n       ++fieldIter;\n     }\ndiff --git a/Modules/Registration/FEM/include/itkNCCRegistrationFunction.hxx b/Modules/Registration/FEM/include/itkNCCRegistrationFunction.hxx\nindex 3a28ee85c8f..7b330a3d80a 100644\n--- a/Modules/Registration/FEM/include/itkNCCRegistrationFunction.hxx\n+++ b/Modules/Registration/FEM/include/itkNCCRegistrationFunction.hxx\n@@ -155,9 +155,8 @@ NCCRegistrationFunction<TFixedImage, TMovingImage, TDisplacementField>::ComputeU\n     }\n   }\n \n-  PixelType update;\n-  update.Fill(0.0);\n-  double updatenorm = 0.0;\n+  PixelType update{};\n+  double    updatenorm = 0.0;\n   if ((sff * smm) != 0.0)\n   {\n     const double factor = 1.0 / std::sqrt(sff * smm);\ndiff --git a/Modules/Registration/Metricsv4/include/itkEuclideanDistancePointSetToPointSetMetricv4.hxx b/Modules/Registration/Metricsv4/include/itkEuclideanDistancePointSetToPointSetMetricv4.hxx\nindex 6d3be753d58..c204bd772c8 100644\n--- a/Modules/Registration/Metricsv4/include/itkEuclideanDistancePointSetToPointSetMetricv4.hxx\n+++ b/Modules/Registration/Metricsv4/include/itkEuclideanDistancePointSetToPointSetMetricv4.hxx\n@@ -28,8 +28,7 @@ typename EuclideanDistancePointSetToPointSetMetricv4<TFixedPointSet, TMovingPoin\n   EuclideanDistancePointSetToPointSetMetricv4<TFixedPointSet, TMovingPointSet, TInternalComputationValueType>::\n     GetLocalNeighborhoodValue(const PointType & point, const PixelType & itkNotUsed(pixel)) const\n {\n-  PointType closestPoint;\n-  closestPoint.Fill(0.0);\n+  PointType closestPoint{};\n \n   PointIdentifier pointId = this->m_MovingTransformedPointsLocator->FindClosestPoint(point);\n   closestPoint = this->m_MovingTransformedPointSet->GetPoint(pointId);\n@@ -53,8 +52,7 @@ EuclideanDistancePointSetToPointSetMetricv4<TFixedPointSet, TMovingPointSet, TIn\n                                          LocalDerivativeType & localDerivative,\n                                          const PixelType &     itkNotUsed(pixel)) const\n {\n-  PointType closestPoint;\n-  closestPoint.Fill(0.0);\n+  PointType closestPoint{};\n \n   PointIdentifier pointId = this->m_MovingTransformedPointsLocator->FindClosestPoint(point);\n   closestPoint = this->m_MovingTransformedPointSet->GetPoint(pointId);\ndiff --git a/Modules/Registration/Metricsv4/include/itkExpectationBasedPointSetToPointSetMetricv4.hxx b/Modules/Registration/Metricsv4/include/itkExpectationBasedPointSetToPointSetMetricv4.hxx\nindex b75da3a36de..3570e909468 100644\n--- a/Modules/Registration/Metricsv4/include/itkExpectationBasedPointSetToPointSetMetricv4.hxx\n+++ b/Modules/Registration/Metricsv4/include/itkExpectationBasedPointSetToPointSetMetricv4.hxx\n@@ -84,8 +84,7 @@ ExpectationBasedPointSetToPointSetMetricv4<TFixedPointSet, TMovingPointSet, TInt\n \n   localDerivative.Fill(0.0);\n \n-  PointType weightedPoint;\n-  weightedPoint.Fill(0.0);\n+  PointType weightedPoint{};\n \n   NeighborsIdentifierType neighborhood;\n \ndiff --git a/Modules/Registration/PDEDeformable/include/itkSymmetricForcesDemonsRegistrationFunction.hxx b/Modules/Registration/PDEDeformable/include/itkSymmetricForcesDemonsRegistrationFunction.hxx\nindex d0dfb02f323..43cd538c5c3 100644\n--- a/Modules/Registration/PDEDeformable/include/itkSymmetricForcesDemonsRegistrationFunction.hxx\n+++ b/Modules/Registration/PDEDeformable/include/itkSymmetricForcesDemonsRegistrationFunction.hxx\n@@ -28,9 +28,7 @@ template <typename TFixedImage, typename TMovingImage, typename TDisplacementFie\n SymmetricForcesDemonsRegistrationFunction<TFixedImage, TMovingImage, TDisplacementField>::\n   SymmetricForcesDemonsRegistrationFunction()\n {\n-  RadiusType r;\n-\n-  r.Fill(0);\n+  RadiusType r{};\n   this->SetRadius(r);\n \n   m_TimeStep = 1.0;\ndiff --git a/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingBSplineVelocityFieldImageRegistrationMethod.hxx b/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingBSplineVelocityFieldImageRegistrationMethod.hxx\nindex ed6bf54aa6c..3ed9bc0338e 100644\n--- a/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingBSplineVelocityFieldImageRegistrationMethod.hxx\n+++ b/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingBSplineVelocityFieldImageRegistrationMethod.hxx\n@@ -352,8 +352,7 @@ TimeVaryingBSplineVelocityFieldImageRegistrationMethod<\n \n   SizeValueType numberOfIntegrationSteps = this->m_NumberOfTimePointSamples + 2;\n \n-  typename DisplacementFieldType::PixelType zeroVector;\n-  zeroVector.Fill(0);\n+  typename DisplacementFieldType::PixelType zeroVector{};\n \n   velocityFieldPointSet->Initialize();\n   velocityFieldWeights->Initialize();\ndiff --git a/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingVelocityFieldImageRegistrationMethodv4.hxx b/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingVelocityFieldImageRegistrationMethodv4.hxx\nindex da58ef27bf3..1d3bef3b237 100644\n--- a/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingVelocityFieldImageRegistrationMethodv4.hxx\n+++ b/Modules/Registration/RegistrationMethodsv4/include/itkTimeVaryingVelocityFieldImageRegistrationMethodv4.hxx\n@@ -66,8 +66,7 @@ TimeVaryingVelocityFieldImageRegistrationMethodv4<TFixedImage,\n {\n   using DisplacementFieldDuplicatorType = ImageDuplicator<DisplacementFieldType>;\n   using DisplacementFieldTransformType = DisplacementFieldTransform<RealType, ImageDimension>;\n-  typename DisplacementFieldType::PixelType zeroVector;\n-  zeroVector.Fill(0);\n+  typename DisplacementFieldType::PixelType zeroVector{};\n \n   // This transform is used for the fixed image\n   using IdentityTransformType = itk::IdentityTransform<RealType, ImageDimension>;\ndiff --git a/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DFilter.hxx b/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DFilter.hxx\nindex 5de52724b54..b79bbc8e219 100644\n--- a/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DFilter.hxx\n+++ b/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DFilter.hxx\n@@ -217,9 +217,8 @@ DeformableSimplexMesh3DFilter<TInputMesh, TOutputMesh>::ComputeGeometry()\n {\n   PointType           Foot;\n   CovariantVectorType normal;\n-  CovariantVectorType z;\n-  z.Fill(0.0);\n-  VectorType tmp;\n+  CovariantVectorType z{};\n+  VectorType          tmp;\n   //   IdentifierType idx = 0;\n \n   const InputMeshType *        inputMesh = this->GetInput(0);\n@@ -294,8 +293,7 @@ DeformableSimplexMesh3DFilter<TInputMesh, TOutputMesh>::ComputeDisplacement()\n \n   typename GeometryMapType::Iterator dataIt = this->m_Data->Begin();\n   SimplexMeshGeometry *              data;\n-  VectorType                         displacement;\n-  displacement.Fill(0.0);\n+  VectorType                         displacement{};\n \n   while (dataIt != this->m_Data->End())\n   {\ndiff --git a/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DGradientConstraintForceFilter.hxx b/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DGradientConstraintForceFilter.hxx\nindex 49ab5e20627..cb21beb25c5 100644\n--- a/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DGradientConstraintForceFilter.hxx\n+++ b/Modules/Segmentation/DeformableMesh/include/itkDeformableSimplexMesh3DGradientConstraintForceFilter.hxx\n@@ -393,8 +393,7 @@ DeformableSimplexMesh3DGradientConstraintForceFilter<TInputMesh, TOutputMesh>::C\n   // now fun begins try to use all the above\n   std::vector<ImageVoxel *>::iterator it;\n   double                              mag, max = 0;\n-  GradientIndexType                   coord3, coord2;\n-  coord2.Fill(0);\n+  GradientIndexType                   coord3, coord2{};\n   for (it = m_Positive.begin(); it != m_Positive.end(); ++it)\n   {\n     coord3[0] = static_cast<GradientIndexValueType>((*it)->GetX());\ndiff --git a/Modules/Segmentation/KLMRegionGrowing/include/itkKLMRegionGrowImageFilter.hxx b/Modules/Segmentation/KLMRegionGrowing/include/itkKLMRegionGrowImageFilter.hxx\nindex 02ce6e92f82..525cb03ef02 100644\n--- a/Modules/Segmentation/KLMRegionGrowing/include/itkKLMRegionGrowImageFilter.hxx\n+++ b/Modules/Segmentation/KLMRegionGrowing/include/itkKLMRegionGrowImageFilter.hxx\n@@ -112,8 +112,7 @@ KLMRegionGrowImageFilter<TInputImage, TOutputImage>::GenerateOutputImage()\n   OutputRegionType region;\n   region.SetSize(gridSize); // Constant grid size\n \n-  OutputImageIndexType tmpIndex;\n-  tmpIndex.Fill(0);\n+  OutputImageIndexType tmpIndex{};\n \n   for (unsigned int iregion = 0; iregion < m_InitialNumberOfRegions; ++iregion)\n   {\n@@ -201,8 +200,7 @@ KLMRegionGrowImageFilter<TInputImage, TOutputImage>::GenerateLabelledImage(Label\n   InputRegionType region;\n   region.SetSize(gridSize); // Constant grid size\n \n-  InputImageIndexType tmpIndex;\n-  tmpIndex.Fill(0);\n+  InputImageIndexType tmpIndex{};\n \n   for (unsigned int iregion = 0; iregion < m_InitialNumberOfRegions; ++iregion)\n   {\n@@ -329,8 +327,7 @@ KLMRegionGrowImageFilter<TInputImage, TOutputImage>::InitializeKLM()\n   InputRegionType region;\n   region.SetSize(gridSize); // Constant grid size\n \n-  InputImageIndexType tmpIndex;\n-  tmpIndex.Fill(0);\n+  InputImageIndexType tmpIndex{};\n \n   for (RegionLabelType iregion = 0; iregion < m_InitialNumberOfRegions; ++iregion)\n   {\ndiff --git a/Modules/Segmentation/MarkovRandomFieldsClassifiers/include/itkRGBGibbsPriorFilter.hxx b/Modules/Segmentation/MarkovRandomFieldsClassifiers/include/itkRGBGibbsPriorFilter.hxx\nindex 21cc8737497..91f67615196 100644\n--- a/Modules/Segmentation/MarkovRandomFieldsClassifiers/include/itkRGBGibbsPriorFilter.hxx\n+++ b/Modules/Segmentation/MarkovRandomFieldsClassifiers/include/itkRGBGibbsPriorFilter.hxx\n@@ -189,9 +189,7 @@ template <typename TInputImage, typename TClassifiedImage>\n void\n RGBGibbsPriorFilter<TInputImage, TClassifiedImage>::GibbsTotalEnergy(int i)\n {\n-  LabelledImageIndexType offsetIndex3D;\n-\n-  offsetIndex3D.Fill(0);\n+  LabelledImageIndexType offsetIndex3D{};\n \n   int size = m_ImageWidth * m_ImageHeight * m_ImageDepth;\n   int frame = m_ImageWidth * m_ImageHeight;\n@@ -315,9 +313,7 @@ RGBGibbsPriorFilter<TInputImage, TClassifiedImage>::GibbsEnergy(unsigned int i,\n   bool         changeflag;\n   double       res = 0.0;\n \n-  LabelledImageIndexType offsetIndex3D;\n-\n-  offsetIndex3D.Fill(0);\n+  LabelledImageIndexType offsetIndex3D{};\n \n   LabelledImagePixelType labelledPixel = 0;\n \n@@ -533,8 +529,7 @@ RGBGibbsPriorFilter<TInputImage, TClassifiedImage>::ApplyGibbsLabeller()\n   changedPixelVec.Fill(typename InputImagePixelType::ValueType{});\n \n   // Set a variable to store the offset index.\n-  LabelledImageIndexType offsetIndex3D;\n-  offsetIndex3D.Fill(0);\n+  LabelledImageIndexType offsetIndex3D{};\n \n   const unsigned int size = m_ImageWidth * m_ImageHeight * m_ImageDepth;\n   const unsigned int frame = m_ImageWidth * m_ImageHeight;\n@@ -656,9 +651,7 @@ RGBGibbsPriorFilter<TInputImage, TClassifiedImage>::LabelRegion(int i, int l, in\n   const unsigned int frame = m_ImageWidth * m_ImageHeight;\n   const unsigned int rowsize = m_ImageWidth;\n \n-  LabelledImageIndexType offsetIndex3D;\n-\n-  offsetIndex3D.Fill(0);\n+  LabelledImageIndexType offsetIndex3D{};\n \n   m_Region[i] = l;\n \ndiff --git a/Modules/Segmentation/Voronoi/include/itkVoronoiDiagram2DGenerator.hxx b/Modules/Segmentation/Voronoi/include/itkVoronoiDiagram2DGenerator.hxx\nindex c7600faad3e..869a911a772 100644\n--- a/Modules/Segmentation/Voronoi/include/itkVoronoiDiagram2DGenerator.hxx\n+++ b/Modules/Segmentation/Voronoi/include/itkVoronoiDiagram2DGenerator.hxx\n@@ -1077,8 +1077,7 @@ VoronoiDiagram2DGenerator<TCoordRepType>::GenerateVDFortune()\n   m_BottomSite = &(m_SeedSites[0]);\n   FortuneSite * currentSite = &(m_SeedSites[1]);\n \n-  PointType currentCircle;\n-  currentCircle.Fill(0.0);\n+  PointType         currentCircle{};\n   FortuneHalfEdge * leftHalfEdge;\n   FortuneHalfEdge * rightHalfEdge;\n   FortuneHalfEdge * left2HalfEdge;\n", "instance_id": "InsightSoftwareConsortium__ITK-4897", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to replace a specific code pattern (`Fill(0)` with `{}` initializer) for local variables in test files within a specific codebase (ITK). It provides the exact search-and-replace pattern used via Notepad++ and specifies the file filters (`itk*Test*.cxx`). The goal of improving code style by using modern C++ initialization is evident, and a link to the pull request is provided for context. However, there are minor ambiguities: the statement does not explicitly discuss the rationale behind the change (e.g., why `{}` is preferred over `Fill(0)` beyond zero-filling), nor does it address potential risks or edge cases (e.g., whether this change could break existing behavior in certain contexts). Additionally, it lacks explicit mention of the scope of impact beyond test files or any specific constraints to consider during the replacement. Despite these minor gaps, the problem is valid and understandable for someone familiar with C++ and the codebase.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward stylistic change with minimal technical complexity. The task requires basic knowledge of C++ initialization syntax and the ability to perform a search-and-replace operation across multiple files. The code changes provided show a consistent pattern of replacing `Fill(0)` with `{}` for various types (e.g., `OutputImageSizeType`, `PointType`, `VectorType`), which is a simple modification that does not alter the logic or behavior of the code (since `{}` achieves the same zero-initialization). The scope of changes is broad, affecting numerous files in the ITK repository (as seen in the diff across multiple modules), but the nature of the change is mechanical and does not require deep understanding of the codebase's architecture or interactions between components. No complex algorithms, design patterns, or domain-specific knowledge are needed beyond basic C++ syntax. Edge cases and error handling are not a concern here, as the replacement is semantically equivalent and unlikely to introduce issues. The primary effort lies in ensuring the replacement is applied correctly across all relevant files, which is more about diligence than technical challenge. Therefore, I assign a difficulty score of 0.15, reflecting a very easy task with minimal cognitive load.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug]: nwbinspector fails on folders located on external ssd for mac due to ._ sidecar files\n### What happened?\n\nTried to run nwbinspector on folder of nwb files located on an external ssd, but ran into an error.\r\n\r\n```bash\r\n>  nwbinspector /Volumes/T7/CatalystNeuro/NWB/Lerner/conversion_nwb                                              \r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/bin/nwbinspector\", line 8, in <module>\r\n    sys.exit(inspect_all_cli())\r\n             ^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/nwbinspector/nwbinspector.py\", line 273, in inspect_all_cli\r\n    messages = list(\r\n               ^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/nwbinspector/nwbinspector.py\", line 408, in inspect_all\r\n    with pynwb.NWBHDF5IO(path=nwbfile_path, mode=\"r\", load_namespaces=True, driver=driver) as io:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/hdmf/utils.py\", line 668, in func_call\r\n    return func(args[0], **pargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/pynwb/__init__.py\", line 274, in __init__\r\n    super().load_namespaces(tm, path, file=file_obj, driver=driver)\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/hdmf/utils.py\", line 668, in func_call\r\n    return func(args[0], **pargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/hdmf/backends/hdf5/h5tools.py\", line 169, in load_namespaces\r\n    open_file_obj = cls.__resolve_file_obj(path, file_obj, driver)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/hdmf/backends/hdf5/h5tools.py\", line 144, in __resolve_file_obj\r\n    file_obj = File(path, 'r', **file_kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/h5py/_hl/files.py\", line 562, in __init__\r\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/anaconda3/envs/lerner_lab_to_nwb_env/lib/python3.12/site-packages/h5py/_hl/files.py\", line 235, in make_fid\r\n    fid = h5f.open(name, flags, fapl=fapl)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\r\nOSError: Unable to open file (file signature not found)\r\n```\n\n### Operating System\n\nmacOS\n\n### Python Version\n\n3.12\n\n### Were you streaming with ROS3?\n\nNo\n\n### Package Versions\n\n<details> aiobotocore==2.11.1\r\naiohttp==3.9.2\r\naioitertools==0.11.0\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nappdirs==1.4.4\r\nappnope==0.1.4\r\narrow==1.3.0\r\nasciitree==0.3.3\r\nasttokens==2.4.1\r\nattrs==23.2.0\r\nbidsschematools==0.7.2\r\nblessed==1.20.0\r\nboto3==1.34.27\r\nbotocore==1.34.27\r\nbqplot==0.12.42\r\ncertifi==2023.11.17\r\ncfgv==3.4.0\r\ncharset-normalizer==3.3.2\r\nci-info==0.3.0\r\nclick==8.1.7\r\nclick-didyoumean==0.3.0\r\ncomm==0.2.1\r\ncontourpy==1.2.0\r\ncycler==0.12.1\r\ndandi==0.59.1\r\ndandischema==0.8.2\r\ndebugpy==1.8.1\r\ndecorator==5.1.1\r\ndistlib==0.3.8\r\ndnspython==2.5.0\r\ndocstring_parser==0.16\r\nemail-validator==2.1.0.post1\r\nentrypoints==0.4\r\net-xmlfile==1.1.0\r\netelemetry==0.3.1\r\nexecuting==2.0.1\r\nfasteners==0.19\r\nfilelock==3.13.1\r\nfonttools==4.47.2\r\nfparse==1.20.1\r\nfqdn==1.5.1\r\nfrozenlist==1.4.1\r\nfscacher==0.4.0\r\nfsspec==2023.12.2\r\ngast==0.4.0\r\nh5py==3.10.0\r\nhdmf==3.13.0\r\nhdmf_zarr==0.7.0\r\nhumanize==4.9.0\r\nidentify==2.5.33\r\nidna==3.6\r\nimageio==2.33.1\r\nimportlib-metadata==4.13.0\r\niniconfig==2.0.0\r\ninterleave==0.2.1\r\nipydatagrid==1.2.1\r\nipydatawidgets==4.3.2\r\nipyfilechooser==0.6.0\r\nipykernel==6.29.3\r\nipympl==0.9.3\r\nipython==8.20.0\r\nipython-genutils==0.2.0\r\nipyvolume==0.6.3\r\nipyvue==1.10.1\r\nipyvuetify==1.8.10\r\nipywebrtc==0.6.0\r\nipywidgets==8.1.1\r\nisodate==0.6.1\r\nisoduration==20.11.0\r\njaraco.classes==3.3.0\r\njedi==0.19.1\r\njmespath==1.0.1\r\njoblib==1.3.2\r\njsonpointer==2.4\r\njsonschema==4.21.1\r\njsonschema-specifications==2023.12.1\r\njupyter_client==8.6.0\r\njupyter_core==5.7.1\r\njupyterlab-widgets==3.0.9\r\nkeyring==24.3.0\r\nkeyrings.alt==5.0.0\r\nkiwisolver==1.4.5\r\nlazy_loader==0.3\r\nmatplotlib==3.8.2\r\nmatplotlib-inline==0.1.6\r\nmore-itertools==10.2.0\r\nmultidict==6.0.4\r\nnatsort==8.4.0\r\nndx-events==0.2.0\r\nndx-fiber-photometry @ git+https://github.com/catalystneuro/ndx-fiber-photometry.git@14e05cda86026db694c1c577918284f435b7ec10\r\nndx-grayscalevolume==0.0.2\r\nndx-icephys-meta==0.1.0\r\nndx-photometry @ git+https://github.com/catalystneuro/ndx-photometry.git@c1284c91a7e0a5dfd19f84bbf683e17fa607d4ec\r\nndx-spectrum==0.2.2\r\nnest-asyncio==1.6.0\r\nnetworkx==3.2.1\r\nneuroconv @ git+https://github.com/catalystneuro/neuroconv.git@f45e77e152369916459a764c918d4c5cbbf1fad1\r\nnodeenv==1.8.0\r\nnumcodecs==0.11.0\r\nnumpy==1.26.3\r\nnwbinspector==0.4.35\r\nnwbwidgets==0.11.3\r\nopenpyxl==3.1.2\r\npackaging==23.2\r\npandas==2.2.0\r\nparse==1.20.1\r\nparso==0.8.3\r\npexpect==4.9.0\r\npillow==10.2.0\r\nplatformdirs==4.1.0\r\nplotly==5.13.1\r\npluggy==1.4.0\r\npre-commit==3.6.2\r\nprompt-toolkit==3.0.43\r\npsutil==5.9.8\r\nptyprocess==0.7.0\r\npure-eval==0.2.2\r\npy2vega==0.6.1\r\npycryptodomex==3.20.0\r\npydantic==2.7.1\r\npydantic_core==2.18.2\r\nPygments==2.17.2\r\npynwb==2.7.0\r\npyout==0.7.3\r\npyparsing==3.1.1\r\npytest==8.1.1\r\npython-dateutil==2.8.2\r\npythreejs==2.4.2\r\npytz==2023.4\r\nPyYAML==6.0.1\r\npyzmq==25.1.2\r\nreferencing==0.33.0\r\nrequests==2.31.0\r\nrfc3339-validator==0.1.4\r\nrfc3987==1.3.8\r\nrpds-py==0.17.1\r\nruamel.yaml==0.18.5\r\nruamel.yaml.clib==0.2.8\r\ns3fs==2023.12.2\r\ns3transfer==0.10.0\r\nscikit-image==0.22.0\r\nscikit-learn==1.4.1.post1\r\nscipy==1.12.0\r\nsemantic-version==2.10.0\r\nsetuptools==69.0.3\r\nsix==1.16.0\r\nstack-data==0.6.3\r\ntdt==0.6.6\r\ntenacity==8.2.3\r\nthreadpoolctl==3.2.0\r\ntifffile==2023.12.9\r\ntornado==6.4\r\ntqdm==4.66.1\r\ntraitlets==5.14.1\r\ntraittypes==0.2.1\r\ntrimesh==4.1.0\r\ntypes-python-dateutil==2.8.19.20240106\r\ntyping_extensions==4.9.0\r\ntzdata==2023.4\r\nuri-template==1.3.0\r\nurllib3==2.0.7\r\nvirtualenv==20.25.0\r\nwcwidth==0.2.13\r\nwebcolors==1.13\r\nwheel==0.42.0\r\nwidgetsnbextension==4.0.9\r\nwrapt==1.16.0\r\nyarl==1.9.4\r\nzarr==2.16.1\r\nzarr-checksum==0.4.0\r\nzipp==3.17.0\r\n </details>\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/NeurodataWithoutBorders/nwbinspector/blob/dev/.github/CODE_OF_CONDUCT.rst)\n- [X] Have you ensured this bug was not already [reported](https://github.com/neurodatawithoutborders/nwbinspector/issues)?\n- [X] To the best of your ability, have you ensured this is a bug within the code that checks the NWBFile, rather than a bug in the NWBFile reader (e.g., [PyNWB](https://github.com/NeurodataWithoutBorders/pynwb) or [MatNWB](https://github.com/NeurodataWithoutBorders/matnwb))?\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 4bcd996f3..33311b094 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -4,6 +4,8 @@\n \n * Fixed the suggested rate in `check_regular_timestamps` to be in Hz [#467](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/467)\n \n+* Added a skip for mac sidecar files (._*): [#470](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/470)\n+\n # v0.4.35\n \n ### Fixes\ndiff --git a/src/nwbinspector/nwbinspector.py b/src/nwbinspector/nwbinspector.py\nindex 994ac30df..8f6f89e7c 100644\n--- a/src/nwbinspector/nwbinspector.py\n+++ b/src/nwbinspector/nwbinspector.py\n@@ -395,6 +395,9 @@ def inspect_all(\n         in_path = Path(path)\n         if in_path.is_dir():\n             nwbfiles = list(in_path.rglob(\"*.nwb\"))\n+\n+            # Remove any macOS sidecar files\n+            nwbfiles = [nwbfile for nwbfile in nwbfiles if not nwbfile.name.startswith(\"._\")]\n         elif in_path.is_file():\n             nwbfiles = [in_path]\n         else:\n", "instance_id": "NeurodataWithoutBorders__nwbinspector-470", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: `nwbinspector` fails when processing folders on an external SSD on macOS due to the presence of macOS sidecar files (._* files). The goal is implicitly understood as fixing the error caused by these files, and the error traceback provides context about where the failure occurs. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the expected behavior (e.g., should the tool ignore these files or handle them differently?), nor does it mention potential edge cases like nested directories or specific file naming patterns beyond the \"._\" prefix. Additionally, there are no examples of file structures or specific test cases to reproduce the issue. Despite these minor gaps, the issue is valid and the intent is reasonably clear, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue requires a simple modification to filter out macOS sidecar files (._* files) from the list of files to be processed by `nwbinspector`. The code change is minimal, involving a single line added to the `inspect_all` function in `nwbinspector.py` to exclude files based on their name prefix. This change is localized to one file and does not impact the broader architecture or require understanding complex interactions within the codebase. The technical concepts involved are basic\u2014list comprehension and string matching in Python\u2014which are straightforward for any developer familiar with the language. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code change beyond the simple filtering logic. Overall, this is a very easy fix that requires minimal effort and expertise to implement and verify.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Python 3.12 not supported\n<!-- Welcome! Thank you for contributing. These HTML comments will not render in the issue.\r\n\r\nBefore creating a new issue:\r\n* Search for relevant issues\r\n* Follow the issue reporting guidelines:\r\nhttps://jupyterlab.readthedocs.io/en/latest/getting_started/issue.html\r\n-->\r\n\r\n## Description\r\n\r\nAttempting to install Jupyter AI on Python 3.12 is not supported, even though our recipe on `conda-forge` merely stipulates `python >=3.8`.\r\n\r\nThe following error occurs when running `pip install jupyter_ai` with Python 3.12 on macOS 13.6.3 on an Apple Silicon processor:\r\n\r\n```\r\n...\r\nUsing cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\r\nBuilding wheels for collected packages: faiss-cpu\r\n  Building wheel for faiss-cpu (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Building wheel for faiss-cpu (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [12 lines of output]\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      running build_ext\r\n      building 'faiss._swigfaiss' extension\r\n      swigging faiss/faiss/python/swigfaiss.i to faiss/faiss/python/swigfaiss_wrap.cpp\r\n      swig -python -c++ -Doverride= -I/usr/local/include -Ifaiss -doxygen -module swigfaiss -o faiss/faiss/python/swigfaiss_wrap.cpp faiss/faiss/python/swigfaiss.i\r\n      Traceback (most recent call last):\r\n        File \"/opt/miniconda3/envs/jupyter-ai-jl4-pip/bin/swig\", line 5, in <module>\r\n          from swig import swig\r\n      ModuleNotFoundError: No module named 'swig'\r\n      error: command '/opt/miniconda3/envs/jupyter-ai-jl4-pip/bin/swig' failed with exit code 1\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for faiss-cpu\r\nFailed to build faiss-cpu\r\nERROR: Could not build wheels for faiss-cpu, which is required to install pyproject.toml-based projects\r\n```\r\n\r\nSee also https://github.com/kyamagu/faiss-wheels/issues/87 for the lack of Python 3.12 wheels for faiss-cpu.\n", "patch": "diff --git a/README.md b/README.md\nindex 682c4bf23..e624d2a86 100644\n--- a/README.md\n+++ b/README.md\n@@ -22,7 +22,7 @@ Documentation is available on [ReadTheDocs](https://jupyter-ai.readthedocs.io/en\n \n You will need to have installed the following software to use Jupyter AI:\n \n-- Python 3.8 - 3.11\n+- Python 3.8 - 3.12\n - JupyterLab 4 or Notebook 7\n \n In addition, you will need access to at least one model provider.\ndiff --git a/docs/source/contributors/index.md b/docs/source/contributors/index.md\nindex 87618b828..a6005433f 100644\n--- a/docs/source/contributors/index.md\n+++ b/docs/source/contributors/index.md\n@@ -18,7 +18,7 @@ Issues and pull requests that violate the above principles may be declined. If y\n \n ## Prerequisites\n \n-You can develop Jupyter AI on any system that can run a supported Python version up to and including 3.11, including recent Windows, macOS, and Linux versions.\n+You can develop Jupyter AI on any system that can run a supported Python version up to and including 3.12, including recent Windows, macOS, and Linux versions.\n \n Each Jupyter AI major version works with only one major version of JupyterLab. Jupyter AI 1.x supports JupyterLab 3.x, and Jupyter AI 2.x supports JupyterLab 4.x.\n \n@@ -35,7 +35,7 @@ Due to a compatibility issue with Webpack, Node.js 18.15.0 does not work with Ju\n After you have installed the prerequisites, create a new conda environment and activate it.\n \n ```\n-conda create -n jupyter-ai -c conda-forge python=3.11 nodejs=20\n+conda create -n jupyter-ai -c conda-forge python=3.12 nodejs=20\n conda activate jupyter-ai\n ```\n \ndiff --git a/docs/source/users/index.md b/docs/source/users/index.md\nindex 2365d4784..8beece4f6 100644\n--- a/docs/source/users/index.md\n+++ b/docs/source/users/index.md\n@@ -11,12 +11,12 @@ please see the {doc}`developer's guide </developers/index>`.\n ## Prerequisites\n \n You can run Jupyter AI on any system that can run a supported Python version\n-from 3.8 to 3.11, including recent Windows, macOS, and Linux versions.\n+from 3.8 to 3.12, including recent Windows, macOS, and Linux versions.\n \n-If you use `conda`, you can install Python 3.11 in your environment by running:\n+If you use `conda`, you can install Python 3.12 in your environment by running:\n \n ```\n-conda install python=3.11\n+conda install python=3.12\n ```\n \n The `jupyter_ai` package, which provides the lab extension and user interface in\n@@ -111,9 +111,9 @@ environment.\n \n First, install\n [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)\n-and create an environment that uses Python 3.11:\n+and create an environment that uses Python 3.12:\n \n-    $ conda create -n jupyter-ai python=3.11\n+    $ conda create -n jupyter-ai python=3.12\n     $ conda activate jupyter-ai\n \n Then, use `conda` to install JupyterLab and Jupyter AI in this Conda environment.\ndiff --git a/packages/jupyter-ai-magics/pyproject.toml b/packages/jupyter-ai-magics/pyproject.toml\nindex e97fd4444..42acc4ad7 100644\n--- a/packages/jupyter-ai-magics/pyproject.toml\n+++ b/packages/jupyter-ai-magics/pyproject.toml\n@@ -16,6 +16,7 @@ classifiers = [\n     \"Programming Language :: Python :: 3.9\",\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n ]\n \n dynamic = [\"version\", \"description\", \"authors\", \"urls\", \"keywords\"]\ndiff --git a/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/README.md b/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/README.md\nindex 0c73388fe..907ae389d 100644\n--- a/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/README.md\n+++ b/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/README.md\n@@ -6,7 +6,7 @@ extension.\n \n ## Requirements\n \n-- Python 3.8 - 3.11\n+- Python 3.8 - 3.12\n - JupyterLab 4\n \n ## Install\ndiff --git a/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/pyproject.toml b/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/pyproject.toml\nindex 9e8a54b12..300f770ab 100644\n--- a/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/pyproject.toml\n+++ b/packages/jupyter-ai-module-cookiecutter/{{cookiecutter.root_dir_name}}/pyproject.toml\n@@ -18,6 +18,7 @@ classifiers = [\n     \"Programming Language :: Python :: 3.9\",\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n ]\n version = \"{{ cookiecutter.version }}\"\n description = \"A Jupyter AI extension.\"\ndiff --git a/packages/jupyter-ai/README.md b/packages/jupyter-ai/README.md\nindex 2ca103efb..6cfe86afb 100644\n--- a/packages/jupyter-ai/README.md\n+++ b/packages/jupyter-ai/README.md\n@@ -24,9 +24,9 @@ Before you can use Jupyter AI, you will need to install any packages and set env\n \n ### With conda\n \n-First, install [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) and create an environment that uses Python 3.11:\n+First, install [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) and create an environment that uses Python 3.12:\n \n-    $ conda create -n jupyter-ai python=3.11\n+    $ conda create -n jupyter-ai python=3.12\n     $ conda activate jupyter-ai\n     $ pip install jupyter_ai\n \ndiff --git a/packages/jupyter-ai/pyproject.toml b/packages/jupyter-ai/pyproject.toml\nindex 15e660cfc..e8deeb133 100644\n--- a/packages/jupyter-ai/pyproject.toml\n+++ b/packages/jupyter-ai/pyproject.toml\n@@ -20,6 +20,7 @@ classifiers = [\n     \"Programming Language :: Python :: 3.9\",\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n ]\n dependencies = [\n     \"jupyter_server>=1.6,<3\",\n", "instance_id": "jupyterlab__jupyter-ai-898", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Jupyter AI does not support Python 3.12 due to a dependency issue with `faiss-cpu` during installation, as evidenced by the provided error log. The goal is implicitly understood as adding support for Python 3.12, and the error output provides context for the root cause (missing `swig` module and lack of pre-built wheels for `faiss-cpu`). However, there are minor ambiguities and missing details. The problem statement does not explicitly define the expected solution or deliverables (e.g., whether to update documentation, fix the dependency issue, or both). Additionally, it lacks mention of specific constraints or edge cases, such as compatibility with other dependencies or platforms beyond macOS on Apple Silicon. While a reference to an external issue is provided, the statement could benefit from clearer guidance on the scope of the fix. Thus, it is rated as \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of this problem is rated as Easy (0.25) based on the provided code changes and the nature of the issue. Here's the breakdown by the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows changes limited to documentation and configuration files (README.md, pyproject.toml, etc.) across multiple locations in the repository. These changes primarily involve updating the supported Python version range to include 3.12. There are no modifications to core logic or functional code, and the changes do not impact the system's architecture. The amount of code change is minimal and straightforward, focusing on textual updates rather than complex implementation.\n\n2. **Number of Technical Concepts:** Solving this problem, as reflected in the code changes, requires basic knowledge of Python versioning and package configuration (e.g., updating classifiers in `pyproject.toml`). However, the underlying issue mentioned in the problem statement (building `faiss-cpu` for Python 3.12) hints at potential complexities not addressed in the diff, such as dependency management, build tools (e.g., `swig`), and platform-specific wheel availability. Since the provided changes do not tackle these deeper issues, the technical concepts involved in the current scope are minimal and simple.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement and code changes do not explicitly address edge cases or error handling. The documentation updates assume Python 3.12 compatibility without addressing potential installation failures or platform-specific issues (e.g., lack of wheels for `faiss-cpu` on certain architectures). However, since the changes are purely documentary, no error handling logic is required in the code itself.\n\n4. **Overall Assessment:** While the problem statement hints at a potentially more complex issue (dependency build failures), the provided code changes suggest a superficial fix limited to updating version references. This task requires minimal understanding of the codebase beyond identifying where version constraints are specified. It does not involve deep architectural changes, complex logic, or advanced technical knowledge. Therefore, it falls into the Easy category (0.2-0.4). I assign a score of 0.25, acknowledging that while the changes are simple, there is an implicit need to verify compatibility beyond documentation, which adds a slight layer of effort.\n\nNote: If the task were to fully resolve the `faiss-cpu` build issue for Python 3.12 (e.g., by providing wheels or fixing the build process), the difficulty would increase significantly (likely to 0.6-0.8) due to the need for expertise in build systems, cross-platform compatibility, and dependency management. However, based on the provided diff, the scope is limited to documentation updates.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature Request] Support tuple values in TensorDictModule `in_keys` arguments.\n## Motivation\r\n\r\nOne limitation that is found when passing a dictionary as `in_keys` to the current `TensorDictModule` is that a specific key in the input `TensorDict` cannot be used more than once in the wrapped function (or wrapped `Module`'s `forward`).\r\n\r\nFor instance, one can do:\r\n\r\n```python\r\nmodule = TensorDictModule(lambda x, *, y: x+y, in_keys={'1': 'x', '2': 'y'}, out_keys=['z'])\r\nmodule(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\r\nprint(td['z']) # tensor(3.)\r\n```\r\nbut the following will crash as expected:\r\n\r\n```python\r\nmodule = TensorDictModule(lambda x, *, y, t : x+y+t, in_keys={'1': 'x', '2': ('y', 't')}, out_keys=['z'])\r\nmodule(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\r\n# TypeError: keywords must be strings\r\n```\r\n\r\nI would clearly see cases where passing the same input value to 2 different arguments in the wrapped function/module would be desirable.\r\n\r\nCould you clarify whether that is a specific design choice or whether there is any reason this feature would cause a problem?\r\n\r\n## Solution\r\n\r\nA simple solution would be to allow tuples and lists a values for the `in_keys` argument and parse accordingly. This can be [achieved](https://github.com/pytorch/tensordict/compare/main...bachdj-px:tensordict:support_tuple_in_keys_dispatch) by iterating over said values and building the lists of in_keys and keywords:\r\n\r\n```python\r\nif isinstance(in_keys, dict):\r\n            # write the kwargs and create a list instead\r\n            _in_keys = []\r\n            self._kwargs = []\r\n            for key, value in in_keys.items():\r\n                self._kwargs.append(value)\r\n                _in_keys.append(key)\r\n                if isinstance(value, tuple) or isinstance(value, list):\r\n                    for _value in value:\r\n                        self._kwargs.append(_value)\r\n                        _in_keys.append(key)\r\n                else:\r\n                    self._kwargs.append(value)\r\n                    _in_keys.append(key)\r\n            in_keys = _in_keys\r\n```\r\n\r\nIn our second example, this would result in:\r\n\r\n```python\r\nin_keys = [\"1\", \"2\", \"2\"]\r\nself._kwargs = [\"x\", \"y\", \"t\"]\r\n```\r\n\r\n## Additional context\r\n\r\nI haven't identified undesired side effects that would come with this new support of various in_keys.\r\n\r\n## Checklist\r\n\r\n- [x] I have checked that there is no similar issue in the repo (**required**)\r\n\n[Feature Request] Support tuple values in TensorDictModule `in_keys` arguments.\n## Motivation\r\n\r\nOne limitation that is found when passing a dictionary as `in_keys` to the current `TensorDictModule` is that a specific key in the input `TensorDict` cannot be used more than once in the wrapped function (or wrapped `Module`'s `forward`).\r\n\r\nFor instance, one can do:\r\n\r\n```python\r\nmodule = TensorDictModule(lambda x, *, y: x+y, in_keys={'1': 'x', '2': 'y'}, out_keys=['z'])\r\nmodule(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\r\nprint(td['z']) # tensor(3.)\r\n```\r\nbut the following will crash as expected:\r\n\r\n```python\r\nmodule = TensorDictModule(lambda x, *, y, t : x+y+t, in_keys={'1': 'x', '2': ('y', 't')}, out_keys=['z'])\r\nmodule(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\r\n# TypeError: keywords must be strings\r\n```\r\n\r\nI would clearly see cases where passing the same input value to 2 different arguments in the wrapped function/module would be desirable.\r\n\r\nCould you clarify whether that is a specific design choice or whether there is any reason this feature would cause a problem?\r\n\r\n## Solution\r\n\r\nA simple solution would be to allow tuples and lists a values for the `in_keys` argument and parse accordingly. This can be [achieved](https://github.com/pytorch/tensordict/compare/main...bachdj-px:tensordict:support_tuple_in_keys_dispatch) by iterating over said values and building the lists of in_keys and keywords:\r\n\r\n```python\r\nif isinstance(in_keys, dict):\r\n            # write the kwargs and create a list instead\r\n            _in_keys = []\r\n            self._kwargs = []\r\n            for key, value in in_keys.items():\r\n                self._kwargs.append(value)\r\n                _in_keys.append(key)\r\n                if isinstance(value, tuple) or isinstance(value, list):\r\n                    for _value in value:\r\n                        self._kwargs.append(_value)\r\n                        _in_keys.append(key)\r\n                else:\r\n                    self._kwargs.append(value)\r\n                    _in_keys.append(key)\r\n            in_keys = _in_keys\r\n```\r\n\r\nIn our second example, this would result in:\r\n\r\n```python\r\nin_keys = [\"1\", \"2\", \"2\"]\r\nself._kwargs = [\"x\", \"y\", \"t\"]\r\n```\r\n\r\n## Additional context\r\n\r\nI haven't identified undesired side effects that would come with this new support of various in_keys.\r\n\r\n## Checklist\r\n\r\n- [x] I have checked that there is no similar issue in the repo (**required**)\r\n\n", "patch": "diff --git a/tensordict/nn/common.py b/tensordict/nn/common.py\nindex e6d150528..395141c0a 100644\n--- a/tensordict/nn/common.py\n+++ b/tensordict/nn/common.py\n@@ -773,11 +773,20 @@ class TensorDictModule(TensorDictModuleBase):\n             order given by the in_keys iterable.\n             If ``in_keys`` is a dictionary, its keys must correspond to the key\n             to be read in the tensordict and its values must match the name of\n-            the keyword argument in the function signature.\n+            the keyword argument in the function signature. If `out_to_in_map` is ``True``,\n+            the mapping gets inverted so that the keys correspond to the keyword\n+            arguments in the function signature.\n         out_keys (iterable of str): keys to be written to the input tensordict. The length of out_keys must match the\n             number of tensors returned by the embedded module. Using \"_\" as a key avoid writing tensor to output.\n \n     Keyword Args:\n+        out_to_in_map (bool, optional): if ``True``, `in_keys` is read as if the keys are the arguments keys of\n+            the :meth:`~.forward` method and the values are the keys in the input :class:`~tensordict.TensorDict`. If\n+            ``False`` or ``None`` (default), keys are considered to be the input keys and values the method's arguments keys.\n+\n+            .. warning::\n+                The default value of `out_to_in_map` will change from ``False`` to ``True`` in the v0.9 release.\n+\n         inplace (bool or string, optional): if ``True`` (default), the output of the module are written in the tensordict\n             provided to the :meth:`~.forward` method. If ``False``, a new :class:`~tensordict.TensorDict` with and empty\n             batch-size and no device is created. if ``\"empty\"``, :meth:`~tensordict.TensorDict.empty` will be used to\n@@ -865,12 +874,24 @@ class TensorDictModule(TensorDictModuleBase):\n \n     Examples:\n         >>> module = TensorDictModule(lambda x, *, y: x+y,\n-        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'],\n+        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'], out_to_in_map=False\n         ...     )\n         >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\n         >>> td['z']\n         tensor(3.)\n \n+    If `out_to_in_map` is set to ``True``, then the `in_keys` mapping is reversed. This way,\n+    one can use the same input key for different keyword arguments.\n+\n+    Examples:\n+        >>> module = TensorDictModule(lambda x, *, y, z: x+y+z,\n+        ...     in_keys={'x': '1', 'y': '2', z: '2'}, out_keys=['t'], out_to_in_map=True\n+        ...     )\n+        >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))\n+        >>> td['t']\n+        tensor(5.)\n+\n+\n     Functional calls to a tensordict module is easy:\n \n     Examples:\n@@ -923,17 +944,39 @@ def __init__(\n         in_keys: NestedKey | List[NestedKey] | Dict[NestedKey:str],\n         out_keys: NestedKey | List[NestedKey],\n         *,\n+        out_to_in_map: bool | None = None,\n         inplace: bool | str = True,\n     ) -> None:\n         super().__init__()\n \n+        if out_to_in_map is not None and not isinstance(in_keys, dict):\n+            warnings.warn(\n+                \"out_to_in_map is not None but is only used when in_key is a dictionary.\"\n+            )\n+\n         if isinstance(in_keys, dict):\n+            if out_to_in_map is None:\n+                out_to_in_map = False\n+                warnings.warn(\n+                    \"Using a dictionary in_keys without specifying out_to_in_map is deprecated. \"\n+                    \"By default, out_to_in_map is `False` (`in_keys` keys as tensordict pointers, \"\n+                    \"values as kwarg name), but from version>=0.9, default will be `True` \"\n+                    \"(`in_keys` keys as func kwarg name, values as tensordict pointers). \"\n+                    \"Please use explicit out_to_in_map to indicate the ordering of the input keys. \",\n+                    DeprecationWarning,\n+                    stacklevel=2,\n+                )\n+\n             # write the kwargs and create a list instead\n             _in_keys = []\n             self._kwargs = []\n             for key, value in in_keys.items():\n-                self._kwargs.append(value)\n-                _in_keys.append(key)\n+                if out_to_in_map:  # arg: td_key\n+                    self._kwargs.append(key)\n+                    _in_keys.append(value)\n+                else:  # td_key: arg\n+                    self._kwargs.append(value)\n+                    _in_keys.append(key)\n             in_keys = _in_keys\n         else:\n             if isinstance(in_keys, (str, tuple)):\n", "instance_id": "pytorch__tensordict-1101", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear and provides a good understanding of the issue and the proposed solution. The motivation behind supporting tuple values in `TensorDictModule` `in_keys` arguments is well-explained with examples that demonstrate the current limitation and the desired behavior. The solution section outlines a clear approach to address the issue by allowing tuples and lists as values in the `in_keys` dictionary. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., nested tuples, empty lists/tuples, or conflicts in key mappings) or how they should be handled. Additionally, while the code change is provided, the broader implications or potential side effects on other parts of the codebase are not thoroughly explored in the description. Overall, the statement is valid and clear but lacks some depth in addressing edge cases and broader impacts.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting the initialization logic of the `TensorDictModule` class in a single file (`tensordict/nn/common.py`). The changes involve modifying how `in_keys` are processed when provided as a dictionary, including the addition of a new parameter `out_to_in_map` to control the mapping direction. This requires a moderate understanding of the existing codebase, specifically how `TensorDictModule` handles input mappings and keyword arguments. Second, the technical concepts involved are not overly complex but do require familiarity with Python's dictionary and list handling, as well as an understanding of how keyword arguments are passed to functions or modules in this context. Third, while the problem statement does not explicitly mention edge cases, the code changes introduce potential complexities such as handling invalid or nested tuple/list structures, which may require additional validation logic. Finally, the impact on the system's architecture is minimal, as this is more of a feature enhancement than a structural change, though it does introduce a deprecation warning and a planned change in default behavior, which adds a small layer of complexity for future compatibility. Overall, this task requires understanding multiple concepts and making targeted but non-trivial modifications, placing it in the medium difficulty range at 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Missing support for some alignment keywords on absolutely positioned elements\nhttps://github.com/servo/servo/blob/11dfbd6f90bc7bc8b8d7485695548b5ad685d0e1/components/layout_2020/positioned.rs#L934-L943\r\n\r\nThis has some problems:\r\n - It handles `space-around` and `space-evenly`, which are content distribution keywords, even though here we can only have a self-alignment value\r\n - It's missing some values\r\n\r\nThe syntax is for `align-self` is:\r\n\r\n```\r\nauto | normal | stretch | <baseline-position> | <overflow-position>? <self-position>\r\n<baseline-position> = [ first | last ]? && baseline\r\n<overflow-position> = unsafe | safe\r\n<self-position> = center | start | end | self-start | self-end | flex-start | flex-end\r\n```\r\n\r\nThe syntax for `justify-self` also includes `left` and `right`.\r\n\r\nSo we need to handle:\r\n - `self-start`: like `start` in the block axis (because we lack `writing-mode`), but in the inline axis we need to check the`direction` of the abspos.\r\n - `self-end`: like `end` in the block axis (because we lack `writing-mode`), but in the inline axis we need to check the `direction` of the abspos.\r\n - `left`: we need to check the `direction` of the containing block\r\n - `right`: we need to check the `direction` of the containing block\r\n\r\n`WritingMode` has some methods that can be useful, e.g. if `line_left_is_inline_start()` then `left` should be like `start`. See https://github.com/servo/stylo/blob/main/style/logical_geometry.rs\r\n\r\nI think leaving the rest as `start` is fine (I should double-check at some point).\r\n\r\nTestcase:\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<style>\r\n.wrapper { display: inline-block; position: relative; width: 30px; height: 30px; border: 1px solid; }\r\n.abspos { position: absolute; height: 15px; width: 15px; inset: 0; background: cyan; }\r\n.ltr { direction: ltr }\r\n.rtl { direction: rtl }\r\n</style>\r\n<div class=\"wrapper ltr\"><div class=\"abspos ltr\" style=\"justify-self: left\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos ltr\" style=\"justify-self: right\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos rtl\" style=\"justify-self: left\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos rtl\" style=\"justify-self: right\"></div></div>\r\n<br>\r\n<div class=\"wrapper rtl\"><div class=\"abspos ltr\" style=\"justify-self: left\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos ltr\" style=\"justify-self: right\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos rtl\" style=\"justify-self: left\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos rtl\" style=\"justify-self: right\"></div></div>\r\n<br>\r\n<div class=\"wrapper ltr\"><div class=\"abspos ltr\" style=\"place-self: self-start\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos ltr\" style=\"place-self: self-end\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos rtl\" style=\"place-self: self-start\"></div></div>\r\n<div class=\"wrapper ltr\"><div class=\"abspos rtl\" style=\"place-self: self-end\"></div></div>\r\n<br>\r\n<div class=\"wrapper rtl\"><div class=\"abspos ltr\" style=\"place-self: self-start\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos ltr\" style=\"place-self: self-end\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos rtl\" style=\"place-self: self-start\"></div></div>\r\n<div class=\"wrapper rtl\"><div class=\"abspos rtl\" style=\"place-self: self-end\"></div></div>\r\n```\r\n\r\n| Expected | Actual |\r\n| - | - |\r\n| ![](https://github.com/user-attachments/assets/66b0c29c-35dd-4ff2-bb03-a4ea01cfb684) | ![](https://github.com/user-attachments/assets/e88380a8-7682-4d7b-a169-200f2e0900dc) |\r\n\n", "patch": "diff --git a/components/layout_2020/positioned.rs b/components/layout_2020/positioned.rs\nindex 397f8707b611b..5f5bfee52e9ec 100644\n--- a/components/layout_2020/positioned.rs\n+++ b/components/layout_2020/positioned.rs\n@@ -632,8 +632,20 @@ impl HoistedAbsolutelyPositionedBox {\n \n             let pb = pbm.padding + pbm.border;\n             let margin_rect_size = content_size + pbm.padding_border_sums + margin.sum();\n-            let inline_origin = inline_axis_solver.origin_for_margin_box(margin_rect_size.inline);\n-            let block_origin = block_axis_solver.origin_for_margin_box(margin_rect_size.block);\n+            let inline_origin = inline_axis_solver.origin_for_margin_box(\n+                AxisDirection::Inline,\n+                margin_rect_size.inline,\n+                style.writing_mode,\n+                shared_fragment.original_parent_writing_mode,\n+                containing_block_writing_mode,\n+            );\n+            let block_origin = block_axis_solver.origin_for_margin_box(\n+                AxisDirection::Block,\n+                margin_rect_size.block,\n+                style.writing_mode,\n+                shared_fragment.original_parent_writing_mode,\n+                containing_block_writing_mode,\n+            );\n \n             let content_rect = LogicalRect {\n                 start_corner: LogicalVec2 {\n@@ -868,12 +880,23 @@ impl<'a> AbsoluteAxisSolver<'a> {\n         result\n     }\n \n-    fn origin_for_margin_box(&self, size: Au) -> Au {\n-        let (alignment_container, flip_anchor) = match (\n+    fn origin_for_margin_box(\n+        &self,\n+        axis: AxisDirection,\n+        size: Au,\n+        self_writing_mode: WritingMode,\n+        original_parent_writing_mode: WritingMode,\n+        containing_block_writing_mode: WritingMode,\n+    ) -> Au {\n+        let (alignment_container, alignment_container_writing_mode, flip_anchor) = match (\n             self.box_offsets.start.non_auto(),\n             self.box_offsets.end.non_auto(),\n         ) {\n-            (None, None) => (self.static_position_rect_axis, self.flip_anchor),\n+            (None, None) => (\n+                self.static_position_rect_axis,\n+                original_parent_writing_mode,\n+                self.flip_anchor,\n+            ),\n             (Some(start), Some(end)) => {\n                 let start = start.to_used_value(self.containing_size);\n                 let end = end.to_used_value(self.containing_size);\n@@ -881,7 +904,7 @@ impl<'a> AbsoluteAxisSolver<'a> {\n                     origin: start,\n                     length: self.containing_size - (end + start),\n                 };\n-                (alignment_container, false)\n+                (alignment_container, containing_block_writing_mode, false)\n             },\n             // If a single offset is auto, for alignment purposes it resolves to the amount\n             // that makes the inset-modified containing block be exactly as big as the abspos.\n@@ -892,21 +915,49 @@ impl<'a> AbsoluteAxisSolver<'a> {\n             },\n         };\n \n+        assert_eq!(\n+            self_writing_mode.is_horizontal(),\n+            original_parent_writing_mode.is_horizontal(),\n+            \"Mixed horizontal and vertical writing modes are not supported yet\"\n+        );\n+        assert_eq!(\n+            self_writing_mode.is_horizontal(),\n+            containing_block_writing_mode.is_horizontal(),\n+            \"Mixed horizontal and vertical writing modes are not supported yet\"\n+        );\n+        let self_value_matches_container = || {\n+            axis == AxisDirection::Block ||\n+                self_writing_mode.is_bidi_ltr() == alignment_container_writing_mode.is_bidi_ltr()\n+        };\n+\n         // Here we resolve the alignment to either start, center, or end.\n         // Note we need to handle both self-alignment values (when some inset isn't auto)\n         // and distributed alignment values (when both insets are auto).\n         // The latter are treated as their fallback alignment.\n         let alignment = match self.alignment.value() {\n-            // https://drafts.csswg.org/css-align/#valdef-self-position-end\n-            // https://drafts.csswg.org/css-align/#valdef-self-position-flex-end\n-            AlignFlags::END | AlignFlags::FLEX_END => AlignFlags::END,\n             // https://drafts.csswg.org/css-align/#valdef-self-position-center\n             // https://drafts.csswg.org/css-align/#valdef-align-content-space-around\n             // https://drafts.csswg.org/css-align/#valdef-align-content-space-evenly\n             AlignFlags::CENTER | AlignFlags::SPACE_AROUND | AlignFlags::SPACE_EVENLY => {\n                 AlignFlags::CENTER\n             },\n-            // TODO(#34282): handle missing values: self-start, self-end, left, right.\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-self-start\n+            AlignFlags::SELF_START if self_value_matches_container() => AlignFlags::START,\n+            AlignFlags::SELF_START => AlignFlags::END,\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-self-end\n+            AlignFlags::SELF_END if self_value_matches_container() => AlignFlags::END,\n+            AlignFlags::SELF_END => AlignFlags::START,\n+            // https://drafts.csswg.org/css-align/#valdef-justify-content-left\n+            AlignFlags::LEFT if alignment_container_writing_mode.is_bidi_ltr() => AlignFlags::START,\n+            AlignFlags::LEFT => AlignFlags::END,\n+            // https://drafts.csswg.org/css-align/#valdef-justify-content-right\n+            AlignFlags::RIGHT if alignment_container_writing_mode.is_bidi_ltr() => AlignFlags::END,\n+            AlignFlags::RIGHT => AlignFlags::START,\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-end\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-flex-end\n+            AlignFlags::END | AlignFlags::FLEX_END => AlignFlags::END,\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-start\n+            // https://drafts.csswg.org/css-align/#valdef-self-position-flex-start\n             _ => AlignFlags::START,\n         };\n \n", "instance_id": "servo__servo-34365", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with missing support for certain alignment keywords on absolutely positioned elements in the Servo browser engine. It provides a detailed syntax for `align-self` and `justify-self`, specifies the behavior for keywords like `self-start`, `self-end`, `left`, and `right`, and includes a test case with expected and actual visual outputs. However, there are minor ambiguities and missing details. For instance, the statement mentions that \"leaving the rest as `start` is fine\" but lacks confirmation or deeper justification for this decision. Additionally, while the test case is helpful, edge cases (e.g., mixed writing modes or unsupported combinations) are not explicitly discussed, which could lead to uncertainty during implementation. Overall, the problem is valid and clear, but these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is relatively focused, primarily affecting the `positioned.rs` file and specifically the logic for handling alignment in absolutely positioned elements. However, the changes require a deep understanding of CSS alignment specifications, writing modes, and directionality (LTR/RTL), as well as their interaction with the Servo layout engine's internal representations (e.g., `WritingMode`, `AxisDirection`). The technical concepts involved include logical geometry in CSS, bidirectional text handling, and the specific behavior of alignment keywords, which are non-trivial and require domain-specific knowledge of web layout rendering. The code modifications are complex, involving conditional logic to handle different alignment values based on writing modes and directionality, as seen in the diff. Edge cases, such as mixed writing modes, are partially addressed in the code with assertions, but the problem statement does not fully specify how to handle all potential scenarios, adding to the challenge. While the changes do not significantly impact the broader system architecture, they require careful consideration of correctness in a critical rendering component. A score of 0.65 reflects the need for a solid grasp of both the codebase and CSS standards, combined with the complexity of the logic required to implement the solution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CryptoKey.algorithm should return a cached object\nFrom https://w3c.github.io/webcrypto/#cryptokey-interface-members:\r\n>Returns the [cached ECMAScript object](https://w3c.github.io/webcrypto/#concept-cached-object) associated with the [[[algorithm]]](https://w3c.github.io/webcrypto/#dfn-CryptoKey-slot-algorithm) internal slot.\r\n\r\nMeanwhile, the implementation creates a new instance each time it's called: https://github.com/servo/servo/blob/main/components/script/dom/cryptokey.rs#L106-L127\n", "patch": "diff --git a/components/script/dom/cryptokey.rs b/components/script/dom/cryptokey.rs\nindex 88c189feb81a3..cf0e287e518e0 100644\n--- a/components/script/dom/cryptokey.rs\n+++ b/components/script/dom/cryptokey.rs\n@@ -6,13 +6,12 @@ use std::cell::Cell;\n use std::ptr::NonNull;\n \n use dom_struct::dom_struct;\n-use js::jsapi::{JSObject, Value};\n+use js::jsapi::{Heap, JSObject, Value};\n+use js::rust::HandleObject;\n \n-use crate::dom::bindings::cell::DomRefCell;\n use crate::dom::bindings::codegen::Bindings::CryptoKeyBinding::{\n     CryptoKeyMethods, KeyType, KeyUsage,\n };\n-use crate::dom::bindings::codegen::Bindings::SubtleCryptoBinding::{AesKeyAlgorithm, KeyAlgorithm};\n use crate::dom::bindings::reflector::{reflect_dom_object, Reflector};\n use crate::dom::bindings::root::DomRoot;\n use crate::dom::bindings::str::DOMString;\n@@ -22,6 +21,7 @@ use crate::script_runtime::JSContext;\n \n /// The underlying cryptographic data this key represents\n #[allow(dead_code)]\n+#[derive(MallocSizeOf)]\n pub enum Handle {\n     Aes128(Vec<u8>),\n     Aes192(Vec<u8>),\n@@ -32,13 +32,26 @@ pub enum Handle {\n #[dom_struct]\n pub struct CryptoKey {\n     reflector_: Reflector,\n+\n+    /// <https://w3c.github.io/webcrypto/#dom-cryptokey-type>\n     key_type: KeyType,\n+\n+    /// <https://w3c.github.io/webcrypto/#dom-cryptokey-extractable>\n     extractable: Cell<bool>,\n-    // This would normally be KeyAlgorithm but we cannot Send DOMString, which\n-    // is a member of Algorithm\n-    algorithm: DomRefCell<String>,\n+\n+    /// The name of the algorithm used\n+    ///\n+    /// This is always the same as the `name` of the\n+    /// [`[[algorithm]]`](https://w3c.github.io/webcrypto/#dom-cryptokey-algorithm)\n+    /// internal slot, but we store it here again for convenience\n+    algorithm: DOMString,\n+\n+    /// <https://w3c.github.io/webcrypto/#dom-cryptokey-algorithm>\n+    #[ignore_malloc_size_of = \"Defined in mozjs\"]\n+    algorithm_object: Heap<*mut JSObject>,\n+\n+    /// <https://w3c.github.io/webcrypto/#dom-cryptokey-usages>\n     usages: Vec<KeyUsage>,\n-    #[ignore_malloc_size_of = \"Defined in external cryptography crates\"]\n     #[no_trace]\n     handle: Handle,\n }\n@@ -47,15 +60,16 @@ impl CryptoKey {\n     fn new_inherited(\n         key_type: KeyType,\n         extractable: bool,\n-        algorithm: KeyAlgorithm,\n         usages: Vec<KeyUsage>,\n+        algorithm: DOMString,\n         handle: Handle,\n     ) -> CryptoKey {\n         CryptoKey {\n             reflector_: Reflector::new(),\n             key_type,\n             extractable: Cell::new(extractable),\n-            algorithm: DomRefCell::new(algorithm.name.to_string()),\n+            algorithm,\n+            algorithm_object: Heap::default(),\n             usages,\n             handle,\n         }\n@@ -65,24 +79,29 @@ impl CryptoKey {\n         global: &GlobalScope,\n         key_type: KeyType,\n         extractable: bool,\n-        algorithm: KeyAlgorithm,\n+        algorithm: DOMString,\n+        algorithm_object: HandleObject,\n         usages: Vec<KeyUsage>,\n         handle: Handle,\n     ) -> DomRoot<CryptoKey> {\n-        reflect_dom_object(\n+        let object = reflect_dom_object(\n             Box::new(CryptoKey::new_inherited(\n                 key_type,\n                 extractable,\n-                algorithm,\n                 usages,\n+                algorithm,\n                 handle,\n             )),\n             global,\n-        )\n+        );\n+\n+        object.algorithm_object.set(algorithm_object.get());\n+\n+        object\n     }\n \n     pub fn algorithm(&self) -> String {\n-        self.algorithm.borrow().to_string()\n+        self.algorithm.to_string()\n     }\n \n     pub fn usages(&self) -> &[KeyUsage] {\n@@ -105,31 +124,9 @@ impl CryptoKeyMethods for CryptoKey {\n         self.extractable.get()\n     }\n \n-    #[allow(unsafe_code)]\n     /// <https://w3c.github.io/webcrypto/#cryptokey-interface-members>\n-    fn Algorithm(&self, cx: JSContext) -> NonNull<JSObject> {\n-        let parent = KeyAlgorithm {\n-            name: DOMString::from_string(self.algorithm()),\n-        };\n-        let algorithm = match self.handle() {\n-            Handle::Aes128(_) => AesKeyAlgorithm {\n-                parent,\n-                length: 128,\n-            },\n-            Handle::Aes192(_) => AesKeyAlgorithm {\n-                parent,\n-                length: 192,\n-            },\n-            Handle::Aes256(_) => AesKeyAlgorithm {\n-                parent,\n-                length: 256,\n-            },\n-        };\n-        unsafe {\n-            rooted!(in(*cx) let mut alg: Value);\n-            algorithm.to_jsval(*cx, alg.handle_mut());\n-            NonNull::new(alg.to_object()).unwrap()\n-        }\n+    fn Algorithm(&self, _cx: JSContext) -> NonNull<JSObject> {\n+        NonNull::new(self.algorithm_object.get()).unwrap()\n     }\n \n     #[allow(unsafe_code)]\ndiff --git a/components/script/dom/subtlecrypto.rs b/components/script/dom/subtlecrypto.rs\nindex 63088b8c7afc8..cb86981d30da3 100644\n--- a/components/script/dom/subtlecrypto.rs\n+++ b/components/script/dom/subtlecrypto.rs\n@@ -12,7 +12,7 @@ use aes::{Aes128, Aes192, Aes256};\n use base64::prelude::*;\n use dom_struct::dom_struct;\n use js::conversions::ConversionResult;\n-use js::jsapi::JSObject;\n+use js::jsapi::{JSObject, JS_NewObject};\n use js::jsval::ObjectValue;\n use js::rust::MutableHandleObject;\n use js::typedarray::ArrayBufferU8;\n@@ -25,8 +25,8 @@ use crate::dom::bindings::codegen::Bindings::CryptoKeyBinding::{\n     CryptoKeyMethods, KeyType, KeyUsage,\n };\n use crate::dom::bindings::codegen::Bindings::SubtleCryptoBinding::{\n-    AesCbcParams, AesCtrParams, AesKeyGenParams, Algorithm, AlgorithmIdentifier, JsonWebKey,\n-    KeyAlgorithm, KeyFormat, SubtleCryptoMethods,\n+    AesCbcParams, AesCtrParams, AesKeyAlgorithm, AesKeyGenParams, Algorithm, AlgorithmIdentifier,\n+    JsonWebKey, KeyAlgorithm, KeyFormat, SubtleCryptoMethods,\n };\n use crate::dom::bindings::codegen::UnionTypes::{\n     ArrayBufferViewOrArrayBuffer, ArrayBufferViewOrArrayBufferOrJsonWebKey,\n@@ -796,6 +796,7 @@ impl SubtleCrypto {\n \n     /// <https://w3c.github.io/webcrypto/#aes-cbc-operations>\n     /// <https://w3c.github.io/webcrypto/#aes-ctr-operations>\n+    #[allow(unsafe_code)]\n     fn generate_key_aes(\n         &self,\n         usages: Vec<KeyUsage>,\n@@ -827,25 +828,40 @@ impl SubtleCrypto {\n             _ => return Err(Error::NotSupported),\n         };\n \n-        Ok(CryptoKey::new(\n+        let cx = GlobalScope::get_cx();\n+        rooted!(in(*cx) let mut algorithm_object = unsafe {JS_NewObject(*cx, ptr::null()) });\n+        assert!(!algorithm_object.is_null());\n+\n+        AesKeyAlgorithm::from_name_and_size(\n+            name.clone(),\n+            key_gen_params.length,\n+            algorithm_object.handle_mut(),\n+            cx,\n+        );\n+\n+        let crypto_key = CryptoKey::new(\n             &self.global(),\n             KeyType::Secret,\n             extractable,\n-            KeyAlgorithm { name },\n+            name,\n+            algorithm_object.handle(),\n             usages,\n             handle,\n-        ))\n+        );\n+\n+        Ok(crypto_key)\n     }\n \n     /// <https://w3c.github.io/webcrypto/#aes-cbc-operations>\n     /// <https://w3c.github.io/webcrypto/#aes-ctr-operations>\n+    #[allow(unsafe_code)]\n     fn import_key_aes(\n         &self,\n         format: KeyFormat,\n         data: &[u8],\n         extractable: bool,\n         usages: Vec<KeyUsage>,\n-        alg: &str,\n+        alg_name: &str,\n     ) -> Result<DomRoot<CryptoKey>, Error> {\n         if usages.iter().any(|usage| {\n             !matches!(\n@@ -865,15 +881,30 @@ impl SubtleCrypto {\n             256 => Handle::Aes256(data.to_vec()),\n             _ => return Err(Error::Data),\n         };\n-        let name = DOMString::from(alg);\n-        Ok(CryptoKey::new(\n+\n+        let name = DOMString::from(alg_name.to_string());\n+\n+        let cx = GlobalScope::get_cx();\n+        rooted!(in(*cx) let mut algorithm_object = unsafe {JS_NewObject(*cx, ptr::null()) });\n+        assert!(!algorithm_object.is_null());\n+\n+        AesKeyAlgorithm::from_name_and_size(\n+            name.clone(),\n+            (data.len() * 8) as u16,\n+            algorithm_object.handle_mut(),\n+            cx,\n+        );\n+        let crypto_key = CryptoKey::new(\n             &self.global(),\n             KeyType::Secret,\n             extractable,\n-            KeyAlgorithm { name },\n+            name,\n+            algorithm_object.handle(),\n             usages,\n             handle,\n-        ))\n+        );\n+\n+        Ok(crypto_key)\n     }\n \n     /// <https://w3c.github.io/webcrypto/#aes-cbc-operations>\n@@ -939,3 +970,19 @@ fn data_to_jwk_params(alg: &str, size: &str, key: &[u8]) -> (DOMString, DOMStrin\n     data.retain(|c| c != '=');\n     (jwk_alg, DOMString::from(data))\n }\n+\n+impl AesKeyAlgorithm {\n+    /// Fill the object referenced by `out` with an [AesKeyAlgorithm]\n+    /// of the specified name and size.\n+    #[allow(unsafe_code)]\n+    fn from_name_and_size(name: DOMString, size: u16, out: MutableHandleObject, cx: JSContext) {\n+        let key_algorithm = Self {\n+            parent: KeyAlgorithm { name: name.clone() },\n+            length: size,\n+        };\n+\n+        unsafe {\n+            key_algorithm.to_jsobject(*cx, out);\n+        }\n+    }\n+}\n", "instance_id": "servo__servo-34092", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent: it highlights a discrepancy between the expected behavior of `CryptoKey.algorithm` (returning a cached object as per the WebCrypto specification) and the current implementation (creating a new instance each time). It provides a direct reference to the relevant specification and points to the specific code section in the repository that needs modification. However, the statement lacks detailed guidance on how the caching should be implemented, what specific object structure is expected to be cached, and whether there are any edge cases or constraints to consider (e.g., lifetime of the cached object, thread safety). Additionally, there are no examples or test cases provided to validate the expected behavior. Thus, while the goal is clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves modifications across two files (`cryptokey.rs` and `subtlecrypto.rs`), requiring an understanding of how these components interact within the Servo codebase. The changes are not trivial; they involve replacing a dynamic creation of an algorithm object with a cached instance using a `Heap` structure, which suggests familiarity with Rust's memory management and the `js` crate's handling of JavaScript objects. Second, the technical concepts required include Rust's ownership model, unsafe code handling (due to interactions with JavaScript objects via `JS_NewObject` and related APIs), and domain-specific knowledge of the WebCrypto API specification. Third, while the problem statement does not explicitly mention edge cases, the code changes imply potential considerations around the lifetime and mutability of the cached object, as well as ensuring thread safety in a browser context, which adds moderate complexity. Finally, the changes do not appear to impact the broader system architecture significantly but do require careful integration with existing code to avoid introducing bugs in the JavaScript-Rust bridge. Overall, this problem requires understanding multiple concepts and making moderately complex modifications, justifying a difficulty score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add severity check to LoggerInterface\n## What\r\n\r\nProvide a mechanism for the middleware to ask the `LoggerInterface` if the message they are about to construct will be logged, so they can avoid constructing the string if not neccesary.\r\n\r\n## Why\r\n\r\nConstructing log strings can be expensive, especially in hot code paths.  Doing so only for the log severity level to be such that the string is never printed is suboptimal, to say the least.\r\n\r\nSee review conversation https://github.com/OpenAssetIO/OpenAssetIO/pull/1011#discussion_r1256109927\r\n\r\n## Acceptance Criteria\r\n- New method `isSeverityLogged(Severity)` added to `LoggerInterface`\r\n- Default implementation for this method that returns true.\r\n- Implement and obey this additional `LoggerInteface` flag in addition to current `maxSeverity` in `SeverityFilter`\r\n\nAdd severity check to LoggerInterface\n## What\r\n\r\nProvide a mechanism for the middleware to ask the `LoggerInterface` if the message they are about to construct will be logged, so they can avoid constructing the string if not neccesary.\r\n\r\n## Why\r\n\r\nConstructing log strings can be expensive, especially in hot code paths.  Doing so only for the log severity level to be such that the string is never printed is suboptimal, to say the least.\r\n\r\nSee review conversation https://github.com/OpenAssetIO/OpenAssetIO/pull/1011#discussion_r1256109927\r\n\r\n## Acceptance Criteria\r\n- New method `isSeverityLogged(Severity)` added to `LoggerInterface`\r\n- Default implementation for this method that returns true.\r\n- Implement and obey this additional `LoggerInteface` flag in addition to current `maxSeverity` in `SeverityFilter`\r\n\n", "patch": "diff --git a/RELEASE_NOTES.md b/RELEASE_NOTES.md\nindex f2b4ed379..bea5a48de 100644\n--- a/RELEASE_NOTES.md\n+++ b/RELEASE_NOTES.md\n@@ -15,6 +15,14 @@ v1.0.0-beta.x.y\n   a `CppPluginSystemManagerPlugin` object.\n   [#1115](https://github.com/OpenAssetIO/OpenAssetIO/issues/1115)\n \n+- Added `LoggerInterface.isSeverityLogged` to allow users to\n+  short-circuit log message construction if the logger indicates the\n+  intended severity will be filtered. Updated `SeverityFilter` to use\n+  the most pessimistic of its own severity and that of the wrapped\n+  logger.\n+  [#1014](https://github.com/OpenAssetIO/OpenAssetIO/issues/1014)\n+\n+\n v1.0.0-beta.2.1\n ---------------\n \ndiff --git a/src/openassetio-core/include/openassetio/log/LoggerInterface.hpp b/src/openassetio-core/include/openassetio/log/LoggerInterface.hpp\nindex 4674843ef..44315bf27 100644\n--- a/src/openassetio-core/include/openassetio/log/LoggerInterface.hpp\n+++ b/src/openassetio-core/include/openassetio/log/LoggerInterface.hpp\n@@ -50,13 +50,39 @@ class OPENASSETIO_CORE_EXPORT LoggerInterface {\n    * This method must be implemented to present the supplied message\n    * to the user in an appropriate fashion.\n    *\n-   * @param message The message string to be logged.\n-   *\n    * @param severity One of the severity constants defined in @ref\n    * Severity.\n+   *\n+   * @param message The message string to be logged.\n    */\n   virtual void log(Severity severity, const Str& message) = 0;\n \n+  /**\n+   * Check if a given severity level should/will be filtered out.\n+   *\n+   * The implementation of the logger may have a mechanism by which\n+   * certain severity levels are not output. If a severity level is not\n+   * output, then constructing a string to pass to the logger is wasted\n+   * effort. This method can be queried before constructing a complex\n+   * string, in order to avoid that wasted effort.\n+   *\n+   * Implementors of LoggerInterface subclasses should override this\n+   * method if they wish to conditionally skip logging at particular\n+   * severity levels.\n+   *\n+   * If @ref log is called regardless, with a severity that elicits a\n+   * `false` response from this method, then the logger may still output\n+   * the message, but it is not guaranteed (and is discouraged).\n+   *\n+   * The default implementation returns `true` for all severities.\n+   *\n+   * @param severity Severity level to check.\n+   *\n+   * @return Whether a message will be output if `log` is called\n+   * with the given severity.\n+   */\n+  [[nodiscard]] virtual bool isSeverityLogged(Severity severity) const;\n+\n   /**\n    * @name Conveniences\n    * @{\ndiff --git a/src/openassetio-core/include/openassetio/log/SeverityFilter.hpp b/src/openassetio-core/include/openassetio/log/SeverityFilter.hpp\nindex 55592afbf..b1a82655c 100644\n--- a/src/openassetio-core/include/openassetio/log/SeverityFilter.hpp\n+++ b/src/openassetio-core/include/openassetio/log/SeverityFilter.hpp\n@@ -62,10 +62,35 @@ class OPENASSETIO_CORE_EXPORT SeverityFilter final : public LoggerInterface {\n    */\n   [[nodiscard]] LoggerInterface::Severity getSeverity() const;\n \n+  /**\n+   * Check if given severity will be logged.\n+   *\n+   * Uses the value of getSeverity() as well as querying the @ref\n+   * upstreamLogger, and returns the most pessimistic answer.\n+   *\n+   * This logic is used in @ref log to determine which messages to\n+   * filter out.\n+   *\n+   * @param severity Severity to check.\n+   *\n+   * @return Whether a log message at the given severity will be output.\n+   */\n+  [[nodiscard]] bool isSeverityLogged(Severity severity) const override;\n   /**\n    * @}\n    */\n \n+  /**\n+   * Filter out messages based on severity before delegating to the\n+   * @ref upstreamLogger.\n+   *\n+   * Whether a log is output or not obeys the result of @ref\n+   * isSeverityLogged\n+   *\n+   * @param severity Severity level.\n+   *\n+   * @param message The message to be logged.\n+   */\n   void log(Severity severity, const Str& message) override;\n \n  private:\ndiff --git a/src/openassetio-core/src/log/LoggerInterface.cpp b/src/openassetio-core/src/log/LoggerInterface.cpp\nindex bba8ab0e9..e8470c9e8 100644\n--- a/src/openassetio-core/src/log/LoggerInterface.cpp\n+++ b/src/openassetio-core/src/log/LoggerInterface.cpp\n@@ -10,6 +10,10 @@ inline namespace OPENASSETIO_CORE_ABI_VERSION {\n namespace log {\n LoggerInterface::~LoggerInterface() = default;\n \n+bool LoggerInterface::isSeverityLogged([[maybe_unused]] LoggerInterface::Severity severity) const {\n+  return true;\n+}\n+\n void LoggerInterface::debugApi(const Str &message) { log(Severity::kDebugApi, message); }\n \n void LoggerInterface::debug(const Str &message) { log(Severity::kDebug, message); }\ndiff --git a/src/openassetio-core/src/log/SeverityFilter.cpp b/src/openassetio-core/src/log/SeverityFilter.cpp\nindex b4996bcda..d3ace0acc 100644\n--- a/src/openassetio-core/src/log/SeverityFilter.cpp\n+++ b/src/openassetio-core/src/log/SeverityFilter.cpp\n@@ -34,7 +34,7 @@ SeverityFilter::SeverityFilter(LoggerInterfacePtr upstreamLogger)\n }\n \n void SeverityFilter::log(Severity severity, const Str& message) {\n-  if (severity < minSeverity_) {\n+  if (severity < minSeverity_ || !upstreamLogger_->isSeverityLogged(severity)) {\n     return;\n   }\n   upstreamLogger_->log(severity, message);\n@@ -45,6 +45,10 @@ void SeverityFilter::setSeverity(LoggerInterface::Severity severity) { minSeveri\n LoggerInterface::Severity SeverityFilter::getSeverity() const { return minSeverity_; }\n \n LoggerInterfacePtr SeverityFilter::upstreamLogger() const { return upstreamLogger_; }\n+\n+bool SeverityFilter::isSeverityLogged(LoggerInterface::Severity severity) const {\n+  return severity >= minSeverity_ && upstreamLogger_->isSeverityLogged(severity);\n+}\n }  // namespace log\n }  // namespace OPENASSETIO_CORE_ABI_VERSION\n }  // namespace openassetio\ndiff --git a/src/openassetio-python/cmodule/src/log/LoggerInterfaceBinding.cpp b/src/openassetio-python/cmodule/src/log/LoggerInterfaceBinding.cpp\nindex 96c53d134..8fccaa30e 100644\n--- a/src/openassetio-python/cmodule/src/log/LoggerInterfaceBinding.cpp\n+++ b/src/openassetio-python/cmodule/src/log/LoggerInterfaceBinding.cpp\n@@ -21,6 +21,10 @@ struct PyLoggerInterface : LoggerInterface {\n   void log(Severity severity, const Str& message) override {\n     OPENASSETIO_PYBIND11_OVERRIDE_PURE(void, LoggerInterface, log, severity, message);\n   }\n+\n+  [[nodiscard]] bool isSeverityLogged(Severity severity) const override {\n+    OPENASSETIO_PYBIND11_OVERRIDE(bool, LoggerInterface, isSeverityLogged, severity);\n+  }\n };\n }  // namespace log\n }  // namespace OPENASSETIO_CORE_ABI_VERSION\n@@ -50,6 +54,8 @@ void registerLoggerInterface(const py::module& mod) {\n   loggerInterface.def(py::init())\n       .def(\"log\", &LoggerInterface::log, py::arg(\"severity\"), py::arg(\"message\"),\n            py::call_guard<py::gil_scoped_release>{})\n+      .def(\"isSeverityLogged\", &LoggerInterface::isSeverityLogged, py::arg(\"severity\"),\n+           py::call_guard<py::gil_scoped_release>{})\n       .def(\"debugApi\", &LoggerInterface::debugApi, py::arg(\"message\"),\n            py::call_guard<py::gil_scoped_release>{})\n       .def(\"debug\", &LoggerInterface::debug, py::arg(\"message\"),\n", "instance_id": "OpenAssetIO__OpenAssetIO-1296", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in defining the goal: to add a mechanism in the `LoggerInterface` to check if a log message of a specific severity will be logged, thereby avoiding unnecessary string construction. The \"What\" and \"Why\" sections provide a clear rationale for the change, and the \"Acceptance Criteria\" outline specific requirements, such as adding a new method `isSeverityLogged(Severity)` and updating the `SeverityFilter` class. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., behavior when severity levels are dynamically changed) or how the default implementation should interact with custom logger implementations. Additionally, while the intent is clear, the statement repeats itself (the \"What\" and \"Why\" sections are duplicated), which slightly detracts from its conciseness. Overall, the statement is valid and mostly clear but lacks some minor details that could affect implementation decisions.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting the `LoggerInterface` and `SeverityFilter` classes across a few files in a C++ codebase, with corresponding updates to Python bindings. The modifications involve adding a new virtual method with a default implementation, updating logic in `SeverityFilter` to respect both its own severity threshold and the upstream logger's filtering, and documenting the change in release notes. The changes do not significantly impact the broader system architecture, as they are confined to the logging subsystem and are additive in nature (no major refactoring required). The amount of code change is moderate, involving both header and source file updates, but it is straightforward.\n\n2. **Number of Technical Concepts**: The problem requires understanding of C++ virtual functions and inheritance, as well as how logging systems work with severity levels. Familiarity with the project's logging architecture (e.g., the role of `SeverityFilter` as a middleware) is necessary but not overly complex. Additionally, the Python binding updates require basic knowledge of PyBind11, though the changes are mechanical. No advanced algorithms, design patterns, or domain-specific knowledge beyond logging are needed. The concepts involved are relatively basic for a senior engineer.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes and documentation imply considerations such as ensuring the default implementation of `isSeverityLogged` is safe (always returning `true`) and handling scenarios where the upstream logger and `SeverityFilter` have conflicting severity thresholds (resolved by taking the most pessimistic approach). The error handling requirements are minimal, as the feature is primarily about optimization rather than robustness. Implementing this logic does not introduce significant complexity in terms of edge case handling.\n\n4. **Overall Complexity**: The task requires understanding the intent of avoiding unnecessary string construction and implementing a simple check mechanism. While it spans multiple files and languages (C++ and Python bindings), the logic is straightforward\u2014adding a method, updating filtering logic, and ensuring compatibility. There are no performance-critical optimizations or deep architectural changes involved, making this a relatively simple feature addition.\n\nA score of 0.35 reflects that this task is slightly more involved than a trivial change (e.g., fixing a typo) due to the need to coordinate changes across multiple files and ensure correct behavior in a logging system, but it remains in the easy category as it does not demand deep architectural knowledge or complex problem-solving.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Return different codes based on findings\nWe'd like to use zizmor to gate PRs and prevent GHA code from being merged if it has serious security bugs. Currently, I am outputting the json and then using jq to parse the output and then checking the result in bash. \r\n\r\nWhat would be more ideal is to use zizmor's return code (`$?`). I think most other linters, scanners, etc return non-zero codes based on results. I might suggest returning 0 for any low/medium priority findings and 1 for high. And then maybe in pedantic mode, returning 1 for any findings.\r\n\r\nGreat tool by the way. We have been looking for something like this for a while and I'm trying to learn rust so I can contribute.\n", "patch": "diff --git a/docs/usage.md b/docs/usage.md\nindex 20bf3dc..6c17249 100644\n--- a/docs/usage.md\n+++ b/docs/usage.md\n@@ -38,6 +38,22 @@ zizmor --format sarif\n \n See [Integration](#integration) for suggestions on when to use each format.\n \n+## Exit codes\n+\n+`zizmor` uses various exit codes to summarize the results of a run:\n+\n+| Code | Meaning |\n+| ---- | ------- |\n+| 0    | Successful audit; no findings to report. |\n+| 1    | Error during audit; consult output. |\n+| 10   | One or more findings found; highest finding is \"unknown\" level. |\n+| 11   | One or more findings found; highest finding is \"informational\" level. |\n+| 12   | One or more findings found; highest finding is \"low\" level. |\n+| 13   | One or more findings found; highest finding is \"medium\" level. |\n+| 14   | One or more findings found; highest finding is \"high\" level. |\n+\n+All other exit codes are currently reserved.\n+\n ## Ignoring results\n \n `zizmor`'s defaults are not always 100% right for every possible use case.\ndiff --git a/src/finding/mod.rs b/src/finding/mod.rs\nindex 8f32908..fb81695 100644\n--- a/src/finding/mod.rs\n+++ b/src/finding/mod.rs\n@@ -22,7 +22,7 @@ pub(crate) enum Confidence {\n     High,\n }\n \n-#[derive(Copy, Clone, Debug, Default, Eq, Hash, PartialEq, Serialize)]\n+#[derive(Copy, Clone, Debug, Default, Eq, Hash, Ord, PartialOrd, PartialEq, Serialize)]\n pub(crate) enum Severity {\n     #[default]\n     Unknown,\ndiff --git a/src/main.rs b/src/main.rs\nindex 53d9180..bd8f179 100644\n--- a/src/main.rs\n+++ b/src/main.rs\n@@ -1,4 +1,4 @@\n-use std::{io::stdout, path::PathBuf, time::Duration};\n+use std::{io::stdout, path::PathBuf, process::ExitCode, time::Duration};\n \n use anyhow::{anyhow, Context, Result};\n use audit::WorkflowAudit;\n@@ -6,7 +6,7 @@ use clap::{Parser, ValueEnum};\n use config::Config;\n use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};\n use owo_colors::OwoColorize;\n-use registry::{AuditRegistry, WorkflowRegistry};\n+use registry::{AuditRegistry, FindingRegistry, WorkflowRegistry};\n use state::AuditState;\n \n mod audit;\n@@ -66,7 +66,7 @@ pub(crate) enum OutputFormat {\n     Sarif,\n }\n \n-fn main() -> Result<()> {\n+fn run() -> Result<ExitCode> {\n     human_panic::setup_panic!();\n \n     let args = App::parse();\n@@ -156,7 +156,7 @@ fn main() -> Result<()> {\n         );\n     }\n \n-    let mut results = vec![];\n+    let mut results = FindingRegistry::new(&config);\n     for (_, workflow) in workflow_registry.iter_workflows() {\n         bar.set_message(format!(\n             \"auditing {workflow}\",\n@@ -177,10 +177,6 @@ fn main() -> Result<()> {\n         ));\n     }\n \n-    // TODO: Suboptimal; should probably use `extract_if` once stabilized.\n-    // See: https://github.com/rust-lang/rust/issues/43244\n-    let (ignored, results): (Vec<_>, Vec<_>) = results.into_iter().partition(|r| config.ignores(r));\n-\n     bar.finish_and_clear();\n \n     let format = match args.format {\n@@ -189,11 +185,22 @@ fn main() -> Result<()> {\n     };\n \n     match format {\n-        OutputFormat::Plain => render::render_findings(&workflow_registry, &results, &ignored),\n-        OutputFormat::Json => serde_json::to_writer_pretty(stdout(), &results)?,\n-        OutputFormat::Sarif => {\n-            serde_json::to_writer_pretty(stdout(), &sarif::build(&workflow_registry, results))?\n-        }\n+        OutputFormat::Plain => render::render_findings(&workflow_registry, &results),\n+        OutputFormat::Json => serde_json::to_writer_pretty(stdout(), &results.findings())?,\n+        OutputFormat::Sarif => serde_json::to_writer_pretty(\n+            stdout(),\n+            &sarif::build(&workflow_registry, results.findings()),\n+        )?,\n     };\n-    Ok(())\n+\n+    Ok(results.into())\n+}\n+\n+fn main() -> ExitCode {\n+    // This is a little silly, but returning an ExitCode like this ensures\n+    // we always exit cleanly, rather than performing a hard process exit.\n+    match run() {\n+        Ok(exit) => exit,\n+        Err(_) => ExitCode::FAILURE,\n+    }\n }\ndiff --git a/src/registry.rs b/src/registry.rs\nindex 54fe490..06b5eb0 100644\n--- a/src/registry.rs\n+++ b/src/registry.rs\n@@ -1,11 +1,16 @@\n //! Functionality for registering and managing the lifecycles of\n //! audits.\n \n-use std::{collections::HashMap, path::Path};\n+use std::{collections::HashMap, path::Path, process::ExitCode};\n \n use anyhow::{anyhow, Result};\n \n-use crate::{audit::WorkflowAudit, models::Workflow};\n+use crate::{\n+    audit::WorkflowAudit,\n+    config::Config,\n+    finding::{Finding, Severity},\n+    models::Workflow,\n+};\n \n pub(crate) struct WorkflowRegistry {\n     pub(crate) workflows: HashMap<String, Workflow>,\n@@ -100,3 +105,68 @@ impl AuditRegistry {\n         self.workflow_audits.iter_mut()\n     }\n }\n+\n+/// A registry of all findings discovered during a `zizmor` run.\n+pub(crate) struct FindingRegistry<'a> {\n+    config: &'a Config,\n+    ignored: Vec<Finding<'a>>,\n+    findings: Vec<Finding<'a>>,\n+    highest_severity: Option<Severity>,\n+}\n+\n+impl<'a> FindingRegistry<'a> {\n+    pub(crate) fn new(config: &'a Config) -> Self {\n+        Self {\n+            config,\n+            ignored: Default::default(),\n+            findings: Default::default(),\n+            highest_severity: None,\n+        }\n+    }\n+\n+    /// Adds one or more findings to the current findings set,\n+    /// filtering with the configuration in the process.\n+    pub(crate) fn extend(&mut self, results: Vec<Finding<'a>>) {\n+        // TODO: is it faster to iterate like this, or do `find_by_max`\n+        // and then `extend`?\n+        for result in results {\n+            if self.config.ignores(&result) {\n+                self.ignored.push(result);\n+            } else {\n+                if self\n+                    .highest_severity\n+                    .map_or(true, |s| result.determinations.severity > s)\n+                {\n+                    self.highest_severity = Some(result.determinations.severity);\n+                }\n+\n+                self.findings.push(result);\n+            }\n+        }\n+    }\n+\n+    /// All non-filtered findings.\n+    pub(crate) fn findings(&self) -> &[Finding<'a>] {\n+        &self.findings\n+    }\n+\n+    /// All filtered findings.\n+    pub(crate) fn ignored(&self) -> &[Finding<'a>] {\n+        &self.ignored\n+    }\n+}\n+\n+impl From<FindingRegistry<'_>> for ExitCode {\n+    fn from(value: FindingRegistry<'_>) -> Self {\n+        match value.highest_severity {\n+            Some(sev) => match sev {\n+                Severity::Unknown => ExitCode::from(10),\n+                Severity::Informational => ExitCode::from(11),\n+                Severity::Low => ExitCode::from(12),\n+                Severity::Medium => ExitCode::from(13),\n+                Severity::High => ExitCode::from(14),\n+            },\n+            None => ExitCode::SUCCESS,\n+        }\n+    }\n+}\ndiff --git a/src/render.rs b/src/render.rs\nindex 021b06f..0d1f9c9 100644\n--- a/src/render.rs\n+++ b/src/render.rs\n@@ -4,7 +4,7 @@ use std::collections::{hash_map::Entry, HashMap};\n \n use crate::{\n     finding::{Finding, Location, Severity},\n-    registry::WorkflowRegistry,\n+    registry::{FindingRegistry, WorkflowRegistry},\n };\n use annotate_snippets::{Level, Renderer, Snippet};\n use anstream::println;\n@@ -66,26 +66,22 @@ pub(crate) fn finding_snippet<'w>(\n     snippets\n }\n \n-pub(crate) fn render_findings(\n-    registry: &WorkflowRegistry,\n-    findings: &[Finding],\n-    ignored: &[Finding],\n-) {\n-    for finding in findings {\n+pub(crate) fn render_findings(registry: &WorkflowRegistry, findings: &FindingRegistry) {\n+    for finding in findings.findings() {\n         render_finding(registry, finding);\n         println!();\n     }\n \n-    if findings.is_empty() {\n+    if findings.findings().is_empty() {\n         println!(\n             \"{no_findings} ({nignored} ignored)\",\n             no_findings = \"No findings to report. Good job!\".green(),\n-            nignored = ignored.len().bright_yellow()\n+            nignored = findings.ignored().len().bright_yellow()\n         );\n     } else {\n         let mut findings_by_severity = HashMap::new();\n \n-        for finding in findings {\n+        for finding in findings.findings() {\n             match findings_by_severity.entry(&finding.determinations.severity) {\n                 Entry::Occupied(mut e) => {\n                     *e.get_mut() += 1;\n@@ -98,8 +94,8 @@ pub(crate) fn render_findings(\n \n         println!(\n             \"{nfindings} findings ({nignored} ignored): {nunknown} unknown, {ninformational} informational, {nlow} low, {nmedium} medium, {nhigh} high\",\n-            nfindings = (findings.len() + ignored.len()).green(),\n-            nignored = ignored.len().bright_yellow(),\n+            nfindings = (findings.findings().len() + findings.ignored().len()).green(),\n+            nignored = findings.ignored().len().bright_yellow(),\n             nunknown = findings_by_severity.get(&Severity::Unknown).unwrap_or(&0),\n             ninformational = findings_by_severity.get(&Severity::Informational).unwrap_or(&0).purple(),\n             nlow = findings_by_severity.get(&Severity::Low).unwrap_or(&0).cyan(),\ndiff --git a/src/sarif.rs b/src/sarif.rs\nindex d15f98f..94b2c97 100644\n--- a/src/sarif.rs\n+++ b/src/sarif.rs\n@@ -10,7 +10,7 @@ use crate::{\n     registry::WorkflowRegistry,\n };\n \n-pub(crate) fn build(registry: &WorkflowRegistry, findings: Vec<Finding<'_>>) -> Sarif {\n+pub(crate) fn build(registry: &WorkflowRegistry, findings: &[Finding]) -> Sarif {\n     Sarif::builder()\n         .version(\"2.1.0\")\n         .schema(\"https://docs.oasis-open.org/sarif/sarif/v2.1.0/errata01/os/schemas/sarif-external-property-file-schema-2.1.0.json\")\n@@ -18,7 +18,7 @@ pub(crate) fn build(registry: &WorkflowRegistry, findings: Vec<Finding<'_>>) ->\n         .build()\n }\n \n-fn build_run(registry: &WorkflowRegistry, findings: Vec<Finding<'_>>) -> Run {\n+fn build_run(registry: &WorkflowRegistry, findings: &[Finding]) -> Run {\n     Run::builder()\n         .tool(\n             Tool::builder()\n@@ -37,7 +37,7 @@ fn build_run(registry: &WorkflowRegistry, findings: Vec<Finding<'_>>) -> Run {\n         .build()\n }\n \n-fn build_results(registry: &WorkflowRegistry, findings: Vec<Finding<'_>>) -> Vec<SarifResult> {\n+fn build_results(registry: &WorkflowRegistry, findings: &[Finding]) -> Vec<SarifResult> {\n     findings.iter().map(|f| build_result(registry, f)).collect()\n }\n \n", "instance_id": "woodruffw__zizmor-133", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in expressing the goal: to modify the `zizmor` tool to return specific exit codes based on the severity of findings during a security audit, rather than relying on external parsing with `jq` and bash scripts. The desired behavior is outlined with a suggestion to return 0 for low/medium findings and 1 for high findings, with an additional mode for pedantic checks. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define what constitutes \"low/medium/high\" severity (though this might be inferred from the codebase), nor does it specify how the pedantic mode should be toggled or configured. Additionally, there are no examples of input/output or specific constraints on how the exit codes should interact with other parts of the system. Despite these minor gaps, the intent and primary requirements are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`main.rs`, `registry.rs`, `finding/mod.rs`, `render.rs`, `sarif.rs`, and documentation in `usage.md`), requiring a moderate understanding of the codebase structure and interactions between components like finding registration, severity handling, and output rendering. The changes involve introducing a new `FindingRegistry` struct to manage findings and their severities, modifying the main function to return an `ExitCode` based on the highest severity, and updating rendering logic to reflect filtered and ignored findings. This requires understanding Rust-specific concepts such as `ExitCode`, trait implementations (`From` for conversion to `ExitCode`), and lifetime annotations, as well as domain-specific logic related to security audit severity levels. However, the problem does not appear to impact the core architecture significantly, nor does it introduce complex algorithms or performance-critical modifications. Edge cases, such as handling empty findings or invalid configurations, are implicitly addressed in the code changes but not explicitly mentioned in the problem statement, adding a slight layer of complexity. Overall, this task requires a moderate level of expertise in Rust and familiarity with the codebase, but it does not demand advanced or highly specialized knowledge, placing it at the lower end of the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[SETS] discard : Remove the specified item\n### Feature Description\r\n\r\n**Add support for discard() method for sets in the Python translation.**\r\n`discard(element)` : Removes an element from the set if it is a member. If the element is not a member, it does nothing.\r\n\r\n### Test Code\r\n\r\nThis Code Should Work \r\n\r\n```python\r\nmy_set = {1, 2, 3}\r\nmy_set.discard(2)\r\nprint(my_set)\r\n>>> {1, 3}\r\n```\r\n\r\n### Proposed Solution\r\nSee description in #1531 \n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex aeeb45cffd..c2ae1c053d 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -13,6 +13,7 @@ All notable changes to this project will be documented in this file.\n -   #1740 : Add Python support for set method `copy()`.\n -   #1750 : Add Python support for set method `remove()`.\n -   #1787 : Ensure `STC` is installed with Pyccel.\n+-   #1743 : Add Python support for set method `discard()`.\n \n ### Fixed\n \n@@ -67,7 +68,7 @@ All notable changes to this project will be documented in this file.\n -   #1425 : Add support for `numpy.isnan`, `numpy.isinf` and `numpy.isfinite`.\n -   #1738 : Add Python support for creating scalar sets with `{}`.\n -   #1738 : Add Python support for set method `add`.\n--   #1749 : Add Python support for set method `pop()`\n+-   #1749 : Add Python support for set method `pop()`.\n \n ### Fixed\n \ndiff --git a/pyccel/ast/builtin_methods/set_methods.py b/pyccel/ast/builtin_methods/set_methods.py\nindex 81575075c3..feba49d7ff 100644\n--- a/pyccel/ast/builtin_methods/set_methods.py\n+++ b/pyccel/ast/builtin_methods/set_methods.py\n@@ -13,8 +13,15 @@\n from pyccel.ast.internals import PyccelInternalFunction\n from pyccel.ast.basic import TypedAstNode\n \n-__all__ = ('SetAdd', 'SetClear', 'SetMethod', 'SetCopy', 'SetPop', 'SetRemove')\n-\n+__all__ = (\n+    'SetAdd',\n+    'SetClear',\n+    'SetCopy',\n+    'SetDiscard',\n+    'SetMethod',\n+    'SetPop',\n+    'SetRemove'\n+)\n \n class SetMethod(PyccelInternalFunction):\n     \"\"\"\n@@ -125,6 +132,7 @@ def __init__(self, set_variable):\n         self._class_type = set_variable._class_type\n         super().__init__(set_variable)\n \n+\n class SetPop(SetMethod):\n     \"\"\"\n     Represents a call to the .pop() method.\n@@ -150,6 +158,7 @@ def __init__(self, set_variable):\n         self._class_type = set_variable.class_type.element_type\n         super().__init__(set_variable)\n \n+\n class SetRemove(SetMethod):\n     \"\"\"\n     Represents a call to the .remove() method.\n@@ -183,3 +192,37 @@ def __init__(self, set_variable, item) -> None:\n         if not is_homogeneous:\n             raise TypeError(f\"Can't remove an element of type {item.dtype} from a set of {set_variable.dtype}\")\n         super().__init__(set_variable, item)\n+\n+\n+class SetDiscard(SetMethod):\n+    \"\"\"\n+    Represents a call to the .discard() method.\n+\n+    The discard() is a built-in method to remove elements from the set.\n+    The discard() method takes exactly one argument. \n+    This method does not return any value.   \n+\n+    Parameters\n+    ----------\n+    set_variable : TypedAstNode\n+        The name of the set.\n+\n+    item : TypedAstNode\n+        The item to search for, and remove.\n+    \"\"\"\n+    __slots__ = ()\n+    _shape = None\n+    _order = None\n+    _rank = 0\n+    _class_type = VoidType()\n+    name = 'discard'\n+\n+    def __init__(self, set_variable, item) -> None:\n+        expected_type = set_variable.class_type.element_type\n+        is_homogeneous = (\n+            expected_type == item.class_type and\n+            set_variable.rank - 1 == item.rank\n+        )\n+        if not is_homogeneous:\n+            raise TypeError(\"Expecting an argument of the same type as the elements of the set\")\n+        super().__init__(set_variable, item)\ndiff --git a/pyccel/ast/class_defs.py b/pyccel/ast/class_defs.py\nindex c7bf31821c..2fe2f838b9 100644\n--- a/pyccel/ast/class_defs.py\n+++ b/pyccel/ast/class_defs.py\n@@ -6,7 +6,7 @@\n This module contains all types which define a python class which is automatically recognised by pyccel\n \"\"\"\n \n-from pyccel.ast.builtin_methods.set_methods  import SetAdd, SetClear, SetCopy, SetPop, SetRemove\n+from pyccel.ast.builtin_methods.set_methods  import SetAdd, SetClear, SetCopy, SetPop, SetRemove, SetDiscard\n from pyccel.ast.builtin_methods.list_methods import (ListAppend, ListInsert, ListPop,\n                                                      ListClear, ListExtend, ListRemove,\n                                                      ListCopy)\n@@ -157,6 +157,7 @@\n             PyccelFunctionDef('add', func_class = SetAdd ),\n             PyccelFunctionDef('clear', func_class = SetClear),\n             PyccelFunctionDef('copy', func_class = SetCopy),\n+            PyccelFunctionDef('discard', func_class = SetDiscard),\n             PyccelFunctionDef('pop', func_class = SetPop),\n             PyccelFunctionDef('remove', func_class = SetRemove),\n         ])\n", "instance_id": "pyccel__pyccel-1773", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding support for the `discard()` method for sets in a Python translation context. It specifies the behavior of `discard()` (removing an element if it exists, doing nothing otherwise) and provides a simple test case to illustrate the expected output. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention edge cases (e.g., behavior with empty sets or invalid element types) or constraints on the input types. Additionally, the reference to \"description in #1531\" is unclear as it is not provided in the context, leaving potential gaps in understanding the full scope or motivation behind the feature. Despite these minor issues, the core intent and requirements are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, primarily involving the addition of a new `SetDiscard` class in `set_methods.py` and corresponding updates to related files like `class_defs.py` and the changelog. The modifications span a few files but do not appear to impact the broader system architecture or require deep refactoring. The amount of code added is moderate, focusing on implementing a single method with a clear purpose.\n\n2. **Technical Concepts Required:** Solving this problem requires understanding Python's set operations, specifically the behavior of `discard()` versus `remove()` (the former does not raise an error if the element is not found). It also involves familiarity with the project's internal AST (Abstract Syntax Tree) representation and class definitions in the `pyccel` codebase, as seen in the use of `TypedAstNode` and type checking logic. While these concepts are not trivial, they are not overly complex for someone with moderate experience in Python and compiler/AST-related development.\n\n3. **Edge Cases and Error Handling:** The code changes include basic type checking to ensure the element being discarded matches the set's element type, which addresses a key edge case. However, the problem statement does not explicitly call out other potential edge cases (e.g., empty sets), and the provided code does not add extensive error handling beyond type compatibility. This keeps the complexity of edge case handling low.\n\n4. **Overall Complexity:** The task involves replicating the pattern of existing set methods (like `remove()` or `pop()`), as seen in the structure of the `SetDiscard` class mirroring `SetRemove`. This reduces the cognitive load since the developer can follow established patterns. The primary challenge lies in understanding the project's custom type system and ensuring consistency with other set operations, but this does not push the difficulty into the medium or hard range.\n\nA score of 0.35 reflects an \"Easy\" problem that requires some understanding of the codebase and moderate modifications but does not demand deep architectural changes or advanced technical expertise. It is slightly above the lower end of the easy range due to the need to grasp the project's AST and type system conventions.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove support for Django 3.2\nDjango 3.2 LTS is end-of-life as of April 1, 2024.  Testing against it can be removed and some code can be simplified.\r\n\r\nRef: https://www.djangoproject.com/download/\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 1e243749..4213aabb 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -55,7 +55,7 @@ repos:\n   rev: '1.20.0'\n   hooks:\n   - id: django-upgrade\n-    args: [--target-version, '3.2']\n+    args: [--target-version, '4.2']\n - repo: https://github.com/hhatto/autopep8\n   rev: 'v2.3.1'\n   hooks:\ndiff --git a/README.md b/README.md\nindex 02a6fddc..45cfd192 100644\n--- a/README.md\n+++ b/README.md\n@@ -30,7 +30,7 @@ Silk is a live profiling and inspection tool for the Django framework. Silk inte\n \n Silk has been tested with:\n \n-* Django: 3.2, 4.2, 5.0, 5.1\n+* Django: 4.2, 5.0, 5.1\n * Python: 3.8, 3.9, 3.10, 3.11, 3.12\n \n ## Installation\ndiff --git a/docs/index.rst b/docs/index.rst\nindex 9fec7dee..011a0bff 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -57,5 +57,5 @@ Features\n Requirements\n ------------\n \n-* Django: 3.2, 4.2, 5.0, 5.1\n+* Django: 4.2, 5.0, 5.1\n * Python: 3.8, 3.9, 3.10\ndiff --git a/project/project/settings.py b/project/project/settings.py\nindex e3586db7..8428f3f5 100644\n--- a/project/project/settings.py\n+++ b/project/project/settings.py\n@@ -22,7 +22,6 @@\n \n ROOT_URLCONF = 'project.urls'\n \n-# Django 3.2+\n DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'\n \n MIDDLEWARE = [\ndiff --git a/setup.py b/setup.py\nindex e076f135..73a6ae83 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -21,7 +21,6 @@\n         'Development Status :: 5 - Production/Stable',\n         'Environment :: Web Environment',\n         'Framework :: Django',\n-        'Framework :: Django :: 3.2',\n         'Framework :: Django :: 4.2',\n         'Framework :: Django :: 5.0',\n         'Framework :: Django :: 5.1',\n@@ -37,7 +36,7 @@\n         'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n     ],\n     install_requires=[\n-        'Django>=3.2',\n+        'Django>=4.2',\n         'sqlparse',\n         'autopep8',\n         'gprof2dot>=2017.09.19',\ndiff --git a/silk/models.py b/silk/models.py\nindex e86a31d6..140d2f3c 100644\n--- a/silk/models.py\n+++ b/silk/models.py\n@@ -6,6 +6,8 @@\n \n import sqlparse\n from django.conf import settings\n+from django.core.files.storage import storages\n+from django.core.files.storage.handler import InvalidStorageError\n from django.db import models, transaction\n from django.db.models import (\n     BooleanField,\n@@ -27,19 +29,11 @@\n from silk.utils.profile_parser import parse_profile\n \n try:\n-    # New in Django 4.2\n-    from django.core.files.storage import storages\n-    from django.core.files.storage.handler import InvalidStorageError\n-    try:\n-        silk_storage = storages['SILKY_STORAGE']\n-    except InvalidStorageError:\n-        from django.utils.module_loading import import_string\n-        storage_class = SilkyConfig().SILKY_STORAGE_CLASS or settings.DEFAULT_FILE_STORAGE\n-        silk_storage = import_string(storage_class)()\n-except ImportError:\n-    # Deprecated since Django 4.2, Removed in Django 5.1\n-    from django.core.files.storage import get_storage_class\n-    silk_storage = get_storage_class(SilkyConfig().SILKY_STORAGE_CLASS)()\n+    silk_storage = storages['SILKY_STORAGE']\n+except InvalidStorageError:\n+    from django.utils.module_loading import import_string\n+    storage_class = SilkyConfig().SILKY_STORAGE_CLASS or settings.DEFAULT_FILE_STORAGE\n+    silk_storage = import_string(storage_class)()\n \n \n # Seperated out so can use in tests w/o models\ndiff --git a/tox.ini b/tox.ini\nindex 9da52bf8..c439f10f 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -8,7 +8,6 @@ python =\n \n [gh-actions:env]\n DJANGO =\n-    3.2: dj32\n     4.2: dj42\n     5.0: dj50\n     5.1: dj51\n@@ -16,7 +15,6 @@ DJANGO =\n \n [tox]\n envlist =\n-    py{38,39,310}-dj32-{sqlite3,mysql,postgresql}\n     py{38,39,310,311,312}-dj42-{sqlite3,mysql,postgresql}\n     py{310,311,312}-dj{50,51,main}-{sqlite3,mysql,postgresql}\n \n@@ -29,7 +27,6 @@ deps =\n     -rrequirements.txt\n     mysql: mysqlclient\n     postgresql: psycopg2-binary\n-    dj32: django>=3.2,<3.3\n     dj42: django>=4.2,<4.3\n     dj50: django>=5.0,<5.1\n     dj51: django>=5.1,<5.2\n", "instance_id": "jazzband__django-silk-736", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to remove support for Django 3.2 due to its end-of-life status as of April 1, 2024. It specifies the goal of simplifying code and removing testing against Django 3.2, and provides a reference to the Django download page for context. However, it lacks specific details about the expected scope of changes (e.g., which files or modules need modification) and does not mention any potential challenges or edge cases that might arise during the removal process. While the intent is clear, the absence of detailed requirements or examples of specific code simplifications results in minor ambiguities, justifying a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls in the Easy range (0.2-0.4) due to the following reasons based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The code changes span multiple files (e.g., configuration files like `tox.ini` and `setup.py`, documentation files like `README.md`, and source code in `silk/models.py`). However, the modifications are relatively straightforward, involving updates to version numbers, removal of Django 3.2 references, and simplification of compatibility code. The changes do not significantly impact the system's architecture and are mostly isolated to configuration and documentation, with minimal logic changes in the codebase (e.g., updating storage handling logic in `silk/models.py`).\n\n2. **Number of Technical Concepts:** The task requires basic familiarity with Django versioning, Python package configuration (`setup.py`), and testing environments (`tox.ini`). It also involves understanding Django's storage API changes between versions (e.g., the shift in storage handling from Django 3.2 to 4.2+). However, these concepts are not particularly complex for a developer with moderate experience in Python and Django, and no advanced algorithms or design patterns are involved.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce significant new error handling logic. The primary risk is ensuring that removing Django 3.2 support does not break compatibility for users or downstream dependencies, but the changes appear to handle this by updating minimum version requirements. The storage logic update in `silk/models.py` simplifies compatibility code without introducing complex edge cases.\n\n4. **Overall Complexity:** The task involves understanding a small subset of the codebase and making targeted updates. It does not require deep architectural changes or extensive debugging. The amount of code change is moderate but consists of repetitive updates (e.g., version string changes) rather than intricate logic modifications.\n\nA score of 0.25 reflects an Easy task that requires some understanding of the codebase and Django versioning but does not pose significant technical challenges or require advanced expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Distributing command sequences throughout CmdSequencers\n### Discussed in https://github.com/nasa/fprime/discussions/2608\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **zimri-leisher** March 20, 2024</sup>\r\nHello,\r\nWe'd like to run multiple command sequences at the same time. As I understand, that means we need more than one command sequencer. However, it would be nice not to have to decide which command sequencer we want to run a command on. Instead, we want to just say \"run this sequence on an available cmd sequencer\".\r\nAny suggestions for this? It seems like it would be a common problem. Or is this a bad idea and we should have to explicitly declare which command sequencer we want to use?</div>\nDistributing command sequences throughout CmdSequencers\n### Discussed in https://github.com/nasa/fprime/discussions/2608\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **zimri-leisher** March 20, 2024</sup>\r\nHello,\r\nWe'd like to run multiple command sequences at the same time. As I understand, that means we need more than one command sequencer. However, it would be nice not to have to decide which command sequencer we want to run a command on. Instead, we want to just say \"run this sequence on an available cmd sequencer\".\r\nAny suggestions for this? It seems like it would be a common problem. Or is this a bad idea and we should have to explicitly declare which command sequencer we want to use?</div>\n", "patch": "diff --git a/.github/actions/spelling/expect.txt b/.github/actions/spelling/expect.txt\nindex a55382cce5..d608eeac1f 100644\n--- a/.github/actions/spelling/expect.txt\n+++ b/.github/actions/spelling/expect.txt\n@@ -152,6 +152,7 @@ CMDPACKET\n CMDREG\n cmds\n CMDSEQ\n+cmdsequencer\n cnt\n cntx\n cobj\n@@ -548,6 +549,7 @@ lammertbies\n LASTLOG\n LBLOCK\n LCHILD\n+leisher\n lemstarch\n lestarch\n levelname\n@@ -1212,4 +1214,5 @@ xsh\n xsltproc\n xxxx\n yacgen\n+zimri\n zmq\n\\ No newline at end of file\ndiff --git a/Svc/CMakeLists.txt b/Svc/CMakeLists.txt\nindex 37de2e0110..2ab95efc03 100644\n--- a/Svc/CMakeLists.txt\n+++ b/Svc/CMakeLists.txt\n@@ -42,6 +42,7 @@ add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/PassiveRateGroup\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/PolyDb/\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/PrmDb/\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/RateGroupDriver/\")\n+add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/SeqDispatcher/\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/StaticMemory/\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/TlmChan/\")\n add_fprime_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/TlmPacketizer/\")\ndiff --git a/Svc/CmdSequencer/CmdSequencer.fpp b/Svc/CmdSequencer/CmdSequencer.fpp\nindex 52df683856..df55efaa74 100644\n--- a/Svc/CmdSequencer/CmdSequencer.fpp\n+++ b/Svc/CmdSequencer/CmdSequencer.fpp\n@@ -85,6 +85,9 @@ module Svc {\n     @ Schedule in port\n     async input port schedIn: Svc.Sched\n \n+    @ Notifies that a sequence has started running\n+    output port seqStartOut: Svc.CmdSeqIn\n+\n     # ----------------------------------------------------------------------\n     # Commands\n     # ----------------------------------------------------------------------\ndiff --git a/Svc/CmdSequencer/CmdSequencerImpl.cpp b/Svc/CmdSequencer/CmdSequencerImpl.cpp\nindex 7e1c84d25b..88e2018a62 100644\n--- a/Svc/CmdSequencer/CmdSequencerImpl.cpp\n+++ b/Svc/CmdSequencer/CmdSequencerImpl.cpp\n@@ -122,6 +122,9 @@ namespace Svc {\n         // Check the step mode. If it is auto, start the sequence\n         if (AUTO == this->m_stepMode) {\n             this->m_runMode = RUNNING;\n+            if(this->isConnected_seqStartOut_OutputPort(0)) {\n+                this->seqStartOut_out(0, this->m_sequence->getStringFileName());\n+            }\n             this->performCmd_Step();\n         }\n \n@@ -159,7 +162,7 @@ namespace Svc {\n     //! Handler for input port seqRunIn\n     void CmdSequencerComponentImpl::seqRunIn_handler(\n            NATIVE_INT_TYPE portNum,\n-           Fw::String &filename\n+           const Fw::StringBase& filename\n        ) {\n \n         if (!this->requireRunMode(STOPPED)) {\n@@ -190,6 +193,9 @@ namespace Svc {\n         // Check the step mode. If it is auto, start the sequence\n         if (AUTO == this->m_stepMode) {\n             this->m_runMode = RUNNING;\n+            if(this->isConnected_seqStartOut_OutputPort(0)) {\n+                this->seqStartOut_out(0, this->m_sequence->getStringFileName());\n+            }\n             this->performCmd_Step();\n         }\n \n@@ -359,6 +365,9 @@ namespace Svc {\n         this->m_runMode = RUNNING;\n         this->performCmd_Step();\n         this->log_ACTIVITY_HI_CS_CmdStarted(this->m_sequence->getLogFileName());\n+        if(this->isConnected_seqStartOut_OutputPort(0)) {\n+            this->seqStartOut_out(0, this->m_sequence->getStringFileName());\n+        }\n         this->cmdResponse_out(opcode, cmdSeq, Fw::CmdResponse::OK);\n     }\n \ndiff --git a/Svc/CmdSequencer/CmdSequencerImpl.hpp b/Svc/CmdSequencer/CmdSequencerImpl.hpp\nindex 6e595000b4..655cebd4f3 100644\n--- a/Svc/CmdSequencer/CmdSequencerImpl.hpp\n+++ b/Svc/CmdSequencer/CmdSequencerImpl.hpp\n@@ -235,6 +235,10 @@ namespace Svc {\n           //! \\return The log file name\n           Fw::LogStringArg& getLogFileName();\n \n+          //! Get the normal string file name\n+          //! \\return The normal string file name\n+          Fw::String& getStringFileName();\n+\n           //! Get the sequence header\n           const Header& getHeader() const;\n \n@@ -277,6 +281,9 @@ namespace Svc {\n           //! Copy of file name for events\n           Fw::LogStringArg m_logFileName;\n \n+          //! Copy of file name for ports\n+          Fw::String m_stringFileName;\n+\n           //! Serialize buffer to hold the binary sequence data\n           Fw::ExternalSerializeBuffer m_buffer;\n \n@@ -582,7 +589,7 @@ namespace Svc {\n       //! Handler for input port seqRunIn\n       void seqRunIn_handler(\n           NATIVE_INT_TYPE portNum, //!< The port number\n-          Fw::String &filename //!< The sequence file\n+          const Fw::StringBase& filename //!< The sequence file\n       );\n \n       //! Handler for ping port\ndiff --git a/Svc/CmdSequencer/Sequence.cpp b/Svc/CmdSequencer/Sequence.cpp\nindex 235de84c13..30cc2ccb50 100644\n--- a/Svc/CmdSequencer/Sequence.cpp\n+++ b/Svc/CmdSequencer/Sequence.cpp\n@@ -112,6 +112,7 @@ namespace Svc {\n     {\n         this->m_fileName = fileName;\n         this->m_logFileName = fileName;\n+        this->m_stringFileName = fileName;\n     }\n \n     Fw::CmdStringArg& CmdSequencerComponentImpl::Sequence ::\n@@ -126,5 +127,11 @@ namespace Svc {\n         return this->m_logFileName;\n     }\n \n+    Fw::String& CmdSequencerComponentImpl::Sequence ::\n+      getStringFileName()\n+    {\n+        return this->m_stringFileName;\n+    }\n+\n }\n \ndiff --git a/Svc/Seq/Seq.fpp b/Svc/Seq/Seq.fpp\nindex 66a5d9888c..1caf1ce037 100644\n--- a/Svc/Seq/Seq.fpp\n+++ b/Svc/Seq/Seq.fpp\n@@ -2,7 +2,7 @@ module Svc {\n \n   @ Port to request a sequence be run\n   port CmdSeqIn(\n-                 ref filename: Fw.String @< The sequence file\n+                 filename: string size 240 @< The sequence file\n                )\n \n   @ Port to cancel a sequence\ndiff --git a/Svc/SeqDispatcher/CMakeLists.txt b/Svc/SeqDispatcher/CMakeLists.txt\nnew file mode 100644\nindex 0000000000..bb92108919\n--- /dev/null\n+++ b/Svc/SeqDispatcher/CMakeLists.txt\n@@ -0,0 +1,24 @@\n+####\n+# F prime CMakeLists.txt:\n+#\n+# SOURCE_FILES: combined list of source and autocoding files\n+# MOD_DEPS: (optional) module dependencies\n+# UT_SOURCE_FILES: list of source files for unit tests\n+#\n+####\n+set(SOURCE_FILES\n+  \"${CMAKE_CURRENT_LIST_DIR}/SeqDispatcher.fpp\"\n+  \"${CMAKE_CURRENT_LIST_DIR}/SeqDispatcher.cpp\"\n+)\n+\n+register_fprime_module()\n+\n+### UTS ###\n+set(UT_AUTO_HELPERS ON)\n+\n+set(UT_SOURCE_FILES\n+  \"${FPRIME_FRAMEWORK_PATH}/Svc/SeqDispatcher/SeqDispatcher.fpp\"\n+  \"${CMAKE_CURRENT_LIST_DIR}/test/ut/SeqDispatcherTester.cpp\"\n+  \"${CMAKE_CURRENT_LIST_DIR}/test/ut/SeqDispatcherTestMain.cpp\"\n+)\n+register_fprime_ut()\ndiff --git a/Svc/SeqDispatcher/SeqDispatcher.cpp b/Svc/SeqDispatcher/SeqDispatcher.cpp\nnew file mode 100644\nindex 0000000000..c55bda62c0\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcher.cpp\n@@ -0,0 +1,186 @@\n+// ======================================================================\n+// \\title  SeqDispatcher.cpp\n+// \\author zimri.leisher\n+// \\brief  cpp file for SeqDispatcher component implementation class\n+// ======================================================================\n+\n+#include <Svc/SeqDispatcher/SeqDispatcher.hpp>\n+\n+namespace Svc {\n+\n+// ----------------------------------------------------------------------\n+// Construction, initialization, and destruction\n+// ----------------------------------------------------------------------\n+\n+SeqDispatcher ::SeqDispatcher(const char* const compName)\n+    : SeqDispatcherComponentBase(compName) {}\n+\n+SeqDispatcher ::~SeqDispatcher() {}\n+\n+FwIndexType SeqDispatcher::getNextAvailableSequencerIdx() {\n+  for (FwIndexType i = 0; i < SeqDispatcherSequencerPorts; i++) {\n+    if (this->isConnected_seqRunOut_OutputPort(i) && \n+        this->m_entryTable[i].state == SeqDispatcher_CmdSequencerState::AVAILABLE) {\n+      return i;\n+    }\n+  }\n+  return -1;\n+}\n+\n+void SeqDispatcher::runSequence(FwIndexType sequencerIdx,\n+                                const Fw::StringBase& fileName,\n+                                Fw::Wait block) {\n+  // this function is only designed for internal usage\n+  // we can guarantee it cannot be called with input that would fail\n+  FW_ASSERT(sequencerIdx >= 0 && sequencerIdx < SeqDispatcherSequencerPorts,\n+            sequencerIdx);\n+  FW_ASSERT(this->isConnected_seqRunOut_OutputPort(sequencerIdx));\n+  FW_ASSERT(this->m_entryTable[sequencerIdx].state == \n+              SeqDispatcher_CmdSequencerState::AVAILABLE, \n+            this->m_entryTable[sequencerIdx].state);\n+\n+  if (block == Fw::Wait::NO_WAIT) {\n+    this->m_entryTable[sequencerIdx].state =\n+        SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_NO_BLOCK;\n+  } else {\n+    this->m_entryTable[sequencerIdx].state =\n+        SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_BLOCK;\n+  }\n+\n+  this->m_sequencersAvailable--;\n+  this->tlmWrite_sequencersAvailable(this->m_sequencersAvailable);\n+  this->m_entryTable[sequencerIdx].sequenceRunning = fileName;\n+\n+  this->m_dispatchedCount++;\n+  this->tlmWrite_dispatchedCount(this->m_dispatchedCount);\n+  this->seqRunOut_out(sequencerIdx,\n+                      this->m_entryTable[sequencerIdx].sequenceRunning);\n+}\n+\n+void SeqDispatcher::seqStartIn_handler(\n+    NATIVE_INT_TYPE portNum, //!< The port number\n+    const Fw::StringBase& fileName //!< The sequence file name\n+) {\n+  FW_ASSERT(portNum >= 0 && portNum < SeqDispatcherSequencerPorts, portNum);\n+  if (this->m_entryTable[portNum].state ==\n+          SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_BLOCK ||\n+      this->m_entryTable[portNum].state ==\n+          SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_NO_BLOCK) {\n+    // we were aware of this sequencer running a sequence\n+    if (this->m_entryTable[portNum].sequenceRunning != fileName) {\n+      // uh oh. entry table is wrong\n+      // let's just update it to be correct. nothing we can do about\n+      // it except raise a warning and update our state\n+      this->log_WARNING_HI_ConflictingSequenceStarted(static_cast<U16>(portNum), fileName, this->m_entryTable[portNum].sequenceRunning);\n+      this->m_entryTable[portNum].sequenceRunning = fileName;\n+    }\n+  } else {\n+    // we were not aware that this sequencer was running. ground must have\n+    // directly commanded that specific sequencer\n+\n+    // warn because this may be unintentional\n+    this->log_WARNING_LO_UnexpectedSequenceStarted(static_cast<U16>(portNum), fileName);\n+\n+    // update the state\n+    this->m_entryTable[portNum].state =\n+        SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_NO_BLOCK;\n+    this->m_entryTable[portNum].sequenceRunning = fileName;\n+    this->m_sequencersAvailable--;\n+    this->tlmWrite_sequencersAvailable(this->m_sequencersAvailable);\n+  }\n+}\n+\n+void SeqDispatcher::seqDoneIn_handler(\n+    NATIVE_INT_TYPE portNum,         //!< The port number\n+    FwOpcodeType opCode,             //!< Command Op Code\n+    U32 cmdSeq,                      //!< Command Sequence\n+    const Fw::CmdResponse& response  //!< The command response argument\n+) {\n+  FW_ASSERT(portNum >= 0 && portNum < SeqDispatcherSequencerPorts, portNum);\n+  if (this->m_entryTable[portNum].state !=\n+          SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_BLOCK &&\n+      this->m_entryTable[portNum].state !=\n+          SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_NO_BLOCK) {\n+    // this sequencer was not running a sequence that we were aware of.\n+\n+    // we should have caught this in seqStartIn and updated the state\n+    // accordingly, but somehow we didn't? very sad and shouldn't happen\n+\n+    // anyways, don't have to do anything cuz now that this seq we didn't know\n+    // about is done, the sequencer is available again (which is its current\n+    // state in our internal entry table already)\n+    this->log_WARNING_LO_UnknownSequenceFinished(static_cast<U16>(portNum));\n+  } else {\n+    // ok, a sequence has finished that we knew about\n+    if (this->m_entryTable[portNum].state ==\n+        SeqDispatcher_CmdSequencerState::RUNNING_SEQUENCE_BLOCK) {\n+      // we need to give a cmd response cuz some other sequence is being blocked\n+      // by this\n+      this->cmdResponse_out(this->m_entryTable[portNum].opCode,\n+                            this->m_entryTable[portNum].cmdSeq, response);\n+\n+      if (response == Fw::CmdResponse::EXECUTION_ERROR) {\n+        // dispatched sequence errored\n+        this->m_errorCount++;\n+        this->tlmWrite_errorCount(this->m_errorCount);\n+      }\n+    }\n+  }\n+\n+  // all command responses mean the sequence is no longer running\n+  // so component should be available\n+  this->m_entryTable[portNum].state = SeqDispatcher_CmdSequencerState::AVAILABLE;\n+  this->m_entryTable[portNum].sequenceRunning = \"<no seq>\";\n+  this->m_sequencersAvailable++;\n+  this->tlmWrite_sequencersAvailable(this->m_sequencersAvailable);\n+}\n+\n+//! Handler for input port seqRunIn\n+void SeqDispatcher::seqRunIn_handler(NATIVE_INT_TYPE portNum,\n+                                     const Fw::StringBase& fileName) {\n+  FwIndexType idx = this->getNextAvailableSequencerIdx();\n+  // no available sequencers\n+  if (idx == -1) {\n+    this->log_WARNING_HI_NoAvailableSequencers();\n+    return;\n+  }\n+\n+  this->runSequence(idx, fileName, Fw::Wait::NO_WAIT);\n+}\n+// ----------------------------------------------------------------------\n+// Command handler implementations\n+// ----------------------------------------------------------------------\n+\n+void SeqDispatcher ::RUN_cmdHandler(const FwOpcodeType opCode,\n+                                    const U32 cmdSeq,\n+                                    const Fw::CmdStringArg& fileName,\n+                                    Fw::Wait block) {\n+  FwIndexType idx = this->getNextAvailableSequencerIdx();\n+  // no available sequencers\n+  if (idx == -1) {\n+    this->log_WARNING_HI_NoAvailableSequencers();\n+    this->cmdResponse_out(opCode, cmdSeq, Fw::CmdResponse::EXECUTION_ERROR);\n+    return;\n+  }\n+\n+  this->runSequence(idx, fileName, block);\n+\n+  if (block == Fw::Wait::NO_WAIT) {\n+    // return instantly\n+    this->cmdResponse_out(opCode, cmdSeq, Fw::CmdResponse::OK);\n+  } else {\n+    // otherwise don't return a response yet. just save the opCode and cmdSeq\n+    // so we can return a response later\n+    this->m_entryTable[idx].opCode = opCode;\n+    this->m_entryTable[idx].cmdSeq = cmdSeq;\n+  }\n+}\n+\n+void SeqDispatcher::LOG_STATUS_cmdHandler(\n+    const FwOpcodeType opCode,          /*!< The opcode*/\n+    const U32 cmdSeq) {                   /*!< The command sequence number*/\n+  for(FwIndexType idx = 0; idx < SeqDispatcherSequencerPorts; idx++) {\n+    this->log_ACTIVITY_LO_LogSequencerStatus(static_cast<U16>(idx), this->m_entryTable[idx].state, Fw::LogStringArg(this->m_entryTable[idx].sequenceRunning));\n+  }\n+}\n+}  // end namespace components\ndiff --git a/Svc/SeqDispatcher/SeqDispatcher.fpp b/Svc/SeqDispatcher/SeqDispatcher.fpp\nnew file mode 100644\nindex 0000000000..3fc6d9702f\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcher.fpp\n@@ -0,0 +1,55 @@\n+module Svc {\n+    @ Dispatches command sequences to available command sequencers\n+    active component SeqDispatcher {\n+\n+        enum CmdSequencerState {\n+            AVAILABLE = 0\n+            RUNNING_SEQUENCE_BLOCK = 1\n+            RUNNING_SEQUENCE_NO_BLOCK = 2\n+        }\n+\n+        include \"SeqDispatcherCommands.fppi\"\n+        include \"SeqDispatcherTelemetry.fppi\"\n+        include \"SeqDispatcherEvents.fppi\"\n+\n+        @ Dispatches a sequence to the first available command sequencer\n+        async input port seqRunIn: Svc.CmdSeqIn\n+\n+        output port seqRunOut: [SeqDispatcherSequencerPorts] Svc.CmdSeqIn\n+\n+        @ Called by a command sequencer whenever it has finished any sequence\n+        async input port seqDoneIn: [SeqDispatcherSequencerPorts] Fw.CmdResponse\n+\n+        @ Called by cmdsequencer whenever it starts any sequence\n+        async input port seqStartIn: [SeqDispatcherSequencerPorts] Svc.CmdSeqIn\n+\n+        match seqRunOut with seqDoneIn\n+\n+        match seqRunOut with seqStartIn\n+\n+        ###############################################################################\n+        # Standard AC Ports: Required for Channels, Events, Commands, and Parameters  #\n+        ###############################################################################\n+        @ Port for requesting the current time\n+        time get port timeCaller\n+\n+        @ Port for sending command registrations\n+        command reg port cmdRegOut\n+\n+        @ Port for receiving commands\n+        command recv port cmdIn\n+\n+        @ Port for sending command responses\n+        command resp port cmdResponseOut\n+\n+        @ Port for sending textual representation of events\n+        text event port logTextOut\n+\n+        @ Port for sending events to downlink\n+        event port logOut\n+\n+        @ Port for sending telemetry channels to downlink\n+        telemetry port tlmOut\n+\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/Svc/SeqDispatcher/SeqDispatcher.hpp b/Svc/SeqDispatcher/SeqDispatcher.hpp\nnew file mode 100644\nindex 0000000000..9c8b9f6806\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcher.hpp\n@@ -0,0 +1,95 @@\n+// ======================================================================\n+// \\title  SeqDispatcher.hpp\n+// \\author zimri.leisher\n+// \\brief  hpp file for SeqDispatcher component implementation class\n+// ======================================================================\n+\n+#ifndef SeqDispatcher_HPP\n+#define SeqDispatcher_HPP\n+\n+#include \"Svc/SeqDispatcher/SeqDispatcherComponentAc.hpp\"\n+#include \"Svc/SeqDispatcher/SeqDispatcher_CmdSequencerStateEnumAc.hpp\"\n+#include \"FppConstantsAc.hpp\"\n+#include \"Fw/Types/WaitEnumAc.hpp\"\n+#include \"Fw/Types/StringBase.hpp\"\n+\n+namespace Svc {\n+\n+class SeqDispatcher : public SeqDispatcherComponentBase {\n+  public:\n+    // ----------------------------------------------------------------------\n+    // Construction, initialization, and destruction\n+    // ----------------------------------------------------------------------\n+\n+    //! Construct object SeqDispatcher\n+    //!\n+    SeqDispatcher(const char* const compName /*!< The component name*/\n+    );\n+\n+    //! Destroy object SeqDispatcher\n+    //!\n+    ~SeqDispatcher();\n+\n+  PROTECTED:\n+\n+    //! Handler for input port seqDoneIn\n+    void\n+    seqDoneIn_handler(NATIVE_INT_TYPE portNum,         //!< The port number\n+                      FwOpcodeType opCode,             //!< Command Op Code\n+                      U32 cmdSeq,                      //!< Command Sequence\n+                      const Fw::CmdResponse& response  //!< The command response argument\n+    );\n+\n+    //! Handler for input port seqStartIn\n+    void seqStartIn_handler(NATIVE_INT_TYPE portNum, //!< The port number\n+                            const Fw::StringBase& fileName //!< The sequence file\n+    );\n+\n+    //! Handler for input port seqRunIn\n+    void seqRunIn_handler(NATIVE_INT_TYPE portNum, //!< The port number\n+                          const Fw::StringBase& fileName //!< The sequence file\n+    );\n+\n+  PRIVATE:\n+\n+    // number of sequences dispatched (successful or otherwise)\n+    U32 m_dispatchedCount = 0;\n+    // number of errors from dispatched sequences (CmdResponse::EXECUTION_ERROR)\n+    U32 m_errorCount = 0;\n+    // number of sequencers in state AVAILABLE\n+    U32 m_sequencersAvailable = SeqDispatcherSequencerPorts;\n+\n+    struct DispatchEntry {\n+        FwOpcodeType opCode;  //!< opcode of entry\n+        U32 cmdSeq;\n+        // store the state of each sequencer\n+        SeqDispatcher_CmdSequencerState state;\n+        // store the sequence currently running for each sequencer\n+        Fw::String sequenceRunning = \"<no seq>\";\n+    } m_entryTable[SeqDispatcherSequencerPorts];  //!< table of dispatch\n+                                                  //!< entries\n+\n+    FwIndexType getNextAvailableSequencerIdx();\n+\n+    void runSequence(FwIndexType sequencerIdx, \n+                     const Fw::StringBase& fileName, \n+                     Fw::Wait block);\n+\n+    // ----------------------------------------------------------------------\n+    // Command handler implementations\n+    // ----------------------------------------------------------------------\n+\n+    //! Implementation for RUN command handler\n+    //!\n+    void RUN_cmdHandler(const FwOpcodeType opCode,        /*!< The opcode*/\n+                        const U32 cmdSeq,                 /*!< The command sequence number*/\n+                        const Fw::CmdStringArg& fileName, /*!< The name of the sequence file*/\n+                        Fw::Wait block);\n+\n+    void LOG_STATUS_cmdHandler(const FwOpcodeType opCode, /*!< The opcode*/\n+                               const U32 cmdSeq);         /*!< The command sequence number*/\n+};\n+\n+}  // end namespace components\n+\n+#endif\ndiff --git a/Svc/SeqDispatcher/SeqDispatcherCommands.fppi b/Svc/SeqDispatcher/SeqDispatcherCommands.fppi\nnew file mode 100644\nindex 0000000000..378c2f6ec7\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcherCommands.fppi\n@@ -0,0 +1,9 @@\n+@ Dispatches a sequence to the first available sequencer\n+async command RUN(\n+                      fileName: string size 240 @< The name of the sequence file\n+                      $block: Fw.Wait @< Return command status when complete or not\n+                    ) \\\n+    opcode 0\n+\n+@ Logs via Events the state of each connected command sequencer\n+async command LOG_STATUS() opcode 1\n\\ No newline at end of file\ndiff --git a/Svc/SeqDispatcher/SeqDispatcherEvents.fppi b/Svc/SeqDispatcher/SeqDispatcherEvents.fppi\nnew file mode 100644\nindex 0000000000..9b92eca254\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcherEvents.fppi\n@@ -0,0 +1,38 @@\n+event InvalidSequencer(\n+    idx: U16\n+) \\\n+    severity warning high \\\n+    format \"Invalid sequence index {}\"\n+\n+event NoAvailableSequencers() \\\n+    severity warning high \\\n+    format \"No available cmd sequencers to dispatch a sequence to\"\n+\n+event UnknownSequenceFinished(\n+    idx: U16\n+) \\\n+    severity warning low \\\n+    format \"Sequencer {} completed a sequence with no matching start notification\"\n+\n+event ConflictingSequenceStarted(\n+    idx: U16,\n+    newSequence: string size 240,\n+    sequenceInInternalState: string size 240\n+) \\\n+    severity warning high \\\n+    format \"Sequencer {} started a sequence {} while still running {}\"\n+\n+event UnexpectedSequenceStarted(\n+    idx: U16,\n+    newSequence: string size 240\n+) \\\n+    severity warning low \\\n+    format \"Sequencer {} was externally commanded to start a sequence {}\"\n+\n+event LogSequencerStatus(\n+    idx: U16\n+    state: CmdSequencerState\n+    filename: string size 240\n+) \\\n+    severity activity low \\\n+    format \"Sequencer {} with state {} is running file {}\"\n\\ No newline at end of file\ndiff --git a/Svc/SeqDispatcher/SeqDispatcherTelemetry.fppi b/Svc/SeqDispatcher/SeqDispatcherTelemetry.fppi\nnew file mode 100644\nindex 0000000000..3728c9a063\n--- /dev/null\n+++ b/Svc/SeqDispatcher/SeqDispatcherTelemetry.fppi\n@@ -0,0 +1,8 @@\n+@ Number of sequences dispatched\n+telemetry dispatchedCount: U32\n+@ Number of sequences dispatched that returned an error. Note: if a sequence\n+@ was run in non-blocking mode, even if the sequence errors out, this error\n+@ count will never increase\n+telemetry errorCount: U32\n+@ Number of sequencers in an available state\n+telemetry sequencersAvailable: U32\n\\ No newline at end of file\ndiff --git a/Svc/SeqDispatcher/docs/sdd.md b/Svc/SeqDispatcher/docs/sdd.md\nnew file mode 100644\nindex 0000000000..86fa11160f\n--- /dev/null\n+++ b/Svc/SeqDispatcher/docs/sdd.md\n@@ -0,0 +1,51 @@\n+# components::SeqDispatcher\n+\n+Dispatches command sequences to available command sequencers, allowing the spacecraft controllers to run multiple sequences at once without having to manually manage which `CmdSequencer`s those sequences run on.\n+\n+### Usage\n+* Call the `RUN` command just like you would call it on a `CmdSequencer`\n+* If any connected `CmdSequencer` is available, it will route the sequence to the first one it finds\n+* `RUN` can be made blocking or non-blocking, just like `CmdSequencer`'s `RUN`\n+\n+## State diagram\n+![State diagram of the SeqDispatcher](seq_dispatcher_model.png \"SeqDispatcher model\")\n+\n+## Port Descriptions\n+|Type| Name | Description |\n+|async input|seqRunIn|Equivalent to the RUN cmd, dispatches a sequence to the first available sequencer|\n+|output|seqRunOut|This is used by the SeqDispatcher to send sequence run calls to sequencers|\n+|async input|seqDoneIn|Called by a command sequencer whenever it has finished any sequence|\n+|async input|seqStartIn|Called by a command sequencer whenever it starts any sequence|\n+\n+## Commands\n+| Name | Description |\n+|RUN|Dispatches a sequence to the first available sequencer|\n+|LOG_STATUS|Logs via Events the state of each connected command sequencer|\n+\n+## Events\n+| Name | Description |\n+|InvalidSequencer|The given sequencer index is invalid for an unspecified reason|\n+|NoAvailableSequencers|There are no available sequencers to dispatch a sequence to|\n+|UnknownSequenceFinished|We received a call to seqDoneIn that didn't have a corresponding seqStartIn call|\n+|UnexpectedSequenceStarted|We received a call to seqStartIn but we didn't receive a call to seqDoneIn before that|\n+|LogSequencerStatus|Shows the current state and sequence filename for a particular sequencer. Produced by the LOG_STATUS command|\n+\n+\n+\n+## Telemetry\n+| Name | Description |\n+|dispatchedCount|Number of sequences dispatched|\n+|errorCount|Number of sequences dispatched that returned an error. Note: if a sequence was run in non-blocking mode, even if the sequence errors out, this error count will never increase|\n+|sequencersAvailable|Number of sequencers ready to run a sequence|\n+\n+## Unit Tests\n+Add unit test descriptions in the chart below\n+| Name | Description |\n+|testDispatch|Tests the basic dispatch functionality of the `SeqDispatcher`|\n+|testLogStatus|Tests the LOG_STATUS command|\n+\n+## Requirements\n+Add requirements in the chart below\n+| Name | Description | Validation |\n+|---|---|---|\n+|---|---|---|\n\\ No newline at end of file\ndiff --git a/Svc/SeqDispatcher/docs/seq_dispatcher_model.png b/Svc/SeqDispatcher/docs/seq_dispatcher_model.png\nnew file mode 100644\nindex 0000000000..a85d7c7f45\nBinary files /dev/null and b/Svc/SeqDispatcher/docs/seq_dispatcher_model.png differ\ndiff --git a/config/AcConstants.fpp b/config/AcConstants.fpp\nindex 2f0f10706c..3ddcff26e6 100644\n--- a/config/AcConstants.fpp\n+++ b/config/AcConstants.fpp\n@@ -18,6 +18,9 @@ constant CmdDispatcherComponentCommandPorts = 30\n @ Used for uplink/sequencer buffer/response ports\n constant CmdDispatcherSequencePorts = 5\n \n+@ Used for dispatching sequences to command sequencers\n+constant SeqDispatcherSequencerPorts = 2\n+\n @ Used for sizing the command splitter input arrays\n constant CmdSplitterPorts = CmdDispatcherSequencePorts\n \ndiff --git a/docs/UsersGuide/dev/configuring-fprime.md b/docs/UsersGuide/dev/configuring-fprime.md\nindex 6464fb8732..dc9ee6ea55 100644\n--- a/docs/UsersGuide/dev/configuring-fprime.md\n+++ b/docs/UsersGuide/dev/configuring-fprime.md\n@@ -53,7 +53,7 @@ number of components.\n | CmdDispatcherSequencePorts         | Number of incoming ports to command dispatcher, e.g. uplink and command sequencer                     | 5       | Positive integer |\n | RateGroupDriverRateGroupPorts      | Number of rate group driver output ports. Limits total number of different rate groups                | 3       | Positive integer |\n | HealthPingPorts                    | Number of health ping output ports. Limits number of components attached to health component          | 25      | Positive integer |\n-\n+| SeqDispatcherSequencerPorts         | Number of CmdSequencers that the SeqDispatcher can dispatch sequences to | 2 | Positive integer\n \n ## FpConfig.h\n \n", "instance_id": "nasa__fprime-2731", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to enable running multiple command sequences concurrently without manually specifying which command sequencer to use. It outlines the goal of dispatching sequences to available sequencers automatically, which is a valid and understandable requirement. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define constraints such as the maximum number of sequencers, expected behavior when no sequencers are available, or specific requirements for error handling and recovery. Additionally, it lacks examples or use cases that could illustrate the desired functionality in different scenarios. While the discussion context from GitHub provides some insight, critical details about edge cases and integration with the existing system are not fully specified in the problem description itself. Hence, it is rated as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem is rated at 0.65, placing it in the \"Hard\" category due to several factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are significant, involving the creation of a new component (`SeqDispatcher`) and modifications to the existing `CmdSequencer` component. This includes adding new files, updating build configurations, and integrating with the broader F' (fprime) framework. The changes span multiple files and modules, requiring an understanding of the interactions between components like command dispatching, sequencing, and telemetry. While the changes do not appear to fundamentally alter the system's architecture, they introduce a new layer of abstraction for sequence dispatching, which adds complexity to the codebase.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several technical concepts, including asynchronous port communication in the F' framework, state management for tracking sequencer availability, telemetry and event logging, and command handling with blocking and non-blocking behaviors. Additionally, the developer must understand the F' framework's component model, serialization mechanisms (e.g., `Fw::StringBase`), and how to extend existing components with new functionality. While these concepts are not extraordinarily advanced, their combination and application within the context of a real-time embedded system increase the complexity.\n\n3. **Potential Edge Cases and Error Handling**: The code changes address several edge cases, such as handling situations where no sequencers are available, detecting unexpected sequence starts or completions, and managing conflicts in sequence execution. The implementation includes warning events and telemetry updates to handle these scenarios, which adds to the complexity. However, the problem statement itself does not explicitly call out all possible edge cases (e.g., sequencer failures, sequence prioritization), leaving some room for interpretation. The error handling logic in the code is moderately complex, requiring careful state tracking and appropriate responses to various conditions.\n\n4. **Overall Complexity**: The task requires a deep understanding of the F' framework's architecture, particularly how command sequencing and dispatching work. Implementing the `SeqDispatcher` involves not just writing new code but ensuring it integrates seamlessly with existing components without introducing regressions. The need to handle both blocking and non-blocking sequence execution, maintain accurate state information, and provide telemetry and event feedback adds to the challenge. This is not a trivial bug fix or simple feature addition; it is a significant enhancement that impacts how sequences are managed in the system.\n\nGiven these considerations, a difficulty score of 0.65 reflects the hard nature of the problem, requiring substantial effort and expertise in the F' framework and embedded systems programming, though it does not reach the level of \"Very Hard\" as it does not involve advanced algorithmic challenges or system-level redesigns.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "v2.4.0 did not fix `ImportError` raised by  `lxml`\nHello,\r\n\r\nThe changes made on v2.4.0 do not fix the original issue (#38). When installing `html_sanitizer` as a dependency (using `pip`) we still have the same ImportError :\r\n\r\n```\r\nImportError: lxml.html.clean module is now a separate project lxml_html_clean.\r\nInstall lxml[html_clean] or lxml_html_clean directly.\r\n```\r\n\r\nDuring `pip install`, we receive this warning:\r\n\r\n```\r\nCollecting lxml[html-clean]>=5.2.0 (from html_sanitizer>=2.4.0->-r .\\req-htmls.txt (line 1))\r\n  Obtaining dependency information for lxml[html-clean]>=5.2.0 from https://files.pythonhosted.org/packages/a7/64/eedb4435eb693812b9b517c6ce0beac932c1e45c4031df33688484fd83ea/lxml-5.2.0-cp311-cp311-win_amd64.whl.metadata\r\n  Using cached lxml-5.2.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\r\nWARNING: lxml 5.2.0 does not provide the extra 'html-clean'\r\n```\r\n\r\nThere seems to be a ongoing issue with `pip` (https://github.com/pypa/pip/issues/11445) not properly resolving extras when they contain underscores.\r\n\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex 4564272..67d9904 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -35,7 +35,8 @@ dynamic = [\n ]\n dependencies = [\n   \"beautifulsoup4\",\n-  \"lxml[html_clean]>=5.2.0\",\n+  \"lxml>=5.2.0\",\n+  \"lxml-html-clean>=0.1.0\",\n ]\n [project.urls]\n Homepage = \"https://github.com/matthiask/html-sanitizer/\"\ndiff --git a/tox.ini b/tox.ini\nindex a1ab64d..6425151 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -1,7 +1,8 @@\n [testenv]\n deps =\n     wheel\n-    lxml[html_clean]\n+    lxml\n+    lxml-html-clean\n     beautifulsoup4\n     coverage\n changedir = {toxinidir}\n", "instance_id": "matthiask__html-sanitizer-41", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an `ImportError` related to the `lxml` library and its `html_clean` module when installing `html_sanitizer` as a dependency. It provides specific error messages and a reference to a related `pip` issue, which helps in understanding the root cause (a problem with resolving extras containing underscores). However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior or the desired outcome beyond \"fixing the issue.\" Additionally, there are no examples of how the dependency should ideally resolve or any mention of potential edge cases related to different Python or `pip` versions. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue revolves around a dependency specification problem in a Python project, and the code changes required are minimal and straightforward. The modifications involve updating dependency declarations in `pyproject.toml` and `tox.ini` to explicitly separate `lxml` and `lxml-html-clean` instead of using the `html_clean` extra. This requires only basic knowledge of Python dependency management and configuration files, with no complex logic, algorithms, or deep understanding of the codebase architecture. The scope of changes is limited to two configuration files, with no impact on the actual application code or system design. There are no significant edge cases or error handling requirements mentioned or implied in the problem statement or code changes. Overall, this is a simple fix that a junior developer with basic Python experience could handle with minimal effort.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Behaviour of scalar fixed-point division by zero\nWe need to determine a coherent behavior for scalar fixed-point division by zero. Currently, the Python interpreter crashes on fixed-point division by zero if the resulting `bit` attribute is smaller than or equal to one limb (`floating point exception (core dumped)`). The interpreter hangs indefinitely if the resulting division `bit` attribute is greater than one limb.\r\n\r\nI suggest we raise a [`ZeroDivisionError`](https://docs.python.org/3/library/exceptions.html#ZeroDivisionError) on scalar fixed-point division by zero. For `APyFixedArray`, I believe the best option is to say it is undefined, only to guarantee that the program does not crash or hang indefinitely. Anything else is going to make SIMD execution very difficult.\r\n\r\n## Behaviour of other Python types\r\n```python3\r\n>>> 1.0 / 0.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nZeroDivisionError: float division by zero\r\n```\r\n\r\n```python3\r\n>>> import numpy as np\r\n>>> np.array([1.0, 1.0]) / np.array([0.0, 0.0])\r\nRuntimeWarning: divide by zero encountered in divide\r\narray([inf, inf])\r\n```\r\n\r\n```python3\r\n>>> np.array([1.0, 1.0]) // np.array([0.0, 0.0])\r\nRuntimeWarning: divide by zero encountered in floor_divide\r\narray([inf, inf])\r\n```\r\n\r\n```python3\r\n>>> np.array([1, 1]) // np.array([0, 0])\r\narray([0, 0])\r\n```\n", "patch": "diff --git a/src/apyfixed.cc b/src/apyfixed.cc\nindex a2239d0ed..65a1f7b78 100644\n--- a/src/apyfixed.cc\n+++ b/src/apyfixed.cc\n@@ -201,8 +201,8 @@ APyFixed APyFixed::operator*(const APyFixed& rhs) const\n APyFixed APyFixed::operator/(const APyFixed& rhs) const\n {\n     if (rhs.is_zero()) {\n-        // TODO: Raise a Python `ZeroDivisionError` here when possible\n-        throw nb::value_error(\"fixed-point division by zero\");\n+        PyErr_SetString(PyExc_ZeroDivisionError, \"fixed-point division by zero\");\n+        throw nb::python_error();\n     }\n \n     const int res_int_bits = int_bits() + rhs.frac_bits() + 1;\n", "instance_id": "apytypes__apytypes-416", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in defining the goal: to handle scalar fixed-point division by zero in a coherent manner by raising a `ZeroDivisionError` in Python. It provides context by comparing the behavior with other Python types (like float and NumPy arrays) and specifies different behaviors based on the size of the `bit` attribute (crashing or hanging). However, there are minor ambiguities and missing details. For instance, the statement mentions that for `APyFixedArray`, the behavior should be undefined but guaranteed not to crash or hang, yet it does not elaborate on how this should be implemented or what \"undefined\" behavior entails in practice. Additionally, while the problem focuses on scalar fixed-point division, it does not explicitly address potential edge cases beyond division by zero (e.g., overflow or underflow during bit calculations). Constraints on input sizes or bit attributes are also not fully specified. Overall, the problem is valid and mostly clear, but these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is minimal and localized to a single file (`apyfixed.cc`) and a specific function (`operator/` in the `APyFixed` class). The modification involves replacing a placeholder comment and a generic `value_error` with a proper Python `ZeroDivisionError` using `PyErr_SetString` and throwing a `python_error`. This is a straightforward change with no impact on the broader system architecture or interactions with other modules.\n\n2. **Number of Technical Concepts:** The solution requires basic knowledge of C++ exception handling and interfacing with Python via the `nanobind` library (indicated by the `nb::` namespace). Specifically, understanding how to set a Python exception (`PyErr_SetString`) and throw it using `nb::python_error()` is necessary. These concepts are relatively simple for someone familiar with Python-C++ bindings, and no advanced algorithms, design patterns, or domain-specific knowledge beyond fixed-point arithmetic basics are required.\n\n3. **Potential Edge Cases and Error Handling:** The problem explicitly focuses on the division-by-zero case, and the code change directly addresses this by raising an exception. No additional edge cases (e.g., overflow, underflow, or specific bit attribute constraints) are mentioned in the problem statement or reflected in the code diff. The error handling logic is minimal and straightforward, involving only the setting of a standard Python exception.\n\n4. **Overall Complexity:** The task does not require deep understanding of the codebase beyond the specific function being modified. The change is small in scope, with no performance considerations or complex logic involved. It is essentially a bug fix to replace improper error handling with the correct Python exception.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and minimal technical depth. It is slightly above the \"Very Easy\" range due to the need to understand Python-C++ exception handling, but it remains an easy problem overall.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FDS-2218] Update the README\n## problem\r\nThe `README.md` needed to be updated for clarity in certain parts, particularly the installation instructions and use-case examples.\r\n\r\n## solution\r\n- [x] Overall re-structure and stylistic changes\r\n- [x] Add a `TL;DR` section to provide a quick summary of `schematic` and its uses\r\n- [x] Improve developer setup and contribution instructions\r\n\r\nItems from Jira tickets:\r\n- [x] Update development environment instructions for a wider audience: [ticket](https://sagebionetworks.jira.com/browse/FDS-1172) \r\n- [x] Consolidate installation instructions between Confluence and GitHub: [ticket](https://sagebionetworks.jira.com/browse/FDS-2476)\r\n- [x] Point readers to service desk for help: [ticket](https://sagebionetworks.jira.com/browse/FDS-470)\r\n- [x] Fix hyperlink in **Testing** section: [ticket](https://sagebionetworks.jira.com/browse/FDS-2467)\r\n- [x] Fix CLI reference link: [ticket](https://sagebionetworks.jira.com/browse/FDS-2449)\r\n\r\n## testing\r\nN/A\n", "patch": "diff --git a/CONTRIBUTION.md b/CONTRIBUTION.md\nindex a9876d4df..930fbea81 100644\n--- a/CONTRIBUTION.md\n+++ b/CONTRIBUTION.md\n@@ -4,78 +4,78 @@ When contributing to this repository, please first discuss the change you wish t\n \n Please note we have a [code of conduct](CODE_OF_CONDUCT.md), please follow it in all your interactions with the project.\n \n-## How to contribute\n+## How to report bugs or feature requests\n \n-### Reporting bugs or feature requests\n-\n-You can use [Sage Bionetwork's FAIR Data service desk](https://sagebionetworks.jira.com/servicedesk/customer/portal/5/group/8) to **create bug and feature requests**. Providing enough details to the developers to verify and troubleshoot your issue is paramount:\n+You can **create bug and feature requests** through [Sage Bionetwork's FAIR Data service desk](https://sagebionetworks.jira.com/servicedesk/customer/portal/5/group/8). Providing enough details to the developers to verify and troubleshoot your issue is paramount:\n - **Provide a clear and descriptive title as well as a concise summary** of the issue to identify the problem.\n - **Describe the exact steps which reproduce the problem** in as many details as possible.\n - **Describe the behavior you observed after following the steps** and point out what exactly is the problem with that behavior.\n - **Explain which behavior you expected to see** instead and why.\n - **Provide screenshots of the expected or actual behaviour** where applicable.\n \n-### General contribution instructions\n+## How to contribute code\n \n-1. Follow the [Github docs](https://help.github.com/articles/fork-a-repo/) to make a copy (a fork) of the repository to your own Github account.\n-2. [Clone the forked repository](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github/cloning-a-repository) to your local machine so you can begin making changes.\n-3. Make sure this repository is set as the [upstream remote repository](https://docs.github.com/en/github/collaborating-with-pull-requests/working-with-forks/configuring-a-remote-for-a-fork) so you are able to fetch the latest commits.\n-4. Push all your changes to the `develop` branch of the forked repository.\n+### The development environment setup\n \n-*Note*: Make sure you have you have the latest version of the `develop` branch on your local machine.\n+For setting up your environment, please follow the instructions in the `README.md` under `Installation Guide For: Contributors`.\n \n-```\n-git checkout develop\n-git pull upstream develop\n-```\n+### The development workflow\n \n-5. Create pull requests to the upstream repository.\n+For new features, bugs, enhancements:\n \n-### The development lifecycle\n+#### 1. Branch Setup\n+* Pull the latest code from the develop branch in the upstream repository.\n+* Checkout a new branch formatted like so: `develop-<feature/fix-name>` from the develop branch\n \n-1. Pull the latest content from the `develop` branch of this central repository (not your fork).\n-2. Create a branch off the `develop` branch. Name the branch appropriately, either briefly summarizing the bug (ex., `spatil/add-restapi-layer`) or feature or simply use the issue number in the name (ex., `spatil/issue-414-fix`).\n-3. After completing work and testing locally, push the code to the appropriate branch on your fork.\n-4. In Github, create a pull request from the bug/feature branch of your fork to the `develop` branch of the central repository.\n+#### 2. Development Workflow\n+* Develop on your new branch.\n+* Ensure pyproject.toml and poetry.lock files are compatible with your environment.\n+* Add changed files for tracking and commit changes using [best practices](https://www.perforce.com/blog/vcs/git-best-practices-git-commit)\n+* Have granular commits: not \u201ctoo many\u201d file changes, and not hundreds of code lines of changes\n+* You can choose to create a draft PR if you prefer to develop this way\n \n-> A Sage Bionetworks engineer must review and accept your pull request. A code review (which happens with both the contributor and the reviewer present) is required for contributing.\n+#### 3. Branch Management\n+* Push code to `develop-<feature/fix-name>` in upstream repo:\n+  ```\n+  git push <upstream> develop-<feature/fix-name>\n+  ```\n+* Branch off `develop-<feature/fix-name>` if you need to work on multiple features associated with the same code base\n+* After feature work is complete and before creating a PR to the develop branch in upstream\n+    a. ensure that code runs locally\n+    b. test for logical correctness locally\n+    c. run `pre-commit` to style code if the hook is not installed\n+    c. wait for git workflow to complete (e.g. tests are run) on github\n \n-### Development environment setup\n+#### 4. Pull Request and Review\n+* Create a PR from `develop-<feature/fix-name>` into the develop branch of the upstream repo\n+* Request a code review on the PR\n+* Once code is approved merge in the develop branch. We suggest creating a merge commit for a cleaner commit history on the `develop` branch.\n+* Once the actions pass on the main branch, delete the `develop-<feature/fix-name>` branch\n \n-1. Install [package dependencies](https://sage-schematic.readthedocs.io/en/develop/README.html#installation-requirements-and-pre-requisites).\n-2. Clone the `schematic` package repository.\n+### Updating readthedocs documentation\n+1. Navigate to the docs directory.\n+2. Run make html to regenerate the build after changes.\n+3. Contact the development team to publish the updates.\n \n-```\n-git clone https://github.com/Sage-Bionetworks/schematic.git\n-```\n+*Helpful resources*:\n \n-3. [Create and activate](https://sage-schematic.readthedocs.io/en/develop/README.html#virtual-environment-setup) a virtual environment.\n-4. Run the following commands to build schematic and install the package along with all of its dependencies:\n+1. [Getting started with Sphinx](https://www.sphinx-doc.org/en/master/usage/quickstart.html)\n+2. [Installing Sphinx](https://www.sphinx-doc.org/en/master/usage/installation.html)\n \n-```\n-cd schematic  # change directory to schematic\n-git checkout develop  # switch to develop branch of schematic\n-poetry build # build source and wheel archives\n-pip install dist/schematicpy-x.y.z-py3-none-any.whl  # install wheel file\n-```\n-\n-*Note*: Use the appropriate version number (based on the version of the codebase you are pulling) while installing the wheel file above.\n-\n-5. [Obtain](https://sage-schematic.readthedocs.io/en/develop/README.html#obtain-google-credentials-file-s) appropriate Google credentials file(s).\n-6. [Obtain and Fill in](https://sage-schematic.readthedocs.io/en/develop/README.html#fill-in-configuration-file-s) the `config.yml` file and the `.synapseConfig` file as well as described in the `Fill in Configuration File(s)` part of the documentation.\n-7. [Run](https://docs.pytest.org/en/stable/usage.html) the test suite.\n+### Update toml file and lock file\n+If you install external libraries by using `poetry add <name of library>`, please make sure that you include `pyproject.toml` and `poetry.lock` file in your commit.\n \n-*Note*: To ensure that all tests run successfully, contact your DCC liason and request to be added to the `schematic-dev` [team](https://www.synapse.org/#!Team:3419888) on Synapse.\n+### Code style\n \n-8. To test new changes made to any of the modules within `schematic`, do the following:\n+To ensure consistent code formatting across the project, we use the `pre-commit` hook. You can manually run `pre-commit` across the respository before making a pull request like so:\n \n ```\n-# make changes to any files or modules\n-pip uninstall schematicpy  # uninstall package\n-poetry build\n-pip install dist/schematicpy-x.y.z-py3-none-any.whl  # install wheel file\n+pre-commit run --all-files\n ```\n \n+Further, please consult the [Google Python style guide](http://google.github.io/styleguide/pyguide.html) prior to contributing code to this project.\n+Be consistent and follow existing code conventions and spirit.\n+\n ## Release process\n \n Once the code has been merged into the `develop` branch on this repo, there are two processes that need to be completed to ensure a _release_ is complete.\n@@ -109,12 +109,13 @@ poetry publish  # publish the package to PyPI\n \n > You'll need to [register](https://pypi.org/account/register/) for a PyPI account before uploading packages to the package index. Similarly for [Test PyPI](https://test.pypi.org/account/register/) as well.\n \n-## Testing\n+## Testing \n \n-All code added to the client must have tests. The Python client uses pytest to run tests. The test code is located in the [tests](https://github.com/Sage-Bionetworks/schematic/tree/develop-docs-update/tests) subdirectory.\n+* All new code must include tests.\n \n-You can run the test suite in the following way:\n+* Tests are written using pytest and are located in the [tests/](https://github.com/Sage-Bionetworks/schematic/tree/develop/tests) subdirectory.\n \n+* Run tests with:\n ```\n pytest -vs tests/\n ```\n@@ -128,7 +129,3 @@ pytest -vs tests/\n 5. Once the PR is merged, leave the original copies on Synapse to maintain support for feature branches that were forked from `develop` before your update.\n    - If the old copies are problematic and need to be removed immediately (_e.g._ contain sensitive data), proceed with the deletion and alert the other contributors that they need to merge the latest `develop` branch into their feature branches for their tests to work.\n \n-## Code style\n-\n-* Please consult the [Google Python style guide](http://google.github.io/styleguide/pyguide.html) prior to contributing code to this project.\n-* Be consistent and follow existing code conventions and spirit.\ndiff --git a/README.md b/README.md\nindex cf1cd96f6..72a30b70f 100644\n--- a/README.md\n+++ b/README.md\n@@ -1,65 +1,119 @@\n # Schematic\n [![Build Status](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2FSage-Bionetworks%2Fschematic%2Fbadge%3Fref%3Ddevelop&style=flat)](https://actions-badge.atrox.dev/Sage-Bionetworks/schematic/goto?ref=develop) [![Documentation Status](https://readthedocs.org/projects/sage-schematic/badge/?version=develop)](https://sage-schematic.readthedocs.io/en/develop/?badge=develop) [![PyPI version](https://badge.fury.io/py/schematicpy.svg)](https://badge.fury.io/py/schematicpy)\n \n-# Table of contents\n+# TL;DR\n+\n+* `schematic` (Schema Engine for Manifest Ingress and Curation) is a python-based software tool that streamlines the retrieval, validation, and submission of metadata for biomedical datasets hosted on Sage Bionetworks' Synapse platform.\n+* Users can work with `schematic` in several ways, including through the CLI (see [Command Line Usage](#command-line-usage) for examples), through Docker (see [Docker Usage](#docker-usage) for examples), or with python.\n+* `schematic` needs to communicate with Synapse and Google Sheets in order for its processes to work. As such, users will need to set up their credentials for authentication with Synapse and the Google Sheets API.\n+* To get started with `schematic`, follow one of the Installation Guides depending on your use case:\n+   * [Installation Guide For: Schematic CLI users](#installation-guide-for-users)\n+   * [Installation Guide For: Contributors](#installation-guide-for-contributors)\n+\n+# Table of Contents\n - [Schematic](#schematic)\n-- [Table of contents](#table-of-contents)\n+- [TL;DR](#tldr)\n+- [Table of Contents](#table-of-contents)\n - [Introduction](#introduction)\n - [Installation](#installation)\n   - [Installation Requirements](#installation-requirements)\n-  - [Installation guide for Schematic CLI users](#installation-guide-for-schematic-cli-users)\n-  - [Installation guide for developers/contributors](#installation-guide-for-developerscontributors)\n-    - [Development environment setup](#development-environment-setup)\n-    - [Development process instruction](#development-process-instruction)\n-    - [Example For REST API ](#example-for-rest-api-)\n-      - [Use file path of `config.yml` to run API endpoints:](#use-file-path-of-configyml-to-run-api-endpoints)\n-      - [Use content of `config.yml` and `schematic_service_account_creds.json`as an environment variable to run API endpoints:](#use-content-of-configyml-and-schematic_service_account_credsjsonas-an-environment-variable-to-run-api-endpoints)\n-    - [Example For Schematic on mac/linux ](#example-for-schematic-on-maclinux-)\n-    - [Example For Schematic on Windows ](#example-for-schematic-on-windows-)\n-- [Other Contribution Guidelines](#other-contribution-guidelines)\n-  - [Updating readthedocs documentation](#updating-readthedocs-documentation)\n-  - [Update toml file and lock file](#update-toml-file-and-lock-file)\n-  - [Reporting bugs or feature requests](#reporting-bugs-or-feature-requests)\n+  - [Installation Guide For: Users](#installation-guide-for-users)\n+    - [1. Verify your python version](#1-verify-your-python-version)\n+    - [2. Set up your virtual environment](#2-set-up-your-virtual-environment)\n+      - [2a. Set up your virtual environment with `venv`](#2a-set-up-your-virtual-environment-with-venv)\n+      - [2b. Set up your virtual environment with `conda`](#2b-set-up-your-virtual-environment-with-conda)\n+    - [3. Install `schematic` dependencies](#3-install-schematic-dependencies)\n+    - [4. Set up configuration files](#4-set-up-configuration-files)\n+    - [5. Get your data model as a `JSON-LD` schema file](#5-get-your-data-model-as-a-json-ld-schema-file)\n+    - [6. Obtain Google credential files](#6-obtain-google-credential-files)\n+    - [7. Verify your setup](#7-verify-your-setup)\n+  - [Installation Guide For: Contributors](#installation-guide-for-contributors)\n+    - [1. Clone the `schematic` package repository](#1-clone-the-schematic-package-repository)\n+    - [2. Install `poetry`](#2-install-poetry)\n+    - [3. Start the virtual environment](#3-start-the-virtual-environment)\n+    - [4. Install `schematic` dependencies](#4-install-schematic-dependencies)\n+    - [5. Set up configuration files](#5-set-up-configuration-files)\n+    - [6. Obtain Google credential files](#6-obtain-google-credential-files)\n+    - [7. Set up pre-commit hooks](#7-set-up-pre-commit-hooks)\n+    - [8. Verify your setup](#8-verify-your-setup)\n - [Command Line Usage](#command-line-usage)\n-- [Testing](#testing)\n-  - [Updating Synapse test resources](#updating-synapse-test-resources)\n-- [Code style](#code-style)\n+- [Docker Usage](#docker-usage)\n+  - [Running the REST API](#running-the-rest-api)\n+    - [Example 1: Using the `config.yml` path](#example-1-using-the-configyml-path)\n+    - [Example 2: Use environment variables](#example-2-use-environment-variables)\n+  - [Running `schematic` to Validate Manifests](#running-schematic-to-validate-manifests)\n+    - [Example for macOS/Linux](#example-for-macoslinux)\n+    - [Example for Windows](#example-for-windows)\n - [Contributors](#contributors)\n \n+\n # Introduction\n SCHEMATIC is an acronym for _Schema Engine for Manifest Ingress and Curation_. The Python based infrastructure provides a _novel_ schema-based, metadata ingress ecosystem, that is meant to streamline the process of biomedical dataset annotation, metadata validation and submission to a data repository for various data contributors.\n \n # Installation\n ## Installation Requirements\n-* Python version 3.9.0\u2264x<3.11.0\n+* Your installed python version must be 3.9.0 \u2264 version < 3.11.0\n * You need to be a registered and certified user on [`synapse.org`](https://www.synapse.org/)\n \n-Note: Our credential policy for Google credentials in order to create Google sheet files from Schematic, see tutorial ['HERE'](https://scribehow.com/shared/Get_Credentials_for_Google_Drive_and_Google_Sheets_APIs_to_use_with_schematicpy__yqfcJz_rQVeyTcg0KQCINA). If you plan to use `config.yml`, please ensure that the path of `schematic_service_account_creds.json` is indicated there (see `google_sheets > service_account_creds` section)\n+> [!NOTE]  \n+> To create Google Sheets files from Schematic, please follow our credential policy for Google credentials. You can find a detailed tutorial [here](https://scribehow.com/shared/Get_Credentials_for_Google_Drive_and_Google_Sheets_APIs_to_use_with_schematicpy__yqfcJz_rQVeyTcg0KQCINA).\n+> If you're using config.yml, make sure to specify the path to `schematic_service_account_creds.json` (see the `google_sheets > service_account_creds` section for more information).\n \n-## Installation guide for Schematic CLI users\n-1. **Verifying Python Version Compatibility**\n+## Installation Guide For: Users\n \n-To ensure compatibility with Schematic, please follow these steps:\n+The instructions below assume you have already installed [python](https://www.python.org/downloads/), with the release version meeting the constraints set in the [Installation Requirements](#installation-requirements) section, and do not have a Python environment already active.\n \n-Check your own Python version:\n+### 1. Verify your python version\n+\n+Ensure your python version meets the requirements from the [Installation Requirements](#installation-requirements) section using the following command:\n ```\n python3 --version\n ```\n+If your current Python version is not supported by Schematic, you can switch to the supported version using a tool like [pyenv](https://github.com/pyenv/pyenv?tab=readme-ov-file#switch-between-python-versions). Follow the instructions in the pyenv documentation to install and switch between Python versions easily.\n+\n+> [!NOTE]\n+> You can double-check the current supported python version by opening up the [pyproject.toml](https://github.com/Sage-Bionetworks/schematic/blob/main/pyproject.toml#L39) file in this repository and find the supported versions of python in the script.\n+\n+### 2. Set up your virtual environment\n \n-Check the Supported Python Version: Open the pyproject.toml file in the Schematic repository to find the version of Python that is supported. You can view this file directly on GitHub [here](https://github.com/Sage-Bionetworks/schematic/blob/main/pyproject.toml#L39).\n+Once you are working with a python version supported by `schematic`, you will need to activate a virtual environment within which you can install the package. Below we will show how to create your virtual environment either with `venv` or with `conda`.\n \n-Switching Python Versions: If your current Python version is not supported by Schematic, you can switch to the supported version using tools like [pyenv](https://github.com/pyenv/pyenv?tab=readme-ov-file#switch-between-python-versions). Follow the instructions in the pyenv documentation to install and switch between Python versions easily.\n+#### 2a. Set up your virtual environment with `venv`\n \n-2. **Setting Up the Virtual Environment**\n+Python 3 has built-in support for virtual environments with the `venv` module, so you no longer need to install `virtualenv`:\n \n-After switching to the version of Python supported by Schematic, please activate a virtual environment within which you can install the package:\n ```\n python3 -m venv .venv\n source .venv/bin/activate\n ```\n-Note: Python 3 has built-in support for virtual environments with the venv module, so you no longer need to install virtualenv.\n \n-3. **Installing Schematic**\n+#### 2b. Set up your virtual environment with `conda`\n+\n+`conda` is a powerful package and environment management tool that allows users to create isolated environments used particularly in data science and machine learning workflows. If you would like to manage your environments with `conda`, continue reading:\n+\n+1. **Download your preferred `conda` installer**: Begin by [installing `conda`](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html). We personally recommend working with `Miniconda` which is a lightweight installer for `conda` that includes only `conda` and its dependencies.\n+\n+2. **Execute the `conda` installer**: Once you have downloaded your preferred installer, execute it using `bash` or `zsh`, depending on the shell configured for your terminal environment. For example:\n+\n+   ```\n+   bash Miniconda3-latest-MacOSX-arm64.sh\n+   ```\n+\n+3. **Verify your `conda` setup**: Follow the prompts to complete your setup. Then verify your setup by running the `conda` command.\n+   \n+4. **Create your `schematic` environment**: Begin by creating a fresh `conda` environment for `schematic` like so:\n+\n+   ```\n+   conda create --name 'schematicpy' python=3.10\n+   ```\n+\n+5. **Activate the environment**: Once your environment is set up, you can now activate your new environment with `conda`:\n+\n+   ```\n+   conda activate schematicpy\n+   ```\n+\n+### 3. Install `schematic` dependencies\n \n Install the package using [pip](https://pip.pypa.io/en/stable/quickstart/):\n \n@@ -67,140 +121,248 @@ Install the package using [pip](https://pip.pypa.io/en/stable/quickstart/):\n python3 -m pip install schematicpy\n ```\n \n-If you run into error: Failed building wheel for numpy, the error might be able to resolve by upgrading pip. Please try to upgrade pip by:\n+If you run into `ERROR: Failed building wheel for numpy`, the error might be able to resolve by upgrading pip. Please try to upgrade pip by:\n \n ```\n pip3 install --upgrade pip\n ```\n \n-## Installation guide for developers/contributors \n+### 4. Set up configuration files\n+\n+The following section will walk through setting up your configuration files with your credentials to allow for communication between `schematic` and the Synapse API.\n+\n+There are two main configuration files that need to be created + modified:\n+- `.synapseConfig`\n+- `config.yml`\n+\n+**Create and modify the `.synapseConfig`**\n+\n+The `.synapseConfig` file is what enables communication between `schematic` and the Synapse API using your credentials.\n+You can automatically generate a `.synapseConfig` file by running the following in your command line and following the prompts.\n+\n+>[!TIP]\n+>You can generate a new authentication token on the Synapse website by going to `Account Settings` > `Personal Access Tokens`.\n+\n+```\n+synapse config\n+```\n+\n+After following the prompts, a new `.synapseConfig` file and `.synapseCache` folder will be created in your home directory. You can view these hidden\n+assets in your home directory with the following command:\n+\n+```\n+ls -a ~\n+```\n+\n+The `.synapseConfig` is used to log into Synapse if you are not using an environment variable (i.e. `SYNAPSE_ACCESS_TOKEN`) for authentication, and the `.synapseCache` is where your assets are stored if you are not working with the CLI and/or you have specified `.synapseCache` as the location in which to store your manfiests, in your `config.yml` (more on the `config.yml` below).\n+\n+**Create and modify the `config.yml`**\n+\n+In this repository there is a `config_example.yml` file with default configurations to various components that are required before running `schematic`,\n+such as the Synapse ID of the main file view containing all your project assets, the base name of your manifest files, etc.\n+\n+Download the `config_example.yml` as a new file called `config.yml` and modify its contents according to your use case.\n+\n+For example, if you wanted to change the folder where manifests are downloaded your config should look like:\n+\n+```text\n+manifest:\n+  manifest_folder: \"my_manifest_folder_path\"\n+```\n+\n+> [!IMPORTANT]\n+> Be sure to update your `config.yml` with the location of your `.synapseConfig` created in the step above, to avoid authentication errors. Paths can be specified relative to the `config.yml` file or as absolute paths.\n+\n+> [!NOTE]\n+> `config.yml` is ignored by git.\n+\n+### 5. Get your data model as a `JSON-LD` schema file\n+\n+Now you need a schema file, e.g. `model.jsonld`, to have a data model that schematic can work with. While you can download a super basic example data model [here](https://raw.githubusercontent.com/Sage-Bionetworks/schematic/refs/heads/develop/tests/data/example.model.jsonld), you\u2019ll probably be working with a DCC-specific data model. For non-Sage employees/contributors using the CLI, you might care only about the minimum needed artifact, which is the  `.jsonld`; locate and download only that from the right repo.\n+\n+Here are some example repos with schema files:\n+* https://github.com/ncihtan/data-models/\n+* https://github.com/nf-osi/nf-metadata-dictionary/\n+\n+> [!IMPORTANT]\n+> Your local working directory would typically have `model.jsonld` and `config.yml` side-by-side. The path to your data model should match what is in `config.yml`\n+\n+### 6. Obtain Google credential files\n+\n+Any function that interacts with a google sheet (such as `schematic manifest get`) requires google cloud credentials.\n \n-When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.\n+1. **Option 1**: [Here](https://scribehow.com/shared/Get_Credentials_for_Google_Drive_and_Google_Sheets_APIs_to_use_with_schematicpy__yqfcJz_rQVeyTcg0KQCINA?referrer=workspace)\u2019s a step-by-step guide on how to create these credentials in Google Cloud.\n+   * Depending on your institution's policies, your institutional Google account may or may not have the required permissions to complete this. A possible workaround is to use a personal or temporary Google account.\n+\n+> [!WARNING]\n+> At the time of writing, Sage Bionetworks employees do not have the appropriate permissions to create projects with their Sage Bionetworks Google accounts. You would follow instructions using a personal Google account. \n+\n+2. **Option 2**: Ask your DCC/development team if they have credentials previously set up with a service account.\n+\n+Once you have obtained credentials, be sure that the json file generated is named in the same way as the `service_acct_creds` parameter in your `config.yml` file.\n+\n+> [!NOTE]\n+> Running `schematic init` is no longer supported due to security concerns. To obtain  `schematic_service_account_creds.json`, please follow the instructions [here](https://scribehow.com/shared/Enable_Google_Drive_and_Google_Sheets_APIs_for_project__yqfcJz_rQVeyTcg0KQCINA). \n+schematic uses Google\u2019s API to generate google sheet templates that users fill in to provide (meta)data.\n+Most Google sheet functionality could be authenticated with service account. However, more complex Google sheet functionality\n+requires token-based authentication. As browser support that requires the token-based authentication diminishes, we are hoping to deprecate\n+token-based authentication and keep only service account authentication in the future. \n+\n+> [!NOTE]\n+> Use the ``schematic_service_account_creds.json`` file for the service\n+> account mode of authentication (*for Google services/APIs*). Service accounts\n+> are special Google accounts that can be used by applications to access Google APIs\n+> programmatically via OAuth2.0, with the advantage being that they do not require\n+> human authorization. \n+\n+### 7. Verify your setup\n+After running the steps above, your setup is complete, and you can test it on a `python` instance or by running a command based on the examples in the [Command Line Usage](#command-line-usage) section.\n+\n+## Installation Guide For: Contributors\n+\n+The instructions below assume you have already installed [python](https://www.python.org/downloads/), with the release version meeting the constraints set in the [Installation Requirements](#installation-requirements) section, and do not have an environment already active (e.g. with `pyenv`). For development, we recommend working with versions > python 3.9 to avoid issues with `pre-commit`'s default hook configuration.\n+\n+When contributing to this repository, please first discuss the change you wish to make via the [service desk](https://sagebionetworks.jira.com/servicedesk/customer/portal/5/group/8) so that we may track these changes.\n+\n+Once you have finished setting up your development environment using the instructions below, please follow the guidelines in [CONTRIBUTION.md](https://github.com/Sage-Bionetworks/schematic/blob/develop-fds-2218-update-readme/CONTRIBUTION.md) during your development.\n \n Please note we have a [code of conduct](CODE_OF_CONDUCT.md), please follow it in all your interactions with the project.\n \n-### Development environment setup\n-1. Clone the `schematic` package repository.\n+### 1. Clone the `schematic` package repository\n+\n+For development, you will be working with the latest version of `schematic` on the repository to ensure compatibility between its latest state and your changes. Ensure your current working directory is where\n+you would like to store your local fork before running the following command:\n+\n ```\n git clone https://github.com/Sage-Bionetworks/schematic.git\n ```\n-2. Install `poetry` (version 1.3.0 or later) using either the [official installer](https://python-poetry.org/docs/#installing-with-the-official-installer) or [pipx](https://python-poetry.org/docs/#installing-with-pipx). If you have an older installation of Poetry, we recommend uninstalling it first. \n \n-3. Start the virtual environment by doing: \n+### 2. Install `poetry` \n+\n+Install `poetry` (version 1.3.0 or later) using either the [official installer](https://python-poetry.org/docs/#installing-with-the-official-installer) or `pip`. If you have an older installation of Poetry, we recommend uninstalling it first.\n+\n+```\n+pip install poetry\n+```\n+\n+Check to make sure your version of poetry is > v1.3.0\n+\n+```\n+poetry --version\n+```\n+\n+### 3. Start the virtual environment\n+\n+`cd` into your cloned `schematic` repository, and initialize the virtual environment using the following command with `poetry`:\n+\n ```\n poetry shell\n ```\n-4. Install the dependencies by doing: \n+\n+To make sure your poetry version and python version are consistent with the versions you expect, you can run the following command:\n+\n+```\n+poetry debug info\n+```\n+\n+### 4. Install `schematic` dependencies\n+\n+Before you begin, make sure you are in the latest `develop` of the repository.\n+\n+The following command will install the dependencies based on what we specify in the `poetry.lock` file of this repository. If this step is taking a long time, try to go back to Step 2 and check your version of `poetry`. Alternatively, you can try deleting the lock file and regenerate it by doing `poetry install` (Please note this method should be used as a last resort because this would force other developers to change their development environment)\n+\n ```\n poetry install --all-extras\n ```\n-This command will install the dependencies based on what we specify in poetry.lock. If this step is taking a long time, try to go back to step 2 and check your version of poetry. Alternatively, you could also try deleting the lock file and regenerate it by doing `poetry install` (Please note this method should be used as a last resort because this would force other developers to change their development environment)\n \n+### 5. Set up configuration files\n \n-5. Fill in credential files: \n-*Note*: If you won't interact with Synapse, please ignore this section.\n+The following section will walk through setting up your configuration files with your credentials to allow for communication between `schematic` and the Synapse API.\n \n-There are two main configuration files that need to be edited:\n-- config.yml\n-- [synapseConfig](https://raw.githubusercontent.com/Sage-Bionetworks/synapsePythonClient/master/synapseclient/.synapseConfig)\n+There are two main configuration files that need to be created + modified:\n+- `.synapseConfig`\n+- `config.yml`\n \n-<strong>Configure .synapseConfig File</strong>\n+**Create and modify the `.synapseConfig`**\n \n-Download a copy of the ``.synapseConfig`` file, open the file in the editor of your \n-choice and edit the `username` and `authtoken` attribute under the `authentication` \n-section. **Note:** You must place the file at the root of the project like\n-`{project_root}/.synapseConfig` in order for any authenticated tests to work.\n+The `.synapseConfig` file is what enables communication between `schematic` and the Synapse API using your credentials.\n+You can automatically generate a `.synapseConfig` file by running the following in your command line and following the prompts.\n \n-*Note*: You could also visit [configparser](https://docs.python.org/3/library/configparser.html#module-configparser>) doc to see the format that `.synapseConfig` must have. For instance:\n->[authentication]<br> username = ABC <br> authtoken = abc\n+>[!TIP]\n+>You can generate a new authentication token on the Synapse website by going to `Account Settings` > `Personal Access Tokens`.\n \n-<strong>Configure config.yml File</strong>\n+```\n+synapse config\n+```\n \n-There are some defaults in schematic that can be configured. These fields are in ``config_example.yml``:\n+After following the prompts, a new `.synapseConfig` file and `.synapseCache` folder will be created in your home directory. You can view these hidden\n+assets in your home directory with the following command:\n \n-```text\n+```\n+ls -a ~\n+```\n \n-# This is an example config for Schematic.\n-# All listed values are those that are the default if a config is not used.\n-# Save this as config.yml, this will be gitignored.\n-# Remove any fields in the config you don't want to change\n-# Change the values of any fields you do want to change\n-\n-\n-# This describes where assets such as manifests are stored\n-asset_store:\n-  # This is when assets are stored in a synapse project\n-  synapse:\n-    # Synapse ID of the file view listing all project data assets.\n-    master_fileview_id: \"syn23643253\"\n-    # Path to the synapse config file, either absolute or relative to this file\n-    config: \".synapseConfig\"\n-    # Base name that manifest files will be saved as\n-    manifest_basename: \"synapse_storage_manifest\"\n-\n-# This describes information about manifests as it relates to generation and validation\n-manifest:\n-  # Location where manifests will saved to\n-  manifest_folder: \"manifests\"\n-  # Title or title prefix given to generated manifest(s)\n-  title: \"example\"\n-  # Data types of manifests to be generated or data type (singular) to validate manifest against\n-  data_type:\n-    - \"Biospecimen\"\n-    - \"Patient\"\n-\n-# Describes the location of your schema\n-model:\n-  # Location of your schema jsonld, it must be a path relative to this file or absolute\n-  location: \"tests/data/example.model.jsonld\"\n-\n-# This section is for using google sheets with Schematic\n-google_sheets:\n-  # Path to the synapse config file, either absolute or relative to this file\n-  service_acct_creds: \"schematic_service_account_creds.json\"\n-  # When doing google sheet validation (regex match) with the validation rules.\n-  #   true is alerting the user and not allowing entry of bad values.\n-  #   false is warning but allowing the entry on to the sheet.\n-  strict_validation: true\n-```\n-\n-If you want to change any of these copy ``config_example.yml`` to ``config.yml``, change any fields you want to, and remove any fields you don't.\n-\n-For example if you wanted to change the folder where manifests are downloaded your config should look like:\n+The `.synapseConfig` is used to log into Synapse if you are not using an environment variable (i.e. `SYNAPSE_ACCESS_TOKEN`) for authentication, and the `.synapseCache` is where your assets are stored if you are not working with the CLI and/or you have specified `.synapseCache` as the location in which to store your manfiests, in your `config.yml` (more on the `config.yml` below).\n \n-```text\n+> [!IMPORTANT]\n+> When developing on `schematic`, keep your `.synapseConfig` in your current working directory to avoid authentication errors.\n+\n+**Create and modify the `config.yml`**\n+\n+In this repository there is a `config_example.yml` file with default configurations to various components that are required before running `schematic`,\n+such as the Synapse ID of the main file view containing all your project assets, the base name of your manifest files, etc.\n \n+Copy the contents of the `config_example.yml` (located in the base directory of the cloned `schematic` repo) into a new file called `config.yml`\n+\n+```\n+cp config_example.yml config.yml\n+```\n+\n+Once you've copied the file, modify its contents according to your use case. For example, if you wanted to change the folder where manifests are downloaded your config should look like:\n+\n+```text\n manifest:\n   manifest_folder: \"my_manifest_folder_path\"\n ```\n \n-_Note_: `config.yml` is ignored by git.\n+> [!IMPORTANT]\n+> Be sure to update your `config.yml` with the location of your `.synapseConfig` created in the step above, to avoid authentication errors. Paths can be specified relative to the `config.yml` file or as absolute paths.\n \n-_Note_: Paths can be specified relative to the `config.yml` file or as absolute paths.\n+> [!NOTE]\n+> `config.yml` is ignored by git.\n \n-6. Login to Synapse by using the command line\n-On the CLI in your virtual environment, run the following command: \n-```\n-synapse login -u <synapse username> -p <synapse password> --rememberMe\n-```\n+### 6. Obtain Google credential files\n \n-7. Obtain Google credential Files\n-Running `schematic init` is no longer supported due to security concerns. To obtain  `schematic_service_account_creds.json`, please follow the instructions [here](https://scribehow.com/shared/Enable_Google_Drive_and_Google_Sheets_APIs_for_project__yqfcJz_rQVeyTcg0KQCINA). \n+Any function that interacts with a google sheet (such as `schematic manifest get`) requires google cloud credentials.\n \n-> As v22.12.1 version of schematic, using `token` mode of authentication (in other words, using `token.pickle` and `credentials.json`) is no longer supported due to Google's decision to move away from using OAuth out-of-band (OOB) flow. Click [here](https://developers.google.com/identity/protocols/oauth2/resources/oob-migration) to learn more. \n+1. **Option 1**: [Here](https://scribehow.com/shared/Get_Credentials_for_Google_Drive_and_Google_Sheets_APIs_to_use_with_schematicpy__yqfcJz_rQVeyTcg0KQCINA?referrer=workspace)\u2019s a step-by-step guide on how to create these credentials in Google Cloud.\n+   * Depending on your institution's policies, your institutional Google account may or may not have the required permissions to complete this. A possible workaround is to use a personal or temporary Google account.\n \n-*Notes*: Use the ``schematic_service_account_creds.json`` file for the service\n-account mode of authentication (*for Google services/APIs*). Service accounts \n-are special Google accounts that can be used by applications to access Google APIs \n-programmatically via OAuth2.0, with the advantage being that they do not require \n-human authorization. \n+> [!WARNING]\n+> At the time of writing, Sage Bionetworks employees do not have the appropriate permissions to create projects with their Sage Bionetworks Google accounts. You would follow instructions using a personal Google account. \n \n-*Background*: schematic uses Google\u2019s API to generate google sheet templates that users fill in to provide (meta)data.\n+2. **Option 2**: Ask your DCC/development team if they have credentials previously set up with a service account.\n+\n+Once you have obtained credentials, be sure that the json file generated is named in the same way as the `service_acct_creds` parameter in your `config.yml` file.\n+\n+> [!IMPORTANT]\n+> For testing, make sure there is no environment variable `SCHEMATIC_SERVICE_ACCOUNT_CREDS`. Check the file `.env` to ensure this is not set. Also, check that config files used for testing, such as `config_example.yml` do not contain service_acct_creds_synapse_id.\n+\n+> [!NOTE]\n+> Running `schematic init` is no longer supported due to security concerns. To obtain  `schematic_service_account_creds.json`, please follow the instructions [here](https://scribehow.com/shared/Enable_Google_Drive_and_Google_Sheets_APIs_for_project__yqfcJz_rQVeyTcg0KQCINA). \n+schematic uses Google\u2019s API to generate google sheet templates that users fill in to provide (meta)data.\n Most Google sheet functionality could be authenticated with service account. However, more complex Google sheet functionality\n requires token-based authentication. As browser support that requires the token-based authentication diminishes, we are hoping to deprecate\n token-based authentication and keep only service account authentication in the future. \n \n-8. Set up pre-commit hooks\n+> [!NOTE]\n+> Use the ``schematic_service_account_creds.json`` file for the service\n+> account mode of authentication (*for Google services/APIs*). Service accounts\n+> are special Google accounts that can be used by applications to access Google APIs\n+> programmatically via OAuth2.0, with the advantage being that they do not require\n+> human authorization. \n+\n+### 7. Set up pre-commit hooks\n \n This repository is configured to utilize pre-commit hooks as part of the development process. To enable these hooks, please run the following command and look for the following success message:\n ```\n@@ -208,35 +370,55 @@ $ pre-commit install\n pre-commit installed at .git/hooks/pre-commit\n ```\n \n-### Development process instruction\n+You can run `pre-commit` manually across the entire repository like so:\n \n-For new features, bugs, enhancements\n+```\n+pre-commit run --all-files\n+```\n \n-1. Pull the latest code from [develop branch in the upstream repo](https://github.com/Sage-Bionetworks/schematic)\n-2. Checkout a new branch develop-<feature/fix-name> from the develop branch\n-3. Do development on branch develop-<feature/fix-name>\n-   a. may need to ensure that schematic poetry toml and lock files are compatible with your local environment\n-4. Add changed files for tracking and commit changes using [best practices](https://www.perforce.com/blog/vcs/git-best-practices-git-commit)\n-5. Have granular commits: not \u201ctoo many\u201d file changes, and not hundreds of code lines of changes\n-6. Commits with work in progress are encouraged:\n-   a. add WIP to the beginning of the commit message for \u201cWork In Progress\u201d commits\n-7. Keep commit messages descriptive but less than a page long, see best practices\n-8. Push code to develop-<feature/fix-name> in upstream repo\n-9. Branch out off develop-<feature/fix-name> if needed to work on multiple features associated with the same code base\n-10. After feature work is complete and before creating a PR to the develop branch in upstream\n-    a. ensure that code runs locally\n-    b. test for logical correctness locally\n-    c. wait for git workflow to complete (e.g. tests are run) on github\n-11. Create a PR from develop-<feature/fix-name> into the develop branch of the upstream repo\n-12. Request a code review on the PR\n-13. Once code is approved merge in the develop branch\n-14. Delete the develop-<feature/fix-name> branch\n+After running this step, your setup is complete, and you can test it on a python instance or by running a command based on the examples in the [Command Line Usage](#command-line-usage) section.\n \n-*Note*: Make sure you have the latest version of the `develop` branch on your local machine.\n+### 8. Verify your setup\n+After running the steps above, your setup is complete, and you can test it on a `python` instance or by running a command based on the examples in the [Command Line Usage](#command-line-usage) section.\n \n-### Example For REST API <br>\n+# Command Line Usage\n+1. Generate a new manifest as a google sheet\n \n-#### Use file path of `config.yml` to run API endpoints: \n+```\n+schematic manifest -c /path/to/config.yml get -dt <your data type> -s\n+```\n+\n+2. Grab an existing manifest from synapse \n+\n+```\n+schematic manifest -c /path/to/config.yml get -dt <your data type> -d <your synapse dataset folder id> -s\n+```\n+\n+3. Validate a manifest\n+\n+```\n+schematic model -c /path/to/config.yml validate -dt <your data type> -mp <your csv manifest path>\n+```\n+\n+4. Submit a manifest as a file\n+\n+```\n+schematic model -c /path/to/config.yml submit -mp <your csv manifest path> -d <your synapse dataset folder id> -vc <your data type> -mrt file_only\n+```\n+\n+Please visit more documentation [here](https://sage-schematic.readthedocs.io/en/stable/cli_reference.html#) for more information. \n+\n+# Docker Usage\n+\n+Here we will demonstrate how to run `schematic` with Docker, with different use-cases for running API endpoints, validating the manifests, and\n+using how to use `schematic` based on your OS (macOS/Linux).\n+\n+### Running the REST API\n+\n+Use the Docker image to run `schematic`s REST API. You can either use the file path for the `config.yml` created using the installation instructions,\n+or set up authentication with environment variables.\n+\n+#### Example 1: Using the `config.yml` path \n ```\n docker run --rm -p 3001:3001 \\\n   -v $(pwd):/schematic -w /schematic --name schematic \\\n@@ -246,7 +428,7 @@ docker run --rm -p 3001:3001 \\\n   python /usr/src/app/run_api.py\n ``` \n \n-#### Use content of `config.yml` and `schematic_service_account_creds.json`as an environment variable to run API endpoints: \n+#### Example 2: Use environment variables\n 1. save content of `config.yml` as to environment variable `SCHEMATIC_CONFIG_CONTENT` by doing: `export SCHEMATIC_CONFIG_CONTENT=$(cat /path/to/config.yml)`\n \n 2. Similarly, save the content of `schematic_service_account_creds.json` as `SERVICE_ACCOUNT_CREDS` by doing: `export SERVICE_ACCOUNT_CREDS=$(cat /path/to/schematic_service_account_creds.json)`\n@@ -262,11 +444,18 @@ docker run --rm -p 3001:3001 \\\n   sagebionetworks/schematic \\\n   python /usr/src/app/run_api.py\n ``` \n+### Running `schematic` to Validate Manifests\n+You can also use Docker to run `schematic` commands like validating manifests. Below are examples for different platforms.\n \n+#### Example for macOS/Linux\n \n-### Example For Schematic on mac/linux <br>\n-To run example below, first clone schematic into your home directory  `git clone https://github.com/sage-bionetworks/schematic ~/schematic` <br>\n-Then update .synapseConfig with your credentials\n+1. Clone the repository:\n+```\n+git clone https://github.com/sage-bionetworks/schematic ~/schematic\n+```\n+2. Update the `.synapseConfig` with your credentials. See the installation instructions for how to do this.\n+\n+3. Run Docker:\n ```\n docker run \\\n   -v ~/schematic:/schematic \\\n@@ -280,7 +469,9 @@ docker run \\\n   -js /schematic/tests/data/example.model.jsonld\n ``` \n \n-### Example For Schematic on Windows <br>\n+#### Example for Windows\n+\n+Run the following command to validate manifests:\n ```\n docker run -v %cd%:/schematic \\\n   -w /schematic \\\n@@ -290,82 +481,6 @@ docker run -v %cd%:/schematic \\\n   -c config.yml validate -mp tests/data/mock_manifests/inValid_Test_Manifest.csv -dt MockComponent -js /schematic/data/example.model.jsonld\n ```\n \n-# Other Contribution Guidelines\n-## Updating readthedocs documentation\n-1. `cd docs`\n-2. After making relevant changes, you could run the `make html` command to re-generate the `build` folder.\n-3. Please contact the dev team to publish your updates\n-\n-*Other helpful resources*:\n-\n-1. [Getting started with Sphinx](https://haha.readthedocs.io/en/latest/intro/getting-started-with-sphinx.html)\n-2. [Installing Sphinx](https://haha.readthedocs.io/en/latest/intro/getting-started-with-sphinx.html)\n-\n-## Update toml file and lock file\n-If you install external libraries by using `poetry add <name of library>`, please make sure that you include `pyproject.toml` and `poetry.lock` file in your commit.\n-\n-## Reporting bugs or feature requests\n-You can **create bug and feature requests** through [Sage Bionetwork's FAIR Data service desk](https://sagebionetworks.jira.com/servicedesk/customer/portal/5/group/8). Providing enough details to the developers to verify and troubleshoot your issue is paramount:\n-- **Provide a clear and descriptive title as well as a concise summary** of the issue to identify the problem.\n-- **Describe the exact steps which reproduce the problem** in as many details as possible.\n-- **Describe the behavior you observed after following the steps** and point out what exactly is the problem with that behavior.\n-- **Explain which behavior you expected to see** instead and why.\n-- **Provide screenshots of the expected or actual behaviour** where applicable.\n-\n-# Command Line Usage\n-1. Generate a new manifest as a google sheet\n-\n-```\n-schematic manifest -c /path/to/config.yml get -dt <your data type> -s\n-```\n-\n-2. Grab an existing manifest from synapse \n-\n-```\n-schematic manifest -c /path/to/config.yml get -dt <your data type> -d <your synapse dataset folder id> -s\n-```\n-\n-3. Validate a manifest\n-\n-```\n-schematic model -c /path/to/config.yml validate -dt <your data type> -mp <your csv manifest path>\n-```\n-\n-4. Submit a manifest as a file\n-\n-```\n-schematic model -c /path/to/config.yml submit -mp <your csv manifest path> -d <your synapse dataset folder id> -vc <your data type> -mrt file_only\n-```\n-\n-Please visit more documentation [here](https://sage-schematic.readthedocs.io/en/develop/cli_reference.html) for more information. \n-\n-\n-\n-# Testing \n-\n-All code added to the client must have tests. The Python client uses pytest to run tests. The test code is located in the [tests](https://github.com/Sage-Bionetworks/schematic/tree/develop-docs-update/tests) subdirectory.\n-\n-You can run the test suite in the following way:\n-\n-```\n-pytest -vs tests/\n-```\n-\n-## Updating Synapse test resources\n-\n-1. Duplicate the entity being updated (or folder if applicable).\n-2. Edit the duplicates (_e.g._ annotations, contents, name).\n-3. Update the test suite in your branch to use these duplicates, including the expected values in the test assertions.\n-4. Open a PR as per the usual process (see above).\n-5. Once the PR is merged, leave the original copies on Synapse to maintain support for feature branches that were forked from `develop` before your update.\n-   - If the old copies are problematic and need to be removed immediately (_e.g._ contain sensitive data), proceed with the deletion and alert the other contributors that they need to merge the latest `develop` branch into their feature branches for their tests to work.\n-\n-# Code style\n-\n-* Please consult the [Google Python style guide](http://google.github.io/styleguide/pyguide.html) prior to contributing code to this project.\n-* Be consistent and follow existing code conventions and spirit.\n-\n-\n # Contributors\n \n Main contributors and developers:\ndiff --git a/schematic/manifest/generator.py b/schematic/manifest/generator.py\nindex d954506a5..47acad4b4 100644\n--- a/schematic/manifest/generator.py\n+++ b/schematic/manifest/generator.py\n@@ -27,6 +27,7 @@\n     build_service_account_creds,\n     execute_google_api_requests,\n     export_manifest_drive_service,\n+    google_api_execute_wrapper,\n )\n from schematic.utils.schema_utils import (\n     DisplayLabelType,\n@@ -190,11 +191,11 @@ def _gdrive_copy_file(self, origin_file_id, copy_title):\n         copied_file = {\"name\": copy_title}\n \n         # return new copy sheet ID\n-        return (\n+        return google_api_execute_wrapper(\n             self.drive_service.files()\n             .copy(fileId=origin_file_id, body=copied_file)\n-            .execute()[\"id\"]\n-        )\n+            .execute\n+        )[\"id\"]\n \n     def _create_empty_manifest_spreadsheet(self, title: str) -> str:\n         \"\"\"\n@@ -215,12 +216,11 @@ def _create_empty_manifest_spreadsheet(self, title: str) -> str:\n         else:\n             spreadsheet_body = {\"properties\": {\"title\": title}}\n \n-            spreadsheet_id = (\n+            spreadsheet_id = google_api_execute_wrapper(\n                 self.sheet_service.spreadsheets()\n                 .create(body=spreadsheet_body, fields=\"spreadsheetId\")\n-                .execute()\n-                .get(\"spreadsheetId\")\n-            )\n+                .execute\n+            ).get(\"spreadsheetId\")\n \n         return spreadsheet_id\n \n@@ -265,7 +265,7 @@ def callback(request_id, response, exception):\n                 fields=\"id\",\n             )\n         )\n-        batch.execute()\n+        google_api_execute_wrapper(batch.execute)\n \n     def _store_valid_values_as_data_dictionary(\n         self, column_id: int, valid_values: list, spreadsheet_id: str\n@@ -297,7 +297,7 @@ def _store_valid_values_as_data_dictionary(\n             + str(len(values) + 1)\n         )\n         valid_values = [{\"userEnteredValue\": \"=\" + target_range}]\n-        response = (\n+        response = google_api_execute_wrapper(\n             self.sheet_service.spreadsheets()\n             .values()\n             .update(\n@@ -306,7 +306,7 @@ def _store_valid_values_as_data_dictionary(\n                 valueInputOption=\"RAW\",\n                 body=body,\n             )\n-            .execute()\n+            .execute\n         )\n         return valid_values\n \n@@ -560,15 +560,31 @@ def _gs_add_and_format_columns(self, required_metadata_fields, spreadsheet_id):\n         range = \"Sheet1!A1:\" + str(end_col_letter) + \"1\"\n \n         # adding columns\n-        self.sheet_service.spreadsheets().values().update(\n-            spreadsheetId=spreadsheet_id, range=range, valueInputOption=\"RAW\", body=body\n-        ).execute()\n+        google_api_execute_wrapper(\n+            self.sheet_service.spreadsheets()\n+            .values()\n+            .update(\n+                spreadsheetId=spreadsheet_id,\n+                range=range,\n+                valueInputOption=\"RAW\",\n+                body=body,\n+            )\n+            .execute\n+        )\n \n         # adding columns to 2nd sheet that can be used for storing data validation ranges (this avoids limitations on number of dropdown items in excel and openoffice)\n         range = \"Sheet2!A1:\" + str(end_col_letter) + \"1\"\n-        self.sheet_service.spreadsheets().values().update(\n-            spreadsheetId=spreadsheet_id, range=range, valueInputOption=\"RAW\", body=body\n-        ).execute()\n+        google_api_execute_wrapper(\n+            self.sheet_service.spreadsheets()\n+            .values()\n+            .update(\n+                spreadsheetId=spreadsheet_id,\n+                range=range,\n+                valueInputOption=\"RAW\",\n+                body=body,\n+            )\n+            .execute\n+        )\n \n         # format column header row\n         header_format_body = {\n@@ -612,10 +628,10 @@ def _gs_add_and_format_columns(self, required_metadata_fields, spreadsheet_id):\n             ]\n         }\n \n-        response = (\n+        response = google_api_execute_wrapper(\n             self.sheet_service.spreadsheets()\n             .batchUpdate(spreadsheetId=spreadsheet_id, body=header_format_body)\n-            .execute()\n+            .execute\n         )\n         return response, ordered_metadata_fields\n \n@@ -664,13 +680,13 @@ def _gs_add_additional_metadata(\n             \"data\": data,\n         }\n \n-        response = (\n+        response = google_api_execute_wrapper(\n             self.sheet_service.spreadsheets()\n             .values()\n             .batchUpdate(\n                 spreadsheetId=spreadsheet_id, body=batch_update_values_request_body\n             )\n-            .execute()\n+            .execute\n         )\n         return response\n \n@@ -765,11 +781,11 @@ def _request_regex_match_vr_formatting(\n         split_rules = validation_rules[0].split(\" \")\n         if split_rules[0] == \"regex\" and split_rules[1] == \"match\":\n             # Set things up:\n-            ## Extract the regular expression we are validating against.\n+            # Extract the regular expression we are validating against.\n             regular_expression = split_rules[2]\n-            ## Define text color to update to upon correct user entry\n+            # Define text color to update to upon correct user entry\n             text_color = {\"red\": 0, \"green\": 0, \"blue\": 0}\n-            ## Define google sheets regular expression formula\n+            # Define google sheets regular expression formula\n             gs_formula = [\n                 {\n                     \"userEnteredValue\": '=REGEXMATCH(INDIRECT(\"RC\",FALSE), \"{}\")'.format(\n@@ -777,11 +793,11 @@ def _request_regex_match_vr_formatting(\n                     )\n                 }\n             ]\n-            ## Set validaiton strictness based on user specifications.\n+            # Set validaiton strictness based on user specifications.\n             if split_rules[-1].lower() == \"strict\":\n                 strict = True\n \n-            ## Create error message for users if they enter value with incorrect formatting\n+            # Create error message for users if they enter value with incorrect formatting\n             input_message = (\n                 f\"Values in this column are being validated \"\n                 f\"against the following regular expression ({regular_expression}) \"\n@@ -790,7 +806,7 @@ def _request_regex_match_vr_formatting(\n             )\n \n             # Create Requests:\n-            ## Change request to change the text color of the column we are validating to red.\n+            # Change request to change the text color of the column we are validating to red.\n             requests_vr_format_body = self._request_update_base_color(\n                 i,\n                 color={\n@@ -800,10 +816,10 @@ def _request_regex_match_vr_formatting(\n                 },\n             )\n \n-            ## Create request to for conditionally formatting user input.\n+            # Create request to for conditionally formatting user input.\n             requests_vr = self._request_regex_vr(gs_formula, i, text_color)\n \n-            ## Create request to generate data validator.\n+            # Create request to generate data validator.\n             requests_data_validation_vr = self._get_column_data_validation_values(\n                 spreadsheet_id,\n                 valid_values=gs_formula,\ndiff --git a/schematic/store/synapse.py b/schematic/store/synapse.py\nindex 7ccb810cf..861789374 100644\n--- a/schematic/store/synapse.py\n+++ b/schematic/store/synapse.py\n@@ -23,6 +23,7 @@\n from schematic_db.rdb.synapse_database import SynapseDatabase\n from synapseclient import (\n     Column,\n+    Entity,\n     EntityViewSchema,\n     EntityViewType,\n     File,\n@@ -33,6 +34,7 @@\n     as_table_columns,\n )\n from synapseclient.api import get_entity_id_bundle2\n+from synapseclient.core.constants.concrete_types import PROJECT_ENTITY\n from synapseclient.core.exceptions import (\n     SynapseAuthenticationError,\n     SynapseHTTPError,\n@@ -566,6 +568,55 @@ def getFilesInStorageDataset(\n             self.syn, datasetId, includeTypes=[\"folder\", \"file\"]\n         )\n \n+        current_entity_location = self.syn.get(entity=datasetId, downloadFile=False)\n+\n+        def walk_back_to_project(\n+            current_location: Entity, location_prefix: str, skip_entry: bool\n+        ) -> str:\n+            \"\"\"\n+            Recursively walk back up the project structure to get the paths of the\n+            names of each of the directories where we started the walk function.\n+\n+            Args:\n+                current_location (Entity): The current entity location in the project structure.\n+                location_prefix (str): The prefix to prepend to the path.\n+                skip_entry (bool): Whether to skip the current entry in the path. When\n+                    this is True it means we are looking at our starting point. If our\n+                    starting point is the project itself we can go ahead and return\n+                    back the project as the prefix.\n+\n+            Returns:\n+                str: The path of the names of each of the directories up to the project root.\n+            \"\"\"\n+            if (\n+                skip_entry\n+                and \"concreteType\" in current_location\n+                and current_location[\"concreteType\"] == PROJECT_ENTITY\n+            ):\n+                return f\"{current_location.name}/{location_prefix}\"\n+\n+            updated_prefix = (\n+                location_prefix\n+                if skip_entry\n+                else f\"{current_location.name}/{location_prefix}\"\n+            )\n+            if (\n+                \"concreteType\" in current_location\n+                and current_location[\"concreteType\"] == PROJECT_ENTITY\n+            ):\n+                return updated_prefix\n+            return walk_back_to_project(\n+                current_location=self.syn.get(entity=current_location[\"parentId\"]),\n+                location_prefix=updated_prefix,\n+                skip_entry=False,\n+            )\n+\n+        prefix = walk_back_to_project(\n+            current_location=current_entity_location,\n+            location_prefix=\"\",\n+            skip_entry=True,\n+        )\n+\n         project = self.getDatasetProject(datasetId)\n         project_name = self.syn.get(project, downloadFile=False).name\n         file_list = []\n@@ -585,17 +636,16 @@ def getFilesInStorageDataset(\n                     if fullpath:\n                         # append directory path to filename\n                         if dirpath[0].startswith(f\"{project_name}/\"):\n+                            path_without_project_prefix = (\n+                                dirpath[0] + \"/\"\n+                            ).removeprefix(f\"{project_name}/\")\n                             path_filename = (\n-                                dirpath[0] + \"/\" + path_filename[0],\n+                                prefix + path_without_project_prefix + path_filename[0],\n                                 path_filename[1],\n                             )\n                         else:\n                             path_filename = (\n-                                project_name\n-                                + \"/\"\n-                                + dirpath[0]\n-                                + \"/\"\n-                                + path_filename[0],\n+                                prefix + dirpath[0] + \"/\" + path_filename[0],\n                                 path_filename[1],\n                             )\n \ndiff --git a/schematic/utils/google_api_utils.py b/schematic/utils/google_api_utils.py\nindex b705e0419..6f09c0ea7 100644\n--- a/schematic/utils/google_api_utils.py\n+++ b/schematic/utils/google_api_utils.py\n@@ -2,14 +2,23 @@\n \n # pylint: disable=logging-fstring-interpolation\n \n-import os\n-import logging\n import json\n-from typing import Any, Union, no_type_check, TypedDict\n+import logging\n+import os\n+from typing import Any, Callable, TypedDict, Union, no_type_check\n \n import pandas as pd\n-from googleapiclient.discovery import build, Resource  # type: ignore\n from google.oauth2 import service_account  # type: ignore\n+from googleapiclient.discovery import Resource, build  # type: ignore\n+from googleapiclient.errors import HttpError  # type: ignore\n+from tenacity import (\n+    retry,\n+    retry_if_exception_type,\n+    stop_after_attempt,\n+    wait_chain,\n+    wait_fixed,\n+)\n+\n from schematic.configuration.configuration import CONFIG\n \n logger = logging.getLogger(__name__)\n@@ -86,10 +95,10 @@ def execute_google_api_requests(service, requests_body, **kwargs) -> Any:\n         and kwargs[\"service_type\"] == \"batch_update\"\n     ):\n         # execute all requests\n-        response = (\n+        response = google_api_execute_wrapper(\n             service.spreadsheets()\n             .batchUpdate(spreadsheetId=kwargs[\"spreadsheet_id\"], body=requests_body)\n-            .execute()\n+            .execute\n         )\n \n         return response\n@@ -118,10 +127,10 @@ def export_manifest_drive_service(\n \n     # use google drive\n     # Pylint seems to have trouble with the google api classes, recognizing their methods\n-    data = (\n+    data = google_api_execute_wrapper(\n         drive_service.files()  # pylint: disable=no-member\n         .export(fileId=spreadsheet_id, mimeType=mime_type)\n-        .execute()\n+        .execute\n     )\n \n     # open file and write data\n@@ -145,3 +154,25 @@ def export_manifest_csv(file_path: str, manifest: Union[pd.DataFrame, str]) -> N\n         manifest.to_csv(file_path, index=False)\n     else:\n         export_manifest_drive_service(manifest, file_path, mime_type=\"text/csv\")\n+\n+\n+@retry(\n+    stop=stop_after_attempt(5),\n+    wait=wait_chain(\n+        *[wait_fixed(1) for i in range(2)]\n+        + [wait_fixed(2) for i in range(2)]\n+        + [wait_fixed(5)]\n+    ),\n+    retry=retry_if_exception_type(HttpError),\n+    reraise=True,\n+)\n+def google_api_execute_wrapper(api_function_to_call: Callable[[], Any]) -> Any:\n+    \"\"\"Retry wrapper for Google API calls, with a backoff strategy.\n+\n+    Args:\n+        api_function_to_call (Callable[[], Any]): The function to call\n+\n+    Returns:\n+        Any: The result of the API call\n+    \"\"\"\n+    return api_function_to_call()\n", "instance_id": "Sage-Bionetworks__schematic-1520", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement for updating the README and related documentation is comprehensive and clear. The goal is explicitly defined as improving clarity in installation instructions and use-case examples, with a detailed breakdown of tasks such as restructuring the README, adding a TL;DR section, improving setup instructions, and addressing specific Jira tickets (e.g., fixing hyperlinks, consolidating instructions). The solution section lists completed items with checkboxes, ensuring transparency on what has been addressed. There are no significant ambiguities, as the scope of changes (documentation updates) and specific areas of focus are well-articulated. Additionally, references to external tickets provide context for the changes. The lack of a testing section (marked as N/A) is appropriate given the nature of the task (documentation updates do not typically require code testing). Overall, the problem description includes detailed requirements and aligns well with the provided code changes.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it primarily involves updating documentation files (README.md and CONTRIBUTION.md) with no significant code modifications to the core functionality of the codebase. The changes are confined to two markdown files, requiring minimal technical depth beyond basic Markdown formatting and understanding of the project's structure for accurate documentation. The code changes in schematic/manifest/generator.py, schematic/store/synapse.py, and schematic/utils/google_api_utils.py appear to be unrelated to the primary task of updating the README (likely included as part of a broader commit or diff), but even these changes are relatively straightforward, involving minor refactoring (e.g., adding a retry wrapper for Google API calls, improving path handling in Synapse storage). No complex algorithms, design patterns, or deep architectural understanding are required for the documentation updates, which form the core of the problem statement. Edge cases and error handling are irrelevant to the documentation task, and the scope of impact is limited to user-facing instructions rather than system behavior. The unrelated code changes, while slightly more technical (e.g., adding retry logic with tenacity), still fall into the easy category but do not significantly raise the overall difficulty due to their limited relevance to the stated problem. Thus, a score of 0.15 reflects the simplicity of the primary task with a slight bump for the minor code changes included in the diff.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] cudaErrorIllegalAddress while copying values to nullable string series\n**Describe the bug**\r\n`cudaErrorIllegalAddress` while copying values to nullable string series. This is a regression from the behavior in `24.02`.\r\n\r\n**Steps/Code to reproduce bug**\r\n```\r\ncudf.Series(['a', None])[1:2] = ['b'] # cudaErrorIllegalAddress\r\n```\r\nHowever, the following _does_ work:\r\n```\r\ncudf.Series(['a', 'b'])[1:2] = ['c'] # works\r\n```\r\nas does:\r\n```\r\ncudf.Series([0, 1])[1:2] = [2] # works\r\n```\r\n\r\n**Expected behavior**\r\nexpected the value to be copied to the specified range without causing `cudaErrorIllegalAddress`\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: conda\r\n\r\n**Additional context**\r\nMorpheus depends heavily on this behavior.\r\n\n", "patch": "diff --git a/cpp/src/strings/copying/copy_range.cu b/cpp/src/strings/copying/copy_range.cu\nindex 9f8c47602f8..2434de1795e 100644\n--- a/cpp/src/strings/copying/copy_range.cu\n+++ b/cpp/src/strings/copying/copy_range.cu\n@@ -40,20 +40,14 @@ struct compute_element_size {\n   size_type source_begin;\n   size_type target_begin;\n   size_type target_end;\n-  bool source_has_nulls;\n-  bool target_has_nulls;\n \n   __device__ cudf::size_type operator()(cudf::size_type idx)\n   {\n     if (idx >= target_begin && idx < target_end) {\n       auto const str_idx = source_begin + (idx - target_begin);\n-      return source_has_nulls && d_source.is_null_nocheck(str_idx)\n-               ? 0\n-               : d_source.element<string_view>(str_idx).size_bytes();\n+      return d_source.is_null(str_idx) ? 0 : d_source.element<string_view>(str_idx).size_bytes();\n     } else {\n-      return target_has_nulls && d_target.is_null_nocheck(idx)\n-               ? 0\n-               : d_target.element<string_view>(idx).size_bytes();\n+      return d_target.is_null(idx) ? 0 : d_target.element<string_view>(idx).size_bytes();\n     }\n   }\n };\n@@ -97,20 +91,9 @@ std::unique_ptr<column> copy_range(strings_column_view const& source,\n       mr);\n   }();\n \n-  auto [check_source, check_target] = [target, null_count = null_count] {\n-    // check validities for both source & target\n-    if (target.has_nulls()) { return std::make_pair(true, true); }\n-    // check validities for source only\n-    if (null_count > 0) { return std::make_pair(true, false); }\n-    // no need to check validities\n-    return std::make_pair(false, false);\n-  }();\n-\n   // create offsets\n   auto sizes_begin = cudf::detail::make_counting_transform_iterator(\n-    0,\n-    compute_element_size{\n-      d_source, d_target, source_begin, target_begin, target_end, check_source, check_target});\n+    0, compute_element_size{d_source, d_target, source_begin, target_begin, target_end});\n   auto [offsets_column, chars_bytes] = cudf::strings::detail::make_offsets_child_column(\n     sizes_begin, sizes_begin + target.size(), stream, mr);\n   auto d_offsets = cudf::detail::offsetalator_factory::make_input_iterator(offsets_column->view());\n", "instance_id": "rapidsai__cudf-16626", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the bug: a `cudaErrorIllegalAddress` error occurs when copying values to a nullable string series in a specific scenario. It provides a reproducible code snippet and contrasts it with working cases, which helps in understanding the issue. The expected behavior is also stated (i.e., copying values without errors). However, there are minor ambiguities and missing details. For instance, the problem does not explicitly discuss the root cause or potential edge cases beyond the provided example. Additionally, there is no mention of specific constraints or requirements for the fix (e.g., performance considerations or compatibility with other features). While the environment and context (Docker, conda, Morpheus dependency) are provided, they do not fully clarify the scope of testing or deployment implications. Overall, the statement is valid and clear but lacks some depth in edge case specification and detailed requirements.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the clarity of the problem is decent, but the lack of explicit root cause analysis means the developer must deduce the issue from the code changes and error context, which requires a deep understanding of CUDA and the cuDF library. The code changes are localized to a single file (`copy_range.cu`), specifically targeting the handling of null checks in string copying operations. However, the modification involves critical logic in a GPU-accelerated environment, requiring knowledge of CUDA programming, memory management, and the cuDF library's internal representation of nullable data structures. The removal of explicit null checking flags (`check_source`, `check_target`) and simplification of null handling logic (`is_null` instead of `is_null_nocheck`) suggests a nuanced bug related to memory access or validity checks, which is inherently complex in a parallel computing context. \n\nFrom a technical concepts perspective, this problem demands familiarity with CUDA kernel programming, memory safety in GPU contexts, and the specifics of cuDF's string column handling. The developer must also understand the implications of nullability in data structures and how it affects memory operations. While the scope of code changes is relatively small (a few lines in one file), the impact is significant as it affects core functionality (copying data in nullable series), and a mistake could introduce regressions or performance issues. Edge cases are not extensively detailed in the problem statement, but the nature of nullable data implies potential complexities around null handling, boundary conditions in slicing, and memory alignment in GPU operations. Error handling is implicitly involved, as the bug manifests as a memory access error, and the fix must ensure robust behavior without introducing new errors.\n\nOverall, this problem requires a deep understanding of the codebase's architecture (specifically cuDF's string handling and CUDA interactions), complex modifications in a critical area, and careful consideration of edge cases in a GPU context. I rate it at 0.65, reflecting a hard problem that demands specialized knowledge and experience, though it does not reach the extreme complexity of system-level redesign or novel algorithm development.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ZCAM\nZCAM is roughly what Jzazbz is based on.\r\n\r\nThe article is here: https://opg.optica.org/oe/fulltext.cfm?uri=oe-29-4-6036&id=447640.\r\nSupplemental information containing info regarding the inversion of the conversion and some comparable examples is found here: https://opticapublishing.figshare.com/articles/journal_contribution/Supplementary_document_for_ZCAM_a_psychophysical_model_for_colour_appearance_prediction_-_5022171_pdf/13640927.\r\n\r\nThe article also references using a two-stage chromatic adaptation by Qiyan Zhai and Ming R. Luo using CAM02: https://opg.optica.org/oe/fulltext.cfm?uri=oe-26-6-7724&id=383537. CAM16 could also be used.\r\n\r\nI technically have it working, though I haven't calculated the achromatic response or anything like that yet. We are using a reference lighting of absolute XYZ of [256, 264, 202] and an adapting luminance of 264 with a background luminance of 100. This is from an example in the supplemental article.\r\n\r\n![zcam](https://github.com/facelessuser/coloraide/assets/1055125/5f25b5dc-bedb-42da-93a6-e907b00d1f55)\r\n\r\nWe seem to match the article's examples close enough.  Their exact white point for D65 isn't specified, so it is likely responsible for the slight difference.\r\n\r\n<img width=\"422\" alt=\"Screenshot 2023-07-04 at 7 54 15 AM\" src=\"https://github.com/facelessuser/coloraide/assets/1055125/858f5e62-1ee6-477a-904e-e1231e14c49e\">\r\n\r\n```py\r\n>>> from coloraide.everything import ColorAll as Color\r\n>>> Color('xyz-d65', [185 / 100, 206 / 100, 163 / 100]).convert('zcam')[:]\r\n[92.25044378072363, 10.525621789845456, 196.32457375575555, 1.0]\r\n```\r\n\r\nAnyways, I don't know what a good default for this would even be or what immediate use people would have for it. What drove us to add CAM16 was HCT. Using these \"CAM\" spaces are inherently slow, especially when compared to using Jzazbz which streamlines calculations making it more accessible.\r\n\r\nIt may end up in extras at least.\r\n\r\n\r\n\r\n\n", "patch": "diff --git a/coloraide/__meta__.py b/coloraide/__meta__.py\nindex 98bbf444..9bb3bf1f 100644\n--- a/coloraide/__meta__.py\n+++ b/coloraide/__meta__.py\n@@ -193,5 +193,5 @@ def parse_version(ver: str) -> Version:\n     return Version(major, minor, micro, release, pre, post, dev)\n \n \n-__version_info__ = Version(3, 1, 2, \"final\")\n+__version_info__ = Version(3, 2, 0, \"final\")\n __version__ = __version_info__._get_canonical()\ndiff --git a/coloraide/everything.py b/coloraide/everything.py\nindex a3368bcd..4d45250f 100644\n--- a/coloraide/everything.py\n+++ b/coloraide/everything.py\n@@ -26,6 +26,7 @@\n from .spaces.acescct import ACEScct\n from .spaces.cam16_jmh import CAM16JMh\n from .spaces.cam16_ucs import CAM16UCS, CAM16LCD, CAM16SCD\n+from .spaces.zcam_jmh import ZCAMJMh\n from .spaces.hct import HCT\n from .spaces.ucs import UCS\n from .spaces.rec709 import Rec709\n@@ -85,6 +86,7 @@ class ColorAll(Base):\n         RYB(),\n         RYBBiased(),\n         Cubehelix(),\n+        ZCAMJMh(),\n \n         # Delta E\n         DE99o(),\ndiff --git a/coloraide/spaces/cam16_jmh.py b/coloraide/spaces/cam16_jmh.py\nindex 165667f6..c1f2879a 100644\n--- a/coloraide/spaces/cam16_jmh.py\n+++ b/coloraide/spaces/cam16_jmh.py\n@@ -112,7 +112,8 @@ class Environment:\n         The equation is `L = (E * R) / \u03c0`, where `E` is the illuminance in lux, `R` is the reflectance,\n         and `L` is the luminance. If we assume a perfectly reflecting diffuser, `R` is assumed as 1.\n         For the \"gray world\" assumption, we must also divide by 5 (or multiply by 0.2 - 20%).\n-        This results in `La = E / \u03c0 * 0.2`. Some also simplify this to simply `lux / \u03c0`.\n+        This results in `La = E / \u03c0 * 0.2`. You can also ignore this gray world assumption converting\n+        lux directly to nits (cd/m2) `lux / \u03c0`.\n \n     background_luminance: The background is the region immediately surrounding the stimulus and\n         for images is the neighboring portion of the image. Generally, this value is set to a value of 20.\n@@ -269,7 +270,7 @@ def cam16_to_xyz_d65(\n     return util.scale1(alg.matmul(MI6_INV, alg.multiply(rgb_c, env.d_rgb_inv, dims=alg.D1), dims=alg.D2_D1))\n \n \n-def xyz_d65_to_cam16(xyzd65: Vector, env: Environment) -> Vector:\n+def xyz_d65_to_cam16(xyzd65: Vector, env: Environment, calc_hue_quadrature: bool = False) -> Vector:\n     \"\"\"From XYZ to CAM16.\"\"\"\n \n     # Cone response\n@@ -317,7 +318,7 @@ def xyz_d65_to_cam16(xyzd65: Vector, env: Environment) -> Vector:\n     h = util.constrain_hue(math.degrees(h_rad))\n \n     # Hue quadrature\n-    H = hue_quadrature(h)\n+    H = hue_quadrature(h) if calc_hue_quadrature else alg.NaN\n \n     # Saturation\n     s = 50 * alg.nth_root(env.c * alpha / (env.a_w + 4), 2)\n@@ -363,7 +364,7 @@ class CAM16JMh(LChish, Space):\n         background_luminance=20,\n         # Average surround\n         surround='average',\n-        # No discount of illuminant\n+        # Do not discount illuminant\n         discounting=False\n     )\n     CHANNELS = (\ndiff --git a/coloraide/spaces/jzazbz.py b/coloraide/spaces/jzazbz.py\nindex e3fd2d78..95548a2a 100644\n--- a/coloraide/spaces/jzazbz.py\n+++ b/coloraide/spaces/jzazbz.py\n@@ -15,7 +15,7 @@\n from ..channels import Channel, FLG_MIRROR_PERCENT\n from .. import util\n from .. import algebra as alg\n-from ..types import Vector  # noqa: F401\n+from ..types import Vector, Matrix  # noqa: F401\n from .lab import Lab\n \n B = 1.15\n@@ -40,71 +40,82 @@\n \n \n # XYZ transform matrices\n-xyz_to_lms_m = [\n+XYZ_TO_LMS = [\n     [0.41478972, 0.579999, 0.014648],\n     [-0.20151, 1.120649, 0.0531008],\n     [-0.0166008, 0.2648, 0.6684799]\n ]\n \n-lms_to_xyz_mi = [\n+LMS_TO_XYZ = [\n     [1.9242264357876069, -1.0047923125953657, 0.037651404030617994],\n     [0.35031676209499907, 0.7264811939316552, -0.06538442294808501],\n     [-0.09098281098284755, -0.31272829052307394, 1.5227665613052603]\n ]\n \n # LMS to Izazbz matrices\n-lms_p_to_izazbz_m = [\n+LMS_P_TO_IZAZBZ = [\n     [0.5, 0.5, 0],\n     [3.524, -4.066708, 0.542708],\n     [0.199076, 1.096799, -1.295875]\n ]\n \n-izazbz_to_lms_p_mi = [\n+IZAZBZ_TO_LMS_P = [\n     [1.0, 0.13860504327153927, 0.05804731615611883],\n     [1.0, -0.1386050432715393, -0.058047316156118904],\n     [1.0, -0.09601924202631895, -0.811891896056039]\n ]\n \n \n-def jzazbz_to_xyz_d65(jzazbz: Vector) -> Vector:\n-    \"\"\"From Jzazbz to XYZ.\"\"\"\n+def xyz_d65_to_izazbz(xyz: Vector, lms_matrix: Matrix, m2: float) -> Vector:\n+    \"\"\"Absolute XYZ to Izazbz.\"\"\"\n \n-    jz, az, bz = jzazbz\n+    xa, ya, za = xyz\n+    xm = (B * xa) - ((B - 1) * za)\n+    ym = (G * ya) - ((G - 1) * xa)\n \n-    # Calculate Iz\n-    iz = alg.zdiv((jz + D0), (1 + D - D * (jz + D0)))\n+    # Convert to LMS\n+    lms = alg.matmul(XYZ_TO_LMS, [xm, ym, za], dims=alg.D2_D1)\n+\n+    # PQ encode the LMS\n+    pqlms = util.pq_st2084_oetf(lms, m2=m2)\n+\n+    # Calculate Izazbz\n+    return alg.matmul(lms_matrix, pqlms, dims=alg.D2_D1)\n+\n+\n+def izazbz_to_xyz_d65(izazbz: Vector, lms_matrix: Matrix, m2: float) -> Vector:\n+    \"\"\"Izazbz to absolute XYZ.\"\"\"\n \n     # Convert to LMS prime\n-    pqlms = alg.matmul(izazbz_to_lms_p_mi, [iz, az, bz], dims=alg.D2_D1)\n+    pqlms = alg.matmul(lms_matrix, izazbz, dims=alg.D2_D1)\n \n     # Decode PQ LMS to LMS\n-    lms = util.pq_st2084_eotf(pqlms, m2=M2)\n+    lms = util.pq_st2084_eotf(pqlms, m2=m2)\n \n     # Convert back to absolute XYZ D65\n-    xm, ym, za = alg.matmul(lms_to_xyz_mi, lms, dims=alg.D2_D1)\n+    xm, ym, za = alg.matmul(LMS_TO_XYZ, lms, dims=alg.D2_D1)\n     xa = (xm + ((B - 1) * za)) / B\n     ya = (ym + ((G - 1) * xa)) / G\n \n-    # Convert back to normal XYZ D65\n-    return util.absxyz_to_xyz([xa, ya, za], YW)\n+    return [xa, ya, za]\n \n \n-def xyz_d65_to_jzazbz(xyzd65: Vector) -> Vector:\n-    \"\"\"From XYZ to Jzazbz.\"\"\"\n+def jzazbz_to_xyz_d65(jzazbz: Vector) -> Vector:\n+    \"\"\"From Jzazbz to XYZ.\"\"\"\n \n-    # Convert from XYZ D65 to an absolute XYZ D5\n-    xa, ya, za = util.xyz_to_absxyz(xyzd65, YW)\n-    xm = (B * xa) - ((B - 1) * za)\n-    ym = (G * ya) - ((G - 1) * xa)\n+    jz, az, bz = jzazbz\n \n-    # Convert to LMS\n-    lms = alg.matmul(xyz_to_lms_m, [xm, ym, za], dims=alg.D2_D1)\n+    # Calculate Iz\n+    iz = alg.zdiv((jz + D0), (1 + D - D * (jz + D0)))\n \n-    # PQ encode the LMS\n-    pqlms = util.pq_st2084_oetf(lms, m2=M2)\n+    # Convert back to normal XYZ D65\n+    return util.absxyz_to_xyz(izazbz_to_xyz_d65([iz, az, bz], IZAZBZ_TO_LMS_P, M2), YW)\n \n-    # Calculate Izazbz\n-    iz, az, bz = alg.matmul(lms_p_to_izazbz_m, pqlms, dims=alg.D2_D1)\n+\n+def xyz_d65_to_jzazbz(xyzd65: Vector) -> Vector:\n+    \"\"\"From XYZ to Jzazbz.\"\"\"\n+\n+    iz, az, bz = xyz_d65_to_izazbz(util.xyz_to_absxyz(xyzd65, YW), LMS_P_TO_IZAZBZ,  M2)\n \n     # Calculate Jz\n     jz = ((1 + D) * iz) / (1 + (D * iz)) - D0\ndiff --git a/coloraide/spaces/zcam_jmh.py b/coloraide/spaces/zcam_jmh.py\nnew file mode 100644\nindex 00000000..c184b2eb\n--- /dev/null\n+++ b/coloraide/spaces/zcam_jmh.py\n@@ -0,0 +1,444 @@\n+\"\"\"\n+ZCAM.\n+\n+```\n+- ZCAM: https://opg.optica.org/oe/fulltext.cfm?uri=oe-29-4-6036&id=447640.\n+- Supplemental ZCAM (inverse transform): https://opticapublishing.figshare.com/articles/journal_contribution/\\\n+  Supplementary_document_for_ZCAM_a_psychophysical_model_for_colour_appearance_prediction_-_5022171_pdf/13640927.\n+- Two-stage chromatic adaptation by Qiyan Zhai and Ming R. Luo using CAM02: https://opg.optica.org/oe/\\\n+  fulltext.cfm?uri=oe-26-6-7724&id=383537\n+```\n+\"\"\"\n+from __future__ import annotations\n+import math\n+import bisect\n+from .. import util\n+from .. import algebra as alg\n+from ..spaces import Space\n+from ..cat import WHITES\n+from ..channels import Channel, FLG_ANGLE\n+from ..types import Vector, VectorLike\n+from .lch import LCh, ACHROMATIC_THRESHOLD\n+from .jzazbz import izazbz_to_xyz_d65, xyz_d65_to_izazbz\n+from .. import cat\n+\n+DEF_ILLUMINANT_BI = util.xyz_to_absxyz(util.xy_to_xyz(cat.WHITES['2deg']['E']), yw=100.0)\n+CAT02 = cat.CAT02.MATRIX\n+CAT02_INV = alg.inv(CAT02)\n+\n+# ZCAM uses a slightly different matrix than Jzazbz\n+# It updates how `Iz` is calculated.\n+LMS_P_TO_IZAZBZ = [\n+    [0.0, 1.0, 0.0],\n+    [3.524, -4.066708, 0.542708],\n+    [0.199076, 1.096799, -1.295875]\n+]\n+IZAZBZ_TO_LMS_P = alg.inv(LMS_P_TO_IZAZBZ)\n+\n+SURROUND = {\n+    'dark': (0.8, 0.525, 0.8),\n+    'dim': (0.9, 0.59, 0.9),\n+    'average': (1, 0.69, 1)\n+}\n+\n+HUE_QUADRATURE = {\n+    # Red, Yellow, Green, Blue, Red\n+    \"h\": (33.44, 89.29, 146.30, 238.36, 393.44),\n+    \"e\": (0.68, 0.64, 1.52, 0.77, 0.68),\n+    \"H\": (0.0, 100.0, 200.0, 300.0, 400.0)\n+}\n+\n+\n+def hue_quadrature(h: float) -> float:\n+    \"\"\"\n+    Hue to hue quadrature.\n+\n+    https://onlinelibrary.wiley.com/doi/pdf/10.1002/col.22324\n+    \"\"\"\n+\n+    hp = util.constrain_hue(h)\n+    if hp <= HUE_QUADRATURE['h'][0]:\n+        hp += 360\n+\n+    i = bisect.bisect_left(HUE_QUADRATURE['h'], hp) - 1\n+    hi, hii = HUE_QUADRATURE['h'][i:i + 2]\n+    ei, eii = HUE_QUADRATURE['e'][i:i + 2]\n+    Hi = HUE_QUADRATURE['H'][i]\n+\n+    t = (hp - hi) / ei\n+    return Hi + (100 * t) / (t + (hii - hp) / eii)\n+\n+\n+def inv_hue_quadrature(Hz: float) -> float:\n+    \"\"\"Hue quadrature to hue.\"\"\"\n+\n+    Hp = (Hz % 400 + 400) % 400\n+    i = math.floor(0.01 * Hp)\n+    Hp = Hp % 100\n+    hi, hii = HUE_QUADRATURE['h'][i:i + 2]\n+    ei, eii = HUE_QUADRATURE['e'][i:i + 2]\n+\n+    return util.constrain_hue((Hp * (eii * hi - ei * hii) - 100 * hi * eii) / (Hp * (eii - ei) - 100 * eii))\n+\n+\n+def adapt(\n+    xyz_b: Vector,\n+    xyz_wb: Vector,\n+    xyz_wd: Vector,\n+    db: float,\n+    dd: float,\n+    xyz_wo: Vector = DEF_ILLUMINANT_BI\n+) -> Vector:\n+    \"\"\"\n+    Use 2 step chromatic adaptation by Qiyan Zhai and Ming R. Luo using CAM02.\n+\n+    https://opg.optica.org/oe/fulltext.cfm?uri=oe-26-6-7724&id=383537\n+\n+    `xyz_b`: the sample color\n+    `xyz_wb`: input illuminant of the sample color\n+    `xyz_wd`: output illuminant\n+    `xyz_wo`: the baseline illuminant, by default we use equal energy.\n+    \"\"\"\n+\n+    yb = xyz_wb[1] / xyz_wo[1]\n+    yd = xyz_wd[1] / xyz_wo[1]\n+\n+    rgb_b = alg.dot(CAT02, xyz_b, dims=alg.D2_D1)\n+    rgb_wb = alg.dot(CAT02, xyz_wb, dims=alg.D2_D1)\n+    rgb_wd = alg.dot(CAT02, xyz_wd, dims=alg.D2_D1)\n+    rgb_wo = alg.dot(CAT02, xyz_wo, dims=alg.D2_D1)\n+\n+    d_rgb_wb = alg.add(\n+        alg.multiply(db * yb, alg.divide(rgb_wo, rgb_wb, dims=alg.D1), dims=alg.SC_D1),\n+        1 - db,\n+        dims=alg.D1_SC\n+    )\n+    d_rgb_wd = alg.add(\n+        alg.multiply(dd * yd, alg.divide(rgb_wo, rgb_wd, dims=alg.D1), dims=alg.SC_D1),\n+        1 - dd,\n+        dims=alg.D1_SC\n+    )\n+    d_rgb = alg.divide(d_rgb_wb, d_rgb_wd, dims=alg.D1)\n+    rgb_d = alg.multiply(d_rgb, rgb_b, dims=alg.D1)\n+    return alg.dot(CAT02_INV, rgb_d, dims=alg.D2_D1)\n+\n+\n+class Environment:\n+    \"\"\"\n+    Class to calculate and contain any required environmental data (viewing conditions included).\n+\n+    While originally for CIECAM models, the following applies to ZCAM as well.\n+    Usage Guidelines for CIECAM97s (Nathan Moroney)\n+    https://www.imaging.org/site/PDFS/Papers/2000/PICS-0-81/1611.pdf\n+\n+    white: This is the (x, y) chromaticity points for the white point. ZCAM is designed to use D65.\n+        Generally, D65 should always be used, but we allow the possibility of variants of D65. This should\n+        be the same value as set in the color class `WHITE` value.\n+\n+    ref_white: The reference white in XYZ scaled by 100.\n+\n+    adapting_luminance: This is the the luminance of the adapting field. The units are in cd/m2.\n+        The equation is `L = (E * R) / \u03c0`, where `E` is the illuminance in lux, `R` is the reflectance,\n+        and `L` is the luminance. If we assume a perfectly reflecting diffuser, `R` is assumed as 1.\n+        For the \"gray world\" assumption, we must also divide by 5 (or multiply by 0.2 - 20%).\n+        This results in `La = E / \u03c0 * 0.2`. You can also ignore this gray world assumption converting\n+        lux directly to nits (cd/m2) `lux / \u03c0`.\n+\n+    background_luminance: The background is the region immediately surrounding the stimulus and\n+        for images is the neighboring portion of the image. Generally, this value is set to a value of 20.\n+        This implicitly assumes a gray world assumption.\n+\n+    surround: The surround is categorical and is defined based on the relationship between the relative\n+        luminance of the surround and the luminance of the scene or image white. While there are 4 defined\n+        surrounds, usually just `average`, `dim`, and `dark` are used.\n+\n+        Dark    | 0%        | Viewing film projected in a dark room\n+        Dim     | 0% to 20% | Viewing television\n+        Average | > 20%     | Viewing surface colors\n+\n+    discounting: Whether we are discounting the illuminance. Done when eye is assumed to be fully adapted.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        white: VectorLike,\n+        reference_white: VectorLike,\n+        adapting_luminance: float,\n+        background_luminance: float,\n+        surround: str,\n+        discounting: bool\n+    ):\n+        \"\"\"\n+        Initialize environmental viewing conditions.\n+\n+        Using the specified viewing conditions, and general environmental data,\n+        initialize anything that we can ahead of time to speed up the process.\n+        \"\"\"\n+\n+        self.output_white = util.xyz_to_absxyz(util.xy_to_xyz(white), yw=100)\n+        self.ref_white = list(reference_white)\n+        self.surround = surround\n+        self.discounting = discounting\n+        xyz_w = self.ref_white\n+\n+        # The average luminance of the environment in `cd/m^2cd/m` (a.k.a. nits)\n+        self.la = adapting_luminance\n+        # The relative luminance of the nearby background\n+        self.yb = background_luminance\n+        # Absolute luminance of the reference white.\n+        yw = xyz_w[1]\n+        self.fb = math.sqrt(self.yb / yw)\n+        self.fl = 0.171 * alg.nth_root(self.la, 3) * (1 - math.exp((-48 / 9) * self.la))\n+\n+        # Surround: dark, dim, and average\n+        f, self.c, _ = SURROUND[self.surround]\n+        self.fs = self.c\n+        self.epsilon = 3.7035226210190005e-11\n+        self.rho = 1.7 * 2523 / (2 ** 5)\n+        self.b = 1.15\n+        self.g = 0.66\n+\n+        self.izw = xyz_d65_to_izazbz(xyz_w, LMS_P_TO_IZAZBZ, self.rho)[0] - self.epsilon\n+        self.qzw = (\n+            2700 * alg.spow(self.izw, (1.6 * self.fs) / (self.fb ** 0.12)) *\n+            ((self.fs ** 2.2) * (self.fb ** 0.5) * (self.fl ** 0.2))\n+        )\n+\n+        # Degree of adaptation calculating if not discounting illuminant (assumed eye is fully adapted)\n+        self.d = alg.clamp(f * (1 - 1 / 3.6 * math.exp((-self.la - 42) / 92)), 0, 1) if not self.discounting else 1\n+\n+\n+def zcam_to_xyz_d65(\n+    Jz: float | None = None,\n+    Cz: float | None = None,\n+    hz: float | None = None,\n+    Qz: float | None = None,\n+    Mz: float | None = None,\n+    Sz: float | None = None,\n+    Vz: float | None = None,\n+    Kz: float | None = None,\n+    Wz: float | None = None,\n+    Hz: float | None = None,\n+    env: Environment | None = None\n+) -> Vector:\n+    \"\"\"\n+    From ZCAM to XYZ.\n+\n+    Reverse calculation can actually be obtained from a small subset of the ZCAM components\n+    Really, only one suitable value is needed for each type of attribute: (lightness/brightness),\n+    (chroma/colorfulness/saturation), (hue/hue quadrature). If more than one for a given\n+    category is given, we will fail as we have no idea which is the right one to use. Also,\n+    if none are given, we must fail as well as there is nothing to calculate with.\n+    \"\"\"\n+\n+    # These check ensure one, and only one attribute for a given category is provided.\n+    if not ((Jz is not None) ^ (Qz is not None)):\n+        raise ValueError(\"Conversion requires one and only one: 'Jz' or 'Qz'\")\n+\n+    if not (\n+        (Cz is not None) ^ (Mz is not None) ^ (Sz is not None) ^ (Vz is not None) ^ (Kz is not None) ^ (Wz is not None)\n+    ):\n+        raise ValueError(\"Conversion requires one and only one: 'Cz', 'Mz', 'Sz', 'Vz', 'Kz', or 'Wz'\")\n+\n+    # Hue is absolutely required\n+    if not ((hz is not None) ^ (Hz is not None)):\n+        raise ValueError(\"Conversion requires one and only one: 'hz' or 'Hz'\")\n+\n+    # We need viewing conditions\n+    if env is None:\n+        raise ValueError(\"No viewing conditions/environment provided\")\n+\n+    # Black\n+    if Jz == 0.0 or Qz == 0.0:\n+        return [0.0, 0.0, 0.0]\n+\n+    # Break hue into Cartesian components\n+    h_rad = 0.0\n+    if hz is None:\n+        hz = inv_hue_quadrature(Hz)  # type: ignore[arg-type]\n+    h_rad = math.radians(hz % 360)\n+    cos_h = math.cos(h_rad)\n+    sin_h = math.sin(h_rad)\n+    hp = hz\n+    if hp <= HUE_QUADRATURE['h'][0]:\n+        hp += 360\n+    ez = 1.015 + math.cos(math.radians(89.038 + hp))\n+\n+    # Calculate `iz` from one of the lightness derived coordinates.\n+    if Qz is None:\n+        Qz = (Jz * 0.01) * env.qzw  # type: ignore[operator]\n+\n+    if Jz is None:\n+        Jz = 100 * (Qz / env.qzw)\n+\n+    iz = alg.nth_root(\n+        Qz / ((env.fs ** 2.2) * (env.fb ** 0.5) * (env.fl ** 0.2) * 2700), (1.6 * env.fs) / (env.fb ** 0.12)\n+    )\n+\n+    # Calculate `Mz` from the various chroma like parameters.\n+    if Sz is not None:\n+        Cz = Qz * Sz ** 2 / (100 * env.qzw * env.fl ** 1.2)\n+    elif Vz is not None:\n+        Cz = math.sqrt((Vz ** 2 - (Jz - 58) ** 2) / 3.4)\n+    elif Kz is not None:\n+        Cz = math.sqrt((((Kz - 100) / - 0.8) ** 2 - (Jz ** 2)) / 8)\n+    elif Wz is not None:\n+        Cz = math.sqrt((Wz - 100) ** 2 - (100 - Jz) ** 2)\n+\n+    if Cz is not None:\n+        Mz = (Cz / 100) * env.qzw\n+\n+    Czp = alg.spow(\n+        (Mz * (env.izw ** (0.78)) * (env.fb ** 0.1)) / (100 * (ez ** 0.068) * (env.fl ** 0.2)),\n+        1.0 / 0.37 / 2\n+    )\n+\n+    # Convert back to XYZ\n+    az, bz = cos_h * Czp, sin_h * Czp\n+    iz += env.epsilon\n+    xyz_abs = izazbz_to_xyz_d65([iz, az, bz], IZAZBZ_TO_LMS_P, env.rho)\n+\n+    return util.absxyz_to_xyz(adapt(xyz_abs, env.output_white, env.ref_white, env.d, env.d))\n+\n+\n+def xyz_d65_to_zcam(xyzd65: Vector, env: Environment, calc_hue_quadrature: bool = False) -> Vector:\n+    \"\"\"From XYZ to ZCAM.\"\"\"\n+\n+    # Steps 4 - 7\n+    iz, az, bz = xyz_d65_to_izazbz(\n+        adapt(util.xyz_to_absxyz(xyzd65), env.ref_white, env.output_white, env.d, env.d),\n+        LMS_P_TO_IZAZBZ,\n+        env.rho\n+    )\n+\n+    # Step 8\n+    iz -= env.epsilon\n+\n+    # Step 9\n+    hz = util.constrain_hue(math.degrees(math.atan2(bz, az)))\n+\n+    # Step 10\n+    Hz = hue_quadrature(hz) if calc_hue_quadrature else alg.NaN\n+\n+    # Step 11\n+    hp = hz\n+    if hp <= HUE_QUADRATURE['h'][0]:\n+        hp += 360\n+    ez = 1.015 + math.cos(math.radians(89.038 + hp))\n+\n+    # Step 12\n+    Qz = (\n+        2700 * alg.spow(iz, (1.6 * env.fs) / (env.fb ** 0.12)) *\n+        ((env.fs ** 2.2) * (env.fb ** 0.5) * (env.fl ** 0.2))\n+    )\n+\n+    # Step 13\n+    Jz = 100 * (Qz / env.qzw)\n+\n+    # Step 14\n+    Mz = (\n+        100 * ((az ** 2 + bz ** 2) ** (0.37)) *\n+        ((alg.spow(ez, 0.068) * (env.fl ** 0.2)) / ((env.fb ** 0.1) * alg.spow(env.izw, 0.78)))\n+    )\n+\n+    # Step 15\n+    Cz = 100 * (Mz / env.qzw)\n+\n+    # Step 16\n+    Sz = 100 * (env.fl ** 0.6) * math.sqrt(Mz / Qz) if Qz else 0.0\n+\n+    # Step 17\n+    Vz = math.sqrt((Jz - 58) ** 2 + 3.4 * (Cz ** 2))\n+\n+    # Step 18\n+    Kz = 100 - 0.8 * math.sqrt(Jz ** 2 + 8 * (Cz ** 2))\n+\n+    # Step 19\n+    Wz = 100 - math.sqrt((100 - Jz) ** 2 + Cz ** 2)\n+\n+    return [Jz, Cz, hz, Qz, Mz, Sz, Vz, Kz, Wz, Hz]\n+\n+\n+def xyz_d65_to_zcam_jmh(xyzd65: Vector, env: Environment) -> Vector:\n+    \"\"\"XYZ to ZCAM JMh.\"\"\"\n+\n+    zcam = xyz_d65_to_zcam(xyzd65, env)\n+    Jz, Mz, hz = zcam[0], zcam[4], zcam[2]\n+    return [Jz, Mz, hz]\n+\n+\n+def zcam_jmh_to_xyz_d65(jmh: Vector, env: Environment) -> Vector:\n+    \"\"\"ZCAM JMh to XYZ.\"\"\"\n+\n+    Jz, Mz, hz = jmh\n+    return zcam_to_xyz_d65(Jz=Jz, Mz=Mz, hz=hz, env=env)\n+\n+\n+class ZCAMJMh(LCh, Space):\n+    \"\"\"ZCAM class (JMh).\"\"\"\n+\n+    BASE = \"xyz-d65\"\n+    NAME = \"zcam-jmh\"\n+    SERIALIZE = (\"--zcam-jmh\",)\n+    CHANNEL_ALIASES = {\n+        \"lightness\": \"jz\",\n+        \"colorfulness\": 'mz',\n+        \"hue\": 'hz',\n+        'j': 'jz',\n+        'm': \"mz\",\n+        'h': 'hz'\n+    }\n+    WHITE = WHITES['2deg']['D65']\n+    DYNAMIC_RANGE = 'hdr'\n+\n+    # Assuming sRGB which has a lux of 64\n+    ENV = Environment(\n+        # D65 white point.\n+        white=WHITE,\n+        # The reference white in XYZ scaled by 100\n+        reference_white=util.xyz_to_absxyz(util.xy_to_xyz(WHITE), 100),\n+        # Assuming sRGB which has a lux of 64: `((E * R) / PI)` where `R = 1`.\n+        # Divided by 5 (or multiplied by 20%) assuming gray world.\n+        adapting_luminance=64 / math.pi * 0.2,\n+        # 20% relative to an XYZ luminance of 100 (scaled by 100) for the gray world assumption.\n+        background_luminance=20,\n+        # Assume an average surround\n+        surround='average',\n+        # Do not discount illuminant.\n+        discounting=False\n+    )\n+    CHANNELS = (\n+        Channel(\"jz\", 0.0, 100.0),\n+        Channel(\"mz\", 0, 60.0),\n+        Channel(\"hz\", 0.0, 360.0, flags=FLG_ANGLE)\n+    )\n+\n+    def normalize(self, coords: Vector) -> Vector:\n+        \"\"\"Normalize.\"\"\"\n+\n+        if coords[1] < 0.0:\n+            return self.from_base(self.to_base(coords))\n+        coords[2] %= 360.0\n+        return coords\n+\n+    def is_achromatic(self, coords: Vector) -> bool | None:\n+        \"\"\"Check if color is achromatic.\"\"\"\n+\n+        # Account for both positive and negative chroma\n+        return coords[0] == 0 or abs(coords[1]) < ACHROMATIC_THRESHOLD\n+\n+    def hue_name(self) -> str:\n+        \"\"\"Hue name.\"\"\"\n+\n+        return \"hz\"\n+\n+    def to_base(self, coords: Vector) -> Vector:\n+        \"\"\"From ZCAM JMh to XYZ.\"\"\"\n+\n+        return zcam_jmh_to_xyz_d65(coords, self.ENV)\n+\n+    def from_base(self, coords: Vector) -> Vector:\n+        \"\"\"From XYZ to ZCAM JMh.\"\"\"\n+\n+        return xyz_d65_to_zcam_jmh(coords, self.ENV)\ndiff --git a/docs/src/dictionary/en-custom.txt b/docs/src/dictionary/en-custom.txt\nindex 22d66eb6..33590b35 100644\n--- a/docs/src/dictionary/en-custom.txt\n+++ b/docs/src/dictionary/en-custom.txt\n@@ -98,6 +98,7 @@ Interpolator\n Itten\n Iz\n Izazbz\n+Izazbz\n JCh\n JMh\n JND\n@@ -107,8 +108,10 @@ Jsh\n Judd\n Jz\n JzCzhz\n+JzMzhz\n Jzazbz\n Kries\n+Kz\n LCh\n LChish\n LChuv\n@@ -117,6 +120,7 @@ Lab\n Labish\n Lilley\n Lstar\n+Luo\n Luv\n MATLAB\n MERCHANTABILITY\n@@ -128,6 +132,7 @@ MkDocs\n Mollon\n Monochromacy\n Moroney\n+Mz\n NONINFRINGEMENT\n NaN\n NaNs\n@@ -154,7 +159,10 @@ PyPI\n Pyodide\n QCh\n QMh\n+Qiyan\n Qsh\n+Qz\n+QzMzhz\n RGB\n RGBish\n RLAB\n@@ -167,8 +175,10 @@ SL\n SMPTE\n SRGB\n SVG\n+Safdar\n Scalable\n Sharma\n+Sz\n TODO\n TORTIOUS\n Tetradic\n@@ -189,14 +199,18 @@ Verou\n Vi\u00e9not\n Von\n Vos\n+Vz\n WCAG\n WCG\n+Wz\n XD\n XYB\n XYZ\n YCbCr\n Yoshi\n+ZCAM\n ZD\n+Zhai\n absolutizing\n accessor\n accessors\ndiff --git a/docs/src/markdown/colors/index.md b/docs/src/markdown/colors/index.md\nindex 2bdffa40..ff06cec7 100644\n--- a/docs/src/markdown/colors/index.md\n+++ b/docs/src/markdown/colors/index.md\n@@ -85,6 +85,8 @@ flowchart TB\n         cam16-jmh --- cam16-scd\n         cam16-jmh --- cam16-lcd\n \n+    xyz-d65 --- zcam-jmh\n+\n     xyz-d65 --- hct\n \n     xyz-d65 --- jzazbz --- jzczhz\n@@ -168,6 +170,7 @@ flowchart TB\n     xyb(XYB)\n     ryb(RYB)\n     cubehelix(Cubehelix)\n+    zcam-jmh(ZCAM JMh)\n \n     click xyz-d65 \"./xyz_d65/\" _self\n     click xyz-d50 \"./xyz_d50/\" _self\n@@ -228,6 +231,7 @@ flowchart TB\n     click xyb \"./xyb/\" _self\n     click ryb \"./ryb/\" _self\n     click cubehelix \"./cubehelix/\" _self\n+    click zcam-jmh \"./zcam_jmh/\" _self\n ```\n ///\n \n@@ -253,7 +257,7 @@ Color Space                                     | ID\n [Cubehelix](./cubehelix.md)                     | `cubehelix`\n [DIN99o](./din99o.md)                           | `din99o`\n [Display P3](./display_p3.md)                   | `display-p3`\n-[Linear Display P3](.)                          | `display-p3-linear`\n+[Linear Display P3](./display_p3_linear.md)     | `display-p3-linear`\n [HCT](./hct.md)                                 | `hct`\n [HPLuv](./hpluv.md)                             | `hpluv`\n [HSI](./hsi.md)                                 | `hsi`\n@@ -297,3 +301,4 @@ Color Space                                     | ID\n [xyY](./xyy.md)                                 | `xyy`\n [XYZ (D50)](./xyz_d50.md)                       | `xyz-d50`\n [XYZ (D65)](./xyz_d65.md)                       | `xyz-d65`\n+[ZCAM JMh](./zcam_jmh.md)                       | `zcam-jmh`\ndiff --git a/docs/src/markdown/colors/zcam_jmh.md b/docs/src/markdown/colors/zcam_jmh.md\nnew file mode 100644\nindex 00000000..de8a32d3\n--- /dev/null\n+++ b/docs/src/markdown/colors/zcam_jmh.md\n@@ -0,0 +1,149 @@\n+# ZCAM JMh\n+\n+/// failure | The ZCAM JMh color space is not registered in `Color` by default\n+///\n+\n+/// html | div.info-container\n+//// info | Properties\n+    attrs: {class: inline end}\n+\n+**Name:** `zcam-jmh`\n+\n+**White Point:** D65 / 2\u02da\n+\n+**Coordinates:**\n+\n+Name | Range^\\*^\n+---- | -----\n+`jz`  | [0, 100]\n+`mz`  | [0, 60]\n+`hz`  | [0, 360)\n+\n+^\\*^ Space is not bound to the range and is only used as a reference to define percentage inputs/outputs in\n+relation to the Display P3 color space.\n+////\n+\n+//// html | figure\n+![ZCAM JMh](../images/zcam-jmh-3d.png)\n+\n+///// html | figcaption\n+The sRGB gamut represented within the ZCAM JMh color space.\n+/////\n+////\n+\n+A color appearance model (CAM) is a mathematical model that seeks to describe the perceptual aspects of human color\n+vision, i.e. viewing conditions under which the appearance of a color does not tally with the corresponding physical\n+measurement of the stimulus source.\n+\n+ZCAM is a CAM model that builds off earlier work done with [Jzazbz](./jzazbz.md) by Safdar. It uses a Perceptual\n+Quantizer (PQ) curve, developed to uniformly encode a luminance range of 0.001 to 10,000 cd/m2.\n+\n+The model defines numerous different attributes:\n+\n+Name | Description\n+---- | -----------\n+Jz   | Lightness\n+Cz   | Chroma\n+hz   | hue\n+Qz   | Brightness\n+Mz   | Colorfulness\n+Sz   | Saturation\n+Vz   | Vividness\n+Kz   | Blackness\n+Wz   | Whiteness\n+Hz   | Hue Quadrature\n+\n+A color space can be constructed by using a subset of these attributes: JzCzhz, JzMzhz, QzMzhz, etc. The provided color\n+spaces uses JzMzhz.\n+\n+[Learn more](https://opg.optica.org/oe/fulltext.cfm?uri=oe-29-4-6036&id=447640).\n+///\n+\n+## Viewing Conditions\n+\n+ZCAM is a color appearance model and can be configured with different viewing environments. A ZCAM color space will\n+also have an associated environment object. This environment object determines the viewing conditions. Colors will\n+appear different based on the viewing conditions.\n+\n+Viewing\\ Conditions    | Description\n+---------------------- | -----------\n+White                  | This is the white point and output white and should be the same as defined in the color class. This is provided as (x, y) chromaticity coordinates. ZCAM expects and was designed for this to be D65.\n+Reference\\ White       | The absolute reference white where `Yw` is scaled to the luminance.\n+Adapting\\ Luminance    | The luminance of the adapting field (`La`). The units are in cd/m2.\n+Background\\ Luminance  | The background luminance (`Yb`) the relative luminance of the nearby background (out to 10\u00b0), relative to the the reference white's luminance (`Y`). Usually 20 providing a gray world assumption.\n+Surround               | A description of the peripheral area. Use \"dark\" for a movie theater, \"dim\" for e.g. viewing a bright television in a dimly lit room, or \"average\" for surface colors.\n+Discounting            | Discounts the illuminant. If true, the eye is assumed to be fully adapted to the illuminant. Otherwise, the degree of discounting is based on other parameters. When the eye is not fully adapted, it can affect the way colors appear and the chromatic response.\n+\n+ColorAide must provide some defaults, so ZCAM comes with a default set of viewing conditions that uses a D65 white\n+point, a reference white that uses D65 scaled to 100, an adapting luminance of 64 lux or a value of ~4 cd/m^2^, it uses\n+the \"gray world\" assumption and sets the background to 20, an \"average\" surround and leaves discounting set to\n+`#!py False`.\n+\n+The default settings do not have to be used and a new ZCAM variant with different viewing conditions can be created.\n+When doing this, the space should be derived from the default\n+\n+```py play\n+from coloraide import Color as Base\n+from coloraide.spaces.zcam_jmh import ZCAMJMh, Environment\n+from coloraide.cat import WHITES\n+from coloraide import util\n+import math\n+\n+cdm2 = 1000 / math.pi\n+\n+class CustomZCAMJMh(ZCAMJMh):\n+    NAME = \"zcam-custom\"\n+    SERIALIZE = (\"--zcam-custom\",)\n+    WHITE = WHITES['2deg']['D65']\n+    ENV = Environment(\n+        white=WHITE,\n+        reference_white=[c * cdm2 for c in util.xy_to_xyz(WHITE)],\n+        adapting_luminance=cdm2,\n+        background_luminance=100,\n+        surround='average',\n+        discounting=False\n+    )\n+\n+class Color(Base): ...\n+\n+Color.register([ZCAMJMh(), CustomZCAMJMh()])\n+\n+Color('red').convert('zcam-jmh')\n+Color('red').convert('zcam-custom')\n+```\n+\n+## Channel Aliases\n+\n+Channels | Aliases\n+-------- | -------\n+`jz`      | `lightness`, `j`\n+`mz`      | `colorfulness`, `m`\n+`hz`      | `hue`, `h`\n+\n+## Input/Output\n+\n+The ZCAM JMh space is not currently supported in the CSS spec, the parsed input and string output formats use\n+the `#!css-color color()` function format using the custom name `#!css-color --zcam-jmh`:\n+\n+```css-color\n+color(--zcam-jmh jz mz hz / a)  // Color function\n+```\n+\n+The string representation of the color object and the default string output use the\n+`#!css-color color(--zcam-jmh jz mz hz / a)` form.\n+\n+```py play\n+Color(\"zcam-jmh\", [51.197, 43.776, 42.477], 1)\n+Color(\"zcam-jmh\", [71.271, 32.313, 75.038], 1).to_string()\n+```\n+\n+## Registering\n+\n+```py\n+from coloraide import Color as Base\n+from coloraide_extras.spaces.zcam_jmh import ZCAMJMh\n+\n+class Color(Base): ...\n+\n+Color.register(ZCAMJMh())\n+```\ndiff --git a/docs/src/markdown/demos/3d_models.html b/docs/src/markdown/demos/3d_models.html\nindex 867f2777..58ce033c 100644\n--- a/docs/src/markdown/demos/3d_models.html\n+++ b/docs/src/markdown/demos/3d_models.html\n@@ -899,7 +899,7 @@ <h1>ColorAide Color Space Models</h1>\n let colorSpaces = null\n let colorGamuts = null\n let lastModel = null\n-let package = 'coloraide-3.1.2-py3-none-any.whl'\n+let package = 'coloraide-3.2-py3-none-any.whl'\n const defaultSpace = 'lab'\n const defaultGamut = 'srgb'\n const exceptions = new Set(['hwb', 'ryb', 'ryb-biased'])\ndiff --git a/docs/src/markdown/demos/colorpicker.html b/docs/src/markdown/demos/colorpicker.html\nindex 2217c92f..73d1aba1 100644\n--- a/docs/src/markdown/demos/colorpicker.html\n+++ b/docs/src/markdown/demos/colorpicker.html\n@@ -421,7 +421,7 @@ <h1>ColorAide Color Picker</h1>\n     let pyodide = null\n     let webspace = ''\n     let initial = 'oklab(0.69 0.13 -0.1 / 0.85)'\n-    let package = 'coloraide-3.1.2-py3-none-any.whl'\n+    let package = 'coloraide-3.2-py3-none-any.whl'\n \n     const base = `${window.location.origin}/${window.location.pathname.split('/')[1]}/playground/`\n     package = base + package\ndiff --git a/docs/src/markdown/images/zcam-jmh-3d.png b/docs/src/markdown/images/zcam-jmh-3d.png\nnew file mode 100644\nindex 00000000..e65dbadf\nBinary files /dev/null and b/docs/src/markdown/images/zcam-jmh-3d.png differ\ndiff --git a/docs/src/mkdocs.yml b/docs/src/mkdocs.yml\nindex 300a9a9f..97fb3d74 100644\n--- a/docs/src/mkdocs.yml\n+++ b/docs/src/mkdocs.yml\n@@ -114,6 +114,7 @@ nav:\n         - JzCzhz: colors/jzczhz.md\n         - CAM16 JMh: colors/cam16_jmh.md\n         - HCT: colors/hct.md\n+        - ZCAM JMh: colors/zcam_jmh.md\n \n       - ACES Spaces:\n         - ACES 2065-1: colors/aces2065_1.md\n@@ -306,7 +307,7 @@ extra_css:\n   - assets/coloraide-extras/extra.css\n extra_javascript:\n   - https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js\n-  - playground-config-410dc711.js\n+  - playground-config-6f907977.js\n   - https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js\n   - assets/coloraide-extras/extra-notebook.js\n \ndiff --git a/docs/theme/playground-config-410dc711.js b/docs/theme/playground-config-6f907977.js\nsimilarity index 76%\nrename from docs/theme/playground-config-410dc711.js\nrename to docs/theme/playground-config-6f907977.js\nindex 6e84393f..c8ad6270 100644\n--- a/docs/theme/playground-config-410dc711.js\n+++ b/docs/theme/playground-config-6f907977.js\n@@ -1,5 +1,5 @@\n var colorNotebook = {\n-    \"playgroundWheels\": ['Pygments-2.16.1-py3-none-any.whl', 'coloraide-3.1.2-py3-none-any.whl'],\n-    \"notebookWheels\": ['pyyaml', 'Markdown-3.5.1-py3-none-any.whl', 'pymdown_extensions-10.5-py3-none-any.whl', 'Pygments-2.16.1-py3-none-any.whl', 'coloraide-3.1.2-py3-none-any.whl'],\n+    \"playgroundWheels\": ['Pygments-2.16.1-py3-none-any.whl', 'coloraide-3.2-py3-none-any.whl'],\n+    \"notebookWheels\": ['pyyaml', 'Markdown-3.5.1-py3-none-any.whl', 'pymdown_extensions-10.5-py3-none-any.whl', 'Pygments-2.16.1-py3-none-any.whl', 'coloraide-3.2-py3-none-any.whl'],\n     \"defaultPlayground\": \"import coloraide\\ncoloraide.__version__\\nColor('red')\"\n }\ndiff --git a/mkdocs.yml b/mkdocs.yml\nindex d78f7c19..fd13730f 100644\n--- a/mkdocs.yml\n+++ b/mkdocs.yml\n@@ -114,6 +114,7 @@ nav:\n         - JzCzhz: colors/jzczhz.md\n         - CAM16 JMh: colors/cam16_jmh.md\n         - HCT: colors/hct.md\n+        - ZCAM JMh: colors/zcam_jmh.md\n \n       - ACES Spaces:\n         - ACES 2065-1: colors/aces2065_1.md\n@@ -306,7 +307,7 @@ extra_css:\n   - assets/coloraide-extras/extra-728132a6d6.css\n extra_javascript:\n   - https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js\n-  - playground-config-410dc711.js\n+  - playground-config-6f907977.js\n   - https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js\n   - assets/coloraide-extras/extra-notebook-KS-5jx8t.js\n \ndiff --git a/tools/gen_3d_models.py b/tools/gen_3d_models.py\nindex 96613f8a..b6365cd5 100644\n--- a/tools/gen_3d_models.py\n+++ b/tools/gen_3d_models.py\n@@ -72,7 +72,8 @@ def plot_model(name, title, filename, gamut='srgb', elev=45, azim=-60.0):\n     'ucs': {'title': TEMPLATE.format('UCS'), 'filename': 'ucs-3d.png', 'azim': 60, 'elev': 10},\n     'ryb': {'title': 'RYB Color Space', 'filename': 'ryb-3d.png', 'gamut': 'ryb'},\n     'ryb-biased': {'title': 'RYB Color Space (Biased)', 'filename': 'ryb-biased-3d.png', 'gamut': 'ryb-biased'},\n-    'cubehelix': {'title': TEMPLATE.format('Cubehelix'), 'filename': 'cubehelix-3d.png'}\n+    'cubehelix': {'title': TEMPLATE.format('Cubehelix'), 'filename': 'cubehelix-3d.png'},\n+    'zcam-jmh': {'title': TEMPLATE.format('ZCAM JMh'), 'filename': 'zcam-jmh-3d.png', 'azim': 320},\n }\n \n \n", "instance_id": "facelessuser__coloraide-410", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement for implementing ZCAM, a color appearance model, is mostly clear but has some ambiguities and missing details. The goal of adding ZCAM as a color space to the existing library (ColorAide) is evident, and references to relevant articles and supplemental information are provided, which helps in understanding the theoretical background. Additionally, example outputs and comparisons with the article's results are included, which provide a benchmark for correctness. However, critical details are missing or ambiguous, such as explicit input/output formats for the ZCAM color space beyond the provided example, specific constraints or requirements for performance (given the mention of slowness compared to Jzazbz), and a clear definition of what constitutes a \"good default\" for viewing conditions. Edge cases, such as handling invalid inputs or extreme values, are not mentioned in the problem statement. While the intent and high-level requirements are understandable, these gaps prevent the statement from being comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are extensive, involving the addition of a new file (`zcam_jmh.py`) with over 400 lines of code for the ZCAM implementation, modifications to existing files like `jzazbz.py` and `cam16_jmh.py` to support shared functionality, and updates to metadata and documentation. The changes span multiple modules and require understanding interactions between color spaces (e.g., XYZ, Jzazbz) and chromatic adaptation models (e.g., CAT02). While the impact on the system's architecture is moderate (adding a new color space without fundamentally altering the core), the volume of code and cross-module dependencies increase the complexity.\n\n2. **Number of Technical Concepts**: Solving this problem requires a deep understanding of several advanced concepts, including color appearance models (CAMs), chromatic adaptation (using a two-stage approach with CAT02), perceptual quantizer (PQ) curves for high dynamic range, and mathematical transformations (e.g., matrix operations, hue quadrature calculations). Domain-specific knowledge of color science is critical, as evidenced by the references to academic papers and the need to match example outputs from the supplemental material. Additionally, familiarity with Python and the ColorAide library's structure (e.g., how color spaces are registered and interact) is necessary. These concepts are complex and require both theoretical and practical expertise.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes reveal significant error handling logic, particularly in the reverse transformation (`zcam_to_xyz_d65`), where multiple input combinations are validated to ensure only one attribute per category is provided. Handling achromatic colors, black points, and normalization of hue values also introduces complexity. The environmental parameters (viewing conditions) add another layer of variability that must be handled correctly to avoid incorrect color perceptions. These edge cases are non-trivial and require careful implementation.\n\n4. **Overall Complexity**: The combination of implementing a new color model based on academic research, integrating it into an existing library with proper abstraction, and ensuring correctness against provided examples makes this a challenging task. It requires not only coding skills but also a strong grasp of color science and the ability to translate mathematical models into efficient code. While not at the extreme end of difficulty (e.g., no system-level or distributed system challenges), it demands significant expertise and effort, justifying a score of 0.75. This score reflects a problem that is hard but achievable with advanced knowledge and careful attention to detail.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Comment on table get table not found error\n### Describe the bug\n\nWhen using risingwave to create table and schema and then comment on the table, there is bug related to `COMMENT ON` command. It will output the error of table not found, but the table actually exists.\n\n### Error message/log\n\n```text\nCaused by these errors (recent errors listed first):\r\n  1: Catalog error\r\n  2: table not found: test_table\r\n```\n```\n\n\n### To Reproduce\n\n- Create the schema `create schema test_schema_h04rwzvk;`\r\n- Create the table `create table test_schema_h04rwzvk.test_table (id int);`\r\n- Try to comment `COMMENT ON TABLE \"test_schema_h04rwzvk\".\"test_table\" IS 'test table description';`\n\n### Expected behavior\n\n_No response_\n\n### How did you deploy RisingWave?\n\nI am using `risingwave playground`.\n\n### The version of RisingWave\n\n_No response_\n\n### Additional context\n\n_No response_\nSqlsmith: Sql feature generation\nAs we add more features, these should be tested via sqlsmith as well.\r\n\r\nSQL Features:\r\n- [ ] Generate other table functions: `generate_series`, `unnest`. See: https://github.com/singularity-data/risingwave/issues/4540\r\n- [ ] Generate array_agg, jsonb_agg, jsonb_object_agg\r\n- [ ] Generate select from System Tables\r\n- [ ] Generate `NATURAL JOIN`\r\n- [ ] Generate `JOIN` with `USING`\r\n- [ ] Generate query w/o `GROUP BY`\r\n- [ ] Randomly toggle session variables\r\n- [ ] Generate `pow`/`exp`.\r\n- [ ] Generate TopN by group: https://www.risingwave.dev/docs/current/sql-pattern-topn/\r\n- [ ] Generate jsonb exprs, see: https://github.com/risingwavelabs/risingwave/issues/7714\r\n- [ ] #7132\r\n- [x] Generate order-by-clauses with `NULLS { FIRST | LAST }` (will be done in https://github.com/risingwavelabs/risingwave/pull/10864/commits/000cbc11cbc04f20abfd127b1bf4b3fd24e75c44)\r\n- [ ] Generate `max`,`min`,`count` for `timestamptz`\r\n- [ ] Generate `bit_and`, `bit_or` and `big_xor` in batch queries, and `bit_xor` in streaming queries.\r\n- [ ] Generate array range access.\r\n- [ ] https://github.com/risingwavelabs/risingwave/issues/10241\r\n- [ ] Generate window functions other than `row_number`, `rank`.\r\n- [ ] Generate `ALTER` DDL statements.\\\r\n- [ ] https://github.com/risingwavelabs/risingwave/issues/10516\r\n- [ ] Generate temporal filter in sqlsmith\n", "patch": "diff --git a/src/frontend/src/handler/comment.rs b/src/frontend/src/handler/comment.rs\nindex b35a65112ffa6..4140805eb5798 100644\n--- a/src/frontend/src/handler/comment.rs\n+++ b/src/frontend/src/handler/comment.rs\n@@ -48,8 +48,8 @@ pub async fn handle_comment(\n                 )?;\n \n                 let (database_id, schema_id) =\n-                    session.get_database_and_schema_id_for_create(schema)?;\n-                let table = binder.bind_table(None, &table, None)?;\n+                    session.get_database_and_schema_id_for_create(schema.clone())?;\n+                let table = binder.bind_table(schema.as_deref(), &table, None)?;\n                 binder.bind_columns_to_context(col.real_value(), &table.table_catalog.columns)?;\n \n                 let column = binder.bind_column(object_name.0.as_slice())?;\n@@ -66,8 +66,8 @@ pub async fn handle_comment(\n                 let (schema, table) =\n                     Binder::resolve_schema_qualified_name(session.database(), object_name)?;\n                 let (database_id, schema_id) =\n-                    session.get_database_and_schema_id_for_create(schema)?;\n-                let table = binder.bind_table(None, &table, None)?;\n+                    session.get_database_and_schema_id_for_create(schema.clone())?;\n+                let table = binder.bind_table(schema.as_deref(), &table, None)?;\n \n                 PbComment {\n                     table_id: table.table_id.into(),\n", "instance_id": "risingwavelabs__risingwave-19612", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the `COMMENT ON` command in RisingWave when trying to comment on a table, resulting in a \"table not found\" error despite the table existing. It provides steps to reproduce the issue, including specific SQL commands, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the expected behavior section is marked as \"No response,\" leaving uncertainty about what the correct outcome should be. Additionally, there are no mentions of specific edge cases or constraints (e.g., does this happen only with certain schemas or table names with special characters?). The inclusion of unrelated content about SQL feature generation for Sqlsmith also introduces noise, diluting the focus on the core bug. Overall, while the problem is understandable, these gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows modifications in a single file (`comment.rs`) and involves only a small change in how schema and table binding is handled. The change replaces `None` with `schema.as_deref()` in the `bind_table` function call, indicating a targeted fix related to schema resolution. It does not appear to impact the broader system architecture or require changes across multiple modules, keeping the scope limited.\n\n2. **Technical Concepts Involved**: Solving this issue requires understanding Rust (specifically how references and optional values are handled with `as_deref()`), as well as familiarity with the RisingWave codebase's binder logic for resolving schema-qualified names. The concept of schema and table binding in a database system is moderately complex but not overly advanced, likely familiar to someone with database or SQL-related experience. No advanced algorithms, design patterns, or external libraries seem to be involved.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code change does not introduce new error handling logic. However, the nature of the bug (table not found despite existing) suggests potential issues with schema resolution or name parsing, which might involve implicit edge cases like case sensitivity or special characters in names. These are not addressed in the diff, but they do not seem to significantly increase the complexity of the fix.\n\n4. **Overall Complexity**: The fix appears to be a straightforward adjustment in how the schema is passed to the binder, requiring a basic-to-moderate understanding of the codebase's catalog and binding mechanisms. It does not demand deep architectural changes or advanced technical knowledge beyond typical database frontend logic.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires understanding some code logic and making a simple modification, with minimal impact on the broader system and no significant edge case handling explicitly required in the provided solution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add `(D)Point.move(d)`\nHi Matthias,\r\n\r\nI noticed basically all shape-like objects except the point ones have a `move(d)` function. Would it be possible to add this for Point as well?\r\n\r\nAlso, one additional question, but maybe that's not possible due to footprint matching. Would it be possible to add 0 as a default value for `dx`/`dy`? Rational being then (in python at least), you can do `point.move(dy=500)` without needing to specify `dx` and the other way around. This would be purely convenience, so if not easily achievable, that's totally okay!\n", "patch": "diff --git a/src/db/db/gsiDeclDbBox.cc b/src/db/db/gsiDeclDbBox.cc\nindex d2c5fcc8f..6bf0601da 100644\n--- a/src/db/db/gsiDeclDbBox.cc\n+++ b/src/db/db/gsiDeclDbBox.cc\n@@ -380,36 +380,34 @@ struct box_defs\n       \"\\n\"\n       \"@return The scaled box\\n\"\n     ) +\n-    method_ext (\"move\", &box_defs<C>::move, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"move\", &box_defs<C>::move, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the box by a certain distance\\n\"\n       \"\\n\"\n-      \"\\n\"\n       \"This is a convenience method which takes two values instead of a Point object.\\n\"\n       \"This method has been introduced in version 0.23.\\n\"\n       \"\\n\"\n       \"@return A reference to this box.\\n\"\n     ) +\n-    method_ext (\"moved\", &box_defs<C>::moved, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"moved\", &box_defs<C>::moved, gsi::arg (\"dx, 0\"), gsi::arg (\"dy\", 0),\n       \"@brief Moves the box by a certain distance\\n\"\n       \"\\n\"\n-      \"\\n\"\n       \"This is a convenience method which takes two values instead of a Point object.\\n\"\n       \"This method has been introduced in version 0.23.\\n\"\n       \"\\n\"\n       \"@return The moved box.\\n\"\n     ) +\n-    method (\"move\", &C::move, gsi::arg (\"distance\"),\n+    method (\"move\", &C::move, gsi::arg (\"d\"),\n       \"@brief Moves the box by a certain distance\\n\"\n       \"\\n\"\n       \"\\n\"\n       \"Moves the box by a given offset and returns the moved\\n\"\n       \"box. Does not check for coordinate overflows.\\n\"\n       \"\\n\"\n-      \"@param distance The offset to move the box.\\n\"\n+      \"@param d The offset to move the box.\\n\"\n       \"\\n\"\n       \"@return A reference to this box.\\n\"\n     ) +\n-    method (\"moved\", &C::moved, gsi::arg (\"distance\"),\n+    method (\"moved\", &C::moved, gsi::arg (\"d\"),\n       \"@brief Returns the box moved by a certain distance\\n\"\n       \"\\n\"\n       \"\\n\"\n@@ -417,7 +415,7 @@ struct box_defs\n       \"box. Does not modify this box. Does not check for coordinate\\n\"\n       \"overflows.\\n\"\n       \"\\n\"\n-      \"@param distance The offset to move the box.\\n\"\n+      \"@param d The offset to move the box.\\n\"\n       \"\\n\"\n       \"@return The moved box.\\n\"\n     ) +\ndiff --git a/src/db/db/gsiDeclDbEdge.cc b/src/db/db/gsiDeclDbEdge.cc\nindex 1abff3145..c50c89c18 100644\n--- a/src/db/db/gsiDeclDbEdge.cc\n+++ b/src/db/db/gsiDeclDbEdge.cc\n@@ -207,17 +207,17 @@ struct edge_defs\n       \"\\n\"\n       \"This method has been introduced in version 0.25.\\n\"\n     ) +\n-    method (\"moved\", &C::moved, gsi::arg (\"p\"),\n+    method (\"moved\", &C::moved, gsi::arg (\"v\"),\n       \"@brief Returns the moved edge (does not modify self)\\n\"\n       \"\\n\"\n       \"Moves the edge by the given offset and returns the \\n\"\n       \"moved edge. The edge is not modified.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the edge.\\n\"\n+      \"@param v The distance to move the edge.\\n\"\n       \"\\n\"\n       \"@return The moved edge.\\n\"\n     ) +\n-    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Returns the moved edge (does not modify self)\\n\"\n       \"\\n\"\n       \"Moves the edge by the given offset and returns the \\n\"\n@@ -325,17 +325,17 @@ struct edge_defs\n       \"\\n\"\n       \"@return The transformed edge.\\n\"\n     ) +\n-    method (\"move\", &C::move, gsi::arg (\"p\"),\n+    method (\"move\", &C::move, gsi::arg (\"v\"),\n       \"@brief Moves the edge.\\n\"\n       \"\\n\"\n       \"Moves the edge by the given offset and returns the \\n\"\n       \"moved edge. The edge is overwritten.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the edge.\\n\"\n+      \"@param v The distance to move the edge.\\n\"\n       \"\\n\"\n       \"@return The moved edge.\\n\"\n     ) +\n-    method_ext (\"move\", &move_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the edge.\\n\"\n       \"\\n\"\n       \"Moves the edge by the given offset and returns the \\n\"\ndiff --git a/src/db/db/gsiDeclDbEdgePairs.cc b/src/db/db/gsiDeclDbEdgePairs.cc\nindex 780b95442..3fa1633f0 100644\n--- a/src/db/db/gsiDeclDbEdgePairs.cc\n+++ b/src/db/db/gsiDeclDbEdgePairs.cc\n@@ -735,49 +735,49 @@ Class<db::EdgePairs> decl_EdgePairs (decl_dbShapeCollection, \"db\", \"EdgePairs\",\n     \"\\n\"\n     \"The 'join_with' alias has been introduced in version 0.28.12.\"\n   ) +\n-  method_ext (\"move\", &move_p, gsi::arg (\"p\"),\n+  method_ext (\"move\", &move_p, gsi::arg (\"v\"),\n     \"@brief Moves the edge pair collection\\n\"\n     \"\\n\"\n     \"Moves the edge pairs by the given offset and returns the \\n\"\n     \"moved edge pair collection. The edge pair collection is overwritten.\\n\"\n     \"\\n\"\n-    \"@param p The distance to move the edge pairs.\\n\"\n+    \"@param v The distance to move the edge pairs.\\n\"\n     \"\\n\"\n     \"@return The moved edge pairs (self).\\n\"\n     \"\\n\"\n     \"Starting with version 0.25 the displacement is of vector type.\"\n   ) +\n-  method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"move\", &move_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n     \"@brief Moves the edge pair collection\\n\"\n     \"\\n\"\n     \"Moves the edge pairs by the given offset and returns the \\n\"\n     \"moved edge pairs. The edge pair collection is overwritten.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the edge pairs.\\n\"\n-    \"@param y The y distance to move the edge pairs.\\n\"\n+    \"@param dx The x distance to move the edge pairs.\\n\"\n+    \"@param dy The y distance to move the edge pairs.\\n\"\n     \"\\n\"\n     \"@return The moved edge pairs (self).\\n\"\n   ) +\n-  method_ext (\"moved\", &moved_p, gsi::arg (\"p\"),\n+  method_ext (\"moved\", &moved_p, gsi::arg (\"v\"),\n     \"@brief Returns the moved edge pair collection (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the edge pairs by the given offset and returns the \\n\"\n     \"moved edge pairs. The edge pair collection is not modified.\\n\"\n     \"\\n\"\n-    \"@param p The distance to move the edge pairs.\\n\"\n+    \"@param v The distance to move the edge pairs.\\n\"\n     \"\\n\"\n     \"@return The moved edge pairs.\\n\"\n     \"\\n\"\n     \"Starting with version 0.25 the displacement is of vector type.\"\n   ) +\n-  method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Returns the moved edge pair collection (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the edge pairs by the given offset and returns the \\n\"\n     \"moved edge pairs. The edge pair collection is not modified.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the edge pairs.\\n\"\n-    \"@param y The y distance to move the edge pairs.\\n\"\n+    \"@param dx The x distance to move the edge pairs.\\n\"\n+    \"@param dy The y distance to move the edge pairs.\\n\"\n     \"\\n\"\n     \"@return The moved edge pairs.\\n\"\n   ) +\ndiff --git a/src/db/db/gsiDeclDbEdges.cc b/src/db/db/gsiDeclDbEdges.cc\nindex 6d86681e5..78776b43f 100644\n--- a/src/db/db/gsiDeclDbEdges.cc\n+++ b/src/db/db/gsiDeclDbEdges.cc\n@@ -1621,14 +1621,14 @@ Class<db::Edges> decl_Edges (decl_dbShapeCollection, \"db\", \"Edges\",\n     \"\\n\"\n     \"Starting with version 0.25 the displacement type is a vector.\"\n   ) +\n-  method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Moves the edge collection\\n\"\n     \"\\n\"\n     \"Moves the edge collection by the given offset and returns the \\n\"\n     \"moved edge collection. The edge collection is overwritten.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the edge collection.\\n\"\n-    \"@param y The y distance to move the edge collection.\\n\"\n+    \"@param dx The x distance to move the edge collection.\\n\"\n+    \"@param dy The y distance to move the edge collection.\\n\"\n     \"\\n\"\n     \"@return The moved edge collection (self).\\n\"\n   ) +\n@@ -1644,14 +1644,14 @@ Class<db::Edges> decl_Edges (decl_dbShapeCollection, \"db\", \"Edges\",\n     \"\\n\"\n     \"Starting with version 0.25 the displacement type is a vector.\"\n   ) +\n-  method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"v\"),\n+  method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dv\", 0),\n     \"@brief Returns the moved edge collection (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the edge collection by the given offset and returns the \\n\"\n     \"moved edge collection. The edge collection is not modified.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the edge collection.\\n\"\n-    \"@param y The y distance to move the edge collection.\\n\"\n+    \"@param dx The x distance to move the edge collection.\\n\"\n+    \"@param dy The y distance to move the edge collection.\\n\"\n     \"\\n\"\n     \"@return The moved edge collection.\\n\"\n   ) +\ndiff --git a/src/db/db/gsiDeclDbPath.cc b/src/db/db/gsiDeclDbPath.cc\nindex 1f8bc9b45..35cc28f52 100644\n--- a/src/db/db/gsiDeclDbPath.cc\n+++ b/src/db/db/gsiDeclDbPath.cc\n@@ -208,17 +208,17 @@ struct path_defs\n       \"Returns the scaled object. All coordinates are multiplied with the given factor and if \"\n       \"necessary rounded.\"\n     ) +\n-    method (\"move\", &C::move, gsi::arg (\"p\"),\n+    method (\"move\", &C::move, gsi::arg (\"v\"),\n       \"@brief Moves the path.\\n\"\n       \"\\n\"\n       \"Moves the path by the given offset and returns the \\n\"\n       \"moved path. The path is overwritten.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the path.\\n\"\n+      \"@param v The distance to move the path.\\n\"\n       \"\\n\"\n       \"@return The moved path.\\n\"\n     ) +\n-    method_ext (\"move\", &move_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the path.\\n\"\n       \"\\n\"\n       \"Moves the path by the given offset and returns the \\n\"\n@@ -231,17 +231,17 @@ struct path_defs\n       \"\\n\"\n       \"This version has been added in version 0.23.\\n\"\n     ) +\n-    method (\"moved\", &C::moved, gsi::arg (\"p\"),\n+    method (\"moved\", &C::moved, gsi::arg (\"v\"),\n       \"@brief Returns the moved path (does not change self)\\n\"\n       \"\\n\"\n       \"Moves the path by the given offset and returns the \\n\"\n       \"moved path. The path is not modified.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the path.\\n\"\n+      \"@param v The distance to move the path.\\n\"\n       \"\\n\"\n       \"@return The moved path.\\n\"\n     ) +\n-    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Returns the moved path (does not change self)\\n\"\n       \"\\n\"\n       \"Moves the path by the given offset and returns the \\n\"\ndiff --git a/src/db/db/gsiDeclDbPoint.cc b/src/db/db/gsiDeclDbPoint.cc\nindex 34de7a1ad..02ce8cc84 100644\n--- a/src/db/db/gsiDeclDbPoint.cc\n+++ b/src/db/db/gsiDeclDbPoint.cc\n@@ -36,6 +36,7 @@ template <class C>\n struct point_defs\n {\n   typedef typename C::coord_type coord_type;\n+  typedef typename C::vector_type vector_type;\n \n   static C *from_string (const char *s)\n   {\n@@ -97,6 +98,28 @@ struct point_defs\n     return std::hfunc (*pt);\n   }\n \n+  static C move_d (C *p, const vector_type &d)\n+  {\n+    *p += d;\n+    return *p;\n+  }\n+\n+  static C move_xy (C *p, coord_type dx, coord_type dy)\n+  {\n+    *p += vector_type (dx, dy);\n+    return *p;\n+  }\n+\n+  static C moved_d (const C *p, const vector_type &d)\n+  {\n+    return *p + d;\n+  }\n+\n+  static C moved_xy (const C *p, coord_type dx, coord_type dy)\n+  {\n+    return *p + vector_type (dx, dy);\n+  }\n+\n   static gsi::Methods methods ()\n   {\n     return\n@@ -170,6 +193,56 @@ struct point_defs\n       \"\\n\"\n       \"This method has been introduced in version 0.25.\\n\"\n     ) +\n+    method_ext (\"move\", &move_d, gsi::arg (\"v\"),\n+      \"@brief Moves the point.\\n\"\n+      \"\\n\"\n+      \"This method is equivalent to '+='. It was introduced to harmonize the API \"\n+      \"with the other objects. The point is modified.\\n\"\n+      \"\\n\"\n+      \"@param v The distance to move the point.\\n\"\n+      \"\\n\"\n+      \"@return The moved point.\\n\"\n+      \"\\n\"\n+      \"This method has been introduced in version 0.29.9.\"\n+    ) +\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n+      \"@brief Moves the point.\\n\"\n+      \"\\n\"\n+      \"Moves the point by the given offset and returns the \\n\"\n+      \"moved point. The point is modified.\\n\"\n+      \"\\n\"\n+      \"@param dx The x distance to move the point.\\n\"\n+      \"@param dy The y distance to move the point.\\n\"\n+      \"\\n\"\n+      \"@return The moved point.\\n\"\n+      \"\\n\"\n+      \"This method has been introduced in version 0.29.9.\"\n+    ) +\n+    method_ext (\"moved\", &moved_d, gsi::arg (\"v\"),\n+      \"@brief Returns the moved point.\\n\"\n+      \"\\n\"\n+      \"This method is equivalent to '+'. It was introduced to harmonize the API \"\n+      \"with the other objects. The point is not modified.\\n\"\n+      \"\\n\"\n+      \"@param v The distance to move the point.\\n\"\n+      \"\\n\"\n+      \"@return The moved point.\\n\"\n+      \"\\n\"\n+      \"This method has been introduced in version 0.29.9.\"\n+    ) +\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n+      \"@brief Returns the moved point.\\n\"\n+      \"\\n\"\n+      \"Moves the point by the given offset and returns the \\n\"\n+      \"moved point. The point is not modified.\\n\"\n+      \"\\n\"\n+      \"@param dx The x distance to move the point.\\n\"\n+      \"@param dy The y distance to move the point.\\n\"\n+      \"\\n\"\n+      \"@return The moved point.\\n\"\n+      \"\\n\"\n+      \"This method has been introduced in version 0.29.9.\"\n+    ) +\n     method (\"x\", &C::x,\n       \"@brief Accessor to the x coordinate\\n\"\n     ) +\ndiff --git a/src/db/db/gsiDeclDbPolygon.cc b/src/db/db/gsiDeclDbPolygon.cc\nindex f552ac1ff..97670a515 100644\n--- a/src/db/db/gsiDeclDbPolygon.cc\n+++ b/src/db/db/gsiDeclDbPolygon.cc\n@@ -429,45 +429,45 @@ struct simple_polygon_defs\n       \"Returns the scaled object. All coordinates are multiplied with the given factor and if \"\n       \"necessary rounded.\"\n     ) +\n-    method (\"move\", &C::move, gsi::arg (\"p\"),\n+    method (\"move\", &C::move, gsi::arg (\"v\"),\n       \"@brief Moves the simple polygon.\\n\"\n       \"\\n\"\n       \"Moves the simple polygon by the given offset and returns the \\n\"\n       \"moved simple polygon. The polygon is overwritten.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the simple polygon.\\n\"\n+      \"@param v The distance to move the simple polygon.\\n\"\n       \"\\n\"\n       \"@return The moved simple polygon.\\n\"\n     ) +\n-    method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the polygon.\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n       \"moved polygon. The polygon is overwritten.\\n\"\n       \"\\n\"\n-      \"@param x The x distance to move the polygon.\\n\"\n-      \"@param y The y distance to move the polygon.\\n\"\n+      \"@param dx The x distance to move the polygon.\\n\"\n+      \"@param dy The y distance to move the polygon.\\n\"\n       \"\\n\"\n       \"@return The moved polygon (self).\\n\"\n     ) +\n-    method (\"moved\", &C::moved, gsi::arg (\"p\"),\n+    method (\"moved\", &C::moved, gsi::arg (\"v\"),\n       \"@brief Returns the moved simple polygon\\n\"\n       \"\\n\"\n       \"Moves the simple polygon by the given offset and returns the \\n\"\n       \"moved simple polygon. The polygon is not modified.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the simple polygon.\\n\"\n+      \"@param v The distance to move the simple polygon.\\n\"\n       \"\\n\"\n       \"@return The moved simple polygon.\\n\"\n     ) +\n-    method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Returns the moved polygon (does not modify self)\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n       \"moved polygon. The polygon is not modified.\\n\"\n       \"\\n\"\n-      \"@param x The x distance to move the polygon.\\n\"\n-      \"@param y The y distance to move the polygon.\\n\"\n+      \"@param dx The x distance to move the polygon.\\n\"\n+      \"@param dy The y distance to move the polygon.\\n\"\n       \"\\n\"\n       \"@return The moved polygon.\\n\"\n       \"\\n\"\n@@ -1470,30 +1470,30 @@ struct polygon_defs\n       \"Returns the scaled object. All coordinates are multiplied with the given factor and if \"\n       \"necessary rounded.\"\n     ) +\n-    method (\"move\", &C::move, gsi::arg (\"p\"),\n+    method (\"move\", &C::move, gsi::arg (\"v\"),\n       \"@brief Moves the polygon.\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n       \"moved polygon. The polygon is overwritten.\\n\"\n       \"\\n\"\n-      \"@param p The distance to move the polygon.\\n\"\n+      \"@param v The distance to move the polygon.\\n\"\n       \"\\n\"\n       \"@return The moved polygon (self).\\n\"\n       \"\\n\"\n       \"This method has been introduced in version 0.23.\\n\"\n     ) +\n-    method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the polygon.\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n       \"moved polygon. The polygon is overwritten.\\n\"\n       \"\\n\"\n-      \"@param x The x distance to move the polygon.\\n\"\n-      \"@param y The y distance to move the polygon.\\n\"\n+      \"@param dx The x distance to move the polygon.\\n\"\n+      \"@param dy The y distance to move the polygon.\\n\"\n       \"\\n\"\n       \"@return The moved polygon (self).\\n\"\n     ) +\n-    method (\"moved\", &C::moved, gsi::arg (\"p\"),\n+    method (\"moved\", &C::moved, gsi::arg (\"v\"),\n       \"@brief Returns the moved polygon (does not modify self)\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n@@ -1505,14 +1505,14 @@ struct polygon_defs\n       \"\\n\"\n       \"This method has been introduced in version 0.23.\\n\"\n     ) +\n-    method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Returns the moved polygon (does not modify self)\\n\"\n       \"\\n\"\n       \"Moves the polygon by the given offset and returns the \\n\"\n       \"moved polygon. The polygon is not modified.\\n\"\n       \"\\n\"\n-      \"@param x The x distance to move the polygon.\\n\"\n-      \"@param y The y distance to move the polygon.\\n\"\n+      \"@param dx The x distance to move the polygon.\\n\"\n+      \"@param dy The y distance to move the polygon.\\n\"\n       \"\\n\"\n       \"@return The moved polygon.\\n\"\n       \"\\n\"\ndiff --git a/src/db/db/gsiDeclDbRegion.cc b/src/db/db/gsiDeclDbRegion.cc\nindex e42ab0b24..ee52cda6f 100644\n--- a/src/db/db/gsiDeclDbRegion.cc\n+++ b/src/db/db/gsiDeclDbRegion.cc\n@@ -3039,14 +3039,14 @@ Class<db::Region> decl_Region (decl_dbShapeCollection, \"db\", \"Region\",\n     \"\\n\"\n     \"@return The moved region (self).\\n\"\n   ) +\n-  method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Moves the region\\n\"\n     \"\\n\"\n     \"Moves the region by the given offset and returns the \\n\"\n     \"moved region. The region is overwritten.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the region.\\n\"\n-    \"@param y The y distance to move the region.\\n\"\n+    \"@param dx The x distance to move the region.\\n\"\n+    \"@param dy The y distance to move the region.\\n\"\n     \"\\n\"\n     \"@return The moved region (self).\\n\"\n   ) +\n@@ -3058,18 +3058,18 @@ Class<db::Region> decl_Region (decl_dbShapeCollection, \"db\", \"Region\",\n     \"\\n\"\n     \"Starting with version 0.25 this method accepts a vector argument.\\n\"\n     \"\\n\"\n-    \"@param p The distance to move the region.\\n\"\n+    \"@param v The distance to move the region.\\n\"\n     \"\\n\"\n     \"@return The moved region.\\n\"\n   ) +\n-  method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Returns the moved region (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the region by the given offset and returns the \\n\"\n     \"moved region. The region is not modified.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the region.\\n\"\n-    \"@param y The y distance to move the region.\\n\"\n+    \"@param dx The x distance to move the region.\\n\"\n+    \"@param dy The y distance to move the region.\\n\"\n     \"\\n\"\n     \"@return The moved region.\\n\"\n   ) +\ndiff --git a/src/db/db/gsiDeclDbText.cc b/src/db/db/gsiDeclDbText.cc\nindex be37cf36e..d37c71bb6 100644\n--- a/src/db/db/gsiDeclDbText.cc\n+++ b/src/db/db/gsiDeclDbText.cc\n@@ -296,18 +296,18 @@ struct text_defs\n       \"\\n\"\n       \"See \\\\valign= for a description of this property.\\n\"\n     ) +\n-    method_ext (\"move\", &move, gsi::arg (\"distance\"),\n+    method_ext (\"move\", &move, gsi::arg (\"v\"),\n       \"@brief Moves the text by a certain distance (modifies self)\\n\"\n       \"\\n\"\n       \"\\n\"\n       \"Moves the text by a given offset and returns the moved\\n\"\n       \"text. Does not check for coordinate overflows.\\n\"\n       \"\\n\"\n-      \"@param p The offset to move the text.\\n\"\n+      \"@param v The offset to move the text.\\n\"\n       \"\\n\"\n       \"@return A reference to this text object\\n\"\n     ) +\n-    method_ext (\"move\", &move_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Moves the text by a certain distance (modifies self)\\n\"\n       \"\\n\"\n       \"\\n\"\n@@ -321,7 +321,7 @@ struct text_defs\n       \"\\n\"\n       \"This method was introduced in version 0.23.\"\n     ) +\n-    method_ext (\"moved\", &moved, gsi::arg (\"distance\"),\n+    method_ext (\"moved\", &moved, gsi::arg (\"v\"),\n       \"@brief Returns the text moved by a certain distance (does not modify self)\\n\"\n       \"\\n\"\n       \"\\n\"\n@@ -329,11 +329,11 @@ struct text_defs\n       \"text. Does not modify *this. Does not check for coordinate\\n\"\n       \"overflows.\\n\"\n       \"\\n\"\n-      \"@param p The offset to move the text.\\n\"\n+      \"@param v The offset to move the text.\\n\"\n       \"\\n\"\n       \"@return The moved text.\\n\"\n     ) +\n-    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\"), gsi::arg (\"dy\"),\n+    method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n       \"@brief Returns the text moved by a certain distance (does not modify self)\\n\"\n       \"\\n\"\n       \"\\n\"\ndiff --git a/src/db/db/gsiDeclDbTexts.cc b/src/db/db/gsiDeclDbTexts.cc\nindex 359249795..5696f682e 100644\n--- a/src/db/db/gsiDeclDbTexts.cc\n+++ b/src/db/db/gsiDeclDbTexts.cc\n@@ -484,45 +484,45 @@ Class<db::Texts> decl_Texts (decl_dbShapeCollection, \"db\", \"Texts\",\n     \"\\n\"\n     \"The 'join_with' alias has been introduced in version 0.28.12.\"\n   ) +\n-  method_ext (\"move\", &move_p, gsi::arg (\"p\"),\n+  method_ext (\"move\", &move_p, gsi::arg (\"v\"),\n     \"@brief Moves the text collection\\n\"\n     \"\\n\"\n     \"Moves the texts by the given offset and returns the \\n\"\n     \"moved text collection. The text collection is overwritten.\\n\"\n     \"\\n\"\n-    \"@param p The distance to move the texts.\\n\"\n+    \"@param v The distance to move the texts.\\n\"\n     \"\\n\"\n     \"@return The moved texts (self).\\n\"\n   ) +\n-  method_ext (\"move\", &move_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"move\", &move_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Moves the text collection\\n\"\n     \"\\n\"\n     \"Moves the edge pairs by the given offset and returns the \\n\"\n     \"moved texts. The edge pair collection is overwritten.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the texts.\\n\"\n-    \"@param y The y distance to move the texts.\\n\"\n+    \"@param dx The x distance to move the texts.\\n\"\n+    \"@param dy The y distance to move the texts.\\n\"\n     \"\\n\"\n     \"@return The moved texts (self).\\n\"\n   ) +\n-  method_ext (\"moved\", &moved_p, gsi::arg (\"p\"),\n+  method_ext (\"moved\", &moved_p, gsi::arg (\"v\"),\n     \"@brief Returns the moved text collection (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the texts by the given offset and returns the \\n\"\n     \"moved texts. The text collection is not modified.\\n\"\n     \"\\n\"\n-    \"@param p The distance to move the texts.\\n\"\n+    \"@param v The distance to move the texts.\\n\"\n     \"\\n\"\n     \"@return The moved texts.\\n\"\n   ) +\n-  method_ext (\"moved\", &moved_xy, gsi::arg (\"x\"), gsi::arg (\"y\"),\n+  method_ext (\"moved\", &moved_xy, gsi::arg (\"dx\", 0), gsi::arg (\"dy\", 0),\n     \"@brief Returns the moved edge pair collection (does not modify self)\\n\"\n     \"\\n\"\n     \"Moves the texts by the given offset and returns the \\n\"\n     \"moved texts. The text collection is not modified.\\n\"\n     \"\\n\"\n-    \"@param x The x distance to move the texts.\\n\"\n-    \"@param y The y distance to move the texts.\\n\"\n+    \"@param dx The x distance to move the texts.\\n\"\n+    \"@param dy The y distance to move the texts.\\n\"\n     \"\\n\"\n     \"@return The moved texts.\\n\"\n   ) +\n", "instance_id": "KLayout__klayout-1928", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to add a `move` function to the `Point` class for consistency with other shape-like objects in the codebase. The request also includes a secondary convenience feature of setting default values for `dx` and `dy` parameters to allow partial specification in Python (e.g., `point.move(dy=500)`). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior of the `move` function (though it can be inferred from context and code changes), nor does it mention any specific constraints or edge cases to consider. Additionally, the feasibility of the default value feature due to \"footprint matching\" is left vague, and no examples or expected outputs are provided. Despite these minor gaps, the intent and scope are reasonably clear, especially when paired with the provided code changes, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is moderate but straightforward, involving the addition of `move` and `moved` methods to the `Point` class and updating parameter naming conventions and default values across multiple files for consistency. The changes span several files (e.g., `gsiDeclDbPoint.cc`, `gsiDeclDbBox.cc`, etc.), but they are largely repetitive and follow a clear pattern, requiring minimal deep understanding of the broader codebase architecture. Second, the technical concepts involved are relatively basic, including C++ method definitions, operator overloading (e.g., `+=` for point movement), and familiarity with the GSI (Generic Scripting Interface) framework used for method declarations. No complex algorithms, design patterns, or domain-specific knowledge beyond basic geometry (point movement) are required. Third, the problem does not explicitly mention edge cases or error handling requirements, and the code changes do not introduce significant new error handling logic (e.g., coordinate overflow checks are explicitly not performed, as noted in the documentation). Finally, while the changes affect multiple files, the impact on the system's architecture is negligible, as this is primarily an API harmonization task rather than a structural modification. A score of 0.30 reflects the need for understanding some code logic and making consistent modifications across files, but the task remains relatively simple for a developer familiar with C++ and the codebase's conventions.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Implement Noise and Attenuation Functions from Ryan et. al. 2015\nFrom [Reducing bias due to noise and attenuation in open-ocean echo integration data](https://academic.oup.com/icesjms/article/72/8/2482/2459093), implement the following noise filters:\r\n\r\n- [x] IN filter\r\n- [x] AS filter\r\n- [x] TN filter\r\n\r\nImplementation of some (if not all) of these filters can be found in [echopy](https://github.com/open-ocean-sounding/echopy) written in pure numpy. We want to implement these filters using Xarray, and if Dask Delaying is not too involved, implement that as well. If not, we can write a separate issue for that.\n", "patch": "diff --git a/echopype/clean/__init__.py b/echopype/clean/__init__.py\nindex bfcef8186..9ce5d2908 100644\n--- a/echopype/clean/__init__.py\n+++ b/echopype/clean/__init__.py\n@@ -1,6 +1,15 @@\n-from .api import estimate_noise, remove_noise\n+from .api import (\n+    estimate_background_noise,\n+    mask_attenuated_signal,\n+    mask_impulse_noise,\n+    mask_transient_noise,\n+    remove_background_noise,\n+)\n \n __all__ = [\n-    \"estimate_noise\",\n-    \"remove_noise\",\n+    \"estimate_background_noise\",\n+    \"mask_attenuated_signal\",\n+    \"mask_impulse_noise\",\n+    \"mask_transient_noise\",\n+    \"remove_background_noise\",\n ]\ndiff --git a/echopype/clean/api.py b/echopype/clean/api.py\nindex 7ef198e8b..b771cf476 100644\n--- a/echopype/clean/api.py\n+++ b/echopype/clean/api.py\n@@ -2,74 +2,380 @@\n Functions for reducing variabilities in backscatter data.\n \"\"\"\n \n+from functools import partial\n+from typing import Union\n+\n+import numpy as np\n+import xarray as xr\n+\n+from ..commongrid.utils import _setup_and_validate\n+from ..utils.compute import _lin2log, _log2lin\n+from ..utils.log import _init_logger\n from ..utils.prov import add_processing_level, echopype_prov_attrs, insert_input_processing_level\n-from .noise_est import NoiseEst\n+from .utils import (\n+    add_remove_background_noise_attrs,\n+    calc_transient_noise_pooled_Sv,\n+    downsample_upsample_along_depth,\n+    echopy_attenuated_signal_mask,\n+    echopy_impulse_noise_mask,\n+)\n+\n+logger = _init_logger(__name__)\n \n \n-def estimate_noise(ds_Sv, ping_num, range_sample_num, noise_max=None):\n+def mask_transient_noise(\n+    ds_Sv: xr.Dataset,\n+    func: str = \"nanmean\",\n+    depth_bin: str = \"20m\",\n+    num_side_pings: int = 50,\n+    exclude_above: Union[int, float] = 250.0,\n+    transient_noise_threshold: Union[int, float] = 12.0,\n+) -> xr.DataArray:\n     \"\"\"\n-    Estimate background noise by computing mean calibrated power of a collection of pings.\n+    Locate and create a mask for transient noise using a pooling comparison.\n+\n+    Parameters\n+    ----------\n+    ds_Sv : xarray.Dataset\n+        Calibrated Sv data with depth data variable.\n+    func: str, default `nanmean`\n+        Pooling function used in the pooled Sv aggregation.\n+    depth_bin : str, default `20`m\n+        Pooling radius along ``depth`` in meters.\n+    num_side_pings : int, default `50`\n+        Number of side pings to look at for the pooling.\n+    exclude_above : Union[int, float], default `250`m\n+        Exclude all depth above (closer to the surface than) this value.\n+    transient_noise_threshold : Union[int, float], default `10.0`dB\n+        Transient noise threshold value (dB) for the pooling comparison.\n+\n+    Returns\n+    -------\n+    xr.Dataset\n+        Xarray boolean array transient noise mask.\n+\n+    References\n+    ----------\n+    This function's implementation is based on the following text reference:\n+\n+        Ryan et al. (2015) Reducing bias due to noise and attenuation in\n+        open-ocean echo integration data, ICES Journal of Marine Science, 72: 2482\u20132493.\n+\n+    Additionally, this code was derived from echopy's numpy single-channel implementation of\n+    transient noise masking and translated into xarray code:\n+    https://github.com/open-ocean-sounding/echopy/blob/master/echopy/processing/mask_transient.py # noqa\n+    \"\"\"\n+    if \"depth\" not in ds_Sv.data_vars:\n+        raise ValueError(\n+            \"Masking attenuated signal requires depth data variable in `ds_Sv`. \"\n+            \"Consider adding depth with `ds_Sv = ep.consolidate.add_depth(ds_Sv)`.\"\n+        )\n+\n+    # Copy `ds_Sv`\n+    ds_Sv = ds_Sv.copy()\n+\n+    # Check for appropriate function passed in\n+    if func != \"nanmean\" and func != \"nanmedian\":\n+        raise ValueError(\"Input `func` is `nanmode`. `func` must be `nanmean` or `nanmedian`.\")\n+\n+    # Warn when `func=nanmedian` since the sorting overhead makes it incredibly slow compared to\n+    # `nanmean`.\n+    logger.warning(\n+        \"Consider using `func=nanmean`. `func=nanmedian` is an incredibly slow operation due to \"\n+        \"the overhead sorting.\"\n+    )\n+\n+    # Setup depth bin\n+    ds_Sv, depth_bin = _setup_and_validate(ds_Sv, \"depth\", depth_bin)\n+\n+    # Compute pooled Sv\n+    pooled_Sv = calc_transient_noise_pooled_Sv(\n+        ds_Sv, func, depth_bin, num_side_pings, exclude_above\n+    )\n+\n+    # Compute transient noise mask\n+    transient_noise_mask = ds_Sv[\"Sv\"] - pooled_Sv > transient_noise_threshold\n+\n+    return transient_noise_mask\n+\n+\n+def mask_impulse_noise(\n+    ds_Sv: xr.Dataset,\n+    depth_bin: str = \"5m\",\n+    num_side_pings: int = 2,\n+    impulse_noise_threshold: Union[int, float] = 10.0,\n+) -> xr.DataArray:\n+    \"\"\"\n+    Locate and create a mask for impulse noise using a ping-wise two-sided comparison.\n+\n+    Parameters\n+    ----------\n+    ds_Sv : xarray.Dataset\n+        Calibrated Sv data with depth data variable.\n+    depth_bin : str, default `5`m\n+        Donwsampling bin size along ``depth`` in meters.\n+    num_side_pings : int, default `2`\n+        Number of side pings to look at for the two-side comparison.\n+    impulse_noise_threshold : Union[int, float], default `10.0`dB\n+        Impulse noise threshold value (dB) for the two-side comparison.\n+\n+    Returns\n+    -------\n+    xr.Dataset\n+        Xarray boolean array impulse noise mask.\n+\n+    References\n+    ----------\n+    This function's implementation is based on the following text reference:\n+\n+        Ryan et al. (2015) Reducing bias due to noise and attenuation in\n+        open-ocean echo integration data, ICES Journal of Marine Science, 72: 2482\u20132493.\n+\n+    Additionally, code was derived from echopy's numpy single-channel implementation of\n+    impulse noise masking and translated into xarray code:\n+    https://github.com/open-ocean-sounding/echopy/blob/master/echopy/processing/mask_impulse.py # noqa\n+    \"\"\"\n+    if \"depth\" not in ds_Sv.data_vars:\n+        raise ValueError(\n+            \"Masking attenuated signal requires depth data variable in `ds_Sv`. \"\n+            \"Consider adding depth with `ds_Sv = ep.consolidate.add_depth(ds_Sv)`.\"\n+        )\n+\n+    # Copy `ds_Sv`\n+    ds_Sv = ds_Sv.copy()\n+\n+    # Setup and validate Sv and depth bin\n+    ds_Sv, depth_bin = _setup_and_validate(ds_Sv, \"depth\", depth_bin)\n+\n+    # Downsample and Upsample Sv along depth\n+    _, upsampled_Sv = downsample_upsample_along_depth(ds_Sv, depth_bin)\n \n-    See ``remove_noise`` for reference.\n+    # Create partial of `echopy_impulse_noise_mask`\n+    partial_echopy_impulse_noise_mask = partial(\n+        echopy_impulse_noise_mask,\n+        num_side_pings=num_side_pings,\n+        impulse_noise_threshold=impulse_noise_threshold,\n+    )\n+\n+    # Create noise mask\n+    impulse_noise_mask = xr.apply_ufunc(\n+        partial_echopy_impulse_noise_mask,\n+        upsampled_Sv,\n+        input_core_dims=[[\"range_sample\", \"ping_time\"]],\n+        output_core_dims=[[\"range_sample\", \"ping_time\"]],\n+        dask=\"parallelized\",\n+        vectorize=True,\n+        output_dtypes=[np.float64],\n+    )\n+\n+    return impulse_noise_mask\n+\n+\n+def mask_attenuated_signal(\n+    ds_Sv: xr.Dataset,\n+    upper_limit_sl: Union[int, float] = 400.0,\n+    lower_limit_sl: Union[int, float] = 500.0,\n+    num_side_pings: int = 15,\n+    attenuation_signal_threshold: Union[int, float] = 8.0,\n+) -> xr.DataArray:\n+    \"\"\"\n+    Locate attenuated signals and create an attenuated signal mask.\n+\n+    Parameters\n+    ----------\n+    ds_Sv : xarray.Dataset\n+        Calibrated Sv data with depth data variable.\n+    upper_limit_sl : Union[int, float], default `400`m\n+        Upper limit of deep scattering layer line (m).\n+    lower_limit_sl : Union[int, float], default `500`m\n+        Lower limit of deep scattering layer line (m).\n+    num_side_pings : int, default `15`\n+        Number of preceding & subsequent pings defining the block.\n+    attenuation_signal_threshold : Union[int, float], default `8.0`dB\n+        Attenuation signal threshold value (dB) for the ping-block comparison.\n+\n+    Returns\n+    -------\n+    xr.Dataset\n+        Xarray boolean array attenuated signal mask.\n+\n+    References\n+    ----------\n+    This function's implementation is based on the following text reference:\n+\n+        Ryan et al. (2015) Reducing bias due to noise and attenuation in\n+        open-ocean echo integration data, ICES Journal of Marine Science, 72: 2482\u20132493.\n+\n+    Additionally, code was derived from echopy's numpy single-channel implementation of\n+    attenuation signal masking and translated into xarray code:\n+    https://github.com/open-ocean-sounding/echopy/blob/master/echopy/processing/mask_attenuated.py # noqa\n+    \"\"\"\n+    if \"depth\" not in ds_Sv.data_vars:\n+        raise ValueError(\n+            \"Masking attenuated signal requires depth data variable in `ds_Sv`. \"\n+            \"Consider adding depth with `ds_Sv = ep.consolidate.add_depth(ds_Sv)`.\"\n+        )\n+\n+    # Check range values\n+    if upper_limit_sl > lower_limit_sl:\n+        raise ValueError(\"Minimum range has to be shorter than maximum range\")\n+\n+    # Copy `ds_Sv`\n+    ds_Sv = ds_Sv.copy()\n+\n+    # Return empty masks if searching range is outside the echosounder range\n+    if (upper_limit_sl > ds_Sv[\"depth\"].max()) or (lower_limit_sl < ds_Sv[\"depth\"].min()):\n+        attenuated_mask = xr.zeros_like(ds_Sv[\"Sv\"], dtype=bool)\n+        return attenuated_mask\n+\n+    # Create partial of echopy attenuation mask computation\n+    partial_echopy_attenuation_mask = partial(\n+        echopy_attenuated_signal_mask,\n+        upper_limit_sl=upper_limit_sl,\n+        lower_limit_sl=lower_limit_sl,\n+        num_side_pings=num_side_pings,\n+        attenuation_signal_threshold=attenuation_signal_threshold,\n+    )\n+\n+    # Compute attenuated signal mask\n+    attenuated_mask = xr.apply_ufunc(\n+        partial_echopy_attenuation_mask,\n+        ds_Sv[\"Sv\"],\n+        ds_Sv[\"depth\"],\n+        input_core_dims=[[\"ping_time\", \"range_sample\"], [\"ping_time\", \"range_sample\"]],\n+        output_core_dims=[[\"ping_time\", \"range_sample\"]],\n+        dask=\"parallelized\",\n+        vectorize=True,\n+        output_dtypes=[bool],\n+    )\n+\n+    return attenuated_mask\n+\n+\n+def estimate_background_noise(\n+    ds_Sv: xr.Dataset, ping_num: int, range_sample_num: int, noise_max: float = None\n+) -> xr.DataArray:\n+    \"\"\"\n+    Estimate background noise by computing mean calibrated power of a collection of pings.\n \n     Parameters\n     ----------\n     ds_Sv : xr.Dataset\n-        dataset containing ``Sv`` and ``echo_range`` [m]\n+        Dataset containing ``Sv`` and ``echo_range`` [m].\n     ping_num : int\n-        number of pings to obtain noise estimates\n+        Number of pings to obtain noise estimates\n     range_sample_num : int\n-        number of samples along the ``range_sample`` dimension to obtain noise estimates\n-    noise_max : float\n-        the upper limit for background noise expected under the operating conditions\n+        Number of samples along the ``range_sample`` dimension to obtain noise estimates.\n+    noise_max : float, default `None`\n+        The upper limit for background noise expected under the operating conditions.\n \n     Returns\n     -------\n     A DataArray containing noise estimated from the input ``ds_Sv``\n+\n+    Notes\n+    -----\n+    This function's implementation is based on the following text reference:\n+\n+        De Robertis & Higginbottom. 2007.\n+        A post-processing technique to estimate the signal-to-noise ratio\n+        and remove echosounder background noise.\n+        ICES Journal of Marine Sciences 64(6): 1282\u20131291.\n     \"\"\"\n-    noise_obj = NoiseEst(ds_Sv=ds_Sv.copy(), ping_num=ping_num, range_sample_num=range_sample_num)\n-    noise_obj.estimate_noise(noise_max=noise_max)\n-    return noise_obj.Sv_noise\n+    # Compute transmission loss\n+    spreading_loss = 20 * np.log10(ds_Sv[\"echo_range\"].where(ds_Sv[\"echo_range\"] >= 1, other=1))\n+    absorption_loss = 2 * ds_Sv[\"sound_absorption\"] * ds_Sv[\"echo_range\"]\n+\n+    # Compute power binned averages\n+    power_cal = _log2lin(ds_Sv[\"Sv\"] - spreading_loss - absorption_loss)\n+    power_cal_binned_avg = 10 * np.log10(\n+        power_cal.coarsen(\n+            ping_time=ping_num,\n+            range_sample=range_sample_num,\n+            boundary=\"pad\",\n+        ).mean()\n+    )\n+\n+    # Compute noise\n+    noise = power_cal_binned_avg.min(dim=\"range_sample\", skipna=True)\n+\n+    # Align noise `ping_time` to the first index of each coarsened `ping_time` bin\n+    noise = noise.assign_coords(ping_time=ping_num * np.arange(len(noise[\"ping_time\"])))\n+\n+    # Limit max noise level\n+    noise = noise.where(noise < noise_max, noise_max) if noise_max is not None else noise\n+\n+    # Upsample noise to original ping time dimension\n+    Sv_noise = (\n+        noise.reindex({\"ping_time\": power_cal[\"ping_time\"]}, method=\"ffill\")\n+        + spreading_loss\n+        + absorption_loss\n+    )\n+\n+    return Sv_noise\n \n \n @add_processing_level(\"L*B\")\n-def remove_noise(ds_Sv, ping_num, range_sample_num, noise_max=None, SNR_threshold=3):\n+def remove_background_noise(\n+    ds_Sv: xr.Dataset,\n+    ping_num: int,\n+    range_sample_num: int,\n+    noise_max: float = None,\n+    SNR_threshold: float = 3.0,\n+) -> xr.Dataset:\n     \"\"\"\n     Remove noise by using estimates of background noise\n     from mean calibrated power of a collection of pings.\n \n-    Reference: De Robertis & Higginbottom. 2007.\n-    A post-processing technique to estimate the signal-to-noise ratio\n-    and remove echosounder background noise.\n-    ICES Journal of Marine Sciences 64(6): 1282\u20131291.\n-\n     Parameters\n     ----------\n     ds_Sv : xr.Dataset\n         dataset containing ``Sv`` and ``echo_range`` [m]\n     ping_num : int\n-        number of pings to obtain noise estimates\n+        Number of pings to obtain noise estimates.\n     range_sample_num : int\n-        number of samples along the ``range_sample`` dimension to obtain noise estimates\n-    noise_max : float\n-        the upper limit for background noise expected under the operating conditions\n-    SNR_threshold : float\n-        acceptable signal-to-noise ratio, default to 3 dB\n+        Number of samples along the ``range_sample`` dimension to obtain noise estimates.\n+    noise_max : float, default `None`\n+        The upper limit for background noise expected under the operating conditions.\n+    SNR_threshold : float, default `3.0`\n+        Acceptable signal-to-noise ratio, default to 3 dB.\n \n     Returns\n     -------\n     The input dataset with additional variables, including\n     the corrected Sv (``Sv_corrected``) and the noise estimates (``Sv_noise``)\n+\n+    Notes\n+    -----\n+    This function's implementation is based on the following text reference:\n+\n+        De Robertis & Higginbottom. 2007.\n+        A post-processing technique to estimate the signal-to-noise ratio\n+        and remove echosounder background noise.\n+        ICES Journal of Marine Sciences 64(6): 1282\u20131291.\n     \"\"\"\n-    noise_obj = NoiseEst(ds_Sv=ds_Sv.copy(), ping_num=ping_num, range_sample_num=range_sample_num)\n-    noise_obj.remove_noise(noise_max=noise_max, SNR_threshold=SNR_threshold)\n-    ds_Sv = noise_obj.ds_Sv\n+    # Compute Sv_noise\n+    Sv_noise = estimate_background_noise(ds_Sv, ping_num, range_sample_num, noise_max=noise_max)\n+\n+    # Correct Sv for noise\n+    linear_corrected_Sv = _log2lin(ds_Sv[\"Sv\"]) - _log2lin(Sv_noise)\n+    corrected_Sv = _lin2log(linear_corrected_Sv.where(linear_corrected_Sv > 0, other=np.nan))\n+    corrected_Sv = corrected_Sv.where(corrected_Sv - Sv_noise > SNR_threshold, other=np.nan)\n \n+    # Assemble output dataset\n+    ds_Sv[\"Sv_noise\"] = Sv_noise\n+    ds_Sv[\"Sv_noise\"] = add_remove_background_noise_attrs(\n+        ds_Sv[\"Sv_noise\"], \"noise\", ping_num, range_sample_num, SNR_threshold, noise_max\n+    )\n+    ds_Sv[\"Sv_corrected\"] = corrected_Sv\n+    ds_Sv[\"Sv_corrected\"] = add_remove_background_noise_attrs(\n+        ds_Sv[\"Sv_corrected\"], \"corrected\", ping_num, range_sample_num, SNR_threshold, noise_max\n+    )\n     prov_dict = echopype_prov_attrs(process_type=\"processing\")\n-    prov_dict[\"processing_function\"] = \"clean.remove_noise\"\n+    prov_dict[\"processing_function\"] = \"clean.remove_background_noise\"\n     ds_Sv = ds_Sv.assign_attrs(prov_dict)\n \n-    # The output ds_Sv is built as a copy of the input ds_Sv, so the step below is\n+    # The output `ds_Sv` is built as a copy of the input `ds_Sv`, so the step below is\n     # not needed, strictly speaking. But doing makes the decorator function more generic\n     ds_Sv = insert_input_processing_level(ds_Sv, input_ds=ds_Sv)\n \ndiff --git a/echopype/clean/noise_est.py b/echopype/clean/noise_est.py\ndeleted file mode 100644\nindex b9fbb2355..000000000\n--- a/echopype/clean/noise_est.py\n+++ /dev/null\n@@ -1,134 +0,0 @@\n-import numpy as np\n-\n-from ..utils import uwa\n-\n-\n-class NoiseEst:\n-    \"\"\"\n-    Attributes\n-    ----------\n-    ds_Sv : xr.Dataset\n-        dataset containing ``Sv`` and ``echo_range`` [m]\n-    ping_num : int\n-        number of pings to obtain noise estimates\n-    range_sample_num : int\n-        number of samples along ``echo_range`` to obtain noise estimates\n-    \"\"\"\n-\n-    def __init__(self, ds_Sv, ping_num, range_sample_num):\n-        self.ds_Sv = ds_Sv\n-        self.ping_num = ping_num\n-        self.range_sample_num = range_sample_num\n-        self.spreading_loss = None\n-        self.absorption_loss = None\n-        self.Sv_noise = None\n-\n-        self._compute_transmission_loss()\n-        self._compute_power_cal()\n-\n-    def _compute_transmission_loss(self):\n-        \"\"\"Compute transmission loss\"\"\"\n-        if \"sound_absorption\" not in self.ds_Sv:\n-            sound_absorption = uwa.calc_absorption(\n-                frequency=self.ds_Sv.frequency_nominal,\n-                temperature=self.ds_Sv[\"temperature\"],\n-                salinity=self.ds_Sv[\"salinity\"],\n-                pressure=self.ds_Sv[\"pressure\"],\n-            )\n-        else:\n-            sound_absorption = self.ds_Sv[\"sound_absorption\"]\n-\n-        # Transmission loss\n-        self.spreading_loss = 20 * np.log10(\n-            self.ds_Sv[\"echo_range\"].where(self.ds_Sv[\"echo_range\"] >= 1, other=1)\n-        )\n-        self.absorption_loss = 2 * sound_absorption * self.ds_Sv[\"echo_range\"]\n-\n-    def _compute_power_cal(self):\n-        \"\"\"Compute calibrated power without TVG, linear domain\"\"\"\n-        self.power_cal = 10 ** (\n-            (self.ds_Sv[\"Sv\"] - self.spreading_loss - self.absorption_loss) / 10\n-        )\n-\n-    def estimate_noise(self, noise_max=None):\n-        \"\"\"Estimate noise from a collected of pings\n-\n-        Parameters\n-        ----------\n-        noise_max : Union[int, float]\n-            the upper limit for background noise expected under the operating conditions\n-        \"\"\"\n-        power_cal_binned_avg = 10 * np.log10(  # binned averages of calibrated power\n-            self.power_cal.coarsen(\n-                ping_time=self.ping_num,\n-                range_sample=self.range_sample_num,\n-                boundary=\"pad\",\n-            ).mean()\n-        )\n-        noise = power_cal_binned_avg.min(dim=\"range_sample\", skipna=True)\n-\n-        # align ping_time to first of each ping collection\n-        noise[\"ping_time\"] = self.power_cal[\"ping_time\"][:: self.ping_num]\n-\n-        if noise_max is not None:\n-            noise = noise.where(noise < noise_max, noise_max)  # limit max noise level\n-        self.Sv_noise = (\n-            noise.reindex(\n-                {\"ping_time\": self.power_cal[\"ping_time\"]}, method=\"ffill\"\n-            )  # forward fill empty index\n-            + self.spreading_loss\n-            + self.absorption_loss\n-        )\n-\n-    def remove_noise(self, noise_max=None, SNR_threshold=3):\n-        \"\"\"\n-        Remove noise by using estimates of background noise\n-        from mean calibrated power of a collection of pings.\n-\n-        This method adds two data variables to the input ``ds_Sv``:\n-        - corrected Sv (``Sv_corrected``)\n-        - noise estimates (``Sv_noise``)\n-\n-        Reference: De Robertis & Higginbottom. 2007.\n-        A post-processing technique to estimate the signal-to-noise ratio\n-        and remove echosounder background noise.\n-        ICES Journal of Marine Sciences 64(6): 1282\u20131291.\n-\n-        Parameters\n-        ----------\n-        noise_max : float\n-            the upper limit for background noise expected under the operating conditions\n-        SNR_threshold : float\n-            acceptable signal-to-noise ratio, default to 3 dB\n-        \"\"\"\n-        # Compute Sv_noise\n-        self.estimate_noise(noise_max=noise_max)\n-\n-        # Sv corrected for noise\n-        # linear domain\n-        fac = 10 ** (self.ds_Sv[\"Sv\"] / 10) - 10 ** (self.Sv_noise / 10)\n-        Sv_corr = 10 * np.log10(fac.where(fac > 0, other=np.nan))\n-        Sv_corr = Sv_corr.where(\n-            Sv_corr - self.Sv_noise > SNR_threshold, other=np.nan\n-        )  # other=-999 (from paper)\n-\n-        # Assemble output dataset\n-        def add_attrs(sv_type, da):\n-            da.attrs = {\n-                \"long_name\": f\"Volume backscattering strength, {sv_type} (Sv re 1 m-1)\",\n-                \"units\": \"dB\",\n-                \"actual_range\": [\n-                    round(float(da.min().values), 2),\n-                    round(float(da.max().values), 2),\n-                ],\n-                \"noise_ping_num\": self.ping_num,\n-                \"noise_range_sample_num\": self.range_sample_num,\n-                \"SNR_threshold\": SNR_threshold,\n-                \"noise_max\": noise_max,\n-            }\n-\n-        self.ds_Sv[\"Sv_noise\"] = self.Sv_noise\n-        add_attrs(\"noise\", self.ds_Sv[\"Sv_noise\"])\n-\n-        self.ds_Sv[\"Sv_corrected\"] = Sv_corr\n-        add_attrs(\"corrected\", self.ds_Sv[\"Sv_corrected\"])\ndiff --git a/echopype/clean/utils.py b/echopype/clean/utils.py\nnew file mode 100644\nindex 000000000..e33910a84\n--- /dev/null\n+++ b/echopype/clean/utils.py\n@@ -0,0 +1,238 @@\n+import flox.xarray\n+import numpy as np\n+import xarray as xr\n+\n+from ..commongrid.utils import _convert_bins_to_interval_index\n+from ..utils.compute import _lin2log, _log2lin\n+\n+\n+def calc_transient_noise_pooled_Sv(\n+    ds_Sv: xr.Dataset, func: str, depth_bin: float, num_side_pings: int, exclude_above: float\n+) -> xr.DataArray:\n+    \"\"\"\n+    Compute pooled Sv array for transient noise masking.\n+    \"\"\"\n+    # Create ping time indices array\n+    ping_time_indices = xr.DataArray(\n+        np.arange(len(ds_Sv[\"ping_time\"]), dtype=int),\n+        dims=[\"ping_time\"],\n+        coords=[ds_Sv[\"ping_time\"]],\n+        name=\"ping_time_indices\",\n+    )\n+\n+    # Create NaN pooled Sv array\n+    pooled_Sv = xr.full_like(ds_Sv[\"Sv\"], np.nan)\n+\n+    # Set min max values\n+    depth_values_min = ds_Sv[\"depth\"].min()\n+    depth_values_max = ds_Sv[\"depth\"].max()\n+    ping_time_index_min = 0\n+    ping_time_index_max = len(ds_Sv[\"ping_time\"])\n+\n+    # Iterate through the channel dimension\n+    for channel_index in range(len(ds_Sv[\"channel\"])):\n+\n+        # Set channel arrays\n+        chan_Sv = ds_Sv[\"Sv\"].isel(channel=channel_index)\n+        chan_depth = ds_Sv[\"depth\"].isel(channel=channel_index)\n+\n+        # Iterate through the range sample dimension\n+        for range_sample_index in range(len(ds_Sv[\"range_sample\"])):\n+\n+            # Iterate through the ping time dimension\n+            for ping_time_index in range(len(ds_Sv[\"ping_time\"])):\n+\n+                # Grab current depth\n+                current_depth = ds_Sv[\"depth\"].isel(\n+                    channel=channel_index,\n+                    range_sample=range_sample_index,\n+                    ping_time=ping_time_index,\n+                )\n+\n+                # Check if current value is within a valid window\n+                if (\n+                    (current_depth - depth_bin >= depth_values_min)\n+                    & (current_depth + depth_bin <= depth_values_max)\n+                    & (current_depth - depth_bin >= exclude_above)\n+                    & (ping_time_index - num_side_pings >= ping_time_index_min)\n+                    & (ping_time_index + num_side_pings <= ping_time_index_max)\n+                ):\n+\n+                    # Compute aggregate window Sv value\n+                    window_mask = (\n+                        (current_depth - depth_bin <= chan_depth)\n+                        & (chan_depth <= current_depth + depth_bin)\n+                        & (ping_time_index - num_side_pings <= ping_time_indices)\n+                        & (ping_time_indices <= ping_time_index + num_side_pings)\n+                    )\n+                    window_Sv = chan_Sv.where(window_mask, other=np.nan).pipe(_log2lin)\n+                    aggregate_window_Sv_value = window_Sv.pipe(\n+                        np.nanmean if func == \"nanmean\" else np.nanmedian\n+                    )\n+                    aggregate_window_Sv_value = _lin2log(aggregate_window_Sv_value)\n+\n+                    # Put aggregate value in pooled Sv array\n+                    pooled_Sv[\n+                        dict(\n+                            channel=channel_index,\n+                            range_sample=range_sample_index,\n+                            ping_time=ping_time_index,\n+                        )\n+                    ] = aggregate_window_Sv_value\n+\n+    return pooled_Sv\n+\n+\n+def downsample_upsample_along_depth(ds_Sv: xr.Dataset, depth_bin: float) -> xr.DataArray:\n+    \"\"\"\n+    Downsample and upsample Sv to mimic what was done in echopy impulse\n+    noise masking.\n+    \"\"\"\n+    # Validate and compute range interval\n+    depth_min = ds_Sv[\"depth\"].min()\n+    depth_max = ds_Sv[\"depth\"].max()\n+    range_interval = np.arange(depth_min, depth_max + depth_bin, depth_bin)\n+    range_interval = _convert_bins_to_interval_index(range_interval)\n+\n+    # Downsample Sv along range sample\n+    downsampled_Sv = flox.xarray.xarray_reduce(\n+        ds_Sv[\"Sv\"].pipe(_log2lin),\n+        ds_Sv[\"channel\"],\n+        ds_Sv[\"ping_time\"],\n+        ds_Sv[\"depth\"],\n+        expected_groups=(None, None, range_interval),\n+        isbin=[False, False, True],\n+        method=\"map-reduce\",\n+        func=\"nanmean\",\n+        skipna=True,\n+    ).pipe(_lin2log)\n+\n+    # Assign a depth bin index to each Sv depth value\n+    depth_bin_assignment = xr.DataArray(\n+        np.digitize(\n+            ds_Sv[\"depth\"], [interval.left for interval in downsampled_Sv[\"depth_bins\"].data]\n+        ),\n+        dims=[\"channel\", \"ping_time\", \"range_sample\"],\n+    )\n+\n+    # Initialize upsampled Sv\n+    upsampled_Sv = ds_Sv[\"Sv\"].copy()\n+\n+    # Iterate through all channels\n+    for channel_index in range(len(depth_bin_assignment[\"channel\"])):\n+        # Iterate through all ping times\n+        for ping_time_index in range(len(depth_bin_assignment[\"ping_time\"])):\n+            # Get unique range sample values along a single ping from the digitized depth array:\n+            # NOTE: The unique index corresponds to the first unique value's position which in\n+            # turn corresponds to the first range sample value contained in each depth bin.\n+            _, unique_range_sample_indices = np.unique(\n+                depth_bin_assignment.isel(channel=channel_index, ping_time=ping_time_index).data,\n+                return_index=True,\n+            )\n+\n+            # Select a single ping downsampled Sv vector\n+            subset_downsampled_Sv = downsampled_Sv.isel(\n+                channel=channel_index, ping_time=ping_time_index\n+            )\n+\n+            # Substitute depth bin coordinate in the downsampled Sv to be the range sample value\n+            # corresponding to the first element (lowest depth value) of each depth bin, and rename\n+            # `depth_bin` coordinate to `range_sample`.\n+            subset_downsampled_Sv = subset_downsampled_Sv.assign_coords(\n+                {\"depth_bins\": unique_range_sample_indices}\n+            ).rename({\"depth_bins\": \"range_sample\"})\n+\n+            # Upsample via `reindex` `ffill`\n+            upsampled_Sv[dict(channel=channel_index, ping_time=ping_time_index)] = (\n+                subset_downsampled_Sv.reindex(\n+                    {\"range_sample\": ds_Sv[\"range_sample\"]}, method=\"ffill\"\n+                )\n+            )\n+\n+    return downsampled_Sv, upsampled_Sv\n+\n+\n+def echopy_impulse_noise_mask(\n+    Sv: np.ndarray, num_side_pings: int, impulse_noise_threshold: float\n+) -> np.ndarray:\n+    \"\"\"Single-channel impulse noise mask computation from echopy.\"\"\"\n+    # Construct the two ping side-by-side comparison arrays\n+    dummy = np.zeros((Sv.shape[0], num_side_pings)) * np.nan\n+    comparison_forward = Sv - np.c_[Sv[:, num_side_pings:], dummy]\n+    comparison_backward = Sv - np.c_[dummy, Sv[:, 0:-num_side_pings]]\n+    comparison_forward[np.isnan(comparison_forward)] = np.inf\n+    comparison_backward[np.isnan(comparison_backward)] = np.inf\n+\n+    # Create mask by checking if comparison arrays are above `impulse_noise_threshold`\n+    maskf = comparison_forward > impulse_noise_threshold\n+    maskb = comparison_backward > impulse_noise_threshold\n+    mask = maskf & maskb\n+\n+    return mask\n+\n+\n+def echopy_attenuated_signal_mask(\n+    Sv: np.ndarray,\n+    depth: np.ndarray,\n+    upper_limit_sl: float,\n+    lower_limit_sl: float,\n+    num_side_pings: int,\n+    attenuation_signal_threshold: float,\n+) -> np.ndarray:\n+    \"\"\"Single-channel attenuated signal mask computation from echopy.\"\"\"\n+    # Initialize mask\n+    attenuated_mask = np.zeros(Sv.shape, dtype=bool)\n+\n+    for ping_time_idx in range(Sv.shape[0]):\n+\n+        # Find indices for upper and lower SL limits\n+        up = np.argmin(abs(depth[ping_time_idx, :] - upper_limit_sl))\n+        lw = np.argmin(abs(depth[ping_time_idx, :] - lower_limit_sl))\n+\n+        # Mask when attenuation masking is feasible\n+        if not (\n+            (ping_time_idx - num_side_pings < 0)\n+            | (ping_time_idx + num_side_pings > Sv.shape[0] - 1)\n+            | np.all(np.isnan(Sv[ping_time_idx, up:lw]))\n+        ):\n+            # Compare ping and block medians, and mask ping if difference greater than\n+            # threshold.\n+            pingmedian = _lin2log(np.nanmedian(_log2lin(Sv[ping_time_idx, up:lw])))\n+            blockmedian = _lin2log(\n+                np.nanmedian(\n+                    _log2lin(\n+                        Sv[\n+                            (ping_time_idx - num_side_pings) : (ping_time_idx + num_side_pings),\n+                            up:lw,\n+                        ]\n+                    )\n+                )\n+            )\n+            if (pingmedian - blockmedian) < attenuation_signal_threshold:\n+                attenuated_mask[ping_time_idx, :] = True\n+\n+    return attenuated_mask\n+\n+\n+def add_remove_background_noise_attrs(\n+    da: xr.DataArray,\n+    sv_type: str,\n+    ping_num: int,\n+    range_sample_num: int,\n+    SNR_threshold: float,\n+    noise_max: float,\n+) -> xr.DataArray:\n+    \"\"\"Add attributes to a `remove_background_noise` data array.\"\"\"\n+    da.attrs = {\n+        \"long_name\": f\"Volume backscattering strength, {sv_type} (Sv re 1 m-1)\",\n+        \"units\": \"dB\",\n+        \"actual_range\": [\n+            round(float(da.min().values), 2),\n+            round(float(da.max().values), 2),\n+        ],\n+        \"noise_ping_num\": ping_num,\n+        \"noise_range_sample_num\": range_sample_num,\n+        \"SNR_threshold\": SNR_threshold,\n+        \"noise_max\": noise_max,\n+    }\n+    return da\n", "instance_id": "OSOceanAcoustics__echopype-1316", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to implement noise and attenuation filters (IN, AS, TN) from a specific research paper using Xarray, with a reference to an existing implementation in NumPy from the echopy library. It also mentions the potential use of Dask for delayed computation, with a fallback to a separate issue if it's too complex. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the expected input and output formats for the filters, nor does it specify constraints or performance requirements. Additionally, while it references the research paper and echopy library, it lacks detailed examples or edge cases to guide the implementation. The mention of Dask is vague, as it does not clarify the criteria for \"too involved.\" Despite these gaps, the overall goal and scope are understandable, especially with the provided code changes as context.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving multiple functions across different files (e.g., `api.py`, `utils.py`, and removal of `noise_est.py`), with a complete refactoring of noise estimation and removal logic, as well as the addition of new noise filtering functions (transient, impulse, and attenuated signal masks). This requires understanding interactions between different parts of the codebase, such as how data flows through Xarray datasets and how existing utilities are leveraged. Second, the number of technical concepts involved is substantial, including proficiency in Xarray for data manipulation, NumPy for numerical operations, and potentially Dask for parallel processing (though not fully implemented here). It also demands domain-specific knowledge of oceanographic data processing and noise filtering algorithms as described in the referenced paper (Ryan et al., 2015), which adds to the complexity. Third, the implementation handles edge cases and error conditions, such as validating input data (e.g., presence of depth variable), managing NaN values, and setting thresholds for noise detection, which requires careful logic design. While the problem does not appear to impact the overall system architecture fundamentally, the translation of algorithms from NumPy (echopy) to Xarray, along with the need to maintain correctness and performance, makes this a challenging task. It requires a deep understanding of both the scientific domain and the technical tools, but it does not reach the \"Very Hard\" level (0.8-1.0) as it does not involve system-level redesign or extremely intricate distributed systems logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "QLever only parses WKT points with decimal points\nThe regular expression used to detect WKT points expects decimal points to be present in the coordinates, see https://github.com/ad-freiburg/qlever/blob/f856919842dc3492d2236fe4822d6662b582edcc/src/util/GeoSparqlHelpers.cpp#L27\r\n\r\nThis results in many correct WKT points within the allowed value range not being folded into the ID, see this query for examples: \r\n\r\nhttps://qlever.cs.uni-freiburg.de/osm-planet/uo4jNv\n", "patch": "diff --git a/src/util/GeoSparqlHelpers.cpp b/src/util/GeoSparqlHelpers.cpp\nindex 3cc636491..796205dae 100644\n--- a/src/util/GeoSparqlHelpers.cpp\n+++ b/src/util/GeoSparqlHelpers.cpp\n@@ -26,7 +26,7 @@ static constexpr auto wktPointRegex = ctll::fixed_string(\n     \"^\\\\s*[Pp][Oo][Ii][Nn][Tt]\\\\s*\\\\(\\\\s*\"\n     \"(-?[0-9]+|-?[0-9]+\\\\.[0-9]+)\"\n     \"\\\\s+\"\n-    \"(-?[0-9+]|-?[0-9]+\\\\.[0-9]+)\"\n+    \"(-?[0-9]+|-?[0-9]+\\\\.[0-9]+)\"\n     \"\\\\s*\\\\)\\\\s*$\");\n \n // Parse a single WKT point and returns a pair of longitude and latitude. If\n", "instance_id": "ad-freiburg__qlever-1593", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the current regular expression in QLever for parsing WKT points only accepts coordinates with decimal points, which excludes valid integer coordinates within the allowed range. It provides a specific reference to the problematic code line in the GitHub repository and includes a link to a query demonstrating examples of the issue. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected input format after the fix (e.g., whether negative signs or specific ranges are strictly enforced beyond the regex change). Additionally, edge cases or specific constraints (e.g., handling of whitespace variations or invalid formats) are not mentioned, which could lead to uncertainty during implementation or testing. Overall, the goal is clear, but the lack of exhaustive details on requirements or edge cases prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward fix to a regular expression in a single line of code within one file (GeoSparqlHelpers.cpp). The code change is minimal, correcting a typo-like error in the regex pattern (removing an erroneous '+' character in the latitude part of the WKT point pattern). The scope is extremely limited, with no impact on the broader codebase architecture or interactions between modules. The technical concepts required are basic: understanding regular expressions and their syntax, which is a fundamental skill in most programming languages, including C++ (used here). No advanced algorithms, design patterns, or domain-specific knowledge beyond basic string parsing are needed. Edge cases or error handling are not explicitly mentioned in the problem statement, and the provided code change does not introduce or modify any error handling logic. The task is akin to fixing a minor bug or typo, requiring minimal effort and understanding of the surrounding code. Therefore, a difficulty score of 0.15 is appropriate, reflecting a very easy problem with a trivial solution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Refactor: remove implicit use of db and db.session in model, controllers, etc\n**BIG NOTE: This work is in progress in branch `issue_503_db_session_refactoring`**\r\n\r\nThis is a general issue to group several independent issues with the code.  In summary, there are many places where there is an implicit dependency on the `db.session` which Flask manages, which therefore couples the code to Flask unnecessarily.\r\n\r\nFor example, the various services have `from lute import db`, and then service methods use `db.session.query`.  It feels to me that instead the service should be classes instantiated with a `session` object, and use `self.session` in each method as needed.  Then the flask controllers should instantiate the services with the `db.session`, and use them as before.\r\n\r\n**Notes on the work:**\r\n\r\n* I think that each of these items should be handled separately.\r\n* If anyone wants to try working on any of them, I'll create a separate issue and assign it to you (or me), so that we can stay reasonably organized.\r\n* Each change to any object API will impact unit tests as well.\r\n* I think that these should be tackled in the order presented (approximately), as changes to lower-level items can generally be done gradually before the higher-level items are changed\r\n\r\nI'll add to the list of items below as I come across places to work on.\r\n\r\n## business repos\r\n\r\n- [x] lute/term/model.py, `Repository` : change the constructor to take a session, and use self.session instead of self.db.session\r\n- [x] lute/book/model.py, `Repository` : change the constructor to take a session, and use self.session instead of self.db.session\r\n\r\n## ORM-level repositories in lute/models/\r\n\r\nMany classes have static methods (e.g. `find` etc) that use a `db.session`.  I'd like to take all of those and move them to Repository classes, e.g, `BookTag` will have `BookTagRepository` which will take a session arg in its constructor, and then all static methods can be moved out of BookTag and into the new repo class as instance methods.\r\n\r\n- [x] book.py BookTagRepository\r\n- [x] book.py BookRepository\r\n- [x] book.py TextRepository\r\n- [x] language.py LanguageRepository\r\n- [x] term.py TermTagRepository\r\n- [x] term.py StatusRepository\r\n- [x] term.py TermRepository\r\n- [x] setting.py ... hmmm not sure about this one, separate repos for each one?\r\n\r\n## Change parser settings reads\r\n\r\n- [x] currently the Japanese parser reads from the db, but it uses the db.session ... I feel that the settings should be passed in perhaps as a setter method.  Different parsers might want different settings ... or perhaps the settings should be stored in environment variables when the settings are set (or loaded)\r\n\r\n## Convert services to classes (?) or pass session explicitly\r\n\r\nChange \"service\" modules to objects that take a db.session as a parameter, and use that `self.session` everywhere instead of `db.session` ?? Optionally, pass the session as a parameter to each function that needs it.\r\n\r\n- [x] lute/termimport/service.py\r\n- [x] lute/term_parent_map/service.py\r\n- [x] lute/read/service.py\r\n- [x] lute/read/render/service.py\r\n- [x] lute/language/service.py\r\n- [x] lute/book/service.py\r\n- [x] lute/backup/service.py\r\n- [x] lute/themes/service.py\r\n- [x] lute/stats/service.py\r\n\r\n## remove db imports where possible\r\n\r\n- [x] lute/db/data_cleanup.py:from lute.db import db\r\n- [x] lute/db/management.py:from lute.db import db\r\n- [x] lute/db/demo.py:from lute.db import db\n", "patch": "diff --git a/lute/app_factory.py b/lute/app_factory.py\nindex 6b127ab9..7d6893ff 100644\n--- a/lute/app_factory.py\n+++ b/lute/app_factory.py\n@@ -25,8 +25,9 @@\n from lute.config.app_config import AppConfig\n from lute.db import db\n from lute.db.setup.main import setup_db\n+from lute.db.management import add_default_user_settings\n from lute.db.data_cleanup import clean_data\n-import lute.backup.service as backupservice\n+from lute.backup.service import Service as BackupService\n import lute.db.demo\n import lute.utils.formutils\n \n@@ -34,8 +35,9 @@\n \n from lute.models.book import Book\n from lute.models.language import Language\n-from lute.models.setting import BackupSettings, UserSetting\n-from lute.book.stats import mark_stale\n+from lute.settings.current import refresh_global_settings, current_settings\n+from lute.models.repositories import UserSettingRepository\n+from lute.book.stats import Service as StatsService\n \n from lute.book.routes import bp as book_bp\n from lute.bookmarks.routes import bp as bookmarks_bp\n@@ -111,7 +113,8 @@ def inject_menu_bar_vars():\n         \"\"\"\n         Inject backup settings into the all templates for the menu bar.\n         \"\"\"\n-        bs = BackupSettings.get_backup_settings()\n+        us_repo = UserSettingRepository(db.session)\n+        bs = us_repo.get_backup_settings()\n         have_languages = len(db.session.query(Language).all()) > 0\n         ret = {\n             \"have_languages\": have_languages,\n@@ -119,27 +122,31 @@ def inject_menu_bar_vars():\n             \"backup_directory\": bs.backup_dir,\n             \"backup_last_display_date\": bs.last_backup_display_date,\n             \"backup_time_since\": bs.time_since_last_backup,\n-            \"user_settings\": json.dumps(UserSetting.all_settings()),\n+            \"user_settings\": json.dumps(current_settings),\n         }\n         return ret\n \n     @app.route(\"/\")\n     def index():\n-        is_production = not lute.db.demo.contains_demo_data()\n-        bkp_settings = BackupSettings.get_backup_settings()\n+        is_production = not lute.db.demo.contains_demo_data(db.session)\n+        us_repo = UserSettingRepository(db.session)\n+        bkp_settings = us_repo.get_backup_settings()\n \n         have_books = len(db.session.query(Book).all()) > 0\n         have_languages = len(db.session.query(Language).all()) > 0\n-        language_choices = lute.utils.formutils.language_choices(\"(all languages)\")\n-        current_language_id = lute.utils.formutils.valid_current_language_id()\n+        language_choices = lute.utils.formutils.language_choices(\n+            db.session, \"(all languages)\"\n+        )\n+        current_language_id = lute.utils.formutils.valid_current_language_id(db.session)\n \n-        should_run_auto_backup = backupservice.should_run_auto_backup(bkp_settings)\n+        bs = BackupService(db.session)\n+        should_run_auto_backup = bs.should_run_auto_backup(bkp_settings)\n         # Only back up if we have books, otherwise the backup is\n         # kicked off when the user empties the demo database.\n         if is_production and have_books and should_run_auto_backup:\n             return redirect(\"/backup/backup\", 302)\n \n-        warning_msg = backupservice.backup_warning(bkp_settings)\n+        warning_msg = bs.backup_warning(bkp_settings)\n         backup_show_warning = (\n             bkp_settings.backup_warn\n             and bkp_settings.backup_enabled\n@@ -152,7 +159,7 @@ def index():\n                 hide_homelink=True,\n                 dbname=app_config.dbname,\n                 datapath=app_config.datapath,\n-                tutorial_book_id=lute.db.demo.tutorial_book_id(),\n+                tutorial_book_id=lute.db.demo.tutorial_book_id(db.session),\n                 have_books=have_books,\n                 have_languages=have_languages,\n                 language_choices=language_choices,\n@@ -167,14 +174,15 @@ def index():\n     @app.route(\"/refresh_all_stats\")\n     def refresh_all_stats():\n         books_to_update = db.session.query(Book).filter(Book.archived == 0).all()\n+        svc = StatsService(db.session)\n         for book in books_to_update:\n-            mark_stale(book)\n+            svc.mark_stale(book)\n         return redirect(\"/\", 302)\n \n     @app.route(\"/wipe_database\")\n     def wipe_db():\n-        if lute.db.demo.contains_demo_data():\n-            lute.db.demo.delete_demo_data()\n+        if lute.db.demo.contains_demo_data(db.session):\n+            lute.db.demo.delete_demo_data(db.session)\n             msg = \"\"\"\n             The database has been wiped clean.  Have fun! <br /><br />\n             <i>(Lute has automatically enabled backups --\n@@ -185,8 +193,8 @@ def wipe_db():\n \n     @app.route(\"/remove_demo_flag\")\n     def remove_demo():\n-        if lute.db.demo.contains_demo_data():\n-            lute.db.demo.remove_flag()\n+        if lute.db.demo.contains_demo_data(db.session):\n+            lute.db.demo.remove_flag(db.session)\n             msg = \"\"\"\n             Demo mode deactivated. Have fun! <br /><br />\n             <i>(Lute has automatically enabled backups --\n@@ -305,9 +313,10 @@ def _pragmas_on_connect(dbapi_con, con_record):  # pylint: disable=unused-argume\n \n     with app.app_context():\n         db.create_all()\n-        UserSetting.load()\n+        add_default_user_settings(db.session, app_config.default_user_backup_path)\n+        refresh_global_settings(db.session)\n         # TODO valid parsers: do parser check, mark valid as active, invalid as inactive.\n-        clean_data()\n+        clean_data(db.session)\n     app.db = db\n \n     _add_base_routes(app, app_config)\n@@ -392,6 +401,7 @@ def null_print(s):  # pylint: disable=unused-argument\n     outfunc(\"Initializing app.\")\n     app = _create_app(app_config, extra_config)\n \n+    # Plugins are loaded after the app, as they may use settings etc.\n     _init_parser_plugins(app_config.plugin_datapath, outfunc)\n \n     return app\ndiff --git a/lute/backup/routes.py b/lute/backup/routes.py\nindex ffaf647d..1fc10992 100644\n--- a/lute/backup/routes.py\n+++ b/lute/backup/routes.py\n@@ -16,20 +16,28 @@\n     send_file,\n     flash,\n )\n-from lute.models.setting import BackupSettings\n-from lute.backup.service import create_backup, skip_this_backup, list_backups\n+from lute.db import db\n+from lute.models.repositories import UserSettingRepository\n+from lute.backup.service import Service\n \n \n bp = Blueprint(\"backup\", __name__, url_prefix=\"/backup\")\n \n \n+def _get_settings():\n+    \"Get backup settings.\"\n+    repo = UserSettingRepository(db.session)\n+    return repo.get_backup_settings()\n+\n+\n @bp.route(\"/index\")\n def index():\n     \"\"\"\n     List all backups.\n     \"\"\"\n-    settings = BackupSettings.get_backup_settings()\n-    backups = list_backups(settings.backup_dir)\n+    settings = _get_settings()\n+    service = Service(db.session)\n+    backups = service.list_backups(settings.backup_dir)\n     backups.sort(reverse=True)\n \n     return render_template(\n@@ -40,7 +48,7 @@ def index():\n @bp.route(\"/download/<filename>\")\n def download_backup(filename):\n     \"Download the given backup file.\"\n-    settings = BackupSettings.get_backup_settings()\n+    settings = _get_settings()\n     fullpath = os.path.join(settings.backup_dir, filename)\n     return send_file(fullpath, as_attachment=True)\n \n@@ -56,7 +64,7 @@ def backup():\n     if \"type\" in request.args:\n         backuptype = \"manual\"\n \n-    settings = BackupSettings.get_backup_settings()\n+    settings = _get_settings()\n     return render_template(\n         \"backup/backup.html\", backup_folder=settings.backup_dir, backuptype=backuptype\n     )\n@@ -73,10 +81,11 @@ def do_backup():\n         backuptype = prms[\"type\"]\n \n     c = current_app.env_config\n-    settings = BackupSettings.get_backup_settings()\n+    settings = _get_settings()\n+    service = Service(db.session)\n     is_manual = backuptype.lower() == \"manual\"\n     try:\n-        f = create_backup(c, settings, is_manual=is_manual)\n+        f = service.create_backup(c, settings, is_manual=is_manual)\n         flash(f\"Backup created: {f}\", \"notice\")\n         return jsonify(f)\n     except Exception as e:  # pylint: disable=broad-exception-caught\n@@ -87,5 +96,6 @@ def do_backup():\n @bp.route(\"/skip_this_backup\", methods=[\"GET\"])\n def handle_skip_this_backup():\n     \"Update last backup date so backup not attempted again.\"\n-    skip_this_backup()\n+    service = Service(db.session)\n+    service.skip_this_backup()\n     return redirect(\"/\", 302)\ndiff --git a/lute/backup/service.py b/lute/backup/service.py\nindex 60cf95ce..a3461bb2 100644\n--- a/lute/backup/service.py\n+++ b/lute/backup/service.py\n@@ -10,8 +10,7 @@\n import time\n from typing import List, Union\n \n-from lute.db import db\n-from lute.models.setting import SystemSetting\n+from lute.models.repositories import UserSettingRepository\n from lute.models.book import Book\n from lute.models.term import Term\n \n@@ -70,124 +69,125 @@ def size(self) -> str:\n         return f\"{s} bytes\"\n \n \n-def create_backup(app_config, settings, is_manual=False, suffix=None):\n-    \"\"\"\n-    Create backup using current app config, settings.\n-\n-    is_manual is True if this is a user-triggered manual\n-    backup, otherwise is False.\n-\n-    suffix can be specified for test.\n+class Service:\n+    \"Service.\"\n \n-    settings are from Setting.get_backup_settings().\n-      - backup_enabled\n-      - backup_dir\n-      - backup_auto\n-      - backup_warn\n-      - backup_count\n-      - last_backup_datetime\n-    \"\"\"\n-    if not os.path.exists(settings.backup_dir):\n-        raise BackupException(\"Missing directory \" + settings.backup_dir)\n-\n-    ### Timing helper for when implement audio backup.\n-    # def _print_now(msg):\n-    #     now = datetime.now().strftime(\"%H-%M-%S\")\n-    #     print(f\"{now} - {msg}\", flush=True)\n+    def __init__(self, session):\n+        self.session = session\n \n-    _mirror_images_dir(app_config.userimagespath, settings.backup_dir)\n+    def create_backup(self, app_config, settings, is_manual=False, suffix=None):\n+        \"\"\"\n+        Create backup using current app config, settings.\n \n-    prefix = \"manual_\" if is_manual else \"\"\n-    if suffix is None:\n-        suffix = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n-    fname = f\"{prefix}lute_backup_{suffix}.db\"\n-    backupfile = os.path.join(settings.backup_dir, fname)\n+        is_manual is True if this is a user-triggered manual\n+        backup, otherwise is False.\n \n-    f = _create_db_backup(app_config.dbfilename, backupfile)\n-    _remove_excess_backups(settings.backup_count, settings.backup_dir)\n-    return f\n+        suffix can be specified for test.\n \n+        settings are from BackupSettings.\n+          - backup_enabled\n+          - backup_dir\n+          - backup_auto\n+          - backup_warn\n+          - backup_count\n+          - last_backup_datetime\n+        \"\"\"\n+        if not os.path.exists(settings.backup_dir):\n+            raise BackupException(\"Missing directory \" + settings.backup_dir)\n \n-def should_run_auto_backup(backup_settings):\n-    \"\"\"\n-    True (if applicable) if last backup was old.\n-    \"\"\"\n-    bs = backup_settings\n-    if bs.backup_enabled is False or bs.backup_auto is False:\n-        return False\n+        ### Timing helper for when implement audio backup.\n+        # def _print_now(msg):\n+        #     now = datetime.now().strftime(\"%H-%M-%S\")\n+        #     print(f\"{now} - {msg}\", flush=True)\n \n-    last = bs.last_backup_datetime\n-    if last is None:\n-        return True\n+        self._mirror_images_dir(app_config.userimagespath, settings.backup_dir)\n \n-    curr = int(time.time())\n-    diff = curr - last\n-    return diff > 24 * 60 * 60\n+        prefix = \"manual_\" if is_manual else \"\"\n+        if suffix is None:\n+            suffix = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n+        fname = f\"{prefix}lute_backup_{suffix}.db\"\n+        backupfile = os.path.join(settings.backup_dir, fname)\n \n+        f = self._create_db_backup(app_config.dbfilename, backupfile)\n+        self._remove_excess_backups(settings.backup_count, settings.backup_dir)\n+        return f\n \n-def backup_warning(backup_settings):\n-    \"Get warning if needed.\"\n-    if not backup_settings.backup_warn:\n-        return \"\"\n+    def should_run_auto_backup(self, backup_settings):\n+        \"\"\"\n+        True (if applicable) if last backup was old.\n+        \"\"\"\n+        bs = backup_settings\n+        if bs.backup_enabled is False or bs.backup_auto is False:\n+            return False\n+\n+        last = bs.last_backup_datetime\n+        if last is None:\n+            return True\n+\n+        curr = int(time.time())\n+        diff = curr - last\n+        return diff > 24 * 60 * 60\n+\n+    def backup_warning(self, backup_settings):\n+        \"Get warning if needed.\"\n+        if not backup_settings.backup_warn:\n+            return \"\"\n+\n+        have_books = self.session.query(self.session.query(Book).exists()).scalar()\n+        have_terms = self.session.query(self.session.query(Term).exists()).scalar()\n+        if have_books is False and have_terms is False:\n+            return \"\"\n+\n+        last = backup_settings.last_backup_datetime\n+        if last is None:\n+            return \"Never backed up.\"\n+\n+        curr = int(time.time())\n+        diff = curr - last\n+        old_backup_msg = \"Last backup was more than 1 week ago.\"\n+        if diff > 7 * 24 * 60 * 60:\n+            return old_backup_msg\n \n-    have_books = db.session.query(db.session.query(Book).exists()).scalar()\n-    have_terms = db.session.query(db.session.query(Term).exists()).scalar()\n-    if have_books is False and have_terms is False:\n         return \"\"\n \n-    last = backup_settings.last_backup_datetime\n-    if last is None:\n-        return \"Never backed up.\"\n-\n-    curr = int(time.time())\n-    diff = curr - last\n-    old_backup_msg = \"Last backup was more than 1 week ago.\"\n-    if diff > 7 * 24 * 60 * 60:\n-        return old_backup_msg\n-\n-    return \"\"\n-\n-\n-def _create_db_backup(dbfilename, backupfile):\n-    \"Make a backup.\"\n-    shutil.copy(dbfilename, backupfile)\n-    f = f\"{backupfile}.gz\"\n-    with open(backupfile, \"rb\") as in_file, gzip.open(\n-        f, \"wb\", compresslevel=4\n-    ) as out_file:\n-        shutil.copyfileobj(in_file, out_file)\n-    os.remove(backupfile)\n-    SystemSetting.set_last_backup_datetime(int(time.time()))\n-    return f\n-\n-\n-def skip_this_backup():\n-    \"Set the last backup time to today.\"\n-    SystemSetting.set_last_backup_datetime(int(time.time()))\n-\n-\n-def _remove_excess_backups(count, outdir):\n-    \"Remove old backups.\"\n-    files = [f for f in list_backups(outdir) if not f.is_manual]\n-    files.sort(reverse=True)\n-    to_remove = files[count:]\n-    for f in to_remove:\n-        os.remove(f.filepath)\n-\n-\n-def _mirror_images_dir(userimagespath, outdir):\n-    \"Copy the images to backup.\"\n-    target_dir = os.path.join(outdir, \"userimages_backup\")\n-    target_dir = os.path.abspath(target_dir)\n-    if not os.path.exists(target_dir):\n-        os.mkdir(target_dir)\n-    shutil.copytree(userimagespath, target_dir, dirs_exist_ok=True)\n-\n-\n-def list_backups(outdir) -> List[DatabaseBackupFile]:\n-    \"List all backup files.\"\n-    return [\n-        DatabaseBackupFile(os.path.join(outdir, f))\n-        for f in os.listdir(outdir)\n-        if re.match(r\"(manual_)?lute_backup_\", f)\n-    ]\n+    def _create_db_backup(self, dbfilename, backupfile):\n+        \"Make a backup.\"\n+        shutil.copy(dbfilename, backupfile)\n+        f = f\"{backupfile}.gz\"\n+        with open(backupfile, \"rb\") as in_file, gzip.open(\n+            f, \"wb\", compresslevel=4\n+        ) as out_file:\n+            shutil.copyfileobj(in_file, out_file)\n+        os.remove(backupfile)\n+        r = UserSettingRepository(self.session)\n+        r.set_last_backup_datetime(int(time.time()))\n+        return f\n+\n+    def skip_this_backup(self):\n+        \"Set the last backup time to today.\"\n+        r = UserSettingRepository(self.session)\n+        r.set_last_backup_datetime(int(time.time()))\n+\n+    def _remove_excess_backups(self, count, outdir):\n+        \"Remove old backups.\"\n+        files = [f for f in self.list_backups(outdir) if not f.is_manual]\n+        files.sort(reverse=True)\n+        to_remove = files[count:]\n+        for f in to_remove:\n+            os.remove(f.filepath)\n+\n+    def _mirror_images_dir(self, userimagespath, outdir):\n+        \"Copy the images to backup.\"\n+        target_dir = os.path.join(outdir, \"userimages_backup\")\n+        target_dir = os.path.abspath(target_dir)\n+        if not os.path.exists(target_dir):\n+            os.mkdir(target_dir)\n+        shutil.copytree(userimagespath, target_dir, dirs_exist_ok=True)\n+\n+    def list_backups(self, outdir) -> List[DatabaseBackupFile]:\n+        \"List all backup files.\"\n+        return [\n+            DatabaseBackupFile(os.path.join(outdir, f))\n+            for f in os.listdir(outdir)\n+            if re.match(r\"(manual_)?lute_backup_\", f)\n+        ]\ndiff --git a/lute/book/datatables.py b/lute/book/datatables.py\nindex a4b799f1..726ad1be 100644\n--- a/lute/book/datatables.py\n+++ b/lute/book/datatables.py\n@@ -2,11 +2,10 @@\n Show books in datatables.\n \"\"\"\n \n-from lute.db import db\n from lute.utils.data_tables import DataTablesSqliteQuery, supported_parser_type_criteria\n \n \n-def get_data_tables_list(parameters, is_archived):\n+def get_data_tables_list(parameters, is_archived, session):\n     \"Book json data for datatables.\"\n     archived = \"true\" if is_archived else \"false\"\n \n@@ -70,7 +69,5 @@ def get_data_tables_list(parameters, is_archived):\n     if language_id != 0:\n         base_sql += f\" and LgID = {language_id}\"\n \n-    session = db.session\n     connection = session.connection()\n-\n     return DataTablesSqliteQuery.get_data(base_sql, parameters, connection)\ndiff --git a/lute/book/forms.py b/lute/book/forms.py\nindex 39c32013..2ed147ef 100644\n--- a/lute/book/forms.py\n+++ b/lute/book/forms.py\n@@ -9,7 +9,7 @@\n from wtforms.validators import DataRequired, Length, NumberRange\n from flask_wtf import FlaskForm\n from flask_wtf.file import FileField, FileAllowed\n-from lute.book import service\n+from lute.book.service import Service\n \n \n class NewBookForm(FlaskForm):\n@@ -78,6 +78,7 @@ def _values(field_data):\n \n         obj.book_tags = _values(self.book_tags.data)\n \n+        service = Service()\n         if self.textfile.data:\n             obj.text = service.get_file_content(self.textfile.data)\n         f = self.audiofile.data\n@@ -149,6 +150,7 @@ def _values(field_data):\n         obj.book_tags = _values(self.book_tags.data)\n \n         f = self.audiofile.data\n+        service = Service()\n         if f:\n             obj.audio_filename = service.save_audio_file(f)\n             obj.audio_bookmarks = None\ndiff --git a/lute/book/model.py b/lute/book/model.py\nindex e80e4dcd..872a948e 100644\n--- a/lute/book/model.py\n+++ b/lute/book/model.py\n@@ -3,7 +3,11 @@\n \"\"\"\n \n from lute.models.book import Book as DBBook, BookTag\n-from lute.models.language import Language\n+from lute.models.repositories import (\n+    BookRepository,\n+    BookTagRepository,\n+    LanguageRepository,\n+)\n \n \n class Book:  # pylint: disable=too-many-instance-attributes\n@@ -40,26 +44,27 @@ class Repository:\n     Maps Book BO to and from lute.model.Book.\n     \"\"\"\n \n-    def __init__(self, _db):\n-        self.db = _db\n+    def __init__(self, _session):\n+        self.session = _session\n+        self.book_repo = BookRepository(self.session)\n \n     def load(self, book_id):\n         \"Loads a Book business object for the DBBook.\"\n-        dbb = DBBook.find(book_id)\n+        dbb = self.book_repo.find(book_id)\n         if dbb is None:\n             raise ValueError(f\"No book with id {book_id} found\")\n         return self._build_business_book(dbb)\n \n     def find_by_title(self, book_title, language_id):\n         \"Loads a Book business object for the book with a given title.\"\n-        dbb = DBBook.find_by_title(book_title, language_id)\n+        dbb = self.book_repo.find_by_title(book_title, language_id)\n         if dbb is None:\n             return None\n         return self._build_business_book(dbb)\n \n     def get_book_tags(self):\n         \"Get all available book tags, helper method.\"\n-        bts = self.db.session.query(BookTag).all()\n+        bts = self.session.query(BookTag).all()\n         return [t.text for t in bts]\n \n     def add(self, book):\n@@ -69,7 +74,7 @@ def add(self, book):\n         clients should not change it.\n         \"\"\"\n         dbbook = self._build_db_book(book)\n-        self.db.session.add(dbbook)\n+        self.session.add(dbbook)\n         return dbbook\n \n     def delete(self, book):\n@@ -78,38 +83,40 @@ def delete(self, book):\n         \"\"\"\n         if book.id is None:\n             raise ValueError(f\"book {book.title} not saved\")\n-        b = DBBook.find(book.id)\n-        self.db.session.delete(b)\n+        b = self.book_repo.find(book.id)\n+        self.session.delete(b)\n \n     def commit(self):\n         \"\"\"\n         Commit everything.\n         \"\"\"\n-        self.db.session.commit()\n+        self.session.commit()\n \n     def _build_db_book(self, book):\n         \"Convert a book business object to a DBBook.\"\n \n+        lang_repo = LanguageRepository(self.session)\n         lang = None\n         if book.language_id:\n-            lang = Language.find(book.language_id)\n+            lang = lang_repo.find(book.language_id)\n         elif book.language_name:\n-            lang = Language.find_by_name(book.language_name)\n+            lang = lang_repo.find_by_name(book.language_name)\n \n         b = None\n         if book.id is None:\n             b = DBBook.create_book(book.title, lang, book.text, book.max_page_tokens)\n         else:\n-            b = DBBook.find(book.id)\n+            b = self.book_repo.find(book.id)\n         b.title = book.title\n         b.source_uri = book.source_uri\n         b.audio_filename = book.audio_filename\n         b.audio_current_pos = book.audio_current_pos\n         b.audio_bookmarks = book.audio_bookmarks\n \n+        btr = BookTagRepository(self.session)\n         booktags = []\n         for s in book.book_tags:\n-            booktags.append(BookTag.find_or_create_by_text(s))\n+            booktags.append(btr.find_or_create_by_text(s))\n         b.remove_all_book_tags()\n         for tt in booktags:\n             b.add_book_tag(tt)\ndiff --git a/lute/book/routes.py b/lute/book/routes.py\nindex 8271bbfe..809a15a7 100644\n--- a/lute/book/routes.py\n+++ b/lute/book/routes.py\n@@ -12,16 +12,19 @@\n     flash,\n )\n from lute.utils.data_tables import DataTablesFlaskParamParser\n-from lute.book import service\n+from lute.book.service import Service, BookImportException\n from lute.book.datatables import get_data_tables_list\n from lute.book.forms import NewBookForm, EditBookForm\n-from lute.book.stats import get_stats\n+from lute.book.stats import Service as StatsService\n import lute.utils.formutils\n from lute.db import db\n \n from lute.models.language import Language\n-from lute.models.book import Book as DBBook\n-from lute.models.setting import UserSetting\n+from lute.models.repositories import (\n+    BookRepository,\n+    UserSettingRepository,\n+    LanguageRepository,\n+)\n from lute.book.model import Book, Repository\n \n \n@@ -46,7 +49,7 @@ def datatables_source(is_archived):\n     # (currently unused)\n     parameters = DataTablesFlaskParamParser.parse_params(request.form)\n     _load_term_custom_filters(request.form, parameters)\n-    data = get_data_tables_list(parameters, is_archived)\n+    data = get_data_tables_list(parameters, is_archived, db.session)\n     return jsonify(data)\n \n \n@@ -59,8 +62,10 @@ def datatables_active_source():\n @bp.route(\"/archived\", methods=[\"GET\"])\n def archived():\n     \"List archived books.\"\n-    language_choices = lute.utils.formutils.language_choices(\"(all languages)\")\n-    current_language_id = lute.utils.formutils.valid_current_language_id()\n+    language_choices = lute.utils.formutils.language_choices(\n+        db.session, \"(all languages)\"\n+    )\n+    current_language_id = lute.utils.formutils.valid_current_language_id(db.session)\n \n     return render_template(\n         \"book/index.html\",\n@@ -80,9 +85,10 @@ def datatables_archived_source():\n def _book_from_url(url):\n     \"Create a new book, or flash an error if can't parse.\"\n     b = Book()\n+    service = Service()\n     try:\n         b = service.book_from_url(url)\n-    except service.BookImportException as e:\n+    except BookImportException as e:\n         flash(e.message, \"notice\")\n         b = Book()\n     return b\n@@ -107,8 +113,8 @@ def new():\n         b = _book_from_url(import_url)\n \n     form = NewBookForm(obj=b)\n-    form.language_id.choices = lute.utils.formutils.language_choices()\n-    repo = Repository(db)\n+    form.language_id.choices = lute.utils.formutils.language_choices(db.session)\n+    repo = Repository(db.session)\n \n     if form.validate_on_submit():\n         try:\n@@ -116,11 +122,12 @@ def new():\n             book = repo.add(b)\n             repo.commit()\n             return redirect(f\"/read/{book.id}/page/1\", 302)\n-        except service.BookImportException as e:\n+        except BookImportException as e:\n             flash(e.message, \"notice\")\n \n     # Don't set the current language before submit.\n-    current_language_id = int(UserSetting.get_value(\"current_language_id\"))\n+    usrepo = UserSettingRepository(db.session)\n+    current_language_id = int(usrepo.get_value(\"current_language_id\"))\n     form.language_id.data = current_language_id\n \n     return render_template(\n@@ -136,7 +143,7 @@ def new():\n @bp.route(\"/edit/<int:bookid>\", methods=[\"GET\", \"POST\"])\n def edit(bookid):\n     \"Edit a book - can only change a few fields.\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     b = repo.load(bookid)\n     form = EditBookForm(obj=b)\n \n@@ -147,7 +154,8 @@ def edit(bookid):\n         flash(f\"{b.title} updated.\")\n         return redirect(\"/\", 302)\n \n-    lang = Language.find(b.language_id)\n+    lang_repo = LanguageRepository(db.session)\n+    lang = lang_repo.find(b.language_id)\n     return render_template(\n         \"book/edit.html\",\n         book=b,\n@@ -162,10 +170,16 @@ def import_webpage():\n     return render_template(\"book/import_webpage.html\")\n \n \n+def _find_book(bookid):\n+    \"Find book from db.\"\n+    br = BookRepository(db.session)\n+    return br.find(bookid)\n+\n+\n @bp.route(\"/archive/<int:bookid>\", methods=[\"POST\"])\n def archive(bookid):\n     \"Archive a book.\"\n-    b = DBBook.find(bookid)\n+    b = _find_book(bookid)\n     b.archived = True\n     db.session.add(b)\n     db.session.commit()\n@@ -175,7 +189,7 @@ def archive(bookid):\n @bp.route(\"/unarchive/<int:bookid>\", methods=[\"POST\"])\n def unarchive(bookid):\n     \"Archive a book.\"\n-    b = DBBook.find(bookid)\n+    b = _find_book(bookid)\n     b.archived = False\n     db.session.add(b)\n     db.session.commit()\n@@ -185,7 +199,7 @@ def unarchive(bookid):\n @bp.route(\"/delete/<int:bookid>\", methods=[\"POST\"])\n def delete(bookid):\n     \"Archive a book.\"\n-    b = DBBook.find(bookid)\n+    b = _find_book(bookid)\n     db.session.delete(b)\n     db.session.commit()\n     return redirect(\"/\", 302)\n@@ -194,8 +208,15 @@ def delete(bookid):\n @bp.route(\"/table_stats/<int:bookid>\", methods=[\"GET\"])\n def table_stats(bookid):\n     \"Get the stats, return ajax.\"\n-    b = DBBook.find(bookid)\n-    stats = get_stats(b)\n+    b = _find_book(bookid)\n+    if b is None or b.language is None:\n+        # Playwright tests were sometimes passing an id that didn't exist ...\n+        # I believe this is due to page caching, i.e. the book listing\n+        # is showing books and IDs that no longer exist after cache reset.\n+        # TODO fix_hack: get rid of this hack.\n+        return jsonify({})\n+    svc = StatsService(db.session)\n+    stats = svc.get_stats(b)\n     ret = {\n         \"distinctterms\": stats.distinctterms,\n         \"distinctunknowns\": stats.distinctunknowns,\ndiff --git a/lute/book/service.py b/lute/book/service.py\nindex e957d534..b1e37de4 100644\n--- a/lute/book/service.py\n+++ b/lute/book/service.py\n@@ -29,179 +29,174 @@ def __init__(self, message=\"A custom error occurred\", cause=None):\n         super().__init__(message)\n \n \n-def _secure_unique_fname(filename):\n-    \"\"\"\n-    Return secure name pre-pended with datetime string.\n-    \"\"\"\n-    current_datetime = datetime.now()\n-    formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n-    f = \"_\".join([formatted_datetime, secure_filename(filename)])\n-    return f\n+class Service:\n+    \"Service.\"\n \n+    def _secure_unique_fname(self, filename):\n+        \"\"\"\n+        Return secure name pre-pended with datetime string.\n+        \"\"\"\n+        current_datetime = datetime.now()\n+        formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n+        f = \"_\".join([formatted_datetime, secure_filename(filename)])\n+        return f\n \n-def save_audio_file(audio_file_field_data):\n-    \"\"\"\n-    Save the file to disk, return its filename.\n-    \"\"\"\n-    filename = _secure_unique_fname(audio_file_field_data.filename)\n-    fp = os.path.join(current_app.env_config.useraudiopath, filename)\n-    audio_file_field_data.save(fp)\n-    return filename\n+    def save_audio_file(self, audio_file_field_data):\n+        \"\"\"\n+        Save the file to disk, return its filename.\n+        \"\"\"\n+        filename = self._secure_unique_fname(audio_file_field_data.filename)\n+        fp = os.path.join(current_app.env_config.useraudiopath, filename)\n+        audio_file_field_data.save(fp)\n+        return filename\n \n+    def get_file_content(self, filefielddata):\n+        \"\"\"\n+        Get the content of the file.\n+        \"\"\"\n+        content = None\n+        _, ext = os.path.splitext(filefielddata.filename)\n+        ext = (ext or \"\").lower()\n+        if ext == \".txt\":\n+            content = self.get_textfile_content(filefielddata)\n+        if ext == \".epub\":\n+            content = self.get_epub_content(filefielddata)\n+        if ext == \".pdf\":\n+            msg = \"\"\"\n+            Note: pdf imports can be inaccurate, due to how PDFs are encoded.\n+            Please be aware of this while reading.\n+            \"\"\"\n+            flash(msg, \"notice\")\n+            content = self.get_pdf_content_from_form(filefielddata)\n+        if ext == \".srt\":\n+            content = self.get_srt_content(filefielddata)\n+        if ext == \".vtt\":\n+            content = self.get_vtt_content(filefielddata)\n+\n+        if content is None:\n+            raise ValueError(f'Unknown file extension \"{ext}\"')\n+        if content.strip() == \"\":\n+            raise BookImportException(f\"{filefielddata.filename} is empty.\")\n+        return content\n \n-def get_file_content(filefielddata):\n-    \"\"\"\n-    Get the content of the file.\n-    \"\"\"\n-    content = None\n-    _, ext = os.path.splitext(filefielddata.filename)\n-    ext = (ext or \"\").lower()\n-    if ext == \".txt\":\n-        content = get_textfile_content(filefielddata)\n-    if ext == \".epub\":\n-        content = get_epub_content(filefielddata)\n-    if ext == \".pdf\":\n-        msg = \"\"\"\n-        Note: pdf imports can be inaccurate, due to how PDFs are encoded.\n-        Please be aware of this while reading.\n+    def get_textfile_content(self, filefielddata):\n+        \"Get content as a single string.\"\n+        content = \"\"\n+        try:\n+            content = filefielddata.read()\n+            return str(content, \"utf-8\")\n+        except UnicodeDecodeError as e:\n+            f = filefielddata.filename\n+            msg = f\"{f} is not utf-8 encoding, please convert it to utf-8 first (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n+\n+    def get_epub_content(self, epub_file_field_data):\n         \"\"\"\n-        flash(msg, \"notice\")\n-        content = get_pdf_content_from_form(filefielddata)\n-    if ext == \".srt\":\n-        content = get_srt_content(filefielddata)\n-    if ext == \".vtt\":\n-        content = get_vtt_content(filefielddata)\n-\n-    if content is None:\n-        raise ValueError(f'Unknown file extension \"{ext}\"')\n-    if content.strip() == \"\":\n-        raise BookImportException(f\"{filefielddata.filename} is empty.\")\n-    return content\n-\n-\n-def get_textfile_content(filefielddata):\n-    \"Get content as a single string.\"\n-    content = \"\"\n-    try:\n-        content = filefielddata.read()\n-        return str(content, \"utf-8\")\n-    except UnicodeDecodeError as e:\n-        f = filefielddata.filename\n-        msg = f\"{f} is not utf-8 encoding, please convert it to utf-8 first (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n-\n-\n-def get_epub_content(epub_file_field_data):\n-    \"\"\"\n-    Get the content of the epub as a single string.\n-    \"\"\"\n-    content = \"\"\n-    try:\n-        if hasattr(epub_file_field_data.stream, \"seekable\"):\n-            epub = Epub(stream=epub_file_field_data.stream)\n-            content = epub.get_text()\n-        else:\n-            # We get a SpooledTemporaryFile from the form but this doesn't\n-            # implement all file-like methods until python 3.11. So we need\n-            # to rewrite it into a TemporaryFile\n-            with TemporaryFile() as tf:\n-                epub_file_field_data.stream.seek(0)\n-                tf.write(epub_file_field_data.stream.read())\n-                epub = Epub(stream=tf)\n+        Get the content of the epub as a single string.\n+        \"\"\"\n+        content = \"\"\n+        try:\n+            if hasattr(epub_file_field_data.stream, \"seekable\"):\n+                epub = Epub(stream=epub_file_field_data.stream)\n                 content = epub.get_text()\n-    except EpubError as e:\n-        msg = f\"Could not parse {epub_file_field_data.filename} (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n-    return content\n-\n-\n-def get_pdf_content_from_form(pdf_file_field_data):\n-    \"Get content as a single string from a PDF file using PyPDF2.\"\n-    content = \"\"\n-    try:\n-        pdf_reader = PdfReader(pdf_file_field_data)\n-\n-        for page in pdf_reader.pages:\n-            content += page.extract_text()\n-\n+            else:\n+                # We get a SpooledTemporaryFile from the form but this doesn't\n+                # implement all file-like methods until python 3.11. So we need\n+                # to rewrite it into a TemporaryFile\n+                with TemporaryFile() as tf:\n+                    epub_file_field_data.stream.seek(0)\n+                    tf.write(epub_file_field_data.stream.read())\n+                    epub = Epub(stream=tf)\n+                    content = epub.get_text()\n+        except EpubError as e:\n+            msg = f\"Could not parse {epub_file_field_data.filename} (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n         return content\n-    except Exception as e:\n-        msg = f\"Could not parse {pdf_file_field_data.filename} (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n-\n-\n-def get_srt_content(srt_file_field_data):\n-    \"\"\"\n-    Get the content of the srt as a single string.\n-    \"\"\"\n-    content = \"\"\n-    try:\n-        srt_content = srt_file_field_data.read().decode(\"utf-8-sig\")\n-\n-        parser = SrtParser(StringIO(srt_content))\n-        parser.parse()\n \n-        content = \"\\n\".join(subtitle.text for subtitle in parser.subtitles)\n+    def get_pdf_content_from_form(self, pdf_file_field_data):\n+        \"Get content as a single string from a PDF file using PyPDF2.\"\n+        content = \"\"\n+        try:\n+            pdf_reader = PdfReader(pdf_file_field_data)\n \n-        return content\n-    except Exception as e:\n-        msg = f\"Could not parse {srt_file_field_data.filename} (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n+            for page in pdf_reader.pages:\n+                content += page.extract_text()\n \n+            return content\n+        except Exception as e:\n+            msg = f\"Could not parse {pdf_file_field_data.filename} (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n \n-def get_vtt_content(vtt_file_field_data):\n-    \"\"\"\n-    Get the content of the vtt as a single string.\n-    \"\"\"\n-    content = \"\"\n-    try:\n-        vtt_content = vtt_file_field_data.read().decode(\"utf-8-sig\")\n+    def get_srt_content(self, srt_file_field_data):\n+        \"\"\"\n+        Get the content of the srt as a single string.\n+        \"\"\"\n+        content = \"\"\n+        try:\n+            srt_content = srt_file_field_data.read().decode(\"utf-8-sig\")\n \n-        # Check if it is from YouTube\n-        lines = vtt_content.split(\"\\n\")\n-        if lines[1].startswith(\"Kind:\") and lines[2].startswith(\"Language:\"):\n-            vtt_content = \"\\n\".join(lines[:1] + lines[3:])\n+            parser = SrtParser(StringIO(srt_content))\n+            parser.parse()\n \n-        parser = WebVttParser(StringIO(vtt_content))\n-        parser.parse()\n+            content = \"\\n\".join(subtitle.text for subtitle in parser.subtitles)\n \n-        content = \"\\n\".join(subtitle.text for subtitle in parser.subtitles)\n+            return content\n+        except Exception as e:\n+            msg = f\"Could not parse {srt_file_field_data.filename} (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n \n-        return content\n-    except Exception as e:\n-        msg = f\"Could not parse {vtt_file_field_data.filename} (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n-\n-\n-def book_from_url(url):\n-    \"Parse the url and load a new Book.\"\n-    s = None\n-    try:\n-        timeout = 20  # seconds\n-        response = requests.get(url, timeout=timeout)\n-        response.raise_for_status()\n-        s = response.text\n-    except requests.exceptions.RequestException as e:\n-        msg = f\"Could not parse {url} (error: {str(e)})\"\n-        raise BookImportException(message=msg, cause=e) from e\n-\n-    soup = BeautifulSoup(s, \"html.parser\")\n-    extracted_text = []\n-\n-    # Add elements in order found.\n-    for element in soup.descendants:\n-        if element.name in (\"h1\", \"h2\", \"h3\", \"h4\", \"p\"):\n-            extracted_text.append(element.text)\n-\n-    title_node = soup.find(\"title\")\n-    orig_title = title_node.string if title_node else url\n-\n-    short_title = orig_title[:150]\n-    if len(orig_title) > 150:\n-        short_title += \" ...\"\n-\n-    b = Book()\n-    b.title = short_title\n-    b.source_uri = url\n-    b.text = \"\\n\\n\".join(extracted_text)\n-    return b\n+    def get_vtt_content(self, vtt_file_field_data):\n+        \"\"\"\n+        Get the content of the vtt as a single string.\n+        \"\"\"\n+        content = \"\"\n+        try:\n+            vtt_content = vtt_file_field_data.read().decode(\"utf-8-sig\")\n+\n+            # Check if it is from YouTube\n+            lines = vtt_content.split(\"\\n\")\n+            if lines[1].startswith(\"Kind:\") and lines[2].startswith(\"Language:\"):\n+                vtt_content = \"\\n\".join(lines[:1] + lines[3:])\n+\n+            parser = WebVttParser(StringIO(vtt_content))\n+            parser.parse()\n+\n+            content = \"\\n\".join(subtitle.text for subtitle in parser.subtitles)\n+\n+            return content\n+        except Exception as e:\n+            msg = f\"Could not parse {vtt_file_field_data.filename} (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n+\n+    def book_from_url(self, url):\n+        \"Parse the url and load a new Book.\"\n+        s = None\n+        try:\n+            timeout = 20  # seconds\n+            response = requests.get(url, timeout=timeout)\n+            response.raise_for_status()\n+            s = response.text\n+        except requests.exceptions.RequestException as e:\n+            msg = f\"Could not parse {url} (error: {str(e)})\"\n+            raise BookImportException(message=msg, cause=e) from e\n+\n+        soup = BeautifulSoup(s, \"html.parser\")\n+        extracted_text = []\n+\n+        # Add elements in order found.\n+        for element in soup.descendants:\n+            if element.name in (\"h1\", \"h2\", \"h3\", \"h4\", \"p\"):\n+                extracted_text.append(element.text)\n+\n+        title_node = soup.find(\"title\")\n+        orig_title = title_node.string if title_node else url\n+\n+        short_title = orig_title[:150]\n+        if len(orig_title) > 150:\n+            short_title += \" ...\"\n+\n+        b = Book()\n+        b.title = short_title\n+        b.source_uri = url\n+        b.text = \"\\n\\n\".join(extracted_text)\n+        return b\ndiff --git a/lute/book/stats.py b/lute/book/stats.py\nindex f1a69c67..94702ad5 100644\n--- a/lute/book/stats.py\n+++ b/lute/book/stats.py\n@@ -4,145 +4,138 @@\n \n import json\n from sqlalchemy import select, text\n-from lute.read.render.service import get_multiword_indexer, get_textitems\n-from lute.db import db\n-from lute.models.book import Book\n-from lute.models.setting import UserSetting\n+from lute.read.render.service import Service as RenderService\n+from lute.models.book import Book, BookStats\n+from lute.models.repositories import UserSettingRepository\n \n # from lute.utils.debug_helpers import DebugTimer\n \n \n-def _last_n_pages(book, txindex, n):\n-    \"Get next n pages, or at least n pages.\"\n-    start_index = max(0, txindex - n)\n-    end_index = txindex + n\n-    texts = book.texts[start_index:end_index]\n-    return texts[-n:]\n-\n-\n-def calc_status_distribution(book):\n-    \"\"\"\n-    Calculate statuses and count of unique words per status.\n-\n-    Does a full render of a small number of pages\n-    to calculate the distribution.\n-    \"\"\"\n-\n-    # DebugTimer.clear_total_summary()\n-    # dt = DebugTimer(\"get_status_distribution\", display=False)\n-\n-    txindex = 0\n-    if (book.current_tx_id or 0) != 0:\n-        for t in book.texts:\n-            if t.id == book.current_tx_id:\n-                break\n-            txindex += 1\n-\n-    sample_size = int(UserSetting.get_value(\"stats_calc_sample_size\") or 5)\n-    texts = _last_n_pages(book, txindex, sample_size)\n-\n-    # Getting the individual paragraphs per page, and then combining,\n-    # is much faster than combining all pages into one giant page.\n-    mw = get_multiword_indexer(book.language)\n-    textitems = []\n-    for tx in texts:\n-        textitems.extend(get_textitems(tx.text, book.language, mw))\n-    # # Old slower code:\n-    # text_sample = \"\\n\".join([t.text for t in texts])\n-    # paras = get_paragraphs(text_sample, book.language) ... etc.\n-    # dt.step(\"get_paragraphs\")\n-\n-    textitems = [ti for ti in textitems if ti.is_word]\n-    statterms = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 98: [], 99: []}\n-    for ti in textitems:\n-        statterms[ti.wo_status or 0].append(ti.text_lc)\n-\n-    stats = {}\n-    for statusval, allterms in statterms.items():\n-        uniques = list(set(allterms))\n-        statterms[statusval] = uniques\n-        stats[statusval] = len(uniques)\n-\n-    # dt.step(\"compiled\")\n-    # DebugTimer.total_summary()\n-\n-    return stats\n-\n-\n-##################################################\n-# Stats table refresh.\n-\n-\n-class BookStats(db.Model):\n-    \"The stats table.\"\n-    __tablename__ = \"bookstats\"\n-\n-    BkID = db.Column(db.Integer, primary_key=True)\n-    distinctterms = db.Column(db.Integer)\n-    distinctunknowns = db.Column(db.Integer)\n-    unknownpercent = db.Column(db.Integer)\n-    status_distribution = db.Column(db.String, nullable=True)\n-\n-\n-def refresh_stats():\n-    \"Refresh stats for all books requiring update.\"\n-    sql = \"delete from bookstats where status_distribution is null\"\n-    db.session.execute(text(sql))\n-    db.session.commit()\n-    book_ids_with_stats = select(BookStats.BkID).scalar_subquery()\n-    books_to_update = (\n-        db.session.query(Book).filter(~Book.id.in_(book_ids_with_stats)).all()\n-    )\n-    books = [b for b in books_to_update if b.is_supported]\n-    for book in books:\n-        stats = _calculate_stats(book)\n-        _update_stats(book, stats)\n-\n-\n-def mark_stale(book):\n-    \"Mark a book's stats as stale to force refresh.\"\n-    bk_id = book.id\n-    db.session.query(BookStats).filter_by(BkID=bk_id).delete()\n-    db.session.commit()\n-\n-\n-def get_stats(book):\n-    \"Gets stats from the cache if available, or calculates.\"\n-    bk_id = book.id\n-    stats = db.session.query(BookStats).filter_by(BkID=bk_id).first()\n-    if stats is None or stats.status_distribution is None:\n-        newstats = _calculate_stats(book)\n-        _update_stats(book, newstats)\n-        stats = db.session.query(BookStats).filter_by(BkID=bk_id).first()\n-    return stats\n-\n-\n-def _calculate_stats(book):\n-    \"Calc stats for the book using the status distribution.\"\n-    status_distribution = calc_status_distribution(book)\n-    unknowns = status_distribution[0]\n-    allunique = sum(status_distribution.values())\n-\n-    percent = 0\n-    if allunique > 0:  # In case not parsed.\n-        percent = round(100.0 * unknowns / allunique)\n-\n-    return {\n-        \"allunique\": allunique,\n-        \"unknowns\": unknowns,\n-        \"percent\": percent,\n-        \"distribution\": json.dumps(status_distribution),\n-    }\n-\n-\n-def _update_stats(book, stats):\n-    \"Update BookStats for the given book.\"\n-    s = db.session.query(BookStats).filter_by(BkID=book.id).first()\n-    if s is None:\n-        s = BookStats(BkID=book.id)\n-    s.distinctterms = stats[\"allunique\"]\n-    s.distinctunknowns = stats[\"unknowns\"]\n-    s.unknownpercent = stats[\"percent\"]\n-    s.status_distribution = stats[\"distribution\"]\n-    db.session.add(s)\n-    db.session.commit()\n+class Service:\n+    \"Service.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def _last_n_pages(self, book, txindex, n):\n+        \"Get next n pages, or at least n pages.\"\n+        start_index = max(0, txindex - n)\n+        end_index = txindex + n\n+        texts = book.texts[start_index:end_index]\n+        return texts[-n:]\n+\n+    def _get_sample_texts(self, book):\n+        \"Get texts to use as sample.\"\n+        txindex = 0\n+        if (book.current_tx_id or 0) != 0:\n+            for t in book.texts:\n+                if t.id == book.current_tx_id:\n+                    break\n+                txindex += 1\n+\n+        repo = UserSettingRepository(self.session)\n+        sample_size = int(repo.get_value(\"stats_calc_sample_size\") or 5)\n+        texts = self._last_n_pages(book, txindex, sample_size)\n+        return texts\n+\n+    def calc_status_distribution(self, book):\n+        \"\"\"\n+        Calculate statuses and count of unique words per status.\n+\n+        Does a full render of a small number of pages\n+        to calculate the distribution.\n+        \"\"\"\n+\n+        # DebugTimer.clear_total_summary()\n+        # dt = DebugTimer(\"get_status_distribution\", display=False)\n+        texts = self._get_sample_texts(book)\n+\n+        # Getting the individual paragraphs per page, and then combining,\n+        # is much faster than combining all pages into one giant page.\n+        service = RenderService(self.session)\n+        mw = service.get_multiword_indexer(book.language)\n+        textitems = []\n+        for tx in texts:\n+            textitems.extend(service.get_textitems(tx.text, book.language, mw))\n+        # # Old slower code:\n+        # text_sample = \"\\n\".join([t.text for t in texts])\n+        # paras = get_paragraphs(text_sample, book.language) ... etc.\n+        # dt.step(\"get_paragraphs\")\n+\n+        textitems = [ti for ti in textitems if ti.is_word]\n+        statterms = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 98: [], 99: []}\n+        for ti in textitems:\n+            statterms[ti.wo_status or 0].append(ti.text_lc)\n+\n+        stats = {}\n+        for statusval, allterms in statterms.items():\n+            uniques = list(set(allterms))\n+            statterms[statusval] = uniques\n+            stats[statusval] = len(uniques)\n+\n+        # dt.step(\"compiled\")\n+        # DebugTimer.total_summary()\n+\n+        return stats\n+\n+    ##################################################\n+    # Stats table refresh.\n+\n+    def refresh_stats(self):\n+        \"Refresh stats for all books requiring update.\"\n+        sql = \"delete from bookstats where status_distribution is null\"\n+        self.session.execute(text(sql))\n+        self.session.commit()\n+        book_ids_with_stats = select(BookStats.BkID).scalar_subquery()\n+        books_to_update = (\n+            self.session.query(Book).filter(~Book.id.in_(book_ids_with_stats)).all()\n+        )\n+        books = [b for b in books_to_update if b.is_supported]\n+        for book in books:\n+            stats = self._calculate_stats(book)\n+            self._update_stats(book, stats)\n+\n+    def mark_stale(self, book):\n+        \"Mark a book's stats as stale to force refresh.\"\n+        bk_id = book.id\n+        self.session.query(BookStats).filter_by(BkID=bk_id).delete()\n+        self.session.commit()\n+\n+    def get_stats(self, book):\n+        \"Gets stats from the cache if available, or calculates.\"\n+        bk_id = book.id\n+        stats = self.session.query(BookStats).filter_by(BkID=bk_id).first()\n+        if stats is None or stats.status_distribution is None:\n+            newstats = self._calculate_stats(book)\n+            self._update_stats(book, newstats)\n+            stats = self.session.query(BookStats).filter_by(BkID=bk_id).first()\n+        return stats\n+\n+    def _calculate_stats(self, book):\n+        \"Calc stats for the book using the status distribution.\"\n+        status_distribution = self.calc_status_distribution(book)\n+        unknowns = status_distribution[0]\n+        allunique = sum(status_distribution.values())\n+\n+        percent = 0\n+        if allunique > 0:  # In case not parsed.\n+            percent = round(100.0 * unknowns / allunique)\n+\n+        return {\n+            \"allunique\": allunique,\n+            \"unknowns\": unknowns,\n+            \"percent\": percent,\n+            \"distribution\": json.dumps(status_distribution),\n+        }\n+\n+    def _update_stats(self, book, stats):\n+        \"Update BookStats for the given book.\"\n+        s = self.session.query(BookStats).filter_by(BkID=book.id).first()\n+        if s is None:\n+            s = BookStats(BkID=book.id)\n+        s.distinctterms = stats[\"allunique\"]\n+        s.distinctunknowns = stats[\"unknowns\"]\n+        s.unknownpercent = stats[\"percent\"]\n+        s.status_distribution = stats[\"distribution\"]\n+        self.session.add(s)\n+        self.session.commit()\ndiff --git a/lute/bookmarks/datatables.py b/lute/bookmarks/datatables.py\nindex f70c8fcc..6dcdeac2 100644\n--- a/lute/bookmarks/datatables.py\n+++ b/lute/bookmarks/datatables.py\n@@ -2,11 +2,10 @@\n Show bookmarks in datatables.\n \"\"\"\n \n-from lute.db import db\n from lute.utils.data_tables import DataTablesSqliteQuery\n \n \n-def get_data_tables_list(parameters, book_id):\n+def get_data_tables_list(parameters, book_id, session):\n     \"Bookmark json data for datatables.\"\n \n     base_sql = f\"\"\"\n@@ -16,7 +15,5 @@ def get_data_tables_list(parameters, book_id):\n       WHERE tx.TxBkID = { book_id }\n     \"\"\"\n \n-    session = db.session\n     connection = session.connection()\n-\n     return DataTablesSqliteQuery.get_data(base_sql, parameters, connection)\ndiff --git a/lute/bookmarks/routes.py b/lute/bookmarks/routes.py\nindex bb1ed9b9..e9b9b00f 100644\n--- a/lute/bookmarks/routes.py\n+++ b/lute/bookmarks/routes.py\n@@ -4,7 +4,8 @@\n \n from flask import Blueprint, request, render_template, jsonify\n from lute.bookmarks.datatables import get_data_tables_list\n-from lute.models.book import Book, Text, TextBookmark\n+from lute.models.book import Text, TextBookmark\n+from lute.models.repositories import BookRepository\n from lute.utils.data_tables import DataTablesFlaskParamParser\n from lute.db import db\n \n@@ -15,14 +16,15 @@\n def datatables_bookmarks(bookid):\n     \"Get datatables json for bookmarks.\"\n     parameters = DataTablesFlaskParamParser.parse_params(request.form)\n-    data = get_data_tables_list(parameters, bookid)\n+    data = get_data_tables_list(parameters, bookid, db.session)\n     return jsonify(data)\n \n \n @bp.route(\"/<int:bookid>\", methods=[\"GET\"])\n def bookmarks(bookid):\n     \"Get all bookarks for given bookid.\"\n-    book = Book.find(bookid)\n+    br = BookRepository(db.session)\n+    book = br.find(bookid)\n \n     text_dir = \"rtl\" if book.language.right_to_left else \"ltr\"\n     return render_template(\"bookmarks/list.html\", book=book, text_dir=text_dir)\ndiff --git a/lute/cli/import_books.py b/lute/cli/import_books.py\nindex a044f381..bd582c66 100644\n--- a/lute/cli/import_books.py\n+++ b/lute/cli/import_books.py\n@@ -8,7 +8,7 @@\n \n from lute.book.model import Book, Repository\n from lute.db import db\n-from lute.models.language import Language\n+from lute.models.repositories import LanguageRepository\n \n \n def import_books_from_csv(file, language, tags, commit):\n@@ -26,7 +26,9 @@ def import_books_from_csv(file, language, tags, commit):\n                 database. If false, a list of books to be imported will be\n                 printed out, but no changes will be made.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n+    lang_repo = LanguageRepository(db.session)\n+\n     count = 0\n     with open(file, newline=\"\", encoding=\"utf-8\") as f:\n         r = csv.DictReader(f)\n@@ -37,7 +39,7 @@ def import_books_from_csv(file, language, tags, commit):\n             if not book.language_name:\n                 print(f\"Skipping book with unspecified language: {book.title}\")\n                 continue\n-            lang = Language.find_by_name(book.language_name)\n+            lang = lang_repo.find_by_name(book.language_name)\n             if not lang:\n                 print(\n                     f\"Skipping book with unknown language ({book.language_name}): {book.title}\"\ndiff --git a/lute/cli/language_term_export.py b/lute/cli/language_term_export.py\nindex 2a5aad2b..47fde2f3 100644\n--- a/lute/cli/language_term_export.py\n+++ b/lute/cli/language_term_export.py\n@@ -15,7 +15,7 @@\n import csv\n from lute.db import db\n from lute.models.book import Book\n-from lute.read.render.service import get_textitems, get_multiword_indexer\n+from lute.read.render.service import Service\n \n \n def _add_term_to_dict(t, terms):\n@@ -54,11 +54,12 @@ def _process_book(b, terms, multiword_indexer):\n     \"Process pages in book, add to output.\"\n     print(f\"Processing {b.title} ...\")\n     i = 0\n+    service = Service(db.session)\n     for text in b.texts:\n         i += 1\n         if i % 10 == 0:\n             print(f\"  page {i} of {b.page_count}\", end=\"\\r\")\n-        textitems = get_textitems(text.text, b.language, multiword_indexer)\n+        textitems = service.get_textitems(text.text, b.language, multiword_indexer)\n         displayed_terms = [\n             ti.term for ti in textitems if ti.is_word and ti.term is not None\n         ]\n@@ -111,10 +112,11 @@ def _finalize_output(terms):\n \n def _load_indexers(books):\n     \"Load multiword indexers for book languages.\"\n+    service = Service(db.session)\n     ret = {}\n     lang_map = {book.language.id: book.language for book in books}\n     for langid, lang in lang_map.items():\n-        ret[langid] = get_multiword_indexer(lang)\n+        ret[langid] = service.get_multiword_indexer(lang)\n     return ret\n \n \ndiff --git a/lute/db/data_cleanup.py b/lute/db/data_cleanup.py\nindex 2c7aef95..91f1c169 100644\n--- a/lute/db/data_cleanup.py\n+++ b/lute/db/data_cleanup.py\n@@ -5,11 +5,10 @@\n These cleanup routines will be called by the app_factory.\n \"\"\"\n \n-from lute.db import db\n from lute.models.book import Text\n \n \n-def _set_texts_word_count():\n+def _set_texts_word_count(session):\n     \"\"\"\n     texts.TxWordCount should be set for all texts.\n \n@@ -18,7 +17,7 @@ def _set_texts_word_count():\n \n     Ref https://github.com/jzohrab/lute-v3/issues/95\n     \"\"\"\n-    calc_counts = db.session.query(Text).filter(Text.word_count.is_(None)).all()\n+    calc_counts = session.query(Text).filter(Text.word_count.is_(None)).all()\n \n     # Don't recalc with invalid parsers!!!!\n     recalc = [t for t in calc_counts if t.book.language.is_supported]\n@@ -31,10 +30,10 @@ def _set_texts_word_count():\n         pt = t.book.language.get_parsed_tokens(t.text)\n         words = [w for w in pt if w.is_word]\n         t.word_count = len(words)\n-        db.session.add(t)\n-    db.session.commit()\n+        session.add(t)\n+    session.commit()\n \n \n-def clean_data():\n+def clean_data(session):\n     \"Clean all data as required.\"\n-    _set_texts_word_count()\n+    _set_texts_word_count(session)\ndiff --git a/lute/db/demo.py b/lute/db/demo.py\nindex 01b706c9..98f7e570 100644\n--- a/lute/db/demo.py\n+++ b/lute/db/demo.py\n@@ -9,11 +9,10 @@\n \"\"\"\n \n from sqlalchemy import text\n-import lute.language.service\n+from lute.language.service import Service\n from lute.book.model import Repository\n-from lute.book.stats import refresh_stats\n-from lute.models.setting import SystemSetting\n-from lute.db import db\n+from lute.book.stats import Service as StatsService\n+from lute.models.repositories import SystemSettingRepository\n import lute.db.management\n \n \n@@ -39,97 +38,100 @@ def _demo_languages():\n     ]\n \n \n-def contains_demo_data():\n+def contains_demo_data(session):\n     \"\"\"\n     True if IsDemoData setting is present.\n     \"\"\"\n-    ss = SystemSetting.get_value(\"IsDemoData\")\n+    repo = SystemSettingRepository(session)\n+    ss = repo.get_value(\"IsDemoData\")\n     if ss is None:\n         return False\n     return True\n \n \n-def remove_flag():\n+def remove_flag(session):\n     \"\"\"\n     Remove IsDemoData setting.\n     \"\"\"\n-    if not contains_demo_data():\n+    if not contains_demo_data(session):\n         raise RuntimeError(\"Can't delete non-demo data.\")\n \n-    SystemSetting.delete_key(\"IsDemoData\")\n-    db.session.commit()\n+    repo = SystemSettingRepository(session)\n+    repo.delete_key(\"IsDemoData\")\n+    session.commit()\n \n \n-def tutorial_book_id():\n+def tutorial_book_id(session):\n     \"\"\"\n     Return the book id of the tutorial.\n     \"\"\"\n-    if not contains_demo_data():\n+    if not contains_demo_data(session):\n         return None\n     sql = \"\"\"select BkID from books\n     inner join languages on LgID = BkLgID\n     where LgName = 'English' and BkTitle = 'Tutorial'\n     \"\"\"\n-    r = db.session.execute(text(sql)).first()\n+    r = session.execute(text(sql)).first()\n     if r is None:\n         return None\n     return int(r[0])\n \n \n-def delete_demo_data():\n+def delete_demo_data(session):\n     \"\"\"\n     If this is a demo, wipe everything.\n     \"\"\"\n-    if not contains_demo_data():\n+    if not contains_demo_data(session):\n         raise RuntimeError(\"Can't delete non-demo data.\")\n-    remove_flag()\n-    lute.db.management.delete_all_data()\n+    remove_flag(session)\n+    lute.db.management.delete_all_data(session)\n \n \n # Loading demo data.\n \n \n-def load_demo_languages():\n+def load_demo_languages(session):\n     \"\"\"\n     Load selected predefined languages.  Assume everything is supported.\n \n     This method will also be called during acceptance tests, so it's public.\n     \"\"\"\n     demo_langs = _demo_languages()\n-    langs = [\n-        lute.language.service.get_language_def(langname)[\"language\"]\n-        for langname in demo_langs\n-    ]\n+    service = Service(session)\n+    langs = [service.get_language_def(langname)[\"language\"] for langname in demo_langs]\n     supported = [lang for lang in langs if lang.is_supported]\n     for lang in supported:\n-        db.session.add(lang)\n-    db.session.commit()\n+        session.add(lang)\n+    session.commit()\n \n \n-def load_demo_stories():\n+def load_demo_stories(session):\n     \"Load the stories.\"\n     demo_langs = _demo_languages()\n-    langdefs = [\n-        lute.language.service.get_language_def(langname) for langname in demo_langs\n-    ]\n+    service = Service(session)\n+    langdefs = [service.get_language_def(langname) for langname in demo_langs]\n     langdefs = [d for d in langdefs if d[\"language\"].is_supported]\n \n-    r = Repository(db)\n+    r = Repository(session)\n     for d in langdefs:\n         for b in d[\"books\"]:\n             r.add(b)\n     r.commit()\n \n-    SystemSetting.set_value(\"IsDemoData\", True)\n-    db.session.commit()\n-    refresh_stats()\n+    repo = SystemSettingRepository(session)\n+    repo.set_value(\"IsDemoData\", True)\n+    session.commit()\n+\n+    svc = StatsService(session)\n+    svc.refresh_stats()\n \n \n-def load_demo_data():\n+def load_demo_data(session):\n     \"\"\"\n     Load the data.\n     \"\"\"\n-    load_demo_languages()\n-    load_demo_stories()\n-    SystemSetting.set_value(\"IsDemoData\", True)\n-    db.session.commit()\n+    load_demo_languages(session)\n+    load_demo_stories(session)\n+    repo = SystemSettingRepository(session)\n+    repo.set_value(\"IsDemoData\", True)\n+    session.commit()\ndiff --git a/lute/db/management.py b/lute/db/management.py\nindex 81bd3ccf..bf03eb5b 100644\n--- a/lute/db/management.py\n+++ b/lute/db/management.py\n@@ -2,12 +2,14 @@\n Db management.\n \"\"\"\n \n+import os\n from sqlalchemy import text\n-from lute.db import db\n+from flask import current_app\n from lute.models.setting import UserSetting\n+from lute.models.repositories import UserSettingRepository\n \n \n-def delete_all_data():\n+def delete_all_data(session):\n     \"\"\"\n     DANGEROUS!  Delete everything, restore user settings, clear sys settings.\n \n@@ -23,6 +25,119 @@ def delete_all_data():\n         \"delete from settings\",\n     ]\n     for s in statements:\n-        db.session.execute(text(s))\n-    db.session.commit()\n-    UserSetting.load()\n+        session.execute(text(s))\n+    session.commit()\n+    add_default_user_settings(session, current_app.env_config.default_user_backup_path)\n+\n+\n+def _revised_mecab_path(repo):\n+    \"\"\"\n+    Change the mecab_path if it's not found, and a\n+    replacement is found.\n+\n+    Lute Docker images are built to be multi-arch, and\n+    interestingly (annoyingly), mecab libraries are installed into\n+    different locations depending on the architecture, even with\n+    the same Dockerfile and base image.\n+\n+    Returns: new mecab path if old one is missing _and_\n+    new one found, otherwise just return the old one.\n+    \"\"\"\n+\n+    mp = repo.get_value(\"mecab_path\")\n+    if mp is not None and os.path.exists(mp):\n+        return mp\n+\n+    # See develop docs for notes on how to find the libmecab path!\n+    candidates = [\n+        # linux/arm64\n+        \"/lib/aarch64-linux-gnu/libmecab.so.2\",\n+        # linux/amd64\n+        \"/lib/x86_64-linux-gnu/libmecab.so.2\",\n+        # github CI, ubuntu-latest\n+        \"/lib/x86_64-linux-gnu/libmecab.so.2\",\n+    ]\n+    replacements = [p for p in candidates if os.path.exists(p)]\n+    if len(replacements) > 0:\n+        return replacements[0]\n+    # Replacement not found, leave current value as-is.\n+    return mp\n+\n+\n+def add_default_user_settings(session, default_user_backup_path):\n+    \"\"\"\n+    Load missing user settings with default values.\n+    \"\"\"\n+    repo = UserSettingRepository(session)\n+\n+    # These keys are rendered into the global javascript namespace var\n+    # LUTE_USER_SETTINGS, so if any of these keys change, check the usage\n+    # of that variable as well.\n+    keys_and_defaults = {\n+        \"backup_enabled\": True,\n+        \"backup_auto\": True,\n+        \"backup_warn\": True,\n+        \"backup_dir\": default_user_backup_path,\n+        \"backup_count\": 5,\n+        \"lastbackup\": None,\n+        \"mecab_path\": None,\n+        \"japanese_reading\": \"hiragana\",\n+        \"current_theme\": \"-\",\n+        \"custom_styles\": \"/* Custom css to modify Lute's appearance. */\",\n+        \"show_highlights\": True,\n+        \"current_language_id\": 0,\n+        # Behaviour:\n+        \"open_popup_in_new_tab\": False,\n+        \"stop_audio_on_term_form_open\": True,\n+        \"stats_calc_sample_size\": 5,\n+        # Keyboard shortcuts.  These have default values assigned\n+        # as they were the hotkeys defined in the initial Lute\n+        # release.\n+        \"hotkey_StartHover\": \"escape\",\n+        \"hotkey_PrevWord\": \"arrowleft\",\n+        \"hotkey_NextWord\": \"arrowright\",\n+        \"hotkey_StatusUp\": \"arrowup\",\n+        \"hotkey_StatusDown\": \"arrowdown\",\n+        \"hotkey_Bookmark\": \"b\",\n+        \"hotkey_CopySentence\": \"c\",\n+        \"hotkey_CopyPara\": \"shift+c\",\n+        \"hotkey_TranslateSentence\": \"t\",\n+        \"hotkey_TranslatePara\": \"shift+t\",\n+        \"hotkey_NextTheme\": \"m\",\n+        \"hotkey_ToggleHighlight\": \"h\",\n+        \"hotkey_ToggleFocus\": \"f\",\n+        \"hotkey_Status1\": \"1\",\n+        \"hotkey_Status2\": \"2\",\n+        \"hotkey_Status3\": \"3\",\n+        \"hotkey_Status4\": \"4\",\n+        \"hotkey_Status5\": \"5\",\n+        \"hotkey_StatusIgnore\": \"i\",\n+        \"hotkey_StatusWellKnown\": \"w\",\n+        # New hotkeys.  These must have empty values, because\n+        # users may have already setup their hotkeys, and we can't\n+        # assume that a given key combination is free:\n+        \"hotkey_CopyPage\": \"\",\n+        \"hotkey_DeleteTerm\": \"\",\n+        \"hotkey_EditPage\": \"\",\n+        \"hotkey_TranslatePage\": \"\",\n+        \"hotkey_PrevUnknownWord\": \"\",\n+        \"hotkey_NextUnknownWord\": \"\",\n+        \"hotkey_PrevSentence\": \"\",\n+        \"hotkey_NextSentence\": \"\",\n+    }\n+    for k, v in keys_and_defaults.items():\n+        if not repo.key_exists(k):\n+            s = UserSetting()\n+            s.key = k\n+            s.value = v\n+            session.add(s)\n+    session.commit()\n+\n+    # Revise the mecab path if necessary.\n+    # Note this is done _after_ the defaults are loaded,\n+    # because the user may have already loaded the defaults\n+    # (e.g. on machine upgrade) and stored them in the db,\n+    # so we may have to _update_ the existing setting.\n+    revised_mecab_path = _revised_mecab_path(repo)\n+    repo.set_value(\"mecab_path\", revised_mecab_path)\n+    session.commit()\ndiff --git a/lute/db/schema/migrations/20241103_change_lastbackup_to_user_setting.sql b/lute/db/schema/migrations/20241103_change_lastbackup_to_user_setting.sql\nnew file mode 100644\nindex 00000000..df3463a0\n--- /dev/null\n+++ b/lute/db/schema/migrations/20241103_change_lastbackup_to_user_setting.sql\n@@ -0,0 +1,3 @@\n+-- Change lastbackup to from system to user key.\n+\n+update settings set StKeyType = 'user' where StKey = 'lastbackup';\ndiff --git a/lute/dev_api/routes.py b/lute/dev_api/routes.py\nindex 3f3016c6..7c5a543a 100644\n--- a/lute/dev_api/routes.py\n+++ b/lute/dev_api/routes.py\n@@ -18,7 +18,7 @@\n from sqlalchemy import text\n from flask import Blueprint, current_app, Response, jsonify, redirect, flash\n from lute.models.language import Language\n-from lute.models.setting import UserSetting\n+from lute.models.repositories import UserSettingRepository\n import lute.parse.registry\n from lute.db import db\n import lute.db.management\n@@ -38,7 +38,7 @@ def _ensure_is_test_db():\n @bp.route(\"/wipe_db\", methods=[\"GET\"])\n def wipe_db():\n     \"Clean it all.\"\n-    lute.db.management.delete_all_data()\n+    lute.db.management.delete_all_data(db.session)\n     flash(\"db wiped\")\n     return redirect(\"/\", 302)\n \n@@ -46,8 +46,8 @@ def wipe_db():\n @bp.route(\"/load_demo\", methods=[\"GET\"])\n def load_demo():\n     \"Clean out everything, and load the demo.\"\n-    lute.db.management.delete_all_data()\n-    lute.db.demo.load_demo_data()\n+    lute.db.management.delete_all_data(db.session)\n+    lute.db.demo.load_demo_data(db.session)\n     flash(\"demo loaded\")\n     return redirect(\"/\", 302)\n \n@@ -55,8 +55,8 @@ def load_demo():\n @bp.route(\"/load_demo_languages\", methods=[\"GET\"])\n def load_demo_languages():\n     \"Clean out everything, and load the demo langs with dummy dictionaries.\"\n-    lute.db.management.delete_all_data()\n-    lute.db.demo.load_demo_languages()\n+    lute.db.management.delete_all_data(db.session)\n+    lute.db.demo.load_demo_languages(db.session)\n     langs = db.session.query(Language).all()\n     for lang in langs:\n         d = lang.dictionaries[0]\n@@ -70,7 +70,7 @@ def load_demo_languages():\n @bp.route(\"/load_demo_stories\", methods=[\"GET\"])\n def load_demo_stories():\n     \"Stories only.  No db wipe.\"\n-    lute.db.demo.load_demo_stories()\n+    lute.db.demo.load_demo_stories(db.session)\n     flash(\"stories loaded\")\n     return redirect(\"/\", 302)\n \n@@ -148,7 +148,8 @@ def disable_parser(parsername, renameto):\n @bp.route(\"/disable_backup\", methods=[\"GET\"])\n def disable_backup():\n     \"Disables backup -- tests don't need to back up.\"\n-    UserSetting.set_value(\"backup_enabled\", False)\n+    repo = UserSettingRepository(db.session)\n+    repo.set_value(\"backup_enabled\", False)\n     db.session.commit()\n     flash(\"backup disabled\")\n     return redirect(\"/\", 302)\ndiff --git a/lute/language/routes.py b/lute/language/routes.py\nindex c877f98d..0eecbd5d 100644\n--- a/lute/language/routes.py\n+++ b/lute/language/routes.py\n@@ -6,8 +6,8 @@\n from sqlalchemy.exc import IntegrityError\n from flask import Blueprint, current_app, render_template, redirect, url_for, flash\n from lute.models.language import Language\n-from lute.models.setting import UserSetting\n-import lute.language.service\n+from lute.models.repositories import LanguageRepository, UserSettingRepository\n+from lute.language.service import Service\n from lute.language.forms import LanguageForm\n from lute.db import db\n from lute.parse.registry import supported_parsers\n@@ -116,7 +116,8 @@ def new(langname):\n     \"\"\"\n     Create a new language.\n     \"\"\"\n-    predefined = lute.language.service.predefined_languages()\n+    service = Service(db.session)\n+    predefined = service.predefined_languages()\n     language = Language()\n     if langname is not None:\n         candidates = [lang for lang in predefined if lang.name == langname]\n@@ -135,7 +136,8 @@ def new(langname):\n         # adds language Y, the filter stays on X, which may be\n         # disconcerting/confusing.  Forcing a reselect is painless and\n         # unambiguous.\n-        UserSetting.set_value(\"current_language_id\", 0)\n+        repo = UserSettingRepository(db.session)\n+        repo.set_value(\"current_language_id\", 0)\n         db.session.commit()\n         return redirect(\"/\")\n \n@@ -154,14 +156,16 @@ def delete(langid):\n     language = db.session.get(Language, langid)\n     if not language:\n         flash(f\"Language {langid} not found\")\n-    Language.delete(language)\n+    r = LanguageRepository(db.session)\n+    r.delete(language)\n     return redirect(url_for(\"language.index\"))\n \n \n @bp.route(\"/list_predefined\", methods=[\"GET\"])\n def list_predefined():\n     \"Show predefined languages that are not already in the db.\"\n-    predefined = lute.language.service.predefined_languages()\n+    service = Service(db.session)\n+    predefined = service.predefined_languages()\n     existing_langs = db.session.query(Language).all()\n     existing_names = [l.name for l in existing_langs]\n     new_langs = [p for p in predefined if p.name not in existing_names]\n@@ -171,8 +175,10 @@ def list_predefined():\n @bp.route(\"/load_predefined/<langname>\", methods=[\"GET\"])\n def load_predefined(langname):\n     \"Load a predefined language and its stories.\"\n-    lang_id = lute.language.service.load_language_def(langname)\n-    UserSetting.set_value(\"current_language_id\", lang_id)\n+    service = Service(db.session)\n+    lang_id = service.load_language_def(langname)\n+    repo = UserSettingRepository(db.session)\n+    repo.set_value(\"current_language_id\", lang_id)\n     db.session.commit()\n     flash(f\"Loaded {langname} and sample book(s)\")\n     return redirect(\"/\")\ndiff --git a/lute/language/service.py b/lute/language/service.py\nindex 7c5231bb..744a3269 100644\n--- a/lute/language/service.py\n+++ b/lute/language/service.py\n@@ -6,75 +6,75 @@\n import yaml\n from lute.models.language import Language\n from lute.book.model import Book, Repository\n-from lute.db import db\n \n \n-def get_supported_defs():\n-    \"Return supported language definitions.\"\n-    ret = []\n-    def_glob = os.path.join(_language_defs_path(), \"**\", \"definition.yaml\")\n-    for f in glob(def_glob):\n-        lang = None\n-        with open(f, \"r\", encoding=\"utf-8\") as df:\n-            d = yaml.safe_load(df)\n-            lang = Language.from_dict(d)\n-        if lang.is_supported:\n-            entry = {\"language\": lang, \"books\": _get_books(f, lang.name)}\n-            ret.append(entry)\n-    ret.sort(key=lambda x: x[\"language\"].name)\n-    return ret\n+class Service:\n+    \"Service.\"\n \n+    def __init__(self, session):\n+        self.session = session\n \n-def predefined_languages():\n-    \"Languages defined in yaml files.\"\n-    return [d[\"language\"] for d in get_supported_defs()]\n+    def get_supported_defs(self):\n+        \"Return supported language definitions.\"\n+        ret = []\n+        def_glob = os.path.join(self._language_defs_path(), \"**\", \"definition.yaml\")\n+        for f in glob(def_glob):\n+            lang = None\n+            with open(f, \"r\", encoding=\"utf-8\") as df:\n+                d = yaml.safe_load(df)\n+                lang = Language.from_dict(d)\n+            if lang.is_supported:\n+                entry = {\"language\": lang, \"books\": self._get_books(f, lang.name)}\n+                ret.append(entry)\n+        ret.sort(key=lambda x: x[\"language\"].name)\n+        return ret\n \n+    def predefined_languages(self):\n+        \"Languages defined in yaml files.\"\n+        return [d[\"language\"] for d in self.get_supported_defs()]\n \n-def _get_books(lang_definition_filename, lang_name):\n-    \"Get the stories in the same directory as the definition.yaml.\"\n-    books = []\n-    d, f = os.path.split(lang_definition_filename)\n-    story_glob = os.path.join(d, \"*.txt\")\n-    for filename in glob(story_glob):\n-        with open(filename, \"r\", encoding=\"utf-8\") as f:\n-            content = f.read()\n-        title_match = re.search(r\"title:\\s*(.*)\\n\", content)\n-        title = title_match.group(1).strip()\n-        content = re.sub(r\"#.*\\n\", \"\", content)\n-        b = Book()\n-        b.language_name = lang_name\n-        b.title = title\n-        b.text = content\n-        books.append(b)\n-    return books\n+    def _get_books(self, lang_definition_filename, lang_name):\n+        \"Get the stories in the same directory as the definition.yaml.\"\n+        books = []\n+        d, f = os.path.split(lang_definition_filename)\n+        story_glob = os.path.join(d, \"*.txt\")\n+        for filename in glob(story_glob):\n+            with open(filename, \"r\", encoding=\"utf-8\") as f:\n+                content = f.read()\n+            title_match = re.search(r\"title:\\s*(.*)\\n\", content)\n+            title = title_match.group(1).strip()\n+            content = re.sub(r\"#.*\\n\", \"\", content)\n+            b = Book()\n+            b.language_name = lang_name\n+            b.title = title\n+            b.text = content\n+            books.append(b)\n+        return books\n \n+    def get_language_def(self, lang_name):\n+        \"Get a lang def and its stories.\"\n+        defs = self.get_supported_defs()\n+        ret = [d for d in defs if d[\"language\"].name == lang_name]\n+        if len(ret) == 0:\n+            raise RuntimeError(f\"Missing language def name {lang_name}\")\n+        return ret[0]\n \n-def get_language_def(lang_name):\n-    \"Get a lang def and its stories.\"\n-    defs = get_supported_defs()\n-    ret = [d for d in defs if d[\"language\"].name == lang_name]\n-    if len(ret) == 0:\n-        raise RuntimeError(f\"Missing language def name {lang_name}\")\n-    return ret[0]\n+    def load_language_def(self, lang_name):\n+        \"Load a language def and its stories, save to database.\"\n+        load_def = self.get_language_def(lang_name)\n+        lang = load_def[\"language\"]\n+        self.session.add(lang)\n+        self.session.commit()\n \n+        r = Repository(self.session)\n+        for b in load_def[\"books\"]:\n+            r.add(b)\n+        r.commit()\n \n-def load_language_def(lang_name):\n-    \"Load a language def and its stories, save to database.\"\n-    load_def = get_language_def(lang_name)\n-    lang = load_def[\"language\"]\n-    db.session.add(lang)\n-    db.session.commit()\n+        return lang.id\n \n-    r = Repository(db)\n-    for b in load_def[\"books\"]:\n-        r.add(b)\n-    r.commit()\n-\n-    return lang.id\n-\n-\n-def _language_defs_path():\n-    \"Path to the definitions and stories.\"\n-    thisdir = os.path.dirname(__file__)\n-    d = os.path.join(thisdir, \"..\", \"db\", \"language_defs\")\n-    return os.path.abspath(d)\n+    def _language_defs_path(self):\n+        \"Path to the definitions and stories.\"\n+        thisdir = os.path.dirname(__file__)\n+        d = os.path.join(thisdir, \"..\", \"db\", \"language_defs\")\n+        return os.path.abspath(d)\ndiff --git a/lute/models/book.py b/lute/models/book.py\nindex b25a7de4..da75bba0 100644\n--- a/lute/models/book.py\n+++ b/lute/models/book.py\n@@ -2,7 +2,6 @@\n Book entity.\n \"\"\"\n \n-from sqlalchemy import and_\n from lute.db import db\n from lute.parse.base import SentenceGroupIterator\n \n@@ -30,19 +29,6 @@ def make_book_tag(text, comment=\"\"):\n         tt.comment = comment\n         return tt\n \n-    @staticmethod\n-    def find_by_text(text):\n-        \"Find a tag by text, or None if not found.\"\n-        return db.session.query(BookTag).filter(BookTag.text == text).first()\n-\n-    @staticmethod\n-    def find_or_create_by_text(text):\n-        \"Return tag or create one.\"\n-        ret = BookTag.find_by_text(text)\n-        if ret is not None:\n-            return ret\n-        return BookTag.make_book_tag(text)\n-\n \n class Book(\n     db.Model\n@@ -194,20 +180,6 @@ def split_text_at_page_breaks(txt):\n \n         return b\n \n-    @staticmethod\n-    def find(book_id):\n-        \"Get by ID.\"\n-        return db.session.query(Book).filter(Book.id == book_id).first()\n-\n-    @staticmethod\n-    def find_by_title(book_title, language_id):\n-        \"Get by title.\"\n-        return (\n-            db.session.query(Book)\n-            .filter(and_(Book.title == book_title, Book.language_id == language_id))\n-            .first()\n-        )\n-\n \n # TODO zzfuture fix: rename class and table to Page/pages\n class Text(db.Model):\n@@ -321,11 +293,6 @@ def _remove_sentences(self):\n             sentence.text = None\n         self.sentences = []\n \n-    @staticmethod\n-    def find(text_id):\n-        \"Get by ID.\"\n-        return db.session.query(Text).filter(Text.id == text_id).first()\n-\n \n class Sentence(db.Model):\n     \"\"\"\n@@ -390,3 +357,14 @@ class TextBookmark(db.Model):\n     title = db.Column(\"TbTitle\", db.Text, nullable=False)\n \n     text = db.relationship(\"Text\", back_populates=\"bookmarks\")\n+\n+\n+class BookStats(db.Model):\n+    \"The stats table.\"\n+    __tablename__ = \"bookstats\"\n+\n+    BkID = db.Column(db.Integer, primary_key=True)\n+    distinctterms = db.Column(db.Integer)\n+    distinctunknowns = db.Column(db.Integer)\n+    unknownpercent = db.Column(db.Integer)\n+    status_distribution = db.Column(db.String, nullable=True)\ndiff --git a/lute/models/language.py b/lute/models/language.py\nindex 3da47b3e..a85d0d23 100644\n--- a/lute/models/language.py\n+++ b/lute/models/language.py\n@@ -3,7 +3,6 @@\n \"\"\"\n \n import re\n-from sqlalchemy import text, func\n from lute.db import db\n from lute.parse.registry import get_parser, is_supported\n \n@@ -127,30 +126,6 @@ def all_dictionaries(cls):\n             }\n         return lang_dicts\n \n-    @staticmethod\n-    def delete(language):\n-        \"\"\"\n-        Hacky method to delete language and all terms, books, and dicts\n-        associated with it.\n-\n-        There is _certainly_ a better way to do this using\n-        Sqlalchemy relationships and cascade deletes, but I\n-        was running into problems with it (things not cascading,\n-        or warnings (\"SAWarning: Object of type <Term> not in\n-        session, add operation along 'Language.terms' will not\n-        proceed\") during test runs.  It would be nice to have\n-        a \"correct\" mapping, but this is good enough for now.\n-\n-        TODO zzfuture fix: fix Language-Book and -Term mappings.\n-        \"\"\"\n-        sqls = [\n-            \"pragma foreign_keys = ON\",\n-            f\"delete from languages where LgID = {language.id}\",\n-        ]\n-        for s in sqls:\n-            db.session.execute(text(s))\n-        db.session.commit()\n-\n     @property\n     def parser(self):\n         \"Note: this throws if the parser is not supported!!!\"\n@@ -167,20 +142,6 @@ def get_parsed_tokens(self, s):\n     def get_lowercase(self, s) -> str:\n         return self.parser.get_lowercase(s)\n \n-    @staticmethod\n-    def find(language_id):\n-        \"Get by ID.\"\n-        return db.session.query(Language).filter(Language.id == language_id).first()\n-\n-    @staticmethod\n-    def find_by_name(name):\n-        \"Get by name.\"\n-        return (\n-            db.session.query(Language)\n-            .filter(func.lower(Language.name) == func.lower(name))\n-            .first()\n-        )\n-\n     def to_dict(self):\n         \"Return dictionary of data, for serialization.\"\n         ret = {}\ndiff --git a/lute/models/repositories.py b/lute/models/repositories.py\nnew file mode 100644\nindex 00000000..271d9bc0\n--- /dev/null\n+++ b/lute/models/repositories.py\n@@ -0,0 +1,253 @@\n+\"\"\"\n+Repositories.\n+\"\"\"\n+\n+from sqlalchemy import text as sqltext, and_, func\n+from lute.db import db\n+from lute.models.setting import UserSetting, BackupSettings, SystemSetting\n+from lute.models.language import Language\n+from lute.models.term import Term, TermTag\n+from lute.models.book import Book, BookTag\n+\n+\n+class SettingRepositoryBase:\n+    \"Repository.\"\n+\n+    def __init__(self, session, classtype):\n+        self.session = session\n+        self.classtype = classtype\n+\n+    def key_exists_precheck(self, keyname):\n+        \"\"\"\n+        Check key validity for certain actions.\n+        \"\"\"\n+\n+    def set_value(self, keyname, keyvalue):\n+        \"Set, but don't save, a setting.\"\n+        self.key_exists_precheck(keyname)\n+        s = (\n+            self.session.query(self.classtype)\n+            .filter(self.classtype.key == keyname)\n+            .first()\n+        )\n+        if s is None:\n+            s = self.classtype()\n+            s.key = keyname\n+        s.value = keyvalue\n+        self.session.add(s)\n+\n+    def key_exists(self, keyname):\n+        \"True if exists.\"\n+        s = (\n+            self.session.query(self.classtype)\n+            .filter(self.classtype.key == keyname)\n+            .first()\n+        )\n+        no_key = s is None\n+        return not no_key\n+\n+    def get_value(self, keyname):\n+        \"Get the saved key, or None if it doesn't exist.\"\n+        self.key_exists_precheck(keyname)\n+        s = (\n+            self.session.query(self.classtype)\n+            .filter(self.classtype.key == keyname)\n+            .first()\n+        )\n+        if s is None:\n+            return None\n+        return s.value\n+\n+    def delete_key(self, keyname):\n+        \"Delete a key.\"\n+        s = (\n+            self.session.query(self.classtype)\n+            .filter(self.classtype.key == keyname)\n+            .first()\n+        )\n+        if s is not None:\n+            self.session.delete(s)\n+\n+\n+class MissingUserSettingKeyException(Exception):\n+    \"\"\"\n+    Cannot set or get unknown user keys.\n+    \"\"\"\n+\n+\n+class UserSettingRepository(SettingRepositoryBase):\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        super().__init__(session, UserSetting)\n+\n+    def key_exists_precheck(self, keyname):\n+        \"\"\"\n+        User keys must exist.\n+        \"\"\"\n+        if not self.key_exists(keyname):\n+            raise MissingUserSettingKeyException(keyname)\n+\n+    def get_backup_settings(self):\n+        \"Convenience method.\"\n+        bs = BackupSettings()\n+\n+        def _bool(v):\n+            return v in (1, \"1\", \"y\", True)\n+\n+        bs.backup_enabled = _bool(self.get_value(\"backup_enabled\"))\n+        bs.backup_auto = _bool(self.get_value(\"backup_auto\"))\n+        bs.backup_warn = _bool(self.get_value(\"backup_warn\"))\n+        bs.backup_dir = self.get_value(\"backup_dir\")\n+        bs.backup_count = int(self.get_value(\"backup_count\") or 5)\n+        bs.last_backup_datetime = self.get_last_backup_datetime()\n+        return bs\n+\n+    def get_last_backup_datetime(self):\n+        \"Get the last_backup_datetime as int, or None.\"\n+        v = self.get_value(\"lastbackup\")\n+        if v is None:\n+            return None\n+        return int(v)\n+\n+    def set_last_backup_datetime(self, v):\n+        \"Set and save the last backup time.\"\n+        self.set_value(\"lastbackup\", v)\n+        self.session.commit()\n+\n+\n+class SystemSettingRepository(SettingRepositoryBase):\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        super().__init__(session, SystemSetting)\n+\n+\n+class LanguageRepository:\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def find(self, language_id):\n+        \"Get by ID.\"\n+        return self.session.query(Language).filter(Language.id == language_id).first()\n+\n+    def find_by_name(self, name):\n+        \"Get by name.\"\n+        return (\n+            self.session.query(Language)\n+            .filter(func.lower(Language.name) == func.lower(name))\n+            .first()\n+        )\n+\n+    def delete(self, language):\n+        \"\"\"\n+        Hacky method to delete language and all terms, books, and dicts\n+        associated with it.\n+\n+        There is _certainly_ a better way to do this using\n+        Sqlalchemy relationships and cascade deletes, but I\n+        was running into problems with it (things not cascading,\n+        or warnings (\"SAWarning: Object of type <Term> not in\n+        session, add operation along 'Language.terms' will not\n+        proceed\") during test runs.  It would be nice to have\n+        a \"correct\" mapping, but this is good enough for now.\n+\n+        TODO zzfuture fix: fix Language-Book and -Term mappings.\n+        \"\"\"\n+        sqls = [\n+            \"pragma foreign_keys = ON\",\n+            f\"delete from languages where LgID = {language.id}\",\n+        ]\n+        for s in sqls:\n+            self.session.execute(sqltext(s))\n+        self.session.commit()\n+\n+\n+class TermTagRepository:\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def find(self, termtag_id):\n+        \"Get by ID.\"\n+        return self.session.query(TermTag).filter(TermTag.id == termtag_id).first()\n+\n+    def find_by_text(self, text):\n+        \"Find a tag by text, or None if not found.\"\n+        return self.session.query(TermTag).filter(TermTag.text == text).first()\n+\n+    def find_or_create_by_text(self, text):\n+        \"Return tag or create one.\"\n+        ret = self.find_by_text(text)\n+        if ret is not None:\n+            return ret\n+        return TermTag(text)\n+\n+\n+class TermRepository:\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def find_by_spec(self, spec):\n+        \"\"\"\n+        Find by the given spec term's language ID and text.\n+        Returns None if not found.\n+        \"\"\"\n+        langid = spec.language.id\n+        text_lc = spec.text_lc\n+        query = self.session.query(Term).filter(\n+            and_(Term.language_id == langid, Term.text_lc == text_lc)\n+        )\n+        terms = query.all()\n+        if not terms:\n+            return None\n+        return terms[0]\n+\n+    def delete_empty_images(self):\n+        \"\"\"\n+        Data clean-up: delete empty images.\n+\n+        The code was leaving empty images in the db, which are obviously no good.\n+        This is a hack to clean up the data.\n+        \"\"\"\n+        sql = \"delete from wordimages where trim(WiSource) = ''\"\n+        self.session.execute(sqltext(sql))\n+        self.session.commit()\n+\n+\n+class BookTagRepository:\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def find_or_create_by_text(self, text):\n+        \"Return tag or create one.\"\n+        ret = db.session.query(BookTag).filter(BookTag.text == text).first()\n+        if ret is not None:\n+            return ret\n+        return BookTag.make_book_tag(text)\n+\n+\n+class BookRepository:\n+    \"Repository.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def find(self, book_id):\n+        \"Get by ID.\"\n+        return self.session.query(Book).filter(Book.id == book_id).first()\n+\n+    def find_by_title(self, book_title, language_id):\n+        \"Get by title.\"\n+        return (\n+            self.session.query(Book)\n+            .filter(and_(Book.title == book_title, Book.language_id == language_id))\n+            .first()\n+        )\ndiff --git a/lute/models/setting.py b/lute/models/setting.py\nindex 118363cc..dcb573d4 100644\n--- a/lute/models/setting.py\n+++ b/lute/models/setting.py\n@@ -2,10 +2,8 @@\n Lute settings, in settings key-value table.\n \"\"\"\n \n-import os\n import datetime\n import time\n-from flask import current_app\n from lute.db import db\n \n \n@@ -23,237 +21,12 @@ class SettingBase(db.Model):\n     value = db.Column(\"StValue\", db.String, nullable=False)\n     __mapper_args__ = {\"polymorphic_on\": keytype}\n \n-    @classmethod\n-    def key_exists_precheck(cls, keyname):\n-        \"\"\"\n-        Check key validity for certain actions.\n-        \"\"\"\n-\n-    @classmethod\n-    def set_value_post(cls, keyname, keyvalue):\n-        \"\"\"\n-        Post-setting value for certain keys.\"\n-        \"\"\"\n-\n-    @classmethod\n-    def set_value(cls, keyname, keyvalue):\n-        \"Set, but don't save, a setting.\"\n-        cls.key_exists_precheck(keyname)\n-        s = db.session.query(cls).filter(cls.key == keyname).first()\n-        if s is None:\n-            s = cls()\n-            s.key = keyname\n-        s.value = keyvalue\n-        db.session.add(s)\n-        cls.set_value_post(keyname, keyvalue)\n-\n-    @classmethod\n-    def key_exists(cls, keyname):\n-        \"True if exists.\"\n-        s = db.session.query(cls).filter(cls.key == keyname).first()\n-        no_key = s is None\n-        return not no_key\n-\n-    @classmethod\n-    def get_value(cls, keyname):\n-        \"Get the saved key, or None if it doesn't exist.\"\n-        cls.key_exists_precheck(keyname)\n-        s = db.session.query(cls).filter(cls.key == keyname).first()\n-        if s is None:\n-            return None\n-        return s.value\n-\n-    @classmethod\n-    def delete_key(cls, keyname):\n-        \"Delete a key.\"\n-        s = db.session.query(cls).filter(cls.key == keyname).first()\n-        if s is not None:\n-            db.session.delete(s)\n-\n-\n-class MissingUserSettingKeyException(Exception):\n-    \"\"\"\n-    Cannot set or get unknown user keys.\n-    \"\"\"\n-\n \n class UserSetting(SettingBase):\n     \"User setting.\"\n     __tablename__ = None\n     __mapper_args__ = {\"polymorphic_identity\": \"user\"}\n \n-    @classmethod\n-    def key_exists_precheck(cls, keyname):\n-        \"\"\"\n-        User keys must exist.\n-        \"\"\"\n-        if not UserSetting.key_exists(keyname):\n-            raise MissingUserSettingKeyException(keyname)\n-\n-    @classmethod\n-    def set_value_post(cls, keyname, keyvalue):\n-        \"\"\"\n-        Setting some keys runs other code.\n-        \"\"\"\n-        if keyname == \"mecab_path\":\n-            mp = \"MECAB_PATH\"\n-            if keyvalue is None or keyvalue == \"\":\n-                if mp in os.environ:\n-                    del os.environ[mp]\n-            else:\n-                os.environ[mp] = keyvalue.strip()\n-\n-    @staticmethod\n-    def _revised_mecab_path():\n-        \"\"\"\n-        Change the mecab_path if it's not found, and a\n-        replacement is found.\n-\n-        Lute Docker images are built to be multi-arch, and\n-        interestingly (annoyingly), mecab libraries are installed into\n-        different locations depending on the architecture, even with\n-        the same Dockerfile and base image.\n-\n-        Returns: new mecab path if old one is missing _and_\n-        new one found, otherwise just return the old one.\n-        \"\"\"\n-\n-        mp = UserSetting.get_value(\"mecab_path\")\n-        if mp is not None and os.path.exists(mp):\n-            return mp\n-\n-        # See develop docs for notes on how to find the libmecab path!\n-        candidates = [\n-            # linux/arm64\n-            \"/lib/aarch64-linux-gnu/libmecab.so.2\",\n-            # linux/amd64\n-            \"/lib/x86_64-linux-gnu/libmecab.so.2\",\n-            # github CI, ubuntu-latest\n-            \"/lib/x86_64-linux-gnu/libmecab.so.2\",\n-        ]\n-        replacements = [p for p in candidates if os.path.exists(p)]\n-        if len(replacements) > 0:\n-            return replacements[0]\n-        # Replacement not found, leave current value as-is.\n-        return mp\n-\n-    @staticmethod\n-    def load():\n-        \"\"\"\n-        Load missing user settings with default values.\n-        \"\"\"\n-        app_config = current_app.env_config\n-\n-        # These keys are rendered into the global javascript namespace var\n-        # LUTE_USER_SETTINGS, so if any of these keys change, check the usage\n-        # of that variable as well.\n-        keys_and_defaults = {\n-            \"backup_enabled\": True,\n-            \"backup_auto\": True,\n-            \"backup_warn\": True,\n-            \"backup_dir\": app_config.default_user_backup_path,\n-            \"backup_count\": 5,\n-            \"mecab_path\": None,\n-            \"japanese_reading\": \"hiragana\",\n-            \"current_theme\": \"-\",\n-            \"custom_styles\": \"/* Custom css to modify Lute's appearance. */\",\n-            \"show_highlights\": True,\n-            \"current_language_id\": 0,\n-            # Behaviour:\n-            \"open_popup_in_new_tab\": False,\n-            \"stop_audio_on_term_form_open\": True,\n-            \"stats_calc_sample_size\": 5,\n-            # Keyboard shortcuts.  These have default values assigned\n-            # as they were the hotkeys defined in the initial Lute\n-            # release.\n-            \"hotkey_StartHover\": \"escape\",\n-            \"hotkey_PrevWord\": \"arrowleft\",\n-            \"hotkey_NextWord\": \"arrowright\",\n-            \"hotkey_StatusUp\": \"arrowup\",\n-            \"hotkey_StatusDown\": \"arrowdown\",\n-            \"hotkey_Bookmark\": \"b\",\n-            \"hotkey_CopySentence\": \"c\",\n-            \"hotkey_CopyPara\": \"shift+c\",\n-            \"hotkey_TranslateSentence\": \"t\",\n-            \"hotkey_TranslatePara\": \"shift+t\",\n-            \"hotkey_NextTheme\": \"m\",\n-            \"hotkey_ToggleHighlight\": \"h\",\n-            \"hotkey_ToggleFocus\": \"f\",\n-            \"hotkey_Status1\": \"1\",\n-            \"hotkey_Status2\": \"2\",\n-            \"hotkey_Status3\": \"3\",\n-            \"hotkey_Status4\": \"4\",\n-            \"hotkey_Status5\": \"5\",\n-            \"hotkey_StatusIgnore\": \"i\",\n-            \"hotkey_StatusWellKnown\": \"w\",\n-            # New hotkeys.  These must have empty values, because\n-            # users may have already setup their hotkeys, and we can't\n-            # assume that a given key combination is free:\n-            \"hotkey_CopyPage\": \"\",\n-            \"hotkey_DeleteTerm\": \"\",\n-            \"hotkey_EditPage\": \"\",\n-            \"hotkey_TranslatePage\": \"\",\n-            \"hotkey_PrevUnknownWord\": \"\",\n-            \"hotkey_NextUnknownWord\": \"\",\n-            \"hotkey_PrevSentence\": \"\",\n-            \"hotkey_NextSentence\": \"\",\n-        }\n-        for k, v in keys_and_defaults.items():\n-            if not UserSetting.key_exists(k):\n-                s = UserSetting()\n-                s.key = k\n-                s.value = v\n-                db.session.add(s)\n-        db.session.commit()\n-\n-        # Revise the mecab path if necessary.\n-        # Note this is done _after_ the defaults are loaded,\n-        # because the user may have already loaded the defaults\n-        # (e.g. on machine upgrade) and stored them in the db,\n-        # so we may have to _update_ the existing setting.\n-        revised_mecab_path = UserSetting._revised_mecab_path()\n-        UserSetting.set_value(\"mecab_path\", revised_mecab_path)\n-        db.session.commit()\n-\n-    @staticmethod\n-    def all_settings():\n-        \"\"\"\n-        Get dict of all settings, for rendering into Javascript global space.\n-        \"\"\"\n-        settings = db.session.query(UserSetting).all()\n-        ret = {}\n-        for s in settings:\n-            ret[s.key] = s.value\n-\n-        # Convert some ints into bools.\n-        boolkeys = [\"open_popup_in_new_tab\", \"stop_audio_on_term_form_open\"]\n-        for k in boolkeys:\n-            ret[k] = ret[k] == \"1\"\n-\n-        return ret\n-\n-\n-class SystemSetting(SettingBase):\n-    \"System setting.\"\n-    __tablename__ = None\n-    __mapper_args__ = {\"polymorphic_identity\": \"system\"}\n-\n-    # Helpers for certain sys settings.\n-\n-    @classmethod\n-    def get_last_backup_datetime(cls):\n-        \"Get the last_backup_datetime as int, or None.\"\n-        v = cls.get_value(\"lastbackup\")\n-        if v is None:\n-            return None\n-        return int(v)\n-\n-    @classmethod\n-    def set_last_backup_datetime(cls, v):\n-        \"Set and save the last backup time.\"\n-        cls.set_value(\"lastbackup\", v)\n-        db.session.commit()\n-\n \n class BackupSettings:\n     \"\"\"\n@@ -262,16 +35,12 @@ class BackupSettings:\n     \"\"\"\n \n     def __init__(self):\n-        def _bool(k):\n-            v = UserSetting.get_value(k)\n-            return v in (1, \"1\", \"y\", True)\n-\n-        self.backup_enabled = _bool(\"backup_enabled\")\n-        self.backup_auto = _bool(\"backup_auto\")\n-        self.backup_warn = _bool(\"backup_warn\")\n-        self.backup_dir = UserSetting.get_value(\"backup_dir\")\n-        self.backup_count = int(UserSetting.get_value(\"backup_count\") or 5)\n-        self.last_backup_datetime = SystemSetting.get_last_backup_datetime()\n+        self.backup_enabled = None\n+        self.backup_auto = None\n+        self.backup_warn = None\n+        self.backup_dir = None\n+        self.backup_count = None\n+        self.last_backup_datetime = None\n \n     @property\n     def last_backup_display_date(self):\n@@ -281,11 +50,6 @@ def last_backup_display_date(self):\n             return None\n         return datetime.datetime.fromtimestamp(t).strftime(\"%Y-%m-%d %H:%M:%S\")\n \n-    @staticmethod\n-    def get_backup_settings():\n-        \"Get BackupSettings.\"\n-        return BackupSettings()\n-\n     @property\n     def time_since_last_backup(self):\n         \"\"\"\n@@ -320,3 +84,9 @@ def time_since_last_backup(self):\n             message = f\"{abs(delta)} seconds\"\n \n         return message + \" ago\"\n+\n+\n+class SystemSetting(SettingBase):\n+    \"System setting.\"\n+    __tablename__ = None\n+    __mapper_args__ = {\"polymorphic_identity\": \"system\"}\ndiff --git a/lute/models/term.py b/lute/models/term.py\nindex 2204828f..1d284b44 100644\n--- a/lute/models/term.py\n+++ b/lute/models/term.py\n@@ -2,7 +2,6 @@\n Term entity.\n \"\"\"\n \n-from sqlalchemy import text as sqltext, and_\n from lute.db import db\n \n wordparents = db.Table(\n@@ -72,24 +71,6 @@ def comment(self, c):\n         \"Set cleaned comment.\"\n         self._comment = c if c is not None else \"\"\n \n-    @staticmethod\n-    def find(termtag_id):\n-        \"Get by ID.\"\n-        return db.session.query(TermTag).filter(TermTag.id == termtag_id).first()\n-\n-    @staticmethod\n-    def find_by_text(text):\n-        \"Find a tag by text, or None if not found.\"\n-        return db.session.query(TermTag).filter(TermTag.text == text).first()\n-\n-    @staticmethod\n-    def find_or_create_by_text(text):\n-        \"Return tag or create one.\"\n-        ret = TermTag.find_by_text(text)\n-        if ret is not None:\n-            return ret\n-        return TermTag(text)\n-\n \n class TermTextChangedException(Exception):\n     \"\"\"\n@@ -309,18 +290,6 @@ def set_current_image(self, s):\n             ti.source = s.strip()\n             self.images.append(ti)\n \n-    @staticmethod\n-    def delete_empty_images():\n-        \"\"\"\n-        Data clean-up: delete empty images.\n-\n-        The code was leaving empty images in the db, which are obviously no good.\n-        This is a hack to clean up the data.\n-        \"\"\"\n-        sql = \"delete from wordimages where trim(WiSource) = ''\"\n-        db.session.execute(sqltext(sql))\n-        db.session.commit()\n-\n     def get_flash_message(self):\n         \"Get the flash message.\"\n         if not self.term_flash_message:\n@@ -343,27 +312,6 @@ def pop_flash_message(self):\n         self.term_flash_message = None\n         return m\n \n-    @staticmethod\n-    def find(term_id):\n-        \"Get by ID.\"\n-        return db.session.query(Term).filter(Term.id == term_id).first()\n-\n-    @staticmethod\n-    def find_by_spec(spec):\n-        \"\"\"\n-        Find by the given spec term's language ID and text.\n-        Returns None if not found.\n-        \"\"\"\n-        langid = spec.language.id\n-        text_lc = spec.text_lc\n-        query = db.session.query(Term).filter(\n-            and_(Term.language_id == langid, Term.text_lc == text_lc)\n-        )\n-        terms = query.all()\n-        if not terms:\n-            return None\n-        return terms[0]\n-\n \n class Status(db.Model):  # pylint: disable=too-few-public-methods\n     \"\"\"\n@@ -379,8 +327,3 @@ class Status(db.Model):  # pylint: disable=too-few-public-methods\n     id = db.Column(\"StID\", db.SmallInteger, primary_key=True)\n     text = db.Column(\"StText\", db.String(250))\n     abbreviation = db.Column(\"StAbbreviation\", db.String(250))\n-\n-    @staticmethod\n-    def all():\n-        \"Get all statuses.\"\n-        return db.session.query(Status).all()\ndiff --git a/lute/parse/mecab_parser.py b/lute/parse/mecab_parser.py\nindex dbe0b0b8..b3faf8a1 100644\n--- a/lute/parse/mecab_parser.py\n+++ b/lute/parse/mecab_parser.py\n@@ -18,7 +18,7 @@\n from natto import MeCab\n import jaconv\n from lute.parse.base import ParsedToken, AbstractParser\n-from lute.models.setting import UserSetting, MissingUserSettingKeyException\n+from lute.settings.current import current_settings\n \n \n class JapaneseParser(AbstractParser):\n@@ -30,7 +30,7 @@ class JapaneseParser(AbstractParser):\n     The parser uses natto-py library, and so should\n     be able to find mecab automatically; if it can't,\n     you may need to set the MECAB_PATH env variable,\n-    managed by UserSetting.set_value(\"mecab_path\", p)\n+    managed by UserSettingRepository.set_value(\"mecab_path\", p)\n     \"\"\"\n \n     _is_supported = None\n@@ -42,13 +42,21 @@ def is_supported(cls):\n         True if a natto MeCab can be instantiated,\n         otherwise false.\n         \"\"\"\n-        mecab_path = os.environ.get(\"MECAB_PATH\", \"<NOTSET>\")\n-        if (\n-            mecab_path == JapaneseParser._old_mecab_path\n-        ) and JapaneseParser._is_supported is not None:\n+\n+        mecab_path = current_settings.get(\"mecab_path\", \"\") or \"\"\n+        mecab_path = mecab_path.strip()\n+        path_unchanged = mecab_path == JapaneseParser._old_mecab_path\n+        if path_unchanged and JapaneseParser._is_supported is not None:\n             return JapaneseParser._is_supported\n \n-        b = False\n+        # Natto uses the MECAB_PATH env key if it's set.\n+        env_key = \"MECAB_PATH\"\n+        if mecab_path != \"\":\n+            os.environ[env_key] = mecab_path\n+        else:\n+            os.environ.pop(env_key, None)\n+\n+        mecab_works = False\n \n         # Calling MeCab() prints to stderr even if the\n         # exception is caught.  Suppress that output noise.\n@@ -56,15 +64,15 @@ def is_supported(cls):\n         try:\n             sys.stderr = temp_err\n             MeCab()\n-            b = True\n+            mecab_works = True\n         except:  # pylint: disable=bare-except\n-            b = False\n+            mecab_works = False\n         finally:\n             sys.stderr = sys.__stderr__\n \n         JapaneseParser._old_mecab_path = mecab_path\n-        JapaneseParser._is_supported = b\n-        return b\n+        JapaneseParser._is_supported = mecab_works\n+        return mecab_works\n \n     @classmethod\n     def name(cls):\n@@ -135,13 +143,9 @@ def get_reading(self, text: str):\n         if self._string_is_hiragana(text):\n             return None\n \n-        jp_reading_setting = \"\"\n-        try:\n-            jp_reading_setting = UserSetting.get_value(\"japanese_reading\")\n-        except MissingUserSettingKeyException:\n-            # During loading of demo data, the key isn't set, but the\n-            # reading isn't needed either, as this is only called when\n-            # calculating stats.\n+        jp_reading_setting = current_settings.get(\"japanese_reading\", \"\").strip()\n+        if jp_reading_setting == \"\":\n+            # Don't set reading if nothing specified.\n             return None\n \n         flags = r\"-O yomi\"\ndiff --git a/lute/read/render/service.py b/lute/read/render/service.py\nindex 3216d8e8..dafaf584 100644\n--- a/lute/read/render/service.py\n+++ b/lute/read/render/service.py\n@@ -10,209 +10,209 @@\n from lute.parse.base import ParsedToken\n from lute.read.render.calculate_textitems import get_textitems as calc_get_textitems\n from lute.read.render.multiword_indexer import MultiwordTermIndexer\n-from lute.db import db\n \n # from lute.utils.debug_helpers import DebugTimer\n \n \n-def find_all_Terms_in_string(s, language):  # pylint: disable=too-many-locals\n-    \"\"\"\n-    Find all terms contained in the string s.\n+class Service:\n+    \"Service.\"\n \n-    For example\n-    - given s = \"Here is a cat\"\n-    - given terms in the db: [ \"cat\", \"a cat\", \"dog\" ]\n+    def __init__(self, session):\n+        self.session = session\n \n-    This would return the terms \"cat\" and \"a cat\".\n-    \"\"\"\n-    cleaned = re.sub(r\" +\", \" \", s)\n-    tokens = language.get_parsed_tokens(cleaned)\n-    return _find_all_terms_in_tokens(tokens, language)\n+    def find_all_Terms_in_string(self, s, language):  # pylint: disable=too-many-locals\n+        \"\"\"\n+        Find all terms contained in the string s.\n+\n+        For example\n+        - given s = \"Here is a cat\"\n+        - given terms in the db: [ \"cat\", \"a cat\", \"dog\" ]\n+\n+        This would return the terms \"cat\" and \"a cat\".\n+        \"\"\"\n+        cleaned = re.sub(r\" +\", \" \", s)\n+        tokens = language.get_parsed_tokens(cleaned)\n+        return self._find_all_terms_in_tokens(tokens, language)\n+\n+    def _get_multiword_terms(self, language):\n+        \"Get all multiword terms.\"\n+        sql = sqltext(\n+            \"\"\"\n+            SELECT WoID, WoTextLC FROM words\n+            WHERE WoLgID=:language_id and WoTokenCount>1\n+            \"\"\"\n+        )\n+        sql = sql.bindparams(language_id=language.id)\n+        return self.session.execute(sql).all()\n+\n+    def _find_all_multi_word_term_text_lcs_in_content(self, text_lcs, language):\n+        \"Find multiword terms, return list of text_lcs.\"\n+\n+        # There are a few ways of finding multi-word Terms\n+        # (with token_count > 1) in the content:\n+        #\n+        # 1. load each mword term text_lc via sql and check.\n+        # 2. using the model\n+        # 3. SQL with \"LIKE\"\n+        #\n+        # During reasonable test runs with my data, the times in seconds\n+        # for each are similar (~0.02, ~0.05, ~0.025).  This method is\n+        # only used for small amounts of data, and the user experience hit\n+        # is negligible, so I'll use the first method which IMO is the clearest\n+        # code.\n+\n+        zws = \"\\u200B\"  # zero-width space\n+        content = zws + zws.join(text_lcs) + zws\n+\n+        # Method 1:\n+        reclist = self._get_multiword_terms(language)\n+        return [p[1] for p in reclist if f\"{zws}{p[1]}{zws}\" in content]\n+\n+        ## # Method 2: use the model.\n+        ## contained_term_qry = self.session.query(Term).filter(\n+        ##     Term.language == language,\n+        ##     Term.token_count > 1,\n+        ##     func.instr(content, Term.text_lc) > 0,\n+        ## )\n+        ## return [r.text_lc for r in contained_term_qry.all()]\n+\n+        ## # Method 3: Query with LIKE\n+        ## sql = sqltext(\n+        ##     \"\"\"\n+        ##     SELECT WoTextLC FROM words\n+        ##     WHERE WoLgID=:lid and WoTokenCount>1\n+        ##     AND :content LIKE '%' || :zws || WoTextLC || :zws || '%'\n+        ##     \"\"\"\n+        ## )\n+        ## sql = sql.bindparams(lid=language.id, content=content, zws=zws)\n+        ## recs = self.session.execute(sql).all()\n+        ## return [r[0] for r in recs]\n+\n+    def _find_all_terms_in_tokens(self, tokens, language, kwtree=None):\n+        \"\"\"\n+        Find all terms contained in the (ordered) parsed tokens tokens.\n+\n+        For example\n+        - given tokens = \"Here\", \" \", \"is\", \" \", \"a\", \" \", \"cat\"\n+        - given terms in the db: [ \"cat\", \"a/ /cat\", \"dog\" ]\n+\n+        This would return the terms \"cat\" and \"a/ /cat\".\n \n+        Method:\n+        - build list of lowercase text in the tokens\n+        - append all multword term strings that exist in the content\n+        - query for Terms that exist in the list\n \n-def _get_multiword_terms(language):\n-    \"Get all multiword terms.\"\n-    sql = sqltext(\n+        Note: this method only uses indexes for multiword terms, as any\n+        content analyzed is first parsed into tokens before being passed\n+        to this routine.  There's no need to search for single-word Terms\n+        in the tokenized strings, they can be found by a simple query.\n         \"\"\"\n-        SELECT WoID, WoTextLC FROM words\n-        WHERE WoLgID=:language_id and WoTokenCount>1\n+\n+        # Performance: About half of the time in this routine is spent in\n+        # Step 1 (finding multiword terms), the rest in step 2 (the actual\n+        # query).\n+        # dt = DebugTimer(\"_find_all_terms_in_tokens\", display=True)\n+\n+        parser = language.parser\n+        text_lcs = [parser.get_lowercase(t.token) for t in tokens]\n+\n+        # Step 1: get the multiwords in the content.\n+        if kwtree is None:\n+            mword_terms = self._find_all_multi_word_term_text_lcs_in_content(\n+                text_lcs, language\n+            )\n+        else:\n+            results = kwtree.search_all(text_lcs)\n+            mword_terms = [r[0] for r in results]\n+        # dt.step(\"filtered mword terms\")\n+\n+        # Step 2: load the Term objects.\n+        #\n+        # The Term fetch is actually performant -- there is no\n+        # real difference between loading the Term objects versus\n+        # loading raw data with SQL and getting dicts.\n+        #\n+        # Code for getting raw data:\n+        # param_keys = [f\"w{i}\" for i, _ in enumerate(text_lcs)]\n+        # keys_placeholders = ','.join([f\":{k}\" for k in param_keys])\n+        # param_dict = dict(zip(param_keys, text_lcs))\n+        # param_dict[\"langid\"] = language.id\n+        # sql = sqltext(f\"\"\"SELECT WoID, WoTextLC FROM words\n+        #     WHERE WoLgID=:langid and WoTextLC in ({keys_placeholders})\"\"\")\n+        # sql = sql.bindparams(language.id, *text_lcs)\n+        # results = self.session.execute(sql, param_dict).fetchall()\n+        text_lcs.extend(mword_terms)\n+        tok_strings = list(set(text_lcs))\n+        terms_matching_tokens_qry = self.session.query(Term).filter(\n+            Term.text_lc.in_(tok_strings), Term.language == language\n+        )\n+        all_terms = terms_matching_tokens_qry.all()\n+        # dt.step(\"exec query\")\n+\n+        return all_terms\n+\n+    ## Getting paragraphs ##############################\n+\n+    def get_textitems(self, s, language, multiword_term_indexer=None):\n+        \"\"\"\n+        Get array of TextItems for the string s.\n+\n+        The multiword_term_indexer is a big performance boost, but takes\n+        time to initialize.\n+        \"\"\"\n+        # Hacky reset of state of ParsedToken state.\n+        # _Shouldn't_ be needed but doesn't hurt, even if it's lame.\n+        ParsedToken.reset_counters()\n+\n+        cleaned = re.sub(r\" +\", \" \", s)\n+        tokens = language.get_parsed_tokens(cleaned)\n+        terms = self._find_all_terms_in_tokens(tokens, language, multiword_term_indexer)\n+        textitems = calc_get_textitems(tokens, terms, language, multiword_term_indexer)\n+        return textitems\n+\n+    def get_multiword_indexer(self, language):\n+        \"Return indexer loaded with all multiword terms.\"\n+        mw = MultiwordTermIndexer()\n+        for r in self._get_multiword_terms(language):\n+            mw.add(r[1])\n+        return mw\n+\n+    def get_paragraphs(self, s, language):\n         \"\"\"\n-    )\n-    sql = sql.bindparams(language_id=language.id)\n-    return db.session.execute(sql).all()\n-\n-\n-def _find_all_multi_word_term_text_lcs_in_content(text_lcs, language):\n-    \"Find multiword terms, return list of text_lcs.\"\n-\n-    # There are a few ways of finding multi-word Terms\n-    # (with token_count > 1) in the content:\n-    #\n-    # 1. load each mword term text_lc via sql and check.\n-    # 2. using the model\n-    # 3. SQL with \"LIKE\"\n-    #\n-    # During reasonable test runs with my data, the times in seconds\n-    # for each are similar (~0.02, ~0.05, ~0.025).  This method is\n-    # only used for small amounts of data, and the user experience hit\n-    # is negligible, so I'll use the first method which IMO is the clearest\n-    # code.\n-\n-    zws = \"\\u200B\"  # zero-width space\n-    content = zws + zws.join(text_lcs) + zws\n-\n-    # Method 1:\n-    reclist = _get_multiword_terms(language)\n-    return [p[1] for p in reclist if f\"{zws}{p[1]}{zws}\" in content]\n-\n-    ## # Method 2: use the model.\n-    ## contained_term_qry = db.session.query(Term).filter(\n-    ##     Term.language == language,\n-    ##     Term.token_count > 1,\n-    ##     func.instr(content, Term.text_lc) > 0,\n-    ## )\n-    ## return [r.text_lc for r in contained_term_qry.all()]\n-\n-    ## # Method 3: Query with LIKE\n-    ## sql = sqltext(\n-    ##     \"\"\"\n-    ##     SELECT WoTextLC FROM words\n-    ##     WHERE WoLgID=:lid and WoTokenCount>1\n-    ##     AND :content LIKE '%' || :zws || WoTextLC || :zws || '%'\n-    ##     \"\"\"\n-    ## )\n-    ## sql = sql.bindparams(lid=language.id, content=content, zws=zws)\n-    ## recs = db.session.execute(sql).all()\n-    ## return [r[0] for r in recs]\n-\n-\n-def _find_all_terms_in_tokens(tokens, language, kwtree=None):\n-    \"\"\"\n-    Find all terms contained in the (ordered) parsed tokens tokens.\n-\n-    For example\n-    - given tokens = \"Here\", \" \", \"is\", \" \", \"a\", \" \", \"cat\"\n-    - given terms in the db: [ \"cat\", \"a/ /cat\", \"dog\" ]\n-\n-    This would return the terms \"cat\" and \"a/ /cat\".\n-\n-    Method:\n-    - build list of lowercase text in the tokens\n-    - append all multword term strings that exist in the content\n-    - query for Terms that exist in the list\n-\n-    Note: this method only uses indexes for multiword terms, as any\n-    content analyzed is first parsed into tokens before being passed\n-    to this routine.  There's no need to search for single-word Terms\n-    in the tokenized strings, they can be found by a simple query.\n-    \"\"\"\n-\n-    # Performance: About half of the time in this routine is spent in\n-    # Step 1 (finding multiword terms), the rest in step 2 (the actual\n-    # query).\n-    # dt = DebugTimer(\"_find_all_terms_in_tokens\", display=True)\n-\n-    parser = language.parser\n-    text_lcs = [parser.get_lowercase(t.token) for t in tokens]\n-\n-    # Step 1: get the multiwords in the content.\n-    if kwtree is None:\n-        mword_terms = _find_all_multi_word_term_text_lcs_in_content(text_lcs, language)\n-    else:\n-        results = kwtree.search_all(text_lcs)\n-        mword_terms = [r[0] for r in results]\n-    # dt.step(\"filtered mword terms\")\n-\n-    # Step 2: load the Term objects.\n-    #\n-    # The Term fetch is actually performant -- there is no\n-    # real difference between loading the Term objects versus\n-    # loading raw data with SQL and getting dicts.\n-    #\n-    # Code for getting raw data:\n-    # param_keys = [f\"w{i}\" for i, _ in enumerate(text_lcs)]\n-    # keys_placeholders = ','.join([f\":{k}\" for k in param_keys])\n-    # param_dict = dict(zip(param_keys, text_lcs))\n-    # param_dict[\"langid\"] = language.id\n-    # sql = sqltext(f\"\"\"SELECT WoID, WoTextLC FROM words\n-    #     WHERE WoLgID=:langid and WoTextLC in ({keys_placeholders})\"\"\")\n-    # sql = sql.bindparams(language.id, *text_lcs)\n-    # results = db.session.execute(sql, param_dict).fetchall()\n-    text_lcs.extend(mword_terms)\n-    tok_strings = list(set(text_lcs))\n-    terms_matching_tokens_qry = db.session.query(Term).filter(\n-        Term.text_lc.in_(tok_strings), Term.language == language\n-    )\n-    all_terms = terms_matching_tokens_qry.all()\n-    # dt.step(\"exec query\")\n-\n-    return all_terms\n-\n-\n-## Getting paragraphs ##############################\n-\n-\n-def get_textitems(s, language, multiword_term_indexer=None):\n-    \"\"\"\n-    Get array of TextItems for the string s.\n-\n-    The multiword_term_indexer is a big performance boost, but takes\n-    time to initialize.\n-    \"\"\"\n-    # Hacky reset of state of ParsedToken state.\n-    # _Shouldn't_ be needed but doesn't hurt, even if it's lame.\n-    ParsedToken.reset_counters()\n-\n-    cleaned = re.sub(r\" +\", \" \", s)\n-    tokens = language.get_parsed_tokens(cleaned)\n-    terms = _find_all_terms_in_tokens(tokens, language, multiword_term_indexer)\n-    textitems = calc_get_textitems(tokens, terms, language, multiword_term_indexer)\n-    return textitems\n-\n-\n-def get_multiword_indexer(language):\n-    \"Return indexer loaded with all multiword terms.\"\n-    mw = MultiwordTermIndexer()\n-    for r in _get_multiword_terms(language):\n-        mw.add(r[1])\n-    return mw\n-\n-\n-def get_paragraphs(s, language):\n-    \"\"\"\n-    Get array of arrays of TextItems for the given string s.\n-\n-    This doesn't use an indexer, as it should only be used\n-    for a single page of text!\n-    \"\"\"\n-    textitems = get_textitems(s, language)\n-\n-    def _split_textitems_by_paragraph(textitems):\n-        \"Split by \u00b6\"\n-        ret = []\n-        curr_para = []\n-        for t in textitems:\n-            if t.text == \"\u00b6\":\n+        Get array of arrays of TextItems for the given string s.\n+\n+        This doesn't use an indexer, as it should only be used\n+        for a single page of text!\n+        \"\"\"\n+        textitems = self.get_textitems(s, language)\n+\n+        def _split_textitems_by_paragraph(textitems):\n+            \"Split by \u00b6\"\n+            ret = []\n+            curr_para = []\n+            for t in textitems:\n+                if t.text == \"\u00b6\":\n+                    ret.append(curr_para)\n+                    curr_para = []\n+                else:\n+                    curr_para.append(t)\n+            if len(curr_para) > 0:\n                 ret.append(curr_para)\n-                curr_para = []\n-            else:\n-                curr_para.append(t)\n-        if len(curr_para) > 0:\n-            ret.append(curr_para)\n-        return ret\n-\n-    def _split_by_sentence_number(p):\n-        sentences = [\n-            list(sentence)\n-            for _, sentence in itertools.groupby(p, key=lambda t: t.sentence_number)\n+            return ret\n+\n+        def _split_by_sentence_number(p):\n+            sentences = [\n+                list(sentence)\n+                for _, sentence in itertools.groupby(p, key=lambda t: t.sentence_number)\n+            ]\n+            for s in sentences:\n+                s[0].add_html_class(\"sentencestart\")\n+            return sentences\n+\n+        paras = [\n+            _split_by_sentence_number(list(sentences))\n+            for sentences in _split_textitems_by_paragraph(textitems)\n         ]\n-        for s in sentences:\n-            s[0].add_html_class(\"sentencestart\")\n-        return sentences\n-\n-    paras = [\n-        _split_by_sentence_number(list(sentences))\n-        for sentences in _split_textitems_by_paragraph(textitems)\n-    ]\n \n-    return paras\n+        return paras\ndiff --git a/lute/read/routes.py b/lute/read/routes.py\nindex d763a701..f3f24fad 100644\n--- a/lute/read/routes.py\n+++ b/lute/read/routes.py\n@@ -4,12 +4,13 @@\n \n from datetime import datetime\n from flask import Blueprint, flash, request, render_template, redirect, jsonify\n-from lute.read.service import set_unknowns_to_known, start_reading, get_popup_data\n+from lute.read.service import Service\n from lute.read.forms import TextForm\n from lute.term.model import Repository\n from lute.term.routes import handle_term_form\n-from lute.models.book import Book, Text\n-from lute.models.setting import UserSetting\n+from lute.settings.current import current_settings\n+from lute.models.book import Text\n+from lute.models.repositories import BookRepository\n from lute.db import db\n \n \n@@ -21,7 +22,7 @@ def _render_book_page(book, pagenum):\n     Render a particular book page.\n     \"\"\"\n     lang = book.language\n-    show_highlights = bool(int(UserSetting.get_value(\"show_highlights\")))\n+    show_highlights = current_settings[\"show_highlights\"]\n     term_dicts = lang.all_dictionaries()[lang.id][\"term\"]\n \n     return render_template(\n@@ -39,6 +40,12 @@ def _render_book_page(book, pagenum):\n     )\n \n \n+def _find_book(bookid):\n+    \"Find book from db.\"\n+    br = BookRepository(db.session)\n+    return br.find(bookid)\n+\n+\n @bp.route(\"/<int:bookid>\", methods=[\"GET\"])\n def read(bookid):\n     \"\"\"\n@@ -46,7 +53,7 @@ def read(bookid):\n \n     This is called from the book listing, on Lute index.\n     \"\"\"\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     if book is None:\n         flash(f\"No book matching id {bookid}\")\n         return redirect(\"/\", 302)\n@@ -54,7 +61,7 @@ def read(bookid):\n     page_num = 1\n     text = book.texts[0]\n     if book.current_tx_id:\n-        text = Text.find(book.current_tx_id)\n+        text = db.session.get(Text, book.current_tx_id)\n         page_num = text.order\n \n     return _render_book_page(book, page_num)\n@@ -67,7 +74,7 @@ def read_page(bookid, pagenum):\n \n     Called from term Sentences link.\n     \"\"\"\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     if book is None:\n         flash(f\"No book matching id {bookid}\")\n         return redirect(\"/\", 302)\n@@ -84,13 +91,14 @@ def page_done():\n     pagenum = int(data.get(\"pagenum\"))\n     restknown = data.get(\"restknown\")\n \n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     text = book.text_at_page(pagenum)\n     text.read_date = datetime.now()\n     db.session.add(text)\n     db.session.commit()\n+    service = Service(db.session)\n     if restknown:\n-        set_unknowns_to_known(text)\n+        service.set_unknowns_to_known(text)\n     return jsonify(\"ok\")\n \n \n@@ -99,7 +107,7 @@ def delete_page(bookid, pagenum):\n     \"\"\"\n     Delete page.\n     \"\"\"\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     if book is None:\n         flash(f\"No book matching id {bookid}\")\n         return redirect(\"/\", 302)\n@@ -119,7 +127,7 @@ def delete_page(bookid, pagenum):\n def new_page(bookid, position, pagenum):\n     \"Create a new page.\"\n     form = TextForm()\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n \n     if form.validate_on_submit():\n         t = None\n@@ -149,7 +157,7 @@ def save_player_data():\n     \"Save current player position, bookmarks.  Called on a loop by the player.\"\n     data = request.json\n     bookid = int(data.get(\"bookid\"))\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     book.audio_current_pos = float(data.get(\"position\"))\n     book.audio_bookmarks = data.get(\"bookmarks\")\n     db.session.add(book)\n@@ -160,11 +168,12 @@ def save_player_data():\n @bp.route(\"/renderpage/<int:bookid>/<int:pagenum>\", methods=[\"GET\"])\n def render_page(bookid, pagenum):\n     \"Method called by ajax, render the given page.\"\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     if book is None:\n         flash(f\"No book matching id {bookid}\")\n         return redirect(\"/\", 302)\n-    paragraphs = start_reading(book, pagenum, db.session)\n+    service = Service(db.session)\n+    paragraphs = service.start_reading(book, pagenum)\n     return render_template(\"read/page_content.html\", paragraphs=paragraphs)\n \n \n@@ -180,13 +189,14 @@ def term_form(langid, text):\n     Create a multiword term for the given text, replacing the LUTESLASH hack.\n     \"\"\"\n     usetext = text.replace(\"LUTESLASH\", \"/\")\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = repo.find_or_new(langid, usetext)\n     if term.status == 0:\n         term.status = 1\n     return handle_term_form(\n         term,\n         repo,\n+        db.session,\n         \"/read/frameform.html\",\n         render_template(\"/read/updated.html\", term_text=term.text),\n         embedded_in_reading_frame=True,\n@@ -198,7 +208,7 @@ def edit_term_form(term_id):\n     \"\"\"\n     Edit a term.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = repo.load(term_id)\n     # print(f\"editing term {term_id}\", flush=True)\n     if term.status == 0:\n@@ -206,6 +216,7 @@ def edit_term_form(term_id):\n     return handle_term_form(\n         term,\n         repo,\n+        db.session,\n         \"/read/frameform.html\",\n         render_template(\"/read/updated.html\", term_text=term.text),\n         embedded_in_reading_frame=True,\n@@ -217,7 +228,8 @@ def term_popup(termid):\n     \"\"\"\n     Get popup html for DBTerm, or None if nothing should be shown.\n     \"\"\"\n-    d = get_popup_data(termid)\n+    service = Service(db.session)\n+    d = service.get_popup_data(termid)\n     if d is None:\n         return \"\"\n     return render_template(\n@@ -240,7 +252,7 @@ def flashcopied():\n @bp.route(\"/editpage/<int:bookid>/<int:pagenum>\", methods=[\"GET\", \"POST\"])\n def edit_page(bookid, pagenum):\n     \"Edit the text on a page.\"\n-    book = Book.find(bookid)\n+    book = _find_book(bookid)\n     text = book.text_at_page(pagenum)\n     if text is None:\n         return redirect(\"/\", 302)\ndiff --git a/lute/read/service.py b/lute/read/service.py\nindex 0f72b308..7225135a 100644\n--- a/lute/read/service.py\n+++ b/lute/read/service.py\n@@ -5,182 +5,189 @@\n import functools\n from lute.models.term import Term, Status\n from lute.models.book import Text\n-from lute.book.stats import mark_stale\n-from lute.read.render.service import get_paragraphs, find_all_Terms_in_string\n+from lute.book.stats import Service as StatsService\n+from lute.read.render.service import Service as RenderService\n from lute.read.render.calculate_textitems import get_string_indexes\n from lute.term.model import Repository\n-from lute.db import db\n \n # from lute.utils.debug_helpers import DebugTimer\n \n \n-def set_unknowns_to_known(text: Text):\n-    \"\"\"\n-    Given a text, create new Terms with status Well-Known\n-    for any new Terms.\n-    \"\"\"\n-    paragraphs = get_paragraphs(text.text, text.book.language)\n-    _save_new_status_0_terms(paragraphs)\n-\n-    unknowns = [\n-        ti.term\n-        for para in paragraphs\n-        for sentence in para\n-        for ti in sentence\n-        if ti.is_word and ti.term.status == 0\n-    ]\n-\n-    batch_size = 100\n-    i = 0\n-\n-    for t in unknowns:\n-        t.status = Status.WELLKNOWN\n-        db.session.add(t)\n-        i += 1\n-        if i % batch_size == 0:\n-            db.session.commit()\n-\n-    # Commit any remaining.\n-    db.session.commit()\n-\n-\n-def bulk_status_update(text: Text, terms_text_array, new_status):\n-    \"\"\"\n-    Given a text and list of terms, update or create new terms\n-    and set the status.\n-    \"\"\"\n-    language = text.book.language\n-    repo = Repository(db)\n-    for term_text in terms_text_array:\n-        t = repo.find_or_new(language.id, term_text)\n-        t.status = new_status\n-        repo.add(t)\n-    repo.commit()\n-\n-\n-def _save_new_status_0_terms(paragraphs):\n-    \"Add status 0 terms for new textitems in paragraph.\"\n-    tis_with_new_terms = [\n-        ti\n-        for para in paragraphs\n-        for sentence in para\n-        for ti in sentence\n-        if ti.is_word and ti.term.id is None and ti.term.status == 0\n-    ]\n-\n-    for ti in tis_with_new_terms:\n-        db.session.add(ti.term)\n-    db.session.commit()\n-\n-\n-def start_reading(dbbook, pagenum, db_session):\n-    \"Start reading a page in the book, getting paragraphs.\"\n-\n-    text = dbbook.text_at_page(pagenum)\n-    text.load_sentences()\n-\n-    mark_stale(dbbook)\n-    dbbook.current_tx_id = text.id\n-    db_session.add(dbbook)\n-    db_session.add(text)\n-    db_session.commit()\n-\n-    lang = text.book.language\n-    paragraphs = get_paragraphs(text.text, lang)\n-    _save_new_status_0_terms(paragraphs)\n-\n-    return paragraphs\n-\n-\n-def get_popup_data(termid):\n-    \"Get popup data, or None if popup shouldn't be shown.\"\n-    term = Term.find(termid)\n-\n-    if term.status == Status.UNKNOWN:\n-        return None\n-\n-    def has_popup_data(cterm):\n-        return (\n-            (cterm.translation or \"\").strip() != \"\"\n-            or (cterm.romanization or \"\").strip() != \"\"\n-            or cterm.get_current_image() is not None\n-        )\n-\n-    if not has_popup_data(term) and len(term.parents) == 0:\n-        return None\n-\n-    term_tags = [tt.text for tt in term.term_tags]\n-\n-    def make_array(t):\n-        ret = {\n-            \"term\": t.text,\n-            \"roman\": t.romanization,\n-            \"trans\": t.translation if t.translation else \"-\",\n-            \"tags\": [tt.text for tt in t.term_tags],\n+class Service:\n+    \"Service.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def set_unknowns_to_known(self, text: Text):\n+        \"\"\"\n+        Given a text, create new Terms with status Well-Known\n+        for any new Terms.\n+        \"\"\"\n+        rs = RenderService(self.session)\n+        paragraphs = rs.get_paragraphs(text.text, text.book.language)\n+        self._save_new_status_0_terms(paragraphs)\n+\n+        unknowns = [\n+            ti.term\n+            for para in paragraphs\n+            for sentence in para\n+            for ti in sentence\n+            if ti.is_word and ti.term.status == 0\n+        ]\n+\n+        batch_size = 100\n+        i = 0\n+\n+        for t in unknowns:\n+            t.status = Status.WELLKNOWN\n+            self.session.add(t)\n+            i += 1\n+            if i % batch_size == 0:\n+                self.session.commit()\n+\n+        # Commit any remaining.\n+        self.session.commit()\n+\n+    def bulk_status_update(self, text: Text, terms_text_array, new_status):\n+        \"\"\"\n+        Given a text and list of terms, update or create new terms\n+        and set the status.\n+        \"\"\"\n+        language = text.book.language\n+        repo = Repository(self.session)\n+        for term_text in terms_text_array:\n+            t = repo.find_or_new(language.id, term_text)\n+            t.status = new_status\n+            repo.add(t)\n+        repo.commit()\n+\n+    def _save_new_status_0_terms(self, paragraphs):\n+        \"Add status 0 terms for new textitems in paragraph.\"\n+        tis_with_new_terms = [\n+            ti\n+            for para in paragraphs\n+            for sentence in para\n+            for ti in sentence\n+            if ti.is_word and ti.term.id is None and ti.term.status == 0\n+        ]\n+\n+        for ti in tis_with_new_terms:\n+            self.session.add(ti.term)\n+        self.session.commit()\n+\n+    def start_reading(self, dbbook, pagenum):\n+        \"Start reading a page in the book, getting paragraphs.\"\n+\n+        text = dbbook.text_at_page(pagenum)\n+        text.load_sentences()\n+\n+        svc = StatsService(self.session)\n+        svc.mark_stale(dbbook)\n+        dbbook.current_tx_id = text.id\n+        self.session.add(dbbook)\n+        self.session.add(text)\n+        self.session.commit()\n+\n+        lang = text.book.language\n+        rs = RenderService(self.session)\n+        paragraphs = rs.get_paragraphs(text.text, lang)\n+        self._save_new_status_0_terms(paragraphs)\n+\n+        return paragraphs\n+\n+    def get_popup_data(self, termid):\n+        \"Get popup data, or None if popup shouldn't be shown.\"\n+        term = self.session.get(Term, termid)\n+\n+        if term.status == Status.UNKNOWN:\n+            return None\n+\n+        def has_popup_data(cterm):\n+            return (\n+                (cterm.translation or \"\").strip() != \"\"\n+                or (cterm.romanization or \"\").strip() != \"\"\n+                or cterm.get_current_image() is not None\n+            )\n+\n+        if not has_popup_data(term) and len(term.parents) == 0:\n+            return None\n+\n+        term_tags = [tt.text for tt in term.term_tags]\n+\n+        def make_array(t):\n+            ret = {\n+                \"term\": t.text,\n+                \"roman\": t.romanization,\n+                \"trans\": t.translation if t.translation else \"-\",\n+                \"tags\": [tt.text for tt in t.term_tags],\n+            }\n+            return ret\n+\n+        parent_terms = [p.text for p in term.parents]\n+        parent_terms = \", \".join(parent_terms)\n+\n+        parents = term.parents\n+        if len(parents) == 1 and parents[0].translation == term.translation:\n+            parents = []\n+        parent_data = [make_array(p) for p in parents]\n+\n+        def sort_components(components):\n+            # Sort components by min position in string and length.\n+            component_and_pos = []\n+            for c in components:\n+                c_indices = [\n+                    loc[1] for loc in get_string_indexes([c.text_lc], term.text_lc)\n+                ]\n+\n+                # Sometimes the components aren't found\n+                # in the string, which makes no sense ...\n+                # ref https://github.com/LuteOrg/lute-v3/issues/474\n+                if len(c_indices) > 0:\n+                    component_and_pos.append([c, min(c_indices)])\n+\n+            def compare(a, b):\n+                # Lowest position (closest to front of string) sorts first.\n+                if a[1] != b[1]:\n+                    return -1 if (a[1] < b[1]) else 1\n+                # Longest sorts first.\n+                alen = len(a[0].text)\n+                blen = len(b[0].text)\n+                return -1 if (alen > blen) else 1\n+\n+            component_and_pos.sort(key=functools.cmp_to_key(compare))\n+            return [c[0] for c in component_and_pos]\n+\n+        rs = RenderService(self.session)\n+        components = [\n+            c\n+            for c in rs.find_all_Terms_in_string(term.text, term.language)\n+            if c.id != term.id\n+        ]\n+        components = sort_components(components)\n+\n+        component_data = [make_array(c) for c in components]\n+        component_data = [c for c in component_data if c[\"trans\"] != \"-\"]\n+\n+        images = [term.get_current_image()] if term.get_current_image() else []\n+        for p in term.parents:\n+            if p.get_current_image():\n+                images.append(p.get_current_image())\n+        # DISABLED CODE: Don't include component images in the hover for now,\n+        # it can get confusing!\n+        # ref https://github.com/LuteOrg/lute-v3/issues/355\n+        # for c in components:\n+        #     if c.get_current_image():\n+        #         images.append(c.get_current_image())\n+\n+        images = list(set(images))\n+\n+        return {\n+            \"term\": term,\n+            \"flashmsg\": term.get_flash_message(),\n+            \"term_tags\": term_tags,\n+            \"term_images\": images,\n+            \"parentdata\": parent_data,\n+            \"parentterms\": parent_terms,\n+            \"components\": component_data,\n         }\n-        return ret\n-\n-    parent_terms = [p.text for p in term.parents]\n-    parent_terms = \", \".join(parent_terms)\n-\n-    parents = term.parents\n-    if len(parents) == 1 and parents[0].translation == term.translation:\n-        parents = []\n-    parent_data = [make_array(p) for p in parents]\n-\n-    def sort_components(components):\n-        # Sort components by min position in string and length.\n-        component_and_pos = []\n-        for c in components:\n-            c_indices = [\n-                loc[1] for loc in get_string_indexes([c.text_lc], term.text_lc)\n-            ]\n-\n-            # Sometimes the components aren't found\n-            # in the string, which makes no sense ...\n-            # ref https://github.com/LuteOrg/lute-v3/issues/474\n-            if len(c_indices) > 0:\n-                component_and_pos.append([c, min(c_indices)])\n-\n-        def compare(a, b):\n-            # Lowest position (closest to front of string) sorts first.\n-            if a[1] != b[1]:\n-                return -1 if (a[1] < b[1]) else 1\n-            # Longest sorts first.\n-            alen = len(a[0].text)\n-            blen = len(b[0].text)\n-            return -1 if (alen > blen) else 1\n-\n-        component_and_pos.sort(key=functools.cmp_to_key(compare))\n-        return [c[0] for c in component_and_pos]\n-\n-    components = [\n-        c for c in find_all_Terms_in_string(term.text, term.language) if c.id != term.id\n-    ]\n-    components = sort_components(components)\n-\n-    component_data = [make_array(c) for c in components]\n-    component_data = [c for c in component_data if c[\"trans\"] != \"-\"]\n-\n-    images = [term.get_current_image()] if term.get_current_image() else []\n-    for p in term.parents:\n-        if p.get_current_image():\n-            images.append(p.get_current_image())\n-    # DISABLED CODE: Don't include component images in the hover for now,\n-    # it can get confusing!\n-    # ref https://github.com/LuteOrg/lute-v3/issues/355\n-    # for c in components:\n-    #     if c.get_current_image():\n-    #         images.append(c.get_current_image())\n-\n-    images = list(set(images))\n-\n-    return {\n-        \"term\": term,\n-        \"flashmsg\": term.get_flash_message(),\n-        \"term_tags\": term_tags,\n-        \"term_images\": images,\n-        \"parentdata\": parent_data,\n-        \"parentterms\": parent_terms,\n-        \"components\": component_data,\n-    }\ndiff --git a/lute/settings/current.py b/lute/settings/current.py\nnew file mode 100644\nindex 00000000..8a1bd8e2\n--- /dev/null\n+++ b/lute/settings/current.py\n@@ -0,0 +1,33 @@\n+\"\"\"\n+Current user settings stored in UserSettings.\n+\n+Storing a global dict to allow for db-less access, they're\n+global settings, after all.\n+\n+They're written to at load (or when the settings change).\n+\"\"\"\n+\n+from lute.models.setting import UserSetting\n+\n+# The current user settings, key/value dict.\n+current_settings = {}\n+\n+\n+def refresh_global_settings(session):\n+    \"Refresh all settings dictionary.\"\n+    # Have to reload to not mess up any references\n+    # (e.g. during testing).\n+    current_settings.clear()\n+\n+    settings = session.query(UserSetting).all()\n+    for s in settings:\n+        current_settings[s.key] = s.value\n+\n+    # Convert some ints into bools.\n+    boolkeys = [\n+        \"open_popup_in_new_tab\",\n+        \"stop_audio_on_term_form_open\",\n+        \"show_highlights\",\n+    ]\n+    for k in boolkeys:\n+        current_settings[k] = current_settings[k] == \"1\"\ndiff --git a/lute/settings/forms.py b/lute/settings/forms.py\nnew file mode 100644\nindex 00000000..9d7284fe\n--- /dev/null\n+++ b/lute/settings/forms.py\n@@ -0,0 +1,80 @@\n+\"\"\"\n+Settings form.\n+\"\"\"\n+\n+import os\n+from flask_wtf import FlaskForm\n+from wtforms import BooleanField, StringField, IntegerField, TextAreaField, SelectField\n+from wtforms.validators import InputRequired, NumberRange\n+from wtforms import ValidationError\n+\n+\n+class UserSettingsForm(FlaskForm):\n+    \"\"\"\n+    Settings.\n+\n+    Note the field names here must match the keys in the settings table.\n+    \"\"\"\n+\n+    backup_enabled = BooleanField(\"Backup Enabled\")\n+    backup_dir = StringField(\"Backup directory\")\n+    backup_auto = BooleanField(\"Run backups automatically (daily)\")\n+    backup_warn = BooleanField(\"Warn if backup hasn't run in a week\")\n+    backup_count = IntegerField(\n+        \"Retain backup count\",\n+        validators=[InputRequired(), NumberRange(min=1)],\n+        render_kw={\n+            \"title\": \"Count of zipfiles to retain, oldest files are deleted first\"\n+        },\n+    )\n+\n+    current_theme = SelectField(\"Theme\")\n+    custom_styles = TextAreaField(\"Custom styles\")\n+    show_highlights = BooleanField(\"Highlight terms by status\")\n+\n+    open_popup_in_new_tab = BooleanField(\"Open popup in new tab\")\n+    stop_audio_on_term_form_open = BooleanField(\"Stop audio on term form open\")\n+    stats_calc_sample_size = IntegerField(\n+        \"Book stats page sample size\",\n+        validators=[InputRequired(), NumberRange(min=1, max=500)],\n+        render_kw={\"title\": \"Number of pages to use for book stats calculation.\"},\n+    )\n+\n+    mecab_path = StringField(\"MECAB_PATH environment variable\")\n+    reading_choices = [\n+        (\"katakana\", \"Katakana\"),\n+        (\"hiragana\", \"Hiragana\"),\n+        (\"alphabet\", \"Romaji\"),\n+    ]\n+    japanese_reading = SelectField(\"Pronunciation characters\", choices=reading_choices)\n+\n+    def validate_backup_dir(self, field):\n+        \"Field must be set if enabled.\"\n+        if self.backup_enabled.data is False:\n+            return\n+        v = field.data\n+        if (v or \"\").strip() == \"\":\n+            raise ValidationError(\"Backup directory required\")\n+\n+        abspath = os.path.abspath(v)\n+        if v != abspath:\n+            msg = f'Backup dir must be absolute path.  Did you mean \"{abspath}\"?'\n+            raise ValidationError(msg)\n+        if not os.path.exists(v):\n+            raise ValidationError(f'Directory \"{v}\" does not exist.')\n+        if not os.path.isdir(v):\n+            raise ValidationError(f'\"{v}\" is not a directory.')\n+\n+\n+class UserShortcutsForm(FlaskForm):\n+    \"\"\"\n+    Shortcuts form.\n+\n+    The route manages getting and storing the settings\n+    from the db, as there's a variable number of settings,\n+    and it's easier to just work with the data directly\n+    rather than trying to create a variable number of fields.\n+\n+    I'm only using this form to get the validate_on_submit()!\n+    There's likely a better way to do this.\n+    \"\"\"\ndiff --git a/lute/settings/routes.py b/lute/settings/routes.py\nindex 43d61432..e88254b5 100644\n--- a/lute/settings/routes.py\n+++ b/lute/settings/routes.py\n@@ -2,7 +2,6 @@\n Settings routes.\n \"\"\"\n \n-import os\n from flask import (\n     Blueprint,\n     current_app,\n@@ -12,74 +11,17 @@\n     flash,\n     jsonify,\n )\n-from flask_wtf import FlaskForm\n-from wtforms import BooleanField, StringField, IntegerField, TextAreaField, SelectField\n-from wtforms.validators import InputRequired, NumberRange\n-from wtforms import ValidationError\n+from wtforms import BooleanField\n from lute.models.language import Language\n from lute.models.setting import UserSetting\n-from lute.themes.service import list_themes\n+from lute.models.repositories import UserSettingRepository\n+from lute.themes.service import Service as ThemeService\n+from lute.settings.forms import UserSettingsForm, UserShortcutsForm\n+from lute.settings.current import refresh_global_settings\n from lute.db import db\n from lute.parse.mecab_parser import JapaneseParser\n \n \n-class UserSettingsForm(FlaskForm):\n-    \"\"\"\n-    Settings.\n-\n-    Note the field names here must match the keys in the settings table.\n-    \"\"\"\n-\n-    backup_enabled = BooleanField(\"Backup Enabled\")\n-    backup_dir = StringField(\"Backup directory\")\n-    backup_auto = BooleanField(\"Run backups automatically (daily)\")\n-    backup_warn = BooleanField(\"Warn if backup hasn't run in a week\")\n-    backup_count = IntegerField(\n-        \"Retain backup count\",\n-        validators=[InputRequired(), NumberRange(min=1)],\n-        render_kw={\n-            \"title\": \"Count of zipfiles to retain, oldest files are deleted first\"\n-        },\n-    )\n-\n-    current_theme = SelectField(\"Theme\")\n-    custom_styles = TextAreaField(\"Custom styles\")\n-    show_highlights = BooleanField(\"Highlight terms by status\")\n-\n-    open_popup_in_new_tab = BooleanField(\"Open popup in new tab\")\n-    stop_audio_on_term_form_open = BooleanField(\"Stop audio on term form open\")\n-    stats_calc_sample_size = IntegerField(\n-        \"Book stats page sample size\",\n-        validators=[InputRequired(), NumberRange(min=1, max=500)],\n-        render_kw={\"title\": \"Number of pages to use for book stats calculation.\"},\n-    )\n-\n-    mecab_path = StringField(\"MECAB_PATH environment variable\")\n-    reading_choices = [\n-        (\"katakana\", \"Katakana\"),\n-        (\"hiragana\", \"Hiragana\"),\n-        (\"alphabet\", \"Romaji\"),\n-    ]\n-    japanese_reading = SelectField(\"Pronunciation characters\", choices=reading_choices)\n-\n-    def validate_backup_dir(self, field):\n-        \"Field must be set if enabled.\"\n-        if self.backup_enabled.data is False:\n-            return\n-        v = field.data\n-        if (v or \"\").strip() == \"\":\n-            raise ValidationError(\"Backup directory required\")\n-\n-        abspath = os.path.abspath(v)\n-        if v != abspath:\n-            msg = f'Backup dir must be absolute path.  Did you mean \"{abspath}\"?'\n-            raise ValidationError(msg)\n-        if not os.path.exists(v):\n-            raise ValidationError(f'Directory \"{v}\" does not exist.')\n-        if not os.path.isdir(v):\n-            raise ValidationError(f'\"{v}\" is not a directory.')\n-\n-\n bp = Blueprint(\"settings\", __name__, url_prefix=\"/settings\")\n \n \n@@ -89,7 +31,8 @@ def edit_settings():\n     form = UserSettingsForm()\n \n     with current_app.app_context():\n-        form.current_theme.choices = list_themes()\n+        svc = ThemeService(db.session)\n+        form.current_theme.choices = svc.list_themes()\n \n     ac = current_app.env_config\n     if ac.is_docker:\n@@ -98,12 +41,14 @@ def edit_settings():\n         # Backup dir gets mounted from host.\n         form.backup_dir.render_kw = kw\n \n+    repo = UserSettingRepository(db.session)\n     if form.validate_on_submit():\n         # Update the settings in the database\n         for field in form:\n             if field.id not in (\"csrf_token\", \"submit\"):\n-                UserSetting.set_value(field.id, field.data)\n+                repo.set_value(field.id, field.data)\n         db.session.commit()\n+        refresh_global_settings(db.session)\n \n         flash(\"Settings updated\", \"success\")\n         return redirect(\"/\")\n@@ -111,7 +56,7 @@ def edit_settings():\n     # Load current settings from the database\n     for field in form:\n         if field.id != \"csrf_token\":\n-            field.data = UserSetting.get_value(field.id)\n+            field.data = repo.get_value(field.id)\n         if isinstance(field, BooleanField):\n             # Hack: set boolean settings to ints, otherwise they're always checked.\n             field.data = int(field.data or 0)\n@@ -129,10 +74,11 @@ def test_parse():\n \n     \"\"\"\n     mecab_path = request.args.get(\"mecab_path\", None)\n-    old_setting = UserSetting.get_value(\"mecab_path\")\n+    repo = UserSettingRepository(db.session)\n+    old_setting = repo.get_value(\"mecab_path\")\n     result = {\"failure\": \"tbd\"}\n     try:\n-        UserSetting.set_value(\"mecab_path\", mecab_path)\n+        repo.set_value(\"mecab_path\", mecab_path)\n         # Parsing requires a language, even if it's a dummy.\n         lang = Language()\n         p = JapaneseParser()\n@@ -145,7 +91,7 @@ def test_parse():\n         message = f\"{type(e).__name__}: { str(e) }\"\n         result = {\"result\": \"failure\", \"message\": message}\n     finally:\n-        UserSetting.set_value(\"mecab_path\", old_setting)\n+        repo.set_value(\"mecab_path\", old_setting)\n \n     return jsonify(result)\n \n@@ -153,32 +99,20 @@ def test_parse():\n @bp.route(\"/set/<key>/<value>\", methods=[\"POST\"])\n def set_key_value(key, value):\n     \"Set a UserSetting key to value.\"\n-    old_value = UserSetting.get_value(key)\n+    repo = UserSettingRepository(db.session)\n+    old_value = repo.get_value(key)\n     try:\n-        UserSetting.set_value(key, value)\n+        repo.set_value(key, value)\n         result = {\"result\": \"success\", \"message\": \"OK\"}\n     except Exception as e:  # pylint: disable=broad-exception-caught\n         message = f\"{type(e).__name__}: { str(e) }\"\n-        UserSetting.set_value(key, old_value)\n+        repo.set_value(key, old_value)\n         result = {\"result\": \"failure\", \"message\": message}\n     db.session.commit()\n+    refresh_global_settings(db.session)\n     return jsonify(result)\n \n \n-class UserShortcutsForm(FlaskForm):\n-    \"\"\"\n-    Shortcuts form.\n-\n-    The route manages getting and storing the settings\n-    from the db, as there's a variable number of settings,\n-    and it's easier to just work with the data directly\n-    rather than trying to create a variable number of fields.\n-\n-    I'm only using this form to get the validate_on_submit()!\n-    There's likely a better way to do this.\n-    \"\"\"\n-\n-\n def _get_categorized_hotkeys():\n     \"\"\"\n     Return hotkey UserSetting keys and values,\n@@ -236,14 +170,16 @@ def _get_categorized_hotkeys():\n @bp.route(\"/shortcuts\", methods=[\"GET\", \"POST\"])\n def edit_shortcuts():\n     \"Edit shortcuts.\"\n+    repo = UserSettingRepository(db.session)\n     form = UserShortcutsForm()\n     if form.validate_on_submit():\n         # print(request.form, flush=True)\n         # Update the settings in the database\n         for k, v in request.form.items():\n             # print(f\"{k} = {v}\", flush=True)\n-            UserSetting.set_value(k, v)\n+            repo.set_value(k, v)\n         db.session.commit()\n+        refresh_global_settings(db.session)\n         flash(\"Shortcuts updated\", \"success\")\n         return redirect(\"/\")\n \ndiff --git a/lute/stats/routes.py b/lute/stats/routes.py\nindex d2ecfe8f..25f577d0 100644\n--- a/lute/stats/routes.py\n+++ b/lute/stats/routes.py\n@@ -4,6 +4,7 @@\n \n from flask import Blueprint, render_template, jsonify\n from lute.stats.service import get_chart_data, get_table_data\n+from lute.db import db\n \n bp = Blueprint(\"stats\", __name__, url_prefix=\"/stats\")\n \n@@ -11,12 +12,12 @@\n @bp.route(\"/\")\n def index():\n     \"Main page.\"\n-    read_table_data = get_table_data()\n+    read_table_data = get_table_data(db.session)\n     return render_template(\"stats/index.html\", read_table_data=read_table_data)\n \n \n @bp.route(\"/data\")\n def get_data():\n     \"Ajax call.\"\n-    chartdata = get_chart_data()\n+    chartdata = get_chart_data(db.session)\n     return jsonify(chartdata)\ndiff --git a/lute/stats/service.py b/lute/stats/service.py\nindex 9613796b..146475f7 100644\n--- a/lute/stats/service.py\n+++ b/lute/stats/service.py\n@@ -4,10 +4,9 @@\n \n from datetime import datetime, timedelta\n from sqlalchemy import text\n-from lute.db import db\n \n \n-def _get_data_per_lang():\n+def _get_data_per_lang(session):\n     \"Return dict of lang name to dict[date_yyyymmdd}: count\"\n     ret = {}\n     sql = \"\"\"\n@@ -22,7 +21,7 @@ def _get_data_per_lang():\n     ) raw\n     group by lang, dt\n     \"\"\"\n-    result = db.session.execute(text(sql)).all()\n+    result = session.execute(text(sql)).all()\n     for row in result:\n         langname = row[0]\n         if langname not in ret:\n@@ -53,9 +52,9 @@ def _charting_data(readbydate):\n     return data\n \n \n-def get_chart_data():\n+def get_chart_data(session):\n     \"Get data for chart for each language.\"\n-    raw_data = _get_data_per_lang()\n+    raw_data = _get_data_per_lang(session)\n     chartdata = {}\n     for k, v in raw_data.items():\n         chartdata[k] = _charting_data(v)\n@@ -90,9 +89,9 @@ def _in_range(i):\n     }\n \n \n-def get_table_data():\n+def get_table_data(session):\n     \"Wordcounts by lang in time intervals.\"\n-    raw_data = _get_data_per_lang()\n+    raw_data = _get_data_per_lang(session)\n \n     ret = []\n     for langname, readbydate in raw_data.items():\ndiff --git a/lute/templates/term/_form.html b/lute/templates/term/_form.html\nindex 54da14be..da9cc588 100644\n--- a/lute/templates/term/_form.html\n+++ b/lute/templates/term/_form.html\n@@ -23,7 +23,7 @@\n     {{ form.hidden_tag() }}\n     <div id=\"term\">\n       <div id=\"languageSel\"\n-        {% if term.language_id is none and term.language is none %}\n+        {% if term.language_id is none %}\n         {# show the language select box. #}\n         {% else %}\n         style=\"display:none;\"\ndiff --git a/lute/term/datatables.py b/lute/term/datatables.py\nindex a07bfba6..36b82161 100644\n--- a/lute/term/datatables.py\n+++ b/lute/term/datatables.py\n@@ -2,11 +2,10 @@\n Show terms in datatables.\n \"\"\"\n \n-from lute.db import db\n from lute.utils.data_tables import DataTablesSqliteQuery, supported_parser_type_criteria\n \n \n-def get_data_tables_list(parameters):\n+def get_data_tables_list(parameters, session):\n     \"Term json data for datatables.\"\n \n     base_sql = \"\"\"SELECT\n@@ -93,5 +92,5 @@ def get_data_tables_list(parameters):\n \n     # Phew.\n     return DataTablesSqliteQuery.get_data(\n-        base_sql + \" WHERE \" + \" AND \".join(wheres), parameters, db.session.connection()\n+        base_sql + \" WHERE \" + \" AND \".join(wheres), parameters, session.connection()\n     )\ndiff --git a/lute/term/forms.py b/lute/term/forms.py\nindex 7fe3ee88..c5f0e9d3 100644\n--- a/lute/term/forms.py\n+++ b/lute/term/forms.py\n@@ -16,8 +16,8 @@\n from wtforms import ValidationError\n from wtforms.validators import DataRequired\n \n-from lute.models.language import Language\n from lute.models.term import Term\n+from lute.models.repositories import LanguageRepository, TermRepository\n \n \n class TermForm(FlaskForm):\n@@ -63,7 +63,8 @@ class TermForm(FlaskForm):\n     def __init__(self, *args, **kwargs):\n         \"Call the constructor of the superclass (FlaskForm)\"\n         super().__init__(*args, **kwargs)\n-        term = kwargs.get(\"obj\")\n+        term = kwargs[\"obj\"]\n+        self.session = kwargs[\"session\"]\n \n         def _data(arr):\n             \"Get data in proper format for tagify.\"\n@@ -102,7 +103,8 @@ def validate_text(self, field):  # pylint: disable=unused-argument\n         if self.language_id.data in (None, 0):\n             return\n         langid = int(self.language_id.data)\n-        lang = Language.find(langid)\n+        language_repo = LanguageRepository(self.session)\n+        lang = language_repo.find(langid)\n         if lang is None:\n             return\n \n@@ -110,7 +112,8 @@ def validate_text(self, field):  # pylint: disable=unused-argument\n         if orig_text in (\"\", None):\n             # New term - throw if already exists.\n             spec = Term(lang, self.text.data)\n-            self.duplicated_term = Term.find_by_spec(spec)\n+            term_repo = TermRepository(self.session)\n+            self.duplicated_term = term_repo.find_by_spec(spec)\n             if self.duplicated_term is not None:\n                 raise ValidationError(\"Term already exists\")\n \ndiff --git a/lute/term/model.py b/lute/term/model.py\nindex dc89064c..8c03e49b 100644\n--- a/lute/term/model.py\n+++ b/lute/term/model.py\n@@ -9,7 +9,11 @@\n import sqlalchemy\n \n from lute.models.term import Term as DBTerm, TermTag\n-from lute.models.language import Language\n+from lute.models.repositories import (\n+    LanguageRepository,\n+    TermRepository,\n+    TermTagRepository,\n+)\n \n \n class Term:  # pylint: disable=too-many-instance-attributes\n@@ -20,12 +24,6 @@ class Term:  # pylint: disable=too-many-instance-attributes\n     def __init__(self):\n         # The ID of the DBTerm.\n         self.id = None\n-        # A language object is required as the Term bus. object\n-        # must downcase the text and the original_text to see\n-        # if anything has changed.\n-        self._language = None\n-        # Ideally this wouldn't be needed, but the term form\n-        # populates this field with the (primitive) language id.\n         self.language_id = None\n         # The text.\n         self.text = None\n@@ -46,19 +44,6 @@ def __repr__(self):\n             f'<Term BO \"{self.text}\" lang_id={self.language_id} lang={self.language}>'\n         )\n \n-    @property\n-    def language(self):\n-        \"Use or get the language.\"\n-        if self._language is not None:\n-            return self._language\n-        return Language.find(self.language_id)\n-\n-    @language.setter\n-    def language(self, lang):\n-        if not isinstance(lang, Language):\n-            raise ValueError(\"not a language\")\n-        self._language = lang\n-\n \n class TermReference:\n     \"Where a Term has been used in books.\"\n@@ -78,8 +63,8 @@ class Repository:\n     Maps Term BO to and from lute.model.Term.\n     \"\"\"\n \n-    def __init__(self, _db):\n-        self.db = _db\n+    def __init__(self, _session):\n+        self.session = _session\n \n         # Identity map for business lookup.\n         # Note that the same term is stored\n@@ -105,13 +90,29 @@ def _search_identity_map(self, langid, txt):\n \n     def load(self, term_id):\n         \"Loads a Term business object for the DBTerm with the id.\"\n-        dbt = DBTerm.find(term_id)\n+        dbt = self.session.get(DBTerm, term_id)\n         if dbt is None:\n             raise ValueError(f\"No term with id {term_id} found\")\n         term = self._build_business_term(dbt)\n         self._add_to_identity_map(term)\n         return term\n \n+    def _search_spec_term(self, langid, text):\n+        \"\"\"\n+        Make a term to get the correct text_lc to search for.\n+        This ensures that the spec term is properly parsed\n+        and downcased.\n+        \"\"\"\n+        lang_repo = LanguageRepository(self.session)\n+        lang = lang_repo.find(langid)\n+        return DBTerm(lang, text)\n+\n+    def _find_by_spec(self, langid, text):\n+        \"Do a search using a spec term.\"\n+        spec = self._search_spec_term(langid, text)\n+        repo = TermRepository(self.session)\n+        return repo.find_by_spec(spec)\n+\n     def find(self, langid, text):\n         \"\"\"\n         Return a Term business object for the DBTerm with the langid and text.\n@@ -121,8 +122,7 @@ def find(self, langid, text):\n         if term is not None:\n             return term\n \n-        spec = self._search_spec_term(langid, text)\n-        dbt = DBTerm.find_by_spec(spec)\n+        dbt = self._find_by_spec(langid, text)\n         if dbt is None:\n             return None\n         term = self._build_business_term(dbt)\n@@ -168,7 +168,6 @@ def find_or_new(self, langid, text):\n \n         spec = self._search_spec_term(langid, text)\n         t = Term()\n-        t.language = spec.language\n         t.language_id = langid\n         t.text = spec.text\n         t.text_lc = spec.text_lc\n@@ -227,11 +226,11 @@ def find_matches(self, langid, text, max_results=50):\n         # print(params)\n \n         alchsql = sqlalchemy.text(sql_query)\n-        return self.db.session.execute(alchsql, params).fetchall()\n+        return self.session.execute(alchsql, params).fetchall()\n \n     def get_term_tags(self):\n         \"Get all available term tags, helper method.\"\n-        tags = self.db.session.query(TermTag).all()\n+        tags = self.session.query(TermTag).all()\n         return [t.text for t in tags]\n \n     def add(self, term):\n@@ -241,7 +240,7 @@ def add(self, term):\n         clients should not change it.\n         \"\"\"\n         dbterm = self._build_db_term(term)\n-        self.db.session.add(dbterm)\n+        self.session.add(dbterm)\n         return dbterm\n \n     def delete(self, term):\n@@ -250,30 +249,18 @@ def delete(self, term):\n         \"\"\"\n         dbt = None\n         if term.id is not None:\n-            dbt = DBTerm.find(term.id)\n+            dbt = self.session.get(DBTerm, term.id)\n         else:\n-            spec = self._search_spec_term(term.language_id, term.text)\n-            dbt = DBTerm.find_by_spec(spec)\n+            dbt = self._find_by_spec(term.language_id, term.text)\n         if dbt is not None:\n-            self.db.session.delete(dbt)\n+            self.session.delete(dbt)\n \n     def commit(self):\n         \"\"\"\n         Commit everything, flush the map to force refetches.\n         \"\"\"\n         self.identity_map = {}\n-        self.db.session.commit()\n-\n-    def _search_spec_term(self, langid, text):\n-        \"\"\"\n-        Make a term to get the correct text_lc to search for.\n-\n-        Creating a term does parsing and correct downcasing,\n-        so term.language.id and term.text_lc match what the\n-        db would contain.\n-        \"\"\"\n-        lang = Language.find(langid)\n-        return DBTerm(lang, text)\n+        self.session.commit()\n \n     def _build_db_term(self, term):\n         \"Convert a term business object to a DBTerm.\"\n@@ -284,11 +271,12 @@ def _build_db_term(self, term):\n         t = None\n         if term.id is not None:\n             # This is an existing term, so use it directly.\n-            t = DBTerm.find(term.id)\n+            t = self.session.get(DBTerm, term.id)\n         else:\n             # New term, or finding by text.\n             spec = self._search_spec_term(term.language_id, term.text)\n-            t = DBTerm.find_by_spec(spec) or DBTerm()\n+            term_repo = TermRepository(self.session)\n+            t = term_repo.find_by_spec(spec) or DBTerm()\n             t.language = spec.language\n \n         t.text = term.text\n@@ -304,9 +292,10 @@ def _build_db_term(self, term):\n         else:\n             t.pop_flash_message()\n \n+        tt_repo = TermTagRepository(self.session)\n         termtags = []\n         for s in list(set(term.term_tags)):\n-            termtags.append(TermTag.find_or_create_by_text(s))\n+            termtags.append(tt_repo.find_or_create_by_text(s))\n         t.remove_all_term_tags()\n         for tt in termtags:\n             t.add_term_tag(tt)\n@@ -334,9 +323,7 @@ def _build_db_term(self, term):\n         return t\n \n     def _find_or_create_parent(self, pt, language, term, termtags) -> DBTerm:\n-        spec = self._search_spec_term(language.id, pt)\n-        p = DBTerm.find_by_spec(spec)\n-\n+        p = self._find_by_spec(language.id, pt)\n         new_or_unknown_parent = p is None or p.status == 0\n         new_term = term.id is None\n \n@@ -366,7 +353,6 @@ def _build_business_term(self, dbterm):\n         \"Create a Term bus. object from a lute.model.term.Term.\"\n         term = Term()\n         term.id = dbterm.id\n-        term.language = dbterm.language\n         term.language_id = dbterm.language.id\n \n         text = dbterm.text\n@@ -404,7 +390,8 @@ def find_references(self, term):\n         Return references of term, children, and parents.\n         \"\"\"\n         spec = self._search_spec_term(term.language_id, term.text)\n-        searchterm = DBTerm.find_by_spec(spec)\n+        term_repo = TermRepository(self.session)\n+        searchterm = term_repo.find_by_spec(spec)\n         if searchterm is None:\n             searchterm = spec\n \n@@ -460,7 +447,7 @@ def _get_references(self, term):\n \n         pattern = f\"%{chr(0x200B)}{term_lc}{chr(0x200B)}%\"\n         params = {\"pattern\": pattern}\n-        result = self.db.session.execute(query, params)\n+        result = self.session.execute(query, params)\n         return self._build_term_references(term_lc, result)\n \n     def _get_all_refs(self, terms):\ndiff --git a/lute/term/routes.py b/lute/term/routes.py\nindex 804ebecc..6430495d 100644\n--- a/lute/term/routes.py\n+++ b/lute/term/routes.py\n@@ -14,8 +14,12 @@\n     send_file,\n )\n from lute.models.language import Language\n-from lute.models.term import Term as DBTerm, Status\n-from lute.models.setting import UserSetting\n+from lute.models.term import Status\n+from lute.models.repositories import (\n+    LanguageRepository,\n+    TermRepository,\n+    UserSettingRepository,\n+)\n from lute.utils.data_tables import DataTablesFlaskParamParser\n from lute.term.datatables import get_data_tables_list\n from lute.term.model import Repository, Term\n@@ -30,11 +34,12 @@\n @bp.route(\"/index/<search>\", methods=[\"GET\"])\n def index(search):\n     \"Index page.\"\n-    DBTerm.delete_empty_images()\n+    repo = TermRepository(db.session)\n+    repo.delete_empty_images()\n     languages = db.session.query(Language).order_by(Language.name).all()\n     langopts = [(lang.id, lang.name) for lang in languages]\n     langopts = [(0, \"(all)\")] + langopts\n-    statuses = [s for s in Status.all() if s.id != Status.UNKNOWN]\n+    statuses = [s for s in db.session.query(Status).all() if s.id != Status.UNKNOWN]\n     return render_template(\n         \"term/index.html\",\n         initial_search=search,\n@@ -64,7 +69,7 @@ def datatables_active_source():\n     \"Datatables data for terms.\"\n     parameters = DataTablesFlaskParamParser.parse_params(request.form)\n     _load_term_custom_filters(request.form, parameters)\n-    data = get_data_tables_list(parameters)\n+    data = get_data_tables_list(parameters, db.session)\n     return jsonify(data)\n \n \n@@ -75,7 +80,7 @@ def export_terms():\n     _load_term_custom_filters(request.form, parameters)\n     parameters[\"length\"] = 1000000\n     outfile = os.path.join(current_app.env_config.temppath, \"export_terms.csv\")\n-    data = get_data_tables_list(parameters)\n+    data = get_data_tables_list(parameters, db.session)\n     term_data = data[\"data\"]\n \n     # Term data is an array of dicts, with the sql field name as dict\n@@ -106,8 +111,13 @@ def export_terms():\n \n \n def handle_term_form(\n-    term, repo, form_template_name, return_on_success, embedded_in_reading_frame=False\n-):\n+    term,\n+    repo,\n+    session,\n+    form_template_name,\n+    return_on_success,\n+    embedded_in_reading_frame=False,\n+):  # pylint: disable=too-many-arguments\n     \"\"\"\n     Handle a form post.\n \n@@ -115,16 +125,13 @@ def handle_term_form(\n     lives in an iframe in the reading frames and returns a different\n     template on success.\n     \"\"\"\n-    # print(f\"in handle_term_form with term.id = {term.id}\", flush=True)\n-    form = TermForm(obj=term)\n-    # parents = [{\"value\": p} for p in term.parents]\n-    # form.parentslist.data = json.dumps(parents)\n+    form = TermForm(obj=term, session=session)\n \n     # Flash messages get added on things like term imports.\n     # The user opening the form is treated as an acknowledgement.\n     term.flash_message = None\n \n-    form.language_id.choices = lute.utils.formutils.language_choices()\n+    form.language_id.choices = lute.utils.formutils.language_choices(session)\n \n     if form.validate_on_submit():\n         form.populate_obj(term)\n@@ -136,8 +143,10 @@ def handle_term_form(\n     # See DUPLICATE_TERM_CHECK comments in other files.\n \n     hide_pronunciation = False\n-    term_language = term._language  # pylint: disable=protected-access\n-\n+    language_repo = LanguageRepository(session)\n+    term_language = language_repo.find(\n+        term.language_id or -1\n+    )  # -1 hack for no lang set.\n     if term_language is not None:\n         hide_pronunciation = not term_language.show_romanization\n \n@@ -149,7 +158,8 @@ def handle_term_form(\n     else:\n         # The language select control is shown and this is a new term,\n         # so use the default value.\n-        current_language_id = int(UserSetting.get_value(\"current_language_id\"))\n+        us_repo = UserSettingRepository(db.session)\n+        current_language_id = int(us_repo.get_value(\"current_language_id\"))\n         form.language_id.data = current_language_id\n \n     return render_template(\n@@ -169,7 +179,7 @@ def _handle_form(term, repo, redirect_to=\"/term/index\"):\n     Handle the form post, redirecting to specified url.\n     \"\"\"\n     return handle_term_form(\n-        term, repo, \"/term/formframes.html\", redirect(redirect_to, 302)\n+        term, repo, db.session, \"/term/formframes.html\", redirect(redirect_to, 302)\n     )\n \n \n@@ -178,7 +188,7 @@ def edit(termid):\n     \"\"\"\n     Edit a term.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = repo.load(termid)\n     if term.status == 0:\n         term.status = 1\n@@ -190,7 +200,7 @@ def edit_by_text(langid, text):\n     \"\"\"\n     Edit a term.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = repo.find_or_new(langid, text)\n     if term.status == 0:\n         term.status = 1\n@@ -202,7 +212,7 @@ def new():\n     \"\"\"\n     Create a term.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = Term()\n     return _handle_form(term, repo, \"/term/new\")\n \n@@ -212,7 +222,7 @@ def search_by_text_in_language(text, langid):\n     \"JSON data for parent data.\"\n     if text.strip() == \"\" or langid == 0:\n         return []\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     matches = repo.find_matches(langid, text)\n \n     def _make_entry(t):\n@@ -230,7 +240,7 @@ def _make_entry(t):\n @bp.route(\"/sentences/<int:langid>/<text>\", methods=[\"GET\"])\n def sentences(langid, text):\n     \"Get sentences for terms.\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     # Use find_or_new(): if the user clicks on a parent tag\n     # in the term form, and the parent does not exist yet, then\n     # we're creating a new term.\n@@ -262,7 +272,7 @@ def bulk_update_status():\n       updates: [ { new_status: 1, termids: [ 42, ] }, ... }, ]\n     }\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n \n     data = request.get_json()\n     updates = data.get(\"updates\")\n@@ -285,7 +295,7 @@ def bulk_set_parent():\n     termids = data.get(\"wordids\")\n     parenttext = data.get(\"parenttext\")\n     parent = None\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     for tid in termids:\n         term = repo.load(int(tid))\n         if parent is None:\n@@ -304,7 +314,7 @@ def bulk_delete():\n     \"Delete terms.\"\n     data = request.get_json()\n     termids = data.get(\"wordids\")\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     for tid in termids:\n         term = repo.load(int(tid))\n         repo.delete(term)\n@@ -317,7 +327,7 @@ def delete(termid):\n     \"\"\"\n     Delete a term.\n     \"\"\"\n-    repo = Repository(db)\n+    repo = Repository(db.session)\n     term = repo.load(termid)\n     repo.delete(term)\n     repo.commit()\ndiff --git a/lute/term_parent_map/routes.py b/lute/term_parent_map/routes.py\nindex 61a8b821..5a5a2d33 100644\n--- a/lute/term_parent_map/routes.py\n+++ b/lute/term_parent_map/routes.py\n@@ -10,7 +10,7 @@\n from lute.db import db\n from lute.models.book import Book\n from lute.models.language import Language\n-from lute.term_parent_map.service import export_unknown_terms\n+from lute.term_parent_map.service import Service\n \n \n bp = Blueprint(\"term_parent_map\", __name__, url_prefix=\"/term_parent_map\")\n@@ -42,5 +42,6 @@ def export_book(bookid):\n     \"Generate a file and return it.\"\n     outfile = os.path.join(current_app.env_config.temppath, \"export_book.txt\")\n     book = db.session.get(Book, bookid)\n-    export_unknown_terms(book, outfile)\n+    service = Service(db.session)\n+    service.export_unknown_terms(book, outfile)\n     return send_file(outfile, as_attachment=True, download_name=\"unknown_terms.txt\")\ndiff --git a/lute/term_parent_map/service.py b/lute/term_parent_map/service.py\nindex e85897c7..25c82237 100644\n--- a/lute/term_parent_map/service.py\n+++ b/lute/term_parent_map/service.py\n@@ -2,30 +2,38 @@\n Term parent mapping.\n \"\"\"\n \n-from lute.db import db\n from lute.models.term import Term\n \n \n ## Exports\n \n \n-# TODO issue_336_export_unknown_book_terms: move this where needed.\n-def export_unknown_terms(book, outfile):\n-    \"Export unknown terms in the book to outfile.\"\n-    lang = book.language\n-    unique_tokens = {\n-        t for txt in book.texts for t in lang.get_parsed_tokens(txt.text) if t.is_word\n-    }\n-    unique_lcase_toks = {lang.get_lowercase(t.token) for t in unique_tokens}\n-\n-    lgid = lang.id\n-    known_terms_lc = (\n-        db.session.query(Term.text_lc)\n-        .filter(Term.language_id == lgid, Term.token_count == 1)\n-        .all()\n-    )\n-    known_terms_lc = [word[0] for word in known_terms_lc]\n-\n-    newtoks = [t for t in unique_lcase_toks if t not in known_terms_lc]\n-    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n-        f.write(\"\\n\".join(newtoks))\n+class Service:\n+    \"Service.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    # TODO issue_336_export_unknown_book_terms: move this where needed.\n+    def export_unknown_terms(self, book, outfile):\n+        \"Export unknown terms in the book to outfile.\"\n+        lang = book.language\n+        unique_tokens = {\n+            t\n+            for txt in book.texts\n+            for t in lang.get_parsed_tokens(txt.text)\n+            if t.is_word\n+        }\n+        unique_lcase_toks = {lang.get_lowercase(t.token) for t in unique_tokens}\n+\n+        lgid = lang.id\n+        known_terms_lc = (\n+            self.session.query(Term.text_lc)\n+            .filter(Term.language_id == lgid, Term.token_count == 1)\n+            .all()\n+        )\n+        known_terms_lc = [word[0] for word in known_terms_lc]\n+\n+        newtoks = [t for t in unique_lcase_toks if t not in known_terms_lc]\n+        with open(outfile, \"w\", encoding=\"utf-8\") as f:\n+            f.write(\"\\n\".join(newtoks))\ndiff --git a/lute/termimport/routes.py b/lute/termimport/routes.py\nindex e9a5a8bc..7cccd4fe 100644\n--- a/lute/termimport/routes.py\n+++ b/lute/termimport/routes.py\n@@ -8,7 +8,8 @@\n from wtforms.validators import DataRequired\n from flask_wtf import FlaskForm\n from flask_wtf.file import FileField\n-from lute.termimport.service import import_file, BadImportFileError\n+from lute.termimport.service import Service, BadImportFileError\n+from lute.db import db\n \n \n bp = Blueprint(\"termimport\", __name__, url_prefix=\"/termimport\")\n@@ -26,7 +27,7 @@ class TermImportForm(FlaskForm):\n def term_import_index():\n     \"Import posted file.\"\n     form = TermImportForm()\n-\n+    service = Service(db.session)\n     if form.validate_on_submit():\n         text_file = form.text_file.data\n         if text_file:\n@@ -35,7 +36,7 @@ def term_import_index():\n             )\n             text_file.save(temp_file_name)\n             try:\n-                stats = import_file(\n+                stats = service.import_file(\n                     temp_file_name,\n                     form.create_terms.data,\n                     form.update_terms.data,\ndiff --git a/lute/termimport/service.py b/lute/termimport/service.py\nindex a59a0540..3d7a3cba 100644\n--- a/lute/termimport/service.py\n+++ b/lute/termimport/service.py\n@@ -4,9 +4,8 @@\n \n import csv\n \n-from lute.db import db\n from lute.models.term import Status\n-from lute.models.language import Language\n+from lute.models.repositories import LanguageRepository\n from lute.term.model import Term, Repository\n \n \n@@ -20,279 +19,279 @@ class BadImportFileError(Exception):\n     \"\"\"\n \n \n-def import_file(filename, create_terms=True, update_terms=True, new_as_unknowns=False):\n-    \"\"\"\n-    Validate and import file.\n-\n-    Throws BadImportFileError if file contains invalid data.\n-    \"\"\"\n-    import_data = _load_import_file(filename)\n-    _validate_data(import_data)\n-    return _do_import(import_data, create_terms, update_terms, new_as_unknowns)\n-\n-\n-def _load_import_file(filename, encoding=\"utf-8-sig\"):\n-    \"Create array of hashes from file.\"\n-    importdata = []\n-    with open(filename, \"r\", encoding=encoding) as f:\n-        reader = csv.DictReader(f)\n-        if reader.fieldnames:  # Avoid empty file error\n-            reader.fieldnames = [name.lower() for name in reader.fieldnames]\n-\n-        fieldnames = reader.fieldnames\n-        if fieldnames is None:\n+class Service:\n+    \"Service.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def import_file(\n+        self, filename, create_terms=True, update_terms=True, new_as_unknowns=False\n+    ):\n+        \"\"\"\n+        Validate and import file.\n+\n+        Throws BadImportFileError if file contains invalid data.\n+        \"\"\"\n+        import_data = self._load_import_file(filename)\n+        self._validate_data(import_data)\n+        return self._do_import(import_data, create_terms, update_terms, new_as_unknowns)\n+\n+    def _load_import_file(self, filename, encoding=\"utf-8-sig\"):\n+        \"Create array of hashes from file.\"\n+        importdata = []\n+        with open(filename, \"r\", encoding=encoding) as f:\n+            reader = csv.DictReader(f)\n+            if reader.fieldnames:  # Avoid empty file error\n+                reader.fieldnames = [name.lower() for name in reader.fieldnames]\n+\n+            fieldnames = reader.fieldnames\n+            if fieldnames is None:\n+                raise BadImportFileError(\"No terms in file\")\n+            self._validate_data_fields(fieldnames)\n+\n+            line_num = 0\n+            for line in reader:\n+                line_num += 1\n+                if None in line.values():\n+                    raise BadImportFileError(f\"Missing values on line {line_num}\")\n+                if None in line.keys():\n+                    raise BadImportFileError(f\"Extra values on line {line_num}\")\n+                importdata.append(line)\n+\n+        if len(importdata) == 0:\n             raise BadImportFileError(\"No terms in file\")\n-        _validate_data_fields(fieldnames)\n-\n-        line_num = 0\n-        for line in reader:\n-            line_num += 1\n-            if None in line.values():\n-                raise BadImportFileError(f\"Missing values on line {line_num}\")\n-            if None in line.keys():\n-                raise BadImportFileError(f\"Extra values on line {line_num}\")\n-            importdata.append(line)\n-\n-    if len(importdata) == 0:\n-        raise BadImportFileError(\"No terms in file\")\n-\n-    return importdata\n-\n-\n-def _validate_data_fields(field_list):\n-    \"Check the keys in the file.\"\n-    required = [\"language\", \"term\"]\n-    for k in required:\n-        if k not in field_list:\n-            raise BadImportFileError(f\"Missing required field '{k}'\")\n-\n-    allowed = required + [\n-        \"translation\",\n-        \"parent\",\n-        \"status\",\n-        \"tags\",\n-        \"pronunciation\",\n-        \"link_status\",\n-    ]\n-    ignored = [\"added\"]\n-    for k in field_list:\n-        if k not in allowed and k not in ignored:\n-            raise BadImportFileError(f\"Unknown field '{k}'\")\n-\n-\n-def _validate_data(import_data):\n-    \"\"\"\n-    Check the data.\n-    \"\"\"\n-    _validate_languages(import_data)\n-    _validate_terms_exist(import_data)\n-    _validate_statuses(import_data)\n-    _validate_no_duplicate_terms(import_data)\n-\n-\n-def _create_langs_dict(import_data):\n-    \"Create dictionary of language name to Language.\"\n-    lang_dict = {}\n-    langs = [hsh[\"language\"].strip() for hsh in import_data]\n-    for lang_name in list(set(langs)):\n-        lang_dict[lang_name] = Language.find_by_name(lang_name)\n-    return lang_dict\n-\n-\n-def _get_status(s):\n-    \"Convert status to db value.\"\n-    status_map = {\n-        \"\": 1,\n-        \"1\": 1,\n-        \"2\": 2,\n-        \"3\": 3,\n-        \"4\": 4,\n-        \"5\": 5,\n-        \"W\": Status.WELLKNOWN,\n-        \"I\": Status.IGNORED,\n-    }\n-    return status_map.get(s)\n-\n-\n-def _validate_languages(import_data):\n-    \"Validate the languages.\"\n-    lang_dict = _create_langs_dict(import_data)\n-    for lang_name, lang in lang_dict.items():\n-        if lang is None:\n-            raise BadImportFileError(f\"Unknown language '{lang_name}'\")\n-\n-\n-def _validate_statuses(import_data):\n-    \"All statuses must be valid.\"\n-    statuses = [hsh[\"status\"].strip() for hsh in import_data if \"status\" in hsh]\n-    for s in set(statuses):\n-        if _get_status(s) is None:\n+\n+        return importdata\n+\n+    def _validate_data_fields(self, field_list):\n+        \"Check the keys in the file.\"\n+        required = [\"language\", \"term\"]\n+        for k in required:\n+            if k not in field_list:\n+                raise BadImportFileError(f\"Missing required field '{k}'\")\n+\n+        allowed = required + [\n+            \"translation\",\n+            \"parent\",\n+            \"status\",\n+            \"tags\",\n+            \"pronunciation\",\n+            \"link_status\",\n+        ]\n+        ignored = [\"added\"]\n+        for k in field_list:\n+            if k not in allowed and k not in ignored:\n+                raise BadImportFileError(f\"Unknown field '{k}'\")\n+\n+    def _validate_data(self, import_data):\n+        \"\"\"\n+        Check the data.\n+        \"\"\"\n+        self._validate_languages(import_data)\n+        self._validate_terms_exist(import_data)\n+        self._validate_statuses(import_data)\n+        self._validate_no_duplicate_terms(import_data)\n+\n+    def _create_langs_dict(self, import_data):\n+        \"Create dictionary of language name to Language.\"\n+        repo = LanguageRepository(self.session)\n+        lang_dict = {}\n+        langs = [hsh[\"language\"].strip() for hsh in import_data]\n+        for lang_name in list(set(langs)):\n+            lang_dict[lang_name] = repo.find_by_name(lang_name)\n+        return lang_dict\n+\n+    def _get_status(self, s):\n+        \"Convert status to db value.\"\n+        status_map = {\n+            \"\": 1,\n+            \"1\": 1,\n+            \"2\": 2,\n+            \"3\": 3,\n+            \"4\": 4,\n+            \"5\": 5,\n+            \"W\": Status.WELLKNOWN,\n+            \"I\": Status.IGNORED,\n+        }\n+        return status_map.get(s)\n+\n+    def _validate_languages(self, import_data):\n+        \"Validate the languages.\"\n+        lang_dict = self._create_langs_dict(import_data)\n+        for lang_name, lang in lang_dict.items():\n+            if lang is None:\n+                raise BadImportFileError(f\"Unknown language '{lang_name}'\")\n+\n+    def _validate_statuses(self, import_data):\n+        \"All statuses must be valid.\"\n+        statuses = [hsh[\"status\"].strip() for hsh in import_data if \"status\" in hsh]\n+        for s in set(statuses):\n+            if self._get_status(s) is None:\n+                raise BadImportFileError(\n+                    \"Status must be one of 1, 2, 3, 4, 5, I, W, or blank\"\n+                )\n+\n+    def _validate_terms_exist(self, import_data):\n+        \"All records must have a term.\"\n+        blanks = [hsh for hsh in import_data if hsh[\"term\"].strip() == \"\"]\n+        if len(blanks) > 0:\n+            raise BadImportFileError(\"Term is required\")\n+\n+    def _validate_no_duplicate_terms(self, import_data):\n+        \"\"\"\n+        Duplicate terms aren't allowed.\n+\n+        If file contained two duplicate terms, which is the \"correct\" one?\n+        \"\"\"\n+\n+        def make_lang_term_string(hsh):\n+            t = hsh[\"term\"].strip()\n+            # Have to also clear unicode whitespace.\n+            t = \" \".join(t.split())\n+            return f\"{hsh['language']}: {t.lower()}\"\n+\n+        lang_terms = [make_lang_term_string(hsh) for hsh in import_data]\n+        term_counts = {}\n+        for term in lang_terms:\n+            term_counts[term] = term_counts.get(term, 0) + 1\n+        duplicates = [term for term, count in term_counts.items() if count > 1]\n+        if len(duplicates) != 0:\n             raise BadImportFileError(\n-                \"Status must be one of 1, 2, 3, 4, 5, I, W, or blank\"\n+                f\"Duplicate terms in import: {', '.join(duplicates)}\"\n             )\n \n-\n-def _validate_terms_exist(import_data):\n-    \"All records must have a term.\"\n-    blanks = [hsh for hsh in import_data if hsh[\"term\"].strip() == \"\"]\n-    if len(blanks) > 0:\n-        raise BadImportFileError(\"Term is required\")\n-\n-\n-def _validate_no_duplicate_terms(import_data):\n-    \"\"\"\n-    Duplicate terms aren't allowed.\n-\n-    If file contained two duplicate terms, which is the \"correct\" one?\n-    \"\"\"\n-\n-    def make_lang_term_string(hsh):\n-        t = hsh[\"term\"].strip()\n-        # Have to also clear unicode whitespace.\n-        t = \" \".join(t.split())\n-        return f\"{hsh['language']}: {t.lower()}\"\n-\n-    lang_terms = [make_lang_term_string(hsh) for hsh in import_data]\n-    term_counts = {}\n-    for term in lang_terms:\n-        term_counts[term] = term_counts.get(term, 0) + 1\n-    duplicates = [term for term, count in term_counts.items() if count > 1]\n-    if len(duplicates) != 0:\n-        raise BadImportFileError(f\"Duplicate terms in import: {', '.join(duplicates)}\")\n-\n-\n-def _import_term_skip_parents(repo, rec, lang, set_to_unknown=False):\n-    \"Add a single record to the repo.\"\n-    t = Term()\n-    t.language = lang\n-    t.language_id = lang.id\n-    t.text = rec[\"term\"]\n-    if \"translation\" in rec:\n-        t.translation = rec[\"translation\"]\n-    if \"status\" in rec:\n-        status = _get_status(rec[\"status\"])\n-        if status is not None:\n-            t.status = int(status)\n-    if set_to_unknown:\n-        t.status = 0\n-    if \"pronunciation\" in rec:\n-        t.romanization = rec[\"pronunciation\"]\n-    if \"tags\" in rec:\n-        tags = list(map(str.strip, rec[\"tags\"].split(\",\")))\n-        t.term_tags = [t for t in tags if t != \"\"]\n-    repo.add(t)\n-\n-\n-def _update_term_skip_parents(t, repo, rec):\n-    \"Update a term in the repo.\"\n-    # Don't change the lang or text of the term\n-    # t.language = lang\n-    # t.language_id = lang.id\n-    # t.text = rec[\"term\"]\n-    if \"translation\" in rec:\n-        t.translation = rec[\"translation\"]\n-    if \"status\" in rec:\n-        status = _get_status(rec[\"status\"])\n-        if status is not None:\n-            t.status = int(status)\n-    if \"pronunciation\" in rec:\n-        t.romanization = rec[\"pronunciation\"]\n-    if \"tags\" in rec:\n-        tags = list(map(str.strip, rec[\"tags\"].split(\",\")))\n-        t.term_tags = [t for t in tags if t != \"\"]\n-\n-    repo.add(t)\n-\n-\n-def _set_term_parents(repo, rec, lang):\n-    \"Set the term parents.\"\n-    t = repo.find(lang.id, rec[\"term\"])\n-    parents = list(map(str.strip, rec[\"parent\"].split(\",\")))\n-    t.parents = [p for p in parents if p != \"\"]\n-    if \"link_status\" in rec and len(parents) == 1:\n-        sync_status = rec[\"link_status\"] or \"\"\n-        t.sync_status = sync_status.strip().lower() == \"y\"\n-    if len(t.parents) != 1:\n-        t.sync_status = False\n-\n-    # If syncing to parent, and the term status was not explicitly set,\n-    # then \"inherit\" the parent status.\n-    if t.sync_status and len(t.parents) == 1 and \"status\" not in rec:\n-        p = repo.find(lang.id, t.parents[0])\n-        if p is not None:\n-            t.status = p.status\n-\n-    repo.add(t)\n-\n-\n-def _do_import(\n-    import_data, create_terms=True, update_terms=True, new_as_unknowns=False\n-):\n-    \"\"\"\n-    Import records.\n-\n-    If create_terms is True, create new terms.\n-    If update_terms is True, update existing terms.\n-    If new_as_unknowns is True, new terms are given status 0.\n-\n-    The import is done in two passes:\n-    1. import the basic terms, without setting their parents\n-    2. update the terms with parents\n-\n-    The two passes are done because the import file may\n-    contain a parent in its own row, and we want that to be\n-    imported first to get its own specified data.\n-    \"\"\"\n-    # pylint: disable=too-many-locals\n-\n-    repo = Repository(db)\n-\n-    skipped = 0\n-\n-    # Keep track of the created and updated terms: we only want to\n-    # update these ones in pass #2.\n-    created_terms = []\n-    updated_terms = []\n-\n-    def term_string(lang, term):\n-        return f\"{lang.id}-{term}\"\n-\n-    for batch in [import_data[i : i + 100] for i in range(0, len(import_data), 100)]:\n-        langs_dict = _create_langs_dict(batch)\n-        for hsh in batch:\n-            lang = langs_dict[hsh[\"language\"]]\n-            t = repo.find(lang.id, hsh[\"term\"])\n-            ts = term_string(lang, hsh[\"term\"])\n-\n-            if create_terms and t is None:\n-                # Create a brand-new term.\n-                _import_term_skip_parents(repo, hsh, lang, new_as_unknowns)\n-                created_terms.append(ts)\n-\n-            elif update_terms and t is not None:\n-                # Can only update existing terms.\n-                _update_term_skip_parents(t, repo, hsh)\n-                updated_terms.append(ts)\n-\n-            else:\n-                skipped += 1\n-\n-        repo.commit()\n-\n-    pass_2 = [t for t in import_data if \"parent\" in t and t[\"parent\"] != \"\"]\n-    for batch in [pass_2[i : i + 100] for i in range(0, len(pass_2), 100)]:\n-        langs_dict = _create_langs_dict(batch)\n-        for hsh in batch:\n-            lang = langs_dict[hsh[\"language\"]]\n-            ts = term_string(lang, hsh[\"term\"])\n-            if ts in created_terms or ts in updated_terms:\n-                _set_term_parents(repo, hsh, lang)\n-        repo.commit()\n-\n-    stats = {\n-        \"created\": len(created_terms),\n-        \"updated\": len(updated_terms),\n-        \"skipped\": skipped,\n-    }\n-\n-    return stats\n+    def _import_term_skip_parents(self, repo, rec, lang, set_to_unknown=False):\n+        \"Add a single record to the repo.\"\n+        t = Term()\n+        t.language = lang\n+        t.language_id = lang.id\n+        t.text = rec[\"term\"]\n+        if \"translation\" in rec:\n+            t.translation = rec[\"translation\"]\n+        if \"status\" in rec:\n+            status = self._get_status(rec[\"status\"])\n+            if status is not None:\n+                t.status = int(status)\n+        if set_to_unknown:\n+            t.status = 0\n+        if \"pronunciation\" in rec:\n+            t.romanization = rec[\"pronunciation\"]\n+        if \"tags\" in rec:\n+            tags = list(map(str.strip, rec[\"tags\"].split(\",\")))\n+            t.term_tags = [t for t in tags if t != \"\"]\n+        repo.add(t)\n+\n+    def _update_term_skip_parents(self, t, repo, rec):\n+        \"Update a term in the repo.\"\n+        # Don't change the lang or text of the term\n+        # t.language = lang\n+        # t.language_id = lang.id\n+        # t.text = rec[\"term\"]\n+        if \"translation\" in rec:\n+            t.translation = rec[\"translation\"]\n+        if \"status\" in rec:\n+            status = self._get_status(rec[\"status\"])\n+            if status is not None:\n+                t.status = int(status)\n+        if \"pronunciation\" in rec:\n+            t.romanization = rec[\"pronunciation\"]\n+        if \"tags\" in rec:\n+            tags = list(map(str.strip, rec[\"tags\"].split(\",\")))\n+            t.term_tags = [t for t in tags if t != \"\"]\n+\n+        repo.add(t)\n+\n+    def _set_term_parents(self, repo, rec, lang):\n+        \"Set the term parents.\"\n+        t = repo.find(lang.id, rec[\"term\"])\n+        parents = list(map(str.strip, rec[\"parent\"].split(\",\")))\n+        t.parents = [p for p in parents if p != \"\"]\n+        if \"link_status\" in rec and len(parents) == 1:\n+            sync_status = rec[\"link_status\"] or \"\"\n+            t.sync_status = sync_status.strip().lower() == \"y\"\n+        if len(t.parents) != 1:\n+            t.sync_status = False\n+\n+        # If syncing to parent, and the term status was not explicitly set,\n+        # then \"inherit\" the parent status.\n+        if t.sync_status and len(t.parents) == 1 and \"status\" not in rec:\n+            p = repo.find(lang.id, t.parents[0])\n+            if p is not None:\n+                t.status = p.status\n+\n+        repo.add(t)\n+\n+    def _do_import(\n+        self, import_data, create_terms=True, update_terms=True, new_as_unknowns=False\n+    ):\n+        \"\"\"\n+        Import records.\n+\n+        If create_terms is True, create new terms.\n+        If update_terms is True, update existing terms.\n+        If new_as_unknowns is True, new terms are given status 0.\n+\n+        The import is done in two passes:\n+        1. import the basic terms, without setting their parents\n+        2. update the terms with parents\n+\n+        The two passes are done because the import file may\n+        contain a parent in its own row, and we want that to be\n+        imported first to get its own specified data.\n+        \"\"\"\n+        # pylint: disable=too-many-locals\n+\n+        repo = Repository(self.session)\n+\n+        skipped = 0\n+\n+        # Keep track of the created and updated terms: we only want to\n+        # update these ones in pass #2.\n+        created_terms = []\n+        updated_terms = []\n+\n+        def term_string(lang, term):\n+            return f\"{lang.id}-{term}\"\n+\n+        for batch in [\n+            import_data[i : i + 100] for i in range(0, len(import_data), 100)\n+        ]:\n+            langs_dict = self._create_langs_dict(batch)\n+            for hsh in batch:\n+                lang = langs_dict[hsh[\"language\"]]\n+                t = repo.find(lang.id, hsh[\"term\"])\n+                ts = term_string(lang, hsh[\"term\"])\n+\n+                if create_terms and t is None:\n+                    # Create a brand-new term.\n+                    self._import_term_skip_parents(repo, hsh, lang, new_as_unknowns)\n+                    created_terms.append(ts)\n+\n+                elif update_terms and t is not None:\n+                    # Can only update existing terms.\n+                    self._update_term_skip_parents(t, repo, hsh)\n+                    updated_terms.append(ts)\n+\n+                else:\n+                    skipped += 1\n+\n+            repo.commit()\n+\n+        pass_2 = [t for t in import_data if \"parent\" in t and t[\"parent\"] != \"\"]\n+        for batch in [pass_2[i : i + 100] for i in range(0, len(pass_2), 100)]:\n+            langs_dict = self._create_langs_dict(batch)\n+            for hsh in batch:\n+                lang = langs_dict[hsh[\"language\"]]\n+                ts = term_string(lang, hsh[\"term\"])\n+                if ts in created_terms or ts in updated_terms:\n+                    self._set_term_parents(repo, hsh, lang)\n+            repo.commit()\n+\n+        stats = {\n+            \"created\": len(created_terms),\n+            \"updated\": len(updated_terms),\n+            \"skipped\": skipped,\n+        }\n+\n+        return stats\ndiff --git a/lute/termtag/datatables.py b/lute/termtag/datatables.py\nindex 9aba0b4f..353f20cc 100644\n--- a/lute/termtag/datatables.py\n+++ b/lute/termtag/datatables.py\n@@ -2,11 +2,10 @@\n Show terms in datatables.\n \"\"\"\n \n-from lute.db import db\n from lute.utils.data_tables import DataTablesSqliteQuery\n \n \n-def get_data_tables_list(parameters):\n+def get_data_tables_list(parameters, session):\n     \"json data for datatables.\"\n     base_sql = \"\"\"SELECT\n           TgID,\n@@ -21,6 +20,5 @@ def get_data_tables_list(parameters):\n             group by WtTgID\n           ) src on src.WtTgID = TgID\n     \"\"\"\n-    session = db.session\n     connection = session.connection()\n     return DataTablesSqliteQuery.get_data(base_sql, parameters, connection)\ndiff --git a/lute/termtag/routes.py b/lute/termtag/routes.py\nindex b1e10b22..6b8446d4 100644\n--- a/lute/termtag/routes.py\n+++ b/lute/termtag/routes.py\n@@ -5,6 +5,7 @@\n from sqlalchemy import text\n from flask import Blueprint, request, jsonify, render_template, redirect\n from lute.models.term import TermTag\n+from lute.models.repositories import TermTagRepository\n from lute.utils.data_tables import DataTablesFlaskParamParser\n from lute.termtag.datatables import get_data_tables_list\n from lute.db import db\n@@ -24,7 +25,7 @@ def index(search):\n def datatables_active_source():\n     \"Datatables data for terms.\"\n     parameters = DataTablesFlaskParamParser.parse_params(request.form)\n-    data = get_data_tables_list(parameters)\n+    data = get_data_tables_list(parameters, db.session)\n     return jsonify(data)\n \n \n@@ -48,7 +49,8 @@ def edit(termtagid):\n     \"\"\"\n     Edit a termtag\n     \"\"\"\n-    termtag = TermTag.find(termtagid)\n+    repo = TermTagRepository(db.session)\n+    termtag = repo.find(termtagid)\n     return _handle_form(termtag, \"termtag/edit.html\")\n \n \n@@ -66,7 +68,8 @@ def delete(termtagid):\n     \"\"\"\n     Delete a termtag.\n     \"\"\"\n-    termtag = TermTag.find(termtagid)\n+    repo = TermTagRepository(db.session)\n+    termtag = repo.find(termtagid)\n     db.session.delete(termtag)\n \n     # ANNOYING HACK.  Per GitHub issue 455, the records\ndiff --git a/lute/themes/routes.py b/lute/themes/routes.py\nindex dc131255..4cd86e13 100644\n--- a/lute/themes/routes.py\n+++ b/lute/themes/routes.py\n@@ -2,8 +2,9 @@\n \n from flask import Blueprint, Response, jsonify\n \n-from lute.themes.service import get_current_css, next_theme\n-from lute.models.setting import UserSetting\n+from lute.themes.service import Service\n+from lute.models.repositories import UserSettingRepository\n+from lute.settings.current import current_settings\n from lute.db import db\n \n bp = Blueprint(\"themes\", __name__, url_prefix=\"/theme\")\n@@ -12,7 +13,8 @@\n @bp.route(\"/current\", methods=[\"GET\"])\n def current_theme():\n     \"Return current css.\"\n-    response = Response(get_current_css(), 200)\n+    service = Service(db.session)\n+    response = Response(service.get_current_css(), 200)\n     response.content_type = \"text/css; charset=utf-8\"\n     return response\n \n@@ -22,7 +24,8 @@ def custom_styles():\n     \"\"\"\n     Return the custom settings for inclusion in the base.html.\n     \"\"\"\n-    css = UserSetting.get_value(\"custom_styles\")\n+    repo = UserSettingRepository(db.session)\n+    css = repo.get_value(\"custom_styles\")\n     response = Response(css, 200)\n     response.content_type = \"text/css; charset=utf-8\"\n     return response\n@@ -31,14 +34,17 @@ def custom_styles():\n @bp.route(\"/next\", methods=[\"POST\"])\n def set_next_theme():\n     \"Go to next theme.\"\n-    next_theme()\n+    service = Service(db.session)\n+    service.next_theme()\n     return jsonify(\"ok\")\n \n \n @bp.route(\"/toggle_highlight\", methods=[\"POST\"])\n def toggle_highlight():\n     \"Fix the highlight.\"\n-    b = bool(int(UserSetting.get_value(\"show_highlights\")))\n-    UserSetting.set_value(\"show_highlights\", not b)\n+    new_setting = not current_settings[\"show_highlights\"]\n+    repo = UserSettingRepository(db.session)\n+    repo.set_value(\"show_highlights\", new_setting)\n     db.session.commit()\n+    current_settings[\"show_highlights\"] = new_setting\n     return jsonify(\"ok\")\ndiff --git a/lute/themes/service.py b/lute/themes/service.py\nindex 8676aa85..47e4a2b8 100644\n--- a/lute/themes/service.py\n+++ b/lute/themes/service.py\n@@ -1,87 +1,90 @@\n \"\"\"\n Theming service.\n \n-Themes are stored in the css folder.  Current theme is saved in\n-UserSetting.\n+Themes are stored in the css folder, current theme in UserSetting.\n \"\"\"\n \n import os\n from glob import glob\n from flask import current_app\n-from lute.models.setting import UserSetting\n-from lute.db import db\n+from lute.models.repositories import UserSettingRepository\n \n default_entry = (\"-\", \"(default)\")\n \n \n-def _css_path():\n-    \"\"\"\n-    Path to css in this folder.\n-    \"\"\"\n-    thisdir = os.path.dirname(__file__)\n-    theme_dir = os.path.join(thisdir, \"css\")\n-    return os.path.abspath(theme_dir)\n-\n-\n-def list_themes():\n-    \"\"\"\n-    List of theme file names and user-readable name.\n-    \"\"\"\n+class Service:\n+    \"Service.\"\n+\n+    def __init__(self, session):\n+        self.session = session\n+\n+    def _css_path(self):\n+        \"\"\"\n+        Path to css in this folder.\n+        \"\"\"\n+        thisdir = os.path.dirname(__file__)\n+        theme_dir = os.path.join(thisdir, \"css\")\n+        return os.path.abspath(theme_dir)\n+\n+    def list_themes(self):\n+        \"\"\"\n+        List of theme file names and user-readable name.\n+        \"\"\"\n+\n+        def _make_display_name(s):\n+            ret = os.path.basename(s)\n+            ret = ret.replace(\".css\", \"\").replace(\"_\", \" \")\n+            return ret\n+\n+        g = glob(os.path.join(self._css_path(), \"*.css\"))\n+        themes = [(os.path.basename(f), _make_display_name(f)) for f in g]\n+        theme_basenames = [t[0] for t in themes]\n+\n+        g = glob(os.path.join(current_app.env_config.userthemespath, \"*.css\"))\n+        additional_user_themes = [\n+            (os.path.basename(f), _make_display_name(f))\n+            for f in g\n+            if os.path.basename(f) not in theme_basenames\n+        ]\n+\n+        themes += additional_user_themes\n+        sorted_themes = sorted(themes, key=lambda x: x[1])\n+        return [default_entry] + sorted_themes\n+\n+    def get_current_css(self):\n+        \"\"\"\n+        Return the current css pointed at by the current_theme user setting.\n+        \"\"\"\n+        repo = UserSettingRepository(self.session)\n+        current_theme = repo.get_value(\"current_theme\")\n+        if current_theme == default_entry[0]:\n+            return \"\"\n \n-    def _make_display_name(s):\n-        ret = os.path.basename(s)\n-        ret = ret.replace(\".css\", \"\").replace(\"_\", \" \")\n+        def _get_theme_css_in_dir(d):\n+            \"Get css, or '' if no file.\"\n+            fname = os.path.join(d, current_theme)\n+            if not os.path.exists(fname):\n+                return \"\"\n+            with open(fname, \"r\", encoding=\"utf-8\") as f:\n+                return f.read()\n+\n+        ret = _get_theme_css_in_dir(self._css_path())\n+        add = _get_theme_css_in_dir(current_app.env_config.userthemespath)\n+        if add != \"\":\n+            ret += f\"\\n\\n/* Additional user css */\\n\\n{add}\"\n         return ret\n \n-    g = glob(os.path.join(_css_path(), \"*.css\"))\n-    themes = [(os.path.basename(f), _make_display_name(f)) for f in g]\n-    theme_basenames = [t[0] for t in themes]\n-\n-    g = glob(os.path.join(current_app.env_config.userthemespath, \"*.css\"))\n-    additional_user_themes = [\n-        (os.path.basename(f), _make_display_name(f))\n-        for f in g\n-        if os.path.basename(f) not in theme_basenames\n-    ]\n-\n-    themes += additional_user_themes\n-    sorted_themes = sorted(themes, key=lambda x: x[1])\n-    return [default_entry] + sorted_themes\n-\n-\n-def get_current_css():\n-    \"\"\"\n-    Return the current css pointed at by the current_theme user setting.\n-    \"\"\"\n-    current_theme = UserSetting.get_value(\"current_theme\")\n-    if current_theme == default_entry[0]:\n-        return \"\"\n-\n-    def _get_theme_css_in_dir(d):\n-        \"Get css, or '' if no file.\"\n-        fname = os.path.join(d, current_theme)\n-        if not os.path.exists(fname):\n-            return \"\"\n-        with open(fname, \"r\", encoding=\"utf-8\") as f:\n-            return f.read()\n-\n-    ret = _get_theme_css_in_dir(_css_path())\n-    add = _get_theme_css_in_dir(current_app.env_config.userthemespath)\n-    if add != \"\":\n-        ret += f\"\\n\\n/* Additional user css */\\n\\n{add}\"\n-    return ret\n-\n-\n-def next_theme():\n-    \"\"\"\n-    Move to the next theme in the list of themes.\n-    \"\"\"\n-    current_theme = UserSetting.get_value(\"current_theme\")\n-    themes = [t[0] for t in list_themes()]\n-    themes.append(default_entry[0])\n-    for i in range(0, len(themes)):  # pylint: disable=consider-using-enumerate\n-        if themes[i] == current_theme:\n-            new_index = i + 1\n-            break\n-    UserSetting.set_value(\"current_theme\", themes[new_index])\n-    db.session.commit()\n+    def next_theme(self):\n+        \"\"\"\n+        Move to the next theme in the list of themes.\n+        \"\"\"\n+        repo = UserSettingRepository(self.session)\n+        current_theme = repo.get_value(\"current_theme\")\n+        themes = [t[0] for t in self.list_themes()]\n+        themes.append(default_entry[0])\n+        for i in range(0, len(themes)):  # pylint: disable=consider-using-enumerate\n+            if themes[i] == current_theme:\n+                new_index = i + 1\n+                break\n+        repo.set_value(\"current_theme\", themes[new_index])\n+        self.session.commit()\ndiff --git a/lute/utils/formutils.py b/lute/utils/formutils.py\nindex 3607eafc..92e1dcea 100644\n--- a/lute/utils/formutils.py\n+++ b/lute/utils/formutils.py\n@@ -3,18 +3,17 @@\n \"\"\"\n \n from lute.models.language import Language\n-from lute.models.setting import UserSetting\n-from lute.db import db\n+from lute.models.repositories import UserSettingRepository\n \n \n-def language_choices(dummy_entry_placeholder=\"-\"):\n+def language_choices(session, dummy_entry_placeholder=\"-\"):\n     \"\"\"\n     Return the list of languages for select boxes.\n \n     If only one lang exists, only return that,\n     otherwise add a '-' dummy entry at the top.\n     \"\"\"\n-    langs = db.session.query(Language).order_by(Language.name).all()\n+    langs = session.query(Language).order_by(Language.name).all()\n     supported = [lang for lang in langs if lang.is_supported]\n     lang_choices = [(s.id, s.name) for s in supported]\n     # Add a dummy placeholder even if there are no languages.\n@@ -23,19 +22,20 @@ def language_choices(dummy_entry_placeholder=\"-\"):\n     return lang_choices\n \n \n-def valid_current_language_id():\n+def valid_current_language_id(session):\n     \"\"\"\n     Get the current language id from UserSetting, ensuring\n     it's still valid.  If not, change it.\n     \"\"\"\n-    current_language_id = UserSetting.get_value(\"current_language_id\")\n+    repo = UserSettingRepository(session)\n+    current_language_id = repo.get_value(\"current_language_id\")\n     current_language_id = int(current_language_id)\n \n-    valid_language_ids = [int(p[0]) for p in language_choices()]\n+    valid_language_ids = [int(p[0]) for p in language_choices(session)]\n     if current_language_id in valid_language_ids:\n         return current_language_id\n \n     current_language_id = valid_language_ids[0]\n-    UserSetting.set_value(\"current_language_id\", current_language_id)\n-    db.session.commit()\n+    repo.set_value(\"current_language_id\", current_language_id)\n+    session.commit()\n     return current_language_id\ndiff --git a/tasks.py b/tasks.py\nindex 4675c75c..8a71ebfd 100644\n--- a/tasks.py\n+++ b/tasks.py\n@@ -15,6 +15,8 @@\n import os\n import sys\n import subprocess\n+import threading\n+import time\n from datetime import datetime\n import requests\n from invoke import task, Collection\n@@ -103,6 +105,65 @@ def _site_is_running(useport=None):\n         return False\n \n \n+def _wait_for_running_site(port):\n+    \"Wait until the site is running.\"\n+    url = f\"http://localhost:{port}\"\n+    is_running = False\n+    attempt_count = 0\n+    print(f\"Wait until site is running at {url}.\", flush=True)\n+    while attempt_count < 10 and not is_running:\n+        attempt_count += 1\n+        try:\n+            print(f\"  Attempt {attempt_count}\", flush=True)\n+            requests.get(url, timeout=5)\n+            is_running = True\n+        except requests.exceptions.ConnectionError:\n+            time.sleep(1)\n+    if not is_running:\n+        raise Exception(\"Site didn't start?\")  # pylint: disable=broad-exception-raised\n+\n+\n+def _run_browser_tests(c, port, run_test):\n+    \"Start server on port if necessary, and run tests.\"\n+    tests_failed = False\n+    if _site_is_running(port):\n+        c.run(\" \".join(run_test))\n+    else:\n+\n+        def print_subproc_output(pipe, label):\n+            \"\"\"Prints output from a given pipe with a label.\"\"\"\n+            for line in iter(pipe.readline, b\"\"):\n+                print(f\"[{label}] {line.decode().strip()}\", flush=True)\n+            pipe.close()\n+\n+        cmd = [\"python\", \"-m\", \"tests.acceptance.start_acceptance_app\", f\"{port}\"]\n+        with subprocess.Popen(\n+            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n+        ) as app_process:\n+            _wait_for_running_site(port)\n+            stdout_thread = threading.Thread(\n+                target=print_subproc_output, args=(app_process.stdout, \"STDOUT\")\n+            )\n+            stderr_thread = threading.Thread(\n+                target=print_subproc_output, args=(app_process.stderr, \"STDERR\")\n+            )\n+            stdout_thread.start()\n+            stderr_thread.start()\n+            try:\n+                subprocess.run(run_test, check=True)\n+            except subprocess.CalledProcessError:\n+                # This just means a test failed.  We don't need to see\n+                # a stack trace, the assert failures are already displayed.\n+                tests_failed = True\n+            finally:\n+                app_process.terminate()\n+                stdout_thread.join()\n+                stderr_thread.join()\n+\n+    if tests_failed:\n+        raise RuntimeError(\"tests failed\")\n+\n+\n @task(\n     pre=[_ensure_test_db],\n     help={\n@@ -150,23 +211,7 @@ def accept(  # pylint: disable=too-many-arguments\n     if verbose:\n         run_test.append(\"-vv\")\n \n-    tests_failed = False\n-    if _site_is_running(port):\n-        c.run(\" \".join(run_test))\n-    else:\n-        cmd = [\"python\", \"-m\", \"tests.acceptance.start_acceptance_app\", f\"{port}\"]\n-        with subprocess.Popen(cmd) as app_process:\n-            try:\n-                subprocess.run(run_test, check=True)\n-            except subprocess.CalledProcessError:\n-                # This just means a test failed.  We don't need to see\n-                # a stack trace, the assert failures are already displayed.\n-                tests_failed = True\n-            finally:\n-                app_process.terminate()\n-\n-    if tests_failed:\n-        raise RuntimeError(\"tests failed\")\n+    _run_browser_tests(c, 5001, run_test)\n \n \n @task(pre=[_ensure_test_db])\n@@ -179,25 +224,7 @@ def playwright(c):\n     If Lute's not running on specified port, start a server.\n     \"\"\"\n     run_test = [\"pytest\", \"tests/playwright/playwright.py\", \"-s\"]\n-\n-    tests_failed = False\n-    port = 5001\n-    if _site_is_running(port):\n-        c.run(\" \".join(run_test))\n-    else:\n-        cmd = [\"python\", \"-m\", \"tests.acceptance.start_acceptance_app\", f\"{port}\"]\n-        with subprocess.Popen(cmd) as app_process:\n-            try:\n-                subprocess.run(run_test, check=True)\n-            except subprocess.CalledProcessError:\n-                # This just means a test failed.  We don't need to see\n-                # a stack trace, the assert failures are already displayed.\n-                tests_failed = True\n-            finally:\n-                app_process.terminate()\n-\n-    if tests_failed:\n-        raise RuntimeError(\"tests failed\")\n+    _run_browser_tests(c, 5001, run_test)\n \n \n @task(pre=[_ensure_test_db], help={\"html\": \"open html report\"})\n@@ -217,7 +244,7 @@ def coverage(c, html=False):\n         c.run(cmd)\n \n \n-@task(post=[lint])\n+@task\n def black(c):\n     \"black-format things.\"\n     c.run(\"python -m black .\")\n", "instance_id": "LuteOrg__lute-v3-509", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in its intent to refactor the codebase to remove implicit dependencies on `db.session` by introducing explicit session passing and repository patterns. It outlines the general goal of decoupling the code from Flask's managed database session and provides specific areas of the codebase to address (e.g., business repositories, ORM-level repositories, services, etc.). However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, it lacks explicit examples of expected input/output for the refactored components, does not fully specify how the session should be managed in edge cases (e.g., transaction boundaries or rollback scenarios), and does not clarify the exact impact on existing unit tests beyond a general note. Additionally, the statement mentions that the work is in progress on a specific branch, but it does not detail the current state of that branch or potential conflicts. Despite these minor gaps, the problem's core objective and scope are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated at 0.75, placing it in the 'Hard' category (0.6-0.8). This score reflects the significant complexity and depth of understanding required to execute the refactoring across a large codebase. Several factors contribute to this assessment:\n\n1. **Scope and Depth of Code Changes**: The refactoring spans multiple layers of the application, including models, controllers, services, and utility modules, as evidenced by the extensive code changes in numerous files (e.g., `app_factory.py`, `backup/service.py`, `book/model.py`, etc.). It involves converting static methods to instance methods with explicit session passing, creating repository classes, and updating dependency injection patterns. This is not a localized change but a systemic one, impacting the architecture of how database interactions are handled throughout the application.\n\n2. **Technical Concepts Involved**: Solving this requires a deep understanding of several advanced concepts, including Flask's application context and database session management, SQLAlchemy's session lifecycle, object-oriented design patterns (e.g., repository pattern), and dependency injection. Additionally, familiarity with Python's module system and how imports affect runtime behavior is crucial, as the refactoring removes direct `db` imports in favor of explicit session passing. The developer must also understand the implications of these changes on transaction management and thread safety, especially in a Flask environment.\n\n3. **Edge Cases and Error Handling**: While the problem statement does not explicitly mention edge cases, the code changes reveal potential complexities in handling session lifecycle (e.g., ensuring sessions are properly committed or rolled back in error scenarios) and maintaining data consistency across refactored components. For instance, in services like `BackupService` or `StatsService`, improper session handling could lead to data integrity issues. The refactoring also impacts unit tests, requiring updates to mock or provide session objects, which adds another layer of complexity.\n\n4. **Impact on Codebase Architecture**: This refactoring fundamentally alters how database access is managed, moving from an implicit global `db.session` to an explicit session-passing model. This change affects nearly every interaction with the database, requiring a thorough understanding of the existing architecture to avoid introducing bugs or performance regressions. The need to update or create repository classes for each model (e.g., `BookRepository`, `TermRepository`) and ensure consistent session usage across services and controllers adds to the challenge.\n\nWhile this task does not reach the 'Very Hard' category (0.8-1.0) due to the absence of highly specialized domain knowledge or system-level considerations (e.g., distributed systems or low-level optimizations), it demands a high level of expertise in web application architecture and database interaction patterns. The extensive scope, combined with the need for precision to avoid breaking existing functionality, justifies a difficulty score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "query string not updated in liveview example \ud83d\udc1b\n**Describe the bug** \ud83d\udc1b\r\nIn liveview the query string does not update\r\n\r\n**To Reproduce** \ud83d\udc68\u200d\ud83d\udd2c\r\n1. Start the testc12.nim example\r\n2. Open http://localhost:5000/search?q=1\r\n3. Note that it shows \"You search: 1\" as expected\r\n4.  Open http://localhost:5000/search?q=2\r\n5.  It still shows \"You search: 1\"\r\n\r\n**Expected behavior** \ud83e\udd14\r\nI would expect the displayed value to change when the query parameter is changed.\r\n\r\n**Environment**\r\nHappyX 4.4.1\r\nNim 2.1.9 [Linux: amd64]\r\nopensuse Tumbleweed\r\nFirefox 127.0.2\nquery string not updated in liveview example \ud83d\udc1b\n**Describe the bug** \ud83d\udc1b\r\nIn liveview the query string does not update\r\n\r\n**To Reproduce** \ud83d\udc68\u200d\ud83d\udd2c\r\n1. Start the testc12.nim example\r\n2. Open http://localhost:5000/search?q=1\r\n3. Note that it shows \"You search: 1\" as expected\r\n4.  Open http://localhost:5000/search?q=2\r\n5.  It still shows \"You search: 1\"\r\n\r\n**Expected behavior** \ud83e\udd14\r\nI would expect the displayed value to change when the query parameter is changed.\r\n\r\n**Environment**\r\nHappyX 4.4.1\r\nNim 2.1.9 [Linux: amd64]\r\nopensuse Tumbleweed\r\nFirefox 127.0.2\n", "patch": "diff --git a/.github/workflows/language_bindings.yml b/.github/workflows/language_bindings.yml\nindex 2f187c151..eb5ae1283 100644\n--- a/.github/workflows/language_bindings.yml\n+++ b/.github/workflows/language_bindings.yml\n@@ -80,9 +80,9 @@ jobs:\n           echo \"Win Module compiled with\"\n           echo \"nim c --app:lib --out:../bindings/python/happyx/happyx.pyd -d:useRealtimeGC --mm:arc --tlsEmulation:off --passL:-static --t:-flto --l:-flto --opt:speed --threads:off -d:release -d:httpx -d:export2py happyx\"\n           echo \"Linux amd64\"\n-          nim c --app:lib --out:../bindings/python/happyxpy/happyx_unix_amd64.so -d:useRealtimeGC --mm:arc -t:-flto -l:-flto --opt:speed --threads:off -d:release -d:happyxDebug -x:off -a:off -d:beast -d:export2py happyx\n+          nim c --app:lib --out:../bindings/python/happyx/happyx_unix_amd64.so -d:useRealtimeGC --mm:arc -t:-flto -l:-flto --opt:speed --threads:off -d:release -d:happyxDebug -x:off -a:off -d:beast -d:export2py happyx\n           echo \"Linux arm64\"\n-          nim c --app:lib --cpu:arm64 --out:../bindings/python/happyxpy/happyx_unix_arm64.so -d:useRealtimeGC --mm:arc -t:-flto -l:-flto --opt:speed --threads:off -d:release -d:happyxDebug -x:off -a:off -d:beast -d:export2py happyx\n+          nim c --app:lib --cpu:arm64 --out:../bindings/python/happyx/happyx_unix_arm64.so -d:useRealtimeGC --mm:arc -t:-flto -l:-flto --opt:speed --threads:off -d:release -d:happyxDebug -x:off -a:off -d:beast -d:export2py happyx\n         shell: bash\n       - name: Build Python Package \ud83c\udfd7\n         run: |\ndiff --git a/README.md b/README.md\nindex d17b04ccf..06d6602fd 100755\n--- a/README.md\n+++ b/README.md\n@@ -19,7 +19,7 @@\n \n [![VS Code Plugin](https://img.shields.io/badge/Plugin-1b1e2b?style=for-the-badge&logo=visualstudiocode&logoColor=f1fa8c&label=VS%20Code&labelColor=2b2e3b)](https://github.com/HapticX/hpx-vs-code)\n \n-[![Python Bindings](https://img.shields.io/badge/Bindings-1b1e2b?style=for-the-badge&logo=python&logoColor=f1fa8c&label=Python&labelColor=2b2e3b)](https://pypi.org/project/happyxpy/)\n+[![Python Bindings](https://img.shields.io/badge/Bindings-1b1e2b?style=for-the-badge&logo=python&logoColor=f1fa8c&label=Python&labelColor=2b2e3b)](https://pypi.org/project/happyx/)\n [![NodeJS Bindings](https://img.shields.io/badge/Bindings-1b1e2b?style=for-the-badge&logo=npm&logoColor=f1fa8c&label=NodeJS&labelColor=2b2e3b)](https://www.npmjs.com/package/happyx)\n \n </div>\n@@ -53,7 +53,7 @@ HappyX is macro-oriented web framework so you mustn't write a lot of code \u2728\n - Automatic translate with `-d:hpxTranslate` or `-d:translate` flags.\n - CLI tool for `creating`, `serving` and `building` your projects.\n - Hot code reloading (only for `SPA` projects as of now).\n-- [Python bindings](https://pypi.org/project/happyxpy/)\n+- [Python bindings](https://pypi.org/project/happyx/)\n - [NodeJS bindings](https://www.npmjs.com/package/happyx)\n \n ## Minimal Example \ud83d\udc69\u200d\ud83d\udcbb\n@@ -101,7 +101,7 @@ nimble install https://github.com/HapticX/happyx\n \n ### PyPI\n ```shell\n-pip install happyxpy\n+pip install happyx\n ```\n \n ## Usage \u25b6\ndiff --git a/bindings/python/MANIFEST.in b/bindings/python/MANIFEST.in\nindex e373ebe0f..bee3b735d 100644\n--- a/bindings/python/MANIFEST.in\n+++ b/bindings/python/MANIFEST.in\n@@ -1,3 +1,3 @@\n # MANIFEST.in\n-include happyxpy/*.pyd\n-include happyxpy/*.so\n+include happyx/*.pyd\n+include happyx/*.so\ndiff --git a/bindings/python/README.md b/bindings/python/README.md\nindex 6f539749a..d78db77aa 100644\n--- a/bindings/python/README.md\n+++ b/bindings/python/README.md\n@@ -15,7 +15,7 @@\n \n You can install HappyX via `pypi`:\n ```bash\n-pip install happyxpy\n+pip install happyx\n ```\n \n ## Usage \ud83d\udd0c\n@@ -23,7 +23,7 @@ pip install happyxpy\n ### Hello World \ud83d\udc4b\n \n ```py\n-from happyxpy import Server\n+from happyx import Server\n \n \n app = Server('127.0.0.1', 5000)  # host and port are optional params\n@@ -41,7 +41,7 @@ app.start()\n ### JSON/HTML/File Responses \ud83d\udee0\n \n ```py\n-from happyxpy import Server, JsonResponse, HtmlResponse, FileResponse\n+from happyx import Server, JsonResponse, HtmlResponse, FileResponse\n \n \n app = Server()\ndiff --git a/bindings/python/happyxpy/__init__.py b/bindings/python/happyx/__init__.py\nsimilarity index 99%\nrename from bindings/python/happyxpy/__init__.py\nrename to bindings/python/happyx/__init__.py\nindex ad164b886..17f0cd538 100644\n--- a/bindings/python/happyxpy/__init__.py\n+++ b/bindings/python/happyx/__init__.py\n@@ -1,7 +1,7 @@\n from collections import defaultdict\n from enum import IntEnum\n \n-from .server import Server, happyx\n+from .server import HappyX, happyx\n \n try:\n     from jinja2 import Template\ndiff --git a/bindings/python/happyxpy/constants.py b/bindings/python/happyx/constants.py\nsimilarity index 100%\nrename from bindings/python/happyxpy/constants.py\nrename to bindings/python/happyx/constants.py\ndiff --git a/bindings/python/happyx/happyx_win.pyd b/bindings/python/happyx/happyx_win.pyd\nnew file mode 100644\nindex 000000000..f3553f834\nBinary files /dev/null and b/bindings/python/happyx/happyx_win.pyd differ\ndiff --git a/bindings/python/happyxpy/server.py b/bindings/python/happyx/server.py\nsimilarity index 96%\nrename from bindings/python/happyxpy/server.py\nrename to bindings/python/happyx/server.py\nindex 6237f4277..c0f1ec27c 100644\n--- a/bindings/python/happyxpy/server.py\n+++ b/bindings/python/happyx/server.py\n@@ -1,6 +1,7 @@\n import asyncio\n import platform\n-from typing import Callable, Any, List\n+from threading import Thread\n+from typing import Callable, Any, List, Awaitable\n from re import match\n \n from .constants import SWAGGER_HTML_SOURCE, REDOC_HTML_SOURCE\n@@ -9,22 +10,23 @@\n _platform = platform.system().lower()\n \n if _platform == 'windows':\n-    import happyxpy.happyx_win as happyx\n+    import happyx.happyx_win as happyx\n else:\n     if _cpu == 'arm':\n-        import happyxpy.happyx_unix_arm as happyx\n+        import happyx.happyx_unix_arm as happyx\n     elif _cpu in ['arm64', 'aarch64']:\n-        import happyxpy.happyx_unix_arm64 as happyx\n+        import happyx.happyx_unix_arm64 as happyx\n     elif _cpu in ['amd64', 'x86_64']:\n-        import happyxpy.happyx_unix_amd64 as happyx\n+        import happyx.happyx_unix_amd64 as happyx\n     else:\n-        import happyxpy.happyx_unix_amd64 as happyx\n+        import happyx.happyx_unix_amd64 as happyx\n \n \n Callback = Callable[[Any], Any]\n+AsyncCallback = Awaitable[Any]\n \n \n-class Server:\n+class HappyX:\n     \"\"\"\n     Provides HTTP Server made with HappyX\n     \"\"\"\n@@ -64,6 +66,7 @@ def __init__(\n         self.host = host\n         self.port = port\n         self._server = happyx.new_server(host, port)\n+        self._loop = None\n         self.path = path\n         # OpenAPI docs\n         self._swagger_url = swagger_url\n@@ -345,7 +348,7 @@ def mount(self, route: str = None, other = None) -> None:\n         route {str} -- mounting path (default taken from other Server)\n         other {Server} -- other server that should be mounted (default is None)\n         \"\"\"\n-        if not isinstance(other, Server):\n+        if not isinstance(other, HappyX):\n             raise ValueError('mounting canceled! Other is not Server')\n         if route is None:\n             if other.path is None:\ndiff --git a/bindings/python/happyxpy/happyx_win.pyd b/bindings/python/happyxpy/happyx_win.pyd\ndeleted file mode 100644\nindex c320e2c35..000000000\nBinary files a/bindings/python/happyxpy/happyx_win.pyd and /dev/null differ\ndiff --git a/bindings/python/setup.py b/bindings/python/setup.py\nindex c7d9271ed..ef50e736e 100644\n--- a/bindings/python/setup.py\n+++ b/bindings/python/setup.py\n@@ -11,14 +11,14 @@\n \n \n if _platform == 'windows':\n-    import happyxpy.happyx_win as happyx\n+    import happyx.happyx_win as happyx\n else:\n     if _cpu == 'arm64':\n-        import happyxpy.happyx_unix_arm64 as happyx\n+        import happyx.happyx_unix_arm64 as happyx\n     elif _cpu in ['amd64', 'x86_64']:\n-        import happyxpy.happyx_unix_amd64 as happyx\n+        import happyx.happyx_unix_amd64 as happyx\n     else:\n-        import happyxpy.happyx_unix_amd64 as happyx\n+        import happyx.happyx_unix_amd64 as happyx\n \n \n # Load readme\n@@ -27,7 +27,7 @@\n \n \n setup(\n-    name='happyxpy',\n+    name='happyx',\n     description='HappyX web framework bindings for Python \ud83d\udc0d',\n     long_description=long_description,\n     long_description_content_type='text/markdown',\n@@ -40,7 +40,7 @@\n     packages=find_packages(),\n     include_package_data=True,\n     install_requires=['jinja2'],\n-    py_modules=['happyxpy'],\n+    py_modules=['happyx'],\n     license='MIT',\n     classifiers=[\n         'Development Status :: 5 - Production/Stable',\ndiff --git a/build_py.cmd b/build_py.cmd\nindex 2cfb4af17..9ed0f9aa2 100644\n--- a/build_py.cmd\n+++ b/build_py.cmd\n@@ -3,7 +3,7 @@ cd src\n \n nim c ^\n   --app:lib ^\n-  --out:../bindings/python/happyxpy/happyx_win.pyd ^\n+  --out:../bindings/python/happyx/happyx_win.pyd ^\n   --tlsEmulation:off ^\n   --mm:arc ^\n   --passL:-static ^\ndiff --git a/build_py_debug.cmd b/build_py_debug.cmd\nindex c706c2596..1f00f7c2f 100644\n--- a/build_py_debug.cmd\n+++ b/build_py_debug.cmd\n@@ -1,4 +1,4 @@\n echo \"Build Python Win Bindings\"\n cd src\n-nim c --app:lib --out:../bindings/python/happyxpy/happyx_win.pyd -d:useRealtimeGC --mm:arc --tlsEmulation:off --passL:-static --threads:off -d:debug -d:httpx -d:export2py happyx\n+nim c --app:lib --out:../bindings/python/happyx/happyx_win.pyd -d:useRealtimeGC --mm:arc --tlsEmulation:off --passL:-static --threads:off -d:debug -d:httpx -d:export2py happyx\n cd ../\ndiff --git a/examples/website/src/docs/introduction.nim b/examples/website/src/docs/introduction.nim\nindex 8364be34e..1a108890b 100644\n--- a/examples/website/src/docs/introduction.nim\n+++ b/examples/website/src/docs/introduction.nim\n@@ -82,8 +82,8 @@ component Introduction:\n           tTr:\n             tTd: \"PyPI\"\n             tTd:\n-              tA(href = \"https://pypi.org/project/happyxpy/\"):\n-                tImg(class = \"h-12 lg:h-10 xl:h-8\", alt = \"PyPI Downloads\", src = \"https://img.shields.io/pypi/dm/happyxpy?style=for-the-badge\")\n+              tA(href = \"https://pypi.org/project/happyx/\"):\n+                tImg(class = \"h-12 lg:h-10 xl:h-8\", alt = \"PyPI Downloads\", src = \"https://img.shields.io/pypi/dm/happyx?style=for-the-badge\")\n           tTr:\n             tTd: \"npm\"\n             tTd:\ndiff --git a/examples/website/src/ui/code/python.nim b/examples/website/src/ui/code/python.nim\nindex 71a126079..d7825601e 100644\n--- a/examples/website/src/ui/code/python.nim\n+++ b/examples/website/src/ui/code/python.nim\n@@ -1,5 +1,5 @@\n const\n-  pythonHelloWorldExample* = \"\"\"from happyxpy import Server\n+  pythonHelloWorldExample* = \"\"\"from happyx import Server\n \n # Create application\n app = Server('127.0.0.1', 5000)\n@@ -41,7 +41,7 @@ def handle(user_id: int):\n     print(user_id)\n     return {'response': user_id}\n \"\"\"\n-  pythonCustomRouteParamType* = \"\"\"from happyxpy import Server, register_route_param_type\n+  pythonCustomRouteParamType* = \"\"\"from happyx import Server, register_route_param_type\n \n \n app = Server()\n@@ -62,7 +62,7 @@ def handle(data: MyUniqueIdentifier):\n \n app.start()\n \"\"\"\n-  pySsrAdvancedHelloWorld* = \"\"\"from happyxpy import Server, JsonResponse, Response\n+  pySsrAdvancedHelloWorld* = \"\"\"from happyx import Server, JsonResponse, Response\n \n \n app = Server('127.0.0.1', 5000)\n@@ -110,7 +110,7 @@ def test_all():\n \n app.start()\n \"\"\"\n-  pySsrAdditionalRoutes* = \"\"\"from happyxpy import Server, HttpRequest\n+  pySsrAdditionalRoutes* = \"\"\"from happyx import Server, HttpRequest\n \n \n app = Server(\"127.0.0.1\", 5000)\n@@ -177,7 +177,7 @@ def read_users():\n \n app.start()\n \"\"\"\n-  pyMongoDb* = \"\"\"from happyxpy import Server\n+  pyMongoDb* = \"\"\"from happyx import Server\n from pymongo import MongoClient\n from datetime import datetime\n \n@@ -224,7 +224,7 @@ def read_users():\n \n app.start()\n \"\"\"\n-  pySqlalchemy* = \"\"\"from happyxpy import Server\n+  pySqlalchemy* = \"\"\"from happyx import Server\n from sqlalchemy import create_engine, Column, Integer, DateTime\n from sqlalchemy.ext.declarative import declarative_base\n from sqlalchemy.orm import sessionmaker\n@@ -280,7 +280,7 @@ def read_users():\n \n app.start()\n \"\"\"\n-  pySqlalchemy1* = \"\"\"from happyxpy import Server\n+  pySqlalchemy1* = \"\"\"from happyx import Server\n from sqlalchemy import create_engine, Column, Integer, DateTime\n from sqlalchemy.ext.declarative import declarative_base\n from sqlalchemy.orm import sessionmaker\n@@ -337,7 +337,7 @@ def read_users():\n app.start()\n \"\"\"\n   pyPostgreSql* = \"\"\"import psycopg2\n-from happyxpy import Server\n+from happyx import Server\n from datetime import datetime\n \n # Connect to postgresql\n@@ -407,7 +407,7 @@ def read_users():\n app.start()\n \"\"\"\n   pyPostgreSql1* = \"\"\"import psycopg2\n-from happyxpy import Server\n+from happyx import Server\n from datetime import datetime\n \n # Connect to postgresql\n@@ -473,7 +473,7 @@ def read_users():\n     return response\n \n app.start()\"\"\"\n-  pyMounting* = \"\"\"from happyxpy import Server\n+  pyMounting* = \"\"\"from happyx import Server\n \n app = Server()\n profile = Server()\ndiff --git a/happyx.nimble b/happyx.nimble\nindex 60f053488..469bc9309 100755\n--- a/happyx.nimble\n+++ b/happyx.nimble\n@@ -2,7 +2,7 @@\n \n description = \"Macro-oriented asynchronous web-framework written with \u2665\"\n author = \"HapticX\"\n-version = \"4.4.1\"\n+version = \"4.4.2\"\n license = \"MIT\"\n srcDir = \"src\"\n installExt = @[\"nim\"]\ndiff --git a/src/happyx/cli/consts.nim b/src/happyx/cli/consts.nim\nindex bdc94cefc..80a373df4 100644\n--- a/src/happyx/cli/consts.nim\n+++ b/src/happyx/cli/consts.nim\n@@ -18,7 +18,7 @@ nimble install happyx@#head\n \"\"\"\n   readmeInstallPy* = \"\"\"## Install\n ```shell\n-pip install happyxpy\n+pip install happyx\n ```\n \"\"\"\n   readmeInstallJs* = \"\"\"## Install\ndiff --git a/src/happyx/cli/html2tag_command.nim b/src/happyx/cli/html2tag_command.nim\nindex c4c774163..8d8333712 100644\n--- a/src/happyx/cli/html2tag_command.nim\n+++ b/src/happyx/cli/html2tag_command.nim\n@@ -2,7 +2,7 @@ import\n   ./utils\n \n \n-proc html2tagCommand*(output: string = \"\", args: seq[string]): int =\n+proc html2tagCommand*(output: string = \"\", toProc: bool = false, args: seq[string]): int =\n   ## Converts HTML into `buildHtml` macro\n   styledEcho fgGreen, emoji[\"\ud83d\udd28\"](), \" Convert HTML into happyx file\"\n   var o = output\n@@ -33,7 +33,11 @@ proc html2tagCommand*(output: string = \"\", args: seq[string]): int =\n \n   var\n     tree = parseHtml(input)\n-    outputData = \"import happyx\\n\\n\\nvar html = buildHtml:\\n\"\n+    outputData =\n+      if toProc:\n+        \"import happyx\\n\\n\\nproc \" & args[0].replace(re2\"\\-(\\w|\\d)\", \"_$1\") & \"*(): TagRef = buildHtml:\"\n+      else:\n+        \"import happyx\\n\\n\\nvar html = buildHtml:\\n\"\n   xmlTree2Text(outputData, tree, 2)\n \n   outputData = outputData.replace(re2\"\"\"( +)(tScript.*?:)\\s+(\\\"{3})\\s*([\\s\\S]+?)(\\\"{3})\"\"\", \"$1$2 $3\\n  $1$4$1$5\")\ndiff --git a/src/happyx/cli/serve_command.nim b/src/happyx/cli/serve_command.nim\nindex ddd0f73de..0bfa65363 100644\n--- a/src/happyx/cli/serve_command.nim\n+++ b/src/happyx/cli/serve_command.nim\n@@ -2,7 +2,7 @@ import\n   ./utils\n \n \n-proc serveCommand*(host: string = \"0.0.0.0\", port: int = 80): int =\n+proc serveCommand*(host: string = \"0.0.0.0\", port: int = 80, buildDirectory: string = \"build\"): int =\n   ## Serve SPA for production\n   var\n     project = compileProject()\n@@ -22,17 +22,17 @@ proc serveCommand*(host: string = \"0.0.0.0\", port: int = 80): int =\n   \n   serve host, port:\n     get \"/\":\n-      let f = open(getCurrentDir() / \"build\" / \"index.html\")\n+      let f = open(getCurrentDir() / buildDirectory / \"index.html\")\n       var data = f.readAll()\n       f.close()\n       req.answerHtml(data)\n  \n     get \"/{file:path}\":\n       var result = \"\"\n-      let path = getCurrentDir() / \"build\" / file.replace('\\\\', '/').replace('/', DirSep)\n+      let path = getCurrentDir() / buildDirectory / file.replace('\\\\', '/').replace('/', DirSep)\n       echo \"File: \", file\n       echo \"Path: \", path\n       if fileExists(path):\n         await req.answerFile(path, forceResponse = true)\n   shutdownCli()\n-  QuitSuccess\n\\ No newline at end of file\n+  QuitSuccess\ndiff --git a/src/happyx/cli/utils.nim b/src/happyx/cli/utils.nim\nindex cd615a0b9..cd86700c3 100644\n--- a/src/happyx/cli/utils.nim\n+++ b/src/happyx/cli/utils.nim\n@@ -547,11 +547,11 @@ proc xml2Text*(xml: XmlNode): string =\n       result &= \"(\"\n       var attrs: seq[string] = @[]\n       for key, value in xml.attrs:\n-        let k = if key notin NimKeywords: key else: \"`\" & key & \"`\"\n+        var k = if key notin NimKeywords: key else: \"`\" & key & \"`\"\n         if value == \"\":\n-          attrs.add(k)\n+          attrs.add(\"\\\"\" & k & \"\\\"\")\n         else:\n-          attrs.add(k & \" = \\\"\" & value & \"\\\"\")\n+          attrs.add(\"\\\"\" & k & \"\\\" = \\\"\" & value & \"\\\"\")\n       result &= attrs.join(\", \") & \")\"\n     if xml.len > 0:\n       result &= \":\"\ndiff --git a/src/happyx/core/constants.nim b/src/happyx/core/constants.nim\nindex f712f7eb1..acbf9f246 100755\n--- a/src/happyx/core/constants.nim\n+++ b/src/happyx/core/constants.nim\n@@ -90,7 +90,7 @@ const\n     \"slot\", \"small\", \"source\", \"span\", \"strong\", \"style\", \"sub\", \"summary\", \"sup\",\n     \"svg\", \"cicle\", \"path\", \"g\",\n     \"table\", \"tbody\", \"td\", \"template\", \"textarea\", \"tfoot\", \"th\", \"thead\", \"time\",\n-    \"title\", \"tr\", \"track\", \"u\", \"ul\", \"var\", \"video\", \"wbr\",\n+    \"title\", \"tr\", \"track\", \"text\", \"u\", \"ul\", \"var\", \"video\", \"wbr\",\n   ]\n   availableCryptoMethods = [\"sha224\", \"sha256\", \"sha384\", \"sha512\"]\n   # Nim version\n@@ -99,7 +99,7 @@ const\n   # Framework version\n   HpxMajor* = 4\n   HpxMinor* = 4\n-  HpxPatch* = 1\n+  HpxPatch* = 2\n   HpxVersion* = $HpxMajor & \".\" & $HpxMinor & \".\" & $HpxPatch\n \n \ndiff --git a/src/happyx/hpx.nim b/src/happyx/hpx.nim\nindex 4689278c9..e8b95ce22 100755\n--- a/src/happyx/hpx.nim\n+++ b/src/happyx/hpx.nim\n@@ -22,9 +22,9 @@ import illwill except\n \n proc buildCommandAux(optSize: bool = false, no_compile: bool = false): int = buildCommand(optSize, no_compile)\n proc mainHelpMessageAux() = mainHelpMessage()\n-proc html2tagCommandAux(output: string = \"\", args: seq[string]): int = html2tagCommand(output, args)\n+proc html2tagCommandAux(output: string = \"\", toProc: bool = false, args: seq[string]): int = html2tagCommand(output, toProc, args)\n proc updateCommandAux(args: seq[string]): int = updateCommand(args)\n-proc serveCommandAux(host: string = \"0.0.0.0\", port: int = 80): int = serveCommand(host, port)\n+proc serveCommandAux(host: string = \"0.0.0.0\", port: int = 80, buildDirectory: string = \"build\"): int = serveCommand(host, port, buildDirectory)\n proc projectInfoCommandAux(): int = projectInfoCommand()\n proc flagsCommandAux(): int = flagsCommand()\n proc translateCsvCommandAux(filename: string, output: string = \"\"): int =\ndiff --git a/src/happyx/spa/renderer.nim b/src/happyx/spa/renderer.nim\nindex 06e88b2d2..ce458b704 100644\n--- a/src/happyx/spa/renderer.nim\n+++ b/src/happyx/spa/renderer.nim\n@@ -121,7 +121,14 @@ var\n   currentRoute*: cstring = \"/\"  ## Current route path\n   scopedCycleCounter*: int = 0\n when enableLiveViews and not defined(js):\n-  var liveviewRoutes* = newTable[string, proc(): TagRef]()\n+  import std/httpcore\n+  var liveviewRoutes* = newTable[string, proc(\n+    query: StringTableRef,\n+    queryArr: TableRef[string, seq[string]],\n+    reqMethod: HttpMethod,\n+    inCookies: StringTableRef,\n+    headers: HttpHeaders\n+  ): TagRef]()\n   var components* = newTable[string, BaseComponent]()\n when enableDefaultComponents:\n   var\n@@ -202,8 +209,10 @@ else:\n     componentsResult[comp.uniqCompId] = \"route:\" & path\n   proc js*(comp: BaseComponent, script: string) =\n     componentsResult[comp.uniqCompId] = \"script:\" & fmt\"<script>{script}</script>\"\n-  proc rerender*(hostname, urlPath: string) =\n-    requestResult[hostname] = \"rerender:\" & liveviewRoutes[urlPath]().children[1].ugly()\n+  proc rerender*(query, queryArr, reqMethod, inCookies, headers: auto, hostname, urlPath: string) =\n+    requestResult[hostname] = \"rerender:\" & liveviewRoutes[urlPath](\n+      query, queryArr, reqMethod, inCookies, headers\n+    ).children[1].ugly()\n   proc bck*(hostname, urlPath: string) =\n     requestResult[hostname] = \"bck\"\n   proc frwrd*(hostname, urlPath: string) =\n@@ -302,7 +311,6 @@ when defined(js):\n         if vdom.childNodes[i].nodeType != NodeType.TextNode:\n           diff(vdom.childNodes[i].TagRef, dom.childNodes[i])\n   proc prerenderLazyProcs*(tag: TagRef) {.exportc: \"prrndr\".} =\n-    echo tag.nodeType, \", \", tag.lazy\n     if tag.lazy:\n       let t = tag.lazyFunc()\n       t.prerenderLazyProcs()\ndiff --git a/src/happyx/spa/state.nim b/src/happyx/spa/state.nim\nindex 8e9412389..88bf7dcb8 100644\n--- a/src/happyx/spa/state.nim\n+++ b/src/happyx/spa/state.nim\n@@ -49,7 +49,7 @@ func remember*[T](val: T): State[T] =\n   State[T](val: val)\n \n \n-func watchImpl[T](state: State[T], o, n: T) =\n+func watchImpl*[T](state: State[T], o, n: T) =\n   for w in state.watchers:\n     w(o, n)\n \n@@ -68,7 +68,7 @@ else:\n     if self.watchers.len > 0:\n       self.watchImpl(self.val, value)\n     self.val = value\n-    rerender(hostname, urlPath)\n+    rerender(query, queryArr, reqMethod, inCookies, headers, hostname, urlPath)\n \n \n func `$`*[T](self: State[T]): string =\n@@ -136,7 +136,7 @@ else:\n         self.watchImpl(before, self.val)\n       else:\n         `op`(self.val, other.val)\n-      rerender(hostname, urlPath)\n+      rerender(query, queryArr, reqMethod, inCookies, headers, hostname, urlPath)\n     template `funcname`*[T](other: T, self: State[T]) =\n       if self.watchers.len > 0:\n         let before = self.val\n@@ -144,7 +144,7 @@ else:\n         self.watchImpl(before, self.val)\n       else:\n         `op`(self.val, other)\n-      rerender(hostname, urlPath)\n+      rerender(query, queryArr, reqMethod, inCookies, headers, hostname, urlPath)\n     template `funcname`*[T](self: State[T], other: T) =\n       if self.watchers.len > 0:\n         let before = self.val\n@@ -152,7 +152,7 @@ else:\n         self.watchImpl(before, self.val)\n       else:\n         `op`(self.val, other)\n-      rerender(hostname, urlPath)\n+      rerender(query, queryArr, reqMethod, inCookies, headers, hostname, urlPath)\n \n \n template boolOperator(funcname, op: untyped): untyped =\n@@ -338,7 +338,7 @@ else:\n     if self.watchers.len > 0:\n       self.watchImpl(self.val, value)\n     self.val = value\n-    rerender(hostname, urlPath)\n+    rerender(query, queryArr, reqMethod, inCookies, headers, hostname, urlPath)\n \n \n func `[]`*[T, U](self: State[array[T, U]], idx: int): T =\ndiff --git a/src/happyx/spa/tag.nim b/src/happyx/spa/tag.nim\nindex 7189c5636..b9afeb032 100644\n--- a/src/happyx/spa/tag.nim\n+++ b/src/happyx/spa/tag.nim\n@@ -203,7 +203,7 @@ when defined(js):\n         e.setAttribute(cstring(key), cstring(val))\n \n   proc newElement(name: string): TagRef {.exportc: \"nwelm\".} =\n-    if name.toLower() in [\"svg\", \"path\", \"circle\", \"rect\", \"g\", \"defs\", \"animate\", \"ellipse\", \"polygon\", \"mask\"]:\n+    if name.toLower() in SvgElements:\n       result = TagRef()\n       let n = cstring(name)\n       {.emit: \"`result` = document.createElementNS('http://www.w3.org/2000/svg', `n`)\".}\ndiff --git a/src/happyx/ssr/liveviews/liveviews.nim b/src/happyx/ssr/liveviews/liveviews.nim\nindex b53505a68..61fc4f758 100644\n--- a/src/happyx/ssr/liveviews/liveviews.nim\n+++ b/src/happyx/ssr/liveviews/liveviews.nim\n@@ -69,10 +69,20 @@ proc handleLiveViews*(body: NimNode) =\n                   newCall(\"tDiv\", newNimNode(nnkExprEqExpr).add(ident\"id\", newLit\"scripts\"))\n                 ))\n               ))\n-            ), @[ident\"TagRef\"])\n+            ), @[\n+              ident\"TagRef\",\n+              newIdentDefs(ident\"query\", ident\"StringTableRef\", newEmptyNode()),\n+              newIdentDefs(ident\"queryArr\", newNimNode(nnkBracketExpr).add(ident\"TableRef\", ident\"string\", newNimNode(nnkBracketExpr).add(ident\"seq\", ident\"string\")), newEmptyNode()),\n+              newIdentDefs(ident\"reqMethod\", ident\"HttpMethod\", newEmptyNode()),\n+              newIdentDefs(ident\"inCookies\", ident\"StringTableRef\", newEmptyNode()),\n+              newIdentDefs(ident\"headers\", ident\"HttpHeaders\", newEmptyNode()),\n+            ])\n           ),\n         )),\n-        newLetStmt(ident\"_html\", newCall(newNimNode(nnkBracketExpr).add(ident\"liveviewRoutes\", path))),\n+        newLetStmt(ident\"_html\", newCall(\n+          newNimNode(nnkBracketExpr).add(ident\"liveviewRoutes\", path),\n+          ident\"query\", ident\"queryArr\", ident\"reqMethod\", ident\"inCookies\", ident\"headers\",\n+        )),\n         newCall(\"add\", newNimNode(nnkBracketExpr).add(ident\"_html\", newLit(1)), newCall(\"buildHtml\", newStmtList(script))),\n         newNimNode(nnkReturnStmt).add(ident\"_html\"),\n       ))\n", "instance_id": "HapticX__happyx-323", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the query string not updating in a LiveView example. It provides specific steps to reproduce the issue, the expected behavior, and the environment details, which are helpful for understanding the context. However, there are minor ambiguities and missing details. For instance, it does not specify whether the issue is related to client-side rendering, server-side state management, or a specific part of the LiveView implementation. Additionally, there are no mentions of edge cases or specific constraints that might affect the solution. The problem description could benefit from more technical depth regarding the expected interaction between the query parameters and the LiveView rendering logic. Hence, I assign a clarity score of 2 (Mostly Clear) due to the presence of minor missing details.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category with a score of 0.65 due to several factors. First, while the provided code changes primarily involve renaming and restructuring Python bindings (e.g., changing 'happyxpy' to 'happyx'), these changes do not directly address the core issue of query string updates in LiveView. The relevant changes in the LiveView handling (e.g., modifications in `liveviews.nim` and `renderer.nim`) indicate a deeper issue related to how query parameters and HTTP request data are passed to the rendering logic. Solving the actual bug requires understanding and modifying the LiveView mechanism to handle query parameter updates dynamically, which involves multiple technical concepts such as HTTP request handling, state management in a Single Page Application (SPA), and server-side rendering (SSR) interactions. The changes in `liveviews.nim` to include query parameters, HTTP method, cookies, and headers in the rendering function suggest a need to integrate these elements into the rendering process, which adds complexity. Additionally, the scope of changes spans multiple files and modules, requiring a good grasp of the HappyX framework's architecture, particularly its SPA and SSR components. While the problem does not seem to involve extensive edge cases or performance-critical modifications, it does require careful handling of state updates and potential synchronization issues between client and server. Therefore, I assign a difficulty score of 0.65, reflecting a hard problem that demands a deep understanding of the framework and targeted, complex modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for finger_unlock action in the /houses/<house_id>/activities API response\nI have a Yale Keypad Touch configured to work with my August WiFi lock. In Home Assistant, I have created an automation that gets triggered when the August WiFi lock is unlocked. This automation works fine when I enter my PIN on the Yale Keypad. However, when I try to use my fingerprint to unlock the automation doesn't get triggered. \r\n\r\nAfter reading through the **yalexs** code base, I believe the automation isn't triggered because the `finger_unlock` action isn't expected in the response from the `/houses/<house_id>/activities` API.\r\n\r\nBelow is a sample event structure where the `finger_unlock` action appears.\r\n\r\n```\r\n{\r\n    \"id\": \"*** Redacted ID ***\",\r\n    \"timestamp\": 1726770544000,\r\n    \"icon\": \"https://d33mytkkohwnk6.cloudfront.net/app/ActivityFeedIcons/finger_unlock@3x.png\",\r\n    \"action\": \"finger_unlock\",\r\n    \"deviceID\": \"*** Redacted Device ID ***\",\r\n    \"deviceType\": \"lock\",\r\n    \"user\": {\r\n        \"UserID\": \"*** Redacted User ID ***\",\r\n        \"FirstName\": \"Thomas\",\r\n        \"LastName\": \"Nelson\"\r\n    },\r\n    \"title\": \"<b>Thomas Nelson</b> unlocked <b>Front Door</b> with fingerprint\"\r\n}\r\n```\r\n\r\nPlease let me know if any more details would be helpful.\n", "patch": "diff --git a/yalexs/activity.py b/yalexs/activity.py\nindex ef452fc..0b7d3c7 100644\n--- a/yalexs/activity.py\n+++ b/yalexs/activity.py\n@@ -40,6 +40,8 @@\n ACTION_HOMEKEY_UNLOCK = \"homekey_unlock\"\n ACTION_LINKED_LOCK = \"linked_lock\"\n ACTION_LINKED_UNLOCK = \"linked_unlock\"\n+ACTION_LOCK_FINGER_LOCK = \"finger_lock\"\n+ACTION_LOCK_FINGER_UNLOCK = \"finger_unlock\"\n \n ACTION_DOOR_OPEN = \"dooropen\"\n ACTION_DOOR_OPEN_2 = \"door_open\"\n@@ -100,6 +102,8 @@\n     ACTION_LOCK_MANUAL_LOCK,\n     ACTION_LOCK_MANUAL_UNLATCH,\n     ACTION_LOCK_MANUAL_UNLOCK,\n+    ACTION_LOCK_FINGER_LOCK,\n+    ACTION_LOCK_FINGER_UNLOCK,\n }\n \n \n@@ -122,6 +126,8 @@\n     ACTION_LOCK_MANUAL_LOCK: (\"Manual\", \"Lock\"),\n     ACTION_LOCK_MANUAL_UNLATCH: (\"Manual\", \"Unlatch\"),\n     ACTION_LOCK_MANUAL_UNLOCK: (\"Manual\", \"Unlock\"),\n+    ACTION_LOCK_FINGER_LOCK: (\"Fingerprint\", \"Lock\"),\n+    ACTION_LOCK_FINGER_UNLOCK: (\"Fingerprint\", \"Unlock\"),\n }\n \n \n@@ -137,6 +143,8 @@\n     ACTION_LOCK_ONETOUCHLOCK_2,\n     ACTION_LOCK_PIN_UNLATCH,\n     ACTION_LOCK_PIN_UNLOCK,\n+    ACTION_LOCK_FINGER_LOCK,\n+    ACTION_LOCK_FINGER_UNLOCK,\n }\n REMOTE_ACTIONS = {\n     ACTION_LOCK_REMOTE_LOCK,\n@@ -196,6 +204,8 @@\n     ACTION_LOCK_MANUAL_LOCK: LockStatus.LOCKED,\n     ACTION_LOCK_MANUAL_UNLATCH: LockStatus.UNLATCHED,\n     ACTION_LOCK_MANUAL_UNLOCK: LockStatus.UNLOCKED,\n+    ACTION_LOCK_FINGER_LOCK: LockStatus.LOCKED,\n+    ACTION_LOCK_FINGER_UNLOCK: LockStatus.UNLOCKED,\n }\n \n \n", "instance_id": "bdraco__yalexs-186", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to add support for the `finger_unlock` action in the `/houses/<house_id>/activities` API response within the `yalexs` codebase. It provides a useful sample event structure that includes the `finger_unlock` action, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly mention whether other fingerprint-related actions (like `finger_lock`) are also expected to be supported, though the code changes suggest this. Additionally, there are no explicit mentions of edge cases, constraints, or potential side effects of adding this action (e.g., compatibility with existing automations or API consumers). While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The code changes are straightforward and localized to a single file (`yalexs/activity.py`), involving the addition of two new action constants (`finger_lock` and `finger_unlock`) and updating related sets and dictionaries to recognize these actions as valid lock/unlock operations. The scope of the change is minimal, with no apparent impact on the broader system architecture or interactions with other modules. The technical concepts required are basic: understanding Python constants, sets, and dictionaries, as well as the domain-specific mapping of actions to lock statuses. No complex algorithms, design patterns, or advanced language features are involved. Edge cases and error handling are not explicitly mentioned in the problem statement or reflected in the code changes, suggesting they are either not relevant or overlooked. Overall, this task requires minimal effort and a basic understanding of the codebase, making it an easy modification to implement and test.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Bump sphinx-notes/pages from 2 to 3\nBumps [sphinx-notes/pages](https://github.com/sphinx-notes/pages) from 2 to 3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/sphinx-notes/pages/releases\">sphinx-notes/pages's releases</a>.</em></p>\n<blockquote>\n<h2>3.0</h2>\n<ul>\n<li>Support installing dependencies from <code>pyproject.toml</code> (<a href=\"https://redirect.github.com/sphinx-notes/pages/issues/17\">#17</a> <a href=\"https://redirect.github.com/sphinx-notes/pages/issues/21\">#21</a>)</li>\n<li>Support specifying extra Sphinx options (<a href=\"https://redirect.github.com/sphinx-notes/pages/issues/14\">#14</a> <a href=\"https://redirect.github.com/sphinx-notes/pages/issues/25\">#25</a>)</li>\n<li>Support customize checkout (<a href=\"https://redirect.github.com/sphinx-notes/pages/issues/26\">#26</a>)</li>\n<li>Support deploy pages from a branch (we deploy with GitHub Actions by default) (<a href=\"https://redirect.github.com/sphinx-notes/pages/issues/23\">#23</a> <a href=\"https://redirect.github.com/sphinx-notes/pages/issues/27\">#27</a>)</li>\n<li>And more in <a href=\"https://github.com/sphinx-notes/pages/releases/tag/3.0beta\">https://github.com/sphinx-notes/pages/releases/tag/3.0beta</a></li>\n</ul>\n<h2>3.0beta</h2>\n<ul>\n<li>Support incremental build (cache)</li>\n<li>All in one:  integrated <code>actions/setup-python</code>, <code>actinos/checkout</code>, ...</li>\n<li>Support <a href=\"https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow\">Publishing Pages with Github Actions</a></li>\n</ul>\n<h2>2.1</h2>\n<p>No release notes provided.</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/69fc0bda46fd0e65a70ff8262dbaae475be8747d\"><code>69fc0bd</code></a> Revert &quot;No longer need to fix file permissions&quot; (<a href=\"https://redirect.github.com/sphinx-notes/pages/issues/36\">#36</a>)</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/124a17183032c80adea7f687880b40be1db5d31e\"><code>124a171</code></a> fix: Use actions/deploy-pages@v4</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/27f1b67a4bca6286778b8e13afe5a2d49e83fd71\"><code>27f1b67</code></a> Merge pull request <a href=\"https://redirect.github.com/sphinx-notes/pages/issues/35\">#35</a> from sphinx-notes/feat/github-problem-matcher</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/8caa61b65d60a557bc1c51933d2d228c09ef8537\"><code>8caa61b</code></a> Use my fork of upload-pages-artifact</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/da89835536f82917ffa5d63482ac3002f48fe942\"><code>da89835</code></a> No longer need to fix file permissions</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/d38e8a56ad9d4cc8c9ffec6df60ac430d262f8bb\"><code>d38e8a5</code></a> Enable github problem matcher</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/1ef210dab7429dfcbdb06346d279b746573d147a\"><code>1ef210d</code></a> Correctly document Sphinx version</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/e208343effcd277d6f2fa2bcf6d3fab5e31152fa\"><code>e208343</code></a> Merge pull request <a href=\"https://redirect.github.com/sphinx-notes/pages/issues/32\">#32</a> from sphinx-notes/upgrade-actions-deploy-v2</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/00ee1f7cc3d61e9726f283048cc89ede75aad320\"><code>00ee1f7</code></a> Try fix file premissions</li>\n<li><a href=\"https://github.com/sphinx-notes/pages/commit/afe513b283f1ab94527d884edce176bf0a5f780e\"><code>afe513b</code></a> Upgrade actions/deploy@v2</li>\n<li>Additional commits viewable in <a href=\"https://github.com/sphinx-notes/pages/compare/v2...v3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n<details>\n<summary>Most Recent Ignore Conditions Applied to This Pull Request</summary>\n\n| Dependency Name | Ignore Conditions |\n| --- | --- |\n| sphinx-notes/pages | [> 3] |\n</details>\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx-notes/pages&package-manager=github_actions&previous-version=2&new-version=3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>\n", "patch": "diff --git a/.github/dependabot.yml b/.github/dependabot.yml\nindex 7f65ab85a..9e350e041 100644\n--- a/.github/dependabot.yml\n+++ b/.github/dependabot.yml\n@@ -20,4 +20,4 @@ updates:\n     schedule:\n       interval: \"daily\"\n     ignore:\n-      - dependency-name: \"spinx-notes/pages\"\n+      - dependency-name: \"sphinx-notes/pages\"\ndiff --git a/.github/workflows/python-publish.yml b/.github/workflows/python-publish.yml\nindex ba9130fdd..70fdd26c3 100644\n--- a/.github/workflows/python-publish.yml\n+++ b/.github/workflows/python-publish.yml\n@@ -93,7 +93,7 @@ jobs:\n         run: |\n           docker compose up -d\n       - name: Build the documentation\n-        uses: sphinx-notes/pages@v3\n+        uses: sphinx-notes/pages@v2\n         # Note: This action has a newer version (v3 atm), but it doesn't has the feature to specify the target path.\n         # We need that in order to be able to store (and deploy) multiple versions of the documentation.\n         with:\n", "instance_id": "bo4e__BO4E-python-782", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent to bump the version of `sphinx-notes/pages` from 2 to 3, as it provides detailed release notes and commit history for the dependency update. The goal of updating the dependency version is evident, and the context of the update (new features like support for `pyproject.toml`, extra Sphinx options, etc.) is well-documented in the release notes. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly discuss the potential impact of the version bump on the existing codebase or workflow, nor does it specify whether compatibility issues or additional configurations are expected with the new version. Additionally, the code changes show a discrepancy where the `python-publish.yml` file is set to `v2` instead of `v3`, which contradicts the stated intent of the PR and introduces confusion. Edge cases or specific testing requirements for the update are also not mentioned. Overall, while the problem is valid and mostly clear, these minor gaps in detail and consistency lower the clarity score from a 3 to a 2.", "difficulty_explanation": "The difficulty of this problem is very low, as it primarily involves a straightforward dependency version update, which is a routine task in software maintenance. Analyzing the code changes, the scope is minimal, affecting only two files: a typo correction in `dependabot.yml` (fixing the dependency name from \"spinx-notes/pages\" to \"sphinx-notes/pages\") and a commented intent in `python-publish.yml` to keep the version at `v2` despite the PR's goal of updating to `v3`. This discrepancy suggests a potential oversight or intentional rollback, but resolving it requires minimal effort\u2014either updating the version to `v3` or clarifying the intent in documentation. The technical concepts involved are basic, limited to understanding GitHub Actions workflows and dependency management, with no complex algorithms, design patterns, or domain-specific knowledge required. The changes are isolated, do not impact the broader system architecture, and involve no significant edge case handling or error conditions beyond standard dependency update checks (e.g., ensuring compatibility, which is not explicitly required in the problem statement). Given the simplicity of the task and the small scope of changes, I assign a difficulty score of 0.1, placing it in the \"very easy\" category, as it requires only basic modifications and minimal understanding of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Make repository name available in context\n### Is there an existing issue or pull request for this?\r\n\r\n- [X] I have searched the existing issues and pull requests\r\n\r\n### Feature description\r\n\r\nI am creating a consolidated changelog for multiple repositories building a monolith using the `--repository` flag with three repos. The changelog gets generated but theres no reference which changelog refers to which repo, just the repos most current tags.\r\nIn the current context there's just details about commits but not about the repo name or url.\r\n\r\n### Desired solution\r\n\r\nIt would be nice to have the repo name (and url?) available in the context to have something like this:\r\n\r\n`config:` [{{ version | trim_start_matches(pat=\"v\") }}] - {{ timestamp | date(format=\"%Y-%m-%d\") }} -- {{ repo.name }}\r\n\r\nwhich would look like this: \r\n\r\n## [1.2.3] - 2024-06-07 - orhun/git-cliff\r\n\r\nchanges ...\r\n\r\n## [4.5.6] - 2024-06-07 - another/repo\r\n\r\nchanges...\r\n\r\n### Alternatives considered\r\n\r\nThe closes thing I currently see to achieve this are commit parsers which allow me to run commands on the OS. But I am not sure how to use that in the body heading to call git and get the repo name. I tried putting a placeholder in the body\r\n`## [unreleased] - REPO_URL` and then tried a postprocessor:\r\n\r\npostprocessors = [\r\n  { pattern = 'REPO_URL', replace_command = \"basename -s .git `git config --get remote.origin.url`\" },\r\n]\r\n\r\nthis leads to the entire changelog dissappering and setting the reponame where the CI runs three times and not the individual ones :( \r\n\r\n### Additional context\r\n\r\n_No response_\n", "patch": "diff --git a/git-cliff-core/src/changelog.rs b/git-cliff-core/src/changelog.rs\nindex 5680d49f76..c6a84d9986 100644\n--- a/git-cliff-core/src/changelog.rs\n+++ b/git-cliff-core/src/changelog.rs\n@@ -613,7 +613,7 @@ mod test {\n \t\t\t\theader:         Some(String::from(\"# Changelog\")),\n \t\t\t\tbody:           Some(String::from(\n \t\t\t\t\tr#\"{% if version %}\n-\t\t\t\t## Release [{{ version }}] - {{ timestamp | date(format=\"%Y-%m-%d\") }}\n+\t\t\t\t## Release [{{ version }}] - {{ timestamp | date(format=\"%Y-%m-%d\") }} - ({{ repository }})\n \t\t\t\t{% if commit_id %}({{ commit_id }}){% endif %}{% else %}\n \t\t\t\t## Unreleased{% endif %}\n \t\t\t\t{% for group, commits in commits | group_by(attribute=\"group\") %}\n@@ -881,6 +881,7 @@ mod test {\n \t\t\tcommit_id: Some(String::from(\"0bc123\")),\n \t\t\ttimestamp: 50000000,\n \t\t\tprevious: None,\n+\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t#[cfg(feature = \"github\")]\n \t\t\tgithub: crate::remote::RemoteReleaseMetadata {\n \t\t\t\tcontributors: vec![],\n@@ -941,6 +942,7 @@ mod test {\n \t\t\t\tcommit_id: None,\n \t\t\t\ttimestamp: 1000,\n \t\t\t\tprevious: Some(Box::new(test_release)),\n+\t\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t\t#[cfg(feature = \"github\")]\n \t\t\t\tgithub: crate::remote::RemoteReleaseMetadata {\n \t\t\t\t\tcontributors: vec![],\n@@ -974,7 +976,7 @@ mod test {\n \t\t\tString::from(\n \t\t\t\tr#\"# Changelog\n \n-\t\t\t## Release [v1.1.0] - 1970-01-01\n+\t\t\t## Release [v1.1.0] - 1970-01-01 - (/root/repo)\n \n \n \t\t\t### Bug Fixes\n@@ -992,7 +994,7 @@ mod test {\n \t\t\t#### ui\n \t\t\t- do exciting stuff\n \n-\t\t\t## Release [v1.0.0] - 1971-08-02\n+\t\t\t## Release [v1.0.0] - 1971-08-02 - (/root/repo)\n \t\t\t(0bc123)\n \n \t\t\t### Bug Fixes\n@@ -1117,7 +1119,7 @@ chore(deps): fix broken deps\n \t\t\t#### app\n \t\t\t- merge #5\n \n-\t\t\t## Release [v1.0.0] - 1971-08-02\n+\t\t\t## Release [v1.0.0] - 1971-08-02 - (/root/repo)\n \t\t\t(0bc123)\n \n \t\t\t### Bug Fixes\ndiff --git a/git-cliff-core/src/release.rs b/git-cliff-core/src/release.rs\nindex 05720d2572..9ee23835fa 100644\n--- a/git-cliff-core/src/release.rs\n+++ b/git-cliff-core/src/release.rs\n@@ -20,30 +20,32 @@ use serde::{\n #[serde(rename_all = \"camelCase\")]\n pub struct Release<'a> {\n \t/// Release version, git tag.\n-\tpub version:   Option<String>,\n+\tpub version:    Option<String>,\n \t/// git tag's message.\n-\tpub message:   Option<String>,\n+\tpub message:    Option<String>,\n \t/// Commits made for the release.\n-\tpub commits:   Vec<Commit<'a>>,\n+\tpub commits:    Vec<Commit<'a>>,\n \t/// Commit ID of the tag.\n \t#[serde(rename = \"commit_id\")]\n-\tpub commit_id: Option<String>,\n+\tpub commit_id:  Option<String>,\n \t/// Timestamp of the release in seconds, from epoch.\n-\tpub timestamp: i64,\n+\tpub timestamp:  i64,\n \t/// Previous release.\n-\tpub previous:  Option<Box<Release<'a>>>,\n+\tpub previous:   Option<Box<Release<'a>>>,\n+\t/// Repository path.\n+\tpub repository: Option<String>,\n \t/// Contributors.\n \t#[cfg(feature = \"github\")]\n-\tpub github:    RemoteReleaseMetadata,\n+\tpub github:     RemoteReleaseMetadata,\n \t/// Contributors.\n \t#[cfg(feature = \"gitlab\")]\n-\tpub gitlab:    RemoteReleaseMetadata,\n+\tpub gitlab:     RemoteReleaseMetadata,\n \t/// Contributors.\n \t#[cfg(feature = \"gitea\")]\n-\tpub gitea:     RemoteReleaseMetadata,\n+\tpub gitea:      RemoteReleaseMetadata,\n \t/// Contributors.\n \t#[cfg(feature = \"bitbucket\")]\n-\tpub bitbucket: RemoteReleaseMetadata,\n+\tpub bitbucket:  RemoteReleaseMetadata,\n }\n \n #[cfg(feature = \"github\")]\n@@ -187,6 +189,7 @@ mod test {\n \t\t\t\t\tversion: Some(String::from(version)),\n \t\t\t\t\t..Default::default()\n \t\t\t\t})),\n+\t\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t\t#[cfg(feature = \"github\")]\n \t\t\t\tgithub: crate::remote::RemoteReleaseMetadata {\n \t\t\t\t\tcontributors: vec![],\n@@ -395,6 +398,7 @@ mod test {\n \t\t\t\tversion: Some(String::from(\"1.0.0\")),\n \t\t\t\t..Default::default()\n \t\t\t})),\n+\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\tgithub: RemoteReleaseMetadata {\n \t\t\t\tcontributors: vec![],\n \t\t\t},\n@@ -681,6 +685,7 @@ mod test {\n \t\t\t\tversion: Some(String::from(\"1.0.0\")),\n \t\t\t\t..Default::default()\n \t\t\t})),\n+\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t#[cfg(feature = \"github\")]\n \t\t\tgithub: RemoteReleaseMetadata {\n \t\t\t\tcontributors: vec![],\n@@ -1025,6 +1030,7 @@ mod test {\n \t\t\t\tversion: Some(String::from(\"1.0.0\")),\n \t\t\t\t..Default::default()\n \t\t\t})),\n+\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t#[cfg(feature = \"github\")]\n \t\t\tgithub: RemoteReleaseMetadata {\n \t\t\t\tcontributors: vec![],\ndiff --git a/git-cliff-core/src/repo.rs b/git-cliff-core/src/repo.rs\nindex 50645c883d..25d9a23baf 100644\n--- a/git-cliff-core/src/repo.rs\n+++ b/git-cliff-core/src/repo.rs\n@@ -32,7 +32,9 @@ static TAG_SIGNATURE_REGEX: Lazy<Regex> = lazy_regex!(\n ///\n /// [`Repository`]: GitRepository\n pub struct Repository {\n-\tinner: GitRepository,\n+\tinner:    GitRepository,\n+\t/// Repository path.\n+\tpub path: PathBuf,\n }\n \n impl Repository {\n@@ -40,7 +42,8 @@ impl Repository {\n \tpub fn init(path: PathBuf) -> Result<Self> {\n \t\tif path.exists() {\n \t\t\tOk(Self {\n-\t\t\t\tinner: GitRepository::open(path)?,\n+\t\t\t\tinner: GitRepository::open(&path)?,\n+\t\t\t\tpath,\n \t\t\t})\n \t\t} else {\n \t\t\tErr(Error::IoError(io::Error::new(\ndiff --git a/git-cliff-core/src/template.rs b/git-cliff-core/src/template.rs\nindex 3fe0e8a3a9..6d2e611805 100644\n--- a/git-cliff-core/src/template.rs\n+++ b/git-cliff-core/src/template.rs\n@@ -206,6 +206,7 @@ mod test {\n \t\t\tcommit_id: None,\n \t\t\ttimestamp: 0,\n \t\t\tprevious: None,\n+\t\t\trepository: Some(String::from(\"/root/repo\")),\n \t\t\t#[cfg(feature = \"github\")]\n \t\t\tgithub: crate::remote::RemoteReleaseMetadata {\n \t\t\t\tcontributors: vec![],\ndiff --git a/git-cliff/src/lib.rs b/git-cliff/src/lib.rs\nindex 6fc8e8c458..a42da54993 100644\n--- a/git-cliff/src/lib.rs\n+++ b/git-cliff/src/lib.rs\n@@ -245,6 +245,8 @@ fn process_repository<'a>(\n \tfor git_commit in commits.iter().rev() {\n \t\tlet commit = Commit::from(git_commit);\n \t\tlet commit_id = commit.id.to_string();\n+\t\treleases[release_index].repository =\n+\t\t\tSome(repository.path.to_string_lossy().to_string());\n \t\tif args.sort == Sort::Newest {\n \t\t\treleases[release_index].commits.insert(0, commit);\n \t\t} else {\ndiff --git a/website/docs/templating/context.md b/website/docs/templating/context.md\nindex 9d61db8015..c979659e95 100644\n--- a/website/docs/templating/context.md\n+++ b/website/docs/templating/context.md\n@@ -62,6 +62,7 @@ following context is generated to use for templating:\n   ],\n   \"commit_id\": \"a440c6eb26404be4877b7e3ad592bfaa5d4eb210 (release commit)\",\n   \"timestamp\": 1625169301,\n+  \"repository\": \"/path/to/repository\",\n   \"previous\": {\n     \"version\": \"previous release\"\n   }\n@@ -156,6 +157,7 @@ If [`conventional_commits`](/docs/configuration/git#conventional_commits) is set\n   ],\n   \"commit_id\": \"a440c6eb26404be4877b7e3ad592bfaa5d4eb210 (release commit)\",\n   \"timestamp\": 1625169301,\n+  \"repository\": \"/path/to/repository\",\n   \"previous\": {\n     \"version\": \"previous release\"\n   }\ndiff --git a/website/docs/usage/multiple-repos.md b/website/docs/usage/multiple-repos.md\nindex d6fe294fd8..0e4a6bdd92 100644\n--- a/website/docs/usage/multiple-repos.md\n+++ b/website/docs/usage/multiple-repos.md\n@@ -11,3 +11,11 @@ git cliff --repository path1 path2\n ```\n \n Note that the changelog will be generated using the merged history of the given repositories.\n+\n+:::tip\n+\n+You can use the `{{ repository }}` variable in the template to display which release belongs to which repository.\n+\n+See [context](/docs/templating/context) for more information.\n+\n+:::\n", "instance_id": "orhun__git-cliff-721", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal: to include the repository name (and potentially URL) in the changelog context for a tool that generates changelogs for multiple repositories. The desired output format is provided with an example of how the changelog should look, which helps in understanding the intent. The user also describes an alternative approach they attempted, which adds context to their struggle and the limitations of current solutions. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define whether the repository name should be the full path, a basename, or a user-configurable value. Additionally, while the URL is mentioned as a potential addition, there are no specifics on how it should be retrieved or formatted (e.g., from git remote configuration or elsewhere). Constraints or edge cases, such as handling repositories without a remote URL or invalid paths, are not mentioned. Overall, the statement is valid and mostly clear but lacks some precision in requirements and edge case specifications.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.35, placing it in the \"Easy\" category. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes provided involve modifications across multiple files (`changelog.rs`, `release.rs`, `repo.rs`, `template.rs`, `lib.rs`, and documentation files). However, the changes are relatively straightforward, primarily involving adding a new field (`repository`) to the `Release` struct and propagating it through the codebase to be used in templates. The modifications do not significantly impact the system's architecture; they are mostly additive and localized to specific structs and template rendering logic. The amount of code change is moderate but not extensive, focusing on data structure updates and template string adjustments.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding Rust's struct definitions, serde for serialization, and template rendering logic (likely using a library like Tera or Handlebars, as implied by the template syntax). Additionally, familiarity with Git repository handling (via the `git2` crate or similar) is needed to extract repository paths. These concepts are not overly complex for a developer familiar with Rust and Git tools, though they do require some knowledge of how data flows through the application (from repository initialization to changelog rendering). No advanced algorithms, design patterns, or domain-specific knowledge beyond Git and changelog generation are necessary.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest a basic implementation that assigns a repository path to each release. Potential edge cases include handling repositories with no valid path, non-existent directories, or special characters in paths that might break template rendering. The provided code does not address these, and error handling logic appears minimal. However, adding such handling would not significantly increase the complexity, as it would involve simple checks and optional values, which are idiomatic in Rust.\n\n4. **Overall Complexity**: The task requires understanding some code logic (how releases are tied to repositories and how templates are rendered) and making simple modifications to structs and template strings. It does not involve deep architectural changes or complex interactions between disparate parts of the codebase. The problem is more about plumbing data through existing structures than solving a conceptually difficult issue.\n\nGiven these factors, the problem is on the higher end of \"Easy\" due to the need to touch multiple files and understand the data flow, but it does not reach \"Medium\" difficulty as the changes are straightforward and do not involve complex logic, significant edge case handling, or advanced technical concepts.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Matching the nature of value_type of Kokkos::RandomAccessIterator to that of C++ standard library\nAcknowledging @masterleinad for figuring out the underlying issue and the fix.\r\n\r\nCurrently, the `value_type` of a `Kokkos::RandomAccessIterator` iterator is `const` if it is a const iterator like what is returned by `cbegin()`. However, the `value_type` of `std::vector.cbegin()` is non-`const`.\r\n\r\nFor example, on the develop branch with commit (81fc5622ad2d9b57427df91f2bee8b3a9e5c8ca9), the following `static_assert`s on the type of the `value_type`s will pass:\r\n\r\n```C++\r\nint main() {\r\n  // setup\r\n  const int N = 1e8;\r\n  std::vector<int> out(N);\r\n  const std::vector<int> in(N);\r\n  static_assert(std::is_same_v<decltype(in.begin())::value_type, int>);\r\n  static_assert(std::is_same_v<decltype(in.cbegin())::value_type, int>);\r\n  Kokkos::View<int*> bla(\"bla\", 10);\r\n  auto kokkos_it = Kokkos::Experimental::begin(bla);\r\n  auto kokkos_const_it = Kokkos::Experimental::cbegin(bla);\r\n\r\n  static_assert(std::is_same_v<decltype(kokkos_it)::value_type, int>);\r\n  static_assert(std::is_same_v<decltype(kokkos_const_it)::value_type, const int>);\r\n  return 0;\r\n}\r\n```\r\n\r\nRefer: https://godbolt.org/z/W64daPebd\r\n\r\nRefer [here](https://cplusplus.github.io/LWG/issue322) and [here](https://stackoverflow.com/questions/12819405/why-is-stditerator-traitsvalue-type-non-const-even-for-a-const-iterator/12821204#12821204) for some motivation about why value_type is non-const in C++. In short, \"constness doesn't matter for `value_type`, since a value implies a copy. The type of `std::iterator_traits<const T*>::reference` is `const T&` however\".\r\n\r\nIt is better to make the `value_type` behave in the same manner as it is in the C++ standard. It will help with calling third party libraries when they are available (e.g., Nvidia Thrust). See below about how an error is encountered when working with `thrust::inclusive_scan`. This change will also help with more streamlined development of the Kokkos library.\r\n\r\nIn this issue, the type alias value_type will be changed from `typename view_type::value_type` to `typename view_type::non_const_value_type` in `class RandomAccessIterator` in `algorithms/src/std_algorithms/impl/Kokkos_RandomAccessIterator.hpp`.\r\n\r\nAccording to the needs, existing tests will be modified and new tests will be added.\r\n\r\nHow to create error condition when working with current value_type of RandomAccessIterator?\r\nRefer to https://github.com/science-enthusiast/kokkos/tree/thrust_inclusive_scan\r\n\r\nIn this branch, `algorithms/unit_tests/TestStdAlgorithmsInclusiveScan.cpp` and `algorithms/src/std_algorithms/Kokkos_InclusiveScan.hpp` are modified by replacing calls to `Kokkos::Experimental::cbegin()` and `Kokkos::Experimental::cend()` with `Kokkos::Experimental::begin()` and `Kokkos::Experimental::end()`.\r\nThe error condition can be recreated by modifying `run_single_scenario()` in `algorithms/unit_tests/TestStdAlgorithmsInclusiveScan.cpp` to pass `KE::cbegin()` and `KE::cend()` (which is how they are passed in the develop branch) instead of `KE::begin()` and `KE::end()`. The error log is [build_error.txt](https://github.com/user-attachments/files/17491221/build_error.txt).\n", "patch": "diff --git a/algorithms/src/std_algorithms/impl/Kokkos_RandomAccessIterator.hpp b/algorithms/src/std_algorithms/impl/Kokkos_RandomAccessIterator.hpp\nindex 28d3133d516..e8c638c94c7 100644\n--- a/algorithms/src/std_algorithms/impl/Kokkos_RandomAccessIterator.hpp\n+++ b/algorithms/src/std_algorithms/impl/Kokkos_RandomAccessIterator.hpp\n@@ -36,7 +36,7 @@ class RandomAccessIterator< ::Kokkos::View<DataType, Args...> > {\n   using iterator_type = RandomAccessIterator<view_type>;\n \n   using iterator_category = std::random_access_iterator_tag;\n-  using value_type        = typename view_type::value_type;\n+  using value_type        = typename view_type::non_const_value_type;\n   using difference_type   = ptrdiff_t;\n   using pointer           = typename view_type::pointer_type;\n   using reference         = typename view_type::reference_type;\n", "instance_id": "kokkos__kokkos-7485", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `value_type` of `Kokkos::RandomAccessIterator` being `const` for const iterators, unlike the C++ standard library where `value_type` remains non-const. It provides context with code examples, references to relevant discussions, and links to demonstrate the issue (e.g., Godbolt, GitHub branches, and error logs). The goal of aligning `value_type` with the C++ standard for compatibility with third-party libraries like Nvidia Thrust is well-articulated. However, there are minor ambiguities: the problem statement does not explicitly detail the full scope of testing modifications or potential edge cases that might arise from this change. Additionally, while it mentions modifying tests and adding new ones, specifics about what these tests should cover are missing. Overall, the statement is valid and clear but lacks some finer details on edge cases and testing requirements.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code change itself is minimal, involving a single-line modification in `Kokkos_RandomAccessIterator.hpp` to update the `value_type` alias from `view_type::value_type` to `view_type::non_const_value_type`. However, the problem also mentions modifying existing tests and adding new ones, which implies additional work beyond this single file. Still, the impact appears localized to the iterator implementation and related test files, without evidence of broader architectural changes or complex inter-module dependencies.\n\n2. **Technical Concepts Involved:** Solving this requires a moderate understanding of C++ iterator traits, type aliases, and the behavior of `const` versus non-`const` types in the context of the C++ standard library. Familiarity with Kokkos (a parallel programming library) and its `View` types is necessary, but the concepts are not overly complex for someone with experience in C++ template programming. No advanced algorithms, design patterns, or domain-specific knowledge beyond C++ iterators and Kokkos basics are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases or error handling requirements related to this change. However, changing the `value_type` could potentially introduce subtle issues in downstream code or third-party library interactions (e.g., Nvidia Thrust compatibility as mentioned). The error condition described (with `thrust::inclusive_scan`) suggests some complexity in ensuring compatibility, but it is not extensively detailed. The lack of explicit edge case discussion reduces the perceived difficulty.\n\n4. **Overall Complexity:** The core fix is straightforward, and the primary challenge lies in understanding the implications of the type change and updating or adding tests to validate the behavior. This does not require deep architectural knowledge of the Kokkos library or extensive refactoring, making it a relatively easy task for a developer familiar with C++ and iterators.\n\nA score of 0.30 reflects the simplicity of the code change balanced against the moderate need to understand iterator semantics and ensure compatibility through testing. It is not a trivial typo fix (which would be closer to 0.0-0.2), but it also does not involve complex logic or broad system impact (which would push it toward 0.4 or higher).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[mbedtls] bump version to v3.6.0 (LTS)\nThis PR introduces native support for MbedTLS v3.6.0 (LTS). Backward compatibility is maintained, and a CI check for version 2.28.7 has been added. Note that this PR prepares for native [PSA API support](https://github.com/openthread/openthread/issues/10253), although it still utilizes the legacy `mbedtls_` crypto API.\r\n\r\nThe PR includes three independent commits:\r\n\r\n**[mbedtls] Migrate to MbedTLS v3.6.0**\r\n\r\nThis commit updates the in-tree version of MbedTLS to v3.6.0.\r\n\r\nAdditionally, the build configuration has been modified by adding compiler options and isystem includes to address issues listed here: https://github.com/Mbed-TLS/mbedtls/issues/9156, which should be resolved in future revisions.\r\n\r\n**[mbedtls] Modify GitHub workflows to test MbedTLS v2.28.8**\r\n\r\nThis commit adds a CI check for MbedTLS version 2.28.8, instead of testing version 3, which is now enabled by default.\r\n\r\n**[mbedtls] Optimize AES configuration**\r\n\r\nThis commit optimizes the configuration of AES encryption specifically for OpenThread.\r\n\r\nIt partially addresses https://github.com/openthread/openthread/issues/10252. Further enhancements will follow.\r\n\r\n\n", "patch": "diff --git a/src/cli/cli_tcp.cpp b/src/cli/cli_tcp.cpp\nindex 6d3158d1cf4..d243251a4d4 100644\n--- a/src/cli/cli_tcp.cpp\n+++ b/src/cli/cli_tcp.cpp\n@@ -143,16 +143,12 @@ template <> otError TcpExample::Process<Cmd(\"init\")>(Arg aArgs[])\n             mUseCircularSendBuffer = true;\n             mUseTls                = true;\n \n-            // mbedtls_debug_set_threshold(0);\n-\n-            otPlatCryptoRandomInit();\n             mbedtls_x509_crt_init(&mSrvCert);\n             mbedtls_pk_init(&mPKey);\n \n             mbedtls_ssl_init(&mSslContext);\n             mbedtls_ssl_config_init(&mSslConfig);\n             mbedtls_ssl_conf_rng(&mSslConfig, Crypto::MbedTls::CryptoSecurePrng, nullptr);\n-            // mbedtls_ssl_conf_dbg(&mSslConfig, MbedTlsDebugOutput, this);\n             mbedtls_ssl_conf_authmode(&mSslConfig, MBEDTLS_SSL_VERIFY_NONE);\n             mbedtls_ssl_conf_ciphersuites(&mSslConfig, sCipherSuites);\n \n@@ -261,6 +257,23 @@ template <> otError TcpExample::Process<Cmd(\"init\")>(Arg aArgs[])\n     mInitialized = true;\n \n exit:\n+    if (error != OT_ERROR_NONE)\n+    {\n+#if OPENTHREAD_CONFIG_TLS_ENABLE\n+        if (mUseTls)\n+        {\n+            mbedtls_ssl_config_free(&mSslConfig);\n+            mbedtls_ssl_free(&mSslContext);\n+\n+            mbedtls_pk_free(&mPKey);\n+            mbedtls_x509_crt_free(&mSrvCert);\n+        }\n+#endif // OPENTHREAD_CONFIG_TLS_ENABLE\n+\n+        otTcpCircularSendBufferForceDiscardAll(&mSendBuffer);\n+        OT_UNUSED_VARIABLE(otTcpCircularSendBufferDeinitialize(&mSendBuffer));\n+    }\n+\n     return error;\n }\n \n@@ -286,7 +299,6 @@ template <> otError TcpExample::Process<Cmd(\"deinit\")>(Arg aArgs[])\n #if OPENTHREAD_CONFIG_TLS_ENABLE\n     if (mUseTls)\n     {\n-        otPlatCryptoRandomDeinit();\n         mbedtls_ssl_config_free(&mSslConfig);\n         mbedtls_ssl_free(&mSslContext);\n \n", "instance_id": "openthread__openthread-10300", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of upgrading MbedTLS to version 3.6.0 (LTS) while maintaining backward compatibility and preparing for future PSA API support. It outlines the purpose of the three commits (version migration, CI workflow updates, and AES optimization) and provides context with links to related issues. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention specific challenges or constraints in migrating to the new version (beyond referencing an external issue), nor does it detail the expected behavior or potential risks of the migration. Additionally, while the commits are described, the exact impact on the codebase or specific areas of concern (e.g., compatibility issues or performance implications) are not fully elaborated. Edge cases or specific testing requirements beyond CI checks are also not mentioned. Overall, the statement is valid and clear but lacks some depth in critical details.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes involves multiple aspects of the codebase, including build configurations, CI workflows, and specific optimizations (e.g., AES configuration for OpenThread), as well as direct modifications to TLS-related code as seen in the diff. While the provided diff focuses on a single file (`cli_tcp.cpp`), the problem description implies broader changes across the repository (e.g., MbedTLS library update and CI scripts), suggesting a need to understand interactions between different components. Second, the technical concepts required include a deep understanding of MbedTLS (both legacy and newer APIs), TLS/SSL configurations, cryptographic operations, and build system modifications (e.g., compiler options and isystem includes). Additionally, maintaining backward compatibility with older versions (e.g., 2.28.8) adds complexity, as does preparing for future PSA API support. Third, the code changes in the diff show modifications to error handling and resource cleanup logic (e.g., freeing MbedTLS resources on error), indicating attention to edge cases, though the problem statement does not explicitly highlight specific edge cases or performance concerns. Finally, while the changes do not appear to fundamentally alter the system architecture, they impact a critical security component (MbedTLS), requiring careful validation and testing. A score of 0.65 reflects the need for a solid understanding of cryptographic libraries, version migration challenges, and cross-version compatibility, placing this task above medium difficulty but not at the extreme end of complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[regression] Mypy fails with shiny==1.0.0\nSeems like the same issue was fixed before but crept back in: https://github.com/posit-dev/py-shiny/issues/1332\r\n\r\nShiny is incompatible with `mypy`:\r\n\r\n```\r\n% mypy .\r\nvenv/lib/python3.11/site-packages/shiny/render/_data_frame_utils/_selection.py:154: error: invalid syntax  [syntax]\r\nFound 1 error in 1 file (errors prevented further checking)\r\n```\r\n\r\nUnfortunately seems like it's [not possible](https://stackoverflow.com/questions/78345753/mypy-checking-pyi-in-venv-despite-excluding-it) to edit one's `mypy` configuration to get past a syntax error in a dependency. \r\n\r\nI'm working around it by just deleting line 154 of `_selection.py` for now. \n[regression] Mypy fails with `shiny==0.9.0`\nHi, \r\nI've found the following regression.\r\n\r\n## Problem\r\n\r\nWith `mypy==1.9.0` and `shiny==0.9.0` and \r\n\r\n```py\r\nimport shiny\r\n```\r\n\r\nin `app.py`.\r\n\r\n`mypy app.py` raises\r\n\r\n```\r\n\u279c mypy app.py\r\n/Users/pasza/Library/Caches/pypoetry/virtualenvs/test-shiny09-qk-unJu3-py3.12/lib/python3.12/site-packages/shiny/render/_data_frame_utils/_selection.py:155: error: invalid syntax  [syntax]\r\nFound 1 error in 1 file (errors prevented further checking)\r\n```\r\n\r\nThis doesn't happen with `shiny==0.8.1`, I get the expected `Success: no issues found in 1 source file`.\r\n\r\n## Reason\r\n\r\nThe reason is the commented-out code in the source code \ud83d\ude48 \r\nIn `_selection.py`, lines 154-155 are:\r\n\r\n```py\r\n# class CellSelectionAll(TypedDict):\r\n#     type: Literal[\"all\"]\r\n```\r\n\r\n`mypy` (I guess) interprets `# type:` as some kind of type hint for the thing in front of the `#` sign. Or something like that, just like `# type: ignore`. Or maybe `Literal[\"all\"]` is just not proper syntax for `# type:...`\r\n\r\n## Potential fix\r\n\r\nRemove unused code, or mangle the commented code to not make it clash with `mypy`.\r\n\r\n## After thought\r\n\r\nMaybe this check could've/should've been added to CICD?\r\n\r\nBest, \r\nPasza\r\n\r\n<details>\r\n\r\n```\r\n[tool.poetry]\r\nname = \"test-shiny09\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Piotr Pasza Storozenko <piotr@appsilon.com>\"]\r\nreadme = \"README.md\"\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.12\"\r\nshiny = \"0.9.0\"\r\nmypy = \"^1.9.0\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n```\r\n\r\n</details>\n", "patch": "diff --git a/Makefile b/Makefile\nindex 22a67d7cd..cc6eb67f2 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -225,6 +225,14 @@ ci-install-rsconnect: FORCE\n \tuv pip install \"rsconnect-python @ git+https://github.com/rstudio/rsconnect-python.git\"\n \n \n+# This is just to check if mypy can run for other users.\n+# Not added to `make check` or `make check-fix` as all lint errors are supporessed (as we use pyright).\n+ci-check-mypy-can-run: FORCE\n+\t@echo \"-------- Checking types with mypy -----------\"\n+\tuv pip install mypy\n+\tmypy shiny\n+\n+\n # ## If caching is ever used, we could run:\n # install-deps: FORCE ## install latest dependencies\n # \tpip install --editable \".[dev,test]\" --upgrade --upgrade-strategy eager\ndiff --git a/pyproject.toml b/pyproject.toml\nindex ca94dbb50..079839741 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -144,3 +144,10 @@ ignore = [\"E302\", \"E501\", \"F403\", \"F405\", \"W503\", \"E203\", \"E701\", \"E704\"]\n [tool.isort]\n profile = \"black\"\n skip = [\"__init__.py\", \"typings/\", \"_dev/\", \".venv\", \"venv\", \".tox\", \"build\"]\n+\n+[tool.mypy]\n+# The goal of our usage of mypy is to make to sure mypy can run, not that it catches any errors (we use pyright to find our errors).\n+# Therefore, ignore_errors but do not ignore runtime errors while checking\n+# Note: This setting can not be done via CLI and must be set within a config\n+ignore_errors = true\n+exclude = [\"shiny/api-examples\", \"shiny/templates\"]\ndiff --git a/shiny/render/_data_frame_utils/_selection.py b/shiny/render/_data_frame_utils/_selection.py\nindex fb86a3175..52125f5b6 100644\n--- a/shiny/render/_data_frame_utils/_selection.py\n+++ b/shiny/render/_data_frame_utils/_selection.py\n@@ -150,8 +150,8 @@ def _has_rect(self) -> bool:\n # Should only contain a single selection area\n \n # Do not include `BrowserCellSelectionAll` as it should be represented by a row, column, or region with appropriate values.\n-# class BrowserCellSelectionAll(TypedDict):\n-#    type: Literal[\"all\"]\n+# # class BrowserCellSelectionAll(TypedDict):\n+# #    type: Literal[\"all\"]\n \n \n class BrowserCellSelectionNone(TypedDict):\n", "instance_id": "posit-dev__py-shiny-1650", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: a regression in the `shiny` library (versions 0.9.0 and 1.0.0) causes `mypy` to fail due to a syntax error in commented-out code within a specific file (`_selection.py`). The goal is evident\u2014fix the compatibility issue with `mypy`\u2014and the input (a Python project using `shiny`) and output (successful `mypy` run without syntax errors) are implied. The statement also provides context about the root cause (commented-out code being misinterpreted by `mypy`) and references a previous issue for additional background. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior beyond \"fixing the syntax error,\" nor does it mention specific constraints or edge cases (e.g., ensuring the commented code's purpose is preserved or potential impacts on other tools). Additionally, while a potential fix is suggested (removing or mangling the commented code), it is not detailed enough to guide implementation comprehensively. Thus, while the problem is valid and mostly clear, it lacks some minor details, warranting a score of 2 (Mostly Clear).\n", "difficulty_explanation": "\nThe difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The required modification is minimal and localized to a single file (`_selection.py`). The change involves simply commenting out or altering a few lines of already commented code to prevent `mypy` from misinterpreting it. The diff provided shows a straightforward update (adding an extra `#` to mangle the comment). Additionally, changes to `Makefile` and `pyproject.toml` are supplementary to enable `mypy` checks in CI and configure it to ignore errors, which are also minor and do not impact the core system architecture. The overall amount of code change is trivial, with no significant interaction with other parts of the codebase.\n\n2. **Number of Technical Concepts**: The problem requires basic knowledge of Python and familiarity with static type checking tools like `mypy`. The concept involved is simple\u2014understanding how `mypy` parses comments and type hints (e.g., `# type:`). No advanced language features, algorithms, design patterns, or domain-specific knowledge are needed. The CI configuration changes involve basic Makefile and `pyproject.toml` syntax, which are also straightforward for anyone with minimal experience.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not mention any specific edge cases, and the code change does not introduce new logic that would require error handling. The fix is purely syntactic and does not alter the runtime behavior of the application. The only potential concern is ensuring that mangling the comment does not obscure its original intent (if it is meant for documentation or future use), but this is not highlighted as a requirement and is negligible in complexity.\n\n4. **Overall Complexity**: This is essentially a bug fix that requires minimal effort\u2014akin to fixing a typo or adjusting a configuration. It does not demand deep understanding of the codebase, complex logic, or significant testing beyond verifying that `mypy` no longer reports the syntax error.\n\nGiven these factors, I assign a difficulty score of 0.15, reflecting a very easy task that requires only basic modifications and minimal technical depth. The primary challenge is identifying the problematic lines (already done in the problem statement) and applying a trivial fix.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Bump Python versions in workflow files + replace asyncore\n### AutoKey is a Xorg application and will not function in a Wayland session. Do you use Xorg (X11) or Wayland?\n\nXorg\n\n### Has this issue already been reported?\n\n- [X] I have searched through the existing issues.\n\n### Is this a question rather than an issue?\n\n- [X] This is not a question.\n\n### What type of issue is this?\n\nEnhancement\n\n### Choose one or more terms that describe this issue:\n\n- [ ] autokey triggers\n- [ ] autokey-gtk\n- [ ] autokey-qt\n- [ ] beta\n- [ ] bug\n- [ ] critical\n- [X] development\n- [ ] documentation\n- [ ] enhancement\n- [ ] installation/configuration\n- [ ] phrase expansion\n- [ ] scripting\n- [ ] technical debt\n- [ ] user interface\n\n### Other terms that describe this issue if not provided above:\n\nupdate\n\n### Which Linux distribution did you use?\n\n_No response_\n\n### Which AutoKey GUI did you use?\n\nNone\n\n### Which AutoKey version did you use?\n\n_No response_\n\n### How did you install AutoKey?\n\n_No response_\n\n### Can you briefly describe the issue?\n\nUpdate the Python versions in the [build.yml](https://github.com/autokey/autokey/blob/master/.github/workflows/build.yml) file, and the [python-test.yml](https://github.com/autokey/autokey/blob/master/.github/workflows/python-test.yml) file.\n\n### Can the issue be reproduced?\n\nNone\n\n### What are the steps to reproduce the issue?\n\n_No response_\n\n### What should have happened?\n\n_No response_\n\n### What actually happened?\n\n_No response_\n\n### Do you have screenshots?\n\n_No response_\n\n### Can you provide the output of the AutoKey command?\n\n_No response_\n\n### Anything else?\n\n_No response_\n<br/>\n<hr/>\n\n<details><summary>This repo is using Opire - what does it mean? \ud83d\udc47</summary><br/>\ud83d\udcb5 Everyone can add rewards for this issue commenting <code>/reward 100</code> (replace <code>100</code> with the amount).<br/>\ud83d\udd75\ufe0f\u200d\u2642\ufe0f If someone starts working on this issue to earn the rewards, they can comment <code>/try</code> to let everyone know!<br/>\ud83d\ude4c And when they open the PR, they can comment <code>/claim #964</code> either in the PR description or in a PR's comment.<br/><br/>\ud83e\ude99 Also, everyone can tip any user commenting <code>/tip 20 @Elliria</code> (replace <code>20</code> with the amount, and <code>@Elliria</code> with the user to tip).<br/><br/>\ud83d\udcd6 If you want to learn more, check out our <a href=\"https://docs.opire.dev\">documentation</a>.</details>\n", "patch": "diff --git a/.github/workflows/build.yml b/.github/workflows/build.yml\nindex 606188f7..2715973a 100644\n--- a/.github/workflows/build.yml\n+++ b/.github/workflows/build.yml\n@@ -16,7 +16,7 @@ jobs:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n-        python-version: [3.10.1]\n+        python-version: [3.11.10]\n     steps:\n     - uses: actions/checkout@v4\n       with:\n@@ -64,7 +64,7 @@ jobs:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n-        python-version: [3.10.1]\n+        python-version: [3.11.10]\n     steps:\n     - uses: actions/checkout@v4\n     - name: Set up Python ${{ matrix.python-version }}\n@@ -110,7 +110,7 @@ jobs:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n-        python-version: [3.10.1]\n+        python-version: [3.11.10]\n     needs: [build_pypi_wheel, build_deb]\n     steps:\n     - uses: actions/checkout@v4\ndiff --git a/CHANGELOG.rst b/CHANGELOG.rst\nindex 2a34f4c1..06aedd07 100644\n--- a/CHANGELOG.rst\n+++ b/CHANGELOG.rst\n@@ -10,6 +10,8 @@ Important misc changes\n - Bump action versions in pages.yml to satisfy part of issue #963.\n - Bump action versions in build.yml to satisfy part of issue #963.\n - Bump action versions in python-test.yml to satisfy part of issue #963.\n+- Bump Python version in build.yml to satisfy part of issue #964.\n+- Bump Python versions in python-test.yml to satisfy part of issue #964.\n \n Other\n +++++\n", "instance_id": "autokey__autokey-968", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update Python versions in specific workflow files (build.yml and python-test.yml) as part of an enhancement for the AutoKey project. The goal is explicitly mentioned, and the relevant files are identified. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not specify why the Python version needs to be updated (e.g., compatibility issues, deprecation of older versions, or new feature requirements). Additionally, there is no mention of potential constraints, such as ensuring compatibility with other parts of the codebase or testing requirements after the update. Edge cases or potential risks (e.g., breaking changes in the new Python version) are also not addressed. Despite these gaps, the intent and scope of the task are understandable, especially when combined with the provided code changes, which align with the stated goal.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves straightforward modifications to configuration files. The code changes are minimal and localized to updating version numbers in GitHub workflow files (build.yml) and adding corresponding entries in the CHANGELOG.rst file. The task requires only basic familiarity with GitHub Actions syntax and no deep understanding of the broader codebase or complex programming concepts. There are no significant technical challenges, as the changes do not impact the system's architecture, involve intricate logic, or require handling edge cases or error conditions. The scope is limited to a few lines of code in specific files, with no interaction between different modules or components. The primary effort lies in identifying the correct files and ensuring the version numbers are updated consistently, which is a trivial task for anyone with basic experience in software development or CI/CD configurations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "build(deps): bump actions/download-artifact from 3 to 4.1.7 in /.github/workflows\nBumps [actions/download-artifact](https://github.com/actions/download-artifact) from 3 to 4.1.7.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/download-artifact/releases\">actions/download-artifact's releases</a>.</em></p>\n<blockquote>\n<h2>v4.1.7</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update <code>@\u200bactions/artifact</code> dependency by <a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/325\">actions/download-artifact#325</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v4.1.6...v4.1.7\">https://github.com/actions/download-artifact/compare/v4.1.6...v4.1.7</a></p>\n<h2>v4.1.6</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>updating <code>@actions/artifact</code> dependency to v2.1.6 by <a href=\"https://github.com/eggyhead\"><code>@\u200beggyhead</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/324\">actions/download-artifact#324</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v4.1.5...v4.1.6\">https://github.com/actions/download-artifact/compare/v4.1.5...v4.1.6</a></p>\n<h2>v4.1.5</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update readme with v3/v2/v1 deprecation notice by <a href=\"https://github.com/robherley\"><code>@\u200brobherley</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/322\">actions/download-artifact#322</a></li>\n<li>Update dependencies <code>@actions/core</code> to v1.10.1 and <code>@actions/artifact</code> to v2.1.5</li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v4.1.4...v4.1.5\">https://github.com/actions/download-artifact/compare/v4.1.4...v4.1.5</a></p>\n<h2>v4.1.4</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update <code>@\u200bactions/artifact</code> by <a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/307\">actions/download-artifact#307</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v4...v4.1.4\">https://github.com/actions/download-artifact/compare/v4...v4.1.4</a></p>\n<h2>v4.1.3</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update release-new-action-version.yml by <a href=\"https://github.com/konradpabjan\"><code>@\u200bkonradpabjan</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/292\">actions/download-artifact#292</a></li>\n<li>Update toolkit dependency with updated unzip logic by <a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/299\">actions/download-artifact#299</a></li>\n<li>Update <code>@\u200bactions/artifact</code> by <a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/303\">actions/download-artifact#303</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/download-artifact/pull/299\">actions/download-artifact#299</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v4...v4.1.3\">https://github.com/actions/download-artifact/compare/v4...v4.1.3</a></p>\n<h2>v4.1.2</h2>\n<ul>\n<li>Bump <code>@\u200bactions/artifacts</code> to latest version to include <a href=\"https://redirect.github.com/actions/toolkit/pull/1648\">updated GHES host check</a></li>\n</ul>\n<h2>v4.1.1</h2>\n<ul>\n<li>Fix transient request timeouts <a href=\"https://redirect.github.com/actions/download-artifact/issues/249\">actions/download-artifact#249</a></li>\n<li>Bump <code>@actions/artifacts</code> to latest version</li>\n</ul>\n<h2>v4.1.0</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Some cleanup by <a href=\"https://github.com/robherley\"><code>@\u200brobherley</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/247\">actions/download-artifact#247</a></li>\n<li>Fix default for run-id by <a href=\"https://github.com/stchr\"><code>@\u200bstchr</code></a> in <a href=\"https://redirect.github.com/actions/download-artifact/pull/252\">actions/download-artifact#252</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/download-artifact/commit/65a9edc5881444af0b9093a5e628f2fe47ea3b2e\"><code>65a9edc</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/325\">#325</a> from bethanyj28/main</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/fdd1595981c1a29187d3de99c28c28a166bc38f7\"><code>fdd1595</code></a> licensed</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/c13dba102f4bb92b3f679fa086db9e2973960ca7\"><code>c13dba1</code></a> update <code>@\u200bactions/artifact</code> dependency</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/0daa75ebeac4617faeb127496dbd716b8bcce26e\"><code>0daa75e</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/324\">#324</a> from actions/eggyhead/use-artifact-v2.1.6</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/9c19ed7fe5d278cd354c7dfd5d3b88589c7e2395\"><code>9c19ed7</code></a> Merge branch 'main' into eggyhead/use-artifact-v2.1.6</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/3d3ea8741ef44e86f7392b41e391bde3c36219bd\"><code>3d3ea87</code></a> updating license</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/89af5db8211998d3ca691103a86b0b9362a94286\"><code>89af5db</code></a> updating artifact package v2.1.6</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/b4aefff88e83a2676a730654e1ce3dce61880379\"><code>b4aefff</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/323\">#323</a> from actions/eggyhead/update-artifact-v215</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/8caf195ad4b1dee92908e23f56eeb0696f1dd42d\"><code>8caf195</code></a> package lock update</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/d7a2ec411d177e8ca679ac5969b70be59c322700\"><code>d7a2ec4</code></a> updating package version</li>\n<li>Additional commits viewable in <a href=\"https://github.com/actions/download-artifact/compare/v3...v4.1.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/download-artifact&package-manager=github_actions&previous-version=3&new-version=4.1.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tarantool/tarantool-python/network/alerts).\n\n</details>\n", "patch": "diff --git a/.github/workflows/packing.yml b/.github/workflows/packing.yml\nindex c2456477..6613f218 100644\n--- a/.github/workflows/packing.yml\n+++ b/.github/workflows/packing.yml\n@@ -6,6 +6,10 @@ on:\n   pull_request_target:\n     types: [labeled]\n \n+concurrency:\n+  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n+  cancel-in-progress: true\n+\n jobs:\n   pack_pip:\n     # We want to run on external PRs, but not on our own internal\n@@ -43,7 +47,7 @@ jobs:\n         run: make pip-dist-check\n \n       - name: Archive pip artifacts\n-        uses: actions/upload-artifact@v3\n+        uses: actions/upload-artifact@v4.4.0\n         with:\n           name: pip_dist\n           path: pip_dist\n@@ -84,7 +88,7 @@ jobs:\n           tarantool-version: '2.11'\n \n       - name: Download pip package artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: pip_dist\n           path: pip_dist\n@@ -134,7 +138,7 @@ jobs:\n         run: python3 .github/scripts/remove_source_code.py\n \n       - name: Download pip package artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: pip_dist\n           path: pip_dist\n@@ -202,7 +206,7 @@ jobs:\n         run: pip3 install twine\n \n       - name: Download pip package artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: pip_dist\n           path: pip_dist\n@@ -271,7 +275,7 @@ jobs:\n         run: make rpm-dist-check\n \n       - name: Archive RPM artifacts\n-        uses: actions/upload-artifact@v3\n+        uses: actions/upload-artifact@v4.4.0\n         with:\n           name: rpm_dist_${{ matrix.target.os }}_${{ matrix.target.dist }}\n           path: rpm_dist\n@@ -320,11 +324,11 @@ jobs:\n \n       - name: Install tarantool\n         run: |\n-          curl -L https://tarantool.io/yeohchA/release/2/installer.sh | bash\n+          curl -L https://tarantool.io/release/2/installer.sh | bash\n           dnf install -y tarantool tarantool-devel\n \n       - name: Download RPM artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: rpm_dist_${{ matrix.target.os }}_${{ matrix.target.dist }}\n           path: rpm_dist\n@@ -372,7 +376,7 @@ jobs:\n         run: sudo apt install -y curl make\n \n       - name: Download RPM artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: rpm_dist_${{ matrix.target.os }}_${{ matrix.target.dist }}\n           path: rpm_dist\n@@ -433,7 +437,7 @@ jobs:\n         run: make deb-dist-check\n \n       - name: Archive deb artifacts\n-        uses: actions/upload-artifact@v3\n+        uses: actions/upload-artifact@v4.4.0\n         with:\n           name: deb_dist\n           path: deb_dist\n@@ -484,13 +488,13 @@ jobs:\n       - name: Install tarantool ${{ matrix.tarantool }}\n         run: |\n           apt install -y curl\n-          curl -L https://tarantool.io/yeohchA/release/2/installer.sh | bash\n+          curl -L https://tarantool.io/release/2/installer.sh | bash\n           apt install -y tarantool tarantool-dev\n         env:\n           DEBIAN_FRONTEND: noninteractive\n \n       - name: Download deb artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: deb_dist\n           path: deb_dist\n@@ -542,7 +546,7 @@ jobs:\n         run: sudo apt install -y curl make\n \n       - name: Download deb artifacts\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4.1.8\n         with:\n           name: deb_dist\n           path: deb_dist\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 28f93b19..8edd47e7 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -4,6 +4,11 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## Unreleased\n+\n+### Changed\n+- Drop Python 3.6 support (PR #327).\n+\n ## 1.2.0 - 2024-03-27\n \n ### Added\ndiff --git a/requirements.txt b/requirements.txt\nindex d88dbea3..afcf7b25 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,3 +1,2 @@\n msgpack\n pytz\n-dataclasses; python_version <= '3.6'\ndiff --git a/setup.py b/setup.py\nindex 4ee0797c..3fa65c9c 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -7,7 +7,7 @@\n import codecs\n import os\n \n-from setuptools import setup, find_packages\n+from setuptools import find_packages, setup\n from setuptools.command.build_py import build_py\n \n # Extra commands for documentation management\n@@ -112,7 +112,7 @@ def get_dependencies(filename):\n     command_options=command_options,\n     install_requires=get_dependencies('requirements.txt'),\n     setup_requires=[\n-        'setuptools_scm==6.4.2',\n+        'setuptools_scm==7.1.0',\n     ],\n-    python_requires='>=3.6',\n+    python_requires='>=3.7',\n )\n", "instance_id": "tarantool__tarantool-python-327", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update the version of the `actions/download-artifact` GitHub Action from version 3 to 4.1.7, along with related updates to other actions and dependencies in the workflow files. It provides detailed release notes and commit history for the updated dependency, which helps in understanding the changes introduced by the new version. However, there are minor ambiguities: the problem statement does not explicitly discuss potential compatibility issues or specific reasons for the update (e.g., security fixes, new features, or breaking changes that might affect the workflow). Additionally, while the code changes are visible, the problem statement does not specify if any manual intervention or testing is required post-update to ensure the CI/CD pipeline continues to function as expected. Overall, the goal is clear, but some minor contextual details are missing that could impact the implementation or validation of the change.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it primarily involves a straightforward dependency update in a GitHub Actions workflow configuration file. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are limited to updating version numbers in a single workflow file (`.github/workflows/packing.yml`) for `actions/download-artifact` and `actions/upload-artifact`, along with minor unrelated updates in other files (e.g., dropping Python 3.6 support, updating `setuptools_scm` version). The modifications are localized and do not impact the broader codebase architecture or logic. The amount of code change is minimal, involving only version string updates in the YAML file and small dependency adjustments in `requirements.txt` and `setup.py`.\n\n2. **Number of Technical Concepts:** The task requires basic familiarity with GitHub Actions and dependency management. No advanced programming language features, algorithms, or design patterns are involved. The concepts are limited to understanding how GitHub Actions versions work and the implications of dependency updates, which are relatively simple for anyone with basic CI/CD experience.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement and code changes do not explicitly mention edge cases or error handling requirements. However, there is a minor risk of compatibility issues with the updated action versions, which might require validation of the CI/CD pipeline after the update. This risk is minimal and does not significantly increase the difficulty, as GitHub Actions updates are generally backward-compatible or well-documented if breaking changes exist.\n\n4. **Overall Complexity:** This is a routine maintenance task that does not require deep understanding of the codebase or complex modifications. It can be completed quickly by someone with basic knowledge of GitHub workflows, and the impact is confined to the CI/CD configuration rather than the core application logic.\n\nGiven these factors, I assign a difficulty score of 0.15, reflecting a very easy task with minimal technical challenge. The only slight complexity arises from the need to verify that the updated actions do not break existing workflows, but this is a standard precaution for any dependency update and does not warrant a higher difficulty rating.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Libraries written by 4.5.0 are not backwards compatible for s3 https st\u043erages\n### Describe the bug\n\nClient using 4.5.0 are [no longer writing](https://github.com/man-group/ArcticDB/blob/5dd6a642c3ab70be0ead45665c8cfcfa8cffafa3/cpp/arcticdb/storage/storage_override.hpp#L118) the `https` flag in the library config, which is needed by the older clients to correctly for their http requests.\r\n\r\nWe need to change it, so:\r\n- the `https`' value is written when creating the library - so the libraries are backwards compatible and can be read by older clients\r\n- the `https`' flag is overridden when the config is read - so the newer clients can read libraries created by 4.5.0\n\n### Steps/Code to Reproduce\n\n```python\r\n\r\nac = Arctic(\"s3s://...\")\r\nlib = ac.create_library(\"test\")\r\nlib.write(\"sym\", pd.DataFrame())\r\n\r\nconfig = ac._library_manager.get_library_config(\"test\")\r\ns3_storage = get_s3_storage_config(config)\r\nassert s3_storage.https\r\n```\n\n### Expected Results\n\nThe assert check should succeed as the `https` flag's value should reflect that the library was created with `s3s/https` in mind.\n\n### OS, Python Version and ArcticDB Version\n\nPython: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\r\nOS: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 ArcticDB: 4.5.0\n\n### Backend storage used\n\nS3\n\n### Additional Context\n\n_No response_\n", "patch": "diff --git a/cpp/arcticdb/storage/library_manager.cpp b/cpp/arcticdb/storage/library_manager.cpp\nindex 034cffa0e6..fd0548caf4 100644\n--- a/cpp/arcticdb/storage/library_manager.cpp\n+++ b/cpp/arcticdb/storage/library_manager.cpp\n@@ -30,21 +30,23 @@ const std::string BAD_CONFIG_IN_ATTEMPTED_WRITE = \"Attempting to write forbidden\n template<typename T>\n struct StorageVisitor {\n     arcticdb::proto::storage::LibraryConfig& lib_cfg_proto;\n+    bool override_https;\n \n     void operator()(const T& storage_override) {\n         for(auto& storage: *lib_cfg_proto.mutable_storage_by_id()){\n-            storage_override.modify_storage_config(storage.second);\n+            storage_override.modify_storage_config(storage.second, override_https);\n         }\n     }\n };\n \n void apply_storage_override(const StorageOverride& storage_override,\n-                                   arcticdb::proto::storage::LibraryConfig& lib_cfg_proto) {\n+                                   arcticdb::proto::storage::LibraryConfig& lib_cfg_proto,\n+                                   bool override_https) {\n     util::variant_match(\n             storage_override.variant(),\n-            StorageVisitor<S3Override>{lib_cfg_proto},\n-            StorageVisitor<AzureOverride>{lib_cfg_proto},\n-            StorageVisitor<LmdbOverride>{lib_cfg_proto},\n+            StorageVisitor<S3Override>{lib_cfg_proto, override_https},\n+            StorageVisitor<AzureOverride>{lib_cfg_proto, override_https},\n+            StorageVisitor<LmdbOverride>{lib_cfg_proto, override_https},\n             [] (const std::monostate&) {});\n }\n \n@@ -91,7 +93,7 @@ void LibraryManager::write_library_config(const py::object& lib_cfg, const Libra\n     google::protobuf::Any output = {};\n     python_util::pb_from_python(lib_cfg, lib_cfg_proto);\n \n-    apply_storage_override(storage_override, lib_cfg_proto);\n+    apply_storage_override(storage_override, lib_cfg_proto, false);\n \n     output.PackFrom(lib_cfg_proto);\n \n@@ -191,7 +193,7 @@ arcticdb::proto::storage::LibraryConfig LibraryManager::get_config_internal(cons\n     auto any = segment_in_memory.metadata();\n     arcticdb::proto::storage::LibraryConfig lib_cfg_proto;\n     any->UnpackTo(&lib_cfg_proto);\n-    apply_storage_override(storage_override, lib_cfg_proto);\n+    apply_storage_override(storage_override, lib_cfg_proto, true);\n     return lib_cfg_proto;\n }\n \ndiff --git a/cpp/arcticdb/storage/storage_override.hpp b/cpp/arcticdb/storage/storage_override.hpp\nindex 40f42d2305..5fe0f7c643 100644\n--- a/cpp/arcticdb/storage/storage_override.hpp\n+++ b/cpp/arcticdb/storage/storage_override.hpp\n@@ -102,7 +102,8 @@ class S3Override {\n         ssl_ = ssl;\n     }\n \n-    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage) const {\n+    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage,\n+                                bool override_https) const {\n         if(storage.config().Is<arcticdb::proto::s3_storage::Config>()) {\n             arcticdb::proto::s3_storage::Config s3_storage;\n             storage.config().UnpackTo(&s3_storage);\n@@ -115,9 +116,12 @@ class S3Override {\n             s3_storage.set_use_virtual_addressing(use_virtual_addressing_);\n             s3_storage.set_ca_cert_path(ca_cert_path_);\n             s3_storage.set_ca_cert_dir(ca_cert_dir_);\n-            s3_storage.set_https(https_);\n             s3_storage.set_ssl(ssl_);\n \n+            if(override_https) {\n+                s3_storage.set_https(https_);\n+            }\n+\n             util::pack_to_any(s3_storage, *storage.mutable_config());\n         }\n     }\n@@ -162,7 +166,8 @@ class AzureOverride {\n         ca_cert_dir_ = ca_cert_dir;\n     }\n \n-    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage) const {\n+    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage,\n+                                bool override_https ARCTICDB_UNUSED) const {\n         if(storage.config().Is<arcticdb::proto::azure_storage::Config>()) {\n             arcticdb::proto::azure_storage::Config azure_storage;\n             storage.config().UnpackTo(&azure_storage);\n@@ -199,7 +204,8 @@ class LmdbOverride {\n         map_size_ = map_size;\n     }\n \n-    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage) const {\n+    void modify_storage_config(arcticdb::proto::storage::VariantStorage& storage,\n+                                bool override_https ARCTICDB_UNUSED) const {\n         if(storage.config().Is<arcticdb::proto::lmdb_storage::Config>()) {\n             arcticdb::proto::lmdb_storage::Config lmdb_storage;\n             storage.config().UnpackTo(&lmdb_storage);\n", "instance_id": "man-group__ArcticDB-1840", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: libraries created with version 4.5.0 of ArcticDB are not backwards compatible for S3 HTTPS storage due to the omission of the `https` flag in the library configuration. The goal is to ensure backwards compatibility by writing the `https` flag during library creation and overriding it during reading for newer clients. The steps to reproduce and expected results are provided with a clear code snippet, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly mention potential edge cases (e.g., what happens if the storage type is not S3, or if there are mixed configurations). Additionally, the constraints or side effects of overriding the `https` flag are not discussed, which could impact the solution's robustness. Overall, while the core issue and intent are clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting two files (`library_manager.cpp` and `storage_override.hpp`) and involving modifications to specific functions related to storage configuration handling. The changes introduce a new parameter (`override_https`) to control the behavior of setting the `https` flag, which requires understanding the interaction between library configuration reading/writing and storage overrides. Second, the technical concepts involved include C++ template programming, variant handling (using `util::variant_match`), and protocol buffers for configuration serialization, which are moderately complex but not overly advanced for someone familiar with the codebase. Third, the problem does not explicitly mention edge cases, but the code changes suggest a need to handle different storage types (S3, Azure, LMDB) correctly, which adds a layer of complexity in ensuring the override logic does not break other configurations. However, the overall impact on the system's architecture appears minimal, as the changes are localized and do not seem to require a deep refactoring of core components. The amount of code change is small, but the logic requires careful consideration to avoid introducing bugs in configuration handling. Therefore, I rate this as a medium difficulty task (0.45), as it involves understanding multiple concepts and making targeted but non-trivial modifications across a couple of files.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Display cell Outputs-per-Second metric\nEvery so often when working with a SQL Cell in Jupyterlab I wish to know the estimated QPS for that query formulation. It is difficult to do it using %%timeit and you would have to replicate this for every SQL cell. So, it would be a nice QoL improvement if this plugin had an EPS metric on top of the regular elapsed time display.\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9d23639..42d11b7 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,3 +1,9 @@\n+## [3.2.0](https://github.com/deshaw/jupyterlab-execute-time/compare/v3.1.2...v3.2.0) (unreleased)\n+\n+### Added\n+\n+- Add an option to show the cell outputs per second [#116](https://github.com/deshaw/jupyterlab-execute-time/pull/116)\n+\n ## [3.1.2](https://github.com/deshaw/jupyterlab-execute-time/compare/v3.1.0...v3.1.2) (2024-02-14)\n \n ### Fixed\ndiff --git a/schema/settings.json b/schema/settings.json\nindex 675150f..cf1a23b 100644\n--- a/schema/settings.json\n+++ b/schema/settings.json\n@@ -59,6 +59,12 @@\n       \"description\": \"Show the formatted date string in the given format. For example \\\"yyy-MM-dd HH:mm:ss\\\" results in \\\"2023-02-28 14:25:57 \\\", \\\"dd/MM/YYY HH:mm:ss (O)\\\" results in \\\"28/02/2023 14:25:57 (GMT +3)\\\". Find out more on unicode date field symbols on this link [https://www.unicode.org/reports/tr35/tr35-dates.html#Date_Field_Symbol_Table]\",\n       \"default\": \"yyy-MM-dd HH:mm:ss\",\n       \"pattern\": \"[yGuqQMLwdeEcabBhHkKmsSzOxX\\\\W].*\"\n+    },\n+    \"showOutputsPerSecond\": {\n+      \"type\": \"boolean\",\n+      \"title\": \"Show Outputs Per Second\",\n+      \"description\": \"After a cell has finished running, show the outputs per second calculated from the last cell execution time and number of outputs.\",\n+      \"default\": false\n     }\n   }\n }\ndiff --git a/src/ExecuteTimeWidget.ts b/src/ExecuteTimeWidget.ts\nindex 1d40d63..439bc28 100644\n--- a/src/ExecuteTimeWidget.ts\n+++ b/src/ExecuteTimeWidget.ts\n@@ -32,6 +32,7 @@ export interface IExecuteTimeSettings {\n   showDate: boolean;\n   historyCount: number;\n   dateFormat: string;\n+  showOutputsPerSecond: boolean;\n }\n \n export default class ExecuteTimeWidget extends Widget {\n@@ -290,11 +291,13 @@ export default class ExecuteTimeWidget extends Widget {\n       if (isLikelyAborted) {\n         msg = '';\n       } else if (endTime) {\n-        if (\n-          this._settings.minTime <=\n-          differenceInMilliseconds(endTime, startTime) / 1000.0\n-        ) {\n+        const executionTimeMillis = differenceInMilliseconds(\n+          endTime,\n+          startTime\n+        );\n+        if (this._settings.minTime <= executionTimeMillis / 1000.0) {\n           const executionTime = getTimeDiff(endTime, startTime);\n+          const executionsPerSecond = 1000.0 / executionTimeMillis;\n           const lastExecutionTime = executionTimeNode.getAttribute(\n             PREV_DATA_EXECUTION_TIME_ATTR\n           );\n@@ -324,6 +327,15 @@ export default class ExecuteTimeWidget extends Widget {\n             msg += ` at ${getTimeString(endTime, this._settings.dateFormat)}`;\n           }\n           msg += ` in ${executionTime}`;\n+\n+          const numberOfOutputs = cell.model.outputs.length;\n+          if (this._settings.showOutputsPerSecond && numberOfOutputs > 0) {\n+            const outputsPerSecond = executionsPerSecond / numberOfOutputs;\n+            msg += `, ${numberOfOutputs} output${\n+              numberOfOutputs === 1 ? '' : 's'\n+            }`;\n+            msg += ` at ${outputsPerSecond.toFixed(2)}/s`;\n+          }\n         }\n       } else if (startTime) {\n         if (this._settings.showLiveExecutionTime) {\n@@ -429,6 +441,9 @@ export default class ExecuteTimeWidget extends Widget {\n       );\n     }\n \n+    this._settings.showOutputsPerSecond = settings.get('showOutputsPerSecond')\n+      .composite as boolean;\n+\n     const cells = this._panel.context.model.cells;\n     if (this._settings.enabled) {\n       cells.changed.connect(this.updateConnectedCell);\n@@ -513,5 +528,6 @@ export default class ExecuteTimeWidget extends Widget {\n     showDate: true,\n     historyCount: 5,\n     dateFormat: 'yyy-MM-dd HH:mm:ss',\n+    showOutputsPerSecond: false,\n   };\n }\n", "instance_id": "deshaw__jupyterlab-execute-time-116", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in expressing the goal of adding an \"Outputs-per-Second\" (OPS) metric to a JupyterLab plugin for SQL cells, alongside the existing elapsed time display. It provides a basic context for why this feature is desired (Quality of Life improvement for query performance estimation). However, there are minor ambiguities and missing details. For instance, it does not explicitly define how the OPS metric should be calculated (though the code changes imply a specific approach), nor does it mention any specific constraints or edge cases (e.g., handling cells with no outputs or very short execution times). Additionally, there are no examples of expected input/output formats for the display. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting a single file (`ExecuteTimeWidget.ts`) for the core logic, with minor updates to settings (`settings.json`) and documentation (`CHANGELOG.md`). The modifications involve adding a new setting, updating the widget's configuration, and calculating/displaying the OPS metric. There is no significant impact on the broader system architecture or interactions with other modules, and the amount of code change is small (a few lines of logic and configuration).\n\n2. **Technical Concepts Required**: The solution requires basic understanding of TypeScript (given the file extension and context), familiarity with the JupyterLab plugin structure, and simple arithmetic for calculating OPS (outputs divided by execution time). The concepts involved\u2014such as updating a UI display, handling settings, and basic time calculations\u2014are straightforward for a developer with moderate experience. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes handle a basic one by checking if the number of outputs is greater than 0 before displaying the OPS metric. However, other potential edge cases (e.g., extremely short execution times leading to division issues or overflow in OPS calculation) are not addressed in the code or problem statement. The error handling required is minimal and not complex.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase and making simple modifications to add a new feature. It does not require deep knowledge of the entire system or complex refactoring. The primary challenge might be ensuring the calculation and display format are user-friendly, but this is a minor concern.\n\nGiven these points, a difficulty score of 0.30 reflects an easy task that requires some code logic understanding and minor feature addition, but remains straightforward for a developer with basic to intermediate skills in the relevant technologies.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "MVP: Wrapping read_parquet\n**Feature request**\r\nBlocks #9. Implement a wrapper for read_parquet that provides a more tailored interface for loading a \"base\" (main) parquet file and a set of additional parquet files that are automatically packed into nested columns in the base parquet file. E.g.\r\n```\r\nread_parquet(data=\"base.parquet\", to_pack={\"dia\": \"nested.parquet\"})\r\n```\r\n**Before submitting**\r\nPlease check the following:\r\n\r\n- [x] I have described the purpose of the suggested change, specifying what I need the enhancement to accomplish, i.e. what problem it solves.\r\n- [x] I have included any relevant links, screenshots, environment information, and data relevant to implementing the requested feature, as well as pseudocode for how I want to access the new functionality.\r\n- [x] If I have ideas for how the new feature could be implemented, I have provided explanations and/or pseudocode and/or task lists for the steps.\r\n\n", "patch": "diff --git a/src/nested_pandas/__init__.py b/src/nested_pandas/__init__.py\nindex 613f651..e63bd1c 100644\n--- a/src/nested_pandas/__init__.py\n+++ b/src/nested_pandas/__init__.py\n@@ -1,8 +1,9 @@\n from .example_module import greetings, meaning\n from .nestedframe import NestedFrame\n+from .nestedframe.io import read_parquet\n \n # Import for registering\n from .series.accessor import NestSeriesAccessor  # noqa: F401\n from .series.dtype import NestedDtype\n \n-__all__ = [\"greetings\", \"meaning\", \"NestedDtype\", \"NestedFrame\"]\n+__all__ = [\"greetings\", \"meaning\", \"NestedDtype\", \"NestedFrame\", \"read_parquet\"]\ndiff --git a/src/nested_pandas/nestedframe/__init__.py b/src/nested_pandas/nestedframe/__init__.py\nindex 54af689..a656cf3 100644\n--- a/src/nested_pandas/nestedframe/__init__.py\n+++ b/src/nested_pandas/nestedframe/__init__.py\n@@ -1,1 +1,2 @@\n from .core import NestedFrame  # noqa\n+from .io import read_parquet  # noqa\ndiff --git a/src/nested_pandas/nestedframe/io.py b/src/nested_pandas/nestedframe/io.py\nnew file mode 100644\nindex 0000000..e0f773f\n--- /dev/null\n+++ b/src/nested_pandas/nestedframe/io.py\n@@ -0,0 +1,75 @@\n+# typing.Self and \"|\" union syntax don't exist in Python 3.9\n+from __future__ import annotations\n+\n+import pandas as pd\n+from pandas._libs import lib\n+from pandas._typing import (\n+    DtypeBackend,\n+    FilePath,\n+    ReadBuffer,\n+)\n+\n+from .core import NestedFrame\n+\n+\n+def read_parquet(\n+    data: FilePath | ReadBuffer[bytes],\n+    to_pack: dict,\n+    columns: list[str] | None = None,\n+    pack_columns: dict | None = None,\n+    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,\n+) -> NestedFrame:\n+    \"\"\"\n+    Load a parquet object from a file path and load a set of other\n+    parquet objects to pack into the resulting NestedFrame.\n+\n+    Docstring based on the Pandas equivalent.\n+\n+    #TODO after MVP: Include full kwarg-set\n+    #TODO: Switch dtype backend default?\n+\n+    Parameters\n+    ----------\n+    data : str, path object or file-like object\n+        String, path object (implementing ``os.PathLike[str]``), or file-like\n+        object implementing a binary ``read()`` function.\n+        The string could be a URL. Valid URL schemes include http, ftp, s3,\n+        gs, and file. For file URLs, a host is expected. A local file could be:\n+        ``file://localhost/path/to/table.parquet``.\n+        A file URL can also be a path to a directory that contains multiple\n+        partitioned parquet files. Both pyarrow and fastparquet support\n+        paths to directories as well as file URLs. A directory path could be:\n+        ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``.\n+    to_pack: dict,\n+        A dictionary of parquet data paths (same criteria as `data`), where\n+        each key reflects the desired column name to pack the data into and\n+        each value reflects the parquet data to pack.\n+    columns : list, default=None\n+        If not None, only these columns will be read from the file.\n+    pack_columns: dict, default=None\n+        If not None, selects a set of columns from each keyed nested parquet\n+        object to read from the nested files.\n+    dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'\n+        Back-end data type applied to the resultant :class:`DataFrame`\n+        (still experimental). Behaviour is as follows:\n+\n+        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n+          (default).\n+        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n+          DataFrame.\n+\n+    Returns\n+    -------\n+    NestedFrame\n+    \"\"\"\n+\n+    df = NestedFrame(pd.read_parquet(data, engine=\"pyarrow\", columns=columns, dtype_backend=dtype_backend))\n+\n+    for pack_key in to_pack:\n+        col_subset = pack_columns.get(pack_key, None) if pack_columns is not None else None\n+        packed = pd.read_parquet(\n+            to_pack[pack_key], engine=\"pyarrow\", columns=col_subset, dtype_backend=dtype_backend\n+        )\n+        df = df.add_nested(packed, pack_key)\n+\n+    return df\n", "instance_id": "lincc-frameworks__nested-pandas-21", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to create a wrapper for `read_parquet` that supports loading a base parquet file and additional parquet files to be packed into nested columns. The example usage provided (`read_parquet(data=\"base.parquet\", to_pack={\"dia\": \"nested.parquet\"})`) helps illustrate the desired functionality. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the nested parquet files should be integrated into the base file (e.g., matching rows or specific column mappings). Additionally, edge cases such as mismatched schemas, file access errors, or performance considerations for large files are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving the creation of a new file (`io.py`) and updates to import statements in two other files. The changes are localized and do not appear to impact the broader system architecture significantly. Second, the technical concepts required include familiarity with the pandas library (specifically `read_parquet`), handling file I/O, and working with a custom `NestedFrame` class, which seems to be a central abstraction in this codebase. The logic for packing nested data into columns using `add_nested` is straightforward but requires understanding the `NestedFrame` implementation. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential issues like schema mismatches or file access errors, though the current implementation does not handle these explicitly. Overall, this task requires understanding multiple concepts and making targeted modifications, but it does not involve deep architectural changes or highly complex logic, placing it in the 0.4-0.6 range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Django hijack doesnt work with `<span>` in button (icon)\nStorry:\r\n\r\nFor our highly customized django admin we want to have a button in \"hijack\" button.\r\n\r\nSomething like this:\r\n\r\n```html\r\n{% load i18n l10n hijack %}\r\n{% if request.user|can_hijack:another_user %}\r\n\r\n  {% blocktranslate trimmed with username=another_user.get_full_name asvar button_title %}\r\n    impersonate {{ username }}\r\n  {% endblocktranslate %}\r\n\r\n  <button\r\n    type=\"button\"\r\n    data-hijack-user=\"{{ another_user.pk|unlocalize }}\"\r\n    data-hijack-next=\"{{ next }}\"\r\n    data-hijack-url=\"{% url 'hijack:acquire' %}\"\r\n    title=\"{{ button_title|capfirst }}\"\r\n    class=\"btn-outline\"\r\n    data-testid=\"hijack_user_button_{{ another_user.pk }}\"\r\n  >\r\n    <span\r\n      class=\"material-symbols-outlined\"\r\n    >\r\n      supervisor_account\r\n    </span>\r\n  </button>\r\n{% endif %}\r\n\r\n```\r\n\r\nThe problem is, if you are \"lucky\" enough, you can click not on the button, but on the icon.\r\n\r\nThen in script you will get an `undefined` user pk, as the `span` has not `user` assigned.\r\n\r\n### Temporary fix:\r\n```html\r\n    <span\r\n      class=\"material-symbols-outlined\"\r\n      {#  These are temporery fix, before this PR is merged and included in django hijack: #}\r\n      data-hijack-user=\"{{ another_user.pk|unlocalize }}\"\r\n      data-hijack-next=\"{{ next }}\"\r\n      data-hijack-url=\"{% url 'hijack:acquire' %}\"\r\n      {# Temp fix end #}\r\n    >\r\n      supervisor_account\r\n    </span>\r\n```\r\n\r\n### Final fix:\r\nUse \"currentTarget\"\n", "patch": "diff --git a/hijack/static/hijack/hijack.js b/hijack/static/hijack/hijack.js\nindex 8a4b43a..470d29c 100644\n--- a/hijack/static/hijack/hijack.js\n+++ b/hijack/static/hijack/hijack.js\n@@ -33,7 +33,7 @@ export function mount (fn, query) {\n  * @return {Promise<void>}\n  */\n export async function hijack (event) {\n-  const element = event.target\n+  const element = event.currentTarget\n   const form = new FormData()\n   form.append('csrfmiddlewaretoken', document.querySelector('input[name=csrfmiddlewaretoken]').value)\n   form.append('user_pk', element.dataset.hijackUser)\n", "instance_id": "django-hijack__django-hijack-745", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a bug in the Django Hijack library where clicking on a `<span>` element inside a button does not correctly retrieve the user PK due to event target mismatch. The goal (fixing the event target to use `currentTarget`) and the context (customized Django admin with a hijack button) are provided, along with a temporary workaround and the proposed final fix. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the expected behavior after the fix (though it can be inferred), nor does it discuss potential edge cases or constraints, such as browser compatibility issues or nested elements within the button. Additionally, the description of \"if you are lucky enough, you can click not on the button, but on the icon\" is somewhat vague and could be more precise about the conditions under which the issue occurs. Overall, the statement is valid and mostly clear but lacks some minor clarifying details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The issue is a straightforward bug fix in the JavaScript code of the Django Hijack library, requiring a single-line change from `event.target` to `event.currentTarget` in the `hijack.js` file. This modification addresses the event delegation issue where the target might be a child element (like a `<span>`) rather than the button itself. The scope of the change is minimal, confined to a single file and a single function, with no impact on the broader system architecture or other modules. The technical concepts involved are basic: understanding JavaScript event handling and the difference between `event.target` and `event.currentTarget`, which are fundamental DOM event concepts. No advanced algorithms, design patterns, or domain-specific knowledge are required. Additionally, the problem statement and code changes do not indicate complex edge cases or additional error handling beyond the core fix, though one might consider browser compatibility for `currentTarget` (which is widely supported and thus not a significant concern). Given the simplicity of the fix, the limited scope, and the basic technical knowledge required, a difficulty score of 0.25 is appropriate, reflecting an easy task that requires minimal effort and understanding of some code logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Feature parity between GoLang and C\nOur GoLang binding for USearch is missing several APIs that are available in the C version. Let's add these to make the GoLang version just as powerful!\r\n\r\nMissing APIs:\r\n\r\n1. `SerializedLength`: Get file size after serialization.\r\n2. `SaveBuffer`: Save index to an in-memory buffer.\r\n3. `LoadBuffer`: Load index from an in-memory buffer.\r\n4. `ViewBuffer`: View index from an in-memory buffer without copying.\r\n5. `Metadata`: Load index metadata from a file.\r\n6. `MetadataBuffer`: Load index metadata from a buffer.\r\n7. `ExpansionAdd`: Get expansion value for index creation.\r\n8. `ExpansionSearch`: Get expansion value for search.\r\n9. `ChangeExpansionAdd`: Set new expansion value for index creation.\r\n10. `ChangeExpansionSearch`: Set new expansion value for search.\r\n11. `HardwareAcceleration`: Get SIMD capabilities.\r\n12. `MemoryUsage`: Get memory usage of the index.\r\n13. `Distance`: Compute distance between two vectors.\r\n14. `ExactSearch`: Multi-threaded exact nearest neighbors search.\r\n15. `Rename`: Rename a vector to a different key.\r\n\r\n\r\n### Can you contribute to the implementation?\r\n\r\n- [X] I can contribute\r\n\r\n### Is your feature request specific to a certain interface?\r\n\r\nOther bindings\r\n\r\n### Contact Details\r\n\r\n_No response_\r\n\r\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's Code of Conduct\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex 5e7a197b..ea441b31 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -56,6 +56,9 @@ target/\n .build\n .swiftpm\n \n+# Golang builds\n+golang/usearch.h\n+\n # C# builds\n csharp/**/[Bb]in/\n csharp/**/[Oo]bj/\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex adbc0719..072f3340 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -335,10 +335,10 @@ So one should first compile the C library, link it with GoLang, and only then ru\n cmake -B build_release -DUSEARCH_BUILD_LIB_C=1 -DUSEARCH_BUILD_TEST_C=1 -DUSEARCH_USE_OPENMP=1 -DUSEARCH_USE_SIMSIMD=1 \n cmake --build build_release --config Release -j\n \n-mv c/libusearch_c.so golang/ # or .dylib to install the library on MacOS\n-cp c/usearch.h golang/ # to make the header available to GoLang\n+cp build_release/libusearch_c.so golang/ # or .dylib to install the library on MacOS\n+cp c/usearch.h golang/                   # to make the header available to GoLang\n \n-cd golang && go test -v ; cd ..\n+cd golang && LD_LIBRARY_PATH=. go test -v ; cd ..\n ```\n \n ## Java\ndiff --git a/golang/lib.go b/golang/lib.go\nindex 6b61b1f1..35099626 100644\n--- a/golang/lib.go\n+++ b/golang/lib.go\n@@ -56,6 +56,27 @@ func (m Metric) String() string {\n \t\tpanic(\"Unknown metric\")\n \t}\n }\n+func (m Metric) CValue() C.usearch_metric_kind_t {\n+\tswitch m {\n+\tcase L2sq:\n+\t\treturn C.usearch_metric_l2sq_k\n+\tcase InnerProduct:\n+\t\treturn C.usearch_metric_ip_k\n+\tcase Cosine:\n+\t\treturn C.usearch_metric_cos_k\n+\tcase Haversine:\n+\t\treturn C.usearch_metric_haversine_k\n+\tcase Pearson:\n+\t\treturn C.usearch_metric_pearson_k\n+\tcase Hamming:\n+\t\treturn C.usearch_metric_hamming_k\n+\tcase Tanimoto:\n+\t\treturn C.usearch_metric_tanimoto_k\n+\tcase Sorensen:\n+\t\treturn C.usearch_metric_sorensen_k\n+\t}\n+    return C.usearch_metric_l2sq_k\n+}\n \n // Quantization represents the type for different scalar kinds used in quantization.\n type Quantization uint8\n@@ -137,28 +158,7 @@ func NewIndex(conf IndexConfig) (index *Index, err error) {\n \toptions.expansion_add = expansion_add\n \toptions.expansion_search = expansion_search\n \toptions.multi = multi\n-\n-\t// Map the metric kind to a C enum\n-\tswitch conf.Metric {\n-\tcase L2sq:\n-\t\toptions.metric_kind = C.usearch_metric_l2sq_k\n-\tcase InnerProduct:\n-\t\toptions.metric_kind = C.usearch_metric_ip_k\n-\tcase Cosine:\n-\t\toptions.metric_kind = C.usearch_metric_cos_k\n-\tcase Haversine:\n-\t\toptions.metric_kind = C.usearch_metric_haversine_k\n-\tcase Pearson:\n-\t\toptions.metric_kind = C.usearch_metric_pearson_k\n-\tcase Hamming:\n-\t\toptions.metric_kind = C.usearch_metric_hamming_k\n-\tcase Tanimoto:\n-\t\toptions.metric_kind = C.usearch_metric_tanimoto_k\n-\tcase Sorensen:\n-\t\toptions.metric_kind = C.usearch_metric_sorensen_k\n-\tdefault:\n-\t\toptions.metric_kind = C.usearch_metric_unknown_k\n-\t}\n+    options.metric_kind = conf.Metric.CValue()\n \n \t// Map the quantization method\n \tswitch conf.Quantization {\n@@ -196,6 +196,66 @@ func (index *Index) Len() (len uint, err error) {\n \treturn len, err\n }\n \n+// SerializedLength reports the expected file size after serialization.\n+func (index *Index) SerializedLength() (len uint, err error) {\n+\tvar errorMessage *C.char\n+\tlen = uint(C.usearch_serialized_length((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), (*C.usearch_error_t)(&errorMessage)))\n+\tif errorMessage != nil {\n+\t\terr = errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn len, err\n+}\n+\n+// MemoryUsage reports the memory usage of the index\n+func (index *Index) MemoryUsage() (len uint, err error) {\n+\tvar errorMessage *C.char\n+\tlen = uint(C.usearch_memory_usage((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), (*C.usearch_error_t)(&errorMessage)))\n+\tif errorMessage != nil {\n+\t\terr = errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn len, err\n+}\n+\n+// ExpansionAdd returns the expansion value used during index creation\n+func (index *Index) ExpansionAdd() (val uint, err error) {\n+\tvar errorMessage *C.char\n+\tval = uint(C.usearch_expansion_add((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), (*C.usearch_error_t)(&errorMessage)))\n+\tif errorMessage != nil {\n+\t\terr = errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn val, err\n+}\n+\n+// ExpansionSearch returns the expansion value used during search\n+func (index *Index) ExpansionSearch() (val uint, err error) {\n+\tvar errorMessage *C.char\n+\tval = uint(C.usearch_expansion_search((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), (*C.usearch_error_t)(&errorMessage)))\n+\tif errorMessage != nil {\n+\t\terr = errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn val, err\n+}\n+\n+// ChangeExpansionAdd sets the expansion value used during index creation\n+func (index *Index) ChangeExpansionAdd(val uint) error {\n+\tvar errorMessage *C.char\n+\tC.usearch_change_expansion_add((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), C.size_t(val), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n+// ChangeExpansionSearch sets the expansion value used during search\n+func (index *Index) ChangeExpansionSearch(val uint) error {\n+\tvar errorMessage *C.char\n+\tC.usearch_change_expansion_search((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), C.size_t(val), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n // Connectivity returns the connectivity parameter of the index.\n func (index *Index) Connectivity() (con uint, err error) {\n \tvar errorMessage *C.char\n@@ -226,6 +286,17 @@ func (index *Index) Capacity() (cap uint, err error) {\n \treturn cap, err\n }\n \n+// HardwareAcceleration returns a string showing the SIMD capability for the index\n+func (index *Index) HardwareAcceleration() (string, error) {\n+\tvar str *C.char\n+\tvar errorMessage *C.char\n+\tstr = C.usearch_hardware_acceleration((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn C.GoString(nil), errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn C.GoString(str), nil\n+}\n+\n // Destroy frees the resources associated with the index.\n func (index *Index) Destroy() error {\n \tif index.opaque_handle == nil {\n@@ -316,6 +387,27 @@ func (index *Index) Get(key Key, count uint) (vectors []float32, err error) {\n \treturn vectors, nil\n }\n \n+// Rename the vector at key from to key to\n+func (index *Index) Rename(from Key, to Key) error {\n+\tvar errorMessage *C.char\n+\tC.usearch_rename((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), C.usearch_key_t(from), C.usearch_key_t(to), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n+// Distance computes the distance between two vectors\n+func Distance(vec1 []float32, vec2 []float32, dims uint, metric Metric) (float32, error) {\n+\n+\tvar errorMessage *C.char\n+\tdist := C.usearch_distance(unsafe.Pointer(&vec1[0]), unsafe.Pointer(&vec2[0]), C.usearch_scalar_f32_k, C.size_t(dims), metric.CValue(), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn 0, errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn float32(dist), nil\n+}\n+\n // Search performs k-Approximate Nearest Neighbors Search for the closest vectors to the query vector.\n func (index *Index) Search(query []float32, limit uint) (keys []Key, distances []float32, err error) {\n \tif index.opaque_handle == nil {\n@@ -338,6 +430,189 @@ func (index *Index) Search(query []float32, limit uint) (keys []Key, distances [\n \treturn keys, distances, nil\n }\n \n+// ExactSearch is a multithreaded exact nearest neighbors search\n+func ExactSearch(dataset []float32, queries []float32, dataset_size uint, queries_size uint,\n+                dataset_stride uint, queries_stride uint, dims uint, metric Metric, \n+                count uint, threads uint, keys_stride uint, distances_stride uint) (keys []Key, distances []float32, err error) {\n+\tif (len(dataset) % int(dims)) != 0 {\n+\t\treturn nil, nil, errors.New(\"Dataset length must be a multiple of the dimensions\")\n+\t}\n+\tif (len(queries) % int(dims)) != 0 {\n+\t\treturn nil, nil, errors.New(\"Queries length must be a multiple of the dimensions\")\n+\t}\n+\n+\tkeys = make([]Key, count)\n+\tdistances = make([]float32, count)\n+\tvar errorMessage *C.char\n+\tC.usearch_exact_search(unsafe.Pointer(&dataset[0]), C.size_t(dataset_size), C.size_t(dataset_stride), unsafe.Pointer(&queries[0]), C.size_t(queries_size), C.size_t(queries_stride), \n+            C.usearch_scalar_f32_k, C.size_t(dims), metric.CValue(), C.size_t(count), C.size_t(threads),\n+            (*C.usearch_key_t)(&keys[0]), C.size_t(keys_stride), (*C.usearch_distance_t)(&distances[0]), C.size_t(distances_stride), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn nil, nil, errors.New(C.GoString(errorMessage))\n+\t}\n+\n+\tkeys = keys[:count]\n+\tdistances = distances[:count]\n+\treturn keys, distances, nil\n+}\n+\n+// Save saves the index to a specified buffer.\n+func (index *Index) SaveBuffer(buf []byte, buffer_size uint) error {\n+\tif index.opaque_handle == nil {\n+\t\tpanic(\"Index is uninitialized\")\n+\t}\n+\n+\tvar errorMessage *C.char\n+\tC.usearch_save_buffer((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), unsafe.Pointer(&buf[0]), C.size_t(buffer_size), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n+// Loads the index from a specified buffer.\n+func (index *Index) LoadBuffer(buf []byte, buffer_size uint) error {\n+\tif index.opaque_handle == nil {\n+\t\tpanic(\"Index is uninitialized\")\n+\t}\n+\n+\tvar errorMessage *C.char\n+\tC.usearch_load_buffer((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), unsafe.Pointer(&buf[0]), C.size_t(buffer_size), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n+// Loads the index from a specified buffer without copying the data.\n+func (index *Index) ViewBuffer(buf []byte, buffer_size uint) error {\n+\tif index.opaque_handle == nil {\n+\t\tpanic(\"Index is uninitialized\")\n+\t}\n+\n+\tvar errorMessage *C.char\n+\tC.usearch_view_buffer((C.usearch_index_t)(unsafe.Pointer(index.opaque_handle)), unsafe.Pointer(&buf[0]), C.size_t(buffer_size), (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn errors.New(C.GoString(errorMessage))\n+\t}\n+\treturn nil\n+}\n+\n+// Loads the metadata from a specified buffer.\n+func MetadataBuffer(buf []byte, buffer_size uint) (c IndexConfig, err error) {\n+\tif buf == nil {\n+\t\tpanic(\"Buffer is uninitialized\")\n+\t}\n+\tc = IndexConfig{}\n+\n+\toptions := C.struct_usearch_init_options_t{}\n+\n+\tvar errorMessage *C.char\n+\tC.usearch_metadata_buffer(unsafe.Pointer(&buf[0]), C.size_t(buffer_size), &options, (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn c, errors.New(C.GoString(errorMessage))\n+\t}\n+\n+\tc.Dimensions = uint(options.dimensions)\n+\tc.Connectivity = uint(options.connectivity)\n+\tc.ExpansionAdd = uint(options.expansion_add)\n+\tc.ExpansionSearch = uint(options.expansion_search)\n+\tc.Multi = bool(options.multi)\n+\n+\t// Map the metric kind\n+\tswitch options.metric_kind {\n+    case C.usearch_metric_l2sq_k:\n+\t    c.Metric = L2sq\n+\tcase C.usearch_metric_ip_k:\n+\t    c.Metric = InnerProduct\n+    case C.usearch_metric_cos_k:\n+\t    c.Metric = Cosine\n+    case C.usearch_metric_haversine_k:\n+\t    c.Metric = Haversine\n+    case C.usearch_metric_pearson_k:\n+\t    c.Metric = Pearson\n+    case C.usearch_metric_hamming_k:\n+\t    c.Metric = Hamming\n+    case C.usearch_metric_tanimoto_k:\n+\t    c.Metric = Tanimoto\n+    case C.usearch_metric_sorensen_k:\n+\t    c.Metric = Sorensen\n+    }\n+\n+\t// Map the quantization method\n+\tswitch options.quantization {\n+    case C.usearch_scalar_f16_k:\n+\t    c.Quantization = F16\n+    case C.usearch_scalar_f32_k:\n+\t    c.Quantization = F32\n+\tcase C.usearch_scalar_f64_k:\n+\t    c.Quantization = F64\n+\tcase C.usearch_scalar_i8_k:\n+\t    c.Quantization = I8\n+\tcase C.usearch_scalar_b1_k:\n+\t    c.Quantization = B1\n+\t}\n+\n+\treturn c, nil\n+}\n+\n+// Metadata loads the metadata from a specified file.\n+func Metadata(path string) (c IndexConfig, err error)  {\n+\n+\tc_path := C.CString(path)\n+\tdefer C.free(unsafe.Pointer(c_path))\n+\n+\toptions := C.struct_usearch_init_options_t{}\n+\n+\tvar errorMessage *C.char\n+\tC.usearch_metadata(c_path, &options, (*C.usearch_error_t)(&errorMessage))\n+\tif errorMessage != nil {\n+\t\treturn c, errors.New(C.GoString(errorMessage))\n+\t}\n+\n+\tc.Dimensions = uint(options.dimensions)\n+\tc.Connectivity = uint(options.connectivity)\n+\tc.ExpansionAdd = uint(options.expansion_add)\n+\tc.ExpansionSearch = uint(options.expansion_search)\n+\tc.Multi = bool(options.multi)\n+\n+\t// Map the metric kind\n+\tswitch options.metric_kind {\n+    case C.usearch_metric_l2sq_k:\n+\t    c.Metric = L2sq\n+\tcase C.usearch_metric_ip_k:\n+\t    c.Metric = InnerProduct\n+    case C.usearch_metric_cos_k:\n+\t    c.Metric = Cosine\n+    case C.usearch_metric_haversine_k:\n+\t    c.Metric = Haversine\n+    case C.usearch_metric_pearson_k:\n+\t    c.Metric = Pearson\n+    case C.usearch_metric_hamming_k:\n+\t    c.Metric = Hamming\n+    case C.usearch_metric_tanimoto_k:\n+\t    c.Metric = Tanimoto\n+    case C.usearch_metric_sorensen_k:\n+\t    c.Metric = Sorensen\n+    }\n+\n+\t// Map the quantization method\n+\tswitch options.quantization {\n+    case C.usearch_scalar_f16_k:\n+\t    c.Quantization = F16\n+    case C.usearch_scalar_f32_k:\n+\t    c.Quantization = F32\n+\tcase C.usearch_scalar_f64_k:\n+\t    c.Quantization = F64\n+\tcase C.usearch_scalar_i8_k:\n+\t    c.Quantization = I8\n+\tcase C.usearch_scalar_b1_k:\n+\t    c.Quantization = B1\n+\t}\n+\n+\treturn c, nil\n+}\n+\n // Save saves the index to a specified file.\n func (index *Index) Save(path string) error {\n \tif index.opaque_handle == nil {\n", "instance_id": "unum-cloud__usearch-435", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to achieve feature parity between the GoLang and C bindings of the USearch library by adding several missing APIs. It lists 15 specific APIs that need to be implemented, which provides a clear scope of work. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not specify the expected input/output formats for each API, nor does it mention any constraints or edge cases that might need to be handled. Additionally, there are no examples or references to how these APIs are implemented in the C version, which could help clarify the expected behavior. While the intent and high-level goals are clear, these missing details could lead to potential misunderstandings during implementation.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, as it involves adding multiple new functions (15 APIs) to the GoLang binding, primarily in the `lib.go` file. While the changes are mostly localized to a single file, they require a good understanding of the interaction between Go and C through cgo, as well as familiarity with the USearch library's C API. The technical concepts involved include cgo for interfacing with C code, handling pointers and unsafe operations in Go, memory management for buffers, and understanding vector search algorithms (e.g., exact nearest neighbors search). The amount of code change is substantial, with many new functions added, though most follow a similar pattern of calling C functions and handling errors. Edge cases and error handling are present, as seen in the code changes (e.g., checking for uninitialized indices, validating input lengths), but they are not overly complex. There is no significant impact on the system's architecture, as this is primarily an extension of existing functionality. Overall, this task requires a moderate level of expertise in Go and cgo, along with careful attention to detail for error handling and memory safety, placing it in the medium difficulty range of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Unexpected result when using `INTERVAL` and `INNER JOIN` subquery\n### What happens?\n\nConsider the following test case. The second query with a `WHERE` filter returns 36 rows, and it's more than the first query without a `WHERE` filter, which is unexpected. \r\n\r\nConsidering PostgreSQL returns 36 rows for both queries, I suppose the first query is unexpected.\n\n### To Reproduce\n\n```sql\r\nCREATE  TABLE  t0(c1 INTERVAL);\r\nINSERT INTO t0(c1) VALUES ('2 years 3 months');\r\nINSERT INTO t0(c1) VALUES ('-1734799452 DAYS'), ('2 DAYS');\r\nINSERT INTO t0(c1) VALUES ('13 days');\r\nINSERT INTO t0(c1) VALUES ('1 month');\r\nINSERT INTO t0(c1) VALUES ('3 days');\r\n\r\nSELECT * FROM t0 INNER  JOIN  (SELECT  INTERVAL 1000 DAY AS col0 FROM t0) AS sub0  ON (t0.c1 < sub0.col0); -- 24 rows (unexpected)\r\nSELECT * FROM t0 INNER  JOIN  (SELECT  INTERVAL 1000 DAY AS col0 FROM t0) AS sub0  ON (t0.c1 < sub0.col0) WHERE (NOT (t0.c1 != t0.c1)); -- 36 rows\r\n\r\n\r\n```\n\n### OS:\n\nUbuntu 22.04\n\n### DuckDB Version:\n\nv1.1.4-dev1859 b484c2d96f\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nSuyang Zhong\n\n### Affiliation:\n\nNUS\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/extension/core_functions/function_list.cpp b/extension/core_functions/function_list.cpp\nindex a534c43400af..c6b773ae3bc2 100644\n--- a/extension/core_functions/function_list.cpp\n+++ b/extension/core_functions/function_list.cpp\n@@ -282,6 +282,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tDUCKDB_SCALAR_FUNCTION_SET(MonthNameFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(NanosecondsFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(NextAfterFun),\n+\tDUCKDB_SCALAR_FUNCTION(NormalizedIntervalFun),\n \tDUCKDB_SCALAR_FUNCTION_ALIAS(NowFun),\n \tDUCKDB_SCALAR_FUNCTION_ALIAS(OrdFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(ParseDirnameFun),\ndiff --git a/extension/core_functions/include/core_functions/scalar/date_functions.hpp b/extension/core_functions/include/core_functions/scalar/date_functions.hpp\nindex 72057aceb118..c7192d89f29d 100644\n--- a/extension/core_functions/include/core_functions/scalar/date_functions.hpp\n+++ b/extension/core_functions/include/core_functions/scalar/date_functions.hpp\n@@ -372,6 +372,15 @@ struct NanosecondsFun {\n \tstatic ScalarFunctionSet GetFunctions();\n };\n \n+struct NormalizedIntervalFun {\n+\tstatic constexpr const char *Name = \"normalized_interval\";\n+\tstatic constexpr const char *Parameters = \"interval\";\n+\tstatic constexpr const char *Description = \"Normalizes an INTERVAL to an equivalent interval\";\n+\tstatic constexpr const char *Example = \"normalized_interval(INTERVAL '30 days')\";\n+\n+\tstatic ScalarFunction GetFunction();\n+};\n+\n struct QuarterFun {\n \tstatic constexpr const char *Name = \"quarter\";\n \tstatic constexpr const char *Parameters = \"ts\";\ndiff --git a/extension/core_functions/scalar/date/epoch.cpp b/extension/core_functions/scalar/date/epoch.cpp\nindex 01481d721381..c30530341afc 100644\n--- a/extension/core_functions/scalar/date/epoch.cpp\n+++ b/extension/core_functions/scalar/date/epoch.cpp\n@@ -28,6 +28,23 @@ ScalarFunction ToTimestampFun::GetFunction() {\n \treturn ScalarFunction({LogicalType::DOUBLE}, LogicalType::TIMESTAMP_TZ, EpochSecFunction);\n }\n \n+struct NormalizedIntervalOperator {\n+\ttemplate <typename INPUT_TYPE, typename RESULT_TYPE>\n+\tstatic RESULT_TYPE Operation(INPUT_TYPE input) {\n+\t\treturn input.Normalize();\n+\t}\n+};\n+\n+static void NormalizedIntervalFunction(DataChunk &input, ExpressionState &state, Vector &result) {\n+\tD_ASSERT(input.ColumnCount() == 1);\n+\n+\tUnaryExecutor::Execute<interval_t, interval_t, NormalizedIntervalOperator>(input.data[0], result, input.size());\n+}\n+\n+ScalarFunction NormalizedIntervalFun::GetFunction() {\n+\treturn ScalarFunction({LogicalType::INTERVAL}, LogicalType::INTERVAL, NormalizedIntervalFunction);\n+}\n+\n struct TimeTZSortKeyOperator {\n \ttemplate <typename INPUT_TYPE, typename RESULT_TYPE>\n \tstatic RESULT_TYPE Operation(INPUT_TYPE input) {\n@@ -44,5 +61,4 @@ static void TimeTZSortKeyFunction(DataChunk &input, ExpressionState &state, Vect\n ScalarFunction TimeTZSortKeyFun::GetFunction() {\n \treturn ScalarFunction({LogicalType::TIME_TZ}, LogicalType::UBIGINT, TimeTZSortKeyFunction);\n }\n-\n } // namespace duckdb\ndiff --git a/extension/core_functions/scalar/date/functions.json b/extension/core_functions/scalar/date/functions.json\nindex cd74605174b5..6da0714bc1ee 100644\n--- a/extension/core_functions/scalar/date/functions.json\n+++ b/extension/core_functions/scalar/date/functions.json\n@@ -276,6 +276,14 @@\n         \"example\": \"nanosecond(timestamp_ns '2021-08-03 11:59:44.123456789') => 44123456789\",\n         \"type\": \"scalar_function_set\"\n     },\n+    {\n+        \"struct\": \"NormalizedIntervalFun\",\n+        \"name\": \"normalized_interval\",\n+        \"parameters\": \"interval\",\n+        \"description\": \"Normalizes an INTERVAL to an equivalent interval\",\n+        \"example\": \"normalized_interval(INTERVAL '30 days')\",\n+        \"type\": \"scalar_function\"\n+    },\n     {\n         \"name\": \"quarter\",\n         \"parameters\": \"ts\",\ndiff --git a/src/include/duckdb/common/types/interval.hpp b/src/include/duckdb/common/types/interval.hpp\nindex 85a90666c698..fdda9ee2afe2 100644\n--- a/src/include/duckdb/common/types/interval.hpp\n+++ b/src/include/duckdb/common/types/interval.hpp\n@@ -27,6 +27,11 @@ struct interval_t { // NOLINT\n \tint64_t micros;\n \n \tinline void Normalize(int64_t &months, int64_t &days, int64_t &micros) const;\n+\n+\t// Normalize to interval bounds.\n+\tinline static void Borrow(const int64_t msf, int64_t &lsf, int32_t &f, const int64_t scale);\n+\tinline interval_t Normalize() const;\n+\n \tinline bool operator==(const interval_t &right) const {\n \t\t//\tQuick equality check\n \t\tconst auto &left = *this;\n@@ -165,6 +170,7 @@ class Interval {\n \t\treturn left > right;\n \t}\n };\n+\n void interval_t::Normalize(int64_t &months, int64_t &days, int64_t &micros) const {\n \tauto &input = *this;\n \n@@ -182,4 +188,30 @@ void interval_t::Normalize(int64_t &months, int64_t &days, int64_t &micros) cons\n \tmonths += carry_months;\n }\n \n+void interval_t::Borrow(const int64_t msf, int64_t &lsf, int32_t &f, const int64_t scale) {\n+\tif (msf > NumericLimits<int32_t>::Maximum()) {\n+\t\tf = NumericLimits<int32_t>::Maximum();\n+\t\tlsf += (msf - f) * scale;\n+\t} else if (msf < NumericLimits<int32_t>::Minimum()) {\n+\t\tf = NumericLimits<int32_t>::Minimum();\n+\t\tlsf += (msf - f) * scale;\n+\t} else {\n+\t\tf = UnsafeNumericCast<int32_t>(msf);\n+\t}\n+}\n+\n+interval_t interval_t::Normalize() const {\n+\tinterval_t result;\n+\n+\tint64_t mm;\n+\tint64_t dd;\n+\tNormalize(mm, dd, result.micros);\n+\n+\t//  Borrow right on overflow\n+\tBorrow(mm, dd, result.months, Interval::DAYS_PER_MONTH);\n+\tBorrow(dd, result.micros, result.days, Interval::MICROS_PER_DAY);\n+\n+\treturn result;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/planner/collation_binding.cpp b/src/planner/collation_binding.cpp\nindex d620b63fcbd7..aabde7f55b62 100644\n--- a/src/planner/collation_binding.cpp\n+++ b/src/planner/collation_binding.cpp\n@@ -87,10 +87,32 @@ bool PushTimeTZCollation(ClientContext &context, unique_ptr<Expression> &source,\n \treturn true;\n }\n \n+bool PushIntervalCollation(ClientContext &context, unique_ptr<Expression> &source, const LogicalType &sql_type,\n+                           CollationType) {\n+\tif (sql_type.id() != LogicalTypeId::INTERVAL) {\n+\t\treturn false;\n+\t}\n+\n+\tauto &catalog = Catalog::GetSystemCatalog(context);\n+\tauto &function_entry = catalog.GetEntry<ScalarFunctionCatalogEntry>(context, DEFAULT_SCHEMA, \"normalized_interval\");\n+\tif (function_entry.functions.Size() != 1) {\n+\t\tthrow InternalException(\"normalized_interval should only have a single overload\");\n+\t}\n+\tauto &scalar_function = function_entry.functions.GetFunctionReferenceByOffset(0);\n+\tvector<unique_ptr<Expression>> children;\n+\tchildren.push_back(std::move(source));\n+\n+\tFunctionBinder function_binder(context);\n+\tauto function = function_binder.BindScalarFunction(scalar_function, std::move(children));\n+\tsource = std::move(function);\n+\treturn true;\n+}\n+\n // timetz_byte_comparable\n CollationBinding::CollationBinding() {\n \tRegisterCollation(CollationCallback(PushVarcharCollation));\n \tRegisterCollation(CollationCallback(PushTimeTZCollation));\n+\tRegisterCollation(CollationCallback(PushIntervalCollation));\n }\n \n void CollationBinding::RegisterCollation(CollationCallback callback) {\n", "instance_id": "duckdb__duckdb-15022", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `INTERVAL` type and `INNER JOIN` subquery in DuckDB, where the row count differs unexpectedly between two SQL queries (24 vs. 36 rows). It provides a reproducible SQL example, relevant environment details (OS, DuckDB version), and a clear indication of the unexpected behavior compared to PostgreSQL. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or output beyond stating that PostgreSQL returns 36 rows for both queries. It also lacks detailed discussion of edge cases or specific constraints related to `INTERVAL` comparisons. While the issue is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files (e.g., `function_list.cpp`, `interval.hpp`, `collation_binding.cpp`), indicating a need to understand and modify different parts of the DuckDB codebase, including core functions and planner logic. The changes involve adding a new `normalized_interval` function to handle `INTERVAL` normalization, which requires a deep understanding of DuckDB's internal representation of intervals and how they are compared in queries. \n\nSecond, the technical concepts involved are moderately complex, including knowledge of DuckDB's type system, scalar function registration, interval arithmetic, and collation binding for query optimization. The normalization logic itself requires handling overflow and borrowing across different time units (months, days, microseconds), which adds to the intricacy.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes imply the need to handle potential overflows in interval components (e.g., using `Borrow` to manage bounds), which suggests implicit error handling requirements. The impact on the system's architecture is moderate, as it introduces a new function and modifies how intervals are processed in comparisons, potentially affecting query execution behavior across the system.\n\nOverall, this problem requires a solid grasp of DuckDB's internals and careful implementation to ensure correctness, especially in comparison operations. It is not at the highest difficulty level (0.8-1.0) because it does not involve system-wide refactoring or highly advanced domain-specific knowledge, but it still poses a significant challenge due to the depth of understanding and precision required, justifying a score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "An alias of returning clause is dicarded in ToString of UpdateStatement and DeleteStatement\n### What happens?\n\nThis issue is related to #14523\r\n\r\nIn both following SQLs:\r\n\r\n```\r\nUPDATE Foo SET id = 1 RETURNING id AS updated;\r\nDELETE FROM Foo WHERE id = 1 RETURNING id AS deleted;\r\n```\r\n\r\n1. `SQLStatement` is extracted from Connection.\r\n2. Call `SQLStatement::ToString` method.\r\n\r\nA SQL discarded alias of returning clause is returned.\r\n\r\nBut  a RETURNING clause of UPDATE and DELETE statement is not documented  in the current official document.\r\n\r\nhttps://duckdb.org/docs/sql/statements/update#syntax\r\nhttps://duckdb.org/docs/sql/statements/delete#syntax\r\n\r\nSo I don't judge it as BUG.\r\nI also want to know whether it is documented the RETURNING clause of UPDATE and DELETE statement in the future version.\n\n### To Reproduce\n\n### UPDATE statement\r\n\r\nCode:\r\n```cpp\r\nstd::string sql(\"UPDATE Foo SET id = 1 RETURNING id AS updated\");\r\nauto stmts = conn.ExtractStatements(sql);\r\n\r\nstd::out << stmts[0]->ToString() << std::end;\r\n```\r\n\r\nActual result:\r\n```\r\nUPDATE Foo SET id = 1 RETURNING id\r\n```\r\n\r\nExpected result:\r\n```\r\nUPDATE Foo SET id = 1 RETURNING id AS updated\r\n```\r\n\r\n### DELETE statement\r\n\r\nCode:\r\n```cpp\r\nstd::string sql(\"DELETE FROM Foo WHERE id = 1 RETURNING id AS deleted\");\r\nauto stmts = conn.ExtractStatements(sql);\r\n\r\nstd::out << stmts[0]->ToString() << std::end;\r\n```\r\n\r\nActual result:\r\n```\r\nDELETE FROM Foo WHERE id = 1 RETURNING id\r\n```\r\n\r\nExpected result:\r\n```\r\nDELETE FROM Foo WHERE id = 1 RETURNING id AS deleted\r\n```\n\n### OS:\n\nMacOS Ventura 13.6.7 (x86_64 16GB RAM)\n\n### DuckDB Version:\n\n1.1.3\n\n### DuckDB Client:\n\nC++\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nKazuhiko TAMURA\n\n### Affiliation:\n\nhave no job\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have not tested with any build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNo - Other reason (please specify in the issue body)\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/parser/statement/delete_statement.cpp b/src/parser/statement/delete_statement.cpp\nindex 1de2767ca27d..0d97068e48bd 100644\n--- a/src/parser/statement/delete_statement.cpp\n+++ b/src/parser/statement/delete_statement.cpp\n@@ -43,7 +43,11 @@ string DeleteStatement::ToString() const {\n \t\t\tif (i > 0) {\n \t\t\t\tresult += \", \";\n \t\t\t}\n-\t\t\tresult += returning_list[i]->ToString();\n+\t\t\tauto column = returning_list[i]->ToString();\n+\t\t\tif (!returning_list[i]->alias.empty()) {\n+\t\t\t\tcolumn += StringUtil::Format(\" AS %s\", KeywordHelper::WriteOptionallyQuoted(returning_list[i]->alias));\n+\t\t\t}\n+\t\t\tresult += column;\n \t\t}\n \t}\n \treturn result;\ndiff --git a/src/parser/statement/update_statement.cpp b/src/parser/statement/update_statement.cpp\nindex 6c0c799191f4..835200943ddf 100644\n--- a/src/parser/statement/update_statement.cpp\n+++ b/src/parser/statement/update_statement.cpp\n@@ -65,7 +65,11 @@ string UpdateStatement::ToString() const {\n \t\t\tif (i > 0) {\n \t\t\t\tresult += \", \";\n \t\t\t}\n-\t\t\tresult += returning_list[i]->ToString();\n+\t\t\tauto column = returning_list[i]->ToString();\n+\t\t\tif (!returning_list[i]->alias.empty()) {\n+\t\t\t\tcolumn += StringUtil::Format(\" AS %s\", KeywordHelper::WriteOptionallyQuoted(returning_list[i]->alias));\n+\t\t\t}\n+\t\t\tresult += column;\n \t\t}\n \t}\n \treturn result;\n", "instance_id": "duckdb__duckdb-14765", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `ToString()` method of `UpdateStatement` and `DeleteStatement` in DuckDB discards aliases in the `RETURNING` clause of SQL statements. The goal is evident (preserve the alias in the output string), and the input/output expectations are demonstrated with examples for both `UPDATE` and `DELETE` statements. The issue is contextualized with references to related issues and documentation, and reproduction steps are provided with code snippets. However, there are minor ambiguities: the problem statement does not explicitly discuss edge cases (e.g., handling of special characters in aliases or empty aliases), and it raises a question about future documentation without clarifying whether this impacts the solution's requirements. Additionally, while the expected behavior is clear, there is no mention of potential constraints or performance considerations for the fix. Overall, the statement is valid and clear but lacks some minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling in the Easy range (0.2-0.4). The issue requires a straightforward modification to the `ToString()` method in two files (`delete_statement.cpp` and `update_statement.cpp`) to include the alias in the output string if it exists. The scope of the code changes is minimal, affecting only a small part of the codebase with nearly identical changes in both files, and does not impact the broader system architecture. The technical concepts involved are basic: string manipulation, conditional checks, and familiarity with the existing class structure (e.g., accessing the `alias` field). No advanced algorithms, design patterns, or domain-specific knowledge are required beyond basic C++ programming. The provided code changes are simple, involving just a few lines per file to check for a non-empty alias and append it to the result string. While edge cases like special characters in aliases or formatting issues could theoretically arise, they are not mentioned in the problem statement, and the fix does not appear to require complex error handling. Overall, this is a simple bug fix that requires minimal understanding of the codebase beyond the affected functions, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Inconsistent results when casting `BIT` and `VARBINARY`\n### What happens?\n\nConsider the following test case generated by my fuzzer. The second query and the third query could have inconsistent results. I suppose the second query should be consistently `true,true` and the third query should thus be `ab,cd`. \n\n### To Reproduce\n\n```sql\r\nCREATE TABLE  t1(c0 VARBINARY NOT NULL);\r\nINSERT INTO t1(c0) VALUES ('ab'), ('cd');\r\n-- INSERT INTO t1(c0) VALUES ('\ud83e\udd86'); -- error\r\n-- INSERT INTO t1(c0) VALUES (NULL); -- error\r\n\r\nSELECT t1.c0 FROM t1; -- 'ab', 'cd'\r\nSELECT ((CAST(CAST(t1.c0 AS BIT) AS VARBINARY))<=(t1.c0)) FROM t1; -- true, true (incosistent, could sometimes be false, false)\r\nSELECT t1.c0 FROM t1 WHERE ((CAST(CAST(t1.c0 AS BIT) AS VARBINARY))<=(t1.c0));\r\n-- Expected: 'ab', 'cd' (2 rows)\r\n-- Actual: could be 0 row, 1 row or 2 rows\r\n```\r\n\r\nNot sure if the two `INSERT` are needed. Sometimes I could also reproduce it without executing them, but sometimes can't.\r\n\r\nKindly inform me if it's an expected behavior and whether you could reproduce the issue.\n\n### OS:\n\nubuntu 22.04\n\n### DuckDB Version:\n\nv1.1.1-dev100 9af117f0e6\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nSuyang Zhong\n\n### Affiliation:\n\nNUS\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/common/types/bit.cpp b/src/common/types/bit.cpp\nindex ebd4ec3ac6e8..f263c2c42975 100644\n--- a/src/common/types/bit.cpp\n+++ b/src/common/types/bit.cpp\n@@ -180,7 +180,7 @@ void Bit::BitToBlob(string_t bit, string_t &output_blob) {\n \tidx_t size = output_blob.GetSize();\n \n \toutput[0] = UnsafeNumericCast<char>(GetFirstByte(bit));\n-\tif (size > 2) {\n+\tif (size >= 2) {\n \t\t++output;\n \t\t// First byte in bitstring contains amount of padded bits,\n \t\t// second byte in bitstring is the padded byte,\n", "instance_id": "duckdb__duckdb-13908", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue of inconsistent results when casting between `BIT` and `VARBINARY` in DuckDB. It provides a reproducible SQL test case, expected versus actual behavior, and relevant environment details (OS, DuckDB version, etc.). The goal is evident: to address the inconsistency in query results. However, there are minor ambiguities that prevent a perfect score. For instance, the statement mentions uncertainty about the necessity of certain `INSERT` statements for reproduction, and it lacks explicit mention of specific edge cases or constraints beyond the provided example. Additionally, it does not clarify whether this behavior is tied to specific data sizes, encodings, or hardware configurations beyond the provided setup. These missing details could complicate reproduction or understanding of the full scope of the issue.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal, involving a single line modification in a specific function (`Bit::BitToBlob`) within one file (`bit.cpp`). This change does not appear to impact the broader system architecture or require understanding complex interactions across multiple modules. Second, the technical concepts involved are relatively straightforward: the fix adjusts a conditional check (`size > 2` to `size >= 2`) to handle binary data conversion, likely addressing a boundary condition in byte handling. This requires basic knowledge of C++ and an understanding of binary data manipulation, but no advanced algorithms, design patterns, or domain-specific knowledge are evident. Third, while the problem statement hints at inconsistent behavior (potentially tied to edge cases like data size or content), the provided fix does not explicitly address complex error handling or performance considerations. Overall, solving this issue requires understanding some code logic and making a simple modification, fitting the lower end of the difficulty spectrum. The score of 0.35 reflects that it is slightly more involved than a trivial fix (e.g., a typo) due to the need to understand the context of binary casting and verify the impact of the change, but it remains a relatively contained and approachable task for a developer familiar with the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`USE` does not affect for the table referenced after the keyword ON for the `CREATE UNIQUE INDEX ... ON ...`\n### What happens?\r\n\r\nThe `USE` command does not affect for the table referenced after the keyword ON for the `CREATE UNIQUE INDEX ... ON ...` statement.\r\n\r\n```sql\r\ncreate schema mydb;\r\nuse mydb;\r\ncreate table mytable (i bigint primary key, s varchar(20) NOT NULL);\r\nCREATE UNIQUE INDEX mytable_s ON mytable(s NULLS FIRST);\r\n```\r\n```\r\nCatalog Error: Table with name mytable does not exist!\r\nDid you mean \"memory.mydb.mytable\"?\r\n```\r\n\r\nThis issue occurs in both `duckdb` command line and go-duckdb.\r\n\r\n### To Reproduce\r\n\r\nuse `duckdb` command line\r\n\r\n```sql\r\ncreate schema mydb;\r\nuse mydb;\r\ncreate table mytable (i bigint primary key, s varchar(20) NOT NULL);\r\nCREATE UNIQUE INDEX mytable_s ON mytable(s NULLS FIRST);\r\n```\r\n```\r\nCatalog Error: Table with name mytable does not exist!\r\nDid you mean \"memory.mydb.mytable\"?\r\n```\r\n\r\n### OS:\r\n\r\nmacOS m1 arm\r\n\r\n### DuckDB Version:\r\n\r\nv1.0.0 1f98600c2c\r\n\r\n### DuckDB Client:\r\n\r\nduckdb cli\r\n\r\n### Full Name:\r\n\r\nYusong Gao\r\n\r\n### Affiliation:\r\n\r\nApeCloud\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n`USE` does not affect for the table referenced after the keyword ON for the `CREATE UNIQUE INDEX ... ON ...`\n### What happens?\r\n\r\nThe `USE` command does not affect for the table referenced after the keyword ON for the `CREATE UNIQUE INDEX ... ON ...` statement.\r\n\r\n```sql\r\ncreate schema mydb;\r\nuse mydb;\r\ncreate table mytable (i bigint primary key, s varchar(20) NOT NULL);\r\nCREATE UNIQUE INDEX mytable_s ON mytable(s NULLS FIRST);\r\n```\r\n```\r\nCatalog Error: Table with name mytable does not exist!\r\nDid you mean \"memory.mydb.mytable\"?\r\n```\r\n\r\nThis issue occurs in both `duckdb` command line and go-duckdb.\r\n\r\n### To Reproduce\r\n\r\nuse `duckdb` command line\r\n\r\n```sql\r\ncreate schema mydb;\r\nuse mydb;\r\ncreate table mytable (i bigint primary key, s varchar(20) NOT NULL);\r\nCREATE UNIQUE INDEX mytable_s ON mytable(s NULLS FIRST);\r\n```\r\n```\r\nCatalog Error: Table with name mytable does not exist!\r\nDid you mean \"memory.mydb.mytable\"?\r\n```\r\n\r\n### OS:\r\n\r\nmacOS m1 arm\r\n\r\n### DuckDB Version:\r\n\r\nv1.0.0 1f98600c2c\r\n\r\n### DuckDB Client:\r\n\r\nduckdb cli\r\n\r\n### Full Name:\r\n\r\nYusong Gao\r\n\r\n### Affiliation:\r\n\r\nApeCloud\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n", "patch": "diff --git a/src/parser/parsed_data/create_index_info.cpp b/src/parser/parsed_data/create_index_info.cpp\nindex 8f74c05c7086..3d41ea4fea2a 100644\n--- a/src/parser/parsed_data/create_index_info.cpp\n+++ b/src/parser/parsed_data/create_index_info.cpp\n@@ -4,7 +4,7 @@\n \n namespace duckdb {\n \n-CreateIndexInfo::CreateIndexInfo() : CreateInfo(CatalogType::INDEX_ENTRY) {\n+CreateIndexInfo::CreateIndexInfo() : CreateInfo(CatalogType::INDEX_ENTRY, INVALID_SCHEMA) {\n }\n \n CreateIndexInfo::CreateIndexInfo(const duckdb::CreateIndexInfo &info)\n", "instance_id": "duckdb__duckdb-13663", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `USE` command does not affect table references in the `CREATE UNIQUE INDEX ... ON ...` statement, leading to a catalog error where the table cannot be found. The statement includes reproducible SQL code, error messages, and relevant environment details (OS, DuckDB version, client). However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior (e.g., should the table name be resolved using the current schema set by `USE`?) nor does it discuss potential edge cases or alternative scenarios (e.g., behavior with fully qualified table names or multiple schemas). Additionally, the problem statement repeats itself unnecessarily, which slightly detracts from its clarity. Overall, it is valid and clear but lacks some depth in specifying expected outcomes and edge cases.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the provided code changes and the nature of the issue. The code modification is minimal, involving a single-line change in the constructor of `CreateIndexInfo` to initialize with `INVALID_SCHEMA` instead of the default. This suggests the fix is related to how schema resolution is handled for index creation, likely a small bug in the catalog or parser logic of DuckDB. The scope of the change is limited to a single file and does not appear to impact broader system architecture or require extensive refactoring. The technical concepts involved seem straightforward, likely requiring basic knowledge of DuckDB's catalog system and schema resolution mechanics, which are not overly complex for someone familiar with database internals or the DuckDB codebase. Edge cases and error handling do not seem to be a significant concern based on the problem statement or code change, as the fix appears to address a specific schema resolution issue without introducing new error conditions. Overall, this is a relatively simple bug fix that requires understanding some code logic but does not demand deep architectural changes or advanced technical expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "feat: Adds hires fix on layer diffusion\n* feat: Adds hires fix to layer diffusion\r\n* fix: Ensures steps are used as intended in stable cascade\r\n* chore: Adds pipeline design for layer diffusion on standard SD\r\n* fix: Uses denoise to calculate a lower step count when doing hires fix second pass\r\n  - Previous to this change, hires fix was doing the full amount of steps on each pass. This is unnecessary as there are (massive) diminishing returns on the second pass. \r\n    - For example, 10 steps versus 20 steps on the second pass can be virtually indistinguishable from each other, so this logic will ultimately help reduce the time spent generating these images. This is especially true for high-step requests. \r\n  - It is worth pointing out that the reference images used to confirm images remain the same version-to-version of horde-engine did not require updating. These images are checked using a form of cosine similarity, so at least to the computer, the 'new' generated hires fix images are basically identical before and after the change.\r\n\r\nAll tox passes on my end\n", "patch": "diff --git a/hordelib/comfy_horde.py b/hordelib/comfy_horde.py\nindex e762f1ac..6dbc9f40 100644\n--- a/hordelib/comfy_horde.py\n+++ b/hordelib/comfy_horde.py\n@@ -213,14 +213,7 @@ def do_comfy_import(\n \n         # comfy.model_management.unet_offload_device = _unet_offload_device_hijack\n \n-        total_vram = get_torch_total_vram_mb()\n-        total_ram = psutil.virtual_memory().total / (1024 * 1024)\n-        free_ram = psutil.virtual_memory().available / (1024 * 1024)\n-\n-        free_vram = get_torch_free_vram_mb()\n-\n-        logger.debug(f\"Total VRAM {total_vram:0.0f} MB, Total System RAM {total_ram:0.0f} MB\")\n-        logger.debug(f\"Free VRAM {free_vram:0.0f} MB, Free System RAM {free_ram:0.0f} MB\")\n+    log_free_ram()\n     output_collector.replay()\n \n \n@@ -308,6 +301,11 @@ def get_torch_free_vram_mb():\n     return round(_comfy_get_free_memory() / (1024 * 1024))\n \n \n+def log_free_ram():\n+    logger.debug(f\"Free VRAM: {get_torch_free_vram_mb():0.0f} MB\")\n+    logger.debug(f\"Free RAM: {psutil.virtual_memory().available / (1024 * 1024):0.0f} MB\")\n+\n+\n class Comfy_Horde:\n     \"\"\"Handles horde-specific behavior against ComfyUI.\"\"\"\n \n@@ -446,7 +444,7 @@ def _this_dir(self, filename: str, subdir=\"\") -> str:\n \n     def _load_custom_nodes(self) -> None:\n         \"\"\"Force ComfyUI to load its normal custom nodes and the horde custom nodes.\"\"\"\n-        _comfy_nodes.init_custom_nodes()\n+        _comfy_nodes.init_extra_nodes(init_custom_nodes=True)\n \n     def _get_executor(self):\n         \"\"\"Return the ComfyUI PromptExecutor object.\"\"\"\n@@ -744,7 +742,7 @@ def _run_pipeline(\n         # if time.time() - self._gc_timer > Comfy_Horde.GC_TIME:\n         #     self._gc_timer = time.time()\n         #     garbage_collect()\n-\n+        log_free_ram()\n         return self.images\n \n     # Run a pipeline that returns an image in pixel space\ndiff --git a/hordelib/consts.py b/hordelib/consts.py\nindex 3c335967..212f165e 100644\n--- a/hordelib/consts.py\n+++ b/hordelib/consts.py\n@@ -6,7 +6,7 @@\n \n from hordelib.config_path import get_hordelib_path\n \n-COMFYUI_VERSION = \"16eabdf70dbdb64dc4822908f0fe455c56d11ec3\"\n+COMFYUI_VERSION = \"2dc84d14447782683862616eaf8c19c0c1feacf3\"\n \"\"\"The exact version of ComfyUI version to load.\"\"\"\n \n REMOTE_PROXY = \"\"\ndiff --git a/hordelib/horde.py b/hordelib/horde.py\nindex 76ef9d90..718cbfd2 100644\n--- a/hordelib/horde.py\n+++ b/hordelib/horde.py\n@@ -11,6 +11,7 @@\n from collections.abc import Callable\n from copy import deepcopy\n from enum import Enum, auto\n+from types import FunctionType\n \n from horde_sdk.ai_horde_api.apimodels import ImageGenerateJobPopResponse\n from horde_sdk.ai_horde_api.apimodels.base import (\n@@ -77,6 +78,17 @@ def __init__(\n         self.faults = faults\n \n \n+def _calc_upscale_sampler_steps(payload):\n+    \"\"\"Calculates the amount of hires_fix upscaler steps based on the denoising used and the steps used for the\n+    primary image\"\"\"\n+    upscale_steps = round(payload[\"ddim_steps\"] * (0.9 - payload[\"hires_fix_denoising_strength\"]))\n+    if upscale_steps < 3:\n+        upscale_steps = 3\n+\n+    logger.debug(f\"Upscale steps calculated as {upscale_steps}\")\n+    return upscale_steps\n+\n+\n class HordeLib:\n     _instance: HordeLib | None = None\n     _initialised = False\n@@ -227,7 +239,7 @@ class HordeLib:\n         \"upscale_sampler.denoise\": \"hires_fix_denoising_strength\",\n         \"upscale_sampler.seed\": \"seed\",\n         \"upscale_sampler.cfg\": \"cfg_scale\",\n-        \"upscale_sampler.steps\": \"ddim_steps\",\n+        \"upscale_sampler.steps\": _calc_upscale_sampler_steps,\n         \"upscale_sampler.sampler_name\": \"sampler_name\",\n         \"controlnet_apply.strength\": \"control_strength\",\n         \"controlnet_model_loader.control_net_name\": \"control_type\",\n@@ -243,6 +255,8 @@ class HordeLib:\n         \"sampler_stage_c.denoise\": \"denoising_strength\",\n         \"sampler_stage_b.seed\": \"seed\",\n         \"sampler_stage_c.seed\": \"seed\",\n+        \"sampler_stage_b.steps\": \"ddim_steps*0.33\",\n+        \"sampler_stage_c.steps\": \"ddim_steps*0.67\",\n         \"model_loader_stage_c.ckpt_name\": \"stable_cascade_stage_c\",\n         \"model_loader_stage_c.model_name\": \"stable_cascade_stage_c\",\n         \"model_loader_stage_c.horde_model_name\": \"model_name\",\n@@ -251,8 +265,10 @@ class HordeLib:\n         \"model_loader_stage_b.horde_model_name\": \"model_name\",\n         # Stable Cascade 2pass\n         \"2pass_sampler_stage_c.sampler_name\": \"sampler_name\",\n+        \"2pass_sampler_stage_c.steps\": \"ddim_steps*0.67\",\n         \"2pass_sampler_stage_c.denoise\": \"hires_fix_denoising_strength\",\n         \"2pass_sampler_stage_b.sampler_name\": \"sampler_name\",\n+        \"2pass_sampler_stage_b.steps\": \"ddim_steps*0.33\",\n         # QR Codes\n         \"sampler_bg.sampler_name\": \"sampler_name\",\n         \"sampler_bg.cfg\": \"cfg_scale\",\n@@ -519,8 +535,15 @@ def _apply_aihorde_compatibility_hacks(self, payload: dict) -> tuple[dict, list[\n \n         # Turn off hires fix if we're not generating a hires image, or if the params are just confused\n         try:\n-            if \"hires_fix\" in payload and (payload[\"width\"] <= 512 or payload[\"height\"] <= 512):\n-                payload[\"hires_fix\"] = False\n+            if \"hires_fix\" in payload:\n+                if SharedModelManager.manager.compvis.model_reference[model].get(\n+                    \"baseline\",\n+                ) == \"stable diffusion 1\" and (payload[\"width\"] <= 512 or payload[\"height\"] <= 512):\n+                    payload[\"hires_fix\"] = False\n+                elif SharedModelManager.manager.compvis.model_reference[model].get(\n+                    \"baseline\",\n+                ) == \"stable_diffusion_xl\" and (payload[\"width\"] <= 1024 or payload[\"height\"] <= 1024):\n+                    payload[\"hires_fix\"] = False\n         except (TypeError, KeyError):\n             payload[\"hires_fix\"] = False\n \n@@ -792,8 +815,18 @@ def _final_pipeline_adjustments(self, payload, pipeline_data) -> tuple[dict, lis\n         # Translate the payload parameters into pipeline parameters\n         pipeline_params = {}\n         for newkey, key in HordeLib.PAYLOAD_TO_PIPELINE_PARAMETER_MAPPING.items():\n-            if key in payload:\n-                pipeline_params[newkey] = payload.get(key)\n+            multiplier = None\n+            # We allow a multiplier in the param, so that I can adjust easily the\n+            # values for steps on things like stable cascade\n+            if isinstance(key, FunctionType):\n+                pipeline_params[newkey] = key(payload)\n+            elif \"*\" in key:\n+                key, multiplier = key.split(\"*\", 1)\n+            elif key in payload:\n+                if multiplier:\n+                    pipeline_params[newkey] = round(payload.get(key) * float(multiplier))\n+                else:\n+                    pipeline_params[newkey] = payload.get(key)\n             else:\n                 logger.error(f\"Parameter {key} not found\")\n         # We inject these parameters to ensure the HordeCheckpointLoader knows what file to load, if necessary\n@@ -827,8 +860,12 @@ def _final_pipeline_adjustments(self, payload, pipeline_data) -> tuple[dict, lis\n             original_height = pipeline_params.get(\"empty_latent_image.height\")\n \n             if original_width is None or original_height is None:\n-                logger.error(\"empty_latent_image.width or empty_latent_image.height not found. Using 512x512.\")\n-                original_width, original_height = (512, 512)\n+                if model_details and model_details.get(\"baseline\") == \"stable diffusion 1\":\n+                    logger.error(\"empty_latent_image.width or empty_latent_image.height not found. Using 512x512.\")\n+                    original_width, original_height = (512, 512)\n+                else:\n+                    logger.error(\"empty_latent_image.width or empty_latent_image.height not found. Using 1024x1024.\")\n+                    original_width, original_height = (1024, 1024)\n \n             new_width, new_height = (None, None)\n \n@@ -1041,6 +1078,8 @@ def _final_pipeline_adjustments(self, payload, pipeline_data) -> tuple[dict, lis\n                     self.generator.reconnect_input(pipeline_data, \"layer_diffuse_apply.model\", \"model_loader\")\n                     self.generator.reconnect_input(pipeline_data, \"output_image.images\", \"layer_diffuse_decode_rgba\")\n                     self.generator.reconnect_input(pipeline_data, \"layer_diffuse_decode_rgba.images\", \"vae_decode\")\n+                    if payload.get(\"hires_fix\") is True:\n+                        self.generator.reconnect_input(pipeline_data, \"upscale_sampler.model\", \"layer_diffuse_apply\")\n                     if model_details.get(\"baseline\") == \"stable diffusion 1\":\n                         pipeline_params[\"layer_diffuse_apply.config\"] = \"SD15, Attention Injection, attn_sharing\"\n                         pipeline_params[\"layer_diffuse_decode_rgba.sd_version\"] = \"SD15\"\n@@ -1489,6 +1528,11 @@ def basic_inference_rawpng(self, payload: dict) -> list[io.BytesIO]:\n \n     def image_upscale(self, payload) -> ResultingImageReturn:\n         logger.debug(\"image_upscale called\")\n+\n+        from hordelib.comfy_horde import log_free_ram\n+\n+        log_free_ram()\n+\n         # AIHorde hacks to payload\n         payload, compatibility_faults = self._apply_aihorde_compatibility_hacks(payload)\n         # Remember if we were passed width and height, we wouldn't normally be passed width and height\n@@ -1522,10 +1566,16 @@ def image_upscale(self, payload) -> ResultingImageReturn:\n         if not isinstance(image, Image.Image):\n             raise RuntimeError(f\"Expected a PIL.Image.Image but got {type(image)}\")\n \n+        log_free_ram()\n         return ResultingImageReturn(image=image, rawpng=rawpng, faults=compatibility_faults + final_adjustment_faults)\n \n     def image_facefix(self, payload) -> ResultingImageReturn:\n         logger.debug(\"image_facefix called\")\n+\n+        from hordelib.comfy_horde import log_free_ram\n+\n+        log_free_ram()\n+\n         # AIHorde hacks to payload\n         payload, compatibility_faults = self._apply_aihorde_compatibility_hacks(payload)\n         # Check payload types/values and normalise it's format\n@@ -1547,4 +1597,6 @@ def image_facefix(self, payload) -> ResultingImageReturn:\n         if not isinstance(image, Image.Image):\n             raise RuntimeError(f\"Expected a PIL.Image.Image but got {type(image)}\")\n \n+        log_free_ram()\n+\n         return ResultingImageReturn(image=image, rawpng=rawpng, faults=compatibility_faults + final_adjustment_faults)\ndiff --git a/hordelib/nodes/node_controlnet_model_loader.py b/hordelib/nodes/node_controlnet_model_loader.py\nindex 4352161f..cc168660 100644\n--- a/hordelib/nodes/node_controlnet_model_loader.py\n+++ b/hordelib/nodes/node_controlnet_model_loader.py\n@@ -18,16 +18,21 @@ def INPUT_TYPES(s):\n     CATEGORY = \"loaders\"\n \n     def load_controlnet(self, model, control_net_name, model_manager):\n+        from hordelib.comfy_horde import log_free_ram\n+\n         logger.debug(f\"Loading controlnet {control_net_name} through our custom node\")\n+        log_free_ram()\n \n         if not model_manager or not model_manager.manager or not model_manager.manager.controlnet:\n             logger.error(\"controlnet model_manager appears to be missing!\")\n             raise RuntimeError  # XXX better guarantees need to be made\n \n-        return model_manager.manager.controlnet.merge_controlnet(\n+        merge_result = model_manager.manager.controlnet.merge_controlnet(\n             control_net_name,\n             model,\n         )\n+        log_free_ram()\n+        return merge_result\n \n \n NODE_CLASS_MAPPINGS = {\"HordeDiffControlNetLoader\": HordeDiffControlNetLoader}\ndiff --git a/hordelib/nodes/node_lora_loader.py b/hordelib/nodes/node_lora_loader.py\nindex 2c0fa1c1..6d817d93 100644\n--- a/hordelib/nodes/node_lora_loader.py\n+++ b/hordelib/nodes/node_lora_loader.py\n@@ -27,11 +27,18 @@ def INPUT_TYPES(s):\n     CATEGORY = \"loaders\"\n \n     def load_lora(self, model, clip, lora_name, strength_model, strength_clip):\n+        from hordelib.comfy_horde import log_free_ram\n+\n+        log_free_ram()\n+\n         _test_exception = os.getenv(\"FAILURE_TEST\", False)\n         if _test_exception:\n             raise Exception(\"This tests exceptions being thrown from within the pipeline\")\n \n+        logger.debug(f\"Loading lora {lora_name} through our custom node\")\n+\n         if strength_model == 0 and strength_clip == 0:\n+            logger.debug(\"Strengths are 0, skipping lora loading\")\n             return (model, clip)\n \n         if lora_name is None or lora_name == \"\" or lora_name == \"None\":\n@@ -67,6 +74,7 @@ def load_lora(self, model, clip, lora_name, strength_model, strength_clip):\n             self.loaded_lora = (lora_path, lora)\n \n         model_lora, clip_lora = comfy.sd.load_lora_for_models(model, clip, lora, strength_model, strength_clip)\n+        log_free_ram()\n         return (model_lora, clip_lora)\n \n \ndiff --git a/hordelib/nodes/node_model_loader.py b/hordelib/nodes/node_model_loader.py\nindex 6f37b22b..4ca17900 100644\n--- a/hordelib/nodes/node_model_loader.py\n+++ b/hordelib/nodes/node_model_loader.py\n@@ -10,6 +10,7 @@\n from loguru import logger\n \n from hordelib.shared_model_manager import SharedModelManager\n+from hordelib.comfy_horde import log_free_ram\n \n \n # Don't let the name fool you, this class is trying to load all the files that will be necessary\n@@ -44,6 +45,7 @@ def load_checkpoint(\n         output_clip=True,\n         preloading=False,\n     ):\n+        log_free_ram()\n         if file_type is not None:\n             logger.debug(f\"Loading model {horde_model_name}:{file_type}\")\n         else:\n@@ -77,7 +79,7 @@ def load_checkpoint(\n                 make_regular_vae(same_loaded_model[0][2])\n \n             logger.debug(\"Model was previously loaded, returning it.\")\n-\n+            log_free_ram()\n             return same_loaded_model[0]\n \n         if not ckpt_name:\n@@ -133,6 +135,7 @@ def load_checkpoint(\n             result[0].model.apply(make_regular)\n             make_regular_vae(result[2])\n \n+        log_free_ram()\n         return result\n \n \ndiff --git a/hordelib/nodes/node_upscale_model_loader.py b/hordelib/nodes/node_upscale_model_loader.py\nindex ff32ee80..1a345feb 100644\n--- a/hordelib/nodes/node_upscale_model_loader.py\n+++ b/hordelib/nodes/node_upscale_model_loader.py\n@@ -18,11 +18,15 @@ def INPUT_TYPES(s):\n     CATEGORY = \"loaders\"\n \n     def load_model(self, model_name):\n+        from hordelib.comfy_horde import log_free_ram\n+\n+        log_free_ram()\n         model_path = folder_paths.get_full_path(\"upscale_models\", model_name)\n         sd = comfy.utils.load_torch_file(model_path, safe_load=True)\n         if \"module.layers.0.residual_group.blocks.0.norm1.weight\" in sd:\n             sd = comfy.utils.state_dict_prefix_replace(sd, {\"module.\": \"\"})\n         out = model_loading.load_state_dict(sd).eval()\n+        log_free_ram()\n         return (out,)\n \n \ndiff --git a/hordelib/pipeline_designs/pipeline_stable_diffusion.json b/hordelib/pipeline_designs/pipeline_stable_diffusion.json\nindex d68daa96..6052bc37 100644\n--- a/hordelib/pipeline_designs/pipeline_stable_diffusion.json\n+++ b/hordelib/pipeline_designs/pipeline_stable_diffusion.json\n@@ -1,6 +1,6 @@\n {\n-  \"last_node_id\": 15,\n-  \"last_link_id\": 23,\n+  \"last_node_id\": 17,\n+  \"last_link_id\": 36,\n   \"nodes\": [\n     {\n       \"id\": 7,\n@@ -14,7 +14,7 @@\n         \"1\": 90.3333740234375\n       },\n       \"flags\": {},\n-      \"order\": 6,\n+      \"order\": 8,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -54,7 +54,7 @@\n         \"1\": 83.00006103515625\n       },\n       \"flags\": {},\n-      \"order\": 5,\n+      \"order\": 7,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -93,7 +93,7 @@\n         \"1\": 58\n       },\n       \"flags\": {},\n-      \"order\": 3,\n+      \"order\": 5,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -133,13 +133,13 @@\n         \"1\": 400\n       },\n       \"flags\": {},\n-      \"order\": 10,\n+      \"order\": 12,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n           \"name\": \"images\",\n           \"type\": \"IMAGE\",\n-          \"link\": 21\n+          \"link\": 36\n         }\n       ],\n       \"title\": \"output_image\",\n@@ -148,6 +148,84 @@\n         \"ComfyUI\"\n       ]\n     },\n+    {\n+      \"id\": 15,\n+      \"type\": \"RepeatImageBatch\",\n+      \"pos\": [\n+        257,\n+        1026\n+      ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 58\n+      },\n+      \"flags\": {},\n+      \"order\": 3,\n+      \"mode\": 0,\n+      \"inputs\": [\n+        {\n+          \"name\": \"image\",\n+          \"type\": \"IMAGE\",\n+          \"link\": 22\n+        }\n+      ],\n+      \"outputs\": [\n+        {\n+          \"name\": \"IMAGE\",\n+          \"type\": \"IMAGE\",\n+          \"links\": [\n+            23\n+          ],\n+          \"shape\": 3,\n+          \"slot_index\": 0\n+        }\n+      ],\n+      \"title\": \"repeat_image_batch\",\n+      \"properties\": {\n+        \"Node name for S&R\": \"RepeatImageBatch\"\n+      },\n+      \"widgets_values\": [\n+        1\n+      ]\n+    },\n+    {\n+      \"id\": 11,\n+      \"type\": \"LoadImage\",\n+      \"pos\": [\n+        -91,\n+        768\n+      ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 314\n+      },\n+      \"flags\": {},\n+      \"order\": 0,\n+      \"mode\": 0,\n+      \"outputs\": [\n+        {\n+          \"name\": \"IMAGE\",\n+          \"type\": \"IMAGE\",\n+          \"links\": [\n+            22\n+          ],\n+          \"slot_index\": 0\n+        },\n+        {\n+          \"name\": \"MASK\",\n+          \"type\": \"MASK\",\n+          \"links\": null\n+        }\n+      ],\n+      \"title\": \"image_loader\",\n+      \"properties\": {\n+        \"Node name for S&R\": \"LoadImage\"\n+      },\n+      \"widgets_values\": [\n+        \"example.png\",\n+        \"image\"\n+      ]\n+    },\n     {\n       \"id\": 12,\n       \"type\": \"VAEEncode\",\n@@ -160,7 +238,7 @@\n         \"1\": 46\n       },\n       \"flags\": {},\n-      \"order\": 7,\n+      \"order\": 6,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -188,114 +266,180 @@\n       }\n     },\n     {\n-      \"id\": 5,\n-      \"type\": \"EmptyLatentImage\",\n+      \"id\": 3,\n+      \"type\": \"KSampler\",\n       \"pos\": [\n-        575,\n-        565\n+        1019,\n+        107\n       ],\n       \"size\": {\n         \"0\": 315,\n-        \"1\": 106\n+        \"1\": 262\n       },\n       \"flags\": {},\n-      \"order\": 1,\n+      \"order\": 9,\n       \"mode\": 0,\n+      \"inputs\": [\n+        {\n+          \"name\": \"model\",\n+          \"type\": \"MODEL\",\n+          \"link\": 35\n+        },\n+        {\n+          \"name\": \"positive\",\n+          \"type\": \"CONDITIONING\",\n+          \"link\": 4\n+        },\n+        {\n+          \"name\": \"negative\",\n+          \"type\": \"CONDITIONING\",\n+          \"link\": 6\n+        },\n+        {\n+          \"name\": \"latent_image\",\n+          \"type\": \"LATENT\",\n+          \"link\": 34\n+        }\n+      ],\n       \"outputs\": [\n         {\n           \"name\": \"LATENT\",\n           \"type\": \"LATENT\",\n           \"links\": [\n-            18\n+            20,\n+            24\n           ],\n           \"slot_index\": 0\n         }\n       ],\n-      \"title\": \"empty_latent_image\",\n+      \"title\": \"sampler\",\n       \"properties\": {\n-        \"Node name for S&R\": \"EmptyLatentImage\"\n+        \"Node name for S&R\": \"KSampler\"\n       },\n       \"widgets_values\": [\n-        512,\n-        512,\n-        1\n+        283224582220517,\n+        \"randomize\",\n+        20,\n+        8,\n+        \"euler\",\n+        \"normal\",\n+        0.8\n       ]\n     },\n     {\n-      \"id\": 11,\n-      \"type\": \"LoadImage\",\n+      \"id\": 16,\n+      \"type\": \"LayeredDiffusionApply\",\n       \"pos\": [\n-        -91,\n-        768\n-      ],\n-      \"size\": [\n-        315,\n-        314\n+        597,\n+        254\n       ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 82\n+      },\n       \"flags\": {},\n-      \"order\": 2,\n+      \"order\": 4,\n       \"mode\": 0,\n+      \"inputs\": [\n+        {\n+          \"name\": \"model\",\n+          \"type\": \"MODEL\",\n+          \"link\": 26\n+        }\n+      ],\n       \"outputs\": [\n         {\n-          \"name\": \"IMAGE\",\n-          \"type\": \"IMAGE\",\n-          \"links\": [\n-            22\n-          ],\n+          \"name\": \"MODEL\",\n+          \"type\": \"MODEL\",\n+          \"links\": [],\n+          \"shape\": 3,\n           \"slot_index\": 0\n-        },\n-        {\n-          \"name\": \"MASK\",\n-          \"type\": \"MASK\",\n-          \"links\": null\n         }\n       ],\n-      \"title\": \"image_loader\",\n+      \"title\": \"layer_diffuse_apply\",\n       \"properties\": {\n-        \"Node name for S&R\": \"LoadImage\"\n+        \"Node name for S&R\": \"LayeredDiffusionApply\"\n       },\n       \"widgets_values\": [\n-        \"example.png\",\n-        \"image\"\n+        \"SD15, Attention Injection, attn_sharing\",\n+        1\n       ]\n     },\n     {\n-      \"id\": 15,\n-      \"type\": \"RepeatImageBatch\",\n+      \"id\": 17,\n+      \"type\": \"LayeredDiffusionDecodeRGBA\",\n       \"pos\": [\n-        257,\n-        1026\n+        1621,\n+        234\n       ],\n       \"size\": {\n         \"0\": 315,\n-        \"1\": 58\n+        \"1\": 102\n       },\n       \"flags\": {},\n-      \"order\": 4,\n+      \"order\": 11,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n-          \"name\": \"image\",\n+          \"name\": \"samples\",\n+          \"type\": \"LATENT\",\n+          \"link\": 24\n+        },\n+        {\n+          \"name\": \"images\",\n           \"type\": \"IMAGE\",\n-          \"link\": 22\n+          \"link\": 27\n         }\n       ],\n       \"outputs\": [\n         {\n           \"name\": \"IMAGE\",\n           \"type\": \"IMAGE\",\n+          \"links\": [],\n+          \"shape\": 3,\n+          \"slot_index\": 0\n+        }\n+      ],\n+      \"title\": \"layer_diffuse_decode_rgba\",\n+      \"properties\": {\n+        \"Node name for S&R\": \"LayeredDiffusionDecodeRGBA\"\n+      },\n+      \"widgets_values\": [\n+        \"SD15\",\n+        16\n+      ]\n+    },\n+    {\n+      \"id\": 5,\n+      \"type\": \"EmptyLatentImage\",\n+      \"pos\": [\n+        575,\n+        565\n+      ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 106\n+      },\n+      \"flags\": {},\n+      \"order\": 1,\n+      \"mode\": 0,\n+      \"outputs\": [\n+        {\n+          \"name\": \"LATENT\",\n+          \"type\": \"LATENT\",\n           \"links\": [\n-            23\n+            34\n           ],\n-          \"shape\": 3,\n           \"slot_index\": 0\n         }\n       ],\n-      \"title\": \"repeat_image_batch\",\n+      \"title\": \"empty_latent_image\",\n       \"properties\": {\n-        \"Node name for S&R\": \"RepeatImageBatch\"\n+        \"Node name for S&R\": \"EmptyLatentImage\"\n       },\n       \"widgets_values\": [\n+        512,\n+        512,\n         1\n       ]\n     },\n@@ -311,14 +455,15 @@\n         \"1\": 98\n       },\n       \"flags\": {},\n-      \"order\": 0,\n+      \"order\": 2,\n       \"mode\": 0,\n       \"outputs\": [\n         {\n           \"name\": \"MODEL\",\n           \"type\": \"MODEL\",\n           \"links\": [\n-            1\n+            26,\n+            35\n           ],\n           \"slot_index\": 0\n         },\n@@ -348,66 +493,6 @@\n         \"Deliberate.ckpt\"\n       ]\n     },\n-    {\n-      \"id\": 3,\n-      \"type\": \"KSampler\",\n-      \"pos\": [\n-        1019,\n-        107\n-      ],\n-      \"size\": {\n-        \"0\": 315,\n-        \"1\": 262\n-      },\n-      \"flags\": {},\n-      \"order\": 8,\n-      \"mode\": 0,\n-      \"inputs\": [\n-        {\n-          \"name\": \"model\",\n-          \"type\": \"MODEL\",\n-          \"link\": 1\n-        },\n-        {\n-          \"name\": \"positive\",\n-          \"type\": \"CONDITIONING\",\n-          \"link\": 4\n-        },\n-        {\n-          \"name\": \"negative\",\n-          \"type\": \"CONDITIONING\",\n-          \"link\": 6\n-        },\n-        {\n-          \"name\": \"latent_image\",\n-          \"type\": \"LATENT\",\n-          \"link\": 18\n-        }\n-      ],\n-      \"outputs\": [\n-        {\n-          \"name\": \"LATENT\",\n-          \"type\": \"LATENT\",\n-          \"links\": [\n-            20\n-          ],\n-          \"slot_index\": 0\n-        }\n-      ],\n-      \"title\": \"sampler\",\n-      \"properties\": {\n-        \"Node name for S&R\": \"KSampler\"\n-      },\n-      \"widgets_values\": [\n-        62706718437716,\n-        \"randomize\",\n-        20,\n-        8,\n-        \"euler\",\n-        \"normal\",\n-        1\n-      ]\n-    },\n     {\n       \"id\": 14,\n       \"type\": \"VAEDecode\",\n@@ -420,7 +505,7 @@\n         \"1\": 46\n       },\n       \"flags\": {},\n-      \"order\": 9,\n+      \"order\": 10,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -439,7 +524,8 @@\n           \"name\": \"IMAGE\",\n           \"type\": \"IMAGE\",\n           \"links\": [\n-            21\n+            27,\n+            36\n           ],\n           \"slot_index\": 0\n         }\n@@ -451,14 +537,6 @@\n     }\n   ],\n   \"links\": [\n-    [\n-      1,\n-      4,\n-      0,\n-      3,\n-      0,\n-      \"MODEL\"\n-    ],\n     [\n       4,\n       6,\n@@ -507,14 +585,6 @@\n       1,\n       \"VAE\"\n     ],\n-    [\n-      18,\n-      5,\n-      0,\n-      3,\n-      3,\n-      \"LATENT\"\n-    ],\n     [\n       19,\n       4,\n@@ -531,14 +601,6 @@\n       0,\n       \"LATENT\"\n     ],\n-    [\n-      21,\n-      14,\n-      0,\n-      9,\n-      0,\n-      \"IMAGE\"\n-    ],\n     [\n       22,\n       11,\n@@ -554,10 +616,66 @@\n       12,\n       0,\n       \"IMAGE\"\n+    ],\n+    [\n+      24,\n+      3,\n+      0,\n+      17,\n+      0,\n+      \"LATENT\"\n+    ],\n+    [\n+      26,\n+      4,\n+      0,\n+      16,\n+      0,\n+      \"MODEL\"\n+    ],\n+    [\n+      27,\n+      14,\n+      0,\n+      17,\n+      1,\n+      \"IMAGE\"\n+    ],\n+    [\n+      34,\n+      5,\n+      0,\n+      3,\n+      3,\n+      \"LATENT\"\n+    ],\n+    [\n+      35,\n+      4,\n+      0,\n+      3,\n+      0,\n+      \"MODEL\"\n+    ],\n+    [\n+      36,\n+      14,\n+      0,\n+      9,\n+      0,\n+      \"IMAGE\"\n     ]\n   ],\n   \"groups\": [],\n   \"config\": {},\n-  \"extra\": {},\n+  \"extra\": {\n+    \"ds\": {\n+      \"scale\": 0.8264462809917354,\n+      \"offset\": [\n+        -271.8083588837497,\n+        68.64949882587337\n+      ]\n+    }\n+  },\n   \"version\": 0.4\n }\ndiff --git a/hordelib/pipeline_designs/pipeline_stable_diffusion_hires_fix.json b/hordelib/pipeline_designs/pipeline_stable_diffusion_hires_fix.json\nindex df6843b0..de9a550a 100644\n--- a/hordelib/pipeline_designs/pipeline_stable_diffusion_hires_fix.json\n+++ b/hordelib/pipeline_designs/pipeline_stable_diffusion_hires_fix.json\n@@ -1,6 +1,6 @@\n {\n-  \"last_node_id\": 22,\n-  \"last_link_id\": 36,\n+  \"last_node_id\": 24,\n+  \"last_link_id\": 45,\n   \"nodes\": [\n     {\n       \"id\": 12,\n@@ -14,13 +14,13 @@\n         \"1\": 468.13226318359375\n       },\n       \"flags\": {},\n-      \"order\": 12,\n+      \"order\": 14,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n           \"name\": \"images\",\n           \"type\": \"IMAGE\",\n-          \"link\": 34\n+          \"link\": 43\n         }\n       ],\n       \"title\": \"output_image\",\n@@ -41,7 +41,7 @@\n         \"1\": 58\n       },\n       \"flags\": {},\n-      \"order\": 3,\n+      \"order\": 5,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -82,7 +82,7 @@\n         \"1\": 180.6060791015625\n       },\n       \"flags\": {},\n-      \"order\": 6,\n+      \"order\": 8,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -122,7 +122,7 @@\n         \"1\": 164.31304931640625\n       },\n       \"flags\": {},\n-      \"order\": 5,\n+      \"order\": 7,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -162,7 +162,7 @@\n         \"1\": 130\n       },\n       \"flags\": {},\n-      \"order\": 9,\n+      \"order\": 10,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -237,13 +237,13 @@\n         \"1\": 262\n       },\n       \"flags\": {},\n-      \"order\": 8,\n+      \"order\": 9,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n           \"name\": \"model\",\n           \"type\": \"MODEL\",\n-          \"link\": 18\n+          \"link\": 44\n         },\n         {\n           \"name\": \"positive\",\n@@ -276,7 +276,7 @@\n         \"Node name for S&R\": \"KSampler\"\n       },\n       \"widgets_values\": [\n-        458841867575267,\n+        709052412707182,\n         \"randomize\",\n         12,\n         8,\n@@ -297,7 +297,7 @@\n         \"1\": 46\n       },\n       \"flags\": {},\n-      \"order\": 7,\n+      \"order\": 6,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -325,53 +325,81 @@\n       }\n     },\n     {\n-      \"id\": 16,\n-      \"type\": \"CheckpointLoaderSimple\",\n+      \"id\": 18,\n+      \"type\": \"LoadImage\",\n       \"pos\": [\n-        -420,\n-        334\n+        261,\n+        934\n       ],\n       \"size\": {\n         \"0\": 315,\n-        \"1\": 98\n+        \"1\": 314\n       },\n       \"flags\": {},\n       \"order\": 1,\n       \"mode\": 0,\n       \"outputs\": [\n         {\n-          \"name\": \"MODEL\",\n-          \"type\": \"MODEL\",\n+          \"name\": \"IMAGE\",\n+          \"type\": \"IMAGE\",\n           \"links\": [\n-            18,\n-            23\n+            35\n           ],\n           \"slot_index\": 0\n         },\n         {\n-          \"name\": \"CLIP\",\n-          \"type\": \"CLIP\",\n-          \"links\": [\n-            24\n-          ],\n-          \"slot_index\": 1\n-        },\n+          \"name\": \"MASK\",\n+          \"type\": \"MASK\",\n+          \"links\": null\n+        }\n+      ],\n+      \"title\": \"image_loader\",\n+      \"properties\": {\n+        \"Node name for S&R\": \"LoadImage\"\n+      },\n+      \"widgets_values\": [\n+        \"example.png\",\n+        \"image\"\n+      ]\n+    },\n+    {\n+      \"id\": 22,\n+      \"type\": \"RepeatImageBatch\",\n+      \"pos\": [\n+        624,\n+        934\n+      ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 58\n+      },\n+      \"flags\": {},\n+      \"order\": 3,\n+      \"mode\": 0,\n+      \"inputs\": [\n         {\n-          \"name\": \"VAE\",\n-          \"type\": \"VAE\",\n+          \"name\": \"image\",\n+          \"type\": \"IMAGE\",\n+          \"link\": 35\n+        }\n+      ],\n+      \"outputs\": [\n+        {\n+          \"name\": \"IMAGE\",\n+          \"type\": \"IMAGE\",\n           \"links\": [\n-            28,\n-            32\n+            36\n           ],\n-          \"slot_index\": 2\n+          \"shape\": 3,\n+          \"slot_index\": 0\n         }\n       ],\n-      \"title\": \"model_loader\",\n+      \"title\": \"repeat_image_batch\",\n       \"properties\": {\n-        \"Node name for S&R\": \"CheckpointLoaderSimple\"\n+        \"Node name for S&R\": \"RepeatImageBatch\"\n       },\n       \"widgets_values\": [\n-        \"Deliberate.ckpt\"\n+        1\n       ]\n     },\n     {\n@@ -386,13 +414,13 @@\n         \"1\": 262\n       },\n       \"flags\": {},\n-      \"order\": 10,\n+      \"order\": 11,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n           \"name\": \"model\",\n           \"type\": \"MODEL\",\n-          \"link\": 23,\n+          \"link\": 45,\n           \"slot_index\": 0\n         },\n         {\n@@ -419,7 +447,8 @@\n           \"name\": \"LATENT\",\n           \"type\": \"LATENT\",\n           \"links\": [\n-            33\n+            33,\n+            42\n           ],\n           \"slot_index\": 0\n         }\n@@ -429,7 +458,7 @@\n         \"Node name for S&R\": \"KSampler\"\n       },\n       \"widgets_values\": [\n-        65484213431324,\n+        932988885298999,\n         \"randomize\",\n         14,\n         8,\n@@ -438,6 +467,50 @@\n         0.5\n       ]\n     },\n+    {\n+      \"id\": 24,\n+      \"type\": \"LayeredDiffusionDecodeRGBA\",\n+      \"pos\": [\n+        2135,\n+        -102\n+      ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 102\n+      },\n+      \"flags\": {},\n+      \"order\": 13,\n+      \"mode\": 0,\n+      \"inputs\": [\n+        {\n+          \"name\": \"samples\",\n+          \"type\": \"LATENT\",\n+          \"link\": 42\n+        },\n+        {\n+          \"name\": \"images\",\n+          \"type\": \"IMAGE\",\n+          \"link\": 40\n+        }\n+      ],\n+      \"outputs\": [\n+        {\n+          \"name\": \"IMAGE\",\n+          \"type\": \"IMAGE\",\n+          \"links\": [],\n+          \"shape\": 3,\n+          \"slot_index\": 0\n+        }\n+      ],\n+      \"title\": \"layer_diffuse_decode_rgba\",\n+      \"properties\": {\n+        \"Node name for S&R\": \"LayeredDiffusionDecodeRGBA\"\n+      },\n+      \"widgets_values\": [\n+        \"SD15\",\n+        16\n+      ]\n+    },\n     {\n       \"id\": 21,\n       \"type\": \"VAEDecode\",\n@@ -450,7 +523,7 @@\n         \"1\": 46\n       },\n       \"flags\": {},\n-      \"order\": 11,\n+      \"order\": 12,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n@@ -469,7 +542,8 @@\n           \"name\": \"IMAGE\",\n           \"type\": \"IMAGE\",\n           \"links\": [\n-            34\n+            40,\n+            43\n           ],\n           \"slot_index\": 0\n         }\n@@ -480,80 +554,92 @@\n       }\n     },\n     {\n-      \"id\": 18,\n-      \"type\": \"LoadImage\",\n+      \"id\": 16,\n+      \"type\": \"CheckpointLoaderSimple\",\n       \"pos\": [\n-        261,\n-        934\n-      ],\n-      \"size\": [\n-        315,\n-        314\n+        -420,\n+        334\n       ],\n+      \"size\": {\n+        \"0\": 315,\n+        \"1\": 98\n+      },\n       \"flags\": {},\n       \"order\": 2,\n       \"mode\": 0,\n       \"outputs\": [\n         {\n-          \"name\": \"IMAGE\",\n-          \"type\": \"IMAGE\",\n+          \"name\": \"MODEL\",\n+          \"type\": \"MODEL\",\n           \"links\": [\n-            35\n+            37,\n+            44,\n+            45\n           ],\n           \"slot_index\": 0\n         },\n         {\n-          \"name\": \"MASK\",\n-          \"type\": \"MASK\",\n-          \"links\": null\n+          \"name\": \"CLIP\",\n+          \"type\": \"CLIP\",\n+          \"links\": [\n+            24\n+          ],\n+          \"slot_index\": 1\n+        },\n+        {\n+          \"name\": \"VAE\",\n+          \"type\": \"VAE\",\n+          \"links\": [\n+            28,\n+            32\n+          ],\n+          \"slot_index\": 2\n         }\n       ],\n-      \"title\": \"image_loader\",\n+      \"title\": \"model_loader\",\n       \"properties\": {\n-        \"Node name for S&R\": \"LoadImage\"\n+        \"Node name for S&R\": \"CheckpointLoaderSimple\"\n       },\n       \"widgets_values\": [\n-        \"example.png\",\n-        \"image\"\n+        \"Deliberate.ckpt\"\n       ]\n     },\n     {\n-      \"id\": 22,\n-      \"type\": \"RepeatImageBatch\",\n+      \"id\": 23,\n+      \"type\": \"LayeredDiffusionApply\",\n       \"pos\": [\n-        624,\n-        934\n+        443,\n+        -13\n       ],\n       \"size\": {\n         \"0\": 315,\n-        \"1\": 58\n+        \"1\": 82\n       },\n       \"flags\": {},\n       \"order\": 4,\n       \"mode\": 0,\n       \"inputs\": [\n         {\n-          \"name\": \"image\",\n-          \"type\": \"IMAGE\",\n-          \"link\": 35\n+          \"name\": \"model\",\n+          \"type\": \"MODEL\",\n+          \"link\": 37\n         }\n       ],\n       \"outputs\": [\n         {\n-          \"name\": \"IMAGE\",\n-          \"type\": \"IMAGE\",\n-          \"links\": [\n-            36\n-          ],\n+          \"name\": \"MODEL\",\n+          \"type\": \"MODEL\",\n+          \"links\": [],\n           \"shape\": 3,\n           \"slot_index\": 0\n         }\n       ],\n-      \"title\": \"repeat_image_batch\",\n+      \"title\": \"layer_diffuse_apply\",\n       \"properties\": {\n-        \"Node name for S&R\": \"RepeatImageBatch\"\n+        \"Node name for S&R\": \"LayeredDiffusionApply\"\n       },\n       \"widgets_values\": [\n+        \"SD15, Attention Injection, attn_sharing\",\n         1\n       ]\n     }\n@@ -607,22 +693,6 @@\n       3,\n       \"LATENT\"\n     ],\n-    [\n-      18,\n-      16,\n-      0,\n-      3,\n-      0,\n-      \"MODEL\"\n-    ],\n-    [\n-      23,\n-      16,\n-      0,\n-      11,\n-      0,\n-      \"MODEL\"\n-    ],\n     [\n       24,\n       16,\n@@ -679,14 +749,6 @@\n       0,\n       \"LATENT\"\n     ],\n-    [\n-      34,\n-      21,\n-      0,\n-      12,\n-      0,\n-      \"IMAGE\"\n-    ],\n     [\n       35,\n       18,\n@@ -702,6 +764,54 @@\n       19,\n       0,\n       \"IMAGE\"\n+    ],\n+    [\n+      37,\n+      16,\n+      0,\n+      23,\n+      0,\n+      \"MODEL\"\n+    ],\n+    [\n+      40,\n+      21,\n+      0,\n+      24,\n+      1,\n+      \"IMAGE\"\n+    ],\n+    [\n+      42,\n+      11,\n+      0,\n+      24,\n+      0,\n+      \"LATENT\"\n+    ],\n+    [\n+      43,\n+      21,\n+      0,\n+      12,\n+      0,\n+      \"IMAGE\"\n+    ],\n+    [\n+      44,\n+      16,\n+      0,\n+      3,\n+      0,\n+      \"MODEL\"\n+    ],\n+    [\n+      45,\n+      16,\n+      0,\n+      11,\n+      0,\n+      \"MODEL\"\n     ]\n   ],\n   \"groups\": [\n@@ -751,6 +861,14 @@\n     }\n   ],\n   \"config\": {},\n-  \"extra\": {},\n+  \"extra\": {\n+    \"ds\": {\n+      \"scale\": 0.683013455365071,\n+      \"offset\": [\n+        -1143.7145712697502,\n+        509.85654565587373\n+      ]\n+    }\n+  },\n   \"version\": 0.4\n }\ndiff --git a/hordelib/pipelines/pipeline_stable_diffusion_hires_fix.json b/hordelib/pipelines/pipeline_stable_diffusion_hires_fix.json\nindex fed95de1..8d0e1c04 100644\n--- a/hordelib/pipelines/pipeline_stable_diffusion_hires_fix.json\n+++ b/hordelib/pipelines/pipeline_stable_diffusion_hires_fix.json\n@@ -1,7 +1,7 @@\n {\n   \"3\": {\n     \"inputs\": {\n-      \"seed\": 458841867575267,\n+      \"seed\": 709052412707182,\n       \"steps\": 12,\n       \"cfg\": 8,\n       \"sampler_name\": \"dpmpp_sde\",\n@@ -84,7 +84,7 @@\n   },\n   \"11\": {\n     \"inputs\": {\n-      \"seed\": 65484213431324,\n+      \"seed\": 932988885298999,\n       \"steps\": 14,\n       \"cfg\": 8,\n       \"sampler_name\": \"dpmpp_2m\",\n@@ -201,5 +201,37 @@\n     \"_meta\": {\n       \"title\": \"repeat_image_batch\"\n     }\n+  },\n+  \"23\": {\n+    \"inputs\": {\n+      \"config\": \"SD15, Attention Injection, attn_sharing\",\n+      \"weight\": 1,\n+      \"model\": [\n+        \"16\",\n+        0\n+      ]\n+    },\n+    \"class_type\": \"LayeredDiffusionApply\",\n+    \"_meta\": {\n+      \"title\": \"layer_diffuse_apply\"\n+    }\n+  },\n+  \"24\": {\n+    \"inputs\": {\n+      \"sd_version\": \"SD15\",\n+      \"sub_batch_size\": 16,\n+      \"samples\": [\n+        \"11\",\n+        0\n+      ],\n+      \"images\": [\n+        \"21\",\n+        0\n+      ]\n+    },\n+    \"class_type\": \"LayeredDiffusionDecodeRGBA\",\n+    \"_meta\": {\n+      \"title\": \"layer_diffuse_decode_rgba\"\n+    }\n   }\n }\ndiff --git a/hordelib/utils/ioredirect.py b/hordelib/utils/ioredirect.py\nindex a51dec0f..5243245c 100644\n--- a/hordelib/utils/ioredirect.py\n+++ b/hordelib/utils/ioredirect.py\n@@ -106,6 +106,7 @@ def write(self, message: str):\n \n             # Remove any double spaces\n             message = self.pattern_double_space.sub(\" \", message)\n+            from hordelib.comfy_horde import log_free_ram\n \n             if (\n                 self.slow_message_count < 5\n@@ -115,11 +116,14 @@ def write(self, message: str):\n                 self.slow_message_count += 1\n                 if self.slow_message_count == 5:\n                     logger.warning(\"Suppressing further slow job warnings. Please investigate.\")\n+\n+                    log_free_ram()\n                 else:\n                     rate_unit = \"iterations per second\" if is_iterations_per_second else \"*seconds per iterations*\"\n                     logger.warning(f\"Job Slowdown: Job is running at {iteration_rate} {rate_unit}.\")\n \n             if found_current_step == 0:\n+                log_free_ram()\n                 logger.info(\"Job will show progress for the first three steps, and then every 10 steps.\")\n \n             # Log the first 3 steps, then every 10 steps, then the last step\ndiff --git a/images_expected/image_to_image_hires_fix_large.png b/images_expected/image_to_image_hires_fix_large.png\nindex 48369d61..8e69efcd 100644\nBinary files a/images_expected/image_to_image_hires_fix_large.png and b/images_expected/image_to_image_hires_fix_large.png differ\ndiff --git a/images_expected/img2img_hires_fix_n_iter_0.png b/images_expected/img2img_hires_fix_n_iter_0.png\nnew file mode 100644\nindex 00000000..eacefbcf\nBinary files /dev/null and b/images_expected/img2img_hires_fix_n_iter_0.png differ\ndiff --git a/images_expected/img2img_hires_fix_n_iter_1.png b/images_expected/img2img_hires_fix_n_iter_1.png\nnew file mode 100644\nindex 00000000..a922f65a\nBinary files /dev/null and b/images_expected/img2img_hires_fix_n_iter_1.png differ\ndiff --git a/images_expected/layer_diffusion_hires_fix.png b/images_expected/layer_diffusion_hires_fix.png\nnew file mode 100644\nindex 00000000..aecb9ae5\nBinary files /dev/null and b/images_expected/layer_diffusion_hires_fix.png differ\ndiff --git a/images_expected/qr_code.png b/images_expected/qr_code.png\nindex 5ca125ab..23c98035 100644\nBinary files a/images_expected/qr_code.png and b/images_expected/qr_code.png differ\ndiff --git a/images_expected/qr_code_out_of_bounds.png b/images_expected/qr_code_out_of_bounds.png\nindex 2d35251d..5e8b9e85 100644\nBinary files a/images_expected/qr_code_out_of_bounds.png and b/images_expected/qr_code_out_of_bounds.png differ\ndiff --git a/images_expected/sdxl_text_to_image_hires_fix.png b/images_expected/sdxl_text_to_image_hires_fix.png\nnew file mode 100644\nindex 00000000..7c095e98\nBinary files /dev/null and b/images_expected/sdxl_text_to_image_hires_fix.png differ\ndiff --git a/images_expected/text_to_image_hires_fix_n_iter_1.png b/images_expected/text_to_image_hires_fix_n_iter_1.png\nindex fce6993b..9f78f932 100644\nBinary files a/images_expected/text_to_image_hires_fix_n_iter_1.png and b/images_expected/text_to_image_hires_fix_n_iter_1.png differ\ndiff --git a/requirements.txt b/requirements.txt\nindex 6874f189..ab2ca5ec 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,10 +1,10 @@\n # Add this in for tox, comment out for build\n --extra-index-url https://download.pytorch.org/whl/cu121\n-horde_sdk>=0.12.0\n+horde_sdk>=0.13.0\n horde_model_reference>=0.7.0\n pydantic\n numpy==1.26.4\n-torch>=2.1.0\n+torch>=2.3.1\n # xformers>=0.0.19\n torchvision\n # torchaudio\n@@ -13,7 +13,7 @@ torchsde\n einops\n open-clip-torch\n transformers>=4.25.1\n-safetensors>=0.3.0\n+safetensors>=0.4.2\n pytorch_lightning\n pynvml\n aiohttp\n@@ -48,3 +48,5 @@ fuzzywuzzy\n strenum\n kornia\n qrcode\n+spandrel\n+spandrel_extra_arches\n", "instance_id": "Haidra-Org__hordelib-280", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a hires fix to layer diffusion and optimizing the step count for the second pass to reduce processing time. It provides context about the diminishing returns of additional steps in the second pass and mentions that the change does not impact the visual output significantly (as verified by cosine similarity checks). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the input and output formats for the hires fix feature, nor does it specify constraints or edge cases that might arise from reducing step counts (e.g., under what conditions the reduced steps might fail to produce acceptable results). Additionally, terms like \"layer diffusion\" and \"stable cascade\" are used without detailed explanation, which could be unclear to someone unfamiliar with the domain or codebase. Overall, while the intent and high-level impact are understandable, some critical details are omitted, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files and modules (e.g., `comfy_horde.py`, `horde.py`, pipeline JSON files, and custom node implementations). These changes involve not just adding new logic but also modifying existing workflows, such as adjusting step calculations for hires fix and integrating layer diffusion with hires fix capabilities. Second, the problem requires understanding several technical concepts, including image generation pipelines (Stable Diffusion, hires fix), sampling techniques (e.g., denoising strength, step calculations), and specific libraries or frameworks like ComfyUI. The implementation of dynamic step calculation based on denoising strength (`_calc_upscale_sampler_steps`) and handling different model baselines (SD1 vs. SDXL) adds to the complexity. Third, the changes impact the system's architecture to some extent, as they modify pipeline designs and parameter mappings, which could affect how different components interact during image generation. Finally, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest considerations for different model types, image resolutions, and performance optimization, which require careful handling to avoid unintended side effects. A score of 0.65 reflects the need for a deep understanding of the codebase and domain-specific knowledge, along with the complexity of coordinating changes across multiple parts of the system, though it does not reach the \"Very Hard\" range as it does not involve system-level redesign or extremely intricate algorithms.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[python/r/c++] Revisit `shape` for component arrays\n## PRs\r\n\r\n**Merged PRs:**\r\n\r\n<details>\r\n\r\n* #2909 `kerl/schevo-timestamp-methodize`\r\n* #2913 `kerl/name-neaten`\r\n* #2908 `kerl/ut-soma-exc-simplify`\r\n* #2910 `kerl/test-common-parameterize`\r\n* #2918 `kerl/cpp-test-deadstrip`\r\n* #2919 `kerl/minor-unit-test-helper-mod`\r\n* #2936 `kerl/cpp-ut-helper-neaten`\r\n* #2938 `kerl/more-cur-dom-parameterize`\r\n* #2915 `kerl/cpp-strict-int64-shape`\r\n* #2911 `kerl/arrow-util-current-domain-optional`\r\n* #2939 `kerl/step-two-temp`\r\n* #2947 `kerl/cpp-ndarray-resize-testing`\r\n* #2945 `kerl/dataframe-test-fixture`\r\n* #2944 `kerl/cpp-variant-indexed-dataframes`\r\n* #2916 `kerl/sdf-shape`\r\n* #2917 `kerl/cpp-resizes`\r\n* #2948 `kerl/upgrade-shape-int64`\r\n* #2951 `kerl/sdf-test-accessors`\r\n* #2953 `kerl/py-r-accessor-plumbing`\r\n* #2957 `kerl/sdf-domain-accessors`\r\n* #2960 `kerl/dense-link`\r\n* #2963 `kerl/nightly-fix`\r\n* #2970 `kerl/dense-writeable-after-create`\r\n* #2968 `kerl/minor-trim`\r\n* #2969 `kerl/more-py-domain-name-neaten`\r\n* #2972 `kerl/libtiledbsoma-env-logging-level`\r\n* #2962 `kerl/py-r-creation-paths`\r\n* #2950 `kerl/py-r-test-2`\r\n* #2994 `kerl/nanoarrow-helpers`\r\n* #3011 `kerl/polydom3`\r\n* #3017 `kerl/polydom5`\r\n* #3018 `kerl/polydom6`\r\n* #2990 `kerl/variant-nnz-bug`\r\n* #3019 `kerl/index-swap`\r\n* #3020 `kerl/ut-max-shape`\r\n* #3012 `kerl/polydom4`\r\n* #3025 `kerl/fix-3020-merge`\r\n* #3026 `kerl/one-more-rename`\r\n* #3028 `kerl/ff-not`\r\n* #3029 `kerl/ut-vg`\r\n* #3030 `kerl/table-utils-memory`\r\n* #3067 `kerl/improve-sdf-test-field-names`\r\n* #3068 `kerl/ut-generate`\r\n* #3069 `kerl/cpp-sdf-domain-at-create`\r\n* #3027 `kerl/hll-domainish`\r\n* #3088 `kerl/max-domain-int64`\r\n* #3090 `kerl/maybe-resize-soma-joinid-cpp-tweak`\r\n* #3032 `kerl/sdf-domain-at-create` -- fixes #2967\r\n* #3091 `kerl/maybe-resize-soma-joinid-py-r`\r\n* #3095 `kerl/cpp-exp-resize-prep`\r\n* #3089 `kerl/r-dataframe-shapeable`\r\n* https://github.com/single-cell-data/SOMA/pull/233\r\n* #3125 `kerl/cpp-ut-name-shortens`\r\n* #3127 `kerl/helper-rename`\r\n* #3130 `kerl/cpp-can-resizers-names`\r\n* #3132 `kerl/cpp-dataframe-sizing-helpers`\r\n* #3139 `kerl/cpp-dataframe-upgrade-test`\r\n* #3140 `kerl/py-resizer-connects`\r\n* #3151 `kerl/py-can-upgrade-shape`\r\n* #3152 `kerl/registration-shape-acceessors`\r\n* #3156 `kerl/py-exp-shaping`\r\n* #3157 `kerl/py-exp-shaping2`\r\n* #3148 `kerl/py-exp-resize`\r\n* #3191 `kerl/py-domain-at-create-ut-1`\r\n* #3190 `kerl/py-domain-at-create-ut-2`\r\n* #3192 `kerl/py-domain-at-create-ut-3`\r\n* #3193 `kerl/py-domain-at-create-ut-4`\r\n* #3194 `kerl/py-domain-at-create-ut-5`\r\n* #3203 `kerl/min-size-2`\r\n* #3208 `kerl/r-min-sizing`\r\n* #3211 `kerl/cpp-ugr-dom`\r\n* #3232 `kerl/ff-interop`\r\n* #3230 `kerl/ffon`\r\n* #3234 `kerl/docstring-prune`\r\n* #3236 `kerl/prefixing`\r\n* #3241 `kerl/fix-bad-merge`\r\n* #3235 `kerl/py-r-ugr-dom`\r\n* #3237 `kerl/py-r-ugr-dom-2`\r\n* #3238 `kerl/py-r-ugr-dom-3`\r\n* #3253 `kerl/set-coords-rename`\r\n* #3261 `kerl/pybind11-nda-sizing`\r\n* #3265 `kerl/dense-227-a`\r\n* #3263 `kerl/dense-range-trim`\r\n* #3268 `kerl/dim-explosion`\r\n* #3269 `kerl/python-227-dense-ned-read`\r\n* #3270 `kerl/r-227-dense-fixes`\r\n* #3280 `kerl/r-dense-227-more`\r\n* #3286 `kerl/more-fn4m`\r\n* #3283 `kerl/readthedocs-pre-1.15`\r\n* #3288 `kerl/dense-ugrsh`\r\n* #3289 `kerl/notebook-shape-upgrade`\r\n* #3285 `kerl/new-shape-doc-updates`\r\n* #3295 `kerl/notebook-data-refresh`\r\n* #3290 `kerl/notebook-new-shape-refresh`\r\n* #3301 `kerl/ffena`\r\n* #3303 `kerl/r-data-refresh`\r\n* #3300 `kerl/sdf-sjid-lower-zero`\r\n* #3296 `kerl/dense-example-data-refresh`\r\n* #3294 `kerl/new-shape-notebook-and-vignette`\r\n* #3308 `kerl/upgrade-experiment-resources`\r\n* #3309 `kerl/fix-notebook-merge`\r\n* #3321 `kerl/more-use-shape`\r\n* #3358 `kerl/revert-3300`\r\n* #3368 `kerl/227a`\r\n* #3369 `kerl/ucd1`\r\n* #3370 `kerl/ucd2`\r\n* #3371 `kerl/ucd3`\r\n* #3372 `kerl/ucd4`\r\n* #3396 `kerl/domain-at-create-docstrings`\r\n* #3302 `kerl/new-shape-vignette`\r\n* https://github.com/single-cell-data/SOMA/pull/250\r\n* #3399 `kerl/new-shape-more-docstrings`\r\n* #3400 `kerl/check-only-r`\r\n* https://github.com/TileDB-Inc/somacore-feedstock/pull/27\r\n\r\n</details>\r\n\r\n**Closed/abandoned PRs:**\r\n\r\n<details>\r\n\r\n* #2785 -- This was only dogfooding for the core 2.25 release -- not to be merged\r\n* #2952 `kerl/feature-flag-temp` -- folded into 2962\r\n* #2995 `kerl/polydom`\r\n* #2964 `kerl/tiledbsoma-io-test`\r\n* #3189 `kerl/min-size`\r\n* #3220 `kerl/cpp-ugr-dom-2`\r\n* #3244 `kerl/dense-227-fixes`\r\n* #3276 `kerl/dataframe-shape`\r\n\r\n</details>\r\n\r\n**Issues which are related but non-blocking:**\r\n\r\n* https://github.com/single-cell-data/SOMA/pull/216\r\n* https://github.com/TileDB-Inc/TileDB/pull/5303\r\n* #2966\r\n* #3081\r\n* Note: R append mode does not exist yet -- see #1630 -- so an experiment-level resizer is not a priority in R\r\n* #3271 \r\n* #3272\r\n* #3273\r\n\r\n**See also:** [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Problem to be solved\r\n\r\nUsers want to know the `shape` of an array, in the SciPy sense:\r\n\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`.\r\n  * Some users need the ability to grow their datasets later, using either `tiledbsoma.io`'s append mode, or subsequent writes using the `tiledbsoma` API.\r\n  * Note that the cellxgene census doesn't need this: eact week's published census has fixed shape, and any updates will happen in new storage, on a new week.\r\n\r\nUsing TileDB-SOMA up until the present:\r\n\r\n* The TIleDB `domain` is immutable after array creation\r\n  * This does bounds-checking for reads and writes, which is good\r\n  * To leverage this to function as a `shape`, users would need to set the `domain` at array-creation time. However, users lose the ability to grow their datasets later.\r\n* There is a `non_empty_domain` accessor\r\n  * This only indicates min/max coordinates _at which data exists_. Consider an `X` array for 100 cells and 200 genes. If non-zero expression counts exist only for cell join IDs 2-17, then the `non_empty_domain` will indicate `(2,17)` along `soma_dim_0`.\r\n  * Consider an `obms[\"X_pca\"]` within the same experiment. This may be 100 cells by 50 PCA components: we need a placd to store the number 50.\r\n  * Therefore users cannot leverage this to function as a `shape` accessor.\r\n* We have offered a `used_shape` accessor since TileDB-SOMA 1.5.\r\n  * This functions as a `shape` accessor, in the SciPy sense, but it is not multi-writer safe.\r\n\r\nNew feature for TileDB-SOMA 1.15:\r\n\r\n* Arrays will have a  `shape`\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`\r\n* The `used_shape` accessor will be deprecated in TileDB-SOMA 1.13, and slated for removal in TileDB-SOMA 1.14.\r\n\r\nCompatiblity:\r\n\r\nThis will now require users to do an explicit `resize` before appending/growing TileDB-SOMA Experiments. Guidance in the form of example notebooks will be provided.\r\n\r\n##  Tracking\r\n\r\nSee also: [[sc-41074]](https://app.shortcut.com/tiledb-inc/story/41074) and [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Scheduling\r\n\r\nSupport arrives in TileDB Core 2.25. Deprecations for TileDB-SOMA will be released with 1.13. Full support within TileDB-SOMA will be release in 1.14.\r\n\r\n## Details\r\n\r\nSOMA API mods as we've discussed in a Google doc are as follows.\r\n\r\n### `SOMADataFrame`\r\n\r\n* `create`: Retain the `domain` argument\r\n  * Issue:\r\n    * Core has a `(lo, hi)` tuple per dim, e.g. `(0,99)` or `(10,19)`\r\n    * SOMA has count per dim, with 0 implicit: e.g. 100 or 20\r\n    * For `SparseNDArray` and `DenseNDArray` core can have `(lo, hi)` and SOMA can have `count`\r\n    * For `DataFrame` there can be multiple dims --- default is a single `soma_joinid`\r\n    * That could be treated either in `(lo, hi)` fashion or `count` fashion\r\n    * However additional dims (e.g. `cell_type`) can be on any type, including strings, floats, etc. where there is no implicit lo=0\r\n    * Therefore we need to keep the current SOMA API wherein `DataFrame` takes a `domain` argument (in `(lo, hi)` fashion) and not a `shape` argument (in `count` fashion)\r\n\r\n### `SparseNDArray and DenseNDArray`\r\n\r\n* `create`\r\n  * Have an optional shape argument which is of type `Tuple[Int,...]` where each element is the cell count of the corresponding dimension\r\n    * If unsupplied, or if supplied but None in any slot: use the minimum 0 in each slot \u2013 nothing larger makes sense since we will not support downsize\r\n  * User guidance should make clear that it will not be possible to create an \u2018old\u2019 style array with the \u2018new style\u2019 API. (See also the upgrade logic below.)\r\n\r\n### All three of `SOMADataFrame`, `SparseNDArray`, `DenseNDArray`\r\n\r\n* `write`\r\n  * For new arrays, created with the new shape feature:\r\n    * Core will bounds-check that coordinates provided at `write` time are within the current shape\r\n    * Core will raise `tiledb.cc.TileDBError` to TileDB-SOMA, which will catch and raise `IndexError`, and R-standard behavior on the R side\r\n  * For old arrays created before this feature:\r\n    * Core will not bounds-check that coordinates provided at write time are within the current shape\r\n* Existing `used_shape` accessor\r\n  * TileDB-SOMA will deprecate this over a release cycle.\r\n  * For new arrays: raise `NotImplementedError`\r\n  * For old arrays: return what\u2019s currently returned, with a deprecation warning.\r\n  * Mechanism for determining old vs. new: `array.schema.version` (the core storage version).\r\n* Existing `shape` accessor\r\n  * For new arrays:\r\n    * Have this return the new shape as proposed by core, no longer returning the TileDB domain.\r\n  * For old arrays created before this feature:\r\n    * Return the TileDB domain as now.\r\n* Existing `non_empty_domain` accessor\r\n  * Same behavior for old and new arrays (unaffected by this proposal).\r\n  * Keep this accessor supported, but, with user notes that it\u2019s generally non-useful\r\n  * This should return None (or R equivalent) when there is a schema but no data have been written.\r\n* New `maxshape` accessor\r\n  * Maps the core-level `(lo, hi)` accessor for domain to count-style accessor hi+1. E.g. if the core domain is either `(0,99)` or `(50,99)` then TileDB-SOMA `maxshape` will say 100.\r\n  * Same behavior for old and new arrays.\r\n  * Let users query for what the TileDB domain is, with user notes that it\u2019s the maximum that users can reshape to.\r\n  * Issac suggests: maybe `domain` or `maxshape` (see h5py).\r\n* New `resize` mutator\r\n  * Note: `reshape` means something else in the community (numpy, zarr, h5py), e.g. a 5x20 (total 100 cells) being reinterpreted as 4x25 (still 100 cells). The standard name for changing cell-count is `resize`.\r\n  * For old arrays created before this feature: raise `NotImplementedError`.\r\n  * For new arrays:\r\n    * Will raise `ValueError` if the new shape is smaller on any dim than currently in storage\r\n    * Regardless of whether any data have been written whatsoever\r\n    * Will raise `ValueError` if the new shape exceeds the TileDB domain from create time, which will serve TileDB-SOMA in a role of \u201cmax possible shape the user can reshape to\u201d\r\n    * Otherwise, any calls to write from this point will bounds-check writes within this new shape\r\n    * **We don\u2019t expect resize to be multi-writer safe with regard to write ; user notes must be clear on this point**\r\n* New `tiledbsoma_upgrade_shape` method for SparseNDArray and DenseNDArray\r\n  * This will leverage `array.schema.version` to see if an upgrade is needed\r\n  * Leverage core support for storage-version updates\r\n  * This will take a shape argument as in `create`\r\n  * For arrays created with \u201cjust-right\u201d size: this will succeed\r\n  * For arrays created with \u201croom-for-growth\u201d / \u201ctwo billion-ish\u201d size: this will succeed\r\n  * If the user passes a shape which exceeds the current TileDB domain: this will fail\r\n* New `tiledbsoma_upgrade_domain method` for `DataFrame`\r\n  * Same as for `SparseNDArray`/`DenseNDArray` except it will take a domain at the SOMA-API level just as `DataFrame`'s create method\r\n\r\n### `tiledbsoma.io`\r\n\r\n* The user-facing API has no shape arguments and thus won\u2019t need changing.\r\n* Internally to `tiledbsoma.io`, we\u2019ll still ask the tiledbsoma API for the \u201cbig domain\u201d (2 billionish)\r\n* Append mode:\r\n  * Will need a new `resize` method at the `Experiment` level\r\n  * Users will need to:\r\n    * Register as now\r\n    * Call the experiment-level `resize`\r\n      * Could be `exp.resize(...)`, or (better) this could be `tiledbsoma.io.reshape_experiment`\r\n  * In either case: this method will take the new `obs` and `var` counts as inputs:\r\n    * `exp.obs.reshape` to new `obs` count\r\n    * `exp.ms[name].var.reshape` to new `var` count\r\n    * `exp.ms[name].X[name].reshape` to new `obs` count x `var` count\r\n    * `exp.ms[name].obsm[name].reshape` to new `obs` count x same width\r\n    * `exp.ms[name].obsp[name].reshape` to new `obs` count x `obs` count\r\n    * `exp.ms[name].varm[name].reshape` to new `var` count x same width\r\n    * `exp.ms[name].varp[name].reshape` to new `var` count x `var` count\r\n  * Do the individual append-mode writes as now\r\n\n[c++] Fix upgrade-shape for dataframes with non-standard dimensions\nAs detailed on #3416.\n", "patch": "diff --git a/libtiledbsoma/src/soma/soma_array.cc b/libtiledbsoma/src/soma/soma_array.cc\nindex e25958b460..337b1c86da 100644\n--- a/libtiledbsoma/src/soma/soma_array.cc\n+++ b/libtiledbsoma/src/soma/soma_array.cc\n@@ -1038,8 +1038,7 @@ void SOMAArray::_set_soma_joinid_shape_helper(\n                         tiledb::impl::type_to_str(dim.type())));\n                 }\n                 ndrect.set_range<int64_t>(dim_name, 0, newshape - 1);\n-                continue;\n-\n+            } else {\n                 switch (dim.type()) {\n                     case TILEDB_STRING_ASCII:\n                     case TILEDB_STRING_UTF8:\n", "instance_id": "single-cell-data__TileDB-SOMA-3416", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal of introducing a `shape` feature for TileDB-SOMA arrays, which allows bounds-checking for reads and writes, retains value regardless of data occupancy, and supports resizing. It provides detailed API modifications for different array types (`SOMADataFrame`, `SparseNDArray`, `DenseNDArray`) and outlines compatibility concerns, deprecation plans, and user guidance. The context of the problem, including the limitations of existing features like `domain`, `non_empty_domain`, and `used_shape`, is well-explained. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the specific issue mentioned in the code change (\"Fix upgrade-shape for dataframes with non-standard dimensions\") is only briefly referenced with a link to another issue (#3416), without a clear explanation of what \"non-standard dimensions\" entails or the exact nature of the fix. Additionally, while edge cases like multi-writer safety are mentioned, they are not deeply explored in terms of implementation challenges or specific constraints. Overall, the statement is valid and clear but lacks some finer details on the specific code change context and edge case handling.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range. Here's the reasoning based on the evaluated factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal, affecting a single file (`soma_array.cc`) and a small section of logic within the `_set_soma_joinid_shape_helper` function. It appears to be a conditional logic tweak for handling dimension ranges, likely related to the `soma_joinid` dimension during shape upgrades. However, the broader context of the problem statement suggests that the overall feature implementation spans multiple components (e.g., API changes for `create`, `write`, `resize`, accessors like `shape` and `maxshape`, and compatibility logic for old vs. new arrays). While the specific diff is small, understanding its role in the larger `shape` feature requires familiarity with the codebase's architecture, particularly how dimensions and shapes are managed across different array types.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding several concepts, including TileDB's core storage mechanisms (e.g., `domain`, `non_empty_domain`), SOMA's API design, bounds-checking logic, and version-based compatibility handling. The code change itself involves C++ specifics like type handling (`TILEDB_STRING_ASCII`, etc.) and dimension range setting. Additionally, the broader feature touches on multi-language support (Python, R, C++), requiring knowledge of language bindings (e.g., Pybind11) and cross-language consistency. While these concepts are not extraordinarily complex individually, their combination and the need to ensure backward compatibility add moderate difficulty.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement highlights some edge cases, such as multi-writer safety issues with `resize` (explicitly noted as unsupported) and the need to handle old vs. new array schemas differently. The code change likely addresses an edge case related to non-standard dimensions in dataframes, though specifics are unclear without further context from #3416. Error handling is mentioned (e.g., raising `IndexError` for out-of-bounds writes), but the complexity of implementing this across various scenarios (e.g., resizing beyond domain limits, upgrading shapes) suggests moderate challenges in ensuring robustness.\n\n4. **Overall Impact and Complexity**: While the specific code change is straightforward, fitting it into the larger feature implementation requires understanding interactions between core TileDB functionality and SOMA's higher-level abstractions. The problem does not seem to impact the system's core architecture fundamentally but does involve careful handling of compatibility and user-facing API changes. The difficulty is not high enough to warrant a \"hard\" rating (0.6-0.8) since it doesn't appear to require deep algorithmic innovation or system-level refactoring, but it exceeds \"easy\" (0.2-0.4) due to the need for cross-component awareness and handling of compatibility constraints.\n\nIn summary, this problem is of medium difficulty, requiring a solid understanding of the TileDB-SOMA codebase, moderate handling of edge cases, and integration into a broader feature set, but it does not pose highly complex or system-wide challenges.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[python/r/c++] Revisit `shape` for component arrays\n## PRs\r\n\r\n**Merged PRs:**\r\n\r\n<details>\r\n\r\n* #2909 `kerl/schevo-timestamp-methodize`\r\n* #2913 `kerl/name-neaten`\r\n* #2908 `kerl/ut-soma-exc-simplify`\r\n* #2910 `kerl/test-common-parameterize`\r\n* #2918 `kerl/cpp-test-deadstrip`\r\n* #2919 `kerl/minor-unit-test-helper-mod`\r\n* #2936 `kerl/cpp-ut-helper-neaten`\r\n* #2938 `kerl/more-cur-dom-parameterize`\r\n* #2915 `kerl/cpp-strict-int64-shape`\r\n* #2911 `kerl/arrow-util-current-domain-optional`\r\n* #2939 `kerl/step-two-temp`\r\n* #2947 `kerl/cpp-ndarray-resize-testing`\r\n* #2945 `kerl/dataframe-test-fixture`\r\n* #2944 `kerl/cpp-variant-indexed-dataframes`\r\n* #2916 `kerl/sdf-shape`\r\n* #2917 `kerl/cpp-resizes`\r\n* #2948 `kerl/upgrade-shape-int64`\r\n* #2951 `kerl/sdf-test-accessors`\r\n* #2953 `kerl/py-r-accessor-plumbing`\r\n* #2957 `kerl/sdf-domain-accessors`\r\n* #2960 `kerl/dense-link`\r\n* #2963 `kerl/nightly-fix`\r\n* #2970 `kerl/dense-writeable-after-create`\r\n* #2968 `kerl/minor-trim`\r\n* #2969 `kerl/more-py-domain-name-neaten`\r\n* #2972 `kerl/libtiledbsoma-env-logging-level`\r\n* #2962 `kerl/py-r-creation-paths`\r\n* #2950 `kerl/py-r-test-2`\r\n* #2994 `kerl/nanoarrow-helpers`\r\n* #3011 `kerl/polydom3`\r\n* #3017 `kerl/polydom5`\r\n* #3018 `kerl/polydom6`\r\n* #2990 `kerl/variant-nnz-bug`\r\n* #3019 `kerl/index-swap`\r\n* #3020 `kerl/ut-max-shape`\r\n* #3012 `kerl/polydom4`\r\n* #3025 `kerl/fix-3020-merge`\r\n* #3026 `kerl/one-more-rename`\r\n* #3028 `kerl/ff-not`\r\n* #3029 `kerl/ut-vg`\r\n* #3030 `kerl/table-utils-memory`\r\n* #3067 `kerl/improve-sdf-test-field-names`\r\n* #3068 `kerl/ut-generate`\r\n* #3069 `kerl/cpp-sdf-domain-at-create`\r\n* #3027 `kerl/hll-domainish`\r\n* #3088 `kerl/max-domain-int64`\r\n* #3090 `kerl/maybe-resize-soma-joinid-cpp-tweak`\r\n* #3032 `kerl/sdf-domain-at-create` -- fixes #2967\r\n* #3091 `kerl/maybe-resize-soma-joinid-py-r`\r\n* #3095 `kerl/cpp-exp-resize-prep`\r\n* #3089 `kerl/r-dataframe-shapeable`\r\n* https://github.com/single-cell-data/SOMA/pull/233\r\n* #3125 `kerl/cpp-ut-name-shortens`\r\n* #3127 `kerl/helper-rename`\r\n* #3130 `kerl/cpp-can-resizers-names`\r\n* #3132 `kerl/cpp-dataframe-sizing-helpers`\r\n* #3139 `kerl/cpp-dataframe-upgrade-test`\r\n* #3140 `kerl/py-resizer-connects`\r\n* #3151 `kerl/py-can-upgrade-shape`\r\n* #3152 `kerl/registration-shape-acceessors`\r\n* #3156 `kerl/py-exp-shaping`\r\n* #3157 `kerl/py-exp-shaping2`\r\n* #3148 `kerl/py-exp-resize`\r\n* #3191 `kerl/py-domain-at-create-ut-1`\r\n* #3190 `kerl/py-domain-at-create-ut-2`\r\n* #3192 `kerl/py-domain-at-create-ut-3`\r\n* #3193 `kerl/py-domain-at-create-ut-4`\r\n* #3194 `kerl/py-domain-at-create-ut-5`\r\n* #3203 `kerl/min-size-2`\r\n* #3208 `kerl/r-min-sizing`\r\n* #3211 `kerl/cpp-ugr-dom`\r\n* #3232 `kerl/ff-interop`\r\n* #3230 `kerl/ffon`\r\n* #3234 `kerl/docstring-prune`\r\n* #3236 `kerl/prefixing`\r\n* #3241 `kerl/fix-bad-merge`\r\n* #3235 `kerl/py-r-ugr-dom`\r\n* #3237 `kerl/py-r-ugr-dom-2`\r\n* #3238 `kerl/py-r-ugr-dom-3`\r\n* #3253 `kerl/set-coords-rename`\r\n* #3261 `kerl/pybind11-nda-sizing`\r\n* #3265 `kerl/dense-227-a`\r\n* #3263 `kerl/dense-range-trim`\r\n* #3268 `kerl/dim-explosion`\r\n* #3269 `kerl/python-227-dense-ned-read`\r\n* #3270 `kerl/r-227-dense-fixes`\r\n* #3280 `kerl/r-dense-227-more`\r\n* #3286 `kerl/more-fn4m`\r\n* #3283 `kerl/readthedocs-pre-1.15`\r\n* #3288 `kerl/dense-ugrsh`\r\n* #3289 `kerl/notebook-shape-upgrade`\r\n* #3285 `kerl/new-shape-doc-updates`\r\n* #3295 `kerl/notebook-data-refresh`\r\n* #3290 `kerl/notebook-new-shape-refresh`\r\n* #3301 `kerl/ffena`\r\n* #3303 `kerl/r-data-refresh`\r\n* #3300 `kerl/sdf-sjid-lower-zero`\r\n* #3296 `kerl/dense-example-data-refresh`\r\n* #3294 `kerl/new-shape-notebook-and-vignette`\r\n* #3308 `kerl/upgrade-experiment-resources`\r\n* #3309 `kerl/fix-notebook-merge`\r\n* #3321 `kerl/more-use-shape`\r\n* #3358 `kerl/revert-3300`\r\n* #3368 `kerl/227a`\r\n* #3369 `kerl/ucd1`\r\n* #3370 `kerl/ucd2`\r\n* #3371 `kerl/ucd3`\r\n* #3372 `kerl/ucd4`\r\n* #3396 `kerl/domain-at-create-docstrings`\r\n* #3302 `kerl/new-shape-vignette`\r\n* https://github.com/single-cell-data/SOMA/pull/250\r\n* #3399 `kerl/new-shape-more-docstrings`\r\n* #3400 `kerl/check-only-r`\r\n* https://github.com/TileDB-Inc/somacore-feedstock/pull/27\r\n\r\n</details>\r\n\r\n**Closed/abandoned PRs:**\r\n\r\n<details>\r\n\r\n* #2785 -- This was only dogfooding for the core 2.25 release -- not to be merged\r\n* #2952 `kerl/feature-flag-temp` -- folded into 2962\r\n* #2995 `kerl/polydom`\r\n* #2964 `kerl/tiledbsoma-io-test`\r\n* #3189 `kerl/min-size`\r\n* #3220 `kerl/cpp-ugr-dom-2`\r\n* #3244 `kerl/dense-227-fixes`\r\n* #3276 `kerl/dataframe-shape`\r\n\r\n</details>\r\n\r\n**Issues which are related but non-blocking:**\r\n\r\n* https://github.com/single-cell-data/SOMA/pull/216\r\n* https://github.com/TileDB-Inc/TileDB/pull/5303\r\n* #2966\r\n* #3081\r\n* Note: R append mode does not exist yet -- see #1630 -- so an experiment-level resizer is not a priority in R\r\n* #3271 \r\n* #3272\r\n* #3273\r\n\r\n**See also:** [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Problem to be solved\r\n\r\nUsers want to know the `shape` of an array, in the SciPy sense:\r\n\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`.\r\n  * Some users need the ability to grow their datasets later, using either `tiledbsoma.io`'s append mode, or subsequent writes using the `tiledbsoma` API.\r\n  * Note that the cellxgene census doesn't need this: eact week's published census has fixed shape, and any updates will happen in new storage, on a new week.\r\n\r\nUsing TileDB-SOMA up until the present:\r\n\r\n* The TIleDB `domain` is immutable after array creation\r\n  * This does bounds-checking for reads and writes, which is good\r\n  * To leverage this to function as a `shape`, users would need to set the `domain` at array-creation time. However, users lose the ability to grow their datasets later.\r\n* There is a `non_empty_domain` accessor\r\n  * This only indicates min/max coordinates _at which data exists_. Consider an `X` array for 100 cells and 200 genes. If non-zero expression counts exist only for cell join IDs 2-17, then the `non_empty_domain` will indicate `(2,17)` along `soma_dim_0`.\r\n  * Consider an `obms[\"X_pca\"]` within the same experiment. This may be 100 cells by 50 PCA components: we need a placd to store the number 50.\r\n  * Therefore users cannot leverage this to function as a `shape` accessor.\r\n* We have offered a `used_shape` accessor since TileDB-SOMA 1.5.\r\n  * This functions as a `shape` accessor, in the SciPy sense, but it is not multi-writer safe.\r\n\r\nNew feature for TileDB-SOMA 1.15:\r\n\r\n* Arrays will have a  `shape`\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`\r\n* The `used_shape` accessor will be deprecated in TileDB-SOMA 1.13, and slated for removal in TileDB-SOMA 1.14.\r\n\r\nCompatiblity:\r\n\r\nThis will now require users to do an explicit `resize` before appending/growing TileDB-SOMA Experiments. Guidance in the form of example notebooks will be provided.\r\n\r\n##  Tracking\r\n\r\nSee also: [[sc-41074]](https://app.shortcut.com/tiledb-inc/story/41074) and [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Scheduling\r\n\r\nSupport arrives in TileDB Core 2.25. Deprecations for TileDB-SOMA will be released with 1.13. Full support within TileDB-SOMA will be release in 1.14.\r\n\r\n## Details\r\n\r\nSOMA API mods as we've discussed in a Google doc are as follows.\r\n\r\n### `SOMADataFrame`\r\n\r\n* `create`: Retain the `domain` argument\r\n  * Issue:\r\n    * Core has a `(lo, hi)` tuple per dim, e.g. `(0,99)` or `(10,19)`\r\n    * SOMA has count per dim, with 0 implicit: e.g. 100 or 20\r\n    * For `SparseNDArray` and `DenseNDArray` core can have `(lo, hi)` and SOMA can have `count`\r\n    * For `DataFrame` there can be multiple dims --- default is a single `soma_joinid`\r\n    * That could be treated either in `(lo, hi)` fashion or `count` fashion\r\n    * However additional dims (e.g. `cell_type`) can be on any type, including strings, floats, etc. where there is no implicit lo=0\r\n    * Therefore we need to keep the current SOMA API wherein `DataFrame` takes a `domain` argument (in `(lo, hi)` fashion) and not a `shape` argument (in `count` fashion)\r\n\r\n### `SparseNDArray and DenseNDArray`\r\n\r\n* `create`\r\n  * Have an optional shape argument which is of type `Tuple[Int,...]` where each element is the cell count of the corresponding dimension\r\n    * If unsupplied, or if supplied but None in any slot: use the minimum 0 in each slot \u2013 nothing larger makes sense since we will not support downsize\r\n  * User guidance should make clear that it will not be possible to create an \u2018old\u2019 style array with the \u2018new style\u2019 API. (See also the upgrade logic below.)\r\n\r\n### All three of `SOMADataFrame`, `SparseNDArray`, `DenseNDArray`\r\n\r\n* `write`\r\n  * For new arrays, created with the new shape feature:\r\n    * Core will bounds-check that coordinates provided at `write` time are within the current shape\r\n    * Core will raise `tiledb.cc.TileDBError` to TileDB-SOMA, which will catch and raise `IndexError`, and R-standard behavior on the R side\r\n  * For old arrays created before this feature:\r\n    * Core will not bounds-check that coordinates provided at write time are within the current shape\r\n* Existing `used_shape` accessor\r\n  * TileDB-SOMA will deprecate this over a release cycle.\r\n  * For new arrays: raise `NotImplementedError`\r\n  * For old arrays: return what\u2019s currently returned, with a deprecation warning.\r\n  * Mechanism for determining old vs. new: `array.schema.version` (the core storage version).\r\n* Existing `shape` accessor\r\n  * For new arrays:\r\n    * Have this return the new shape as proposed by core, no longer returning the TileDB domain.\r\n  * For old arrays created before this feature:\r\n    * Return the TileDB domain as now.\r\n* Existing `non_empty_domain` accessor\r\n  * Same behavior for old and new arrays (unaffected by this proposal).\r\n  * Keep this accessor supported, but, with user notes that it\u2019s generally non-useful\r\n  * This should return None (or R equivalent) when there is a schema but no data have been written.\r\n* New `maxshape` accessor\r\n  * Maps the core-level `(lo, hi)` accessor for domain to count-style accessor hi+1. E.g. if the core domain is either `(0,99)` or `(50,99)` then TileDB-SOMA `maxshape` will say 100.\r\n  * Same behavior for old and new arrays.\r\n  * Let users query for what the TileDB domain is, with user notes that it\u2019s the maximum that users can reshape to.\r\n  * Issac suggests: maybe `domain` or `maxshape` (see h5py).\r\n* New `resize` mutator\r\n  * Note: `reshape` means something else in the community (numpy, zarr, h5py), e.g. a 5x20 (total 100 cells) being reinterpreted as 4x25 (still 100 cells). The standard name for changing cell-count is `resize`.\r\n  * For old arrays created before this feature: raise `NotImplementedError`.\r\n  * For new arrays:\r\n    * Will raise `ValueError` if the new shape is smaller on any dim than currently in storage\r\n    * Regardless of whether any data have been written whatsoever\r\n    * Will raise `ValueError` if the new shape exceeds the TileDB domain from create time, which will serve TileDB-SOMA in a role of \u201cmax possible shape the user can reshape to\u201d\r\n    * Otherwise, any calls to write from this point will bounds-check writes within this new shape\r\n    * **We don\u2019t expect resize to be multi-writer safe with regard to write ; user notes must be clear on this point**\r\n* New `tiledbsoma_upgrade_shape` method for SparseNDArray and DenseNDArray\r\n  * This will leverage `array.schema.version` to see if an upgrade is needed\r\n  * Leverage core support for storage-version updates\r\n  * This will take a shape argument as in `create`\r\n  * For arrays created with \u201cjust-right\u201d size: this will succeed\r\n  * For arrays created with \u201croom-for-growth\u201d / \u201ctwo billion-ish\u201d size: this will succeed\r\n  * If the user passes a shape which exceeds the current TileDB domain: this will fail\r\n* New `tiledbsoma_upgrade_domain method` for `DataFrame`\r\n  * Same as for `SparseNDArray`/`DenseNDArray` except it will take a domain at the SOMA-API level just as `DataFrame`'s create method\r\n\r\n### `tiledbsoma.io`\r\n\r\n* The user-facing API has no shape arguments and thus won\u2019t need changing.\r\n* Internally to `tiledbsoma.io`, we\u2019ll still ask the tiledbsoma API for the \u201cbig domain\u201d (2 billionish)\r\n* Append mode:\r\n  * Will need a new `resize` method at the `Experiment` level\r\n  * Users will need to:\r\n    * Register as now\r\n    * Call the experiment-level `resize`\r\n      * Could be `exp.resize(...)`, or (better) this could be `tiledbsoma.io.reshape_experiment`\r\n  * In either case: this method will take the new `obs` and `var` counts as inputs:\r\n    * `exp.obs.reshape` to new `obs` count\r\n    * `exp.ms[name].var.reshape` to new `var` count\r\n    * `exp.ms[name].X[name].reshape` to new `obs` count x `var` count\r\n    * `exp.ms[name].obsm[name].reshape` to new `obs` count x same width\r\n    * `exp.ms[name].obsp[name].reshape` to new `obs` count x `obs` count\r\n    * `exp.ms[name].varm[name].reshape` to new `var` count x same width\r\n    * `exp.ms[name].varp[name].reshape` to new `var` count x `var` count\r\n  * Do the individual append-mode writes as now\r\n\n[python/r/c++] Revisit `shape` for component arrays\n## PRs\r\n\r\n**Merged PRs:**\r\n\r\n<details>\r\n\r\n* #2909 `kerl/schevo-timestamp-methodize`\r\n* #2913 `kerl/name-neaten`\r\n* #2908 `kerl/ut-soma-exc-simplify`\r\n* #2910 `kerl/test-common-parameterize`\r\n* #2918 `kerl/cpp-test-deadstrip`\r\n* #2919 `kerl/minor-unit-test-helper-mod`\r\n* #2936 `kerl/cpp-ut-helper-neaten`\r\n* #2938 `kerl/more-cur-dom-parameterize`\r\n* #2915 `kerl/cpp-strict-int64-shape`\r\n* #2911 `kerl/arrow-util-current-domain-optional`\r\n* #2939 `kerl/step-two-temp`\r\n* #2947 `kerl/cpp-ndarray-resize-testing`\r\n* #2945 `kerl/dataframe-test-fixture`\r\n* #2944 `kerl/cpp-variant-indexed-dataframes`\r\n* #2916 `kerl/sdf-shape`\r\n* #2917 `kerl/cpp-resizes`\r\n* #2948 `kerl/upgrade-shape-int64`\r\n* #2951 `kerl/sdf-test-accessors`\r\n* #2953 `kerl/py-r-accessor-plumbing`\r\n* #2957 `kerl/sdf-domain-accessors`\r\n* #2960 `kerl/dense-link`\r\n* #2963 `kerl/nightly-fix`\r\n* #2970 `kerl/dense-writeable-after-create`\r\n* #2968 `kerl/minor-trim`\r\n* #2969 `kerl/more-py-domain-name-neaten`\r\n* #2972 `kerl/libtiledbsoma-env-logging-level`\r\n* #2962 `kerl/py-r-creation-paths`\r\n* #2950 `kerl/py-r-test-2`\r\n* #2994 `kerl/nanoarrow-helpers`\r\n* #3011 `kerl/polydom3`\r\n* #3017 `kerl/polydom5`\r\n* #3018 `kerl/polydom6`\r\n* #2990 `kerl/variant-nnz-bug`\r\n* #3019 `kerl/index-swap`\r\n* #3020 `kerl/ut-max-shape`\r\n* #3012 `kerl/polydom4`\r\n* #3025 `kerl/fix-3020-merge`\r\n* #3026 `kerl/one-more-rename`\r\n* #3028 `kerl/ff-not`\r\n* #3029 `kerl/ut-vg`\r\n* #3030 `kerl/table-utils-memory`\r\n* #3067 `kerl/improve-sdf-test-field-names`\r\n* #3068 `kerl/ut-generate`\r\n* #3069 `kerl/cpp-sdf-domain-at-create`\r\n* #3027 `kerl/hll-domainish`\r\n* #3088 `kerl/max-domain-int64`\r\n* #3090 `kerl/maybe-resize-soma-joinid-cpp-tweak`\r\n* #3032 `kerl/sdf-domain-at-create` -- fixes #2967\r\n* #3091 `kerl/maybe-resize-soma-joinid-py-r`\r\n* #3095 `kerl/cpp-exp-resize-prep`\r\n* #3089 `kerl/r-dataframe-shapeable`\r\n* https://github.com/single-cell-data/SOMA/pull/233\r\n* #3125 `kerl/cpp-ut-name-shortens`\r\n* #3127 `kerl/helper-rename`\r\n* #3130 `kerl/cpp-can-resizers-names`\r\n* #3132 `kerl/cpp-dataframe-sizing-helpers`\r\n* #3139 `kerl/cpp-dataframe-upgrade-test`\r\n* #3140 `kerl/py-resizer-connects`\r\n* #3151 `kerl/py-can-upgrade-shape`\r\n* #3152 `kerl/registration-shape-acceessors`\r\n* #3156 `kerl/py-exp-shaping`\r\n* #3157 `kerl/py-exp-shaping2`\r\n* #3148 `kerl/py-exp-resize`\r\n* #3191 `kerl/py-domain-at-create-ut-1`\r\n* #3190 `kerl/py-domain-at-create-ut-2`\r\n* #3192 `kerl/py-domain-at-create-ut-3`\r\n* #3193 `kerl/py-domain-at-create-ut-4`\r\n* #3194 `kerl/py-domain-at-create-ut-5`\r\n* #3203 `kerl/min-size-2`\r\n* #3208 `kerl/r-min-sizing`\r\n* #3211 `kerl/cpp-ugr-dom`\r\n* #3232 `kerl/ff-interop`\r\n* #3230 `kerl/ffon`\r\n* #3234 `kerl/docstring-prune`\r\n* #3236 `kerl/prefixing`\r\n* #3241 `kerl/fix-bad-merge`\r\n* #3235 `kerl/py-r-ugr-dom`\r\n* #3237 `kerl/py-r-ugr-dom-2`\r\n* #3238 `kerl/py-r-ugr-dom-3`\r\n* #3253 `kerl/set-coords-rename`\r\n* #3261 `kerl/pybind11-nda-sizing`\r\n* #3265 `kerl/dense-227-a`\r\n* #3263 `kerl/dense-range-trim`\r\n* #3268 `kerl/dim-explosion`\r\n* #3269 `kerl/python-227-dense-ned-read`\r\n* #3270 `kerl/r-227-dense-fixes`\r\n* #3280 `kerl/r-dense-227-more`\r\n* #3286 `kerl/more-fn4m`\r\n* #3283 `kerl/readthedocs-pre-1.15`\r\n* #3288 `kerl/dense-ugrsh`\r\n* #3289 `kerl/notebook-shape-upgrade`\r\n* #3285 `kerl/new-shape-doc-updates`\r\n* #3295 `kerl/notebook-data-refresh`\r\n* #3290 `kerl/notebook-new-shape-refresh`\r\n* #3301 `kerl/ffena`\r\n* #3303 `kerl/r-data-refresh`\r\n* #3300 `kerl/sdf-sjid-lower-zero`\r\n* #3296 `kerl/dense-example-data-refresh`\r\n* #3294 `kerl/new-shape-notebook-and-vignette`\r\n* #3308 `kerl/upgrade-experiment-resources`\r\n* #3309 `kerl/fix-notebook-merge`\r\n* #3321 `kerl/more-use-shape`\r\n* #3358 `kerl/revert-3300`\r\n* #3368 `kerl/227a`\r\n* #3369 `kerl/ucd1`\r\n* #3370 `kerl/ucd2`\r\n* #3371 `kerl/ucd3`\r\n* #3372 `kerl/ucd4`\r\n* #3396 `kerl/domain-at-create-docstrings`\r\n* #3302 `kerl/new-shape-vignette`\r\n* https://github.com/single-cell-data/SOMA/pull/250\r\n* #3399 `kerl/new-shape-more-docstrings`\r\n* #3400 `kerl/check-only-r`\r\n* https://github.com/TileDB-Inc/somacore-feedstock/pull/27\r\n\r\n</details>\r\n\r\n**Closed/abandoned PRs:**\r\n\r\n<details>\r\n\r\n* #2785 -- This was only dogfooding for the core 2.25 release -- not to be merged\r\n* #2952 `kerl/feature-flag-temp` -- folded into 2962\r\n* #2995 `kerl/polydom`\r\n* #2964 `kerl/tiledbsoma-io-test`\r\n* #3189 `kerl/min-size`\r\n* #3220 `kerl/cpp-ugr-dom-2`\r\n* #3244 `kerl/dense-227-fixes`\r\n* #3276 `kerl/dataframe-shape`\r\n\r\n</details>\r\n\r\n**Issues which are related but non-blocking:**\r\n\r\n* https://github.com/single-cell-data/SOMA/pull/216\r\n* https://github.com/TileDB-Inc/TileDB/pull/5303\r\n* #2966\r\n* #3081\r\n* Note: R append mode does not exist yet -- see #1630 -- so an experiment-level resizer is not a priority in R\r\n* #3271 \r\n* #3272\r\n* #3273\r\n\r\n**See also:** [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Problem to be solved\r\n\r\nUsers want to know the `shape` of an array, in the SciPy sense:\r\n\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`.\r\n  * Some users need the ability to grow their datasets later, using either `tiledbsoma.io`'s append mode, or subsequent writes using the `tiledbsoma` API.\r\n  * Note that the cellxgene census doesn't need this: eact week's published census has fixed shape, and any updates will happen in new storage, on a new week.\r\n\r\nUsing TileDB-SOMA up until the present:\r\n\r\n* The TIleDB `domain` is immutable after array creation\r\n  * This does bounds-checking for reads and writes, which is good\r\n  * To leverage this to function as a `shape`, users would need to set the `domain` at array-creation time. However, users lose the ability to grow their datasets later.\r\n* There is a `non_empty_domain` accessor\r\n  * This only indicates min/max coordinates _at which data exists_. Consider an `X` array for 100 cells and 200 genes. If non-zero expression counts exist only for cell join IDs 2-17, then the `non_empty_domain` will indicate `(2,17)` along `soma_dim_0`.\r\n  * Consider an `obms[\"X_pca\"]` within the same experiment. This may be 100 cells by 50 PCA components: we need a placd to store the number 50.\r\n  * Therefore users cannot leverage this to function as a `shape` accessor.\r\n* We have offered a `used_shape` accessor since TileDB-SOMA 1.5.\r\n  * This functions as a `shape` accessor, in the SciPy sense, but it is not multi-writer safe.\r\n\r\nNew feature for TileDB-SOMA 1.15:\r\n\r\n* Arrays will have a  `shape`\r\n* Reads and writes are bounds-checked against the shape\r\n* This retains its value regardless of which values of a sparse array are or are not actually occupied\r\n* Users can `resize`\r\n* The `used_shape` accessor will be deprecated in TileDB-SOMA 1.13, and slated for removal in TileDB-SOMA 1.14.\r\n\r\nCompatiblity:\r\n\r\nThis will now require users to do an explicit `resize` before appending/growing TileDB-SOMA Experiments. Guidance in the form of example notebooks will be provided.\r\n\r\n##  Tracking\r\n\r\nSee also: [[sc-41074]](https://app.shortcut.com/tiledb-inc/story/41074) and [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\n## Scheduling\r\n\r\nSupport arrives in TileDB Core 2.25. Deprecations for TileDB-SOMA will be released with 1.13. Full support within TileDB-SOMA will be release in 1.14.\r\n\r\n## Details\r\n\r\nSOMA API mods as we've discussed in a Google doc are as follows.\r\n\r\n### `SOMADataFrame`\r\n\r\n* `create`: Retain the `domain` argument\r\n  * Issue:\r\n    * Core has a `(lo, hi)` tuple per dim, e.g. `(0,99)` or `(10,19)`\r\n    * SOMA has count per dim, with 0 implicit: e.g. 100 or 20\r\n    * For `SparseNDArray` and `DenseNDArray` core can have `(lo, hi)` and SOMA can have `count`\r\n    * For `DataFrame` there can be multiple dims --- default is a single `soma_joinid`\r\n    * That could be treated either in `(lo, hi)` fashion or `count` fashion\r\n    * However additional dims (e.g. `cell_type`) can be on any type, including strings, floats, etc. where there is no implicit lo=0\r\n    * Therefore we need to keep the current SOMA API wherein `DataFrame` takes a `domain` argument (in `(lo, hi)` fashion) and not a `shape` argument (in `count` fashion)\r\n\r\n### `SparseNDArray and DenseNDArray`\r\n\r\n* `create`\r\n  * Have an optional shape argument which is of type `Tuple[Int,...]` where each element is the cell count of the corresponding dimension\r\n    * If unsupplied, or if supplied but None in any slot: use the minimum 0 in each slot \u2013 nothing larger makes sense since we will not support downsize\r\n  * User guidance should make clear that it will not be possible to create an \u2018old\u2019 style array with the \u2018new style\u2019 API. (See also the upgrade logic below.)\r\n\r\n### All three of `SOMADataFrame`, `SparseNDArray`, `DenseNDArray`\r\n\r\n* `write`\r\n  * For new arrays, created with the new shape feature:\r\n    * Core will bounds-check that coordinates provided at `write` time are within the current shape\r\n    * Core will raise `tiledb.cc.TileDBError` to TileDB-SOMA, which will catch and raise `IndexError`, and R-standard behavior on the R side\r\n  * For old arrays created before this feature:\r\n    * Core will not bounds-check that coordinates provided at write time are within the current shape\r\n* Existing `used_shape` accessor\r\n  * TileDB-SOMA will deprecate this over a release cycle.\r\n  * For new arrays: raise `NotImplementedError`\r\n  * For old arrays: return what\u2019s currently returned, with a deprecation warning.\r\n  * Mechanism for determining old vs. new: `array.schema.version` (the core storage version).\r\n* Existing `shape` accessor\r\n  * For new arrays:\r\n    * Have this return the new shape as proposed by core, no longer returning the TileDB domain.\r\n  * For old arrays created before this feature:\r\n    * Return the TileDB domain as now.\r\n* Existing `non_empty_domain` accessor\r\n  * Same behavior for old and new arrays (unaffected by this proposal).\r\n  * Keep this accessor supported, but, with user notes that it\u2019s generally non-useful\r\n  * This should return None (or R equivalent) when there is a schema but no data have been written.\r\n* New `maxshape` accessor\r\n  * Maps the core-level `(lo, hi)` accessor for domain to count-style accessor hi+1. E.g. if the core domain is either `(0,99)` or `(50,99)` then TileDB-SOMA `maxshape` will say 100.\r\n  * Same behavior for old and new arrays.\r\n  * Let users query for what the TileDB domain is, with user notes that it\u2019s the maximum that users can reshape to.\r\n  * Issac suggests: maybe `domain` or `maxshape` (see h5py).\r\n* New `resize` mutator\r\n  * Note: `reshape` means something else in the community (numpy, zarr, h5py), e.g. a 5x20 (total 100 cells) being reinterpreted as 4x25 (still 100 cells). The standard name for changing cell-count is `resize`.\r\n  * For old arrays created before this feature: raise `NotImplementedError`.\r\n  * For new arrays:\r\n    * Will raise `ValueError` if the new shape is smaller on any dim than currently in storage\r\n    * Regardless of whether any data have been written whatsoever\r\n    * Will raise `ValueError` if the new shape exceeds the TileDB domain from create time, which will serve TileDB-SOMA in a role of \u201cmax possible shape the user can reshape to\u201d\r\n    * Otherwise, any calls to write from this point will bounds-check writes within this new shape\r\n    * **We don\u2019t expect resize to be multi-writer safe with regard to write ; user notes must be clear on this point**\r\n* New `tiledbsoma_upgrade_shape` method for SparseNDArray and DenseNDArray\r\n  * This will leverage `array.schema.version` to see if an upgrade is needed\r\n  * Leverage core support for storage-version updates\r\n  * This will take a shape argument as in `create`\r\n  * For arrays created with \u201cjust-right\u201d size: this will succeed\r\n  * For arrays created with \u201croom-for-growth\u201d / \u201ctwo billion-ish\u201d size: this will succeed\r\n  * If the user passes a shape which exceeds the current TileDB domain: this will fail\r\n* New `tiledbsoma_upgrade_domain method` for `DataFrame`\r\n  * Same as for `SparseNDArray`/`DenseNDArray` except it will take a domain at the SOMA-API level just as `DataFrame`'s create method\r\n\r\n### `tiledbsoma.io`\r\n\r\n* The user-facing API has no shape arguments and thus won\u2019t need changing.\r\n* Internally to `tiledbsoma.io`, we\u2019ll still ask the tiledbsoma API for the \u201cbig domain\u201d (2 billionish)\r\n* Append mode:\r\n  * Will need a new `resize` method at the `Experiment` level\r\n  * Users will need to:\r\n    * Register as now\r\n    * Call the experiment-level `resize`\r\n      * Could be `exp.resize(...)`, or (better) this could be `tiledbsoma.io.reshape_experiment`\r\n  * In either case: this method will take the new `obs` and `var` counts as inputs:\r\n    * `exp.obs.reshape` to new `obs` count\r\n    * `exp.ms[name].var.reshape` to new `var` count\r\n    * `exp.ms[name].X[name].reshape` to new `obs` count x `var` count\r\n    * `exp.ms[name].obsm[name].reshape` to new `obs` count x same width\r\n    * `exp.ms[name].obsp[name].reshape` to new `obs` count x `obs` count\r\n    * `exp.ms[name].varm[name].reshape` to new `var` count x same width\r\n    * `exp.ms[name].varp[name].reshape` to new `var` count x `var` count\r\n  * Do the individual append-mode writes as now\r\n\n[ci] Default the new-shape feature to enabled, still testing both\n**Issue and/or context:** As tracked on issue #2407 / [[sc-51048]](https://app.shortcut.com/tiledb-inc/story/51048).\r\n\r\nNote that the intended Python and R API changes are all agreed on and finalized as described in #2407.\r\n\r\n**Changes:**\r\n\r\nWhile we're still running CI both ways, set the default to true for sandbox/dogfooding within the team, as discussed earlier this week.\r\n\r\n**Notes for Reviewer:**\r\n\r\nThis is atop #3230.\n", "patch": "diff --git a/.github/workflows/python-ci-single.yml b/.github/workflows/python-ci-single.yml\nindex f07b1da23e..408ed20e8e 100644\n--- a/.github/workflows/python-ci-single.yml\n+++ b/.github/workflows/python-ci-single.yml\n@@ -133,14 +133,14 @@ jobs:\n       # Setting PYTHONPATH ensures the tests load the in-tree source code under apis/python/src\n       # instead of the copy we `pip install`ed to site-packages above. That's needed for the code\n       # coverage analysis to work.\n-      run: PYTHONPATH=$(pwd)/apis/python/src python -m pytest --cov=apis/python/src --cov-report=xml apis/python/tests -v --durations=20 --maxfail=50\n+      run: export SOMA_PY_NEW_SHAPE=false; PYTHONPATH=$(pwd)/apis/python/src python -m pytest --cov=apis/python/src --cov-report=xml apis/python/tests -v --durations=20 --maxfail=50\n \n     - name: Run pytests for Python with new shape\n       shell: bash\n       # Setting PYTHONPATH ensures the tests load the in-tree source code under apis/python/src\n       # instead of the copy we `pip install`ed to site-packages above. That's needed for the code\n       # coverage analysis to work.\n-      run: export SOMA_PY_NEW_SHAPE=true; PYTHONPATH=$(pwd)/apis/python/src python -m pytest --cov=apis/python/src --cov-report=xml apis/python/tests -v --durations=20 --maxfail=50\n+      run: PYTHONPATH=$(pwd)/apis/python/src python -m pytest --cov=apis/python/src --cov-report=xml apis/python/tests -v --durations=20 --maxfail=50\n \n     - name: Report coverage to Codecov\n       if: inputs.report_codecov\ndiff --git a/.github/workflows/r-ci.yml b/.github/workflows/r-ci.yml\nindex 5a50f47065..b5e71f61b8 100644\n--- a/.github/workflows/r-ci.yml\n+++ b/.github/workflows/r-ci.yml\n@@ -150,12 +150,12 @@ jobs:\n \n       - name: Test without new shape\n         if: ${{ matrix.covr == 'no' }}\n-        run: cd apis/r/tests && Rscript testthat.R\n+        run: export SOMA_R_NEW_SHAPE=false && cd apis/r/tests && Rscript testthat.R\n \n       # https://github.com/single-cell-data/TileDB-SOMA/issues/2407\n       - name: Test with new shape\n         if: ${{ matrix.covr == 'no' }}\n-        run: export SOMA_R_NEW_SHAPE=true && cd apis/r/tests && Rscript testthat.R\n+        run: cd apis/r/tests && Rscript testthat.R\n \n       - name: Coverage\n         if: ${{ matrix.os == 'ubuntu-latest' && matrix.covr == 'yes' && github.event_name == 'workflow_dispatch' }}\ndiff --git a/apis/python/src/tiledbsoma/_flags.py b/apis/python/src/tiledbsoma/_flags.py\nindex 2ce5a8f0be..f0aa8c881b 100644\n--- a/apis/python/src/tiledbsoma/_flags.py\n+++ b/apis/python/src/tiledbsoma/_flags.py\n@@ -10,4 +10,4 @@\n # removed once https://github.com/single-cell-data/TileDB-SOMA/issues/2407 is\n # complete.\n \n-NEW_SHAPE_FEATURE_FLAG_ENABLED = os.getenv(\"SOMA_PY_NEW_SHAPE\") is not None\n+NEW_SHAPE_FEATURE_FLAG_ENABLED = os.getenv(\"SOMA_PY_NEW_SHAPE\") != \"false\"\ndiff --git a/apis/r/R/Init.R b/apis/r/R/Init.R\nindex d25e9c8d6a..91afe50afc 100644\n--- a/apis/r/R/Init.R\n+++ b/apis/r/R/Init.R\n@@ -20,10 +20,10 @@\n \n     # This is temporary for https://github.com/single-cell-data/TileDB-SOMA/issues/2407\n     # It will be removed once 2407 is complete.\n-    if (Sys.getenv(\"SOMA_R_NEW_SHAPE\") != \"\") {\n-      .pkgenv[[\"use_current_domain_transitional_internal_only\"]] <- TRUE\n-    } else {\n+    if (Sys.getenv(\"SOMA_R_NEW_SHAPE\") == \"false\") {\n       .pkgenv[[\"use_current_domain_transitional_internal_only\"]] <- FALSE\n+    } else {\n+      .pkgenv[[\"use_current_domain_transitional_internal_only\"]] <- TRUE\n     }\n }\n \n", "instance_id": "single-cell-data__TileDB-SOMA-3230", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in defining the goal of introducing a new `shape` feature for TileDB-SOMA arrays, which allows bounds-checking for reads/writes and supports resizing. It provides detailed context about the limitations of the current system (e.g., immutable `domain`, limitations of `non_empty_domain` and `used_shape`), the new functionality to be added, and the compatibility considerations with old and new arrays. The API modifications for different array types (`SOMADataFrame`, `SparseNDArray`, `DenseNDArray`) are well-documented, including specific behaviors for accessors and mutators like `resize`. Additionally, the statement includes scheduling, tracking, and compatibility guidance, which adds to its comprehensiveness.\n\nHowever, there are minor ambiguities and missing details that prevent a perfect score. For instance, while the problem statement mentions that `resize` is not multi-writer safe, it does not elaborate on the potential consequences or specific scenarios where this could cause issues. Edge cases, such as behavior during concurrent access or failure modes during resizing, are not explicitly addressed. Furthermore, the problem statement lists a large number of related PRs and issues, but it lacks a concise summary of how these relate to the current task or what specific dependencies exist. This could make it harder for a developer to prioritize or understand the full scope without diving into external resources. Overall, the statement is clear on the \"what\" and \"why,\" but lacks some depth on the \"how\" for edge cases and operational constraints.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors across the evaluation criteria:\n\n1. **Scope and Depth of Code Changes**: The code changes provided in the diff are relatively small and focused, primarily toggling a feature flag default to enabled for the new `shape` functionality in Python and R CI workflows and internal code. However, the problem statement implies a much broader scope of work, as evidenced by the extensive list of merged PRs (over 100) related to this feature. This suggests that the actual implementation spans multiple modules, languages (Python, R, C++), and layers of the TileDB-SOMA stack (core, API, IO). The changes impact critical components like array creation, reading/writing, and resizing, which likely affect the system's architecture and require coordination across different parts of the codebase. While the specific diff provided is minor, the overall feature development is substantial.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding several advanced concepts, including TileDB's core storage mechanisms (e.g., `domain`, `non_empty_domain`), array schemas, and version compatibility. Developers must handle language-specific bindings (Python via Pybind11, R, C++), manage feature flags for transitional periods, and ensure backward compatibility with older arrays. Additionally, the problem involves domain-specific knowledge of single-cell data analysis (e.g., handling `obs` and `var` counts in experiments) and familiarity with scientific computing conventions (e.g., SciPy's `shape` concept). The need to deprecate old accessors (`used_shape`) and introduce new ones (`maxshape`, `resize`) adds to the conceptual load, as does ensuring bounds-checking at the core level.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement explicitly mentions several edge cases, such as ensuring `resize` does not allow downsizing below existing data, enforcing bounds-checking only on new arrays, and handling compatibility with old arrays via schema versioning. Error handling is critical, with specific exceptions (`ValueError`, `IndexError`, `NotImplementedError`) to be raised under defined conditions. The lack of multi-writer safety for `resize` introduces additional complexity, as developers must document and possibly mitigate race conditions or data corruption risks. These edge cases are non-trivial and require careful design to avoid user-facing issues.\n\n4. **Overall Complexity**: While the specific code change in the diff (flipping a feature flag default) is straightforward, the broader context of the problem indicates a high level of difficulty. Implementing and testing the `shape` feature across multiple languages and array types, ensuring compatibility, and managing a phased deprecation cycle (TileDB-SOMA 1.13 to 1.15) demand deep knowledge of the codebase and significant experience. The problem also involves user guidance (e.g., notebooks, documentation), which adds to the workload. The sheer volume of related PRs and issues suggests a long-term, complex effort with potential for subtle bugs or performance impacts during resizing or bounds-checking.\n\nIn summary, while the provided code diff is simple, the overall problem of introducing and stabilizing the `shape` feature is hard due to its architectural impact, the breadth of technical concepts involved, and the need to handle complex edge cases. A score of 0.75 reflects the challenge of coordinating this feature across a large, multi-language codebase while maintaining robustness and compatibility.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "from_croco() enhancement; correct conversion from z to sigma grid\nIn Parcels v3.1.0, we implemented support for CROCO sigma grids. We assumed that the conversion from the depth-coordinate (z) to the terrain-following coordinate (sigma) was simply `sigma(t, y, x) = z(t, y, x) / H(y, x)`, where `H` is the local seafloor depth. \r\n\r\nHowever, following the discussion in https://github.com/OceanParcels/Parcels/discussions/1752, we now realise that this conversion is a bit more complex, and requires not only the free-surface field (`eta`), but also a a critical depth controlling the stretching `depth_c` , and a dimensionless vertical coordinate stretching function `C[zi]` as a function of vertical grid\r\n\r\nSee http://cfconventions.org/cf-conventions/cf-conventions.html#_ocean_s_coordinate_generic_form_1 for detailed information about this conversion\r\n\r\nIt would be good to implement this conversion under the hood in Parcels too, so that the mapping between `sigma` and `z` is more in line with CROCO numerics\n", "patch": "", "instance_id": "OceanParcels__Parcels-1772", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of enhancing the `from_croco()` function to correct the conversion from z to sigma grid in Parcels v3.1.0. It provides context about the incorrect assumption made in the initial implementation and references a discussion and external documentation for further details on the correct conversion formula. The mention of required variables like `eta`, `depth_c`, and the stretching function `C[zi]` adds some specificity to the requirements. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem does not explicitly define the expected input and output formats for the conversion logic, nor does it mention specific edge cases or constraints (e.g., handling invalid or missing data for `eta` or `H`). Additionally, while the external link to the CF conventions is helpful, the problem statement could benefit from summarizing the key aspects of the conversion formula directly. Overall, the statement is valid and clear but lacks some critical details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code changes, while not fully detailed in the provided snippet (as no actual code changes are included), is implied to involve modifying or adding logic to the `from_croco()` function or related components in the Parcels library. This likely requires understanding and updating a specific module, potentially affecting how data is processed across the codebase, though it may not impact the overall system architecture significantly. Second, the technical concepts involved include domain-specific knowledge of oceanographic numerical models (CROCO sigma grids), understanding of terrain-following coordinate systems, and familiarity with the mathematical formulation of the conversion as described in the CF conventions. Implementing this correctly would require translating a complex formula involving multiple variables (`eta`, `depth_c`, `C[zi]`, and `H`) into code, which adds to the complexity. Third, while edge cases are not explicitly mentioned in the problem statement, handling potential issues such as invalid or missing data, numerical stability in the conversion, or boundary conditions in the grid would likely be necessary, adding moderate complexity to error handling. Finally, as a senior engineer, I estimate that solving this problem requires a deep understanding of both the Parcels codebase and the domain-specific logic, placing it above medium difficulty but not at the extreme end of very hard, as it does not appear to involve system-level redesign or highly intricate distributed systems concepts. A score of 0.65 reflects the need for specialized knowledge and careful implementation, balanced by the relatively focused scope of the change.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
