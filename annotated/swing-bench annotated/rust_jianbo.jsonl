{"problem_statement": "`forge test --gas-report` Omits Fallback with Calldata\n### Component\n\nForge\n\n### Have you ensured that all of these are up to date?\n\n- [x] Foundry\n- [x] Foundryup\n\n### What version of Foundry are you on?\n\nforge Version: 1.0.0-nightly Commit SHA: db3d9fc95398450dbed83d4841042c62c155bcfc Build Timestamp: 2025-02-11T00:21:38.355325000Z (1739233298) Build Profile: maxperf\n\n### What version of Foundryup are you on?\n\nfoundryup: 1.0.0\n\n### What command(s) is the bug in?\n\nforge test\n\n### Operating System\n\nmacOS (Apple Silicon)\n\n### Describe the bug\n\n`forge test --gas-report` doesn't include gas usage for fallback functions invoked *with* calldata (e.g., `address(contract).call(\"hello\")`), but *does* include fallback functions invoked *without* calldata (e.g., `address(contract).call(\"\")`).  This makes gas reporting inaccurate.\n\n```solidity\n// SPDX-License-Identifier: UNLICENSED\npragma solidity ^0.8.13;\n\ncontract Counter {\n    uint256 public number;\n\n    function setNumber(uint256 newNumber) public {\n        number = newNumber;\n    }\n\n    function increment() public {\n        number++;\n    }\n\n    fallback() external {\n        number++;\n    }\n}\n```\n\n```solidity\n// SPDX-License-Identifier: UNLICENSED\npragma solidity ^0.8.13;\n\nimport {Test, console} from \"forge-std/Test.sol\";\nimport {Counter} from \"../src/Counter.sol\";\n\ncontract CounterTest is Test {\n    Counter public counter;\n\n    function setUp() public {\n        counter = new Counter();\n        // counter.setNumber(0);\n    }\n\n    function test_Fallback() external {\n        (bool success,) = address(counter).call(\"hello\");\n        require(success);\n    }\n}\n\n```\n\n", "patch": "diff --git a/crates/evm/traces/src/decoder/mod.rs b/crates/evm/traces/src/decoder/mod.rs\nindex f904bb46ecc5..d96e8d5dc6c8 100644\n--- a/crates/evm/traces/src/decoder/mod.rs\n+++ b/crates/evm/traces/src/decoder/mod.rs\n@@ -366,7 +366,12 @@ impl CallTraceDecoder {\n             let [func, ..] = &functions[..] else {\n                 return DecodedCallTrace {\n                     label,\n-                    call_data: None,\n+                    call_data: self.fallback_contracts.get(&trace.address).map(|_| {\n+                        DecodedCallData {\n+                            signature: \"fallback()\".to_string(),\n+                            args: vec![cdata.to_string()],\n+                        }\n+                    }),\n                     return_data: self.default_return_data(trace),\n                 };\n             };\n@@ -376,7 +381,7 @@ impl CallTraceDecoder {\n             let mut call_data = self.decode_function_input(trace, func);\n             if let Some(fallback_functions) = self.fallback_contracts.get(&trace.address) {\n                 if !fallback_functions.contains(&func.signature()) {\n-                    call_data.signature = \"fallback()\".into();\n+                    call_data.signature = \"fallback()\".to_string();\n                 }\n             }\n \n@@ -388,7 +393,8 @@ impl CallTraceDecoder {\n         } else {\n             let has_receive = self.receive_contracts.contains(&trace.address);\n             let signature =\n-                if cdata.is_empty() && has_receive { \"receive()\" } else { \"fallback()\" }.into();\n+                if cdata.is_empty() && has_receive { \"receive()\" } else { \"fallback()\" }\n+                    .to_string();\n             let args = if cdata.is_empty() { Vec::new() } else { vec![cdata.to_string()] };\n             DecodedCallTrace {\n                 label,\n", "instance_id": "foundry-rs__foundry-9859", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue with `forge test --gas-report` not including gas usage for fallback functions invoked with calldata. It provides a specific example with Solidity code to demonstrate the bug, which helps in understanding the context and reproducing the issue. The goal is evident: to ensure gas reporting includes fallback functions with calldata. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior for edge cases (e.g., malformed calldata or specific fallback function implementations) and no detailed constraints or requirements for the fix. Additionally, the problem statement does not specify whether the fix should account for other related features or interactions in the Foundry toolset. Overall, it is clear enough to understand the issue but misses some finer details that could impact the solution's completeness.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively narrow, confined to a single file (`decoder/mod.rs`) and a specific part of the logic related to decoding call traces in the Foundry EVM tracing module. The changes involve modifying how fallback functions are handled and reported, which requires understanding the existing logic for decoding call data and the distinction between `fallback()` and `receive()` functions in Solidity. The technical concepts involved include Rust programming, familiarity with EVM tracing, and understanding Solidity contract behavior, which are moderately complex but not overly advanced for someone with experience in blockchain development or low-level EVM tools. The code changes themselves are small (a few lines) and do not impact the broader system architecture significantly. However, there is a need to consider potential edge cases, such as ensuring the modified logic does not break existing behavior for `receive()` calls or other fallback scenarios, though these are not explicitly mentioned in the problem statement. Overall, this problem requires a moderate level of understanding and careful modification, placing it at the lower end of the medium difficulty range (0.45).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.45}
{"problem_statement": "Fungible and fungibles adapters should allow account death like burns and mints\nThe fungible and fungibles adapters set the preservation of the burn operation to `Expendable`: [here](https://github.com/paritytech/polkadot-sdk/blob/f3ab3854e1df9e0498599f01ba4f9f152426432a/polkadot/xcm/xcm-builder/src/fungible_adapter.rs#L217) and [here](https://github.com/paritytech/polkadot-sdk/blob/f3ab3854e1df9e0498599f01ba4f9f152426432a/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs#L303).\r\nThis allows accounts to transfer all their funds (and getting reaped because of it) out of their account.\r\nHowever, this only works if they're using the `WithdrawAsset` and `DepositAsset` instructions, which correspond to `burn_from` and `mint_into`.\r\nIf `TransferAsset` is used, as is the case with [limited_reserve_assets_transfer](https://github.com/paritytech/polkadot-sdk/blob/f3ab3854e1df9e0498599f01ba4f9f152426432a/polkadot/xcm/pallet-xcm/src/lib.rs#L1881), then the [`transfer` function will be called with `Preserve`](https://github.com/paritytech/polkadot-sdk/blob/f3ab3854e1df9e0498599f01ba4f9f152426432a/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs#L61).\r\n\r\nThis distinction is arbitrary and we should change the way `TransferAsset` is handled to also allow transferring all the funds of an account. It results in confusing errors for users.\r\nIf needed, we could add a [Hint](https://github.com/paritytech/polkadot-sdk/blob/f3ab3854e1df9e0498599f01ba4f9f152426432a/polkadot/xcm/src/v5/mod.rs#L1145) for changing the default of ALL operations from `Expendable` to `Preserve`.\r\n\n", "patch": "diff --git a/polkadot/xcm/xcm-builder/src/fungible_adapter.rs b/polkadot/xcm/xcm-builder/src/fungible_adapter.rs\nindex 25a705a39eb73..2da772deb0ed7 100644\n--- a/polkadot/xcm/xcm-builder/src/fungible_adapter.rs\n+++ b/polkadot/xcm/xcm-builder/src/fungible_adapter.rs\n@@ -20,11 +20,7 @@ use super::MintLocation;\n use core::{marker::PhantomData, result};\n use frame_support::traits::{\n \ttokens::{\n-\t\tfungible,\n-\t\tFortitude::Polite,\n-\t\tPrecision::Exact,\n-\t\tPreservation::{Expendable, Preserve},\n-\t\tProvenance::Minted,\n+\t\tfungible, Fortitude::Polite, Precision::Exact, Preservation::Expendable, Provenance::Minted,\n \t},\n \tGet,\n };\n@@ -64,7 +60,7 @@ impl<\n \t\t\t.ok_or(MatchError::AccountIdConversionFailed)?;\n \t\tlet dest = AccountIdConverter::convert_location(to)\n \t\t\t.ok_or(MatchError::AccountIdConversionFailed)?;\n-\t\tFungible::transfer(&source, &dest, amount, Preserve)\n+\t\tFungible::transfer(&source, &dest, amount, Expendable)\n \t\t\t.map_err(|error| XcmError::FailedToTransactAsset(error.into()))?;\n \t\tOk(what.clone().into())\n \t}\ndiff --git a/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs b/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs\nindex a259afc6e6825..59b4ccb13d0c5 100644\n--- a/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs\n+++ b/polkadot/xcm/xcm-builder/src/fungibles_adapter.rs\n@@ -19,10 +19,7 @@\n use core::{marker::PhantomData, result};\n use frame_support::traits::{\n \ttokens::{\n-\t\tfungibles,\n-\t\tFortitude::Polite,\n-\t\tPrecision::Exact,\n-\t\tPreservation::{Expendable, Preserve},\n+\t\tfungibles, Fortitude::Polite, Precision::Exact, Preservation::Expendable,\n \t\tProvenance::Minted,\n \t},\n \tContains, Get,\n@@ -58,7 +55,7 @@ impl<\n \t\t\t.ok_or(MatchError::AccountIdConversionFailed)?;\n \t\tlet dest = AccountIdConverter::convert_location(to)\n \t\t\t.ok_or(MatchError::AccountIdConversionFailed)?;\n-\t\tAssets::transfer(asset_id, &source, &dest, amount, Preserve)\n+\t\tAssets::transfer(asset_id, &source, &dest, amount, Expendable)\n \t\t\t.map_err(|e| XcmError::FailedToTransactAsset(e.into()))?;\n \t\tOk(what.clone().into())\n \t}\ndiff --git a/prdoc/pr_7243.prdoc b/prdoc/pr_7243.prdoc\nnew file mode 100644\nindex 0000000000000..c04017c29ce43\n--- /dev/null\n+++ b/prdoc/pr_7243.prdoc\n@@ -0,0 +1,13 @@\n+# Schema: Polkadot SDK PRDoc Schema (prdoc) v1.0.0\n+# See doc at https://raw.githubusercontent.com/paritytech/polkadot-sdk/master/prdoc/schema_user.json\n+\n+title: 'transfer function Preservation is changed to Expendable in fungible and fungibles adapter'\n+\n+doc:\n+  - audience: Runtime Dev\n+    description: |\n+      The Preservation of transfer method of fungible and fungibles adapters is changed from Preserve to Expendable. So the behavior of the TransferAsset will be consistent with the WithdrawAsset function, as in fungible and fungibles adapter.\n+\n+crates: \n+- name: staging-xcm-builder\n+  bump: patch\n\\ No newline at end of file\n", "instance_id": "paritytech__polkadot-sdk-7526", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue and the desired outcome. It explains the discrepancy in behavior between `TransferAsset` and other operations like `WithdrawAsset` and `DepositAsset` in the context of fungible and fungibles adapters. The goal of allowing accounts to transfer all funds (and potentially be reaped) using `TransferAsset` is evident, and links to specific code locations in the repository provide helpful context. However, there are minor ambiguities: the statement does not explicitly define what \"account death\" or \"reaping\" entails in technical terms, which could be unclear to someone unfamiliar with the domain. Additionally, while a potential solution (adding a `Hint` for changing defaults) is mentioned, it is not elaborated upon, leaving some uncertainty about whether this is part of the required change or just a suggestion. Edge cases or potential side effects of changing the preservation mode to `Expendable` are also not discussed, which could be critical for understanding the full scope of the problem. Overall, the statement is valid and clear but lacks some minor details for complete comprehension.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the relatively straightforward nature of the required changes and the limited scope of impact. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are minimal and localized to two files (`fungible_adapter.rs` and `fungibles_adapter.rs`). The modification involves changing a single parameter (`Preserve` to `Expendable`) in the `transfer` function calls in both files. This does not impact the broader system architecture or require extensive refactoring, as it is a targeted adjustment to behavior. The overall amount of code change is very small, and no interaction with other modules or complex dependencies is evident from the diff.\n\n2. **Number of Technical Concepts**: Solving this problem requires a basic understanding of the Polkadot SDK's XCM (Cross-Consensus Messaging) framework and the specific traits (`fungible` and `fungibles`) used for asset handling. Familiarity with the `Preservation` enum and its implications (`Preserve` vs. `Expendable`) is necessary, but these are not particularly advanced concepts for someone with experience in Rust or blockchain development. No complex algorithms, design patterns, or external libraries are involved beyond the existing framework.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases or error conditions to handle, and the code changes do not introduce new error handling logic. However, changing the preservation mode to `Expendable` might implicitly allow accounts to be reaped (deleted) when their balance reaches zero, which could have downstream effects not covered in the diff (e.g., ensuring dependent systems handle account deletion gracefully). While this is a potential concern, it does not appear to be within the scope of the current change, and the problem does not require addressing these edge cases directly.\n\n4. **Overall Complexity**: The task is primarily about aligning the behavior of `TransferAsset` with other operations by modifying a configuration parameter. This does not require deep architectural knowledge of the codebase beyond the specific adapter modules. The change is simple and does not involve performance optimizations, intricate logic, or domain-specific challenges beyond basic familiarity with asset transfers in a blockchain context.\n\nGiven these factors, a difficulty score of 0.30 reflects an \"Easy\" problem that requires understanding some code logic and making a simple modification. It is slightly above the lower end of the range due to the need for contextual knowledge of the Polkadot SDK and XCM, but it remains a straightforward fix for a developer with moderate experience in Rust or blockchain systems.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.3}
{"problem_statement": "MINOR: Add validation to the node_list method in GrpcPlacementService and fill in the test case\n### What type of enhancement is this?\r\n\r\nRefactor\r\n\r\n### What does the enhancement do?\r\n\r\nAdd validation to the node_list method in GrpcPlacementService and fill in the test case. \r\n\r\nTips:\r\nyou can implement the `ValidateExt` trait on `NodeListQuest`, then invoke ` validate_ext ` function to achieve the validation functionality.\r\n\r\n- you can locate the ValidateExt trait in the file: src\\placement-center\\src\\server\\grpc\\validate.rs\r\n- you need to add validation to the node_list method in this file:  src\\placement-center\\src\\server\\grpc\\service_placement.rs\r\n- you need to fill in the test case in this file: src\\grpc-clients\\tests\\placement_test.rs\r\n\r\nReferences\u00a0\r\n- https://github.com/robustmq/robustmq/pull/548)https://github.com/robustmq/robustmq/pull/548\r\n\r\n### Implementation challenges\r\n\r\n_No response_\n", "patch": "diff --git a/src/placement-center/src/server/grpc/service_inner.rs b/src/placement-center/src/server/grpc/service_inner.rs\nindex 3bddadbae..e0d3b104c 100644\n--- a/src/placement-center/src/server/grpc/service_inner.rs\n+++ b/src/placement-center/src/server/grpc/service_inner.rs\n@@ -98,6 +98,9 @@ impl PlacementCenterService for GrpcPlacementService {\n         request: Request<NodeListRequest>,\n     ) -> Result<Response<NodeListReply>, Status> {\n         let req = request.into_inner();\n+\n+        let _ = req.validate_ext()?;\n+\n         let mut nodes = Vec::new();\n \n         for raw in self\ndiff --git a/src/placement-center/src/server/grpc/validate.rs b/src/placement-center/src/server/grpc/validate.rs\nindex 49a5c25b5..3139da1be 100644\n--- a/src/placement-center/src/server/grpc/validate.rs\n+++ b/src/placement-center/src/server/grpc/validate.rs\n@@ -16,8 +16,8 @@ use std::net::{IpAddr, SocketAddr};\n \n use common_base::error::common::CommonError;\n use protocol::placement_center::placement_center_inner::{\n-    ClusterType, DeleteIdempotentDataRequest, GetResourceConfigRequest, RegisterNodeRequest,\n-    SetIdempotentDataRequest, SetResourceConfigRequest, UnRegisterNodeRequest,\n+    ClusterType, DeleteIdempotentDataRequest, GetResourceConfigRequest, NodeListRequest,\n+    RegisterNodeRequest, SetIdempotentDataRequest, SetResourceConfigRequest, UnRegisterNodeRequest,\n };\n use protocol::placement_center::placement_center_mqtt::GetShareSubLeaderRequest;\n use tonic::Status;\n@@ -158,6 +158,13 @@ impl ValidateExt for GetShareSubLeaderRequest {\n     }\n }\n \n+impl ValidateExt for NodeListRequest {\n+    fn validate_ext(&self) -> Result<(), Status> {\n+        ensure_param_not_empty(\"cluster_name\", &self.cluster_name)?;\n+        Ok(())\n+    }\n+}\n+\n #[cfg(test)]\n mod validate_test {\n     use super::*;\n", "instance_id": "robustmq__robustmq-735", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in terms of the goal, which is to add validation to the `node_list` method in `GrpcPlacementService` and to fill in a test case. It provides specific file locations for the implementation and references a related pull request for context. Additionally, it offers a tip on using the `ValidateExt` trait for validation. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes valid input for the `NodeListRequest` (beyond a non-empty `cluster_name` as seen in the code changes). It also lacks details on specific edge cases or error conditions to handle during validation. Furthermore, the requirements for the test case in `placement_test.rs` are not specified, leaving uncertainty about the scope or expected behavior of the test. Overall, while the intent and general approach are clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the following analysis of the factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are minimal and localized to two files: `service_inner.rs` for invoking the validation and `validate.rs` for implementing the `ValidateExt` trait on `NodeListRequest`. The changes involve adding a single validation check for a non-empty `cluster_name` and calling the validation method. There is no indication of broader impact on the system's architecture or interactions with other modules beyond the immediate context of the `node_list` method. The amount of code change is small, consisting of a few lines.\n\n2. **Number of Technical Concepts:** The problem requires understanding of Rust's trait system to implement `ValidateExt` for `NodeListRequest`, as well as familiarity with gRPC and the `tonic` framework for handling `Status` errors. Additionally, it involves basic validation logic (checking for non-empty strings). These concepts are relatively straightforward for someone with moderate Rust experience and do not involve complex algorithms, design patterns, or domain-specific knowledge beyond the provided codebase structure.\n\n3. **Potential Edge Cases and Error Handling:** The code changes show a simple validation for a non-empty `cluster_name`, which suggests minimal complexity in edge cases. There are no explicit mentions of additional edge cases or error conditions in the problem statement, and the provided validation logic does not require sophisticated error handling beyond returning a `Status` error via `tonic`. The problem does not appear to demand extensive consideration of edge cases at this stage.\n\n4. **Overall Complexity:** The task is a straightforward enhancement involving a small refactor to add validation and potentially write a test case (though the test case content is not provided in the diff). It does not require deep understanding of the broader codebase or complex modifications. The primary challenge might be locating and understanding the existing `ValidateExt` trait and ensuring the validation logic aligns with project conventions, but this is not particularly difficult.\n\nGiven these points, the problem falls into the easy category (0.2-0.4), as it requires understanding some code logic and making simple modifications. I\u2019ve assigned a score of 0.25 to reflect that it is on the lower end of this range due to the minimal scope and simplicity of the required changes.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.25}
{"problem_statement": "Unable to register on_open event for combo box\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nI can only register `on_close event, but not `on_open` event.\r\nIt's useful to refresh items when combo box is selected.\r\n\r\nCurrently I can use `on_option_hovered`, but it triggers everytime mouse move over an option\n\n### What is the expected behavior?\n\nComboBox::new().on_open\n\n### Version\n\ncrates.io release\n\n### Operating System\n\nWindows\n\n### Do you have any log output?\n\n_No response_\n", "patch": "diff --git a/widget/src/combo_box.rs b/widget/src/combo_box.rs\nindex 62785b2c39..a51701ca61 100644\n--- a/widget/src/combo_box.rs\n+++ b/widget/src/combo_box.rs\n@@ -41,6 +41,7 @@ pub struct ComboBox<\n     selection: text_input::Value,\n     on_selected: Box<dyn Fn(T) -> Message>,\n     on_option_hovered: Option<Box<dyn Fn(T) -> Message>>,\n+    on_open: Option<Message>,\n     on_close: Option<Message>,\n     on_input: Option<Box<dyn Fn(String) -> Message>>,\n     menu_class: <Theme as menu::Catalog>::Class<'a>,\n@@ -77,6 +78,7 @@ where\n             on_selected: Box::new(on_selected),\n             on_option_hovered: None,\n             on_input: None,\n+            on_open: None,\n             on_close: None,\n             menu_class: <Theme as Catalog>::default_menu(),\n             padding: text_input::DEFAULT_PADDING,\n@@ -104,6 +106,13 @@ where\n         self\n     }\n \n+    /// Sets the message that will be produced when the  [`ComboBox`] is\n+    /// opened.\n+    pub fn on_open(mut self, message: Message) -> Self {\n+        self.on_open = Some(message);\n+        self\n+    }\n+\n     /// Sets the message that will be produced when the outside area\n     /// of the [`ComboBox`] is pressed.\n     pub fn on_close(mut self, message: Message) -> Self {\n@@ -632,15 +641,19 @@ where\n             text_input_state.is_focused()\n         };\n \n-        if started_focused && !is_focused && !published_message_to_shell {\n-            if let Some(message) = self.on_close.take() {\n-                shell.publish(message);\n-            }\n-        }\n-\n-        // Focus changed, invalidate widget tree to force a fresh `view`\n         if started_focused != is_focused {\n+            // Focus changed, invalidate widget tree to force a fresh `view`\n             shell.invalidate_widgets();\n+\n+            if !published_message_to_shell {\n+                if is_focused {\n+                    if let Some(on_open) = self.on_open.take() {\n+                        shell.publish(on_open);\n+                    }\n+                } else if let Some(on_close) = self.on_close.take() {\n+                    shell.publish(on_close);\n+                }\n+            }\n         }\n \n         event_status\n", "instance_id": "iced-rs__iced-2534", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in expressing the user's intent: they want to register an `on_open` event for a combo box in the `iced` framework, similar to the existing `on_close` event, to refresh items when the combo box is opened. The goal is evident, and the issue is tied to a specific feature request. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify under what exact conditions the `on_open` event should trigger (e.g., only on initial focus or every time the combo box gains focus). Additionally, there are no examples or detailed use cases provided to clarify the expected behavior beyond a brief mention of refreshing items. Edge cases, such as repeated opening/closing or interaction with other events, are not addressed. Despite these minor gaps, the intent and context are sufficiently clear for a developer familiar with UI frameworks to understand the request.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes are confined to a single file (`combo_box.rs`) and involve straightforward modifications to the `ComboBox` struct and its associated logic. The changes include adding a new field (`on_open`), a setter method for it, and updating the event handling logic to publish a message when the combo box gains focus. The modifications are localized and do not impact the broader system architecture or require understanding complex interactions across multiple modules. The amount of code change is minimal, consisting of a few lines to mirror the existing `on_close` functionality.\n\n2. **Number of Technical Concepts:** Solving this problem requires basic familiarity with Rust (struct definitions, ownership, and closures) and a general understanding of event-driven UI frameworks like `iced`. The concepts involved\u2014adding a new event handler and publishing a message\u2014are not complex and are similar to the existing `on_close` implementation. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond basic UI event handling.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases, and the code changes do not introduce significant error handling logic. The implementation mirrors the existing `on_close` behavior, which suggests that any edge cases (e.g., repeated focus changes) are likely handled by the framework's existing mechanisms. The developer does not need to address complex edge cases beyond ensuring the event triggers appropriately on focus gain.\n\n4. **Overall Complexity:** The task is a simple feature addition that builds directly on existing functionality. It requires minimal debugging or testing beyond verifying that the `on_open` event triggers as expected. A developer with basic to intermediate experience in Rust and UI frameworks could implement this change with little effort.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only a basic understanding of the codebase and straightforward code modifications.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.25}
{"problem_statement": "tiny_skia backend panics if you have an Combobox menu with zero search results\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\n- Run the Combobox example with tiny-skia backend\r\n```sh\r\nICED_BACKEND=tiny-skia cargo r\r\n```\r\n- Write \"ee\" into the combobox text field\r\n- The application panics with:\r\n```\r\nthread 'main' panicked at tiny_skia/src/backend.rs:162:17:\r\nQuad with non-normal height!\r\n```\n\n### What is the expected behavior?\n\nThe application should not panic even if there are no results in the combobox menu.\n\n### Version\n\nmaster\n\n### Operating System\n\nLinux\n\n### Do you have any log output?\n\n```shell\n> RUST_BACKTRACE=1 ICED_BACKEND=tiny-skia cargo r\r\n    Finished dev [unoptimized + debuginfo] target(s) in 0.11s\r\n     Running `/local/home/blazekra/data/git/iced/target/debug/combo_box`\r\nthread 'main' panicked at tiny_skia/src/backend.rs:162:17:\r\nQuad with non-normal height!\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/std/src/panicking.rs:645:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/panicking.rs:72:14\r\n   2: iced_tiny_skia::backend::Backend::draw_primitive\r\n             at /local/home/blazekra/data/git/iced/tiny_skia/src/backend.rs:162:17\r\n   3: iced_tiny_skia::backend::Backend::draw_primitive\r\n             at /local/home/blazekra/data/git/iced/tiny_skia/src/backend.rs:700:21\r\n   4: iced_tiny_skia::backend::Backend::draw_primitive\r\n             at /local/home/blazekra/data/git/iced/tiny_skia/src/backend.rs:748:21\r\n   5: iced_tiny_skia::backend::Backend::draw\r\n             at /local/home/blazekra/data/git/iced/tiny_skia/src/backend.rs:103:17\r\n   6: iced_tiny_skia::window::compositor::present\r\n             at /local/home/blazekra/data/git/iced/tiny_skia/src/window/compositor.rs:202:5\r\n   7: <iced_renderer::compositor::Compositor as iced_graphics::compositor::Compositor>::present::{{closure}}\r\n             at /local/home/blazekra/data/git/iced/renderer/src/compositor.rs:118:17\r\n   8: iced_graphics::renderer::Renderer<B>::with_primitives\r\n             at /local/home/blazekra/data/git/iced/graphics/src/renderer.rs:56:9\r\n   9: <iced_renderer::compositor::Compositor as iced_graphics::compositor::Compositor>::present\r\n             at /local/home/blazekra/data/git/iced/renderer/src/compositor.rs:117:18\r\n  10: iced_winit::application::run_instance::{{closure}}\r\n             at /local/home/blazekra/data/git/iced/winit/src/application.rs:458:23\r\n  11: iced_winit::application::run::{{closure}}\r\n             at /local/home/blazekra/data/git/iced/winit/src/application.rs:228:24\r\n  12: core::ops::function::impls::<impl core::ops::function::FnMut<A> for &mut F>::call_mut\r\n             at /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/ops/function.rs:294:13\r\n  13: core::ops::function::impls::<impl core::ops::function::FnMut<A> for &mut F>::call_mut\r\n             at /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/ops/function.rs:294:13\r\n  14: winit::platform_impl::platform::wayland::event_loop::EventLoop<T>::single_iteration\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/wayland/event_loop/mod.rs:504:17\r\n  15: winit::platform_impl::platform::wayland::event_loop::EventLoop<T>::poll_events_with_timeout\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/wayland/event_loop/mod.rs:323:9\r\n  16: winit::platform_impl::platform::wayland::event_loop::EventLoop<T>::pump_events\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/wayland/event_loop/mod.rs:237:13\r\n  17: winit::platform_impl::platform::wayland::event_loop::EventLoop<T>::run_on_demand\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/wayland/event_loop/mod.rs:201:19\r\n  18: winit::platform_impl::platform::EventLoop<T>::run_on_demand\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/mod.rs:821:56\r\n  19: winit::platform_impl::platform::EventLoop<T>::run\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/platform_impl/linux/mod.rs:814:9\r\n  20: winit::event_loop::EventLoop<T>::run\r\n             at /local/home/blazekra/.cargo/git/checkouts/winit-57d3141eaf559308/b91e39e/src/event_loop.rs:249:9\r\n  21: iced_winit::application::run\r\n             at /local/home/blazekra/data/git/iced/winit/src/application.rs:243:13\r\n  22: iced::application::Application::run\r\n             at /local/home/blazekra/data/git/iced/src/application.rs:208:12\r\n  23: iced::sandbox::Sandbox::run\r\n             at /local/home/blazekra/data/git/iced/src/sandbox.rs:153:9\r\n  24: combo_box::main\r\n             at ./src/main.rs:7:5\r\n  25: core::ops::function::FnOnce::call_once\r\n             at /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/ops/function.rs:250:5\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n```\n\n", "patch": "diff --git a/widget/src/combo_box.rs b/widget/src/combo_box.rs\nindex e4f4a41fb1..253850dffe 100644\n--- a/widget/src/combo_box.rs\n+++ b/widget/src/combo_box.rs\n@@ -700,38 +700,47 @@ where\n                 ..\n             } = tree.state.downcast_mut::<Menu<T>>();\n \n-            let bounds = layout.bounds();\n-\n             self.state.sync_filtered_options(filtered_options);\n \n-            let mut menu = menu::Menu::new(\n-                menu,\n-                &filtered_options.options,\n-                hovered_option,\n-                |x| {\n-                    tree.children[0]\n-                        .state\n-                        .downcast_mut::<text_input::State<Renderer::Paragraph>>(\n-                        )\n-                        .unfocus();\n-\n-                    (self.on_selected)(x)\n-                },\n-                self.on_option_hovered.as_deref(),\n-                &self.menu_class,\n-            )\n-            .width(bounds.width)\n-            .padding(self.padding);\n-\n-            if let Some(font) = self.font {\n-                menu = menu.font(font);\n-            }\n+            if filtered_options.options.is_empty() {\n+                None\n+            } else {\n+                let bounds = layout.bounds();\n+\n+                let mut menu = menu::Menu::new(\n+                    menu,\n+                    &filtered_options.options,\n+                    hovered_option,\n+                    |x| {\n+                        tree.children[0]\n+                    .state\n+                    .downcast_mut::<text_input::State<Renderer::Paragraph>>(\n+                    )\n+                    .unfocus();\n+\n+                        (self.on_selected)(x)\n+                    },\n+                    self.on_option_hovered.as_deref(),\n+                    &self.menu_class,\n+                )\n+                .width(bounds.width)\n+                .padding(self.padding);\n+\n+                if let Some(font) = self.font {\n+                    menu = menu.font(font);\n+                }\n \n-            if let Some(size) = self.size {\n-                menu = menu.text_size(size);\n-            }\n+                if let Some(size) = self.size {\n+                    menu = menu.text_size(size);\n+                }\n \n-            Some(menu.overlay(layout.position() + translation, bounds.height))\n+                Some(\n+                    menu.overlay(\n+                        layout.position() + translation,\n+                        bounds.height,\n+                    ),\n+                )\n+            }\n         } else {\n             None\n         }\n", "instance_id": "iced-rs__iced-2364", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a panic occurs in the tiny_skia backend when a Combobox menu has zero search results. It provides specific steps to reproduce the issue, including the command to run and the input (\"ee\") that triggers the panic, along with a detailed stack trace. The expected behavior (no panic with zero results) is also explicitly stated. However, there are minor ambiguities, such as the lack of detailed context about the tiny_skia backend's interaction with the Combobox widget or specific constraints on how the fix should be implemented. Additionally, edge cases beyond the zero-results scenario are not mentioned, which could be relevant for a comprehensive solution. Overall, the statement is valid and clear but misses some minor details that could aid in fully understanding the problem's scope.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a localized change within a single file (`combo_box.rs`) and a specific function. The modification involves adding a conditional check to return `None` when there are no filtered options, preventing the rendering of a menu that could cause a panic. The change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is relatively small, focusing on wrapping existing logic in an `if-else` block.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Rust's control flow (conditionals), familiarity with the codebase's widget rendering logic, and an awareness of how the tiny_skia backend interacts with the UI components. No advanced algorithms, design patterns, or domain-specific knowledge are needed. The primary concept is straightforward: avoid rendering a menu when there are no options to display.\n\n3. **Edge Cases and Error Handling**: The problem statement highlights a specific edge case (zero search results), and the code change directly addresses it by skipping menu creation in this scenario. No additional complex error handling or unforeseen edge cases are apparent from the problem description or code diff. The fix is a simple guard clause, reducing the likelihood of introducing new issues.\n\n4. **Overall Complexity**: The issue does not require deep knowledge of the entire codebase or intricate debugging beyond identifying the panic condition. The stack trace provided in the problem statement points directly to the rendering logic in `tiny_skia`, and the fix aligns with a logical check to prevent rendering under problematic conditions. This makes the problem approachable for someone with moderate familiarity with Rust and UI rendering concepts.\n\nA score of 0.30 reflects that while the problem is not trivial (it requires understanding the context of the panic and the widget's state), it is still a relatively straightforward bug fix that doesn't demand advanced expertise or extensive modifications. It sits comfortably in the \"Easy\" category, as it involves simple logic modification within a constrained scope.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.3}
{"problem_statement": "Short-hand model configuration doesn't work if there aren't any models explicitly defined\nThe following `tensorzero.toml`...\r\n\r\n```toml\r\n[functions.chatbot]\r\ntype = \"chat\"\r\n\r\n[functions.chatbot.variants.openai]\r\ntype = \"chat_completion\"\r\nmodel = \"openai::gpt-4o-mini-2024-07-18\"\r\n```\r\n\r\n...leads to:\r\n\r\n```\r\ngateway-1  | {\"timestamp\":\"2024-12-26T04:46:24.615350Z\",\"level\":\"ERROR\",\"message\":\"missing field `models`\\n\",\"spans\":[{\"name\":\"load\"}]}\r\ngateway-1  | {\"timestamp\":\"2024-12-26T04:46:24.615395Z\",\"level\":\"ERROR\",\"message\":\"Failed to load config: missing field `models`\\n\"}\r\n```\r\n\r\nThe following `tensorzero.toml` works.\r\n\r\n```toml\r\n[models.not_used]\r\nrouting = [\"openai\"]\r\n\r\n[models.not_used.providers.openai]\r\ntype = \"openai\"\r\nmodel_name = \"not_used\"\r\n\r\n[functions.chatbot]\r\ntype = \"chat\"\r\n\r\n[functions.chatbot.variants.openai]\r\ntype = \"chat_completion\"\r\nmodel = \"openai::gpt-4o-mini-2024-07-18\"\r\n```\r\n\n", "patch": "diff --git a/gateway/src/config_parser.rs b/gateway/src/config_parser.rs\nindex b3a65233b..4b29fad65 100644\n--- a/gateway/src/config_parser.rs\n+++ b/gateway/src/config_parser.rs\n@@ -277,6 +277,7 @@ impl<'c> Config<'c> {\n #[serde(deny_unknown_fields)]\n struct UninitializedConfig {\n     pub gateway: Option<GatewayConfig>,\n+    #[serde(default)]\n     pub models: ModelTable, // model name => model config\n     #[serde(default)]\n     pub embedding_models: HashMap<String, EmbeddingModelConfig>, // embedding model name => embedding model config\n@@ -664,10 +665,16 @@ mod tests {\n             .remove(\"models\")\n             .expect(\"Failed to remove `[models]` section\");\n \n+        // Remove all functions except generate_draft so we are sure what error will be thrown\n+        config[\"functions\"]\n+            .as_table_mut()\n+            .unwrap()\n+            .retain(|k, _| k == \"generate_draft\");\n+\n         assert_eq!(\n             Config::load_from_toml(config, base_path).unwrap_err(),\n             Error::new(ErrorDetails::Config {\n-                message: \"missing field `models`\\n\".to_string()\n+                message: \"Model name 'gpt-3.5-turbo' not found in model table\".to_string()\n             })\n         );\n     }\n@@ -1332,6 +1339,40 @@ mod tests {\n         assert_eq!(templates.len(), 10);\n     }\n \n+    #[test]\n+    fn test_config_load_shorthand_models_only() {\n+        let config_str = r#\"\n+        # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+        # \u2502                                  GENERAL                                   \u2502\n+        # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+\n+        [gateway]\n+        bind_address = \"0.0.0.0:3000\"\n+\n+\n+        # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+        # \u2502                                 FUNCTIONS                                  \u2502\n+        # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+\n+        [functions.generate_draft]\n+        type = \"chat\"\n+        system_schema = \"fixtures/config/functions/generate_draft/system_schema.json\"\n+\n+        [functions.generate_draft.variants.openai_promptA]\n+        type = \"chat_completion\"\n+        weight = 0.9\n+        model = \"openai::gpt-3.5-turbo\"\n+        system_template = \"fixtures/config/functions/generate_draft/promptA/system_template.minijinja\"\n+        \"#;\n+        env::set_var(\"OPENAI_API_KEY\", \"sk-something\");\n+        env::set_var(\"ANTHROPIC_API_KEY\", \"sk-something\");\n+        env::set_var(\"AZURE_OPENAI_API_KEY\", \"sk-something\");\n+\n+        let config = toml::from_str(config_str).expect(\"Failed to parse sample config\");\n+        let base_path = PathBuf::from(env!(\"CARGO_MANIFEST_DIR\"));\n+        Config::load_from_toml(config, base_path.clone()).expect(\"Failed to load config\");\n+    }\n+\n     /// Get a sample valid config for testing\n     fn get_sample_valid_config() -> toml::Table {\n         let config_str = r#\"\n", "instance_id": "tensorzero__tensorzero-617", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a shorthand model configuration in a `tensorzero.toml` file fails to work if no models are explicitly defined, resulting in an error about a missing `models` field. The provided examples of failing and working configurations help illustrate the problem. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define what constitutes a \"shorthand model configuration\" or provide a comprehensive explanation of the expected behavior when models are not defined. Additionally, edge cases or constraints (e.g., whether this applies to all model types or specific providers) are not mentioned. While the intent is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to a single file (`config_parser.rs`) and involve relatively small modifications. The diff shows updates to a struct field with a `#[serde(default)]` attribute to handle missing `models` and additions to test cases to validate the new behavior. There is no indication of widespread impact on the codebase or architectural changes, making the scope manageable.\n\n2. **Technical Concepts Involved**: Solving this requires understanding of Rust's `serde` library for serialization/deserialization, specifically how default values are handled with `#[serde(default)]`. Additionally, familiarity with TOML parsing and basic error handling in Rust is needed. These concepts are not overly complex for an intermediate Rust developer, though they do require some specific knowledge of the language's ecosystem.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest a focus on handling the absence of a `models` field gracefully. The test case added (`test_config_load_shorthand_models_only`) ensures the configuration loads without explicit model definitions, indicating some attention to error handling. However, the complexity of these edge cases appears minimal, as the solution primarily involves setting a default empty value.\n\n4. **Overall Complexity**: The problem requires understanding a specific part of the configuration loading logic and making a targeted fix. It does not involve deep architectural changes, complex algorithms, or extensive domain-specific knowledge beyond configuration parsing. The test modifications also suggest a straightforward validation of the fix.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires some understanding of Rust and `serde` but does not pose significant challenges in terms of codebase impact or conceptual depth.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "FormatWith should implement fmt::Debug\nThe value returned by `format_with` should implement `std::fmt::Debug`. Similar to the value returned by `std::format_args`, the Debug representation should be the same as the Display one. This allows using it in places like `std::fmt::DebugStruct`.\n", "patch": "diff --git a/src/format.rs b/src/format.rs\nindex c4cb65dcb..83382c5a5 100644\n--- a/src/format.rs\n+++ b/src/format.rs\n@@ -71,6 +71,16 @@ where\n     }\n }\n \n+impl<'a, I, F> fmt::Debug for FormatWith<'a, I, F>\n+where\n+    I: Iterator,\n+    F: FnMut(I::Item, &mut dyn FnMut(&dyn fmt::Display) -> fmt::Result) -> fmt::Result,\n+{\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        fmt::Display::fmt(self, f)\n+    }\n+}\n+\n impl<'a, I> Format<'a, I>\n where\n     I: Iterator,\n", "instance_id": "rust-itertools__itertools-931", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it specifies that the `FormatWith` type should implement the `fmt::Debug` trait in Rust, and that its `Debug` representation should match its `Display` representation, similar to `std::format_args`. The goal is understandable, and the context of usage (e.g., integration with `std::fmt::DebugStruct`) is provided. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly mention any constraints or specific edge cases to handle when implementing `Debug`. Additionally, it lacks examples of expected input/output behavior or potential pitfalls in ensuring `Debug` and `Display` representations are identical. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The code change provided is minimal and localized to a single file (`format.rs`), involving the addition of a straightforward implementation of the `fmt::Debug` trait for the `FormatWith` type. The implementation simply delegates to the existing `fmt::Display` implementation, which requires only basic knowledge of Rust's trait system and formatting traits (`Debug` and `Display`). The scope of the change is small, with no impact on the broader codebase architecture or interactions between modules. The technical concepts involved are limited to understanding Rust's trait implementations and the relationship between `Debug` and `Display`, which are fundamental and not particularly complex. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the provided code change does not introduce or modify any error handling logic. Overall, this task requires minimal effort and a basic-to-intermediate understanding of Rust, making it an easy problem to solve.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.25}
{"problem_statement": "New `MessageType` documented\nhttps://github.com/discord/discord-api-docs/commit/e5cd463cd1412df2e55fce8ecb81e7d1ed5755b0 documented new Message Types:\r\n\r\n```md\r\n| GUILD_APPLICATION_PREMIUM_SUBSCRIPTION       | 32    | true      |\r\n| GUILD_INCIDENT_ALERT_MODE_ENABLED            | 36    | true      |\r\n| GUILD_INCIDENT_ALERT_MODE_DISABLED           | 37    | true      |\r\n| GUILD_INCIDENT_REPORT_RAID                   | 38    | true      |\r\n| GUILD_INCIDENT_REPORT_FALSE_ALARM            | 39    | true      |\r\n```\r\n\r\nAnd messages of the type `GUILD_APPLICATION_PREMIUM_SUBSCRIPTION` can now be deleted.\r\n\r\nThis affects [`src/model/channel/message.rs#972`](https://github.com/serenity-rs/serenity/blob/current/src/model/channel/message.rs#L972)\n", "patch": "diff --git a/src/model/channel/message.rs b/src/model/channel/message.rs\nindex 60a76459074..b522acd73b7 100644\n--- a/src/model/channel/message.rs\n+++ b/src/model/channel/message.rs\n@@ -1026,6 +1026,10 @@ enum_number! {\n         StageSpeaker = 29,\n         StageTopic = 31,\n         GuildApplicationPremiumSubscription = 32,\n+        GuildIncidentAlertModeEnabled = 36,\n+        GuildIncidentAlertModeDisabled = 37,\n+        GuildIncidentReportRaid = 38,\n+        GuildIncidentReportFalseAlarm = 39,\n         _ => Unknown(u8),\n     }\n }\n", "instance_id": "serenity-rs__serenity-2891", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in terms of intent: it documents the addition of new message types to an existing enum in a Rust codebase, as per updates in the Discord API documentation. It provides specific details about the new message types (names and numeric values) and references the exact location in the codebase that needs modification. However, there are minor ambiguities and missing details. For instance, it mentions that messages of type `GUILD_APPLICATION_PREMIUM_SUBSCRIPTION` can now be deleted, but it does not clarify if this requires additional code changes beyond updating the enum or if there are specific behaviors or constraints associated with the new message types. Additionally, there are no examples of usage or edge cases mentioned, which could impact how these new types are handled in the broader context of the application. Overall, the statement is valid and clear for the immediate task but lacks comprehensive details for potential downstream effects or interactions with other parts of the system.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The code change involves a straightforward update to an existing enum in a single file (`message.rs`) by adding four new variants with their corresponding numeric values. This requires minimal understanding of Rust's enum syntax and the `enum_number!` macro (which appears to be a custom or library-provided macro for mapping enum variants to numbers). The scope of the change is extremely limited, affecting only a few lines of code with no apparent impact on the broader architecture or interactions with other modules. There are no complex technical concepts, algorithms, or design patterns involved\u2014just a simple data structure update. Additionally, the problem statement and code changes do not indicate any specific edge cases or error handling requirements beyond the enum update. The task is essentially a basic modification that even a junior developer with minimal Rust experience could handle with ease. The only slight complexity might be verifying that the macro or surrounding code correctly handles the new values, but this is negligible given the simplicity of the change.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.15}
{"problem_statement": "BUG: qsv frequency -l ignores limits greater than 10\ngiven the following CSV file:\r\n\r\n```sh\r\n(for i in $(seq 2001 2024); do for j in $(seq 1 $((i-2000))); do echo \"$i\"; done; done) | qsv rename -n year > data.csv\r\n```\r\n\r\nthen:\r\n\r\n* `qsv frequency data.csv` correctly gives the 10 most recent years.\r\n* `qsv frequency -l 0 data.csv` correctly counts every year\r\n* `qsv frequency -l 3 data.csv` (or any number up to 10) correctly truncates to that number\r\n* `qsv frequency -l 20 data.csv` (or any number greater than 10) only gives 10 results\r\n\r\nThis is with qsv 0.128.0 (several different builds). On 0.123.0 it correctly returns 20, so there's a regression sometime after that, but I don't have any other versions installed atm to track it down further.\n", "patch": "diff --git a/src/cmd/frequency.rs b/src/cmd/frequency.rs\nindex 869f16053..c386614bd 100644\n--- a/src/cmd/frequency.rs\n+++ b/src/cmd/frequency.rs\n@@ -242,12 +242,20 @@ impl Args {\n         let unique_counts_len = counts.len();\n         if self.flag_lmt_threshold == 0 || self.flag_lmt_threshold >= unique_counts_len {\n             // check if the column has all unique values\n-            // by checking if counts length is equal to ftable length\n+            // do this by looking at the counts vec\n+            // and see if it has a count of 1, indicating all unique values\n+            let all_unique = counts[if self.flag_asc {\n+                unique_counts_len - 1\n+            } else {\n+                0\n+            }]\n+            .1 == 1;\n+\n             let abs_limit = self.flag_limit.unsigned_abs();\n-            let unique_limited = if self.flag_limit > 0\n+            let unique_limited = if all_unique\n+                && self.flag_limit > 0\n                 && self.flag_unq_limit != abs_limit\n                 && self.flag_unq_limit > 0\n-                && unique_counts_len == ftab.len()\n             {\n                 counts.truncate(self.flag_unq_limit);\n                 true\n@@ -435,13 +443,9 @@ impl Args {\n                 if self.flag_no_trim {\n                     // case-sensitive, don't trim whitespace\n                     for (i, field) in nsel.select(row_buffer.into_iter()).enumerate() {\n-                        field_buffer = {\n-                            if let Ok(s) = simdutf8::basic::from_utf8(field) {\n-                                s.as_bytes().to_vec()\n-                            } else {\n-                                field.to_vec()\n-                            }\n-                        };\n+                        // no need to convert to string and back to bytes for a \"case-sensitive\"\n+                        // comparison we can just use the field directly\n+                        field_buffer = field.to_vec();\n \n                         // safety: we do get_unchecked_mut on freq_tables for the same reason above\n                         if !field_buffer.is_empty() {\n", "instance_id": "dathere__qsv-1966", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the `qsv frequency` command's limit flag (`-l`). It provides specific examples of expected behavior versus actual behavior, including the regression introduced after version 0.123.0. The input data generation command and the expected output for different limit values are well-documented. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior for all edge cases (e.g., negative limits or extremely large limits), and it lacks a detailed explanation of the desired output format beyond the number of results. Additionally, while the regression version range is mentioned, it could benefit from more context about the feature's intended design or constraints. Overall, the statement is valid and clear but misses some minor details that could aid in fully understanding the requirements.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows modifications in a single file (`frequency.rs`) and focuses on specific logic within the `Args` implementation. The changes are relatively localized, involving updates to how limits are applied and how uniqueness is determined. There is no indication of widespread impact on the codebase or architectural changes. The amount of code change is small, with modifications to conditional logic and data handling.\n\n2. **Technical Concepts Involved:** Solving this requires understanding Rust's syntax and control flow, as well as familiarity with the specific logic of the `qsv` tool for frequency counting. Key concepts include vector manipulation, conditional checks for uniqueness, and handling command-line arguments. Additionally, there is a minor optimization in avoiding unnecessary UTF-8 conversion for case-sensitive comparisons. These concepts are not overly complex for an experienced developer, though they require some domain knowledge of the tool's functionality.\n\n3. **Edge Cases and Error Handling:** The problem statement highlights a specific bug with limits greater than 10, which the code changes address by adjusting how limits are applied based on uniqueness. However, the statement does not mention other potential edge cases (e.g., negative limits, non-numeric inputs, or performance with very large datasets), and the code changes do not introduce new error handling mechanisms. The complexity of edge cases appears moderate and is partially addressed in the fix.\n\n4. **Overall Complexity:** The bug fix involves understanding the intent behind the limit and uniqueness flags and modifying conditional logic accordingly. While this requires careful reading of the existing code to ensure correctness, it does not demand deep architectural changes or advanced algorithms. The regression aspect adds a slight layer of complexity in identifying the root cause, but the provided diff already resolves the issue.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires moderate understanding of the code logic and targeted modifications, without significant challenges in terms of scope, technical depth, or edge case handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "Implement `Deserializer` for `Map<String, Value>` and `&Map<String, Value>`\nSometimes I want to work with a `Value` that I know to be a json object. `Map<String, Value>` is the correct type to use for this but it is not possible to deserialize from a `Map<String, Value>` to a more concrete without going through `Value`. It would be nice if `Map<String, Value>` implemented `Deserializer`  so this all just works.\r\n\r\nMy actual use case is that I have a `&Map<String, Value>` that I would like to deserialize to a concrete struct, which is harder to come up with a workaround for. With an owned value I could just do `T::deserialize(Value::Object(map))` but that doesn't work for a reference.\n", "patch": "diff --git a/src/map.rs b/src/map.rs\nindex 520cd6cf5..c4a2434d4 100644\n--- a/src/map.rs\n+++ b/src/map.rs\n@@ -535,6 +535,22 @@ macro_rules! delegate_iterator {\n     }\n }\n \n+impl<'de> de::IntoDeserializer<'de, crate::Error> for Map<String, Value> {\n+    type Deserializer = Self;\n+\n+    fn into_deserializer(self) -> Self::Deserializer {\n+        self\n+    }\n+}\n+\n+impl<'de> de::IntoDeserializer<'de, crate::Error> for &'de Map<String, Value> {\n+    type Deserializer = Self;\n+\n+    fn into_deserializer(self) -> Self::Deserializer {\n+        self\n+    }\n+}\n+\n //////////////////////////////////////////////////////////////////////////////\n \n /// A view into a single entry in a map, which may either be vacant or occupied.\ndiff --git a/src/value/de.rs b/src/value/de.rs\nindex 936725635..848d8ce1d 100644\n--- a/src/value/de.rs\n+++ b/src/value/de.rs\n@@ -187,21 +187,72 @@ where\n     }\n }\n \n-fn visit_object<'de, V>(object: Map<String, Value>, visitor: V) -> Result<V::Value, Error>\n-where\n-    V: Visitor<'de>,\n-{\n-    let len = object.len();\n-    let mut deserializer = MapDeserializer::new(object);\n-    let map = tri!(visitor.visit_map(&mut deserializer));\n-    let remaining = deserializer.iter.len();\n-    if remaining == 0 {\n-        Ok(map)\n-    } else {\n-        Err(serde::de::Error::invalid_length(\n-            len,\n-            &\"fewer elements in map\",\n-        ))\n+impl<'de> serde::Deserializer<'de> for Map<String, Value> {\n+    type Error = Error;\n+\n+    fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        let len = self.len();\n+        let mut deserializer = MapDeserializer::new(self);\n+        let map = tri!(visitor.visit_map(&mut deserializer));\n+        let remaining = deserializer.iter.len();\n+        if remaining == 0 {\n+            Ok(map)\n+        } else {\n+            Err(serde::de::Error::invalid_length(\n+                len,\n+                &\"fewer elements in map\",\n+            ))\n+        }\n+    }\n+\n+    fn deserialize_enum<V>(\n+        self,\n+        _name: &'static str,\n+        _variants: &'static [&'static str],\n+        visitor: V,\n+    ) -> Result<V::Value, Self::Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        let mut iter = self.into_iter();\n+        let (variant, value) = match iter.next() {\n+            Some(v) => v,\n+            None => {\n+                return Err(serde::de::Error::invalid_value(\n+                    Unexpected::Map,\n+                    &\"map with a single key\",\n+                ));\n+            }\n+        };\n+        // enums are encoded in json as maps with a single key:value pair\n+        if iter.next().is_some() {\n+            return Err(serde::de::Error::invalid_value(\n+                Unexpected::Map,\n+                &\"map with a single key\",\n+            ));\n+        }\n+\n+        visitor.visit_enum(EnumDeserializer {\n+            variant,\n+            value: Some(value),\n+        })\n+    }\n+\n+    fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        drop(self);\n+        visitor.visit_unit()\n+    }\n+\n+    forward_to_deserialize_any! {\n+        bool i8 i16 i32 i64 i128 u8 u16 u32 u64 u128 f32 f64 char str string\n+        bytes byte_buf option unit unit_struct newtype_struct seq tuple\n+        tuple_struct map struct identifier\n     }\n }\n \n@@ -222,7 +273,7 @@ impl<'de> serde::Deserializer<'de> for Value {\n             #[cfg(not(any(feature = \"std\", feature = \"alloc\")))]\n             Value::String(_) => unreachable!(),\n             Value::Array(v) => visit_array(v, visitor),\n-            Value::Object(v) => visit_object(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n         }\n     }\n \n@@ -253,44 +304,24 @@ impl<'de> serde::Deserializer<'de> for Value {\n     #[inline]\n     fn deserialize_enum<V>(\n         self,\n-        _name: &str,\n-        _variants: &'static [&'static str],\n+        name: &'static str,\n+        variants: &'static [&'static str],\n         visitor: V,\n     ) -> Result<V::Value, Error>\n     where\n         V: Visitor<'de>,\n     {\n-        let (variant, value) = match self {\n-            Value::Object(value) => {\n-                let mut iter = value.into_iter();\n-                let (variant, value) = match iter.next() {\n-                    Some(v) => v,\n-                    None => {\n-                        return Err(serde::de::Error::invalid_value(\n-                            Unexpected::Map,\n-                            &\"map with a single key\",\n-                        ));\n-                    }\n-                };\n-                // enums are encoded in json as maps with a single key:value pair\n-                if iter.next().is_some() {\n-                    return Err(serde::de::Error::invalid_value(\n-                        Unexpected::Map,\n-                        &\"map with a single key\",\n-                    ));\n-                }\n-                (variant, Some(value))\n-            }\n-            Value::String(variant) => (variant, None),\n-            other => {\n-                return Err(serde::de::Error::invalid_type(\n-                    other.unexpected(),\n-                    &\"string or map\",\n-                ));\n-            }\n-        };\n-\n-        visitor.visit_enum(EnumDeserializer { variant, value })\n+        match self {\n+            Value::Object(value) => value.deserialize_enum(name, variants, visitor),\n+            Value::String(variant) => visitor.visit_enum(EnumDeserializer {\n+                variant,\n+                value: None,\n+            }),\n+            other => Err(serde::de::Error::invalid_type(\n+                other.unexpected(),\n+                &\"string or map\",\n+            )),\n+        }\n     }\n \n     #[inline]\n@@ -420,7 +451,7 @@ impl<'de> serde::Deserializer<'de> for Value {\n         V: Visitor<'de>,\n     {\n         match self {\n-            Value::Object(v) => visit_object(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n             _ => Err(self.invalid_type(&visitor)),\n         }\n     }\n@@ -436,7 +467,7 @@ impl<'de> serde::Deserializer<'de> for Value {\n     {\n         match self {\n             Value::Array(v) => visit_array(v, visitor),\n-            Value::Object(v) => visit_object(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n             _ => Err(self.invalid_type(&visitor)),\n         }\n     }\n@@ -550,8 +581,10 @@ impl<'de> VariantAccess<'de> for VariantDeserializer {\n     where\n         V: Visitor<'de>,\n     {\n+        use serde::de::Deserializer;\n+\n         match self.value {\n-            Some(Value::Object(v)) => visit_object(v, visitor),\n+            Some(Value::Object(v)) => v.deserialize_any(visitor),\n             Some(other) => Err(serde::de::Error::invalid_type(\n                 other.unexpected(),\n                 &\"struct variant\",\n@@ -692,21 +725,71 @@ where\n     }\n }\n \n-fn visit_object_ref<'de, V>(object: &'de Map<String, Value>, visitor: V) -> Result<V::Value, Error>\n-where\n-    V: Visitor<'de>,\n-{\n-    let len = object.len();\n-    let mut deserializer = MapRefDeserializer::new(object);\n-    let map = tri!(visitor.visit_map(&mut deserializer));\n-    let remaining = deserializer.iter.len();\n-    if remaining == 0 {\n-        Ok(map)\n-    } else {\n-        Err(serde::de::Error::invalid_length(\n-            len,\n-            &\"fewer elements in map\",\n-        ))\n+impl<'de> serde::Deserializer<'de> for &'de Map<String, Value> {\n+    type Error = Error;\n+\n+    fn deserialize_any<V>(self, visitor: V) -> Result<V::Value, Self::Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        let len = self.len();\n+        let mut deserializer = MapRefDeserializer::new(self);\n+        let map = tri!(visitor.visit_map(&mut deserializer));\n+        let remaining = deserializer.iter.len();\n+        if remaining == 0 {\n+            Ok(map)\n+        } else {\n+            Err(serde::de::Error::invalid_length(\n+                len,\n+                &\"fewer elements in map\",\n+            ))\n+        }\n+    }\n+\n+    fn deserialize_enum<V>(\n+        self,\n+        _name: &'static str,\n+        _variants: &'static [&'static str],\n+        visitor: V,\n+    ) -> Result<V::Value, Self::Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        let mut iter = self.into_iter();\n+        let (variant, value) = match iter.next() {\n+            Some(v) => v,\n+            None => {\n+                return Err(serde::de::Error::invalid_value(\n+                    Unexpected::Map,\n+                    &\"map with a single key\",\n+                ));\n+            }\n+        };\n+        // enums are encoded in json as maps with a single key:value pair\n+        if iter.next().is_some() {\n+            return Err(serde::de::Error::invalid_value(\n+                Unexpected::Map,\n+                &\"map with a single key\",\n+            ));\n+        }\n+\n+        visitor.visit_enum(EnumRefDeserializer {\n+            variant,\n+            value: Some(value),\n+        })\n+    }\n+\n+    fn deserialize_ignored_any<V>(self, visitor: V) -> Result<V::Value, Error>\n+    where\n+        V: Visitor<'de>,\n+    {\n+        visitor.visit_unit()\n+    }\n+\n+    forward_to_deserialize_any! {\n+        bool i8 i16 i32 i64 i128 u8 u16 u32 u64 u128 f32 f64 char str string\n+        bytes byte_buf option unit unit_struct newtype_struct seq tuple\n+        tuple_struct map struct identifier\n     }\n }\n \n@@ -723,7 +806,7 @@ impl<'de> serde::Deserializer<'de> for &'de Value {\n             Value::Number(n) => n.deserialize_any(visitor),\n             Value::String(v) => visitor.visit_borrowed_str(v),\n             Value::Array(v) => visit_array_ref(v, visitor),\n-            Value::Object(v) => visit_object_ref(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n         }\n     }\n \n@@ -752,44 +835,24 @@ impl<'de> serde::Deserializer<'de> for &'de Value {\n \n     fn deserialize_enum<V>(\n         self,\n-        _name: &str,\n-        _variants: &'static [&'static str],\n+        name: &'static str,\n+        variants: &'static [&'static str],\n         visitor: V,\n     ) -> Result<V::Value, Error>\n     where\n         V: Visitor<'de>,\n     {\n-        let (variant, value) = match self {\n-            Value::Object(value) => {\n-                let mut iter = value.into_iter();\n-                let (variant, value) = match iter.next() {\n-                    Some(v) => v,\n-                    None => {\n-                        return Err(serde::de::Error::invalid_value(\n-                            Unexpected::Map,\n-                            &\"map with a single key\",\n-                        ));\n-                    }\n-                };\n-                // enums are encoded in json as maps with a single key:value pair\n-                if iter.next().is_some() {\n-                    return Err(serde::de::Error::invalid_value(\n-                        Unexpected::Map,\n-                        &\"map with a single key\",\n-                    ));\n-                }\n-                (variant, Some(value))\n-            }\n-            Value::String(variant) => (variant, None),\n-            other => {\n-                return Err(serde::de::Error::invalid_type(\n-                    other.unexpected(),\n-                    &\"string or map\",\n-                ));\n-            }\n-        };\n-\n-        visitor.visit_enum(EnumRefDeserializer { variant, value })\n+        match self {\n+            Value::Object(value) => value.deserialize_enum(name, variants, visitor),\n+            Value::String(variant) => visitor.visit_enum(EnumRefDeserializer {\n+                variant,\n+                value: None,\n+            }),\n+            other => Err(serde::de::Error::invalid_type(\n+                other.unexpected(),\n+                &\"string or map\",\n+            )),\n+        }\n     }\n \n     #[inline]\n@@ -917,7 +980,7 @@ impl<'de> serde::Deserializer<'de> for &'de Value {\n         V: Visitor<'de>,\n     {\n         match self {\n-            Value::Object(v) => visit_object_ref(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n             _ => Err(self.invalid_type(&visitor)),\n         }\n     }\n@@ -933,7 +996,7 @@ impl<'de> serde::Deserializer<'de> for &'de Value {\n     {\n         match self {\n             Value::Array(v) => visit_array_ref(v, visitor),\n-            Value::Object(v) => visit_object_ref(v, visitor),\n+            Value::Object(v) => v.deserialize_any(visitor),\n             _ => Err(self.invalid_type(&visitor)),\n         }\n     }\n@@ -1030,8 +1093,10 @@ impl<'de> VariantAccess<'de> for VariantRefDeserializer<'de> {\n     where\n         V: Visitor<'de>,\n     {\n+        use serde::de::Deserializer;\n+\n         match self.value {\n-            Some(Value::Object(v)) => visit_object_ref(v, visitor),\n+            Some(Value::Object(v)) => v.deserialize_any(visitor),\n             Some(other) => Err(serde::de::Error::invalid_type(\n                 other.unexpected(),\n                 &\"struct variant\",\n", "instance_id": "serde-rs__json-1135", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the goal: to implement `Deserializer` for `Map<String, Value>` and `&Map<String, Value>` to facilitate deserialization into concrete structs, especially for references. It provides context about the issue (inability to deserialize directly without going through `Value`) and mentions a specific use case with references. However, it lacks critical details such as explicit input/output formats, constraints, or examples of expected behavior. Edge cases or potential challenges in deserialization (e.g., nested structures, invalid data) are not addressed, which could lead to ambiguity during implementation. Overall, while the intent is understandable, minor details are missing that would make the problem fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving modifications across multiple files (`map.rs` and `value/de.rs`) and affecting core deserialization logic in what appears to be a JSON library (likely `serde_json`). The changes require implementing `Deserializer` traits for both owned and borrowed `Map<String, Value>`, which involves understanding and extending existing deserialization mechanisms. Second, the technical concepts required are moderately advanced, including familiarity with Rust's trait system, lifetime management (especially for references), and the `serde` framework's deserialization model (e.g., `Visitor` pattern, `MapDeserializer`, handling various data types). Third, the code changes are extensive, replacing and refactoring existing logic (e.g., moving `visit_object` and `visit_object_ref` functionality into direct `Deserializer` implementations) while ensuring compatibility with the broader system. Finally, there are implicit edge cases and error handling requirements, such as ensuring correct handling of map lengths, enum deserialization (single key-value pair validation), and maintaining consistency with existing `Value` deserialization behavior, which adds complexity. While not at the extreme end of difficulty (e.g., no system-level or distributed system challenges), this problem demands a deep understanding of Rust and `serde`, making it challenging for someone without specific experience in serialization libraries. Thus, a score of 0.65 reflects the hard but not insurmountable nature of the task.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.65}
{"problem_statement": "Alternative append API\nAt the moment in the API, I have to provide a `Read` and then the `append` function(s) will `io::copy` over the bytes in one go. This is fine for many use cases, but not always desirable.\r\n\r\nAs an alternative that may work better in some cases, I propose having an API that instead of taking a `Read` returns a `Write`. One can then write into there in order to write into the archive. Lifetimes are use to enforce that no other functions can be called on the archive in the meantime, and the `Drop` implementation takes care of finalizing the file (i.e. add the padding).\n", "patch": "diff --git a/src/builder.rs b/src/builder.rs\nindex 9b799329..0f4ef8ff 100644\n--- a/src/builder.rs\n+++ b/src/builder.rs\n@@ -165,6 +165,47 @@ impl<W: Write> Builder<W> {\n         self.append(&header, data)\n     }\n \n+    /// Adds a new entry to this archive and returns an [`EntryWriter`] for\n+    /// adding its contents.\n+    ///\n+    /// This function is similar to [`Self::append_data`] but returns a\n+    /// [`io::Write`] implementation instead of taking data as a parameter.\n+    ///\n+    /// Similar constraints around the position of the archive and completion\n+    /// apply as with [`Self::append_data`]. It requires the underlying writer\n+    /// to implement [`Seek`] to update the header after writing the data.\n+    ///\n+    /// # Errors\n+    ///\n+    /// This function will return an error for any intermittent I/O error which\n+    /// occurs when either reading or writing.\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// use std::io::Cursor;\n+    /// use std::io::Write as _;\n+    /// use tar::{Builder, Header};\n+    ///\n+    /// let mut header = Header::new_gnu();\n+    ///\n+    /// let mut ar = Builder::new(Cursor::new(Vec::new()));\n+    /// let mut entry = ar.append_writer(&mut header, \"hi.txt\").unwrap();\n+    /// entry.write_all(b\"Hello, \").unwrap();\n+    /// entry.write_all(b\"world!\\n\").unwrap();\n+    /// entry.finish().unwrap();\n+    /// ```\n+    pub fn append_writer<'a, P: AsRef<Path>>(\n+        &'a mut self,\n+        header: &'a mut Header,\n+        path: P,\n+    ) -> io::Result<EntryWriter<'a>>\n+    where\n+        W: Seek,\n+    {\n+        EntryWriter::start(self.get_mut(), header, path.as_ref())\n+    }\n+\n     /// Adds a new link (symbolic or hard) entry to this archive with the specified path and target.\n     ///\n     /// This function is similar to [`Self::append_data`] which supports long filenames,\n@@ -440,6 +481,92 @@ impl<W: Write> Builder<W> {\n     }\n }\n \n+trait SeekWrite: Write + Seek {\n+    fn as_write(&mut self) -> &mut dyn Write;\n+}\n+\n+impl<T: Write + Seek> SeekWrite for T {\n+    fn as_write(&mut self) -> &mut dyn Write {\n+        self\n+    }\n+}\n+\n+/// A writer for a single entry in a tar archive.\n+///\n+/// This struct is returned by [`Builder::append_writer`] and provides a\n+/// [`Write`] implementation for adding content to an archive entry.\n+///\n+/// After writing all data to the entry, it must be finalized either by\n+/// explicitly calling [`EntryWriter::finish`] or by letting it drop.\n+pub struct EntryWriter<'a> {\n+    obj: &'a mut dyn SeekWrite,\n+    header: &'a mut Header,\n+    written: u64,\n+}\n+\n+impl EntryWriter<'_> {\n+    fn start<'a>(\n+        obj: &'a mut dyn SeekWrite,\n+        header: &'a mut Header,\n+        path: &Path,\n+    ) -> io::Result<EntryWriter<'a>> {\n+        prepare_header_path(obj.as_write(), header, path)?;\n+\n+        // Reserve space for header, will be overwritten once data is written.\n+        obj.write_all([0u8; 512].as_ref())?;\n+\n+        Ok(EntryWriter {\n+            obj,\n+            header,\n+            written: 0,\n+        })\n+    }\n+\n+    /// Finish writing the current entry in the archive.\n+    pub fn finish(self) -> io::Result<()> {\n+        let mut this = std::mem::ManuallyDrop::new(self);\n+        this.do_finish()\n+    }\n+\n+    fn do_finish(&mut self) -> io::Result<()> {\n+        // Pad with zeros if necessary.\n+        let buf = [0u8; 512];\n+        let remaining = u64::wrapping_sub(512, self.written) % 512;\n+        self.obj.write_all(&buf[..remaining as usize])?;\n+        let written = (self.written + remaining) as i64;\n+\n+        // Seek back to the header position.\n+        self.obj.seek(io::SeekFrom::Current(-written - 512))?;\n+\n+        self.header.set_size(self.written);\n+        self.header.set_cksum();\n+        self.obj.write_all(self.header.as_bytes())?;\n+\n+        // Seek forward to restore the position.\n+        self.obj.seek(io::SeekFrom::Current(written))?;\n+\n+        Ok(())\n+    }\n+}\n+\n+impl Write for EntryWriter<'_> {\n+    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {\n+        let len = self.obj.write(buf)?;\n+        self.written += len as u64;\n+        Ok(len)\n+    }\n+\n+    fn flush(&mut self) -> io::Result<()> {\n+        self.obj.flush()\n+    }\n+}\n+\n+impl Drop for EntryWriter<'_> {\n+    fn drop(&mut self) {\n+        let _ = self.do_finish();\n+    }\n+}\n+\n fn append(mut dst: &mut dyn Write, header: &Header, mut data: &mut dyn Read) -> io::Result<()> {\n     dst.write_all(header.as_bytes())?;\n     let len = io::copy(&mut data, &mut dst)?;\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 52251cd2..78d89a05 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -24,7 +24,7 @@\n use std::io::{Error, ErrorKind};\n \n pub use crate::archive::{Archive, Entries};\n-pub use crate::builder::Builder;\n+pub use crate::builder::{Builder, EntryWriter};\n pub use crate::entry::{Entry, Unpacked};\n pub use crate::entry_type::EntryType;\n pub use crate::header::GnuExtSparseHeader;\n", "instance_id": "alexcrichton__tar-rs-376", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of introducing an alternative API for appending data to a tar archive by returning a `Write` implementation instead of taking a `Read` input. It mentions the use of lifetimes to enforce exclusive access and the `Drop` implementation for finalizing the file, which provides some insight into the design intent. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss specific constraints or requirements for the underlying writer (e.g., the need for `Seek` functionality, which is evident in the code changes but not mentioned in the statement). Additionally, edge cases or potential issues (e.g., handling partial writes, errors during finalization) are not addressed in the problem description. While the intent and high-level idea are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving additions to a single module (`builder.rs`) with a new struct (`EntryWriter`), a trait (`SeekWrite`), and associated logic for writing and finalizing tar entries. The changes are localized but require understanding the existing tar archive builder logic and interactions with I/O operations. Second, the technical concepts involved include Rust's trait system, lifetime management, I/O handling with `Write` and `Seek` traits, and low-level manipulation of file headers and padding in a tar archive format. These concepts are moderately complex, especially for someone unfamiliar with Rust's ownership model or tar file structure. Third, the problem requires handling edge cases such as padding data to align with tar block sizes and updating headers after writing, which adds to the complexity. However, the changes do not significantly impact the broader system architecture, and the domain-specific knowledge (tar format) is relatively contained. Overall, this problem requires a solid understanding of Rust and I/O operations, along with careful handling of state and file positioning, placing it in the medium difficulty range at 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.55}
{"problem_statement": "[BUG] Presence filters are not properly handled by lldap\nPresence filters (`(unix-uid=*)`) are not respected by lldap.  Instead they are currently executed as a schema check rather than a data check.\r\n\r\n[Reference](https://ldap.com/ldap-filters/) [documentation](https://ldapwiki.com/wiki/Wiki.jsp?page=Presence%20SearchFilter) for Presence Filters.\r\n\r\n**To Reproduce**\r\n\r\n1. Create a custom attribute.\r\n2. Populate it with a value on some users and not all users.\r\n3. Perform ldapsearch `(your-attribute=*)`\r\n4. Observe that all users are returned, not just the ones with the attribute set.\r\n\r\n**Expected behavior**\r\n\r\nlldap should return only the users with the attribute present when queried with a presence filter.\r\n\r\n**Additional context**\r\n\r\nThis silent failure misbehavior has security consequences.  For instance, the documentation for `libnss-ldapd` contains this passage:\r\n\r\n```text\r\nFor  example,  to  check  that  the  user has a proper authorizedService value if the attribute is present (this almost emulates the pam_check_service_attr option in PADL's\r\npam_ldap):\r\n\r\n(&(objectClass=posixAccount)(uid=$username)(|(authorizedService=$service)(!(authorizedService=*))))\r\n```\r\n\r\nIf users were to trust this (or indeed chose to use `libpam-ldap` instead which likely does something similar _without_ showing the user the exact details of the query when `authorizedService` or `host` checks are performed), lldap would be exposing them to explicitly unauthorized logins on Hosts B-Z from users that should be authorized only on Host A.\n", "patch": "diff --git a/server/src/domain/handler.rs b/server/src/domain/handler.rs\nindex 06ee8798..c5d5ad39 100644\n--- a/server/src/domain/handler.rs\n+++ b/server/src/domain/handler.rs\n@@ -61,6 +61,7 @@ pub enum UserRequestFilter {\n     MemberOf(GroupName),\n     // Same, by id.\n     MemberOfId(GroupId),\n+    CustomAttributePresent(AttributeName),\n }\n \n impl From<bool> for UserRequestFilter {\n@@ -85,6 +86,7 @@ pub enum GroupRequestFilter {\n     // Check if the group contains a user identified by uid.\n     Member(UserId),\n     AttributeEquality(AttributeName, Serialized),\n+    CustomAttributePresent(AttributeName),\n }\n \n impl From<bool> for GroupRequestFilter {\ndiff --git a/server/src/domain/ldap/group.rs b/server/src/domain/ldap/group.rs\nindex 313d40df..a3584ea7 100644\n--- a/server/src/domain/ldap/group.rs\n+++ b/server/src/domain/ldap/group.rs\n@@ -228,10 +228,13 @@ fn convert_group_filter(\n         LdapFilter::Not(filter) => Ok(GroupRequestFilter::Not(Box::new(rec(filter)?))),\n         LdapFilter::Present(field) => {\n             let field = AttributeName::from(field.as_str());\n-            Ok(GroupRequestFilter::from(!matches!(\n-                map_group_field(&field, schema),\n-                GroupFieldType::NoMatch\n-            )))\n+            Ok(match map_group_field(&field, schema) {\n+                GroupFieldType::Attribute(name, _, _) => {\n+                    GroupRequestFilter::CustomAttributePresent(name)\n+                }\n+                GroupFieldType::NoMatch => GroupRequestFilter::from(false),\n+                _ => GroupRequestFilter::from(true),\n+            })\n         }\n         LdapFilter::Substring(field, substring_filter) => {\n             let field = AttributeName::from(field.as_str());\ndiff --git a/server/src/domain/ldap/user.rs b/server/src/domain/ldap/user.rs\nindex bbd6c698..325200b9 100644\n--- a/server/src/domain/ldap/user.rs\n+++ b/server/src/domain/ldap/user.rs\n@@ -250,13 +250,13 @@ fn convert_user_filter(\n         }\n         LdapFilter::Present(field) => {\n             let field = AttributeName::from(field.as_str());\n-            // Check that it's a field we support.\n-            Ok(UserRequestFilter::from(\n-                field.as_str() == \"objectclass\"\n-                    || field.as_str() == \"dn\"\n-                    || field.as_str() == \"distinguishedname\"\n-                    || !matches!(map_user_field(&field, schema), UserFieldType::NoMatch),\n-            ))\n+            Ok(match map_user_field(&field, schema) {\n+                UserFieldType::Attribute(name, _, _) => {\n+                    UserRequestFilter::CustomAttributePresent(name)\n+                }\n+                UserFieldType::NoMatch => UserRequestFilter::from(false),\n+                _ => UserRequestFilter::from(true),\n+            })\n         }\n         LdapFilter::Substring(field, substring_filter) => {\n             let field = AttributeName::from(field.as_str());\ndiff --git a/server/src/domain/sql_group_backend_handler.rs b/server/src/domain/sql_group_backend_handler.rs\nindex 6f88fb27..ffa707af 100644\n--- a/server/src/domain/sql_group_backend_handler.rs\n+++ b/server/src/domain/sql_group_backend_handler.rs\n@@ -16,14 +16,18 @@ use sea_orm::{\n };\n use tracing::instrument;\n \n-fn attribute_condition(name: AttributeName, value: Serialized) -> Cond {\n+fn attribute_condition(name: AttributeName, value: Option<Serialized>) -> Cond {\n     Expr::in_subquery(\n         Expr::col(GroupColumn::GroupId.as_column_ref()),\n         model::GroupAttributes::find()\n             .select_only()\n             .column(model::GroupAttributesColumn::GroupId)\n             .filter(model::GroupAttributesColumn::AttributeName.eq(name))\n-            .filter(model::GroupAttributesColumn::Value.eq(value))\n+            .filter(\n+                value\n+                    .map(|value| model::GroupAttributesColumn::Value.eq(value))\n+                    .unwrap_or_else(|| SimpleExpr::Value(true.into())),\n+            )\n             .into_query(),\n     )\n     .into_condition()\n@@ -71,7 +75,8 @@ fn get_group_filter_expr(filter: GroupRequestFilter) -> Cond {\n         ))))\n         .like(filter.to_sql_filter())\n         .into_condition(),\n-        AttributeEquality(name, value) => attribute_condition(name, value),\n+        AttributeEquality(name, value) => attribute_condition(name, Some(value)),\n+        CustomAttributePresent(name) => attribute_condition(name, None),\n     }\n }\n \ndiff --git a/server/src/domain/sql_schema_backend_handler.rs b/server/src/domain/sql_schema_backend_handler.rs\nindex c52f998c..288055ad 100644\n--- a/server/src/domain/sql_schema_backend_handler.rs\n+++ b/server/src/domain/sql_schema_backend_handler.rs\n@@ -175,7 +175,9 @@ impl SqlBackendHandler {\n mod tests {\n     use super::*;\n     use crate::domain::{\n-        handler::AttributeList, sql_backend_handler::tests::*, types::AttributeType,\n+        handler::{AttributeList, UpdateUserRequest, UserBackendHandler, UserRequestFilter},\n+        sql_backend_handler::tests::*,\n+        types::{AttributeType, AttributeValue, Serialized},\n     };\n     use pretty_assertions::assert_eq;\n \n@@ -268,6 +270,43 @@ mod tests {\n             .contains(&expected_value));\n     }\n \n+    #[tokio::test]\n+    async fn test_user_attribute_present_filter() {\n+        let fixture = TestFixture::new().await;\n+        let new_attribute = CreateAttributeRequest {\n+            name: \"new_attribute\".into(),\n+            attribute_type: AttributeType::Integer,\n+            is_list: true,\n+            is_visible: false,\n+            is_editable: false,\n+        };\n+        fixture\n+            .handler\n+            .add_user_attribute(new_attribute)\n+            .await\n+            .unwrap();\n+        fixture\n+            .handler\n+            .update_user(UpdateUserRequest {\n+                user_id: \"bob\".into(),\n+                insert_attributes: vec![AttributeValue {\n+                    name: \"new_attribute\".into(),\n+                    value: Serialized::from(&3),\n+                }],\n+                ..Default::default()\n+            })\n+            .await\n+            .unwrap();\n+        let users = get_user_names(\n+            &fixture.handler,\n+            Some(UserRequestFilter::CustomAttributePresent(\n+                \"new_attribute\".into(),\n+            )),\n+        )\n+        .await;\n+        assert_eq!(users, vec![\"bob\"]);\n+    }\n+\n     #[tokio::test]\n     async fn test_group_attribute_add_and_delete() {\n         let fixture = TestFixture::new().await;\ndiff --git a/server/src/domain/sql_user_backend_handler.rs b/server/src/domain/sql_user_backend_handler.rs\nindex 1ba60b6d..a3768b5e 100644\n--- a/server/src/domain/sql_user_backend_handler.rs\n+++ b/server/src/domain/sql_user_backend_handler.rs\n@@ -22,14 +22,18 @@ use sea_orm::{\n use std::collections::HashSet;\n use tracing::instrument;\n \n-fn attribute_condition(name: AttributeName, value: Serialized) -> Cond {\n+fn attribute_condition(name: AttributeName, value: Option<Serialized>) -> Cond {\n     Expr::in_subquery(\n         Expr::col(UserColumn::UserId.as_column_ref()),\n         model::UserAttributes::find()\n             .select_only()\n             .column(model::UserAttributesColumn::UserId)\n             .filter(model::UserAttributesColumn::AttributeName.eq(name))\n-            .filter(model::UserAttributesColumn::Value.eq(value))\n+            .filter(\n+                value\n+                    .map(|value| model::UserAttributesColumn::Value.eq(value))\n+                    .unwrap_or_else(|| SimpleExpr::Constant(true.into())),\n+            )\n             .into_query(),\n     )\n     .into_condition()\n@@ -79,7 +83,7 @@ fn get_user_filter_expr(filter: UserRequestFilter) -> Cond {\n                 ColumnTrait::eq(&column, value).into_condition()\n             }\n         }\n-        AttributeEquality(column, value) => attribute_condition(column, value),\n+        AttributeEquality(column, value) => attribute_condition(column, Some(value)),\n         MemberOf(group) => user_id_subcondition(\n             Expr::col((group_table, GroupColumn::LowercaseDisplayName))\n                 .eq(group.as_str().to_lowercase())\n@@ -98,6 +102,7 @@ fn get_user_filter_expr(filter: UserRequestFilter) -> Cond {\n                 .like(filter.to_sql_filter())\n                 .into_condition()\n         }\n+        CustomAttributePresent(name) => attribute_condition(name, None),\n     }\n }\n \n", "instance_id": "lldap__lldap-953", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue with presence filters in lldap, where they are incorrectly handled as schema checks rather than data checks. It provides a reference to LDAP filter documentation, steps to reproduce the issue, expected behavior, and additional context about security implications. However, there are minor ambiguities: the problem statement does not explicitly define the exact input/output formats for the LDAP queries or provide detailed examples of the schema or data setup beyond a high-level description. Additionally, edge cases or specific constraints (e.g., handling of multi-valued attributes or case sensitivity) are not mentioned, which could impact the implementation. Overall, while the goal and issue are clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (handler.rs, ldap/group.rs, ldap/user.rs, sql_group_backend_handler.rs, sql_user_backend_handler.rs, and sql_schema_backend_handler.rs), requiring modifications to both user and group filter handling logic as well as database query construction. This indicates a need to understand interactions between different parts of the codebase, including LDAP filter parsing, attribute mapping, and SQL query generation using the sea_orm library. The changes involve adding new variants (e.g., CustomAttributePresent) to enums and updating conditional logic to handle presence filters correctly, which is moderately complex. \n\nSecond, the technical concepts required include familiarity with Rust (enums, pattern matching, and ownership), LDAP filter semantics (specifically presence filters), and SQL subqueries for attribute presence checks. While these concepts are not extremely advanced, they require a solid understanding of both the domain (LDAP) and the codebase's architecture. \n\nThird, the problem does not explicitly mention edge cases in the statement, but the code changes suggest handling scenarios like missing attributes or schema mismatches, which adds some complexity to ensure correctness. Error handling modifications are minimal, as the focus is on adjusting filter logic rather than adding new error paths. \n\nFinally, the impact on the system's architecture is limited to the filter handling and query logic, without broader systemic changes. Given the moderate scope of changes, the need for cross-module understanding, and the domain-specific knowledge required, a difficulty score of 0.55 reflects a medium-level challenge that is approachable for a developer with intermediate experience in Rust and LDAP systems but requires careful attention to detail.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.55}
{"problem_statement": "await not allowed in comprehension in async def\n## Feature\r\n\r\n<!-- What Python feature is missing from RustPython? Give a short description of the feature and how you ran into its absence. -->\r\n\r\ncode\r\n```python\r\nasync def f():\r\n    [await a for a in range(1)]\r\n```\r\n\r\nexpected: No error\r\n\r\nActual:\r\n```sh\r\n  File \"z.py\", line 2\r\n    [await a for a in range(1)]\r\n     ^\r\nSyntaxError: 'await' outside async function\r\n```\r\n\r\n## Python Documentation or reference to CPython source code\r\n\r\n<!-- Give a link to the feature in the CPython documentation (https://docs.python.org/3/) in order to assist in its implementation. -->\r\n\n", "patch": "diff --git a/compiler/codegen/src/compile.rs b/compiler/codegen/src/compile.rs\nindex 7b912f43d2..dc9f8a976e 100644\n--- a/compiler/codegen/src/compile.rs\n+++ b/compiler/codegen/src/compile.rs\n@@ -87,6 +87,14 @@ impl CompileContext {\n     }\n }\n \n+#[derive(Debug, Clone, Copy, PartialEq)]\n+enum ComprehensionType {\n+    Generator,\n+    List,\n+    Set,\n+    Dict,\n+}\n+\n /// Compile an located_ast::Mod produced from rustpython_parser::parse()\n pub fn compile_top(\n     ast: &located_ast::Mod,\n@@ -2431,6 +2439,8 @@ impl Compiler {\n                         );\n                         Ok(())\n                     },\n+                    ComprehensionType::List,\n+                    Self::contains_await(elt),\n                 )?;\n             }\n             Expr::SetComp(located_ast::ExprSetComp {\n@@ -2452,6 +2462,8 @@ impl Compiler {\n                         );\n                         Ok(())\n                     },\n+                    ComprehensionType::Set,\n+                    Self::contains_await(elt),\n                 )?;\n             }\n             Expr::DictComp(located_ast::ExprDictComp {\n@@ -2480,19 +2492,28 @@ impl Compiler {\n \n                         Ok(())\n                     },\n+                    ComprehensionType::Dict,\n+                    Self::contains_await(key) || Self::contains_await(value),\n                 )?;\n             }\n             Expr::GeneratorExp(located_ast::ExprGeneratorExp {\n                 elt, generators, ..\n             }) => {\n-                self.compile_comprehension(\"<genexpr>\", None, generators, &|compiler| {\n-                    compiler.compile_comprehension_element(elt)?;\n-                    compiler.mark_generator();\n-                    emit!(compiler, Instruction::YieldValue);\n-                    emit!(compiler, Instruction::Pop);\n+                self.compile_comprehension(\n+                    \"<genexpr>\",\n+                    None,\n+                    generators,\n+                    &|compiler| {\n+                        compiler.compile_comprehension_element(elt)?;\n+                        compiler.mark_generator();\n+                        emit!(compiler, Instruction::YieldValue);\n+                        emit!(compiler, Instruction::Pop);\n \n-                    Ok(())\n-                })?;\n+                        Ok(())\n+                    },\n+                    ComprehensionType::Generator,\n+                    Self::contains_await(elt),\n+                )?;\n             }\n             Expr::Starred(_) => {\n                 return Err(self.error(CodegenErrorType::InvalidStarExpr));\n@@ -2744,9 +2765,35 @@ impl Compiler {\n         init_collection: Option<Instruction>,\n         generators: &[located_ast::Comprehension],\n         compile_element: &dyn Fn(&mut Self) -> CompileResult<()>,\n+        comprehension_type: ComprehensionType,\n+        element_contains_await: bool,\n     ) -> CompileResult<()> {\n         let prev_ctx = self.ctx;\n-        let is_async = generators.iter().any(|g| g.is_async);\n+        let has_an_async_gen = generators.iter().any(|g| g.is_async);\n+\n+        // async comprehensions are allowed in various contexts:\n+        // - list/set/dict comprehensions in async functions\n+        // - always for generator expressions\n+        // Note: generators have to be treated specially since their async version is a fundamentally\n+        // different type (aiter vs iter) instead of just an awaitable.\n+\n+        // for if it actually is async, we check if any generator is async or if the element contains await\n+\n+        // if the element expression contains await, but the context doesn't allow for async,\n+        // then we continue on here with is_async=false and will produce a syntax once the await is hit\n+\n+        let is_async_list_set_dict_comprehension = comprehension_type\n+            != ComprehensionType::Generator\n+            && (has_an_async_gen || element_contains_await) // does it have to be async? (uses await or async for)\n+            && prev_ctx.func == FunctionContext::AsyncFunction; // is it allowed to be async? (in an async function)\n+\n+        let is_async_generator_comprehension = comprehension_type == ComprehensionType::Generator\n+            && (has_an_async_gen || element_contains_await);\n+\n+        // since one is for generators, and one for not generators, they should never both be true\n+        debug_assert!(!(is_async_list_set_dict_comprehension && is_async_generator_comprehension));\n+\n+        let is_async = is_async_list_set_dict_comprehension || is_async_generator_comprehension;\n \n         self.ctx = CompileContext {\n             loop_data: None,\n@@ -2838,7 +2885,7 @@ impl Compiler {\n \n             // End of for loop:\n             self.switch_to_block(after_block);\n-            if is_async {\n+            if has_an_async_gen {\n                 emit!(self, Instruction::EndAsyncFor);\n             }\n         }\n@@ -2877,7 +2924,7 @@ impl Compiler {\n         self.compile_expression(&generators[0].iter)?;\n \n         // Get iterator / turn item into an iterator\n-        if is_async {\n+        if has_an_async_gen {\n             emit!(self, Instruction::GetAIter);\n         } else {\n             emit!(self, Instruction::GetIter);\n@@ -2885,11 +2932,15 @@ impl Compiler {\n \n         // Call just created <listcomp> function:\n         emit!(self, Instruction::CallFunctionPositional { nargs: 1 });\n-        if is_async {\n+        if is_async_list_set_dict_comprehension {\n+            // async, but not a generator and not an async for\n+            // in this case, we end up with an awaitable\n+            // that evaluates to the list/set/dict, so here we add an await\n             emit!(self, Instruction::GetAwaitable);\n             self.emit_load_const(ConstantData::None);\n             emit!(self, Instruction::YieldFrom);\n         }\n+\n         Ok(())\n     }\n \n@@ -3016,6 +3067,117 @@ impl Compiler {\n     fn mark_generator(&mut self) {\n         self.current_code_info().flags |= bytecode::CodeFlags::IS_GENERATOR\n     }\n+\n+    /// Whether the expression contains an await expression and\n+    /// thus requires the function to be async.\n+    /// Async with and async for are statements, so I won't check for them here\n+    fn contains_await(expression: &located_ast::Expr) -> bool {\n+        use located_ast::*;\n+\n+        match &expression {\n+            Expr::Call(ExprCall {\n+                func,\n+                args,\n+                keywords,\n+                ..\n+            }) => {\n+                Self::contains_await(func)\n+                    || args.iter().any(Self::contains_await)\n+                    || keywords.iter().any(|kw| Self::contains_await(&kw.value))\n+            }\n+            Expr::BoolOp(ExprBoolOp { values, .. }) => values.iter().any(Self::contains_await),\n+            Expr::BinOp(ExprBinOp { left, right, .. }) => {\n+                Self::contains_await(left) || Self::contains_await(right)\n+            }\n+            Expr::Subscript(ExprSubscript { value, slice, .. }) => {\n+                Self::contains_await(value) || Self::contains_await(slice)\n+            }\n+            Expr::UnaryOp(ExprUnaryOp { operand, .. }) => Self::contains_await(operand),\n+            Expr::Attribute(ExprAttribute { value, .. }) => Self::contains_await(value),\n+            Expr::Compare(ExprCompare {\n+                left, comparators, ..\n+            }) => Self::contains_await(left) || comparators.iter().any(Self::contains_await),\n+            Expr::Constant(ExprConstant { .. }) => false,\n+            Expr::List(ExprList { elts, .. }) => elts.iter().any(Self::contains_await),\n+            Expr::Tuple(ExprTuple { elts, .. }) => elts.iter().any(Self::contains_await),\n+            Expr::Set(ExprSet { elts, .. }) => elts.iter().any(Self::contains_await),\n+            Expr::Dict(ExprDict { keys, values, .. }) => {\n+                keys.iter()\n+                    .any(|key| key.as_ref().map_or(false, Self::contains_await))\n+                    || values.iter().any(Self::contains_await)\n+            }\n+            Expr::Slice(ExprSlice {\n+                lower, upper, step, ..\n+            }) => {\n+                lower.as_ref().map_or(false, |l| Self::contains_await(l))\n+                    || upper.as_ref().map_or(false, |u| Self::contains_await(u))\n+                    || step.as_ref().map_or(false, |s| Self::contains_await(s))\n+            }\n+            Expr::Yield(ExprYield { value, .. }) => {\n+                value.as_ref().map_or(false, |v| Self::contains_await(v))\n+            }\n+            Expr::Await(ExprAwait { .. }) => true,\n+            Expr::YieldFrom(ExprYieldFrom { value, .. }) => Self::contains_await(value),\n+            Expr::JoinedStr(ExprJoinedStr { values, .. }) => {\n+                values.iter().any(Self::contains_await)\n+            }\n+            Expr::FormattedValue(ExprFormattedValue {\n+                value,\n+                conversion: _,\n+                format_spec,\n+                ..\n+            }) => {\n+                Self::contains_await(value)\n+                    || format_spec\n+                        .as_ref()\n+                        .map_or(false, |fs| Self::contains_await(fs))\n+            }\n+            Expr::Name(located_ast::ExprName { .. }) => false,\n+            Expr::Lambda(located_ast::ExprLambda { body, .. }) => Self::contains_await(body),\n+            Expr::ListComp(located_ast::ExprListComp {\n+                elt, generators, ..\n+            }) => {\n+                Self::contains_await(elt)\n+                    || generators.iter().any(|gen| Self::contains_await(&gen.iter))\n+            }\n+            Expr::SetComp(located_ast::ExprSetComp {\n+                elt, generators, ..\n+            }) => {\n+                Self::contains_await(elt)\n+                    || generators.iter().any(|gen| Self::contains_await(&gen.iter))\n+            }\n+            Expr::DictComp(located_ast::ExprDictComp {\n+                key,\n+                value,\n+                generators,\n+                ..\n+            }) => {\n+                Self::contains_await(key)\n+                    || Self::contains_await(value)\n+                    || generators.iter().any(|gen| Self::contains_await(&gen.iter))\n+            }\n+            Expr::GeneratorExp(located_ast::ExprGeneratorExp {\n+                elt, generators, ..\n+            }) => {\n+                Self::contains_await(elt)\n+                    || generators.iter().any(|gen| Self::contains_await(&gen.iter))\n+            }\n+            Expr::Starred(expr) => Self::contains_await(&expr.value),\n+            Expr::IfExp(located_ast::ExprIfExp {\n+                test, body, orelse, ..\n+            }) => {\n+                Self::contains_await(test)\n+                    || Self::contains_await(body)\n+                    || Self::contains_await(orelse)\n+            }\n+\n+            Expr::NamedExpr(located_ast::ExprNamedExpr {\n+                target,\n+                value,\n+                range: _,\n+            }) => Self::contains_await(target) || Self::contains_await(value),\n+        }\n+    }\n }\n \n trait EmitArg<Arg: OpArgType> {\n", "instance_id": "RustPython__RustPython-5334", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: RustPython does not support the use of 'await' within comprehensions inside an async function, which is a valid feature in Python. The expected behavior (no error) and actual behavior (SyntaxError) are provided with a minimal code example. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention whether this issue applies to all types of comprehensions (list, set, dict, generator) or just the list comprehension shown in the example. Additionally, there are no references to specific Python documentation or CPython source code (the section for documentation is left blank), which would have helped clarify the exact behavior and edge cases to implement. While the intent is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the 'Hard' category due to several factors. First, the scope of code changes is significant, as it involves modifying the compiler's code generation logic in RustPython, specifically in the `compile.rs` file, which is a core component of the system. The changes span multiple functions and introduce new logic for handling async comprehensions, including distinguishing between different comprehension types (list, set, dict, generator) and detecting 'await' expressions. This requires a deep understanding of the RustPython compiler internals, bytecode generation, and Python's async/await semantics.\n\nSecond, the number of technical concepts involved is substantial. The solution requires knowledge of Rust (for implementing the changes), Python's async model (to understand the expected behavior), and compiler design (to modify code generation for comprehensions). Specific concepts include handling async iterators (`GetAIter`), awaitable objects (`GetAwaitable`), and generator expressions, as well as traversing AST nodes to detect 'await' expressions recursively. The introduction of a new `ComprehensionType` enum and the `contains_await` method indicates a need for careful design to ensure correctness across different contexts.\n\nThird, the code changes impact a critical part of the codebase (the compiler), which could have broader implications on how async code is processed. While the changes are localized to one file, they affect the behavior of multiple comprehension types and require understanding the interaction between async functions, generators, and await expressions. The amount of code added is non-trivial, with over 100 lines of new logic, including a complex recursive function to detect 'await' in expressions.\n\nFinally, potential edge cases and error handling add to the difficulty. The problem statement does not explicitly mention edge cases, but the code changes handle different comprehension types and async contexts, suggesting the need to consider scenarios like nested comprehensions, mixed async/sync generators, and invalid uses of 'await'. The solution also needs to ensure that syntax errors are raised appropriately when 'await' is used in unsupported contexts, which adds to the complexity of testing and validation.\n\nOverall, this problem requires a solid grasp of both Python and Rust, as well as compiler internals, placing it in the 0.6-0.8 range. I assign a score of 0.65 to reflect the challenging nature of the task, which is above medium difficulty but not at the extreme end of hard due to the localized nature of the changes (single file) and the absence of system-wide architectural redesign.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.65}
{"problem_statement": "rustix::fs::OFlags::Async is missing\n`linux-raw-sys` defines `FASYNC` instead of `O_ASYNC` [`linux-raw-sys/.../general.rs`](https://docs.rs/linux-raw-sys/0.6.5/src/linux_raw_sys/x86_64/general.rs.html#1061)\r\n\r\n> The Linux header file `<asm/fcntl.h>` doesn't define `O_ASYNC`; the (BSD-derived) `FASYNC` synonym is defined instead.\r\n([`man 2 open`](https://man.archlinux.org/man/open.2#NOTES))\r\n\r\nNote that `O_ASYNC` can't be used for `open(2)`, so this should probably be noted if `Async` is ever added to `OFlags`\n", "patch": "diff --git a/src/backend/libc/fs/types.rs b/src/backend/libc/fs/types.rs\nindex c92250475..f134d409b 100644\n--- a/src/backend/libc/fs/types.rs\n+++ b/src/backend/libc/fs/types.rs\n@@ -338,6 +338,20 @@ bitflags! {\n         #[cfg(any(linux_kernel, solarish))]\n         const LARGEFILE = bitcast!(c::O_LARGEFILE);\n \n+        /// `O_ASYNC`, `FASYNC`\n+        ///\n+        /// Note that this flag can't be used with [`rustix::fs::open`] family of functions, use\n+        /// [`rustix::fs::fcntl_setfl`] instead\n+        #[cfg(not(any(\n+            target_os = \"aix\",\n+            target_os = \"espidf\",\n+            target_os = \"haiku\",\n+            target_os = \"wasi\",\n+            target_os = \"vita\",\n+            solarish\n+        )))]\n+        const ASYNC = bitcast!(c::O_ASYNC);\n+\n         /// <https://docs.rs/bitflags/*/bitflags/#externally-defined-flags>\n         const _ = !0;\n     }\ndiff --git a/src/backend/linux_raw/fs/types.rs b/src/backend/linux_raw/fs/types.rs\nindex 629ca4a22..1f20ac6a3 100644\n--- a/src/backend/linux_raw/fs/types.rs\n+++ b/src/backend/linux_raw/fs/types.rs\n@@ -257,6 +257,12 @@ bitflags! {\n         /// It will may be reported in return of `fcntl_getfl`, though.\n         const LARGEFILE = linux_raw_sys::general::O_LARGEFILE;\n \n+        /// `O_ASYNC`, `FASYNC`\n+        ///\n+        /// Note that this flag can't be used with [`rustix::fs::open`] family of functions, use\n+        /// [`rustix::fs::fcntl_setfl`] instead\n+        const ASYNC = linux_raw_sys::general::FASYNC;\n+\n         /// <https://docs.rs/bitflags/*/bitflags/#externally-defined-flags>\n         const _ = !0;\n     }\n", "instance_id": "bytecodealliance__rustix-1312", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the missing `OFlags::Async` in `rustix::fs` due to the use of `FASYNC` instead of `O_ASYNC` in `linux-raw-sys`. It provides relevant references to documentation and man pages, which help in understanding the context. The goal of adding support for `O_ASYNC` (or its synonym `FASYNC`) is implied, and a specific note about its usage limitation with `open(2)` is mentioned. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or constraints for the implementation (e.g., whether this flag should behave differently across platforms beyond the provided conditional compilation). Additionally, edge cases or potential pitfalls related to the usage of `O_ASYNC`/`FASYNC` are not discussed, which could be critical given the note about its incompatibility with `open(2)`. Overall, while the problem is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is limited to two files (`types.rs` in both `libc` and `linux_raw` backends), and the modifications are straightforward\u2014adding a new flag `ASYNC` to the `OFlags` bitflags with appropriate documentation and conditional compilation. The changes do not impact the broader system architecture or require deep understanding of interactions between modules; they are isolated to the definition of flags. Second, the technical concepts involved are relatively basic: familiarity with Rust's `bitflags` macro, conditional compilation (`#[cfg]` attributes), and understanding of POSIX file flags (`O_ASYNC`/`FASYNC`). These are not particularly complex for a developer with moderate Rust experience. Third, the problem does not explicitly mention edge cases beyond the usage limitation with `open(2)`, which is already addressed in the code comments, and no additional error handling logic is required in the provided changes. The overall amount of code change is minimal, and the task primarily involves adding a missing constant with proper documentation. Therefore, a difficulty score of 0.25 reflects the simplicity of the task while acknowledging the need for some domain knowledge about file flags and platform-specific behaviors.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.25}
{"problem_statement": "Content_Disposition should NOT be mandatory on Multipart\nThis behaviour was added for issue https://github.com/actix/actix-web/issues/1947 and in PR https://github.com/actix/actix-web/pull/2451 or https://github.com/actix/actix-web/pull/2090 but i believe the claims about it being mandatory in the original issue are a misconception.\r\n\r\nso since version 0.4.0-beta.8\r\n\r\nsee https://www.w3.org/Protocols/HTTP/Issues/content-disposition.txt\r\n```\r\n1.5  Content-Disposition and Multipart\r\n\r\n   If a Content-Disposition header is used on a multipart body part, it\r\n   applies to the multipart as a whole, not the individual subparts.\r\n   The disposition types of the subparts do not need to be consulted\r\n   until the multipart itself is presented.  When the multipart is\r\n   displayed, then the dispositions of the subparts should be respected.\r\n\r\n   If the `inline' disposition is used, the multipart should be\r\n   displayed as normal; however, an `attachment' subpart should require\r\n   action from the user to display.\r\n\r\n   If the `attachment' disposition is used, presentation of the\r\n   multipart should not proceed without explicit user action.  Once the\r\n   user has chosen to display the multipart, the individual subpart\r\n   dispositions should be consulted to determine how to present the\r\n   subparts.\r\n```\r\n\r\nAlthough it may be needed on multipart/form-data (https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Disposition) I believe it is otherwise optional and protocols like MTOM seem to use it that way....\r\n\r\n## Expected Behaviour\r\n\r\nmultipart/related requests should be able to parse without a Content-Disposition header\r\n\r\n## Current Behaviour\r\n\r\nFails with \"mp err NoContentDisposition\"\r\n\r\n## Possible Solution\r\n\r\nmake multipart:Server:Field.cd an Option again\r\n\r\n## Steps to Reproduce (for bugs)\r\n\r\nstart SoapUI\r\n* make a random request\r\n* enable and force MTOM\r\n* send request\r\n\r\nrequest sample\r\n```\r\nPOST http://myserver.local/webServiceEndpointTest HTTP/1.1\r\nAccept-Encoding: gzip,deflate\r\nContent-Type: multipart/related; type=\"application/xop+xml\"; start=\"<rootpart@soapui.org>\"; start-info=\"text/xml\"; boundary=\"----=_Part_0_1662012483.1700221162492\"\r\nSOAPAction: \"soapbodystufwithchildren\"\r\nMIME-Version: 1.0\r\nContent-Length: 9370\r\nHost: myserver.local\r\nConnection: Keep-Alive\r\nUser-Agent: Apache-HttpClient/4.5.5 (Java/16.0.2)\r\n\r\n\r\n------=_Part_0_1662012483.1700221162492\r\nContent-Type: application/xop+xml; charset=UTF-8; type=\"text/xml\"\r\nContent-Transfer-Encoding: 8bit\r\nContent-ID: <rootpart@soapui.org>\r\n\r\n<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\">\r\n<soapenv:Header/>\r\n<soapenv:Body>\r\n<soapbodystufwithchildren />\r\n</soapenv:Body>\r\n</soapenv:Envelope>\r\n------=_Part_0_1662012483.1700221162492--\r\n\r\n```\r\n\r\nerror:\r\n```\r\nmp err NoContentDisposition\r\n```\r\n\r\nwhich originates i think from server.rs:352\r\n```rust\r\nlet cd = if let Some(content_disposition) = content_disposition {\r\n                content_disposition\r\n            } else {\r\n                return Poll::Ready(Some(Err(MultipartError::NoContentDisposition)));\r\n            };\r\n```\r\n\r\n\r\n## Context\r\n\r\ncan't make webservice endpoints when MTOM is used\r\n\r\n- Actix Web Version: 4\r\n\n", "patch": "diff --git a/actix-multipart-derive/src/lib.rs b/actix-multipart-derive/src/lib.rs\nindex 9552ad2d904..44db1fa0e45 100644\n--- a/actix-multipart-derive/src/lib.rs\n+++ b/actix-multipart-derive/src/lib.rs\n@@ -138,7 +138,7 @@ struct ParsedField<'t> {\n /// `#[multipart(duplicate_field = \"<behavior>\")]` attribute:\n ///\n /// - \"ignore\": (default) Extra fields are ignored. I.e., the first one is persisted.\n-/// - \"deny\": A `MultipartError::UnsupportedField` error response is returned.\n+/// - \"deny\": A `MultipartError::UnknownField` error response is returned.\n /// - \"replace\": Each field is processed, but only the last one is persisted.\n ///\n /// Note that `Vec` fields will ignore this option.\n@@ -229,7 +229,7 @@ pub fn impl_multipart_form(input: proc_macro::TokenStream) -> proc_macro::TokenS\n     // Return value when a field name is not supported by the form\n     let unknown_field_result = if attrs.deny_unknown_fields {\n         quote!(::std::result::Result::Err(\n-            ::actix_multipart::MultipartError::UnsupportedField(field.name().to_string())\n+            ::actix_multipart::MultipartError::UnknownField(field.name().unwrap().to_string())\n         ))\n     } else {\n         quote!(::std::result::Result::Ok(()))\n@@ -292,7 +292,7 @@ pub fn impl_multipart_form(input: proc_macro::TokenStream) -> proc_macro::TokenS\n                 limits: &'t mut ::actix_multipart::form::Limits,\n                 state: &'t mut ::actix_multipart::form::State,\n             ) -> ::std::pin::Pin<::std::boxed::Box<dyn ::std::future::Future<Output = ::std::result::Result<(), ::actix_multipart::MultipartError>> + 't>> {\n-                match field.name() {\n+                match field.name().unwrap() {\n                     #handle_field_impl\n                     _ => return ::std::boxed::Box::pin(::std::future::ready(#unknown_field_result)),\n                 }\ndiff --git a/actix-multipart/CHANGES.md b/actix-multipart/CHANGES.md\nindex a91edf9c81e..dea24ab9d55 100644\n--- a/actix-multipart/CHANGES.md\n+++ b/actix-multipart/CHANGES.md\n@@ -2,6 +2,15 @@\n \n ## Unreleased\n \n+- Add `MultipartError::ContentTypeIncompatible` variant.\n+- Add `MultipartError::ContentDispositionNameMissing` variant.\n+- Rename `MultipartError::{NoContentDisposition => ContentDispositionMissing}` variant.\n+- Rename `MultipartError::{NoContentType => ContentTypeMissing}` variant.\n+- Rename `MultipartError::{ParseContentType => ContentTypeParse}` variant.\n+- Rename `MultipartError::{Boundary => BoundaryMissing}` variant.\n+- Rename `MultipartError::{UnsupportedField => UnknownField}` variant.\n+- Remove top-level re-exports of `test` utilities.\n+\n ## 0.6.2\n \n - Add testing utilities under new module `test`.\ndiff --git a/actix-multipart/Cargo.toml b/actix-multipart/Cargo.toml\nindex 5e9b78d848c..c5ff7dd10ae 100644\n--- a/actix-multipart/Cargo.toml\n+++ b/actix-multipart/Cargo.toml\n@@ -63,7 +63,9 @@ actix-multipart-rfc7578 = \"0.10\"\n actix-rt = \"2.2\"\n actix-test = \"0.1\"\n actix-web = \"4\"\n+assert_matches = \"1\"\n awc = \"3\"\n+env_logger = \"0.11\"\n futures-util = { version = \"0.3.17\", default-features = false, features = [\"alloc\"] }\n multer = \"3\"\n tokio = { version = \"1.24.2\", features = [\"sync\"] }\ndiff --git a/actix-multipart/README.md b/actix-multipart/README.md\nindex d61347f321e..917dceece1f 100644\n--- a/actix-multipart/README.md\n+++ b/actix-multipart/README.md\n@@ -54,15 +54,15 @@ async fn main() -> std::io::Result<()> {\n }\n ```\n \n-<!-- cargo-rdme end -->\n-\n-[More available in the examples repo &rarr;](https://github.com/actix/examples/tree/master/forms/multipart)\n-\n-Curl request :\n+cURL request:\n \n-```bash\n+```sh\n curl -v --request POST \\\n   --url http://localhost:8080/videos \\\n   -F 'json={\"name\": \"Cargo.lock\"};type=application/json' \\\n   -F file=@./Cargo.lock\n ```\n+\n+<!-- cargo-rdme end -->\n+\n+[More available in the examples repo &rarr;](https://github.com/actix/examples/tree/master/forms/multipart)\ndiff --git a/actix-multipart/examples/form.rs b/actix-multipart/examples/form.rs\nnew file mode 100644\nindex 00000000000..a90aeff96e2\n--- /dev/null\n+++ b/actix-multipart/examples/form.rs\n@@ -0,0 +1,36 @@\n+use actix_multipart::form::{json::Json as MpJson, tempfile::TempFile, MultipartForm};\n+use actix_web::{middleware::Logger, post, App, HttpServer, Responder};\n+use serde::Deserialize;\n+\n+#[derive(Debug, Deserialize)]\n+struct Metadata {\n+    name: String,\n+}\n+\n+#[derive(Debug, MultipartForm)]\n+struct UploadForm {\n+    #[multipart(limit = \"100MB\")]\n+    file: TempFile,\n+    json: MpJson<Metadata>,\n+}\n+\n+#[post(\"/videos\")]\n+async fn post_video(MultipartForm(form): MultipartForm<UploadForm>) -> impl Responder {\n+    format!(\n+        \"Uploaded file {}, with size: {}\\ntemporary file ({}) was deleted\\n\",\n+        form.json.name,\n+        form.file.size,\n+        form.file.file.path().display(),\n+    )\n+}\n+\n+#[actix_web::main]\n+async fn main() -> std::io::Result<()> {\n+    env_logger::init_from_env(env_logger::Env::new().default_filter_or(\"info\"));\n+\n+    HttpServer::new(move || App::new().service(post_video).wrap(Logger::default()))\n+        .workers(2)\n+        .bind((\"127.0.0.1\", 8080))?\n+        .run()\n+        .await\n+}\ndiff --git a/actix-multipart/src/error.rs b/actix-multipart/src/error.rs\nindex 77b5a559f82..30ef63c1a87 100644\n--- a/actix-multipart/src/error.rs\n+++ b/actix-multipart/src/error.rs\n@@ -11,77 +11,95 @@ use derive_more::{Display, Error, From};\n #[derive(Debug, Display, From, Error)]\n #[non_exhaustive]\n pub enum MultipartError {\n-    /// Content-Disposition header is not found or is not equal to \"form-data\".\n-    ///\n-    /// According to [RFC 7578 \u00a74.2](https://datatracker.ietf.org/doc/html/rfc7578#section-4.2) a\n-    /// Content-Disposition header must always be present and equal to \"form-data\".\n-    #[display(fmt = \"No Content-Disposition `form-data` header\")]\n-    NoContentDisposition,\n+    /// Could not find Content-Type header.\n+    #[display(fmt = \"Could not find Content-Type header\")]\n+    ContentTypeMissing,\n \n-    /// Content-Type header is not found\n-    #[display(fmt = \"No Content-Type header found\")]\n-    NoContentType,\n+    /// Could not parse Content-Type header.\n+    #[display(fmt = \"Could not parse Content-Type header\")]\n+    ContentTypeParse,\n \n-    /// Can not parse Content-Type header\n-    #[display(fmt = \"Can not parse Content-Type header\")]\n-    ParseContentType,\n+    /// Parsed Content-Type did not have \"multipart\" top-level media type.\n+    ///\n+    /// Also raised when extracting a [`MultipartForm`] from a request that does not have the\n+    /// \"multipart/form-data\" media type.\n+    ///\n+    /// [`MultipartForm`]: struct@crate::form::MultipartForm\n+    #[display(fmt = \"Parsed Content-Type did not have \"multipart\" top-level media type\")]\n+    ContentTypeIncompatible,\n \n-    /// Multipart boundary is not found\n+    /// Multipart boundary is not found.\n     #[display(fmt = \"Multipart boundary is not found\")]\n-    Boundary,\n+    BoundaryMissing,\n+\n+    /// Content-Disposition header was not found or not of disposition type \"form-data\" when parsing\n+    /// a \"form-data\" field.\n+    ///\n+    /// As per [RFC 7578 \u00a74.2], a \"multipart/form-data\" field's Content-Disposition header must\n+    /// always be present and have a disposition type of \"form-data\".\n+    ///\n+    /// [RFC 7578 \u00a74.2]: https://datatracker.ietf.org/doc/html/rfc7578#section-4.2\n+    #[display(fmt = \"Content-Disposition header was not found when parsing a \\\"form-data\\\" field\")]\n+    ContentDispositionMissing,\n+\n+    /// Content-Disposition name parameter was not found when parsing a \"form-data\" field.\n+    ///\n+    /// As per [RFC 7578 \u00a74.2], a \"multipart/form-data\" field's Content-Disposition header must\n+    /// always include a \"name\" parameter.\n+    ///\n+    /// [RFC 7578 \u00a74.2]: https://datatracker.ietf.org/doc/html/rfc7578#section-4.2\n+    #[display(fmt = \"Content-Disposition header was not found when parsing a \\\"form-data\\\" field\")]\n+    ContentDispositionNameMissing,\n \n-    /// Nested multipart is not supported\n+    /// Nested multipart is not supported.\n     #[display(fmt = \"Nested multipart is not supported\")]\n     Nested,\n \n-    /// Multipart stream is incomplete\n+    /// Multipart stream is incomplete.\n     #[display(fmt = \"Multipart stream is incomplete\")]\n     Incomplete,\n \n-    /// Error during field parsing\n-    #[display(fmt = \"{}\", _0)]\n+    /// Field parsing failed.\n+    #[display(fmt = \"Error during field parsing\")]\n     Parse(ParseError),\n \n-    /// Payload error\n-    #[display(fmt = \"{}\", _0)]\n+    /// HTTP payload error.\n+    #[display(fmt = \"Payload error\")]\n     Payload(PayloadError),\n \n-    /// Not consumed\n-    #[display(fmt = \"Multipart stream is not consumed\")]\n+    /// Stream is not consumed.\n+    #[display(fmt = \"Stream is not consumed\")]\n     NotConsumed,\n \n-    /// An error from a field handler in a form\n-    #[display(\n-        fmt = \"An error occurred processing field `{}`: {}\",\n-        field_name,\n-        source\n-    )]\n+    /// Form field handler raised error.\n+    #[display(fmt = \"An error occurred processing field: {name}\")]\n     Field {\n-        field_name: String,\n+        name: String,\n         source: actix_web::Error,\n     },\n \n-    /// Duplicate field\n-    #[display(fmt = \"Duplicate field found for: `{}`\", _0)]\n+    /// Duplicate field found (for structure that opted-in to denying duplicate fields).\n+    #[display(fmt = \"Duplicate field found: {_0}\")]\n     #[from(ignore)]\n     DuplicateField(#[error(not(source))] String),\n \n-    /// Missing field\n-    #[display(fmt = \"Field with name `{}` is required\", _0)]\n+    /// Required field is missing.\n+    #[display(fmt = \"Required field is missing: {_0}\")]\n     #[from(ignore)]\n     MissingField(#[error(not(source))] String),\n \n-    /// Unknown field\n-    #[display(fmt = \"Unsupported field `{}`\", _0)]\n+    /// Unknown field (for structure that opted-in to denying unknown fields).\n+    #[display(fmt = \"Unknown field: {_0}\")]\n     #[from(ignore)]\n-    UnsupportedField(#[error(not(source))] String),\n+    UnknownField(#[error(not(source))] String),\n }\n \n-/// Return `BadRequest` for `MultipartError`\n+/// Return `BadRequest` for `MultipartError`.\n impl ResponseError for MultipartError {\n     fn status_code(&self) -> StatusCode {\n         match &self {\n             MultipartError::Field { source, .. } => source.as_response_error().status_code(),\n+            MultipartError::ContentTypeIncompatible => StatusCode::UNSUPPORTED_MEDIA_TYPE,\n             _ => StatusCode::BAD_REQUEST,\n         }\n     }\n@@ -93,7 +111,7 @@ mod tests {\n \n     #[test]\n     fn test_multipart_error() {\n-        let resp = MultipartError::Boundary.error_response();\n+        let resp = MultipartError::BoundaryMissing.error_response();\n         assert_eq!(resp.status(), StatusCode::BAD_REQUEST);\n     }\n }\ndiff --git a/actix-multipart/src/extractor.rs b/actix-multipart/src/extractor.rs\nindex 56ed69ae457..ab98f887e7d 100644\n--- a/actix-multipart/src/extractor.rs\n+++ b/actix-multipart/src/extractor.rs\n@@ -10,6 +10,7 @@ use crate::server::Multipart;\n /// Content-type: multipart/form-data;\n ///\n /// # Examples\n+///\n /// ```\n /// use actix_web::{web, HttpResponse, Error};\n /// use actix_multipart::Multipart;\n@@ -35,9 +36,6 @@ impl FromRequest for Multipart {\n \n     #[inline]\n     fn from_request(req: &HttpRequest, payload: &mut Payload) -> Self::Future {\n-        ready(Ok(match Multipart::boundary(req.headers()) {\n-            Ok(boundary) => Multipart::from_boundary(boundary, payload.take()),\n-            Err(err) => Multipart::from_error(err),\n-        }))\n+        ready(Ok(Multipart::from_req(req, payload)))\n     }\n }\ndiff --git a/actix-multipart/src/form/bytes.rs b/actix-multipart/src/form/bytes.rs\nindex 3c5e2eb108b..c152db3d053 100644\n--- a/actix-multipart/src/form/bytes.rs\n+++ b/actix-multipart/src/form/bytes.rs\n@@ -41,8 +41,9 @@ impl<'t> FieldReader<'t> for Bytes {\n                 content_type: field.content_type().map(ToOwned::to_owned),\n                 file_name: field\n                     .content_disposition()\n+                    .expect(\"multipart form fields should have a content-disposition header\")\n                     .get_filename()\n-                    .map(str::to_owned),\n+                    .map(ToOwned::to_owned),\n             })\n         })\n     }\ndiff --git a/actix-multipart/src/form/json.rs b/actix-multipart/src/form/json.rs\nindex bb4e03bf614..3504c340cc4 100644\n--- a/actix-multipart/src/form/json.rs\n+++ b/actix-multipart/src/form/json.rs\n@@ -32,7 +32,6 @@ where\n     fn read_field(req: &'t HttpRequest, field: Field, limits: &'t mut Limits) -> Self::Future {\n         Box::pin(async move {\n             let config = JsonConfig::from_req(req);\n-            let field_name = field.name().to_owned();\n \n             if config.validate_content_type {\n                 let valid = if let Some(mime) = field.content_type() {\n@@ -43,17 +42,19 @@ where\n \n                 if !valid {\n                     return Err(MultipartError::Field {\n-                        field_name,\n+                        name: field.form_field_name,\n                         source: config.map_error(req, JsonFieldError::ContentType),\n                     });\n                 }\n             }\n \n+            let form_field_name = field.form_field_name.clone();\n+\n             let bytes = Bytes::read_field(req, field, limits).await?;\n \n             Ok(Json(serde_json::from_slice(bytes.data.as_ref()).map_err(\n                 |err| MultipartError::Field {\n-                    field_name,\n+                    name: form_field_name,\n                     source: config.map_error(req, JsonFieldError::Deserialize(err)),\n                 },\n             )?))\ndiff --git a/actix-multipart/src/form/mod.rs b/actix-multipart/src/form/mod.rs\nindex 68cdefec588..9441d249fc8 100644\n--- a/actix-multipart/src/form/mod.rs\n+++ b/actix-multipart/src/form/mod.rs\n@@ -80,13 +80,13 @@ where\n         state: &'t mut State,\n         duplicate_field: DuplicateField,\n     ) -> Self::Future {\n-        if state.contains_key(field.name()) {\n+        if state.contains_key(&field.form_field_name) {\n             match duplicate_field {\n                 DuplicateField::Ignore => return Box::pin(ready(Ok(()))),\n \n                 DuplicateField::Deny => {\n                     return Box::pin(ready(Err(MultipartError::DuplicateField(\n-                        field.name().to_owned(),\n+                        field.form_field_name,\n                     ))))\n                 }\n \n@@ -95,7 +95,7 @@ where\n         }\n \n         Box::pin(async move {\n-            let field_name = field.name().to_owned();\n+            let field_name = field.form_field_name.clone();\n             let t = T::read_field(req, field, limits).await?;\n             state.insert(field_name, Box::new(t));\n             Ok(())\n@@ -123,10 +123,8 @@ where\n         Box::pin(async move {\n             // Note: Vec GroupReader always allows duplicates\n \n-            let field_name = field.name().to_owned();\n-\n             let vec = state\n-                .entry(field_name)\n+                .entry(field.form_field_name.clone())\n                 .or_insert_with(|| Box::<Vec<T>>::default())\n                 .downcast_mut::<Vec<T>>()\n                 .unwrap();\n@@ -159,13 +157,13 @@ where\n         state: &'t mut State,\n         duplicate_field: DuplicateField,\n     ) -> Self::Future {\n-        if state.contains_key(field.name()) {\n+        if state.contains_key(&field.form_field_name) {\n             match duplicate_field {\n                 DuplicateField::Ignore => return Box::pin(ready(Ok(()))),\n \n                 DuplicateField::Deny => {\n                     return Box::pin(ready(Err(MultipartError::DuplicateField(\n-                        field.name().to_owned(),\n+                        field.form_field_name,\n                     ))))\n                 }\n \n@@ -174,7 +172,7 @@ where\n         }\n \n         Box::pin(async move {\n-            let field_name = field.name().to_owned();\n+            let field_name = field.form_field_name.clone();\n             let t = T::read_field(req, field, limits).await?;\n             state.insert(field_name, Box::new(t));\n             Ok(())\n@@ -281,6 +279,9 @@ impl Limits {\n /// [`MultipartCollect`] trait. You should use the [`macro@MultipartForm`] macro to derive this\n /// for your struct.\n ///\n+/// Note that this extractor rejects requests with any other Content-Type such as `multipart/mixed`,\n+/// `multipart/related`, or non-multipart media types.\n+///\n /// Add a [`MultipartFormConfig`] to your app data to configure extraction.\n #[derive(Deref, DerefMut)]\n pub struct MultipartForm<T: MultipartCollect>(pub T);\n@@ -294,14 +295,24 @@ impl<T: MultipartCollect> MultipartForm<T> {\n \n impl<T> FromRequest for MultipartForm<T>\n where\n-    T: MultipartCollect,\n+    T: MultipartCollect + 'static,\n {\n     type Error = Error;\n     type Future = LocalBoxFuture<'static, Result<Self, Self::Error>>;\n \n     #[inline]\n     fn from_request(req: &HttpRequest, payload: &mut dev::Payload) -> Self::Future {\n-        let mut payload = Multipart::new(req.headers(), payload.take());\n+        let mut multipart = Multipart::from_req(req, payload);\n+\n+        let content_type = match multipart.content_type_or_bail() {\n+            Ok(content_type) => content_type,\n+            Err(err) => return Box::pin(ready(Err(err.into()))),\n+        };\n+\n+        if content_type.subtype() != mime::FORM_DATA {\n+            // this extractor only supports multipart/form-data\n+            return Box::pin(ready(Err(MultipartError::ContentTypeIncompatible.into())));\n+        };\n \n         let config = MultipartFormConfig::from_req(req);\n         let mut limits = Limits::new(config.total_limit, config.memory_limit);\n@@ -313,14 +324,20 @@ where\n         Box::pin(\n             async move {\n                 let mut state = State::default();\n-                // We need to ensure field limits are shared for all instances of this field name\n+\n+                // ensure limits are shared for all fields with this name\n                 let mut field_limits = HashMap::<String, Option<usize>>::new();\n \n-                while let Some(field) = payload.try_next().await? {\n+                while let Some(field) = multipart.try_next().await? {\n+                    debug_assert!(\n+                        !field.form_field_name.is_empty(),\n+                        \"multipart form fields should have names\",\n+                    );\n+\n                     // Retrieve the limit for this field\n                     let entry = field_limits\n-                        .entry(field.name().to_owned())\n-                        .or_insert_with(|| T::limit(field.name()));\n+                        .entry(field.form_field_name.clone())\n+                        .or_insert_with(|| T::limit(&field.form_field_name));\n \n                     limits.field_limit_remaining.clone_from(entry);\n \n@@ -329,6 +346,7 @@ where\n                     // Update the stored limit\n                     *entry = limits.field_limit_remaining;\n                 }\n+\n                 let inner = T::from_state(state)?;\n                 Ok(MultipartForm(inner))\n             }\n@@ -752,6 +770,41 @@ mod tests {\n         assert_eq!(response.status(), StatusCode::BAD_REQUEST);\n     }\n \n+    #[actix_rt::test]\n+    async fn non_multipart_form_data() {\n+        #[derive(MultipartForm)]\n+        struct TestNonMultipartFormData {\n+            #[allow(unused)]\n+            #[multipart(limit = \"30B\")]\n+            foo: Text<String>,\n+        }\n+\n+        async fn non_multipart_form_data_route(\n+            _form: MultipartForm<TestNonMultipartFormData>,\n+        ) -> String {\n+            unreachable!(\"request is sent with multipart/mixed\");\n+        }\n+\n+        let srv = actix_test::start(|| {\n+            App::new().route(\"/\", web::post().to(non_multipart_form_data_route))\n+        });\n+\n+        let mut form = multipart::Form::default();\n+        form.add_text(\"foo\", \"foo\");\n+\n+        // mangle content-type, keeping the boundary\n+        let ct = form.content_type().replacen(\"/form-data\", \"/mixed\", 1);\n+\n+        let res = Client::default()\n+            .post(srv.url(\"/\"))\n+            .content_type(ct)\n+            .send_body(multipart::Body::from(form))\n+            .await\n+            .unwrap();\n+\n+        assert_eq!(res.status(), StatusCode::UNSUPPORTED_MEDIA_TYPE);\n+    }\n+\n     #[should_panic(expected = \"called `Result::unwrap()` on an `Err` value: Connect(Disconnected)\")]\n     #[actix_web::test]\n     async fn field_try_next_panic() {\ndiff --git a/actix-multipart/src/form/tempfile.rs b/actix-multipart/src/form/tempfile.rs\nindex 9371a026b09..f329876f200 100644\n--- a/actix-multipart/src/form/tempfile.rs\n+++ b/actix-multipart/src/form/tempfile.rs\n@@ -42,38 +42,36 @@ impl<'t> FieldReader<'t> for TempFile {\n     fn read_field(req: &'t HttpRequest, mut field: Field, limits: &'t mut Limits) -> Self::Future {\n         Box::pin(async move {\n             let config = TempFileConfig::from_req(req);\n-            let field_name = field.name().to_owned();\n             let mut size = 0;\n \n-            let file = config\n-                .create_tempfile()\n-                .map_err(|err| config.map_error(req, &field_name, TempFileError::FileIo(err)))?;\n+            let file = config.create_tempfile().map_err(|err| {\n+                config.map_error(req, &field.form_field_name, TempFileError::FileIo(err))\n+            })?;\n \n-            let mut file_async =\n-                tokio::fs::File::from_std(file.reopen().map_err(|err| {\n-                    config.map_error(req, &field_name, TempFileError::FileIo(err))\n-                })?);\n+            let mut file_async = tokio::fs::File::from_std(file.reopen().map_err(|err| {\n+                config.map_error(req, &field.form_field_name, TempFileError::FileIo(err))\n+            })?);\n \n             while let Some(chunk) = field.try_next().await? {\n                 limits.try_consume_limits(chunk.len(), false)?;\n                 size += chunk.len();\n                 file_async.write_all(chunk.as_ref()).await.map_err(|err| {\n-                    config.map_error(req, &field_name, TempFileError::FileIo(err))\n+                    config.map_error(req, &field.form_field_name, TempFileError::FileIo(err))\n                 })?;\n             }\n \n-            file_async\n-                .flush()\n-                .await\n-                .map_err(|err| config.map_error(req, &field_name, TempFileError::FileIo(err)))?;\n+            file_async.flush().await.map_err(|err| {\n+                config.map_error(req, &field.form_field_name, TempFileError::FileIo(err))\n+            })?;\n \n             Ok(TempFile {\n                 file,\n                 content_type: field.content_type().map(ToOwned::to_owned),\n                 file_name: field\n                     .content_disposition()\n+                    .expect(\"multipart form fields should have a content-disposition header\")\n                     .get_filename()\n-                    .map(str::to_owned),\n+                    .map(ToOwned::to_owned),\n                 size,\n             })\n         })\n@@ -137,7 +135,7 @@ impl TempFileConfig {\n         };\n \n         MultipartError::Field {\n-            field_name: field_name.to_owned(),\n+            name: field_name.to_owned(),\n             source,\n         }\n     }\ndiff --git a/actix-multipart/src/form/text.rs b/actix-multipart/src/form/text.rs\nindex 83e2115240e..67a434ee6aa 100644\n--- a/actix-multipart/src/form/text.rs\n+++ b/actix-multipart/src/form/text.rs\n@@ -36,7 +36,6 @@ where\n     fn read_field(req: &'t HttpRequest, field: Field, limits: &'t mut Limits) -> Self::Future {\n         Box::pin(async move {\n             let config = TextConfig::from_req(req);\n-            let field_name = field.name().to_owned();\n \n             if config.validate_content_type {\n                 let valid = if let Some(mime) = field.content_type() {\n@@ -49,22 +48,24 @@ where\n \n                 if !valid {\n                     return Err(MultipartError::Field {\n-                        field_name,\n+                        name: field.form_field_name,\n                         source: config.map_error(req, TextError::ContentType),\n                     });\n                 }\n             }\n \n+            let form_field_name = field.form_field_name.clone();\n+\n             let bytes = Bytes::read_field(req, field, limits).await?;\n \n             let text = str::from_utf8(&bytes.data).map_err(|err| MultipartError::Field {\n-                field_name: field_name.clone(),\n+                name: form_field_name.clone(),\n                 source: config.map_error(req, TextError::Utf8Error(err)),\n             })?;\n \n             Ok(Text(serde_plain::from_str(text).map_err(|err| {\n                 MultipartError::Field {\n-                    field_name,\n+                    name: form_field_name,\n                     source: config.map_error(req, TextError::Deserialize(err)),\n                 }\n             })?))\ndiff --git a/actix-multipart/src/lib.rs b/actix-multipart/src/lib.rs\nindex 51b06db38be..853648bebff 100644\n--- a/actix-multipart/src/lib.rs\n+++ b/actix-multipart/src/lib.rs\n@@ -5,7 +5,7 @@\n //! ```no_run\n //! use actix_web::{post, App, HttpServer, Responder};\n //!\n-//! use actix_multipart::form::{json::Json as MPJson, tempfile::TempFile, MultipartForm};\n+//! use actix_multipart::form::{json::Json as MpJson, tempfile::TempFile, MultipartForm};\n //! use serde::Deserialize;\n //!\n //! #[derive(Debug, Deserialize)]\n@@ -17,7 +17,7 @@\n //! struct UploadForm {\n //!     #[multipart(limit = \"100MB\")]\n //!     file: TempFile,\n-//!     json: MPJson<Metadata>,\n+//!     json: MpJson<Metadata>,\n //! }\n //!\n //! #[post(\"/videos\")]\n@@ -36,6 +36,15 @@\n //!         .await\n //! }\n //! ```\n+//!\n+//! cURL request:\n+//!\n+//! ```sh\n+//! curl -v --request POST \\\n+//!   --url http://localhost:8080/videos \\\n+//!   -F 'json={\"name\": \"Cargo.lock\"};type=application/json' \\\n+//!   -F file=@./Cargo.lock\n+//! ```\n \n #![deny(rust_2018_idioms, nonstandard_style)]\n #![warn(future_incompatible)]\n@@ -57,7 +66,4 @@ pub mod test;\n pub use self::{\n     error::MultipartError,\n     server::{Field, Multipart},\n-    test::{\n-        create_form_data_payload_and_headers, create_form_data_payload_and_headers_with_boundary,\n-    },\n };\ndiff --git a/actix-multipart/src/server.rs b/actix-multipart/src/server.rs\nindex 0256aa7bffe..76eae11ffcd 100644\n--- a/actix-multipart/src/server.rs\n+++ b/actix-multipart/src/server.rs\n@@ -10,12 +10,15 @@ use std::{\n };\n \n use actix_web::{\n+    dev,\n     error::{ParseError, PayloadError},\n     http::header::{self, ContentDisposition, HeaderMap, HeaderName, HeaderValue},\n+    HttpRequest,\n };\n use bytes::{Bytes, BytesMut};\n use futures_core::stream::{LocalBoxStream, Stream};\n use local_waker::LocalWaker;\n+use mime::Mime;\n \n use crate::error::MultipartError;\n \n@@ -23,87 +26,79 @@ const MAX_HEADERS: usize = 32;\n \n /// The server-side implementation of `multipart/form-data` requests.\n ///\n-/// This will parse the incoming stream into `MultipartItem` instances via its\n-/// Stream implementation.\n-/// `MultipartItem::Field` contains multipart field. `MultipartItem::Multipart`\n-/// is used for nested multipart streams.\n+/// This will parse the incoming stream into `MultipartItem` instances via its `Stream`\n+/// implementation. `MultipartItem::Field` contains multipart field. `MultipartItem::Multipart` is\n+/// used for nested multipart streams.\n pub struct Multipart {\n     safety: Safety,\n-    error: Option<MultipartError>,\n     inner: Option<InnerMultipart>,\n-}\n-\n-enum InnerMultipartItem {\n-    None,\n-    Field(Rc<RefCell<InnerField>>),\n-}\n-\n-#[derive(PartialEq, Debug)]\n-enum InnerState {\n-    /// Stream eof\n-    Eof,\n-\n-    /// Skip data until first boundary\n-    FirstBoundary,\n-\n-    /// Reading boundary\n-    Boundary,\n-\n-    /// Reading Headers,\n-    Headers,\n-}\n-\n-struct InnerMultipart {\n-    payload: PayloadRef,\n-    boundary: String,\n-    state: InnerState,\n-    item: InnerMultipartItem,\n+    error: Option<MultipartError>,\n }\n \n impl Multipart {\n-    /// Create multipart instance for boundary.\n-    pub fn new<S>(headers: &HeaderMap, stream: S) -> Multipart\n+    /// Creates multipart instance from parts.\n+    pub fn new<S>(headers: &HeaderMap, stream: S) -> Self\n     where\n         S: Stream<Item = Result<Bytes, PayloadError>> + 'static,\n     {\n-        match Self::boundary(headers) {\n-            Ok(boundary) => Multipart::from_boundary(boundary, stream),\n-            Err(err) => Multipart::from_error(err),\n+        match Self::find_ct_and_boundary(headers) {\n+            Ok((ct, boundary)) => Self::from_ct_and_boundary(ct, boundary, stream),\n+            Err(err) => Self::from_error(err),\n         }\n     }\n \n-    /// Extract boundary info from headers.\n-    pub(crate) fn boundary(headers: &HeaderMap) -> Result<String, MultipartError> {\n-        headers\n+    /// Creates multipart instance from parts.\n+    pub(crate) fn from_req(req: &HttpRequest, payload: &mut dev::Payload) -> Self {\n+        match Self::find_ct_and_boundary(req.headers()) {\n+            Ok((ct, boundary)) => Self::from_ct_and_boundary(ct, boundary, payload.take()),\n+            Err(err) => Self::from_error(err),\n+        }\n+    }\n+\n+    /// Extract Content-Type and boundary info from headers.\n+    pub(crate) fn find_ct_and_boundary(\n+        headers: &HeaderMap,\n+    ) -> Result<(Mime, String), MultipartError> {\n+        let content_type = headers\n             .get(&header::CONTENT_TYPE)\n-            .ok_or(MultipartError::NoContentType)?\n+            .ok_or(MultipartError::ContentTypeMissing)?\n             .to_str()\n             .ok()\n-            .and_then(|content_type| content_type.parse::<mime::Mime>().ok())\n-            .ok_or(MultipartError::ParseContentType)?\n+            .and_then(|content_type| content_type.parse::<Mime>().ok())\n+            .ok_or(MultipartError::ContentTypeParse)?;\n+\n+        if content_type.type_() != mime::MULTIPART {\n+            return Err(MultipartError::ContentTypeIncompatible);\n+        }\n+\n+        let boundary = content_type\n             .get_param(mime::BOUNDARY)\n-            .map(|boundary| boundary.as_str().to_owned())\n-            .ok_or(MultipartError::Boundary)\n+            .ok_or(MultipartError::BoundaryMissing)?\n+            .as_str()\n+            .to_owned();\n+\n+        Ok((content_type, boundary))\n     }\n \n-    /// Create multipart instance for given boundary and stream\n-    pub(crate) fn from_boundary<S>(boundary: String, stream: S) -> Multipart\n+    /// Constructs a new multipart reader from given Content-Type, boundary, and stream.\n+    pub(crate) fn from_ct_and_boundary<S>(ct: Mime, boundary: String, stream: S) -> Multipart\n     where\n         S: Stream<Item = Result<Bytes, PayloadError>> + 'static,\n     {\n         Multipart {\n-            error: None,\n             safety: Safety::new(),\n             inner: Some(InnerMultipart {\n-                boundary,\n                 payload: PayloadRef::new(PayloadBuffer::new(stream)),\n+                content_type: ct,\n+                boundary,\n                 state: InnerState::FirstBoundary,\n                 item: InnerMultipartItem::None,\n             }),\n+            error: None,\n         }\n     }\n \n-    /// Create Multipart instance from MultipartError\n+    /// Constructs a new multipart reader from given `MultipartError`.\n     pub(crate) fn from_error(err: MultipartError) -> Multipart {\n         Multipart {\n             error: Some(err),\n@@ -111,6 +106,21 @@ impl Multipart {\n             inner: None,\n         }\n     }\n+\n+    /// Return requests parsed Content-Type or raise the stored error.\n+    pub(crate) fn content_type_or_bail(&mut self) -> Result<mime::Mime, MultipartError> {\n+        if let Some(err) = self.error.take() {\n+            return Err(err);\n+        }\n+\n+        Ok(self\n+            .inner\n+            .as_ref()\n+            // TODO: look into using enum instead of two options\n+            .expect(\"multipart requests should have state\")\n+            .content_type\n+            .clone())\n+    }\n }\n \n impl Stream for Multipart {\n@@ -141,8 +151,46 @@ impl Stream for Multipart {\n     }\n }\n \n+#[derive(PartialEq, Debug)]\n+enum InnerState {\n+    /// Stream EOF.\n+    Eof,\n+\n+    /// Skip data until first boundary.\n+    FirstBoundary,\n+\n+    /// Reading boundary.\n+    Boundary,\n+\n+    /// Reading Headers.\n+    Headers,\n+}\n+\n+enum InnerMultipartItem {\n+    None,\n+    Field(Rc<RefCell<InnerField>>),\n+}\n+\n+struct InnerMultipart {\n+    /// Request's payload stream & buffer.\n+    payload: PayloadRef,\n+\n+    /// Request's Content-Type.\n+    ///\n+    /// Guaranteed to have \"multipart\" top-level media type, i.e., `multipart/*`.\n+    content_type: Mime,\n+\n+    /// Field boundary.\n+    boundary: String,\n+\n+    state: InnerState,\n+    item: InnerMultipartItem,\n+}\n+\n impl InnerMultipart {\n-    fn read_headers(payload: &mut PayloadBuffer) -> Result<Option<HeaderMap>, MultipartError> {\n+    fn read_field_headers(\n+        payload: &mut PayloadBuffer,\n+    ) -> Result<Option<HeaderMap>, MultipartError> {\n         match payload.read_until(b\"\\r\\n\\r\\n\")? {\n             None => {\n                 if payload.eof {\n@@ -153,6 +201,7 @@ impl InnerMultipart {\n             }\n             Some(bytes) => {\n                 let mut hdrs = [httparse::EMPTY_HEADER; MAX_HEADERS];\n+\n                 match httparse::parse_headers(&bytes, &mut hdrs) {\n                     Ok(httparse::Status::Complete((_, hdrs))) => {\n                         // convert headers\n@@ -193,7 +242,7 @@ impl InnerMultipart {\n                     || &chunk[..2] != b\"--\"\n                     || &chunk[2..boundary.len() + 2] != boundary.as_bytes()\n                 {\n-                    Err(MultipartError::Boundary)\n+                    Err(MultipartError::BoundaryMissing)\n                 } else if &chunk[boundary.len() + 2..] == b\"\\r\\n\" {\n                     Ok(Some(false))\n                 } else if &chunk[boundary.len() + 2..boundary.len() + 4] == b\"--\"\n@@ -202,7 +251,7 @@ impl InnerMultipart {\n                 {\n                     Ok(Some(true))\n                 } else {\n-                    Err(MultipartError::Boundary)\n+                    Err(MultipartError::BoundaryMissing)\n                 }\n             }\n         }\n@@ -217,7 +266,7 @@ impl InnerMultipart {\n             match payload.readline()? {\n                 Some(chunk) => {\n                     if chunk.is_empty() {\n-                        return Err(MultipartError::Boundary);\n+                        return Err(MultipartError::BoundaryMissing);\n                     }\n                     if chunk.len() < boundary.len() {\n                         continue;\n@@ -282,7 +331,7 @@ impl InnerMultipart {\n                 }\n             }\n \n-            let headers = if let Some(mut payload) = self.payload.get_mut(safety) {\n+            let field_headers = if let Some(mut payload) = self.payload.get_mut(safety) {\n                 match self.state {\n                     // read until first boundary\n                     InnerState::FirstBoundary => {\n@@ -317,7 +366,7 @@ impl InnerMultipart {\n \n                 // read field headers for next field\n                 if self.state == InnerState::Headers {\n-                    if let Some(headers) = InnerMultipart::read_headers(&mut payload)? {\n+                    if let Some(headers) = InnerMultipart::read_field_headers(&mut payload)? {\n                         self.state = InnerState::Boundary;\n                         headers\n                     } else {\n@@ -331,31 +380,37 @@ impl InnerMultipart {\n                 return Poll::Pending;\n             };\n \n-            // According to RFC 7578 \u00a74.2, a Content-Disposition header must always be present and\n-            // set to \"form-data\".\n-\n-            let content_disposition = headers\n+            let field_content_disposition = field_headers\n                 .get(&header::CONTENT_DISPOSITION)\n                 .and_then(|cd| ContentDisposition::from_raw(cd).ok())\n                 .filter(|content_disposition| {\n-                    let is_form_data =\n-                        content_disposition.disposition == header::DispositionType::FormData;\n+                    matches!(\n+                        content_disposition.disposition,\n+                        header::DispositionType::FormData,\n+                    )\n+                });\n \n-                    let has_field_name = content_disposition\n-                        .parameters\n-                        .iter()\n-                        .any(|param| matches!(param, header::DispositionParam::Name(_)));\n+            let form_field_name = if self.content_type.subtype() == mime::FORM_DATA {\n+                // According to RFC 7578 \u00a74.2, which relates to \"multipart/form-data\" requests\n+                // specifically, fields must have a Content-Disposition header, its disposition\n+                // type must be set as \"form-data\", and it must have a name parameter.\n \n-                    is_form_data && has_field_name\n-                });\n+                let Some(cd) = &field_content_disposition else {\n+                    return Poll::Ready(Some(Err(MultipartError::ContentDispositionMissing)));\n+                };\n \n-            let cd = if let Some(content_disposition) = content_disposition {\n-                content_disposition\n+                let Some(field_name) = cd.get_name() else {\n+                    return Poll::Ready(Some(Err(MultipartError::ContentDispositionNameMissing)));\n+                };\n+\n+                Some(field_name.to_owned())\n             } else {\n-                return Poll::Ready(Some(Err(MultipartError::NoContentDisposition)));\n+                None\n             };\n \n-            let ct: Option<mime::Mime> = headers\n+            // TODO: check out other multipart/* RFCs for specific requirements\n+\n+            let field_content_type: Option<Mime> = field_headers\n                 .get(&header::CONTENT_TYPE)\n                 .and_then(|ct| ct.to_str().ok())\n                 .and_then(|ct| ct.parse().ok());\n@@ -363,23 +418,24 @@ impl InnerMultipart {\n             self.state = InnerState::Boundary;\n \n             // nested multipart stream is not supported\n-            if let Some(mime) = &ct {\n+            if let Some(mime) = &field_content_type {\n                 if mime.type_() == mime::MULTIPART {\n                     return Poll::Ready(Some(Err(MultipartError::Nested)));\n                 }\n             }\n \n-            let field =\n-                InnerField::new_in_rc(self.payload.clone(), self.boundary.clone(), &headers)?;\n+            let field_inner =\n+                InnerField::new_in_rc(self.payload.clone(), self.boundary.clone(), &field_headers)?;\n \n-            self.item = InnerMultipartItem::Field(Rc::clone(&field));\n+            self.item = InnerMultipartItem::Field(Rc::clone(&field_inner));\n \n             Poll::Ready(Some(Ok(Field::new(\n+                field_content_type,\n+                field_content_disposition,\n+                form_field_name,\n+                field_headers,\n                 safety.clone(cx),\n-                headers,\n-                ct,\n-                cd,\n-                field,\n+                field_inner,\n             ))))\n         }\n     }\n@@ -392,26 +448,42 @@ impl Drop for InnerMultipart {\n     }\n }\n \n-/// A single field in a multipart stream\n+/// A single field in a multipart stream.\n pub struct Field {\n-    ct: Option<mime::Mime>,\n-    cd: ContentDisposition,\n+    /// Field's Content-Type.\n+    content_type: Option<Mime>,\n+\n+    /// Field's Content-Disposition.\n+    content_disposition: Option<ContentDisposition>,\n+\n+    /// Form field name.\n+    ///\n+    /// A non-optional storage for form field names to avoid unwraps in `form` module. Will be an\n+    /// empty string in non-form contexts.\n+    ///\n+    // INVARIANT: always non-empty when request content-type is multipart/form-data.\n+    pub(crate) form_field_name: String,\n+\n+    /// Field's header map.\n     headers: HeaderMap,\n-    inner: Rc<RefCell<InnerField>>,\n+\n     safety: Safety,\n+    inner: Rc<RefCell<InnerField>>,\n }\n \n impl Field {\n     fn new(\n-        safety: Safety,\n+        content_type: Option<Mime>,\n+        content_disposition: Option<ContentDisposition>,\n+        form_field_name: Option<String>,\n         headers: HeaderMap,\n-        ct: Option<mime::Mime>,\n-        cd: ContentDisposition,\n+        safety: Safety,\n         inner: Rc<RefCell<InnerField>>,\n     ) -> Self {\n         Field {\n-            ct,\n-            cd,\n+            content_type,\n+            content_disposition,\n+            form_field_name: form_field_name.unwrap_or_default(),\n             headers,\n             inner,\n             safety,\n@@ -428,34 +500,36 @@ impl Field {\n     /// According to [RFC 7578](https://www.rfc-editor.org/rfc/rfc7578#section-4.4), if it is not\n     /// present, it should default to \"text/plain\". Note it is the responsibility of the client to\n     /// provide the appropriate content type, there is no attempt to validate this by the server.\n-    pub fn content_type(&self) -> Option<&mime::Mime> {\n-        self.ct.as_ref()\n+    pub fn content_type(&self) -> Option<&Mime> {\n+        self.content_type.as_ref()\n     }\n \n-    /// Returns the field's Content-Disposition.\n+    /// Returns this field's parsed Content-Disposition header, if set.\n+    ///\n+    /// # Validation\n     ///\n-    /// Per [RFC 7578 \u00a74.2]: \"Each part MUST contain a Content-Disposition header field where the\n-    /// disposition type is `form-data`. The Content-Disposition header field MUST also contain an\n-    /// additional parameter of `name`; the value of the `name` parameter is the original field name\n-    /// from the form.\"\n+    /// Per [RFC 7578 \u00a74.2], the parts of a multipart/form-data payload MUST contain a\n+    /// Content-Disposition header field where the disposition type is `form-data` and MUST also\n+    /// contain an additional parameter of `name` with its value being the original field name from\n+    /// the form. This requirement is enforced during extraction for multipart/form-data requests,\n+    /// but not other kinds of multipart requests (such as multipart/related).\n     ///\n-    /// This crate validates that it exists before returning a `Field`. As such, it is safe to\n-    /// unwrap `.content_disposition().get_name()`. The [name](Self::name) method is provided as\n-    /// a convenience.\n+    /// As such, it is safe to `.unwrap()` calls `.content_disposition()` if you've verified.\n+    ///\n+    /// The [`name()`](Self::name) method is also provided as a convenience for obtaining the\n+    /// aforementioned name parameter.\n     ///\n     /// [RFC 7578 \u00a74.2]: https://datatracker.ietf.org/doc/html/rfc7578#section-4.2\n-    pub fn content_disposition(&self) -> &ContentDisposition {\n-        &self.cd\n+    pub fn content_disposition(&self) -> Option<&ContentDisposition> {\n+        self.content_disposition.as_ref()\n     }\n \n-    /// Returns the field's name.\n+    /// Returns the field's name, if set.\n     ///\n-    /// See [content_disposition](Self::content_disposition) regarding guarantees about existence of\n-    /// the name field.\n-    pub fn name(&self) -> &str {\n-        self.content_disposition()\n-            .get_name()\n-            .expect(\"field name should be guaranteed to exist in multipart form-data\")\n+    /// See [`content_disposition()`](Self::content_disposition) regarding guarantees on presence of\n+    /// the \"name\" field.\n+    pub fn name(&self) -> Option<&str> {\n+        self.content_disposition()?.get_name()\n     }\n }\n \n@@ -465,6 +539,7 @@ impl Stream for Field {\n     fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {\n         let this = self.get_mut();\n         let mut inner = this.inner.borrow_mut();\n+\n         if let Some(mut buffer) = inner\n             .payload\n             .as_ref()\n@@ -486,7 +561,7 @@ impl Stream for Field {\n \n impl fmt::Debug for Field {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        if let Some(ct) = &self.ct {\n+        if let Some(ct) = &self.content_type {\n             writeln!(f, \"\\nField: {}\", ct)?;\n         } else {\n             writeln!(f, \"\\nField:\")?;\n@@ -570,6 +645,7 @@ impl InnerField {\n     }\n \n     /// Reads content chunk of body part with unknown length.\n+    ///\n     /// The `Content-Length` header for body part is not necessary.\n     fn read_stream(\n         payload: &mut PayloadBuffer,\n@@ -704,8 +780,8 @@ impl PayloadRef {\n         }\n     }\n \n-    fn get_mut(&self, s: &Safety) -> Option<RefMut<'_, PayloadBuffer>> {\n-        if s.current() {\n+    fn get_mut(&self, safety: &Safety) -> Option<RefMut<'_, PayloadBuffer>> {\n+        if safety.current() {\n             Some(self.payload.borrow_mut())\n         } else {\n             None\n@@ -722,10 +798,11 @@ impl Clone for PayloadRef {\n }\n \n /// Counter. It tracks of number of clones of payloads and give access to payload only to top most.\n-/// * When dropped, parent task is awakened. This is to support the case where Field is\n-/// dropped in a separate task than Multipart.\n-/// * Assumes that parent owners don't move to different tasks; only the top-most is allowed to.\n-/// * If dropped and is not top most owner, is_clean flag is set to false.\n+///\n+/// - When dropped, parent task is awakened. This is to support the case where `Field` is dropped in\n+///   a separate task than `Multipart`.\n+/// - Assumes that parent owners don't move to different tasks; only the top-most is allowed to.\n+/// - If dropped and is not top most owner, is_clean flag is set to false.\n #[derive(Debug)]\n struct Safety {\n     task: LocalWaker,\n@@ -876,6 +953,7 @@ mod tests {\n         test::TestRequest,\n         FromRequest,\n     };\n+    use assert_matches::assert_matches;\n     use bytes::BufMut as _;\n     use futures_util::{future::lazy, StreamExt as _};\n     use tokio::sync::mpsc;\n@@ -888,8 +966,8 @@ mod tests {\n     #[actix_rt::test]\n     async fn test_boundary() {\n         let headers = HeaderMap::new();\n-        match Multipart::boundary(&headers) {\n-            Err(MultipartError::NoContentType) => {}\n+        match Multipart::find_ct_and_boundary(&headers) {\n+            Err(MultipartError::ContentTypeMissing) => {}\n             _ => unreachable!(\"should not happen\"),\n         }\n \n@@ -899,8 +977,8 @@ mod tests {\n             header::HeaderValue::from_static(\"test\"),\n         );\n \n-        match Multipart::boundary(&headers) {\n-            Err(MultipartError::ParseContentType) => {}\n+        match Multipart::find_ct_and_boundary(&headers) {\n+            Err(MultipartError::ContentTypeParse) => {}\n             _ => unreachable!(\"should not happen\"),\n         }\n \n@@ -909,8 +987,8 @@ mod tests {\n             header::CONTENT_TYPE,\n             header::HeaderValue::from_static(\"multipart/mixed\"),\n         );\n-        match Multipart::boundary(&headers) {\n-            Err(MultipartError::Boundary) => {}\n+        match Multipart::find_ct_and_boundary(&headers) {\n+            Err(MultipartError::BoundaryMissing) => {}\n             _ => unreachable!(\"should not happen\"),\n         }\n \n@@ -923,8 +1001,8 @@ mod tests {\n         );\n \n         assert_eq!(\n-            Multipart::boundary(&headers).unwrap(),\n-            \"5c02368e880e436dab70ed54e1c58209\"\n+            Multipart::find_ct_and_boundary(&headers).unwrap().1,\n+            \"5c02368e880e436dab70ed54e1c58209\",\n         );\n     }\n \n@@ -1059,7 +1137,7 @@ mod tests {\n         let mut multipart = Multipart::new(&headers, payload);\n         match multipart.next().await {\n             Some(Ok(mut field)) => {\n-                let cd = field.content_disposition();\n+                let cd = field.content_disposition().unwrap();\n                 assert_eq!(cd.disposition, DispositionType::FormData);\n                 assert_eq!(cd.parameters[0], DispositionParam::Name(\"file\".into()));\n \n@@ -1121,7 +1199,7 @@ mod tests {\n         let mut multipart = Multipart::new(&headers, payload);\n         match multipart.next().await.unwrap() {\n             Ok(mut field) => {\n-                let cd = field.content_disposition();\n+                let cd = field.content_disposition().unwrap();\n                 assert_eq!(cd.disposition, DispositionType::FormData);\n                 assert_eq!(cd.parameters[0], DispositionParam::Name(\"file\".into()));\n \n@@ -1245,7 +1323,7 @@ mod tests {\n \n     #[actix_rt::test]\n     async fn test_multipart_from_error() {\n-        let err = MultipartError::NoContentType;\n+        let err = MultipartError::ContentTypeMissing;\n         let mut multipart = Multipart::from_error(err);\n         assert!(multipart.next().await.unwrap().is_err())\n     }\n@@ -1254,9 +1332,8 @@ mod tests {\n     async fn test_multipart_from_boundary() {\n         let (_, payload) = create_stream();\n         let (_, headers) = create_simple_request_with_header();\n-        let boundary = Multipart::boundary(&headers);\n-        assert!(boundary.is_ok());\n-        let _ = Multipart::from_boundary(boundary.unwrap(), payload);\n+        let (ct, boundary) = Multipart::find_ct_and_boundary(&headers).unwrap();\n+        let _ = Multipart::from_ct_and_boundary(ct, boundary, payload);\n     }\n \n     #[actix_rt::test]\n@@ -1278,11 +1355,43 @@ mod tests {\n     }\n \n     #[actix_rt::test]\n-    async fn no_content_disposition() {\n+    async fn no_content_disposition_form_data() {\n         let bytes = Bytes::from(\n             \"testasdadsad\\r\\n\\\n              --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\\\n-             Content-Type: text/plain; charset=utf-8\\r\\nContent-Length: 4\\r\\n\\r\\n\\\n+             Content-Type: text/plain; charset=utf-8\\r\\n\\\n+             Content-Length: 4\\r\\n\\\n+             \\r\\n\\\n+             test\\r\\n\\\n+             --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\",\n+        );\n+        let mut headers = HeaderMap::new();\n+        headers.insert(\n+            header::CONTENT_TYPE,\n+            header::HeaderValue::from_static(\n+                \"multipart/form-data; boundary=\\\"abbc761f78ff4d7cb7573b5a23f96ef0\\\"\",\n+            ),\n+        );\n+        let payload = SlowStream::new(bytes);\n+\n+        let mut multipart = Multipart::new(&headers, payload);\n+        let res = multipart.next().await.unwrap();\n+        assert_matches!(\n+            res.expect_err(\n+                \"according to RFC 7578, form-data fields require a content-disposition header\"\n+            ),\n+            MultipartError::ContentDispositionMissing\n+        );\n+    }\n+\n+    #[actix_rt::test]\n+    async fn no_content_disposition_non_form_data() {\n+        let bytes = Bytes::from(\n+            \"testasdadsad\\r\\n\\\n+             --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\\\n+             Content-Type: text/plain; charset=utf-8\\r\\n\\\n+             Content-Length: 4\\r\\n\\\n+             \\r\\n\\\n              test\\r\\n\\\n              --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\",\n         );\n@@ -1297,20 +1406,18 @@ mod tests {\n \n         let mut multipart = Multipart::new(&headers, payload);\n         let res = multipart.next().await.unwrap();\n-        assert!(res.is_err());\n-        assert!(matches!(\n-            res.unwrap_err(),\n-            MultipartError::NoContentDisposition,\n-        ));\n+        res.unwrap();\n     }\n \n     #[actix_rt::test]\n-    async fn no_name_in_content_disposition() {\n+    async fn no_name_in_form_data_content_disposition() {\n         let bytes = Bytes::from(\n             \"testasdadsad\\r\\n\\\n              --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\\\n              Content-Disposition: form-data; filename=\\\"fn.txt\\\"\\r\\n\\\n-             Content-Type: text/plain; charset=utf-8\\r\\nContent-Length: 4\\r\\n\\r\\n\\\n+             Content-Type: text/plain; charset=utf-8\\r\\n\\\n+             Content-Length: 4\\r\\n\\\n+             \\r\\n\\\n              test\\r\\n\\\n              --abbc761f78ff4d7cb7573b5a23f96ef0\\r\\n\",\n         );\n@@ -1318,18 +1425,17 @@ mod tests {\n         headers.insert(\n             header::CONTENT_TYPE,\n             header::HeaderValue::from_static(\n-                \"multipart/mixed; boundary=\\\"abbc761f78ff4d7cb7573b5a23f96ef0\\\"\",\n+                \"multipart/form-data; boundary=\\\"abbc761f78ff4d7cb7573b5a23f96ef0\\\"\",\n             ),\n         );\n         let payload = SlowStream::new(bytes);\n \n         let mut multipart = Multipart::new(&headers, payload);\n         let res = multipart.next().await.unwrap();\n-        assert!(res.is_err());\n-        assert!(matches!(\n-            res.unwrap_err(),\n-            MultipartError::NoContentDisposition,\n-        ));\n+        assert_matches!(\n+            res.expect_err(\"according to RFC 7578, form-data fields require a name attribute\"),\n+            MultipartError::ContentDispositionNameMissing\n+        );\n     }\n \n     #[actix_rt::test]\n@@ -1362,7 +1468,7 @@ mod tests {\n         let mut field = multipart.next().await.unwrap().unwrap();\n \n         let task = rt::spawn(async move {\n-            rt::time::sleep(Duration::from_secs(1)).await;\n+            rt::time::sleep(Duration::from_millis(500)).await;\n             assert_eq!(field.next().await.unwrap().unwrap(), \"test\");\n             drop(field);\n         });\ndiff --git a/actix-web/src/http/header/content_disposition.rs b/actix-web/src/http/header/content_disposition.rs\nindex 9725cd19b36..824bf1195c9 100644\n--- a/actix-web/src/http/header/content_disposition.rs\n+++ b/actix-web/src/http/header/content_disposition.rs\n@@ -154,7 +154,7 @@ impl DispositionParam {\n     #[inline]\n     pub fn as_name(&self) -> Option<&str> {\n         match self {\n-            DispositionParam::Name(ref name) => Some(name.as_str()),\n+            DispositionParam::Name(name) => Some(name.as_str()),\n             _ => None,\n         }\n     }\n@@ -163,7 +163,7 @@ impl DispositionParam {\n     #[inline]\n     pub fn as_filename(&self) -> Option<&str> {\n         match self {\n-            DispositionParam::Filename(ref filename) => Some(filename.as_str()),\n+            DispositionParam::Filename(filename) => Some(filename.as_str()),\n             _ => None,\n         }\n     }\n@@ -172,7 +172,7 @@ impl DispositionParam {\n     #[inline]\n     pub fn as_filename_ext(&self) -> Option<&ExtendedValue> {\n         match self {\n-            DispositionParam::FilenameExt(ref value) => Some(value),\n+            DispositionParam::FilenameExt(value) => Some(value),\n             _ => None,\n         }\n     }\n", "instance_id": "actix__actix-web-3416", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in its intent to address the issue of making the `Content-Disposition` header optional for certain multipart content types (e.g., `multipart/related`) while maintaining its mandatory status for `multipart/form-data`. It provides relevant references to standards (e.g., W3.org documentation) and includes a sample request and error message to illustrate the issue. Additionally, it specifies the expected behavior and current behavior, along with a possible solution (making `Field.cd` an `Option` again). However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not fully clarify the scope of multipart types beyond `multipart/related` and `multipart/form-data` that should be affected by this change. It also lacks explicit mention of potential edge cases or compatibility concerns with existing codebases relying on the current behavior. While the provided code changes help infer the intent, the problem statement itself could benefit from more detailed constraints or examples covering different multipart scenarios.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules within the `actix-multipart` crate, including error handling, field parsing, and form extraction logic. The changes impact core functionality related to how multipart requests are processed, which requires a deep understanding of the `actix-web` framework's architecture and its handling of HTTP headers and payloads. Second, the problem involves several technical concepts, such as parsing HTTP headers (`Content-Type`, `Content-Disposition`), understanding MIME types, and adhering to RFC standards (e.g., RFC 7578 for `multipart/form-data`). It also requires familiarity with Rust's type system (e.g., changing fields to `Option` types) and async programming with `futures` and `tokio`. Third, the modifications introduce complexity in error handling and edge cases, such as distinguishing between different multipart subtypes (`form-data` vs. `related`) and ensuring backward compatibility with existing error behaviors. While not at the extreme end of difficulty (e.g., requiring system-level or distributed systems knowledge), this problem demands a solid grasp of web protocol intricacies and careful refactoring to avoid breaking existing functionality. The need to update tests and documentation further adds to the workload. Overall, this is a challenging task suitable for someone with intermediate to advanced experience in Rust and web development.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.65}
{"problem_statement": "lsp: It's not obvious when an autocomplete will result in an import\nVersion: Deno 2.1.6\n\nIn projects that use tsc, autocompletes that will result in new imports being added have the text 'Add import from':\n<img width=\"559\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/433972d2-b77d-4034-b68b-0cfcded5d2b2\" />\n\nWhereas in Deno projects don't make this clear:\n<img width=\"553\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9f887a10-7af9-4a26-98e3-51fa79e3fb1b\" />\n\nOn top of that, when you import something with tsc, the entire page jumps down one line which immediately makes it clear that you just imported something. This doesn't happen with Deno, which makes it easy to accidentally import obscure stuff like `serialize` from `node:v8` when you actually meant to autocomplete a local `serializeData` symbol or something similar.\n\nNot a huge problem, since a good linter will catch this, but it's still annoying when you open your page in the browser, only to find out it doesn't work due to some node api being imported.\n", "patch": "diff --git a/cli/lsp/tsc.rs b/cli/lsp/tsc.rs\nindex 5139c8bc15e820..a59c056201ed85 100644\n--- a/cli/lsp/tsc.rs\n+++ b/cli/lsp/tsc.rs\n@@ -1,5 +1,6 @@\n // Copyright 2018-2025 the Deno authors. MIT license.\n \n+use std::borrow::Cow;\n use std::cell::RefCell;\n use std::cmp;\n use std::collections::BTreeMap;\n@@ -3612,7 +3613,26 @@ impl CompletionEntryDetails {\n       None\n     };\n     let mut text_edit = original_item.text_edit.clone();\n+    let mut code_action_descriptions = self\n+      .code_actions\n+      .iter()\n+      .flatten()\n+      .map(|a| Cow::Borrowed(a.description.as_str()))\n+      .collect::<Vec<_>>();\n     if let Some(specifier_rewrite) = &data.specifier_rewrite {\n+      for description in &mut code_action_descriptions {\n+        let specifier_index = description\n+          .char_indices()\n+          .find_map(|(b, c)| (c == '\\'' || c == '\"').then_some(b));\n+        if let Some(i) = specifier_index {\n+          let mut specifier_part = description.to_mut().split_off(i);\n+          specifier_part = specifier_part.replace(\n+            &specifier_rewrite.old_specifier,\n+            &specifier_rewrite.new_specifier,\n+          );\n+          description.to_mut().push_str(&specifier_part);\n+        }\n+      }\n       if let Some(text_edit) = &mut text_edit {\n         let new_text = match text_edit {\n           lsp::CompletionTextEdit::Edit(text_edit) => &mut text_edit.new_text,\n@@ -3639,6 +3659,16 @@ impl CompletionEntryDetails {\n         }\n       }\n     }\n+    let code_action_description =\n+      Some(code_action_descriptions.join(\"\\n\\n\")).filter(|s| !s.is_empty());\n+    let detail = Some(\n+      [code_action_description, detail]\n+        .into_iter()\n+        .flatten()\n+        .collect::<Vec<_>>()\n+        .join(\"\\n\\n\"),\n+    )\n+    .filter(|s| !s.is_empty());\n     let (command, additional_text_edits) = parse_code_actions(\n       self.code_actions.as_ref(),\n       data,\n", "instance_id": "denoland__deno-28088", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: autocompletes in Deno do not indicate when an import will be added, unlike in projects using tsc, and there is no visual feedback (like a page jump) when an import occurs. This can lead to accidental imports of unintended modules. The goal of improving the user experience by making this behavior more explicit is evident. However, the statement lacks specific technical requirements or expectations for the solution (e.g., how the indication should be implemented, whether it should mimic tsc's behavior exactly, or if there are specific UI/UX guidelines to follow). Additionally, edge cases or potential challenges (e.g., handling multiple imports or conflicts) are not mentioned. While the issue is valid and the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of the code changes is relatively focused, primarily affecting a single file (`cli/lsp/tsc.rs`) and a specific part of the logic related to autocompletion and code actions in the LSP (Language Server Protocol) implementation. The changes involve modifying how completion entry details are processed to include descriptions of code actions (e.g., import statements) with specifier rewriting, which requires understanding of the existing codebase's handling of text edits and code actions. \n\nSecond, the technical concepts involved include familiarity with Rust (specifically string manipulation with `Cow`, collections, and string joining), LSP protocols (completion and code action handling), and the internal architecture of Deno's LSP implementation. These are moderately complex, especially for someone unfamiliar with LSP or Deno's internals, but manageable for an experienced developer with Rust and systems programming knowledge.\n\nThird, the code changes are not trivial but also not extensive\u2014around 30 lines of added code with logic for string manipulation and concatenation of descriptions. The impact is localized and does not appear to affect the broader system architecture significantly. However, it does require careful handling to ensure that the rewritten specifiers in descriptions are accurate and do not introduce formatting issues or bugs in the UI feedback.\n\nFinally, while the problem statement does not explicitly mention edge cases, the code changes suggest potential challenges, such as correctly parsing and rewriting specifier strings within descriptions (e.g., handling quotes and ensuring the replacement logic is robust). Error handling does not seem to be a major focus of the change, as the code primarily deals with formatting and concatenation rather than introducing new failure paths.\n\nOverall, this problem requires a moderate level of understanding of the codebase and LSP concepts, along with careful implementation to avoid subtle bugs in string handling or UI feedback. It is not a simple bug fix but also not a deep architectural change, placing it in the medium difficulty range at 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.55}
{"problem_statement": "Incorrect Content-Encoding for pre-compressed zstd files\n### Search for duplicate issues\n\n- [X] I already searched, and this issue is not a duplicate.\n\n### Issue scope\n\nLibrary (crate)\n\n### Describe the bug\n\nWhen serving pre-compressed files using zstd, the Content-Encoding header is set to \"zst\" instead of the correct \"zstd\". This causes browsers and other clients to fail to handle the content properly.\n\n### How to reproduce it\n\n1. Start the static-web-server with compression-static enabled and with zstd compressed files. e.g. \r\n```\r\necho \"Test Content\" > index.html\r\nzstd index.html\r\ndocker run -p 8080:80 -v \"$(pwd):/public\" joseluisq/static-web-server:2-debian@sha256:8208279b906b8382d616162a8ba04eda00e4b978fac368c282dfb4cb1bfec136  --compression-static=true\r\n```\r\n2. Access the server with Accept-Encoding set to zstd, and observe that the Content-Encoding is returned as \"zst\".\r\n```\r\n$ curl 127.0.0.1:8080 -v -H \"Accept-Encoding: zstd\" --compressed \r\n*   Trying 127.0.0.1:8080...\r\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: 127.0.0.1:8080\r\n> User-Agent: curl/7.81.0\r\n> Accept: */*\r\n> Accept-Encoding: zstd\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 200 OK\r\n< last-modified: Wed, 29 May 2024 02:46:36 GMT\r\n< content-type: text/html\r\n< accept-ranges: bytes\r\n< content-encoding: zst\r\n< vary: accept-encoding\r\n< cache-control: public, max-age=86400\r\n< transfer-encoding: chunked\r\n< date: Wed, 29 May 2024 02:46:45 GMT\r\n<\r\ncurl: (61) Unrecognized content encoding type. libcurl understands deflate, gzip, br, zstd content encodings.\r\n```\n\n### Expected behavior\n\nThe Content-Encoding header should be \"zstd\".\n\n### Complementary information\n\n_No response_\n\n### Build target\n\nDocker linux/amd64\n\n### Environment and specs\n\n- [x] **static-web-server:** v2.31.1\r\n- [x] **Rust:** v1.78.0\r\n- [x] **OS:** Docker on Ubuntu 22.04\r\n- [x] **Arch:** x86_64 (64-bit)\r\n- [x] **Docker:** 26.1.3-1\r\n- [x] **Client:** curl\r\n\n\n### Additional context\n\nThis behavior is caused by the precompressed_variant function in src/compression_static.rs.\n", "patch": "diff --git a/src/compression_static.rs b/src/compression_static.rs\nindex 88c6799a..6dd47a04 100644\n--- a/src/compression_static.rs\n+++ b/src/compression_static.rs\n@@ -19,13 +19,13 @@ use crate::headers_ext::ContentCoding;\n use crate::Error;\n \n /// It defines the pre-compressed file variant metadata of a particular file path.\n-pub struct CompressedFileVariant<'a> {\n+pub struct CompressedFileVariant {\n     /// Current file path.\n     pub file_path: PathBuf,\n     /// The metadata of the current file.\n     pub metadata: Metadata,\n-    /// The file extension.\n-    pub extension: &'a str,\n+    /// The content encoding based on the file extension.\n+    pub encoding: ContentCoding,\n }\n \n /// Initializes static compression.\n@@ -57,7 +57,7 @@ pub(crate) fn post_process<T>(\n pub async fn precompressed_variant<'a>(\n     file_path: &Path,\n     headers: &'a HeaderMap<HeaderValue>,\n-) -> Option<CompressedFileVariant<'a>> {\n+) -> Option<CompressedFileVariant> {\n     tracing::trace!(\n         \"preparing pre-compressed file variant path of {}\",\n         file_path.display()\n@@ -119,7 +119,7 @@ pub async fn precompressed_variant<'a>(\n         return Some(CompressedFileVariant {\n             file_path,\n             metadata,\n-            extension: if comp_ext == \"gz\" { \"gzip\" } else { comp_ext },\n+            encoding,\n         });\n     }\n \ndiff --git a/src/fs/meta.rs b/src/fs/meta.rs\nindex 44a1eb09..8a1a782c 100644\n--- a/src/fs/meta.rs\n+++ b/src/fs/meta.rs\n@@ -10,6 +10,7 @@ use http::StatusCode;\n use std::fs::Metadata;\n use std::path::{Path, PathBuf};\n \n+use crate::headers_ext::ContentCoding;\n use crate::Result;\n \n /// It defines a composed file metadata structure containing the current file\n@@ -24,7 +25,7 @@ pub(crate) struct FileMetadata<'a> {\n     // If either `file_path` or `precompressed_variant` is a directory.\n     pub is_dir: bool,\n     // The precompressed file variant for the current `file_path`.\n-    pub precompressed_variant: Option<(PathBuf, &'a str)>,\n+    pub precompressed_variant: Option<(PathBuf, ContentCoding)>,\n }\n \n /// Try to find the file system metadata for the given file path or return a `Not Found` error.\ndiff --git a/src/static_files.rs b/src/static_files.rs\nindex f5e32434..6dccc373 100644\n--- a/src/static_files.rs\n+++ b/src/static_files.rs\n@@ -182,13 +182,22 @@ pub async fn handle<'a>(opts: &HandleOpts<'a>) -> Result<StaticFileResponse, Sta\n \n     // Check for a pre-compressed file variant if present under the `opts.compression_static` context\n     if let Some(precompressed_meta) = precompressed_variant {\n-        let (precomp_path, precomp_ext) = precompressed_meta;\n+        let (precomp_path, precomp_encoding) = precompressed_meta;\n         let mut resp = file_reply(headers_opt, file_path, &metadata, Some(precomp_path)).await?;\n \n         // Prepare corresponding headers to let know how to decode the payload\n         resp.headers_mut().remove(CONTENT_LENGTH);\n-        resp.headers_mut()\n-            .insert(CONTENT_ENCODING, precomp_ext.parse().unwrap());\n+        let encoding = match HeaderValue::from_str(precomp_encoding.as_str()) {\n+            Ok(val) => val,\n+            Err(err) => {\n+                tracing::error!(\n+                    \"unable to parse header value from content encoding: {:?}\",\n+                    err\n+                );\n+                return Err(StatusCode::INTERNAL_SERVER_ERROR);\n+            }\n+        };\n+        resp.headers_mut().insert(CONTENT_ENCODING, encoding);\n \n         return Ok(StaticFileResponse {\n             resp,\n@@ -245,7 +254,7 @@ async fn get_composed_file_metadata<'a>(\n                                 file_path,\n                                 metadata: p.metadata,\n                                 is_dir: false,\n-                                precompressed_variant: Some((p.file_path, p.extension)),\n+                                precompressed_variant: Some((p.file_path, p.encoding)),\n                             });\n                         }\n                     }\n@@ -293,7 +302,7 @@ async fn get_composed_file_metadata<'a>(\n                             file_path,\n                             metadata: p.metadata,\n                             is_dir: false,\n-                            precompressed_variant: Some((p.file_path, p.extension)),\n+                            precompressed_variant: Some((p.file_path, p.encoding)),\n                         });\n                     }\n                 }\n@@ -324,7 +333,7 @@ async fn get_composed_file_metadata<'a>(\n                         file_path,\n                         metadata: p.metadata,\n                         is_dir: false,\n-                        precompressed_variant: Some((p.file_path, p.extension)),\n+                        precompressed_variant: Some((p.file_path, p.encoding)),\n                     });\n                 }\n             }\n@@ -362,7 +371,7 @@ async fn get_composed_file_metadata<'a>(\n                                 file_path,\n                                 metadata: p.metadata,\n                                 is_dir: false,\n-                                precompressed_variant: Some((p.file_path, p.extension)),\n+                                precompressed_variant: Some((p.file_path, p.encoding)),\n                             });\n                         }\n                     }\n", "instance_id": "static-web-server__static-web-server-452", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue of incorrect Content-Encoding headers for zstd pre-compressed files, specifying the incorrect value (\"zst\") and the expected value (\"zstd\"). The steps to reproduce the issue are detailed, including sample commands and expected output from a curl request, which helps in understanding the problem context. Additionally, the statement identifies the root cause in the codebase (the `precompressed_variant` function in `src/compression_static.rs`) and provides relevant environment details such as the version of the static-web-server, Rust, OS, and architecture. There are no significant ambiguities, and the inclusion of reproduction steps and expected behavior makes the problem description actionable and clear. The only minor omission is the lack of explicit mention of potential edge cases or broader implications of the fix, but this does not detract from the overall clarity.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue is a straightforward bug fix involving the correction of a header value for zstd encoding. Analyzing the code changes, the scope is limited to a few files (`compression_static.rs`, `fs/meta.rs`, and `static_files.rs`), and the modifications are mostly structural\u2014replacing a string-based extension with a `ContentCoding` enum or type to ensure the correct header value is used. The changes do not impact the broader architecture of the system, nor do they require deep understanding of complex interactions between modules; they are localized to the handling of pre-compressed file variants and header settings. The technical concepts involved are basic: understanding HTTP headers, Rust structs, and simple type refactoring. No advanced algorithms, design patterns, or domain-specific knowledge are required. Edge cases and error handling are minimally impacted, with the code changes introducing a basic error check when parsing the header value, which is not complex. Overall, this task requires understanding some code logic and making simple modifications, suitable for a developer with intermediate Rust experience.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 3, "human_difficulty": 0.25}
{"problem_statement": "[refactor] have a semi-structured tracing output\nExample\n\n```\nsg: summary|file: scannedCount=199\nsg: entity|file|src/cli/scan.rs: skippedCount=47\nsg: entity|rule|my-rule-id: finalSeverity=off\nsg: entity|rule|my-rule-id: skipReason=ruleFilter\nsg: entity|rule|my-rule-id: skipReason=severityOff\nsg: detail|file*rule|src/cli/scan.rs,my-ast-grep-rule: hitResult=hitByInclude\nsg: detail|file*rule|src/cli/scan.rs,my-ast-grep-rule: hitResult=hitByExclude\nsg: detail|file*rule|src/cli/scan.rs,my-ast-grep-rule: hitResult=normalHit\n```\n\n[EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form) notation, \n\n```ebnf\nOUTPUT := \"sg: \", GRANULARITY,  ENTITY_TYPE, ENTITY_IDENTIFIER_LIST , \": \", KEY_VAL_PAIRS;\nGRANULARITY = \"summary\" | \"entity\" | \"detail\";\nENTITY_TYPE = \"file\" | \"rule\" | \"file*rule\";\nENTITY_IDENTIFER_LIST = \"\" | ( \"|\", IDENTIFIER, { \",\", IDENTIFIER } );\nKEY_VAL_PAIRS = KEY_VAL_PAIR, { \",\", KEY_VAL_PAIR };\nKEY_VAL_PAIR = KEY, \"=\", VAL;\n```\n\nInformal Notation\n\n```\nsg: <GRANULARITY>|<ENTITY_TYPE>|<ENTITY_IDENTIFIERS_SEPARATED_BY_COMMA>: KEY=VAL\n```\n\n", "patch": "diff --git a/crates/cli/src/utils/inspect.rs b/crates/cli/src/utils/inspect.rs\nindex a2ba3f9d86..f80a40e7d5 100644\n--- a/crates/cli/src/utils/inspect.rs\n+++ b/crates/cli/src/utils/inspect.rs\n@@ -16,12 +16,13 @@ use ast_grep_config::RuleConfig;\n use anyhow::Result;\n use clap::ValueEnum;\n \n+use std::fmt;\n use std::io::{Stderr, Write};\n use std::path::Path;\n use std::sync::atomic::{AtomicUsize, Ordering};\n use std::sync::Mutex;\n \n-#[derive(Clone, Copy, ValueEnum, Default, PartialEq, Debug)]\n+#[derive(Clone, Copy, ValueEnum, Default, PartialEq, Eq, PartialOrd, Ord)]\n pub enum Granularity {\n   /// Do not show any tracing information\n   #[default]\n@@ -33,6 +34,16 @@ pub enum Granularity {\n   // Detail,\n }\n \n+impl fmt::Debug for Granularity {\n+  fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+    match self {\n+      Granularity::Nothing => write!(f, \"nothing\"),\n+      Granularity::Summary => write!(f, \"summary\"),\n+      Granularity::Entity => write!(f, \"entity\"),\n+    }\n+  }\n+}\n+\n impl Granularity {\n   pub fn run_trace(&self) -> RunTrace {\n     self.run_trace_impl(std::io::stderr())\n@@ -74,19 +85,6 @@ impl FileTrace {\n   pub fn add_skipped(&self) {\n     self.files_skipped.fetch_add(1, Ordering::AcqRel);\n   }\n-  pub fn print<W: Write>(&self, w: &mut W) -> Result<()> {\n-    write!(\n-      w,\n-      \"Files scanned: {}, Files skipped: {}\",\n-      self.files_scanned.load(Ordering::Acquire),\n-      self.files_skipped.load(Ordering::Acquire)\n-    )?;\n-    Ok(())\n-  }\n-  pub fn print_file<W: Write>(&self, w: &mut W, path: &Path, lang: SgLang) -> Result<()> {\n-    write!(w, \"Parse {} with {lang}\", path.display())?;\n-    Ok(())\n-  }\n }\n \n pub struct TraceInfo<T, W: Write> {\n@@ -96,57 +94,93 @@ pub struct TraceInfo<T, W: Write> {\n   output: Mutex<W>,\n }\n \n+impl<T, W: Write + Sync> TraceInfo<T, W> {\n+  #[inline(always)]\n+  fn semi_structured_print<F>(&self, level: Granularity, f: F) -> Result<()>\n+  where\n+    F: FnOnce(&mut W) -> Result<()>,\n+  {\n+    if self.level < level {\n+      return Ok(());\n+    }\n+    let mut w = self.output.lock().expect(\"lock should not be poisoned\");\n+    write!(w, \"sg: {level:?}\")?;\n+    f(&mut *w)?;\n+    writeln!(&mut *w)?;\n+    Ok(())\n+  }\n+\n+  #[inline(always)]\n+  fn print_summary<F>(&self, entity_type: &str, kv_write: F) -> Result<()>\n+  where\n+    F: FnOnce(&mut W) -> Result<()>,\n+  {\n+    self.semi_structured_print(Granularity::Summary, |w| {\n+      write!(w, \"|{entity_type}: \")?;\n+      kv_write(w)\n+    })\n+  }\n+\n+  #[inline(always)]\n+  fn print_entity<F, D>(&self, entity_type: &str, entity_path: D, kv_write: F) -> Result<()>\n+  where\n+    F: FnOnce(&mut W) -> Result<()>,\n+    D: fmt::Display,\n+  {\n+    self.semi_structured_print(Granularity::Entity, |w| {\n+      write!(w, \"|{entity_type}|{entity_path}: \")?;\n+      kv_write(w)\n+    })\n+  }\n+\n+  fn print_files(&self) -> Result<()> {\n+    self.print_summary(\"file\", |w| {\n+      let scanned = self.file_trace.files_scanned.load(Ordering::Acquire);\n+      let skipped = self.file_trace.files_skipped.load(Ordering::Acquire);\n+      write!(w, \"scannedFileCount={scanned},skippedFileCount={skipped}\")?;\n+      Ok(())\n+    })?;\n+    Ok(())\n+  }\n+}\n+\n impl<W: Write + Sync> TraceInfo<(), W> {\n-  // TODO: support more format?\n   pub fn print(&self) -> Result<()> {\n-    match self.level {\n-      Granularity::Nothing => Ok(()),\n-      Granularity::Summary | Granularity::Entity => {\n-        let mut w = self.output.lock().expect(\"lock should not be poisoned\");\n-        self.file_trace.print(&mut *w)?;\n-        writeln!(&mut *w)?;\n-        Ok(())\n-      }\n-    }\n+    self.print_files()\n   }\n \n   pub fn print_file(&self, path: &Path, lang: SgLang) -> Result<()> {\n-    match self.level {\n-      Granularity::Nothing | Granularity::Summary => Ok(()),\n-      Granularity::Entity => {\n-        let mut w = self.output.lock().expect(\"lock should not be poisoned\");\n-        self.file_trace.print_file(&mut *w, path, lang)?;\n-        writeln!(&mut *w)?;\n-        Ok(())\n-      }\n-    }\n+    self.print_entity(\"file\", path.display(), |w| {\n+      write!(w, \"language={lang}\")?;\n+      Ok(())\n+    })\n   }\n }\n \n-impl<W: Write> TraceInfo<RuleTrace, W> {\n+impl<W: Write + Sync> TraceInfo<RuleTrace, W> {\n   // TODO: support more format?\n   pub fn print(&self) -> Result<()> {\n-    match self.level {\n-      Granularity::Nothing => Ok(()),\n-      Granularity::Summary | Granularity::Entity => {\n-        let mut w = self.output.lock().expect(\"lock should not be poisoned\");\n-        self.file_trace.print(&mut *w)?;\n-        writeln!(&mut *w, \"\\n{}\", self.inner.print())?;\n-        Ok(())\n-      }\n-    }\n+    self.print_files()?;\n+    self.print_summary(\"rule\", |w| {\n+      let (effective, skipped) = (\n+        self.inner.effective_rule_count,\n+        self.inner.skipped_rule_count,\n+      );\n+      write!(\n+        w,\n+        \"effectiveRuleCount={effective},skippedRuleCount={skipped}\"\n+      )?;\n+      Ok(())\n+    })?;\n+    Ok(())\n   }\n   pub fn print_file(&self, path: &Path, lang: SgLang, rules: &[&RuleConfig<SgLang>]) -> Result<()> {\n-    let len = rules.len();\n-    match self.level {\n-      Granularity::Nothing | Granularity::Summary => Ok(()),\n-      Granularity::Entity => {\n-        let mut w = self.output.lock().expect(\"lock should not be poisoned\");\n-        self.file_trace.print_file(&mut *w, path, lang)?;\n-        writeln!(&mut *w, \", applied {len} rule(s)\")?;\n-        Ok(())\n-      }\n-    }\n+    self.print_entity(\"file\", path.display(), |w| {\n+      let len = rules.len();\n+      write!(w, \"language={lang},appliedRuleCount={len}\")?;\n+      Ok(())\n+    })?;\n+    Ok(())\n   }\n }\n \n@@ -155,14 +189,6 @@ pub struct RuleTrace {\n   pub effective_rule_count: usize,\n   pub skipped_rule_count: usize,\n }\n-impl RuleTrace {\n-  pub fn print(&self) -> String {\n-    format!(\n-      \"Effective rules: {}, Skipped rules: {}\",\n-      self.effective_rule_count, self.skipped_rule_count\n-    )\n-  }\n-}\n \n pub type RunTrace = TraceInfo<(), Stderr>;\n pub type ScanTrace = TraceInfo<RuleTrace, Stderr>;\n@@ -186,7 +212,10 @@ mod test {\n       0\n     );\n     assert!(run_trace.print().is_ok());\n-    assert_eq!(ret, \"Files scanned: 0, Files skipped: 0\\n\");\n+    assert_eq!(\n+      ret,\n+      \"sg: summary|file: scannedFileCount=0,skippedFileCount=0\\n\"\n+    );\n \n     let mut ret = String::new();\n     let rule_stats = RuleTrace {\n@@ -208,7 +237,9 @@ mod test {\n     assert!(scan_trace.print().is_ok());\n     assert_eq!(\n       ret,\n-      \"Files scanned: 0, Files skipped: 0\\nEffective rules: 10, Skipped rules: 2\\n\"\n+      r\"sg: summary|file: scannedFileCount=0,skippedFileCount=0\n+sg: summary|rule: effectiveRuleCount=10,skippedRuleCount=2\n+\"\n     );\n   }\n \n", "instance_id": "ast-grep__ast-grep-1588", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to refactor the tracing output into a semi-structured format, as demonstrated by the provided example and EBNF notation. The goal of the refactoring is evident, and the desired output format is well-defined with examples and a formal grammar. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly mention how the new format should integrate with the existing codebase or whether there are specific performance or compatibility constraints to consider. Additionally, edge cases (e.g., handling of special characters in identifiers or key-value pairs, or limits on output length) are not addressed. While the provided example helps clarify the intent, the lack of discussion on potential challenges or constraints slightly reduces the clarity.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are confined to a single file (`crates/cli/src/utils/inspect.rs`) and primarily involve refactoring the output formatting logic. The diff shows a moderate amount of code change (around 100 lines modified), but it does not impact the broader system architecture or require modifications across multiple modules. The refactoring focuses on replacing ad-hoc string formatting with a structured output format, which is a localized change.\n\n2. **Clarity and Complexity of Logic**: The logic required to implement the semi-structured output is straightforward. It involves creating helper methods for printing at different granularity levels and formatting output strings according to the specified EBNF notation. While the problem requires understanding the existing tracing system (e.g., `Granularity` enum, `TraceInfo` struct), the modifications are mostly about restructuring how data is presented rather than altering core functionality.\n\n3. **Technical Concepts Involved**: The solution requires basic Rust knowledge, including traits (e.g., implementing `Debug` for `Granularity`), string formatting with `write!` macros, and handling mutexes for thread-safe output. No advanced algorithms, design patterns, or domain-specific knowledge are needed. The use of `Ordering` for atomic operations and `Mutex` for synchronization is already present in the codebase, and the changes do not introduce new complexity in this area.\n\n4. **Edge Cases and Error Handling**: The problem statement does not mention specific edge cases, and the code changes do not introduce new error handling logic beyond what is already present (e.g., `Result` for I/O operations). Potential edge cases, such as handling special characters in output strings or ensuring thread safety under high contention, are not addressed in the diff or problem statement, but they do not appear critical to the core task.\n\nOverall, this task requires understanding some code logic and making simple modifications to the output formatting. It does not involve deep architectural changes or complex technical challenges, justifying a difficulty score of 0.35. It is slightly above the lower end of the \"Easy\" range due to the need to understand the existing tracing system and ensure thread-safe output, but it remains a relatively straightforward refactoring task.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "split: -C with large argument splits at the wrong place\nEnvironment: Ubuntu 20.04, uutils main branch (git commit dddbc17c592554186cce5d029111e19225a8e04f),  gnu coreutils version 9.5.218-7e5b6\n\nSteps to reproduce:\n```\n# One very long line, one very short line, one very long line.\nprintf '%131070s\\n' '' > expaa\nprintf 'x\\n' > expab\nprintf '%131071s\\n' '' > expac\ncat expaa expab expac > bigin\nsplit -C 131072 bigin\n```\n\nWhat happens now: uutils splits at the wrong place:\n```\n$ wc -c xa?\n131071 xaa\n131072 xab\n     2 xac\n262145 total\n```\n\nWhat I expected to happen: GNU split splits as expected (after each newline):\n```\n$ wc -c xa?\n131071 xaa\n     2 xab\n131072 xac\n262145 total\n```\n\nNotes: this is causing a failure in the GNU test file `tests/split/line-bytes.sh`.\n", "patch": "diff --git a/src/uu/split/src/split.rs b/src/uu/split/src/split.rs\nindex 279e91daea1..053d86e8c28 100644\n--- a/src/uu/split/src/split.rs\n+++ b/src/uu/split/src/split.rs\n@@ -919,204 +919,6 @@ impl Write for LineChunkWriter<'_> {\n     }\n }\n \n-/// Write lines to each sequential output files, limited by bytes.\n-///\n-/// This struct maintains an underlying writer representing the\n-/// current chunk of the output. On each call to [`write`], it writes\n-/// as many lines as possible to the current chunk without exceeding\n-/// the specified byte limit. If a single line has more bytes than the\n-/// limit, then fill an entire single chunk with those bytes and\n-/// handle the remainder of the line as if it were its own distinct\n-/// line. As many new underlying writers are created as needed to\n-/// write all the data in the input buffer.\n-struct LineBytesChunkWriter<'a> {\n-    /// Parameters for creating the underlying writer for each new chunk.\n-    settings: &'a Settings,\n-\n-    /// The maximum number of bytes allowed for a single chunk of output.\n-    chunk_size: u64,\n-\n-    /// Running total of number of chunks that have been completed.\n-    num_chunks_written: usize,\n-\n-    /// Remaining capacity in number of bytes in the current chunk.\n-    ///\n-    /// This number starts at `chunk_size` and decreases as lines are\n-    /// written. Once it reaches zero, a writer for a new chunk is\n-    /// initialized and this number gets reset to `chunk_size`.\n-    num_bytes_remaining_in_current_chunk: usize,\n-\n-    /// The underlying writer for the current chunk.\n-    ///\n-    /// Once the number of bytes written to this writer exceeds\n-    /// `chunk_size`, a new writer is initialized and assigned to this\n-    /// field.\n-    inner: BufWriter<Box<dyn Write>>,\n-\n-    /// Iterator that yields filenames for each chunk.\n-    filename_iterator: FilenameIterator<'a>,\n-}\n-\n-impl<'a> LineBytesChunkWriter<'a> {\n-    fn new(chunk_size: u64, settings: &'a Settings) -> UResult<Self> {\n-        let mut filename_iterator = FilenameIterator::new(&settings.prefix, &settings.suffix)?;\n-        let filename = filename_iterator\n-            .next()\n-            .ok_or_else(|| USimpleError::new(1, \"output file suffixes exhausted\"))?;\n-        if settings.verbose {\n-            println!(\"creating file {}\", filename.quote());\n-        }\n-        let inner = settings.instantiate_current_writer(&filename, true)?;\n-        Ok(LineBytesChunkWriter {\n-            settings,\n-            chunk_size,\n-            num_bytes_remaining_in_current_chunk: usize::try_from(chunk_size).unwrap(),\n-            num_chunks_written: 0,\n-            inner,\n-            filename_iterator,\n-        })\n-    }\n-}\n-\n-impl Write for LineBytesChunkWriter<'_> {\n-    /// Write as many lines to a chunk as possible without\n-    /// exceeding the byte limit. If a single line has more bytes\n-    /// than the limit, then fill an entire single chunk with those\n-    /// bytes and handle the remainder of the line as if it were\n-    /// its own distinct line.\n-    ///\n-    /// For example: if the `chunk_size` is 8 and the input is:\n-    ///\n-    /// ```text\n-    /// aaaaaaaaa\\nbbbb\\ncccc\\ndd\\nee\\n\n-    /// ```\n-    ///\n-    /// then the output gets broken into chunks like this:\n-    ///\n-    /// ```text\n-    /// chunk 0    chunk 1    chunk 2    chunk 3\n-    ///\n-    /// 0            1             2\n-    /// 01234567  89 01234   56789 012   345 6\n-    /// |------|  |-------|  |--------|  |---|\n-    /// aaaaaaaa  a\\nbbbb\\n  cccc\\ndd\\n  ee\\n\n-    /// ```\n-    ///\n-    /// Implements `--line-bytes=SIZE`\n-    fn write(&mut self, mut buf: &[u8]) -> std::io::Result<usize> {\n-        // The total number of bytes written during the loop below.\n-        //\n-        // It is necessary to keep this running total because we may\n-        // be making multiple calls to `write()` on multiple different\n-        // underlying writers and we want the final reported number of\n-        // bytes written to reflect the total number of bytes written\n-        // to all of the underlying writers.\n-        let mut total_bytes_written = 0;\n-\n-        // Loop until we have written all bytes in the input buffer\n-        // (or an IO error occurs).\n-        loop {\n-            // If the buffer is empty, then we are done writing.\n-            if buf.is_empty() {\n-                return Ok(total_bytes_written);\n-            }\n-\n-            // If we have filled the current chunk with bytes, then\n-            // start a new chunk and initialize its corresponding\n-            // writer.\n-            if self.num_bytes_remaining_in_current_chunk == 0 {\n-                self.num_chunks_written += 1;\n-                let filename = self.filename_iterator.next().ok_or_else(|| {\n-                    std::io::Error::new(ErrorKind::Other, \"output file suffixes exhausted\")\n-                })?;\n-                if self.settings.verbose {\n-                    println!(\"creating file {}\", filename.quote());\n-                }\n-                self.inner = self.settings.instantiate_current_writer(&filename, true)?;\n-                self.num_bytes_remaining_in_current_chunk = self.chunk_size.try_into().unwrap();\n-            }\n-\n-            // Find the first separator (default - newline character) in the buffer.\n-            let sep = self.settings.separator;\n-            match memchr::memchr(sep, buf) {\n-                // If there is no separator character and the buffer is\n-                // not empty, then write as many bytes as we can and\n-                // then move on to the next chunk if necessary.\n-                None => {\n-                    let end = self.num_bytes_remaining_in_current_chunk;\n-\n-                    // This is ugly but here to match GNU behavior. If the input\n-                    // doesn't end with a separator, pretend that it does for handling\n-                    // the second to last segment chunk. See `line-bytes.sh`.\n-                    if end == buf.len()\n-                        && self.num_bytes_remaining_in_current_chunk\n-                            < self.chunk_size.try_into().unwrap()\n-                        && buf[buf.len() - 1] != sep\n-                    {\n-                        self.num_bytes_remaining_in_current_chunk = 0;\n-                    } else {\n-                        let num_bytes_written = custom_write(\n-                            &buf[..end.min(buf.len())],\n-                            &mut self.inner,\n-                            self.settings,\n-                        )?;\n-                        self.num_bytes_remaining_in_current_chunk -= num_bytes_written;\n-                        total_bytes_written += num_bytes_written;\n-                        buf = &buf[num_bytes_written..];\n-                    }\n-                }\n-\n-                // If there is a separator character and the line\n-                // (including the separator character) will fit in the\n-                // current chunk, then write the entire line and\n-                // continue to the next iteration. (See chunk 1 in the\n-                // example comment above.)\n-                Some(i) if i < self.num_bytes_remaining_in_current_chunk => {\n-                    let num_bytes_written =\n-                        custom_write(&buf[..=i], &mut self.inner, self.settings)?;\n-                    self.num_bytes_remaining_in_current_chunk -= num_bytes_written;\n-                    total_bytes_written += num_bytes_written;\n-                    buf = &buf[num_bytes_written..];\n-                }\n-\n-                // If there is a separator character, the line\n-                // (including the separator character) will not fit in\n-                // the current chunk, *and* no other lines have been\n-                // written to the current chunk, then write as many\n-                // bytes as we can and continue to the next\n-                // iteration. (See chunk 0 in the example comment\n-                // above.)\n-                Some(_)\n-                    if self.num_bytes_remaining_in_current_chunk\n-                        == self.chunk_size.try_into().unwrap() =>\n-                {\n-                    let end = self.num_bytes_remaining_in_current_chunk;\n-                    let num_bytes_written =\n-                        custom_write(&buf[..end], &mut self.inner, self.settings)?;\n-                    self.num_bytes_remaining_in_current_chunk -= num_bytes_written;\n-                    total_bytes_written += num_bytes_written;\n-                    buf = &buf[num_bytes_written..];\n-                }\n-\n-                // If there is a separator character, the line\n-                // (including the separator character) will not fit in\n-                // the current chunk, and at least one other line has\n-                // been written to the current chunk, then signal to\n-                // the next iteration that a new chunk needs to be\n-                // created and continue to the next iteration of the\n-                // loop to try writing the line there.\n-                Some(_) => {\n-                    self.num_bytes_remaining_in_current_chunk = 0;\n-                }\n-            }\n-        }\n-    }\n-\n-    fn flush(&mut self) -> std::io::Result<()> {\n-        self.inner.flush()\n-    }\n-}\n-\n /// Output file parameters\n struct OutFile {\n     filename: String,\n@@ -1629,6 +1431,114 @@ where\n     Ok(())\n }\n \n+/// Like `std::io::Lines`, but includes the line ending character.\n+///\n+/// This struct is generally created by calling `lines_with_sep` on a\n+/// reader.\n+pub struct LinesWithSep<R> {\n+    inner: R,\n+    separator: u8,\n+}\n+\n+impl<R> Iterator for LinesWithSep<R>\n+where\n+    R: BufRead,\n+{\n+    type Item = std::io::Result<Vec<u8>>;\n+\n+    /// Read bytes from a buffer up to the requested number of lines.\n+    fn next(&mut self) -> Option<Self::Item> {\n+        let mut buf = vec![];\n+        match self.inner.read_until(self.separator, &mut buf) {\n+            Ok(0) => None,\n+            Ok(_) => Some(Ok(buf)),\n+            Err(e) => Some(Err(e)),\n+        }\n+    }\n+}\n+\n+/// Like `std::str::lines` but includes the line ending character.\n+///\n+/// The `separator` defines the character to interpret as the line\n+/// ending. For the usual notion of \"line\", set this to `b'\\n'`.\n+pub fn lines_with_sep<R>(reader: R, separator: u8) -> LinesWithSep<R>\n+where\n+    R: BufRead,\n+{\n+    LinesWithSep {\n+        inner: reader,\n+        separator,\n+    }\n+}\n+\n+fn line_bytes<R>(settings: &Settings, reader: &mut R, chunk_size: usize) -> UResult<()>\n+where\n+    R: BufRead,\n+{\n+    let mut filename_iterator = FilenameIterator::new(&settings.prefix, &settings.suffix)?;\n+\n+    // Initialize the writer just to satisfy the compiler. It is going\n+    // to be overwritten for sure at the beginning of the loop below\n+    // because we start with `remaining == 0`, indicating that a new\n+    // chunk should start.\n+    let mut writer: BufWriter<Box<dyn Write>> =\n+        BufWriter::new(Box::new(std::io::Cursor::new(vec![])));\n+\n+    let mut remaining = 0;\n+    for line in lines_with_sep(reader, settings.separator) {\n+        let line = line?;\n+        let mut line = &line[..];\n+        loop {\n+            if remaining == 0 {\n+                let filename = filename_iterator\n+                    .next()\n+                    .ok_or_else(|| USimpleError::new(1, \"output file suffixes exhausted\"))?;\n+                if settings.verbose {\n+                    println!(\"creating file {}\", filename.quote());\n+                }\n+                writer = settings.instantiate_current_writer(&filename, true)?;\n+                remaining = chunk_size;\n+            }\n+\n+            // Special case: if this is the last line and it doesn't end\n+            // with a newline character, then count its length as though\n+            // it did end with a newline. If that puts it over the edge\n+            // of this chunk, continue to the next chunk.\n+            if line.len() == remaining\n+                && remaining < chunk_size\n+                && line[line.len() - 1] != settings.separator\n+            {\n+                remaining = 0;\n+                continue;\n+            }\n+\n+            // If the entire line fits in this chunk, write it and\n+            // continue to the next line.\n+            if line.len() <= remaining {\n+                custom_write_all(line, &mut writer, settings)?;\n+                remaining -= line.len();\n+                break;\n+            }\n+\n+            // If the line is too large to fit in *any* chunk and we are\n+            // at the start of a new chunk, write as much as we can of\n+            // it and pass the remainder along to the next chunk.\n+            if line.len() > chunk_size && remaining == chunk_size {\n+                custom_write_all(&line[..chunk_size], &mut writer, settings)?;\n+                line = &line[chunk_size..];\n+                remaining = 0;\n+                continue;\n+            }\n+\n+            // If the line is too large to fit in *this* chunk, but\n+            // might otherwise fit in the next chunk, then just continue\n+            // to the next chunk and let it be handled there.\n+            remaining = 0;\n+        }\n+    }\n+    Ok(())\n+}\n+\n #[allow(clippy::cognitive_complexity)]\n fn split(settings: &Settings) -> UResult<()> {\n     let r_box = if settings.input == \"-\" {\n@@ -1701,23 +1611,6 @@ fn split(settings: &Settings) -> UResult<()> {\n                 },\n             }\n         }\n-        Strategy::LineBytes(chunk_size) => {\n-            let mut writer = LineBytesChunkWriter::new(chunk_size, settings)?;\n-            match std::io::copy(&mut reader, &mut writer) {\n-                Ok(_) => Ok(()),\n-                Err(e) => match e.kind() {\n-                    // TODO Since the writer object controls the creation of\n-                    // new files, we need to rely on the `std::io::Result`\n-                    // returned by its `write()` method to communicate any\n-                    // errors to this calling scope. If a new file cannot be\n-                    // created because we have exceeded the number of\n-                    // allowable filenames, we use `ErrorKind::Other` to\n-                    // indicate that. A special error message needs to be\n-                    // printed in that case.\n-                    ErrorKind::Other => Err(USimpleError::new(1, format!(\"{e}\"))),\n-                    _ => Err(uio_error!(e, \"input/output error\")),\n-                },\n-            }\n-        }\n+        Strategy::LineBytes(chunk_size) => line_bytes(settings, &mut reader, chunk_size as usize),\n     }\n }\n", "instance_id": "uutils__coreutils-7128", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, providing a specific issue with the `split` utility in the uutils project when using the `-C` option with large arguments. It includes detailed steps to reproduce the issue, expected behavior (based on GNU coreutils), and actual behavior, along with a reference to a failing test case. However, there are minor ambiguities: the problem statement does not explicitly define the expected logic for splitting (e.g., whether to prioritize newline boundaries over byte limits in all cases), and it lacks detailed discussion of edge cases beyond the provided example. While the example is helpful, additional constraints or scenarios (e.g., behavior with different separators or input without newlines) could make the requirements more comprehensive. Thus, it falls into the \"Mostly Clear\" category with minor details missing.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range due to several factors. First, the clarity of the problem requires some inference about the desired behavior (e.g., splitting strictly after newlines within byte limits), which adds to the cognitive load. Second, the scope of code changes is significant: the provided diff shows a complete rewrite of the `LineBytesChunkWriter` logic, replacing it with a new `line_bytes` function and introducing a custom iterator `LinesWithSep`. This involves modifying core logic in a single file but impacts a critical functionality of the `split` utility, requiring a deep understanding of the existing codebase's architecture for handling file chunks. Third, the technical concepts involved include Rust's I/O handling (`BufRead`, `Write`), iterator design, and byte-level manipulation, along with ensuring compatibility with GNU behavior, which adds domain-specific complexity. Finally, edge cases are evident both in the problem statement (e.g., handling lines longer than the chunk size, last line without a newline) and in the code changes, which explicitly address these scenarios with conditional logic. The combination of understanding the existing implementation, designing a new approach, and handling edge cases makes this a challenging task, though not at the extreme end of difficulty since it is confined to a single utility and does not involve system-level or distributed systems concerns. A score of 0.65 reflects this balance of complexity and depth.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.65}
{"problem_statement": "split-pane --move-pane-id can kill panes\n### What Operating System(s) are you seeing this problem on?\n\nmacOS\n\n### Which Wayland compositor or X11 Window manager(s) are you using?\n\nAeroSpace\n\n### WezTerm version\n\n20240203-110809-5046fc22 + main\n\n### Did you try the latest nightly build to see if the issue is better (or worse!) than your current version?\n\nYes, and I updated the version box above to show the version of the nightly that I tried\n\n### Describe the bug\n\nExample:\r\n```\r\n+-----+\r\n|  A  |\r\n+-----+\r\n|  B  |\r\n+-----+\r\n```\r\nCommand: `split-pane --pane-id $B_PANE_ID --move-pane-id $A_PANE_ID`\r\n\r\nIn `Domain::split_pane` (in `mux/src/domain.rs`), with a `SplitSource::MovePane`, the splitting pane (pane B) is first translated into a pane index (1) for its tab. Then, pane A is removed from its tab. However, this changes pane B's index (now 0). Subsequently, when we call `tab.split_and_insert(b_old_index, ...`, the old index 1 is incorrect; in this example it is in fact invalid as there is only a single pane, and pane A never gets reattached to the tab.\n\n### To Reproduce\n\nIn a new tab, split the pane vertically. In the bottom pane, run:\r\n`wezterm cli split-pane --move-pane-id $(wezterm cli get-pane-direction up)`\n\n### Configuration\n\nno config\n\n### Expected Behavior\n\nTop pane should be moved into the right side of a horizontal split of the previously bottom pane.\n\n### Logs\n\n_No response_\n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/mux/src/domain.rs b/mux/src/domain.rs\nindex 3cb3767d5c3..23177b44633 100644\n--- a/mux/src/domain.rs\n+++ b/mux/src/domain.rs\n@@ -127,7 +127,17 @@ pub trait Domain: Downcast + Send + Sync {\n             }\n         };\n \n-        tab.split_and_insert(pane_index, split_request, Arc::clone(&pane))?;\n+        // pane_index may have changed if src_pane was also in the same tab\n+        let final_pane_index = match tab\n+            .iter_panes_ignoring_zoom()\n+            .iter()\n+            .find(|p| p.pane.pane_id() == pane_id)\n+        {\n+            Some(p) => p.index,\n+            None => anyhow::bail!(\"invalid pane id {}\", pane_id),\n+        };\n+\n+        tab.split_and_insert(final_pane_index, split_request, Arc::clone(&pane))?;\n         Ok(pane)\n     }\n \n", "instance_id": "wezterm__wezterm-6029", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the bug encountered when using the `split-pane --move-pane-id` command in WezTerm. It includes a visual example of the pane layout, the specific command causing the issue, and a step-by-step explanation of the bug's root cause in the code (incorrect pane index after removal). Additionally, it offers reproduction steps and specifies the expected behavior. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases beyond the provided example (e.g., multiple panes or different split directions), and it lacks detailed constraints or requirements for the fix. While the issue is well-described for the given scenario, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`mux/src/domain.rs`) and involves a small modification (adding logic to recalculate the pane index after a pane is removed). It does not impact the broader system architecture or require changes across multiple modules. The diff is concise, with only about 10 lines of meaningful code added.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Rust (specifically, working with iterators and references via `Arc`), as well as familiarity with the internal logic of pane management in the WezTerm codebase. The concept of recalculating an index after a structural change in a collection is straightforward and does not involve advanced algorithms, design patterns, or domain-specific knowledge beyond the immediate context of the application.\n\n3. **Edge Cases and Error Handling**: The problem statement and code change address a specific bug related to pane index invalidation. The fix includes a simple error check (`anyhow::bail!`) if the pane ID is not found, which is a minimal addition to error handling. However, the problem does not explicitly mention other potential edge cases (e.g., zoomed panes, multiple splits, or concurrent modifications), and the code change does not account for them either. This keeps the complexity low.\n\n4. **Overall Complexity**: The bug fix requires understanding a specific sequence of operations (pane removal affecting indices) and applying a logical solution (recalculating the index dynamically). This is a relatively simple bug fix that does not demand deep architectural knowledge of the codebase or advanced problem-solving skills.\n\nA score of 0.35 reflects that while the problem is slightly more involved than a trivial fix (e.g., changing a constant), it remains within the realm of easy tasks due to its localized scope, minimal technical depth, and straightforward logic. It requires moderate familiarity with the codebase but does not pose significant challenges in terms of complexity or impact.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "Bookmark endpoint does not handle meta images with relative url\nWhen extracting the content of an og:image meta tag. The enpoint just takes the value of content and sets it on the resource. However there are cases where these urls are relative causing them not to work when displayed in the data-browser.\n", "patch": "diff --git a/lib/src/plugins/bookmark.rs b/lib/src/plugins/bookmark.rs\nindex e804c2c07..ffe6861cf 100644\n--- a/lib/src/plugins/bookmark.rs\n+++ b/lib/src/plugins/bookmark.rs\n@@ -135,39 +135,37 @@ impl Parser {\n \n     pub fn get_meta(&self) -> SiteMeta {\n         let document = parse_html().one(self.internal_html.clone());\n-        let mut title = None;\n-        let mut description = None;\n-        let mut image = None;\n \n-        if let Ok(title_element) = document.select_first(\"title\") {\n-            title = Some(title_element.text_contents());\n-        }\n+        let title = document\n+            .select_first(\"title\")\n+            .ok()\n+            .map(|element| element.text_contents());\n \n-        if let Ok(description_element) =\n-            document.select_first(\"meta[name='description'], meta[property='og:description']\")\n-        {\n-            description = Some(\n-                description_element\n+        let description = document\n+            .select_first(\"meta[name='description'], meta[property='og:description']\")\n+            .ok()\n+            .map(|element| {\n+                element\n                     .attributes\n                     .borrow()\n                     .get(\"content\")\n                     .unwrap_or(\"\")\n-                    .to_string(),\n-            );\n-        }\n-\n-        if let Ok(image_element) =\n-            document.select_first(\"meta[property='og:image'], meta[name='twitter:image']\")\n-        {\n-            image = Some(\n-                image_element\n+                    .to_string()\n+            });\n+\n+        let image = document\n+            .select_first(\"meta[property='og:image'], meta[name='twitter:image']\")\n+            .ok()\n+            .map(|element| {\n+                let image_url = element\n                     .attributes\n                     .borrow()\n                     .get(\"content\")\n                     .unwrap_or(\"\")\n-                    .to_string(),\n-            );\n-        }\n+                    .to_string();\n+\n+                self.relative_to_absolute_url(&image_url)\n+            });\n \n         SiteMeta {\n             title,\n@@ -184,6 +182,34 @@ impl Parser {\n         Ok(self.internal_html.clone())\n     }\n \n+    fn relative_to_absolute_url(&self, url: &str) -> String {\n+        if url.starts_with('/') {\n+            // If it starts with //, it's protocol-relative\n+            if url.starts_with(\"//\") {\n+                return format!(\"https:{}\", url);\n+            }\n+            // Get the base URL (scheme + authority)\n+            let base = format!(\n+                \"{}://{}\",\n+                self.url.scheme(),\n+                self.url.host_str().unwrap_or(\"\")\n+            );\n+\n+            return format!(\"{}{}\", base, url);\n+        }\n+\n+        if !url.contains(\"://\") {\n+            // Handle relative URLs without leading slash\n+            return self\n+                .url\n+                .join(url)\n+                .map(|u| u.to_string())\n+                .unwrap_or(url.to_string());\n+        }\n+\n+        url.to_string()\n+    }\n+\n     fn resolve_url(&self, url: &str) -> String {\n         if Url::parse(url).is_err() {\n             return self.url.join(url).unwrap().as_str().to_string();\n", "instance_id": "atomicdata-dev__atomic-server-1034", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the bookmark endpoint does not handle relative URLs for meta images (specifically og:image), which causes them to fail when displayed in a data browser. The goal is implied\u2014to convert relative URLs to absolute URLs for proper rendering. However, the statement lacks critical details such as specific input/output expectations, examples of relative URLs (e.g., protocol-relative, path-relative), and any constraints or edge cases to consider (e.g., malformed URLs, invalid base URLs). Additionally, there is no mention of how the solution should behave if the base URL cannot be determined or if there are security concerns with URL manipulation. Despite these minor ambiguities, the intent of the problem is understandable, and the code changes provide context for the expected solution, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to several factors. First, the scope of code changes is limited to a single file (bookmark.rs) and primarily involves modifying the logic for handling image URLs within the get_meta method, along with adding a new helper function (relative_to_absolute_url). The amount of code change is moderate, with around 30-40 lines added or modified, and it does not impact the broader system architecture. Second, the technical concepts required are relatively straightforward: understanding URL parsing, string manipulation, and basic web concepts like protocol-relative and path-relative URLs. Familiarity with Rust's ownership and borrowing (e.g., handling &str and String) is necessary but not complex for an experienced developer. Third, the problem involves some edge case handling, such as protocol-relative URLs (starting with \"//\"), path-relative URLs (starting with \"/\"), and fully relative URLs (no leading slash), which the code addresses by constructing absolute URLs using the base URL. However, the problem statement does not explicitly mention additional edge cases like malformed URLs or error conditions, and the code does not introduce extensive error handling beyond fallback behaviors. Overall, solving this requires understanding some code logic and making targeted modifications, but it does not demand deep architectural changes or advanced concepts, justifying a difficulty score of 0.35.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "Set `Accept-Ranges: none` header when compressing responses\n## What is the problem your feature solves, or the need it fulfills?\r\n\r\nWhen adjusting response headers, `ResponseCompressionCtx` will currently remove the `Content-Length` header as the old header does not apply. A potentially present `Accept-Ranges: bytes` header is left untouched however, even though ranged requests no longer make sense.\r\n\r\n## Describe the solution you'd like\r\n\r\nCall `resp.insert_header(&ACCEPT_RANGES, HeaderValue::from_static(\"none\"))` when compressing. This will both overwrite any existing `Accept-Ranges` header and explicitly indicate to the client that ranged requests are unsupported.\r\n\r\n## Describe alternatives you've considered\r\n\r\nN/A\r\n\r\n## Additional context\r\n\r\n[Accept-Ranges header spec](https://www.rfc-editor.org/rfc/rfc9110#field.accept-ranges)\n", "patch": "diff --git a/.bleep b/.bleep\nindex ceecaa73e..220b9e992 100644\n--- a/.bleep\n+++ b/.bleep\n@@ -1,1 +1,1 @@\n-d2eaddcd278a00f25792bf06f046b39aa321abe3\n\\ No newline at end of file\n+b01a9bc71ff892b2fdbb47f6bb3f9eac88907435\n\\ No newline at end of file\ndiff --git a/pingora-core/src/connectors/l4.rs b/pingora-core/src/connectors/l4.rs\nindex d226a8bbe..016203431 100644\n--- a/pingora-core/src/connectors/l4.rs\n+++ b/pingora-core/src/connectors/l4.rs\n@@ -34,7 +34,7 @@ pub trait Connect: std::fmt::Debug {\n }\n \n /// Establish a connection (l4) to the given peer using its settings and an optional bind address.\n-pub async fn connect<P>(peer: &P, bind_to: Option<InetSocketAddr>) -> Result<Stream>\n+pub(crate) async fn connect<P>(peer: &P, bind_to: Option<InetSocketAddr>) -> Result<Stream>\n where\n     P: Peer + Send + Sync,\n {\ndiff --git a/pingora-core/src/connectors/mod.rs b/pingora-core/src/connectors/mod.rs\nindex d13f3a937..09ed01aa2 100644\n--- a/pingora-core/src/connectors/mod.rs\n+++ b/pingora-core/src/connectors/mod.rs\n@@ -15,7 +15,7 @@\n //! Connecting to servers\n \n pub mod http;\n-mod l4;\n+pub mod l4;\n mod offload;\n mod tls;\n \ndiff --git a/pingora-core/src/lib.rs b/pingora-core/src/lib.rs\nindex 4c3e29a38..7a70d5668 100644\n--- a/pingora-core/src/lib.rs\n+++ b/pingora-core/src/lib.rs\n@@ -58,6 +58,9 @@ pub use pingora_boringssl as tls;\n #[cfg(all(not(feature = \"boringssl\"), feature = \"openssl\"))]\n pub use pingora_openssl as tls;\n \n+#[cfg(all(not(feature = \"boringssl\"), not(feature = \"openssl\")))]\n+pub mod tls;\n+\n pub mod prelude {\n     pub use crate::server::configuration::Opt;\n     pub use crate::server::Server;\ndiff --git a/pingora-core/src/protocols/http/compression/mod.rs b/pingora-core/src/protocols/http/compression/mod.rs\nindex 6236a0d82..9bf443b69 100644\n--- a/pingora-core/src/protocols/http/compression/mod.rs\n+++ b/pingora-core/src/protocols/http/compression/mod.rs\n@@ -19,7 +19,6 @@\n use super::HttpTask;\n \n use bytes::Bytes;\n-use http::header::ACCEPT_RANGES;\n use log::warn;\n use pingora_error::{ErrorType, Result};\n use pingora_http::{RequestHeader, ResponseHeader};\n@@ -210,6 +209,17 @@ impl ResponseCompressionCtx {\n                     return;\n                 }\n \n+                if depends_on_accept_encoding(\n+                    resp,\n+                    levels.iter().any(|level| *level != 0),\n+                    decompress_enable,\n+                ) {\n+                    // The response depends on the Accept-Encoding header, make sure to indicate it\n+                    // in the Vary response header.\n+                    // https://www.rfc-editor.org/rfc/rfc9110#name-vary\n+                    add_vary_header(resp, &http::header::ACCEPT_ENCODING);\n+                }\n+\n                 let action = decide_action(resp, accept_encoding);\n                 let encoder = match action {\n                     Action::Noop => None,\n@@ -428,6 +438,47 @@ fn test_accept_encoding_req_header() {\n     assert_eq!(ac_list[1], Algorithm::Gzip);\n }\n \n+// test whether the response depends on Accept-Encoding header\n+fn depends_on_accept_encoding(\n+    resp: &ResponseHeader,\n+    compress_enabled: bool,\n+    decompress_enabled: &[bool],\n+) -> bool {\n+    use http::header::CONTENT_ENCODING;\n+\n+    (decompress_enabled.iter().any(|enabled| *enabled)\n+        && resp.headers.get(CONTENT_ENCODING).is_some())\n+        || (compress_enabled && compressible(resp))\n+}\n+\n+#[test]\n+fn test_decide_on_accept_encoding() {\n+    let mut resp = ResponseHeader::build(200, None).unwrap();\n+    resp.insert_header(\"content-length\", \"50\").unwrap();\n+    resp.insert_header(\"content-type\", \"text/html\").unwrap();\n+    resp.insert_header(\"content-encoding\", \"gzip\").unwrap();\n+\n+    // enabled\n+    assert!(depends_on_accept_encoding(&resp, false, &[true]));\n+\n+    // decompress disabled => disabled\n+    assert!(!depends_on_accept_encoding(&resp, false, &[false]));\n+\n+    // no content-encoding => disabled\n+    resp.remove_header(\"content-encoding\");\n+    assert!(!depends_on_accept_encoding(&resp, false, &[true]));\n+\n+    // compress enabled and compressible response => enabled\n+    assert!(depends_on_accept_encoding(&resp, true, &[false]));\n+\n+    // compress disabled and compressible response => disabled\n+    assert!(!depends_on_accept_encoding(&resp, false, &[false]));\n+\n+    // compress enabled and not compressible response => disabled\n+    resp.insert_header(\"content-type\", \"text/html+zip\").unwrap();\n+    assert!(!depends_on_accept_encoding(&resp, true, &[false]));\n+}\n+\n // filter response header to see if (de)compression is needed\n fn decide_action(resp: &ResponseHeader, accept_encoding: &[Algorithm]) -> Action {\n     use http::header::CONTENT_ENCODING;\n@@ -580,14 +631,104 @@ fn compressible(resp: &ResponseHeader) -> bool {\n     }\n }\n \n+// add Vary header with the specified value or extend an existing Vary header value\n+fn add_vary_header(resp: &mut ResponseHeader, value: &http::header::HeaderName) {\n+    use http::header::{HeaderValue, VARY};\n+\n+    let already_present = resp.headers.get_all(VARY).iter().any(|existing| {\n+        existing\n+            .as_bytes()\n+            .split(|b| *b == b',')\n+            .map(|mut v| {\n+                // This is equivalent to slice.trim_ascii() which is unstable\n+                while let [first, rest @ ..] = v {\n+                    if first.is_ascii_whitespace() {\n+                        v = rest;\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                while let [rest @ .., last] = v {\n+                    if last.is_ascii_whitespace() {\n+                        v = rest;\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                v\n+            })\n+            .any(|v| v == b\"*\" || v.eq_ignore_ascii_case(value.as_ref()))\n+    });\n+\n+    if !already_present {\n+        resp.append_header(&VARY, HeaderValue::from_name(value.clone()))\n+            .unwrap();\n+    }\n+}\n+\n+#[test]\n+fn test_add_vary_header() {\n+    let mut header = ResponseHeader::build(200, None).unwrap();\n+    add_vary_header(&mut header, &http::header::ACCEPT_ENCODING);\n+    assert_eq!(\n+        header\n+            .headers\n+            .get_all(\"Vary\")\n+            .into_iter()\n+            .collect::<Vec<_>>(),\n+        vec![\"accept-encoding\"]\n+    );\n+\n+    let mut header = ResponseHeader::build(200, None).unwrap();\n+    header.insert_header(\"Vary\", \"Accept-Language\").unwrap();\n+    add_vary_header(&mut header, &http::header::ACCEPT_ENCODING);\n+    assert_eq!(\n+        header\n+            .headers\n+            .get_all(\"Vary\")\n+            .into_iter()\n+            .collect::<Vec<_>>(),\n+        vec![\"Accept-Language\", \"accept-encoding\"]\n+    );\n+\n+    let mut header = ResponseHeader::build(200, None).unwrap();\n+    header\n+        .insert_header(\"Vary\", \"Accept-Language, Accept-Encoding\")\n+        .unwrap();\n+    add_vary_header(&mut header, &http::header::ACCEPT_ENCODING);\n+    assert_eq!(\n+        header\n+            .headers\n+            .get_all(\"Vary\")\n+            .into_iter()\n+            .collect::<Vec<_>>(),\n+        vec![\"Accept-Language, Accept-Encoding\"]\n+    );\n+\n+    let mut header = ResponseHeader::build(200, None).unwrap();\n+    header.insert_header(\"Vary\", \"*\").unwrap();\n+    add_vary_header(&mut header, &http::header::ACCEPT_ENCODING);\n+    assert_eq!(\n+        header\n+            .headers\n+            .get_all(\"Vary\")\n+            .into_iter()\n+            .collect::<Vec<_>>(),\n+        vec![\"*\"]\n+    );\n+}\n+\n fn adjust_response_header(resp: &mut ResponseHeader, action: &Action) {\n-    use http::header::{HeaderValue, CONTENT_ENCODING, CONTENT_LENGTH, TRANSFER_ENCODING};\n+    use http::header::{\n+        HeaderValue, ACCEPT_RANGES, CONTENT_ENCODING, CONTENT_LENGTH, TRANSFER_ENCODING,\n+    };\n \n     fn set_stream_headers(resp: &mut ResponseHeader) {\n         // because the transcoding is streamed, content length is not known ahead\n         resp.remove_header(&CONTENT_LENGTH);\n         // remove Accept-Ranges header because range requests will no longer work\n         resp.remove_header(&ACCEPT_RANGES);\n+\n         // we stream body now TODO: chunked is for h1 only\n         resp.insert_header(&TRANSFER_ENCODING, HeaderValue::from_static(\"chunked\"))\n             .unwrap();\n@@ -616,6 +757,7 @@ fn test_adjust_response_header() {\n     let mut header = ResponseHeader::build(200, None).unwrap();\n     header.insert_header(\"content-length\", \"20\").unwrap();\n     header.insert_header(\"content-encoding\", \"gzip\").unwrap();\n+    header.insert_header(\"accept-ranges\", \"bytes\").unwrap();\n     adjust_response_header(&mut header, &Noop);\n     assert_eq!(\n         header.headers.get(\"content-encoding\").unwrap().as_bytes(),\n@@ -631,6 +773,7 @@ fn test_adjust_response_header() {\n     let mut header = ResponseHeader::build(200, None).unwrap();\n     header.insert_header(\"content-length\", \"20\").unwrap();\n     header.insert_header(\"content-encoding\", \"gzip\").unwrap();\n+    header.insert_header(\"accept-ranges\", \"bytes\").unwrap();\n     adjust_response_header(&mut header, &Decompress(Gzip));\n     assert!(header.headers.get(\"content-encoding\").is_none());\n     assert!(header.headers.get(\"content-length\").is_none());\n@@ -638,6 +781,7 @@ fn test_adjust_response_header() {\n         header.headers.get(\"transfer-encoding\").unwrap().as_bytes(),\n         b\"chunked\"\n     );\n+    assert!(header.headers.get(\"accept-ranges\").is_none());\n \n     // compress\n     let mut header = ResponseHeader::build(200, None).unwrap();\ndiff --git a/pingora-core/src/tls/mod.rs b/pingora-core/src/tls/mod.rs\nnew file mode 100644\nindex 000000000..277b5b409\n--- /dev/null\n+++ b/pingora-core/src/tls/mod.rs\n@@ -0,0 +1,806 @@\n+// Copyright 2024 Cloudflare, Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+//! This module contains a dummy TLS implementation for the scenarios where real TLS\n+//! implementations are unavailable.\n+\n+macro_rules! impl_display {\n+    ($ty:ty) => {\n+        impl std::fmt::Display for $ty {\n+            fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> Result<(), std::fmt::Error> {\n+                Ok(())\n+            }\n+        }\n+    };\n+}\n+\n+macro_rules! impl_deref {\n+    ($from:ty => $to:ty) => {\n+        impl std::ops::Deref for $from {\n+            type Target = $to;\n+            fn deref(&self) -> &$to {\n+                panic!(\"Not implemented\");\n+            }\n+        }\n+        impl std::ops::DerefMut for $from {\n+            fn deref_mut(&mut self) -> &mut $to {\n+                panic!(\"Not implemented\");\n+            }\n+        }\n+    };\n+}\n+\n+pub mod ssl {\n+    use super::error::ErrorStack;\n+    use super::x509::verify::X509VerifyParamRef;\n+    use super::x509::{X509VerifyResult, X509};\n+\n+    /// An error returned from an ALPN selection callback.\n+    pub struct AlpnError;\n+    impl AlpnError {\n+        /// Terminate the handshake with a fatal alert.\n+        pub const ALERT_FATAL: AlpnError = Self {};\n+\n+        /// Do not select a protocol, but continue the handshake.\n+        pub const NOACK: AlpnError = Self {};\n+    }\n+\n+    /// A type which allows for configuration of a client-side TLS session before connection.\n+    pub struct ConnectConfiguration;\n+    impl_deref! {ConnectConfiguration => SslRef}\n+    impl ConnectConfiguration {\n+        /// Configures the use of Server Name Indication (SNI) when connecting.\n+        pub fn set_use_server_name_indication(&mut self, _use_sni: bool) {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Configures the use of hostname verification when connecting.\n+        pub fn set_verify_hostname(&mut self, _verify_hostname: bool) {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns an `Ssl` configured to connect to the provided domain.\n+        pub fn into_ssl(self, _domain: &str) -> Result<Ssl, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Like `SslContextBuilder::set_verify`.\n+        pub fn set_verify(&mut self, _mode: SslVerifyMode) {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Like `SslContextBuilder::set_alpn_protos`.\n+        pub fn set_alpn_protos(&mut self, _protocols: &[u8]) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a mutable reference to the X509 verification configuration.\n+        pub fn param_mut(&mut self) -> &mut X509VerifyParamRef {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// An SSL error.\n+    #[derive(Debug)]\n+    pub struct Error;\n+    impl_display!(Error);\n+    impl Error {\n+        pub fn code(&self) -> ErrorCode {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// An error code returned from SSL functions.\n+    #[derive(PartialEq)]\n+    pub struct ErrorCode(i32);\n+    impl ErrorCode {\n+        /// An error occurred in the SSL library.\n+        pub const SSL: ErrorCode = Self(0);\n+    }\n+\n+    /// An identifier of a session name type.\n+    pub struct NameType;\n+    impl NameType {\n+        pub const HOST_NAME: NameType = Self {};\n+    }\n+\n+    /// The state of an SSL/TLS session.\n+    pub struct Ssl;\n+    impl Ssl {\n+        /// Creates a new `Ssl`.\n+        pub fn new(_ctx: &SslContextRef) -> Result<Ssl, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl_deref! {Ssl => SslRef}\n+\n+    /// A type which wraps server-side streams in a TLS session.\n+    pub struct SslAcceptor;\n+    impl SslAcceptor {\n+        /// Creates a new builder configured to connect to non-legacy clients. This should\n+        /// generally be considered a reasonable default choice.\n+        pub fn mozilla_intermediate_v5(\n+            _method: SslMethod,\n+        ) -> Result<SslAcceptorBuilder, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A builder for `SslAcceptor`s.\n+    pub struct SslAcceptorBuilder;\n+    impl SslAcceptorBuilder {\n+        /// Consumes the builder, returning a `SslAcceptor`.\n+        pub fn build(self) -> SslAcceptor {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the callback used by a server to select a protocol for Application Layer Protocol\n+        /// Negotiation (ALPN).\n+        pub fn set_alpn_select_callback<F>(&mut self, _callback: F)\n+        where\n+            F: for<'a> Fn(&mut SslRef, &'a [u8]) -> Result<&'a [u8], AlpnError>\n+                + 'static\n+                + Sync\n+                + Send,\n+        {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Loads a certificate chain from a file.\n+        pub fn set_certificate_chain_file<P: AsRef<std::path::Path>>(\n+            &mut self,\n+            _file: P,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Loads the private key from a file.\n+        pub fn set_private_key_file<P: AsRef<std::path::Path>>(\n+            &mut self,\n+            _file: P,\n+            _file_type: SslFiletype,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the maximum supported protocol version.\n+        pub fn set_max_proto_version(\n+            &mut self,\n+            _version: Option<SslVersion>,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to an [`SslCipher`].\n+    pub struct SslCipherRef;\n+    impl SslCipherRef {\n+        /// Returns the name of the cipher.\n+        pub fn name(&self) -> &'static str {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A type which wraps client-side streams in a TLS session.\n+    pub struct SslConnector;\n+    impl SslConnector {\n+        /// Creates a new builder for TLS connections.\n+        pub fn builder(_method: SslMethod) -> Result<SslConnectorBuilder, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a structure allowing for configuration of a single TLS session before connection.\n+        pub fn configure(&self) -> Result<ConnectConfiguration, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a shared reference to the inner raw `SslContext`.\n+        pub fn context(&self) -> &SslContextRef {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A builder for `SslConnector`s.\n+    pub struct SslConnectorBuilder;\n+    impl SslConnectorBuilder {\n+        /// Consumes the builder, returning an `SslConnector`.\n+        pub fn build(self) -> SslConnector {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the list of supported ciphers for protocols before TLSv1.3.\n+        pub fn set_cipher_list(&mut self, _cipher_list: &str) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the context\u2019s supported signature algorithms.\n+        pub fn set_sigalgs_list(&mut self, _sigalgs: &str) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the minimum supported protocol version.\n+        pub fn set_min_proto_version(\n+            &mut self,\n+            _version: Option<SslVersion>,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the maximum supported protocol version.\n+        pub fn set_max_proto_version(\n+            &mut self,\n+            _version: Option<SslVersion>,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Use the default locations of trusted certificates for verification.\n+        pub fn set_default_verify_paths(&mut self) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Loads trusted root certificates from a file.\n+        pub fn set_ca_file<P: AsRef<std::path::Path>>(\n+            &mut self,\n+            _file: P,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Loads a leaf certificate from a file.\n+        pub fn set_certificate_file<P: AsRef<std::path::Path>>(\n+            &mut self,\n+            _file: P,\n+            _file_type: SslFiletype,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Loads the private key from a file.\n+        pub fn set_private_key_file<P: AsRef<std::path::Path>>(\n+            &mut self,\n+            _file: P,\n+            _file_type: SslFiletype,\n+        ) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the TLS key logging callback.\n+        pub fn set_keylog_callback<F>(&mut self, _callback: F)\n+        where\n+            F: Fn(&SslRef, &str) + 'static + Sync + Send,\n+        {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A context object for TLS streams.\n+    pub struct SslContext;\n+    impl SslContext {\n+        /// Creates a new builder object for an `SslContext`.\n+        pub fn builder(_method: SslMethod) -> Result<SslContextBuilder, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl_deref! {SslContext => SslContextRef}\n+\n+    /// A builder for `SslContext`s.\n+    pub struct SslContextBuilder;\n+    impl SslContextBuilder {\n+        /// Consumes the builder, returning a new `SslContext`.\n+        pub fn build(self) -> SslContext {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to [`SslContext`]\n+    pub struct SslContextRef;\n+\n+    /// An identifier of the format of a certificate or key file.\n+    pub struct SslFiletype;\n+    impl SslFiletype {\n+        /// The PEM format.\n+        pub const PEM: SslFiletype = Self {};\n+    }\n+\n+    /// A type specifying the kind of protocol an `SslContext`` will speak.\n+    pub struct SslMethod;\n+    impl SslMethod {\n+        /// Support all versions of the TLS protocol.\n+        pub fn tls() -> SslMethod {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to an [`Ssl`].\n+    pub struct SslRef;\n+    impl SslRef {\n+        /// Like [`SslContextBuilder::set_verify`].\n+        pub fn set_verify(&mut self, _mode: SslVerifyMode) {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the current cipher if the session is active.\n+        pub fn current_cipher(&self) -> Option<&SslCipherRef> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Sets the host name to be sent to the server for Server Name Indication (SNI).\n+        pub fn set_hostname(&mut self, _hostname: &str) -> Result<(), ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the peer\u2019s certificate, if present.\n+        pub fn peer_certificate(&self) -> Option<X509> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the certificate verification result.\n+        pub fn verify_result(&self) -> X509VerifyResult {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a string describing the protocol version of the session.\n+        pub fn version_str(&self) -> &'static str {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the protocol selected via Application Layer Protocol Negotiation (ALPN).\n+        pub fn selected_alpn_protocol(&self) -> Option<&[u8]> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the servername sent by the client via Server Name Indication (SNI).\n+        pub fn servername(&self, _type_: NameType) -> Option<&str> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Options controlling the behavior of certificate verification.\n+    pub struct SslVerifyMode;\n+    impl SslVerifyMode {\n+        /// Verifies that the peer\u2019s certificate is trusted.\n+        pub const PEER: Self = Self {};\n+\n+        /// Disables verification of the peer\u2019s certificate.\n+        pub const NONE: Self = Self {};\n+    }\n+\n+    /// An SSL/TLS protocol version.\n+    pub struct SslVersion;\n+    impl SslVersion {\n+        /// TLSv1.0\n+        pub const TLS1: SslVersion = Self {};\n+\n+        /// TLSv1.2\n+        pub const TLS1_2: SslVersion = Self {};\n+\n+        /// TLSv1.3\n+        pub const TLS1_3: SslVersion = Self {};\n+    }\n+\n+    /// A standard implementation of protocol selection for Application Layer Protocol Negotiation\n+    /// (ALPN).\n+    pub fn select_next_proto<'a>(_server: &[u8], _client: &'a [u8]) -> Option<&'a [u8]> {\n+        panic!(\"Not implemented\");\n+    }\n+}\n+\n+pub mod ssl_sys {\n+    pub const X509_V_OK: i32 = 0;\n+    pub const X509_V_ERR_INVALID_CALL: i32 = 69;\n+}\n+\n+pub mod error {\n+    use super::ssl::Error;\n+\n+    /// Collection of [`Errors`] from OpenSSL.\n+    #[derive(Debug)]\n+    pub struct ErrorStack;\n+    impl_display!(ErrorStack);\n+    impl std::error::Error for ErrorStack {}\n+    impl ErrorStack {\n+        /// Returns the contents of the OpenSSL error stack.\n+        pub fn get() -> ErrorStack {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the errors in the stack.\n+        pub fn errors(&self) -> &[Error] {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+}\n+\n+pub mod x509 {\n+    use super::asn1::{Asn1IntegerRef, Asn1StringRef, Asn1TimeRef};\n+    use super::error::ErrorStack;\n+    use super::hash::{DigestBytes, MessageDigest};\n+    use super::nid::Nid;\n+\n+    /// An `X509` public key certificate.\n+    #[derive(Debug, Clone)]\n+    pub struct X509;\n+    impl_deref! {X509 => X509Ref}\n+    impl X509 {\n+        /// Deserializes a PEM-encoded X509 structure.\n+        pub fn from_pem(_pem: &[u8]) -> Result<X509, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A type to destructure and examine an `X509Name`.\n+    pub struct X509NameEntries<'a> {\n+        marker: std::marker::PhantomData<&'a ()>,\n+    }\n+    impl<'a> Iterator for X509NameEntries<'a> {\n+        type Item = &'a X509NameEntryRef;\n+        fn next(&mut self) -> Option<&'a X509NameEntryRef> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to `X509NameEntry`.\n+    pub struct X509NameEntryRef;\n+    impl X509NameEntryRef {\n+        pub fn data(&self) -> &Asn1StringRef {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to `X509Name`.\n+    pub struct X509NameRef;\n+    impl X509NameRef {\n+        /// Returns the name entries by the nid.\n+        pub fn entries_by_nid(&self, _nid: Nid) -> X509NameEntries<'_> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to `X509`.\n+    pub struct X509Ref;\n+    impl X509Ref {\n+        /// Returns this certificate\u2019s subject name.\n+        pub fn subject_name(&self) -> &X509NameRef {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a digest of the DER representation of the certificate.\n+        pub fn digest(&self, _hash_type: MessageDigest) -> Result<DigestBytes, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns the certificate\u2019s Not After validity period.\n+        pub fn not_after(&self) -> &Asn1TimeRef {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns this certificate\u2019s serial number.\n+        pub fn serial_number(&self) -> &Asn1IntegerRef {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// The result of peer certificate verification.\n+    pub struct X509VerifyResult;\n+    impl X509VerifyResult {\n+        /// Return the integer representation of an `X509VerifyResult`.\n+        pub fn as_raw(&self) -> i32 {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    pub mod store {\n+        use super::super::error::ErrorStack;\n+        use super::X509;\n+\n+        /// A builder type used to construct an `X509Store`.\n+        pub struct X509StoreBuilder;\n+        impl X509StoreBuilder {\n+            /// Returns a builder for a certificate store..\n+            pub fn new() -> Result<X509StoreBuilder, ErrorStack> {\n+                panic!(\"Not implemented\");\n+            }\n+\n+            /// Constructs the `X509Store`.\n+            pub fn build(self) -> X509Store {\n+                panic!(\"Not implemented\");\n+            }\n+\n+            /// Adds a certificate to the certificate store.\n+            pub fn add_cert(&mut self, _cert: X509) -> Result<(), ErrorStack> {\n+                panic!(\"Not implemented\");\n+            }\n+        }\n+\n+        /// A certificate store to hold trusted X509 certificates.\n+        pub struct X509Store;\n+        impl_deref! {X509Store => X509StoreRef}\n+\n+        /// Reference to an `X509Store`.\n+        pub struct X509StoreRef;\n+    }\n+\n+    pub mod verify {\n+        /// Reference to `X509VerifyParam`.\n+        pub struct X509VerifyParamRef;\n+    }\n+}\n+\n+pub mod nid {\n+    /// A numerical identifier for an OpenSSL object.\n+    pub struct Nid;\n+    impl Nid {\n+        pub const COMMONNAME: Nid = Self {};\n+        pub const ORGANIZATIONNAME: Nid = Self {};\n+        pub const ORGANIZATIONALUNITNAME: Nid = Self {};\n+    }\n+}\n+\n+pub mod pkey {\n+    use super::error::ErrorStack;\n+\n+    /// A public or private key.\n+    #[derive(Clone)]\n+    pub struct PKey<T> {\n+        marker: std::marker::PhantomData<T>,\n+    }\n+    impl<T> std::ops::Deref for PKey<T> {\n+        type Target = PKeyRef<T>;\n+        fn deref(&self) -> &PKeyRef<T> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl<T> std::ops::DerefMut for PKey<T> {\n+        fn deref_mut(&mut self) -> &mut PKeyRef<T> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl PKey<Private> {\n+        pub fn private_key_from_pem(_pem: &[u8]) -> Result<PKey<Private>, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to `PKey`.\n+    pub struct PKeyRef<T> {\n+        marker: std::marker::PhantomData<T>,\n+    }\n+\n+    /// A tag type indicating that a key has private components.\n+    #[derive(Clone)]\n+    pub enum Private {}\n+    unsafe impl HasPrivate for Private {}\n+\n+    /// A trait indicating that a key has private components.\n+    pub unsafe trait HasPrivate {}\n+}\n+\n+pub mod hash {\n+    /// A message digest algorithm.\n+    pub struct MessageDigest;\n+    impl MessageDigest {\n+        pub fn sha256() -> MessageDigest {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// The resulting bytes of a digest.\n+    pub struct DigestBytes;\n+    impl AsRef<[u8]> for DigestBytes {\n+        fn as_ref(&self) -> &[u8] {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+}\n+\n+pub mod asn1 {\n+    use super::bn::BigNum;\n+    use super::error::ErrorStack;\n+\n+    /// A reference to an `Asn1Integer`.\n+    pub struct Asn1IntegerRef;\n+    impl Asn1IntegerRef {\n+        /// Converts the integer to a `BigNum`.\n+        pub fn to_bn(&self) -> Result<BigNum, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// A reference to an `Asn1String`.\n+    pub struct Asn1StringRef;\n+    impl Asn1StringRef {\n+        pub fn as_utf8(&self) -> Result<&str, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+\n+    /// Reference to an `Asn1Time`\n+    pub struct Asn1TimeRef;\n+    impl_display! {Asn1TimeRef}\n+}\n+\n+pub mod bn {\n+    use super::error::ErrorStack;\n+\n+    /// Dynamically sized large number implementation\n+    pub struct BigNum;\n+    impl BigNum {\n+        /// Returns a hexadecimal string representation of `self`.\n+        pub fn to_hex_str(&self) -> Result<&str, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+}\n+\n+pub mod ext {\n+    use super::error::ErrorStack;\n+    use super::pkey::{HasPrivate, PKeyRef};\n+    use super::ssl::{Ssl, SslAcceptor, SslRef};\n+    use super::x509::store::X509StoreRef;\n+    use super::x509::verify::X509VerifyParamRef;\n+    use super::x509::X509Ref;\n+\n+    /// Add name as an additional reference identifier that can match the peer's certificate\n+    pub fn add_host(_verify_param: &mut X509VerifyParamRef, _host: &str) -> Result<(), ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Set the verify cert store of `_ssl`\n+    pub fn ssl_set_verify_cert_store(\n+        _ssl: &mut SslRef,\n+        _cert_store: &X509StoreRef,\n+    ) -> Result<(), ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Load the certificate into `_ssl`\n+    pub fn ssl_use_certificate(_ssl: &mut SslRef, _cert: &X509Ref) -> Result<(), ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Load the private key into `_ssl`\n+    pub fn ssl_use_private_key<T>(_ssl: &mut SslRef, _key: &PKeyRef<T>) -> Result<(), ErrorStack>\n+    where\n+        T: HasPrivate,\n+    {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Clear the error stack\n+    pub fn clear_error_stack() {}\n+\n+    /// Create a new [Ssl] from &[SslAcceptor]\n+    pub fn ssl_from_acceptor(_acceptor: &SslAcceptor) -> Result<Ssl, ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Suspend the TLS handshake when a certificate is needed.\n+    pub fn suspend_when_need_ssl_cert(_ssl: &mut SslRef) {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Unblock a TLS handshake after the certificate is set.\n+    pub fn unblock_ssl_cert(_ssl: &mut SslRef) {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Whether the TLS error is SSL_ERROR_WANT_X509_LOOKUP\n+    pub fn is_suspended_for_cert(_error: &super::ssl::Error) -> bool {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Add the certificate into the cert chain of `_ssl`\n+    pub fn ssl_add_chain_cert(_ssl: &mut SslRef, _cert: &X509Ref) -> Result<(), ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Set renegotiation\n+    pub fn ssl_set_renegotiate_mode_freely(_ssl: &mut SslRef) {}\n+\n+    /// Set the curves/groups of `_ssl`\n+    pub fn ssl_set_groups_list(_ssl: &mut SslRef, _groups: &str) -> Result<(), ErrorStack> {\n+        panic!(\"Not implemented\");\n+    }\n+\n+    /// Sets whether a second keyshare to be sent in client hello when PQ is used.\n+    pub fn ssl_use_second_key_share(_ssl: &mut SslRef, _enabled: bool) {}\n+\n+    /// Get a mutable SslRef ouf of SslRef, which is a missing functionality even when holding &mut SslStream\n+    /// # Safety\n+    pub unsafe fn ssl_mut(_ssl: &SslRef) -> &mut SslRef {\n+        panic!(\"Not implemented\");\n+    }\n+}\n+\n+pub mod tokio_ssl {\n+    use std::pin::Pin;\n+    use std::task::{Context, Poll};\n+    use tokio::io::{AsyncRead, AsyncWrite, ReadBuf};\n+\n+    use super::error::ErrorStack;\n+    use super::ssl::{Error, Ssl, SslRef};\n+\n+    /// A TLS session over a stream.\n+    #[derive(Debug)]\n+    pub struct SslStream<S> {\n+        marker: std::marker::PhantomData<S>,\n+    }\n+    impl<S> SslStream<S> {\n+        /// Creates a new `SslStream`.\n+        pub fn new(_ssl: Ssl, _stream: S) -> Result<Self, ErrorStack> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Initiates a client-side TLS handshake.\n+        pub async fn connect(self: Pin<&mut Self>) -> Result<(), Error> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Initiates a server-side TLS handshake.\n+        pub async fn accept(self: Pin<&mut Self>) -> Result<(), Error> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a shared reference to the `Ssl` object associated with this stream.\n+        pub fn ssl(&self) -> &SslRef {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a shared reference to the underlying stream.\n+        pub fn get_ref(&self) -> &S {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        /// Returns a mutable reference to the underlying stream.\n+        pub fn get_mut(&mut self) -> &mut S {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl<S> AsyncRead for SslStream<S>\n+    where\n+        S: AsyncRead + AsyncWrite,\n+    {\n+        fn poll_read(\n+            self: Pin<&mut Self>,\n+            _ctx: &mut Context<'_>,\n+            _buf: &mut ReadBuf<'_>,\n+        ) -> Poll<std::io::Result<()>> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+    impl<S> AsyncWrite for SslStream<S>\n+    where\n+        S: AsyncRead + AsyncWrite,\n+    {\n+        fn poll_write(\n+            self: Pin<&mut Self>,\n+            _ctx: &mut Context<'_>,\n+            _buf: &[u8],\n+        ) -> Poll<std::io::Result<usize>> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        fn poll_flush(self: Pin<&mut Self>, _ctx: &mut Context<'_>) -> Poll<std::io::Result<()>> {\n+            panic!(\"Not implemented\");\n+        }\n+\n+        fn poll_shutdown(\n+            self: Pin<&mut Self>,\n+            _ctx: &mut Context<'_>,\n+        ) -> Poll<std::io::Result<()>> {\n+            panic!(\"Not implemented\");\n+        }\n+    }\n+}\n", "instance_id": "cloudflare__pingora-361", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue and the desired solution. It identifies a specific problem with the `ResponseCompressionCtx` where the `Accept-Ranges: bytes` header is not adjusted during response compression, which could mislead clients about ranged request support. The solution proposed\u2014setting `Accept-Ranges: none`\u2014is straightforward and directly addresses the issue. Additionally, a reference to the relevant RFC specification is provided, which adds context. However, the statement lacks details on potential edge cases (e.g., specific scenarios where this header adjustment might conflict with other response headers or behaviors) and does not provide examples of input/output or test cases to validate the change. There is also no discussion of potential side effects or interactions with other parts of the system. Hence, while the core problem and solution are clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to several factors. First, the scope of the code changes is relatively limited, primarily affecting the `adjust_response_header` function in the HTTP compression module to add logic for setting the `Accept-Ranges: none` header. The provided diff shows a focused change in a single critical area, though additional unrelated changes (e.g., TLS module additions) appear in the diff, which seem out of scope for this specific problem and are likely part of a larger pull request or unrelated commits. Ignoring those, the core modification is straightforward. Second, the technical concepts involved are basic: understanding HTTP headers, response compression logic, and how to manipulate header values in Rust using the provided library. No advanced algorithms, design patterns, or domain-specific knowledge beyond HTTP protocol basics are required. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest minimal complexity in error handling\u2014primarily ensuring the header is correctly set or removed during compression. However, there is a slight increase in difficulty due to the need to understand the existing compression logic and ensure the change does not introduce unintended side effects (e.g., conflicts with other headers or response types). The impact on the system's architecture is negligible as it is a localized change. Overall, a score of 0.35 reflects an Easy problem that requires understanding some code logic and making a simple, targeted modification.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.35}
{"problem_statement": "Add rename-all=prefix:\nThis adds a new rename-all annotation which allows setting a specific prefix to all fields. For example:\r\n\r\n```rust\r\n/// cbindgen:rename-all=prefix:ERR_\r\n#[repr(i32)]\r\npub enum Error {\r\n   BAD = 0,\r\n   VERY_BAD = 1\r\n}\r\n```\r\n\r\nWhile in principle this can be added to the config file, it's primarily useful for overrides on a case-by-case basis.\n", "patch": "diff --git a/src/bindgen/ir/enumeration.rs b/src/bindgen/ir/enumeration.rs\nindex d6ec313a..8927b8be 100644\n--- a/src/bindgen/ir/enumeration.rs\n+++ b/src/bindgen/ir/enumeration.rs\n@@ -152,9 +152,10 @@ impl EnumVariant {\n             annotations.add_default(\"derive-ostream\", AnnotationValue::Bool(b));\n         }\n \n-        let body_rule = enum_annotations\n-            .parse_atom::<RenameRule>(\"rename-variant-name-fields\")\n-            .unwrap_or(config.enumeration.rename_variant_name_fields);\n+        let body_rule = enum_annotations.parse_atom::<RenameRule>(\"rename-variant-name-fields\");\n+        let body_rule = body_rule\n+            .as_ref()\n+            .unwrap_or(&config.enumeration.rename_variant_name_fields);\n \n         let body = match variant.fields {\n             syn::Fields::Unit => VariantBody::Empty(annotations),\n@@ -555,10 +556,10 @@ impl Item for Enum {\n             }\n         }\n \n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.enumeration.rename_variants);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules\n+            .as_ref()\n+            .unwrap_or(&config.enumeration.rename_variants);\n \n         if let Some(r) = rules.not_none() {\n             self.variants = self\ndiff --git a/src/bindgen/ir/function.rs b/src/bindgen/ir/function.rs\nindex 79adfce9..f25d2f88 100644\n--- a/src/bindgen/ir/function.rs\n+++ b/src/bindgen/ir/function.rs\n@@ -156,10 +156,8 @@ impl Function {\n         self.ret.rename_for_config(config, &generic_params);\n \n         // Apply rename rules to argument names\n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.function.rename_args);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules.as_ref().unwrap_or(&config.function.rename_args);\n \n         if let Some(r) = rules.not_none() {\n             let args = std::mem::take(&mut self.args);\ndiff --git a/src/bindgen/ir/structure.rs b/src/bindgen/ir/structure.rs\nindex 9b33a15c..d805d69c 100644\n--- a/src/bindgen/ir/structure.rs\n+++ b/src/bindgen/ir/structure.rs\n@@ -339,10 +339,10 @@ impl Item for Struct {\n         {\n             let names = self.fields.iter_mut().map(|field| &mut field.name);\n \n-            let field_rules = self\n-                .annotations\n-                .parse_atom::<RenameRule>(\"rename-all\")\n-                .unwrap_or(config.structure.rename_fields);\n+            let field_rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+            let field_rules = field_rules\n+                .as_ref()\n+                .unwrap_or(&config.structure.rename_fields);\n \n             if let Some(o) = self.annotations.list(\"field-names\") {\n                 for (dest, src) in names.zip(o) {\ndiff --git a/src/bindgen/ir/union.rs b/src/bindgen/ir/union.rs\nindex 410e21a2..9c425837 100644\n--- a/src/bindgen/ir/union.rs\n+++ b/src/bindgen/ir/union.rs\n@@ -168,10 +168,8 @@ impl Item for Union {\n             field.ty.rename_for_config(config, &self.generic_params);\n         }\n \n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.structure.rename_fields);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules.as_ref().unwrap_or(&config.structure.rename_fields);\n \n         if let Some(o) = self.annotations.list(\"field-names\") {\n             let mut overriden_fields = Vec::new();\ndiff --git a/src/bindgen/rename.rs b/src/bindgen/rename.rs\nindex f594939e..6f4928ef 100644\n--- a/src/bindgen/rename.rs\n+++ b/src/bindgen/rename.rs\n@@ -28,7 +28,7 @@ impl<'a> IdentifierType<'a> {\n }\n \n /// A rule to apply to an identifier when generating bindings.\n-#[derive(Debug, Clone, Copy, Default)]\n+#[derive(Debug, Clone, Default)]\n pub enum RenameRule {\n     /// Do not apply any renaming. The default.\n     #[default]\n@@ -50,10 +50,12 @@ pub enum RenameRule {\n     /// Converts the identifier to SCREAMING_SNAKE_CASE and prefixes enum variants\n     /// with the enum name.\n     QualifiedScreamingSnakeCase,\n+    /// Adds a given prefix\n+    Prefix(String),\n }\n \n impl RenameRule {\n-    pub(crate) fn not_none(self) -> Option<Self> {\n+    pub(crate) fn not_none(&self) -> Option<&Self> {\n         match self {\n             RenameRule::None => None,\n             other => Some(other),\n@@ -61,7 +63,7 @@ impl RenameRule {\n     }\n \n     /// Applies the rename rule to a string\n-    pub fn apply<'a>(self, text: &'a str, context: IdentifierType) -> Cow<'a, str> {\n+    pub fn apply<'a>(&self, text: &'a str, context: IdentifierType) -> Cow<'a, str> {\n         use heck::*;\n \n         if text.is_empty() {\n@@ -90,6 +92,7 @@ impl RenameRule {\n                 result.push_str(&RenameRule::ScreamingSnakeCase.apply(text, context));\n                 result\n             }\n+            RenameRule::Prefix(prefix) => prefix.to_owned() + text,\n         })\n     }\n }\n@@ -98,6 +101,9 @@ impl FromStr for RenameRule {\n     type Err = String;\n \n     fn from_str(s: &str) -> Result<RenameRule, Self::Err> {\n+        const PREFIX: &str = \"prefix:\";\n+        const PREFIX_LEN: usize = PREFIX.len();\n+\n         match s {\n             \"none\" => Ok(RenameRule::None),\n             \"None\" => Ok(RenameRule::None),\n@@ -132,6 +138,8 @@ impl FromStr for RenameRule {\n             \"QualifiedScreamingSnakeCase\" => Ok(RenameRule::QualifiedScreamingSnakeCase),\n             \"qualified_screaming_snake_case\" => Ok(RenameRule::QualifiedScreamingSnakeCase),\n \n+            s if s.starts_with(PREFIX) => Ok(RenameRule::Prefix(s[PREFIX_LEN..].to_string())),\n+\n             _ => Err(format!(\"Unrecognized RenameRule: '{}'.\", s)),\n         }\n     }\n", "instance_id": "mozilla__cbindgen-1021", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a new \"rename-all=prefix\" annotation to allow setting a specific prefix to all fields in Rust code (e.g., for enum variants). It provides a concrete example of how the feature should work with the `ERR_` prefix for an enum. However, there are minor ambiguities and missing details. For instance, it does not explicitly mention how the prefix should interact with existing rename rules or whether there are constraints on the prefix string (e.g., valid characters, length limits). Additionally, edge cases such as empty prefixes, conflicts with existing names, or behavior with nested structures are not addressed. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (`enumeration.rs`, `function.rs`, `structure.rs`, `union.rs`, and `rename.rs`), indicating a need to understand and modify different parts of the codebase consistently. However, the changes are relatively localized within each file, focusing on updating how rename rules are parsed and applied. Second, the technical concepts involved include familiarity with Rust's syntax and annotations, parsing logic (e.g., `parse_atom`), and string manipulation using libraries like `heck` for case conversion. The addition of a new `Prefix` variant to the `RenameRule` enum and its application logic is straightforward but requires understanding the existing rename system. Third, the problem does not explicitly mention edge cases, but the code changes suggest minimal additional error handling (e.g., no validation for the prefix string), which keeps complexity moderate. Finally, the impact on the system's architecture is low, as this is an additive feature rather than a refactoring of core functionality. Overall, this task requires a moderate understanding of the codebase and concepts, placing it at 0.45 on the difficulty scale.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": 2, "human_difficulty": 0.45}
