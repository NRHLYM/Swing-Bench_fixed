{"problem_statement": "When coercing columns to strings, boolean cells turn into null\nIn our project, we have Excel files with mixed data types. Because of this, we need to coerce all columns to strings, and interpret the data downstream. We're currently blocked from adopting `fastexcel` because it does not coerce booleans as expected.\r\n\r\n## How to reproduce\r\n\r\nSuppose you have an Excel file with the following data:\r\n\r\n| Header |\r\n|--------|\r\n| `=TRUE` |\r\n| `=FALSE` |\r\n| `\"some string\"` | \r\n\r\nNow lets read this Excel file into a Polars dataframe, while coercing the column to strings:\r\n`excel_reader.load_sheet(0, header_row=1, dtypes={\"Header\": \"string\"}).to_polars()`\r\n\r\nThis produces the following data frame:\r\n\r\n| Header |\r\n|--------|\r\n| `null` |\r\n| `null` |\r\n| `\"some string\"` | \r\n\r\n## Expected behavior\r\n\r\nWhat I would have expected fastexcel to coerce the boolean values to `\"0\"` and `\"1\"` instead of losing them. That is:\r\n\r\n| Header |\r\n|--------|\r\n| `\"1\"` |\r\n| `\"0\"` |\r\n| `\"some string\"` | \r\n\r\n## Test case\r\n\r\nI've cloned the fastexcel repo and wrote a new unit test for this, which currently fails.\r\n\r\nExcel sheet: [sheet-bool.xlsx](https://github.com/user-attachments/files/16260997/sheet-bool.xlsx)\r\n\r\n```python\r\ndef test_bool_casting_to_string_for_polars() -> None:\r\n    excel_reader = fastexcel.read_excel(path_for_fixture(\"sheet-bool.xlsx\"))\r\n\r\n    actual_polars_df = excel_reader.load_sheet(\r\n        0, header_row=None, dtypes={0: \"string\"}, column_names=[\"0\"]\r\n    ).to_polars()\r\n    expected_polars_df = pl.DataFrame(\r\n        {\r\n            \"0\": [\"1\", \"0\", \"some string\"],\r\n        }\r\n    )\r\n\r\n    pl_assert_frame_equal(actual_polars_df, expected_polars_df)\r\n```\r\n\r\nUnfortunately, I'm not experienced with Rust, so I did not yet figure out where/how to make the change in the Rust code to make the test pass.\r\n\r\n## Closing words\r\n\r\nFirst of all, big thanks for building `fastexcel`! I'm eager to migrate to fastexcel due to its ability to directly output Polars, which our downstream pipelines are based on, which will give us a massive performance boost.\r\n\r\nIt would be much appreciated if you either gave me some pointers on where/how to make this change in Rust code (so I can open a PR), or made the fix yourself.\n", "patch": "diff --git a/src/types/dtype.rs b/src/types/dtype.rs\nindex e264949..9af844a 100644\n--- a/src/types/dtype.rs\n+++ b/src/types/dtype.rs\n@@ -224,6 +224,7 @@ fn int_types() -> &'static HashSet<DType> {\n fn string_types() -> &'static HashSet<DType> {\n     STRING_TYPES_CELL.get_or_init(|| {\n         HashSet::from([\n+            DType::Bool,\n             DType::Int,\n             DType::Float,\n             DType::String,\ndiff --git a/src/types/python/excelsheet/sheet_data.rs b/src/types/python/excelsheet/sheet_data.rs\nindex 5a4b337..88b68d2 100644\n--- a/src/types/python/excelsheet/sheet_data.rs\n+++ b/src/types/python/excelsheet/sheet_data.rs\n@@ -134,6 +134,8 @@ mod array_impls {\n                         .map(|dt| dt.to_string())\n                 } else if cell.is_datetime_iso() {\n                     cell.get_datetime_iso().map(str::to_string)\n+                } else if cell.is_bool() {\n+                    cell.get_bool().map(|v| v.to_string())\n                 } else {\n                     cell.as_string()\n                 }\n", "instance_id": "ToucanToco__fastexcel-251", "clarity": 3, "difficulty": 0.3, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly defines the issue: when coercing Excel data to strings in the `fastexcel` library, boolean values are incorrectly converted to `null` instead of `\"1\"` and `\"0\"`. The statement includes a detailed reproduction scenario with example input and expected output in tabular form, making the goal unambiguous. Additionally, it provides a failing test case in Python, which further clarifies the expected behavior. Constraints and context (e.g., integration with Polars DataFrame) are also mentioned. There are no significant ambiguities, and the inclusion of a test case and Excel file attachment ensures that the problem is fully reproducible. The only minor omission is the lack of explicit mention of edge cases beyond the provided example, but the provided information is sufficient for a complete understanding of the issue.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to two files (`dtype.rs` and `sheet_data.rs`) and involve minimal modifications. The changes include adding `DType::Bool` to the set of types considered as strings and adding a specific handler for boolean values in the data conversion logic. These modifications are straightforward and do not impact the broader architecture of the system or require understanding complex interactions across multiple modules. The amount of code change is small, with only a few lines added.\n\n2. **Technical Concepts Involved**: Solving this problem requires basic familiarity with Rust syntax, specifically working with `HashSet` and conditional logic for type conversion. It also involves understanding how data types are handled in the `fastexcel` library, but this is not particularly complex as the provided diff already points to the relevant areas of the codebase. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic Excel data handling) are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention additional edge cases beyond the provided example (boolean values in Excel). The code changes do not introduce new error handling logic, as they simply extend the existing type coercion mechanism to handle booleans by converting them to strings. While there might be implicit edge cases (e.g., how booleans are represented in different Excel formats or locales), these are not addressed in the problem statement or code changes, and the fix appears to handle the primary case adequately.\n\n4. **Overall Complexity**: The task is a simple bug fix that involves modifying existing logic to handle an additional data type. It does not require deep knowledge of the codebase beyond the specific functions being modified, nor does it involve performance optimization or significant refactoring. The provided diff already outlines the necessary changes, further reducing the effort required to implement the solution.\n\nGiven these considerations, a difficulty score of 0.30 reflects the straightforward nature of the problem, requiring only basic Rust knowledge and minimal code modifications to achieve the desired behavior.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug(aws): vpc peering route table with unrestricted cidr false-positive \nIt seems that the issue may stem from the query found at this link, which likely scans every AWS route. Consequently, it may flag route tables associated with Internet Gateway (IGW) and NAT Gateway configurations, which are expected to have 0.0.0.0/0 blocks, as instances of \"VPC Peering Route Table with Unrestricted CIDR.\" This leads to false-positive findings across all VPCs by default.\r\nExpected Outcome\r\n\r\n### Expected Behavior\r\n\r\nIdeally, the vulnerability detection should only occur for route tables specifically designated for VPC peering.\r\nCurrent Behavior\r\n\r\n### Actual Behavior\r\n\r\nDespite not utilizing VPC peering, the detection of \"VPC Peering Route Table with Unrestricted CIDR\" persists.\r\nSteps to Recreate the Issue\r\n\r\n### Steps to Reproduce the Problem\r\n\r\n1. Utilize the Terraform public VPC module to establish a VPC devoid of any VPC peering.\r\n2. Verify that KICS identifies \"VPC Peering Route Table with Unrestricted CIDR.\"\r\n\r\n### Specifications\r\n\r\n-   Version: v1.7.13\r\n-   Platform: AWS\r\n\n", "patch": "diff --git a/assets/queries/terraform/aws/vpc_peering_route_table_with_unrestricted_cidr/query.rego b/assets/queries/terraform/aws/vpc_peering_route_table_with_unrestricted_cidr/query.rego\nindex 533bac5acf9..75ec74f1bf1 100644\n--- a/assets/queries/terraform/aws/vpc_peering_route_table_with_unrestricted_cidr/query.rego\n+++ b/assets/queries/terraform/aws/vpc_peering_route_table_with_unrestricted_cidr/query.rego\n@@ -18,6 +18,7 @@ CxPolicy[result] {\n \t\t\"issueType\": \"IncorrectValue\",\n \t\t\"keyExpectedValue\": sprintf(\"aws_route[%s] restricts CIDR\", [name]),\n \t\t\"keyActualValue\": sprintf(\"aws_route[%s] does not restrict CIDR\", [name]),\n+\t\t\"searchLine\": common_lib.build_search_line([\"resource\", \"aws_route\", name], []),\n \t}\n }\n \n@@ -30,10 +31,11 @@ CxPolicy[result] {\n \t\t\"documentId\": input.document[i].id,\n \t\t\"resourceType\": \"aws_route\",\n \t\t\"resourceName\": tf_lib.get_resource_name(resource, name),\n-\t\t\"searchKey\": sprintf(\"aws_route[%s].route\", [name]),\n+\t\t\"searchKey\": sprintf(\"aws_route_table[%s].route\", [name]),\n \t\t\"issueType\": \"IncorrectValue\",\n-\t\t\"keyExpectedValue\": sprintf(\"aws_route[%s].route restricts CIDR\", [name]),\n-\t\t\"keyActualValue\": sprintf(\"aws_route[%s].route does not restrict CIDR\", [name]),\n+\t\t\"keyExpectedValue\": sprintf(\"aws_route_table[%s].route restricts CIDR\", [name]),\n+\t\t\"keyActualValue\": sprintf(\"aws_route_table[%s].route does not restrict CIDR\", [name]),\n+\t\t\"searchLine\": common_lib.build_search_line([\"resource\", \"aws_route_table\", name, \"route\"], []),\n \t}\n }\n \n@@ -59,7 +61,7 @@ unrestricted(route) {\n route_table_open_cidr(route) {\n \tis_array(route)\n \tcommon_lib.valid_key(route[r], \"vpc_peering_connection_id\")\n-\tunrestricted(route)\n+\tunrestricted(route[r])\n } else {\n \tis_object(route)\n \tcommon_lib.valid_key(route, \"vpc_peering_connection_id\")\n", "instance_id": "Checkmarx__kics-7067", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue of false-positive detections for \"VPC Peering Route Table with Unrestricted CIDR\" in AWS configurations. It specifies the expected behavior (detection should only apply to VPC peering route tables) and the current behavior (detection occurs even without VPC peering). Steps to reproduce the issue are provided, along with the platform (AWS) and version details. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes a \"restricted CIDR\" or provide examples of acceptable versus unacceptable configurations. Additionally, edge cases or specific constraints for the fix (e.g., performance considerations or compatibility with other rules) are not mentioned. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively limited, primarily affecting a single Rego policy file used for vulnerability scanning in Terraform configurations. The changes involve modifying logic to correctly identify and filter routes related to VPC peering, as seen in the diff (e.g., updating conditions in `unrestricted(route[r])` and adjusting search keys). However, it requires understanding specific technical concepts, such as Rego (the policy language used by Open Policy Agent), Terraform resource definitions (`aws_route` and `aws_route_table`), and AWS networking concepts like VPC peering and CIDR blocks. The problem also demands careful handling of logic to avoid false positives, which introduces moderate complexity in ensuring the policy correctly distinguishes between peering routes and other configurations (e.g., Internet Gateway or NAT Gateway routes). While the changes do not impact the broader system architecture and are confined to a single file, the need to understand domain-specific knowledge (AWS networking) and policy logic pushes this beyond a simple bug fix. Edge cases, such as mixed configurations or invalid route definitions, are not explicitly mentioned but may need consideration during implementation. Overall, this problem requires a moderate level of expertise and effort, fitting within the 0.4-0.6 range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "pytest-embedded-wokwi doesn't work in Windows (RDT-859)\nFollowing 1383c2e7b8417839a21bc55d171ec807b090f3d6, the plugin no longer works on windows, since the code uses `pexpect.spawn`, which is not available on Windows.\r\n\r\nWhen using pytest-embedded-wokwi on windows, you get the following error:\r\n\r\n```\r\nC:\\dev\\esp\\tools\\python_env\\idf5.4_py3.12_env\\Lib\\site-packages\\pytest_embedded\\plugin.py:490: in wrapper\r\n    res = func(*args, **kwargs)\r\nC:\\dev\\esp\\tools\\python_env\\idf5.4_py3.12_env\\Lib\\site-packages\\pytest_embedded\\plugin.py:1139: in wokwi\r\n    return wokwi_gn(**locals())\r\nC:\\dev\\esp\\tools\\python_env\\idf5.4_py3.12_env\\Lib\\site-packages\\pytest_embedded\\dut_factory.py:475: in wokwi_gn   \r\n    return cls(**_drop_none_kwargs(kwargs))\r\nC:\\dev\\esp\\tools\\python_env\\idf5.4_py3.12_env\\Lib\\site-packages\\pytest_embedded_wokwi\\wokwi_cli.py:62: in __init__\r\n    child = pexpect.spawn('wokwi-cli --help')\r\nE   AttributeError: module 'pexpect' has no attribute 'spawn'\r\n```\r\n\r\n\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex fe1cae8e..ef7454b6 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -15,10 +15,10 @@ repos:\n       - id: mixed-line-ending\n         args: [\"-f=lf\"]\n   - repo: https://github.com/charliermarsh/ruff-pre-commit\n-    rev: \"v0.4.3\"\n+    rev: \"v0.6.1\"\n     hooks:\n       - id: ruff\n-        args: [\"--fix\", \"--preview\"]\n+        args: [\"--fix\"]\n       - id: ruff-format\n         args: [\"--preview\"]\n \n@@ -28,8 +28,8 @@ repos:\n     hooks:\n       - id: sphinx-lint\n         args: [--enable=default-role]\n-  - repo: https://github.com/dzhu/rstfmt\n-    rev: v0.0.14\n+  - repo: https://github.com/hfudev/rstfmt\n+    rev: v0.1.4\n     hooks:\n       - id: rstfmt\n         args: [\"-w\", \"-1\"]\n@@ -37,7 +37,7 @@ repos:\n \n   # git commit\n   - repo: https://github.com/espressif/conventional-precommit-linter\n-    rev: v1.8.0\n+    rev: v1.10.0\n     hooks:\n       - id: conventional-precommit-linter\n         stages: [commit-msg]\n", "instance_id": "espressif__pytest-embedded-304", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `pytest-embedded-wokwi` plugin fails on Windows due to the use of `pexpect.spawn`, which is not supported on this platform. The error message and stack trace provided are helpful in pinpointing the root cause. However, the statement lacks critical details about the expected solution or workaround (e.g., whether to replace `pexpect.spawn` with a Windows-compatible alternative or to conditionally skip certain functionality on Windows). Additionally, there are no mentions of edge cases, constraints, or specific requirements for the fix. While the goal is implied (make the plugin work on Windows), the lack of explicit guidance on the approach or desired outcome introduces minor ambiguity.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) for several reasons. First, the scope of the issue is relatively narrow, likely confined to a specific part of the codebase (e.g., replacing or conditionally handling the `pexpect.spawn` call in `wokwi_cli.py`). However, the provided code changes in the diff are unrelated to the problem statement, as they only update pre-commit hooks and tool versions, which do not address the Windows compatibility issue. Assuming the actual fix would involve modifying the plugin code, the required changes would likely be localized to a single file or module.\n\nThe technical concepts involved include understanding Python's `pexpect` library and its limitations on Windows, as well as identifying a suitable alternative (e.g., `subprocess` or `ptyprocess` for Windows compatibility). This requires moderate familiarity with cross-platform development in Python but does not involve advanced algorithms or design patterns. Error handling and edge cases are not explicitly mentioned in the problem statement, but a competent solution would need to consider scenarios like command execution failures or differences in behavior between operating systems, which adds a small layer of complexity.\n\nOverall, solving this issue requires understanding some code logic and making simple to moderate modifications, likely in a single file or function. It does not appear to impact the broader system architecture or require deep knowledge of the codebase. The score of 0.35 reflects a problem that is slightly more involved than a trivial fix due to the need for cross-platform considerations but remains relatively straightforward for an experienced developer.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FR]: API server - provide endpoint \"config download\" to download original configuration\nThe current API call config.list will output the configuration as JSON by design. Provide endpoint that will output original Turbinia config file so it can be edited/re-used with Turbinia.\n", "patch": "diff --git a/turbinia/api/openapi.yaml b/turbinia/api/openapi.yaml\nindex b8c1282d7..a336b1d05 100644\n--- a/turbinia/api/openapi.yaml\n+++ b/turbinia/api/openapi.yaml\n@@ -203,6 +203,20 @@ paths:\n         summary: Get Version\n         tags:\n         - Turbinia Configuration\n+  /api/config/download:\n+      get:\n+        description: Downloads the Turbinia configuration.\n+        operationId: download_config\n+        responses:\n+          '200':\n+            content:\n+              application/text:\n+                schema:\n+                  type: string\n+            description: Successful Response\n+        summary: Download Configuration\n+        tags:\n+        - Turbinia Configuration\n   /api/evidence/query:\n     get:\n       description: \"Queries evidence in Redis that have the specified attribute value.\\n\\\ndiff --git a/turbinia/api/routes/config.py b/turbinia/api/routes/config.py\nindex e0579525f..02671903a 100644\n--- a/turbinia/api/routes/config.py\n+++ b/turbinia/api/routes/config.py\n@@ -18,7 +18,7 @@\n import logging\n \n from fastapi import HTTPException, APIRouter\n-from fastapi.responses import JSONResponse\n+from fastapi.responses import JSONResponse, FileResponse\n from fastapi.requests import Request\n \n from turbinia import __version__\n@@ -58,3 +58,16 @@ async def get_version(request: Request):\n   \"\"\"Returns the Turbinia version.\"\"\"\n   response = {'version': __version__}\n   return JSONResponse(content=response, status_code=200)\n+\n+\n+@router.get('/download')\n+async def download_config(request: Request):\n+  \"\"\"Downloads Turbinia configuration.\"\"\"\n+  try:\n+    path = turbinia_config.CONFIG.__file__\n+    return FileResponse(path)\n+  except Exception as exception:\n+    log.error(f'Error reading configuration file: {exception!s}')\n+    raise HTTPException(\n+        status_code=500,\n+        detail='Error reading configuration file') from exception\n", "instance_id": "google__turbinia-1515", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes the need for a new API endpoint to download the original Turbinia configuration file as opposed to the JSON output provided by the existing `config.list` endpoint. The goal (providing a downloadable config file for editing/reuse) is evident, and the context of the existing functionality is briefly mentioned. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the expected format of the output file (though the code changes imply it\u2019s a direct file download), nor does it mention any specific constraints or requirements for handling the file (e.g., size limits, permissions, or error conditions). Additionally, edge cases such as what happens if the configuration file is missing or inaccessible are not addressed in the problem description. While the intent is understandable, these missing details prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the straightforward nature of the required changes and the limited scope of technical complexity. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes**: The modifications are confined to two files: `openapi.yaml` for updating the API specification and `config.py` for implementing the new endpoint. The changes are minimal, involving the addition of a new route in the API spec and a simple function to serve the configuration file using `FileResponse`. There is no impact on the broader system architecture or interactions with other modules, and the amount of code change is small (a few lines per file).\n\n2. **Number of Technical Concepts**: The solution requires basic familiarity with FastAPI (specifically `FileResponse` and routing), Python file handling (accessing a file path), and API specification updates using OpenAPI YAML. These are relatively simple concepts for anyone with moderate experience in web development or Python. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Edge Cases and Error Handling**: The code changes include basic error handling for file access issues (raising an HTTP 500 error with a log message if an exception occurs). However, the problem statement does not explicitly mention edge cases, and the implementation does not address more nuanced scenarios like file permissions, large file handling, or content-type specifics. The error handling logic added is minimal and straightforward.\n\n4. **Overall Complexity**: The task involves adding a new endpoint to serve a static file, which is a common and simple operation in web frameworks like FastAPI. There are no performance considerations, complex logic, or deep codebase understanding required beyond knowing where the configuration file is stored and how to serve it.\n\nGiven these points, a difficulty score of 0.3 reflects an \"Easy\" problem that requires understanding some code logic (FastAPI routing and file serving) and making simple modifications. It does not rise to the level of \"Medium\" difficulty as it lacks complexity in scope, concepts, or edge case handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "SPEC file in git produces uninstallable RPM on RHEL/Rocky 9\n### Describe the bug\r\n\r\nThe SPEC file found in the repository produces a RPM file which cannot be used on RHEL 9 or Rocky Linux 9. There is no longer any package which provides fence-agents, either as a real package or the metapackage it was. The result is as follows:\r\n\r\n````\r\n[root@host rpm-build]# dnf install cobbler-3.3.4-1.el9.noarch.rpm \r\nLast metadata expiration check: 1:59:17 ago on Sat 09 Mar 2024 01:32:17 PM EST.\r\nError: \r\n Problem: conflicting requests\r\n  - nothing provides fence-agents needed by cobbler-3.3.4-1.el9.noarch from @commandline\r\n(try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages)\r\n````\r\n\r\nInspecting the differences between the SPEC file used for the RPM in the EPEL 9 repository  and what is in git reveals that what is in git has no resemblance to what is in git. This should be resolved so that the same file is found in both places.\r\n<!--- A clear and concise description of what the bug is. -->\r\n\r\n<!--- HINT: You can paste gist.github.com links for long logs or larger files -->\r\n\r\n### Steps to reproduce\r\n\r\n1. Build the RPMs from git by doing `make rpms`\r\n2. Attempt to install the rpm using dnf using the command `dnf install  cobbler-3.3.4-1.el9.noarch.rpm`\r\n\r\n### Expected behavior\r\n\r\nThe RPM should install, along with its missing dependencies.\r\n<!--- A clear and concise description of what you expected to happen. -->\r\n\r\n### Cobbler version\r\n\r\n<!--- Paste output from `cobbler version` -->\r\n````\r\n[root@pell rpm-build]# cobbler version\r\nCobbler 3.3.4\r\n  source: ?, ?\r\n  build time: Mon Feb 26 00:00:00 2024\r\n````\r\n(from the EPEL RPM)\r\n\r\n### Operating system\r\n\r\n<!--- On which operating system do you use Cobbler? -->\r\n\r\nVerified on the following:\r\nRed Hat Enterprise Linux release 9.2 (Plow)\r\nRed Hat Enterprise Linux release 9.3 (Plow)\r\nRocky Linux release 9.3 (Blue Onyx)\r\n\r\n### Cobbler log\r\n\r\n<!--- Paste (partial) output from `/var/log/cobbler/cobbler.log` -->\r\n````paste below\r\n````\r\n\r\n### Screenshots\r\n\r\n<!--- If applicable, add screenshots to help explain your problem. -->\r\n\r\n### Additional information\r\n\r\n<!--- Add any other context about the problem here. -->\r\n\n", "patch": "diff --git a/.github/workflows/packaging.yml b/.github/workflows/packaging.yml\nindex 4352b832e9..a98695d913 100644\n--- a/.github/workflows/packaging.yml\n+++ b/.github/workflows/packaging.yml\n@@ -23,6 +23,20 @@ jobs:\n           name: rpms-rockylinux-8\n           path: |\n             rpm-build/*.rpm\n+  build-rockylinux9-rpms:\n+    runs-on: ubuntu-20.04\n+    steps:\n+      - uses: actions/checkout@v4\n+      - name: Build a Rocky Linux 9 Package\n+        shell: 'script -q -e -c \"bash {0}\"'\n+        run: |\n+          ./docker/rpms/build-and-install-rpms.sh rl9 docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile\n+      - name: Archive RPMs\n+        uses: actions/upload-artifact@v3\n+        with:\n+          name: rpms-rockylinux-8\n+          path: |\n+            rpm-build/*.rpm\n   build-fedora37-rpms:\n     runs-on: ubuntu-20.04\n     steps:\ndiff --git a/Makefile b/Makefile\nindex 647caaf3c6..2b42aa5c31 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -87,8 +87,11 @@ release: clean qa authors sdist ## Creates the full release.\n \t@cp distro_build_configs.sh release/\n \t@cp cobbler.spec release/\n \n-test-centos8: ## Executes the testscript for testing cobbler in a docker container on CentOS8.\n-\t./docker/rpms/build-and-install-rpms.sh el8 docker/rpms/CentOS_8/CentOS8.dockerfile\n+test-rocky8: ## Executes the testscript for testing cobbler in a docker container on Rocky Linux 8.\n+\t./docker/rpms/build-and-install-rpms.sh rl8 docker/rpms/Rocky_Linux_8/Rocky_Linux_8.dockerfile\n+\n+test-rocky9: ## Executes the testscript for testing cobbler in a docker container on Rocky Linux 9.\n+\t./docker/rpms/build-and-install-rpms.sh rl8 docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile\n \n test-fedora37: ## Executes the testscript for testing cobbler in a docker container on Fedora 37.\n \t./docker/rpms/build-and-install-rpms.sh fc37 docker/rpms/Fedora_37/Fedora37.dockerfile\ndiff --git a/changelog.d/3655.fixed b/changelog.d/3655.fixed\nnew file mode 100644\nindex 0000000000..f38f84db9d\n--- /dev/null\n+++ b/changelog.d/3655.fixed\n@@ -0,0 +1,1 @@\n+Fix build on RHEL9 based distributions (fence-agents-all split)\n\\ No newline at end of file\ndiff --git a/cobbler.spec b/cobbler.spec\nindex 4c40b71bed..f431bfebcd 100644\n--- a/cobbler.spec\n+++ b/cobbler.spec\n@@ -172,7 +172,11 @@ Requires(postun): systemd\n Requires:       %{apache_pkg}\n Requires:       %{tftpsrv_pkg}\n Requires:       %{createrepo_pkg}\n+%if 0%{?rhel} && 0%{?rhel} >= 9\n+Requires:       fence-agents-all\n+%else\n Requires:       fence-agents\n+%endif\n Requires:       rsync\n Requires:       xorriso\n Requires:       dosfstools\ndiff --git a/docker-compose.yml b/docker-compose.yml\nindex 717799b1cf..f88b5a0f94 100644\n--- a/docker-compose.yml\n+++ b/docker-compose.yml\n@@ -35,6 +35,15 @@ services:\n     volumes:\n       - ./rpm-build/rl8:/usr/src/cobbler/rpm-build\n \n+  rockylinux9-build:\n+    image: local/cobbler-rockylinux9\n+    container_name: cobbler-rockylinux9\n+    build:\n+      context: .\n+      dockerfile: docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile\n+    volumes:\n+      - ./rpm-build/rl9:/usr/src/cobbler/rpm-build\n+\n   fedora37-build:\n     image: local/cobbler-fedora37\n     container_name: cobbler-fedora37\ndiff --git a/docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile b/docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile\nnew file mode 100644\nindex 0000000000..9ba44d4eac\n--- /dev/null\n+++ b/docker/rpms/Rocky_Linux_9/Rocky_Linux_9.dockerfile\n@@ -0,0 +1,85 @@\n+# vim: ft=dockerfile\n+\n+FROM rockylinux/rockylinux:9\n+\n+RUN dnf makecache && \\\n+    dnf install -y epel-release dnf-utils && \\\n+    dnf config-manager --set-enabled crb && \\\n+    dnf config-manager --set-enabled highavailability && \\\n+    dnf makecache\n+\n+# overlay2 bug with yum/dnf\n+#\n+# OverlayFS only implements a subset of POSIX standards. This can cause RPM db corruption.\n+# See bottom of https://docs.docker.com/storage/storagedriver/overlayfs-driver/\n+# Since there is no dnf-plugin-ovl for CentOS 8 yet, we need to touch /var/lib/rpm/* before\n+# 'dnf install' to avoid the issue.\n+\n+# Dev dependencies\n+RUN touch /var/lib/rpm/* &&   \\\n+    dnf install -y            \\\n+    iproute                   \\\n+    git                       \\\n+    rsync                     \\\n+    make                      \\\n+    openssl                   \\\n+    mod_ssl                   \\\n+    initscripts               \\\n+    python3-sphinx            \\\n+    python3-devel             \\\n+    python3-wheel             \\\n+    python3-distro            \\\n+    python3-pyflakes          \\\n+    python3-pycodestyle       \\\n+    python3-setuptools        \\\n+    python3-sphinx            \\\n+    python3-schema            \\\n+    epel-rpm-macros           \\\n+    rpm-build                 \\\n+    which\n+\n+# Runtime dependencies\n+RUN touch /var/lib/rpm/* &&   \\\n+    dnf install -y            \\\n+    httpd                     \\\n+    python3-gunicorn          \\\n+    python3-mod_wsgi          \\\n+    python3-pyyaml            \\\n+    python3-netaddr           \\\n+    python3-cheetah           \\\n+    python3-magic             \\\n+    python3-dns               \\\n+    python3-ldap              \\\n+    python3-librepo           \\\n+    python3-pymongo           \\\n+    python3-coverage          \\\n+    createrepo_c              \\\n+    dnf-plugins-core          \\\n+    xorriso                   \\\n+    grub2-efi-x64-modules     \\\n+    logrotate                 \\\n+    syslinux                  \\\n+    tftp-server               \\\n+    supervisor                \\\n+    dosfstools\n+\n+# Dependencies for system tests\n+RUN touch /var/lib/rpm/* &&   \\\n+    dnf install -y            \\\n+    shim                      \\\n+    ipxe-bootimgs             \\\n+    dhcp-server               \\\n+    qemu-kvm                  \\\n+    time\n+RUN dnf --enablerepo=plus -y install openldap-servers\n+RUN dnf --enablerepo=highavailability -y install fence-agents-all\n+\n+COPY ./docker/rpms/Fedora_37/supervisord/supervisord.conf /etc/supervisord.conf\n+COPY ./docker/rpms/Fedora_37/supervisord/conf.d /etc/supervisord/conf.d\n+\n+COPY . /usr/src/cobbler\n+WORKDIR /usr/src/cobbler\n+\n+VOLUME /usr/src/cobbler/rpm-build\n+\n+CMD [\"/bin/bash\", \"-c\", \"make rpms\"]\n", "instance_id": "cobbler__cobbler-3837", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the SPEC file in the repository produces an RPM that cannot be installed on RHEL 9 or Rocky Linux 9 due to a missing dependency (fence-agents). It provides steps to reproduce the issue, expected behavior, and relevant error messages, which are helpful for understanding the problem. However, there are minor ambiguities and missing details. For instance, it does not explicitly state what the correct dependency should be (though it can be inferred from the code changes as fence-agents-all for RHEL 9). Additionally, there is no mention of potential edge cases or specific constraints related to different versions of the operating systems beyond RHEL 9 and Rocky Linux 9. While the goal is clear (fix the SPEC file to make the RPM installable), these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes primarily involve updating the SPEC file to conditionally require `fence-agents-all` instead of `fence-agents` for RHEL 9 and above, which is a relatively small and localized change. However, additional changes are made to support building and testing on Rocky Linux 9, including updates to the Makefile, GitHub workflows, Docker configurations, and adding a new Dockerfile. These changes span multiple files but are mostly straightforward additions or modifications to existing patterns. The changes do not significantly impact the system's architecture or require deep refactoring.\n\n2. **Number of Technical Concepts**: Solving this problem requires basic knowledge of RPM packaging and SPEC files, particularly understanding dependency declarations and conditional macros (`%if` for RHEL version checks). Additionally, familiarity with Docker, GitHub Actions, and build scripts is needed to implement the build and test environment for Rocky Linux 9. These concepts are not overly complex for a developer with moderate experience in Linux packaging and CI/CD workflows.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes imply consideration of version-specific dependencies (RHEL 9 vs. older versions). There is no significant error handling logic introduced in the changes, as the focus is on dependency resolution rather than runtime behavior. The complexity of edge cases appears minimal, limited to ensuring compatibility across supported OS versions.\n\n4. **Overall Complexity**: While the changes are spread across multiple files, they follow established patterns (e.g., adding a new build target for Rocky Linux 9 mirrors existing targets for other distributions). The core fix in the SPEC file is simple, and the additional build infrastructure changes are repetitive rather than conceptually challenging. This keeps the difficulty on the lower end of the spectrum, though slightly above the \"Very Easy\" range due to the need to understand packaging and build systems.\n\nA score of 0.35 reflects the need for some understanding of code logic and build processes, along with minor cross-file modifications, but the problem does not demand deep architectural changes or advanced technical expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Phantom Tic-Tac-Toe information state bug\nHey there, I was looking at the phantom tic-tac-toe implementation, specifically the information state tensor:\r\n\r\n```cpp\r\nvoid PhantomTTTState::InformationStateTensor(Player player,\r\n                                             absl::Span<float> values) const {\r\n  SPIEL_CHECK_GE(player, 0);\r\n  SPIEL_CHECK_LT(player, num_players_);\r\n\r\n  // First 27 bits encodes the player's view in the same way as TicTacToe.\r\n  // Then the action sequence follows (one-hot encoded, per action).\r\n  // Encoded in the same way as InformationStateAsString, so full sequences\r\n  // which may contain action value 10 to represent \"I don't know.\"\r\n  const auto& player_view = player == 0 ? x_view_ : o_view_;\r\n  SPIEL_CHECK_EQ(values.size(), kNumCells * kCellStates +\r\n                                    kLongestSequence * (1 + kBitsPerAction));\r\n  std::fill(values.begin(), values.end(), 0.);\r\n  for (int cell = 0; cell < kNumCells; ++cell) {\r\n    values[kNumCells * static_cast<int>(player_view[cell]) + cell] = 1.0;\r\n  }\r\n\r\n  // Now encode the sequence. Each (player, action) pair uses 11 bits:\r\n  //   - first bit is the player taking the action (0 or 1)\r\n  //   - next 10 bits is the one-hot encoded action (10 = \"I don't know\")\r\n  int offset = kNumCells * kCellStates;\r\n  for (const auto& player_with_action : action_sequence_) {\r\n    if (player_with_action.first == player) {\r\n      // Always include the observing player's actions.\r\n      values[offset] = player_with_action.first;  // Player 0 or 1\r\n      values[offset + 1 + player_with_action.second] = 1.0;\r\n    } else if (obs_type_ == ObservationType::kRevealNumTurns) {\r\n      // If the number of turns are revealed, then each of the other player's\r\n      // actions will show up as unknowns.\r\n      values[offset] = player_with_action.first;\r\n      values[offset + 1 + 10] = 1.0;  // I don't know.\r\n    } else {\r\n      // Do not reveal anything about the number of actions taken by opponent.\r\n      SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\r\n    }\r\n\r\n    offset += (1 + kBitsPerAction);\r\n  }\r\n}\r\n```\r\n\r\nFirstly, `values[offset + 1 + 10] = 1.0;  // I don't know.` should be `1 + 9` I believe since actions range from 0-8 and not 1-9.\r\n\r\nSecond, we offset `offset += (1 + kBitsPerAction);` in any case. In the default case of `ObservationType::kRevealNothing`, I think we should only offset in the if and the else if, but now in the else (right now we always offset). \r\n\r\nFor instance, with `ObservationType::kRevealNumTurns` we could get, for player 0's info string:\r\n\r\n```bash\r\nmove 0: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 1: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 2: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 3: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\r\nmove 5: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n```\r\n\r\n(and you can see that the last \"i don't know\" action isn't set to 1, as per my previous point).\r\n\r\nWith `ObservationType::kRevealNothing`, we get:\r\n\r\n```bash\r\nmove 0: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 2: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 3: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r\nmove 4: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\r\nmove 5: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n```\r\n\r\nSo well, we can still see the number of moves the opponent did by counting the lines of zeros. \n", "patch": "diff --git a/open_spiel/games/dark_hex/dark_hex.cc b/open_spiel/games/dark_hex/dark_hex.cc\nindex 018199bb96..0e65c5b012 100644\n--- a/open_spiel/games/dark_hex/dark_hex.cc\n+++ b/open_spiel/games/dark_hex/dark_hex.cc\n@@ -107,11 +107,18 @@ DarkHexState::DarkHexState(std::shared_ptr<const Game> game, int num_cols,\n       game_version_(game_version),\n       num_cols_(num_cols),\n       num_rows_(num_rows),\n-      num_cells_(num_cols * num_rows),\n-      bits_per_action_(num_cells_ + 1),\n-      longest_sequence_(num_cells_ * 2 - 1) {\n+      num_cells_(num_cols * num_rows) {\n   black_view_.resize(num_cols * num_rows, CellState::kEmpty);\n   white_view_.resize(num_cols * num_rows, CellState::kEmpty);\n+  if (obs_type == ObservationType::kRevealNothing) {\n+    bits_per_action_ = num_cells_;\n+    longest_sequence_ = num_cells_;\n+  } else {\n+    SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNumTurns);\n+    // Reserve 0 for the player and 10 as \"I don't know.\"\n+    bits_per_action_ = num_cells_ + 2;\n+    longest_sequence_ = num_cells_ * 2 - 1;\n+  }\n }\n \n void DarkHexState::DoApplyAction(Action move) {\n@@ -218,7 +225,7 @@ void DarkHexState::InformationStateTensor(Player player,\n   const auto& player_view = (player == 0 ? black_view_ : white_view_);\n \n   SPIEL_CHECK_EQ(values.size(), num_cells_ * kCellStates +\n-                                    longest_sequence_ * (1 + bits_per_action_));\n+                                    longest_sequence_ * bits_per_action_);\n   std::fill(values.begin(), values.end(), 0.);\n   for (int cell = 0; cell < num_cells_; ++cell) {\n     values[cell * kCellStates +\n@@ -230,18 +237,25 @@ void DarkHexState::InformationStateTensor(Player player,\n   for (const auto& player_with_action : action_sequence_) {\n     if (player_with_action.first == player) {\n       // Always include the observing player's actions.\n-      values[offset] = player_with_action.first;\n-      values[offset + 1 + player_with_action.second] = 1.0;\n+      if (obs_type_ == ObservationType::kRevealNumTurns) {\n+        values[offset] = player_with_action.first;  // Player 0 or 1\n+        values[offset + 1 + player_with_action.second] = 1.0;\n+      } else {\n+        // Here we don't need to encode the player since we won't see opponent moves.\n+        SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\n+        values[offset + player_with_action.second] = 1.0;\n+      }\n+      offset += bits_per_action_;\n     } else if (obs_type_ == ObservationType::kRevealNumTurns) {\n       // If the number of turns are revealed, then each of the other player's\n       // actions will show up as unknowns. Here, num_cells_ is used to\n       // encode \"unknown\".\n       values[offset] = player_with_action.first;\n       values[offset + 1 + num_cells_] = 1.0;\n+      offset += bits_per_action_;\n     } else {\n       SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\n     }\n-    offset += (1 + bits_per_action_);\n   }\n }\n \n@@ -290,14 +304,17 @@ DarkHexGame::DarkHexGame(const GameParameters& params, GameType game_type)\n           ParameterValue<int>(\"num_cols\", ParameterValue<int>(\"board_size\"))),\n       num_rows_(\n           ParameterValue<int>(\"num_rows\", ParameterValue<int>(\"board_size\"))),\n-      num_cells_(num_cols_ * num_rows_),\n-      bits_per_action_(num_cells_ + 1),\n-      longest_sequence_(num_cells_ * 2 - 1) {\n+      num_cells_(num_cols_ * num_rows_) {\n   std::string obs_type = ParameterValue<std::string>(\"obstype\");\n   if (obs_type == \"reveal-nothing\") {\n     obs_type_ = ObservationType::kRevealNothing;\n+    bits_per_action_ = num_cells_;\n+    longest_sequence_ = num_cells_;\n   } else if (obs_type == \"reveal-numturns\") {\n     obs_type_ = ObservationType::kRevealNumTurns;\n+    // Reserve 0 for the player and 10 as \"I don't know.\"\n+    bits_per_action_ = num_cells_ + 2;\n+    longest_sequence_ = num_cells_ * 2 - 1;\n   } else {\n     SpielFatalError(absl::StrCat(\"Unrecognized observation type: \", obs_type));\n   }\n@@ -314,7 +331,7 @@ DarkHexGame::DarkHexGame(const GameParameters& params, GameType game_type)\n \n std::vector<int> DarkHexGame::InformationStateTensorShape() const {\n   return {num_cells_ * kCellStates +\n-          longest_sequence_ * (1 + bits_per_action_)};\n+          longest_sequence_ * bits_per_action_};\n }\n \n std::vector<int> DarkHexGame::ObservationTensorShape() const {\ndiff --git a/open_spiel/games/dark_hex/dark_hex.h b/open_spiel/games/dark_hex/dark_hex.h\nindex 5c7891aad7..4fcc3b0399 100644\n--- a/open_spiel/games/dark_hex/dark_hex.h\n+++ b/open_spiel/games/dark_hex/dark_hex.h\n@@ -125,8 +125,8 @@ class DarkHexState : public State {\n   const int num_cols_;  // x\n   const int num_rows_;  // y\n   const int num_cells_;\n-  const int bits_per_action_;\n-  const int longest_sequence_;\n+  int bits_per_action_;\n+  int longest_sequence_;\n \n   // Change this to _history on base class\n   std::vector<std::pair<int, Action>> action_sequence_;\n@@ -166,8 +166,8 @@ class DarkHexGame : public Game {\n   const int num_cols_;\n   const int num_rows_;\n   const int num_cells_;\n-  const int bits_per_action_;\n-  const int longest_sequence_;\n+  int bits_per_action_;\n+  int longest_sequence_;\n };\n \n class ImperfectRecallDarkHexState : public DarkHexState {\ndiff --git a/open_spiel/games/phantom_ttt/phantom_ttt.cc b/open_spiel/games/phantom_ttt/phantom_ttt.cc\nindex 7bdc585d2c..6d4f8ea7a0 100644\n--- a/open_spiel/games/phantom_ttt/phantom_ttt.cc\n+++ b/open_spiel/games/phantom_ttt/phantom_ttt.cc\n@@ -94,9 +94,21 @@ ImperfectRecallPTTTGame::ImperfectRecallPTTTGame(const GameParameters& params)\n \n PhantomTTTState::PhantomTTTState(std::shared_ptr<const Game> game,\n                                  ObservationType obs_type)\n-    : State(game), state_(game), obs_type_(obs_type) {\n+    : State(game),\n+      state_(game),\n+      obs_type_(obs_type) {\n   std::fill(begin(x_view_), end(x_view_), CellState::kEmpty);\n   std::fill(begin(o_view_), end(o_view_), CellState::kEmpty);\n+  if (obs_type_ == ObservationType::kRevealNumTurns) {\n+    // Reserve 0 for the player and 10 as \"I don't know.\"\n+    bits_per_action_ = kNumCells + 2;\n+    // Longest sequence is 17 moves, e.g. 0011223344556677889\n+    longest_sequence_ = 2 * kNumCells - 1;\n+  } else {\n+    SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\n+    bits_per_action_ = kNumCells;\n+    longest_sequence_ = kNumCells;\n+  }\n }\n \n void PhantomTTTState::DoApplyAction(Action move) {\n@@ -193,7 +205,7 @@ void PhantomTTTState::InformationStateTensor(Player player,\n   // which may contain action value 10 to represent \"I don't know.\"\n   const auto& player_view = player == 0 ? x_view_ : o_view_;\n   SPIEL_CHECK_EQ(values.size(), kNumCells * kCellStates +\n-                                    kLongestSequence * (1 + kBitsPerAction));\n+                                    longest_sequence_ * bits_per_action_);\n   std::fill(values.begin(), values.end(), 0.);\n   for (int cell = 0; cell < kNumCells; ++cell) {\n     values[kNumCells * static_cast<int>(player_view[cell]) + cell] = 1.0;\n@@ -206,19 +218,25 @@ void PhantomTTTState::InformationStateTensor(Player player,\n   for (const auto& player_with_action : action_sequence_) {\n     if (player_with_action.first == player) {\n       // Always include the observing player's actions.\n-      values[offset] = player_with_action.first;  // Player 0 or 1\n-      values[offset + 1 + player_with_action.second] = 1.0;\n+      if (obs_type_ == ObservationType::kRevealNumTurns) {\n+        values[offset] = player_with_action.first;  // Player 0 or 1\n+        values[offset + 1 + player_with_action.second] = 1.0;\n+      } else {\n+        // Here we don't need to encode the player since we won't see opponent moves.\n+        SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\n+        values[offset + player_with_action.second] = 1.0;\n+      }\n+      offset += bits_per_action_;\n     } else if (obs_type_ == ObservationType::kRevealNumTurns) {\n       // If the number of turns are revealed, then each of the other player's\n       // actions will show up as unknowns.\n       values[offset] = player_with_action.first;\n-      values[offset + 1 + 10] = 1.0;  // I don't know.\n+      values[offset + 1 + kNumCells] = 1.0;  // I don't know.\n+      offset += bits_per_action_;\n     } else {\n       // Do not reveal anything about the number of actions taken by opponent.\n       SPIEL_CHECK_EQ(obs_type_, ObservationType::kRevealNothing);\n     }\n-\n-    offset += (1 + kBitsPerAction);\n   }\n }\n \n@@ -283,8 +301,14 @@ PhantomTTTGame::PhantomTTTGame(const GameParameters& params, GameType game_type)\n   std::string obs_type = ParameterValue<std::string>(\"obstype\");\n   if (obs_type == \"reveal-nothing\") {\n     obs_type_ = ObservationType::kRevealNothing;\n+    bits_per_action_ = kNumCells;\n+    longest_sequence_ = kNumCells;\n   } else if (obs_type == \"reveal-numturns\") {\n     obs_type_ = ObservationType::kRevealNumTurns;\n+    // Reserve 0 for the player and 10 as \"I don't know.\"\n+    bits_per_action_ = kNumCells + 2;\n+    // Longest sequence is 17 moves, e.g. 0011223344556677889\n+    longest_sequence_ = 2 * kNumCells - 1;\n   } else {\n     SpielFatalError(absl::StrCat(\"Unrecognized observation type: \", obs_type));\n   }\n@@ -292,16 +316,16 @@ PhantomTTTGame::PhantomTTTGame(const GameParameters& params, GameType game_type)\n \n std::vector<int> PhantomTTTGame::InformationStateTensorShape() const {\n   // Enc\n-  return {1, kNumCells * kCellStates + kLongestSequence * (1 + kBitsPerAction)};\n+  return {1, kNumCells * kCellStates + longest_sequence_ * bits_per_action_};\n }\n \n std::vector<int> PhantomTTTGame::ObservationTensorShape() const {\n   if (obs_type_ == ObservationType::kRevealNothing) {\n     return {kNumCells * kCellStates};\n   } else if (obs_type_ == ObservationType::kRevealNumTurns) {\n-    return {kNumCells * kCellStates + kLongestSequence};\n+    return {kNumCells * kCellStates + longest_sequence_};\n   } else {\n-    SpielFatalError(\"Uknown observation type\");\n+    SpielFatalError(\"Unknown observation type\");\n   }\n }\n \ndiff --git a/open_spiel/games/phantom_ttt/phantom_ttt.h b/open_spiel/games/phantom_ttt/phantom_ttt.h\nindex df1ac0a0e3..77df2fd46b 100644\n--- a/open_spiel/games/phantom_ttt/phantom_ttt.h\n+++ b/open_spiel/games/phantom_ttt/phantom_ttt.h\n@@ -45,10 +45,6 @@ namespace phantom_ttt {\n \n inline constexpr const char* kDefaultObsType = \"reveal-nothing\";\n \n-// Longest sequence is 17 moves, e.g. 0011223344556677889\n-inline constexpr int kLongestSequence = 2 * tic_tac_toe::kNumCells - 1;\n-inline constexpr int kBitsPerAction = 10;  // Reserve 9 as \"I don't know.\"\n-\n enum class ObservationType {\n   kRevealNothing,\n   kRevealNumTurns,\n@@ -88,6 +84,9 @@ class PhantomTTTState : public State {\n \n   tic_tac_toe::TicTacToeState state_;\n   ObservationType obs_type_;\n+  int bits_per_action_;\n+  int longest_sequence_;\n+\n   // TODO(author2): Use the base class history_ instead.\n   std::vector<std::pair<int, Action>> action_sequence_;\n   std::array<tic_tac_toe::CellState, tic_tac_toe::kNumCells> x_view_;\n@@ -119,13 +118,17 @@ class PhantomTTTGame : public Game {\n   // These will depend on the obstype parameter.\n   std::vector<int> InformationStateTensorShape() const override;\n   std::vector<int> ObservationTensorShape() const override;\n-  int MaxGameLength() const override { return kLongestSequence; }\n+  int MaxGameLength() const override { \n+    return tic_tac_toe::kNumCells * 2 - 1;\n+  }\n \n   ObservationType obs_type() const { return obs_type_; }\n \n  private:\n   std::shared_ptr<const tic_tac_toe::TicTacToeGame> game_;\n   ObservationType obs_type_;\n+  int bits_per_action_;\n+  int longest_sequence_;\n };\n \n // Implements the FOE abstraction from Lanctot et al. '12\n", "instance_id": "google-deepmind__open_spiel-1275", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement for the Phantom Tic-Tac-Toe information state bug is mostly clear, as it identifies specific issues in the code related to the encoding of actions in the information state tensor and the handling of offsets under different observation types. It provides detailed examples of the expected and actual outputs, which help in understanding the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for all observation types in every scenario, and the explanation of why certain offsets or encodings are incorrect could be more precise. Additionally, while the issue with revealing the number of opponent moves under `kRevealNothing` is mentioned, the desired fix or behavior is not fully articulated beyond adjusting offsets. Overall, the statement is valid and clear but lacks some minor details for a fully comprehensive description, such as explicit expected outcomes for all cases or constraints on the solution.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (`phantom_ttt.cc`, `phantom_ttt.h`, `dark_hex.cc`, `dark_hex.h`), affecting both the Tic-Tac-Toe and Dark Hex implementations, which requires understanding the shared logic and architecture of information state handling in these games. The changes involve modifying how actions are encoded in the information state tensor and adjusting offset calculations based on observation types, which demands a good grasp of the existing codebase structure and the domain-specific logic of imperfect information games. \n\nTechnically, the problem requires understanding C++ concepts such as class member initialization, conditional logic, and array manipulation, as well as domain-specific knowledge of game theory concepts like information states and observation types. While the concepts themselves are not overly complex, applying them correctly across multiple related components adds to the challenge. The code changes are moderate in size but impactful, as they alter the representation of game states, which could affect downstream components like AI agents or game solvers that rely on these tensors.\n\nEdge cases and error handling are implicitly involved, as the problem deals with different observation types (`kRevealNothing` vs. `kRevealNumTurns`) and ensuring that the tensor encoding does not leak unintended information about opponent moves. However, the problem statement does not explicitly call for extensive new error handling beyond fixing the existing logic. Overall, this problem requires a moderate level of expertise to navigate the codebase, understand the intent of the information state encoding, and implement the fixes without introducing new issues, placing it in the 0.4-0.6 range with a score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: `group_by_dynamic` `period` and `every` get applied in reverse order\n### Checks\n\n- [X] I have checked that this issue has not already been reported.\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\n\n### Reproducible example\n\n```python\r\nIn [18]: df = pl.DataFrame({'a': [datetime(2020, 1, 1), datetime(2020, 2, 1), datetime(2020, 3, 1)], 'b': [1,2,3]})\r\n\r\nIn [19]: df.group_by_dynamic('a', every='1mo', period='45d', include_boundaries=True).agg(pl.col('b').sum())\r\nOut[19]:\r\nshape: (3, 4)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 _lower_boundary     \u2506 _upper_boundary     \u2506 a                   \u2506 b   \u2502\r\n\u2502 ---                 \u2506 ---                 \u2506 ---                 \u2506 --- \u2502\r\n\u2502 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 i64 \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 2020-01-01 00:00:00 \u2506 2020-02-15 00:00:00 \u2506 2020-01-01 00:00:00 \u2506 3   \u2502\r\n\u2502 2020-02-01 00:00:00 \u2506 2020-03-15 00:00:00 \u2506 2020-02-01 00:00:00 \u2506 5   \u2502\r\n\u2502 2020-03-01 00:00:00 \u2506 2020-04-15 00:00:00 \u2506 2020-03-01 00:00:00 \u2506 3   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\n\n### Log output\n\n_No response_\n\n### Issue description\n\nIt looks like `period` is getting applied before `every`, as opposed to\r\n```\r\n[start + every, start + every + period)\r\n```\r\nwhich is what's documented\r\n\r\nI think the documentation expresses the desired behaviour - if you pass `period=45d`, then you expect each period to be 45 days long\n\n### Expected behavior\n\n```\r\nshape: (3, 4)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 _lower_boundary     \u2506 _upper_boundary     \u2506 a                   \u2506 b   \u2502\r\n\u2502 ---                 \u2506 ---                 \u2506 ---                 \u2506 --- \u2502\r\n\u2502 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 i64 \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 2020-01-01 00:00:00 \u2506 2020-02-15 00:00:00 \u2506 2020-01-01 00:00:00 \u2506 3   \u2502\r\n\u2502 2020-02-01 00:00:00 \u2506 2022-03-18 00:00:00 \u2506 2020-02-01 00:00:00 \u2506 5   \u2502\r\n\u2502 2020-03-01 00:00:00 \u2506 2022-04-15 00:00:00 \u2506 2020-03-01 00:00:00 \u2506 3   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n### Installed versions\n\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.12.0\r\nIndex type:          UInt32\r\nPlatform:            Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nPython:              3.12.5 (main, Aug 14 2024, 05:08:31) [Clang 18.1.8 ]\r\nLTS CPU:             False\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  <not installed>\r\naltair               5.4.1\r\ncloudpickle          3.0.0\r\nconnectorx           <not installed>\r\ndeltalake            <not installed>\r\nfastexcel            <not installed>\r\nfsspec               2024.9.0\r\ngevent               <not installed>\r\ngreat_tables         0.11.0\r\nmatplotlib           3.9.2\r\nnest_asyncio         1.6.0\r\nnumpy                2.1.3\r\nopenpyxl             <not installed>\r\npandas               2.2.3\r\npyarrow              18.0.0\r\npydantic             <not installed>\r\npyiceberg            <not installed>\r\nsqlalchemy           <not installed>\r\ntorch                <not installed>\r\nxlsx2csv             <not installed>\r\nxlsxwriter           <not installed>\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/crates/polars-time/src/windows/window.rs b/crates/polars-time/src/windows/window.rs\nindex c7a29b846c58..9609f2abc514 100644\n--- a/crates/polars-time/src/windows/window.rs\n+++ b/crates/polars-time/src/windows/window.rs\n@@ -327,15 +327,15 @@ impl Iterator for BoundsIter<'_> {\n                 // Issue is that `next` needs to return `Option`.\n                 TimeUnit::Nanoseconds => {\n                     self.bi.start = self.window.every.add_ns(self.bi.start, self.tz).unwrap();\n-                    self.bi.stop = self.window.every.add_ns(self.bi.stop, self.tz).unwrap();\n+                    self.bi.stop = self.window.period.add_ns(self.bi.start, self.tz).unwrap();\n                 },\n                 TimeUnit::Microseconds => {\n                     self.bi.start = self.window.every.add_us(self.bi.start, self.tz).unwrap();\n-                    self.bi.stop = self.window.every.add_us(self.bi.stop, self.tz).unwrap();\n+                    self.bi.stop = self.window.period.add_us(self.bi.start, self.tz).unwrap();\n                 },\n                 TimeUnit::Milliseconds => {\n                     self.bi.start = self.window.every.add_ms(self.bi.start, self.tz).unwrap();\n-                    self.bi.stop = self.window.every.add_ms(self.bi.stop, self.tz).unwrap();\n+                    self.bi.stop = self.window.period.add_ms(self.bi.start, self.tz).unwrap();\n                 },\n             }\n             Some(out)\n", "instance_id": "pola-rs__polars-19706", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear, providing a reproducible example, expected behavior, and a description of the issue with the `group_by_dynamic` function in the Polars library. The issue is well-documented with input data, actual output, and expected output, which helps in understanding the bug related to the order of applying `period` and `every` parameters. However, there are minor ambiguities, such as the lack of explicit mention of edge cases (e.g., how the function should behave with negative or zero values for `every` or `period`, or with different time units). Additionally, the problem statement does not fully clarify the broader implications of the bug across different use cases or configurations. Hence, it falls under \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of solving this problem is rated as Easy (0.2-0.4) due to several factors. First, the scope of the code change is limited to a single file (`window.rs`) and involves a straightforward modification in the logic of how `period` and `every` are applied in the `BoundsIter` iterator. The change replaces the use of `every` with `period` for calculating the `stop` boundary, which is a simple fix in terms of code volume and complexity. Second, the technical concepts required are relatively basic: understanding Rust iterators, time unit handling, and the specific logic of time window calculations in the Polars library. No advanced algorithms, design patterns, or deep architectural changes are needed. Third, while the problem statement does not explicitly mention edge cases, the code change does not appear to introduce new error handling requirements beyond what is already in place (e.g., the `unwrap()` calls suggest existing assumptions about valid time calculations). Finally, the impact is localized and does not affect the broader system architecture. Overall, this is a simple bug fix requiring moderate understanding of the specific function's logic, fitting within the Easy difficulty range at 0.30.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "rx.digraph_find_cycle() does not find the cycle in a graph\n<!-- \u26a0\ufe0f If you do not respect this template, your issue will be closed -->\r\n<!-- \u26a0\ufe0f Make sure to browse the opened and closed issues -->\r\n\r\n### Information\r\n\r\n- **rustworkx version**: 0.14.2\r\n- **Python version**: 3.10.14\r\n- **Rust version**: rustc 1.66.0-nightly (8b705839c 2022-09-26)\r\n- **Operating system**: Linux\r\n\r\n### What is the current behavior?\r\n`rx.is_directed_acyclic_graph(graph)` returns False but `rx.digraph_find_cycle(graph)` fails to find the cycle in the graph.\r\n\r\n### What is the expected behavior?\r\nWhen `rx.is_directed_acyclic_graph(graph)` returns False, `rx.digraph_find_cycle(graph)` should find a cycle in the graph.\r\n\r\n### Steps to reproduce the problem\r\nRunning the following script several times. It quite often reproduces the bug. The graph is always correctly identified as not a DAG, but quite often `rx.digraph_find_cycle(graph)` does not find a cycle.\r\n```\r\nimport rustworkx as rx\r\n\r\nedges = [\r\n    (6, 7),\r\n    (8, 9),\r\n    (15, 16),\r\n    (0, 6),\r\n    (2, 8),\r\n    (9, 10),\r\n    (7, 10),\r\n    (10, 11),\r\n    (11, 12),\r\n    (10, 12),\r\n    (12, 13),\r\n    (12, 14),\r\n    (4, 15),\r\n    (16, 17),\r\n    (14, 3),\r\n    (13, 17),\r\n    (17, 18),\r\n    (18, 19),\r\n    (17, 19),\r\n    (19, 20),\r\n    (19, 5),\r\n    (20, 1),\r\n    (27, 28),\r\n    (28, 29),\r\n    (34, 35),\r\n    (21, 27),\r\n    (23, 29),\r\n    (29, 30),\r\n    (30, 31),\r\n    (29, 31),\r\n    (31, 32),\r\n    (31, 33),\r\n    (25, 34),\r\n    (32, 24),\r\n    (35, 36),\r\n    (33, 36),\r\n    (36, 37),\r\n    (37, 38),\r\n    (36, 38),\r\n    (38, 39),\r\n    (38, 26),\r\n    (39, 22),\r\n    (42, 45),\r\n    (44, 45),\r\n    (40, 44),\r\n    (45, 46),\r\n    (46, 47),\r\n    (45, 47),\r\n    (47, 48),\r\n    (47, 49),\r\n    (48, 43),\r\n    (49, 41),\r\n    (56, 57),\r\n    (57, 58),\r\n    (61, 63),\r\n    (50, 56),\r\n    (52, 57),\r\n    (58, 59),\r\n    (57, 59),\r\n    (59, 60),\r\n    (54, 62),\r\n    (59, 61),\r\n    (60, 53),\r\n    (62, 63),\r\n    (63, 64),\r\n    (64, 65),\r\n    (63, 65),\r\n    (65, 66),\r\n    (65, 67),\r\n    (66, 51),\r\n    (67, 55),\r\n    (74, 75),\r\n    (75, 76),\r\n    (80, 81),\r\n    (68, 74),\r\n    (70, 76),\r\n    (76, 77),\r\n    (77, 78),\r\n    (76, 78),\r\n    (78, 79),\r\n    (78, 80),\r\n    (72, 81),\r\n    (79, 71),\r\n    (81, 82),\r\n    (82, 83),\r\n    (81, 83),\r\n    (83, 84),\r\n    (83, 69),\r\n    (84, 73),\r\n    (1, 42),\r\n    (3, 23),\r\n    (24, 72),\r\n    (22, 50),\r\n    (5, 54),\r\n    (41, 40),\r\n    (26, 52),\r\n    (69, 70),\r\n]\r\nedges = [x + (None,) for x in edges]\r\nnodes = set()\r\nfor edge in edges:\r\n    nodes.add(edge[0])\r\n    nodes.add(edge[1])\r\nnodes = list(nodes)\r\n\r\n# Create a directed graph (PyDiGraph)\r\ngraph = rx.PyDiGraph()\r\n\r\n# Add nodes and edges\r\ngraph.add_nodes_from(nodes)\r\ngraph.add_edges_from(edges)\r\n\r\n# Check if the graph is a DAG\r\nis_dag = rx.is_directed_acyclic_graph(graph)\r\nprint(\"Is the graph a DAG? {}\".format(is_dag))\r\n\r\n# Attempt to find a cycle\r\ncycle = rx.digraph_find_cycle(graph)\r\nprint(\"Cycle found : {}\".format(cycle))\r\n```\r\nQuite often the script produces:\r\n```\r\nIs the graph a DAG? False\r\nCycle found : EdgeList[]\r\n```\n", "patch": "diff --git a/releasenotes/notes/fix-digraph-find-cycle-141e302ff4a8fcd4.yaml b/releasenotes/notes/fix-digraph-find-cycle-141e302ff4a8fcd4.yaml\nnew file mode 100644\nindex 000000000..578459129\n--- /dev/null\n+++ b/releasenotes/notes/fix-digraph-find-cycle-141e302ff4a8fcd4.yaml\n@@ -0,0 +1,12 @@\n+---\n+fixes:\n+  - |\n+    Fixed the behavior of :func:`~rustworkx.digraph_find_cycle` when\n+    no source node was provided. Previously, the function would start looking\n+    for a cycle at an arbitrary node which was not guaranteed to return a cycle.\n+    Now, the function will smartly choose a source node to start the search from\n+    such that if a cycle exists, it will be found.\n+other:\n+  - |\n+    The `rustworkx-core` function `rustworkx_core::connectivity::find_cycle` now\n+    requires the `petgraph::visit::Visitable` trait.\ndiff --git a/rustworkx-core/src/connectivity/find_cycle.rs b/rustworkx-core/src/connectivity/find_cycle.rs\nindex 4bbd755e5..c04847c50 100644\n--- a/rustworkx-core/src/connectivity/find_cycle.rs\n+++ b/rustworkx-core/src/connectivity/find_cycle.rs\n@@ -11,8 +11,9 @@\n // under the License.\n \n use hashbrown::{HashMap, HashSet};\n+use petgraph::algo;\n use petgraph::visit::{\n-    EdgeCount, GraphBase, IntoNeighborsDirected, IntoNodeIdentifiers, NodeCount,\n+    EdgeCount, GraphBase, IntoNeighborsDirected, IntoNodeIdentifiers, NodeCount, Visitable,\n };\n use petgraph::Direction::Outgoing;\n use std::hash::Hash;\n@@ -57,22 +58,22 @@ where\n     G: GraphBase,\n     G: NodeCount,\n     G: EdgeCount,\n-    for<'b> &'b G: GraphBase<NodeId = G::NodeId> + IntoNodeIdentifiers + IntoNeighborsDirected,\n+    for<'b> &'b G:\n+        GraphBase<NodeId = G::NodeId> + IntoNodeIdentifiers + IntoNeighborsDirected + Visitable,\n     G::NodeId: Eq + Hash,\n {\n     // Find a cycle in the given graph and return it as a list of edges\n-    let mut graph_nodes: HashSet<G::NodeId> = graph.node_identifiers().collect();\n     let mut cycle: Vec<(G::NodeId, G::NodeId)> = Vec::with_capacity(graph.edge_count());\n-    let temp_value: G::NodeId;\n-    // If source is not set get an arbitrary node from the set of graph\n-    // nodes we've not \"examined\"\n+    // If source is not set get a node in an arbitrary cycle if it exists,\n+    // otherwise return that there is no cycle\n     let source_index = match source {\n         Some(source_value) => source_value,\n-        None => {\n-            temp_value = *graph_nodes.iter().next().unwrap();\n-            graph_nodes.remove(&temp_value);\n-            temp_value\n-        }\n+        None => match find_node_in_arbitrary_cycle(&graph) {\n+            Some(node_in_cycle) => node_in_cycle,\n+            None => {\n+                return Vec::new();\n+            }\n+        },\n     };\n     // Stack (ie \"pushdown list\") of vertices already in the spanning tree\n     let mut stack: Vec<G::NodeId> = vec![source_index];\n@@ -119,11 +120,47 @@ where\n     cycle\n }\n \n+fn find_node_in_arbitrary_cycle<G>(graph: &G) -> Option<G::NodeId>\n+where\n+    G: GraphBase,\n+    G: NodeCount,\n+    G: EdgeCount,\n+    for<'b> &'b G:\n+        GraphBase<NodeId = G::NodeId> + IntoNodeIdentifiers + IntoNeighborsDirected + Visitable,\n+    G::NodeId: Eq + Hash,\n+{\n+    for scc in algo::kosaraju_scc(&graph) {\n+        if scc.len() > 1 {\n+            return Some(scc[0]);\n+        }\n+    }\n+    for node in graph.node_identifiers() {\n+        for neighbor in graph.neighbors_directed(node, Outgoing) {\n+            if neighbor == node {\n+                return Some(node);\n+            }\n+        }\n+    }\n+    None\n+}\n+\n #[cfg(test)]\n mod tests {\n     use crate::connectivity::find_cycle;\n     use petgraph::prelude::*;\n \n+    // Utility to assert cycles in the response\n+    macro_rules! assert_cycle {\n+        ($g: expr, $cycle: expr) => {{\n+            for i in 0..$cycle.len() {\n+                let (s, t) = $cycle[i];\n+                assert!($g.contains_edge(s, t));\n+                let (next_s, _) = $cycle[(i + 1) % $cycle.len()];\n+                assert_eq!(t, next_s);\n+            }\n+        }};\n+    }\n+\n     #[test]\n     fn test_find_cycle_source() {\n         let edge_list = vec![\n@@ -141,20 +178,13 @@ mod tests {\n             (8, 9),\n         ];\n         let graph = DiGraph::<i32, i32>::from_edges(edge_list);\n-        let mut res: Vec<(usize, usize)> = find_cycle(&graph, Some(NodeIndex::new(0)))\n-            .iter()\n-            .map(|(s, t)| (s.index(), t.index()))\n-            .collect();\n-        assert_eq!(res, [(0, 1), (1, 2), (2, 3), (3, 0)]);\n-        res = find_cycle(&graph, Some(NodeIndex::new(1)))\n-            .iter()\n-            .map(|(s, t)| (s.index(), t.index()))\n-            .collect();\n-        assert_eq!(res, [(1, 2), (2, 3), (3, 0), (0, 1)]);\n-        res = find_cycle(&graph, Some(NodeIndex::new(5)))\n-            .iter()\n-            .map(|(s, t)| (s.index(), t.index()))\n-            .collect();\n+        for i in [0, 1, 2, 3].iter() {\n+            let idx = NodeIndex::new(*i);\n+            let res = find_cycle(&graph, Some(idx));\n+            assert_cycle!(graph, res);\n+            assert_eq!(res[0].0, idx);\n+        }\n+        let res = find_cycle(&graph, Some(NodeIndex::new(5)));\n         assert_eq!(res, []);\n     }\n \n@@ -176,10 +206,32 @@ mod tests {\n         ];\n         let mut graph = DiGraph::<i32, i32>::from_edges(edge_list);\n         graph.add_edge(NodeIndex::new(1), NodeIndex::new(1), 0);\n-        let res: Vec<(usize, usize)> = find_cycle(&graph, Some(NodeIndex::new(0)))\n-            .iter()\n-            .map(|(s, t)| (s.index(), t.index()))\n-            .collect();\n-        assert_eq!(res, [(1, 1)]);\n+        let res = find_cycle(&graph, Some(NodeIndex::new(0)));\n+        assert_eq!(res[0].0, NodeIndex::new(1));\n+        assert_cycle!(graph, res);\n+    }\n+\n+    #[test]\n+    fn test_self_loop_no_source() {\n+        let edge_list = vec![(0, 1), (1, 2), (2, 3), (2, 2)];\n+        let graph = DiGraph::<i32, i32>::from_edges(edge_list);\n+        let res = find_cycle(&graph, None);\n+        assert_cycle!(graph, res);\n+    }\n+\n+    #[test]\n+    fn test_cycle_no_source() {\n+        let edge_list = vec![(0, 1), (1, 2), (2, 3), (3, 4), (4, 2)];\n+        let graph = DiGraph::<i32, i32>::from_edges(edge_list);\n+        let res = find_cycle(&graph, None);\n+        assert_cycle!(graph, res);\n+    }\n+\n+    #[test]\n+    fn test_no_cycle_no_source() {\n+        let edge_list = vec![(0, 1), (1, 2), (2, 3)];\n+        let graph = DiGraph::<i32, i32>::from_edges(edge_list);\n+        let res = find_cycle(&graph, None);\n+        assert_eq!(res, []);\n     }\n }\n", "instance_id": "Qiskit__rustworkx-1181", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: `rx.digraph_find_cycle()` fails to identify a cycle in a directed graph even when `rx.is_directed_acyclic_graph()` correctly identifies the graph as not being a DAG. The expected behavior is explicitly stated, and a detailed script is provided to reproduce the issue, which is very helpful. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the issue occurs with specific types of cycles (e.g., self-loops, multiple cycles) or graph structures beyond the provided example. Additionally, edge cases such as empty graphs, disconnected components, or graphs with no cycles are not mentioned, which could be relevant for a comprehensive fix. Overall, the statement is valid and clear but lacks exhaustive detail on constraints and edge cases, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of the code changes is relatively focused, primarily affecting the `find_cycle` function in the `rustworkx-core` library, with modifications in a single Rust file. However, the change introduces a new approach to selecting a starting node for cycle detection using Kosaraju's algorithm for strongly connected components (SCCs), which requires a deep understanding of graph theory and algorithms. The technical concepts involved include graph traversal, cycle detection, SCC computation, and familiarity with the `petgraph` library's traits and APIs (e.g., `Visitable`, `IntoNeighborsDirected`). Additionally, the fix impacts the core behavior of a critical graph algorithm, which could have downstream effects on performance and correctness in various use cases, necessitating careful consideration of edge cases like self-loops, disconnected graphs, and graphs with no cycles (as seen in the updated test cases). While the problem does not require extensive architectural changes or system-level considerations, the combination of algorithmic complexity, the need for precise error handling, and the potential impact on library users justifies a difficulty score of 0.65. This reflects a challenging task that demands a solid grasp of Rust, graph algorithms, and library-specific nuances, but it is not at the extreme end of difficulty.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "FR: Check Compatibility with Python 3.12\n### Describe the workflow you want to enable\n\nOther common libraries, e.g., scikit-learn, already support Python 3.12. Accordingly, we should check how we also can support this Python version.\n\n### Describe your proposed solution\n\nWe need to run our tests with Python 3.12 and must scan our requirements for their supported Python versions.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml\nindex 7aed6021..3d4deb23 100644\n--- a/.github/workflows/main.yml\n+++ b/.github/workflows/main.yml\n@@ -9,7 +9,7 @@ jobs:\n     strategy:\n       max-parallel: 4\n       matrix:\n-        python-version: [ '3.8', '3.9', '3.10', '3.11']\n+        python-version: [ '3.9', '3.10', '3.11', '3.12' ]\n \n     steps:\n     - uses: actions/checkout@v4\ndiff --git a/README.rst b/README.rst\nindex 9fb05170..805fed20 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -8,7 +8,7 @@\n .. |Codecov| image:: https://codecov.io/gh/scikit-activeml/scikit-activeml/branch/master/graph/badge.svg\n    :target: https://app.codecov.io/gh/scikit-activeml/scikit-activeml\n \n-.. |PythonVersion| image:: https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue.svg\n+.. |PythonVersion| image:: https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue.svg\n    :target: https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue\n \n .. |PyPi| image:: https://badge.fury.io/py/scikit-activeml.svg\ndiff --git a/requirements.txt b/requirements.txt\nindex e547a64f..ae64c955 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,7 +1,7 @@\n-joblib>=1.2\n-numpy>=1.24\n-scipy>=1.9.2\n-scikit-learn>=1.2\n-matplotlib>=3.6.1\n-iteration-utilities>=0.11\n-makefun>=1.15.2\n\\ No newline at end of file\n+joblib>=1.4.0\n+numpy>=1.26\n+scipy>=1.11.3\n+scikit-learn>=1.3.2\n+matplotlib>=3.7.3\n+iteration-utilities>=0.12.0\n+makefun>=1.15.3\n\\ No newline at end of file\ndiff --git a/requirements_max.txt b/requirements_max.txt\nindex 4865a56d..6ab58cf1 100644\n--- a/requirements_max.txt\n+++ b/requirements_max.txt\n@@ -1,7 +1,7 @@\n joblib<=1.4.2\n-numpy<=1.26.4\n+numpy<=2.0.0\n scipy<=1.14.0\n scikit-learn<=1.5.1\n-matplotlib<=3.7.2\n+matplotlib<=3.9.1\n iteration-utilities<=0.12.1\n-makefun<=1.15.2\n\\ No newline at end of file\n+makefun<=1.15.4\n\\ No newline at end of file\ndiff --git a/setup.py b/setup.py\nindex 9bc8ea03..b3113190 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -28,9 +28,10 @@ def requirements():\n         \"Intended Audience :: Developers\",\n         \"License :: OSI Approved :: BSD License\",\n         \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n+        \"Programming Language :: Python :: 3.11\",\n+        \"Programming Language :: Python :: 3.12\",\n         \"Operating System :: OS Independent\",\n     ],\n     keywords=[\n@@ -43,7 +44,7 @@ def requirements():\n     ],\n     url=\"https://github.com/scikit-activeml/scikit-activeml\",\n     author=\"Marek Herde\",\n-    python_requires=\">=3.8\",\n+    python_requires=\">=3.9\",\n     author_email=\"marek.herde@uni-kassel.de\",\n     license=\"BSD 3-Clause License\",\n     packages=setuptools.find_packages(),\n", "instance_id": "scikit-activeml__scikit-activeml-417", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to enable compatibility with Python 3.12 by running tests and updating requirements. The goal is straightforward: ensure the library supports the new Python version. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention whether there are known compatibility issues with Python 3.12 or if specific features of Python 3.12 need to be addressed. Additionally, there is no mention of potential edge cases, such as deprecated features or changes in Python 3.12 that might break existing code. While the proposed solution (running tests and scanning requirements) is clear, it lacks depth regarding how to handle potential failures or incompatibilities during testing. Overall, the statement is valid and mostly clear but misses some critical details that could impact implementation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively minimal and localized to configuration files (e.g., GitHub Actions workflow, setup.py, requirements.txt, README.rst). The modifications involve updating Python version references and dependency versions, which do not impact the core codebase or system architecture. The changes span multiple files but are straightforward and do not require deep interaction between different parts of the codebase. The overall amount of code change is small.\n\n2. **Number of Technical Concepts:** The problem requires basic knowledge of Python versioning, dependency management, and CI/CD configuration (specifically GitHub Actions). No advanced programming language features, complex algorithms, or design patterns are needed. The task involves updating version strings and ensuring compatibility, which is a routine task for most developers familiar with Python projects. Domain-specific knowledge of machine learning (relevant to scikit-activeml) is not required for these changes.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases or error conditions related to Python 3.12 compatibility. However, implicit edge cases might include dependency conflicts or breaking changes in Python 3.12 that are not yet identified. The provided code changes do not involve adding or modifying error handling logic, as they are purely configuration updates. Any complexity related to edge cases would likely arise during testing (not covered in the code changes shown), but this is outside the scope of the current modifications.\n\n4. **Overall Assessment:** The task is simple and involves routine updates to support a new Python version. It requires minimal understanding of the codebase beyond configuration files and does not involve complex logic or significant risk of introducing bugs. The primary challenge might be verifying compatibility through testing, but the code changes themselves are low-risk and easy to implement. Therefore, a difficulty score of 0.25 reflects the straightforward nature of the task with minor potential for unforeseen issues during testing.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Django 5.1: ImportError: cannot import name 'get_storage_class' from 'django.core.files.storage' \nWhen I tried to use `django-storages` with Django 5.1 I have got following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/petr/soubory/programovani/blenderkit/blenderhub_server/manage.py\", line 28, in <module>\r\n    execute_from_command_line(sys.argv)\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/django/core/management/__init__.py\", line 442, in execute_from_command_line\r\n    utility.execute()\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/django/core/management/__init__.py\", line 416, in execute\r\n    django.setup()\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/django/__init__.py\", line 24, in setup\r\n    apps.populate(settings.INSTALLED_APPS)\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/django/apps/registry.py\", line 124, in populate\r\n    app_config.ready()\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/wagtail_storages/apps.py\", line 10, in ready\r\n    from wagtail_storages import signal_handlers\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/wagtail_storages/signal_handlers.py\", line 9, in <module>\r\n    from wagtail_storages.utils import (\r\n  File \"/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/wagtail_storages/utils.py\", line 4, in <module>\r\n    from django.core.files.storage import get_storage_class\r\nImportError: cannot import name 'get_storage_class' from 'django.core.files.storage' (/home/petr/.cache/pypoetry/virtualenvs/blenderkit-server-eqrfB9dY-py3.11/lib/python3.11/site-packages/django/core/files/storage/__init__.py\r\n```\n", "patch": "diff --git a/tox.ini b/tox.ini\nindex 56dbf87..5a12fa2 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -1,7 +1,8 @@\n [tox]\n envlist =\n-    py{38,39,310,311}-dj42-wagtail{52,60,61}\n-    py{310,311,312}-dj50-wagtail{52,60,61}\n+    py{38,39,310,311}-dj42-wagtail{52,61,62}\n+    py{311,312}-dj{50}-wagtail{52,61,62}\n+    py{311,312}-dj{51}-wagtail{61,62}\n     flake8\n     isort\n     black\n@@ -21,9 +22,11 @@ setenv =\n deps =\n     dj42: Django>=4.2,<4.3\n     dj50: Django>=5.0,<5.1\n+    dj51: Django>=5.1,<5.2\n     wagtail52: wagtail>=5.2,<5.3\n     wagtail60: wagtail>=6.0,<6.1\n     wagtail61: wagtail>=6.1,<6.2\n+    wagtail62: wagtail>=6.2,<6.3\n     wagtailmain: git+https://github.com/wagtail/wagtail.git@main#egg=Wagtail\n \n install_command = pip install -U {opts} {packages}\ndiff --git a/wagtail_storages/utils.py b/wagtail_storages/utils.py\nindex ad19ec7..1eba6b3 100644\n--- a/wagtail_storages/utils.py\n+++ b/wagtail_storages/utils.py\n@@ -1,7 +1,7 @@\n import logging\n \n from django.conf import settings\n-from django.core.files.storage import get_storage_class\n+from django.core.files.storage import default_storage\n \n from wagtail.contrib.frontend_cache.utils import PurgeBatch\n from wagtail.documents import get_document_model\n@@ -47,7 +47,7 @@ def build_absolute_urls_for_all_sites_for_path(path):\n \n def is_s3_boto3_storage_used():\n     return issubclass(\n-        get_storage_class(),\n+        default_storage.__class__,\n         storages.backends.s3boto3.S3Boto3Storage,\n     )\n \n", "instance_id": "torchbox__wagtail-storages-50", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: an `ImportError` when using `django-storages` with Django 5.1 due to the removal or relocation of `get_storage_class` from `django.core.files.storage`. The error traceback provides context about where the issue occurs in the codebase, and the goal of resolving this compatibility issue is implied. However, the statement lacks explicit details about the expected solution or desired behavior (e.g., whether the fix should maintain the same functionality or adapt to a new Django API). Additionally, there are no mentions of edge cases, constraints, or specific requirements for compatibility with other versions of Django or related libraries. While the issue is valid and the context is provided, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, primarily involving a single file (`wagtail_storages/utils.py`) where `get_storage_class` is replaced with `default_storage.__class__`. Additionally, there are updates to the `tox.ini` configuration file to include testing for Django 5.1 and newer Wagtail versions. These changes do not impact the broader system architecture and are limited in scope, requiring minimal understanding of interactions between different parts of the codebase.\n\n2. **Number of Technical Concepts**: Solving this issue requires basic knowledge of Django's storage system and familiarity with how Django handles backward compatibility or API changes across versions. The concept of replacing a deprecated utility function with a direct reference to the storage class is straightforward for someone with moderate experience in Django. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond understanding Python imports and Django's storage backend.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce new error handling logic. However, there is a minor risk of compatibility issues with other storage backends or Django configurations when directly accessing `default_storage.__class__` instead of using a utility function. This risk is not significant enough to elevate the difficulty, as it can be mitigated with basic testing.\n\n4. **Overall Complexity**: The fix involves a simple substitution in the code and configuration updates to support testing with the new Django version. It requires understanding the context of the error (a breaking change in Django 5.1) and applying a straightforward solution. This task is suitable for a developer with intermediate experience in Django and does not demand deep architectural changes or advanced technical expertise.\n\nThus, a difficulty score of 0.30 reflects the simplicity of the fix, the limited scope of changes, and the minimal complexity of the concepts involved, while acknowledging the need for some familiarity with Django's storage system and version compatibility.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "BDay typing\nI was using pylance to check types and the `date + BDay` stubs seems to be incorrect\r\nAll 3 (`__rsub__`, `__add__`, `__radd__`) should return `pd.Timestamp`, but instead they are inferred as `datetime.date`:\r\n```python\r\ndatetime.date(2023, 1, 1) + BDay(2)\r\ndatetime.date(2023, 1, 1) - BDay(2)\r\nBDay(2) + datetime.date(2023, 1, 1)\r\n```\r\n\r\nhttps://github.com/microsoft/pylance-release/issues/4631\n", "patch": "diff --git a/pandas-stubs/_libs/tslibs/offsets.pyi b/pandas-stubs/_libs/tslibs/offsets.pyi\nindex 1acdadf7..4b6a6123 100644\n--- a/pandas-stubs/_libs/tslibs/offsets.pyi\n+++ b/pandas-stubs/_libs/tslibs/offsets.pyi\n@@ -14,6 +14,7 @@ from typing import (\n \n from dateutil.relativedelta import weekday as WeekdayClass\n import numpy as np\n+from pandas import Timestamp\n from pandas.core.indexes.datetimes import DatetimeIndex\n from typing_extensions import Self\n \n@@ -22,7 +23,7 @@ from pandas._typing import npt\n \n from pandas.tseries.holiday import AbstractHolidayCalendar\n \n-_DatetimeT = TypeVar(\"_DatetimeT\", bound=date)\n+_DatetimeT = TypeVar(\"_DatetimeT\", bound=datetime)\n _TimedeltaT = TypeVar(\"_TimedeltaT\", bound=timedelta)\n \n prefix_mapping: dict[str, type]\n@@ -42,26 +43,32 @@ class BaseOffset:\n     @overload\n     def __add__(self, other: npt.NDArray[np.object_]) -> npt.NDArray[np.object_]: ...\n     @overload\n-    def __add__(self, other: BaseOffset) -> Self: ...\n+    def __add__(self, other: _DatetimeT) -> _DatetimeT: ...  # type: ignore[overload-overlap]  # pyright: ignore[reportOverlappingOverload]\n+    @overload\n+    def __add__(self, other: date) -> Timestamp: ...\n     @overload\n-    def __add__(self, other: _DatetimeT) -> _DatetimeT: ...\n+    def __add__(self, other: BaseOffset) -> Self: ...\n     @overload\n     def __add__(self, other: _TimedeltaT) -> _TimedeltaT: ...\n     @overload\n     def __radd__(self, other: npt.NDArray[np.object_]) -> npt.NDArray[np.object_]: ...\n     @overload\n-    def __radd__(self, other: BaseOffset) -> Self: ...\n+    def __radd__(self, other: _DatetimeT) -> _DatetimeT: ...  # type: ignore[overload-overlap]  # pyright: ignore[reportOverlappingOverload]\n     @overload\n-    def __radd__(self, other: _DatetimeT) -> _DatetimeT: ...\n+    def __radd__(self, other: date) -> Timestamp: ...\n+    @overload\n+    def __radd__(self, other: BaseOffset) -> Self: ...\n     @overload\n     def __radd__(self, other: _TimedeltaT) -> _TimedeltaT: ...\n     def __sub__(self, other: BaseOffset) -> Self: ...\n     @overload\n     def __rsub__(self, other: npt.NDArray[np.object_]) -> npt.NDArray[np.object_]: ...\n     @overload\n-    def __rsub__(self, other: BaseOffset) -> Self: ...\n+    def __rsub__(self, other: _DatetimeT) -> _DatetimeT: ...  # type: ignore[overload-overlap]  # pyright: ignore[reportOverlappingOverload]\n     @overload\n-    def __rsub__(self, other: _DatetimeT) -> _DatetimeT: ...\n+    def __rsub__(self, other: date) -> Timestamp: ...\n+    @overload\n+    def __rsub__(self, other: BaseOffset) -> Self: ...\n     @overload\n     def __rsub__(self, other: _TimedeltaT) -> _TimedeltaT: ...\n     def __call__(self, other): ...\n", "instance_id": "pandas-dev__pandas-stubs-1003", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the type inference for operations involving `BDay` (a business day offset in pandas) and `datetime.date` incorrectly returns `datetime.date` instead of `pd.Timestamp`. It provides specific examples of the operations (`__add__`, `__radd__`, `__rsub__`) that are affected and references a related GitHub issue for context. However, the statement lacks explicit mention of edge cases, constraints, or potential side effects of changing the type annotations. Additionally, it does not specify whether this change could impact other parts of the library or if there are specific compatibility concerns with other tools or versions of pandas/pylance. These minor ambiguities prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to a single file (`pandas-stubs/_libs/tslibs/offsets.pyi`), specifically modifying type annotations for the `BaseOffset` class. The diff shows a moderate amount of code change, primarily adjusting overloads for `__add__`, `__radd__`, and `__rsub__` methods to return `pd.Timestamp` when interacting with `date` objects. There is no indication of architectural impact or changes across multiple modules, making the scope limited and manageable.\n\n2. **Number of Technical Concepts:** Solving this requires understanding Python type hints, specifically the use of overloads with `typing.overload`, generic type variables (`TypeVar`), and the pandas library's type stub structure. Familiarity with `datetime.date`, `pd.Timestamp`, and how pandas handles date offsets (`BDay`) is necessary. These concepts are not overly complex for someone with moderate Python experience, especially in the context of type annotations and library maintenance.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases or error conditions, and the code changes do not introduce new error handling logic. However, the developer must consider whether returning `pd.Timestamp` instead of `datetime.date` could break existing code or type checkers in specific scenarios (e.g., downstream dependencies or interactions with other types). This adds a slight layer of complexity, but it remains minimal as the change is purely in type annotations and not runtime behavior.\n\n4. **Overall Assessment:** The task involves straightforward modifications to type stubs, requiring a basic-to-intermediate understanding of Python typing and pandas internals. It does not demand deep architectural changes, complex algorithms, or extensive domain knowledge beyond the pandas library. The primary challenge lies in ensuring the type annotations are correct and consistent with pandas' behavior, which is a relatively simple bug fix in the context of type inference. Therefore, a difficulty score of 0.30 is appropriate, reflecting an \"Easy\" problem that requires understanding some code logic and making targeted modifications.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "pandas.DataFrame.from_dict : issue with dtype and columns arguments types\n**Describe the bug**\r\nissue with types for `pandas.DataFrame.from_dict` method:\r\n\r\n- `dtype` is restricted to str in the stub file when pands allows for `str`, `type` and `None`\r\n- `columns` is restricted to list of strings when pandas allows for much more (at least: str, int, float, datetime)\r\n\r\n**To Reproduce**\r\n1. minimal exemple:\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndata = {'l1': [1, 2, 3],\r\n        'l2': [4, 5, 6]}\r\n\r\n# all of the following are accepted by pandas\r\n# however mypy complains with the last 3\r\npd.DataFrame.from_dict(data, orient='index')  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype='float')  # OK\r\npd.DataFrame.from_dict(data, orient='index', columns=['a', 'b', 'c'])  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype=float)  # NOK\r\npd.DataFrame.from_dict(data, orient='index', dtype=None)  # NOK\r\npd.DataFrame.from_dict(data, orient='index',\r\n                       columns=[1.0, 2, datetime.datetime.now()])  # NOK\r\n```\r\n\r\n2. type checker :  `mypy`\r\n3. typing errors: \r\n\r\n```\r\nfrom_dict_test.py:12: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"type[float]\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:13: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"None\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:15: error: List item 0 has incompatible type \"float\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 1 has incompatible type \"int\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 2 has incompatible type \"datetime\"; expected \"str\"  [list-item]\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Windows\r\n - Windows 10 22H2   build: 19045.4170\r\n - Python 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)] on win32\r\n - mypy                          1.9.0\r\n - pandas-stubs                  2.2.0.240218\r\n\npandas.DataFrame.from_dict : issue with dtype and columns arguments types\n**Describe the bug**\r\nissue with types for `pandas.DataFrame.from_dict` method:\r\n\r\n- `dtype` is restricted to str in the stub file when pands allows for `str`, `type` and `None`\r\n- `columns` is restricted to list of strings when pandas allows for much more (at least: str, int, float, datetime)\r\n\r\n**To Reproduce**\r\n1. minimal exemple:\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndata = {'l1': [1, 2, 3],\r\n        'l2': [4, 5, 6]}\r\n\r\n# all of the following are accepted by pandas\r\n# however mypy complains with the last 3\r\npd.DataFrame.from_dict(data, orient='index')  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype='float')  # OK\r\npd.DataFrame.from_dict(data, orient='index', columns=['a', 'b', 'c'])  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype=float)  # NOK\r\npd.DataFrame.from_dict(data, orient='index', dtype=None)  # NOK\r\npd.DataFrame.from_dict(data, orient='index',\r\n                       columns=[1.0, 2, datetime.datetime.now()])  # NOK\r\n```\r\n\r\n2. type checker :  `mypy`\r\n3. typing errors: \r\n\r\n```\r\nfrom_dict_test.py:12: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"type[float]\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:13: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"None\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:15: error: List item 0 has incompatible type \"float\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 1 has incompatible type \"int\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 2 has incompatible type \"datetime\"; expected \"str\"  [list-item]\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Windows\r\n - Windows 10 22H2   build: 19045.4170\r\n - Python 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)] on win32\r\n - mypy                          1.9.0\r\n - pandas-stubs                  2.2.0.240218\r\n\npandas.DataFrame.from_dict : issue with dtype and columns arguments types\n**Describe the bug**\r\nissue with types for `pandas.DataFrame.from_dict` method:\r\n\r\n- `dtype` is restricted to str in the stub file when pands allows for `str`, `type` and `None`\r\n- `columns` is restricted to list of strings when pandas allows for much more (at least: str, int, float, datetime)\r\n\r\n**To Reproduce**\r\n1. minimal exemple:\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndata = {'l1': [1, 2, 3],\r\n        'l2': [4, 5, 6]}\r\n\r\n# all of the following are accepted by pandas\r\n# however mypy complains with the last 3\r\npd.DataFrame.from_dict(data, orient='index')  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype='float')  # OK\r\npd.DataFrame.from_dict(data, orient='index', columns=['a', 'b', 'c'])  # OK\r\npd.DataFrame.from_dict(data, orient='index', dtype=float)  # NOK\r\npd.DataFrame.from_dict(data, orient='index', dtype=None)  # NOK\r\npd.DataFrame.from_dict(data, orient='index',\r\n                       columns=[1.0, 2, datetime.datetime.now()])  # NOK\r\n```\r\n\r\n2. type checker :  `mypy`\r\n3. typing errors: \r\n\r\n```\r\nfrom_dict_test.py:12: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"type[float]\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:13: error: Argument \"dtype\" to \"from_dict\" of \"DataFrame\" has incompatible type \"None\"; expected \"str\"  [arg-type]\r\nfrom_dict_test.py:15: error: List item 0 has incompatible type \"float\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 1 has incompatible type \"int\"; expected \"str\"  [list-item]\r\nfrom_dict_test.py:15: error: List item 2 has incompatible type \"datetime\"; expected \"str\"  [list-item]\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Windows\r\n - Windows 10 22H2   build: 19045.4170\r\n - Python 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)] on win32\r\n - mypy                          1.9.0\r\n - pandas-stubs                  2.2.0.240218\r\n\n", "patch": "diff --git a/pandas-stubs/_libs/tslibs/timestamps.pyi b/pandas-stubs/_libs/tslibs/timestamps.pyi\nindex af856ae6..9df14b3d 100644\n--- a/pandas-stubs/_libs/tslibs/timestamps.pyi\n+++ b/pandas-stubs/_libs/tslibs/timestamps.pyi\n@@ -219,7 +219,7 @@ class Timestamp(datetime):\n     @overload\n     def __sub__(self, other: TimedeltaSeries) -> TimestampSeries: ...\n     @overload\n-    def __sub__(  # pyright: ignore[reportIncompatibleMethodOverride]\n+    def __sub__(\n         self, other: npt.NDArray[np.timedelta64]\n     ) -> npt.NDArray[np.datetime64]: ...\n     @overload\ndiff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 7ce2f5d6..44b3a080 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -272,13 +272,22 @@ class DataFrame(NDFrame, OpsMixin):\n     def dot(self, other: Series) -> Series: ...\n     def __matmul__(self, other): ...\n     def __rmatmul__(self, other): ...\n+    @overload\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        data: dict[Any, Any],\n+        orient: Literal[\"index\"],\n+        dtype: AstypeArg | None = ...,\n+        columns: Axes | None = ...,\n+    ) -> DataFrame: ...\n+    @overload\n     @classmethod\n     def from_dict(\n         cls,\n         data: dict[Any, Any],\n-        orient: Literal[\"columns\", \"index\", \"tight\"] = ...,\n-        dtype: _str = ...,\n-        columns: list[_str] = ...,\n+        orient: Literal[\"columns\", \"tight\"] = ...,\n+        dtype: AstypeArg | None = ...,\n     ) -> DataFrame: ...\n     def to_numpy(\n         self,\n", "instance_id": "pandas-dev__pandas-stubs-897", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the type hints for the `pandas.DataFrame.from_dict` method, specifically regarding the `dtype` and `columns` arguments. The goal is explicit: update the type annotations in the stub file to match the actual behavior of pandas. The input and output expectations are indirectly clear through the provided reproduction code and the type checker errors. The statement includes a minimal reproducible example, detailed error messages from `mypy`, and specific version information for the environment, which aids in understanding the context. There are no significant ambiguities, and the problem is supported by concrete examples of what works and what doesn't. All necessary details for understanding and addressing the issue are provided, including the expected behavior of pandas versus the current type restrictions in the stub file.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to a single file (`pandas-stubs/core/frame.pyi`) and specifically to the type annotations of the `from_dict` method. The diff shows a small, focused modification involving the addition of an overload for the `orient='index'` case and updating the type hints for `dtype` and `columns`. There is no impact on the broader system architecture or interactions with other modules, as this is purely a type annotation fix in a stub file for static type checking.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of Python type annotations, specifically the use of `typing` module features like `Literal`, `overload`, and union types (via `AstypeArg` and `Axes`). Familiarity with `mypy` and how type stubs work in the context of third-party libraries like `pandas-stubs` is necessary but not overly complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond Python typing are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and code changes do not introduce or require handling of complex edge cases or error conditions. The focus is on aligning type annotations with the runtime behavior of pandas, which is straightforward. The provided examples cover the necessary variations (different types for `dtype` and `columns`), and no additional error handling logic is needed in the stub file.\n\n4. **Overall Complexity**: The task involves minimal code changes (a few lines in a type stub file) and does not require deep understanding of the pandas library's internals, only its public API behavior as documented and demonstrated in the reproduction code. The primary challenge is ensuring the type annotations are correct and comprehensive, which is a relatively simple task for someone familiar with Python typing.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only a basic understanding of type annotations and a small, targeted code modification. It is slightly above the \"Very Easy\" range due to the need to understand type overloads and the specific conventions used in `pandas-stubs`, but it remains an easy problem overall.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Handle # character within string correctly\nCurrently, Regorus treats the # character\r\n- as a regular character within raw strings\r\n- as a comment character within regular strings\r\n\r\nThis behavior is incorrect and does not match OPA which treats the # character as a regular character within both types of strings.\n", "patch": "diff --git a/src/lexer.rs b/src/lexer.rs\nindex f4ae34c7..a7b02c53 100644\n--- a/src/lexer.rs\n+++ b/src/lexer.rs\n@@ -455,7 +455,7 @@ impl<'source> Lexer<'source> {\n             let (offset, ch) = self.peek();\n             let col = self.col + (offset - start) as u16;\n             match ch {\n-                '\"' | '#' | '\\x00' => {\n+                '\"' | '\\x00' => {\n                     break;\n                 }\n                 '\\\\' => {\n", "instance_id": "microsoft__regorus-216", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the current behavior of the Regorus system in handling the '#' character within strings does not align with the expected behavior as per OPA (Open Policy Agent). It specifies that '#' should be treated as a regular character in both raw and regular strings, which contrasts with the current implementation. However, the statement lacks details about specific test cases, examples of input/output, or edge cases that might arise due to this change. Additionally, there is no mention of the broader context or potential side effects of altering the lexer's behavior. While the goal is understandable, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to the lexer logic in a single file (lexer.rs). The code change is minimal, consisting of removing '#' from the list of characters that terminate a string parsing loop in the lexer. This requires only basic understanding of Rust syntax and lexer implementation at a surface level, without needing to dive into complex interactions across the codebase or advanced language features. There are no significant edge cases or error handling requirements explicitly mentioned or implied by the change, as it simply adjusts the character recognition logic. The impact on the system's architecture is negligible, and the scope is confined to a small, isolated part of the code. Therefore, I rate this as very easy, with a difficulty score of 0.15, reflecting a simple bug fix with minimal cognitive load.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve `ASSERT` Macro to Handle Operator Precedence More Intuitively\n# Description\r\nThe current implementation of the `ASSERT` macro requires extra parentheses to handle operator precedence correctly, such as:\r\n``` system verilog\r\n`ASSERT((a===b));  // Correct\r\n```\r\nWithout these parentheses, expressions may lead to incorrect logic due to operator precedence rules.\r\n\r\nFor example, a developer might mistakenly write:\r\n``` system verilog\r\n`ASSERT(a === b);  // Wrong\r\n```\r\nThis could be misinterpreted by the compiler as:\r\n``` system verilog\r\nif (!a === b)  // Wrong behavior\r\n```\r\nInstead of the intended:\r\n``` system verilog\r\nif (!(a === b))  // Correct logic when using parentheses.\r\n```\r\n\r\n\r\n# Proposed Solution:\r\n\r\nhttps://github.com/dpretet/svut/blob/d2b4d6229049574b8d420164edb757bb41b554e4/svut/svut_h.sv#L159 \r\nUse a pair of parentheses to wrap `exp` such as:\r\n``` system verilog\r\nif (!(exp)) begin\r\n```\r\n\r\n# Benefits:\r\n\r\n1. Prevents potential logical errors due to misunderstood operator precedence.\r\n1. Improves developer productivity by making the ASSERT macro behavior more intuitive.\r\n1. Reduces debugging time caused by precedence-related bugs.\r\n\r\nThanks for your work on this framework, it definitely helps me a lot.\r\n\r\nBest Regards\n", "patch": "diff --git a/svut/svut_h.sv b/svut/svut_h.sv\nindex edef5e5..ec02883 100644\n--- a/svut/svut_h.sv\n+++ b/svut/svut_h.sv\n@@ -129,7 +129,7 @@ endfunction\n `define FAIL_IF_NOT(exp, message=\"\") \\\n     svut_status = 0; \\\n     svut_msg = create_msg(\"FAIL_IF_NOT\", message); \\\n-    if (!exp) begin \\\n+    if (!(exp)) begin \\\n         `ERROR(svut_msg); \\\n         svut_status = 1; \\\n     end\n@@ -156,7 +156,7 @@ endfunction\n `define ASSERT(exp, message=\"\") \\\n     svut_status = 0; \\\n     svut_msg = create_msg(\"ASSERT\", message); \\\n-    if (!exp) begin \\\n+    if (!(exp)) begin \\\n         `ERROR(svut_msg); \\\n         svut_status = 1; \\\n     end\ndiff --git a/svut/template.sv b/svut/template.sv\nindex cc4a961..c17e907 100644\n--- a/svut/template.sv\n+++ b/svut/template.sv\n@@ -51,7 +51,7 @@ ${module_inst}\n     //    - `FAIL_IF_EQUAL(aSignal, 23):       Increment error counter if evaluation is equal\n     //    - `FAIL_IF_NOT_EQUAL(aSignal, 45):   Increment error counter if evaluation is not equal\n     //    - `ASSERT(aSignal):                  Increment error counter if evaluation is not true\n-    //    - `ASSERT((aSignal == 0)):           Increment error counter if evaluation is not true\n+    //    - `ASSERT(aSignal == 0):           Increment error counter if evaluation is not true\n     //\n     //  Available flag:\n     //\n", "instance_id": "dpretet__svut-30", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly describes the issue with the current implementation of the `ASSERT` macro in SystemVerilog, specifically regarding operator precedence and the need for extra parentheses to ensure correct logical behavior. The goal is explicitly stated: to make the macro more intuitive by wrapping the expression in parentheses to avoid precedence-related errors. The statement includes examples of correct and incorrect usage, as well as the intended logical behavior, which helps in understanding the problem. Additionally, the proposed solution is linked to a specific line in the codebase, and the benefits of the change are outlined. There are no significant ambiguities or missing critical details in the description, making it a clear and detailed problem statement.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is straightforward and involves a minor modification to the `ASSERT` and `FAIL_IF_NOT` macros in a single file (`svut_h.sv`) by adding parentheses around the expression in the `if` condition to handle operator precedence correctly. The scope of the code change is minimal, affecting only two lines of macro definitions and a minor update to a comment in a template file (`template.sv`). It does not require understanding complex interactions across the codebase or impact the system's architecture. The technical concepts involved are basic\u2014understanding macro definitions and operator precedence in SystemVerilog, which are fundamental for anyone working with this language. There are no significant edge cases or error handling requirements mentioned or needed beyond the simple fix. This task is essentially a small bug fix that requires minimal effort and expertise, suitable for a junior developer or someone with basic familiarity with SystemVerilog.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Suggest that statement functions be made into internal functions\nPeriodically on Fortran Discourse, most recently at https://fortran-lang.discourse.group/t/anonymous-functions/8903, someone new to Fortran asks what some code with statement functions is doing. They are now obsolescent and should be warned about, as in the code\r\n\r\n```Fortran\r\nprogram mwe1\r\n    implicit none\r\n    double precision :: f,x\r\n    f(x)= x**2 + 1.0d0\r\n    print *, f(2.0d0)\r\nend program mwe1\r\n```\r\n\r\nfor which fortitude says only\r\n\r\n```\r\nxstatement_func.f90:3:5: P011 prefer 'real(real64)' to 'double precision' (see 'iso_fortran_env')\r\n  |\r\n1 | program mwe1\r\n2 |     implicit none\r\n3 |     double precision :: f,x\r\n  |     ^^^^^^^^^^^^^^^^ P011\r\n4 |     f(x)= x**2 + 1.0d0\r\n5 |     print *, f(2.0d0)\r\n  |\r\n\r\n\r\nfortitude: 1 files scanned.\r\nNumber of errors: 1\r\n```\n", "patch": "diff --git a/fortitude/src/registry.rs b/fortitude/src/registry.rs\nindex 947bce5..9b8aa00 100644\n--- a/fortitude/src/registry.rs\n+++ b/fortitude/src/registry.rs\n@@ -65,6 +65,9 @@ pub enum Category {\n     /// Check path names, directory structures, etc.\n     #[prefix = \"F\"]\n     Filesystem,\n+    /// Obsolescent features\n+    #[prefix = \"OB\"]\n+    Obsolescent,\n }\n \n pub trait RuleNamespace: Sized {\ndiff --git a/fortitude/src/rules/mod.rs b/fortitude/src/rules/mod.rs\nindex 5116d01..8696dcc 100644\n--- a/fortitude/src/rules/mod.rs\n+++ b/fortitude/src/rules/mod.rs\n@@ -5,6 +5,7 @@ mod filesystem;\n #[macro_use]\n mod macros;\n mod modules;\n+mod obsolescent;\n mod precision;\n mod style;\n mod testing;\n@@ -93,6 +94,8 @@ pub fn code_to_rule(category: Category, code: &str) -> Option<(RuleGroup, Rule)>\n         (Typing, \"043\") => (RuleGroup::Stable, Ast, typing::assumed_size::DeprecatedAssumedSizeCharacter),\n         (Typing, \"051\") => (RuleGroup::Stable, Ast, typing::init_decls::InitialisationInDeclaration),\n \n+        (Obsolescent, \"001\") => (RuleGroup::Stable, Ast, obsolescent::statement_functions::StatementFunction),\n+\n         (Precision, \"001\") => (RuleGroup::Stable, Ast, precision::kind_suffixes::NoRealSuffix),\n         (Precision, \"011\") => (RuleGroup::Stable, Ast, precision::double_precision::DoublePrecision),\n         (Precision, \"021\") => (RuleGroup::Stable, Ast, precision::implicit_kinds::ImplicitRealKind),\ndiff --git a/fortitude/src/rules/obsolescent/mod.rs b/fortitude/src/rules/obsolescent/mod.rs\nnew file mode 100644\nindex 0000000..f8d7860\n--- /dev/null\n+++ b/fortitude/src/rules/obsolescent/mod.rs\n@@ -0,0 +1,27 @@\n+pub mod statement_functions;\n+\n+#[cfg(test)]\n+mod tests {\n+    use std::convert::AsRef;\n+    use std::path::Path;\n+\n+    use anyhow::Result;\n+    use insta::assert_snapshot;\n+    use test_case::test_case;\n+\n+    use crate::registry::Rule;\n+    use crate::settings::Settings;\n+    use crate::test::test_path;\n+\n+    #[test_case(Rule::StatementFunction, Path::new(\"OB001.f90\"))]\n+    fn rules(rule_code: Rule, path: &Path) -> Result<()> {\n+        let snapshot = format!(\"{}_{}\", rule_code.as_ref(), path.to_string_lossy());\n+        let diagnostics = test_path(\n+            Path::new(\"obsolescent\").join(path).as_path(),\n+            &[rule_code],\n+            &Settings::default(),\n+        )?;\n+        assert_snapshot!(snapshot, diagnostics);\n+        Ok(())\n+    }\n+}\ndiff --git a/fortitude/src/rules/obsolescent/statement_functions.rs b/fortitude/src/rules/obsolescent/statement_functions.rs\nnew file mode 100644\nindex 0000000..5a190b1\n--- /dev/null\n+++ b/fortitude/src/rules/obsolescent/statement_functions.rs\n@@ -0,0 +1,52 @@\n+use crate::settings::Settings;\n+use crate::{AstRule, FromAstNode};\n+use ruff_diagnostics::{Diagnostic, Violation};\n+use ruff_macros::{derive_message_formats, violation};\n+use ruff_source_file::SourceFile;\n+use tree_sitter::Node;\n+\n+/// ## What it does\n+/// Checks for statement functions.\n+///\n+/// ## Why is this bad?\n+/// Statement functions are an obsolescent feature from Fortran 77,\n+/// and have been entirely supplanted by internal\n+/// procedures. Statement functions are much more limited in what they\n+/// can do. They were declared obsolescent in Fortran 90 and removed\n+/// in Fortran 95.\n+///\n+/// ## Examples\n+/// Statement functions are easily replaced with internal procedures:\n+///\n+/// ```f90\n+/// real :: f, x\n+/// f(x) = x**2 + x\n+/// ```\n+/// becomes:\n+///\n+/// ```f90\n+/// contains\n+///   real function f(x)\n+///     real, intent(in) :: x\n+///     f = x**2 + x\n+///   end function f\n+/// ```\n+#[violation]\n+pub struct StatementFunction {}\n+\n+impl Violation for StatementFunction {\n+    #[derive_message_formats]\n+    fn message(&self) -> String {\n+        format!(\"statement functions are obsolescent, prefer internal functions\")\n+    }\n+}\n+\n+impl AstRule for StatementFunction {\n+    fn check(_settings: &Settings, node: &Node, _src: &SourceFile) -> Option<Vec<Diagnostic>> {\n+        some_vec![Diagnostic::from_node(StatementFunction {}, node)]\n+    }\n+\n+    fn entrypoints() -> Vec<&'static str> {\n+        vec![\"statement_function\"]\n+    }\n+}\n", "instance_id": "PlasmaFAIR__fortitude-162", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to address the obsolescence of statement functions in Fortran and to implement a warning or diagnostic for their usage in a linter-like tool called \"fortitude.\" It provides a specific example of Fortran code that should trigger a warning and references a discussion forum for context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior of the tool beyond issuing a warning (e.g., should it suggest a replacement code snippet in the output?). Additionally, constraints or specific requirements for how the warning should be formatted or integrated into the existing tool's output are not detailed. While the goal is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files in the \"fortitude\" codebase, including adding a new category for obsolescent features in the registry, creating a new module for obsolescent rules, and implementing a specific rule for statement functions. This requires understanding the structure of the linter tool, particularly how rules are defined and integrated using Rust and the `tree_sitter` library for parsing. Second, the technical concepts involved include Rust's macro system (for diagnostics), AST traversal using `tree_sitter`, and the domain-specific knowledge of Fortran's language features (specifically, statement functions and their obsolescence). While these concepts are not overly complex for an experienced Rust developer, they do require familiarity with the linter's architecture and some language-specific knowledge. Third, the problem does not appear to involve significant edge cases or complex error handling beyond identifying the presence of statement functions in the code, as the implementation simply flags the node without additional logic. Finally, the changes do not impact the broader system architecture but are localized to extending the rule system. Overall, this task requires moderate effort and understanding of multiple components, justifying a score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "yogo needs python >= 3.9, not >= 3.8\nGet this with python 3.8:\r\n\r\n> (yogo_env) paul.lebel@MacBook-Pro yogo % yogo infer --help\r\n> Traceback (most recent call last):\r\n>   File \"/Users/paul.lebel/Documents/python_envs/yogo_env/bin/yogo\", line 8, in <module>\r\n>     sys.exit(main())\r\n>   File \"/Users/paul.lebel/Documents/GitHub/yogo/yogo/__main__.py\", line 19, in main\r\n>     p = global_parser()\r\n>   File \"/Users/paul.lebel/Documents/GitHub/yogo/yogo/utils/argparsers.py\", line 84, in global_parser\r\n>     test_parser(\r\n>   File \"/Users/paul.lebel/Documents/GitHub/yogo/yogo/utils/argparsers.py\", line 263, in test_parser\r\n>     action=argparse.BooleanOptionalAction,\r\n> AttributeError: module 'argparse' has no attribute 'BooleanOptionalAction'\r\n\r\nhttps://nono.ma/attribute-error-module-argparse-has-no-attribute-boolean-optional-action\r\n\n", "patch": "diff --git a/.github/workflows/black.yml b/.github/workflows/black.yml\nindex aee8075c..27cbf4dd 100644\n--- a/.github/workflows/black.yml\n+++ b/.github/workflows/black.yml\n@@ -9,7 +9,7 @@ jobs:\n       - uses: actions/checkout@v4\n       - uses: actions/setup-python@v5\n         with:\n-          python-version: '3.8'\n+          python-version: '3.10'\n           architecture: x64\n       - uses: psf/black@stable\n         with:\ndiff --git a/.github/workflows/type-checking.yml b/.github/workflows/type-checking.yml\nindex 69befca4..f0a1db4e 100644\n--- a/.github/workflows/type-checking.yml\n+++ b/.github/workflows/type-checking.yml\n@@ -9,7 +9,7 @@ jobs:\n       - uses: actions/checkout@v4\n       - uses: actions/setup-python@v5\n         with:\n-          python-version: '3.8'\n+          python-version: '3.10'\n       - name: install checking requirements\n         run: |\n           pip install --upgrade pip\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 76906c53..f6890250 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -8,7 +8,7 @@ keywords = [\"object detection\", \"YOGO\", \"YOLO\", \"deep learning\", \"PyTorch\"]\n license = \"BSD-3-Clause\"\n \n [tool.poetry.dependencies]\n-python = \">=3.8,<3.11\"\n+python = \">=3.9,<3.11\"\n zarr = \">=2.14\"\n torch = \">=1.13.1\"\n torchmetrics = { version = \">=0.11.4\", extras = [\"detection\"] }\n", "instance_id": "czbiohub-sf__yogo-131", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `yogo` tool does not work with Python 3.8 due to the missing `BooleanOptionalAction` attribute in the `argparse` module, which is only available in Python 3.9 and above. The goal of updating the Python version requirement to >=3.9 is implied through the error message and the provided code changes. However, the statement lacks explicit mention of the desired solution or detailed requirements (e.g., whether compatibility with older versions needs to be considered or if there are specific reasons for choosing Python 3.10 in the CI workflows). Additionally, edge cases or potential downstream impacts of this change are not discussed. Despite these minor ambiguities, the intent and issue are understandable with the provided context and error traceback.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is straightforward: the codebase needs to update its Python version requirement from >=3.8 to >=3.9 due to a missing feature in the `argparse` module. The code changes are minimal and involve updating version strings in configuration files (`pyproject.toml` and GitHub workflow YAML files). These changes are isolated to build and dependency configurations and do not require deep understanding of the codebase, complex logic, or architectural modifications. No advanced technical concepts, algorithms, or domain-specific knowledge are needed beyond basic familiarity with Python versioning and dependency management. There are no significant edge cases or error handling requirements mentioned or implied in the problem or changes. The scope of the change is small, affecting only a few lines across three files, with no impact on the core functionality or system architecture. This makes the task very easy, suitable for a beginner or junior developer.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Allow instances of generic data tests to be documented\n### Describe the feature\r\nCurrently, you can document pretty much everything _but_ a [data test](https://docs.getdbt.com/docs/build/data-tests#overview) - if you try, it fails on a parsing error. This ticket is for [generic data tests](https://docs.getdbt.com/docs/build/data-tests#generic-data-tests). A separate ticket addresses [singular data tests](https://docs.getdbt.com/docs/build/data-tests#singular-data-tests): https://github.com/dbt-labs/dbt-core/issues/9005\r\n\r\n### Acceptance criteria\r\n- You can add a description to a generic test\r\n- You can use a docs block to capture your test description\r\n- That description shows up in the docs site\r\n\r\n### Examples\r\n```\r\nversion: 2\r\n\r\nmodels:\r\n  - name: orders\r\n    columns:\r\n      - name: order_id\r\n        tests:\r\n          - unique\r\n            description: The order_id is unique for every row in the orders model\r\n          - not_null\r\n            description: '{{ doc(\"not_null_test\") }}'\r\n```\r\n\r\n### Concerns\r\n\r\n- adding documentation to tests will slow down parsing (full parse), particularly for really common tests like unique and not_null\r\n- We have special processing to speed up unique and not_null test parsing. Would that have to change?\r\n- Ensure the solution is performant\nAllow instances of generic data tests to be documented\n### Describe the feature\r\nCurrently, you can document pretty much everything _but_ a [data test](https://docs.getdbt.com/docs/build/data-tests#overview) - if you try, it fails on a parsing error. This ticket is for [generic data tests](https://docs.getdbt.com/docs/build/data-tests#generic-data-tests). A separate ticket addresses [singular data tests](https://docs.getdbt.com/docs/build/data-tests#singular-data-tests): https://github.com/dbt-labs/dbt-core/issues/9005\r\n\r\n### Acceptance criteria\r\n- You can add a description to a generic test\r\n- You can use a docs block to capture your test description\r\n- That description shows up in the docs site\r\n\r\n### Examples\r\n```\r\nversion: 2\r\n\r\nmodels:\r\n  - name: orders\r\n    columns:\r\n      - name: order_id\r\n        tests:\r\n          - unique\r\n            description: The order_id is unique for every row in the orders model\r\n          - not_null\r\n            description: '{{ doc(\"not_null_test\") }}'\r\n```\r\n\r\n### Concerns\r\n\r\n- adding documentation to tests will slow down parsing (full parse), particularly for really common tests like unique and not_null\r\n- We have special processing to speed up unique and not_null test parsing. Would that have to change?\r\n- Ensure the solution is performant\n", "patch": "diff --git a/.changes/unreleased/Fixes-20241014-212135.yaml b/.changes/unreleased/Fixes-20241014-212135.yaml\nnew file mode 100644\nindex 00000000000..8cc700095ba\n--- /dev/null\n+++ b/.changes/unreleased/Fixes-20241014-212135.yaml\n@@ -0,0 +1,6 @@\n+kind: Fixes\n+body: Allow instances of generic data tests to be documented\n+time: 2024-10-14T21:21:35.767115+01:00\n+custom:\n+    Author: aranke\n+    Issue: \"2578\"\n", "instance_id": "dbt-labs__dbt-core-10850", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear with a well-defined goal: to allow documentation for generic data tests in a data build tool (dbt). It includes acceptance criteria, examples of the desired functionality, and specific concerns about performance. The input format (YAML configuration with test descriptions) and expected output (descriptions appearing in the docs site) are implied through the examples. However, there are minor ambiguities, such as the lack of explicit mention of how the documentation should be parsed or rendered in the docs site, and no detailed discussion of edge cases (e.g., malformed descriptions or conflicts with existing parsing optimizations). Additionally, while performance concerns are raised, there are no specific benchmarks or thresholds provided to define \"performant.\" Overall, the statement provides a solid foundation but leaves some minor details to be inferred or clarified during implementation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, while the provided code changes are minimal (only a changelog entry), the actual implementation (not shown in the diff) likely involves significant modifications to the parsing logic of dbt-core, a complex data transformation tool. This would require a deep understanding of the existing codebase, particularly the parsing mechanisms for YAML configurations and the special optimizations for common tests like `unique` and `not_null`. Second, the scope of changes could impact multiple modules, including parsing, documentation rendering, and potentially the performance optimization layers, as highlighted in the concerns. Third, technical concepts involved include advanced parsing techniques, performance optimization, and possibly integration with documentation generation systems, which require a solid grasp of the domain (data testing in dbt) and the tool's architecture. Fourth, while edge cases are not explicitly mentioned, handling malformed or overly complex descriptions, ensuring backward compatibility, and maintaining performance for large-scale projects are implicit challenges that add to the complexity. Finally, the explicit concern about performance indicates a need for careful design to avoid degrading the user experience. Overall, this problem requires a deep understanding of the system and thoughtful implementation, justifying a difficulty score of 0.65.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "relatedItems field returns None in the serializer if the related item is deleted\n## Steps to reproduce\r\n\r\n1. Create a content 1\r\n2. Create a content 2\r\n3. Edit content 1 and fill in the `relatedItems` field with content 2\r\n4. Delete content 2\r\n5. See content 1 serialization\r\n6. `relatedItems` will be: `[ None ]`\r\n\r\nThis doesn't make sense to me. In the example, I think it's better to return an empty list. However, to do this, we would have to test the type of each element in a list [here](https://github.com/plone/plone.restapi/blob/f90329dfe49aa483517861a610b5175d68e0b6be/src/plone/restapi/serializer/converters.py#L100), to know if the value is a `RelationValue`. Or create a new adapter for the `IRelationList` field\r\n\r\nOr should you handle this in the frontend?\r\n\r\n\n", "patch": "diff --git a/news/1818.bugfix b/news/1818.bugfix\nnew file mode 100644\nindex 0000000000..54260238ec\n--- /dev/null\n+++ b/news/1818.bugfix\n@@ -0,0 +1,1 @@\n+Fix response of `RelationListFieldSerializer` by filtering out invalid items. @Faakhir30\ndiff --git a/src/plone/restapi/serializer/relationfield.py b/src/plone/restapi/serializer/relationfield.py\nindex 28483ce424..d54cadb9d1 100644\n--- a/src/plone/restapi/serializer/relationfield.py\n+++ b/src/plone/restapi/serializer/relationfield.py\n@@ -33,4 +33,16 @@ class RelationChoiceFieldSerializer(DefaultFieldSerializer):\n @adapter(IRelationList, IDexterityContent, Interface)\n @implementer(IFieldSerializer)\n class RelationListFieldSerializer(DefaultFieldSerializer):\n-    pass\n+    def get_value(self, default=[]):\n+        \"\"\"Return field value reduced to list of non-broken Relationvalues.\n+\n+        Args:\n+            default (list, optional): Default field value. Defaults to empty list.\n+\n+        Returns:\n+            list: List of RelationValues\n+        \"\"\"\n+        value = super().get_value()\n+        if not value:\n+            return []\n+        return [el for el in value if el.to_id]\n", "instance_id": "plone__plone.restapi-1818", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when a related item is deleted, the `relatedItems` field in the serializer returns `[None]` instead of an empty list, which the author finds undesirable. Steps to reproduce the issue are provided, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected output format beyond suggesting an empty list, nor does it clarify whether this behavior should be handled in the backend or frontend (as the author poses this as a question). Additionally, edge cases or constraints (e.g., performance implications of filtering, behavior with multiple related items) are not mentioned. While a link to the relevant code is provided, the statement could benefit from more explicit requirements or constraints to remove any doubt about the intended solution.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code change is localized to a single file (`relationfield.py`) and involves modifying a single class (`RelationListFieldSerializer`). The change is minimal, adding a filtering logic to exclude broken or invalid `RelationValue` objects. It does not impact the broader system architecture or require modifications across multiple modules. The diff shows a small, focused update of about 12 lines of code.\n\n2. **Number of Technical Concepts:** The solution requires understanding of Python, specifically class inheritance and list comprehension, as well as familiarity with the Plone framework's `IRelationList` and `RelationValue` concepts. These are relatively straightforward for someone with moderate experience in Python and the Plone ecosystem. No advanced algorithms, design patterns, or complex libraries are involved.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the primary issue of deleted related items. The code change handles the main case by filtering out invalid items (where `to_id` is presumably `None` or invalid), but it does not introduce complex error handling logic. Potential edge cases, such as performance with large lists of relations or behavior with partially broken relations, are not addressed in the problem or solution, but they do not seem critical for this fix.\n\n4. **Overall Complexity:** The logic to filter out invalid items is simple and does not require deep understanding of the codebase beyond the serializer's role. The change is a bug fix rather than a feature addition or architectural refactor, keeping the cognitive load low.\n\nGiven these factors, a difficulty score of 0.30 reflects an easy problem that requires understanding some specific logic in the Plone REST API serializer but involves a straightforward and contained modification. It is not entirely trivial (e.g., not just a typo fix), as it requires domain-specific knowledge of Plone's relation handling, but it remains accessible to developers with basic to intermediate skills in the relevant technologies.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deviations from Berkeley TestFloat\nSome deviations and bugs in floating-point arithmetic were found when testing against Berkeley TestFloat:\r\n* [x] (#408) As already known, the sign of zero is incorrect as it doesn't consider the quantization mode. \r\n* [x] (#407) The result sometimes becomes infinity instead of the maximum normal number for certain quantization modes. This makes sense when reading the definitions again, e.g. for TO_POS (from the 754 standard): \"the result shall be the format\u2019s floating-point number (possibly +\u221e) closest to and no less than the infinitely precise result\". The result should therefore never become negative infinity for TO_POS.\r\n* [x] (#428) f16_mul sometimes fails for directed quantization modes, e.g. the following for TO_POS:\r\n```Python\r\nAPyFloat(1, 1, 1023, 5, 10) * APyFloat(1, 26, 80, 5, 10)\r\n# Becomes APyFloat(sign=0, exp=13, man=79, exp_bits=5, man_bits=10)\r\n# and not APyFloat(sign=0, exp=13, man=80, exp_bits=5, man_bits=10)\r\n```\r\n* [x] (#411) f32_mul sometimes results in a subnormal number instead of zero, however this isn't seen for f64_mul or f16_mul):\r\n```Python\r\nAPyFloat(0, 53, 32895, 8, 23) * APyFloat(1, 1, 1999049, 8, 23)\r\n# Becomes APyFloat(sign=1, exp=0, man=10184, exp_bits=8, man_bits=23)\r\n# and not APyFloat(sign=1, exp=0, man=0, exp_bits=8, man_bits=23)\r\n```\r\n* [x] Added bullet point. The mantissa is sometimes wrong when casting from a larger format to a smaller, similar to the previous bullet. See comment below.\r\n\r\nThese were found from running\r\n```cmd\r\npython run_berkeley_cases.py -op all -qm all -s 1 -l 1\r\n```\r\nbut dedicated test cases should of course be made while fixing these.\n", "patch": "diff --git a/src/apyfloat.cc b/src/apyfloat.cc\nindex a179bcd83..59ff9fcf4 100644\n--- a/src/apyfloat.cc\n+++ b/src/apyfloat.cc\n@@ -224,8 +224,12 @@ APyFloat APyFloat::_checked_cast(\n         res.exp = 0;\n         // Cast mantissa\n         res.man = prev_man;\n-        std::uint8_t man_bits_delta = 1 - new_exp;\n-        res.cast_mantissa_subnormal(man_bits_delta, quantization);\n+        const int man_bits_delta = 1 - new_exp + (man_bits - new_man_bits);\n+        if (man_bits_delta > 0) {\n+            res.cast_mantissa_subnormal(man_bits_delta, quantization);\n+        } else {\n+            res.man <<= -man_bits_delta;\n+        }\n         return res;\n     }\n \n", "instance_id": "apytypes__apytypes-431", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issues related to deviations from Berkeley TestFloat in floating-point arithmetic. It provides specific examples of bugs and expected versus actual outputs for certain test cases, which helps in understanding the nature of the problems. The issues are tied to specific quantization modes and operations like multiplication and casting, with references to specific issues (#408, #407, #428, #411). However, there are minor ambiguities and missing details. For instance, the problem statement does not fully elaborate on the expected behavior for all quantization modes or provide a comprehensive list of edge cases to handle. Additionally, the description of the mantissa issue during casting is vague (\"similar to the previous bullet\") and lacks a concrete example or detailed explanation. While the provided test command and examples are helpful, dedicated test cases are mentioned as a future task rather than being included, which leaves some uncertainty about the full scope of testing required. Overall, the statement is valid and mostly clear but lacks some critical details for a fully comprehensive understanding.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the problem involves understanding and fixing multiple bugs in floating-point arithmetic, which is inherently a complex domain requiring knowledge of IEEE 754 standards, quantization modes, and format-specific behaviors (e.g., f16, f32, f64). The code change provided focuses on a specific issue related to mantissa casting for subnormal numbers, which is a relatively small modification in a single file (apyfloat.cc). However, the problem statement indicates multiple issues across different operations (multiplication, casting) and formats, suggesting that the full resolution likely involves changes in multiple areas of the codebase, even if only one diff is shown. \n\nSecond, the technical concepts required are non-trivial. Solving this requires a deep understanding of floating-point representation, bit-level manipulations, and quantization mode behaviors (e.g., TO_POS), as well as familiarity with the Berkeley TestFloat suite for validation. The code change itself involves handling mantissa shifts and subnormal number casting, which demands precision to avoid introducing new errors. \n\nThird, edge cases and error handling are significant. The problem statement explicitly mentions issues with specific inputs leading to incorrect results (e.g., infinity instead of max normal number, subnormal instead of zero), and the quantization mode adds complexity to how results should be rounded or bounded. These edge cases are not trivial, as they involve rare conditions in floating-point arithmetic that can have subtle downstream effects.\n\nFinally, while the provided code change is localized, the broader context of the problem suggests a need to understand interactions between different parts of the floating-point implementation and ensure consistency across formats and operations. This increases the cognitive load and risk of unintended side effects. A score of 0.65 reflects the hard nature of the problem due to the domain complexity, the need for precise low-level modifications, and the handling of multiple non-trivial edge cases, though it does not reach the \"Very Hard\" range as it does not appear to involve system-level architecture changes or extremely advanced concepts beyond floating-point arithmetic.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] Chunked parquet reader incorrect results for positive values of `n_rows`\nWith 24.12, I have:\n\n```python\n\nimport pylibcudf as plc\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\ndata = {\n            \"a\": [1, 2, 3, None, 4, 5],\n            \"b\": [\"\u1e85\", \"x\", \"y\", \"z\", \"123\", \"abcd\"],\n            \"c\": [None, None, 4, 5, -1, 0],\n        }\n\n\npath = \"./test.parquet\"\npq.write_table(pa.Table.from_pydict(data), path)\n\nreader = plc.io.parquet.ChunkedParquetReader(\n        plc.io.SourceInfo([path]),\n        columns=['a', 'b', 'c'],\n        nrows=2,\n        skip_rows=0,\n        chunk_read_limit=0,\n        pass_read_limit=17179869184 # 16 GiB\n)\n\n# Read data by chunk\nchk = reader.read_chunk()\ntbl = chk.tbl\nnames = chk.column_names()\nconcatenated_columns = tbl.columns()\nwhile reader.has_next():\n    tbl = reader.read_chunk().tbl\n\n    for i in range(tbl.num_columns()):\n        concatenated_columns[i] = plc.concatenate.concatenate(\n            [concatenated_columns[i], tbl._columns[i]]\n        )\n        # Drop residual columns to save memory\n        tbl._columns[i] = None\n\ngpu_result = plc.interop.to_arrow(tbl)\ncpu_result = pq.read_table(path)[:2]\n\nprint(cpu_result.column(1).to_pylist())\nprint(gpu_result.column(1).to_pylist())\n\n```\n\nThis results in\n\n```\n['\u1e85', 'x']\n['\u1e85', 'x\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n```\n\n\n", "patch": "diff --git a/cpp/src/io/parquet/page_decode.cuh b/cpp/src/io/parquet/page_decode.cuh\nindex 9ed2929a70e..ab40a5b9a55 100644\n--- a/cpp/src/io/parquet/page_decode.cuh\n+++ b/cpp/src/io/parquet/page_decode.cuh\n@@ -149,10 +149,21 @@ inline __device__ bool is_bounds_page(page_state_s* const s,\n   size_t const begin      = start_row;\n   size_t const end        = start_row + num_rows;\n \n-  // for non-nested schemas, rows cannot span pages, so use a more restrictive test\n-  return has_repetition\n-           ? ((page_begin <= begin && page_end >= begin) || (page_begin <= end && page_end >= end))\n-           : ((page_begin < begin && page_end > begin) || (page_begin < end && page_end > end));\n+  // Test for list schemas.\n+  auto const is_bounds_page_lists =\n+    ((page_begin <= begin and page_end >= begin) or (page_begin <= end and page_end >= end));\n+\n+  // For non-list schemas, rows cannot span pages, so use a more restrictive test. Make sure to\n+  // relax the test for `page_end` if we adjusted the `num_rows` for the last page to compensate\n+  // for list row size estimates in `generate_list_column_row_count_estimates()` when chunked\n+  // read mode.\n+  auto const test_page_end_nonlists =\n+    s->page.is_num_rows_adjusted ? page_end >= end : page_end > end;\n+\n+  auto const is_bounds_page_nonlists =\n+    (page_begin < begin and page_end > begin) or (page_begin < end and test_page_end_nonlists);\n+\n+  return has_repetition ? is_bounds_page_lists : is_bounds_page_nonlists;\n }\n \n /**\ndiff --git a/cpp/src/io/parquet/page_hdr.cu b/cpp/src/io/parquet/page_hdr.cu\nindex a8a8c441a84..6aec4ce0ec2 100644\n--- a/cpp/src/io/parquet/page_hdr.cu\n+++ b/cpp/src/io/parquet/page_hdr.cu\n@@ -433,6 +433,7 @@ void __launch_bounds__(128) gpuDecodePageHeaders(ColumnChunkDesc* chunks,\n       // definition levels\n       bs->page.chunk_row            = 0;\n       bs->page.num_rows             = 0;\n+      bs->page.is_num_rows_adjusted = false;\n       bs->page.skipped_values       = -1;\n       bs->page.skipped_leaf_values  = 0;\n       bs->page.str_bytes            = 0;\ndiff --git a/cpp/src/io/parquet/parquet_gpu.hpp b/cpp/src/io/parquet/parquet_gpu.hpp\nindex 3b4d0e6dc80..ce9d48693ec 100644\n--- a/cpp/src/io/parquet/parquet_gpu.hpp\n+++ b/cpp/src/io/parquet/parquet_gpu.hpp\n@@ -310,8 +310,10 @@ struct PageInfo {\n   // - In the case of a nested schema, you have to decode the repetition and definition\n   //   levels to extract actual column values\n   int32_t num_input_values;\n-  int32_t chunk_row;  // starting row of this page relative to the start of the chunk\n-  int32_t num_rows;   // number of rows in this page\n+  int32_t chunk_row;          // starting row of this page relative to the start of the chunk\n+  int32_t num_rows;           // number of rows in this page\n+  bool is_num_rows_adjusted;  // Flag to indicate if the number of rows of this page have been\n+                              // adjusted to compensate for the list row size estimates.\n   // the next four are calculated in gpuComputePageStringSizes\n   int32_t num_nulls;       // number of null values (V2 header), but recalculated for string cols\n   int32_t num_valids;      // number of non-null values, taking into account skip_rows/num_rows\ndiff --git a/cpp/src/io/parquet/reader_impl_preprocess.cu b/cpp/src/io/parquet/reader_impl_preprocess.cu\nindex f03f1214b9a..bcdae4cbd3b 100644\n--- a/cpp/src/io/parquet/reader_impl_preprocess.cu\n+++ b/cpp/src/io/parquet/reader_impl_preprocess.cu\n@@ -729,7 +729,10 @@ struct set_final_row_count {\n     if (i < pages.size() - 1 && (pages[i + 1].chunk_idx == page.chunk_idx)) { return; }\n     size_t const page_start_row = chunk.start_row + page.chunk_row;\n     size_t const chunk_last_row = chunk.start_row + chunk.num_rows;\n-    page.num_rows               = chunk_last_row - page_start_row;\n+    // Mark `is_num_rows_adjusted` to signal string decoders that the `num_rows` of this page has\n+    // been adjusted.\n+    page.is_num_rows_adjusted = page.num_rows != (chunk_last_row - page_start_row);\n+    page.num_rows             = chunk_last_row - page_start_row;\n   }\n };\n \n", "instance_id": "rapidsai__cudf-17321", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a bug in the chunked Parquet reader in the `pylibcudf` library when reading positive values of `n_rows`, leading to incorrect results (demonstrated by mismatched output for string column data). The provided Python code snippet effectively illustrates the problem with a reproducible example, showing the expected versus actual output. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for chunked reading with `n_rows` or clarify the constraints on `n_rows` and `skip_rows`. Additionally, edge cases (e.g., behavior with empty files, very large datasets, or specific Parquet encoding schemes) are not mentioned, which could be critical for a comprehensive fix. While the issue is valid and the goal is implied (fix the incorrect output), these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes involves multiple files in the C++ codebase (`page_decode.cuh`, `page_hdr.cu`, `parquet_gpu.hpp`, `reader_impl_preprocess.cu`), indicating a need to understand and modify core components of the Parquet reader implementation. The changes are not trivial; they involve logic adjustments for handling row counts and page boundaries, particularly for list and non-list schemas, as well as introducing a new flag (`is_num_rows_adjusted`) to handle specific cases in chunked reading mode. This suggests a deep understanding of the Parquet file format, GPU-based data processing, and the internal architecture of the library is required. Second, the technical concepts involved are complex, including CUDA programming (given the `.cu` files and GPU-specific logic), Parquet encoding/decoding mechanisms, and handling of nested schemas (repetition and definition levels). Third, while the problem statement does not explicitly mention edge cases, the nature of the bug (incorrect string data output) and the code changes (adjusting row counts and page boundary logic) imply that edge cases such as varying chunk sizes, last-page row adjustments, and string data handling must be carefully considered to avoid introducing new bugs. Finally, the impact of these changes is significant as they affect the core functionality of the Parquet reader, a critical component for data processing, potentially influencing performance and correctness across various use cases. While not at the extreme end of difficulty (e.g., requiring a complete rewrite or advanced domain-specific knowledge beyond Parquet/GPU), this problem demands substantial expertise and careful implementation, justifying a score of 0.75.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add additional `Sv_offset` values to AZFP calibration\nFrom communication with Steve Pearce at ASL Env Science, we can now add the following `Sv_offset` values for calibrating AZFP data.\r\n- 67 kHz: 500 us (1.1 dB)\r\n- 125 kHz: 150 us (1.4 dB), 250 us (1.3 dB)\r\n- 200 kHz: 150 us (1.4 dB), 250 us (1.3 dB)\r\n- 455 kHz: 250 us (1.3 dB)\r\n- 769 kHz: 150 us (1.4 dB)\n", "patch": "diff --git a/echopype/convert/parse_azfp.py b/echopype/convert/parse_azfp.py\nindex 425b0bc1b..702dbf2fa 100644\n--- a/echopype/convert/parse_azfp.py\n+++ b/echopype/convert/parse_azfp.py\n@@ -13,6 +13,44 @@\n \n FILENAME_DATETIME_AZFP = \"\\\\w+.01A\"\n \n+# Common Sv_offset values for frequency > 38 kHz\n+SV_OFFSET_HF = {\n+    300: 1.1,\n+    500: 0.8,\n+    700: 0.5,\n+    900: 0.3,\n+    1000: 0.3,\n+}\n+SV_OFFSET_LF = {\n+    500: 1.1,\n+    1000: 0.7,\n+}\n+SV_OFFSET = {\n+    38000.0: {**SV_OFFSET_LF},\n+    67000.0: {\n+        500: 1.1,\n+        **SV_OFFSET_HF,\n+    },\n+    125000.0: {\n+        150: 1.4,\n+        250: 1.3,\n+        **SV_OFFSET_HF,\n+    },\n+    200000.0: {\n+        150: 1.4,\n+        250: 1.3,\n+        **SV_OFFSET_HF,\n+    },\n+    455000.0: {\n+        250: 1.3,\n+        **SV_OFFSET_HF,\n+    },\n+    769000.0: {\n+        150: 1.4,\n+        **SV_OFFSET_HF,\n+    },\n+}\n+\n HEADER_FIELDS = (\n     (\"profile_flag\", \"u2\"),\n     (\"profile_number\", \"u2\"),\n@@ -480,22 +518,31 @@ def _get_ping_time(self):\n         self.ping_time = ping_time\n \n     @staticmethod\n-    def _calc_Sv_offset(f, pulse_len):\n-        \"\"\"Calculate the compensation factor for Sv calculation.\"\"\"\n-        # TODO: this method seems should be in echopype.process\n-        if f > 38000:\n-            if pulse_len == 300:\n-                return 1.1\n-            elif pulse_len == 500:\n-                return 0.8\n-            elif pulse_len == 700:\n-                return 0.5\n-            elif pulse_len == 900:\n-                return 0.3\n-            elif pulse_len == 1000:\n-                return 0.3\n-        else:\n-            if pulse_len == 500:\n-                return 1.1\n-            elif pulse_len == 1000:\n-                return 0.7\n+    def _calc_Sv_offset(freq, pulse_len):\n+        \"\"\"\n+        Calculate the compensation factor for Sv calculation.\n+\n+        Parameters\n+        ----------\n+        freq : number\n+            transmit frequency\n+        pulse_len : number\n+            pulse length\n+        \"\"\"\n+        # Check if the specified freq is in the allowable Sv_offset dict\n+        if freq not in SV_OFFSET.keys():\n+            raise ValueError(\n+                f\"Frequency {freq} Hz is not in the Sv offset dictionary! \"\n+                \"Please contact AZFP Environmental Sciences \"\n+                \"and raise an issue in the echopype repository.\"\n+            )\n+\n+        # Check if the specified freq-pulse length combination is in the allowable Sv_offset dict\n+        if pulse_len not in SV_OFFSET[freq]:\n+            raise ValueError(\n+                f\"Pulse length {pulse_len} us is not in the Sv offset dictionary! \"\n+                \"Please contact AZFP Environmental Sciences \"\n+                \"and raise an issue in the echopype repository.\"\n+            )\n+\n+        return SV_OFFSET[freq][pulse_len]\n", "instance_id": "OSOceanAcoustics__echopype-1304", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in terms of its goal, which is to add specific `Sv_offset` values for AZFP calibration across different frequencies and pulse lengths. The input data (frequencies and corresponding pulse length offsets) is explicitly provided, and the intent to update calibration logic is evident. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention how these values should be integrated into the codebase (e.g., whether they replace existing values or are additive), nor does it specify any constraints or edge cases to consider (e.g., what to do if a frequency or pulse length is not in the provided list). Additionally, there are no examples or test cases to validate the implementation. Despite these minor gaps, the intent and requirements are reasonably clear, especially when paired with the provided code changes, which clarify the implementation approach.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes are confined to a single file (`parse_azfp.py`) and involve straightforward modifications. The primary updates include adding a dictionary of `Sv_offset` values for various frequencies and pulse lengths, and refactoring the `_calc_Sv_offset` method to use this dictionary with added error handling. The changes do not impact the broader system architecture or require understanding complex interactions between modules. The amount of code change is moderate, with the addition of a static dictionary and a revised function implementation.\n\n2. **Number of Technical Concepts**: The problem requires basic knowledge of Python, specifically dictionaries for data storage and retrieval, and simple control flow for error handling. No advanced language features, algorithms, design patterns, or domain-specific knowledge beyond the provided calibration values are necessary. The logic is straightforward\u2014map input frequencies and pulse lengths to predefined offset values.\n\n3. **Potential Edge Cases and Error Handling**: The code changes introduce basic error handling to check if the provided frequency or pulse length exists in the dictionary, raising a `ValueError` with a helpful message if not. This addresses the primary edge case (invalid input combinations). However, the problem statement itself does not explicitly mention edge cases or additional error conditions to handle, and the implementation does not account for more nuanced scenarios (e.g., rounding of frequency values or handling unexpected input types). The complexity of edge cases is low.\n\n4. **Overall Complexity**: The task involves minimal complexity in terms of logic or implementation. It is essentially a data update combined with a small refactoring of an existing function to use a more structured data source. There are no performance considerations, intricate algorithms, or deep codebase knowledge required beyond understanding the purpose of the `_calc_Sv_offset` method.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task. It requires understanding some code logic (how the offset calculation works) and making simple modifications (adding a dictionary and updating a function), but it does not demand advanced skills or extensive changes. This is a routine update that a junior to mid-level developer with basic Python knowledge could handle with minimal guidance.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Chat Profile Descriptions Cannot be Interacted With\n**Describe the bug**\r\nWhen you hover over a chat profile, Chainlit will display a description written by the developer in the `markdown_description` parameter of `cl.ChatProfile`. If users move their mouse over to the actual description popover, to either scroll (for long descriptions) or to click on links inside of the description (for documentation, model card, etc.) the description goes away. This prevents the inclusion of links or long descriptions as users can't interact with or scroll inside of the description popover.\r\n\r\n**To Reproduce**\r\n1. Follow the **Simple example** in the [Chainlit docs](https://docs.chainlit.io/api-reference/chat-profiles#usage).\r\n2. Put a long description or clickable link in `markdown_description` parameter.\r\n3. Try to scroll or click the link in the chat profile description.\r\n\r\n**Example code**\r\n```python\r\nimport chainlit as cl\r\n\r\n\r\n@cl.set_chat_profiles\r\nasync def chat_profile():\r\n    return [\r\n        cl.ChatProfile(\r\n            name=\"GPT-3.5\",\r\n            markdown_description=\"Click [this](https://openai.com/chatgpt/) for more info on ChatGPT!\",\r\n            icon=\"https://picsum.photos/200\",\r\n        ),\r\n        cl.ChatProfile(\r\n            name=\"GPT-4\",\r\n            markdown_description=\"Click [this](https://openai.com/chatgpt/) for more info on ChatGPT!\",\r\n            icon=\"https://picsum.photos/250\",\r\n        ),\r\n    ]\r\n\r\n@cl.on_chat_start\r\nasync def on_chat_start():\r\n    chat_profile = cl.user_session.get(\"chat_profile\")\r\n    await cl.Message(\r\n        content=f\"starting chat using the {chat_profile} chat profile\"\r\n    ).send()\r\n\r\n```\r\n\r\n**Expected behavior**\r\nIt would be great to give users the ability to click links in the description to get more info about the specific model they are interacting with, such as provider documentation or usage explanation, etc.\r\n\r\n**Desktop (please complete the following information):**\r\n\r\n- OS: Windows 10\r\n- Browser: Edge\r\n- Version: 127.0.2651.86 (Official build) (64-bit)\r\n\r\n**Additional Context**\r\nI'm guessing the issue has to do with these lines in the [chatProfiles.tsx](https://github.com/Chainlit/chainlit/blob/9d6821558bdd57e1e88180f1b440699605974174/frontend/src/components/molecules/chatProfiles.tsx#L126-L149) file not accounting for the mouse hovering over the description popover, but I'm not very familiar with frontend development so I could be very wrong.\r\n\nChat Profile Descriptions Cannot be Interacted With\n**Describe the bug**\r\nWhen you hover over a chat profile, Chainlit will display a description written by the developer in the `markdown_description` parameter of `cl.ChatProfile`. If users move their mouse over to the actual description popover, to either scroll (for long descriptions) or to click on links inside of the description (for documentation, model card, etc.) the description goes away. This prevents the inclusion of links or long descriptions as users can't interact with or scroll inside of the description popover.\r\n\r\n**To Reproduce**\r\n1. Follow the **Simple example** in the [Chainlit docs](https://docs.chainlit.io/api-reference/chat-profiles#usage).\r\n2. Put a long description or clickable link in `markdown_description` parameter.\r\n3. Try to scroll or click the link in the chat profile description.\r\n\r\n**Example code**\r\n```python\r\nimport chainlit as cl\r\n\r\n\r\n@cl.set_chat_profiles\r\nasync def chat_profile():\r\n    return [\r\n        cl.ChatProfile(\r\n            name=\"GPT-3.5\",\r\n            markdown_description=\"Click [this](https://openai.com/chatgpt/) for more info on ChatGPT!\",\r\n            icon=\"https://picsum.photos/200\",\r\n        ),\r\n        cl.ChatProfile(\r\n            name=\"GPT-4\",\r\n            markdown_description=\"Click [this](https://openai.com/chatgpt/) for more info on ChatGPT!\",\r\n            icon=\"https://picsum.photos/250\",\r\n        ),\r\n    ]\r\n\r\n@cl.on_chat_start\r\nasync def on_chat_start():\r\n    chat_profile = cl.user_session.get(\"chat_profile\")\r\n    await cl.Message(\r\n        content=f\"starting chat using the {chat_profile} chat profile\"\r\n    ).send()\r\n\r\n```\r\n\r\n**Expected behavior**\r\nIt would be great to give users the ability to click links in the description to get more info about the specific model they are interacting with, such as provider documentation or usage explanation, etc.\r\n\r\n**Desktop (please complete the following information):**\r\n\r\n- OS: Windows 10\r\n- Browser: Edge\r\n- Version: 127.0.2651.86 (Official build) (64-bit)\r\n\r\n**Additional Context**\r\nI'm guessing the issue has to do with these lines in the [chatProfiles.tsx](https://github.com/Chainlit/chainlit/blob/9d6821558bdd57e1e88180f1b440699605974174/frontend/src/components/molecules/chatProfiles.tsx#L126-L149) file not accounting for the mouse hovering over the description popover, but I'm not very familiar with frontend development so I could be very wrong.\r\n\n", "patch": "diff --git a/frontend/src/components/molecules/chatProfiles.tsx b/frontend/src/components/molecules/chatProfiles.tsx\nindex 06f4f30029..08f2df111c 100644\n--- a/frontend/src/components/molecules/chatProfiles.tsx\n+++ b/frontend/src/components/molecules/chatProfiles.tsx\n@@ -27,6 +27,7 @@ export default function ChatProfiles() {\n   const { clear } = useChatInteract();\n   const [newChatProfile, setNewChatProfile] = useState<string | null>(null);\n   const [openDialog, setOpenDialog] = useState(false);\n+  const [popoverOpen, setPopoverOpen] = useState(false);\n   const navigate = useNavigate();\n \n   const handleClose = () => {\n@@ -58,8 +59,6 @@ export default function ChatProfiles() {\n   const allowHtml = config?.features?.unsafe_allow_html;\n   const latex = config?.features?.latex;\n \n-  const popoverOpen = Boolean(anchorEl);\n-\n   const items = config.chatProfiles.map((item) => {\n     const icon = item.icon?.includes('/public')\n       ? apiClient.buildEndpoint(item.icon)\n@@ -94,7 +93,8 @@ export default function ChatProfiles() {\n               theme.palette.mode === 'light'\n                 ? '0px 2px 4px 0px #0000000D'\n                 : '0px 10px 10px 0px #0000000D',\n-            ml: 2\n+            ml: 2,\n+            pointerEvents: 'auto' // Allow mouse interaction with the chat profile description\n           }\n         }}\n         sx={{\n@@ -110,6 +110,11 @@ export default function ChatProfiles() {\n           horizontal: 'left'\n         }}\n         disableRestoreFocus\n+        onMouseEnter={() => setPopoverOpen(true)}\n+        onMouseLeave={() => {\n+          setPopoverOpen(false);\n+          setAnchorEl(null);\n+        }}\n       >\n         <Box\n           p={2}\n@@ -134,10 +139,17 @@ export default function ChatProfiles() {\n           if (!item) return;\n           setChatProfileDescription(item.markdown_description);\n           setAnchorEl(event.currentTarget);\n+          setPopoverOpen(true);\n         }}\n-        onItemMouseLeave={() => setAnchorEl(null)}\n+        onItemMouseLeave={() => setPopoverOpen(false)}\n         onChange={(e) => {\n           const newValue = e.target.value;\n+\n+          // Close the chat profile description when any selection is made\n+          setPopoverOpen(false);\n+          setAnchorEl(null);\n+\n+          // Handle user selection\n           setNewChatProfile(newValue);\n           if (firstInteraction) {\n             setOpenDialog(true);\n@@ -145,7 +157,10 @@ export default function ChatProfiles() {\n             handleConfirm(newValue);\n           }\n         }}\n-        onClose={() => setAnchorEl(null)}\n+        onClose={() => {\n+          setPopoverOpen(false);\n+          setAnchorEl(null);\n+        }}\n       />\n       <NewChatDialog\n         open={openDialog}\n", "instance_id": "Chainlit__chainlit-1276", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug where chat profile descriptions in Chainlit disappear when users attempt to interact with them (e.g., scroll or click links). The goal is explicitly stated: to allow user interaction with the description popover. The statement includes detailed reproduction steps, example code, expected behavior, and relevant environment information (OS, browser, version). Additionally, the reporter provides a hypothesis about the root cause in the frontend code, which adds helpful context. There are no significant ambiguities, and the problem is supported by clear examples and references to documentation. All critical details, including input (hovering over a profile), output (popover disappearing), and constraints (interaction with links or long text), are well-defined.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to a single file (`chatProfiles.tsx`) and focus on a specific component related to the popover behavior. The modifications involve adding state management for the popover (`popoverOpen`) and adjusting event handlers (`onMouseEnter`, `onMouseLeave`, etc.) to keep the popover open during interaction. The changes are relatively small (under 20 lines of meaningful code) and do not impact the broader system architecture or require cross-module coordination.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of React (or TypeScript JSX) event handling, state management (`useState`), and UI component libraries (likely Material-UI, given the `Popover` component). These are fundamental frontend development concepts that do not demand advanced expertise. No complex algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention complex edge cases beyond the core issue of popover interaction. The code changes introduce logic to close the popover on selection or dialog closure, which handles the primary interaction flow. However, potential minor edge cases (e.g., rapid mouse movements, accessibility concerns like keyboard navigation) are not addressed in the problem statement or code changes, and they do not significantly elevate the difficulty.\n\n4. **Overall Complexity**: The fix involves straightforward modifications to event listeners and state logic to ensure the popover remains interactive. While it requires some understanding of the component's behavior and event propagation, it does not demand deep knowledge of the Chainlit codebase or intricate debugging. The problem is a typical UI bug fix that a developer with moderate frontend experience can handle.\n\nA score of 0.35 reflects that this is slightly more involved than a trivial fix (e.g., changing a constant) due to the need to understand React event handling and test the UI behavior, but it remains within the easy category as it does not require advanced skills or extensive codebase familiarity.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Flink: Add table.exec.iceberg.use-v2-sink option\nThis PR adds a `table.exec.iceberg.use-v2-sink` configuration option allowing to use Flink's Sink v2 API described in the [FLIP-143](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API) document.\r\n\r\nThe configuration option is by default set to `false`.\r\n\r\nThis PR is the follow-up of discussion in #10179 and in #11219\n", "patch": "diff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/FlinkConfigOptions.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/FlinkConfigOptions.java\nindex 7c7afd24ed8e..6362bc447634 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/FlinkConfigOptions.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/FlinkConfigOptions.java\n@@ -91,6 +91,12 @@ private FlinkConfigOptions() {}\n           .defaultValue(false)\n           .withDescription(\"Use the FLIP-27 based Iceberg source implementation.\");\n \n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_USE_V2_SINK =\n+      ConfigOptions.key(\"table.exec.iceberg.use-v2-sink\")\n+          .booleanType()\n+          .defaultValue(false)\n+          .withDescription(\"Use the SinkV2 API based Iceberg sink implementation.\");\n+\n   public static final ConfigOption<SplitAssignerType> TABLE_EXEC_SPLIT_ASSIGNER_TYPE =\n       ConfigOptions.key(\"table.exec.iceberg.split-assigner-type\")\n           .enumType(SplitAssignerType.class)\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 1b9268569d9a..c2c5a6706e92 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n@@ -35,6 +35,7 @@\n import org.apache.flink.types.RowKind;\n import org.apache.flink.util.Preconditions;\n import org.apache.iceberg.flink.sink.FlinkSink;\n+import org.apache.iceberg.flink.sink.IcebergSink;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n \n public class IcebergTableSink implements DynamicTableSink, SupportsPartitioning, SupportsOverwrite {\n@@ -77,14 +78,25 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {\n       @Override\n       public DataStreamSink<?> consumeDataStream(\n           ProviderContext providerContext, DataStream<RowData> dataStream) {\n-        return FlinkSink.forRowData(dataStream)\n-            .tableLoader(tableLoader)\n-            .tableSchema(tableSchema)\n-            .equalityFieldColumns(equalityColumns)\n-            .overwrite(overwrite)\n-            .setAll(writeProps)\n-            .flinkConf(readableConfig)\n-            .append();\n+        if (readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_V2_SINK)) {\n+          return IcebergSink.forRowData(dataStream)\n+              .tableLoader(tableLoader)\n+              .tableSchema(tableSchema)\n+              .equalityFieldColumns(equalityColumns)\n+              .overwrite(overwrite)\n+              .setAll(writeProps)\n+              .flinkConf(readableConfig)\n+              .append();\n+        } else {\n+          return FlinkSink.forRowData(dataStream)\n+              .tableLoader(tableLoader)\n+              .tableSchema(tableSchema)\n+              .equalityFieldColumns(equalityColumns)\n+              .overwrite(overwrite)\n+              .setAll(writeProps)\n+              .flinkConf(readableConfig)\n+              .append();\n+        }\n       }\n     };\n   }\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex c53431490984..3f1fef6580ef 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -41,7 +41,7 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n@@ -381,7 +381,7 @@ public Builder toBranch(String branch) {\n       return this;\n     }\n \n-    private <T> DataStreamSink<T> chainIcebergOperators() {\n+    private DataStreamSink<Void> chainIcebergOperators() {\n       Preconditions.checkArgument(\n           inputCreator != null,\n           \"Please use forRowData() or forMapperOutputType() to initialize the input DataStream.\");\n@@ -472,12 +472,10 @@ List<Integer> checkAndGetEqualityFieldIds() {\n       return equalityFieldIds;\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n-    private <T> DataStreamSink<T> appendDummySink(\n-        SingleOutputStreamOperator<Void> committerStream) {\n-      DataStreamSink<T> resultStream =\n+    private DataStreamSink<Void> appendDummySink(SingleOutputStreamOperator<Void> committerStream) {\n+      DataStreamSink<Void> resultStream =\n           committerStream\n-              .addSink(new DiscardingSink())\n+              .sinkTo(new DiscardingSink<>())\n               .name(operatorName(String.format(\"IcebergSink %s\", this.table.name())))\n               .setParallelism(1);\n       if (uidPrefix != null) {\n", "instance_id": "apache__iceberg-11665", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to add a configuration option (`table.exec.iceberg.use-v2-sink`) to enable the use of Flink's Sink v2 API for Iceberg integration, as described in FLIP-143. It specifies that the option defaults to `false` and references prior discussions for context. However, it lacks critical details such as specific requirements for the Sink v2 API, potential compatibility issues, or expected behavior when the option is toggled. Additionally, edge cases or constraints (e.g., performance implications or limitations of using Sink v2) are not mentioned. While the goal is understandable, the statement could benefit from more comprehensive documentation or examples to clarify the full scope and impact of this change.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a few files (`FlinkConfigOptions.java`, `IcebergTableSink.java`, and `FlinkSink.java`) with a moderate amount of code modification. The changes involve adding a new configuration option and conditionally switching between two sink implementations (Sink v1 and Sink v2) based on the configuration. This requires understanding Flink's API (specifically Sink v1 and v2 differences) and the Iceberg integration within Flink, which introduces some complexity. \n\nSecond, the technical concepts involved include familiarity with Flink's streaming and table APIs, configuration management via `ConfigOptions`, and the differences between Sink v1 and Sink v2 implementations. While these are not overly advanced for someone familiar with Flink, they do require domain-specific knowledge of data streaming frameworks and potentially some understanding of FLIP-143's design goals. \n\nThird, the changes do not appear to impact the broader system architecture significantly, as they are more of an optional feature toggle rather than a core refactoring. However, there is a need to ensure compatibility between the two sink implementations, which might involve subtle edge cases (e.g., behavior differences between Sink v1 and v2 under specific conditions), though these are not explicitly mentioned in the problem statement or code changes. Error handling modifications are minimal in the provided diff.\n\nOverall, this task requires a moderate level of expertise in Flink and careful implementation to ensure the toggle works seamlessly, but it does not involve deep architectural changes or highly complex logic, placing it in the 0.4-0.6 range. I\u2019ve assigned a score of 0.45 to reflect a slightly below-average medium difficulty due to the focused scope of changes, balanced by the need for specific Flink knowledge.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Bad Table Properties cause commit failure\n### Apache Iceberg version\n\n1.6.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug \ud83d\udc1e\n\nToday iceberg does not validate the type of table properties values on and this can be problematic if those table properties value are used for commit. \r\n\r\nExample setup\r\n```\r\n                sql(\r\n                    \"CREATE TABLE foo.bar \"\r\n                        + \"(id BIGINT NOT NULL, data STRING) \"\r\n                        + \"USING iceberg \"\r\n                        + \"TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\",\r\n```\r\n\r\nSee the value for `commit.retry.num-retries` is accidentally set to some non-integer value, we will unable to rectify this through either spark SQL or iceberg API \r\n\r\n```\r\ntable.updateProperties.remove(\"commit.retry.max-wait-ms\").apply.commit\r\n\r\ntable.updateProperties.remove(\"commit.retry.max-wait-ms\").commit\r\njava.lang.NumberFormatException: For input string: \"commit.retry.max-wait-ms\"\r\n  at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:668)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:786)\r\n  at org.apache.iceberg.util.PropertyUtil.propertyAsInt(PropertyUtil.java:64)\r\n  at org.apache.iceberg.TableMetadata.propertyAsInt(TableMetadata.java:472)\r\n  at org.apache.iceberg.PropertiesUpdate.commit(PropertiesUpdate.java:105)\r\n  ... 48 elided\r\n```\r\n\r\nAfter some look, I believe this block of code might be the problem: https://github.com/apache/iceberg/blob/main/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java#L100-L114, where  propertyAsInt will throw NumberFormatException before commit can proceed to rectify or remove incorrectly set table properties. \r\n\r\n\r\nCurrent workaround is move back to previous table metadata before the table properties change, but I plan to contribute the patch to fix the problem in 2 ways\r\n1. add validation for new table where commit related table properties need to have value type checked (as integer)\r\n2. relax the condition in `PropertiesUpdate` class to allow update when existing value is corrupted, this help with existing table to move forward. \r\n\r\n\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time\n", "patch": "diff --git a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\nindex 35338a689205..9389aec50c0a 100644\n--- a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n@@ -98,12 +98,13 @@ public Map<String, String> apply() {\n \n   @Override\n   public void commit() {\n+    // If existing table commit properties in base are corrupted, allow rectification\n     Tasks.foreach(ops)\n-        .retry(base.propertyAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n+        .retry(base.propertyTryAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n         .exponentialBackoff(\n-            base.propertyAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n             2.0 /* exponential */)\n         .onlyRetryOn(CommitFailedException.class)\n         .run(\ndiff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java\nindex d20dd59d2b97..3cdc53995dce 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java\n@@ -134,6 +134,8 @@ static TableMetadata newTableMetadata(\n     // break existing tables.\n     MetricsConfig.fromProperties(properties).validateReferencedColumns(schema);\n \n+    PropertyUtil.validateCommitProperties(properties);\n+\n     return new Builder()\n         .setInitialFormatVersion(formatVersion)\n         .setCurrentSchema(freshSchema, lastColumnId.get())\n@@ -486,6 +488,10 @@ public int propertyAsInt(String property, int defaultValue) {\n     return PropertyUtil.propertyAsInt(properties, property, defaultValue);\n   }\n \n+  public int propertyTryAsInt(String property, int defaultValue) {\n+    return PropertyUtil.propertyTryAsInt(properties, property, defaultValue);\n+  }\n+\n   public long propertyAsLong(String property, long defaultValue) {\n     return PropertyUtil.propertyAsLong(properties, property, defaultValue);\n   }\ndiff --git a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\nindex 68c8f3e9efda..633b0a6ae739 100644\n--- a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n@@ -24,10 +24,23 @@\n import java.util.Set;\n import java.util.function.Predicate;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class PropertyUtil {\n+  private static final Logger LOG = LoggerFactory.getLogger(PropertyUtil.class);\n+\n+  private static final Set<String> COMMIT_PROPERTIES =\n+      ImmutableSet.of(\n+          TableProperties.COMMIT_NUM_RETRIES,\n+          TableProperties.COMMIT_MIN_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_MAX_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_TOTAL_RETRY_TIME_MS);\n \n   private PropertyUtil() {}\n \n@@ -57,6 +70,20 @@ public static double propertyAsDouble(\n     return defaultValue;\n   }\n \n+  public static int propertyTryAsInt(\n+      Map<String, String> properties, String property, int defaultValue) {\n+    String value = properties.get(property);\n+    if (value == null) {\n+      return defaultValue;\n+    }\n+    try {\n+      return Integer.parseInt(value);\n+    } catch (NumberFormatException e) {\n+      LOG.warn(\"Failed to parse value of {} as integer, default to {}\", property, defaultValue, e);\n+      return defaultValue;\n+    }\n+  }\n+\n   public static int propertyAsInt(\n       Map<String, String> properties, String property, int defaultValue) {\n     String value = properties.get(property);\n@@ -100,6 +127,29 @@ public static String propertyAsString(\n     return defaultValue;\n   }\n \n+  /**\n+   * Validate the table commit related properties to have non-negative integer on table creation to\n+   * prevent commit failure\n+   */\n+  public static void validateCommitProperties(Map<String, String> properties) {\n+    for (String commitProperty : COMMIT_PROPERTIES) {\n+      String value = properties.get(commitProperty);\n+      if (value != null) {\n+        int parsedValue;\n+        try {\n+          parsedValue = Integer.parseInt(value);\n+        } catch (NumberFormatException e) {\n+          throw new ValidationException(\n+              \"Table property %s must have integer value\", commitProperty);\n+        }\n+        ValidationException.check(\n+            parsedValue >= 0,\n+            \"Table property %s must have non negative integer value\",\n+            commitProperty);\n+      }\n+    }\n+  }\n+\n   /**\n    * Returns subset of provided map with keys matching the provided prefix. Matching is\n    * case-sensitive and the matching prefix is removed from the keys in returned map.\n", "instance_id": "apache__iceberg-11437", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear and provides a valid description of the issue. It identifies a specific bug in Apache Iceberg related to table property validation, where non-integer values for commit-related properties cause commit failures. The statement includes an example of problematic SQL code, a stack trace of the error (NumberFormatException), and a reference to the problematic code block in the repository. Additionally, the proposed solution approach (validation of new table properties and relaxing conditions for updates) is outlined. However, there are minor ambiguities: the problem statement does not explicitly define all commit-related properties that need validation, nor does it discuss potential edge cases or constraints for the validation logic (e.g., acceptable ranges for integer values beyond non-negativity). These missing details prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files (`PropertiesUpdate.java`, `TableMetadata.java`, and `PropertyUtil.java`) and introduces new logic for property validation and error-tolerant parsing (`propertyTryAsInt`). While the changes are not architecturally significant, they require understanding the interaction between table metadata, property utilities, and commit operations in the Apache Iceberg codebase. Second, the technical concepts involved include exception handling, property parsing, and validation logic, which are moderately complex but not overly advanced. Third, the problem requires handling edge cases like corrupted property values and ensuring that validation does not break existing functionality, though these are not extensively detailed in the statement. Finally, the amount of code change is moderate, with new methods and modifications to existing retry logic. Overall, this problem requires a solid understanding of Java, error handling, and the specific domain of Iceberg table properties, but it does not demand deep architectural changes or highly advanced concepts, justifying a score of 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Flink sink writes duplicate data in upsert mode\n### Apache Iceberg version\r\n\r\n1.2.1\r\n\r\n### Query engine\r\n\r\nFlink 1.14.4\r\n\r\n### Please describe the bug \ud83d\udc1e\r\n\r\nI have a flink upsert job with a checkpoint interval of 5 minutes and an external service periodically(30min) triggers the savepoint, parallelism = 1.\r\n\r\n5 files were generated in one checkpoint cycle, including two data files, two eq delete files, and one pos delete file.\r\nThe 2 data files and 2 eq-delete files contained the same data. When I queried, duplicate data appeared.\r\nI think it is because the subsequent eq delete is not associated with the first data file.\r\n\r\nFlink TM log\r\n```\r\n2024-05-31 16:10:57.457 org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.zstd]\r\n2024-05-31 16:10:57.459 org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.zstd]\r\n2024-05-31 16:10:57.462 org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Checkpoint 5765 has been notified as aborted, would not trigger any checkpoint.\r\n2024-05-31 16:13:58.455 org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.zstd]\r\n2024-05-31 16:13:58.505 org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.zstd]\r\n2024-05-31 16:13:58.507 org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new compressor [.zstd]\r\n```\r\nJM log\r\n\r\n```\r\n2024-05-31 16:08:12.840 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5764 (type=CHECKPOINT) @ 1717142891998 for job fc721024df3d70e3a1f3a46a63e9635a.\r\n2024-05-31 16:08:16.239 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering savepoint for job fc721024df3d70e3a1f3a46a63e9635a.\r\n2024-05-31 16:08:16.242 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5765 (type=SAVEPOINT) @ 1717142896239 for job fc721024df3d70e3a1f3a46a63e9635a.\r\n2024-05-31 16:09:41.531 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5764 for job fc721024df3d70e3a1f3a46a63e9635a (7170 bytes, checkpointDuration=89495 ms, finalizationTime=38 ms).\r\n2024-05-31 16:09:41.532 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 5764 as completed for source Source: TableSourceScan(table=[[default_catalog, default_database, cdc_xxx]], fields=[id, data_status, ...]).\r\n2024-05-31 16:10:46.242 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint 5765 of job fc721024df3d70e3a1f3a46a63e9635a expired before completing.\r\n```\r\n\r\nDownloaded files and sizes:\r\n```\r\n-rw-r--r--@ 1 q  staff   30528 May 31 19:36 00000-1-64548f8b-a8a9-474f-95f8-a59a30dcce2f-01926.parquet\r\n-rw-r--r--@ 1 q  staff     701 May 31 19:36 00000-1-64548f8b-a8a9-474f-95f8-a59a30dcce2f-01927.parquet\r\n-rw-r--r--@ 1 q  staff  741706 May 31 19:36 00000-1-64548f8b-a8a9-474f-95f8-a59a30dcce2f-01928.parquet\r\n-rw-r--r--@ 1 q  staff   17592 May 31 19:37 00000-1-64548f8b-a8a9-474f-95f8-a59a30dcce2f-01929.parquet\r\n-rw-r--r--@ 1 q  staff    1978 May 31 19:37 00000-1-64548f8b-a8a9-474f-95f8-a59a30dcce2f-01930.parquet\r\n```\r\n\n", "patch": "diff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 2256d1e874ce..5cd43a46de37 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -67,7 +67,6 @@\n import org.apache.iceberg.flink.sink.shuffle.StatisticsOrRecord;\n import org.apache.iceberg.flink.sink.shuffle.StatisticsType;\n import org.apache.iceberg.flink.util.FlinkCompatibilityUtil;\n-import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -420,7 +419,7 @@ private <T> DataStreamSink<T> chainIcebergOperators() {\n           distributeDataStream(rowDataInput, equalityFieldIds, flinkRowType, writerParallelism);\n \n       // Add parallel writers that append rows to files\n-      SingleOutputStreamOperator<WriteResult> writerStream =\n+      SingleOutputStreamOperator<FlinkWriteResult> writerStream =\n           appendWriter(distributeStream, flinkRowType, equalityFieldIds, writerParallelism);\n \n       // Add single-parallelism committer that commits files\n@@ -487,7 +486,7 @@ private <T> DataStreamSink<T> appendDummySink(\n     }\n \n     private SingleOutputStreamOperator<Void> appendCommitter(\n-        SingleOutputStreamOperator<WriteResult> writerStream) {\n+        SingleOutputStreamOperator<FlinkWriteResult> writerStream) {\n       IcebergFilesCommitter filesCommitter =\n           new IcebergFilesCommitter(\n               tableLoader,\n@@ -507,7 +506,7 @@ private SingleOutputStreamOperator<Void> appendCommitter(\n       return committerStream;\n     }\n \n-    private SingleOutputStreamOperator<WriteResult> appendWriter(\n+    private SingleOutputStreamOperator<FlinkWriteResult> appendWriter(\n         DataStream<RowData> input,\n         RowType flinkRowType,\n         List<Integer> equalityFieldIds,\n@@ -545,11 +544,11 @@ private SingleOutputStreamOperator<WriteResult> appendWriter(\n       IcebergStreamWriter<RowData> streamWriter =\n           createStreamWriter(tableSupplier, flinkWriteConf, flinkRowType, equalityFieldIds);\n \n-      SingleOutputStreamOperator<WriteResult> writerStream =\n+      SingleOutputStreamOperator<FlinkWriteResult> writerStream =\n           input\n               .transform(\n                   operatorName(ICEBERG_STREAM_WRITER_NAME),\n-                  TypeInformation.of(WriteResult.class),\n+                  TypeInformation.of(FlinkWriteResult.class),\n                   streamWriter)\n               .setParallelism(writerParallelism);\n       if (uidPrefix != null) {\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkWriteResult.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkWriteResult.java\nnew file mode 100644\nindex 000000000000..317fb169ae1b\n--- /dev/null\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkWriteResult.java\n@@ -0,0 +1,40 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.io.WriteResult;\n+\n+public class FlinkWriteResult implements Serializable {\n+  private final long checkpointId;\n+  private final WriteResult writeResult;\n+\n+  public FlinkWriteResult(long checkpointId, WriteResult writeResult) {\n+    this.checkpointId = checkpointId;\n+    this.writeResult = writeResult;\n+  }\n+\n+  public long checkpointId() {\n+    return checkpointId;\n+  }\n+\n+  public WriteResult writeResult() {\n+    return writeResult;\n+  }\n+}\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\nindex b9bceaa9311d..7108c2008341 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n@@ -63,7 +63,7 @@\n import org.slf4j.LoggerFactory;\n \n class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n-    implements OneInputStreamOperator<WriteResult, Void>, BoundedOneInput {\n+    implements OneInputStreamOperator<FlinkWriteResult, Void>, BoundedOneInput {\n \n   private static final long serialVersionUID = 1L;\n   private static final long INITIAL_CHECKPOINT_ID = -1L;\n@@ -96,7 +96,7 @@ class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n \n   // The completed files cache for current checkpoint. Once the snapshot barrier received, it will\n   // be flushed to the 'dataFilesPerCheckpoint'.\n-  private final List<WriteResult> writeResultsOfCurrentCkpt = Lists.newArrayList();\n+  private final Map<Long, List<WriteResult>> writeResultsSinceLastSnapshot = Maps.newHashMap();\n   private final String branch;\n \n   // It will have an unique identifier for one job.\n@@ -212,7 +212,8 @@ public void snapshotState(StateSnapshotContext context) throws Exception {\n \n     // Update the checkpoint state.\n     long startNano = System.nanoTime();\n-    dataFilesPerCheckpoint.put(checkpointId, writeToManifest(checkpointId));\n+    writeToManifestUptoLatestCheckpoint(checkpointId);\n+\n     // Reset the snapshot state to the latest state.\n     checkpointsState.clear();\n     checkpointsState.add(dataFilesPerCheckpoint);\n@@ -220,8 +221,6 @@ public void snapshotState(StateSnapshotContext context) throws Exception {\n     jobIdState.clear();\n     jobIdState.add(flinkJobId);\n \n-    // Clear the local buffer for current checkpoint.\n-    writeResultsOfCurrentCkpt.clear();\n     committerMetrics.checkpointDuration(\n         TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startNano));\n   }\n@@ -426,30 +425,45 @@ private void commitOperation(\n   }\n \n   @Override\n-  public void processElement(StreamRecord<WriteResult> element) {\n-    this.writeResultsOfCurrentCkpt.add(element.getValue());\n+  public void processElement(StreamRecord<FlinkWriteResult> element) {\n+    FlinkWriteResult flinkWriteResult = element.getValue();\n+    List<WriteResult> writeResults =\n+        writeResultsSinceLastSnapshot.computeIfAbsent(\n+            flinkWriteResult.checkpointId(), k -> Lists.newArrayList());\n+    writeResults.add(flinkWriteResult.writeResult());\n   }\n \n   @Override\n   public void endInput() throws IOException {\n     // Flush the buffered data files into 'dataFilesPerCheckpoint' firstly.\n-    long currentCheckpointId = Long.MAX_VALUE;\n-    dataFilesPerCheckpoint.put(currentCheckpointId, writeToManifest(currentCheckpointId));\n-    writeResultsOfCurrentCkpt.clear();\n-\n+    long currentCheckpointId = IcebergStreamWriter.END_INPUT_CHECKPOINT_ID;\n+    writeToManifestUptoLatestCheckpoint(currentCheckpointId);\n     commitUpToCheckpoint(dataFilesPerCheckpoint, flinkJobId, operatorUniqueId, currentCheckpointId);\n   }\n \n+  private void writeToManifestUptoLatestCheckpoint(long checkpointId) throws IOException {\n+    if (!writeResultsSinceLastSnapshot.containsKey(checkpointId)) {\n+      dataFilesPerCheckpoint.put(checkpointId, EMPTY_MANIFEST_DATA);\n+    }\n+\n+    for (Map.Entry<Long, List<WriteResult>> writeResultsOfCheckpoint :\n+        writeResultsSinceLastSnapshot.entrySet()) {\n+      dataFilesPerCheckpoint.put(\n+          writeResultsOfCheckpoint.getKey(),\n+          writeToManifest(writeResultsOfCheckpoint.getKey(), writeResultsOfCheckpoint.getValue()));\n+    }\n+\n+    // Clear the local buffer for current checkpoint.\n+    writeResultsSinceLastSnapshot.clear();\n+  }\n+\n   /**\n    * Write all the complete data files to a newly created manifest file and return the manifest's\n    * avro serialized bytes.\n    */\n-  private byte[] writeToManifest(long checkpointId) throws IOException {\n-    if (writeResultsOfCurrentCkpt.isEmpty()) {\n-      return EMPTY_MANIFEST_DATA;\n-    }\n-\n-    WriteResult result = WriteResult.builder().addAll(writeResultsOfCurrentCkpt).build();\n+  private byte[] writeToManifest(long checkpointId, List<WriteResult> writeResults)\n+      throws IOException {\n+    WriteResult result = WriteResult.builder().addAll(writeResults).build();\n     DeltaManifests deltaManifests =\n         FlinkManifestUtil.writeCompletedFiles(\n             result, () -> manifestOutputFileFactory.create(checkpointId), spec);\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\nindex 9ea0349fb057..bb5efe982ee1 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java\n@@ -29,10 +29,11 @@\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n \n-class IcebergStreamWriter<T> extends AbstractStreamOperator<WriteResult>\n-    implements OneInputStreamOperator<T, WriteResult>, BoundedOneInput {\n+class IcebergStreamWriter<T> extends AbstractStreamOperator<FlinkWriteResult>\n+    implements OneInputStreamOperator<T, FlinkWriteResult>, BoundedOneInput {\n \n   private static final long serialVersionUID = 1L;\n+  static final long END_INPUT_CHECKPOINT_ID = Long.MAX_VALUE;\n \n   private final String fullTableName;\n   private final TaskWriterFactory<T> taskWriterFactory;\n@@ -63,7 +64,7 @@ public void open() {\n \n   @Override\n   public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n-    flush();\n+    flush(checkpointId);\n     this.writer = taskWriterFactory.create();\n   }\n \n@@ -89,7 +90,7 @@ public void endInput() throws IOException {\n     // Note that if the task is not closed after calling endInput, checkpoint may be triggered again\n     // causing files to be sent repeatedly, the writer is marked as null after the last file is sent\n     // to guard against duplicated writes.\n-    flush();\n+    flush(END_INPUT_CHECKPOINT_ID);\n   }\n \n   @Override\n@@ -102,7 +103,7 @@ public String toString() {\n   }\n \n   /** close all open files and emit files to downstream committer operator */\n-  private void flush() throws IOException {\n+  private void flush(long checkpointId) throws IOException {\n     if (writer == null) {\n       return;\n     }\n@@ -110,7 +111,7 @@ private void flush() throws IOException {\n     long startNano = System.nanoTime();\n     WriteResult result = writer.complete();\n     writerMetrics.updateFlushResult(result);\n-    output.collect(new StreamRecord<>(result));\n+    output.collect(new StreamRecord<>(new FlinkWriteResult(checkpointId, result)));\n     writerMetrics.flushDuration(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startNano));\n \n     // Set writer to null to prevent duplicate flushes in the corner case of\n", "instance_id": "apache__iceberg-10526", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue of duplicate data being written in upsert mode by a Flink sink to Apache Iceberg. It provides specific details about the environment (Flink version 1.14.4, Iceberg version 1.2.1), the configuration (checkpoint interval of 5 minutes, savepoint triggering every 30 minutes, parallelism of 1), and the observed behavior (duplicate data in files). Logs and file details are included, which help in understanding the context of the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or the exact nature of the bug (e.g., why the eq-delete files are not associated with the first data file). Additionally, there are no clear examples of input data or queries that result in duplicates, which would help in reproducing the issue. Edge cases or specific conditions under which the issue occurs are also not fully specified. Overall, while the problem is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.75) due to several factors. First, the scope and depth of code changes are significant, as they involve multiple files and classes within the Flink-Iceberg integration module (e.g., `FlinkSink.java`, `IcebergFilesCommitter.java`, `IcebergStreamWriter.java`, and the introduction of a new class `FlinkWriteResult.java`). The changes impact the core logic of checkpointing and file writing, which are critical to the system's data consistency and correctness. Second, the problem requires a deep understanding of several technical concepts, including Flink's checkpointing and savepoint mechanisms, Iceberg's file management and manifest system, and the interaction between these two systems in upsert mode. Knowledge of distributed systems, data consistency in streaming pipelines, and specific Flink operators (e.g., `AbstractStreamOperator`, `OneInputStreamOperator`) is necessary. Third, the code changes introduce a new mechanism to track checkpoint IDs with write results (`FlinkWriteResult`), indicating a need to handle state management and synchronization across checkpoints, which adds complexity. Finally, while the problem statement does not explicitly mention edge cases, the nature of the issue (duplicate data due to checkpoint/savepoint interactions) implies potential edge cases around checkpoint failures, savepoint interruptions, and parallelism changes, which the solution must account for. The combination of architectural impact, required domain knowledge, and complexity of the fix places this problem in the hard category, though not at the extreme end of very hard, as it does not appear to involve entirely new system-level designs or highly intricate distributed algorithms.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ensure microbatch models respect model level `full_refresh` config during batch creation\nIn dbt-core, it is generally expected that values passed via CLI flags take precedence over model level configs. However, `full_refresh` on a model is an exception to this rule, where in the model config takes precedence. This config exists specifically to _prevent_ accidental full refreshes of large incremental models, as doing so can be costly. **_It is actually best practice to set `full_refresh=False` on incremental models_.**\r\n\r\nCurrently, for microbatch models the above is not happening. The CLI flag `--full-refresh` is taking precedence over the model config `full_refresh`. That means that currently if `--full-refresh` is supplied then the microbatch model **_will be refreshed_** even if `full_refresh=False` is set on the model. This needs to not be the case. \n", "patch": "diff --git a/.changes/unreleased/Features-20240926-153210.yaml b/.changes/unreleased/Features-20240926-153210.yaml\nnew file mode 100644\nindex 00000000000..8f8919b918f\n--- /dev/null\n+++ b/.changes/unreleased/Features-20240926-153210.yaml\n@@ -0,0 +1,6 @@\n+kind: Features\n+body: Ensure microbatch models respect `full_refresh` model config\n+time: 2024-09-26T15:32:10.202789-05:00\n+custom:\n+  Author: QMalcolm\n+  Issue: \"10785\"\ndiff --git a/core/dbt/task/run.py b/core/dbt/task/run.py\nindex 2e3f774a07a..bd23bb8819a 100644\n--- a/core/dbt/task/run.py\n+++ b/core/dbt/task/run.py\n@@ -550,12 +550,17 @@ def _is_incremental(self, model) -> bool:\n         relation = self.adapter.get_relation(\n             relation_info.database, relation_info.schema, relation_info.name\n         )\n-        return (\n+        if (\n             relation is not None\n             and relation.type == \"table\"\n             and model.config.materialized == \"incremental\"\n-            and not (getattr(self.config.args, \"FULL_REFRESH\", False) or model.config.full_refresh)\n-        )\n+        ):\n+            if model.config.full_refresh is not None:\n+                return not model.config.full_refresh\n+            else:\n+                return not getattr(self.config.args, \"FULL_REFRESH\", False)\n+        else:\n+            return False\n \n \n class RunTask(CompileTask):\n", "instance_id": "dbt-labs__dbt-core-10788", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: microbatch models in dbt-core are not respecting the model-level `full_refresh` configuration, allowing CLI flags to override it, which contradicts the intended behavior for incremental models. The goal is explicit\u2014ensure that the model config takes precedence over the CLI flag. However, there are minor ambiguities and missing details. For instance, the statement does not specify how microbatch models differ from other models in terms of implementation or behavior, which could be critical for understanding the context. Additionally, there are no examples of input/output or specific test cases to illustrate the problem (e.g., a sample model config and CLI command). Edge cases, such as what happens when `full_refresh` is not set in the model config, are also not addressed in the problem statement, though the code changes provide some insight. Overall, while the core issue is understandable, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to a single function (`_is_incremental`) in one file (`run.py`). The modification involves a small number of lines (around 10 lines changed), focusing on adjusting the logic for determining whether a model should be treated as incremental. There is no indication of broader architectural impact or changes across multiple modules, making the scope limited and manageable.\n\n2. **Clarity and Complexity of Logic**: The logic in the code change is straightforward\u2014prioritizing the model-level `full_refresh` config over the CLI flag when determining if a model is incremental. It involves basic conditional checks and does not require complex algorithms or data structures. The change is a direct fix to enforce the desired precedence rule.\n\n3. **Technical Concepts Involved**: Solving this requires understanding basic Python programming constructs (conditionals, attribute access), familiarity with the dbt-core codebase (specifically how model configurations and CLI arguments are accessed), and the concept of incremental vs. full refresh in the context of data modeling. These are relatively simple concepts for someone with moderate experience in Python and familiarity with dbt, though a newcomer might need to learn the domain-specific context of dbt configurations.\n\n4. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change handles the case where `full_refresh` is not set in the model config by falling back to the CLI flag. This is a minor addition to the logic but does not introduce significant complexity. No additional error handling is required beyond what is already in the code.\n\nOverall, this task requires understanding a specific piece of logic in the dbt-core codebase and making a targeted modification. It does not involve deep architectural changes, advanced language features, or complex domain knowledge beyond the basics of dbt. A score of 0.35 reflects an \"Easy\" problem that is slightly more involved than a trivial fix due to the need for domain context and careful handling of configuration precedence, but it remains well within the capabilities of a junior to mid-level developer with some guidance or documentation.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incorrect decoding of 65-bit integer\n### Description\n\nThere seems to be a case where 65 bit integers are improperly decoded. I discovered this in a large dynamically generated json file at seemingly random times using the msgspec lib until I tracked down and isolated an example (below). Both larger and smaller encoded integers do not seem to have any issue. I expected the value of `19933688932870350000` to be consistently encoded and decoded as such, but instead received the number `1482881526185800828` as the output. As I was trying to figure out what was going on, I tried using the standard library `json.loads()` and got the expected value out. When examining the binary representation of the number, I noticed the incorrect value was equivalent to the correct one, besides the leading 4 bits:\r\n\r\n| | Decimal | Binary value | Bits |\r\n| - | -------- | ------------ | ---- |\r\n| Input | 19933688932870350000 | 10001010010010100010000001110010000110100010010110011010001111100 | 65 |\r\n| Output (incorrect)  | 1482881526185800828 |  1010010010100010000001110010000110100010010110011010001111100 | 61 |\r\n\r\n\r\nHere is a minimally reproducible example:\r\n```python\r\nimport json\r\nimport msgspec\r\n\r\n# incorrect value\r\nencoded_json = msgspec.json.encode({\"test_value\": 19933688932870350000})\r\ndecoded_json =  msgspec.json.decode(encoded_json)\r\nprint(decoded_json)\r\n\r\n# correct value\r\ncomp_encoded_json = msgspec.json.encode({\"test_value\": 19933688932870350000})\r\ncomp_decoded_json = json.loads(comp_encoded_json)\r\nprint(comp_decoded_json)\r\n```\n", "patch": "diff --git a/msgspec/_core.c b/msgspec/_core.c\nindex caf60279..d07ef87d 100644\n--- a/msgspec/_core.c\n+++ b/msgspec/_core.c\n@@ -11649,6 +11649,7 @@ ms_post_decode_float(\n  *************************************************************************/\n \n #define ONE_E18 1000000000000000000ULL\n+#define ONE_E19_MINUS_ONE 9999999999999999999ULL\n \n static MS_NOINLINE PyObject *\n parse_number_fallback(\n@@ -11948,7 +11949,7 @@ parse_number_inline(\n                 (is_float) ||\n                 ((integer_end - integer_start) != 20) ||\n                 (*integer_start != '1') ||\n-                (mantissa <= ONE_E18)\n+                (mantissa <= ONE_E19_MINUS_ONE)\n             )\n         ) {\n             /* We overflowed. Redo parsing, truncating at 19 digits */\n", "instance_id": "jcrist__msgspec-751", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the issue with 65-bit integer decoding using the `msgspec` library. It includes a minimal reproducible example, expected versus actual output, and even a binary representation to illustrate the issue with the leading bits. However, there are minor ambiguities: the problem does not explicitly discuss potential edge cases beyond the specific 65-bit integer issue, nor does it clarify if this behavior is consistent across different environments or library versions. Additionally, constraints or requirements for the fix (e.g., performance considerations or compatibility) are not mentioned. Despite these minor gaps, the core issue and goal are well-defined, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is relatively small, confined to a single file (`_core.c`) and a specific logic adjustment in the number parsing logic. However, the change requires a deep understanding of the `msgspec` library's internal implementation, particularly how it handles large integer parsing and overflow conditions in C, which is a low-level and error-prone domain. The technical concepts involved include bit-level manipulation, integer overflow handling, and familiarity with C's inline parsing logic, which are moderately complex. Additionally, the problem touches on edge cases related to large integers (specifically around 65-bit values), and the fix must ensure that it does not introduce regressions for other integer sizes or parsing scenarios. While the code change itself is minimal (adjusting a constant and condition), understanding the context and validating the solution requires significant expertise in low-level programming and debugging. There are also potential performance implications in parsing logic that need to be considered, though not explicitly mentioned in the problem. Overall, this problem demands a solid grasp of the codebase's internals and careful handling of numerical edge cases, justifying a difficulty score of 0.65.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CKV2_AWS_5 fails to identify opensearch attachment\n**Describe the issue**\r\n\r\ncheckov does not detect when a security group is attached to an AWS OpenSearch instance.\r\n\r\nThe check ID is `CKV2_AWS_5`\r\n\r\n**Examples**\r\n\r\n[Here's an example of a PR with the failing checkout configuration in question.](https://github.com/HHS/simpler-grants-gov/pull/2341/files#diff-609600cbb65809fe1d58326675dd804822d0ca9645e49d442b5a4308b177d5ea)\r\n\r\n**Version:**\r\n - [Github Action version v12.2875.0](https://github.com/bridgecrewio/checkov-action/tree/v12.2875.0/)\r\n - [Checkov version v3.2.257](https://github.com/bridgecrewio/checkov/tree/3.2.257)\r\n\r\n**Additional context**\r\n\r\nI suspect that the problem is that `aws_opensearch_domain` needs to be added to this list: https://github.com/bridgecrewio/checkov/blob/main/checkov/terraform/checks/graph_checks/aws/SGAttachedToResource.yaml\n", "patch": "diff --git a/checkov/terraform/checks/graph_checks/aws/SGAttachedToResource.yaml b/checkov/terraform/checks/graph_checks/aws/SGAttachedToResource.yaml\nindex 8d5a2f9b0ae..690fd4d1892 100644\n--- a/checkov/terraform/checks/graph_checks/aws/SGAttachedToResource.yaml\n+++ b/checkov/terraform/checks/graph_checks/aws/SGAttachedToResource.yaml\n@@ -51,6 +51,8 @@ definition:\n         - aws_mwaa_environment\n         - aws_neptune_cluster\n         - aws_network_interface\n+        - aws_opensearch_domain\n+        - aws_opensearch_vpc_endpoint\n         - aws_quicksight_vpc_connection\n         - aws_rds_cluster\n         - aws_redshift_cluster\n", "instance_id": "bridgecrewio__checkov-6807", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Checkov fails to detect when a security group is attached to an AWS OpenSearch instance for the check ID CKV2_AWS_5. It provides context about the versions of the tools involved (GitHub Action and Checkov), links to an example PR with the failing configuration, and points to a specific file in the repository where the fix is likely needed. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior or output after the fix (e.g., what should Checkov report when a security group is attached to an OpenSearch instance?). Additionally, edge cases or specific constraints related to OpenSearch attachments are not mentioned. While the issue is understandable with the provided context, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The code change required is minimal and straightforward, involving the addition of two resource types (`aws_opensearch_domain` and `aws_opensearch_vpc_endpoint`) to a list in a YAML configuration file. This modification is isolated to a single file and does not require deep understanding of the codebase architecture or complex logic. No advanced programming concepts, algorithms, or domain-specific knowledge beyond basic familiarity with Terraform resource types and Checkov\u2019s configuration are needed. The scope of the change is extremely limited, with no impact on the system's architecture or interactions between modules. Additionally, there are no explicit edge cases or error handling requirements mentioned in the problem statement or evident in the code change. This task is essentially a simple configuration update, akin to fixing a typo or adding a constant, requiring minimal effort and expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ncsixel_as_rgba fails when sixel image height is greater than 13\n`ncsixel_as_rgba` fails when the height of the sixel image exceeds 13 pixels. as far as i can tell, the width is irrelevant -- i tested sixel images over 1000 pixels wide without issue.\r\n\r\nthe sixel images were mostly generated with `img2sixel`, but i also tested with random sixel images i found online, thinking maybe `img2sixel` was doing something outside of the sixel spec.\r\n\r\nbut it always fails when the height exceeds 13 pixels.\r\n\r\nwith `loglevel` set to `trace`, these are the relevant errors:\r\n\r\n```\r\nncsixel_as_rgba:59:expected octothorpe, got 63\r\nncvisual_from_sixel:803:failed converting sixel to rgba\r\nncvisual_from_file failed\r\n```\r\n\r\nand here is an example sixel image that triggers the bug (22x14)\r\n\r\n[alien14.txt](https://github.com/user-attachments/files/15784143/alien14.txt)\r\n\r\nthanks!\r\n\r\n---\r\n\r\n* `export | egrep 'LANG|LC_CTYPE|TERM'`\r\n```\r\ndeclare -x LANG=\"en_US.UTF-8\"\r\ndeclare -x TERM=\"xterm-256color\"\r\ndeclare -x XTERM_LOCALE=\"en_US.UTF-8\"\r\ndeclare -x XTERM_SHELL=\"/usr/bin/bash\"\r\ndeclare -x XTERM_VERSION=\"XTerm(392)\"\r\n```\r\n* notcurses version (available from `notcurses-demo i`)\r\n`3.0.9`\r\n* terminal name + version\r\n`XTerm(392)`\r\n\r\n\nncsixel_as_rgba: incorrect state is set following '-'\noi, there seems to be another bug in `ncsixel_as_rgba` !\r\n\r\n```\r\n      }else if(*sx == '$'){\r\n        x = 0;\r\n        state = STATE_WANT_DATA;\r\n      }else if(*sx == '-'){\r\n        x = 0;\r\n        y += 6;\r\n        state = STATE_WANT_HASH;\r\n      }else{\r\n```\r\n\r\nsetting the state to `STATE_WANT_DATA` after the `-` command fixed it for me\r\n\r\nor more concisely, since the state is already `STATE_WANT_DATA` at this point\r\n\r\n```\r\n      }else if(*sx == '$'){\r\n        x = 0;\r\n      }else if(*sx == '-'){\r\n        x = 0;\r\n        y += 6;\r\n      }else{\r\n```\r\n\r\nthe following sixel images will trigger the bug:\r\n\r\n[laughing.txt](https://github.com/user-attachments/files/15930780/laughing.txt)\r\n[red40.txt](https://github.com/user-attachments/files/15930781/red40.txt)\r\n\r\nbeen using `notcurses` for a little while now -- really great project :)\n", "patch": "diff --git a/src/lib/sixel.h b/src/lib/sixel.h\nindex b95a18379..67f13e588 100644\n--- a/src/lib/sixel.h\n+++ b/src/lib/sixel.h\n@@ -161,7 +161,7 @@ uint32_t* ncsixel_as_rgba(const char *sx, unsigned leny, unsigned lenx){\n       }else if(*sx == '-'){\n         x = 0;\n         y += 6;\n-        state = STATE_WANT_HASH;\n+        state = STATE_WANT_DATA;\n       }else{\n //fprintf(stderr, \"RLE: %d pos: %d x %d\\n\", rle, y, x);\n         if(y + 6 > (leny + 5) / 6 * 6){\n", "instance_id": "dankamongmen__notcurses-2788", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue with `ncsixel_as_rgba` failing for sixel images with a height greater than 13 pixels. It provides specific error messages, example files that trigger the bug, and relevant environment details (e.g., notcurses version, terminal information). The user also identifies a potential fix by suggesting a state change in the code, which aligns with the provided code changes. However, there are minor ambiguities: the problem statement does not fully explain the expected behavior of the sixel parsing logic or the broader context of the state machine in `ncsixel_as_rgba`. Additionally, while example files are provided, there is no detailed explanation of the sixel format or spec compliance, which could be critical for understanding the root cause. Edge cases beyond height > 13 are not explicitly discussed. Overall, the statement is valid and clear but lacks some deeper context and minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal, involving a single line modification in the `ncsixel_as_rgba` function within one file (`sixel.h`). It does not impact the broader architecture or require changes across multiple modules. The fix simply adjusts the state transition after encountering a '-' character, which is a straightforward update.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of state machines and parsing logic, as the issue relates to incorrect state handling in a sixel image parser. Familiarity with C (given the codebase context) and debugging skills are necessary, but no advanced algorithms, design patterns, or domain-specific knowledge beyond basic image format parsing are needed. The concept of state transitions is relatively simple for an experienced developer.\n\n3. **Edge Cases and Error Handling**: The problem statement highlights a specific failure case (height > 13 pixels) and provides example inputs to reproduce the bug. The fix does not appear to introduce new error handling logic or require extensive consideration of additional edge cases beyond the reported issue. However, a developer might need to verify if the state change impacts other sixel parsing scenarios, which adds a small layer of complexity.\n\n4. **Overall Complexity**: The issue is a bug fix rather than a feature addition or architectural change. The provided solution is concise, and the problem does not demand deep knowledge of the entire codebase or complex interactions between components. It is a localized issue with a clear root cause (incorrect state setting after '-'), making it accessible to developers with moderate experience.\n\nA score of 0.30 reflects an \"Easy\" problem that requires understanding some code logic (state machine behavior) and making a simple modification, with minimal impact on the broader system. It is slightly above the lower end of the range due to the need to understand the context of sixel parsing and validate the fix against potential side effects.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CI: ansible_tests.py sets incorrect, world readable/writable, permissions on SSH key mitogen__has_sudo_pubkey.key\nThe wrong base is used when calculating the mode in\r\nhttps://github.com/mitogen-hq/mitogen/blob/a35b208acdb0d6f8a9dcf3e8a0db407a59679499/.ci/ansible_tests.py#L42-L44\r\n\r\nit should be base 8. The correct permission is `u=rw,g=,o=`, the actual result is `0o446` -> `u=r,g=r,o=rw.`\r\nThis key is only used during CI jobs, so I don't think this bug results in a security vulnerability of mitogen. The incorrect permission was introduced in commit https://github.com/mitogen-hq/mitogen/commit/06ae59702c52fc3d9ed554010a260d45733d9012#diff-2b989045c3f414fc662f679fc29531d94bfb5f614f793af91913c5dfa6b2c578R32-R34 on Aug 18 2018 when the script was converted from Bash to Python.\n", "patch": "diff --git a/docs/changelog.rst b/docs/changelog.rst\nindex d579544f3..85ab9d2f5 100644\n--- a/docs/changelog.rst\n+++ b/docs/changelog.rst\n@@ -21,6 +21,8 @@ To avail of fixes in an unreleased version, please download a ZIP file\n In progress (unreleased)\n ------------------------\n \n+* :gh:issue:`1182` CI: Fix incorrect world readable/writable file permissions\n+  on SSH key ``mitogen__has_sudo_pubkey.key`` during Ansible tests.\n \n \n v0.3.16 (2024-11-05)\n", "instance_id": "mitogen-hq__mitogen-1183", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: incorrect file permissions are being set on an SSH key file due to a miscalculation of the mode in a specific script. It identifies the exact location of the problematic code, the incorrect permission value (0o446), the expected permission (u=rw,g=,o=), and even traces the origin of the bug to a specific commit. However, there are minor ambiguities that prevent it from being comprehensive. For instance, it does not explicitly mention the expected correct mode value in octal (though it can be inferred), and there are no details about potential side effects or specific test cases to validate the fix. Additionally, while it states the issue is limited to CI jobs and likely not a security vulnerability, it lacks deeper context about the environment or usage of the key that could aid in understanding the full scope. Overall, the statement is clear enough to act upon but misses some finer details.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it appears to involve a straightforward fix. Based on the problem statement, the issue is isolated to a specific part of the codebase (a single line or small block in ansible_tests.py) where the file mode calculation uses the wrong base (should be base 8 for octal). The required change likely involves correcting a parameter or literal value in the code to set the proper permissions. The code changes provided only show an update to the changelog, not the actual fix in the Python script, but the scope of the modification seems minimal and confined to a single file. No complex interactions with other parts of the codebase or architectural changes are implied. The technical concepts involved are basic\u2014understanding file permissions in Unix-like systems and how they are represented in Python (e.g., using octal notation with os.chmod or similar). There are no significant edge cases or error handling requirements mentioned beyond ensuring the correct permission is applied. This task requires minimal effort and expertise, suitable for a junior developer or someone with basic familiarity with Python and file system operations.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Build not working with Python 3.13.0b1\nBuilding a wheel for `marisa-trie` with Python 3.13.0b1 doesn't work, because there are no `_PyLong_AsByteArray()` and `_PyLong_FromByteArray()` functions anymore.\r\n\r\nThe is a CPython issue on this topic (https://github.com/python/cpython/issues/111140), which resulted in alternative public functions being available, so I guess supporting Python 3.13 with `marisa-trie` should be pretty straight-forward.\r\n\r\nHere is a log of a failed build with Python 3.13.0b1: [build-log-marisa-trie-python-3.13.txt](https://github.com/user-attachments/files/15585389/build-log-marisa-trie-python-3.13.txt)\r\n\n", "patch": "", "instance_id": "pytries__marisa-trie-105", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the build for `marisa-trie` fails with Python 3.13.0b1 due to the removal of `_PyLong_AsByteArray()` and `_PyLong_FromByteArray()` functions in CPython. It also references a related CPython issue and suggests that alternative public functions are available, implying a straightforward fix. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly mention which alternative functions should be used or provide any guidance on where in the codebase the changes are needed. Additionally, edge cases or potential compatibility issues with older Python versions are not addressed. While a build log is provided, it is not embedded or summarized in the statement, requiring extra effort to analyze. Overall, the goal is clear, but the lack of specific technical details or constraints prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is likely limited to specific parts of the codebase dealing with Python's C API for integer serialization, probably within a single or a few files related to the `marisa-trie` library's Python bindings. However, since no code changes are provided in the input, I assume the modification involves replacing calls to the deprecated functions with their alternatives, which requires understanding the CPython API changes for Python 3.13. This introduces a moderate level of complexity, as it involves familiarity with low-level C extensions for Python and potentially navigating the `marisa-trie` codebase to locate the relevant sections. The technical concepts required include knowledge of Python's C API, specifically how integer objects are handled in memory, and possibly some compatibility considerations for older Python versions. Edge cases might include handling large integers or ensuring backward compatibility, though these are not explicitly mentioned in the problem statement. The changes are unlikely to impact the broader system architecture, as this appears to be a targeted compatibility fix rather than a structural overhaul. Overall, this problem requires a moderate understanding of specific technical concepts and targeted code modifications, justifying a difficulty score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`column_names` are taken partially when `use_columns` does not include all columns\nGiven this worksheet data (without any empty area):\r\n\r\n| A   | B   | C   |\r\n| --- | --- | --- |\r\n| 21  | 22  | 23  |\r\n| 31  | 32  | 33  |\r\n| 41  | 42  | 43  |\r\n\r\nThe following code:\r\n\r\n```python\r\nimport fastexcel as fe\r\n\r\n\r\nfile = r'<path to file>'\r\nparams = {'idx_or_name': 0, 'header_row': None, 'skip_rows': 1, 'use_columns': [1, 2], 'column_names': ['Col B', 'Col C']}\r\n\r\nprint(fe.read_excel(file).load_sheet(**params).to_polars())\r\n```\r\n\r\nOutputs:\r\n\r\n```python\r\nshape: (3, 2)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Col C \u2506 __UNNAMED__2 \u2502\r\n\u2502 ---   \u2506 ---          \u2502\r\n\u2502 f64   \u2506 f64          \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 22.0  \u2506 23.0         \u2502\r\n\u2502 32.0  \u2506 33.0         \u2502\r\n\u2502 42.0  \u2506 43.0         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nExpected output:\r\n\r\n```python\r\nshape: (3, 2)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Col B \u2506 Col C \u2502\r\n\u2502 ---   \u2506 ---   \u2502\r\n\u2502 f64   \u2506 f64   \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 22.0  \u2506 23.0  \u2502\r\n\u2502 32.0  \u2506 33.0  \u2502\r\n\u2502 42.0  \u2506 43.0  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n", "patch": "diff --git a/src/types/python/excelsheet/column_info.rs b/src/types/python/excelsheet/column_info.rs\nindex d26882e..8e502ac 100644\n--- a/src/types/python/excelsheet/column_info.rs\n+++ b/src/types/python/excelsheet/column_info.rs\n@@ -182,7 +182,7 @@ impl ColumnInfo {\n     }\n \n     pub fn __repr__(&self) -> String {\n-        format!(\"ColumnInfo<name=\\\"{name}\\\", index={index}, dtype=\\\"{dtype}\\\", dtype_from=\\\"{dtype_from}\\\", column_name_from=\\\"{column_name_from}\\\" >\", name=self.name, index=self.index, dtype=self.dtype.to_string(), dtype_from=self.dtype_from.to_string(), column_name_from=self.column_name_from.to_string())\n+        format!(\"ColumnInfo(name=\\\"{name}\\\", index={index}, dtype=\\\"{dtype}\\\", dtype_from=\\\"{dtype_from}\\\", column_name_from=\\\"{column_name_from}\\\" )\", name=self.name, index=self.index, dtype=self.dtype.to_string(), dtype_from=self.dtype_from.to_string(), column_name_from=self.column_name_from.to_string())\n     }\n \n     pub fn __eq__(&self, other: &Self) -> bool {\ndiff --git a/src/types/python/excelsheet/mod.rs b/src/types/python/excelsheet/mod.rs\nindex 4351935..4792891 100644\n--- a/src/types/python/excelsheet/mod.rs\n+++ b/src/types/python/excelsheet/mod.rs\n@@ -326,9 +326,6 @@ impl ExcelSheet {\n         selected_columns: SelectedColumns,\n         dtypes: Option<DTypeMap>,\n     ) -> FastExcelResult<Self> {\n-        // Ensuring dtypes are compatible with selected columns\n-        // Self::validate_dtypes_and_selected_columns(&selected_columns, &dtypes)?;\n-\n         let mut sheet = ExcelSheet {\n             name,\n             header,\n@@ -344,7 +341,7 @@ impl ExcelSheet {\n             selected_columns: Vec::with_capacity(0),\n         };\n \n-        let available_columns_info = sheet.get_available_columns_info();\n+        let available_columns_info = sheet.get_available_columns_info(&selected_columns)?;\n \n         let mut aliased_available_columns = Vec::with_capacity(available_columns_info.len());\n \n@@ -379,10 +376,13 @@ impl ExcelSheet {\n         Ok(sheet)\n     }\n \n-    fn get_available_columns_info(&self) -> Vec<ColumnInfoBuilder> {\n+    fn get_available_columns_info(\n+        &self,\n+        selected_columns: &SelectedColumns,\n+    ) -> FastExcelResult<Vec<ColumnInfoBuilder>> {\n         let width = self.data.width();\n         match &self.header {\n-            Header::None => (0..width)\n+            Header::None => Ok((0..width)\n                 .map(|col_idx| {\n                     ColumnInfoBuilder::new(\n                         format!(\"__UNNAMED__{col_idx}\"),\n@@ -390,8 +390,8 @@ impl ExcelSheet {\n                         ColumnNameFrom::Generated,\n                     )\n                 })\n-                .collect(),\n-            Header::At(row_idx) => (0..width)\n+                .collect()),\n+            Header::At(row_idx) => Ok((0..width)\n                 .map(|col_idx| {\n                     self.data\n                         .get((*row_idx, col_idx))\n@@ -407,23 +407,73 @@ impl ExcelSheet {\n                             )\n                         })\n                 })\n-                .collect(),\n+                .collect()),\n             Header::With(names) => {\n-                let nameless_start_idx = names.len();\n-                names\n-                    .iter()\n-                    .enumerate()\n-                    .map(|(col_idx, name)| {\n-                        ColumnInfoBuilder::new(name.to_owned(), col_idx, ColumnNameFrom::Provided)\n-                    })\n-                    .chain((nameless_start_idx..width).map(|col_idx| {\n-                        ColumnInfoBuilder::new(\n-                            format!(\"__UNNAMED__{col_idx}\"),\n-                            col_idx,\n-                            ColumnNameFrom::Generated,\n+                if let SelectedColumns::Selection(column_selection) = selected_columns {\n+                    if column_selection.len() != names.len() {\n+                        return Err(FastExcelErrorKind::InvalidParameters(\n+                            \"column_names and use_columns must have the same length\".to_string(),\n                         )\n-                    }))\n-                    .collect()\n+                        .into());\n+                    }\n+                    let selected_indices = column_selection\n+                        .iter()\n+                        .map(|idx_or_name| {\n+                            match idx_or_name {\n+                        IdxOrName::Idx(idx) => Ok(*idx),\n+                        IdxOrName::Name(name) => Err(FastExcelErrorKind::InvalidParameters(\n+                            format!(\"use_columns can only contain integers when used with columns_names, got \\\"{name}\\\"\")\n+                        )\n+                        .into()),\n+                    }\n+                        })\n+                        .collect::<FastExcelResult<Vec<_>>>()?;\n+\n+                    Ok((0..width)\n+                        .map(|col_idx| {\n+                            let provided_name_opt = if let Some(pos_in_names) =\n+                                selected_indices.iter().position(|idx| idx == &col_idx)\n+                            {\n+                                names.get(pos_in_names).cloned()\n+                            } else {\n+                                None\n+                            };\n+\n+                            match provided_name_opt {\n+                                Some(provided_name) => ColumnInfoBuilder::new(\n+                                    provided_name,\n+                                    col_idx,\n+                                    ColumnNameFrom::Provided,\n+                                ),\n+                                None => ColumnInfoBuilder::new(\n+                                    format!(\"__UNNAMED__{col_idx}\"),\n+                                    col_idx,\n+                                    ColumnNameFrom::Generated,\n+                                ),\n+                            }\n+                        })\n+                        .collect())\n+                } else {\n+                    let nameless_start_idx = names.len();\n+                    Ok(names\n+                        .iter()\n+                        .enumerate()\n+                        .map(|(col_idx, name)| {\n+                            ColumnInfoBuilder::new(\n+                                name.to_owned(),\n+                                col_idx,\n+                                ColumnNameFrom::Provided,\n+                            )\n+                        })\n+                        .chain((nameless_start_idx..width).map(|col_idx| {\n+                            ColumnInfoBuilder::new(\n+                                format!(\"__UNNAMED__{col_idx}\"),\n+                                col_idx,\n+                                ColumnNameFrom::Generated,\n+                            )\n+                        }))\n+                        .collect())\n+                }\n             }\n         }\n     }\n", "instance_id": "ToucanToco__fastexcel-217", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear, providing a specific issue with the `column_names` not being applied correctly when `use_columns` does not include all columns. It includes a concrete example with input data, code snippet, actual output, and expected output, which helps in understanding the goal. However, there are minor ambiguities: the problem does not explicitly discuss edge cases (e.g., what happens if `use_columns` and `column_names` have mismatched lengths or invalid indices) or constraints on the input parameters. Additionally, it lacks context about the broader system or library (`fastexcel`) behavior, which might be necessary for a complete understanding. Overall, it is clear enough to work on but misses some details that could prevent misinterpretation or incomplete solutions.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, primarily affecting a single module (`excelsheet/mod.rs`) with significant logic modifications in the `get_available_columns_info` function, while the change in `column_info.rs` is trivial (formatting in `__repr__`). The core change involves understanding and modifying how column names are mapped based on `use_columns` and `column_names`, which requires a good grasp of the existing codebase logic, particularly around `SelectedColumns` and `Header` handling. Second, the technical concepts involved include Rust-specific features like error handling with custom result types (`FastExcelResult`), iterators, and pattern matching, as well as domain-specific logic related to Excel sheet parsing and column indexing. Third, the problem introduces complexity in handling edge cases, such as validating the length match between `use_columns` and `column_names`, and ensuring correct mapping of provided names to selected indices, which is evident in the added error checking logic. However, it does not impact the broader system architecture or require advanced algorithms, keeping it from being classified as hard. Overall, this requires a moderate level of expertise in Rust and the specific library's internals, along with careful consideration of input validation, making it a medium-difficulty task.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deploy Bacalhau using Docker Compose\n### Is your feature request related to a problem? Please describe.\r\n\r\nSince Bacalhau is a distributed system with multiple components, it is critical to have a reliable method for end-to-end testing. Also, it's important that these tests closely resemble a real production environment without relying on mocks. \r\n\r\nAdditionally, it is important to have an easy way for new users of Bacalhau to quickly experiment with the system on their own local machines, without the need, or the complexity, or the overhead to deploy on a public cloud.\r\n\r\n---\r\n### Describe the solution you'd like\r\n\r\nThe suggested solution is to deploy different components of Bacalhau using Docker Compose, while utilizing the Docker-in-Docker capability. \r\n\r\nThis solution:\r\n1. Allows new users to standup a full deployment of Bacalhau on their own machines within seconds, and experiment with it. \r\n2. Gives the ability to write Jobs that uses S3 Input/Output sources by utilizing the Minio component deployed.\r\n3. Removes the need to pull/push images from Docker Hub, and instead utilize the Image Registry component deployed.\r\n4. Allows to easily and quickly run tests against a real Bacalhau deployment, and quickly iterate and test different setups and scenarios. Please note that Docker compose itself can be put inside a container (using Docker-in-Docker)\r\n5. By utilizing Docker networks, it allows us to simulate different scenarios of network failures, latency, and segmentation between Compute Nodes and Requester Nodes.\r\n6. Easily scale to multiple requester and compute nodes.\r\n7. Allows Bacalhau developers to easily run tests locally before pushing their commits.\r\n8. Paves the road for Fuzz testing \r\n9. Paves the way for easier testing TLS NATS communication (authentication and authorization)\r\n\r\n---\r\n### Describe alternatives you've considered\r\nAlternatives would be to standup different processes on a single host, where each process is a different component of Bacalhau\r\n\r\n---\r\n### Additional context**\r\n\r\nFew TODOs to keep in mind for later:\r\n1. Utilize [Sysbox](https://github.com/nestybox/sysbox)\r\n2. Think about Fuzz Tests and best to integrate them\r\n3. IPFS support\r\n4. Think about how to increase the number or requester nodes and compute nodes when needed to test different scenarios.\r\n5. Utilize TestContainers when building the test suite around this setup\nDeploy Bacalhau using Docker Compose\n### Is your feature request related to a problem? Please describe.\r\n\r\nSince Bacalhau is a distributed system with multiple components, it is critical to have a reliable method for end-to-end testing. Also, it's important that these tests closely resemble a real production environment without relying on mocks. \r\n\r\nAdditionally, it is important to have an easy way for new users of Bacalhau to quickly experiment with the system on their own local machines, without the need, or the complexity, or the overhead to deploy on a public cloud.\r\n\r\n---\r\n### Describe the solution you'd like\r\n\r\nThe suggested solution is to deploy different components of Bacalhau using Docker Compose, while utilizing the Docker-in-Docker capability. \r\n\r\nThis solution:\r\n1. Allows new users to standup a full deployment of Bacalhau on their own machines within seconds, and experiment with it. \r\n2. Gives the ability to write Jobs that uses S3 Input/Output sources by utilizing the Minio component deployed.\r\n3. Removes the need to pull/push images from Docker Hub, and instead utilize the Image Registry component deployed.\r\n4. Allows to easily and quickly run tests against a real Bacalhau deployment, and quickly iterate and test different setups and scenarios. Please note that Docker compose itself can be put inside a container (using Docker-in-Docker)\r\n5. By utilizing Docker networks, it allows us to simulate different scenarios of network failures, latency, and segmentation between Compute Nodes and Requester Nodes.\r\n6. Easily scale to multiple requester and compute nodes.\r\n7. Allows Bacalhau developers to easily run tests locally before pushing their commits.\r\n8. Paves the road for Fuzz testing \r\n9. Paves the way for easier testing TLS NATS communication (authentication and authorization)\r\n\r\n---\r\n### Describe alternatives you've considered\r\nAlternatives would be to standup different processes on a single host, where each process is a different component of Bacalhau\r\n\r\n---\r\n### Additional context**\r\n\r\nFew TODOs to keep in mind for later:\r\n1. Utilize [Sysbox](https://github.com/nestybox/sysbox)\r\n2. Think about Fuzz Tests and best to integrate them\r\n3. IPFS support\r\n4. Think about how to increase the number or requester nodes and compute nodes when needed to test different scenarios.\r\n5. Utilize TestContainers when building the test suite around this setup\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex ee7d6b8d4a..ee46828daa 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -8,7 +8,7 @@ repos:\n   - id: detect-aws-credentials\n     args: [--allow-missing-credentials]\n   - id: detect-private-key\n-    exclude: testdata/.*\n+    exclude: 'testdata/.*|test-integration/certificates/.*'\n   - id: check-yaml\n   - id: check-json\n - repo: https://github.com/astral-sh/ruff-pre-commit\n", "instance_id": "bacalhau-project__bacalhau-4596", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of deploying Bacalhau using Docker Compose to facilitate local experimentation and end-to-end testing in a production-like environment. It outlines the motivations (e.g., ease of use for new users, testing without mocks, simulating network conditions) and provides a detailed list of benefits and future considerations (e.g., scaling nodes, fuzz testing, TLS NATS communication). The solution's intent and scope are well-articulated, covering multiple components like Minio for S3 compatibility and Docker-in-Docker setups. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not specify the exact components of Bacalhau to be deployed via Docker Compose, nor does it detail the expected interactions between these components. Constraints or limitations (e.g., system requirements for running such a setup, potential compatibility issues with Docker-in-Docker) are not mentioned. Additionally, while edge cases like network failures are alluded to, there are no concrete examples or specific scenarios to address. Overall, the statement is valid and clear but lacks some finer details for complete comprehension.", "difficulty_explanation": "The difficulty score of 0.75 reflects a hard problem that requires a deep understanding of multiple technical domains and complex modifications, though it falls short of the most extreme challenges. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change in the diff is minimal, merely updating a pre-commit hook exclusion pattern to include integration test certificates. However, this is likely just a small part of the overall implementation. The problem statement implies a much broader scope, involving the creation of a comprehensive Docker Compose setup for a distributed system like Bacalhau. This would require configuring multiple services (e.g., compute nodes, requester nodes, Minio, image registry), defining Docker networks for simulation of failures/latency, and integrating Docker-in-Docker capabilities. Such changes would span multiple files (e.g., docker-compose.yml, custom Dockerfiles, test scripts) and potentially impact the system's architecture by introducing a new deployment paradigm. Understanding and modifying interactions between Bacalhau components in this containerized setup adds significant complexity.\n\n2. **Number of Technical Concepts:** Solving this problem demands familiarity with several advanced concepts: Docker Compose for multi-container orchestration, Docker-in-Docker for nested containerization, Docker networking for simulating real-world conditions, and distributed systems knowledge to ensure Bacalhau's components (e.g., compute and requester nodes) interact correctly. Additionally, domain-specific knowledge of Bacalhau's architecture, S3 compatibility via Minio, and container registry management is required. Future considerations like fuzz testing, TLS NATS communication, and IPFS support further hint at the need for specialized skills. These concepts are moderately to highly complex, especially when combined in a cohesive solution.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement mentions simulating network failures, latency, and segmentation, which inherently involves handling edge cases related to connectivity and performance. Error handling for Docker-in-Docker setups (e.g., resource constraints, permission issues) and ensuring robustness across scaled nodes (e.g., requester and compute nodes) will be necessary. While specific edge cases aren't detailed, the nature of distributed systems testing implies non-trivial error conditions like node failures, data consistency issues, and security concerns (e.g., TLS authentication). Addressing these will require careful design and implementation.\n\n4. **Overall Complexity:** While the provided code diff is trivial, the actual implementation of the described solution is far more involved. It requires not just coding but also system design skills to architect a local deployment that mirrors production. The need to balance user-friendliness (for new users) with robustness (for testing) adds to the challenge. Performance considerations (e.g., resource usage in Docker-in-Docker) and future scalability (e.g., increasing node counts) further elevate the difficulty. This problem sits in the 'hard' range due to the depth of understanding required and the potential impact on the development and testing workflow, though it doesn't reach 'very hard' as it doesn't involve groundbreaking algorithmic innovation or system-level kernel modifications.\n\nIn summary, this task is challenging due to the breadth of technical knowledge required, the complexity of integrating multiple components in a containerized environment, and the need to handle distributed system edge cases, warranting a difficulty score of 0.75.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Return HTTP 410 for legacy API methods\nReturning a helpful message like:\r\n```\r\n{\r\n  \"message\": \"This endpoint is deprecated and no longer available. Please use /new-endpoint.\",\r\n  \"new_parameters\": \"Refer to the new API documentation for updated parameters.\"\r\n}\r\n```\r\nwhen a deprecated method is called.\n", "patch": "diff --git a/pkg/publicapi/endpoint/requester/endpoint.go b/pkg/publicapi/endpoint/requester/endpoint.go\nindex ae0331da99..3c2f9adfa9 100644\n--- a/pkg/publicapi/endpoint/requester/endpoint.go\n+++ b/pkg/publicapi/endpoint/requester/endpoint.go\n@@ -1,15 +1,19 @@\n package requester\n \n import (\n+\t\"fmt\"\n+\t\"net/http\"\n+\t\"os\"\n \t\"sync\"\n \n+\t\"github.com/gorilla/websocket\"\n+\t\"github.com/labstack/echo/v4\"\n+\n \t\"github.com/bacalhau-project/bacalhau/pkg/jobstore\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/model\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/orchestrator\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/publicapi/middleware\"\n \t\"github.com/bacalhau-project/bacalhau/pkg/requester\"\n-\t\"github.com/gorilla/websocket\"\n-\t\"github.com/labstack/echo/v4\"\n )\n \n type EndpointParams struct {\n@@ -31,6 +35,8 @@ type Endpoint struct {\n \twebsocketsMutex sync.RWMutex\n }\n \n+const UseDeprecatedEndpointsForTesting = \"REQUESTER_ENDPOINT_USE_DEPRECATED_ENV\"\n+\n func NewEndpoint(params EndpointParams) *Endpoint {\n \te := &Endpoint{\n \t\trouter:             params.Router,\n@@ -43,15 +49,42 @@ func NewEndpoint(params EndpointParams) *Endpoint {\n \n \tg := e.router.Group(\"/api/v1/requester\")\n \tg.Use(middleware.SetContentType(echo.MIMEApplicationJSON))\n-\tg.POST(\"/list\", e.list)\n-\tg.GET(\"/nodes\", e.nodes)\n-\tg.POST(\"/states\", e.states)\n-\tg.POST(\"/results\", e.results)\n-\tg.POST(\"/events\", e.events)\n-\tg.POST(\"/submit\", e.submit)\n-\tg.POST(\"/cancel\", e.cancel)\n-\tg.POST(\"/debug\", e.debug)\n-\tg.GET(\"/websocket/events\", e.websocketJobEvents)\n+\tif key := os.Getenv(UseDeprecatedEndpointsForTesting); key != \"\" {\n+\t\tg.POST(\"/list\", e.list)\n+\t\tg.GET(\"/nodes\", e.nodes)\n+\t\tg.POST(\"/states\", e.states)\n+\t\tg.POST(\"/results\", e.results)\n+\t\tg.POST(\"/events\", e.events)\n+\t\tg.POST(\"/submit\", e.submit)\n+\t\tg.POST(\"/cancel\", e.cancel)\n+\t\tg.POST(\"/debug\", e.debug)\n+\t\tg.GET(\"/websocket/events\", e.websocketJobEvents)\n+\t\treturn e\n+\t}\n \n+\tregisterDeprecatedLegacyMethods(g)\n \treturn e\n }\n+\n+// registerDeprecatedLegacyMethods registers routes on the router that are 'Gone'.\n+func registerDeprecatedLegacyMethods(group *echo.Group) {\n+\t// Legacy API Endpoints\n+\t// All return status 410 https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/410\n+\tgroup.POST(\"/list\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#list-jobs\"))\n+\tgroup.GET(\"/nodes\", methodGone(\"https://docs.bacalhau.org/references/api/nodes#list-nodes\"))\n+\tgroup.POST(\"/states\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#describe-job\"))\n+\tgroup.POST(\"/results\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#describe-job\"))\n+\tgroup.POST(\"/events\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#job-history\"))\n+\tgroup.POST(\"/submit\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#create-job\"))\n+\tgroup.POST(\"/cancel\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#stop-job\"))\n+\tgroup.POST(\"/debug\", methodGone(\"https://docs.bacalhau.org/references/api/nodes#describe-node\"))\n+\tgroup.GET(\"/websocket/events\", methodGone(\"https://docs.bacalhau.org/references/api/jobs#job-history\"))\n+}\n+\n+const deprecationMessage = \"This endpoint is deprecated and no longer available. Please refer to %s for more information. If you encountered this error using the Bacalhau client or CLI, please update your node by following the instructions here: https://docs.bacalhau.org/getting-started/installation\" //nolint:lll\n+\n+func methodGone(docsLink string) func(c echo.Context) error {\n+\treturn func(c echo.Context) error {\n+\t\treturn c.JSON(http.StatusGone, fmt.Sprintf(deprecationMessage, docsLink))\n+\t}\n+}\n", "instance_id": "bacalhau-project__bacalhau-4109", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to return an HTTP 410 status code for deprecated API methods along with a helpful message directing users to a new endpoint. It specifies the desired output format (a JSON response with a message and additional information). However, there are minor ambiguities and missing details. For instance, it does not explicitly mention whether all legacy endpoints should return the same message or if the message should be customized per endpoint (though the code changes suggest customization via a documentation link). Additionally, there are no explicit mentions of edge cases, constraints, or specific requirements for handling different HTTP methods or client behaviors. While the goal is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to a single file (`endpoint.go`) and primarily involve adding a conditional check for an environment variable and registering deprecated endpoints with a uniform response handler. The modifications do not impact the broader system architecture and are relatively small in terms of lines of code (adding a new function and updating endpoint registration logic). There is no need to understand complex interactions across multiple modules.\n\n2. **Technical Concepts Involved**: The solution requires basic familiarity with Go, specifically with the `net/http` package for HTTP status codes, the `os` package for environment variable checks, and the `echo` framework for routing and response handling. These are straightforward concepts for anyone with moderate experience in Go. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce complex error handling. The implementation simply returns a static HTTP 410 response for deprecated endpoints, with no additional logic for handling unexpected inputs or client behaviors. The use of an environment variable to toggle deprecated endpoints introduces a minor consideration for testing scenarios, but this is not particularly challenging.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase (the endpoint registration logic) and making simple modifications to return a predefined response for specific routes. While it requires some understanding of the routing framework (`echo`), the changes are mechanical and do not involve deep architectural or performance considerations.\n\nA score of 0.30 reflects that this is an easy task requiring minimal effort beyond basic code modifications and a surface-level understanding of the HTTP routing logic in the codebase.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug][move-compiler-v2] Move allows `_` to be used on RHS, and treats `_` parameters as variables\n# \ud83d\udc1b Bug\r\n\r\nWhile thinking about using `_` as a placeholder for some sort of curry operation, I found some strangeness in the way this special variable is handled in both V1 and V2.\r\n\r\n- The value of a function parameter called `_` can be used.\r\n- Having more than one function parameter called `_` leads to a \"duplicate parameter\" warning.\r\n- `let` statements and assignments to `_` do not produce errors, but have no effect:\r\n    - The code `let _ = 3; _` leads to an unbound/undeclared compile-time error in both compiler V1 and V2.\r\n    - The program below shows assignments to `_` which are seen to have no effect on the value of `_` (which is obtained from the parameter, apparently not really the same thing as the LocalVar).  Essentially, assignment to `_` discards a value (as in Rust, actually).\r\n\r\nFor comparison, Rust says:\r\n```\r\nerror: in expressions, `_` can only be used on the left-hand side of an assignment\r\n --> src/main.rs:4:5\r\n  |\r\n4 |     _\r\n  |     ^ `_` not allowed here\r\n\r\n```\r\nand Rust ignores parameters called `_`, rather than generating errors due to multiple such parameters.\r\n\r\n## Action\r\n\r\nIn Move Language V2 we should change this behavior to be more consistent:\r\n- Disallow taking the value of `_`\r\n- Ignore multiple parameters named `_`\r\n\r\nThis will free up `_` so we can use it for currying.\r\n\r\n## Reproducing example\r\n\r\nThe below program demonstrates several of these problems:\r\n```\r\n//# publish\r\nmodule 0xc0ffee::m {\r\n    fun test2(): (u32, u64) {\r\n        (1u32, 2u64)\r\n    }\r\n    \r\n    public fun test(_: u64): u64 {\r\n        let x = _ + 3;\r\n        x + _\r\n    }\r\n\r\n    public fun test3(_: u64): u64 {\r\n        let (_, _) = test2();\r\n        let x = _ + 3;\r\n        _ = _ + 1;\r\n        x + _\r\n    }\r\n\r\n    public fun test4(_: u64): u64 {\r\n        let (_, _x) = test2();\r\n        _ = _ + 2;\r\n        _\r\n    }\r\n\r\n    public fun test5(_: u64): u64 {\r\n        let (_, _) = test2();\r\n        _ = _ + 3;\r\n        _\r\n    }\r\n\r\n    public fun test6(_: u64): u64 {\r\n        let (_x, _) = test2();\r\n         _\r\n    }\r\n\r\n    // public fun test7(_y: u64): u64 {\r\n    //    let _ = _y;\r\n    //    _  // undefined\r\n    // }\r\n}\r\n\r\n//# run 0xc0ffee::m::test --args 4\r\n\r\n//# run 0xc0ffee::m::test3 --args 5\r\n\r\n//# run 0xc0ffee::m::test4 --args 5\r\n\r\n//# run 0xc0ffee::m::test5 --args 5\r\n\r\n//# run 0xc0ffee::m::test6 --args 5\r\n\r\n//# run 0xc0ffee::m::test7 --args 5\r\n\r\n```\r\n\r\nYields the following output, with no compiler errors (after `test7` was commented out):\r\n```\r\nprocessed 6 tasks\r\n\r\ntask 1 'run'. lines 37-37:\r\nreturn values: 11\r\n\r\ntask 2 'run'. lines 39-39:\r\nreturn values: 13\r\n\r\ntask 3 'run'. lines 41-41:\r\nreturn values: 5\r\n\r\ntask 4 'run'. lines 43-43:\r\nreturn values: 5\r\n\r\ntask 5 'run'. lines 45-45:\r\nreturn values: 5\r\n\r\n==> Compiler v2 delivered same results!\r\n```\r\n\r\n\n", "patch": "diff --git a/third_party/move/move-compiler-v2/src/env_pipeline/inliner.rs b/third_party/move/move-compiler-v2/src/env_pipeline/inliner.rs\nindex 77a4ce7eefede..5502f1d9694f4 100644\n--- a/third_party/move/move-compiler-v2/src/env_pipeline/inliner.rs\n+++ b/third_party/move/move-compiler-v2/src/env_pipeline/inliner.rs\n@@ -45,6 +45,7 @@ use log::{debug, trace};\n use move_model::{\n     ast::{Exp, ExpData, Operation, Pattern, Spec, SpecBlockTarget, TempIndex},\n     exp_rewriter::ExpRewriterFunctions,\n+    metadata::LanguageVersion,\n     model::{FunId, GlobalEnv, Loc, NodeId, Parameter, QualifiedId},\n     symbol::Symbol,\n     ty::{ReferenceKind, Type},\n@@ -833,7 +834,11 @@ impl<'env, 'rewriter> InlinedRewriter<'env, 'rewriter> {\n             .map(|param| {\n                 let Parameter(sym, ty, loc) = *param;\n                 let id = env.new_node(loc.clone(), ty.instantiate(self.type_args));\n-                if let Some(new_sym) = self.shadow_stack.get_shadow_symbol(*sym, true) {\n+                if env.language_version().is_at_least(LanguageVersion::V2_0)\n+                    && env.symbol_pool().string(*sym).as_ref() == \"_\"\n+                {\n+                    Pattern::Wildcard(id)\n+                } else if let Some(new_sym) = self.shadow_stack.get_shadow_symbol(*sym, true) {\n                     Pattern::Var(id, new_sym)\n                 } else {\n                     Pattern::Var(id, *sym)\ndiff --git a/third_party/move/move-model/src/builder/exp_builder.rs b/third_party/move/move-model/src/builder/exp_builder.rs\nindex 5fca751d2e0a2..7c7e49313931f 100644\n--- a/third_party/move/move-model/src/builder/exp_builder.rs\n+++ b/third_party/move/move-model/src/builder/exp_builder.rs\n@@ -677,23 +677,31 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         params: &[(PA::Var, EA::Type)],\n         for_move_fun: bool,\n     ) -> Vec<Parameter> {\n+        let is_lang_version_2 = self\n+            .env()\n+            .language_version\n+            .is_at_least(LanguageVersion::V2_0);\n         params\n             .iter()\n             .enumerate()\n             .map(|(idx, (v, ty))| {\n                 let ty = self.translate_type(ty);\n-                let sym = self.symbol_pool().make(v.0.value.as_str());\n+                let var_str = v.0.value.as_str();\n+                let sym = self.symbol_pool().make(var_str);\n                 let loc = self.to_loc(&v.loc());\n-                self.define_local(\n-                    &loc,\n-                    sym,\n-                    ty.clone(),\n-                    None,\n-                    // If this is for a proper Move function (not spec function), add the\n-                    // index so we can resolve this to a `Temporary` expression instead of\n-                    // a `LocalVar`.\n-                    if for_move_fun { Some(idx) } else { None },\n-                );\n+\n+                if !is_lang_version_2 || var_str != \"_\" {\n+                    self.define_local(\n+                        &loc,\n+                        sym,\n+                        ty.clone(),\n+                        None,\n+                        // If this is for a proper Move function (not spec function), add the\n+                        // index so we can resolve this to a `Temporary` expression instead of\n+                        // a `LocalVar`.\n+                        if for_move_fun { Some(idx) } else { None },\n+                    );\n+                }\n                 Parameter(sym, ty, loc)\n             })\n             .collect_vec()\ndiff --git a/third_party/move/move-model/src/builder/module_builder.rs b/third_party/move/move-model/src/builder/module_builder.rs\nindex 9f04fe84d1237..ad2da753e48ce 100644\n--- a/third_party/move/move-model/src/builder/module_builder.rs\n+++ b/third_party/move/move-model/src/builder/module_builder.rs\n@@ -1532,8 +1532,12 @@ impl<'env, 'translator> ModuleBuilder<'env, 'translator> {\n                 et.define_type_param(loc, *name, Type::new_param(pos), kind.clone(), false);\n             }\n             et.enter_scope();\n+            let is_lang_version_2 = et.env().language_version.is_at_least(LanguageVersion::V2_0);\n             for (idx, Parameter(n, ty, loc)) in params.iter().enumerate() {\n-                et.define_local(loc, *n, ty.clone(), None, Some(idx));\n+                let symbol_pool = et.parent.parent.env.symbol_pool();\n+                if !is_lang_version_2 || symbol_pool.string(*n).as_ref() != \"_\" {\n+                    et.define_local(loc, *n, ty.clone(), None, Some(idx));\n+                }\n             }\n             let access_specifiers = et.translate_access_specifiers(&def.access_specifiers);\n             let result = et.translate_seq(&loc, seq, &result_type, &ErrorMessageContext::Return);\n", "instance_id": "aptos-labs__aptos-core-14962", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the handling of the underscore (`_`) identifier in the Move language (V2). It provides a detailed explanation of the current behavior, contrasts it with Rust's behavior, and specifies the desired changes (disallowing the value of `_` to be used and ignoring multiple parameters named `_`). The inclusion of a reproducing example with code and output further aids in understanding the issue. However, there are minor ambiguities: the statement does not explicitly define all edge cases (e.g., how `_` should behave in nested scopes or complex expressions beyond the examples), and it lacks detailed guidance on how the currying feature (mentioned as a future goal) might influence the implementation. Additionally, the expected behavior for assignments to `_` in different contexts could be more explicitly defined. Overall, while the goal and key issues are clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes spans multiple files (`inliner.rs`, `exp_builder.rs`, `module_builder.rs`) within the Move compiler V2, indicating a need to understand and modify different components of the compiler's pipeline (expression translation, inlining, and module building). The changes involve conditional logic based on the language version (`LanguageVersion::V2_0`), requiring familiarity with the compiler's versioning system and symbol pool management. Second, the technical concepts involved include understanding compiler internals (symbol resolution, parameter handling, and pattern matching), as well as language-specific semantics of Move V2 compared to V1 and Rust. Third, while the problem statement does not explicitly mention complex edge cases beyond the provided examples, implementing these changes requires careful consideration of how `_` is handled in various contexts (e.g., as a parameter, in assignments, in pattern matching), which could introduce subtle bugs if not thoroughly tested. Finally, the impact of these changes is significant as they alter the language's core behavior, potentially affecting existing codebases and future features like currying. However, it does not reach the \"Very Hard\" range (0.8-1.0) as it does not involve system-level redesign or highly intricate domain-specific knowledge beyond compiler construction. A score of 0.65 reflects the need for a deep understanding of the Move compiler's architecture and careful handling of language semantics, but it remains within the realm of a focused bug fix rather than a complete overhaul.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "graphviz draw tooltip with special characters\n### Information\r\n\r\n- **rustworkx version**:  0.12.1\r\n- **Python version**: 3.9.13\r\n- **Rust version**: NA\r\n- **Operating system**: Windows 10\r\n\r\n### What is the current behavior?\r\n\r\nAdding a `\\n` or `:` character to the tooltip throws this error:\r\n\r\n```\r\nError: <stdin>: syntax error in line 4 near ']'      \r\nTraceback (most recent call last):\r\n  File main.py\", line 6, in <module>\r\n    graphviz_draw(\r\n  File \"venv\\lib\\site-packages\\rustworkx\\visualization\\graphviz.py\", line 199, in graphviz_draw\r\n    subprocess.run(\r\n  File \"AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 528, in run\r\n    raise CalledProcessError(retcode, process.args,  \r\nsubprocess.CalledProcessError: Command '['dot', '-T', 'svg', '-o', 'graph.svg']' returned non-zero exit status 1.\r\n```\r\n\r\n### What is the expected behavior?\r\nI would like the tooltip to support special characters such as `\\n` and `:`, like label does.\r\nIn http://magjac.com/graphviz-visual-editor/, I'm allowed to do that with:\r\n```\r\ndigraph {\r\n  label=\"Graph Label\"\r\n  Node1 [tooltip=\"Node1 Tooltip\\nlinebreak in tooltip: it's fine\"]\r\n}\r\n```\r\n\r\n### Steps to reproduce the problem\r\n```\r\nimport rustworkx as rx\r\nfrom rustworkx.visualization import graphviz_draw\r\n\r\ngraphviz_draw(\r\n    rx.generators.path_graph(2),\r\n    filename=\"graph.svg\",\r\n    image_type=\"svg\",\r\n    node_attr_fn=lambda x: {\"label\": \"the\\nlabel\", \"tooltip\": \"the\\ntooltip\"},\r\n)\r\n```\r\n\r\n\n", "patch": "diff --git a/releasenotes/notes/fix-graphviz-draw-tooltip-3f697d71c4b79e60.yaml b/releasenotes/notes/fix-graphviz-draw-tooltip-3f697d71c4b79e60.yaml\nnew file mode 100644\nindex 000000000..0d7b184c4\n--- /dev/null\n+++ b/releasenotes/notes/fix-graphviz-draw-tooltip-3f697d71c4b79e60.yaml\n@@ -0,0 +1,14 @@\n+---\n+fixes:\n+  - |\n+      :func:`.graphviz_draw` can now handle special characters\n+\n+          .. jupyter-execute::\n+\n+                import rustworkx as rx\n+                from rustworkx.visualization import graphviz_draw\n+\n+                graphviz_draw(\n+                    rx.generators.path_graph(2),\n+                    node_attr_fn=lambda x: {\"label\": \"the\\nlabel\", \"tooltip\": \"the\\ntooltip\"},\n+                )\ndiff --git a/src/dot_utils.rs b/src/dot_utils.rs\nindex 3411c2f3e..af1f708eb 100644\n--- a/src/dot_utils.rs\n+++ b/src/dot_utils.rs\n@@ -82,17 +82,18 @@ fn attr_map_to_string<'a>(\n     if attrs.is_empty() {\n         return Ok(\"\".to_string());\n     }\n-\n     let attr_string = attrs\n         .iter()\n         .map(|(key, value)| {\n-            if key == \"label\" {\n-                format!(\"{}=\\\"{}\\\"\", key, value)\n-            } else {\n-                format!(\"{}={}\", key, value)\n-            }\n+            let escaped_value = serde_json::to_string(value).map_err(|_err| {\n+                pyo3::exceptions::PyValueError::new_err(\"could not escape character\")\n+            })?;\n+            let escaped_value = &escaped_value.get(1..escaped_value.len() - 1).ok_or(\n+                pyo3::exceptions::PyValueError::new_err(\"could not escape character\"),\n+            )?;\n+            Ok(format!(\"{}=\\\"{}\\\"\", key, escaped_value))\n         })\n-        .collect::<Vec<String>>()\n+        .collect::<PyResult<Vec<String>>>()?\n         .join(\", \");\n     Ok(format!(\"[{}]\", attr_string))\n }\n", "instance_id": "Qiskit__rustworkx-1203", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `graphviz_draw` function in the `rustworkx` library fails to handle special characters like `\\n` and `:` in tooltips, resulting in a syntax error. It provides a reproducible code snippet, the current behavior (error message), and the expected behavior with a reference to how it works in another tool (Graphviz visual editor). However, there are minor ambiguities, such as the lack of explicit mention of all special characters that need to be supported beyond `\\n` and `:`, and no detailed discussion of potential edge cases or constraints (e.g., maximum tooltip length or other formatting rules in Graphviz). Additionally, while the goal is clear, the problem statement does not specify whether the fix should handle escaping in a generic way or only for specific characters. Overall, it is clear enough to understand the issue and intent but misses some finer details.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single function in `dot_utils.rs` (specifically, the `attr_map_to_string` function). The modification involves changing how attribute values are formatted to ensure proper escaping of special characters using `serde_json::to_string`. The diff shows a small, focused change (about 10 lines modified), with no impact on the broader system architecture or multiple modules. The release notes update is trivial and does not add to the difficulty.\n\n2. **Technical Concepts Involved**: The solution requires understanding basic Rust concepts such as string manipulation, error handling with `Result` (via `PyResult`), and the use of the `serde_json` library for escaping special characters. These are relatively straightforward concepts for a Rust developer. Additionally, a basic understanding of Graphviz DOT syntax is needed to recognize why escaping is necessary, but this is not a complex domain-specific requirement.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the specific special characters (`\\n` and `:`), and the code change introduces a generic escaping mechanism using `serde_json::to_string`, which likely handles a wide range of special characters. However, the error handling added in the code (raising a `PyValueError` if escaping fails) is minimal and straightforward. There is no indication of complex edge cases like extremely long tooltips or invalid input handling that would increase difficulty.\n\n4. **Overall Complexity**: The fix is a simple bug resolution that involves modifying how strings are formatted for Graphviz output. It does not require deep knowledge of the `rustworkx` codebase beyond the specific function, nor does it involve complex algorithms, performance optimizations, or architectural changes. The primary challenge is recognizing the need to escape special characters and implementing a reliable method to do so, which is a common task in text processing.\n\nA score of 0.30 reflects that this is an easy problem requiring a targeted fix with minimal impact, suitable for a developer with basic to intermediate Rust skills. It is slightly above the \"very easy\" range due to the need to understand and use `serde_json` for escaping and handle errors appropriately in a Python-Rust interop context (via `pyo3`).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Arx crash if given path doesn't exist. \nHi,\r\n\r\nI'd like to report a crash in `arx`. Apologies if this is a duplicate issue, I\u2019m reporting it quickly.\r\n\r\n### Steps to Reproduce:\r\n1. Tried running the following command:\r\n   ```bash\r\n   arx create --outfile ExampleArchive ExampleArchive.arx\r\n   ```\r\n2. `arx` crashed with the following error message:\r\n   ```bash\r\n   Well, this is embarrassing.\r\n   arx had a problem and crashed...\r\n   ```\r\n\r\n3. Retried with:\r\n   ```bash\r\n   arx create --outfile ExampleArchive/ ExampleArchive.arx\r\n   ```\r\n   It crashed again with a similar error.\r\n\r\n### Environment:\r\n- OS: Arch Linux [64-bit]\r\n- arx version: 0.2.1\r\n- Crate version: 0.2.1\r\n\r\n### Error Details:\r\n- The crash occurred in the file `/libarx-0.2.1/src/create/fs_adder.rs` at line 143.\r\n- Cause: `called Result::unwrap() on an Err value: Error { depth: 0, inner: Io { path: Some(\"ExampleArchive.arx\"), err: Os { code: 2, kind: NotFound, message: \"No such file or directory\" } } }`.\r\n\r\n### Crash Report (as extracted):\r\n```toml\r\n\"name\" = \"arx\"\r\n\"operating_system\" = \"Arch Linux [64-bit]\"\r\n\"crate_version\" = \"0.2.1\"\r\n\"explanation\" = \"\"\"\r\nPanic occurred in file '/home/temo/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libarx-0.2.1/src/create/fs_adder.rs' at line 143\r\n\"\"\"\r\n\"cause\" = 'called `Result::unwrap()` on an `Err` value: Error { depth: 0, inner: Io { path: Some(\"ExampleArchive.arx\"), err: Os { code: 2, kind: NotFound, message: \"No such file or directory\" } } }'\r\n\"method\" = \"Panic\"\r\n\"backtrace\" = \"\"\"\r\n\r\n   0: 0x5e85a33ebce6 - core::result::unwrap_failed::h635713c629d81684\r\n   1: 0x5e85a33ff8d7 - libarx::create::fs_adder::FsAdder::add_from_path_with_filter::h3eaec6ef31c11959\r\n   2: 0x5e85a34071cc - arx::create::create::hb3e18840522cc531\r\n   3: 0x5e85a3423601 - arx::main::h417fd83e1d4276d2\r\n   4: 0x5e85a3416b03 - std::sys_common::backtrace::__rust_begin_short_backtrace::h95ea882ff6bfd673\r\n   5: 0x5e85a3426a39 - std::rt::lang_start::{{closure}}::ha16fb5f386b2120b\r\n   6: 0x5e85a377ab40 - std::rt::lang_start_internal::h5add80e16e4e0045\r\n   7: 0x5e85a3426a25 - main\r\n   8: 0x73b12df7de08 - <unresolved>\r\n   9: 0x73b12df7decc - __libc_start_main\r\n  10: 0x5e85a33ebf05 - _start\r\n  11:        0x0 - <unresolved>\"\"\"\r\n```\r\n\r\nThanks for looking into this! Let me know if you need further details.\r\n\n", "patch": "diff --git a/arx/src/create.rs b/arx/src/create.rs\nindex b77d666..9ca385d 100644\n--- a/arx/src/create.rs\n+++ b/arx/src/create.rs\n@@ -3,7 +3,7 @@ use log::{debug, info};\n use std::cell::Cell;\n use std::fs::File;\n use std::io::{BufRead, BufReader};\n-use std::path::PathBuf;\n+use std::path::{Path, PathBuf};\n use std::rc::Rc;\n use std::sync::Arc;\n \n@@ -20,7 +20,7 @@ pub struct Options {\n         short,\n         long,\n         value_parser,\n-        required_unless_present_any([\"list_compressions\",\"outfile_old\"]),\n+        required_unless_present(\"list_compressions\"),\n         value_hint=ValueHint::FilePath\n     )]\n     outfile: Option<PathBuf>,\n@@ -92,37 +92,64 @@ pub struct Options {\n     #[arg(long, default_value_t = false, action)]\n     progress: bool,\n \n+    #[arg(short, long, required = false, default_value_t = false, action)]\n+    force: bool,\n+\n     #[arg(from_global)]\n     verbose: u8,\n-\n-    #[arg(\n-        short = 'f',\n-        long = \"file\",\n-        hide = true,\n-        conflicts_with(\"outfile\"),\n-        required_unless_present_any([\"list_compressions\", \"outfile\"])\n-    )]\n-    outfile_old: Option<PathBuf>,\n }\n \n fn get_files_to_add(options: &Options) -> Result<Vec<PathBuf>> {\n     let file_list = if let Some(file_list) = &options.file_list {\n         let file = File::open(file_list)\n             .with_context(|| format!(\"Cannot open {}\", file_list.display()))?;\n-        let mut files = Vec::new();\n-        for line in BufReader::new(file).lines() {\n-            files.push(line?.into());\n-        }\n-        files\n+        BufReader::new(file)\n+            .lines()\n+            .map(|l| -> Result<PathBuf> { Ok(l?.into()) })\n+            .collect::<Result<Vec<_>>>()?\n     } else {\n-        options.infiles.clone()\n+        options\n+            .infiles\n+            .iter()\n+            .map(|f| -> Result<PathBuf> {\n+                if f.is_absolute() {\n+                    Err(anyhow!(\"Input file ({}) must be relative.\", f.display()))\n+                } else {\n+                    Ok(f.clone())\n+                }\n+            })\n+            .collect::<Result<Vec<_>>>()?\n     };\n+    Ok(file_list)\n+}\n+\n+fn check_input_paths_exist(file_list: &[PathBuf]) -> Result<()> {\n+    // Check that input files actually exists\n     for file in file_list.iter() {\n-        if file.is_absolute() {\n-            return Err(anyhow!(\"Input file ({}) must be relative.\", file.display()));\n+        if !file.exists() {\n+            return Err(anyhow!(\n+                \"Input {} path doesn't exist or cannot be accessed\",\n+                file.display()\n+            ));\n         }\n     }\n-    Ok(file_list)\n+    Ok(())\n+}\n+\n+fn check_output_path_writable(out_file: &Path, force: bool) -> Result<()> {\n+    if !out_file.parent().unwrap().is_dir() {\n+        Err(anyhow!(\n+            \"Directory {} doesn't exist\",\n+            out_file.parent().unwrap().display()\n+        ))\n+    } else if out_file.exists() && !force {\n+        Err(anyhow!(\n+            \"File {} already exists. Use option --force to overwrite it.\",\n+            out_file.display()\n+        ))\n+    } else {\n+        Ok(())\n+    }\n }\n \n struct ProgressBar {\n@@ -200,13 +227,16 @@ pub fn create(options: Options) -> Result<()> {\n         None => arx::PathBuf::new(),\n     };\n \n-    let out_file = if let Some(ref outfile) = options.outfile_old {\n-        outfile\n-    } else {\n-        options.outfile.as_ref().unwrap()\n-    };\n+    let out_file = options.outfile.as_ref().expect(\n+        \"Clap unsure it is Some, except if we have list_compressions, and so we return early\",\n+    );\n     let out_file = std::env::current_dir()?.join(out_file);\n+    check_output_path_writable(&out_file, options.force)?;\n     let files_to_add = get_files_to_add(&options)?;\n+    if let Some(base_dir) = &options.base_dir {\n+        std::env::set_current_dir(base_dir)?;\n+    };\n+    check_input_paths_exist(&files_to_add)?;\n \n     let jbk_progress: Arc<dyn jbk::creator::Progress> = if options.progress {\n         Arc::new(ProgressBar::new())\n@@ -225,10 +255,6 @@ pub fn create(options: Options) -> Result<()> {\n         options.compression,\n     )?;\n \n-    if let Some(base_dir) = &options.base_dir {\n-        std::env::set_current_dir(base_dir)?;\n-    };\n-\n     let mut fs_adder = arx::create::FsAdder::new(&mut creator, strip_prefix);\n     for infile in files_to_add {\n         fs_adder.add_from_path(&infile, options.recurse)?;\n", "instance_id": "jubako__arx-72", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `arx` tool crashes when a given path does not exist. It provides steps to reproduce the issue, environment details, and a detailed error message with a stack trace, which helps in pinpointing the problem to a specific file and line in the codebase. However, there are minor ambiguities and missing details. For instance, the expected behavior when a path does not exist is not explicitly stated (e.g., should it fail gracefully with a user-friendly error message, or create the necessary directories?). Additionally, while the crash is well-documented, edge cases or specific constraints (e.g., handling permissions, symbolic links, or very long paths) are not mentioned. Overall, the statement is valid and clear enough to understand the core issue, but it lacks comprehensive details on desired behavior and potential edge cases.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are confined to a single file (`arx/src/create.rs`) and focus on adding checks for input and output path existence before proceeding with operations that could cause a crash. The modifications involve adding new functions (`check_input_paths_exist` and `check_output_path_writable`) and updating existing logic to handle errors more gracefully. The changes do not impact the broader system architecture and are relatively localized, though they require understanding the flow of the `create` function and command-line argument handling.\n\n2. **Number of Technical Concepts**: Solving this issue requires familiarity with Rust's standard library, particularly file system operations (`std::path::Path`, `std::fs`), error handling with `Result`, and command-line argument parsing using a library like `clap`. These are intermediate-level concepts in Rust, but not overly complex for someone with moderate experience. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem involves handling basic edge cases, such as non-existent input/output paths and overwriting existing files (with a `--force` flag). The code changes introduce explicit error messages for these scenarios, which is a straightforward improvement over the previous `unwrap()`-induced panic. However, more complex edge cases (e.g., permission issues, symbolic links, or race conditions) are not addressed in the problem statement or code changes, keeping the difficulty moderate.\n\n4. **Overall Complexity**: The task involves understanding the cause of the crash (unwrapping a `Result` that returned an `Err` due to a missing file) and replacing it with proper error handling. This is a common bug-fixing pattern in Rust and does not require deep architectural changes or advanced debugging. The amount of code change is moderate, with logical additions to validate paths and provide user feedback.\n\nIn summary, this problem is relatively easy as it involves straightforward error handling and localized code modifications. It requires a basic-to-intermediate understanding of Rust's error handling and file system APIs, but does not pose significant challenges in terms of codebase complexity or advanced concepts. A score of 0.35 reflects this balance of moderate effort with clear, achievable goals.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "@ multiplication of Series is not inferred to a type (but Unknown)\n**Describe the bug**\r\nWhen multiplying two series with the `@` operator, no type is inferred and the resulting type `Unknown` will be assigned.\r\n\r\n**To Reproduce**\r\n```python\r\nfrom typing import reveal_type\r\n\r\nimport pandas as pd\r\n\r\ns1 = pd.Series([0, 1, 2, 3])\r\ns2 = pd.Series([-1, 2, -3, 4])\r\nsc2 = s1 @ s2\r\nreveal_type(sc2) \u25a0 Type of \"sc2\" is \"Unknown\"\r\n```\r\n\r\nI believe it should return a Scalar at minimum, in some cases Series but it seems like the type hinting of `@` is not in the stubs.\r\n\r\n**Please complete the following information:**\r\n - OS: MacOS\r\n - OS Version 15.0.1\r\n - python version 3.13\r\n - version of type checker 1.13.0\r\n - version of installed `pandas-stubs` 2.2.3.241009\n", "patch": "diff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex 457db8de..eb980e79 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -798,8 +798,18 @@ class Series(IndexOpsMixin[S1], NDFrame):\n     def dot(\n         self, other: ArrayLike | dict[_str, np.ndarray] | Sequence[S1] | Index[S1]\n     ) -> np.ndarray: ...\n-    def __matmul__(self, other): ...\n-    def __rmatmul__(self, other): ...\n+    @overload\n+    def __matmul__(self, other: Series) -> Scalar: ...\n+    @overload\n+    def __matmul__(self, other: DataFrame) -> Series: ...\n+    @overload\n+    def __matmul__(self, other: np.ndarray) -> np.ndarray: ...\n+    @overload\n+    def __rmatmul__(self, other: Series) -> Scalar: ...\n+    @overload\n+    def __rmatmul__(self, other: DataFrame) -> Series: ...\n+    @overload\n+    def __rmatmul__(self, other: np.ndarray) -> np.ndarray: ...\n     @overload\n     def searchsorted(\n         self,\n", "instance_id": "pandas-dev__pandas-stubs-1047", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to type inference when using the `@` operator for matrix multiplication with pandas Series. It provides a reproducible code snippet and specifies the expected behavior (returning a Scalar or Series instead of Unknown). However, there are minor ambiguities: the statement does not explicitly define under what conditions a Scalar or Series should be returned, nor does it mention any specific edge cases or constraints that might affect the type inference. Additionally, while the bug is identified in the context of type hinting in pandas-stubs, it lacks detailed context about the broader impact or specific requirements for the fix beyond updating the type hints. Overall, the problem is valid and clear but misses some finer details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue involves updating type hints in the pandas-stubs library for the `__matmul__` and `__rmatmul__` methods of the Series class to correctly reflect the return types based on the operand (Series, DataFrame, or numpy array). The scope of the code change is limited to a single file (series.pyi) and involves straightforward modifications\u2014adding overloads for different input types with their corresponding return types. The technical concepts required are minimal: a basic understanding of Python type hinting, the `@overload` decorator from the typing module, and familiarity with pandas' data structures (Series and DataFrame). No complex algorithms, design patterns, or deep architectural changes are needed, and the change does not impact the broader codebase or system architecture. Edge cases and error handling are not explicitly mentioned in the problem statement or required in the code changes, as this is purely a type annotation fix rather than a runtime logic modification. Overall, this is a simple task that requires understanding of type annotations and minimal effort to implement.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Series constructor does not accept TypedDict\n**Describe the bug**\r\nWhen creating a series from a TypedDict, Pyright reports\r\n\r\n`No overloads for \"__new__\" match the provided arguments. Argument of type (TypedDict) cannot be assigned to parameter \"data\" of type \"Scalar | _ListLike[Unknown] | dict[HashableT1@__new__, Any] | BaseGroupBy[Unknown] | NaTType | NAType | None\" in function \"__new__\"`\r\n\r\n**To Reproduce**\r\n\r\n```\r\nfrom typing import TypedDict\r\nimport pandas as pd\r\nclass MyDict(TypedDict):\r\n    a: str\r\n    b: str\r\n\r\nmy_dict = MyDict(a=\"\", b=\"\")\r\npd.Series(my_dict)\r\n```\r\nPyright yields \r\n\r\n```\r\nerror: No overloads for \"__new__\" match the provided arguments (reportCallIssue)\r\nerror: Argument of type \"MyDict\" cannot be assigned to parameter \"data\" of type \"Scalar | _ListLike[Unknown] | dict[HashableT1@__new__, Any] | BaseGroupBy[Unknown] | NaTType | NAType | None\" in function \"__new__\"\r\n    Type \"MyDict\" is incompatible with type \"Scalar | _ListLike[Unknown] | dict[HashableT1@__new__, Any] | BaseGroupBy[Unknown] | NaTType | NAType | None\"\r\n      \"MyDict\" is incompatible with \"ExtensionArray\"\r\n      \"MyDict\" is incompatible with \"NAType\"\r\n      \"MyDict\" is incompatible with \"NaTType\"\r\n      \"MyDict\" is incompatible with \"Timedelta\"\r\n      \"MyDict\" is incompatible with \"Timestamp\"\r\n      \"MyDict\" is incompatible with \"bool\"\r\n      \"MyDict\" is incompatible with \"bytes\"\r\n    ... (reportArgumentType)\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Linux\r\n - OS Version: 22\r\n - python version: Python 3.12.4\r\n - version of type checker: Pyright 1.1.374\r\n - version of installed `pandas-stubs`: 2.2.2.240603\r\n\r\n\r\n**Additional context**\r\nSeems like `TypedDict` is incompatible with `dict`, but I don't see why it matters in this specific example. Changing overload 8 (series.pyi(332, 9))\r\n\r\n```\r\n@overload\r\n    def __new__(\r\n        cls,\r\n        data: (\r\n            Scalar\r\n            | _ListLike\r\n            | dict[HashableT1, Any]\r\n            | BaseGroupBy\r\n            | NaTType\r\n            | NAType\r\n            | None\r\n        ) = ...,\r\n        index: Axes | None = ...,\r\n        *,\r\n        dtype: Dtype = ...,\r\n        name: Hashable = ...,\r\n        copy: bool = ...,\r\n    ) -> Series: ...\r\n```\r\nto\r\n```\r\n@overload\r\n    def __new__(\r\n        cls,\r\n        data: (\r\n            Scalar\r\n            | _ListLike\r\n            | Mapping[HashableT1, Any]\r\n            | BaseGroupBy\r\n            | NaTType\r\n            | NAType\r\n            | None\r\n        ) = ...,\r\n        index: Axes | None = ...,\r\n        *,\r\n        dtype: Dtype = ...,\r\n        name: Hashable = ...,\r\n        copy: bool = ...,\r\n    ) -> Series: ...\r\n```\r\nfixes this specific example but I don't know if it breaks something elsewhere.\n", "patch": "diff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex 24bac1a1..9218b674 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -334,7 +334,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n         data: (\n             Scalar\n             | _ListLike\n-            | dict[HashableT1, Any]\n+            | Mapping[HashableT1, Any]\n             | BaseGroupBy\n             | NaTType\n             | NAType\n", "instance_id": "pandas-dev__pandas-stubs-1000", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a bug related to type checking with `TypedDict` when creating a `pandas.Series` object, as reported by Pyright. It provides a reproducible code snippet, the exact error message, and relevant context about the environment (OS, Python version, type checker version, etc.). The goal is evident\u2014fix the type incompatibility issue. However, there are minor ambiguities: the statement does not explicitly discuss potential side effects of the proposed change (replacing `dict` with `Mapping` in the type hint), and it lacks discussion on edge cases or other scenarios where this change might impact behavior. Additionally, while the user suggests a fix, there is uncertainty expressed about whether it \"breaks something elsewhere,\" which indicates missing critical details about the broader implications of the change. Overall, the problem is valid and mostly clear but misses some depth in constraints and potential impacts.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code change is minimal and localized to a single line in a type hint file (`pandas-stubs/core/series.pyi`). It involves replacing `dict` with `Mapping` in an overload definition for the `Series` constructor. This does not impact the runtime behavior of the codebase, as it is purely a type annotation fix for static type checking. There is no interaction with other modules or architectural changes required.\n\n2. **Number of Technical Concepts**: The problem requires understanding of Python type hints, specifically the difference between `dict` and `Mapping` from the `typing` module, and how `TypedDict` interacts with these types. It also involves familiarity with static type checkers like Pyright and the `pandas-stubs` library for type annotations. While these concepts are not trivial for beginners, they are relatively straightforward for someone with moderate experience in Python and type systems. No complex algorithms, design patterns, or domain-specific knowledge beyond type annotations are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the proposed change (using `Mapping` instead of `dict`) is a broader type that should accommodate `TypedDict` and other mapping-like types. However, there is a risk of unintended consequences, as noted by the user, since `Mapping` is a more general type than `dict`. Assessing whether this breaks other type checks or introduces incompatibilities elsewhere in the codebase might require additional investigation, but the change itself does not involve explicit error handling or complex edge case logic in the code.\n\n4. **Overall Complexity**: The fix is simple and does not require deep understanding of the `pandas` library's internals or architecture, as it pertains only to type stubs. The primary challenge lies in verifying that the change does not negatively impact other type checks, which adds a small layer of complexity but does not elevate the problem beyond \"Easy.\"\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the code change, the moderate but not advanced technical concepts involved, and the limited scope of impact. It is a straightforward bug fix in the type annotation domain, suitable for someone with basic to intermediate Python experience and familiarity with type hints.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Handle Rego files of more than 64KB size\nBy design, `u16` is currently used in `Span` to index into source location. The incorrect assumption was that policy files won't be large enough to exceed 64KB. However, AKS-CC files can exceed this limit. \r\n\r\nThere is a bug that regorus panics if such large files are presented.\r\n\r\nThis file size limitation must be removed.\n", "patch": "diff --git a/src/engine.rs b/src/engine.rs\nindex 4eaef8fc..19863ad0 100644\n--- a/src/engine.rs\n+++ b/src/engine.rs\n@@ -65,7 +65,7 @@ impl Engine {\n     /// ```\n     ///\n     pub fn add_policy(&mut self, path: String, rego: String) -> Result<()> {\n-        let source = Source::new(path, rego);\n+        let source = Source::from_contents(path, rego)?;\n         let mut parser = Parser::new(&source)?;\n         self.modules.push(Ref::new(parser.parse()?));\n         // if policies change, interpreter needs to be prepared again\n@@ -294,15 +294,15 @@ impl Engine {\n \n         self.interpreter.create_rule_prefixes()?;\n         let query_module = {\n-            let source = Source::new(\n+            let source = Source::from_contents(\n                 \"<query_module.rego>\".to_owned(),\n                 \"package __internal_query_module\".to_owned(),\n-            );\n+            )?;\n             Ref::new(Parser::new(&source)?.parse()?)\n         };\n \n         // Parse the query.\n-        let query_source = Source::new(\"<query.rego>\".to_string(), query);\n+        let query_source = Source::from_contents(\"<query.rego>\".to_string(), query)?;\n         let mut parser = Parser::new(&query_source)?;\n         let query_node = parser.parse_user_query()?;\n         if query_node.span.text() == \"data\" {\n@@ -411,15 +411,15 @@ impl Engine {\n         self.eval_modules(enable_tracing)?;\n \n         let query_module = {\n-            let source = Source::new(\n+            let source = Source::from_contents(\n                 \"<query_module.rego>\".to_owned(),\n                 \"package __internal_query_module\".to_owned(),\n-            );\n+            )?;\n             Ref::new(Parser::new(&source)?.parse()?)\n         };\n \n         // Parse the query.\n-        let query_source = Source::new(\"<query.rego>\".to_string(), query);\n+        let query_source = Source::from_contents(\"<query.rego>\".to_string(), query)?;\n         let mut parser = Parser::new(&query_source)?;\n         let query_node = parser.parse_user_query()?;\n         let query_schedule = Analyzer::new().analyze_query_snippet(&self.modules, &query_node)?;\ndiff --git a/src/lexer.rs b/src/lexer.rs\nindex f4ae34c7..3cc82d6b 100644\n--- a/src/lexer.rs\n+++ b/src/lexer.rs\n@@ -18,7 +18,7 @@ use anyhow::{anyhow, bail, Result};\n struct SourceInternal {\n     pub file: String,\n     pub contents: String,\n-    pub lines: Vec<(u16, u16)>,\n+    pub lines: Vec<(u32, u32)>,\n }\n \n #[derive(Clone)]\n@@ -61,8 +61,8 @@ impl Debug for Source {\n #[derive(Clone)]\n pub struct SourceStr {\n     source: Source,\n-    start: u16,\n-    end: u16,\n+    start: u32,\n+    end: u32,\n }\n \n impl Debug for SourceStr {\n@@ -78,7 +78,7 @@ impl std::fmt::Display for SourceStr {\n }\n \n impl SourceStr {\n-    pub fn new(source: Source, start: u16, end: u16) -> Self {\n+    pub fn new(source: Source, start: u32, end: u32) -> Self {\n         Self { source, start, end }\n     }\n \n@@ -116,39 +116,43 @@ impl std::cmp::Ord for SourceStr {\n }\n \n impl Source {\n-    pub fn new(file: String, contents: String) -> Source {\n+    pub fn from_contents(file: String, contents: String) -> Result<Source> {\n+        let max_size = u32::MAX as usize - 2; // Account for rows, cols possibly starting at 1, EOF etc.\n+        if contents.len() > max_size {\n+            bail!(\"{file} exceeds maximum allowed policy file size {max_size}\");\n+        }\n         let mut lines = vec![];\n         let mut prev_ch = ' ';\n-        let mut prev_pos = 0u16;\n-        let mut start = 0u16;\n+        let mut prev_pos = 0u32;\n+        let mut start = 0u32;\n         for (i, ch) in contents.char_indices() {\n             if ch == '\\n' {\n                 let end = match prev_ch {\n                     '\\r' => prev_pos,\n-                    _ => i as u16,\n+                    _ => i as u32,\n                 };\n                 lines.push((start, end));\n-                start = i as u16 + 1;\n+                start = i as u32 + 1;\n             }\n             prev_ch = ch;\n-            prev_pos = i as u16;\n+            prev_pos = i as u32;\n         }\n \n         if (start as usize) < contents.len() {\n-            lines.push((start, contents.len() as u16));\n+            lines.push((start, contents.len() as u32));\n         } else if contents.is_empty() {\n             lines.push((0, 0));\n         } else {\n-            let s = (contents.len() - 1) as u16;\n+            let s = (contents.len() - 1) as u32;\n             lines.push((s, s));\n         }\n-        Self {\n+        Ok(Self {\n             src: Rc::new(SourceInternal {\n                 file,\n                 contents,\n                 lines,\n             }),\n-        }\n+        })\n     }\n \n     pub fn from_file<P: AsRef<Path>>(path: P) -> Result<Source> {\n@@ -157,10 +161,7 @@ impl Source {\n             Err(e) => bail!(\"Failed to read {}. {e}\", path.as_ref().display()),\n         };\n         // TODO: retain path instead of converting to string\n-        Ok(Self::new(\n-            path.as_ref().to_string_lossy().to_string(),\n-            contents,\n-        ))\n+        Self::from_contents(path.as_ref().to_string_lossy().to_string(), contents)\n     }\n \n     pub fn file(&self) -> &String {\n@@ -169,7 +170,7 @@ impl Source {\n     pub fn contents(&self) -> &String {\n         &self.src.contents\n     }\n-    pub fn line(&self, idx: u16) -> &str {\n+    pub fn line(&self, idx: u32) -> &str {\n         let idx = idx as usize;\n         if idx < self.src.lines.len() {\n             let (start, end) = self.src.lines[idx];\n@@ -179,7 +180,7 @@ impl Source {\n         }\n     }\n \n-    pub fn message(&self, line: u16, col: u16, kind: &str, msg: &str) -> String {\n+    pub fn message(&self, line: u32, col: u32, kind: &str, msg: &str) -> String {\n         if line as usize > self.src.lines.len() {\n             return format!(\"{}: invalid line {} specified\", self.src.file, line);\n         }\n@@ -206,7 +207,7 @@ impl Source {\n         )\n     }\n \n-    pub fn error(&self, line: u16, col: u16, msg: &str) -> anyhow::Error {\n+    pub fn error(&self, line: u32, col: u32, msg: &str) -> anyhow::Error {\n         anyhow!(self.message(line, col, \"error\", msg))\n     }\n }\n@@ -214,10 +215,10 @@ impl Source {\n #[derive(Clone)]\n pub struct Span {\n     pub source: Source,\n-    pub line: u16,\n-    pub col: u16,\n-    pub start: u16,\n-    pub end: u16,\n+    pub line: u32,\n+    pub col: u32,\n+    pub start: u32,\n+    pub end: u32,\n }\n \n impl Span {\n@@ -272,8 +273,8 @@ pub struct Token(pub TokenKind, pub Span);\n pub struct Lexer<'source> {\n     source: Source,\n     iter: Peekable<CharIndices<'source>>,\n-    line: u16,\n-    col: u16,\n+    line: u32,\n+    col: u32,\n }\n \n impl<'source> Lexer<'source> {\n@@ -312,15 +313,15 @@ impl<'source> Lexer<'source> {\n             }\n         }\n         let end = self.peek().0;\n-        self.col += (end - start) as u16;\n+        self.col += (end - start) as u32;\n         Ok(Token(\n             TokenKind::Ident,\n             Span {\n                 source: self.source.clone(),\n                 line: self.line,\n                 col,\n-                start: start as u16,\n-                end: end as u16,\n+                start: start as u32,\n+                end: end as u32,\n             },\n         ))\n     }\n@@ -363,7 +364,7 @@ impl<'source> Lexer<'source> {\n         }\n \n         let end = self.peek().0;\n-        self.col += (end - start) as u16;\n+        self.col += (end - start) as u32;\n \n         // Check for invalid number.Valid number cannot be followed by\n         // these characters:\n@@ -403,8 +404,8 @@ impl<'source> Lexer<'source> {\n                 source: self.source.clone(),\n                 line: self.line,\n                 col,\n-                start: start as u16,\n-                end: end as u16,\n+                start: start as u32,\n+                end: end as u32,\n             },\n         ))\n     }\n@@ -440,8 +441,8 @@ impl<'source> Lexer<'source> {\n                 source: self.source.clone(),\n                 line,\n                 col,\n-                start: start as u16,\n-                end: end as u16 - 1,\n+                start: start as u32,\n+                end: end as u32 - 1,\n             },\n         ))\n     }\n@@ -453,7 +454,7 @@ impl<'source> Lexer<'source> {\n         let (start, _) = self.peek();\n         loop {\n             let (offset, ch) = self.peek();\n-            let col = self.col + (offset - start) as u16;\n+            let col = self.col + (offset - start) as u32;\n             match ch {\n                 '\"' | '#' | '\\x00' => {\n                     break;\n@@ -468,7 +469,7 @@ impl<'source> Lexer<'source> {\n                         'u' => {\n                             for _i in 0..4 {\n                                 let (offset, ch) = self.peek();\n-                                let col = self.col + (offset - start) as u16;\n+                                let col = self.col + (offset - start) as u32;\n                                 if !ch.is_ascii_hexdigit() {\n                                     return Err(self.source.error(\n                                         line,\n@@ -484,7 +485,7 @@ impl<'source> Lexer<'source> {\n                 }\n                 _ => {\n                     // check for valid json chars\n-                    let col = self.col + (offset - start) as u16;\n+                    let col = self.col + (offset - start) as u32;\n                     if !('\\u{0020}'..='\\u{10FFFF}').contains(&ch) {\n                         return Err(self.source.error(line, col, \"invalid character in string\"));\n                     }\n@@ -499,7 +500,7 @@ impl<'source> Lexer<'source> {\n \n         self.iter.next();\n         let end = self.peek().0;\n-        self.col += (end - start) as u16;\n+        self.col += (end - start) as u32;\n \n         // Ensure that the string is parsable in Rust.\n         match serde_json::from_str::<String>(&self.source.contents()[start - 1..end]) {\n@@ -522,8 +523,8 @@ impl<'source> Lexer<'source> {\n                 source: self.source.clone(),\n                 line,\n                 col: col + 1,\n-                start: start as u16,\n-                end: end as u16 - 1,\n+                start: start as u32,\n+                end: end as u32 - 1,\n             },\n         ))\n     }\n@@ -593,14 +594,14 @@ impl<'source> Lexer<'source> {\n \t\t    source: self.source.clone(),\n \t\t    line: self.line,\n \t\t    col,\n-\t\t    start: start as u16,\n-\t\t    end: start as u16 + 1,\n+\t\t    start: start as u32,\n+\t\t    end: start as u32 + 1,\n \t\t}))\n \t    }\n \t    ':' => {\n \t\tself.col += 1;\n \t\tself.iter.next();\n-\t\tlet mut end = start as u16 + 1;\n+\t\tlet mut end = start as u32 + 1;\n \t\tif self.peek().1 == '=' {\n \t\t    self.col += 1;\n \t\t    self.iter.next();\n@@ -610,7 +611,7 @@ impl<'source> Lexer<'source> {\n \t\t    source: self.source.clone(),\n \t\t    line: self.line,\n \t\t    col,\n-\t\t    start: start as u16,\n+\t\t    start: start as u32,\n \t\t    end\n \t\t}))\n \t    }\n@@ -626,8 +627,8 @@ impl<'source> Lexer<'source> {\n \t\t    source: self.source.clone(),\n \t\t    line: self.line,\n \t\t    col,\n-\t\t    start: start as u16,\n-\t\t    end: self.peek().0 as u16,\n+\t\t    start: start as u32,\n+\t\t    end: self.peek().0 as u32,\n \t\t}))\n \t    }\n \t    '!' if self.peekahead(1).1 == '=' => {\n@@ -638,8 +639,8 @@ impl<'source> Lexer<'source> {\n \t\t    source: self.source.clone(),\n \t\t    line: self.line,\n \t\t    col,\n-\t\t    start: start as u16,\n-\t\t    end: self.peek().0 as u16,\n+\t\t    start: start as u32,\n+\t\t    end: self.peek().0 as u32,\n \t\t}))\n \t    }\n \t    '\"' => self.read_string(),\n@@ -648,8 +649,8 @@ impl<'source> Lexer<'source> {\n \t\tsource: self.source.clone(),\n \t\tline:self.line,\n \t\tcol,\n-\t\tstart: start as u16,\n-\t\tend: start as u16\n+\t\tstart: start as u32,\n+\t\tend: start as u32\n \t    })),\n \t    _ if chr.is_ascii_digit() => self.read_number(),\n \t    _ if chr.is_ascii_alphabetic() || chr == '_' => {\ndiff --git a/src/lib.rs b/src/lib.rs\nindex faef1488..4032552b 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -47,9 +47,9 @@ use std::rc::Rc;\n #[derive(Debug, Clone, Serialize, Eq, PartialEq)]\n pub struct Location {\n     /// Line number. Starts at 1.\n-    pub row: u16,\n+    pub row: u32,\n     /// Column number. Starts at 1.\n-    pub col: u16,\n+    pub col: u32,\n }\n \n /// An expression in a Rego query.\ndiff --git a/src/parser.rs b/src/parser.rs\nindex b8705be2..da6c5f2a 100644\n--- a/src/parser.rs\n+++ b/src/parser.rs\n@@ -16,8 +16,8 @@ pub struct Parser<'source> {\n     source: Source,\n     lexer: Lexer<'source>,\n     tok: Token,\n-    line: u16,\n-    end: u16,\n+    line: u32,\n+    end: u32,\n     future_keywords: BTreeMap<String, Span>,\n     rego_v1: bool,\n }\n@@ -753,7 +753,7 @@ impl<'source> Parser<'source> {\n \n     fn parse_membership_tail(\n         &mut self,\n-        start: u16,\n+        start: u32,\n         mut expr1: Expr,\n         mut expr2: Option<Expr>,\n     ) -> Result<Expr> {\n", "instance_id": "microsoft__regorus-217", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the current use of `u16` in `Span` for indexing source locations causes a panic when handling Rego files larger than 64KB, and this limitation needs to be removed. The goal (removing the file size limitation) and the root cause (incorrect assumption about file sizes with `u16`) are explicitly stated. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the expected behavior or constraints for very large files (e.g., performance considerations or upper limits), nor does it mention specific edge cases to handle beyond the general issue of large files. Additionally, there are no examples or test cases provided to illustrate the failure or expected outcome. Despite these minor gaps, the intent and core issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files (`engine.rs`, `lexer.rs`, `lib.rs`, `parser.rs`), requiring a coordinated update from `u16` to `u32` across various structs and functions like `Span`, `SourceStr`, and `Location`. This is not a trivial single-line fix but involves understanding the data flow and ensuring consistency across the codebase. Second, the technical concepts required are relatively straightforward\u2014primarily understanding Rust's integer types and their implications on memory and range\u2014but the changes touch core components like source parsing and tokenization, which are critical to the system's functionality. Third, while the problem statement does not explicitly mention edge cases beyond file size, the code changes introduce a new check for maximum file size (`u32::MAX - 2`), indicating some error handling logic has been added. However, implementing this fix does not appear to require deep architectural changes or advanced algorithms, nor does it involve complex domain-specific knowledge beyond basic parsing and lexer mechanics. The primary challenge lies in ensuring no downstream effects or overflows occur due to the type change, which requires moderate caution and testing. Therefore, a difficulty score of 0.45 reflects a medium-level problem that demands careful modification across multiple files but does not reach the complexity of hard or very hard tasks.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": " ModuleNotFoundError: No module named 'importlib.metadata'\nIn the last version (17.1), I use `from importlib.metadata import version`. In python 3.8 and later, this is included into CPython but in 3.7, it requires `importlib-metadata`.\r\nIf you are using python 3.7, you need to install `importlib-metadata` the same way you installed HiCMatrix.\r\n@bgruening , do you know if I should add `importlib-metadata` as requirements? Or if there is a way to add `importlib-metadata` as a requirement only for python 3.7 both in pypi and conda recipe?\r\n\n", "patch": "diff --git a/HiCMatrix_env_ci.yml b/HiCMatrix_env_ci.yml\nindex e774e1b..17761cb 100644\n--- a/HiCMatrix_env_ci.yml\n+++ b/HiCMatrix_env_ci.yml\n@@ -7,6 +7,7 @@ dependencies:\n    - scipy >= 1.2\n    - pandas >= 0.25\n    - pytables >= 3.5\n+   - c-blosc2<=2.14.1 # Temp fix see https://github.com/Blosc/c-blosc2/issues/601\n    - cooler >= 0.8.9\n    - intervaltree >= 3.0\n    - pytest\ndiff --git a/hicmatrix/lib/cool.py b/hicmatrix/lib/cool.py\nindex 83bf955..84d68a5 100644\n--- a/hicmatrix/lib/cool.py\n+++ b/hicmatrix/lib/cool.py\n@@ -2,7 +2,10 @@\n import os\n \n import gc\n-from importlib.metadata import version\n+try:\n+    from importlib.metadata import version\n+except ImportError:\n+    from importlib_metadata import version\n \n import cooler\n import h5py\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 305e1a9..f7fee0e 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -4,13 +4,13 @@ build-backend = \"setuptools.build_meta\"\n \n [project]\n name = \"HiCMatrix\"\n-version = \"17.1\"\n+version = \"17.2\"\n authors = [\n   { name = \"Lucille Lopez-Delisle, Joachim Wolff, Leily Rabbani, Vivek Bhardwaj, Fidel Ramirez\", email = \"lucille.delisle@epfl.ch\" },\n ]\n description = \"Helper package which implements HiCMatrix class for HiCExplorer, pyGenomeTracks and scHiCExplorer.\"\n readme = \"README.rst\"\n-requires-python = \">=3.8\"\n+requires-python = \">=3.7\"\n classifiers = [\n     'Intended Audience :: Science/Research',\n     'Topic :: Scientific/Engineering :: Bio-Informatics',\n@@ -21,7 +21,8 @@ dependencies = [\n     \"tables >= 3.5\",\n     \"pandas >= 0.25\",\n     \"cooler >= 0.8.9\",\n-    \"intervaltree >= 3.0\"\n+    \"intervaltree >= 3.0\",\n+    \"importlib_metadata; python_version<'3.8'\"\n ]\n \n [project.urls]\n", "instance_id": "deeptools__HiCMatrix-51", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a `ModuleNotFoundError` for `importlib.metadata` in Python 3.7, which is not included in the standard library for that version, unlike in Python 3.8 and later. The goal of addressing this compatibility issue is evident, and the question about whether to add `importlib-metadata` as a dependency (and how to do so conditionally for Python 3.7) is explicit. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the expected behavior or constraints for environments using Python 3.8+ (where the dependency is unnecessary), nor does it discuss potential edge cases like installation conflicts or version-specific issues with `importlib-metadata`. Additionally, there are no examples or test cases provided to validate the solution. Despite these minor gaps, the intent and core issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are limited to a few files (`hicmatrix/lib/cool.py`, `pyproject.toml`, and `HiCMatrix_env_ci.yml`) and are straightforward. The primary modification in the code is a try-except block to handle the import of `version` from either `importlib.metadata` or `importlib_metadata`, which is a small, localized change. The updates to `pyproject.toml` involve adding a conditional dependency for Python 3.7 and adjusting the required Python version, which is also minor. The change in the environment file adds a temporary fix for another dependency, which is unrelated to the core issue but still simple. Overall, the changes do not impact the system's architecture or require understanding complex interactions across the codebase.\n\n2. **Number of Technical Concepts:** The solution requires basic knowledge of Python's import system, exception handling, and dependency management using tools like `pip` and `conda`. Specifically, understanding how to specify conditional dependencies in `pyproject.toml` (using the `python_version` specifier) is necessary but not particularly complex for someone with moderate Python experience. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code change in `cool.py` inherently handles the primary edge case (Python 3.7 lacking `importlib.metadata`) via a fallback import. There are no complex error handling requirements beyond this, and potential issues like dependency conflicts or version incompatibilities are not addressed in the problem or changes, keeping the scope simple.\n\n4. **Overall Complexity:** The task involves minimal logic and focuses on compatibility fixes rather than implementing new functionality or refactoring. It requires understanding a small part of the codebase and making targeted, well-defined modifications.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic to intermediate Python knowledge and minimal code changes with low risk of broader impact.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Catch literal kinds in `real(x, kind=k)` and similar functions\nThe rules `LiteralKind` and `LiteralKindSuffix` catch the use of number literals as kind specifiers for both variable declarations and on number literals themselves. It's still possible for them to slip through the net via type casting functions:\r\n\r\n```f90\r\nreal(8) :: x        ! This will be caught\r\nx = 2.0_8           ! This will be caught\r\nx = real(2, 8)      ! This will not!\r\nx = real(2, kind=8) ! Should be careful to catch this too\r\n```\n", "patch": "diff --git a/fortitude/src/rules/precision/kind_suffixes.rs b/fortitude/src/rules/precision/kind_suffixes.rs\nindex f242104..4f72355 100644\n--- a/fortitude/src/rules/precision/kind_suffixes.rs\n+++ b/fortitude/src/rules/precision/kind_suffixes.rs\n@@ -25,27 +25,34 @@ use tree_sitter::Node;\n /// print *, pi_2  ! Gives: 3.1415926535897900\n /// ```\n ///\n-/// There are many cases where the difference in precision doesn't matter, such\n-/// as the following operations:\n+/// There are cases where the difference in precision doesn't matter, such\n+/// as:\n ///\n /// ```f90\n /// real(dp) :: x, y\n ///\n /// x = 1.0\n-/// x = 10.0 * y\n+/// y = real(2.0, kind=dp)\n /// ```\n ///\n-/// However, even for 'nice' numbers, it's possible to accidentally lose\n-/// precision in surprising ways:\n+/// A case where a missing suffix may be intentional is when using a `kind`\n+/// statement:\n ///\n /// ```f90\n-/// x = y * sqrt(2.0)\n+/// integer, parameter :: sp = kind(0.0)\n /// ```\n ///\n-/// Ideally, this rule should check how the number is used in a local expression\n-/// and determine whether precision loss is a real risk, but in its current\n-/// implementation it instead requires all real literals to have an explicit\n-/// kind suffix.\n+/// This rule will try to avoid catching these case. However, even for 'nice'\n+/// numbers, it's possible to accidentally lose precision in surprising ways:\n+///\n+/// ```f90\n+/// real(dp) :: x\n+///\n+/// x = sqrt(2.0)\n+/// ```\n+///\n+/// This rule will therefore require an explicit kind statement in the majority\n+/// of cases where a floating point literal is found in an expression.\n ///\n /// ## References\n /// - [Fortran-Lang Best Practices on Floating Point Numbers](https://fortran-lang.org/en/learn/best_practices/floating_point/)\n@@ -72,6 +79,47 @@ impl AstRule for NoRealSuffix {\n         if !regex_is_match!(r\"^(\\d*\\.\\d*|\\d*\\.*\\d*[eE]\\d+)$\", txt) {\n             return None;\n         }\n+\n+        // Determine the immediate context in which we've found the literal.\n+        let mut parent = node.parent()?;\n+        while matches!(\n+            parent.kind(),\n+            \"unary_expression\" | \"parenthesized_expression\" | \"complex_literal\"\n+        ) {\n+            parent = parent.parent()?;\n+        }\n+        let grandparent = parent.parent()?;\n+\n+        // Check for loss of precision\n+        // FIXME: This precision loss test isn't the most reliable\n+        let value_64: f64 = txt.parse().ok()?;\n+        let value_32: f32 = txt.parse().ok()?;\n+        let no_loss = value_64 == 0.0\n+            || (((value_32 as f64) - value_64) / value_64).abs() < 2.0 * f64::EPSILON;\n+\n+        // Ok if being used in a direct assignment, provided no loss of precision\n+        // can occur.\n+        if matches!(parent.kind(), \"assignment_statement\" | \"init_declarator\") && no_loss {\n+            return None;\n+        }\n+\n+        // Ok if being used in a kind statement or a type cast.\n+        // In the latter case, warnings should still be raised if precision would be\n+        // lost.\n+        // If it's the sole argument in a function call, the first parent must be\n+        // \"argument_list\", and the second must be \"call_expression\".\n+        if grandparent.kind() == \"call_expression\" {\n+            if let Some(identifier) = grandparent.child_with_name(\"identifier\") {\n+                let name = identifier.to_text(src.source_text())?.to_lowercase();\n+                if name == \"kind\"\n+                    || (no_loss\n+                        && matches!(name.as_str(), \"real\" | \"cmplx\" | \"dbl\" | \"int\" | \"logical\"))\n+                {\n+                    return None;\n+                }\n+            }\n+        }\n+\n         let literal = txt.to_string();\n         some_vec![Diagnostic::from_node(NoRealSuffix { literal }, node)]\n     }\n", "instance_id": "PlasmaFAIR__fortitude-156", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to address the issue of catching literal kinds in type casting functions like `real(x, kind=k)` in Fortran code. It provides specific examples of cases that should be caught and those that currently slip through, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for all edge cases (e.g., what should happen with mixed precision or invalid kind values). Additionally, while it mentions the need to catch both `real(2, 8)` and `real(2, kind=8)`, it lacks a comprehensive list of functions or contexts to consider beyond `real`. Constraints or performance considerations for the rule implementation are also not discussed. Overall, the statement is valid and clear but leaves some minor details to inference.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`kind_suffixes.rs`) and a specific rule (`NoRealSuffix`) within a larger codebase. The changes involve adding logic to detect specific contexts (e.g., type casting functions like `real` or `kind`) and handle precision loss checks, which requires understanding the existing rule logic and the `tree_sitter` library for parsing code structure. The number of technical concepts involved includes familiarity with Rust, tree-sitter node traversal, regular expressions, and floating-point precision handling, which are moderately complex but not overly advanced. The code changes also require addressing potential edge cases, such as distinguishing between safe and unsafe precision loss, and handling different function names and argument structures, though these are not extensively complex. The impact on the system's architecture is minimal, as it\u2019s a localized rule enhancement rather than a systemic change. Overall, this problem requires a solid understanding of the codebase\u2019s rule system and moderate effort to implement and test the new logic, justifying a score of 0.55 in the medium difficulty range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Drop Support for Python 3.6, 3.7 & Start Support for Python 3.12+\n**Describe the refactoring action**\r\nWe currently test the python versions `python-version: [3.6, 3.7, 3.8, 3.9, 3.10.x, 3.11]` in the [Test Workflow](https://github.com/averbis/averbis-python-api/blob/main/.github/workflows/run_tests.yml) and have `REQUIRES_PYTHON = \">=3.6.0\"` in the [setup.py](https://github.com/averbis/averbis-python-api/blob/main/setup.py).  I would like to change that to Python 3.8+ in the setup.py and test against (3.8,3.9,3.10,3.11,3.12).\r\n\r\n**Additional Context**\r\nPython 3.6 has reached end of life (2021-12) and Python 3.7 has also reached end of life (June 27th 2023).\r\n\r\n**Expected benefit**\r\nSupport for current Python versions.\r\n\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex a5ad1d5..e604019 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -1,4 +1,5 @@\n .idea\n+.venv\n venv\n __pycache__\n .vscode\ndiff --git a/Makefile b/Makefile\nindex 8be1ce4..82c2b44 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -9,4 +9,4 @@ html:\n \tcd docs && make html\n \n license:\n-\tlicenseheaders -t build/apache-2.tmpl -n \"Averbis Python API\" -y \"2021\" -o \"Averbis GmbH\" -u \"https://www.averbis.com\" -x \".*\" \"venv/*\"\n+\tlicenseheaders -t build/apache-2.tmpl -n \"Averbis Python API\" -y \"2024\" -o \"Averbis GmbH\" -u \"https://www.averbis.com\" -x \".*\" \"venv/*\"\ndiff --git a/averbis/core/_rest_client.py b/averbis/core/_rest_client.py\nindex c6adaff..e8e3d30 100644\n--- a/averbis/core/_rest_client.py\n+++ b/averbis/core/_rest_client.py\n@@ -1,5 +1,5 @@\n #\n-# Copyright (c) 2023 Averbis GmbH.\n+# Copyright (c) 2024 Averbis GmbH.\n #\n # This file is part of Averbis Python API.\n # See https://www.averbis.com for further info.\ndiff --git a/setup.py b/setup.py\nindex 19f9e56..b558148 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -1,5 +1,5 @@\n #\n-# Copyright (c) 2021 Averbis GmbH.\n+# Copyright (c) 2024 Averbis GmbH.\n #\n # This file is part of Averbis Python API.\n # See https://www.averbis.com for further info.\n@@ -23,16 +23,14 @@\n \n import io\n import os\n-import sys\n-from shutil import rmtree\n \n-from setuptools import setup, Command, find_packages\n+from setuptools import find_packages, setup\n \n # Package meta-data.\n NAME = \"averbis-python-api\"\n DESCRIPTION = \"Averbis REST API client for Python.\"\n AUTHOR = \"Averbis GmbH\"\n-REQUIRES_PYTHON = \">=3.6.0\"\n+REQUIRES_PYTHON = \">=3.8.0\"\n \n install_requires = [\n     \"requests\",\n@@ -121,12 +119,11 @@\n         # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n         \"Development Status :: 4 - Beta\",\n         \"Programming Language :: Python :: 3 :: Only\",\n-        \"Programming Language :: Python :: 3.6\",\n-        \"Programming Language :: Python :: 3.7\",\n         \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n-        \"License :: OSI Approved :: Apache Software License\"\n+        \"Programming Language :: Python :: 3.12\",\n+        \"License :: OSI Approved :: Apache Software License\",\n     ],\n )\n", "instance_id": "averbis__averbis-python-api-160", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to drop support for Python 3.6 and 3.7, and to add support for Python 3.12+, with a clear reference to updating the setup.py file and the testing workflow. It also provides additional context about the end-of-life status of the older Python versions, which justifies the change. However, there are minor ambiguities: the problem statement does not explicitly mention whether there are any compatibility issues or code changes required in the core library to support Python 3.12, nor does it address potential edge cases or dependencies that might break with the version update. Additionally, while the GitHub workflow file update is mentioned, it is not included in the provided code changes, leaving some uncertainty about the full scope of the task. Overall, the goal and primary actions are clear, but some minor details are missing.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it primarily involves straightforward modifications to configuration files and metadata. The code changes provided are minimal and mostly cosmetic, such as updating the required Python version in setup.py, removing outdated classifiers, and updating copyright years in various files. The scope of changes is limited to a few files (setup.py, .gitignore, Makefile, and a core file for copyright update) and does not impact the core functionality or architecture of the codebase. No deep understanding of the codebase, complex algorithms, or advanced language features is required\u2014just basic knowledge of Python packaging and setup tools. While the problem statement mentions updating the test workflow to include Python 3.12, this is not reflected in the provided code changes, but even if included, it would likely be a simple update to a CI/CD configuration file, which is still a low-complexity task. There are no significant edge cases or error handling requirements mentioned or implied in the changes, as this is primarily a versioning update rather than a functional modification. Overall, this task is very easy and suitable for a junior developer or someone with basic familiarity with Python project configuration.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Address GCC 13.2 compilation issues\n### Description\r\n\r\nThe forth coming LTR Ubuntu 24.04 (on April 25, 2024) is distributed with GCC 13.2 compiler.\r\n\r\nITK should build cleanly with this new standard compiler.\r\n\r\n### Steps to Reproduce\r\n\r\n1. clone itk\r\n2. build in release mode (with Examples turned on)\r\n\r\n\r\n### Expected behavior\r\n\r\nno build warnings\r\nno failing tests\r\n\r\n### Actual behavior\r\n\r\n3 test failures\r\n```\r\n\t1012 - itkGDCMLegacyMultiFrameTest (Failed)\r\n\t1021 - itkGDCMImageReadWriteTest_MultiFrameMRIZSpacing (Failed)\r\n\t1023 - itkGDCM_ComplianceTest_singlebit (Failed)\r\n```\r\n\r\n199+ compiler warnings\r\n\r\n```\r\n```\r\n\r\n### Reproducibility\r\nall the time\r\n\r\n### Versions\r\nITK master branch on 2024-04-23\r\n\r\ngit rev-parse --short HEAD\r\n```\r\n3235b3675c\r\n```\r\n\r\n### Environment\r\n\r\nUbuntu 24.04 \r\nGCC 13.2 compiler\r\n\r\n\r\n### Additional Information\r\n\r\n```\r\n242 |   alignas(SizeValueType) SizeValueType m_InternalArray[VDimension];\r\n      |                                        ^~~~~~~~~~~~~~~\r\nIn member function \u2018constexpr const itk::Size<VDimension>::value_type& itk::Size<VDimension>::operator[](size_type) const [with unsigned int VDimension = 2]\u2019,\r\n    inlined from \u2018itk::Neighborhood<TPixel, VDimension, TAllocator>::SizeValueType itk::Neighborhood<TPixel, VDimension, TAllocator>::GetSize(DimensionValueType) const [with TPixel = float; unsigned int VDimension = 2; TAllocator = itk::NeighborhoodAllocator<float>]\u2019 at [Modules/Core/Common/include/itkNeighborhood.h:147](https://github.com/InsightSoftwareConsortium/ITK/blob/master/Modules/Core/Common/include/itkNeighborhood.h#L147):18,\r\n    inlined from \u2018void itk::NeighborhoodOperator<TPixel, VDimension, TAllocator>::FillCenteredDirectional(const CoefficientVector&) [with TPixel = float; unsigned int VDimension = 2; TAllocator = itk::NeighborhoodAllocator<float>]\u2019 at [Modules/Core/Common/include/itkNeighborhoodOperator.hxx:107](https://github.com/InsightSoftwareConsortium/ITK/blob/master/Modules/Core/Common/include/itkNeighborhoodOperator.hxx#L107):43,\r\n    inlined from \u2018void itk::DerivativeOperator<TPixel, VDimension, TAllocator>::Fill(const typename Superclass::CoefficientVector&) [with TPixel = float; unsigned int VDimension = 2; TAllocator = itk::NeighborhoodAllocator<float>]\u2019 at [Modules/Core/Common/include/itkDerivativeOperator.h:116](https://github.com/InsightSoftwareConsortium/ITK/blob/master/Modules/Core/Common/include/itkDerivativeOperator.h#L116):40,\r\n    inlined from \u2018void itk::NeighborhoodOperator<TPixel, VDimension, TAllocator>::CreateToRadius(const typename Superclass::SizeType&) [with TPixel = float; unsigned int VDimension = 2; TAllocator = itk::NeighborhoodAllocator<float>]\u2019 at [Modules/Core/Common/include/itkNeighborhoodOperator.hxx:81](https://github.com/InsightSoftwareConsortium/ITK/blob/master/Modules/Core/Common/include/itkNeighborhoodOperator.hxx#L81):13:\r\n\r\n```\r\n\r\n\n", "patch": "diff --git a/.clang-format b/.clang-format\nindex f75427a27dc..69950423abc 100644\n--- a/.clang-format\n+++ b/.clang-format\n@@ -152,7 +152,6 @@ StatementMacros:\n   - ITK_GCC_SUPPRESS_Wformat_nonliteral\n   - CLANG_PRAGMA_PUSH\n   - CLANG_PRAGMA_POP\n-  - CLANG_SUPPRESS_Wcpp14_extensions\n   - INTEL_PRAGMA_WARN_PUSH\n   - INTEL_PRAGMA_WARN_POP\n   - INTEL_SUPPRESS_warning_1292\ndiff --git a/Modules/Core/Common/include/itkExtractImageFilter.hxx b/Modules/Core/Common/include/itkExtractImageFilter.hxx\nindex 430b7877d68..2b03120c2e2 100644\n--- a/Modules/Core/Common/include/itkExtractImageFilter.hxx\n+++ b/Modules/Core/Common/include/itkExtractImageFilter.hxx\n@@ -62,7 +62,6 @@ ExtractImageFilter<TInputImage, TOutputImage>::SetExtractionRegion(InputImageReg\n                 \"InputImageDimension must be greater than OutputImageDimension\");\n   m_ExtractionRegion = extractRegion;\n \n-  unsigned int        nonzeroSizeCount = 0;\n   InputImageSizeType  inputSize = extractRegion.GetSize();\n   OutputImageSizeType outputSize;\n   outputSize.Fill(0);\n@@ -73,12 +72,16 @@ ExtractImageFilter<TInputImage, TOutputImage>::SetExtractionRegion(InputImageReg\n    * check to see if the number of non-zero entries in the extraction region\n    * matches the number of dimensions in the output image.\n    */\n+  unsigned int nonzeroSizeCount = 0;\n   for (unsigned int i = 0; i < InputImageDimension; ++i)\n   {\n     if (inputSize[i])\n     {\n-      outputSize[nonzeroSizeCount] = inputSize[i];\n-      outputIndex[nonzeroSizeCount] = extractRegion.GetIndex()[i];\n+      if (nonzeroSizeCount < OutputImageDimension)\n+      {\n+        outputSize[nonzeroSizeCount] = inputSize[i];\n+        outputIndex[nonzeroSizeCount] = extractRegion.GetIndex()[i];\n+      }\n       ++nonzeroSizeCount;\n     }\n   }\n@@ -116,11 +119,7 @@ ExtractImageFilter<TInputImage, TOutputImage>::GenerateOutputInformation()\n   outputPtr->SetLargestPossibleRegion(m_OutputImageRegion);\n \n   // Set the output spacing and origin\n-  const ImageBase<InputImageDimension> * phyData;\n-\n-  phyData = dynamic_cast<const ImageBase<InputImageDimension> *>(this->GetInput());\n-\n-  if (phyData)\n+  if (this->GetInput())\n   {\n     // Copy what we can from the image from spacing and origin of the input\n     // This logic needs to be augmented with logic that select which\ndiff --git a/Modules/Core/Common/include/itkMacro.h b/Modules/Core/Common/include/itkMacro.h\nindex 480c4c2c6d1..031a43e6e04 100644\n--- a/Modules/Core/Common/include/itkMacro.h\n+++ b/Modules/Core/Common/include/itkMacro.h\n@@ -120,15 +120,16 @@ namespace itk\n #if defined(__clang__) && defined(__has_warning)\n #  define CLANG_PRAGMA_PUSH ITK_PRAGMA(clang diagnostic push)\n #  define CLANG_PRAGMA_POP ITK_PRAGMA(clang diagnostic pop)\n-#  if __has_warning(\"-Wc++14-extensions\")\n-#    define CLANG_SUPPRESS_Wcpp14_extensions ITK_PRAGMA(clang diagnostic ignored \"-Wc++14-extensions\")\n-#  else\n-#    define CLANG_SUPPRESS_Wcpp14_extensions\n-#  endif\n #else\n #  define CLANG_PRAGMA_PUSH\n #  define CLANG_PRAGMA_POP\n-#  define CLANG_SUPPRESS_Wcpp14_extensions\n+#endif\n+\n+#if !defined(ITK_LEGACY_REMOVE)\n+// Issue warning if deprecated preprocessor flag is used.\n+#  define CLANG_SUPPRESS_Wcpp14_extensions                                                                \\\n+    [[deprecated(\"Remove deprecated CLANG_SUPPRESS_Wcpp14_extensions c++14 warning suppression\")]] void * \\\n+      CLANG_SUPPRESS_Wcpp14_extensions = nullptr;\n #endif\n \n // Intel compiler convenience macros\ndiff --git a/Modules/Core/Common/include/itkMultiThreaderBase.h b/Modules/Core/Common/include/itkMultiThreaderBase.h\nindex 299d72cfcd8..5aa630aca60 100644\n--- a/Modules/Core/Common/include/itkMultiThreaderBase.h\n+++ b/Modules/Core/Common/include/itkMultiThreaderBase.h\n@@ -238,8 +238,6 @@ ITK_GCC_PRAGMA_DIAG_PUSH()\n ITK_GCC_PRAGMA_DIAG(ignored \"-Wattributes\")\n INTEL_PRAGMA_WARN_PUSH\n INTEL_SUPPRESS_warning_1292\n-CLANG_PRAGMA_PUSH\n-CLANG_SUPPRESS_Wcpp14_extensions\n   // clang-format on\n #  ifdef ITK_LEGACY_SILENT\n   struct ThreadInfoStruct\n@@ -247,7 +245,6 @@ CLANG_SUPPRESS_Wcpp14_extensions\n   struct [[deprecated(\"Use WorkUnitInfo, ThreadInfoStruct is deprecated since ITK 5.0\")]] ThreadInfoStruct\n #  endif\n     // clang-format off\n-CLANG_PRAGMA_POP\n INTEL_PRAGMA_WARN_POP\n   // clang-format on\n   {\ndiff --git a/Modules/Core/Common/include/itkNeighborhood.h b/Modules/Core/Common/include/itkNeighborhood.h\nindex 511f50314cd..6456ed81d84 100644\n--- a/Modules/Core/Common/include/itkNeighborhood.h\n+++ b/Modules/Core/Common/include/itkNeighborhood.h\n@@ -136,7 +136,7 @@ class ITK_TEMPLATE_EXPORT Neighborhood\n   SizeValueType\n   GetRadius(DimensionValueType n) const\n   {\n-    return m_Radius[n];\n+    return m_Radius.at(n);\n   }\n \n   /** Returns the size (total length) of the neighborhood along\n@@ -144,7 +144,7 @@ class ITK_TEMPLATE_EXPORT Neighborhood\n   SizeValueType\n   GetSize(DimensionValueType n) const\n   {\n-    return m_Size[n];\n+    return m_Size.at(n);\n   }\n \n   /** Returns the size (total length of sides) of the neighborhood. */\ndiff --git a/Modules/Core/Common/include/itkNeighborhoodOperator.h b/Modules/Core/Common/include/itkNeighborhoodOperator.h\nindex a95d39f3f7b..55232f9e845 100644\n--- a/Modules/Core/Common/include/itkNeighborhoodOperator.h\n+++ b/Modules/Core/Common/include/itkNeighborhoodOperator.h\n@@ -93,6 +93,11 @@ class ITK_TEMPLATE_EXPORT NeighborhoodOperator : public Neighborhood<TPixel, VDi\n   void\n   SetDirection(const unsigned long direction)\n   {\n+    if (direction >= VDimension)\n+    {\n+      itkExceptionMacro(<< \" Can not set direction \" << direction << \" greater than dimensionality of neighborhood \"\n+                        << VDimension);\n+    }\n     m_Direction = direction;\n   }\n \ndiff --git a/Modules/Filtering/ImageFusion/include/itkLabelMapContourOverlayImageFilter.hxx b/Modules/Filtering/ImageFusion/include/itkLabelMapContourOverlayImageFilter.hxx\nindex 39f36cec512..476359c02fe 100644\n--- a/Modules/Filtering/ImageFusion/include/itkLabelMapContourOverlayImageFilter.hxx\n+++ b/Modules/Filtering/ImageFusion/include/itkLabelMapContourOverlayImageFilter.hxx\n@@ -157,7 +157,7 @@ LabelMapContourOverlayImageFilter<TLabelMap, TFeatureImage, TOutputImage>::Befor\n   srad.Fill(typename RadiusType::SizeValueType{});\n   for (unsigned int i = 0, j = 0; i < ImageDimension; ++i)\n   {\n-    if (j != static_cast<unsigned int>(m_SliceDimension))\n+    if (j != static_cast<unsigned int>(m_SliceDimension) && (j < (ImageDimension - 1)))\n     {\n       srad[j] = m_ContourThickness[i];\n       ++j;\ndiff --git a/Modules/Filtering/ImageIntensity/include/itkSymmetricEigenAnalysisImageFilter.h b/Modules/Filtering/ImageIntensity/include/itkSymmetricEigenAnalysisImageFilter.h\nindex 795acebae12..e21b47d00e1 100644\n--- a/Modules/Filtering/ImageIntensity/include/itkSymmetricEigenAnalysisImageFilter.h\n+++ b/Modules/Filtering/ImageIntensity/include/itkSymmetricEigenAnalysisImageFilter.h\n@@ -56,7 +56,7 @@ class SymmetricEigenAnalysisFunction\n   inline TOutput\n   operator()(const TInput & x) const\n   {\n-    TOutput eigenValues;\n+    TOutput eigenValues{};\n \n     m_Calculator.ComputeEigenValues(x, eigenValues);\n     return eigenValues;\n", "instance_id": "InsightSoftwareConsortium__ITK-4608", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal: ensuring that ITK builds cleanly with GCC 13.2 on Ubuntu 24.04 without warnings or test failures. It provides specific steps to reproduce the issue, expected behavior, actual behavior (including specific failing tests), and relevant environment details. However, there are minor ambiguities and missing details. For instance, while it mentions 199+ compiler warnings, it does not provide a complete list or categorization of the types of warnings (only a snippet is shown). Additionally, the root causes of the test failures are not detailed, leaving some uncertainty about the scope of the underlying issues. Edge cases or specific constraints related to GCC 13.2 compatibility are also not explicitly mentioned. Overall, the statement is valid and clear but lacks some depth in describing the full extent of the issues and potential challenges.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files and modules within the ITK codebase, as seen in the diff (e.g., changes in `itkExtractImageFilter.hxx`, `itkNeighborhood.h`, `itkMacro.h`, etc.), indicating a need to understand interactions across different parts of the system. While the individual changes appear relatively small (e.g., replacing array access with `at()`, removing deprecated warning suppressions, adding bounds checking), they collectively address compatibility with a new compiler version (GCC 13.2), which often involves subtle issues like stricter standard compliance or new warnings.\n\nSecond, the problem requires understanding several technical concepts, including C++ language features (e.g., array bounds safety with `at()`, deprecated attributes, alignment issues with `alignas`), compiler-specific behaviors (GCC 13.2's stricter checks or new warnings), and ITK's internal architecture (e.g., image processing filters, neighborhood operators). Additionally, familiarity with build systems and compiler diagnostics is necessary to address the 199+ warnings mentioned.\n\nThird, the problem involves potential edge cases and error handling, as seen in modifications like adding a bounds check in `SetDirection()` of `NeighborhoodOperator`. While the problem statement does not explicitly detail all edge cases, the nature of compiler compatibility issues often uncovers hidden bugs or non-standard code practices that require careful handling. The failing tests (e.g., `itkGDCMLegacyMultiFrameTest`) suggest deeper issues that may not be fully resolved by the provided changes, adding to the complexity.\n\nFinally, while the changes do not appear to impact the core architecture of ITK, they require a deep understanding of the codebase to ensure that modifications do not introduce regressions, especially given the domain-specific nature of ITK (medical image processing). The combination of multi-file changes, compiler compatibility challenges, and the need to address test failures places this problem in the 0.6-0.8 range. I assign a score of 0.65, as it leans toward the lower end of \"Hard\" due to the relatively localized nature of the provided fixes, though unresolved warnings and test failures could push the difficulty higher depending on their root causes.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG]: 0.9.0 crashes with \"job does not define a strategy or interior matrix\"\n### Pre-submission checks\n\n- [X] I am **not** filing a feature request. These should be filed via the feature request form instead.\n- [X] I have looked through the [open issues](https://github.com/woodruffw/zizmor/issues?q=is%3Aissue+is%3Aopen+) for a duplicate report.\n\n### Expected behavior\n\nGiven this workflow file, zizmor 0.8.0 handles it fine:\r\n``` yaml\r\nname: Workflow\r\n\r\non:\r\n  workflow_call:\r\n    inputs:\r\n      os:\r\n        type: string\r\n\r\njobs:\r\n  deploy:\r\n    runs-on: ${{inputs.os}}\r\n    steps:\r\n      - run: echo \"Hello world\"\r\n```\n\n### Actual behavior\n\nzizmor 0.9.0 crashes with this message:\r\n>self-hosted-runner failed on release.yml\r\n>\r\n>Caused by:\r\n>    job does not define a strategy or interior matrix\r\n\n\n### Reproduction steps\n\n1. Run `zizmor --version` to verify version is `0.9.0`\r\n2. Save the YAML file above as `release.yml`\r\n3. Run `zizmor release.yml`\r\n4. Observe the error\n\n### Logs\n\n_No response_\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/src/audit/self_hosted_runner.rs b/src/audit/self_hosted_runner.rs\nindex c0122f3..733bb67 100644\n--- a/src/audit/self_hosted_runner.rs\n+++ b/src/audit/self_hosted_runner.rs\n@@ -107,7 +107,9 @@ impl WorkflowAudit for SelfHostedRunner {\n                 // The entire `runs-on:` is an expression, which may or may\n                 // not be a self-hosted runner when expanded, like above.\n                 LoE::Expr(exp) => {\n-                    let matrix = Matrix::try_from(&job)?;\n+                    let Ok(matrix) = Matrix::try_from(&job) else {\n+                        continue;\n+                    };\n \n                     let expansions = matrix.expanded_values;\n \n", "instance_id": "woodruffw__zizmor-284", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the issue, including expected behavior, actual behavior, reproduction steps, and a specific error message. The provided YAML file and the context of the crash in version 0.9.0 versus 0.8.0 help in understanding the problem's scope. However, there are minor ambiguities: the statement does not explicitly describe what constitutes a \"strategy or interior matrix\" in the context of the error, which could be critical for someone unfamiliar with the codebase or domain. Additionally, there are no mentions of potential edge cases or specific constraints beyond the provided example. Overall, it is clear enough to understand the issue but lacks some deeper context or exhaustive details about the error's root cause or related scenarios.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4). The code change provided is minimal, involving a single file (`self_hosted_runner.rs`) and a small modification (adding a conditional check to handle a failed `Matrix::try_from` conversion by continuing the loop). This suggests a straightforward bug fix that does not require extensive modifications or deep architectural changes. The technical concepts involved appear to be basic error handling and control flow in Rust, which are not particularly complex for an experienced developer. The scope of the change is limited to a specific part of the codebase, with no indication of widespread impact or interaction with multiple modules. However, understanding the context of the `Matrix` structure and its conversion logic might require some familiarity with the codebase, slightly elevating the difficulty beyond \"Very Easy.\" Edge cases and error handling are implicitly addressed by the fix (skipping invalid matrix conversions), but no complex edge case handling is evident or required based on the problem statement. Overall, this is a relatively simple bug fix that an intermediate developer with basic Rust knowledge could handle with minimal effort.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deviations from Berkeley TestFloat\nSome deviations and bugs in floating-point arithmetic were found when testing against Berkeley TestFloat:\r\n* [x] (#408) As already known, the sign of zero is incorrect as it doesn't consider the quantization mode. \r\n* [x] (#407) The result sometimes becomes infinity instead of the maximum normal number for certain quantization modes. This makes sense when reading the definitions again, e.g. for TO_POS (from the 754 standard): \"the result shall be the format\u2019s floating-point number (possibly +\u221e) closest to and no less than the infinitely precise result\". The result should therefore never become negative infinity for TO_POS.\r\n* [x] (#428) f16_mul sometimes fails for directed quantization modes, e.g. the following for TO_POS:\r\n```Python\r\nAPyFloat(1, 1, 1023, 5, 10) * APyFloat(1, 26, 80, 5, 10)\r\n# Becomes APyFloat(sign=0, exp=13, man=79, exp_bits=5, man_bits=10)\r\n# and not APyFloat(sign=0, exp=13, man=80, exp_bits=5, man_bits=10)\r\n```\r\n* [x] (#411) f32_mul sometimes results in a subnormal number instead of zero, however this isn't seen for f64_mul or f16_mul):\r\n```Python\r\nAPyFloat(0, 53, 32895, 8, 23) * APyFloat(1, 1, 1999049, 8, 23)\r\n# Becomes APyFloat(sign=1, exp=0, man=10184, exp_bits=8, man_bits=23)\r\n# and not APyFloat(sign=1, exp=0, man=0, exp_bits=8, man_bits=23)\r\n```\r\n* [x] Added bullet point. The mantissa is sometimes wrong when casting from a larger format to a smaller, similar to the previous bullet. See comment below.\r\n\r\nThese were found from running\r\n```cmd\r\npython run_berkeley_cases.py -op all -qm all -s 1 -l 1\r\n```\r\nbut dedicated test cases should of course be made while fixing these.\n", "patch": "diff --git a/.github/workflows/run_coverage.yml b/.github/workflows/run_coverage.yml\nindex 0e3936e09..3a668a990 100644\n--- a/.github/workflows/run_coverage.yml\n+++ b/.github/workflows/run_coverage.yml\n@@ -41,6 +41,7 @@ jobs:\n       run: |\n         pytest lib/test --cov-report=xml --cov=lib --log-level=DEBUG --color=yes\n     - name: Format coverage data\n+      if: ${{ !cancelled()}}\n       run: |\n         lcov --rc lcov_branch_coverage=1 --capture --directory . \\\n           --output-file coverage.info\n@@ -50,6 +51,7 @@ jobs:\n          find . -name '*.gc*' -delete\n         # gcovr --xml-pretty --exclude-unreachable-branches --print-summary -o cobertura.xml --root $PWD\n     - name: Upload coverage reports to Codecov\n+      if: ${{ !cancelled()}}\n       uses: codecov/codecov-action@v3\n       env:\n         CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\ndiff --git a/src/apyfloat.cc b/src/apyfloat.cc\nindex 5ef5eed13..89254cf76 100644\n--- a/src/apyfloat.cc\n+++ b/src/apyfloat.cc\n@@ -220,13 +220,14 @@ APyFloat APyFloat::_cast(\n         prev_man = remainder << (man_bits - subn_adjustment);\n     }\n \n-    if (new_exp < -static_cast<std::int64_t>(res.man_bits\n-        )) { // Exponent too small after rounding\n-        return res.construct_zero();\n-    }\n-\n     // Check if the number will be converted to a subnormal\n     if (new_exp <= 0) {\n+        if (new_exp < -static_cast<std::int64_t>(res.man_bits)) {\n+            // Exponent too small after rounding\n+            res.man = quantize_close_to_zero(sign, prev_man, quantization);\n+            res.exp = 0;\n+            return res;\n+        }\n         prev_man |= leading_one();\n         res.exp = 0;\n         // Cast mantissa\n@@ -1081,6 +1082,12 @@ APyFloat APyFloat::operator*(const APyFloat& y) const\n         }\n \n         if (tmp_exp <= 0) {\n+            if (tmp_exp < -static_cast<std::int64_t>(res.man_bits)) {\n+                // Exponent too small after rounding\n+                res.man = quantize_close_to_zero(sign, new_man, quantization);\n+                res.exp = 0;\n+                return res;\n+            }\n             new_man = (new_man >> (-tmp_exp + 1))\n                 | ((new_man & ((1 << (-tmp_exp + 1)) - 1)) != 0);\n             tmp_exp = 0;\ndiff --git a/src/apyfloat_util.h b/src/apyfloat_util.h\nindex a8bfac408..3e74c5356 100644\n--- a/src/apyfloat_util.h\n+++ b/src/apyfloat_util.h\n@@ -161,6 +161,21 @@ void APY_INLINE quantize_mantissa(\n     );\n }\n \n+man_t APY_INLINE\n+quantize_close_to_zero(bool sign, int64_t man, QuantizationMode quantization)\n+{\n+    switch (quantization) {\n+    case QuantizationMode::TRN_AWAY:\n+        return 1;\n+    case QuantizationMode::TRN:\n+        return sign;\n+    case QuantizationMode::TRN_INF:\n+        return !sign;\n+    default:\n+        return 0;\n+    }\n+}\n+\n //! Fast integer power by squaring.\n man_t ipow(man_t base, unsigned int n);\n \ndiff --git a/src/apyfloatarray.cc b/src/apyfloatarray.cc\nindex bda605b6e..3546ec3a6 100644\n--- a/src/apyfloatarray.cc\n+++ b/src/apyfloatarray.cc\n@@ -610,6 +610,13 @@ void APyFloatArray::hadamard_multiplication(\n             }\n \n             if (tmp_exp <= 0) {\n+                if (tmp_exp < -static_cast<std::int64_t>(res.man_bits)) {\n+                    // Exponent too small after rounding\n+                    man_t res_man\n+                        = quantize_close_to_zero(res_sign, new_man, quantization);\n+                    res.data[i] = { res_sign, 0, res_man };\n+                    continue;\n+                }\n                 // Shift and add sticky bit\n                 new_man = (new_man >> (-tmp_exp + 1))\n                     | ((new_man & ((1 << (-tmp_exp + 1)) - 1)) != 0);\n@@ -802,6 +809,13 @@ APyFloatArray APyFloatArray::operator*(const APyFloat& rhs) const\n             }\n \n             if (tmp_exp <= 0) {\n+                if (tmp_exp < -static_cast<std::int64_t>(res.man_bits)) {\n+                    // Exponent too small after rounding\n+                    man_t res_man\n+                        = quantize_close_to_zero(res_sign, new_man, quantization);\n+                    res.data[i] = { res_sign, 0, res_man };\n+                    continue;\n+                }\n                 // Shift and add sticky bit\n                 new_man = (new_man >> (-tmp_exp + 1))\n                     | ((new_man & ((1 << (-tmp_exp + 1)) - 1)) != 0);\n", "instance_id": "apytypes__apytypes-411", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in identifying the issues related to floating-point arithmetic deviations when tested against Berkeley TestFloat. It provides specific examples of bugs (e.g., incorrect sign of zero, infinity results, and mantissa errors during multiplication and casting) with concrete test cases in Python. These examples help in understanding the expected behavior versus the actual buggy output. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the quantization modes or their expected behaviors beyond brief mentions (e.g., TO_POS). Additionally, while it lists multiple issues, it lacks a detailed explanation of the root causes or specific constraints for each bug, which could lead to some uncertainty during implementation. The mention of creating dedicated test cases is noted, but no guidance on how to structure or prioritize these tests is provided. Overall, the statement is valid and mostly clear but misses some critical details and deeper context for a fully comprehensive understanding.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" category due to several factors across the evaluated dimensions. \n\n1. **Scope and Depth of Code Changes:** The code changes span multiple files (`apyfloat.cc`, `apyfloat_util.h`, `apyfloatarray.cc`) and involve modifications to core logic related to floating-point arithmetic, specifically in handling quantization and subnormal numbers during multiplication and casting. These changes are not trivial as they affect fundamental operations in the codebase. While the modifications are localized to specific functions, they require a precise understanding of how these changes impact the overall behavior of floating-point calculations across different formats (f16, f32, f64). The amount of code change is moderate, but the impact is significant due to the critical nature of floating-point precision.\n\n2. **Number of Technical Concepts:** Solving this problem requires a deep understanding of several technical concepts, including floating-point arithmetic (IEEE 754 standard), quantization modes, and handling of subnormal numbers and edge cases like infinity. Familiarity with the internal representation of floating-point numbers (sign, exponent, mantissa) and bitwise operations is essential. Additionally, the developer must understand the specific quantization behaviors (e.g., TRN_AWAY, TRN_INF) and how they affect rounding. While the concepts are not extremely advanced, their combination and application in this context add to the complexity.\n\n3. **Edge Cases and Error Handling:** The problem statement explicitly mentions several edge cases, such as handling very small exponents leading to subnormal numbers or zero, incorrect infinity results, and mantissa errors during format casting. The code changes introduce new logic (e.g., `quantize_close_to_zero`) to handle these edge cases based on quantization modes, which adds complexity. Properly addressing these edge cases requires careful consideration of all possible input combinations and their expected outputs as per the IEEE 754 standard, making this a non-trivial task.\n\n4. **Overall Complexity:** The problem requires a deep understanding of floating-point arithmetic nuances and the specific deviations from the Berkeley TestFloat standard. The changes, while not architectural, impact critical functionality and necessitate precision to avoid introducing new bugs. The need to align with a well-defined standard (IEEE 754) and handle multiple quantization modes further elevates the difficulty. However, it does not reach the \"Very Hard\" category (0.8-1.0) as it does not involve system-level redesign or highly intricate domain-specific challenges beyond floating-point arithmetic.\n\nIn summary, this problem is challenging due to the need for precise modifications in a critical area of the codebase, a solid grasp of floating-point standards, and careful handling of edge cases. A score of 0.65 reflects the hard but manageable nature of the task for an experienced developer with relevant expertise.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Python bindings currently require the use of English BIP39 mnemonics\nPresently, the Python bindings for this library expose a higher-level/convenience `derive_rsa_key_from_phrase` function that takes a 12-word English BIP39 mnemonic phrase and returns a derived RSA key. It does _not_ expose the lower-level Rust `derive_rsa_key` function (which takes bytes). Arguably, this is unnecessarily restrictive, as it prevents the use of other wordlists or derivation schemes in Python applications (and is a restriction which is not present for Rust applications -- which can call `derive_rsa_key` directly); generally speaking, it would be preferable if the choice of wordlist -- including whether to use a wordlist at all -- were ultimately left up to the caller/user, and not enforced as a requirement by this library.\r\n\r\nIn order to facilitate the use of other wordlists for Python applications (as well as to have APIs that are more consistent between Rust and Python consumers), the same lower-level (`derive_rsa_key`) function should be exposed via pyo3 bindings.\r\n\r\n\r\n\n", "patch": "diff --git a/deterministic_keygen.pyi b/deterministic_keygen.pyi\nindex b344fc5..e784ca0 100644\n--- a/deterministic_keygen.pyi\n+++ b/deterministic_keygen.pyi\n@@ -1,2 +1,3 @@\n def generate_phrase() -> str: ...\n+def derive_rsa_key(entropy: bytes, bit_size: int = 2048) -> str: ...\n def derive_rsa_key_from_phrase(phrase: str, bit_size: int = 2048) -> str: ...\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 8881f10..c6acc2c 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -3,6 +3,7 @@ use bip39::{Language, Mnemonic, MnemonicType};\n use blake3;\n use pyo3::exceptions::{PyValueError, PyRuntimeError};\n use pyo3::prelude::*;\n+use pyo3::types::PyBytes;\n use rand_chacha::ChaCha20Rng;\n use rand_chacha::rand_core::SeedableRng;\n use rsa::RsaPrivateKey;\n@@ -45,7 +46,6 @@ fn test_phrase_to_entropy() {\n }\n \n /// Derive an RSA key from a given vector of unsigned 8-bit integers.\n-// #[pyfunction]\n pub fn derive_rsa_key(entropy: &Vec<u8>, bit_size: usize) -> Result<String, Error> {\n     let seed: [u8; 32] = blake3::derive_key(RSA_CONTEXT, &entropy);\n     let mut rng = ChaCha20Rng::from_seed(seed);\n@@ -63,6 +63,17 @@ fn test_derive_rsa_key() {\n     assert_eq!(key1, key2);\n }\n \n+/// Derive an RSA key from a given sequence of bytes.\n+#[pyfunction]\n+#[pyo3(name = \"derive_rsa_key\")]\n+#[pyo3(signature = (entropy, bit_size = 2048))]\n+pub fn py_derive_rsa_key(entropy: &PyBytes, bit_size: usize) -> PyResult<String> {\n+    match derive_rsa_key(&Vec::from(entropy.as_bytes()), bit_size) {\n+        Err(error) => Err(PyRuntimeError::new_err(error.to_string())),\n+        Ok(key) => Ok(key),\n+    }\n+}\n+\n /// Derive an RSA key from a given BIP-39 mnemonic phrase.\n #[pyfunction]\n #[pyo3(signature = (phrase, bit_size = 2048))]\n@@ -81,6 +92,7 @@ pub fn derive_rsa_key_from_phrase(phrase: &str, bit_size: usize) -> PyResult<Str\n #[pymodule]\n fn deterministic_keygen(_py: Python, m: &PyModule) -> PyResult<()> {\n     m.add_function(wrap_pyfunction!(generate_phrase, m)?)?;\n+    m.add_function(wrap_pyfunction!(py_derive_rsa_key, m)?)?;\n     m.add_function(wrap_pyfunction!(derive_rsa_key_from_phrase, m)?)?;\n     Ok(())\n }\n", "instance_id": "crwood__deterministic-keygen-21", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly explains the current limitation of the Python bindings, which restrict users to English BIP39 mnemonics by only exposing a high-level function (`derive_rsa_key_from_phrase`). It also specifies the desired solution: exposing the lower-level `derive_rsa_key` function (which takes bytes as input) to Python via PyO3 bindings to allow flexibility in wordlist choice or derivation schemes. The goal, input, output, and rationale are explicitly defined, with no significant ambiguities. The statement also provides context about consistency between Rust and Python APIs, which further clarifies the intent. While edge cases or specific error handling requirements are not explicitly mentioned, the problem's scope and purpose are detailed enough to guide implementation effectively.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, primarily involving the addition of a new Python binding for an existing Rust function (`derive_rsa_key`). The modifications include uncommenting and adapting the Rust function for PyO3 exposure, adding a new wrapper function (`py_derive_rsa_key`), and updating the Python type hints file (`deterministic_keygen.pyi`). The changes do not impact the broader system architecture or require extensive refactoring, as the core logic of `derive_rsa_key` already exists in the Rust codebase. The amount of code change is minimal, with only a few lines added or modified across two files.\n\n2. **Technical Concepts Involved:** Solving this problem requires a basic understanding of PyO3 (the Rust-Python binding library), including how to annotate Rust functions for Python exposure and handle type conversions (e.g., `PyBytes` to `Vec<u8>`). Familiarity with Rust error handling and mapping errors to Python exceptions (via `PyResult`) is also necessary. However, these concepts are not particularly complex for someone with moderate experience in Rust and Python interoperability. No advanced algorithms, design patterns, or domain-specific knowledge beyond key derivation (already implemented) are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases or new error conditions to handle. The code changes show that error handling is implemented by converting Rust `Result` types to Python exceptions (`PyRuntimeError`), which is straightforward and leverages existing logic in the `derive_rsa_key` function. There are no indications of complex edge cases (e.g., invalid entropy input or bit size constraints) that need to be newly addressed beyond what is already in the Rust implementation.\n\n4. **Overall Complexity:** The task involves understanding a small part of the codebase (the key derivation logic and PyO3 bindings) and making targeted modifications. It does not require deep knowledge of the entire system or intricate interactions between modules. The primary challenge lies in correctly setting up the PyO3 binding, which is a routine task for developers familiar with Rust-Python interoperability.\n\nGiven these considerations, a difficulty score of 0.35 reflects the simplicity of the changes and the moderate but not advanced technical knowledge required. It is slightly above the lower end of the \"Easy\" range due to the need for familiarity with PyO3 bindings, which may not be trivial for complete beginners but is straightforward for experienced developers.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Trailing comment in YAML causes `Error marshaling to json: invalid character '}' looking for beginning of object key string` when output type is JSON\nSops v3.9.1\r\n\r\nTo reproduce:\r\n\r\n```\r\n$ echo -e 'foo: bar\\n#Hello world' | sops --input-type yaml --output-type yaml -e /dev/stdin > test.yaml\r\n$ sops -d test.yaml \r\nfoo: bar\r\n#Hello world\r\n$ sops --output-type json --decrypt --input-type yaml test.yaml \r\nError dumping file: Error marshaling to json: invalid character '}' looking for beginning of object key string\r\n```\r\n\r\n\n", "patch": "diff --git a/stores/json/store.go b/stores/json/store.go\nindex e9df1b554..3212b5b32 100644\n--- a/stores/json/store.go\n+++ b/stores/json/store.go\n@@ -186,18 +186,20 @@ func (store Store) encodeValue(v interface{}) ([]byte, error) {\n \n func (store Store) encodeArray(array []interface{}) ([]byte, error) {\n \tout := \"[\"\n-\tfor i, item := range array {\n+\tempty := true\n+\tfor _, item := range array {\n \t\tif _, ok := item.(sops.Comment); ok {\n \t\t\tcontinue\n \t\t}\n+\t\tif !empty {\n+\t\t\tout += \",\"\n+\t\t}\n \t\tv, err := store.encodeValue(item)\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n \t\tout += string(v)\n-\t\tif i != len(array)-1 {\n-\t\t\tout += \",\"\n-\t\t}\n+\t\tempty = false\n \t}\n \tout += \"]\"\n \treturn []byte(out), nil\n@@ -205,10 +207,14 @@ func (store Store) encodeArray(array []interface{}) ([]byte, error) {\n \n func (store Store) encodeTree(tree sops.TreeBranch) ([]byte, error) {\n \tout := \"{\"\n-\tfor i, item := range tree {\n+\tempty := true\n+\tfor _, item := range tree {\n \t\tif _, ok := item.Key.(sops.Comment); ok {\n \t\t\tcontinue\n \t\t}\n+\t\tif !empty {\n+\t\t\tout += \",\"\n+\t\t}\n \t\tv, err := store.encodeValue(item.Value)\n \t\tif err != nil {\n \t\t\treturn nil, fmt.Errorf(\"Error encoding value %s: %s\", v, err)\n@@ -218,9 +224,7 @@ func (store Store) encodeTree(tree sops.TreeBranch) ([]byte, error) {\n \t\t\treturn nil, fmt.Errorf(\"Error encoding key %s: %s\", k, err)\n \t\t}\n \t\tout += string(k) + `: ` + string(v)\n-\t\tif i != len(tree)-1 {\n-\t\t\tout += \",\"\n-\t\t}\n+\t\tempty = false\n \t}\n \treturn []byte(out + \"}\"), nil\n }\n", "instance_id": "getsops__sops-1647", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a trailing comment in YAML input causes an error when converting to JSON output using the `sops` tool. It provides a reproducible example with specific commands to demonstrate the bug, which is helpful for understanding the context and verifying the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly describe the expected behavior (e.g., should comments be ignored or preserved in JSON output?) or mention any specific constraints or edge cases beyond the provided example. Additionally, it lacks clarity on whether this issue affects other input/output type combinations or if there are broader implications. Despite these minor gaps, the core issue is understandable, and the reproduction steps are actionable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is limited to a single file (`store.go`) and specifically to two functions (`encodeArray` and `encodeTree`) within the JSON store module of the `sops` tool. The modifications involve adjusting how commas are added during JSON serialization to handle comments correctly, which is a relatively straightforward logic change. The amount of code changed is small, focusing on replacing index-based comma placement with a flag-based approach to track whether a non-comment element has been processed. \n\nSecond, the technical concepts required are basic: understanding JSON serialization, iterating over data structures (arrays and trees), and handling conditional logic in Go. No advanced language features, complex algorithms, or deep domain-specific knowledge (beyond familiarity with YAML/JSON formats) are needed. The code changes do not impact the broader system architecture or require understanding intricate interactions between modules.\n\nThird, regarding edge cases and error handling, the problem statement and code changes implicitly address the specific case of trailing comments in YAML input. However, no additional edge cases (e.g., nested comments, malformed input) or error handling modifications are mentioned or required in the provided diff, keeping the complexity low.\n\nOverall, this task requires understanding some code logic and making simple modifications to fix a specific bug, aligning with a difficulty score of 0.30. It is not trivial (as it involves logic changes rather than a typo fix), but it is far from requiring deep architectural changes or advanced technical expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update GitHub actions used in sdist and wheels workflow\nThe build sdist and wheels workflow currently gives the following warnings:\r\n\r\n```\r\nNode.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v2, actions/setup-python@v2, actions/upload-artifact@v2. For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/.\r\n```\n", "patch": "diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nindex d932b39..3a3bf55 100644\n--- a/.github/workflows/release.yml\n+++ b/.github/workflows/release.yml\n@@ -12,12 +12,13 @@ jobs:\n     name: Build sdist\n     runs-on: ubuntu-20.04\n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n \n-      - uses: actions/setup-python@v2\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n-          python-version: \"3.6\"\n+          # Python 3.8 is the oldest version supported by build\n+          python-version: \"3.8\"\n \n       - name: Install pip build\n         run: |\n@@ -28,7 +29,7 @@ jobs:\n         run: |\n           python -m build --sdist .\n \n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: sdist\n           path: |\n@@ -42,12 +43,13 @@ jobs:\n       # Build a wheel without speedups that can run on pure Python\n       GENSHI_BUILD_SPEEDUP: 0\n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n \n-      - uses: actions/setup-python@v2\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n-          python-version: \"3.6\"\n+          # Python 3.8 is the oldest version supported by build\n+          python-version: \"3.8\"\n \n       - name: Install pip build\n         run: |\n@@ -58,7 +60,7 @@ jobs:\n         run: |\n           python -m build --wheel .\n \n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: wheels\n           path: |\n@@ -78,9 +80,9 @@ jobs:\n \n     steps:\n       - name: Download build artifacts to local runner\n-        uses: actions/download-artifact@v2\n+        uses: actions/download-artifact@v4\n \n-      - uses: actions/setup-python@v2\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n           python-version: \"3.10\"\n", "instance_id": "edgewall__genshi-88", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to update deprecated GitHub Actions workflows to use newer versions of Node.js (from 16 to 20) as per the provided warning message. The goal is explicitly mentioned, and the code changes align with the stated objective. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify why specific versions (e.g., `actions/checkout@v4`, `actions/setup-python@v5`) were chosen or if there are compatibility concerns with other parts of the workflow or project dependencies. Additionally, it does not mention any potential risks or side effects of upgrading Python versions (from 3.6 to 3.8) alongside the Node.js updates. While the warning message provides context, the statement lacks explicit mention of testing or validation steps post-update. Overall, it is clear enough to understand the task but misses some minor contextual details that could impact the implementation.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves straightforward updates to configuration files with minimal technical complexity. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are confined to a single file (`.github/workflows/release.yml`) and involve updating version numbers for GitHub Actions (`checkout`, `setup-python`, `upload-artifact`, `download-artifact`) and a Python version change (from 3.6 to 3.8). The modifications are small in volume and do not impact the broader system architecture or require understanding complex interactions within the codebase. It\u2019s a localized configuration update.\n\n2. **Number of Technical Concepts:** The task requires basic familiarity with GitHub Actions and YAML configuration files. No advanced programming language features, algorithms, design patterns, or domain-specific knowledge are needed. The concept of dependency versioning and deprecation warnings is elementary for anyone with basic CI/CD experience.\n\n3. **Edge Cases and Error Handling:** The problem statement and code changes do not mention specific edge cases or error handling requirements. The updates are unlikely to introduce significant risks, though there could be minor compatibility issues (e.g., with the Python version bump or action behavior changes in newer versions). However, these are not explicitly addressed or complex enough to elevate the difficulty.\n\n4. **Overall Complexity:** This is a routine maintenance task that involves minimal risk and effort. It does not require deep debugging, performance optimization, or architectural redesign. The primary challenge might be verifying that the updated workflow runs without issues, but this is a standard step in CI/CD updates and not inherently difficult.\n\nGiven these points, a difficulty score of 0.15 reflects the very easy nature of the task, requiring only basic modifications and minimal technical depth. It is a task suitable for a junior developer or someone with basic familiarity with GitHub Actions.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CIViC harvest fails\n```\r\n[\ue0a0 temp-query-deprecation-fix \u2699 .venv] ~/code/metakb % metakb update -r civic                                                                                                                                                                                                                        \r\nHarvesting sources...                                                                                                                                                                                                                                                                                \r\nHarvesting CIViC...                                                                                                                                                                                                                                                                                  \r\n(CIViCPy cache is also being updated)                                                                                                                                                                                                                                                                \r\nTraceback (most recent call last):                                                                                                                                                                                                                                                                   \r\n  File \"/Users/jss009/code/metakb/.venv/bin/metakb\", line 8, in <module>                                                                                                                                                                                                                             \r\n    sys.exit(cli())                                                                                                                                                                                                                                                                                  \r\n             ^^^^^                                                                                                                                                                                                                                                                                   \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 1205, in __call__                                                                                                                                                                                     \r\n    return anyio.run(self._main, main, args, kwargs, **opts)                                                                                                                                                                                                                                         \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                         \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/anyio/_core/_eventloop.py\", line 74, in run                                                                                                                                                                                     \r\n    return async_backend.run(func, args, {}, backend_options)                                                                                                                                                                                                                                        \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                        \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2034, in run                                                                                                                                                                                 \r\n    return runner.run(wrapper())                                                                                                                                                                                                                                                                     \r\n           ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                                     \r\n  File \"/Users/jss009/.pyenv/versions/3.12.2/lib/python3.12/asyncio/runners.py\", line 118, in run                                                                                                                                                                                                    \r\n    return self._loop.run_until_complete(task)                                                                                                                                                                                                                                                       \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                       \r\n  File \"/Users/jss009/.pyenv/versions/3.12.2/lib/python3.12/asyncio/base_events.py\", line 685, in run_until_complete                                                                                                                                                                                 \r\n    return future.result()                                                                                                                                                                                                                                                                           \r\n           ^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                                           \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2022, in wrapper                                                                                                                                                                             \r\n    return await func(*args)                                                                                                                                                                                                                                                                         \r\n           ^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                                         \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 1208, in _main                                                                                                                                                                                        \r\n    return await main(*args, **kwargs)                                                                                                                                                                                                                                                               \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                               \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 1120, in main                                     \r\n    rv = await self.invoke(ctx)                                                                                                                  \r\n         ^^^^^^^^^^^^^^^^^^^^^^                                                                                                                  \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 1739, in invoke                                   \r\n    return await _process_result(await sub_ctx.command.invoke(sub_ctx))                                                                          \r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                           \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 1485, in invoke                                   \r\n    return await ctx.invoke(self.callback, **ctx.params)                                                                                         \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                         \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/asyncclick/core.py\", line 824, in invoke                                    \r\n    rv = await rv                                                                                                                                \r\n         ^^^^^^^^                                                                                                                                \r\n  File \"/Users/jss009/code/metakb/src/metakb/cli.py\", line 511, in update                                                                        \r\n    _harvest_sources(sources, refresh_source_caches)                                                                                             \r\n  File \"/Users/jss009/code/metakb/src/metakb/cli.py\", line 576, in _harvest_sources                \r\n    source = source_class(update_cache=True, update_from_remote=False)                                                                           \r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                           \r\n  File \"/Users/jss009/code/metakb/src/metakb/harvesters/civic.py\", line 43, in __init__                                       \r\n    civicpy.update_cache(from_remote_cache=update_from_remote)                                                                                   \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/civicpy/civic.py\", line 269, in update_cache  \r\n    genes = _get_elements_by_ids('gene', allow_cached=False, get_all=True)                                          \r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                        \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/civicpy/civic.py\", line 1203, in _get_elements_by_ids\r\n    response_elements = _request_all(element)                                                                                                    \r\n                        ^^^^^^^^^^^^^^^^^^^^^                                                                                                    \r\n  File \"/Users/jss009/code/metakb/.venv/lib/python3.12/site-packages/civicpy/civic.py\", line 1300, in _request_all            \r\n    response = resp.json()['data'][pluralize(element)]                                                                                           \r\n               ~~~~~~~~~~~^^^^^^^^                                                                                                               \r\nKeyError: 'data'\r\n```\r\n\r\nthis appears to be something about the schema that's fixed in `civicpy` 3.1.0, but because our dependency declaration is \"civicpy~=3.0.0\", we're not able to install it.\n", "patch": "diff --git a/Pipfile b/Pipfile\nindex 09ce25b8..df3b5eba 100644\n--- a/Pipfile\n+++ b/Pipfile\n@@ -9,7 +9,7 @@ gene-normalizer = {version = \"~=0.3.0-dev1\", extras = [\"etl\"]}\n variation-normalizer = \"~=0.8.2\"\n disease-normalizer = {version = \"~=0.4.0.dev3\", extras = [\"etl\"]}\n thera-py = {version = \"~=0.5.0.dev3\", extras = [\"etl\"]}\n-civicpy = \"~=3.0.0\"\n+civicpy = \"~=3.1\"\n requests = \"*\"\n pydantic = \"==2.*\"\n requests-cache = \"*\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 6d2eafbc..f99dfa4d 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -28,7 +28,7 @@ dependencies = [\n     \"variation-normalizer~=0.8.2\",\n     \"disease-normalizer[etl]~=0.4.0.dev3\",\n     \"thera-py[etl]~=0.5.0.dev3\",\n-    \"civicpy~=3.0.0\",\n+    \"civicpy~=3.1\",\n     \"requests\",\n     \"pydantic==2.*\",\n     \"requests-cache\",\n", "instance_id": "cancervariants__metakb-354", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a failure in the CIViC harvest process due to a schema-related error in the `civicpy` library, which is fixed in version 3.1.0 but blocked by the current dependency declaration of \"~=3.0.0\". The goal (updating the dependency to resolve the error) is implied, and the error traceback provides context about where the failure occurs. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly confirm whether updating to version 3.1.x will definitively resolve the issue or if additional changes might be required. It also lacks information about potential compatibility issues with other parts of the codebase when updating the dependency. Additionally, edge cases or risks associated with the update are not mentioned. Despite these minor gaps, the intent and primary issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward dependency update in two configuration files (Pipfile and pyproject.toml) to allow the installation of a newer version of the `civicpy` library (from \"~=3.0.0\" to \"~=3.1\"). The scope of the code change is minimal, affecting only version strings in configuration files, with no impact on the actual codebase logic, architecture, or interactions between modules. The technical concepts required are basic\u2014understanding dependency versioning and how to update it in Python project configurations (Pipenv and Poetry). There are no complex algorithms, design patterns, or domain-specific knowledge needed beyond basic Python package management. While the problem statement does not explicitly address edge cases or compatibility risks with the dependency update, the code change itself does not involve implementing error handling or addressing such concerns directly. The task is essentially a trivial configuration adjustment, fitting within the 0.0-0.2 range (Very Easy), and I assign a score of 0.1 to reflect the minimal effort and expertise required.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incorrect Deletion of Snapshot Metadata Due to OutOfMemoryError\n### Apache Iceberg version\n\n1.1.0\n\n### Query engine\n\nFlink\n\n### Please describe the bug \ud83d\udc1e\n\nwhen calling checkCommitStatus method, unexpected errors maybe occur,  such as OutOfMemoryError, during the checkCommitStatus method execution.  the code show as below:\r\n```\r\ncommitStatus =\r\n   checkCommitStatus(\r\n       viewName,\r\n       newMetadataLocation,\r\n       metadata.properties(),\r\n       () -> checkCurrentMetadataLocation(newMetadataLocation));\r\n```\r\nDuring the execution of the `org.apache.iceberg.hive.HiveViewOperations.checkCurrentMetadataLocation` method's refresh operation to download and update the current table metadata, memory consumption occurs, potentially leading to an `OutOfMemoryError`. \r\nIt is important to note that `Tasks` may not handle `Error` exceptions, instead throwing them directly. \r\nFinally, in the `finally` block, the cleanupMetadataAndUnlock function may delete the table snapshot metadata file, even if it has been recently committed.\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time\n", "patch": "diff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\nindex 518daaf6acd1..619f20ab87a3 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n@@ -292,6 +292,7 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n             database,\n             tableName,\n             e);\n+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n         commitStatus =\n             BaseMetastoreOperations.CommitStatus.valueOf(\n                 checkCommitStatus(newMetadataLocation, metadata).name());\ndiff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\nindex 4fc71299d457..dd0d5b3132d0 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\n@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.metastore.api.InvalidObjectException;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.iceberg.BaseMetastoreOperations;\n import org.apache.iceberg.BaseMetastoreTableOperations;\n import org.apache.iceberg.CatalogUtil;\n import org.apache.iceberg.ClientPool;\n@@ -226,6 +227,7 @@ public void doCommit(ViewMetadata base, ViewMetadata metadata) {\n             database,\n             viewName,\n             e);\n+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n         commitStatus =\n             checkCommitStatus(\n                 viewName,\n", "instance_id": "apache__iceberg-11576", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an `OutOfMemoryError` can occur during the `checkCommitStatus` method execution in Apache Iceberg when refreshing table metadata, potentially leading to incorrect deletion of snapshot metadata in a `finally` block. The context of the bug (involving Flink as the query engine and Hive operations) and the specific problematic behavior are outlined. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly describe the expected behavior or the desired fix (e.g., should the metadata deletion be conditional, or should the error be caught and handled differently?). Additionally, edge cases or specific conditions under which the `OutOfMemoryError` occurs are not detailed, which could impact the solution design. Despite these gaps, the core issue is understandable, and the provided code changes align with the described problem, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively small, affecting only two files (`HiveTableOperations.java` and `HiveViewOperations.java`) with minimal lines of code added (setting a default `CommitStatus.UNKNOWN` before calling `checkCommitStatus`). This suggests a focused fix rather than a broad architectural change. However, understanding the problem requires familiarity with Apache Iceberg\u2019s metadata management, Hive integration, and the specific behavior of commit status checks, which involves moderate complexity in terms of technical concepts. Additionally, the developer must consider the implications of an `OutOfMemoryError`\u2014a critical error condition that is not trivially handled\u2014and ensure that the fix does not introduce new issues in the commit workflow. While the provided code change appears straightforward, ensuring correctness likely requires deeper investigation into error propagation and metadata cleanup logic, as well as potential edge cases like other unhandled exceptions or concurrent operations. The problem does not seem to demand advanced domain-specific knowledge beyond the Iceberg/Hive ecosystem or complex algorithmic design, nor does it impact the broader system architecture significantly. Therefore, a difficulty score of 0.45 is appropriate, reflecting a medium-level challenge that requires understanding multiple concepts and careful error handling but does not reach the complexity of a hard or very hard problem.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Core: Refactor internal Avro reader to resolve schemas directly\nThis refactors the Avro generic reader so that it resolves schemas directly (like PyIceberg) rather than creating an Avro schema to trick Avro's `ResolvingDecoder` into projecting columns correctly.\r\n\r\nThis makes the read path easier to maintain because there is no need to hijack and rewrite schemas in `ProjectionDatumReader` using `BuildAvroProjection`. This should make it much easier to add default value support.\nCore: Add internal Avro reader\nThis adds an Avro reader that produces Iceberg's internal object model and uses `StructLike` rather than Avro's `IndexedRecord`.\n", "patch": "diff --git a/core/src/main/java/org/apache/iceberg/avro/AvroWithPartnerVisitor.java b/core/src/main/java/org/apache/iceberg/avro/AvroWithPartnerVisitor.java\nindex b23b195d959a..692c1ead3fbf 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroWithPartnerVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroWithPartnerVisitor.java\n@@ -37,7 +37,7 @@ public interface PartnerAccessors<P> {\n     P listElementPartner(P partnerList);\n   }\n \n-  static class FieldIDAccessors implements AvroWithPartnerVisitor.PartnerAccessors<Type> {\n+  public static class FieldIDAccessors implements AvroWithPartnerVisitor.PartnerAccessors<Type> {\n     private static final FieldIDAccessors INSTANCE = new FieldIDAccessors();\n \n     public static FieldIDAccessors get() {\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/ValueReaders.java b/core/src/main/java/org/apache/iceberg/avro/ValueReaders.java\nindex 246671076c31..67f53d3636a6 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/ValueReaders.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/ValueReaders.java\n@@ -194,11 +194,11 @@ public static ValueReader<Void> skipStruct(List<ValueReader<?>> readers) {\n    * @param idToConstant a map of field ID to constants values\n    * @return a read plan that is a list of (position, reader) pairs\n    */\n-  static List<Pair<Integer, ValueReader<?>>> buildReadPlan(\n+  public static List<Pair<Integer, ValueReader<?>>> buildReadPlan(\n       Types.StructType expected,\n       Schema record,\n       List<ValueReader<?>> fieldReaders,\n-      Map<Integer, Object> idToConstant) {\n+      Map<Integer, ?> idToConstant) {\n     Map<Integer, Integer> idToPos = idToPos(expected);\n \n     List<Pair<Integer, ValueReader<?>>> readPlan = Lists.newArrayList();\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkAvroReader.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkAvroReader.java\nindex 4622d2928ac4..7d92d963a9f4 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkAvroReader.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkAvroReader.java\n@@ -37,16 +37,28 @@\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.catalyst.InternalRow;\n \n+/**\n+ * @deprecated will be removed in 1.8.0; use SparkPlannedAvroReader instead.\n+ */\n+@Deprecated\n public class SparkAvroReader implements DatumReader<InternalRow>, SupportsRowPosition {\n \n   private final Schema readSchema;\n   private final ValueReader<InternalRow> reader;\n   private Schema fileSchema = null;\n \n+  /**\n+   * @deprecated will be removed in 1.8.0; use SparkPlannedAvroReader instead.\n+   */\n+  @Deprecated\n   public SparkAvroReader(org.apache.iceberg.Schema expectedSchema, Schema readSchema) {\n     this(expectedSchema, readSchema, ImmutableMap.of());\n   }\n \n+  /**\n+   * @deprecated will be removed in 1.8.0; use SparkPlannedAvroReader instead.\n+   */\n+  @Deprecated\n   @SuppressWarnings(\"unchecked\")\n   public SparkAvroReader(\n       org.apache.iceberg.Schema expectedSchema, Schema readSchema, Map<Integer, ?> constants) {\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkPlannedAvroReader.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkPlannedAvroReader.java\nnew file mode 100644\nindex 000000000000..dc4af24685b3\n--- /dev/null\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkPlannedAvroReader.java\n@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.data;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.avro.LogicalType;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.Decoder;\n+import org.apache.iceberg.avro.AvroWithPartnerVisitor;\n+import org.apache.iceberg.avro.SupportsRowPosition;\n+import org.apache.iceberg.avro.ValueReader;\n+import org.apache.iceberg.avro.ValueReaders;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+\n+public class SparkPlannedAvroReader implements DatumReader<InternalRow>, SupportsRowPosition {\n+\n+  private final Types.StructType expectedType;\n+  private final Map<Integer, ?> idToConstant;\n+  private ValueReader<InternalRow> reader;\n+\n+  public static SparkPlannedAvroReader create(org.apache.iceberg.Schema schema) {\n+    return create(schema, ImmutableMap.of());\n+  }\n+\n+  public static SparkPlannedAvroReader create(\n+      org.apache.iceberg.Schema schema, Map<Integer, ?> constants) {\n+    return new SparkPlannedAvroReader(schema, constants);\n+  }\n+\n+  private SparkPlannedAvroReader(\n+      org.apache.iceberg.Schema expectedSchema, Map<Integer, ?> constants) {\n+    this.expectedType = expectedSchema.asStruct();\n+    this.idToConstant = constants;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void setSchema(Schema fileSchema) {\n+    this.reader =\n+        (ValueReader<InternalRow>)\n+            AvroWithPartnerVisitor.visit(\n+                expectedType,\n+                fileSchema,\n+                new ReadBuilder(idToConstant),\n+                AvroWithPartnerVisitor.FieldIDAccessors.get());\n+  }\n+\n+  @Override\n+  public InternalRow read(InternalRow reuse, Decoder decoder) throws IOException {\n+    return reader.read(decoder, reuse);\n+  }\n+\n+  @Override\n+  public void setRowPositionSupplier(Supplier<Long> posSupplier) {\n+    if (reader instanceof SupportsRowPosition) {\n+      ((SupportsRowPosition) reader).setRowPositionSupplier(posSupplier);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends AvroWithPartnerVisitor<Type, ValueReader<?>> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ValueReader<?> record(Type partner, Schema record, List<ValueReader<?>> fieldReaders) {\n+      if (partner == null) {\n+        return ValueReaders.skipStruct(fieldReaders);\n+      }\n+\n+      Types.StructType expected = partner.asStructType();\n+      List<Pair<Integer, ValueReader<?>>> readPlan =\n+          ValueReaders.buildReadPlan(expected, record, fieldReaders, idToConstant);\n+\n+      // TODO: should this pass expected so that struct.get can reuse containers?\n+      return SparkValueReaders.struct(readPlan, expected.fields().size());\n+    }\n+\n+    @Override\n+    public ValueReader<?> union(Type partner, Schema union, List<ValueReader<?>> options) {\n+      return ValueReaders.union(options);\n+    }\n+\n+    @Override\n+    public ValueReader<?> array(Type partner, Schema array, ValueReader<?> elementReader) {\n+      return SparkValueReaders.array(elementReader);\n+    }\n+\n+    @Override\n+    public ValueReader<?> arrayMap(\n+        Type partner, Schema map, ValueReader<?> keyReader, ValueReader<?> valueReader) {\n+      return SparkValueReaders.arrayMap(keyReader, valueReader);\n+    }\n+\n+    @Override\n+    public ValueReader<?> map(Type partner, Schema map, ValueReader<?> valueReader) {\n+      return SparkValueReaders.map(SparkValueReaders.strings(), valueReader);\n+    }\n+\n+    @Override\n+    public ValueReader<?> primitive(Type partner, Schema primitive) {\n+      LogicalType logicalType = primitive.getLogicalType();\n+      if (logicalType != null) {\n+        switch (logicalType.getName()) {\n+          case \"date\":\n+            // Spark uses the same representation\n+            return ValueReaders.ints();\n+\n+          case \"timestamp-millis\":\n+            // adjust to microseconds\n+            ValueReader<Long> longs = ValueReaders.longs();\n+            return (ValueReader<Long>) (decoder, ignored) -> longs.read(decoder, null) * 1000L;\n+\n+          case \"timestamp-micros\":\n+            // Spark uses the same representation\n+            return ValueReaders.longs();\n+\n+          case \"decimal\":\n+            return SparkValueReaders.decimal(\n+                ValueReaders.decimalBytesReader(primitive),\n+                ((LogicalTypes.Decimal) logicalType).getScale());\n+\n+          case \"uuid\":\n+            return SparkValueReaders.uuids();\n+\n+          default:\n+            throw new IllegalArgumentException(\"Unknown logical type: \" + logicalType);\n+        }\n+      }\n+\n+      switch (primitive.getType()) {\n+        case NULL:\n+          return ValueReaders.nulls();\n+        case BOOLEAN:\n+          return ValueReaders.booleans();\n+        case INT:\n+          if (partner != null && partner.typeId() == Type.TypeID.LONG) {\n+            return ValueReaders.intsAsLongs();\n+          }\n+          return ValueReaders.ints();\n+        case LONG:\n+          return ValueReaders.longs();\n+        case FLOAT:\n+          if (partner != null && partner.typeId() == Type.TypeID.DOUBLE) {\n+            return ValueReaders.floatsAsDoubles();\n+          }\n+          return ValueReaders.floats();\n+        case DOUBLE:\n+          return ValueReaders.doubles();\n+        case STRING:\n+          return SparkValueReaders.strings();\n+        case FIXED:\n+          return ValueReaders.fixed(primitive.getFixedSize());\n+        case BYTES:\n+          return ValueReaders.bytes();\n+        case ENUM:\n+          return SparkValueReaders.enums(primitive.getEnumSymbols());\n+        default:\n+          throw new IllegalArgumentException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+}\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java\nindex 3cbf38d88bf4..7e65535f5ecb 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java\n@@ -32,6 +32,7 @@\n import org.apache.iceberg.avro.ValueReaders;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.Pair;\n import org.apache.iceberg.util.UUIDUtil;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n@@ -74,6 +75,11 @@ static ValueReader<ArrayBasedMapData> map(ValueReader<?> keyReader, ValueReader<\n     return new MapReader(keyReader, valueReader);\n   }\n \n+  static ValueReader<InternalRow> struct(\n+      List<Pair<Integer, ValueReader<?>>> readPlan, int numFields) {\n+    return new PlannedStructReader(readPlan, numFields);\n+  }\n+\n   static ValueReader<InternalRow> struct(\n       List<ValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n     return new StructReader(readers, struct, idToConstant);\n@@ -249,6 +255,38 @@ public ArrayBasedMapData read(Decoder decoder, Object reuse) throws IOException\n     }\n   }\n \n+  static class PlannedStructReader extends ValueReaders.PlannedStructReader<InternalRow> {\n+    private final int numFields;\n+\n+    protected PlannedStructReader(List<Pair<Integer, ValueReader<?>>> readPlan, int numFields) {\n+      super(readPlan);\n+      this.numFields = numFields;\n+    }\n+\n+    @Override\n+    protected InternalRow reuseOrCreate(Object reuse) {\n+      if (reuse instanceof GenericInternalRow\n+          && ((GenericInternalRow) reuse).numFields() == numFields) {\n+        return (InternalRow) reuse;\n+      }\n+      return new GenericInternalRow(numFields);\n+    }\n+\n+    @Override\n+    protected Object get(InternalRow struct, int pos) {\n+      return null;\n+    }\n+\n+    @Override\n+    protected void set(InternalRow struct, int pos, Object value) {\n+      if (value != null) {\n+        struct.update(pos, value);\n+      } else {\n+        struct.setNullAt(pos);\n+      }\n+    }\n+  }\n+\n   static class StructReader extends ValueReaders.StructReader<InternalRow> {\n     private final int numFields;\n \ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java\nindex 927084caea1c..eb97185e21f1 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java\n@@ -32,9 +32,9 @@\n import org.apache.iceberg.orc.ORC;\n import org.apache.iceberg.parquet.Parquet;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.data.SparkAvroReader;\n import org.apache.iceberg.spark.data.SparkOrcReader;\n import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.spark.data.SparkPlannedAvroReader;\n import org.apache.iceberg.types.TypeUtil;\n import org.apache.spark.sql.catalyst.InternalRow;\n \n@@ -77,7 +77,7 @@ private CloseableIterable<InternalRow> newAvroIterable(\n         .reuseContainers()\n         .project(projection)\n         .split(start, length)\n-        .createReaderFunc(readSchema -> new SparkAvroReader(projection, readSchema, idToConstant))\n+        .createReaderFunc(readSchema -> SparkPlannedAvroReader.create(projection, idToConstant))\n         .withNameMapping(nameMapping())\n         .build();\n   }\n", "instance_id": "apache__iceberg-11299", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in its intent to refactor the Avro reader in Apache Iceberg to resolve schemas directly and simplify maintenance by eliminating the need for schema hijacking in the `ProjectionDatumReader`. It also mentions the addition of an internal Avro reader using `StructLike` instead of Avro's `IndexedRecord`. However, the statement lacks critical details such as specific input/output formats, explicit constraints, or examples of how the refactoring impacts the read path. Additionally, there is no mention of edge cases or specific performance goals. While the high-level goal is understandable, these missing details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving multiple files across the Apache Iceberg core and Spark modules, including the creation of a new class (`SparkPlannedAvroReader`) and modifications to existing readers and utility classes. This indicates a need to understand interactions between different parts of the codebase, particularly how Avro schema resolution integrates with Iceberg's internal object model and Spark's data processing. Second, the technical concepts required are complex, including deep knowledge of Avro's schema resolution, Iceberg's type system, Spark's `InternalRow` representation, and visitor patterns for schema traversal. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes (e.g., handling logical types like `timestamp-millis` and `decimal`) suggest the need to manage various data type conversions and potential compatibility issues, which adds to the complexity. Finally, the refactoring impacts a core component of the data reading path, which could have downstream effects on performance and correctness, requiring careful consideration. A score of 0.75 reflects the need for a deep understanding of the codebase and the complexity of the modifications, though it does not reach the \"Very Hard\" range as it does not appear to involve system-level redesign or highly specialized domain knowledge beyond data processing frameworks.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "JetstreamContext.subscribe() ignores deliver_subject from config argument\n### Observed behavior\n\nWhen `.subscribe()` of `JetstreamContext` is called, and the specified consumer does not exist, the client creates a consumer based on the given config object. However, attribute deliver_subject of config object is ignored, and random inbox subject is used instead.\r\nCreating consumer works fine when using JetstreamManager.\n\n### Expected behavior\n\nCreated consumer should use specified deliver_subject.\n\n### Server and client version\n\nServer version 2.10.21\r\nClient version 2.9.0\n\n### Host environment\n\n_No response_\n\n### Steps to reproduce\n\n1. Create a stream named \"test\".\r\n2. Run the following code:\r\n```python\r\nasync def main():\r\n    nc = await connect(\"nats://localhost:4222\")\r\n    js = nc.jetstream()\r\n\r\n    deliver_subject = \"stream-test.consume3\"\r\n    cc = ConsumerConfig(\r\n        name=\"consumer3\",\r\n        deliver_subject=deliver_subject,\r\n        ack_policy=AckPolicy.ALL,\r\n        deliver_policy=DeliverPolicy.NEW,\r\n        durable_name=\"consumer3\",\r\n    )\r\n\r\n    sub = await js.subscribe(\"\", stream=\"test\", durable=\"consumer3\", cb=cb, config=cc)\r\n    print(\"Subscribed\")\r\n    while True:\r\n        await asyncio.sleep(5)\r\n\r\n```\r\n3. Verify created consumer `nats consumer info test consumer3`:\r\n```\r\nInformation for Consumer test > consumer3 created 2024-09-29T15:45:46+02:00\r\n\r\nConfiguration:\r\n\r\n                Name: consumer3\r\n    Delivery Subject: _INBOX.rvFDfLPnsefqQD5R3OOsOT\r\n      Deliver Policy: New\r\n          Ack Policy: All\r\n            Ack Wait: 30s\r\n       Replay Policy: Instant\r\n     Max Ack Pending: 1,000\r\n        Flow Control: false\r\n\r\nState:\r\n\r\n   Last Delivered Message: Consumer sequence: 0 Stream sequence: 113\r\n     Acknowledgment floor: Consumer sequence: 0 Stream sequence: 0\r\n         Outstanding Acks: 0 out of maximum 1,000\r\n     Redelivered Messages: 0\r\n     Unprocessed Messages: 0\r\n          Active Interest: Active\r\n```\nJetstreamContext.subscribe() ignores deliver_subject from config argument\n### Observed behavior\n\nWhen `.subscribe()` of `JetstreamContext` is called, and the specified consumer does not exist, the client creates a consumer based on the given config object. However, attribute deliver_subject of config object is ignored, and random inbox subject is used instead.\r\nCreating consumer works fine when using JetstreamManager.\n\n### Expected behavior\n\nCreated consumer should use specified deliver_subject.\n\n### Server and client version\n\nServer version 2.10.21\r\nClient version 2.9.0\n\n### Host environment\n\n_No response_\n\n### Steps to reproduce\n\n1. Create a stream named \"test\".\r\n2. Run the following code:\r\n```python\r\nasync def main():\r\n    nc = await connect(\"nats://localhost:4222\")\r\n    js = nc.jetstream()\r\n\r\n    deliver_subject = \"stream-test.consume3\"\r\n    cc = ConsumerConfig(\r\n        name=\"consumer3\",\r\n        deliver_subject=deliver_subject,\r\n        ack_policy=AckPolicy.ALL,\r\n        deliver_policy=DeliverPolicy.NEW,\r\n        durable_name=\"consumer3\",\r\n    )\r\n\r\n    sub = await js.subscribe(\"\", stream=\"test\", durable=\"consumer3\", cb=cb, config=cc)\r\n    print(\"Subscribed\")\r\n    while True:\r\n        await asyncio.sleep(5)\r\n\r\n```\r\n3. Verify created consumer `nats consumer info test consumer3`:\r\n```\r\nInformation for Consumer test > consumer3 created 2024-09-29T15:45:46+02:00\r\n\r\nConfiguration:\r\n\r\n                Name: consumer3\r\n    Delivery Subject: _INBOX.rvFDfLPnsefqQD5R3OOsOT\r\n      Deliver Policy: New\r\n          Ack Policy: All\r\n            Ack Wait: 30s\r\n       Replay Policy: Instant\r\n     Max Ack Pending: 1,000\r\n        Flow Control: false\r\n\r\nState:\r\n\r\n   Last Delivered Message: Consumer sequence: 0 Stream sequence: 113\r\n     Acknowledgment floor: Consumer sequence: 0 Stream sequence: 0\r\n         Outstanding Acks: 0 out of maximum 1,000\r\n     Redelivered Messages: 0\r\n     Unprocessed Messages: 0\r\n          Active Interest: Active\r\n```\n", "patch": "diff --git a/nats/js/client.py b/nats/js/client.py\nindex bc951be1..fac3fc73 100644\n--- a/nats/js/client.py\n+++ b/nats/js/client.py\n@@ -414,9 +414,10 @@ async def cb(msg):\n             if inactive_threshold:\n                 config.inactive_threshold = inactive_threshold\n \n-            # Create inbox for push consumer.\n-            deliver = self._nc.new_inbox()\n-            config.deliver_subject = deliver\n+            # Create inbox for push consumer, if deliver_subject is not assigned already.\n+            if config.deliver_subject is None:\n+                deliver = self._nc.new_inbox()\n+                config.deliver_subject = deliver\n \n             # Auto created consumers use the filter subject.\n             config.filter_subject = subject\n", "instance_id": "nats-io__nats.py-615", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `JetstreamContext.subscribe()` method ignores the `deliver_subject` specified in the configuration and instead uses a random inbox subject. The expected behavior is explicitly stated, and steps to reproduce the issue are provided with a code snippet and output from a verification command. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., what happens if `deliver_subject` is intentionally left as `None` or if there are conflicts with existing subjects). Additionally, there is no mention of constraints or specific requirements for backward compatibility or performance. While the issue is well-documented with examples, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is minimal and localized to a single file (`nats/js/client.py`) and a specific function. It involves a small modification of 3-4 lines to conditionally set the `deliver_subject` only if it is not already assigned. There is no impact on the broader system architecture or interactions with other modules, and the change is straightforward.\n\n2. **Number of Technical Concepts:** Solving this issue requires basic familiarity with Python and an understanding of the NATS Jetstream client library's behavior for consumer configuration. No advanced language features, complex algorithms, or design patterns are involved. The concept of conditionally assigning a value based on whether it exists is a fundamental programming task.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code change does not introduce new error handling logic. However, a developer might need to consider minor edge cases, such as ensuring that an explicitly provided `deliver_subject` is valid or handling scenarios where `None` is intentional. These considerations are not complex and do not significantly increase the difficulty.\n\n4. **Overall Complexity:** The fix is a simple logic adjustment to respect the provided `deliver_subject` in the configuration rather than always overwriting it with a new inbox subject. The change does not require deep understanding of the codebase beyond the specific method being modified, and there are no performance or scalability concerns introduced by the fix.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only a basic understanding of the code logic and a small, targeted modification to resolve the bug.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "DuckDB SEGV when running ill-formed CREATE VIEW statement\n### What happens?\n\nThe latest version of the DuckDB (v1.1.3 19864453f7 and latest main: v1.1.3-dev165 19864453f7) crashes when running the following SQL statement: \r\n\r\n```sql\r\nCREATE VIEW any_view_name AS ( SHOW ALL TABLES );\r\n```\r\n\r\nHere is the stack trace from v1.1.3-dev165 19864453f7:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x0000aaaadba322f0 in duckdb::Transformer::TransformPivotStatement (this=0xffffe4d4e090, select=...) at /home/duckdb/duckdb/src/parser/transform/statement/transform_pivot_stmt.cpp:138\r\n#1  0x0000aaaadba37da4 in duckdb::Transformer::TransformSelectNodeInternal (this=this@entry=0xffffe4d4e090, select=..., is_select=is_select@entry=false) at /home/duckdb/duckdb/src/parser/transform/statement/transform_select.cpp:31\r\n#2  0x0000aaaadba39e4c in duckdb::Transformer::TransformSelectStmt (this=this@entry=0xffffe4d4e090, select=..., is_select=is_select@entry=false) at ../../src/include/duckdb/common/unique_ptr.hpp:39\r\n#3  0x0000aaaadba3a018 in duckdb::Transformer::TransformCreateView (this=this@entry=0xffffe4d4e090, stmt=...) at ../../src/include/duckdb/common/optional_ptr.hpp:42\r\n#4  0x0000aaaadb210e8c in duckdb::Transformer::TransformStatementInternal (this=this@entry=0xffffe4d4e090, stmt=...) at /home/duckdb/duckdb/src/parser/transformer.cpp:153\r\n#5  0x0000aaaadb211130 in duckdb::Transformer::TransformStatement (this=this@entry=0xffffe4d4e090, stmt=...) at /home/duckdb/duckdb/src/parser/transformer.cpp:59\r\n#6  0x0000aaaadb211038 in duckdb::Transformer::TransformStatementInternal (this=this@entry=0xffffe4d4e090, stmt=...) at /home/duckdb/duckdb/src/parser/transformer.cpp:139\r\n#7  0x0000aaaadb211130 in duckdb::Transformer::TransformStatement (this=this@entry=0xffffe4d4e090, stmt=...) at /home/duckdb/duckdb/src/parser/transformer.cpp:59\r\n#8  0x0000aaaadb211224 in duckdb::Transformer::TransformParseTree (this=this@entry=0xffffe4d4e090, tree=<optimized out>, statements=...) at ../../src/include/duckdb/common/optional_ptr.hpp:42\r\n#9  0x0000aaaadb215648 in duckdb::Parser::ParseQuery (this=this@entry=0xffffe4d4e3f8, query=\"CREATE VIEW any_view_name AS ( SHOW ALL TABLES );\") at /home/duckdb/duckdb/src/parser/parser.cpp:219\r\n#10 0x0000aaaadb1f4ecc in duckdb_shell_sqlite3_prepare_v2 (db=0xaaaae503c8c0, zSql=0xaaaae503a290 \"CREATE VIEW any_view_name AS ( SHOW ALL TABLES );\", nByte=<optimized out>, ppStmt=0xffffe4d4e620, pzTail=0xffffe4d4e628)\r\n    at ../../tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:176\r\n#11 0x0000aaaadb1e0f68 in shell_exec (pArg=0xffffe4d4e8b0, zSql=0xaaaae503a290 \"CREATE VIEW any_view_name AS ( SHOW ALL TABLES );\", pzErrMsg=<optimized out>) at ../../tools/shell/shell.c:12993\r\n#12 0x0000aaaadb1e2a28 in runOneSqlLine (p=0xffffe4d4e8b0, zSql=0xaaaae503a290 \"CREATE VIEW any_view_name AS ( SHOW ALL TABLES );\", startline=<optimized out>, in=<optimized out>) at ../../tools/shell/shell.c:19273\r\n#13 0x0000aaaadb1e9bc0 in process_input (p=0xffffe4d4e8b0) at ../../tools/shell/shell.c:19384\r\n#14 0x0000aaaadb1bfb90 in main (argc=<optimized out>, argv=0xffffe4d4fc18) at ../../tools/shell/shell.c:20206\r\n```\n\n### To Reproduce\n\n1. Clone the DuckDB Git from the official repo.\r\n2. Checkout to either the latest main (19864453f7) or release version: 1.1.3 (19864453f7).\r\n3. Compile the DuckDB binary by using either `make reldebug` or `make debug`.\r\n4. Run the compiled DuckDB and input the following SQL:\r\n\r\n```\r\nCREATE VIEW any_view_name AS ( SHOW ALL TABLES );\r\n```\n\n### OS:\n\nUbuntu 20.04 LTS\n\n### DuckDB Version:\n\n1.1.3\n\n### DuckDB Client:\n\ncli\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nYu Liang\n\n### Affiliation:\n\nPennsylvania State University\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n", "patch": "diff --git a/src/parser/transform/statement/transform_create_view.cpp b/src/parser/transform/statement/transform_create_view.cpp\nindex 8504ecfc9a91..24a26c022d0d 100644\n--- a/src/parser/transform/statement/transform_create_view.cpp\n+++ b/src/parser/transform/statement/transform_create_view.cpp\n@@ -21,7 +21,7 @@ unique_ptr<CreateStatement> Transformer::TransformCreateView(duckdb_libpgquery::\n \t}\n \tinfo->on_conflict = TransformOnConflict(stmt.onconflict);\n \n-\tinfo->query = TransformSelectStmt(*PGPointerCast<duckdb_libpgquery::PGSelectStmt>(stmt.query), false);\n+\tinfo->query = TransformSelectStmt(*stmt.query, false);\n \n \tPivotEntryCheck(\"view\");\n \n", "instance_id": "duckdb__duckdb-14705", "clarity": 3, "difficulty": 0.3, "clarity_explanation": "The problem statement is comprehensive and well-documented. It clearly describes the issue\u2014a segmentation fault (SEGV) in DuckDB when executing a specific ill-formed `CREATE VIEW` statement with a `SHOW ALL TABLES` subquery. The statement includes detailed reproduction steps, the exact SQL query causing the crash, a full stack trace, and relevant environment details such as OS, DuckDB version, and hardware. Additionally, it confirms that all necessary code and configuration for reproduction are provided. There are no significant ambiguities, and the goal (fixing the crash) is evident. The only minor omission is the lack of explicit mention of expected behavior (e.g., should the query fail gracefully with an error message?), but this can be inferred as preventing the crash. Overall, the problem description meets the criteria for a score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a minimal change in a single file (`transform_create_view.cpp`), specifically altering one line to fix a type casting issue. The modification does not impact multiple modules or the broader system architecture. It is a localized fix within the parser transformation logic, requiring a small amount of code change.\n\n2. **Number of Technical Concepts**: Solving this issue requires understanding of C++ (specifically pointer casting and type safety), familiarity with DuckDB's parser and transformer internals, and basic debugging skills to interpret the stack trace. While DuckDB's codebase is complex, the specific fix does not demand advanced concepts like intricate algorithms, design patterns, or domain-specific knowledge beyond the parser's role in SQL processing. The concepts involved are moderately complex but manageable for someone with intermediate C++ experience.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention additional edge cases beyond the specific crashing query. The fix itself does not introduce new error handling logic; it corrects an improper cast that likely causes a null pointer dereference or invalid memory access. However, understanding why this specific query causes a crash might require considering how `SHOW ALL TABLES` is handled differently from other subqueries in `CREATE VIEW`, which adds a slight layer of complexity. Still, this does not significantly elevate the difficulty.\n\n4. **Overall Assessment**: The issue is a straightforward bug fix involving a small, targeted code change. It requires some understanding of the DuckDB parser logic and C++ type casting, but it does not involve deep architectural changes, complex algorithms, or extensive edge case handling. The stack trace and reproduction steps make identifying the root cause relatively easy. Therefore, a difficulty score of 0.30 is appropriate, reflecting an \"Easy\" problem that requires understanding some code logic and making a simple modification.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Introduce regression tests with deterministic values\nSince we already know that the output of some metrics changes slightly when we switch to numpy 2.0 (see #42), we should protect against this early on. The best way I see to catch this is to have regression tests and check for _exact_ results on some sample data.\r\n\r\n- [x] Create a regression test branching out from main. Just use random data for each of the main algorithms.\r\n- [x] (Optional) check if there are any major branches in any of the algorithms that only trigger under specific conditions that are not met with the random data. (We can use the debugger for that.)\r\n- [x] (Optional) also test with algorithms that include randomness by keeping seed fixed.\r\n- [x] Store the exact values produced by the algorithms on this data.\r\n- [x] Document the tests.\r\n- [x] Merge tests into dev branch.\r\n- [x] Switch back to the poetry branch (using updated scikit-learn) and check if regression tests still run.\n", "patch": "diff --git a/.github/workflows/ci.yaml b/.github/workflows/ci.yaml\nindex 429bb80..bdc6637 100644\n--- a/.github/workflows/ci.yaml\n+++ b/.github/workflows/ci.yaml\n@@ -1,6 +1,7 @@\n on:\n   push:\n     branches: [\"main\", \"dev\", \"v*\"]\n+  pull_request:\n \n name: build\n \n@@ -9,14 +10,17 @@ jobs:\n     strategy:\n       matrix:\n         python: [\"3.7\", \"3.10\"]\n+        extras: [\"\", \"[RANSAC, qrandom, plots]\"]\n     runs-on: ubuntu-latest\n     steps:\n       - uses: actions/checkout@v4\n       - uses: actions/setup-python@v5\n         with:\n           python-version: ${{ matrix.python }}\n-      - run: pip install .\n+      - run: pip install \".${{ matrix.extras }}\"\n       - run: pip install codecov .\n+        env:\n+          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n       - run: coverage run -m unittest nolds.test_measures\n       - run: codecov\n-        if: ${{ matrix.python == '3.10' }}\n+        if: ${{ matrix.python == '3.10' && matrix.extras != '' }}\ndiff --git a/setup.py b/setup.py\nindex 9bd360a..ac929d4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -62,7 +62,7 @@ def run(self):\n         'setuptools'\n     ],\n     extras_require={\n-        'RANSAC': ['sklearn>=0.19'],\n+        'RANSAC': ['scikit-learn>=0.19'],\n         'qrandom': ['quantumrandom'],\n         'plots': ['matplotlib']\n     },\n", "instance_id": "CSchoel__nolds-54", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to introduce regression tests with deterministic values to catch changes in output due to library updates (e.g., numpy 2.0). The goals are outlined with specific tasks such as creating tests with random data, handling randomness with fixed seeds, and storing exact values for comparison. However, there are minor ambiguities and missing details. For instance, the problem does not specify the exact format or structure of the regression tests, nor does it mention how \"exact results\" should be stored or compared (e.g., file format, precision of values). Additionally, there is no mention of specific edge cases or constraints for the sample data to ensure comprehensive coverage. While the intent is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes appears limited to configuration files (e.g., CI workflow in `.github/workflows/ci.yaml` and `setup.py`) rather than core algorithmic logic, which reduces the overall complexity. However, the task requires understanding multiple technical concepts, such as setting up regression tests, handling randomness with fixed seeds, and integrating with CI pipelines for automated testing across different Python versions and library configurations. The changes also involve modifying dependencies (e.g., updating `scikit-learn` naming in `setup.py`) and ensuring compatibility with optional extras, which adds a layer of complexity. While the provided diff does not show the actual test implementation (which might increase difficulty), the visible changes suggest a moderate impact on the codebase. Edge case handling is implied (e.g., ensuring test coverage for specific algorithmic branches), but not explicitly detailed in the statement or diff, which keeps the difficulty from being higher. Overall, this task requires a solid understanding of testing practices, dependency management, and CI configuration, but does not appear to involve deep architectural changes or highly complex logic, placing it at 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Drop support for Python 3.8, update dependencies\nPython 3.8 is approaching end-of-life, and many of the packages we use have already dropped support. We should also drop support for Python 3.8 and upgrade dependencies to newer versions (e.g scipy, ipython, python-rapidjson).\nCompatibility Layer: Replace flake8 and black with ruff\nWe should use ruff for all of our linting and formatting needs.\nAdd check for security vulnerabilities to CI \nWe should use a tool like [osv](https://pypi.org/project/osv/) to check if any dependencies pyQuil requires has a vulnerability.\r\n\n`pyquil` is incompatible with `pyzx` because it depends on an old version of `lark`\nPre-Report Checklist\r\n--------------------\r\n\r\n- [x] I am running the latest versions of pyQuil and the Forest SDK\r\n- [x] I checked to make sure that this bug has not already been reported\r\n\r\nIssue Description\r\n-----------------\r\n\r\nIt is not possible to install `pyquil` and `pyzx` together in the same environment. I tried `pyquil` versions 3.2.0 to 4.8.0.\r\n\r\n\r\nHow to Reproduce\r\n----------------\r\n\r\n### Code Snippet\r\n\r\n```bash\r\npip install pyzx==0.8.0 pyquil==4.8.0\r\n```\r\n\r\n### Error Output\r\n\r\n```\r\nERROR: Cannot install pyquil==4.8.0 and pyzx==0.8.0 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    pyzx 0.8.0 depends on lark~=1.1.7\r\n    pyquil 4.8.0 depends on lark<0.12.0 and >=0.11.1\r\n```\r\n\r\nEnvironment Context\r\n-------------------\r\n\r\nOperating System: Linux\r\n\r\nPython Version (`python -V`): Python 3.10.12\r\n\r\nQuilc Version (`quilc --version`): N/A\r\n\r\nQVM Version (`qvm --version`): N/A\r\n\r\nPython Environment Details (`pip freeze` or `conda list`):\r\n\r\n```\r\n[empty: new virtual environment]\r\n```\r\n\n", "patch": "", "instance_id": "rigetti__pyquil-1782", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in outlining the tasks to be performed: dropping support for Python 3.8, updating dependencies, replacing linting/formatting tools with `ruff`, adding security vulnerability checks to CI, and resolving a dependency conflict between `pyquil` and `pyzx`. The goals for each task are understandable, and the issue with dependency conflict is well-documented with a reproducible example. However, there are minor ambiguities and missing details. For instance, the statement does not specify the target Python versions to support after dropping 3.8, nor does it provide detailed requirements for the CI security check (e.g., specific thresholds for vulnerabilities or integration steps). Additionally, there are no explicit mentions of edge cases or constraints for the dependency updates or tool replacements. Overall, while the problem is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of changes involves multiple aspects of the codebase, including dependency management (updating versions and resolving conflicts), tooling changes (replacing `flake8` and `black` with `ruff`), and CI pipeline modifications (adding security checks with `osv`). These changes likely span multiple configuration files (e.g., `pyproject.toml`, `requirements.txt`, CI scripts) and may require understanding interactions between dependencies and tools, though no actual code changes are provided in the snippet. Second, the technical concepts involved include familiarity with Python dependency management (e.g., `pip`, version constraints), linting/formatting tools, and CI/CD workflows, which are moderately complex but not overly advanced. Third, potential edge cases include ensuring compatibility with other dependencies after updates, handling breaking changes in newer package versions, and configuring `ruff` to match previous linting/formatting rules\u2014none of which are trivial but not extremely challenging either. Finally, while the changes do not appear to impact core system architecture, they require careful testing to avoid introducing issues in the development workflow. Overall, this problem requires a moderate level of expertise and effort, balancing multiple concepts and changes, hence a score of 0.50.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Unstable Vital Dynamics\nConsider a simulation with the following demographics\r\n```python\r\ndems = [\r\n    ss.Births(pars=dict(birth_rate=20)),\r\n    ss.Deaths(pars=dict(death_rate=20))\r\n]\r\n```\r\nOne might expect a stable population. However, the result is a dramatically declining population.\r\n<img width=\"612\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d1c127a0-4bee-4e7f-9382-d2399fb7ee25\">\r\n\r\nThis result is for two reasons:\r\n1. `Births` takes the floor when deterministically adding agents.\r\n`n_new = int(np.floor(sim.people.alive.count() * scaled_birth_prob))`\r\n\r\n2. Module order matters. Because `Births` comes before `Deaths` in the list of `dems`, agents are added to the population (resulting in a larger population), and then `Deaths` applies the death rate to _all_ agents, including the new agents. A 1% increase followed by a 1% decrease results in a net decline.\r\n\r\nSwitching the order of demographic module updates (and changing the Births floor to something like `sc.randround`) resolves the issue. Why? Because `Deaths` simply schedules agents for death via `ti_death` without actually removing them from the population. The `Births` module then adds agents considering the _same denominator_ as used for deaths.\r\n<img width=\"612\" alt=\"image\" src=\"https://github.com/user-attachments/assets/70d0173b-cae4-4377-a5db-c898b065e556\">\r\n\r\nWe cannot expect users to know they have to put `Deaths` before `Births` to get a stable population.\r\n\r\nNext Steps:\r\n- [ ] Change `Births` away from `floor`. It could apply a probability per-agent\r\n- [ ] Consider a combined `BirthDeath` module that would act on the same denominator.\r\n- [ ] Consider only adding births (including from `Pregnancy`) at the end of the time step, although that will likely create other problems.\r\n- [ ] Consider excluding agents born on the current time step from the death rate (although doing so could have other unintended consequences as we know postnatal mortality is typically quite high.)\r\n- [ ] Consider adjusting the death rate by the birth rate, if `Deaths` follows `Births`.\r\n\r\nThanks to Deven for raising this issue in the context of SEIRD in TBsim.\nmatch_time_inds fails\n```python\r\n    def match_time_inds(self, inds=None):\r\n         \"\"\" Find the nearest matching sim time indices for the current module \"\"\"\r\n         if inds is None: inds = Ellipsis\r\n         self_tvec = self.t.abstvec\r\n         sim_tvec = self.sim.t.abstvec\r\n         if len(self_tvec) == len(sim_tvec): # Shortcut to avoid doing matching\r\n             return inds\r\n         else:\r\n             out = sc.findnearest(sim_tvec, [inds])\r\n             return out\r\n```\r\nThe function is called with `inds=None`, so inds will be set to `Ellipsis`. But `sc.findnearest` doesn't like `Ellipsis`, resulting in an exception.\nStaticNet missing step\n\nAdd negative binomial distribution\n\nAddressing stochasticity in Births related to dicussion in Unstable Vital Dynamics #695\nReplacing random rounding with a binomial draw\r\n\r\n### Description\r\n\r\n\r\n### Checklist\r\n- [ ] Code commented & docstrings added\r\n- [ ] New tests were needed and have been added\r\n- [ ] A new version number was needed & changelog has been updated\r\n- [ ] A new PyPI version needs to be released\n", "patch": "File: starsimhub__starsim-778\nType: block\nName: \nLines: 12-66\nCode:\nfor more information.\n- A negative binomial distribution is now available as ``ss.nbinom()``.\n- ``ss.Births()`` now uses a binomial draw of births per timestep, rather than the expected value.\n- Added ``ss.load()`` and ``ss.save()`` functions, and removed ``ss.Sim.load()``.\n- *GitHub info*: PR `778 <https://github.com/starsimhub/starsim/pull/778>`_\n\n \n \n Version 2.1.1 (2024-11-08)\n Version 2.1.1 (2024-11-08)\n ---------------------------\n ---------------------------\nindex 7ae30537..b721ae3f 100644\nindex 7ae30537..b721ae3f 100644\n \n \n Note: Starsim is a general-purpose, multi-disease framework that builds on our previous suite of disease-specific models, which included `Covasim <https://covasim.org>`_, `HPVsim <https://hpvsim.org>`_, and `FPsim <https://fpsim.org>`_. In cases where a distinction needs to be made, Starsim is also known as the \"Starsim framework\", while this collection of other models is known as the \"Starsim suite\".\n Note: Starsim is a general-purpose, multi-disease framework that builds on our previous suite of disease-specific models, which included `Covasim <https://covasim.org>`_, `HPVsim <https://hpvsim.org>`_, and `FPsim <https://fpsim.org>`_. In cases where a distinction needs to be made, Starsim is also known as the \"Starsim framework\", while this collection of other models is known as the \"Starsim suite\".\n \n \nFor more information about Starsim, please see the `documentation <https://docs.starsim.org>`__.\nFor more information about Starsim, please see the `documentation <https://docs.starsim.org>`__. Information about Starsim for R is available at `r.starsim.org <https://r.starsim.org>`__.\n \n \n \n \n Requirements\n Requirements\n ------------\n ------------\n \n \nPython 3.9-3.12.\nPython 3.9-3.12 or R.\n \n \n We recommend, but do not require, installing Starsim in a virtual environment, such as `Anaconda <https://www.anaconda.com/products>`__.\n We recommend, but do not require, installing Starsim in a virtual environment, such as `Anaconda <https://www.anaconda.com/products>`__.\n \n \n Installation\n Installation\n ------------\n ------------\n \n \nPython\n~~~~~~\n\n Starsim is most easily installed via PyPI: ``pip install starsim``.\n Starsim is most easily installed via PyPI: ``pip install starsim``.\n \n \n Starsim can also be installed locally. To do this, clone first this repository, then run ``pip install -e .`` (don't forget the dot at the end!).\n Starsim can also be installed locally. To do this, clone first this repository, then run ``pip install -e .`` (don't forget the dot at the end!).\n", "instance_id": "starsimhub__starsim-778", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement for \"Unstable Vital Dynamics\" is mostly clear in describing the issue with the population simulation dynamics in the Starsim framework. It provides a detailed explanation of why the population declines unexpectedly due to the order of module updates (`Births` before `Deaths`) and the use of `floor` in birth calculations. The inclusion of images to illustrate the issue and the discussion of potential solutions (e.g., changing `Births` to use a probability per agent, combining modules, etc.) adds to the clarity. However, there are minor ambiguities and missing details. For instance, the problem statement lists multiple potential solutions without specifying a definitive approach or priority, leaving room for interpretation on what exactly needs to be implemented. Additionally, edge cases (e.g., postnatal mortality considerations) are mentioned but not fully explored or constrained, which could lead to uncertainty during implementation. The secondary issues mentioned, such as `match_time_inds` failing and the addition of a negative binomial distribution, are briefly described but lack detailed context or requirements, further reducing clarity. Overall, while the core issue is well-articulated, the lack of specificity in the solution path and incomplete details on secondary issues prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code changes, while not fully detailed in the provided snippet, appears to involve modifications to core simulation logic in the Starsim framework, specifically in how demographic modules (`Births` and `Deaths`) interact and calculate population changes. This likely requires changes across multiple files or modules (e.g., replacing `floor` with a binomial draw as mentioned) and a deep understanding of the simulation's timestep mechanics and module ordering. Second, the problem demands knowledge of specific technical concepts, including stochastic processes (e.g., binomial draws, negative binomial distribution), simulation modeling, and potentially Python libraries like NumPy or SciPy (`sc.randround`, `sc.findnearest`). Third, the problem involves handling complex edge cases, such as postnatal mortality and the impact of module ordering on population dynamics, which could have cascading effects on the simulation's accuracy and stability. Additionally, the proposed solutions (e.g., combining `Births` and `Deaths` into a single module or adjusting rates dynamically) suggest architectural considerations that could impact the broader system design. While the provided code changes appear minor (e.g., documentation updates and version notes), the actual implementation of the discussed fixes is implied to be more substantial based on the problem description. The difficulty is not at the highest end (0.8-1.0) because it does not seem to require advanced system-level or domain-specific knowledge beyond simulation modeling, but it still poses a significant challenge due to the need for careful handling of simulation logic and potential unintended consequences. Thus, a score of 0.65 reflects the hard but not extreme nature of the problem.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
