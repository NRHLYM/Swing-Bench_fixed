{"problem_statement": "Nice to have: 'grab' cursor when dragging the graph\n### What version of pprof are you using?\r\n\r\nGithub head.\r\n\r\n### What UI question did you have in mind ?\r\n\r\nNot a bug. This is merely UI idea that came to mind playing with the web-UI, specifically the 'graph' view.\r\n\r\nWhen dragging the graph view, the mouse-cursor stays the same as regular pointer.\r\nHowever, from a user feedback perspective -- seeing that this is holding the graph and moving it around -- it would be neat if the cursor was changing into the 'grab cursor' symbol (the little hand that 'holds on' to the thing being dragged). \r\n\r\nI think the [CSS cursor property](https://www.w3schools.com/cssref/pr_class_cursor.php) would be `cursor: grabbing`.\n", "patch": "diff --git a/internal/driver/html/graph.css b/internal/driver/html/graph.css\nnew file mode 100644\nindex 00000000..c756ddfd\n--- /dev/null\n+++ b/internal/driver/html/graph.css\n@@ -0,0 +1,7 @@\n+#graph {\n+    cursor: grab;\n+}\n+\n+#graph:active {\n+    cursor: grabbing;\n+}\ndiff --git a/internal/driver/html/graph.html b/internal/driver/html/graph.html\nindex a113549f..d17a0ea7 100644\n--- a/internal/driver/html/graph.html\n+++ b/internal/driver/html/graph.html\n@@ -4,6 +4,7 @@\n   <meta charset=\"utf-8\">\n   <title>{{.Title}}</title>\n   {{template \"css\" .}}\n+  {{template \"graph_css\" .}}\n </head>\n <body>\n   {{template \"header\" .}}\ndiff --git a/internal/driver/webhtml.go b/internal/driver/webhtml.go\nindex 436c408d..0b8630bc 100644\n--- a/internal/driver/webhtml.go\n+++ b/internal/driver/webhtml.go\n@@ -73,6 +73,7 @@ func addTemplates(templates *template.Template) {\n \tdef(\"css\", loadCSS(\"html/common.css\"))\n \tdef(\"header\", loadFile(\"html/header.html\"))\n \tdef(\"graph\", loadFile(\"html/graph.html\"))\n+\tdef(\"graph_css\", loadCSS(\"html/graph.css\"))\n \tdef(\"script\", loadJS(\"html/common.js\"))\n \tdef(\"top\", loadFile(\"html/top.html\"))\n \tdef(\"sourcelisting\", loadFile(\"html/source.html\"))\n", "instance_id": "google__pprof-854", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the desired UI enhancement\u2014changing the cursor to a 'grab' style when dragging a graph in a web UI. The goal is explicitly stated, and a reference to the CSS property (`cursor: grabbing`) is provided, which helps in understanding the intended solution. However, there are minor ambiguities and missing details. For instance, the problem does not specify whether this behavior should apply to all graph interactions or only under certain conditions (e.g., specific graph types or states). Additionally, there are no examples or mockups to illustrate the expected visual feedback, and edge cases (e.g., browser compatibility for CSS cursor properties) are not mentioned. Despite these minor gaps, the overall intent and scope of the problem are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The required code changes are minimal and straightforward, involving the addition of a small CSS file to define cursor styles for the graph element and minor updates to the HTML template and template loading logic in Go to include this CSS. The scope of the changes is limited to a few files (`graph.css`, `graph.html`, and `webhtml.go`) and does not impact the broader architecture or functionality of the codebase. The technical concepts involved are basic\u2014understanding CSS cursor properties and Go's template system\u2014which are fundamental for any developer with web development experience. There are no complex algorithms, design patterns, or domain-specific knowledge required. Additionally, the problem does not mention specific edge cases or error handling needs beyond potential browser compatibility (which is not addressed in the code changes and is not critical for the core task). Overall, this is a simple UI tweak that requires only basic modifications, justifying a difficulty score of 0.15.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Duplicate Job names in helm chart templates\n**Describe the bug**\r\nThere is a bug with the helm chart templates for jobs at `charts/fission-all/templates/analytics`. The Jobs are `post-upgrade-job.yaml` and `post-install-job.yaml`.\r\n\r\nBoth of the files are described using the following:\r\n\r\n```yaml\r\n{{- if .Values.analytics }}\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: {{ template \"fullname\" . }}-{{ .Chart.Version }}\r\n```\r\n\r\nIn the helm values, if analytics are `true` (which is the default), both jobs will try to be created and cause a conflict within helm. If I set `analytics` to false, the chart will install with the default values.\r\n\r\n**Resolution**\r\n\r\nWould it be possible to change the name to \r\n\r\n```yaml\r\nname: {{ template \"fullname\" . }}-{{ .Chart.Version }}-{{ randNumeric 3 }}` \r\n```\r\n\r\nor\r\n\r\n```yaml\r\nname: {{ template \"fullname\" . }}-{{ .Chart.Version }}-post-upgrade` \r\n```\r\n\n", "patch": "diff --git a/charts/fission-all/templates/analytics/post-install-job.yaml b/charts/fission-all/templates/analytics/post-install-job.yaml\nindex 97dfc3a5d3..ad49a3bd3e 100644\n--- a/charts/fission-all/templates/analytics/post-install-job.yaml\n+++ b/charts/fission-all/templates/analytics/post-install-job.yaml\n@@ -2,7 +2,7 @@\n apiVersion: batch/v1\n kind: Job\n metadata:\n-  name: {{ template \"fullname\" . }}-{{ .Chart.Version }}\n+  name: {{ template \"fullname\" . }}-{{ .Chart.Version }}-post-install\n   labels:\n     # The \"release\" convention makes it easy to tie a release to all of the\n     # Kubernetes resources that were created as part of that release.\ndiff --git a/charts/fission-all/templates/analytics/post-upgrade-job.yaml b/charts/fission-all/templates/analytics/post-upgrade-job.yaml\nindex b90cc1b877..16ba41b616 100644\n--- a/charts/fission-all/templates/analytics/post-upgrade-job.yaml\n+++ b/charts/fission-all/templates/analytics/post-upgrade-job.yaml\n@@ -2,7 +2,7 @@\n apiVersion: batch/v1\n kind: Job\n metadata:\n-  name: {{ template \"fullname\" . }}-{{ .Chart.Version }}\n+  name: {{ template \"fullname\" . }}-{{ .Chart.Version }}-post-upgrade\n   labels:\n     # The \"release\" convention makes it easy to tie a release to all of the\n     # Kubernetes resources that were created as part of that release.\n", "instance_id": "fission__fission-3150", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue of duplicate job names in Helm chart templates when the `analytics` value is set to `true`. It identifies the specific files (`post-upgrade-job.yaml` and `post-install-job.yaml`) and provides the problematic YAML snippet. The bug's impact (conflict during Helm installation) and a potential resolution (modifying the job names) are also mentioned. However, there are minor ambiguities: the statement does not explicitly discuss potential side effects of changing the job names (e.g., impact on Helm upgrades or rollbacks) or whether the proposed solution (adding suffixes or random numbers) could introduce other issues. Additionally, it lacks detailed context about the broader Helm chart or Kubernetes environment, which might be necessary for a complete understanding. Overall, it is clear enough to act upon but misses some minor details that could affect the solution's robustness.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward fix with minimal code changes. The scope of the modification is limited to two specific lines in two YAML files, requiring only a basic understanding of Helm chart templating (specifically, how template variables like `fullname` and `Chart.Version` are used). The technical concepts involved are minimal\u2014basic string concatenation in a template\u2014and no complex algorithms, design patterns, or domain-specific knowledge beyond basic Helm/Kubernetes familiarity are needed. The changes do not impact the broader codebase or system architecture, as they are isolated to naming conventions in metadata. There are no significant edge cases or error handling requirements mentioned or implied; the primary concern is avoiding name conflicts, which the proposed solution addresses directly. The amount of code change is trivial, and no deep understanding of interactions between modules is required. Overall, this is a very easy task suitable for a junior developer or someone with basic familiarity with Helm charts.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "PipelineRun fails to timeout properly in v0.63.0 (hits PipelineRunCouldntTimeOut state)\n# Expected Behavior\r\nCreating a pipelinerun with a timeout set that has multiple tasks, where some complete before the timeout expires should properly timeout.\r\n\r\n# Actual Behavior\r\nThe pipelinerun resource hits a \"PipelineRunCouldntTimeout\" state with the following error:\r\n\r\n```\r\n  Normal   PipelineRunCouldntTimeOut  10s                PipelineRun  PipelineRun \"timeout-pipelinerun\" was timed out but had errors trying to time out TaskRuns and/or Runs: failed to patch TaskRun `timeout-pipelinerun-fast-task` with timeout: admission webhook \"validation.webhook.pipeline.tekton.dev\" denied the request: validation failed: invalid value: Once the TaskRun is complete, no updates are allowed: spec\r\n```\r\n\r\n# Steps to Reproduce the Problem\r\n\r\nSee the following gist for an easy way to reproduce: https://gist.github.com/hrivera-ntap/a9dc47e7924feb6dc3ad32ac13751ffb \r\n\r\n1. Create a pipelinerun with multiple tasks and provide a timeout to the pipelinerun\r\n2. PipelineRun will hit PipelineRunCouldntTimeout state\r\n\r\n# Additional Info\r\n\r\n- Kubernetes version:\r\n\r\n  **Output of `kubectl version`:**\r\n\r\n```\r\n(paste your output here)\r\n```\r\n\r\n- Tekton Pipeline version:\r\n\r\nv0.63.0\r\nv0.62.x\r\n\r\n(appears to be a regression introduced after v0.61.1)\r\n\r\n@l-qing : This appears to be closely related to the issue reported in #8172 that you recently fixed by #8173. Mind taking a look?\r\n\r\n\r\n\n", "patch": "diff --git a/pkg/apis/pipeline/errors/errors.go b/pkg/apis/pipeline/errors/errors.go\nindex f81dd2e5f82..fbd487bba33 100644\n--- a/pkg/apis/pipeline/errors/errors.go\n+++ b/pkg/apis/pipeline/errors/errors.go\n@@ -13,7 +13,12 @@ limitations under the License.\n \n package errors\n \n-import \"errors\"\n+import (\n+\t\"errors\"\n+\t\"strings\"\n+\n+\tapierrors \"k8s.io/apimachinery/pkg/api/errors\"\n+)\n \n const UserErrorLabel = \"[User error] \"\n \n@@ -71,3 +76,10 @@ func GetErrorMessage(err error) string {\n \t}\n \treturn err.Error()\n }\n+\n+// IsImmutableTaskRunSpecError returns true if the error is the taskrun spec is immutable\n+func IsImmutableTaskRunSpecError(err error) bool {\n+\t// The TaskRun may have completed and the spec field is immutable.\n+\t// validation code: https://github.com/tektoncd/pipeline/blob/v0.62.0/pkg/apis/pipeline/v1/taskrun_validation.go#L136-L138\n+\treturn apierrors.IsBadRequest(err) && strings.Contains(err.Error(), \"no updates are allowed\")\n+}\ndiff --git a/pkg/reconciler/pipelinerun/cancel.go b/pkg/reconciler/pipelinerun/cancel.go\nindex 7370080d3f7..c198569da87 100644\n--- a/pkg/reconciler/pipelinerun/cancel.go\n+++ b/pkg/reconciler/pipelinerun/cancel.go\n@@ -24,6 +24,7 @@ import (\n \t\"strings\"\n \t\"time\"\n \n+\tpipelineErrors \"github.com/tektoncd/pipeline/pkg/apis/pipeline/errors\"\n \tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"\n \t\"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1\"\n \tclientset \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"\n@@ -88,9 +89,8 @@ func cancelTaskRun(ctx context.Context, taskRunName string, namespace string, cl\n \t\t// still be able to cancel the PipelineRun\n \t\treturn nil\n \t}\n-\tif errors.IsBadRequest(err) && strings.Contains(err.Error(), \"no updates are allowed\") {\n+\tif pipelineErrors.IsImmutableTaskRunSpecError(err) {\n \t\t// The TaskRun may have completed and the spec field is immutable, we should ignore this error.\n-\t\t// validation code: https://github.com/tektoncd/pipeline/blob/v0.62.0/pkg/apis/pipeline/v1/taskrun_validation.go#L136-L138\n \t\treturn nil\n \t}\n \treturn err\ndiff --git a/pkg/reconciler/pipelinerun/timeout.go b/pkg/reconciler/pipelinerun/timeout.go\nindex 8ae29a62d30..845ef3c22dd 100644\n--- a/pkg/reconciler/pipelinerun/timeout.go\n+++ b/pkg/reconciler/pipelinerun/timeout.go\n@@ -21,6 +21,7 @@ import (\n \t\"strings\"\n \t\"time\"\n \n+\tpipelineErrors \"github.com/tektoncd/pipeline/pkg/apis/pipeline/errors\"\n \tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"\n \t\"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1\"\n \tclientset \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"\n@@ -125,6 +126,10 @@ func timeoutPipelineTasksForTaskNames(ctx context.Context, logger *zap.SugaredLo\n \t\tlogger.Infof(\"patching TaskRun %s for timeout\", taskRunName)\n \n \t\tif err := timeoutTaskRun(ctx, taskRunName, pr.Namespace, clientSet); err != nil {\n+\t\t\tif pipelineErrors.IsImmutableTaskRunSpecError(err) {\n+\t\t\t\t// The TaskRun may have completed and the spec field is immutable, we should ignore this error.\n+\t\t\t\tcontinue\n+\t\t\t}\n \t\t\terrs = append(errs, fmt.Errorf(\"failed to patch TaskRun `%s` with timeout: %w\", taskRunName, err).Error())\n \t\t\tcontinue\n \t\t}\n", "instance_id": "tektoncd__pipeline-8236", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a PipelineRun in Tekton Pipeline v0.63.0 fails to timeout properly, entering a \"PipelineRunCouldntTimeOut\" state due to an error when attempting to patch a completed TaskRun. The expected and actual behaviors are outlined, and steps to reproduce the issue are provided via a linked gist, which adds to the clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the desired fix or expected outcome beyond \"properly timeout,\" leaving some room for interpretation. Additionally, while the error message is provided, there is no detailed explanation of the context or constraints around TaskRun immutability or the specific conditions under which the error occurs. Edge cases, such as varying task completion states or timeout durations, are not mentioned. Overall, the statement is valid and clear but lacks some finer details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, affecting only a few files (`errors.go`, `cancel.go`, and `timeout.go`) within the Tekton Pipeline codebase. The changes involve adding a utility function to detect a specific error condition (immutable TaskRun spec) and updating logic in two places to handle this error gracefully by ignoring it. This requires a moderate understanding of the codebase, specifically the error handling and reconciliation logic for PipelineRun and TaskRun resources. \n\nSecond, the technical concepts involved include familiarity with Kubernetes API errors (`apierrors`), Tekton Pipeline's custom resource definitions (CRDs), and the specific validation logic that prevents updates to completed TaskRuns. While these concepts are not overly complex for someone with experience in Kubernetes or Tekton, they do require domain-specific knowledge and an understanding of how webhooks and validation work in this context. \n\nThird, the problem touches on a specific edge case\u2014attempting to patch a TaskRun after it has completed, which triggers an immutability error. The solution handles this by filtering out such errors, which is a straightforward fix but requires recognizing the root cause from the error message and codebase behavior. There are no significant architectural impacts or performance considerations introduced by the changes, and the amount of code modified is small.\n\nOverall, I rate this as medium difficulty (0.45) because it requires understanding multiple concepts (Kubernetes errors, Tekton reconciliation logic) and making targeted modifications across a few files. However, it does not demand deep architectural changes or advanced technical expertise beyond what a mid-level engineer familiar with Kubernetes and Tekton would possess.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "GUI address unix socket broken\n### What happened?\n\nsyncthing used to work with a unix socket as a GUI address, e.g. when using a command line option like this:\r\n\r\n`-gui-address=unix:///run/syncthing/syncthing.sock`\r\n\r\nIt worked until 1.28.0 (including), and then in 1.28.1. it broke and complains.\r\n\r\nI think PR #9675 is the culprit, more specifically commit 65d0ca8aa96efa785fe9527de992e7efaabc22bd\n\n### Syncthing version\n\nv1.28.1\n\n### Platform & operating system\n\nLinux amd64\n\n### Browser version\n\n_No response_\n\n### Relevant log output\n\n```shell\n[start] WARNING: Failed to initialize config: failed to generate default config: get default port (GUI): address /run/syncthing/syncthing.sock: missing port in address\n```\n\n", "patch": "diff --git a/lib/config/config.go b/lib/config/config.go\nindex a16c8923d91..772694859b2 100644\n--- a/lib/config/config.go\n+++ b/lib/config/config.go\n@@ -139,21 +139,23 @@ func New(myID protocol.DeviceID) Configuration {\n }\n \n func (cfg *Configuration) ProbeFreePorts() error {\n-\tguiHost, guiPort, err := net.SplitHostPort(cfg.GUI.Address())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get default port (GUI): %w\", err)\n-\t}\n-\tport, err := strconv.Atoi(guiPort)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"convert default port (GUI): %w\", err)\n-\t}\n-\tport, err = getFreePort(guiHost, port)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get free port (GUI): %w\", err)\n+\tif cfg.GUI.Network() == \"tcp\" {\n+\t\tguiHost, guiPort, err := net.SplitHostPort(cfg.GUI.Address())\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"get default port (GUI): %w\", err)\n+\t\t}\n+\t\tport, err := strconv.Atoi(guiPort)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"convert default port (GUI): %w\", err)\n+\t\t}\n+\t\tport, err = getFreePort(guiHost, port)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"get free port (GUI): %w\", err)\n+\t\t}\n+\t\tcfg.GUI.RawAddress = net.JoinHostPort(guiHost, strconv.Itoa(port))\n \t}\n-\tcfg.GUI.RawAddress = net.JoinHostPort(guiHost, strconv.Itoa(port))\n \n-\tport, err = getFreePort(\"0.0.0.0\", DefaultTCPPort)\n+\tport, err := getFreePort(\"0.0.0.0\", DefaultTCPPort)\n \tif err != nil {\n \t\treturn fmt.Errorf(\"get free port (BEP): %w\", err)\n \t}\n", "instance_id": "syncthing__syncthing-9858", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the GUI address functionality for Unix sockets in Syncthing broke after version 1.28.0 due to a specific commit/PR. It provides relevant context, including the command-line option that no longer works, the version where the issue started, the platform, and a log output indicating the error (\"missing port in address\"). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for Unix socket addresses (e.g., should they bypass port probing?) nor does it mention potential edge cases or constraints related to different operating systems or socket configurations. While the issue is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a relatively small and focused change in a single file (`config.go`). The modification involves adding a conditional check (`cfg.GUI.Network() == \"tcp\"`) to skip port probing for non-TCP addresses like Unix sockets. This change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is minimal, primarily wrapping existing logic in a conditional block.\n\n2. **Technical Concepts Involved**: Solving this issue requires a basic understanding of Go's networking concepts, specifically how addresses are handled (e.g., `net.SplitHostPort`, `cfg.GUI.Network()`). It also involves familiarity with the Syncthing codebase's configuration handling, but the concepts are not particularly complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic networking are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the log output suggests the core issue is the inappropriate application of TCP-specific logic to Unix sockets. The code change addresses this by excluding non-TCP addresses from port probing, which is a straightforward fix. There\u2019s no indication of complex error handling or additional edge cases (e.g., mixed address types, permission issues with sockets) that need to be addressed in this specific fix.\n\n4. **Overall Complexity**: The fix requires understanding a specific bug introduced by a prior commit (as referenced in the problem statement) and applying a logical condition to bypass irrelevant logic for Unix sockets. While it involves some code logic comprehension, the solution is simple and does not require deep architectural changes or extensive debugging.\n\nGiven these points, a difficulty score of 0.30 reflects an easy problem that requires a targeted fix with moderate understanding of the relevant code section but no significant complexity or broad impact.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Device connection state metric\n### Feature description\r\n\r\nA new metric that exposes the connection state of each device as a gauge value (e.g., 0 = disconnected; 1 = connected, ...).\r\n\r\n### Problem or use case\r\n\r\nI have an android device where the operating system frequently kills the syncthing app.\r\nI want to monitor, using prometheus, whether the device is connected with my central syncthing node and get alerted if it is disconnected for an extended period of time.\r\n\r\n### Alternatives or workarounds\r\n\r\nCreate a custom exporter that uses the syncthing API.\r\n\r\n---\r\n\r\nIf this is something you would be interested into having, I can try to open a PR for it.\nDevice connection state metric\n### Feature description\r\n\r\nA new metric that exposes the connection state of each device as a gauge value (e.g., 0 = disconnected; 1 = connected, ...).\r\n\r\n### Problem or use case\r\n\r\nI have an android device where the operating system frequently kills the syncthing app.\r\nI want to monitor, using prometheus, whether the device is connected with my central syncthing node and get alerted if it is disconnected for an extended period of time.\r\n\r\n### Alternatives or workarounds\r\n\r\nCreate a custom exporter that uses the syncthing API.\r\n\r\n---\r\n\r\nIf this is something you would be interested into having, I can try to open a PR for it.\n", "patch": "diff --git a/lib/connections/metrics.go b/lib/connections/metrics.go\nnew file mode 100644\nindex 00000000000..e002833915d\n--- /dev/null\n+++ b/lib/connections/metrics.go\n@@ -0,0 +1,27 @@\n+// Copyright (C) 2024 The Syncthing Authors.\n+//\n+// This Source Code Form is subject to the terms of the Mozilla Public\n+// License, v. 2.0. If a copy of the MPL was not distributed with this file,\n+// You can obtain one at https://mozilla.org/MPL/2.0/.\n+\n+package connections\n+\n+import (\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n+)\n+\n+var (\n+\tmetricDeviceActiveConnections = promauto.NewGaugeVec(prometheus.GaugeOpts{\n+\t\tNamespace: \"syncthing\",\n+\t\tSubsystem: \"connections\",\n+\t\tName:      \"active\",\n+\t\tHelp:      \"Number of currently active connections, per device. If value is 0, the device is disconnected.\",\n+\t}, []string{\"device\"})\n+)\n+\n+func registerDeviceMetrics(deviceID string) {\n+\t// Register metrics for this device, so that counters & gauges are present even\n+\t// when zero.\n+\tmetricDeviceActiveConnections.WithLabelValues(deviceID)\n+}\ndiff --git a/lib/connections/service.go b/lib/connections/service.go\nindex 359dcff4d54..5b69f2f99a6 100644\n--- a/lib/connections/service.go\n+++ b/lib/connections/service.go\n@@ -846,6 +846,7 @@ func (s *service) CommitConfiguration(from, to config.Configuration) bool {\n \tnewDevices := make(map[protocol.DeviceID]bool, len(to.Devices))\n \tfor _, dev := range to.Devices {\n \t\tnewDevices[dev.DeviceID] = true\n+\t\tregisterDeviceMetrics(dev.DeviceID.String())\n \t}\n \n \tfor _, dev := range from.Devices {\n@@ -853,6 +854,7 @@ func (s *service) CommitConfiguration(from, to config.Configuration) bool {\n \t\t\twarningLimitersMut.Lock()\n \t\t\tdelete(warningLimiters, dev.DeviceID)\n \t\t\twarningLimitersMut.Unlock()\n+\t\t\tmetricDeviceActiveConnections.DeleteLabelValues(dev.DeviceID.String())\n \t\t}\n \t}\n \n@@ -1378,6 +1380,9 @@ func (c *deviceConnectionTracker) accountAddedConnection(conn protocol.Connectio\n \tc.wantConnections[d] = int(h.NumConnections)\n \tl.Debugf(\"Added connection for %s (now %d), they want %d connections\", d.Short(), len(c.connections[d]), h.NumConnections)\n \n+\t// Update active connections metric\n+\tmetricDeviceActiveConnections.WithLabelValues(d.String()).Inc()\n+\n \t// Close any connections we no longer want to retain.\n \tc.closeWorsePriorityConnectionsLocked(d, conn.Priority()-upgradeThreshold)\n }\n@@ -1399,6 +1404,10 @@ func (c *deviceConnectionTracker) accountRemovedConnection(conn protocol.Connect\n \t\tdelete(c.connections, d)\n \t\tdelete(c.wantConnections, d)\n \t}\n+\n+\t// Update active connections metric\n+\tmetricDeviceActiveConnections.WithLabelValues(d.String()).Dec()\n+\n \tl.Debugf(\"Removed connection for %s (now %d)\", d.Short(), c.connections[d])\n }\n \ndiff --git a/lib/protocol/metrics.go b/lib/protocol/metrics.go\nindex 1f57e51fd98..d1873b7d16b 100644\n--- a/lib/protocol/metrics.go\n+++ b/lib/protocol/metrics.go\n@@ -58,5 +58,6 @@ func registerDeviceMetrics(deviceID string) {\n \tmetricDeviceSentUncompressedBytes.WithLabelValues(deviceID)\n \tmetricDeviceSentMessages.WithLabelValues(deviceID)\n \tmetricDeviceRecvBytes.WithLabelValues(deviceID)\n+\tmetricDeviceRecvDecompressedBytes.WithLabelValues(deviceID)\n \tmetricDeviceRecvMessages.WithLabelValues(deviceID)\n }\n", "instance_id": "syncthing__syncthing-9528", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a new metric to expose the connection state of devices as a gauge value for monitoring purposes using Prometheus. The use case is well-articulated with a specific example of monitoring an Android device connection to a central Syncthing node. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the full range of possible gauge values beyond 0 (disconnected) and 1 (connected), leaving room for interpretation. Additionally, there are no mentions of specific edge cases, constraints, or performance requirements for the metric implementation. While the intent and purpose are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (`metrics.go`, `service.go`, and `metrics.go` in the `protocol` package), requiring an understanding of the interactions between the connection tracking logic and the metrics system in the Syncthing codebase. The changes include creating a new Prometheus gauge metric, registering it for each device, and updating its value when connections are added or removed, which indicates a moderate level of complexity. Second, the technical concepts involved include familiarity with the Prometheus client library in Go, understanding of gauge metrics, and integration with the existing device connection tracking system. These concepts are not overly advanced but require a solid grasp of Go and metrics instrumentation. Third, the amount of code change is relatively small (adding a new file and modifying a few functions), and it does not appear to impact the broader system architecture significantly. Lastly, while the problem statement does not explicitly mention edge cases, the code changes suggest potential considerations such as ensuring metrics are correctly initialized for new devices and cleaned up for removed devices, as well as handling concurrent updates to the gauge values. Overall, this task requires moderate effort and understanding of specific parts of the codebase, justifying a difficulty score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "HEAD request takes too long time to response\n### Welcome!\n\n- [x] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [x] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\n- I setup a nginx as static file server & gitea as package manangment\n- All staffs runs in docker\n- I setup traefik in docker as gateway, version tested: v3.2.3 & 3.3.0-RC2 & 3.3.0\n- Test url via `curl http://172.16.x.x/` and `curl -I http://172.16.x.x/`\n\n### What did you see instead?\n\n- **`curl http://172.16.x.x/`**: works ok, responsed in <20ms\n- **`curl -I http://172.16.x.x/`**: hangs up for long time, and then:\n  - for some url, like nginx static file url, finally got response after 1minute\n  - for some url, like docker registery head request with 401 response (via gitea), request hangs up infinitely\n- **`curl -I http://172.16.x.x:3000/`**: works ok, reponsed in <20ms (`3000`was the backend port mapped to host)\n\n### What version of Traefik are you using?\n\n**Tested**\n\n- v3.2.4\n- v3.3.0-rc2\n- v3.3.0\n\n### What is your environment & configuration?\n\nTraefik config on backend container really simple and now extra complex configuration:\n```\n      - traefik.enable=true\n      - traefik.http.routers.nginx.entrypoints=web\n      - traefik.http.routers.nginx.rule=Host(`example.com`)\n      - traefik.http.routers.nginx.service=nginx\n      - traefik.http.services.nginx.loadbalancer.server.port=3000\n```\n\n\n### If applicable, please paste the log output in DEBUG level\n\nNo log. If HEAD request hangs up, no access log was shown in terminal.\n", "patch": "diff --git a/pkg/proxy/fast/connpool.go b/pkg/proxy/fast/connpool.go\nindex 375fac2fb4..4c0be0fe96 100644\n--- a/pkg/proxy/fast/connpool.go\n+++ b/pkg/proxy/fast/connpool.go\n@@ -20,8 +20,9 @@ import (\n // rwWithUpgrade contains a ResponseWriter and an upgradeHandler,\n // used to upgrade the connection (e.g. Websockets).\n type rwWithUpgrade struct {\n-\tRW      http.ResponseWriter\n-\tUpgrade upgradeHandler\n+\tReqMethod string\n+\tRW        http.ResponseWriter\n+\tUpgrade   upgradeHandler\n }\n \n // conn is an enriched net.Conn.\n@@ -211,6 +212,10 @@ func (c *conn) handleResponse(r rwWithUpgrade) error {\n \n \tr.RW.WriteHeader(res.StatusCode())\n \n+\tif noResponseBodyExpected(r.ReqMethod) {\n+\t\treturn nil\n+\t}\n+\n \tif res.Header.ContentLength() == 0 {\n \t\treturn nil\n \t}\n@@ -444,8 +449,8 @@ func (c *connPool) askForNewConn(errCh chan<- error) {\n \tc.releaseConn(newConn)\n }\n \n-// isBodyAllowedForStatus reports whether a given response status code\n-// permits a body. See RFC 7230, section 3.3.\n+// isBodyAllowedForStatus reports whether a given response status code permits a body.\n+// See RFC 7230, section 3.3.\n // From https://github.com/golang/go/blame/master/src/net/http/transfer.go#L459\n func isBodyAllowedForStatus(status int) bool {\n \tswitch {\n@@ -458,3 +463,9 @@ func isBodyAllowedForStatus(status int) bool {\n \t}\n \treturn true\n }\n+\n+// noResponseBodyExpected reports whether a given request method permits a body.\n+// From https://github.com/golang/go/blame/master/src/net/http/transfer.go#L250\n+func noResponseBodyExpected(requestMethod string) bool {\n+\treturn requestMethod == \"HEAD\"\n+}\ndiff --git a/pkg/proxy/fast/proxy.go b/pkg/proxy/fast/proxy.go\nindex 717b1ff06e..06e68513e4 100644\n--- a/pkg/proxy/fast/proxy.go\n+++ b/pkg/proxy/fast/proxy.go\n@@ -284,8 +284,9 @@ func (p *ReverseProxy) roundTrip(rw http.ResponseWriter, req *http.Request, outR\n \n \t// Sending the responseWriter unlocks the connection readLoop, to handle the response.\n \tco.RWCh <- rwWithUpgrade{\n-\t\tRW:      rw,\n-\t\tUpgrade: upgradeResponseHandler(req.Context(), reqUpType),\n+\t\tReqMethod: req.Method,\n+\t\tRW:        rw,\n+\t\tUpgrade:   upgradeResponseHandler(req.Context(), reqUpType),\n \t}\n \n \tif err := <-co.ErrCh; err != nil {\n", "instance_id": "traefik__traefik-11442", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: HEAD requests in Traefik are taking an excessively long time to respond or hanging indefinitely in certain scenarios, while regular GET requests and HEAD requests to specific ports work fine. The user provides details about their environment (Traefik versions, Docker setup, and configuration) and the observed behavior with specific examples (e.g., response times and hanging behavior). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for HEAD requests (e.g., should they always return immediately without a body as per HTTP standards?). Additionally, there are no detailed logs or specific reproduction steps beyond the `curl` commands, which could help pinpoint whether the issue lies in Traefik's handling of HEAD requests or in the interaction with backend services like nginx or gitea. Edge cases or specific conditions under which the issue occurs (e.g., specific headers or configurations) are also not mentioned. Despite these minor gaps, the statement is valid and provides enough context to understand the issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem is rated as Hard (0.65) due to several factors across the evaluation criteria:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively focused, modifying two files (`connpool.go` and `proxy.go`) in the Traefik codebase. The changes involve adding logic to handle HEAD requests by checking the request method and skipping response body processing when applicable. While the changes are not extensive in terms of lines of code, they impact a critical part of the proxy logic (connection handling and response processing), which requires a good understanding of Traefik's internal architecture, specifically the connection pooling and request/response handling mechanisms. The modification does not appear to affect the broader system architecture but does touch core functionality, increasing the risk of unintended side effects.\n\n2. **Number of Technical Concepts**: Solving this issue requires understanding several technical concepts, including HTTP protocol specifics (e.g., HEAD request behavior as per RFC 7230), Go's `net/http` package intricacies (e.g., how response writers and connection handling work), and Traefik's custom proxy and connection pooling implementation. The developer must also be familiar with concurrency patterns in Go (e.g., channels used in the `rwWithUpgrade` struct and error handling). While these concepts are not extraordinarily complex for a senior engineer, they do require a solid grasp of HTTP semantics and proxy internals, pushing the difficulty beyond medium.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest that the developer considered the specific behavior of HEAD requests (i.e., no response body expected). However, implementing this fix requires careful consideration of other edge cases, such as how different status codes interact with HEAD requests, potential backend behaviors (e.g., backends sending bodies despite HEAD), and ensuring that skipping body processing does not break other request flows or connection states. Error handling logic in the modified code appears minimal, but the risk of introducing bugs (e.g., connection leaks or incorrect response handling) is non-trivial due to the critical nature of the modified component.\n\n4. **Overall Complexity**: The issue falls into the Hard category because it requires a deep understanding of Traefik's proxy internals and HTTP protocol nuances. While the code change itself is not overly complex, the potential impact on request handling and the need to validate the fix across various configurations and backend behaviors (e.g., nginx, gitea) add to the difficulty. Performance considerations are also relevant, as the original issue involves response timing, and the fix must ensure it does not introduce new bottlenecks or regressions. This places the difficulty slightly above medium but not at the very hard level, as it does not involve advanced system-level changes or domain-specific complexities beyond HTTP proxying.\n\nThus, a score of 0.65 reflects the challenge of understanding and modifying core proxy logic with moderate risk and complexity, suitable for a developer with strong Go and HTTP knowledge but not requiring extreme expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incompatiblity of anonymous PUT request to public bucket with S3 Java SDK version 2.30\nWhen I start minio via Docker locally with a public bucket and I try to put a file there using the latest version of the S3 Java SDK and anonymous credentials, I am getting this exception:\n```\nsoftware.amazon.awssdk.services.s3.model.S3Exception: Missing fields in request. (Service: S3, Status Code: 400, Request ID: ...\n```\n\n## Expected Behavior\nThe file should be added.\n\n## Current Behavior\nI am getting the above exception.\n\n## Steps to Reproduce (for bugs)\n```\ndocker run -p 9000:9000 -p9001:9001 --restart=unless-stopped --name minio -e MINIO_DEFAULT_BUCKETS=my-bucket:public bitnami/minio@sha256:ca31735c5a8e6a85de77c63bea09d8780a788ca9bbf7f1d730610c5da84a5796\n```\n\n```java\nS3AsyncClient asyncClient = S3AsyncClient.builder()\n        .endpointOverride(URI.create(\"http://localhost:9000\"))\n        .forcePathStyle(true)\n        .credentialsProvider(AnonymousCredentialsProvider.create())\n        .build();\n\nfinal var input = new ByteArrayInputStream(\"file content\".getBytes());\nPutObjectRequest putObjectRequest = PutObjectRequest.builder()\n        .bucket(\"my-bucket\")\n        .key(\"/test/filename\")\n        .metadata(Map.of())\n        .contentType(\"text/plain\")\n        .build();\n\nasyncClient.putObject(putObjectRequest, AsyncRequestBody.fromInputStream(\n        input,\n        (long) input.available(),\n        Executors.newSingleThreadExecutor())\n).get();\n```\n\nOn v2.30.31 of the Java SDK, I am getting this stack trace:\n```\nException in thread \"main\" java.util.concurrent.ExecutionException: software.amazon.awssdk.services.s3.model.S3Exception: Missing fields in request. (Service: S3, Status Code: 400, Request ID: 18298B10D2319065, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8) (SDK Attempt Count: 1)\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n        ...\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: Missing fields in request. (Service: S3, Status Code: 400, Request ID: 18298B10D2319065, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8) (SDK Attempt Count: 1)\n\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\n\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeAttemptExecute(AsyncRetryableStage.java:135)\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: Missing fields in request. (Service: S3, Status Code: 400, Request ID: 18298B10D2319065, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8) (SDK Attempt Count: 1)\n\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:152)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage.java:123)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2179)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$execute$0(MakeAsyncHttpRequestStage.java:110)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2179)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.completeResponseFuture(MakeAsyncHttpRequestStage.java:253)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$executeHttpRequest$3(MakeAsyncHttpRequestStage.java:167)\n\tat java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)\n\tat java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)\n\tat java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:482)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n```\n\nIt works in version 2.29.52; there was a change to how checksums are handled in v2.30.X of the SDK.\n\nIn version 2.30.X, it works if I either specify some credentials, or calculate the checksum explicitly:\n```java\n                .credentialsProvider(() -> AwsBasicCredentials.create(\"minioadmin\", \"minioadmin\"))\n```\nor\n```java\n        Crc32Checksum crc32Checksum = new Crc32Checksum();\n        crc32Checksum.update(data, 0, data.length);\n        String checksum = Base64.getEncoder().encodeToString(crc32Checksum.getChecksumBytes());\n\n        // ...\n\n        PutObjectRequest putObjectRequest = PutObjectRequest.builder()\n        // ...\n                .checksumAlgorithm(ChecksumAlgorithm.CRC32)\n                .checksumCRC32(checksum)\n        // ...\n```\n\n## Context\nTrying to use minio for testing before deploying code to run against S3.\n\n## Your Environment\nThis exact docker image:\nbitnami/minio@sha256:ca31735c5a8e6a85de77c63bea09d8780a788ca9bbf7f1d730610c5da84a5796\nRunning on WSL:\nLinux <Device-Name> 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n", "patch": "diff --git a/cmd/auth-handler.go b/cmd/auth-handler.go\nindex dd489591073c1..53d285e4f2aee 100644\n--- a/cmd/auth-handler.go\n+++ b/cmd/auth-handler.go\n@@ -754,8 +754,14 @@ func isPutActionAllowed(ctx context.Context, atype authType, bucketName, objectN\n \t\treturn ErrSignatureVersionNotSupported\n \tcase authTypeSignedV2, authTypePresignedV2:\n \t\tcred, owner, s3Err = getReqAccessKeyV2(r)\n-\tcase authTypeStreamingSigned, authTypePresigned, authTypeSigned, authTypeStreamingSignedTrailer, authTypeStreamingUnsignedTrailer:\n+\tcase authTypeStreamingSigned, authTypePresigned, authTypeSigned, authTypeStreamingSignedTrailer:\n \t\tcred, owner, s3Err = getReqAccessKeyV4(r, region, serviceS3)\n+\tcase authTypeStreamingUnsignedTrailer:\n+\t\tcred, owner, s3Err = getReqAccessKeyV4(r, region, serviceS3)\n+\t\tif s3Err == ErrMissingFields {\n+\t\t\t// Could be anonymous. cred + owner is zero value.\n+\t\t\ts3Err = ErrNone\n+\t\t}\n \t}\n \tif s3Err != ErrNone {\n \t\treturn s3Err\n", "instance_id": "minio__minio-21095", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the issue encountered when using the S3 Java SDK version 2.30 with anonymous credentials to upload a file to a public bucket on a local MinIO instance. It includes the expected behavior, current behavior, steps to reproduce, relevant code snippets, and context about the environment. Additionally, it highlights that the issue does not occur in an earlier SDK version (2.29.52) and provides workarounds (using credentials or explicitly calculating checksums). However, there are minor ambiguities: the problem statement does not explicitly define the root cause (though it hints at changes in checksum handling in SDK v2.30.X), and it lacks detailed constraints or edge cases beyond the specific scenario described. For instance, it does not clarify if the issue applies only to specific configurations of MinIO or other S3-compatible services. Overall, it is clear enough to understand the issue but misses some depth in root cause analysis and broader applicability.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code change is relatively small, confined to a single file (`cmd/auth-handler.go`) and a specific modification in the authentication logic for handling unsigned trailer requests. However, the change requires a deep understanding of MinIO's authentication mechanisms, specifically how different authentication types (e.g., `authTypeStreamingUnsignedTrailer`) are processed and how they interact with the S3 protocol's expectations for anonymous access. The problem also involves nuanced knowledge of S3 compatibility issues, particularly around checksum handling introduced in the SDK update, which adds to the conceptual complexity. Additionally, the solution must handle edge cases, such as ensuring that anonymous requests are correctly identified and processed without breaking other authentication flows. While the code change itself is minimal (adding a conditional check for `ErrMissingFields` and overriding it to `ErrNone` for anonymous access), the impact on the system's behavior is significant, as it alters authentication logic\u2014a critical component. This requires careful testing to avoid introducing security or compatibility issues. The problem does not reach \"Very Hard\" because it does not involve extensive architectural changes or advanced domain-specific knowledge beyond S3 protocols and authentication handling. Thus, a score of 0.65 reflects the need for deep understanding and careful handling of a focused but critical issue.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "KMS : Invalid configuration for static KMS key: 'MINIO_KMS_SECRET_KEY' and 'MINIO_KMS_SECRET_KEY_FILE' are present\nHi there !\r\n\r\nOn a Docker Swarm (or Docker Compose), it's not possible to define environment var `MINIO_KMS_SECRET_KEY_FILE` through secrets : it makes the container crash.\r\n\r\n## Current Behavior\r\nIf the environment var `MINIO_KMS_SECRET_KEY_FILE` is present and `MINIO_KMS_SECRET_KEY` is not, the container will consider that the environment var `MINIO_KMS_SECRET_KEY` is also present and it will stop with the following issue :\r\n`FATAL Invalid KMS configuration specified: kms: invalid configuration for static KMS key: 'MINIO_KMS_SECRET_KEY' and 'MINIO_KMS_SECRET_KEY_FILE' are present`\r\n\r\n## Expected Behavior\r\nIf env var `MINIO_KMS_SECRET_KEY` is not specified, it should not be considered by the container image.\r\n\r\n## Possible Solution\r\nI think something could be found on the kms config file, but I couldn't find exactly what :\r\nhttps://github.com/minio/minio/blob/master/internal/kms/config.go (about lines 253 or 300). Seems like \r\n`os.Unsetenv(EnvKMSSecretKey)` on line 314 is not functional (I don't know Go Language).\r\n\r\n## Steps to Reproduce (for bugs)\r\nSave two secrets in Docker Swarm :\r\n`echo \"Roudoudou\" | docker secret create minio_root_password -`\r\n`echo \"minio_key:JAzw9eJEMMDRr6Sqqz6iC+uVg2Yeu8S/xvJDOlOZifg=\" | docker secret create minio_kms_secret_key -`\r\n\r\nAnd then, try the very basic following docker-compose.yml :\r\n```\r\nservices:\r\n  minio:\r\n    image: minio/minio\r\n    environment:\r\n      - MINIO_ROOT_USER=Admin\r\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password\r\n      - MINIO_KMS_SECRET_KEY_FILE=/run/secrets/minio_kms_secret_key\r\n    command: server /data --console-address ':9001'\r\n    ports:\r\n      - '9001:9001'\r\n      - '9000:9000'\r\n    secrets:\r\n      - minio_root_password\r\n      - minio_kms_secret_key\r\n\r\nsecrets:\r\n  minio_root_password:\r\n    external: true\r\n  minio_kms_secret_key:\r\n    external: true\r\n```\r\nThe container crash and restart with the following error :\r\n`FATAL Invalid KMS configuration specified: kms: invalid configuration for static KMS key: 'MINIO_KMS_SECRET_KEY' and 'MINIO_KMS_SECRET_KEY_FILE' are present`\r\n\r\n## Context\r\nDiscovered while trying to build a stack with Minio to have a very simple encrypted backend to manage (one node, one drive) and a [FileStash](https://www.filestash.app/) connected as frontend.\r\n\r\n## Regression\r\nIt seems to be a regression. \r\nThe Docker Compose work fine with the `minio/minio:RELEASE.2024-05-07T06-41-25Z` image.\r\nIt's broken in the `minio/minio:RELEASE.2024-05-10T01-41-38Z` image.\r\n\r\n=> This PR may be concerned : https://github.com/minio/minio/commit/8b660e18f26b36cc11a51aad0d806d03b917784d\r\n\r\n## Your Environment\r\nminio version RELEASE.2024-07-16T23-46-41Z (commit-id=3535197f993dea840dcb96302ba4d883dc619097)\r\nRuntime: go1.22.5 linux/arm64\r\nFrom Docker image https://hub.docker.com/layers/minio/minio/latest/images/sha256-cb688c06c3135c6d496e2edecd85a91b43e53b6ef17cb2072b0c1832c141d1d2?context=explore\r\n\r\nArch : Linux/Arm64 (Macbook Pro M1 Pro - 16Go).\r\nDarwin 23.5.0 Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000 arm64\r\nDocker version 27.0.3, build 7d4bcd8\r\n\r\nAlso tested on \r\nArch : Linux/Amd64 (Intel NUC Core i3 - 16Go)\r\nLinux 5.15.0-116-generic #126-Ubuntu SMP Mon Jul 1 10:14:24 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\nDocker version 27.0.3, build 7d4bcd8\r\n\r\n\r\n\r\n\r\nDo not hesitate to ask if you need further information or if I can help\r\nGood luck guys !\r\nBe strong ;-)\n", "patch": "diff --git a/cmd/common-main.go b/cmd/common-main.go\nindex be3f4a1a08d8c..70abf6d293213 100644\n--- a/cmd/common-main.go\n+++ b/cmd/common-main.go\n@@ -685,16 +685,6 @@ func loadEnvVarsFromFiles() {\n \t\t}\n \t}\n \n-\tif env.IsSet(kms.EnvKMSSecretKeyFile) {\n-\t\tkmsSecret, err := readFromSecret(env.Get(kms.EnvKMSSecretKeyFile, \"\"))\n-\t\tif err != nil {\n-\t\t\tlogger.Fatal(err, \"Unable to read the KMS secret key inherited from secret file\")\n-\t\t}\n-\t\tif kmsSecret != \"\" {\n-\t\t\tos.Setenv(kms.EnvKMSSecretKey, kmsSecret)\n-\t\t}\n-\t}\n-\n \tif env.IsSet(config.EnvConfigEnvFile) {\n \t\tekvs, err := minioEnvironFromFile(env.Get(config.EnvConfigEnvFile, \"\"))\n \t\tif err != nil && !os.IsNotExist(err) {\n", "instance_id": "minio__minio-20122", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue encountered when using environment variables for KMS configuration in a Docker Swarm or Docker Compose setup with MinIO. The current behavior, expected behavior, steps to reproduce, and context are well-documented, which helps in understanding the problem. Additionally, the user has identified a specific commit that might have introduced the regression and provided environment details. However, there are minor ambiguities: the problem statement does not explicitly clarify the desired logic for handling the environment variables (e.g., should `MINIO_KMS_SECRET_KEY_FILE` take precedence over `MINIO_KMS_SECRET_KEY` or vice versa?). Also, edge cases related to file reading errors or invalid secret formats are not mentioned in the expected behavior. Despite these minor gaps, the statement is actionable and provides sufficient information to start addressing the issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem is rated as Easy (0.30) based on the following analysis of the factors:\n\n1. **Clarity and Complexity of the Problem Description**: While the problem description is mostly clear, the minor ambiguities around precedence and edge cases slightly increase the effort needed to ensure the solution aligns with user expectations. However, the core issue (improper handling of environment variables for KMS configuration) is straightforward to grasp.\n\n2. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`cmd/common-main.go`) and involves removing a specific block of code (about 10 lines) that sets the `MINIO_KMS_SECRET_KEY` environment variable based on the content of a secret file. This change does not appear to impact the broader system architecture or require modifications across multiple modules. The amount of code change is minimal, focusing on altering the logic for environment variable handling.\n\n3. **Number of Technical Concepts**: Solving this issue requires a basic understanding of Go programming, specifically environment variable handling (`os.Setenv`, `env.IsSet`), file reading logic, and the MinIO KMS configuration module. No advanced algorithms, design patterns, or domain-specific knowledge beyond typical server configuration handling are needed. The concepts involved are relatively simple for a developer familiar with Go.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change removes logic that could handle reading from a secret file, which might introduce issues if the file is inaccessible or malformed (though error handling for this was already present in the original code). The solution might need minor adjustments to ensure robustness, but these are not complex. The primary focus is on preventing the container crash due to conflicting environment variables, which is a straightforward fix.\n\nOverall, this problem falls into the Easy category (0.2-0.4) because it involves a small, localized code change with minimal impact on the broader codebase. The technical concepts are basic for a Go developer, and while some consideration of error handling is needed, it does not significantly elevate the difficulty. A score of 0.30 reflects the simplicity of the fix while acknowledging the need for careful validation of the change to avoid unintended side effects in KMS configuration.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incompatibility with S3 behavior when using the AWS CRT-based S3 java client for downloading empty files with `checksumValidation` disabled\n## NOTE\r\nIf this case is urgent, please subscribe to [Subnet](https://min.io/pricing) so that our 24/7 support team may help you faster.\r\n\r\n<!--- Provide a general summary of the issue in the Title above -->\r\n\r\n## Expected Behavior\r\n<!--- If you're describing a bug, tell us what should happen -->\r\n<!--- If you're suggesting a change/improvement, tell us how it should work -->\r\n\r\nThe same behavior of the AWS CRT-based S3 with real S3 endpoint which is to return a `200` status code with `contentLength` equal to 0. Please note that this is what minio (and S3) would return with the standard `S3AsyncClient` java client from the v2 sdk.\r\n\r\n## Current Behavior\r\n<!--- If describing a bug, tell us what happens instead of the expected behavior -->\r\n<!--- If suggesting a change/improvement, explain the difference from current behavior -->\r\n\r\nThe following exception is thrown (status code: 416):\r\n```\r\nsoftware.amazon.awssdk.services.s3.model.S3Exception: The requested range is not satisfiable (Service: S3, Status Code: 416, Request ID: ***, Extended Request ID: ***)\r\n```\r\n\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix/reason for the bug, -->\r\n<!--- or ideas how to implement the addition or change -->\r\nProbably this has to do with the range header being passed where it shouldn't because the CRT client is confused by the `xl.meta` file or some minio specific file metadata? \r\n\r\nUpdate: This only happens when `checksumValidation` on the SDK is disabled \r\n\r\n## Steps to Reproduce (for bugs)\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n<!--- and make sure you have followed https://github.com/minio/minio/tree/release/docs/debugging to capture relevant logs -->\r\n\r\n```java\r\nS3AsyncClient s3CrtAsync = S3AsyncClient.crtBuilder().checksumValidationEnabled(false).build(); // using s3 CRT version 0.29.18\r\ns3CrtAsync.putObject(r -> r.bucket(\"bucket\").key(\"test\"), AsyncRequestBody.empty()).join();\r\ns3CrtAsync.getObject(r -> r.bucket(\"bucket\").key(\"test\"), Path.of(\"/tmp/test\")).join(); // throws range is not satisfiable exception\r\n```\r\n\r\n## Context\r\n<!--- How has this issue affected you? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\nTrying to upgrade from the standard S3 client to the CRT client to enhance performance of download operations, some of our tests that expect empty files started to fail. For more about the CRT client, see: https://aws.amazon.com/blogs/developer/introducing-crt-based-s3-client-and-the-s3-transfer-manager-in-the-aws-sdk-for-java-2-x/\r\n\r\n## Regression\r\n<!-- Is this issue a regression? (Yes / No) --> No\r\n<!-- If Yes, optionally please include minio version or commit id or PR# that caused this regression, if you have these details. -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Version used (`minio --version`):\r\nminio version RELEASE.2024-01-29T03-56-32Z (commit-id=9987ff570bcea7c26a8faec32910f10f49576d0c)\r\nRuntime: go1.21.6 linux/arm64\r\nLicense: GNU AGPLv3 <https://www.gnu.org/licenses/agpl-3.0.html>\r\nCopyright: 2015-2024 MinIO, Inc.\r\n\r\n* Server setup and configuration: Docker image on Mac with vanilla config\r\n\r\n* Operating System and version (`uname -a`):\r\nLinux a7c7d12c247e 6.5.11-linuxkit #1 SMP PREEMPT Mon Dec  4 11:30:00 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux\r\n\n", "patch": "diff --git a/cmd/api-errors.go b/cmd/api-errors.go\nindex c4aeb9a6a99de..a35df607fadb7 100644\n--- a/cmd/api-errors.go\n+++ b/cmd/api-errors.go\n@@ -56,19 +56,23 @@ type APIError struct {\n \tCode           string\n \tDescription    string\n \tHTTPStatusCode int\n+\tObjectSize     string\n+\tRangeRequested string\n }\n \n // APIErrorResponse - error response format\n type APIErrorResponse struct {\n-\tXMLName    xml.Name `xml:\"Error\" json:\"-\"`\n-\tCode       string\n-\tMessage    string\n-\tKey        string `xml:\"Key,omitempty\" json:\"Key,omitempty\"`\n-\tBucketName string `xml:\"BucketName,omitempty\" json:\"BucketName,omitempty\"`\n-\tResource   string\n-\tRegion     string `xml:\"Region,omitempty\" json:\"Region,omitempty\"`\n-\tRequestID  string `xml:\"RequestId\" json:\"RequestId\"`\n-\tHostID     string `xml:\"HostId\" json:\"HostId\"`\n+\tXMLName          xml.Name `xml:\"Error\" json:\"-\"`\n+\tCode             string\n+\tMessage          string\n+\tKey              string `xml:\"Key,omitempty\" json:\"Key,omitempty\"`\n+\tBucketName       string `xml:\"BucketName,omitempty\" json:\"BucketName,omitempty\"`\n+\tResource         string\n+\tRegion           string `xml:\"Region,omitempty\" json:\"Region,omitempty\"`\n+\tRequestID        string `xml:\"RequestId\" json:\"RequestId\"`\n+\tHostID           string `xml:\"HostId\" json:\"HostId\"`\n+\tActualObjectSize string `xml:\"ActualObjectSize,omitempty\" json:\"ActualObjectSize,omitempty\"`\n+\tRangeRequested   string `xml:\"RangeRequested,omitempty\" json:\"RangeRequested,omitempty\"`\n }\n \n // APIErrorCode type of error status.\n@@ -2412,10 +2416,9 @@ func toAPIError(ctx context.Context, err error) APIError {\n \tapiErr := errorCodes.ToAPIErr(toAPIErrorCode(ctx, err))\n \tswitch apiErr.Code {\n \tcase \"NotImplemented\":\n-\t\tdesc := fmt.Sprintf(\"%s (%v)\", apiErr.Description, err)\n \t\tapiErr = APIError{\n \t\t\tCode:           apiErr.Code,\n-\t\t\tDescription:    desc,\n+\t\t\tDescription:    fmt.Sprintf(\"%s (%v)\", apiErr.Description, err),\n \t\t\tHTTPStatusCode: apiErr.HTTPStatusCode,\n \t\t}\n \tcase \"XMinioBackendDown\":\n@@ -2432,7 +2435,19 @@ func toAPIError(ctx context.Context, err error) APIError {\n \t\t\t\tHTTPStatusCode: e.HTTPStatusCode,\n \t\t\t}\n \t\tcase batchReplicationJobError:\n-\t\t\tapiErr = APIError(e)\n+\t\t\tapiErr = APIError{\n+\t\t\t\tDescription:    e.Description,\n+\t\t\t\tCode:           e.Code,\n+\t\t\t\tHTTPStatusCode: e.HTTPStatusCode,\n+\t\t\t}\n+\t\tcase InvalidRange:\n+\t\t\tapiErr = APIError{\n+\t\t\t\tCode:           \"InvalidRange\",\n+\t\t\t\tDescription:    e.Error(),\n+\t\t\t\tHTTPStatusCode: errorCodes[ErrInvalidRange].HTTPStatusCode,\n+\t\t\t\tObjectSize:     strconv.FormatInt(e.ResourceSize, 10),\n+\t\t\t\tRangeRequested: fmt.Sprintf(\"%d-%d\", e.OffsetBegin, e.OffsetEnd),\n+\t\t\t}\n \t\tcase InvalidArgument:\n \t\t\tapiErr = APIError{\n \t\t\t\tCode:           \"InvalidArgument\",\n@@ -2559,13 +2574,15 @@ func getAPIError(code APIErrorCode) APIError {\n func getAPIErrorResponse(ctx context.Context, err APIError, resource, requestID, hostID string) APIErrorResponse {\n \treqInfo := logger.GetReqInfo(ctx)\n \treturn APIErrorResponse{\n-\t\tCode:       err.Code,\n-\t\tMessage:    err.Description,\n-\t\tBucketName: reqInfo.BucketName,\n-\t\tKey:        reqInfo.ObjectName,\n-\t\tResource:   resource,\n-\t\tRegion:     globalSite.Region,\n-\t\tRequestID:  requestID,\n-\t\tHostID:     hostID,\n+\t\tCode:             err.Code,\n+\t\tMessage:          err.Description,\n+\t\tBucketName:       reqInfo.BucketName,\n+\t\tKey:              reqInfo.ObjectName,\n+\t\tResource:         resource,\n+\t\tRegion:           globalSite.Region,\n+\t\tRequestID:        requestID,\n+\t\tHostID:           hostID,\n+\t\tActualObjectSize: err.ObjectSize,\n+\t\tRangeRequested:   err.RangeRequested,\n \t}\n }\ndiff --git a/cmd/batch-handlers.go b/cmd/batch-handlers.go\nindex bd65fa9c74b37..12bf67ac64836 100644\n--- a/cmd/batch-handlers.go\n+++ b/cmd/batch-handlers.go\n@@ -1231,6 +1231,7 @@ type batchReplicationJobError struct {\n \tCode           string\n \tDescription    string\n \tHTTPStatusCode int\n+\tObjectSize     int64\n }\n \n func (e batchReplicationJobError) Error() string {\ndiff --git a/cmd/erasure-object.go b/cmd/erasure-object.go\nindex b819d5ad35050..30181d6a4b258 100644\n--- a/cmd/erasure-object.go\n+++ b/cmd/erasure-object.go\n@@ -251,6 +251,18 @@ func (er erasureObjects) GetObjectNInfo(ctx context.Context, bucket, object stri\n \t\topts.NoDecryption = true\n \t}\n \n+\tif objInfo.Size == 0 {\n+\t\tif _, _, err := rs.GetOffsetLength(objInfo.Size); err != nil {\n+\t\t\t// Make sure to return object info to provide extra information.\n+\t\t\treturn &GetObjectReader{\n+\t\t\t\tObjInfo: objInfo,\n+\t\t\t}, err\n+\t\t}\n+\n+\t\t// Zero byte objects don't even need to further initialize pipes etc.\n+\t\treturn NewGetObjectReaderFromReader(bytes.NewReader(nil), objInfo, opts)\n+\t}\n+\n \tif objInfo.IsRemote() {\n \t\tgr, err := getTransitionedObjectReader(ctx, bucket, object, rs, h, objInfo, opts)\n \t\tif err != nil {\n@@ -260,11 +272,6 @@ func (er erasureObjects) GetObjectNInfo(ctx context.Context, bucket, object stri\n \t\treturn gr.WithCleanupFuncs(nsUnlocker), nil\n \t}\n \n-\tif objInfo.Size == 0 {\n-\t\t// Zero byte objects don't even need to further initialize pipes etc.\n-\t\treturn NewGetObjectReaderFromReader(bytes.NewReader(nil), objInfo, opts)\n-\t}\n-\n \tfn, off, length, err := NewGetObjectReader(rs, objInfo, opts)\n \tif err != nil {\n \t\treturn nil, err\ndiff --git a/cmd/httprange.go b/cmd/httprange.go\nindex db22299054242..2e198d47425fd 100644\n--- a/cmd/httprange.go\n+++ b/cmd/httprange.go\n@@ -60,7 +60,11 @@ func (h *HTTPRangeSpec) GetLength(resourceSize int64) (rangeLength int64, err er\n \t\t}\n \n \tcase h.Start >= resourceSize:\n-\t\treturn 0, errInvalidRange\n+\t\treturn 0, InvalidRange{\n+\t\t\tOffsetBegin:  h.Start,\n+\t\t\tOffsetEnd:    h.End,\n+\t\t\tResourceSize: resourceSize,\n+\t\t}\n \n \tcase h.End > -1:\n \t\tend := h.End\ndiff --git a/cmd/object-api-errors.go b/cmd/object-api-errors.go\nindex 10c34f4158e56..e04cb2dd09764 100644\n--- a/cmd/object-api-errors.go\n+++ b/cmd/object-api-errors.go\n@@ -595,7 +595,7 @@ type InvalidRange struct {\n }\n \n func (e InvalidRange) Error() string {\n-\treturn fmt.Sprintf(\"The requested range \\\"bytes %d -> %d of %d\\\" is not satisfiable.\", e.OffsetBegin, e.OffsetEnd, e.ResourceSize)\n+\treturn fmt.Sprintf(\"The requested range 'bytes=%d-%d' is not satisfiable\", e.OffsetBegin, e.OffsetEnd)\n }\n \n // ObjectTooLarge error returned when the size of the object > max object size allowed (5G) per request.\n@@ -758,6 +758,9 @@ func isErrMethodNotAllowed(err error) bool {\n }\n \n func isErrInvalidRange(err error) bool {\n+\tif errors.Is(err, errInvalidRange) {\n+\t\treturn true\n+\t}\n \t_, ok := err.(InvalidRange)\n \treturn ok\n }\n", "instance_id": "minio__minio-19668", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: there is an incompatibility with the AWS CRT-based S3 Java client when downloading empty files with `checksumValidation` disabled, resulting in a 416 \"Range Not Satisfiable\" error instead of the expected 200 status code with a content length of 0. The expected behavior, current behavior, steps to reproduce, and context are provided, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases beyond empty files, nor does it clarify if other configurations or scenarios might trigger similar issues. Additionally, the \"Possible Solution\" section is speculative and lacks depth, mentioning a potential issue with the range header but not providing concrete guidance. Overall, while the core issue is well-articulated, the lack of comprehensive edge case discussion and detailed solution direction prevents it from being fully comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" category, due to several factors:\n\n1. **Clarity and Complexity of the Problem Description**: While the problem is mostly clear, the underlying issue involves nuanced behavior differences between the AWS CRT-based S3 client and the standard S3 client, specifically with empty files and disabled checksum validation. Understanding why the 416 error occurs requires knowledge of HTTP range requests and S3-specific metadata handling, which adds to the complexity.\n\n2. **Scope and Depth of Code Changes**: The provided code changes span multiple files (`api-errors.go`, `batch-handlers.go`, `erasure-object.go`, `httprange.go`, `object-api-errors.go`) in the MinIO codebase, indicating a moderate scope. The changes involve modifying error handling logic, adding fields to error structures, and adjusting how empty objects are handled in the `GetObjectNInfo` function. While the changes are not architecturally significant, they require understanding interactions between different components (e.g., HTTP range parsing, object retrieval, and error reporting). The amount of code change is moderate, with both structural additions (new fields in structs) and logical modifications (handling zero-byte objects differently).\n\n3. **Number of Technical Concepts**: Solving this requires familiarity with several concepts: HTTP range headers and their validation, S3-compatible storage behavior (especially with empty files), Go-specific error handling patterns, and MinIO's internal object storage and metadata management (e.g., `xl.meta`). Additionally, understanding the AWS CRT client's behavior compared to the standard client involves domain-specific knowledge of AWS SDKs and their configurations. These concepts are moderately complex, especially for someone not deeply familiar with S3 internals or MinIO's codebase.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement focuses on empty files with `checksumValidation` disabled, but the code changes suggest broader implications for range request handling (`InvalidRange` error customization). The modifications add detailed error reporting (e.g., `ActualObjectSize`, `RangeRequested`), indicating a need to handle edge cases around invalid ranges and zero-byte objects. The logic for zero-byte objects is explicitly adjusted to return early in `erasure-object.go`, which mitigates potential errors but requires careful validation to ensure no other side effects are introduced. The complexity of edge cases is moderate, as it primarily deals with empty files and range requests, but ensuring compatibility with various client behaviors adds to the challenge.\n\nOverall, this problem requires a deep understanding of MinIO's codebase, HTTP range handling, and S3 compatibility nuances. The changes, while not system-wide, impact critical paths (object retrieval and error reporting) and necessitate careful consideration of edge cases. Therefore, a score of 0.65 reflects the hard nature of the task, requiring significant expertise and attention to detail, though it does not reach the \"Very Hard\" category as it does not involve fundamental architectural redesign or highly intricate algorithms.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Duplicate Events for Network Disconnect in Docker Events Command\n### Description\n\nWe have found a bug in the `docker events` command that captures Docker events. Specifically, when a network disconnect event occurs, it generates two events instead of one.\n\n### Reproduce\n\n1. Run the `docker events` command to observe events:\n   ```\n   docker events --filter \"event=disconnect\"\n   ```\n2. Run the command to start a container connected to a network:\n   ```\n   docker run -d --name test-container --network test-network nginx\n   ```\n3. Disconnect the network from the container:\n   ```\n   docker network disconnect test-network test-container\n   ```\n\n### Expected behavior\n\nThe `docker events` command should generate a single event for each network disconnect. For example:\n```\n2024-10-30T19:05:20.743496360 network disconnect d331b8f1511379d6ddb431e249c478c3239e2ad54f14896a3c25f41bf8f70974 (container=6835ba7cb725b84add7e83f356b1fbae191b48a442e085d013fd29a153a58c45, name=test-network, type=bridge)\n```\n\n### Actual Behavior\n\nThe  `docker events` command generates two events for each network disconnect. For example:\n```\n2024-10-30T19:05:20.743496360 network disconnect d331b8f1511379d6ddb431e249c478c3239e2ad54f14896a3c25f41bf8f70974 (container=6835ba7cb725b84add7e83f356b1fbae191b48a442e085d013fd29a153a58c45, name=test-network, type=bridge)\n2024-10-30T19:05:20.745043405 network disconnect d331b8f1511379d6ddb431e249c478c3239e2ad54f14896a3c25f41bf8f70974 (container=6835ba7cb725b84add7e83f356b1fbae191b48a442e085d013fd29a153a58c45, name=test-network, type=bridge)\n```\n\n\n### docker version\n```\nClient: Docker Engine - Community\n Version:           27.3.1\n API version:       1.47\n Go version:        go1.22.7\n Git commit:        ce12230\n Built:             Fri Sep 20 11:41:00 2024\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          27.3.1\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.22.7\n  Git commit:       41ca978\n  Built:            Fri Sep 20 11:41:00 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.22\n  GitCommit:        7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\n runc:\n  Version:          1.1.14\n  GitCommit:        v1.1.14-0-g2c9f560\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n```\nClient: Docker Engine - Community\n Version:    27.3.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.17.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.29.7\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 0\n  Paused: 0\n  Stopped: 1\n Images: 21\n Server Version: 27.3.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\n runc version: v1.1.14-0-g2c9f560\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.9.3-76060903-generic\n Operating System: Pop!_OS 22.04 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 15.41GiB\n Name: pop-os\n ID: 1fa450f3-a97d-4219-9081-e02e277aa110\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nThis bug has been tested and confirmed to be valid for the following environments:\n\n- Docker Version: 27.3.1\n- Operating Systems:\n  - Pop!_OS 22.04 LTS\n  - Arch Linux 6.11.5-arch1-1\n\n---\n\n- Docker Version: 27.0.3\n- Operating Systems:\n  - Ubuntu 22.04.4 LTS (Jammy Jellyfish)\n\nWe have checked the source code and identified the part of the code responsible for this behavior. We also have a fix ready and would be happy to contribute a pull request if needed.\n", "patch": "diff --git a/daemon/container_operations.go b/daemon/container_operations.go\nindex a7daf87f0e9ab..7a4e35695b671 100644\n--- a/daemon/container_operations.go\n+++ b/daemon/container_operations.go\n@@ -1125,12 +1125,6 @@ func (daemon *Daemon) DisconnectFromNetwork(ctx context.Context, ctr *container.\n \t\treturn err\n \t}\n \n-\tif n != nil {\n-\t\tdaemon.LogNetworkEventWithAttributes(n, events.ActionDisconnect, map[string]string{\n-\t\t\t\"container\": ctr.ID,\n-\t\t})\n-\t}\n-\n \treturn nil\n }\n \n", "instance_id": "moby__moby-48800", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the `docker events` command where duplicate events are generated for a network disconnect. The goal is explicitly stated, and the expected versus actual behavior is demonstrated with detailed examples, including timestamps and event logs. Steps to reproduce the issue are provided, ensuring that anyone can replicate the problem. Additionally, relevant environment details such as Docker version, operating systems, and system information are included, which helps in understanding the context of the bug. There are no significant ambiguities, and the problem description includes all critical details necessary to understand the issue. The only minor omission is the lack of explicit mention of potential edge cases or specific constraints, but given the focused nature of the bug, this does not detract from the overall clarity.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to several factors. First, the scope of the code change is minimal, involving the removal of a small block of code (6 lines) in a single file (`daemon/container_operations.go`). This change does not impact the broader architecture of the Docker codebase and appears to be a straightforward fix for the duplicate event issue by eliminating an unnecessary event logging call. Second, the technical concepts required to understand and implement the fix are relatively basic, primarily involving familiarity with Go programming and a general understanding of event handling in Docker's daemon. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Docker internals are needed. Third, the problem does not explicitly mention complex edge cases or additional error handling requirements beyond the core issue of duplicate events, and the provided code change does not introduce new error handling logic. Finally, the modification does not require deep interaction with other parts of the codebase, making it a localized fix. Overall, this is a simple bug fix that requires minimal effort and understanding, justifying a difficulty score of 0.25.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "v25 regression: Rootless docker - plugin discovery uses wrong path\n### Description\n\nWe encounter exactly the same problem as earlier described in https://github.com/moby/moby/issues/43111\r\n\r\nThis issue was fixed in v24.0.0, we are now using v25.0.3\n\n### Reproduce\n\n1. run the commands `docker system prune -a -f --volumes` and `docker system df` \r\n2. An error is returned: `Error response from daemon: list: error listing plugins: legacy plugin: error reading dir entries: open /etc/docker/plugins: permission denied`\n\n### Expected behavior\n\nThe commands should be executed without error\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:14:26 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:26 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.10.0-28-amd64\r\n Operating System: Debian GNU/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 23.46GiB\r\n Name: tridev002\r\n ID: KVHD:CZGB:YY5C:SF3K:6OM6:62UH:HZ6V:XF7J:WMBL:KOPK:BEUN:T35H\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Registry Mirrors:\r\n  https://docker.triopsys.net/\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nAs a workaround group and other are given read and execute permissions for /etc/docker and /etc/docker/plugins\n", "patch": "diff --git a/pkg/plugins/discovery.go b/pkg/plugins/discovery.go\nindex 37316ed4829af..503ac574a9091 100644\n--- a/pkg/plugins/discovery.go\n+++ b/pkg/plugins/discovery.go\n@@ -10,6 +10,8 @@ import (\n \t\"strings\"\n \t\"sync\"\n \n+\t\"github.com/containerd/containerd/pkg/userns\"\n+\t\"github.com/containerd/log\"\n \t\"github.com/pkg/errors\"\n )\n \n@@ -56,10 +58,16 @@ func (l *LocalRegistry) Scan() ([]string, error) {\n \n \tfor _, p := range l.specsPaths {\n \t\tdirEntries, err = os.ReadDir(p)\n-\t\tif err != nil && !os.IsNotExist(err) {\n+\t\tif err != nil {\n+\t\t\tif os.IsNotExist(err) {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif os.IsPermission(err) && userns.RunningInUserNS() {\n+\t\t\t\tlog.L.Debug(err.Error())\n+\t\t\t\tcontinue\n+\t\t\t}\n \t\t\treturn nil, errors.Wrap(err, \"error reading dir entries\")\n \t\t}\n-\n \t\tfor _, entry := range dirEntries {\n \t\t\tif entry.IsDir() {\n \t\t\t\tinfos, err := os.ReadDir(filepath.Join(p, entry.Name()))\n", "instance_id": "moby__moby-47587", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear, as it describes a specific regression issue in Docker v25.0.3 related to plugin discovery using the wrong path in a rootless Docker setup. It provides steps to reproduce the issue, the expected behavior, and detailed system information (Docker version, info, etc.), which are helpful for context. Additionally, it references a prior issue (#43111) for background. However, there are minor ambiguities: the problem statement does not explicitly define the root cause or expected solution approach, leaving some interpretation to the developer. Edge cases or specific constraints (e.g., other environments or configurations where this might occur) are not mentioned. While the issue is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively small, confined to a single file (`pkg/plugins/discovery.go`) and a specific function, with modifications involving conditional logic for error handling. The change adds checks for permission errors in a user namespace context, which requires understanding of rootless Docker and user namespaces\u2014a moderately complex concept in container runtimes. Second, the technical concepts involved include familiarity with Go's standard library (`os` package), error handling, logging (`containerd/log`), and domain-specific knowledge of Docker's plugin discovery mechanism and user namespace behavior (`containerd/containerd/pkg/userns`). While these are not overly advanced, they do require a solid understanding of container internals. Third, the change addresses a specific error condition (permission denied in user namespaces), but does not appear to introduce complex edge cases beyond this, nor does it impact the broader system architecture. Overall, this problem requires understanding multiple concepts and making targeted modifications, but it is not deeply invasive or architecturally significant, placing it at a medium difficulty level of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "gemma3 embeddings support\n### What is the issue?\n\ngemma3:1b fails to generate embeddings, other models work.\n\n```sh-session\n$ curl http://localhost:11434/api/embed -d '{\n                                    \"model\": \"gemma3:1b\",\n                                    \"input\": \"Llamas are members of the camelid family\"\n                                  }'\n{\"model\":\"gemma3:1b\",\"embeddings\":[null],\"total_duration\":795587042,\"load_duration\":644392083,\"prompt_eval_count\":10}\u23ce\n```\n\n### Relevant log output\n\n```shell\ntime=2025-03-13T13:49:34.664-04:00 level=INFO source=server.go:624 msg=\"llama runner started in 0.50 seconds\"\ntime=2025-03-13T13:49:34.846-04:00 level=WARN source=runner.go:429 msg=\"generation of embedding outputs not yet supported\"\n[GIN] 2025/03/13 - 13:49:34 | 200 |  818.674917ms |       127.0.0.1 | POST     \"/api/embed\"\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.6.0\n", "patch": "diff --git a/runner/ollamarunner/runner.go b/runner/ollamarunner/runner.go\nindex c1475cbb241..c380ef22145 100644\n--- a/runner/ollamarunner/runner.go\n+++ b/runner/ollamarunner/runner.go\n@@ -691,65 +691,6 @@ type EmbeddingResponse struct {\n \tEmbedding []float32 `json:\"embedding\"`\n }\n \n-func (s *Server) embeddings(w http.ResponseWriter, r *http.Request) {\n-\tvar req EmbeddingRequest\n-\tif err := json.NewDecoder(r.Body).Decode(&req); err != nil {\n-\t\thttp.Error(w, fmt.Sprintf(\"bad request: %s\", err), http.StatusBadRequest)\n-\t\treturn\n-\t}\n-\n-\tw.Header().Set(\"Content-Type\", \"application/json\")\n-\n-\tslog.Debug(\"embedding request\", \"content\", req.Content)\n-\n-\tseq, err := s.NewSequence(req.Content, nil, NewSequenceParams{embedding: true})\n-\tif err != nil {\n-\t\thttp.Error(w, fmt.Sprintf(\"Failed to create new sequence: %v\", err), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\t// Ensure there is a place to put the sequence, released when removed from s.seqs\n-\tif err := s.seqsSem.Acquire(r.Context(), 1); err != nil {\n-\t\tif errors.Is(err, context.Canceled) {\n-\t\t\tslog.Info(\"aborting embeddings request due to client closing the connection\")\n-\t\t} else {\n-\t\t\tslog.Error(\"Failed to acquire semaphore\", \"error\", err)\n-\t\t}\n-\t\treturn\n-\t}\n-\n-\ts.mu.Lock()\n-\tfound := false\n-\tfor i, sq := range s.seqs {\n-\t\tif sq == nil {\n-\t\t\tseq.cache, seq.inputs, err = s.cache.LoadCacheSlot(seq.inputs, req.CachePrompt)\n-\t\t\tif err != nil {\n-\t\t\t\ts.mu.Unlock()\n-\t\t\t\thttp.Error(w, fmt.Sprintf(\"Failed to load cache: %v\", err), http.StatusInternalServerError)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\ts.seqs[i] = seq\n-\t\t\ts.cond.Signal()\n-\t\t\tfound = true\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\ts.mu.Unlock()\n-\n-\tif !found {\n-\t\thttp.Error(w, \"could not find an available sequence\", http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tembedding := <-seq.embedding\n-\n-\tif err := json.NewEncoder(w).Encode(&EmbeddingResponse{\n-\t\tEmbedding: embedding,\n-\t}); err != nil {\n-\t\thttp.Error(w, fmt.Sprintf(\"failed to encode response: %v\", err), http.StatusInternalServerError)\n-\t}\n-}\n-\n type HealthResponse struct {\n \tStatus   string  `json:\"status\"`\n \tProgress float32 `json:\"progress\"`\n@@ -927,9 +868,13 @@ func Execute(args []string) error {\n \tdefer listener.Close()\n \n \tmux := http.NewServeMux()\n-\tmux.HandleFunc(\"/embedding\", server.embeddings)\n-\tmux.HandleFunc(\"/completion\", server.completion)\n-\tmux.HandleFunc(\"/health\", server.health)\n+\t// TODO: support embeddings\n+\tmux.HandleFunc(\"POST /embedding\", func(w http.ResponseWriter, r *http.Request) {\n+\t\thttp.Error(w, \"this model does not support embeddings\", http.StatusNotImplemented)\n+\t})\n+\n+\tmux.HandleFunc(\"POST /completion\", server.completion)\n+\tmux.HandleFunc(\"GET /health\", server.health)\n \n \thttpServer := http.Server{\n \t\tHandler: mux,\ndiff --git a/server/routes.go b/server/routes.go\nindex 3efa12e43bd..bc3fe3fb549 100644\n--- a/server/routes.go\n+++ b/server/routes.go\n@@ -483,8 +483,7 @@ func (s *Server) EmbedHandler(c *gin.Context) {\n \t}\n \n \tif err := g.Wait(); err != nil {\n-\t\tslog.Error(\"embedding generation failed\", \"error\", err)\n-\t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": fmt.Errorf(\"failed to generate embeddings: %v\", err)})\n+\t\tc.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{\"error\": strings.TrimSpace(err.Error())})\n \t\treturn\n \t}\n \n@@ -545,8 +544,7 @@ func (s *Server) EmbeddingsHandler(c *gin.Context) {\n \n \tembedding, err := r.Embedding(c.Request.Context(), req.Prompt)\n \tif err != nil {\n-\t\tslog.Info(fmt.Sprintf(\"embedding generation failed: %v\", err))\n-\t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": fmt.Errorf(\"failed to generate embedding: %v\", err)})\n+\t\tc.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{\"error\": strings.TrimSpace(err.Error())})\n \t\treturn\n \t}\n \n", "instance_id": "ollama__ollama-9742", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `gemma3:1b` model fails to generate embeddings while other models work, as evidenced by the provided curl command output and relevant log messages. The goal of addressing embedding support for this model is implied, and the logs provide context about the current limitation (\"generation of embedding outputs not yet supported\"). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the desired outcome (e.g., whether embeddings should be fully implemented for `gemma3:1b` or if a specific workaround is needed). Additionally, constraints or requirements for supporting embeddings with this model (e.g., hardware limitations, model-specific quirks) are not mentioned. Edge cases or specific input/output formats for embeddings are also not discussed. Despite these gaps, the issue is valid and understandable with the provided context, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes primarily involve removing or commenting out the existing embeddings functionality in `runner.go` and replacing it with a placeholder that returns a \"not implemented\" error. Additionally, minor error handling improvements are made in `routes.go`. The changes are localized to a couple of files and do not significantly impact the broader system architecture. However, fully solving the problem (i.e., implementing embedding support for `gemma3:1b`) is not reflected in the diff and would likely require more substantial changes, potentially involving model-specific logic or integration with underlying libraries. Based on the given diff, the scope is limited and manageable.\n\n2. **Number of Technical Concepts:** The code changes shown require basic understanding of HTTP handlers in Go, JSON encoding/decoding, and error handling using the `http` package and `gin` framework. These are relatively straightforward concepts for a developer familiar with Go web development. However, to fully address embedding support (beyond the placeholder), one might need to understand model-specific embedding generation logic, interaction with the underlying machine learning framework, and possibly GPU/CPU handling on macOS with Apple hardware. Since the diff only shows a temporary workaround, the required concepts are minimal.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, such as specific input formats, large inputs, or model compatibility issues. The code changes simplify error handling by returning a static error message for embeddings, which avoids dealing with complex edge cases at this stage. However, a complete solution would likely need to handle errors related to model loading, input validation, and embedding generation failures, which are not addressed in the diff.\n\n4. **Overall Complexity:** The provided code changes represent a stopgap measure rather than a full solution to the problem. Implementing a placeholder error message is a simple task that requires minimal understanding of the codebase. However, I\u2019ve factored in a slight increase in difficulty (to 0.35) to account for the implied need to eventually understand the model-specific embedding logic and integrate it into the system, which would involve moderate complexity. The current changes do not require deep architectural knowledge or advanced technical concepts, keeping the difficulty in the Easy range.\n\nIn summary, the task as represented by the code changes is straightforward, involving minimal modifications to disable a feature temporarily. However, the broader context of enabling embedding support for `gemma3:1b` hints at additional complexity not yet addressed, justifying a score of 0.35.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Query rewrite operations\nCurrently, the only way we offer to rewrite the query part of the request URI is with the `rewrite` handler (when it has a `?` in the target), or with `uri replace` to perform replacements on the entire URI.\r\n\r\nThese options are limiting, because many query rewrite operations are difficult to perform with those.\r\n\r\nFor example, it's impossible (I can't think of a way) to remove a single query key from a request without knowing all the other possible query keys and re-including them all with `&key={query.key}` in the rewrite.\r\n\r\nIt's also unwieldy to replace a specific query key with another (one way is with `uri replace` but it's awkward and error-prone).\r\n\r\nI think the solution is to add `uri query` which supports multiple different types of operations on queries, similarly to the [`header` directive](https://caddyserver.com/docs/caddyfile/directives/header) which has `+` and `-` and so on.\r\n\r\n```\r\n# add an additional `foo=bar` (even if there was already `foo=baz`)\r\nuri query +foo bar\r\n\r\n# overwrite `foo` with `foo=bar`\r\nuri query foo bar\r\n\r\n# delete `foo`\r\nuri query -foo\r\n\r\n# rename key `foo` to `bar`\r\nuri query foo>bar\r\n\r\n# set a default value (if the key is not already set)\r\nuri query ?foo bar\r\n\r\n# replace values in `foo`, changing `bar` to `baz`\r\nuri query foo bar baz\r\n\r\n# do multiple operations at once\r\nuri query {\r\n\tfoo bar\r\n\tbar baz\r\n\t-baz\r\n}\r\n```\r\n\r\nWe may or may not actually implement all these operations, but I think most of them would be very nice to have.\r\n\r\nShould this be its own directive instead of being added to `uri`? To be discussed.\n", "patch": "diff --git a/modules/caddyhttp/rewrite/caddyfile.go b/modules/caddyhttp/rewrite/caddyfile.go\nindex 31f7e9b4859..0ce5c41d217 100644\n--- a/modules/caddyhttp/rewrite/caddyfile.go\n+++ b/modules/caddyhttp/rewrite/caddyfile.go\n@@ -213,6 +213,9 @@ func applyQueryOps(h httpcaddyfile.Helper, qo *queryOps, args []string) error {\n \t\trenameValKey := strings.Split(key, \">\")\n \t\tqo.Rename = append(qo.Rename, queryOpsArguments{Key: renameValKey[0], Val: renameValKey[1]})\n \n+\tcase len(args) == 3:\n+\t\tqo.Replace = append(qo.Replace, &queryOpsReplacement{Key: key, SearchRegexp: args[1], Replace: args[2]})\n+\n \tdefault:\n \t\tif len(args) != 2 {\n \t\t\treturn h.ArgErr()\ndiff --git a/modules/caddyhttp/rewrite/rewrite.go b/modules/caddyhttp/rewrite/rewrite.go\nindex 1859f9df2b5..3479f064991 100644\n--- a/modules/caddyhttp/rewrite/rewrite.go\n+++ b/modules/caddyhttp/rewrite/rewrite.go\n@@ -118,6 +118,12 @@ func (rewr *Rewrite) Provision(ctx caddy.Context) error {\n \t\trep.re = re\n \t}\n \n+\tfor _, replacementOp := range rewr.Query.Replace {\n+\t\terr := replacementOp.Provision(ctx)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"compiling regular expression %s in query rewrite replace operation: %v\", replacementOp.SearchRegexp, err)\n+\t\t}\n+\t}\n \treturn nil\n }\n \n@@ -490,13 +496,27 @@ type queryOps struct {\n \t// and only appends an additional value for that key if any already exist.\n \tAdd []queryOpsArguments `json:\"add,omitempty\"`\n \n+\t// Replaces query parameters.\n+\tReplace []*queryOpsReplacement `json:\"replace,omitempty\"`\n+\n \t// Deletes a given query key by name.\n \tDelete []string `json:\"delete,omitempty\"`\n }\n \n+// Provision compiles the query replace operation regex.\n+func (replacement *queryOpsReplacement) Provision(_ caddy.Context) error {\n+\tif replacement.SearchRegexp != \"\" {\n+\t\tre, err := regexp.Compile(replacement.SearchRegexp)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"replacement for query field '%s': %v\", replacement.Key, err)\n+\t\t}\n+\t\treplacement.re = re\n+\t}\n+\treturn nil\n+}\n+\n func (q *queryOps) do(r *http.Request, repl *caddy.Replacer) {\n \tquery := r.URL.Query()\n-\n \tfor _, renameParam := range q.Rename {\n \t\tkey := repl.ReplaceAll(renameParam.Key, \"\")\n \t\tval := repl.ReplaceAll(renameParam.Val, \"\")\n@@ -525,6 +545,36 @@ func (q *queryOps) do(r *http.Request, repl *caddy.Replacer) {\n \t\tquery[key] = append(query[key], val)\n \t}\n \n+\tfor _, replaceParam := range q.Replace {\n+\t\tkey := repl.ReplaceAll(replaceParam.Key, \"\")\n+\t\tsearch := repl.ReplaceKnown(replaceParam.Search, \"\")\n+\t\treplace := repl.ReplaceKnown(replaceParam.Replace, \"\")\n+\n+\t\t// replace all query keys...\n+\t\tif key == \"*\" {\n+\t\t\tfor fieldName, vals := range query {\n+\t\t\t\tfor i := range vals {\n+\t\t\t\t\tif replaceParam.re != nil {\n+\t\t\t\t\t\tquery[fieldName][i] = replaceParam.re.ReplaceAllString(query[fieldName][i], replace)\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tquery[fieldName][i] = strings.ReplaceAll(query[fieldName][i], search, replace)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tfor fieldName, vals := range query {\n+\t\t\tfor i := range vals {\n+\t\t\t\tif replaceParam.re != nil {\n+\t\t\t\t\tquery[fieldName][i] = replaceParam.re.ReplaceAllString(query[fieldName][i], replace)\n+\t\t\t\t} else {\n+\t\t\t\t\tquery[fieldName][i] = strings.ReplaceAll(query[fieldName][i], search, replace)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \tfor _, deleteParam := range q.Delete {\n \t\tparam := repl.ReplaceAll(deleteParam, \"\")\n \t\tif param == \"\" {\n@@ -546,5 +596,21 @@ type queryOpsArguments struct {\n \tVal string `json:\"val,omitempty\"`\n }\n \n+type queryOpsReplacement struct {\n+\t// The key to replace in the query string.\n+\tKey string `json:\"key,omitempty\"`\n+\n+\t// The substring to search for.\n+\tSearch string `json:\"search,omitempty\"`\n+\n+\t// The regular expression to search with.\n+\tSearchRegexp string `json:\"search_regexp,omitempty\"`\n+\n+\t// The string with which to replace matches.\n+\tReplace string `json:\"replace,omitempty\"`\n+\n+\tre *regexp.Regexp\n+}\n+\n // Interface guard\n var _ caddyhttp.MiddlewareHandler = (*Rewrite)(nil)\n", "instance_id": "caddyserver__caddy-6165", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of enhancing query rewrite operations for a web server framework (likely Caddy, based on the context). It identifies the limitations of current methods for manipulating query parameters in URIs and proposes a new `uri query` directive with various operations (add, overwrite, delete, rename, set default, and replace). The statement includes specific syntax examples for each operation, which aids in understanding the desired functionality. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not fully specify how conflicts or edge cases (e.g., duplicate keys, malformed queries, or precedence of operations when multiple are defined Stuart Little (1999) - S1 E2 - The New Teacher\n\nAssistant: Additionally, it lacks explicit discussion of constraints or limitations on query parameter values or keys (e.g., size limits, character restrictions). The statement also raises a question about whether this should be a standalone directive or part of the existing `uri` directive but does not provide a clear decision or criteria for resolving this, leaving some design intent unclear. Overall, while the problem's intent and examples provide a solid foundation, these minor gaps in detail slightly reduce the clarity.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving modifications to at least two files (`caddyfile.go` and `rewrite.go`) within the Caddy HTTP module. The changes include adding new functionality for query parameter replacement with support for both string-based and regex-based operations, which requires understanding and extending an existing `queryOps` struct and its associated logic. This involves a non-trivial amount of code (around 50-60 lines of diff), including new data structures (`queryOpsReplacement`), provisioning logic for regex compilation, and request processing logic to apply replacements.\n\nSecond, the technical concepts required include familiarity with Go (struct definitions, pointer handling, slices), HTTP request handling (manipulating `http.Request` and URL query parameters), and regular expressions (compiling and applying regex for replacements). Additionally, the developer must understand the Caddy framework's middleware handler architecture and its configuration parsing mechanism (via `httpcaddyfile.Helper`). While these concepts are not overly advanced, they require a solid grasp of Go and some domain knowledge of HTTP servers, pushing this beyond a simple task.\n\nThird, the problem involves handling edge cases, such as supporting a wildcard key (`*`) for replacing values across all query parameters, and ensuring that replacements (both string and regex-based) are applied correctly without unintended side effects (e.g., infinite loops in regex replacements or malformed URIs). Error handling is also extended with regex compilation errors, though the complexity of these edge cases is moderate as they are somewhat constrained by the HTTP request context.\n\nFinally, the impact on the codebase architecture is limited; this is an additive feature rather than a refactor of core components, though it integrates with existing rewrite logic, requiring careful testing to avoid breaking existing functionality. Overall, this task requires understanding multiple concepts and making targeted but complex modifications, fitting a medium difficulty score of 0.55. It is not hard enough to demand deep architectural changes or advanced domain-specific knowledge, nor is it a trivial bug fix or simple feature addition.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CRITICAL: unwanted squash during rebase\n**Describe the bug/To reproduce**\nA\nB <= start rebase\nmodify foo.py content in my code editor\nin lazygit I select foo;py and amend it (<space>A)\nrebase continue\ncommit A conflicts with B\nSo I must modify foo.py now and solve the problem\nA popup appears should we continue the merge. I hit esape because obviously I gotta check the diff of my commit, possibly edit the commit messag etc.\nCommit seems ok. I append (click A).\nAnd here's the problem. Instead of editing last, conflicting, commit it SQUASH without telling me with the last commit.\nSo now instead of\nA\nB\nI just have \nB (B only has the commit message of B. But it has the code of A and B)\n\nSo basically the bug is: if you ever happen to <space>A while in a rebase conflict then the commit A wil merge with commit B.\n\nSo hopefully no one will say \"hum technically it's not a bug\". IDK the technicalities but I'll add personnal context: I work for weeks on commit. I have to constantly rebase them (company policy + we use gerrit which is rebase only unlike GH). I have long series (+20 patches) and can spend a day rebasing just one. Having one commit randomly merged inside my series is not always obvious to notice. I cannot unmerge the commit. I gotta manually split them or redo the whole rebase. It is *really* distraughting to see my very long rebase turned into random merge sessions.\nI love your app but I cannot stress enough of critical I feel like this bug/bad UX is. \n\n**Expected behavior**\nMake amend works the same for conflicting commit as for non conflicting commit.\n\n**Screenshots**\n\n**Version info:**\ncommit=c03b89227092b852d50015d289a7c6d8c69811c5, build date=2025-01-17T13:37:10Z, build source=binaryRelease, version=0.45.2, os=linux, arch=amd64, git version=2.34.1\n\n\u27e9 git --version\ngit version 2.34.1\n\n**Additional context**\ngit config --list\nuser.name=<Ilan Schemoul>\nuser.email=<ilan.schemoul@intersec.com>\ninclude.path=/home/ilan/dev/tools/dotfiles/gitinclude\ncore.excludesfile=~/.gitignore\ncore.autocrlf=input\nalias.br=branch\nalias.co=checkout\nalias.ci=commit\nalias.ri=rebase -i\nalias.st=status\nalias.stat=status\nalias.pop=reset HEAD~\nalias.undo=reset HEAD~\nalias.glog=log --graph\nalias.tempo=commit -a -m tempo\nalias.su=submodule update --init --recursive\nalias.rh=reset --hard\nalias.cp=cherry-pick\nalias.amend=commit --amend\nalias.squash=commit --amend -C HEAD\nalias.fixup=commit --amend -C HEAD\nalias.unstash=stash apply\nalias.track=add -N\nalias.untrack=rm --cached\nalias.vimdiff=difftool -y -t vimdiff\nalias.workdir=!sh /usr/share/doc/git/contrib/workdir/git-new-workdir\nalias.branch-contains=branch origin/* --remotes --contains\nalias.find-merge=!sh -c 'commit=$0 && branch=${1:-HEAD} && (git rev-list $commit..$branch --ancestry-path | cat -n; git rev-list $commit..$branch --first-parent | cat -n) | sort -k2 -s | uniq -f1 -d | sort -n | tail -1 | cut -f2'\nalias.diff-no-blank=diff --ignore-space-change --ignore-all-space --ignore-blank-lines\nalias.diff-word=diff --word-diff --word-diff-regex=\"[^ ;]+\"\nalias.show-word=show --word-diff --word-diff-regex=\"[^ ;]+\"\nalias.diff-char=diff --word-diff --word-diff-regex=.\nalias.show-char=show --word-diff --word-diff-regex=.\nalias.picore=cherry-pick\nalias.decoupe=bisect\nalias.journal=log\nalias.graffiti=tag\nalias.ajoute=add\nalias.dichotomie=bisect\nalias.transaction=commit\nalias.deploiement=checkout\nalias.cerise=cherry\nalias.recherche-par-expression-rationelle=grep\nalias.fusionne=merge\nalias.planque=stash\nalias.ramasse-miettes=gc\nalias.tire=pull\nalias.pousse=push\nalias.pousse-fort=push-for\nalias.montre=show\nalias.rembobine=reset\nalias.supprime-cette-merde=revert\nalias.balance=blame\nalias.poop=stash pop\nalias.truite=merge\ncolor.ui=auto\ncolor.diff=auto\ncolor.branch=auto\ncolor.status=auto\ncolor.pager=true\ncolor.interactive=auto\ncolor.diff.meta=green\ncolor.diff.frag=yellow\ncolor.diff.old=magenta\ncolor.diff.new=bold cyan\ncolor.status.header=bold blue\ncolor.status.added=green\ncolor.status.changed=bold red\ncolor.status.untracked=bold yellow\nnotes.displayref=refs/notes/*\ncommit.template=/srv/tools/share/dotfiles/git-commit-template\npush.default=upstream\nmerge.verbosity=1\nmerge.conflictstyle=diff3\nmerge.stat=true\nmerge.renamelimit=100000\nmerge.ff=false\nmerge.ours.driver=true\nrebase.stat=true\nrebase.autosquash=true\ndiff.renames=copies\nurl.ssh://git.corp:29418/.insteadof=git.corp:/srv/git\nurl.ssh://git.corp:29418/.insteadof=ssh://git.corp/srv/git\nurl.ssh://git.corp:29418/.insteadof=gitolite@git.corp:\nurl.ssh://git.corp:29418/mmsx.insteadof=git.corp:/srv/git/qdb\nurl.ssh://git.corp:29418/mmsx.insteadof=ssh://git.corp/srv/git/qdb\nurl.ssh://git.corp:29418/mmsx.insteadof=ssh://git.corp:29418/qdb\nurl.ssh://git.corp:29418/mmsx.insteadof=gitolite@git.corp:qdb\nurl.ssh://git.corp:29418/mmsx.insteadof=git.corp:/srv/git/qrrd\nurl.ssh://git.corp:29418/mmsx.insteadof=ssh://git.corp/srv/git/qrrd\nurl.ssh://git.corp:29418/mmsx.insteadof=ssh://git.corp:29418/qrrd\nurl.ssh://git.corp:29418/mmsx.insteadof=gitolite@git.corp:qrrd\nurl.ssh://git.corp:29418/mmsx.insteadof=ssh://git.corp:29418/platform\nrerere.enabled=true\nsubmodule.platform.update=!/srv/tools/share/scripts/git-submodule-update.sh\nsubmodule.lib-common.update=!/srv/tools/share/scripts/git-submodule-update.sh\ntig.bind.generic=r !git rebase -i %(commit)\ntig.bind.generic=p @sh -c \"echo -n %(commit) | xclip\"\ntig.bind.generic=f !git commit --fixup=%(commit)\npull.rebase=true\nmaintenance.repo=/home/ilan/dev/mmsx-master\nmaintenance.repo=/home/ilan/dev/mmsx-2022-dev\nmaintenance.repo=/home/ilan/dev/mmsx-4\nalias.gl=config --global -l\ncore.repositoryformatversion=0\ncore.filemode=true\ncore.bare=false\ncore.logallrefupdates=true\ncore.fsmonitor=false\ncore.untrackedcache=true\nremote.origin.url=ssh://git.corp:29418/mmsx\nremote.origin.fetch=+refs/heads/*:refs/remotes/origin/*\nsubmodule.platform/lib-common.active=true\nsubmodule.platform/lib-common.url=ssh://git.corp:29418/lib-common\nsubmodule.platform/www/modules/tcpdf/external.active=true\nsubmodule.platform/www/modules/tcpdf/external.url=ssh://git.corp:29418/tcpdf\nremote.mob.url=ssh://git.corp:29418/mmsx-mob.git\nremote.mob.fetch=+refs/heads/*:refs/remotes/mob/*\nbranch.2022-dev.remote=origin\nbranch.2022-dev.merge=refs/heads/2022-dev\nmaintenance.auto=false\nmaintenance.strategy=incremental\nlog.excludedecoration=refs/prefetch/\nsubmodule.platform.active=true\nsubmodule.platform.url=ssh://git.corp:29418/platform\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/ckeditor-releases.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/ckeditor-releases.url=ssh://git.corp:29418/ckeditor-releases\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jasmine.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jasmine.url=ssh://git.corp:29418/jasmine\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jasmine-jquery.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jasmine-jquery.url=ssh://git.corp:29418/jasmine-jquery\nbranch.geom.remote=mob\nbranch.geom.merge=refs/heads/ilan/geom-minimal-lib\nbranch.2024-dev.remote=origin\nbranch.2024-dev.merge=refs/heads/2024-dev\nbranch.ARCHIVE-preview.remote=mob\nbranch.ARCHIVE-preview.merge=refs/heads/ilan/keyword-replacement\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/highcharts.com.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/highcharts.com.url=ssh://git.corp:29418/highcharts.com\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jsplumb.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/jsplumb.url=ssh://git.corp:29418/jsplumb\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/pace.active=true\nsubmodule.platform/www/modules/core/htdocs/javascript/ext/pace.url=ssh://git.corp:29418/pace\n\nLazy git config\n\nos:\n  edit: \"command nvim --server $NVIM --remote-send \\\"<C-\\\\><C-N>:FromFTToTab {{filename}}<cr>\\\"\"\n  editAtLine: \"command nvim --server $NVIM --remote-send \\\"<C-\\\\><C-N>:FromFTToTab {{filename}} {{line}}<cr>\\\"\"\n  openDirInEditor: \"command nvim --server $NVIM --remote-send \\\"<C-\\\\><C-N>:FromFTToTab {{filename}}<cr>\\\"\"\n  open: \"command nvim --server $NVIM --remote-send \\\"<C-\\\\><C-N>:FromFTToTab {{filename}}<cr>\\\"\"\ncustomCommands:\n  - key: 'G'\n    showOutput: true\n    context: 'global'\n    command: 'git push-for {{.Form.Branch | quote}}'\n    prompts:\n      - type: 'input'\n        title: 'Which branch ?'\n        key: 'Branch'\n        initialValue: '2024-dev'\n        suggestions:\n          preset: 'branches' # use built-in logic for obtaining branches\n  - key: 'b'\n    showOutput: true\n    context: 'files'\n    command: 'git absorb'\n  - key: 'B'\n    showOutput: true\n    context: 'files'\n    command: 'git absorb --and-rebase'\ngui:\n  nerdFontsVersion: \"3\"\n  # The number of lines you scroll by when scrolling the main window\n  scrollHeight: 10\n  commitHashLength: 3\ndisableStartupPopups: true\ngit:\n  commit:\n    autoWrapWidth: 72\n  log:\n    order: default\n\n\n**Note:** please try updating to the latest version or [manually building](https://github.com/jesseduffield/lazygit/#manual) the latest `master` to see if the issue still occurs.\n\nI won't do master but I have a super fresh version and the behaviour was also exhibited by 0.42. It's a not a new behaviour and unlikely to change in master as 0.45.2 is super new.\n", "patch": "diff --git a/pkg/gui/controllers/files_controller.go b/pkg/gui/controllers/files_controller.go\nindex ac72565ad8f..00317f4cf3d 100644\n--- a/pkg/gui/controllers/files_controller.go\n+++ b/pkg/gui/controllers/files_controller.go\n@@ -690,23 +690,68 @@ func (self *FilesController) refresh() error {\n }\n \n func (self *FilesController) handleAmendCommitPress() error {\n-\tself.c.Confirm(types.ConfirmOpts{\n-\t\tTitle:  self.c.Tr.AmendLastCommitTitle,\n-\t\tPrompt: self.c.Tr.SureToAmend,\n-\t\tHandleConfirm: func() error {\n-\t\t\treturn self.c.Helpers().WorkingTree.WithEnsureCommittableFiles(func() error {\n-\t\t\t\tif len(self.c.Model().Commits) == 0 {\n-\t\t\t\t\treturn errors.New(self.c.Tr.NoCommitToAmend)\n-\t\t\t\t}\n+\tdoAmend := func() error {\n+\t\treturn self.c.Helpers().WorkingTree.WithEnsureCommittableFiles(func() error {\n+\t\t\tif len(self.c.Model().Commits) == 0 {\n+\t\t\t\treturn errors.New(self.c.Tr.NoCommitToAmend)\n+\t\t\t}\n \n-\t\t\t\treturn self.c.Helpers().AmendHelper.AmendHead()\n-\t\t\t})\n-\t\t},\n-\t})\n+\t\t\treturn self.c.Helpers().AmendHelper.AmendHead()\n+\t\t})\n+\t}\n+\n+\tif self.isResolvingConflicts() {\n+\t\treturn self.c.Menu(types.CreateMenuOptions{\n+\t\t\tTitle:      self.c.Tr.AmendCommitTitle,\n+\t\t\tPrompt:     self.c.Tr.AmendCommitWithConflictsMenuPrompt,\n+\t\t\tHideCancel: true, // We want the cancel item first, so we add one manually\n+\t\t\tItems: []*types.MenuItem{\n+\t\t\t\t{\n+\t\t\t\t\tLabel: self.c.Tr.Cancel,\n+\t\t\t\t\tOnPress: func() error {\n+\t\t\t\t\t\treturn nil\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tLabel: self.c.Tr.AmendCommitWithConflictsContinue,\n+\t\t\t\t\tOnPress: func() error {\n+\t\t\t\t\t\treturn self.c.Helpers().MergeAndRebase.ContinueRebase()\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tLabel: self.c.Tr.AmendCommitWithConflictsAmend,\n+\t\t\t\t\tOnPress: func() error {\n+\t\t\t\t\t\treturn doAmend()\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t})\n+\t} else {\n+\t\tself.c.Confirm(types.ConfirmOpts{\n+\t\t\tTitle:  self.c.Tr.AmendLastCommitTitle,\n+\t\t\tPrompt: self.c.Tr.SureToAmend,\n+\t\t\tHandleConfirm: func() error {\n+\t\t\t\treturn doAmend()\n+\t\t\t},\n+\t\t})\n+\t}\n \n \treturn nil\n }\n \n+func (self *FilesController) isResolvingConflicts() bool {\n+\tcommits := self.c.Model().Commits\n+\tfor _, c := range commits {\n+\t\tif c.Status != models.StatusRebasing {\n+\t\t\tbreak\n+\t\t}\n+\t\tif c.Action == models.ActionConflict {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n func (self *FilesController) handleStatusFilterPressed() error {\n \treturn self.c.Menu(types.CreateMenuOptions{\n \t\tTitle: self.c.Tr.FilteringMenuTitle,\ndiff --git a/pkg/gui/controllers/helpers/merge_and_rebase_helper.go b/pkg/gui/controllers/helpers/merge_and_rebase_helper.go\nindex 2fb30372b84..40d9e6df2c6 100644\n--- a/pkg/gui/controllers/helpers/merge_and_rebase_helper.go\n+++ b/pkg/gui/controllers/helpers/merge_and_rebase_helper.go\n@@ -77,6 +77,10 @@ func (self *MergeAndRebaseHelper) CreateRebaseOptionsMenu() error {\n \treturn self.c.Menu(types.CreateMenuOptions{Title: title, Items: menuItems})\n }\n \n+func (self *MergeAndRebaseHelper) ContinueRebase() error {\n+\treturn self.genericMergeCommand(REBASE_OPTION_CONTINUE)\n+}\n+\n func (self *MergeAndRebaseHelper) genericMergeCommand(command string) error {\n \tstatus := self.c.Git().Status.WorkingTreeState()\n \ndiff --git a/pkg/i18n/english.go b/pkg/i18n/english.go\nindex 87744f24d71..28e12635626 100644\n--- a/pkg/i18n/english.go\n+++ b/pkg/i18n/english.go\n@@ -353,6 +353,9 @@ type TranslationSet struct {\n \tScrollDownMainWindow                  string\n \tAmendCommitTitle                      string\n \tAmendCommitPrompt                     string\n+\tAmendCommitWithConflictsMenuPrompt    string\n+\tAmendCommitWithConflictsContinue      string\n+\tAmendCommitWithConflictsAmend         string\n \tDropCommitTitle                       string\n \tDropCommitPrompt                      string\n \tDropUpdateRefPrompt                   string\n@@ -1375,6 +1378,9 @@ func EnglishTranslationSet() *TranslationSet {\n \t\tScrollDownMainWindow:                 \"Scroll down main window\",\n \t\tAmendCommitTitle:                     \"Amend commit\",\n \t\tAmendCommitPrompt:                    \"Are you sure you want to amend this commit with your staged files?\",\n+\t\tAmendCommitWithConflictsMenuPrompt:   \"WARNING: you are about to amend the last finished commit with your resolved conflicts. This is very unlikely to be what you want at this point. More likely, you simply want to continue the rebase instead.\\n\\nDo you still want to amend the previous commit?\",\n+\t\tAmendCommitWithConflictsContinue:     \"No, continue rebase\",\n+\t\tAmendCommitWithConflictsAmend:        \"Yes, amend previous commit\",\n \t\tDropCommitTitle:                      \"Drop commit\",\n \t\tDropCommitPrompt:                     \"Are you sure you want to drop the selected commit(s)?\",\n \t\tDropMergeCommitPrompt:                \"Are you sure you want to drop the selected merge commit? Note that it will also drop all the commits that were merged in by it.\",\n", "instance_id": "jesseduffield__lazygit-4222", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: during a rebase conflict in lazygit, using the amend command (<space>A) results in an unintended squash of commits, which is a critical UX issue for the user, especially in workflows involving long rebase series. The expected behavior is explicitly stated\u2014amend should work consistently for conflicting and non-conflicting commits. The steps to reproduce the bug are provided, along with personal context that highlights the severity of the issue. However, there are minor ambiguities: the problem statement does not fully clarify the exact state of the rebase process or the expected internal behavior of lazygit during conflict resolution (e.g., whether the amend should apply to the conflicting commit or behave differently). Additionally, edge cases or specific scenarios (e.g., multiple conflicts, specific git configurations) are not mentioned, which could impact the solution's completeness. Despite these minor gaps, the statement is actionable and provides sufficient detail for a developer to understand the core issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is moderate, involving modifications across a few files (files_controller.go, merge_and_rebase_helper.go, and i18n/english.go) to handle the amend behavior during rebase conflicts. The changes require adding logic to detect conflict states and present a menu to the user to choose between continuing the rebase or amending the commit, which impacts the user interaction flow but not the core architecture of lazygit. Second, the technical concepts involved include understanding git rebase workflows, conflict resolution states, and lazygit's internal model for commits and UI interactions (e.g., menu creation, confirmation dialogs). These concepts are not overly complex for a developer familiar with git tools, but they do require a solid grasp of the lazygit codebase's structure and helper utilities. Third, the problem introduces the need to handle specific edge cases, such as ensuring the amend operation does not inadvertently squash commits during conflicts and providing clear user feedback through the UI, as seen in the added menu prompts. However, the edge cases are not extraordinarily intricate, and the solution does not appear to require deep system-level changes or advanced algorithms. Overall, this problem requires understanding multiple components and making targeted, moderately complex modifications, justifying a difficulty score of 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Range select with more than just up/down arrows\n**Is your feature request related to a problem? Please describe.**\r\nI can't seem to select a range of text in the Unstaged Changes box without selecting one line at a time by pressing or holding down the up/down arrows. This is very slow and tedious.\r\n\r\n**Describe the solution you'd like**\r\nIdeally, I'd be able to select text using vim-like shortcuts, but I understand that the shortcut space is already quite crowded. But it would be super cool to be able to hit `v` and then have lazygit change keymaps and support all of the standard `vim`-like shortcuts (`ctrl-d` /`ctrl-u`, `gg`/`G`, etc).  But, short of  that, hitting `v` to toggle range selection and then using either `<`/`>` or `,`/`.` to select a range would be sufficient.\r\n\r\n\n", "patch": "diff --git a/pkg/gui/controllers/patch_explorer_controller.go b/pkg/gui/controllers/patch_explorer_controller.go\nindex 6d3290f0411..999dc15e97e 100644\n--- a/pkg/gui/controllers/patch_explorer_controller.go\n+++ b/pkg/gui/controllers/patch_explorer_controller.go\n@@ -244,14 +244,12 @@ func (self *PatchExplorerController) HandleScrollRight() error {\n }\n \n func (self *PatchExplorerController) HandlePrevPage() error {\n-\tself.context.GetState().SetLineSelectMode()\n \tself.context.GetState().AdjustSelectedLineIdx(-self.context.GetViewTrait().PageDelta())\n \n \treturn nil\n }\n \n func (self *PatchExplorerController) HandleNextPage() error {\n-\tself.context.GetState().SetLineSelectMode()\n \tself.context.GetState().AdjustSelectedLineIdx(self.context.GetViewTrait().PageDelta())\n \n \treturn nil\ndiff --git a/pkg/gui/patch_exploring/state.go b/pkg/gui/patch_exploring/state.go\nindex 03c933c7e28..40a28e7f7e8 100644\n--- a/pkg/gui/patch_exploring/state.go\n+++ b/pkg/gui/patch_exploring/state.go\n@@ -123,6 +123,12 @@ func (s *State) SetLineSelectMode() {\n \ts.selectMode = LINE\n }\n \n+func (s *State) DismissHunkSelectMode() {\n+\tif s.SelectingHunk() {\n+\t\ts.selectMode = LINE\n+\t}\n+}\n+\n // For when you move the cursor without holding shift (meaning if we're in\n // a non-sticky range select, we'll cancel it)\n func (s *State) SelectLine(newSelectedLineIdx int) {\n@@ -239,6 +245,7 @@ func (s *State) CurrentLineNumber() int {\n }\n \n func (s *State) AdjustSelectedLineIdx(change int) {\n+\ts.DismissHunkSelectMode()\n \ts.SelectLine(s.selectedLineIdx + change)\n }\n \n@@ -255,12 +262,12 @@ func (s *State) PlainRenderSelected() string {\n }\n \n func (s *State) SelectBottom() {\n-\ts.SetLineSelectMode()\n+\ts.DismissHunkSelectMode()\n \ts.SelectLine(s.patch.LineCount() - 1)\n }\n \n func (s *State) SelectTop() {\n-\ts.SetLineSelectMode()\n+\ts.DismissHunkSelectMode()\n \ts.SelectLine(0)\n }\n \n", "instance_id": "jesseduffield__lazygit-3837", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the user's frustration with the current method of selecting text in the \"Unstaged Changes\" box and their desire for a more efficient range selection mechanism, ideally with vim-like shortcuts. The goal of enabling faster and more intuitive text selection is evident, and the proposed solution (toggling range selection with 'v' and using other keys for navigation) provides a reasonable direction. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify how the range selection should behave in edge cases (e.g., selecting beyond the visible content, handling empty content, or interactions with other key bindings). Additionally, there are no explicit constraints or requirements regarding performance or compatibility with existing features. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is relatively small, affecting only a couple of files (`patch_explorer_controller.go` and `state.go`) and focusing on modifying existing logic for line selection and mode handling. The changes involve straightforward updates, such as removing calls to `SetLineSelectMode()` and introducing a new method `DismissHunkSelectMode()` to manage selection modes, which does not appear to impact the broader system architecture. Second, the technical concepts required are relatively basic, involving an understanding of state management and event handling within the GUI context of the application (likely a terminal-based Git tool like `lazygit`). No advanced algorithms, design patterns, or domain-specific knowledge beyond typical GUI interaction logic are needed. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest a focus on maintaining selection behavior consistency, and the complexity of potential edge cases (e.g., mode transitions or invalid selections) appears manageable with minimal additional error handling. Overall, this task requires understanding some code logic and making simple modifications, fitting within the lower end of the difficulty spectrum, though slightly above the \"Very Easy\" range due to the need to understand the state management logic.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deleting an update-ref entry in a rebase will cause the branch to be deleted\n**Describe the bug**\r\nScenario: you want to make a copy of a branch, and rebase it away from the original branch (e.g. the original branch is off of `main`, and you want to make a copy to see if it also compiles on `devel`).\r\n\r\nIf you have `rebase.updateRefs` set to true in your git config (which you will if you often work with stacked branches), then rebasing the copy onto `devel` will also rebase the original branch there; not what you want. I consider this a bug in git, and posted [lengthy messages](https://public-inbox.org/git/354f9fed-567f-42c8-9da9-148a5e223022@haller-berlin.de/) about this in the git mailing list, but folks either don't agree or don't care enough. (Read the whole thread if you are interested.)\r\n\r\nSo, to work around it, you make an interactive rebase onto `devel`, and delete the `update-ref` entry for the original branch. That should make the original branch stay where it was, right? Now here's the bug: it will instead _delete_ the original branch. Pretty bad.\r\n\r\nThe reason for this is the house-keeping that git does around update-ref commands. When starting a rebase, it remembers which refs are affected, and when changing the todo list using `git rebase --edit-todo` it updates that info based on whether update-ref commands where added or removed. The problem is that lazygit simply removes an update-ref command behind git's back, so this housekeeping doesn't happen.\r\n\r\nThe solution could be, instead of writing our changed git-rebase-todo file back manually, to call `git rebase --edit-todo` and set lazygit as an editor that provides the updated todo file via our daemon mechanism.\r\n\r\n**Version info:**\r\n0.41.0\n", "patch": "diff --git a/pkg/app/daemon/daemon.go b/pkg/app/daemon/daemon.go\nindex 70490aef024..2f1cded7c23 100644\n--- a/pkg/app/daemon/daemon.go\n+++ b/pkg/app/daemon/daemon.go\n@@ -39,6 +39,7 @@ const (\n \tDaemonKindInsertBreak\n \tDaemonKindChangeTodoActions\n \tDaemonKindMoveFixupCommitDown\n+\tDaemonKindWriteRebaseTodo\n )\n \n const (\n@@ -59,6 +60,7 @@ func getInstruction() Instruction {\n \t\tDaemonKindMoveTodosUp:         deserializeInstruction[*MoveTodosUpInstruction],\n \t\tDaemonKindMoveTodosDown:       deserializeInstruction[*MoveTodosDownInstruction],\n \t\tDaemonKindInsertBreak:         deserializeInstruction[*InsertBreakInstruction],\n+\t\tDaemonKindWriteRebaseTodo:     deserializeInstruction[*WriteRebaseTodoInstruction],\n \t}\n \n \treturn mapping[getDaemonKind()](jsonData)\n@@ -330,3 +332,27 @@ func (self *InsertBreakInstruction) run(common *common.Common) error {\n \t\treturn utils.PrependStrToTodoFile(path, []byte(\"break\\n\"))\n \t})\n }\n+\n+type WriteRebaseTodoInstruction struct {\n+\tTodosFileContent []byte\n+}\n+\n+func NewWriteRebaseTodoInstruction(todosFileContent []byte) Instruction {\n+\treturn &WriteRebaseTodoInstruction{\n+\t\tTodosFileContent: todosFileContent,\n+\t}\n+}\n+\n+func (self *WriteRebaseTodoInstruction) Kind() DaemonKind {\n+\treturn DaemonKindWriteRebaseTodo\n+}\n+\n+func (self *WriteRebaseTodoInstruction) SerializedInstructions() string {\n+\treturn serializeInstruction(self)\n+}\n+\n+func (self *WriteRebaseTodoInstruction) run(common *common.Common) error {\n+\treturn handleInteractiveRebase(common, func(path string) error {\n+\t\treturn os.WriteFile(path, self.TodosFileContent, 0o644)\n+\t})\n+}\ndiff --git a/pkg/commands/git_commands/rebase.go b/pkg/commands/git_commands/rebase.go\nindex eeae661678a..2d1de707f38 100644\n--- a/pkg/commands/git_commands/rebase.go\n+++ b/pkg/commands/git_commands/rebase.go\n@@ -204,7 +204,7 @@ type PrepareInteractiveRebaseCommandOpts struct {\n \n // PrepareInteractiveRebaseCommand returns the cmd for an interactive rebase\n // we tell git to run lazygit to edit the todo list, and we pass the client\n-// lazygit a todo string to write to the todo file\n+// lazygit instructions what to do with the todo file\n func (self *RebaseCommands) PrepareInteractiveRebaseCommand(opts PrepareInteractiveRebaseCommandOpts) oscommands.ICmdObj {\n \tex := oscommands.GetLazygitPath()\n \n@@ -250,6 +250,36 @@ func (self *RebaseCommands) PrepareInteractiveRebaseCommand(opts PrepareInteract\n \treturn cmdObj\n }\n \n+// GitRebaseEditTodo runs \"git rebase --edit-todo\", saving the given todosFileContent to the file\n+func (self *RebaseCommands) GitRebaseEditTodo(todosFileContent []byte) error {\n+\tex := oscommands.GetLazygitPath()\n+\n+\tcmdArgs := NewGitCmd(\"rebase\").\n+\t\tArg(\"--edit-todo\").\n+\t\tToArgv()\n+\n+\tdebug := \"FALSE\"\n+\tif self.Debug {\n+\t\tdebug = \"TRUE\"\n+\t}\n+\n+\tself.Log.WithField(\"command\", cmdArgs).Debug(\"RunCommand\")\n+\n+\tcmdObj := self.cmd.New(cmdArgs)\n+\n+\tcmdObj.AddEnvVars(daemon.ToEnvVars(daemon.NewWriteRebaseTodoInstruction(todosFileContent))...)\n+\n+\tcmdObj.AddEnvVars(\n+\t\t\"DEBUG=\"+debug,\n+\t\t\"LANG=en_US.UTF-8\",   // Force using EN as language\n+\t\t\"LC_ALL=en_US.UTF-8\", // Force using EN as language\n+\t\t\"GIT_EDITOR=\"+ex,\n+\t\t\"GIT_SEQUENCE_EDITOR=\"+ex,\n+\t)\n+\n+\treturn cmdObj.Run()\n+}\n+\n // AmendTo amends the given commit with whatever files are staged\n func (self *RebaseCommands) AmendTo(commits []*models.Commit, commitIndex int) error {\n \tcommit := commits[commitIndex]\n@@ -302,11 +332,16 @@ func (self *RebaseCommands) DeleteUpdateRefTodos(commits []*models.Commit) error\n \t\treturn todoFromCommit(commit)\n \t})\n \n-\treturn utils.DeleteTodos(\n+\ttodosFileContent, err := utils.DeleteTodos(\n \t\tfilepath.Join(self.repoPaths.WorktreeGitDirPath(), \"rebase-merge/git-rebase-todo\"),\n \t\ttodosToDelete,\n \t\tself.config.GetCoreCommentChar(),\n \t)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\treturn self.GitRebaseEditTodo(todosFileContent)\n }\n \n func (self *RebaseCommands) MoveTodosDown(commits []*models.Commit) error {\ndiff --git a/pkg/utils/rebase_todo.go b/pkg/utils/rebase_todo.go\nindex 3403eef9700..f2b60a09769 100644\n--- a/pkg/utils/rebase_todo.go\n+++ b/pkg/utils/rebase_todo.go\n@@ -1,6 +1,7 @@\n package utils\n \n import (\n+\t\"bytes\"\n \t\"fmt\"\n \t\"os\"\n \t\"strings\"\n@@ -96,6 +97,12 @@ func WriteRebaseTodoFile(fileName string, todos []todo.Todo, commentChar byte) e\n \treturn err\n }\n \n+func todosToString(todos []todo.Todo, commentChar byte) ([]byte, error) {\n+\tbuffer := bytes.Buffer{}\n+\terr := todo.Write(&buffer, todos, commentChar)\n+\treturn buffer.Bytes(), err\n+}\n+\n func PrependStrToTodoFile(filePath string, linesToPrepend []byte) error {\n \texistingContent, err := os.ReadFile(filePath)\n \tif err != nil {\n@@ -106,16 +113,21 @@ func PrependStrToTodoFile(filePath string, linesToPrepend []byte) error {\n \treturn os.WriteFile(filePath, linesToPrepend, 0o644)\n }\n \n-func DeleteTodos(fileName string, todosToDelete []Todo, commentChar byte) error {\n+// Unlike the other functions in this file, which write the changed todos file\n+// back to disk, this one returns the new content as a byte slice. This is\n+// because when deleting update-ref todos, we must perform a \"git rebase\n+// --edit-todo\" command to pass the changed todos to git so that it can do some\n+// housekeeping around the deleted todos. This can only be done by our caller.\n+func DeleteTodos(fileName string, todosToDelete []Todo, commentChar byte) ([]byte, error) {\n \ttodos, err := ReadRebaseTodoFile(fileName, commentChar)\n \tif err != nil {\n-\t\treturn err\n+\t\treturn nil, err\n \t}\n \trearrangedTodos, err := deleteTodos(todos, todosToDelete)\n \tif err != nil {\n-\t\treturn err\n+\t\treturn nil, err\n \t}\n-\treturn WriteRebaseTodoFile(fileName, rearrangedTodos, commentChar)\n+\treturn todosToString(rearrangedTodos, commentChar)\n }\n \n func deleteTodos(todos []todo.Todo, todosToDelete []Todo) ([]todo.Todo, error) {\n", "instance_id": "jesseduffield__lazygit-3439", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the bug and the desired behavior. It provides a detailed scenario about rebasing branches with `rebase.updateRefs` enabled and the unintended consequence of deleting a branch when an `update-ref` entry is removed during an interactive rebase. The explanation of the root cause (lazygit modifying the todo file directly without git's housekeeping) and the proposed solution (using `git rebase --edit-todo` with lazygit as the editor) are also clear. However, there are minor ambiguities: the problem statement does not explicitly define the expected input/output for the fix, nor does it mention specific edge cases or constraints that might arise during implementation (e.g., handling malformed todo files or conflicts during rebase). Additionally, while the context of the bug is well-described, it assumes familiarity with git internals and rebase mechanics, which might not be immediately accessible to all developers. Hence, I rate it as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category with a score of 0.65 due to several factors. First, the scope of code changes involves multiple files (`daemon.go`, `rebase.go`, and `rebase_todo.go`) and requires modifying the interaction between lazygit and git's rebase mechanism, which indicates a moderate impact on the codebase. The changes are not architectural but do touch on critical functionality related to how rebase todo files are handled. Second, the technical concepts involved include understanding git's interactive rebase internals (specifically `update-ref` housekeeping), environment variable manipulation for editor substitution, and lazygit's daemon mechanism for instruction handling. These concepts are moderately complex and require familiarity with both git's behavior and lazygit's architecture. Third, while the problem statement does not explicitly mention edge cases, the nature of the fix (modifying rebase todo files and invoking `git rebase --edit-todo`) implies potential issues like handling file write errors, malformed todo content, or unexpected git behavior during rebase, which adds to the complexity of error handling. Finally, the solution requires careful integration to ensure that git's housekeeping is correctly triggered, which demands precision and a deep understanding of the rebase workflow. While not at the extreme end of difficulty (e.g., no system-level or distributed system challenges), this problem is challenging enough to warrant a score of 0.65, reflecting the need for significant expertise in git and lazygit internals, as well as careful handling of the rebase process.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Annotations for incompatible native histogram operations with binary operators have confusing message\n### What did you do?\n\nI performed the following queries:\r\n\r\n`series_with_exponential_native_histograms + series_with_custom_buckets_native_histograms`\r\n\r\n`series_with_custom_buckets_native_histograms_layout_1 + series_with_custom_buckets_native_histograms_layout_2`\r\n\r\nAs the names suggest, these series contained native histograms with exponential and custom buckets.\n\n### What did you expect to see?\n\nWarnings that either include the appropriate metric names, or don't mention metric names at all.\r\n\r\nFor example, the first case could return a warning like `PromQL warning: vector contains a mix of histograms with exponential and custom buckets schemas for binary operation (1:1)`.\n\n### What did you see instead? Under which circumstances?\n\nThe first query result included the warning `PromQL warning: vector contains a mix of histograms with exponential and custom buckets schemas for metric name \"\" (1:1)`.\r\n\r\nThe second query result included the warning `PromQL warning: vector contains histograms with incompatible custom buckets for metric name \"\" (1:1)`.\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_\n", "patch": "diff --git a/promql/engine.go b/promql/engine.go\nindex cf669282011..44985e50f90 100644\n--- a/promql/engine.go\n+++ b/promql/engine.go\n@@ -3475,15 +3475,14 @@ func handleVectorBinopError(err error, e *parser.BinaryExpr) annotations.Annotat\n \tif err == nil {\n \t\treturn nil\n \t}\n-\tmetricName := \"\"\n+\top := parser.ItemTypeStr[e.Op]\n \tpos := e.PositionRange()\n \tif errors.Is(err, annotations.PromQLInfo) || errors.Is(err, annotations.PromQLWarning) {\n \t\treturn annotations.New().Add(err)\n \t}\n-\tif errors.Is(err, histogram.ErrHistogramsIncompatibleSchema) {\n-\t\treturn annotations.New().Add(annotations.NewMixedExponentialCustomHistogramsWarning(metricName, pos))\n-\t} else if errors.Is(err, histogram.ErrHistogramsIncompatibleBounds) {\n-\t\treturn annotations.New().Add(annotations.NewIncompatibleCustomBucketsHistogramsWarning(metricName, pos))\n+\t// TODO(NeerajGartia21): Test the exact annotation output once the testing framework can do so.\n+\tif errors.Is(err, histogram.ErrHistogramsIncompatibleSchema) || errors.Is(err, histogram.ErrHistogramsIncompatibleBounds) {\n+\t\treturn annotations.New().Add(annotations.NewIncompatibleBucketLayoutInBinOpWarning(op, pos))\n \t}\n \treturn nil\n }\ndiff --git a/util/annotations/annotations.go b/util/annotations/annotations.go\nindex 5b2fde152bd..95783957a7e 100644\n--- a/util/annotations/annotations.go\n+++ b/util/annotations/annotations.go\n@@ -143,6 +143,7 @@ var (\n \tNativeHistogramNotGaugeWarning             = fmt.Errorf(\"%w: this native histogram metric is not a gauge:\", PromQLWarning)\n \tMixedExponentialCustomHistogramsWarning    = fmt.Errorf(\"%w: vector contains a mix of histograms with exponential and custom buckets schemas for metric name\", PromQLWarning)\n \tIncompatibleCustomBucketsHistogramsWarning = fmt.Errorf(\"%w: vector contains histograms with incompatible custom buckets for metric name\", PromQLWarning)\n+\tIncompatibleBucketLayoutInBinOpWarning     = fmt.Errorf(\"%w: incompatible bucket layout encountered for binary operator\", PromQLWarning)\n \n \tPossibleNonCounterInfo                  = fmt.Errorf(\"%w: metric might not be a counter, name does not end in _total/_sum/_count/_bucket:\", PromQLInfo)\n \tHistogramQuantileForcedMonotonicityInfo = fmt.Errorf(\"%w: input to histogram_quantile needed to be fixed for monotonicity (see https://prometheus.io/docs/prometheus/latest/querying/functions/#histogram_quantile) for metric name\", PromQLInfo)\n@@ -295,9 +296,20 @@ func NewHistogramIgnoredInAggregationInfo(aggregation string, pos posrange.Posit\n \t}\n }\n \n+// NewHistogramIgnoredInMixedRangeInfo is used when a histogram is ignored\n+// in a range vector which contains mix of floats and histograms.\n func NewHistogramIgnoredInMixedRangeInfo(metricName string, pos posrange.PositionRange) error {\n \treturn annoErr{\n \t\tPositionRange: pos,\n \t\tErr:           fmt.Errorf(\"%w %q\", HistogramIgnoredInMixedRangeInfo, metricName),\n \t}\n }\n+\n+// NewIncompatibleBucketLayoutInBinOpWarning is used if binary operators act on a\n+// combination of two incompatible histograms.\n+func NewIncompatibleBucketLayoutInBinOpWarning(operator string, pos posrange.PositionRange) error {\n+\treturn annoErr{\n+\t\tPositionRange: pos,\n+\t\tErr:           fmt.Errorf(\"%w %s\", IncompatibleBucketLayoutInBinOpWarning, operator),\n+\t}\n+}\n", "instance_id": "prometheus__prometheus-15895", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the warning messages for incompatible native histogram operations with binary operators are confusing because they include an empty metric name. The expected behavior (warnings without metric names or with appropriate metric names) and the actual behavior (warnings with an empty metric name) are explicitly stated, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the metric names are always empty or only in certain cases, nor does it provide examples of what the \"appropriate metric names\" should look like. Additionally, there are no details about edge cases or specific constraints (e.g., whether this issue occurs only with certain types of histograms or operators). While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The code changes are localized to two files (`promql/engine.go` and `util/annotations/annotations.go`) and involve straightforward modifications to error messaging logic. The scope of the change is small, focusing on updating warning messages to remove references to metric names and include the binary operator type, which does not impact the broader system architecture or require deep understanding of the codebase. The technical concepts involved are basic: error handling, string formatting, and working with existing annotation structures in Go. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Prometheus query language (PromQL) concepts are required. Edge cases and error handling are not significantly complex, as the change simply generalizes the warning message to avoid mentioning metric names. Overall, this task requires minimal effort and understanding, primarily involving simple updates to existing code with clear intent.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for `quietDeps` for Sass compiler\nRight now the `quietDeps`/`--quiet-deps` option is not available in the [sass](https://gohugo.io/functions/css/sass/) function. It would allow us to not have to silence warnings for all the codebase.\n\nSee references:\nhttps://sass-lang.com/documentation/cli/dart-sass/#quiet-deps\nhttps://sass-lang.com/documentation/js-api/interfaces/stringoptions/#quietDeps\n\nUse case:\nI'm customizing Bootstrap and getting a lot of deprecation warnings after I started to import it.\n", "patch": "diff --git a/go.mod b/go.mod\nindex 2a76b3bea25..f00429b7884 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -10,7 +10,7 @@ require (\n \tgithub.com/bep/debounce v1.2.0\n \tgithub.com/bep/gitmap v1.6.0\n \tgithub.com/bep/goat v0.5.0\n-\tgithub.com/bep/godartsass/v2 v2.4.1\n+\tgithub.com/bep/godartsass/v2 v2.5.0\n \tgithub.com/bep/golibsass v1.2.0\n \tgithub.com/bep/goportabletext v0.1.0\n \tgithub.com/bep/gowebp v0.3.0\ndiff --git a/go.sum b/go.sum\nindex fc3eae36b49..b5e5fd3be4e 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -129,8 +129,8 @@ github.com/bep/gitmap v1.6.0 h1:sDuQMm9HoTL0LtlrfxjbjgAg2wHQd4nkMup2FInYzhA=\n github.com/bep/gitmap v1.6.0/go.mod h1:n+3W1f/rot2hynsqEGxGMErPRgT41n9CkGuzPvz9cIw=\n github.com/bep/goat v0.5.0 h1:S8jLXHCVy/EHIoCY+btKkmcxcXFd34a0Q63/0D4TKeA=\n github.com/bep/goat v0.5.0/go.mod h1:Md9x7gRxiWKs85yHlVTvHQw9rg86Bm+Y4SuYE8CTH7c=\n-github.com/bep/godartsass/v2 v2.4.1 h1:ktbimHvS+FUZ2FQsSEqm5DDfKnr56DVf7GNWuIbA1M8=\n-github.com/bep/godartsass/v2 v2.4.1/go.mod h1:rjsi1YSXAl/UbsGL85RLDEjRKdIKUlMQHr6ChUNYOFU=\n+github.com/bep/godartsass/v2 v2.5.0 h1:tKRvwVdyjCIr48qgtLa4gHEdtRkPF8H1OeEhJAEv7xg=\n+github.com/bep/godartsass/v2 v2.5.0/go.mod h1:rjsi1YSXAl/UbsGL85RLDEjRKdIKUlMQHr6ChUNYOFU=\n github.com/bep/golibsass v1.2.0 h1:nyZUkKP/0psr8nT6GR2cnmt99xS93Ji82ZD9AgOK6VI=\n github.com/bep/golibsass v1.2.0/go.mod h1:DL87K8Un/+pWUS75ggYv41bliGiolxzDKWJAq3eJ1MA=\n github.com/bep/goportabletext v0.1.0 h1:8dqym2So1cEqVZiBa4ZnMM1R9l/DnC1h4ONg4J5kujw=\ndiff --git a/resources/resource_transformers/tocss/dartsass/client.go b/resources/resource_transformers/tocss/dartsass/client.go\nindex 4ab958c01c2..965232ad4de 100644\n--- a/resources/resource_transformers/tocss/dartsass/client.go\n+++ b/resources/resource_transformers/tocss/dartsass/client.go\n@@ -161,6 +161,11 @@ type Options struct {\n \t// The IDs can be found in the Dart Sass log output, e.g. \"import\" in\n \t//    WARN  Dart Sass: DEPRECATED [import].\n \tSilenceDeprecations []string\n+\n+\t// Whether to silence deprecation warnings from dependencies, where a\n+\t// dependency is considered any file transitively imported through a load\n+\t// path. This does not apply to @warn or @debug rules.\n+\tSilenceDependencyDeprecations bool\n }\n \n func decodeOptions(m map[string]any) (opts Options, err error) {\ndiff --git a/resources/resource_transformers/tocss/dartsass/transform.go b/resources/resource_transformers/tocss/dartsass/transform.go\nindex 77bacc11551..e1e9b0be0c6 100644\n--- a/resources/resource_transformers/tocss/dartsass/transform.go\n+++ b/resources/resource_transformers/tocss/dartsass/transform.go\n@@ -86,10 +86,11 @@ func (t *transform) Transform(ctx *resources.ResourceTransformationCtx) error {\n \n \t\t\tvarsStylesheet: godartsass.Import{Content: sass.CreateVarsStyleSheet(sass.TranspilerDart, opts.Vars)},\n \t\t},\n-\t\tOutputStyle:             godartsass.ParseOutputStyle(opts.OutputStyle),\n-\t\tEnableSourceMap:         opts.EnableSourceMap,\n-\t\tSourceMapIncludeSources: opts.SourceMapIncludeSources,\n-\t\tSilenceDeprecations:     opts.SilenceDeprecations,\n+\t\tOutputStyle:                   godartsass.ParseOutputStyle(opts.OutputStyle),\n+\t\tEnableSourceMap:               opts.EnableSourceMap,\n+\t\tSourceMapIncludeSources:       opts.SourceMapIncludeSources,\n+\t\tSilenceDeprecations:           opts.SilenceDeprecations,\n+\t\tSilenceDependencyDeprecations: opts.SilenceDependencyDeprecations,\n \t}\n \n \t// Append any workDir relative include paths\n", "instance_id": "gohugoio__hugo-13540", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add support for the `quietDeps` or `--quiet-deps` option in the Sass compiler function within the Hugo framework. It provides a specific use case (customizing Bootstrap and encountering deprecation warnings) and references to the relevant Sass documentation, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the feature should be integrated into the existing API or configuration options in Hugo, nor does it mention any specific constraints or edge cases to consider (e.g., compatibility with other options or potential conflicts). Additionally, there are no examples of expected input/output behavior for the feature. While the intent is clear, these missing details prevent it from being comprehensively defined, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is relatively limited, primarily involving updates to a dependency (`godartsass/v2` from version 2.4.1 to 2.5.0) and modifications to two files in the `dartsass` module (`client.go` and `transform.go`). The changes are straightforward, adding a new boolean field `SilenceDependencyDeprecations` to the `Options` struct and passing it through to the underlying library call. This does not impact the broader system architecture or require deep refactoring. Second, the technical concepts involved are minimal\u2014basic Go struct manipulation and familiarity with dependency updates. No complex algorithms, design patterns, or domain-specific knowledge beyond basic Sass compilation are required. Third, there are no explicit edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the feature appears to be a simple passthrough to an existing library capability. The primary challenge lies in ensuring the updated dependency version supports the feature as expected and verifying that no unintended side effects occur, but this is a routine task for a developer familiar with Go and dependency management. Overall, the problem requires understanding some code logic and making simple modifications, justifying a difficulty score of 0.35.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deprecate twitter shortcode in favor of x shortcode\nThe need for this became obvious (at least to me) while working to elevate our embedded shortcode documentation to its own section. They re-branded a while ago, and while the public's transition has been slow, the new brand has been stable for quite a while.\r\n\r\n- Twitter => \"X\"\r\n- tweet => \"post\" or \"X post\"\r\n\r\nThis change touches a few things, including:\r\n\r\n- Internal shortcode aliasing (maybe)\r\n- Replacement and deprecation of `privacy` config\r\n- Replacement and deprecation of `services` config\r\n- Renaming (or making copies of) the internal templates (`twitter` and `twitter_simple`)\r\n- Updating tests\r\n\r\nWe'll leave the twitter_cards template alone because, at least for now, the term \"Twitter Cards\" seems to still be the recognized phrase.\r\n\r\nKeep in mind that site and theme authors may have created their own `tweet`, `twitter`, or `twitter_simple` shortcodes, so calling shortcodes by these names should not trigger a warning. We need to handle this on the receiving end (i.e., after we've determined that we're using an embedded shortcode). \r\n\r\nI've got a working branch, but will refrain from submitting unless this proposal is approved.\r\n\n", "patch": "diff --git a/config/allconfig/allconfig.go b/config/allconfig/allconfig.go\nindex ceb63048026..96d10b3bdd7 100644\n--- a/config/allconfig/allconfig.go\n+++ b/config/allconfig/allconfig.go\n@@ -402,6 +402,28 @@ func (c *Config) CompileConfig(logger loggers.Logger) error {\n \t\tc.Pagination.Path = c.PaginatePath\n \t}\n \n+\t// Legacy privacy values.\n+\tif c.Privacy.Twitter.Disable {\n+\t\thugo.Deprecate(\"site config key privacy.twitter.disable\", \"Use privacy.x.disable instead.\", \"v0.141.0\")\n+\t\tc.Privacy.X.Disable = c.Privacy.Twitter.Disable\n+\t}\n+\n+\tif c.Privacy.Twitter.EnableDNT {\n+\t\thugo.Deprecate(\"site config key privacy.twitter.enableDNT\", \"Use privacy.x.enableDNT instead.\", \"v0.141.0\")\n+\t\tc.Privacy.X.EnableDNT = c.Privacy.Twitter.EnableDNT\n+\t}\n+\n+\tif c.Privacy.Twitter.Simple {\n+\t\thugo.Deprecate(\"site config key privacy.twitter.simple\", \"Use privacy.x.simple instead.\", \"v0.141.0\")\n+\t\tc.Privacy.X.Simple = c.Privacy.Twitter.Simple\n+\t}\n+\n+\t// Legacy services values.\n+\tif c.Services.Twitter.DisableInlineCSS {\n+\t\thugo.Deprecate(\"site config key services.twitter.disableInlineCSS\", \"Use services.x.disableInlineCSS instead.\", \"v0.141.0\")\n+\t\tc.Services.X.DisableInlineCSS = c.Services.Twitter.DisableInlineCSS\n+\t}\n+\n \tc.C = &ConfigCompiled{\n \t\tTimeout:             timeout,\n \t\tBaseURL:             baseURL,\ndiff --git a/config/privacy/privacyConfig.go b/config/privacy/privacyConfig.go\nindex 8880b1036d6..900f73540b6 100644\n--- a/config/privacy/privacyConfig.go\n+++ b/config/privacy/privacyConfig.go\n@@ -30,9 +30,10 @@ type Config struct {\n \tDisqus          Disqus\n \tGoogleAnalytics GoogleAnalytics\n \tInstagram       Instagram\n-\tTwitter         Twitter\n+\tTwitter         Twitter // deprecated in favor of X in v0.141.0\n \tVimeo           Vimeo\n \tYouTube         YouTube\n+\tX               X\n }\n \n // Disqus holds the privacy configuration settings related to the Disqus template.\n@@ -58,7 +59,8 @@ type Instagram struct {\n \tSimple bool\n }\n \n-// Twitter holds the privacy configuration settingsrelated to the Twitter shortcode.\n+// Twitter holds the privacy configuration settings related to the Twitter shortcode.\n+// Deprecated in favor of X in v0.141.0.\n type Twitter struct {\n \tService `mapstructure:\",squash\"`\n \n@@ -70,7 +72,7 @@ type Twitter struct {\n \tSimple bool\n }\n \n-// Vimeo holds the privacy configuration settingsrelated to the Vimeo shortcode.\n+// Vimeo holds the privacy configuration settings related to the Vimeo shortcode.\n type Vimeo struct {\n \tService `mapstructure:\",squash\"`\n \n@@ -84,7 +86,7 @@ type Vimeo struct {\n \tSimple bool\n }\n \n-// YouTube holds the privacy configuration settingsrelated to the YouTube shortcode.\n+// YouTube holds the privacy configuration settings related to the YouTube shortcode.\n type YouTube struct {\n \tService `mapstructure:\",squash\"`\n \n@@ -94,6 +96,20 @@ type YouTube struct {\n \tPrivacyEnhanced bool\n }\n \n+// X holds the privacy configuration settings related to the X shortcode.\n+type X struct {\n+\tService `mapstructure:\",squash\"`\n+\n+\t// When set to true, the X post and its embedded page on your site are not\n+\t// used for purposes that include personalized suggestions and personalized\n+\t// ads.\n+\tEnableDNT bool\n+\n+\t// If simple mode is enabled, a static and no-JS version of the X post will\n+\t// be built.\n+\tSimple bool\n+}\n+\n // DecodeConfig creates a privacy Config from a given Hugo configuration.\n func DecodeConfig(cfg config.Provider) (pc Config, err error) {\n \tif !cfg.IsSet(privacyConfigKey) {\ndiff --git a/config/services/servicesConfig.go b/config/services/servicesConfig.go\nindex 1b4317e92f5..f302244d4aa 100644\n--- a/config/services/servicesConfig.go\n+++ b/config/services/servicesConfig.go\n@@ -31,7 +31,8 @@ type Config struct {\n \tDisqus          Disqus\n \tGoogleAnalytics GoogleAnalytics\n \tInstagram       Instagram\n-\tTwitter         Twitter\n+\tTwitter         Twitter // deprecated in favor of X in v0.141.0\n+\tX               X\n \tRSS             RSS\n }\n \n@@ -61,6 +62,7 @@ type Instagram struct {\n }\n \n // Twitter holds the functional configuration settings related to the Twitter shortcodes.\n+// Deprecated in favor of X in v0.141.0.\n type Twitter struct {\n \t// The Simple variant of Twitter is decorated with a basic set of inline styles.\n \t// This means that if you want to provide your own CSS, you want\n@@ -68,6 +70,14 @@ type Twitter struct {\n \tDisableInlineCSS bool\n }\n \n+// X holds the functional configuration settings related to the X shortcodes.\n+type X struct {\n+\t// The Simple variant of X is decorated with a basic set of inline styles.\n+\t// This means that if you want to provide your own CSS, you want\n+\t// to disable the inline CSS provided by Hugo.\n+\tDisableInlineCSS bool\n+}\n+\n // RSS holds the functional configuration settings related to the RSS feeds.\n type RSS struct {\n \t// Limit the number of pages.\ndiff --git a/tpl/tplimpl/embedded/templates/shortcodes/twitter.html b/tpl/tplimpl/embedded/templates/shortcodes/twitter.html\nindex b88cf7ce0ea..7a4adea5d34 100644\n--- a/tpl/tplimpl/embedded/templates/shortcodes/twitter.html\n+++ b/tpl/tplimpl/embedded/templates/shortcodes/twitter.html\n@@ -1,3 +1,4 @@\n+{{- warnf \"The \\\"twitter\\\", \\\"tweet\\\", and \\\"twitter_simple\\\" shortcodes were deprecated in v0.142.0 and will be removed in a future release. Please use the \\\"x\\\" shortcode instead.\" }}\n {{- $pc := .Page.Site.Config.Privacy.Twitter -}}\n {{- if not $pc.Disable -}}\n   {{- if $pc.Simple -}}\ndiff --git a/tpl/tplimpl/embedded/templates/shortcodes/twitter_simple.html b/tpl/tplimpl/embedded/templates/shortcodes/twitter_simple.html\nindex 0fc8613b938..7251f64e31f 100644\n--- a/tpl/tplimpl/embedded/templates/shortcodes/twitter_simple.html\n+++ b/tpl/tplimpl/embedded/templates/shortcodes/twitter_simple.html\n@@ -1,34 +1,37 @@\n-{{- $pc := .Page.Site.Config.Privacy.Twitter -}}\n-{{- $sc := .Page.Site.Config.Services.Twitter -}}\n-{{- if not $pc.Disable -}}\n+{{- warnf \"The \\\"twitter\\\", \\\"tweet\\\", and \\\"twitter_simple\\\" shortcodes were deprecated in v0.142.0 and will be removed in a future release. Please use the \\\"x\\\" shortcode instead.\" }}\n+{{- if not site.Config.Privacy.Twitter.Disable -}}\n   {{- $id := or (.Get \"id\") \"\" -}}\n   {{- $user := or (.Get \"user\") \"\" -}}\n   {{- if and $id $user -}}\n-    {{- template \"render-simple-tweet\" (dict \"id\" $id \"user\" $user \"dnt\" $pc.EnableDNT \"name\" .Name \"position\" .Position) -}}\n+    {{- template \"render-simple-tweet\" (dict \"id\" $id \"user\" $user \"ctx\" .) -}}\n   {{- else -}}\n     {{- errorf \"The %q shortcode requires two named parameters: user and id. See %s\" .Name .Position -}}\n   {{- end -}}\n {{- end -}}\n \n {{- define \"render-simple-tweet\" -}}\n+  {{- $dnt := site.Config.Privacy.Twitter.EnableDNT }}\n   {{- $url := printf \"https://twitter.com/%v/status/%v\" .user .id -}}\n-  {{- $query := querify \"url\" $url \"dnt\" .dnt \"omit_script\" true -}}\n+  {{- $query := querify \"url\" $url \"dnt\" $dnt \"omit_script\" true -}}\n   {{- $request := printf \"https://publish.twitter.com/oembed?%s\" $query -}}\n   {{- with try (resources.GetRemote $request) -}}\n     {{- with .Err -}}\n       {{- errorf \"%s\" . -}}\n     {{- else with .Value -}}\n+      {{- if not site.Config.Services.Twitter.DisableInlineCSS }}\n+        {{- template \"__h_simple_twitter_css\" (dict \"ctx\" $.ctx) }}\n+      {{- end }}\n       {{- (. | transform.Unmarshal).html | safeHTML -}}\n     {{- else -}}\n-      {{- warnidf \"shortcode-twitter-simple-getremote\" \"The %q shortcode was unable to retrieve the remote data. See %s\" .name .position -}}\n+      {{- warnidf \"shortcode-twitter-simple-getremote\" \"The %q shortcode was unable to retrieve the remote data. See %s\" .ctx.Name .ctx.Position -}}\n     {{- end -}}\n   {{- end -}}\n {{- end -}}\n \n {{- define \"__h_simple_twitter_css\" -}}\n-  {{- if not (.Page.Scratch.Get \"__h_simple_twitter_css\") -}}\n+  {{- if not (.ctx.Page.Store.Get \"__h_simple_twitter_css\") -}}\n     {{/* Only include once */}}\n-    {{- .Page.Scratch.Set \"__h_simple_twitter_css\" true -}}\n+    {{- .ctx.Page.Store.Set \"__h_simple_twitter_css\" true -}}\n     <style type=\"text/css\">\n       .twitter-tweet {\n         font:\ndiff --git a/tpl/tplimpl/embedded/templates/shortcodes/x.html b/tpl/tplimpl/embedded/templates/shortcodes/x.html\nnew file mode 100644\nindex 00000000000..78c0ac08bca\n--- /dev/null\n+++ b/tpl/tplimpl/embedded/templates/shortcodes/x.html\n@@ -0,0 +1,29 @@\n+{{- $pc := .Page.Site.Config.Privacy.X -}}\n+{{- if not $pc.Disable -}}\n+  {{- if $pc.Simple -}}\n+    {{- template \"_internal/shortcodes/x_simple.html\" . -}}\n+  {{- else -}}\n+    {{- $id := or (.Get \"id\") \"\" -}}\n+    {{- $user := or (.Get \"user\") \"\" -}}\n+    {{- if and $id $user -}}\n+      {{- template \"render-x\" (dict \"id\" $id \"user\" $user \"dnt\" $pc.EnableDNT \"name\" .Name \"position\" .Position) -}}\n+    {{- else -}}\n+      {{- errorf \"The %q shortcode requires two named parameters: user and id. See %s\" .Name .Position -}}\n+    {{- end -}}\n+  {{- end -}}\n+{{- end -}}\n+\n+{{- define \"render-x\" -}}\n+  {{- $url := printf \"https://x.com/%v/status/%v\" .user .id -}}\n+  {{- $query := querify \"url\" $url \"dnt\" .dnt -}}\n+  {{- $request := printf \"https://publish.x.com/oembed?%s\" $query -}}\n+  {{- with try (resources.GetRemote $request) -}}\n+    {{- with .Err -}}\n+      {{- errorf \"%s\" . -}}\n+    {{- else with .Value -}}\n+      {{- (. | transform.Unmarshal).html | safeHTML -}}\n+    {{- else -}}\n+    {{- warnidf \"shortcode-x-getremote\" \"The %q shortcode was unable to retrieve the remote data. See %s\" .name .position -}}\n+    {{- end -}}\n+  {{- end -}}\n+{{- end -}}\ndiff --git a/tpl/tplimpl/embedded/templates/shortcodes/x_simple.html b/tpl/tplimpl/embedded/templates/shortcodes/x_simple.html\nnew file mode 100644\nindex 00000000000..661ed77560b\n--- /dev/null\n+++ b/tpl/tplimpl/embedded/templates/shortcodes/x_simple.html\n@@ -0,0 +1,60 @@\n+{{- if not site.Config.Privacy.X.Disable -}}\n+  {{- $id := or (.Get \"id\") \"\" -}}\n+  {{- $user := or (.Get \"user\") \"\" -}}\n+  {{- if and $id $user -}}\n+    {{- template \"render-simple-x\" (dict \"id\" $id \"user\" $user \"ctx\" .) -}}\n+  {{- else -}}\n+    {{- errorf \"The %q shortcode requires two named parameters: user and id. See %s\" .Name .Position -}}\n+  {{- end -}}\n+{{- end -}}\n+\n+{{- define \"render-simple-x\" -}}\n+  {{- $dnt := site.Config.Privacy.X.EnableDNT }}\n+  {{- $url := printf \"https://x.com/%v/status/%v\" .user .id -}}\n+  {{- $query := querify \"url\" $url \"dnt\" $dnt \"omit_script\" true -}}\n+  {{- $request := printf \"https://publish.x.com/oembed?%s\" $query -}}\n+  {{- with try (resources.GetRemote $request) -}}\n+    {{- with .Err -}}\n+      {{- errorf \"%s\" . -}}\n+    {{- else with .Value -}}\n+      {{- if not site.Config.Services.X.DisableInlineCSS }}\n+        {{- template \"__h_simple_x_css\" (dict \"ctx\" $.ctx) }}\n+      {{- end }}\n+      {{- (. | transform.Unmarshal).html | safeHTML -}}\n+    {{- else -}}\n+      {{- warnidf \"shortcode-x-simple-getremote\" \"The %q shortcode was unable to retrieve the remote data. See %s\" .ctx.Name .ctx.Position -}}\n+    {{- end -}}\n+  {{- end -}}\n+{{- end -}}\n+\n+{{- define \"__h_simple_x_css\" -}}\n+  {{- if not (.ctx.Page.Store.Get \"__h_simple_x_css\") -}}\n+    {{/* Only include once */}}\n+    {{- .ctx.Page.Store.Set \"__h_simple_x_css\" true -}}\n+    <style type=\"text/css\">\n+      .twitter-tweet {\n+        font:\n+          14px/1.45 -apple-system,\n+          BlinkMacSystemFont,\n+          \"Segoe UI\",\n+          Roboto,\n+          Oxygen-Sans,\n+          Ubuntu,\n+          Cantarell,\n+          \"Helvetica Neue\",\n+          sans-serif;\n+        border-left: 4px solid #2b7bb9;\n+        padding-left: 1.5em;\n+        color: #555;\n+      }\n+      .twitter-tweet a {\n+        color: #2b7bb9;\n+        text-decoration: none;\n+      }\n+      blockquote.twitter-tweet a:hover,\n+      blockquote.twitter-tweet a:focus {\n+        text-decoration: underline;\n+      }\n+    </style>\n+  {{- end -}}\n+{{- end -}}\ndiff --git a/tpl/tplimpl/template.go b/tpl/tplimpl/template.go\nindex 0a593593b88..0ea7117a3ac 100644\n--- a/tpl/tplimpl/template.go\n+++ b/tpl/tplimpl/template.go\n@@ -66,6 +66,8 @@ const (\n // We need this to identify position in templates with base templates applied.\n var identifiersRe = regexp.MustCompile(`at \\<(.*?)(\\.{3})?\\>:`)\n \n+// The tweet and twitter shortcodes were deprecated in favor of the x shortcode\n+// in v0.141.0. We can remove these aliases in v0.155.0 or later.\n var embeddedTemplatesAliases = map[string][]string{\n \t\"shortcodes/twitter.html\": {\"shortcodes/tweet.html\"},\n }\n", "instance_id": "gohugoio__hugo-13266", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in its intent to deprecate the \"Twitter\" shortcode in favor of an \"X\" shortcode, reflecting a rebranding. It outlines the key areas of impact, such as internal shortcode aliasing, configuration settings, templates, and tests. The goal of maintaining backward compatibility for custom shortcodes by site/theme authors is also mentioned, which is a critical detail. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the statement does not explicitly define how deprecation warnings should behave over time (e.g., will they be removed in a future version?), nor does it provide specific examples of input/output for the shortcodes. Additionally, the scope of \"updating tests\" is vague, as it does not specify which tests or how extensive the changes might be. While the overall direction is clear, these minor gaps in detail could lead to some interpretation during implementation.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files and modules, including configuration handling (privacy and services configs), template files for shortcodes, and deprecation logic, as seen in the provided diff. This requires understanding the interactions between configuration parsing, template rendering, and shortcode processing in what appears to be a Hugo-based codebase (a static site generator). The changes involve adding new structures for \"X\" while maintaining backward compatibility with \"Twitter,\" which introduces complexity in aliasing and deprecation handling. \n\nSecond, the technical concepts involved include familiarity with Go (the language used in the codebase), struct definitions, template rendering in Hugo, and HTTP requests for fetching remote data (e.g., oEmbed endpoints for X posts). Additionally, the developer must handle deprecation warnings using a provided utility (`hugo.Deprecate`) and ensure that custom shortcodes by users do not trigger unnecessary warnings, which requires careful logic design.\n\nThird, edge cases and error handling are moderately complex. The code changes show error handling for remote data fetching and parameter validation in shortcodes, but the problem statement does not explicitly call out additional edge cases (e.g., handling invalid or legacy configurations). However, maintaining backward compatibility and ensuring a smooth transition for users implies potential edge cases around configuration migration and custom shortcode conflicts.\n\nFinally, while the changes do not appear to impact the core architecture of the system significantly, they do require a solid understanding of multiple parts of the codebase and careful implementation to avoid breaking existing functionality. The amount of code change is moderate, with new files for \"X\" shortcodes and modifications to existing configuration and template files. Overall, this task is neither trivial nor extremely challenging, fitting into the medium difficulty range of 0.50, as it requires a blend of careful refactoring, cross-module changes, and attention to compatibility.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`union`, `complement`, `symdiff`, and `intersect` not working with resource collections\n### Description\r\n\r\nAs per the title, the set methods provided for collections do not seem to work with global resources. For example:\r\n```html\r\n{{- $styles := resources.Match \"css/**.css\" }}\r\n{{- $katex := resources.Match \"css/**katex*\" }}\r\n{{- $complement := complement $katex $styles }}\r\n```\r\nHere, `$complement` contains all resources from `$styles`, not excluding those also found in `$katex`. The functions `union`, `symdiff`, and `intersect` malfunction in similar ways. `intersect` seems to return an empty collection, irrespective of the resource collections provided.\r\n\r\n### What version of Hugo are you using (`hugo version`)?\r\n\r\n<pre>\r\n$ hugo version\r\nhugo v0.140.0+extended linux/amd64 BuildDate=unknown\r\n</pre>\n", "patch": "diff --git a/resources/resource.go b/resources/resource.go\nindex 4b81a478a42..7ab10b0ae34 100644\n--- a/resources/resource.go\n+++ b/resources/resource.go\n@@ -47,6 +47,7 @@ var (\n \t_ resource.Cloner                    = (*genericResource)(nil)\n \t_ resource.ResourcesLanguageMerger   = (*resource.Resources)(nil)\n \t_ resource.Identifier                = (*genericResource)(nil)\n+\t_ resource.TransientIdentifier       = (*genericResource)(nil)\n \t_ targetPathProvider                 = (*genericResource)(nil)\n \t_ sourcePathProvider                 = (*genericResource)(nil)\n \t_ identity.IdentityGroupProvider     = (*genericResource)(nil)\n@@ -359,6 +360,9 @@ func GetTestInfoForResource(r resource.Resource) GenericResourceTestInfo {\n type genericResource struct {\n \tpublishInit *sync.Once\n \n+\tkey     string\n+\tkeyInit *sync.Once\n+\n \tsd    ResourceSourceDescriptor\n \tpaths internal.ResourcePaths\n \n@@ -444,19 +448,24 @@ func (l *genericResource) Data() any {\n }\n \n func (l *genericResource) Key() string {\n-\tbasePath := l.spec.Cfg.BaseURL().BasePathNoTrailingSlash\n-\tvar key string\n-\tif basePath == \"\" {\n-\t\tkey = l.RelPermalink()\n-\t} else {\n-\t\tkey = strings.TrimPrefix(l.RelPermalink(), basePath)\n-\t}\n+\tl.keyInit.Do(func() {\n+\t\tbasePath := l.spec.Cfg.BaseURL().BasePathNoTrailingSlash\n+\t\tif basePath == \"\" {\n+\t\t\tl.key = l.RelPermalink()\n+\t\t} else {\n+\t\t\tl.key = strings.TrimPrefix(l.RelPermalink(), basePath)\n+\t\t}\n \n-\tif l.spec.Cfg.IsMultihost() {\n-\t\tkey = l.spec.Lang() + key\n-\t}\n+\t\tif l.spec.Cfg.IsMultihost() {\n+\t\t\tl.key = l.spec.Lang() + l.key\n+\t\t}\n+\t})\n+\n+\treturn l.key\n+}\n \n-\treturn key\n+func (l *genericResource) TransientKey() string {\n+\treturn l.Key()\n }\n \n func (l *genericResource) targetPath() string {\n@@ -623,6 +632,7 @@ func (rc *genericResource) cloneWithUpdates(u *transformationUpdate) (baseResour\n \n func (l genericResource) clone() *genericResource {\n \tl.publishInit = &sync.Once{}\n+\tl.keyInit = &sync.Once{}\n \treturn &l\n }\n \ndiff --git a/resources/resource/resourcetypes.go b/resources/resource/resourcetypes.go\nindex 0fb87f37137..b33750e8033 100644\n--- a/resources/resource/resourcetypes.go\n+++ b/resources/resource/resourcetypes.go\n@@ -170,11 +170,19 @@ type ResourcesLanguageMerger interface {\n \n // Identifier identifies a resource.\n type Identifier interface {\n-\t// Key is is mostly for internal use and should be considered opaque.\n+\t// Key is mostly for internal use and should be considered opaque.\n \t// This value may change between Hugo versions.\n \tKey() string\n }\n \n+// TransientIdentifier identifies a transient resource.\n+type TransientIdentifier interface {\n+\t// TransientKey is mostly for internal use and should be considered opaque.\n+\t// This value is implemented by transient resources where pointers may be short lived and\n+\t// not suitable for use as a map keys.\n+\tTransientKey() string\n+}\n+\n // WeightProvider provides a weight.\n type WeightProvider interface {\n \tWeight() int\ndiff --git a/resources/resource_spec.go b/resources/resource_spec.go\nindex d50edeb73ce..912a0d786f0 100644\n--- a/resources/resource_spec.go\n+++ b/resources/resource_spec.go\n@@ -187,6 +187,7 @@ func (r *Spec) NewResource(rd ResourceSourceDescriptor) (resource.Resource, erro\n \t\tStaler:      &AtomicStaler{},\n \t\th:           &resourceHash{},\n \t\tpublishInit: &sync.Once{},\n+\t\tkeyInit:     &sync.Once{},\n \t\tpaths:       rp,\n \t\tspec:        r,\n \t\tsd:          rd,\ndiff --git a/resources/transform.go b/resources/transform.go\nindex 4214067bddc..c5d24066937 100644\n--- a/resources/transform.go\n+++ b/resources/transform.go\n@@ -52,8 +52,10 @@ var (\n \t_ identity.IdentityGroupProvider     = (*resourceAdapterInner)(nil)\n \t_ resource.Source                    = (*resourceAdapter)(nil)\n \t_ resource.Identifier                = (*resourceAdapter)(nil)\n+\t_ resource.TransientIdentifier       = (*resourceAdapter)(nil)\n \t_ targetPathProvider                 = (*resourceAdapter)(nil)\n \t_ sourcePathProvider                 = (*resourceAdapter)(nil)\n+\t_ resource.Identifier                = (*resourceAdapter)(nil)\n \t_ resource.ResourceNameTitleProvider = (*resourceAdapter)(nil)\n \t_ resource.WithResourceMetaProvider  = (*resourceAdapter)(nil)\n \t_ identity.DependencyManagerProvider = (*resourceAdapter)(nil)\n@@ -279,6 +281,10 @@ func (r *resourceAdapter) Key() string {\n \treturn r.target.(resource.Identifier).Key()\n }\n \n+func (r *resourceAdapter) TransientKey() string {\n+\treturn r.Key()\n+}\n+\n func (r *resourceAdapter) targetPath() string {\n \tr.init(false, false)\n \treturn r.target.(targetPathProvider).targetPath()\ndiff --git a/tpl/collections/reflect_helpers.go b/tpl/collections/reflect_helpers.go\nindex 4b222be15ca..6b986cbc4ce 100644\n--- a/tpl/collections/reflect_helpers.go\n+++ b/tpl/collections/reflect_helpers.go\n@@ -20,11 +20,12 @@ import (\n \n \t\"github.com/gohugoio/hugo/common/hashing\"\n \t\"github.com/gohugoio/hugo/common/types\"\n+\t\"github.com/gohugoio/hugo/resources/resource\"\n )\n \n var (\n \tzero      reflect.Value\n-\terrorType = reflect.TypeOf((*error)(nil)).Elem()\n+\terrorType = reflect.TypeFor[error]()\n )\n \n func numberToFloat(v reflect.Value) (float64, error) {\n@@ -56,7 +57,13 @@ func normalize(v reflect.Value) any {\n \t\t\treturn f\n \t\t}\n \t}\n-\treturn types.Unwrapv(v.Interface())\n+\n+\tvv := types.Unwrapv(v.Interface())\n+\tif ip, ok := vv.(resource.TransientIdentifier); ok {\n+\t\treturn ip.TransientKey()\n+\t}\n+\n+\treturn vv\n }\n \n // collects identities from the slices in seqs into a set. Numeric values are normalized,\n@@ -151,7 +158,6 @@ func convertNumber(v reflect.Value, to reflect.Kind) (reflect.Value, error) {\n \t\tcase reflect.Uint64:\n \t\t\tn = reflect.ValueOf(uint64(i))\n \t\t}\n-\n \t}\n \n \tif !n.IsValid() {\n", "instance_id": "gohugoio__hugo-13182", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the set operations (`union`, `complement`, `symdiff`, and `intersect`) do not work as expected with resource collections in Hugo. It provides a specific example of the issue with code snippets and mentions the version of Hugo being used, which helps in contextualizing the problem. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define the expected behavior of these set operations for resource collections (e.g., how resources should be compared for equality or uniqueness). Additionally, edge cases or specific constraints (e.g., handling of empty collections, performance expectations) are not mentioned. While the issue is valid and understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files (`resource.go`, `resourcetypes.go`, `resource_spec.go`, `transform.go`, and `reflect_helpers.go`), indicating a need to understand and modify interactions across different parts of the Hugo codebase. The changes involve introducing a new interface (`TransientIdentifier`) and modifying how resource keys are generated and used, which suggests a moderate impact on the system's architecture, particularly in how resources are identified and compared. \n\nSecond, the technical concepts required include a deep understanding of Go's concurrency primitives (e.g., `sync.Once` for lazy initialization), interface design, and reflection (`reflect` package modifications in `reflect_helpers.go`). Additionally, domain-specific knowledge of Hugo's resource management system is necessary to ensure that the changes align with the expected behavior of resource collections.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes imply handling scenarios like resource key initialization in a thread-safe manner and ensuring consistent behavior across different configurations (e.g., multihost setups). Error handling modifications are minimal, but the complexity of ensuring correct set operations (e.g., proper comparison of resources) introduces potential pitfalls.\n\nOverall, solving this problem requires a solid grasp of the codebase's architecture, careful consideration of resource identity management, and precise modifications to ensure correctness across various use cases. A score of 0.65 reflects the challenge of navigating these complexities, though it does not reach the \"Very Hard\" range as it does not involve system-level redesign or highly intricate algorithms.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Throw the \"Raw HTML omitted\" warning whenever Goldmark replaces raw HTML\nThis usage is somewhat common:\r\n\r\n```text\r\ncolumn<br>one|column<br>two\r\n:--|:--\r\na|b\r\n```\r\n\r\nWhich obviously only works with this site configuration:\r\n\r\n```text\r\n[markup.goldmark.renderer]\r\nunsafe = true\r\n```\r\n\r\nBut when the above is `false` I expected to see the \"Raw HTML omitted...\" warning introduced in v0.137.0. \r\n\r\n\n", "patch": "diff --git a/commands/deploy.go b/commands/deploy.go\nindex f9c22be48f2..eb419dabaf8 100644\n--- a/commands/deploy.go\n+++ b/commands/deploy.go\n@@ -20,7 +20,6 @@ import (\n \t\"context\"\n \n \t\"github.com/gohugoio/hugo/deploy\"\n-\t\"github.com/gohugoio/hugo/deploy/deployconfig\"\n \n \t\"github.com/bep/simplecobra\"\n \t\"github.com/spf13/cobra\"\n@@ -47,17 +46,7 @@ documentation.\n \t\t\treturn deployer.Deploy(ctx)\n \t\t},\n \t\twithc: func(cmd *cobra.Command, r *rootCommand) {\n-\t\t\tcmd.ValidArgsFunction = cobra.NoFileCompletions\n-\t\t\tcmd.Flags().String(\"target\", \"\", \"target deployment from deployments section in config file; defaults to the first one\")\n-\t\t\t_ = cmd.RegisterFlagCompletionFunc(\"target\", cobra.NoFileCompletions)\n-\t\t\tcmd.Flags().Bool(\"confirm\", false, \"ask for confirmation before making changes to the target\")\n-\t\t\tcmd.Flags().Bool(\"dryRun\", false, \"dry run\")\n-\t\t\tcmd.Flags().Bool(\"force\", false, \"force upload of all files\")\n-\t\t\tcmd.Flags().Bool(\"invalidateCDN\", deployconfig.DefaultConfig.InvalidateCDN, \"invalidate the CDN cache listed in the deployment target\")\n-\t\t\tcmd.Flags().Int(\"maxDeletes\", deployconfig.DefaultConfig.MaxDeletes, \"maximum # of files to delete, or -1 to disable\")\n-\t\t\t_ = cmd.RegisterFlagCompletionFunc(\"maxDeletes\", cobra.NoFileCompletions)\n-\t\t\tcmd.Flags().Int(\"workers\", deployconfig.DefaultConfig.Workers, \"number of workers to transfer files. defaults to 10\")\n-\t\t\t_ = cmd.RegisterFlagCompletionFunc(\"workers\", cobra.NoFileCompletions)\n+\t\t\tapplyDeployFlags(cmd, r)\n \t\t},\n \t}\n }\ndiff --git a/commands/deploy_flags.go b/commands/deploy_flags.go\nnew file mode 100644\nindex 00000000000..d4326547ab3\n--- /dev/null\n+++ b/commands/deploy_flags.go\n@@ -0,0 +1,33 @@\n+// Copyright 2024 The Hugo Authors. All rights reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package commands\n+\n+import (\n+\t\"github.com/gohugoio/hugo/deploy/deployconfig\"\n+\t\"github.com/spf13/cobra\"\n+)\n+\n+func applyDeployFlags(cmd *cobra.Command, r *rootCommand) {\n+\tcmd.ValidArgsFunction = cobra.NoFileCompletions\n+\tcmd.Flags().String(\"target\", \"\", \"target deployment from deployments section in config file; defaults to the first one\")\n+\t_ = cmd.RegisterFlagCompletionFunc(\"target\", cobra.NoFileCompletions)\n+\tcmd.Flags().Bool(\"confirm\", false, \"ask for confirmation before making changes to the target\")\n+\tcmd.Flags().Bool(\"dryRun\", false, \"dry run\")\n+\tcmd.Flags().Bool(\"force\", false, \"force upload of all files\")\n+\tcmd.Flags().Bool(\"invalidateCDN\", deployconfig.DefaultConfig.InvalidateCDN, \"invalidate the CDN cache listed in the deployment target\")\n+\tcmd.Flags().Int(\"maxDeletes\", deployconfig.DefaultConfig.MaxDeletes, \"maximum # of files to delete, or -1 to disable\")\n+\t_ = cmd.RegisterFlagCompletionFunc(\"maxDeletes\", cobra.NoFileCompletions)\n+\tcmd.Flags().Int(\"workers\", deployconfig.DefaultConfig.Workers, \"number of workers to transfer files. defaults to 10\")\n+\t_ = cmd.RegisterFlagCompletionFunc(\"workers\", cobra.NoFileCompletions)\n+}\ndiff --git a/commands/deploy_off.go b/commands/deploy_off.go\nindex 7eb6429c513..32a08da2eca 100644\n--- a/commands/deploy_off.go\n+++ b/commands/deploy_off.go\n@@ -44,6 +44,7 @@ func newDeployCommand() simplecobra.Commander {\n \t\t\treturn errors.New(\"deploy not supported in this version of Hugo; install a release with 'withdeploy' in the archive filename or build yourself with the 'withdeploy' build tag. Also see https://github.com/gohugoio/hugo/pull/12995\")\n \t\t},\n \t\twithc: func(cmd *cobra.Command, r *rootCommand) {\n+\t\t\tapplyDeployFlags(cmd, r)\n \t\t\tcmd.Hidden = true\n \t\t},\n \t}\ndiff --git a/common/hugo/vars_withdeploy.go b/common/hugo/vars_withdeploy.go\nnew file mode 100644\nindex 00000000000..88ce9a1cdbe\n--- /dev/null\n+++ b/common/hugo/vars_withdeploy.go\n@@ -0,0 +1,19 @@\n+// Copyright 2024 The Hugo Authors. All rights reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+//go:build withdeploy\n+// +build withdeploy\n+\n+package hugo\n+\n+var IsWithdeploy = true\ndiff --git a/common/hugo/vars_withdeploy_off.go b/common/hugo/vars_withdeploy_off.go\nnew file mode 100644\nindex 00000000000..93556802702\n--- /dev/null\n+++ b/common/hugo/vars_withdeploy_off.go\n@@ -0,0 +1,19 @@\n+// Copyright 2024 The Hugo Authors. All rights reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+//go:build !withdeploy\n+// +build !withdeploy\n+\n+package hugo\n+\n+var IsWithdeploy = false\ndiff --git a/common/hugo/version.go b/common/hugo/version.go\nindex 6cabfdbb964..cf598884069 100644\n--- a/common/hugo/version.go\n+++ b/common/hugo/version.go\n@@ -152,6 +152,9 @@ func BuildVersionString() string {\n \tif IsExtended {\n \t\tversion += \"+extended\"\n \t}\n+\tif IsWithdeploy {\n+\t\tversion += \"+withdeploy\"\n+\t}\n \n \tosArch := bi.GoOS + \"/\" + bi.GoArch\n \ndiff --git a/hugoreleaser.toml b/hugoreleaser.toml\nindex 8c369373981..2cb8e3665ee 100644\n--- a/hugoreleaser.toml\n+++ b/hugoreleaser.toml\n@@ -185,6 +185,25 @@ archive_alias_replacements = { \"linux-amd64.tar.gz\" = \"Linux-64bit.tar.gz\" }\n         [[builds.os.archs]]\n             goarch = \"amd64\"\n \n+[[builds]]\n+    path = \"container1/windows/extended-withdeploy\"\n+\n+    [builds.build_settings]\n+        flags = [\"-buildmode\", \"exe\", \"-tags\", \"extended,withdeploy\"]\n+        env = [\n+            \"CGO_ENABLED=1\",\n+            \"CC=x86_64-w64-mingw32-gcc\",\n+            \"CXX=x86_64-w64-mingw32-g++\",\n+        ]\n+        ldflags = \"-s -w -X github.com/gohugoio/hugo/common/hugo.vendorInfo=gohugoio -extldflags '-static'\"\n+\n+    [[builds.os]]\n+        goos = \"windows\"\n+        [builds.os.build_settings]\n+            binary = \"hugo.exe\"\n+        [[builds.os.archs]]\n+            goarch = \"amd64\"\n+\n [[archives]]\n     paths = [\"builds/container1/unix/regular/**\"]\n [[archives]]\n@@ -212,6 +231,13 @@ archive_alias_replacements = { \"linux-amd64.tar.gz\" = \"Linux-64bit.tar.gz\" }\n         [archives.archive_settings.type]\n             format    = \"zip\"\n             extension = \".zip\"\n+[[archives]]\n+    paths = [\"builds/**/windows/extended-withdeploy/**\"]\n+    [archives.archive_settings]\n+        name_template = \"{{ .Project }}_extended_withdeploy_{{ .Tag | trimPrefix `v` }}_{{ .Goos }}-{{ .Goarch }}\"\n+        [archives.archive_settings.type]\n+            format    = \"zip\"\n+            extension = \".zip\"\n [[archives]]\n     paths = [\"builds/**/regular/linux/{arm64,amd64}\"]\n     [archives.archive_settings]\ndiff --git a/markup/goldmark/hugocontext/hugocontext.go b/markup/goldmark/hugocontext/hugocontext.go\nindex 4971456be42..a10e095efb9 100644\n--- a/markup/goldmark/hugocontext/hugocontext.go\n+++ b/markup/goldmark/hugocontext/hugocontext.go\n@@ -145,6 +145,7 @@ func (r *hugoContextRenderer) SetOption(name renderer.OptionName, value any) {\n \n func (r *hugoContextRenderer) RegisterFuncs(reg renderer.NodeRendererFuncRegisterer) {\n \treg.Register(kindHugoContext, r.handleHugoContext)\n+\treg.Register(ast.KindRawHTML, r.renderRawHTML)\n \treg.Register(ast.KindHTMLBlock, r.renderHTMLBlock)\n }\n \n@@ -155,16 +156,25 @@ func (r *hugoContextRenderer) stripHugoCtx(b []byte) ([]byte, bool) {\n \treturn hugoCtxRe.ReplaceAll(b, nil), true\n }\n \n+func (r *hugoContextRenderer) logRawHTMLEmittedWarn(w util.BufWriter) {\n+\tr.logger.Warnidf(constants.WarnGoldmarkRawHTML, \"Raw HTML omitted from %q; see https://gohugo.io/getting-started/configuration-markup/#rendererunsafe\", r.getPage(w))\n+}\n+\n+func (r *hugoContextRenderer) getPage(w util.BufWriter) any {\n+\tvar p any\n+\tctx, ok := w.(*render.Context)\n+\tif ok {\n+\t\tp, _ = render.GetPageAndPageInner(ctx)\n+\t}\n+\treturn p\n+}\n+\n+// HTML rendering based on Goldmark implementation.\n func (r *hugoContextRenderer) renderHTMLBlock(\n \tw util.BufWriter, source []byte, node ast.Node, entering bool,\n ) (ast.WalkStatus, error) {\n \tn := node.(*ast.HTMLBlock)\n \tif entering {\n-\t\tvar p any\n-\t\tctx, ok := w.(*render.Context)\n-\t\tif ok {\n-\t\t\tp, _ = render.GetPageAndPageInner(ctx)\n-\t\t}\n \t\tif r.Unsafe {\n \t\t\tl := n.Lines().Len()\n \t\t\tfor i := 0; i < l; i++ {\n@@ -173,12 +183,12 @@ func (r *hugoContextRenderer) renderHTMLBlock(\n \t\t\t\tvar stripped bool\n \t\t\t\tlinev, stripped = r.stripHugoCtx(linev)\n \t\t\t\tif stripped {\n-\t\t\t\t\tr.logger.Warnidf(constants.WarnRenderShortcodesInHTML, \".RenderShortcodes detected inside HTML block in %q; this may not be what you intended, see https://gohugo.io/methods/page/rendershortcodes/#limitations\", p)\n+\t\t\t\t\tr.logger.Warnidf(constants.WarnRenderShortcodesInHTML, \".RenderShortcodes detected inside HTML block in %q; this may not be what you intended, see https://gohugo.io/methods/page/rendershortcodes/#limitations\", r.getPage(w))\n \t\t\t\t}\n \t\t\t\tr.Writer.SecureWrite(w, linev)\n \t\t\t}\n \t\t} else {\n-\t\t\tr.logger.Warnidf(constants.WarnGoldmarkRawHTML, \"Raw HTML omitted from %q; see https://gohugo.io/getting-started/configuration-markup/#rendererunsafe\", p)\n+\t\t\tr.logRawHTMLEmittedWarn(w)\n \t\t\t_, _ = w.WriteString(\"<!-- raw HTML omitted -->\\n\")\n \t\t}\n \t} else {\n@@ -194,6 +204,26 @@ func (r *hugoContextRenderer) renderHTMLBlock(\n \treturn ast.WalkContinue, nil\n }\n \n+func (r *hugoContextRenderer) renderRawHTML(\n+\tw util.BufWriter, source []byte, node ast.Node, entering bool,\n+) (ast.WalkStatus, error) {\n+\tif !entering {\n+\t\treturn ast.WalkSkipChildren, nil\n+\t}\n+\tif r.Unsafe {\n+\t\tn := node.(*ast.RawHTML)\n+\t\tl := n.Segments.Len()\n+\t\tfor i := 0; i < l; i++ {\n+\t\t\tsegment := n.Segments.At(i)\n+\t\t\t_, _ = w.Write(segment.Value(source))\n+\t\t}\n+\t\treturn ast.WalkSkipChildren, nil\n+\t}\n+\tr.logRawHTMLEmittedWarn(w)\n+\t_, _ = w.WriteString(\"<!-- raw HTML omitted -->\")\n+\treturn ast.WalkSkipChildren, nil\n+}\n+\n func (r *hugoContextRenderer) handleHugoContext(w util.BufWriter, source []byte, node ast.Node, entering bool) (ast.WalkStatus, error) {\n \tif !entering {\n \t\treturn ast.WalkContinue, nil\n", "instance_id": "gohugoio__hugo-13015", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes a specific issue with the Goldmark library in Hugo where a \"Raw HTML omitted\" warning should be displayed when raw HTML is replaced due to the `unsafe = false` configuration. It provides a relevant example of the input (HTML markup) and the expected behavior (warning message). However, there are minor ambiguities and missing details. For instance, it does not explicitly define the scope of where this warning should appear (e.g., only in logs, or also in the rendered output as a comment). Additionally, it lacks detailed constraints or edge cases, such as how to handle nested HTML or different types of raw HTML content. While the goal is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving modifications across multiple files, including the core rendering logic in `markup/goldmark/hugocontext/hugocontext.go` for handling raw HTML and HTML blocks, as well as refactoring unrelated deployment command logic into separate files. However, the primary change related to the problem statement (adding `renderRawHTML` function and warning logic) is relatively focused and does not significantly impact the broader system architecture. Second, the technical concepts required include familiarity with the Goldmark library (a Markdown parser), Hugo's rendering pipeline, and Go's AST manipulation for node rendering, which are moderately complex but not overly advanced. Third, the problem requires handling specific edge cases, such as ensuring the warning is logged appropriately and consistently for different types of raw HTML content, though these are not extensively complex. Overall, while the changes require understanding multiple components and careful implementation, they do not demand deep architectural refactoring or advanced domain-specific knowledge, placing the difficulty at 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Replace tenv linter with usetesting\n### What would you like to be added?\n\nAfter the `golagci-lint` bump to 1.64.5, we started getting the following warning:\n\n```\nThe linter 'tenv' is deprecated (since v1.64.0) due to: Duplicate feature another linter. Replaced by usetesting\n```\n\nWe should replace the linter with `usetesting` to avoid future failures.\n\nRefer to any recent `pull-etcd-verify` job, i.e., https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/etcd-io_etcd/19484/pull-etcd-verify/1894477775524335616#1:build-log.txt%3A114-119\n\n### Why is this needed?\n\nTo keep our linters up to date.\n", "patch": "diff --git a/tools/.golangci.yaml b/tools/.golangci.yaml\nindex 60925e0a467..f8d602cc08f 100644\n--- a/tools/.golangci.yaml\n+++ b/tools/.golangci.yaml\n@@ -30,12 +30,12 @@ linters:\n     - revive\n     - staticcheck\n     - stylecheck\n-    - tenv\n     - testifylint\n     - unconvert # Remove unnecessary type conversions\n     - unparam\n     - unused\n     - usestdlibvars\n+    - usetesting\n     - whitespace\n linters-settings: # please keep this alphabetized\n   goimports:\n@@ -120,3 +120,5 @@ linters-settings: # please keep this alphabetized\n       # to always require f-functions for stretchr/testify, but not for golang standard lib.\n       # Also refer to https://github.com/etcd-io/etcd/pull/18741#issuecomment-2422395914\n       require-f-funcs: true\n+  usetesting:\n+    os-mkdir-temp: false\n", "instance_id": "etcd-io__etcd-19488", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the deprecated 'tenv' linter with 'usetesting' due to a warning after a version bump in 'golangci-lint'. It provides context about the deprecation and references a specific log for further details, which helps in understanding the issue. However, there are minor ambiguities: the statement does not explicitly describe what 'usetesting' does or how it differs from 'tenv', nor does it provide detailed guidance on configuration or potential compatibility issues. Additionally, while the goal is clear, there are no examples or specific test cases to validate the replacement. Overall, the problem is valid and mostly clear, but it lacks some minor details that could aid in a smoother implementation.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward configuration change in a single file ('.golangci.yaml'). The scope of the code change is minimal, requiring only the replacement of one linter name with another and the addition of a simple configuration setting for 'usetesting'. It does not impact the broader codebase architecture or require understanding complex interactions between modules. The technical concepts involved are basic\u2014familiarity with 'golangci-lint' configuration and YAML syntax, which are not particularly challenging for a developer with even minimal experience in Go or CI/CD tools. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as this is purely a configuration update to address a deprecation warning. Overall, this is a very easy task that requires minimal effort and expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc84 use autostyle for bootstrap questions\n# Overview\n\n- configure the glamour renderer to use AutoStyle to detect light mode backgrounds\n\nLocation: [./ui/bootstrap/model.go](https://github.com/algorandfoundation/nodekit/blob/main/ui/bootstrap/model.go#L98)\n\nExample:\n\n```go\nr, err:= glamour.NewTermRenderer(glamour.WithAutoStyle())\n```\n", "patch": "diff --git a/ui/bootstrap/model.go b/ui/bootstrap/model.go\nindex 11c9537f..c5577f94 100644\n--- a/ui/bootstrap/model.go\n+++ b/ui/bootstrap/model.go\n@@ -95,6 +95,13 @@ func (m Model) View() string {\n \tcase CatchupQuestion:\n \t\tstr = CatchupQuestionMsg\n \t}\n-\tmsg, _ := glamour.Render(str, \"dark\")\n+\tvar msg string\n+\tr, err := glamour.NewTermRenderer(glamour.WithAutoStyle())\n+\tif err != nil {\n+\t\t// Fallback to dark mode\n+\t\tmsg, _ = glamour.Render(str, \"dark\")\n+\t} else {\n+\t\tmsg, _ = r.Render(str)\n+\t}\n \treturn msg\n }\n", "instance_id": "algorandfoundation__nodekit-104", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to configure the `glamour` renderer to use `AutoStyle` for detecting light mode backgrounds in a terminal UI context. It provides a specific location in the codebase and an example of how to implement the change using `glamour.WithAutoStyle()`. However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior of `AutoStyle` (e.g., how it detects light mode or what should happen if detection fails). Additionally, there is no mention of potential edge cases or constraints, such as handling environments where `AutoStyle` might not work as expected. While the goal is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change is localized to a single file (`model.go`) and involves a small, straightforward modification to replace a static style (\"dark\") with a dynamic renderer configuration using `glamour.WithAutoStyle()`. The scope of the change is minimal, affecting only a few lines of code within a single function, with no apparent impact on the broader system architecture or interactions between modules. The technical concepts required are basic: familiarity with Go, the `glamour` library, and error handling. No complex algorithms, design patterns, or domain-specific knowledge are needed. The code change already includes a fallback to \"dark\" mode in case of an error, addressing a potential edge case, though the problem statement does not explicitly mention other edge cases or performance considerations. Overall, this task requires minimal effort and understanding, suitable for a developer with basic to intermediate Go experience.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Follower rejecting leader's pre-vote\nHello, thanks for the library! We were trying out raft v1.7.0, and found that the pre-vote implementation has a bug where a follower rejects the pre-vote req from node X even when when the follower thinks that X is the leader.\r\n\r\nThis is the offending line. https://github.com/hashicorp/raft/blob/main/raft.go#L1752\r\n\r\n```go\r\n// Check if we have an existing leader [who's not the candidate] and also\r\nvar candidate ServerAddress // ISSUE: this is not set at all\r\ncandidateID := ServerID(req.ID)\r\n\r\n// if the Servers list is empty that mean the cluster is very likely trying to bootstrap,\r\n// Grant the vote\r\nif len(r.configurations.latest.Servers) > 0 && !inConfiguration(r.configurations.latest, candidateID) {\r\n\tr.logger.Warn(\"rejecting pre-vote request since node is not in configuration\",\r\n\t\t\"from\", candidate)\r\n\treturn\r\n}\r\n\r\nif leaderAddr, leaderID := r.LeaderWithID(); leaderAddr != \"\" && leaderAddr != candidate { // ISSUE: so this will always return true as long as the follower has a leader\r\n\tr.logger.Warn(\"rejecting pre-vote request since we have a leader\",\r\n\t\t\"from\", candidate,\r\n\t\t\"leader\", leaderAddr,\r\n\t\t\"leader-id\", string(leaderID))\r\n\treturn\r\n}\r\n```\n", "patch": "diff --git a/raft.go b/raft.go\nindex 183f041a..1ebcef1a 100644\n--- a/raft.go\n+++ b/raft.go\n@@ -1749,7 +1749,7 @@ func (r *Raft) requestPreVote(rpc RPC, req *RequestPreVoteRequest) {\n \t}()\n \n \t// Check if we have an existing leader [who's not the candidate] and also\n-\tvar candidate ServerAddress\n+\tcandidate := r.trans.DecodePeer(req.GetRPCHeader().Addr)\n \tcandidateID := ServerID(req.ID)\n \n \t// if the Servers list is empty that mean the cluster is very likely trying to bootstrap,\n", "instance_id": "hashicorp__raft-605", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue with the pre-vote implementation in the Raft consensus algorithm. It points out a specific bug where a follower rejects a pre-vote request from a node it considers the leader due to an uninitialized `candidate` variable and a flawed comparison logic. The offending code line is referenced, and a snippet of the problematic code is provided, which helps in understanding the issue. However, there are minor ambiguities: the statement does not explicitly describe the expected behavior of the pre-vote mechanism in detail (e.g., under what exact conditions a pre-vote should be rejected or accepted), nor does it provide examples or test cases to illustrate the bug. Additionally, the impact of this bug on the overall Raft protocol behavior is not discussed, which could be critical for understanding the severity. Despite these minor gaps, the problem is valid and the goal is reasonably clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem is relatively low, falling in the Easy range (0.2-0.4). The issue is a straightforward bug fix involving a single line of code change in one file (`raft.go`), where the `candidate` variable needs to be initialized with the decoded peer address from the request header. The scope of the change is minimal, with no impact on the broader system architecture or interactions between multiple modules. The technical concepts required to solve this are basic: understanding Go syntax, variable initialization, and a minimal grasp of the Raft protocol's pre-vote mechanism (though deep knowledge of Raft is not strictly necessary for this fix). There are no complex edge cases or error handling requirements introduced by this change, as the fix simply corrects an oversight in variable initialization to ensure the comparison logic works as intended. The problem does not require advanced algorithms, design patterns, or domain-specific knowledge beyond the immediate context of the code snippet. Therefore, a difficulty score of 0.25 is appropriate, reflecting a simple bug fix with minimal complexity.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fetcher does not handle http status codes properly during function specialization\nDuring the function specialization the _fetcher_ sidecar container does not throw an error if the received response from the _storage service_, where it tries to fetch the function archive from, has an non-successful status code (e.g. 404). Instead the _fetcher_ takes the response body anyway and places it under `/userfunc/deployarchive` as plain text which I consider undesirable.\r\n\r\n**Some Background:**\r\nI have replaced the built-in _storage service_ with a custom implementation which relies on proper status code handling in the _fetcher_ sidecar.\n", "patch": "diff --git a/pkg/utils/utils.go b/pkg/utils/utils.go\nindex 4e053a45a4..00ca97e602 100644\n--- a/pkg/utils/utils.go\n+++ b/pkg/utils/utils.go\n@@ -171,6 +171,10 @@ func IsURL(str string) bool {\n \treturn strings.HasPrefix(str, \"http://\") || strings.HasPrefix(str, \"https://\")\n }\n \n+func isHttp2xxSuccessful(status int) bool {\n+\treturn status >= 200 && status < 300\n+}\n+\n func DownloadUrl(ctx context.Context, httpClient *http.Client, url string, localPath string) error {\n \tresp, err := ctxhttp.Get(ctx, httpClient, url)\n \tif err != nil {\n@@ -178,6 +182,10 @@ func DownloadUrl(ctx context.Context, httpClient *http.Client, url string, local\n \t}\n \tdefer resp.Body.Close()\n \n+\tif !isHttp2xxSuccessful(resp.StatusCode) {\n+\t\treturn errors.New(resp.Status)\n+\t}\n+\n \tw, err := os.Create(localPath)\n \tif err != nil {\n \t\treturn err\n", "instance_id": "fission__fission-2959", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the fetcher sidecar container does not handle non-successful HTTP status codes (e.g., 404) properly during function specialization, leading to undesirable behavior of saving the response body as plain text. The goal of ensuring proper status code handling is evident, and some background context about a custom storage service implementation is provided. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define what constitutes a \"successful\" status code (though the code change implies 2xx range), nor does it mention specific edge cases or expected behavior for different types of errors (e.g., should retries be attempted for certain status codes like 503?). Additionally, there are no examples of input/output or detailed constraints about the environment or dependencies. Despite these minor gaps, the core issue and intent are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling in the Easy range (0.2-0.4). The code change is straightforward and localized to a single function in a single file (`pkg/utils/utils.go`), involving the addition of a simple helper function `isHttp2xxSuccessful` to check for 2xx status codes and a conditional check in the `DownloadUrl` function to return an error for non-successful responses. The scope of the change is minimal, with no impact on the broader system architecture or interactions between multiple modules. The technical concepts required are basic: understanding HTTP status codes, Go's `http` package, and simple error handling using the `errors` package. No advanced algorithms, design patterns, or domain-specific knowledge are needed. While the problem statement does not explicitly mention edge cases, the code change implicitly handles a broad range of non-2xx status codes, though it lacks sophistication (e.g., no differentiation between retryable and permanent errors). Overall, this is a simple bug fix requiring minimal effort and understanding of the codebase, justifying a difficulty score of 0.25.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Website wrongly shows specifying-task-level-computeresources as alpha feature\n# Expected Behavior\r\n Specifying-task-level-computeresources is a beta feature.\r\n\r\n# Actual Behavior\r\nThe website wrongly shows specifying-task-level-computeresources as alpha feature\r\n\r\nhttps://tekton.dev/docs/pipelines/taskruns/#specifying-task-level-computeresources\n", "patch": "diff --git a/docs/taskruns.md b/docs/taskruns.md\nindex 213454c1815..d689b69f3ec 100644\n--- a/docs/taskruns.md\n+++ b/docs/taskruns.md\n@@ -374,7 +374,7 @@ may be overridden by a TaskRun's StepSpecs and SidecarSpecs.\n \n ### Specifying Task-level `ComputeResources`\n \n-**([alpha only](https://github.com/tektoncd/pipeline/blob/main/docs/additional-configs.md#alpha-features))**\n+**([beta only](https://github.com/tektoncd/pipeline/blob/main/docs/additional-configs.md#beta-features))**\n \n Task-level compute resources can be configured in `TaskRun.ComputeResources`, or `PipelineRun.TaskRunSpecs.ComputeResources`.\n \n", "instance_id": "tektoncd__pipeline-8004", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the website incorrectly labels a feature as \"alpha\" when it should be \"beta.\" The expected and actual behaviors are explicitly stated, and a link to the relevant documentation is provided, which helps in understanding the context. However, the statement lacks additional details or examples about the feature itself or why the distinction between \"alpha\" and \"beta\" matters. There are no mentions of potential implications of this change or any related dependencies in the documentation or codebase that might need to be considered. Additionally, edge cases or broader impacts (e.g., whether this change affects other parts of the documentation or user perception) are not addressed. Despite these minor omissions, the core issue is straightforward and understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a simple textual correction in a single documentation file. The code change provided is minimal, requiring only the replacement of the word \"alpha\" with \"beta\" in a specific line of the `taskruns.md` file, along with updating the associated link to point to beta features instead of alpha. This task does not require understanding complex logic, interactions between different parts of the codebase, or any advanced programming concepts, algorithms, or domain-specific knowledge beyond basic familiarity with documentation formatting. There are no edge cases or error handling considerations mentioned or implied in the problem statement or code changes. The scope of the change is extremely limited, impacting only a single line in one file with no architectural or systemic implications. Therefore, I assign a difficulty score of 0.1, placing it in the \"Very Easy\" category, as it is essentially a trivial update akin to fixing a typo.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Trying to launch Syncthing when already running resets cert.pem to 0 bytes\n### What happened?\n\n1. Start Syncthing.\n2. Try to start Syncthing again using the same `--config` (or `--home`) directory.\n3. The new instance fails to start with the following error:\n\n```\n[start] 2025/04/12 01:58:55.751711 main.go:432: INFO: syncthing v2.0.0-beta.6 \"Hafnium Hornet\" (go1.24.2 windows-amd64) builder@github.syncthing.net 2025-04-07 09:43:08 UTC [stnoupgrade]\n[start] 2025/04/12 01:58:55.751711 utils.go:64: INFO: Generating ECDSA key and certificate for syncthing...\n[start] 2025/04/12 01:58:55.753718 main.go:440: WARNING: Failed to load/generate certificate: save cert: write R:\\test\\syncthing\\syncthing1\\cert.pem: The process cannot access the file because another process has locked a portion of the file.\n[monitor] 2025/04/12 01:58:55.755224 monitor.go:199: INFO: Syncthing exited: exit status 1\n```\n\n4. Despite that, the `cert.pem` file has been wiped out and has a size of 0 bytes.\n\nTested under Windows 10 x64.\n\nRef 1: https://github.com/syncthing/syncthing/commit/7762e39fb3ad90a13034d43039112583cd0f514b\nRef 2: https://forum.syncthing.net/t/syncthing-device-id-keeps-switching/24147\n\n### Syncthing version\n\nv1.29.4 and beta v2\n\n### Platform & operating system\n\nWindows\n\n### Browser version\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```\n", "patch": "diff --git a/cmd/syncthing/main.go b/cmd/syncthing/main.go\nindex 1faff628ef3..943d11dc87d 100644\n--- a/cmd/syncthing/main.go\n+++ b/cmd/syncthing/main.go\n@@ -376,7 +376,7 @@ func (options serveOptions) Run() error {\n \tif options.Upgrade {\n \t\trelease, err := checkUpgrade()\n \t\tif err == nil {\n-\t\t\tlf := flock.New(locations.Get(locations.CertFile))\n+\t\t\tlf := flock.New(locations.Get(locations.LockFile))\n \t\t\tlocked, err := lf.TryLock()\n \t\t\tif err != nil {\n \t\t\t\tl.Warnln(\"Upgrade:\", err)\n@@ -386,6 +386,8 @@ func (options serveOptions) Run() error {\n \t\t\t} else {\n \t\t\t\terr = upgrade.To(release)\n \t\t\t}\n+\t\t\t_ = lf.Unlock()\n+\t\t\t_ = os.Remove(locations.Get(locations.LockFile))\n \t\t}\n \t\tif err != nil {\n \t\t\tl.Warnln(\"Upgrade:\", err)\n@@ -546,7 +548,7 @@ func syncthingMain(options serveOptions) {\n \t}\n \n \t// Ensure we are the only running instance\n-\tlf := flock.New(locations.Get(locations.CertFile))\n+\tlf := flock.New(locations.Get(locations.LockFile))\n \tlocked, err := lf.TryLock()\n \tif err != nil {\n \t\tl.Warnln(\"Failed to acquire lock:\", err)\n@@ -692,6 +694,10 @@ func syncthingMain(options serveOptions) {\n \t\tpprof.StopCPUProfile()\n \t}\n \n+\t// Best effort remove lockfile, doesn't matter if it succeeds\n+\t_ = lf.Unlock()\n+\t_ = os.Remove(locations.Get(locations.LockFile))\n+\n \tos.Exit(int(status))\n }\n \ndiff --git a/lib/locations/locations.go b/lib/locations/locations.go\nindex 197c686eb67..bbf11d91961 100644\n--- a/lib/locations/locations.go\n+++ b/lib/locations/locations.go\n@@ -33,6 +33,7 @@ const (\n \tAuditLog      LocationEnum = \"auditLog\"\n \tGUIAssets     LocationEnum = \"guiAssets\"\n \tDefFolder     LocationEnum = \"defFolder\"\n+\tLockFile      LocationEnum = \"lockFile\"\n )\n \n type BaseDirEnum string\n@@ -124,6 +125,7 @@ var locationTemplates = map[LocationEnum]string{\n \tAuditLog:      \"${data}/audit-%{timestamp}.log\",\n \tGUIAssets:     \"${config}/gui\",\n \tDefFolder:     \"${userHome}/Sync\",\n+\tLockFile:      \"${data}/syncthing.lock\",\n }\n \n var locations = make(map[LocationEnum]string)\n", "instance_id": "syncthing__syncthing-10054", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: attempting to launch Syncthing when it is already running results in the `cert.pem` file being reset to 0 bytes, accompanied by a specific error message. The goal (preventing the file corruption) and the context (Windows OS, specific Syncthing versions) are provided, along with relevant log output and references to related discussions. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior when a second instance is launched (e.g., should it fail gracefully without touching files?), nor does it mention specific edge cases or constraints beyond the Windows platform. Additionally, the connection between the error message and the file corruption could be elaborated upon for better understanding. Overall, while the issue is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively limited, affecting only two files (`main.go` and `locations.go`) with a small number of lines modified. The changes primarily involve switching the lock file mechanism from using `cert.pem` to a dedicated `syncthing.lock` file and ensuring proper cleanup of the lock file. This requires understanding file locking mechanisms (using the `flock` package) and the Syncthing codebase's configuration and initialization logic, which adds moderate complexity. Second, the technical concepts involved include file locking, process synchronization, and file system operations in Go, which are not overly advanced but require careful handling to avoid race conditions or permission issues. Third, while the problem statement does not explicitly mention edge cases, the code changes imply considerations for scenarios like failed lock acquisition, file deletion errors, and ensuring the lock file does not persist after process termination\u2014though these are handled with \"best effort\" logic, reducing complexity. Finally, the changes do not significantly impact the broader system architecture, as they are confined to instance management logic. Overall, this problem requires a moderate understanding of the codebase and careful implementation, but it does not involve deep architectural changes or highly complex concepts, justifying a score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Routers API endpoint repeats entries with multiple entrypoints on v3.3.2\n### Welcome!\n\n- [x] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [x] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nI have upgraded the docker image from v3.3.1 to v3.3.2.\n\n### What did you see instead?\n\nAfter the upgrade, the `/api/http/routers` endpoint prints routers differently, no longer printing entrypoints in an array but repeating them.\n\nBefore the upgrade (v3.3.1) the output would produce data like this:\n\n```\ncurl -s \"https://traefik.example.org/api/http/routers\"  | jq \".[] | [.name, .using]\" -c\n[\"acme-http@internal\",[\"web\"]]\n[\"dashboard@docker\",[\"metrics\",\"web\",\"websecure\"]]\n[\"whoami@docker\",[\"metrics\",\"web\",\"websecure\"]]\n```\n\nAfter the upgrade (v3.3.2), it becomes this:\n\n```\ncurl -s \"https://traefik.example.org/api/http/routers\"  | jq \".[] | [.name, .using]\" -c\n[\"acme-http@internal\",[\"web\"]]\n[\"metrics-dashboard@docker\",[\"metrics\"]]\n[\"metrics-whoami@docker\",[\"metrics\"]]\n[\"web-dashboard@docker\",[\"web\"]]\n[\"web-whoami@docker\",[\"web\"]]\n[\"websecure-dashboard@docker\",[\"websecure\"]]\n[\"websecure-whoami@docker\",[\"websecure\"]]\n```\n\n### What version of Traefik are you using?\n\n```\n% docker run traefik:v3.3.2 version\nVersion:      3.3.2\nCodename:     saintnectaire\nGo version:   go1.23.4\nBuilt:        2025-01-14T15:52:24Z\nOS/Arch:      linux/amd64\n```\n\n### What is your environment & configuration?\n\nRunning traefik on Docker using Docker compose. I see the same behavior on multiple installations running Ubuntu 24.04.1 and Debian 12.\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/pkg/config/static/entrypoints.go b/pkg/config/static/entrypoints.go\nindex f443abe378..59c75bfe40 100644\n--- a/pkg/config/static/entrypoints.go\n+++ b/pkg/config/static/entrypoints.go\n@@ -60,8 +60,6 @@ func (ep *EntryPoint) SetDefaults() {\n \tep.HTTP.SetDefaults()\n \tep.HTTP2 = &HTTP2Config{}\n \tep.HTTP2.SetDefaults()\n-\tep.Observability = &ObservabilityConfig{}\n-\tep.Observability.SetDefaults()\n }\n \n // HTTPConfig is the HTTP configuration of an entry point.\n@@ -164,14 +162,15 @@ func (u *UDPConfig) SetDefaults() {\n \n // ObservabilityConfig holds the observability configuration for an entry point.\n type ObservabilityConfig struct {\n-\tAccessLogs bool `json:\"accessLogs,omitempty\" toml:\"accessLogs,omitempty\" yaml:\"accessLogs,omitempty\" export:\"true\"`\n-\tTracing    bool `json:\"tracing,omitempty\" toml:\"tracing,omitempty\" yaml:\"tracing,omitempty\" export:\"true\"`\n-\tMetrics    bool `json:\"metrics,omitempty\" toml:\"metrics,omitempty\" yaml:\"metrics,omitempty\" export:\"true\"`\n+\tAccessLogs *bool `json:\"accessLogs,omitempty\" toml:\"accessLogs,omitempty\" yaml:\"accessLogs,omitempty\" export:\"true\"`\n+\tTracing    *bool `json:\"tracing,omitempty\" toml:\"tracing,omitempty\" yaml:\"tracing,omitempty\" export:\"true\"`\n+\tMetrics    *bool `json:\"metrics,omitempty\" toml:\"metrics,omitempty\" yaml:\"metrics,omitempty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n func (o *ObservabilityConfig) SetDefaults() {\n-\to.AccessLogs = true\n-\to.Tracing = true\n-\to.Metrics = true\n+\tdefaultValue := true\n+\to.AccessLogs = &defaultValue\n+\to.Tracing = &defaultValue\n+\to.Metrics = &defaultValue\n }\ndiff --git a/pkg/provider/traefik/internal.go b/pkg/provider/traefik/internal.go\nindex 47f8223213..f1490a41bd 100644\n--- a/pkg/provider/traefik/internal.go\n+++ b/pkg/provider/traefik/internal.go\n@@ -240,9 +240,9 @@ func (i *Provider) entryPointModels(cfg *dynamic.Configuration) {\n \n \t\tif ep.Observability != nil {\n \t\t\thttpModel.Observability = dynamic.RouterObservabilityConfig{\n-\t\t\t\tAccessLogs: &ep.Observability.AccessLogs,\n-\t\t\t\tTracing:    &ep.Observability.Tracing,\n-\t\t\t\tMetrics:    &ep.Observability.Metrics,\n+\t\t\t\tAccessLogs: ep.Observability.AccessLogs,\n+\t\t\t\tTracing:    ep.Observability.Tracing,\n+\t\t\t\tMetrics:    ep.Observability.Metrics,\n \t\t\t}\n \t\t}\n \ndiff --git a/pkg/server/aggregator.go b/pkg/server/aggregator.go\nindex 72934d4690..3cd97f2066 100644\n--- a/pkg/server/aggregator.go\n+++ b/pkg/server/aggregator.go\n@@ -191,14 +191,14 @@ func applyModel(cfg dynamic.Configuration) dynamic.Configuration {\n \t\t\t\t\t\tcp.Observability.AccessLogs = m.Observability.AccessLogs\n \t\t\t\t\t}\n \n-\t\t\t\t\tif cp.Observability.Tracing == nil {\n-\t\t\t\t\t\tcp.Observability.Tracing = m.Observability.Tracing\n-\t\t\t\t\t}\n-\n \t\t\t\t\tif cp.Observability.Metrics == nil {\n \t\t\t\t\t\tcp.Observability.Metrics = m.Observability.Metrics\n \t\t\t\t\t}\n \n+\t\t\t\t\tif cp.Observability.Tracing == nil {\n+\t\t\t\t\t\tcp.Observability.Tracing = m.Observability.Tracing\n+\t\t\t\t\t}\n+\n \t\t\t\t\trtName := name\n \t\t\t\t\tif len(eps) > 1 {\n \t\t\t\t\t\trtName = epName + \"-\" + name\n@@ -215,6 +215,9 @@ func applyModel(cfg dynamic.Configuration) dynamic.Configuration {\n \t\tcfg.HTTP.Routers = rts\n \t}\n \n+\t// Apply default observability model to HTTP routers.\n+\tapplyDefaultObservabilityModel(cfg)\n+\n \tif cfg.TCP == nil || len(cfg.TCP.Models) == 0 {\n \t\treturn cfg\n \t}\n@@ -238,3 +241,38 @@ func applyModel(cfg dynamic.Configuration) dynamic.Configuration {\n \n \treturn cfg\n }\n+\n+// applyDefaultObservabilityModel applies the default observability model to the configuration.\n+// This function is used to ensure that the observability configuration is set for all routers,\n+// and make sure it is serialized and available in the API.\n+// We could have introduced a \"default\" model, but it would have been more complex to manage for now.\n+// This could be generalized in the future.\n+func applyDefaultObservabilityModel(cfg dynamic.Configuration) {\n+\tif cfg.HTTP != nil {\n+\t\tfor _, router := range cfg.HTTP.Routers {\n+\t\t\tif router.Observability == nil {\n+\t\t\t\trouter.Observability = &dynamic.RouterObservabilityConfig{\n+\t\t\t\t\tAccessLogs: pointer(true),\n+\t\t\t\t\tMetrics:    pointer(true),\n+\t\t\t\t\tTracing:    pointer(true),\n+\t\t\t\t}\n+\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\tif router.Observability.AccessLogs == nil {\n+\t\t\t\trouter.Observability.AccessLogs = pointer(true)\n+\t\t\t}\n+\n+\t\t\tif router.Observability.Tracing == nil {\n+\t\t\t\trouter.Observability.Tracing = pointer(true)\n+\t\t\t}\n+\n+\t\t\tif router.Observability.Metrics == nil {\n+\t\t\t\trouter.Observability.Metrics = pointer(true)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func pointer[T any](v T) *T { return &v }\ndiff --git a/pkg/server/middleware/observability.go b/pkg/server/middleware/observability.go\nindex 566d8522ca..d279be9028 100644\n--- a/pkg/server/middleware/observability.go\n+++ b/pkg/server/middleware/observability.go\n@@ -110,7 +110,7 @@ func (o *ObservabilityMgr) ShouldAddAccessLogs(serviceName string, observability\n \t\treturn false\n \t}\n \n-\treturn observabilityConfig == nil || observabilityConfig.AccessLogs != nil && *observabilityConfig.AccessLogs\n+\treturn observabilityConfig == nil || observabilityConfig.AccessLogs == nil || *observabilityConfig.AccessLogs\n }\n \n // ShouldAddMetrics returns whether the metrics should be enabled for the given resource and the observability config.\n@@ -127,7 +127,7 @@ func (o *ObservabilityMgr) ShouldAddMetrics(serviceName string, observabilityCon\n \t\treturn false\n \t}\n \n-\treturn observabilityConfig == nil || observabilityConfig.Metrics != nil && *observabilityConfig.Metrics\n+\treturn observabilityConfig == nil || observabilityConfig.Metrics == nil || *observabilityConfig.Metrics\n }\n \n // ShouldAddTracing returns whether the tracing should be enabled for the given serviceName and the observability config.\n@@ -144,7 +144,7 @@ func (o *ObservabilityMgr) ShouldAddTracing(serviceName string, observabilityCon\n \t\treturn false\n \t}\n \n-\treturn observabilityConfig == nil || observabilityConfig.Tracing != nil && *observabilityConfig.Tracing\n+\treturn observabilityConfig == nil || observabilityConfig.Tracing == nil || *observabilityConfig.Tracing\n }\n \n // MetricsRegistry is an accessor to the metrics registry.\n", "instance_id": "traefik__traefik-11476", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `/api/http/routers` endpoint in Traefik v3.3.2 outputs router entries differently compared to v3.3.1, specifically by repeating entries for each entrypoint instead of grouping them in an array. The statement includes examples of the output before and after the upgrade, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, it does not explicitly state whether this change in output format is considered a bug or an intentional change, nor does it specify the expected behavior (e.g., should the output revert to the old format or is there a different desired format?). Additionally, there are no mentions of specific constraints, edge cases, or performance implications related to the API output change. While the issue is valid and the core problem is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`entrypoints.go`, `internal.go`, `aggregator.go`, and `observability.go`), indicating a need to understand interactions between different parts of the Traefik codebase, particularly around configuration models and observability settings. The changes involve modifying data structures (e.g., changing boolean fields to pointers in `ObservabilityConfig`), adding new logic for applying default observability settings, and adjusting conditional checks for observability features. This requires a moderate understanding of Go-specific concepts like pointers and struct initialization, as well as familiarity with Traefik's internal configuration and router aggregation logic. While the changes do not appear to impact the core architecture significantly, they do require careful handling to ensure consistency across the system, especially in how observability settings are applied and serialized for API output. The problem also implicitly involves edge cases, such as ensuring that default observability settings do not override user-defined configurations, though these are not explicitly mentioned in the problem statement. Overall, this task demands a solid grasp of the codebase and moderate complexity in implementation, but it does not reach the level of hard or very hard due to the absence of advanced algorithms, performance-critical changes, or deep architectural refactoring.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "In v3.0 logs go to STDERR instead of STDOUT\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nI spent a small chunk of time trying to figure out how to separate application logs from access logs when both are enabled.  The documentation for both logs and access logs say \"By default, logs are written to stdout, in text format.\", but in reality, application logs are written to stderr by default.\n\n### What did you see instead?\n\nApplication logging goes to stderr by default, not stdout.  This would just be nice to have documented properly.\n\n### What version of Traefik are you using?\n\nVersion:      3.1.1\r\nCodename:     comte\r\nGo version:   go1.22.5\r\nBuilt:        2024-07-30T13:55:22Z\r\nOS/Arch:      linux/amd64\r\n\n\n### What is your environment & configuration?\n\nRelevant environment variables:\r\n```\r\n        TRAEFIK_LOG         = \"true\"\r\n        TRAEFIK_LOG_LEVEL   = \"DEBUG\"\r\n        TRAEFIK_ACCESSLOG              = \"true\"\r\n```\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/cmd/traefik/logger.go b/cmd/traefik/logger.go\nindex c4f6c9dc18..8ba6ac6826 100644\n--- a/cmd/traefik/logger.go\n+++ b/cmd/traefik/logger.go\n@@ -46,7 +46,7 @@ func setupLogger(staticConfiguration *static.Configuration) {\n }\n \n func getLogWriter(staticConfiguration *static.Configuration) io.Writer {\n-\tvar w io.Writer = os.Stderr\n+\tvar w io.Writer = os.Stdout\n \n \tif staticConfiguration.Log != nil && len(staticConfiguration.Log.FilePath) > 0 {\n \t\t_, _ = os.OpenFile(staticConfiguration.Log.FilePath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0o666)\n", "instance_id": "traefik__traefik-10973", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the application logs in Traefik v3.0 are being written to STDERR by default, contrary to the documentation which states they should go to STDOUT. The goal of aligning the behavior with the documentation (or updating the documentation) is implied, and the provided code change suggests the intent is to modify the behavior to match the documentation. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly state whether the desired solution is to change the code to write logs to STDOUT or to update the documentation to reflect the current behavior (STDERR). Additionally, there are no mentions of potential side effects or constraints, such as whether changing the log output destination could impact other parts of the system or downstream log processing tools. Despite these minor gaps, the issue is valid and the intent is reasonably clear, especially with the context provided by the code change.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward code modification. The change is limited to a single line in a single file (`logger.go`), where the log writer is switched from `os.Stderr` to `os.Stdout`. This requires minimal understanding of the codebase\u2014just basic knowledge of Go's `io.Writer` interface and standard output streams. The scope of the change is extremely narrow, with no impact on the broader system architecture or interactions between modules. There are no complex technical concepts, algorithms, or design patterns involved, nor is there any indication of significant edge cases or error handling requirements beyond the existing logic for file-based logging. The primary risk is potential downstream effects (e.g., scripts or tools expecting logs on STDERR), but this is not mentioned in the problem statement and does not appear to be a concern in the provided code change. Overall, this is a very easy fix, akin to changing a configuration or constant, requiring only basic coding skills.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Content-Security-Policy-Report-Only\n### *Feature request*\r\n\r\nAdd support for Content-Security-Policy-Report-Only. We can use the standard CSP header with 'contentSecurityPolicy' but it would be nice to be able to instead use it in Report-Only mode. This would help many users to develop their CSP using web-tools before deploying it into their 'production' or (in my case as a home user) homelabs. \r\n\r\n### What did you expect to see?\r\n\r\ncontentSecurityPolicyReportOnly in available header middlewares. \r\n\r\n### Reference\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy-Report-Only\n", "patch": "diff --git a/docs/content/middlewares/http/headers.md b/docs/content/middlewares/http/headers.md\nindex d0cb636724..afeb6891f2 100644\n--- a/docs/content/middlewares/http/headers.md\n+++ b/docs/content/middlewares/http/headers.md\n@@ -394,6 +394,10 @@ This overrides the `BrowserXssFilter` option.\n \n The `contentSecurityPolicy` option allows the `Content-Security-Policy` header value to be set with a custom value.\n \n+### `contentSecurityPolicyReportOnly`\n+\n+The `contentSecurityPolicyReportOnly` option allows the `Content-Security-Policy-Report-Only` header value to be set with a custom value.\n+\n ### `publicKey`\n \n The `publicKey` implements HPKP to prevent MITM attacks with forged certificates.\ndiff --git a/docs/content/reference/dynamic-configuration/docker-labels.yml b/docs/content/reference/dynamic-configuration/docker-labels.yml\nindex 3199888948..a3e65baa2c 100644\n--- a/docs/content/reference/dynamic-configuration/docker-labels.yml\n+++ b/docs/content/reference/dynamic-configuration/docker-labels.yml\n@@ -55,6 +55,7 @@\n - \"traefik.http.middlewares.middleware12.headers.allowedhosts=foobar, foobar\"\n - \"traefik.http.middlewares.middleware12.headers.browserxssfilter=true\"\n - \"traefik.http.middlewares.middleware12.headers.contentsecuritypolicy=foobar\"\n+- \"traefik.http.middlewares.middleware12.headers.contentsecuritypolicyreportonly=foobar\"\n - \"traefik.http.middlewares.middleware12.headers.contenttypenosniff=true\"\n - \"traefik.http.middlewares.middleware12.headers.custombrowserxssvalue=foobar\"\n - \"traefik.http.middlewares.middleware12.headers.customframeoptionsvalue=foobar\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.toml b/docs/content/reference/dynamic-configuration/file.toml\nindex a42f9ba17d..cd0ffde605 100644\n--- a/docs/content/reference/dynamic-configuration/file.toml\n+++ b/docs/content/reference/dynamic-configuration/file.toml\n@@ -198,6 +198,7 @@\n         browserXssFilter = true\n         customBrowserXSSValue = \"foobar\"\n         contentSecurityPolicy = \"foobar\"\n+        contentSecurityPolicyReportOnly = \"foobar\"\n         publicKey = \"foobar\"\n         referrerPolicy = \"foobar\"\n         permissionsPolicy = \"foobar\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.yaml b/docs/content/reference/dynamic-configuration/file.yaml\nindex 6f675e626f..d0de416ee7 100644\n--- a/docs/content/reference/dynamic-configuration/file.yaml\n+++ b/docs/content/reference/dynamic-configuration/file.yaml\n@@ -242,6 +242,7 @@ http:\n         browserXssFilter: true\n         customBrowserXSSValue: foobar\n         contentSecurityPolicy: foobar\n+        contentSecurityPolicyReportOnly: foobar\n         publicKey: foobar\n         referrerPolicy: foobar\n         permissionsPolicy: foobar\ndiff --git a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\nindex fa2baaf1ee..761a8f9282 100644\n--- a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n+++ b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n@@ -1309,6 +1309,10 @@ spec:\n                     description: ContentSecurityPolicy defines the Content-Security-Policy\n                       header value.\n                     type: string\n+                  contentSecurityPolicyReportOnly:\n+                    description: ContentSecurityPolicyReportOnly defines the Content-Security-Policy-Report-Only\n+                      header value.\n+                    type: string\n                   contentTypeNosniff:\n                     description: ContentTypeNosniff defines whether to add the X-Content-Type-Options\n                       header with the nosniff value.\ndiff --git a/docs/content/reference/dynamic-configuration/kv-ref.md b/docs/content/reference/dynamic-configuration/kv-ref.md\nindex cd425b7108..10b9a8d002 100644\n--- a/docs/content/reference/dynamic-configuration/kv-ref.md\n+++ b/docs/content/reference/dynamic-configuration/kv-ref.md\n@@ -71,6 +71,7 @@ THIS FILE MUST NOT BE EDITED BY HAND\n | `traefik/http/middlewares/Middleware12/headers/allowedHosts/1` | `foobar` |\n | `traefik/http/middlewares/Middleware12/headers/browserXssFilter` | `true` |\n | `traefik/http/middlewares/Middleware12/headers/contentSecurityPolicy` | `foobar` |\n+| `traefik/http/middlewares/Middleware12/headers/contentSecurityPolicyReportOnly` | `foobar` |\n | `traefik/http/middlewares/Middleware12/headers/contentTypeNosniff` | `true` |\n | `traefik/http/middlewares/Middleware12/headers/customBrowserXSSValue` | `foobar` |\n | `traefik/http/middlewares/Middleware12/headers/customFrameOptionsValue` | `foobar` |\ndiff --git a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\nindex 72b7d69bd1..0d005e64df 100644\n--- a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n+++ b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n@@ -585,6 +585,10 @@ spec:\n                     description: ContentSecurityPolicy defines the Content-Security-Policy\n                       header value.\n                     type: string\n+                  contentSecurityPolicyReportOnly:\n+                    description: ContentSecurityPolicyReportOnly defines the Content-Security-Policy-Report-Only\n+                      header value.\n+                    type: string\n                   contentTypeNosniff:\n                     description: ContentTypeNosniff defines whether to add the X-Content-Type-Options\n                       header with the nosniff value.\ndiff --git a/integration/fixtures/k8s/01-traefik-crd.yml b/integration/fixtures/k8s/01-traefik-crd.yml\nindex fa2baaf1ee..761a8f9282 100644\n--- a/integration/fixtures/k8s/01-traefik-crd.yml\n+++ b/integration/fixtures/k8s/01-traefik-crd.yml\n@@ -1309,6 +1309,10 @@ spec:\n                     description: ContentSecurityPolicy defines the Content-Security-Policy\n                       header value.\n                     type: string\n+                  contentSecurityPolicyReportOnly:\n+                    description: ContentSecurityPolicyReportOnly defines the Content-Security-Policy-Report-Only\n+                      header value.\n+                    type: string\n                   contentTypeNosniff:\n                     description: ContentTypeNosniff defines whether to add the X-Content-Type-Options\n                       header with the nosniff value.\ndiff --git a/pkg/config/dynamic/fixtures/sample.toml b/pkg/config/dynamic/fixtures/sample.toml\nindex f6ab56cf15..c096381aa3 100644\n--- a/pkg/config/dynamic/fixtures/sample.toml\n+++ b/pkg/config/dynamic/fixtures/sample.toml\n@@ -330,6 +330,7 @@\n         browserXssFilter = true\n         customBrowserXSSValue = \"foobar\"\n         contentSecurityPolicy = \"foobar\"\n+        contentSecurityPolicyReportOnly = \"foobar\"\n         publicKey = \"foobar\"\n         referrerPolicy = \"foobar\"\n         isDevelopment = true\ndiff --git a/pkg/config/dynamic/middlewares.go b/pkg/config/dynamic/middlewares.go\nindex 568c7f46b6..4042ed3ebf 100644\n--- a/pkg/config/dynamic/middlewares.go\n+++ b/pkg/config/dynamic/middlewares.go\n@@ -313,6 +313,8 @@ type Headers struct {\n \tCustomBrowserXSSValue string `json:\"customBrowserXSSValue,omitempty\" toml:\"customBrowserXSSValue,omitempty\" yaml:\"customBrowserXSSValue,omitempty\"`\n \t// ContentSecurityPolicy defines the Content-Security-Policy header value.\n \tContentSecurityPolicy string `json:\"contentSecurityPolicy,omitempty\" toml:\"contentSecurityPolicy,omitempty\" yaml:\"contentSecurityPolicy,omitempty\"`\n+\t// ContentSecurityPolicyReportOnly defines the Content-Security-Policy-Report-Only header value.\n+\tContentSecurityPolicyReportOnly string `json:\"contentSecurityPolicyReportOnly,omitempty\" toml:\"contentSecurityPolicyReportOnly,omitempty\" yaml:\"contentSecurityPolicyReportOnly,omitempty\"`\n \t// PublicKey is the public key that implements HPKP to prevent MITM attacks with forged certificates.\n \tPublicKey string `json:\"publicKey,omitempty\" toml:\"publicKey,omitempty\" yaml:\"publicKey,omitempty\"`\n \t// ReferrerPolicy defines the Referrer-Policy header value.\n@@ -376,6 +378,7 @@ func (h *Headers) HasSecureHeadersDefined() bool {\n \t\th.BrowserXSSFilter ||\n \t\th.CustomBrowserXSSValue != \"\" ||\n \t\th.ContentSecurityPolicy != \"\" ||\n+\t\th.ContentSecurityPolicyReportOnly != \"\" ||\n \t\th.PublicKey != \"\" ||\n \t\th.ReferrerPolicy != \"\" ||\n \t\t(h.FeaturePolicy != nil && *h.FeaturePolicy != \"\") ||\ndiff --git a/pkg/middlewares/headers/secure.go b/pkg/middlewares/headers/secure.go\nindex 1766e63563..9769627d1c 100644\n--- a/pkg/middlewares/headers/secure.go\n+++ b/pkg/middlewares/headers/secure.go\n@@ -17,24 +17,25 @@ type secureHeader struct {\n // newSecure constructs a new secure instance with supplied options.\n func newSecure(next http.Handler, cfg dynamic.Headers, contextKey string) *secureHeader {\n \topt := secure.Options{\n-\t\tBrowserXssFilter:        cfg.BrowserXSSFilter,\n-\t\tContentTypeNosniff:      cfg.ContentTypeNosniff,\n-\t\tForceSTSHeader:          cfg.ForceSTSHeader,\n-\t\tFrameDeny:               cfg.FrameDeny,\n-\t\tIsDevelopment:           cfg.IsDevelopment,\n-\t\tSTSIncludeSubdomains:    cfg.STSIncludeSubdomains,\n-\t\tSTSPreload:              cfg.STSPreload,\n-\t\tContentSecurityPolicy:   cfg.ContentSecurityPolicy,\n-\t\tCustomBrowserXssValue:   cfg.CustomBrowserXSSValue,\n-\t\tCustomFrameOptionsValue: cfg.CustomFrameOptionsValue,\n-\t\tPublicKey:               cfg.PublicKey,\n-\t\tReferrerPolicy:          cfg.ReferrerPolicy,\n-\t\tAllowedHosts:            cfg.AllowedHosts,\n-\t\tHostsProxyHeaders:       cfg.HostsProxyHeaders,\n-\t\tSSLProxyHeaders:         cfg.SSLProxyHeaders,\n-\t\tSTSSeconds:              cfg.STSSeconds,\n-\t\tPermissionsPolicy:       cfg.PermissionsPolicy,\n-\t\tSecureContextKey:        contextKey,\n+\t\tBrowserXssFilter:                cfg.BrowserXSSFilter,\n+\t\tContentTypeNosniff:              cfg.ContentTypeNosniff,\n+\t\tForceSTSHeader:                  cfg.ForceSTSHeader,\n+\t\tFrameDeny:                       cfg.FrameDeny,\n+\t\tIsDevelopment:                   cfg.IsDevelopment,\n+\t\tSTSIncludeSubdomains:            cfg.STSIncludeSubdomains,\n+\t\tSTSPreload:                      cfg.STSPreload,\n+\t\tContentSecurityPolicy:           cfg.ContentSecurityPolicy,\n+\t\tContentSecurityPolicyReportOnly: cfg.ContentSecurityPolicyReportOnly,\n+\t\tCustomBrowserXssValue:           cfg.CustomBrowserXSSValue,\n+\t\tCustomFrameOptionsValue:         cfg.CustomFrameOptionsValue,\n+\t\tPublicKey:                       cfg.PublicKey,\n+\t\tReferrerPolicy:                  cfg.ReferrerPolicy,\n+\t\tAllowedHosts:                    cfg.AllowedHosts,\n+\t\tHostsProxyHeaders:               cfg.HostsProxyHeaders,\n+\t\tSSLProxyHeaders:                 cfg.SSLProxyHeaders,\n+\t\tSTSSeconds:                      cfg.STSSeconds,\n+\t\tPermissionsPolicy:               cfg.PermissionsPolicy,\n+\t\tSecureContextKey:                contextKey,\n \t}\n \n \treturn &secureHeader{\ndiff --git a/webui/src/components/_commons/PanelMiddlewares.vue b/webui/src/components/_commons/PanelMiddlewares.vue\nindex 98eff915a3..4921a0b549 100644\n--- a/webui/src/components/_commons/PanelMiddlewares.vue\n+++ b/webui/src/components/_commons/PanelMiddlewares.vue\n@@ -817,6 +817,22 @@\n               </div>\n             </div>\n           </q-card-section>\n+          <!-- EXTRA FIELDS FROM MIDDLEWARES - [headers] - contentSecurityPolicyReportOnly -->\n+          <q-card-section v-if=\"middleware.headers\">\n+            <div class=\"row items-start no-wrap\">\n+              <div class=\"col\">\n+                <div class=\"text-subtitle2\">\n+                  Content Security Policy (Report Only)\n+                </div>\n+                <q-chip\n+                  dense\n+                  class=\"app-chip app-chip-green\"\n+                >\n+                  {{ exData(middleware).contentSecurityPolicyReportOnly }}\n+                </q-chip>\n+              </div>\n+            </div>\n+          </q-card-section>\n           <!-- EXTRA FIELDS FROM MIDDLEWARES - [headers] - publicKey -->\n           <q-card-section v-if=\"middleware.headers\">\n             <div class=\"row items-start no-wrap\">\n", "instance_id": "traefik__traefik-10709", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add support for the `Content-Security-Policy-Report-Only` header as a feature in a middleware system, likely for a web proxy or server software like Traefik. The goal is explicitly stated: to enable a report-only mode for CSP headers to assist users in testing policies before enforcement. A reference to the Mozilla Developer Network documentation is provided, which adds clarity on the expected behavior of the header. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify how the report-only mode should interact with the existing `contentSecurityPolicy` header (e.g., whether they can coexist or if one overrides the other). Additionally, there are no explicit mentions of edge cases, constraints, or specific requirements for error handling. While the intent is clear, these missing details prevent it from being comprehensive, thus warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes span multiple files, including documentation, configuration schemas, and core logic in Go. However, the modifications are relatively straightforward and repetitive, primarily involving adding a new field (`contentSecurityPolicyReportOnly`) alongside the existing `contentSecurityPolicy` field in various configuration formats (TOML, YAML, Docker labels, Kubernetes CRDs) and updating the middleware logic to handle this new header. The changes do not significantly impact the system's architecture; they are additive and follow the existing pattern for header management. The amount of code change is moderate but not extensive, as it mostly involves copy-pasting or mirroring existing logic.\n\n2. **Number of Technical Concepts:** Solving this problem requires a basic understanding of HTTP headers, specifically Content Security Policy (CSP) and its report-only mode, which is a well-documented web standard. In terms of programming, it involves familiarity with Go structs, JSON/TOML/YAML serialization, and middleware design in a web server context (likely Traefik, given the file paths). Additionally, there is a minor UI update in a Vue.js component to display the new field. These concepts are not particularly complex for a developer with moderate experience in web development or middleware systems.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases or error handling requirements, and the code changes do not introduce significant new logic for handling errors. The primary concern might be ensuring that the report-only header does not conflict with the enforcement header, but this is not addressed in the changes and appears to be a straightforward addition. No complex validation or edge case handling is evident in the provided diff.\n\n4. **Overall Complexity:** The task requires understanding the existing codebase structure to place the new field correctly across configuration and logic files, but it does not demand deep architectural changes or advanced algorithms. It is a feature addition that mirrors existing functionality, making it relatively easy to implement for someone familiar with the codebase or similar systems.\n\nGiven these considerations, a difficulty score of 0.35 is appropriate, as it reflects a task that is slightly more involved than a very basic modification due to the multi-file changes and need for consistency across formats, but still falls within the Easy category due to the straightforward nature of the logic and concepts involved.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Panic on add-service-account Endpoint\nHi Minio Team\n\nI tried to create an access key using the following mc command:\n```\nmc idp ldap accesskey create s3 ${user}\nmc: <ERROR> Unable to add service account. We encountered an internal error, please try again.\n```\n\nServer log output:\n```\nERRO: panic: \"PUT /minio/admin/v3/idp/ldap/add-service-account\": runtime error: invalid memory address or nil pointer dereference\ngoroutine 1000 [running]:\nruntime/debug.Stack()\n        runtime/debug/stack.go:26 +0x5e\ngithub.com/minio/minio/cmd.serverMain.func10.setCriticalErrorHandler.2.1()\n        github.com/minio/minio/cmd/generic-handlers.go:583 +0x99\npanic({0x31155a0?, 0x74af810?})\n        runtime/panic.go:792 +0x132\ngithub.com/minio/minio/cmd.commonAddServiceAccount(_, _)\n        github.com/minio/minio/cmd/admin-handlers-users.go:2962 +0xcf7\ngithub.com/minio/minio/cmd.adminAPIHandlers.AddServiceAccountLDAP({}, {0x568f980, 0xc000ffd900}, 0xc001e38dc0)\n        github.com/minio/minio/cmd/admin-handlers-idp-ldap.go:193 +0xd1\nnet/http.HandlerFunc.ServeHTTP(0x60281f1?, {0x568f980?, 0xc000ffd900?}, 0xc001fdf980?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.adminMiddleware.func1.httpTraceHdrs.httpTrace.2({0x568f980, 0xc000ffd900}, 0xc001e38dc0)\n        github.com/minio/minio/cmd/http-tracer.go:190 +0xef\ngithub.com/minio/minio/cmd.adminMiddleware.func1({0x568f980, 0xc000ffd900}, 0xc001e38c80)\n        github.com/minio/minio/cmd/admin-router.go:123 +0x29d\nnet/http.HandlerFunc.ServeHTTP(0x74c0c90?, {0x568f980?, 0xc000ffd900?}, 0x4?)\n        net/http/server.go:2294 +0x29\ngithub.com/klauspost/compress/gzhttp.NewWrapper.func1.1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/klauspost/compress@v1.18.0/gzhttp/compress.go:519 +0x607\nnet/http.HandlerFunc.ServeHTTP(0xc00db2e270?, {0x568ef50?, 0xc00db32000?}, 0x1?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setBucketForwardingMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/generic-handlers.go:488 +0x382\nnet/http.HandlerFunc.ServeHTTP(0xc000d70740?, {0x568ef50?, 0xc00db32000?}, 0x0?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setUploadForwardingMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/generic-handlers.go:601 +0x23c\nnet/http.HandlerFunc.ServeHTTP(0x3648200?, {0x568ef50?, 0xc00db32000?}, 0x39?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setRequestValidityMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/generic-handlers.go:471 +0x16b8\nnet/http.HandlerFunc.ServeHTTP(0xc00db2e1e0?, {0x568ef50?, 0xc00db32000?}, 0x7ffffffe847768a2?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setRequestLimitMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/generic-handlers.go:137 +0x61c\nnet/http.HandlerFunc.ServeHTTP(0x0?, {0x568ef50?, 0xc00db32000?}, 0x2948ce8?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setCrossDomainPolicyMiddleware.func1({0x568ef50?, 0xc00db32000?}, 0x6?)\n        github.com/minio/minio/cmd/crossdomain-xml-handler.go:46 +0xc5\nnet/http.HandlerFunc.ServeHTTP(0xc001e38c80?, {0x568ef50?, 0xc00db32000?}, 0x0?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setBrowserRedirectMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/generic-handlers.go:167 +0x19c\nnet/http.HandlerFunc.ServeHTTP(0x0?, {0x568ef50?, 0xc00db32000?}, 0x31204a7c?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.setAuthMiddleware.func1({0x568ef50, 0xc00db32000}, 0xc001e38c80)\n        github.com/minio/minio/cmd/auth-handler.go:654 +0x66c\nnet/http.HandlerFunc.ServeHTTP(0x56971d0?, {0x568ef50?, 0xc00db32000?}, 0x5651d30?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.httpTracerMiddleware.func1({0x568f550, 0xc00daeeb60}, 0xc001e38b40)\n        github.com/minio/minio/cmd/http-tracer.go:89 +0x362\nnet/http.HandlerFunc.ServeHTTP(0x34aac80?, {0x568f550?, 0xc00daeeb60?}, 0xa?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.addCustomHeadersMiddleware.func1({0x568f550, 0xc00daeeb60}, 0xc001e38b40)\n        github.com/minio/minio/cmd/generic-handlers.go:566 +0x3f1\nnet/http.HandlerFunc.ServeHTTP(0xc001e38a00?, {0x568f550?, 0xc00daeeb60?}, 0xc00d8788d8?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/mux.(*Router).ServeHTTP(0xc000c543c0, {0x568f550, 0xc00daeeb60}, 0xc000c9f400)\n        github.com/minio/mux@v1.9.2/mux.go:228 +0x222\ngithub.com/minio/minio/cmd.corsHandler.(*Cors).Handler.func2({0x568f550, 0xc00daeeb60}, 0xc000c9f400)\n        github.com/rs/cors@v1.11.1/cors.go:289 +0x184\nnet/http.HandlerFunc.ServeHTTP(0xc1faab103119de42?, {0x568f550?, 0xc00daeeb60?}, 0x1000000408b32?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/cmd.serverMain.func10.setCriticalErrorHandler.2({0x568f550?, 0xc00daeeb60?}, 0x7566ac0?)\n        github.com/minio/minio/cmd/generic-handlers.go:590 +0x6c\nnet/http.HandlerFunc.ServeHTTP(0xc00d878b30?, {0x568f550?, 0xc00daeeb60?}, 0x6fc23ac00?)\n        net/http/server.go:2294 +0x29\ngithub.com/minio/minio/internal/http.(*Server).Init.func1({0x568f550?, 0xc00daeeb60?}, 0x1?)\n        github.com/minio/minio/internal/http/server.go:118 +0x23a\nnet/http.HandlerFunc.ServeHTTP(0x419fa5?, {0x568f550?, 0xc00daeeb60?}, 0xc00daeeb01?)\n        net/http/server.go:2294 +0x29\nnet/http.serverHandler.ServeHTTP({0x5685db8?}, {0x568f550?, 0xc00daeeb60?}, 0x6?)\n        net/http/server.go:3301 +0x8e\nnet/http.(*conn).serve(0xc00d5ad170, {0x56971d0, 0xc001e4d950})\n        net/http/server.go:2102 +0x625\ncreated by net/http.(*Server).Serve in goroutine 109\n        net/http/server.go:3454 +0x485\n```\n\nThis error was caused by my specific setup.\nThe username in LDAP (Active Directory) was too long for the `sAMAccountName` attribute.\n`CN` and `sAMAccountName` did not match.\n\n```\nuser_dn_search_filter: (&(objectCategory=user)(sAMAccountName=%s))\n```\n\n\n## Your Environment\nServer Version:\n```\nVersion: RELEASE.2025-04-22T22-12-26Z (go1.24.2 linux/amd64)\n```\n\nClient Version:\n```\nmc --version\nmc version RELEASE.2025-04-16T18-13-26Z (commit-id=b00526b153a31b36767991a4f5ce2cced435ee8e)\n```\n", "patch": "diff --git a/cmd/admin-handlers-users.go b/cmd/admin-handlers-users.go\nindex 8c0c744228334..b5831d3ec3d16 100644\n--- a/cmd/admin-handlers-users.go\n+++ b/cmd/admin-handlers-users.go\n@@ -2816,7 +2816,7 @@ func commonAddServiceAccount(r *http.Request, ldap bool) (context.Context, auth.\n \tdenyOnly := (targetUser == cred.AccessKey || targetUser == cred.ParentUser)\n \tif ldap && !denyOnly {\n \t\tres, _ := globalIAMSys.LDAPConfig.GetValidatedDNForUsername(targetUser)\n-\t\tif res.NormDN == cred.ParentUser {\n+\t\tif res != nil && res.NormDN == cred.ParentUser {\n \t\t\tdenyOnly = true\n \t\t}\n \t}\n", "instance_id": "minio__minio-21235", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a panic occurs in the MinIO server when attempting to create an access key using the `mc idp ldap accesskey create` command due to a specific LDAP setup where the `sAMAccountName` attribute is too long and does not match the `CN`. The statement includes relevant logs, environment details, and the root cause of the issue, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define the expected behavior or constraints for usernames in LDAP integration, nor does it mention specific edge cases beyond the long `sAMAccountName`. Additionally, while the server and client versions are provided, there is no mention of other environmental factors (e.g., specific LDAP server configurations) that might influence reproducibility. Overall, the statement is valid and clear but lacks some minor details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the following analysis of the factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal, involving a single line modification in `admin-handlers-users.go`. It adds a nil check (`res != nil`) before accessing `res.NormDN` to prevent a nil pointer dereference. This change is localized to a single function and does not impact the broader system architecture or require modifications across multiple files or modules. The amount of code change is trivial, making this a straightforward fix.\n\n2. **Number of Technical Concepts**: Solving this issue requires basic knowledge of Go programming, specifically understanding nil pointer dereferences and how to handle them with conditional checks. It also involves a minimal understanding of the MinIO codebase's LDAP integration logic to recognize why `res` might be nil. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic LDAP concepts are needed. The concepts involved are simple and commonly encountered in day-to-day programming.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement identifies a specific edge case (long `sAMAccountName` not matching `CN`), and the code change directly addresses the resulting nil pointer dereference. No additional edge cases or complex error handling logic are introduced or required beyond this fix. The error handling modification is straightforward, involving a simple nil check.\n\n4. **Overall Complexity**: The issue is a classic bug fix for a runtime panic caused by a nil pointer dereference. It does not require deep understanding of the MinIO codebase's architecture, interactions between modules, or performance considerations. The fix is isolated and does not have broader implications for the system. This aligns with an easy difficulty level, as it involves understanding minimal code logic and making a simple statement modification.\n\nIn summary, this problem is easy to solve due to the minimal scope of the change, the simplicity of the technical concepts involved, and the straightforward nature of the error handling required. A score of 0.25 reflects this as a task suitable for developers with basic to intermediate experience in Go.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "object attributes Checksum attribute is empty for SHA1 / SHA256 multi-part uploads\n## Expected Behavior\r\n\r\nIf I upload a multi-part object using --checksum-algorithm CRC32, CRC32C, SHA1, or SHA256 I expect to see a  top-level checksum that is calculated as a hash-of-hashes of the individual parts checksums.\r\n\r\nIf I then get the object attributes at a later point in time I expect to be able to get that same hash-of-hashes via the \"Checksum\" attribute.\r\n\r\n## Current Behavior\r\n\r\nIf I upload a multi-part object using aws-cli s3api I can see it producing a checksum (top level hash-of-hashes checksum) for all four algorithms CRC32, CRC32C, SHA1 and SHA256.  This is the same behavior as with AWS S3.\r\n\r\nIf I use get-object-attributes on the object afterwards I can only retrieve a top-level Checksum attribute for CRC32 and CRC32C uploads, for SHA1 and SHA256 the Checksum attribute is empty.  This behavior is different than what I see on S3, where all four algorithms behave the same way, producing the hash-of-hashes in the response body of complete-multipart-upload request, and producing the expected Checksum attribute on a subsequent get-object-attributes request.\r\n\r\nAn example using CRC32 works as expected:\r\n```\r\n$ ./testsuite/bin/test.multipart.sh --profile s3root_test --bucket test --key hello_world.txt --source ./hello_world.txt --algorithm CRC32C\r\naws --profile s3root_test s3api create-multipart-upload --bucket test --key hello_world.txt --checksum-algorithm CRC32C\r\n{\r\n    \"ChecksumAlgorithm\": \"CRC32C\",\r\n    \"Bucket\": \"test\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"UploadId\": \"MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5LmZkNWQ3ZGI3LTQwZjMtNDc0MS1iMDdmLTEzM2FhYjI1MTNlOXgxNzIzMDYxOTA5NDYyNzM2ODc1\"\r\n}\r\naws --profile s3root_test s3api upload-part --bucket test --key hello_world.txt --part-number 1 --upload-id MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5LmZkNWQ3ZGI3LTQwZjMtNDc0MS1iMDdmLTEzM2FhYjI1MTNlOXgxNzIzMDYxOTA5NDYyNzM2ODc1 --body ./hello_world.txt --checksum-algorithm CRC32C\r\n{\r\n    \"ETag\": \"\\\"bea8252ff4e80f41719ea13cdf007273\\\"\",\r\n    \"ChecksumCRC32C\": \"0s3l1A==\"\r\n}\r\naws --profile s3root_test s3api complete-multipart-upload --bucket test --key hello_world.txt --multipart-upload Parts=[{ETag=\"bea8252ff4e80f41719ea13cdf007273\",ChecksumCRC32C=0s3l1A==,PartNumber=1}] --upload-id MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5LmZkNWQ3ZGI3LTQwZjMtNDc0MS1iMDdmLTEzM2FhYjI1MTNlOXgxNzIzMDYxOTA5NDYyNzM2ODc1\r\n{\r\n    \"VersionId\": \"b20d4c61-aebe-4c9e-afb5-2b97d37a5f66\",\r\n    \"Location\": \"http://test.elm.stanford.edu:9000/test/hello_world.txt\",\r\n    \"Bucket\": \"test\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"ETag\": \"\\\"a9a83095955b3d3efacc7f833a103cb2-1\\\"\",\r\n    \"ChecksumCRC32C\": \"mtDgeA==-1\"\r\n}\r\naws --profile s3root_test s3api get-object-attributes --bucket test --key hello_world.txt --object-attributes ETag Checksum ObjectParts StorageClass ObjectSize\r\n{\r\n    \"LastModified\": \"2024-08-07T20:18:31+00:00\",\r\n    \"VersionId\": \"b20d4c61-aebe-4c9e-afb5-2b97d37a5f66\",\r\n    \"ETag\": \"a9a83095955b3d3efacc7f833a103cb2-1\",\r\n    \"Checksum\": {\r\n        \"ChecksumCRC32C\": \"mtDgeA==\"\r\n    },\r\n    \"ObjectParts\": {\r\n        \"TotalPartsCount\": 1,\r\n        \"PartNumberMarker\": 0,\r\n        \"NextPartNumberMarker\": 1,\r\n        \"MaxParts\": 10000,\r\n        \"IsTruncated\": false,\r\n        \"Parts\": [\r\n            {\r\n                \"PartNumber\": 1,\r\n                \"Size\": 14,\r\n                \"ChecksumCRC32C\": \"0s3l1A==\"\r\n            }\r\n        ]\r\n    },\r\n    \"StorageClass\": \"STANDARD\",\r\n    \"ObjectSize\": 14\r\n}\r\n```\r\n\r\nAn example using SHA1 reveals an empty Checksum attribute in the get-object-attributes response:\r\n```\r\n$ ./testsuite/bin/test.multipart.sh --profile s3root_test --bucket test --key hello_world.txt --source ./hello_world.txt --algorithm SHA1\r\naws --profile s3root_test s3api create-multipart-upload --bucket test --key hello_world.txt --checksum-algorithm SHA1\r\n{\r\n    \"ChecksumAlgorithm\": \"SHA1\",\r\n    \"Bucket\": \"test\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"UploadId\": \"MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5Ljc1ZmYzYzU2LWQ3YmQtNGU5Yi04YzE2LTNiNjJjNWRkMTIzY3gxNzIzMDYyNjQ5MzI5NjI3MTY3\"\r\n}\r\naws --profile s3root_test s3api upload-part --bucket test --key hello_world.txt --part-number 1 --upload-id MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5Ljc1ZmYzYzU2LWQ3YmQtNGU5Yi04YzE2LTNiNjJjNWRkMTIzY3gxNzIzMDYyNjQ5MzI5NjI3MTY3 --body ./hello_world.txt --checksum-algorithm SHA1\r\n{\r\n    \"ETag\": \"\\\"bea8252ff4e80f41719ea13cdf007273\\\"\",\r\n    \"ChecksumSHA1\": \"YP3pwjELDUytTauNEmsEOH77ook=\"\r\n}\r\naws --profile s3root_test s3api complete-multipart-upload --bucket test --key hello_world.txt --multipart-upload Parts=[{ETag=\"bea8252ff4e80f41719ea13cdf007273\",ChecksumSHA1=YP3pwjELDUytTauNEmsEOH77ook=,PartNumber=1}] --upload-id MmRmYWZkY2UtZDkyNS00ZjFiLTg5YzgtNDkzZDFhN2Y0MGE5Ljc1ZmYzYzU2LWQ3YmQtNGU5Yi04YzE2LTNiNjJjNWRkMTIzY3gxNzIzMDYyNjQ5MzI5NjI3MTY3\r\n{\r\n    \"VersionId\": \"2d219002-7c1f-4924-9671-830e8dc71b83\",\r\n    \"Location\": \"http://test.elm.stanford.edu:9000/test/hello_world.txt\",\r\n    \"Bucket\": \"test\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"ETag\": \"\\\"a9a83095955b3d3efacc7f833a103cb2-1\\\"\",\r\n    \"ChecksumSHA1\": \"On8NBgbepwbfKOmCUftizAIGBuA=-1\"\r\n}\r\naws --profile s3root_test s3api get-object-attributes --bucket test --key hello_world.txt --object-attributes ETag Checksum ObjectParts StorageClass ObjectSize\r\n{\r\n    \"LastModified\": \"2024-08-07T20:30:50+00:00\",\r\n    \"VersionId\": \"2d219002-7c1f-4924-9671-830e8dc71b83\",\r\n    \"ETag\": \"a9a83095955b3d3efacc7f833a103cb2-1\",\r\n    \"Checksum\": {},\r\n    \"ObjectParts\": {\r\n        \"TotalPartsCount\": 1,\r\n        \"PartNumberMarker\": 0,\r\n        \"NextPartNumberMarker\": 1,\r\n        \"MaxParts\": 10000,\r\n        \"IsTruncated\": false,\r\n        \"Parts\": [\r\n            {\r\n                \"PartNumber\": 1,\r\n                \"Size\": 14,\r\n                \"ChecksumSHA1\": \"YP3pwjELDUytTauNEmsEOH77ook=\"\r\n            }\r\n        ]\r\n    },\r\n    \"StorageClass\": \"STANDARD\",\r\n    \"ObjectSize\": 14\r\n}\r\n```\r\n\r\nIf I run the SHA1 test against AWS S3 I get back a Checksum attribute.\r\n```\r\n$ ./test.multipart.sh --profile srcc-jrobinso --bucket test-jrobinso --key hello_world.txt --source hello_world.txt --algorithm SHA1\r\naws --profile srcc-jrobinso s3api create-multipart-upload --bucket test-jrobinso --key hello_world.txt --checksum-algorithm SHA1\r\n{\r\n    \"ServerSideEncryption\": \"AES256\",\r\n    \"ChecksumAlgorithm\": \"SHA1\",\r\n    \"Bucket\": \"test-jrobinso\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"UploadId\": \"qG64yr_N.VNs_SL_jHCY4Uj7j7DyUupiDzrR5DlARRIGTiUKfDLebTokJpLGKICMkYFFPdvHTWDpKRN9iV.z6Kw3FNfeFarFx.JdlL1Y0jli0arLrjKDNGakBdu2X59q\"\r\n}\r\naws --profile srcc-jrobinso s3api upload-part --bucket test-jrobinso --key hello_world.txt --part-number 1 --upload-id qG64yr_N.VNs_SL_jHCY4Uj7j7DyUupiDzrR5DlARRIGTiUKfDLebTokJpLGKICMkYFFPdvHTWDpKRN9iV.z6Kw3FNfeFarFx.JdlL1Y0jli0arLrjKDNGakBdu2X59q --body hello_world.txt --checksum-algorithm SHA1\r\n{\r\n    \"ServerSideEncryption\": \"AES256\",\r\n    \"ETag\": \"\\\"bea8252ff4e80f41719ea13cdf007273\\\"\",\r\n    \"ChecksumSHA1\": \"YP3pwjELDUytTauNEmsEOH77ook=\"\r\n}\r\naws --profile srcc-jrobinso s3api complete-multipart-upload --bucket test-jrobinso --key hello_world.txt --multipart-upload Parts=[{ETag=\"bea8252ff4e80f41719ea13cdf007273\",ChecksumSHA1=YP3pwjELDUytTauNEmsEOH77ook=,PartNumber=1}] --upload-id qG64yr_N.VNs_SL_jHCY4Uj7j7DyUupiDzrR5DlARRIGTiUKfDLebTokJpLGKICMkYFFPdvHTWDpKRN9iV.z6Kw3FNfeFarFx.JdlL1Y0jli0arLrjKDNGakBdu2X59q\r\n{\r\n    \"ServerSideEncryption\": \"AES256\",\r\n    \"Location\": \"https://test-jrobinso.s3.us-west-2.amazonaws.com/hello_world.txt\",\r\n    \"Bucket\": \"test-jrobinso\",\r\n    \"Key\": \"hello_world.txt\",\r\n    \"ETag\": \"\\\"a9a83095955b3d3efacc7f833a103cb2-1\\\"\",\r\n    \"ChecksumSHA1\": \"On8NBgbepwbfKOmCUftizAIGBuA=-1\"\r\n}\r\naws --profile srcc-jrobinso s3api get-object-attributes --bucket test-jrobinso --key hello_world.txt --object-attributes ETag Checksum ObjectParts StorageClass ObjectSize\r\n{\r\n    \"LastModified\": \"2024-08-07T20:35:41+00:00\",\r\n    \"ETag\": \"a9a83095955b3d3efacc7f833a103cb2-1\",\r\n    \"Checksum\": {\r\n        \"ChecksumSHA1\": \"On8NBgbepwbfKOmCUftizAIGBuA=\"\r\n    },\r\n    \"ObjectParts\": {\r\n        \"TotalPartsCount\": 1,\r\n        \"PartNumberMarker\": 0,\r\n        \"NextPartNumberMarker\": 1,\r\n        \"MaxParts\": 1000,\r\n        \"IsTruncated\": false,\r\n        \"Parts\": [\r\n            {\r\n                \"PartNumber\": 1,\r\n                \"Size\": 14,\r\n                \"ChecksumSHA1\": \"YP3pwjELDUytTauNEmsEOH77ook=\"\r\n            }\r\n        ]\r\n    },\r\n    \"StorageClass\": \"STANDARD\",\r\n    \"ObjectSize\": 14\r\n}\r\n```\r\n\r\n## Possible Solution\r\n\r\nIt's unclear to me if this is a client usage problem where MinIO is expecting some additional metadata to be set that AWS S3 is not requiring, if MinIO is failing to store the Checksum for SHA1 / SHA256, is failing to calculate the hash-of-hashes dynamically during get-object-attributes, or if the Checksum is being stored but is simply not being returned.\r\n\r\n## Steps to Reproduce (for bugs)\r\n\r\n- use s3api create-multipart-upload / upload-part / complete-multipart-upload to upload a single part file using CRC32\r\n- use s3api get-object-attributes to get the Checksum attribute\r\n- use s3api create-multipart-upload / upload-part / complete-multipart-upload to upload a single part file using SHA1\r\n- use s3api get-object-attributes to get the Checksum attribute\r\n\r\nFor the CRC32 the Checksum attribute is filled in, for SHA1 it is not.\r\n\r\nAn example script that automates this sequence is available in:\r\n\r\nhttps://github.com/jimrobinson/test-multipart/blob/main/test.multipart.sh\r\n\r\nIt requires bash, aws-cli, jq, and tee.  Its usage is given in the examples under the Current Behavior section.\r\n\r\n## Context\r\n\r\nBeing able to validate the expected hash-of-hashes of the individual parts is helpful when building tools to confirm that the uploaded data is the same as what we expect.  While the hash-of-hashes *is* being made available once in the response for the complete-multipart-upload call, it'd be nice to be able to refer back to that value later via get-object-attributes requests.\r\n\r\n## Regression\r\n\r\nUnknown\r\n\r\n## Your Environment\r\n* minio version RELEASE.2024-07-31T05-46-26Z (commit-id=a9dc061d847277e30c5c6918d6de6f0606f9d285)\r\n* Linux elm-test-minio-n0 6.9.12-200.fc40.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Jul 27 15:56:15 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n* aws-cli/2.17.13 Python/3.11.9 Linux/6.9.12-200.fc40.x86_64 exe/x86_64.rocky.9\n", "patch": "diff --git a/cmd/object-handlers.go b/cmd/object-handlers.go\nindex 16b32bf3eda66..4ff2dcc0a50c3 100644\n--- a/cmd/object-handlers.go\n+++ b/cmd/object-handlers.go\n@@ -762,21 +762,14 @@ func (api objectAPIHandlers) getObjectAttributesHandler(ctx context.Context, obj\n \tif _, ok := opts.ObjectAttributes[xhttp.Checksum]; ok {\n \t\tchkSums := objInfo.decryptChecksums(0, r.Header)\n \t\t// AWS does not appear to append part number on this API call.\n-\t\tswitch {\n-\t\tcase chkSums[\"CRC32\"] != \"\":\n-\t\t\tOA.Checksum = new(objectAttributesChecksum)\n-\t\t\tOA.Checksum.ChecksumCRC32 = strings.Split(chkSums[\"CRC32\"], \"-\")[0]\n-\t\tcase chkSums[\"CRC32C\"] != \"\":\n-\t\t\tOA.Checksum = new(objectAttributesChecksum)\n-\t\t\tOA.Checksum.ChecksumCRC32C = strings.Split(chkSums[\"CRC32C\"], \"-\")[0]\n-\t\tcase chkSums[\"SHA256\"] != \"\":\n-\t\t\tOA.Checksum = new(objectAttributesChecksum)\n-\t\t\tOA.Checksum.ChecksumSHA1 = strings.Split(chkSums[\"SHA1\"], \"-\")[0]\n-\t\tcase chkSums[\"SHA1\"] != \"\":\n-\t\t\tOA.Checksum = new(objectAttributesChecksum)\n-\t\t\tOA.Checksum.ChecksumSHA256 = strings.Split(chkSums[\"SHA256\"], \"-\")[0]\n+\t\tif len(chkSums) > 0 {\n+\t\t\tOA.Checksum = &objectAttributesChecksum{\n+\t\t\t\tChecksumCRC32:  strings.Split(chkSums[\"CRC32\"], \"-\")[0],\n+\t\t\t\tChecksumCRC32C: strings.Split(chkSums[\"CRC32C\"], \"-\")[0],\n+\t\t\t\tChecksumSHA1:   strings.Split(chkSums[\"SHA1\"], \"-\")[0],\n+\t\t\t\tChecksumSHA256: strings.Split(chkSums[\"SHA256\"], \"-\")[0],\n+\t\t\t}\n \t\t}\n-\n \t}\n \n \tif _, ok := opts.ObjectAttributes[xhttp.ETag]; ok {\n", "instance_id": "minio__minio-20231", "clarity": 3, "difficulty": 0.3, "clarity_explanation": "The problem statement is comprehensive and well-documented. It clearly outlines the expected behavior, current behavior, and the discrepancy between the two for multi-part uploads using different checksum algorithms (CRC32, CRC32C, SHA1, SHA256) in a MinIO environment compared to AWS S3. The statement includes detailed examples with command-line outputs for both working (CRC32/CRC32C) and non-working (SHA1/SHA256) cases, which effectively illustrate the issue. Steps to reproduce are provided, along with a reference to a test script for automation. The context of why this issue matters is also explained, and there are no significant ambiguities in the problem description. While some minor details (like specific edge cases beyond the provided examples) could be elaborated, the overall clarity is excellent, warranting a score of 3.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is localized to a single file (`object-handlers.go`) and specifically to the `getObjectAttributesHandler` function. The modification involves a small refactoring of how checksums are assigned to the `OA.Checksum` struct, replacing a switch-case with a direct assignment for all checksum types. The change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is minimal, focusing on restructuring a few lines.\n\n2. **Number of Technical Concepts:** Solving this issue requires a basic understanding of Go programming, specifically struct initialization and string manipulation (splitting strings to remove part numbers from checksum values). Familiarity with the MinIO codebase and AWS S3 API behavior is helpful but not deeply complex in this context, as the issue is straightforward\u2014ensuring all checksum types (CRC32, CRC32C, SHA1, SHA256) are returned in the `get-object-attributes` response. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic S3 API semantics are required.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement and code changes do not explicitly address complex edge cases beyond the core issue of missing checksums for SHA1/SHA256. The provided fix assumes checksum values are available in the `chkSums` map and handles them uniformly, which might need validation for cases where checksums are missing or malformed. However, the error handling requirements appear minimal, as the change does not introduce new logic prone to errors beyond what already exists in the codebase.\n\n4. **Overall Complexity:** The issue is a bug fix rather than a feature addition or architectural change. The root cause appears to be a logic error in how checksums are selectively assigned based on type, and the fix is a simple adjustment to ensure all types are handled. This does not require deep dives into the codebase or extensive debugging beyond understanding the `getObjectAttributesHandler` function's purpose.\n\nGiven these factors, a difficulty score of 0.30 is appropriate. It reflects a problem that is easy to solve with basic code modifications and a moderate understanding of the relevant code logic, without significant complexity or impact on the broader system.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Syslog entry: \"superfluous response.WriteHeader call\" in Docker 25.x\n### Description\n\nWhen retrieving the logs for a container (via `docker logs` for example), a log entry is made in `/var/log/syslog`:\r\n\r\n```\r\nFeb 26 16:54:53 be-docker dockerd[410]: 2024/02/26 16:54:53 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)\r\n```\r\n\r\nThis appears to have been introduced alongside the OpenTelemetry addition in Docker 25.0.0 as this log message is not created on previous versions of Docker (for example 24.0.6).\n\n### Reproduce\n\n1. Install Docker 25.x\r\n2. Create a container (any will do)\r\n3. Monitor `/var/log/syslog` (via `tail -f` for example)\r\n4. While monitoring, run `docker logs containername` \r\n5. Note the entry in syslog\n\n### Expected behavior\n\nNo \"superfluous response\" message.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:14:17 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:17 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 7\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 33\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: gelf\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.4.0-171-generic\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.925GiB\r\n Name: be-docker\r\n ID: HJDW:MIDK:VVF2:ZZC5:CCHA:6KOE:CCZU:TXJ3:2YUM:2KHI:NKJX:7ID3\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: jcarppe\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\n```\n\n\n### Additional Info\n\nWhile a single log line entry doesn't seem like much, when using a separate tool (such as Portainer) to view container logs that auto-refreshes, this can result in repeated syslog entries.\n", "patch": "diff --git a/api/server/httputils/write_log_stream.go b/api/server/httputils/write_log_stream.go\nindex 8faacc029eb13..a2086b5276854 100644\n--- a/api/server/httputils/write_log_stream.go\n+++ b/api/server/httputils/write_log_stream.go\n@@ -4,6 +4,7 @@ import (\n \t\"context\"\n \t\"fmt\"\n \t\"io\"\n+\t\"net/http\"\n \t\"net/url\"\n \t\"sort\"\n \n@@ -16,7 +17,11 @@ import (\n \n // WriteLogStream writes an encoded byte stream of log messages from the\n // messages channel, multiplexing them with a stdcopy.Writer if mux is true\n-func WriteLogStream(_ context.Context, w io.Writer, msgs <-chan *backend.LogMessage, config *container.LogsOptions, mux bool) {\n+func WriteLogStream(_ context.Context, w http.ResponseWriter, msgs <-chan *backend.LogMessage, config *container.LogsOptions, mux bool) {\n+\t// See https://github.com/moby/moby/issues/47448\n+\t// Trigger headers to be written immediately.\n+\tw.WriteHeader(http.StatusOK)\n+\n \twf := ioutils.NewWriteFlusher(w)\n \tdefer wf.Close()\n \ndiff --git a/api/server/router/container/container_routes.go b/api/server/router/container/container_routes.go\nindex 3872028652f86..c33d3d29ffe57 100644\n--- a/api/server/router/container/container_routes.go\n+++ b/api/server/router/container/container_routes.go\n@@ -112,9 +112,18 @@ func (s *containerRouter) getContainersStats(ctx context.Context, w http.Respons\n \t}\n \n \treturn s.backend.ContainerStats(ctx, vars[\"name\"], &backend.ContainerStatsConfig{\n-\t\tStream:    stream,\n-\t\tOneShot:   oneShot,\n-\t\tOutStream: w,\n+\t\tStream:  stream,\n+\t\tOneShot: oneShot,\n+\t\tOutStream: func() io.Writer {\n+\t\t\t// Assume that when this is called the request is OK.\n+\t\t\tw.WriteHeader(http.StatusOK)\n+\t\t\tif !stream {\n+\t\t\t\treturn w\n+\t\t\t}\n+\t\t\twf := ioutils.NewWriteFlusher(w)\n+\t\t\twf.Flush()\n+\t\t\treturn wf\n+\t\t},\n \t})\n }\n \ndiff --git a/api/server/router/system/system_routes.go b/api/server/router/system/system_routes.go\nindex 1dd50d32315b1..fca29cb07e515 100644\n--- a/api/server/router/system/system_routes.go\n+++ b/api/server/router/system/system_routes.go\n@@ -263,6 +263,7 @@ func (s *systemRouter) getEvents(ctx context.Context, w http.ResponseWriter, r *\n \t}\n \n \tw.Header().Set(\"Content-Type\", \"application/json\")\n+\tw.WriteHeader(http.StatusOK)\n \toutput := ioutils.NewWriteFlusher(w)\n \tdefer output.Close()\n \toutput.Flush()\ndiff --git a/api/types/backend/backend.go b/api/types/backend/backend.go\nindex e4e760905d01c..b18a265dcc79f 100644\n--- a/api/types/backend/backend.go\n+++ b/api/types/backend/backend.go\n@@ -89,7 +89,7 @@ type LogSelector struct {\n type ContainerStatsConfig struct {\n \tStream    bool\n \tOneShot   bool\n-\tOutStream io.Writer\n+\tOutStream func() io.Writer\n }\n \n // ExecInspect holds information about a running process started\ndiff --git a/daemon/stats.go b/daemon/stats.go\nindex 5dcd6121d4400..314234479991d 100644\n--- a/daemon/stats.go\n+++ b/daemon/stats.go\n@@ -12,7 +12,6 @@ import (\n \t\"github.com/docker/docker/api/types/backend\"\n \t\"github.com/docker/docker/container\"\n \t\"github.com/docker/docker/errdefs\"\n-\t\"github.com/docker/docker/pkg/ioutils\"\n )\n \n // ContainerStats writes information about the container to the stream\n@@ -27,9 +26,11 @@ func (daemon *Daemon) ContainerStats(ctx context.Context, prefixOrName string, c\n \t\treturn errdefs.InvalidParameter(errors.New(\"cannot have stream=true and one-shot=true\"))\n \t}\n \n+\tenc := json.NewEncoder(config.OutStream())\n+\n \t// If the container is either not running or restarting and requires no stream, return an empty stats.\n \tif (!ctr.IsRunning() || ctr.IsRestarting()) && !config.Stream {\n-\t\treturn json.NewEncoder(config.OutStream).Encode(&types.StatsJSON{\n+\t\treturn enc.Encode(&types.StatsJSON{\n \t\t\tName: ctr.Name,\n \t\t\tID:   ctr.ID,\n \t\t})\n@@ -41,15 +42,7 @@ func (daemon *Daemon) ContainerStats(ctx context.Context, prefixOrName string, c\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n-\t\treturn json.NewEncoder(config.OutStream).Encode(stats)\n-\t}\n-\n-\toutStream := config.OutStream\n-\tif config.Stream {\n-\t\twf := ioutils.NewWriteFlusher(outStream)\n-\t\tdefer wf.Close()\n-\t\twf.Flush()\n-\t\toutStream = wf\n+\t\treturn enc.Encode(stats)\n \t}\n \n \tvar preCPUStats types.CPUStats\n@@ -65,12 +58,11 @@ func (daemon *Daemon) ContainerStats(ctx context.Context, prefixOrName string, c\n \t\treturn &ss\n \t}\n \n-\tenc := json.NewEncoder(outStream)\n-\n \tupdates := daemon.subscribeToContainerStats(ctr)\n \tdefer daemon.unsubscribeToContainerStats(ctr, updates)\n \n \tnoStreamFirstFrame := !config.OneShot\n+\n \tfor {\n \t\tselect {\n \t\tcase v, ok := <-updates:\n", "instance_id": "moby__moby-47796", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "No explanation provided", "difficulty_explanation": "The difficulty of solving this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (`write_log_stream.go`, `container_routes.go`, `system_routes.go`, `backend.go`, and `stats.go`), indicating a need to understand and modify interactions across different parts of the Docker codebase. The changes primarily focus on adjusting how HTTP response headers are written and flushed to avoid the \"superfluous response.WriteHeader call\" warning, which requires a moderate understanding of Go's `net/http` package and Docker's internal handling of HTTP responses and streaming.\n\nThe technical concepts involved include familiarity with Go's HTTP handling (`http.ResponseWriter`, header writing), streaming data handling (`ioutils.NewWriteFlusher`), and JSON encoding for API responses. Additionally, since the issue is tied to OpenTelemetry instrumentation (as hinted in the syslog message), there might be a need to understand how telemetry hooks interact with HTTP responses, though the provided code changes do not directly address this aspect. The problem does not seem to require advanced algorithms or design patterns, but it does demand a careful approach to ensure that modifying response writing does not introduce regressions in streaming behavior or API responses.\n\nRegarding edge cases and error handling, the problem statement does not explicitly mention specific scenarios to handle, but the code changes suggest potential risks, such as ensuring that early `WriteHeader` calls do not interfere with error responses or other status codes that might need to be set later. The impact on the system's architecture appears limited, as the changes are mostly localized to HTTP response handling and do not seem to affect core Docker functionality. However, given that Docker is a complex system with high reliability requirements, even seemingly small changes in HTTP handling could have subtle downstream effects, necessitating thorough testing.\n\nOverall, this problem requires a moderate level of expertise in Go, an understanding of HTTP response handling, and familiarity with Docker's codebase structure. It is not overly complex but involves enough moving parts and potential pitfalls to classify it as a medium-difficulty task.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "alist\u7684WebDAV\u670d\u52a1\u534f\u8bae\u4e2d\u80fd\u5426\u6dfb\u52a0alist\u7684hashinfo\uff0c\u4f7f\u7528rclone\u5c31\u53ef\u4ee5\u589e\u52a0alist\u7684WebDAV\u7684\u81ea\u5b9a\u4e49\u652f\u6301\u4e86\n### Please make sure of the following things\r\n\r\n- [ ] I have read the [documentation](https://alist.nn.ci).\r\n- [ ] I'm sure there are no duplicate issues or discussions.\r\n- [ ] I'm sure this feature is not implemented.\r\n- [ ] I'm sure it's a reasonable and popular requirement.\r\n\r\n### Description of the feature / \u9700\u6c42\u63cf\u8ff0\r\n\r\nalist\u7684api\u4e2d\u5b58\u5728hashinfo\uff0c\u4f46WebDAV\u4f20\u9012\u4e0d\u4e86\u4f8b\u5982md5\u7684\u5185\u5bb9\uff0c\u5bfc\u81f4\u4e0b\u8f7d\u540e\u65e0\u6cd5\u8fdb\u884c\u6821\u9a8c\uff0c\u80fd\u5426\u5728\u539f\u6709WebDAV\u534f\u8bae\u57fa\u7840\u4e0a\u8fdb\u884c\u589e\u5f3a\uff0c\u6dfb\u52a0alist\u7684\u76f8\u5173\u5c5e\u6027\u4f8b\u5982hashinfo\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528checksum\u68c0\u67e5\u6587\u4ef6\u4e86\r\n\r\n### Suggested solution / \u5b9e\u73b0\u601d\u8def\r\n\r\n\u5b9e\u73b0\u601d\u8def\u53c2\u8003[https://github.com/rclone/rclone/issues/2379]\uff0c\u53ef\u4ee5\u5b9e\u73b0alist\u7684\u81ea\u5b9a\u4e49\u7684\u6807\u7b7e\uff0c\u4e5f\u53ef\u4ee5\u505a\u517c\u5bb9\uff0c\u76f8\u5173\u8ba8\u8bba[https://central.owncloud.org/t/reading-hashes-md5-or-sha1-via-webdav/14348/3]\uff0c\u4ee5\u4e0b\u65b9\u6848\u53c2\u8003[https://github.com/rclone/rclone/issues/3147]\u3002\r\n\u4e3a\u4e86\u517c\u5bb9\u6027\u53ef\u4ee5\u8fd9\u6837\u5904\u7406\uff0c\u666e\u901a\u5c5e\u6027\u67e5\u8be2\u8bf7\u6c42\u4f8b\u5982\uff1a\r\n```\r\n<?xml version=\"1.0\" ?>\r\n<D:propfind xmlns:D=\"DAV:\">\r\n <D:allprop/>\r\n</D:propfind>\r\n```\r\n\u4e0d\u8fd4\u56de\u81ea\u5b9a\u4e49\u5c5e\u6027\uff0c\u4f7f\u7528\u6807\u51c6\u7684WebDAV\u534f\u8bae\uff0c\u9700\u8981\u6307\u5b9a\u81ea\u5b9a\u4e49\u5c5e\u6027\u624d\u8fd4\u56de\u81ea\u5b9a\u4e49\u5c5e\u6027\r\n```\r\n<?xml version=\"1.0\"?>\r\n<d:propfind  xmlns:d=\"DAV:\" xmlns:al=\"http://alist.nn.ci/ns\">\r\n <d:prop>\r\n  <d:displayname />\r\n  <d:getlastmodified />\r\n  <d:getcontentlength />\r\n  <d:resourcetype />\r\n  <d:getcontenttype />\r\n  <al:checksums />\r\n </d:prop>\r\n</d:propfind>\r\n```\r\n\u6216\u8005\r\n```\r\n<?xml version=\"1.0\"?>\r\n<d:propfind  xmlns:d=\"DAV:\" xmlns:al=\"http://alist.nn.ci/ns\">\r\n <d:prop>\r\n  <d:displayname />\r\n  <d:getlastmodified />\r\n  <d:getcontentlength />\r\n  <d:resourcetype />\r\n  <d:getcontenttype />\r\n  <al:hashinfo />\r\n </d:prop>\r\n</d:propfind>\r\n```\r\n\u8fd4\u56de\u5185\u5bb9\u589e\u52a0\r\n`<al:checksums><al:checksum>SHA1:f572d396fae9206628714fb2ce00f72e94f2258f MD5:b1946ac92492d2347c6235b4d2611184 ADLER32:084b021f</al:checksum></al:checksums>`\r\n\u6216\u8005\r\n`<al:hashinfo><al:md5>b1946ac92492d2347c6235b4d2611184</al:md5></al:hashinfo>`\r\n\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6821\u9a8c\uff0c\u51cf\u5c11\u4f20\u8f93\u9519\u8bef\u5bfc\u81f4\u7684\u6587\u4ef6\u9519\u8bef\r\n\r\n### Additional context / \u9644\u4ef6\r\n\r\n_No response_\n", "patch": "diff --git a/pkg/utils/hash.go b/pkg/utils/hash.go\nindex fa06bcc24c2..a281dd4e6e5 100644\n--- a/pkg/utils/hash.go\n+++ b/pkg/utils/hash.go\n@@ -10,6 +10,7 @@ import (\n \t\"errors\"\n \t\"hash\"\n \t\"io\"\n+\t\"iter\"\n \n \t\"github.com/alist-org/alist/v3/internal/errs\"\n \tlog \"github.com/sirupsen/logrus\"\n@@ -226,3 +227,13 @@ func (hi HashInfo) GetHash(ht *HashType) string {\n func (hi HashInfo) Export() map[*HashType]string {\n \treturn hi.h\n }\n+\n+func (hi HashInfo) All() iter.Seq2[*HashType, string] {\n+\treturn func(yield func(*HashType, string) bool) {\n+\t\tfor hashType, hashValue := range hi.h {\n+\t\t\tif !yield(hashType, hashValue) {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t}\n+}\ndiff --git a/server/webdav/prop.go b/server/webdav/prop.go\nindex 5e053af4b1a..a81f31b05c7 100644\n--- a/server/webdav/prop.go\n+++ b/server/webdav/prop.go\n@@ -9,6 +9,7 @@ import (\n \t\"context\"\n \t\"encoding/xml\"\n \t\"errors\"\n+\t\"fmt\"\n \t\"mime\"\n \t\"net/http\"\n \t\"path\"\n@@ -101,7 +102,7 @@ type DeadPropsHolder interface {\n \tPatch([]Proppatch) ([]Propstat, error)\n }\n \n-// liveProps contains all supported, protected DAV: properties.\n+// liveProps contains all supported properties.\n var liveProps = map[xml.Name]struct {\n \t// findFn implements the propfind function of this property. If nil,\n \t// it indicates a hidden property.\n@@ -160,6 +161,10 @@ var liveProps = map[xml.Name]struct {\n \t\tfindFn: findSupportedLock,\n \t\tdir:    true,\n \t},\n+\t{Space: \"http://owncloud.org/ns\", Local: \"checksums\"}: {\n+\t\tfindFn: findChecksums,\n+\t\tdir:    false,\n+\t},\n }\n \n // TODO(nigeltao) merge props and allprop?\n@@ -483,3 +488,11 @@ func findSupportedLock(ctx context.Context, ls LockSystem, name string, fi model\n \t\t`<D:locktype><D:write/></D:locktype>` +\n \t\t`</D:lockentry>`, nil\n }\n+\n+func findChecksums(ctx context.Context, ls LockSystem, name string, fi model.Obj) (string, error) {\n+\tchecksums := \"\"\n+\tfor hashType, hashValue := range fi.GetHash().All() {\n+\t\tchecksums += fmt.Sprintf(\"<checksum>%s:%s</checksum>\", hashType.Name, hashValue)\n+\t}\n+\treturn checksums, nil\n+}\n", "instance_id": "AlistGo__alist-8064", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal of enhancing the WebDAV protocol in the alist project to include hash information (like MD5, SHA1) for file checksum validation, particularly to support tools like rclone. It provides specific XML examples for querying and returning custom properties, which helps in understanding the desired functionality. References to external discussions and issues on GitHub also add context to the feature request. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not explicitly define all possible hash types to be supported beyond the examples (MD5, SHA1, ADLER32), nor does it address potential constraints or limitations of adding custom properties to WebDAV responses. Additionally, edge cases such as handling files without hash information or performance implications of computing/retrieving hashes are not mentioned. Overall, while the intent and basic requirements are clear, these minor gaps in detail slightly reduce the clarity.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively focused, affecting two files (`hash.go` and `prop.go`). The modifications involve adding a new method to iterate over hash information in `hash.go` and extending the WebDAV property handling in `prop.go` to include checksum data. The changes are localized and do not appear to impact the broader system architecture significantly. However, they do require understanding the interaction between the hash information structure and the WebDAV protocol implementation, which adds a moderate level of complexity.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including the WebDAV protocol (specifically property handling via XML), XML parsing and generation, and the internal representation of hash information in the alist codebase. Additionally, the use of Go's `iter` package for sequence iteration (introduced in Go 1.23) indicates a need to understand modern Go features. While these concepts are not overly complex for an experienced developer, they do require a solid grasp of protocol-level details and Go-specific idioms, contributing to a medium difficulty level.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, such as handling files without available hash information or errors during hash retrieval. The code changes also do not include explicit error handling beyond returning an empty string for checksums if no data is available. This suggests that additional considerations might be needed (e.g., what to return if hash computation fails or if certain hash types are unsupported), but these are not overly complex to address. The lack of explicit discussion on performance implications (e.g., computing hashes for large files) also hints at potential unaddressed challenges, though they are not critical to the core implementation.\n\n4. **Overall Complexity**: The task involves extending an existing protocol (WebDAV) with custom properties, which is a moderately complex feature addition. It requires understanding both the protocol standards and the specific implementation in alist. However, the provided code changes are straightforward, leveraging existing data structures (`HashInfo`) and adding minimal new logic. The problem does not require deep architectural changes, advanced algorithms, or system-level optimizations, which keeps it from being classified as hard or very hard.\n\nIn summary, this problem is of medium difficulty (0.45) due to the need to understand and extend WebDAV property handling, work with XML structures, and integrate with existing hash information logic in a localized manner. It is not overly challenging for a developer with experience in Go and protocol handling, but it does go beyond simple bug fixes or basic feature additions due to the cross-file modifications and protocol-specific knowledge required.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "IPFS API \u9a71\u52a8\uff0c\u4e0b\u8f7d\u5305\u542b\u7a7a\u683c\u7684\u6587\u4ef6\u540d\u65f6\u4f1a\u62a5\u9519\n### Please make sure of the following things\n\n- [X] I have read the [documentation](https://alist.nn.ci).\n\u6211\u5df2\u7ecf\u9605\u8bfb\u4e86[\u6587\u6863](https://alist.nn.ci)\u3002\n\n- [X] I'm sure there are no duplicate issues or discussions.\n\u6211\u786e\u5b9a\u6ca1\u6709\u91cd\u590d\u7684issue\u6216\u8ba8\u8bba\u3002\n\n- [x] I'm sure it's due to `AList` and not something else(such as [Network](https://alist.nn.ci/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host) ,`Dependencies` or `Operational`).\n\u6211\u786e\u5b9a\u662f`AList`\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982[\u7f51\u7edc](https://alist.nn.ci/zh/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host)\uff0c`\u4f9d\u8d56`\u6216`\u64cd\u4f5c`\uff09\u3002\n\n- [X] I'm sure this issue is not fixed in the latest version.\n\u6211\u786e\u5b9a\u8fd9\u4e2a\u95ee\u9898\u5728\u6700\u65b0\u7248\u672c\u4e2d\u6ca1\u6709\u88ab\u4fee\u590d\u3002\n\n\n### AList Version / AList \u7248\u672c\n\nv3.30.0\n\n### Driver used / \u4f7f\u7528\u7684\u5b58\u50a8\u9a71\u52a8\n\nIPFS API\n\n### Describe the bug / \u95ee\u9898\u63cf\u8ff0\n\n\u4f7f\u7528IPFS API \u9a71\u52a8\uff0c\u4e0b\u8f7d\u5305\u542b\u7a7a\u683c\u7684\u6587\u4ef6\u540d\u65f6\u4f1a\u62a5\u9519\n\n### Reproduction / \u590d\u73b0\u94fe\u63a5\n\n\u4e0d\u9700\u8981\u94fe\u63a5\uff0c\u672c\u5730\u8fd0\u884c\u4e00\u4e2aipfs daemon\uff0c\u4e0a\u4f20\u4e00\u4e2a\u6587\u4ef6\uff0c\u5e76\u5c06\u6587\u4ef6\u540d\u4fee\u6539\u6210\u5e26\u7a7a\u683c\u7684\u6587\u4ef6\u540d\u5c31\u53ef\u4ee5\n\n### Config / \u914d\u7f6e\n\n{\r\n  \"force\": false,\r\n  \"site_url\": \"\",\r\n  \"cdn\": \"\",\r\n  \"jwt_secret\": \"***************\",\r\n  \"token_expires_in\": 48,\r\n  \"database\": {\r\n    \"type\": \"sqlite3\",\r\n    \"host\": \"\",\r\n    \"port\": 0,\r\n    \"user\": \"\",\r\n    \"password\": \"\",\r\n    \"name\": \"\",\r\n    \"db_file\": \"data/data.db\",\r\n    \"table_prefix\": \"x_\",\r\n    \"ssl_mode\": \"\"\r\n  },\r\n  \"scheme\": {\r\n    \"address\": \"0.0.0.0\",\r\n    \"http_port\": 5244,\r\n    \"https_port\": -1,\r\n    \"force_https\": false,\r\n    \"cert_file\": \"\",\r\n    \"key_file\": \"\",\r\n    \"unix_file\": \"\",\r\n    \"unix_file_perm\": \"\"\r\n  },\r\n  \"temp_dir\": \"data/temp\",\r\n  \"bleve_dir\": \"data/bleve\",\r\n  \"dist_dir\": \"\",\r\n  \"log\": {\r\n    \"enable\": true,\r\n    \"name\": \"data/log/log.log\",\r\n    \"max_size\": 50,\r\n    \"max_backups\": 30,\r\n    \"max_age\": 28,\r\n    \"compress\": false\r\n  },\r\n  \"delayed_start\": 0,\r\n  \"max_connections\": 0,\r\n  \"tls_insecure_skip_verify\": true,\r\n  \"tasks\": {\r\n    \"download\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 1\r\n    },\r\n    \"transfer\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 2\r\n    },\r\n    \"upload\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 0\r\n    },\r\n    \"copy\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 2\r\n    }\r\n  },\r\n  \"cors\": {\r\n    \"allow_origins\": [\r\n      \"*\"\r\n    ],\r\n    \"allow_methods\": [\r\n      \"*\"\r\n    ],\r\n    \"allow_headers\": [\r\n      \"*\"\r\n    ]\r\n  }\r\n}\n\n### Logs / \u65e5\u5fd7\n\n{\"code\":500,\"message\":\"http request [http://*********:5080/ipfs/*******************************/?filename=test -HDR.ts] failure,status: 400 response:400 Bad Request\",\"data\":null}\n", "patch": "diff --git a/drivers/ipfs_api/driver.go b/drivers/ipfs_api/driver.go\nindex cf21e62da59..f6f81305e20 100644\n--- a/drivers/ipfs_api/driver.go\n+++ b/drivers/ipfs_api/driver.go\n@@ -62,7 +62,7 @@ func (d *IPFS) List(ctx context.Context, dir model.Obj, args model.ListArgs) ([]\n \tfor _, file := range dirs {\n \t\tgateurl := *d.gateURL\n \t\tgateurl.Path = \"ipfs/\" + file.Hash\n-\t\tgateurl.RawQuery = \"filename=\" + file.Name\n+\t\tgateurl.RawQuery = \"filename=\" + url.PathEscape(file.Name)\n \t\tobjlist = append(objlist, &model.ObjectURL{\n \t\t\tObject: model.Object{ID: file.Hash, Name: file.Name, Size: int64(file.Size), IsFolder: file.Type == 1},\n \t\t\tUrl:    model.Url{Url: gateurl.String()},\n@@ -73,7 +73,7 @@ func (d *IPFS) List(ctx context.Context, dir model.Obj, args model.ListArgs) ([]\n }\n \n func (d *IPFS) Link(ctx context.Context, file model.Obj, args model.LinkArgs) (*model.Link, error) {\n-\tlink := d.Gateway + \"/ipfs/\" + file.GetID() + \"/?filename=\" + file.GetName()\n+\tlink := d.Gateway + \"/ipfs/\" + file.GetID() + \"/?filename=\" + url.PathEscape(file.GetName())\n \treturn &model.Link{URL: link}, nil\n }\n \n", "instance_id": "AlistGo__alist-6245", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: downloading files with spaces in their filenames using the IPFS API driver results in an error. The goal is evident (fix the error), and the reproduction steps are provided (upload a file with a space in the name to a local IPFS daemon). However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the expected behavior when handling filenames with spaces, nor does it mention other potential special characters that might cause similar issues. Additionally, while the error log is provided, it lacks deeper context about the root cause (e.g., whether it's a URL encoding issue or something else). Edge cases beyond spaces in filenames are not discussed, which could leave room for incomplete solutions. Overall, the statement is valid and clear enough to understand the issue, but it misses some critical details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue is straightforward: filenames with spaces are not being properly encoded in URLs, leading to HTTP 400 Bad Request errors. The code changes provided are minimal and localized to a single file (`driver.go`) within the IPFS API driver module. The fix involves using `url.PathEscape` to encode the filename in the URL query parameter, which is a basic operation requiring minimal understanding of Go's `net/url` package. The scope of the change is small, affecting only two lines in two functions (`List` and `Link`), with no impact on the broader system architecture or interactions between modules. The technical concepts involved are simple\u2014basic URL encoding and string manipulation in Go. There are no complex algorithms, design patterns, or domain-specific knowledge required beyond understanding how URLs are constructed. While edge cases (e.g., other special characters in filenames) could potentially be considered, the problem statement and code changes focus solely on spaces, and the fix is generic enough to handle other characters via `url.PathEscape`. Error handling is not a significant concern here, as the change does not introduce new error conditions or require additional logic. Overall, this is a simple bug fix that a junior to mid-level developer with basic Go knowledge could handle with ease.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve err reporting on failed config migration\n**Is your feature request related to a problem? Please describe.**\nI recently had my lazygit config become deprecated. Specifically from [this key path migration](https://github.com/jesseduffield/lazygit/blob/40d6800fd35ad2d5c6d96d6f08ffa42d4c764ad3/pkg/config/app_config.go#L228).\n\nHowever my config file is a symlink to my `/nix/store` (it's managed by nix itself) so it's immutable. When I tried to open lazygit after this schema change, I only got this output to my shell:\n\n```\n2025/01/24 10:31:27 Couldn't write migrated config back to `/home/$USER/.config/lazygit/config.yml`: open /home/$USER/.config/lazygit/config.yml: read-only file system\n```\n\nwhich is not intuitive at all. I had to read the source code to finally find this snippet to explain why this happened:\n\n```go\n\t// Write config back if changed\n\tif string(changedContent) != string(content) {\n\t\tif err := os.WriteFile(path, changedContent, 0o644); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Couldn't write migrated config back to `%s`: %s\", path, err)\n\t\t}\n\t\treturn changedContent, nil\n\t}\n```\n\n**Describe the solution you'd like**\nI think lazygit should be telling the user that it's trying to rewrite my config file to self-heal it, _**especially**_ if [the os write leads to a failure](https://github.com/jesseduffield/lazygit/blob/40d6800fd35ad2d5c6d96d6f08ffa42d4c764ad3/pkg/config/app_config.go#L249).\n\n", "patch": "diff --git a/pkg/config/app_config.go b/pkg/config/app_config.go\nindex 381bbe07671..5d240b87d04 100644\n--- a/pkg/config/app_config.go\n+++ b/pkg/config/app_config.go\n@@ -245,9 +245,11 @@ func migrateUserConfig(path string, content []byte) ([]byte, error) {\n \n \t// Write config back if changed\n \tif string(changedContent) != string(content) {\n+\t\tfmt.Println(\"Provided user config is deprecated but auto-fixable. Attempting to write fixed version back to file...\")\n \t\tif err := os.WriteFile(path, changedContent, 0o644); err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"Couldn't write migrated config back to `%s`: %s\", path, err)\n+\t\t\treturn nil, fmt.Errorf(\"While attempting to write back fixed user config to %s, an error occurred: %s\", path, err)\n \t\t}\n+\t\tfmt.Printf(\"Success. New config written to %s\\n\", path)\n \t\treturn changedContent, nil\n \t}\n \n", "instance_id": "jesseduffield__lazygit-4210", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue and the desired improvement. The user explains the context of the problem (a failed config migration due to an immutable file system) and provides specific details about the error message received and the relevant code snippet from the source. The goal of improving error reporting to make it more intuitive is evident, and the proposed solution (informing the user about the attempt to rewrite the config file) is articulated. However, there are minor ambiguities: the problem statement does not explicitly define the expected format or content of the improved error message beyond a general suggestion, nor does it mention any specific edge cases or constraints (e.g., how to handle different types of file system errors or user permissions). Additionally, there are no examples of what the ideal output should look like in various scenarios. Thus, while the problem is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward modification to improve error reporting. The code change is minimal and localized to a single file (`app_config.go`) and a specific function, requiring only the addition of informative print statements and a slight rephrasing of the error message. The scope of the change is small, with no impact on the broader codebase architecture or interactions between modules. The technical concepts involved are basic\u2014understanding Go's `fmt` package for printing messages and handling errors with `fmt.Errorf`. No advanced language features, algorithms, design patterns, or domain-specific knowledge are required. Regarding edge cases and error handling, the problem does not introduce new edge cases beyond the existing file write failure scenario, and the modification does not require complex error handling logic. Overall, this is a very easy task that a junior developer with basic Go knowledge could handle with minimal effort.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Crash when filtering by author\n**Describe the bug**\r\nThe application crashes when filtering the commits by user (but not always, can't really pinpoint it, it's something when refreshing the view).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Open `lazygit` in the `lazygit` repo\r\n2. Go to `commits` panel\r\n3. Filter the commits by author, e.g. `Luke Swan` (note: can pretty consistently crash it with this author or myself, but not all of them)\r\n4. Exit that view, press `<c-s>` again and just press `Filter by <the current author>`\r\n5. Crash\r\n\r\n**Expected behavior**\r\nNo crashes.\r\n\r\n**Screenshots**\r\nAsciinema recording [here](https://asciinema.org/a/XJ0t35wROY2fBlsZM9ujgH7eb).\r\n\r\n**Version info:**\r\n`lazygit` version: built latest `master` as of today, `git` version `2.46.0`\r\n\r\n\r\n**Additional context**\r\nIt seems like [these columnPositions](https://github.com/jesseduffield/lazygit/blob/a3560eb451fcf375378a316cbc2f4e08e24bf756/pkg/gui/context/local_commits_context.go#L212) have only one element so both accesses are out of bounds.\r\n\r\n**Note:** please try updating to the latest version or [manually building](https://github.com/jesseduffield/lazygit/#manual) the latest `master` to see if the issue still occurs.\r\n\r\n<!--\r\nIf you want to try and debug this issue yourself, you can run `lazygit --debug` in one terminal panel and `lazygit --logs` in another to view the logs.\r\n-->\r\n\n", "patch": "diff --git a/pkg/gui/context/local_commits_context.go b/pkg/gui/context/local_commits_context.go\nindex 0cb7a628c18..eecb161077e 100644\n--- a/pkg/gui/context/local_commits_context.go\n+++ b/pkg/gui/context/local_commits_context.go\n@@ -202,6 +202,15 @@ func shouldShowGraph(c *ContextCommon) bool {\n }\n \n func searchModelCommits(caseSensitive bool, commits []*models.Commit, columnPositions []int, searchStr string) []gocui.SearchPosition {\n+\tif columnPositions == nil {\n+\t\t// This should never happen. We are being called at a time where our\n+\t\t// entire view content is scrolled out of view, so that we didn't draw\n+\t\t// anything the last time we rendered. If we run into a scenario where\n+\t\t// this happens, we should fix it, but until we found them all, at least\n+\t\t// make sure we don't crash.\n+\t\treturn []gocui.SearchPosition{}\n+\t}\n+\n \tnormalize := lo.Ternary(caseSensitive, func(s string) string { return s }, strings.ToLower)\n \treturn lo.FilterMap(commits, func(commit *models.Commit, idx int) (gocui.SearchPosition, bool) {\n \t\t// The XStart and XEnd values are only used if the search string can't\ndiff --git a/pkg/gui/controllers/helpers/refresh_helper.go b/pkg/gui/controllers/helpers/refresh_helper.go\nindex e29879b9ddc..579bc9d3e33 100644\n--- a/pkg/gui/controllers/helpers/refresh_helper.go\n+++ b/pkg/gui/controllers/helpers/refresh_helper.go\n@@ -765,8 +765,18 @@ func (self *RefreshHelper) refreshView(context types.Context) error {\n \n \terr := self.c.PostRefreshUpdate(context)\n \n-\t// Re-applying the search must be done after re-rendering the view though,\n-\t// so that the \"x of y\" status is shown correctly.\n-\tself.searchHelper.ReApplySearch(context)\n+\tself.c.AfterLayout(func() error {\n+\t\t// Re-applying the search must be done after re-rendering the view though,\n+\t\t// so that the \"x of y\" status is shown correctly.\n+\t\t//\n+\t\t// Also, it must be done after layout, because otherwise FocusPoint\n+\t\t// hasn't been called yet (see ListContextTrait.FocusLine), which means\n+\t\t// that the scroll position might be such that the entire visible\n+\t\t// content is outside the viewport. And this would cause problems in\n+\t\t// searchModelCommits.\n+\t\tself.searchHelper.ReApplySearch(context)\n+\t\treturn nil\n+\t})\n+\n \treturn err\n }\ndiff --git a/pkg/gui/controllers/local_commits_controller.go b/pkg/gui/controllers/local_commits_controller.go\nindex 3900d7f32b7..2d4232f33a6 100644\n--- a/pkg/gui/controllers/local_commits_controller.go\n+++ b/pkg/gui/controllers/local_commits_controller.go\n@@ -207,14 +207,8 @@ func (self *LocalCommitsController) GetKeybindings(opts types.KeybindingsOpts) [\n \t\t\tDescription:       self.c.Tr.MarkAsBaseCommit,\n \t\t\tTooltip:           self.c.Tr.MarkAsBaseCommitTooltip,\n \t\t},\n-\t\t// overriding these navigation keybindings because we might need to load\n+\t\t// overriding this navigation keybinding because we might need to load\n \t\t// more commits on demand\n-\t\t{\n-\t\t\tKey:         opts.GetKey(opts.Config.Universal.StartSearch),\n-\t\t\tHandler:     self.openSearch,\n-\t\t\tDescription: self.c.Tr.StartSearch,\n-\t\t\tTag:         \"navigation\",\n-\t\t},\n \t\t{\n \t\t\tKey:         opts.GetKey(opts.Config.Universal.GotoBottom),\n \t\t\tHandler:     self.gotoBottom,\n@@ -228,6 +222,14 @@ func (self *LocalCommitsController) GetKeybindings(opts types.KeybindingsOpts) [\n \t}\n \n \tbindings := append(outsideFilterModeBindings, []*types.Binding{\n+\t\t// overriding this navigation keybinding because we might need to load\n+\t\t// more commits on demand\n+\t\t{\n+\t\t\tKey:         opts.GetKey(opts.Config.Universal.StartSearch),\n+\t\t\tHandler:     self.openSearch,\n+\t\t\tDescription: self.c.Tr.StartSearch,\n+\t\t\tTag:         \"navigation\",\n+\t\t},\n \t\t{\n \t\t\tKey:               opts.GetKey(opts.Config.Commits.AmendToCommit),\n \t\t\tHandler:           self.withItem(self.amendTo),\ndiff --git a/pkg/gui/presentation/commits.go b/pkg/gui/presentation/commits.go\nindex f9bdb4eb750..234a84e5c73 100644\n--- a/pkg/gui/presentation/commits.go\n+++ b/pkg/gui/presentation/commits.go\n@@ -65,7 +65,7 @@ func GetCommitListDisplayStrings(\n \t\treturn nil\n \t}\n \n-\tif startIdx > len(commits) {\n+\tif startIdx >= len(commits) {\n \t\treturn nil\n \t}\n \ndiff --git a/pkg/utils/formatting.go b/pkg/utils/formatting.go\nindex b7817346ab7..b13a2ffa8ed 100644\n--- a/pkg/utils/formatting.go\n+++ b/pkg/utils/formatting.go\n@@ -54,6 +54,10 @@ func WithPadding(str string, padding int, alignment Alignment) string {\n // returns a list of strings that should be joined with \"\\n\", and an array of\n // the column positions\n func RenderDisplayStrings(displayStringsArr [][]string, columnAlignments []Alignment) ([]string, []int) {\n+\tif len(displayStringsArr) == 0 {\n+\t\treturn []string{}, nil\n+\t}\n+\n \tdisplayStringsArr, columnAlignments, removedColumns := excludeBlankColumns(displayStringsArr, columnAlignments)\n \tpadWidths := getPadWidths(displayStringsArr)\n \tcolumnConfigs := make([]ColumnConfig, len(padWidths))\n", "instance_id": "jesseduffield__lazygit-3838", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the bug: the application crashes when filtering commits by author under specific conditions. Steps to reproduce are provided, along with version information and a reference to a potential cause (out-of-bounds access in `columnPositions`). However, there are minor ambiguities and missing details. For instance, the statement mentions that the crash does not always occur and is hard to pinpoint, which introduces uncertainty about the exact conditions triggering the issue. Additionally, there are no explicit mentions of expected input/output formats or specific edge cases beyond the reproduction steps. While the provided asciinema recording and additional context are helpful, the lack of comprehensive details about the root cause or specific scenarios (beyond certain authors) prevents this from being a fully comprehensive description. Hence, a score of 2 (Mostly Clear) is appropriate.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes spans multiple files (`local_commits_context.go`, `refresh_helper.go`, `local_commits_controller.go`, `commits.go`, and `formatting.go`), indicating a need to understand interactions across different parts of the codebase. The changes involve critical logic related to rendering, searching, and refreshing views in a GUI-based Git tool (`lazygit`), which suggests a moderate impact on the system's behavior, though not a full architectural overhaul. Second, the number of technical concepts involved is significant: developers need to understand Go-specific features (e.g., slices, nil checks), GUI rendering logic (e.g., viewport and scroll position handling), and the specific library (`gocui`) used for terminal UI interactions. Additionally, the problem requires knowledge of the application's custom context and refresh mechanisms. Third, the code changes address potential edge cases, such as nil checks for `columnPositions` and ensuring searches are applied after layout to avoid viewport issues, which adds complexity to error handling. While the modifications themselves are not extremely intricate (e.g., adding nil checks, deferring search application), the need to debug and understand the root cause of an intermittent crash, combined with the multi-file impact and GUI-specific logic, elevates the difficulty. A score of 0.65 reflects this as a challenging but not extremely advanced problem, requiring a solid grasp of the codebase and careful handling of edge cases without necessitating advanced algorithmic or system-level expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Option to hide panels\n**Is your feature request related to a problem? Please describe.**\r\nThe \"Files - Submodules\" panel is too small to see all the files that have changes.\r\n\r\n**Describe the solution you'd like**\r\nIf I could hide the \"Local Branches - Remotes - Tags\" panel and/or the \"Commits - Reflog\" panel and/or the \"Stash\" panel, then the \"Files - Submodules\" panel could take up that extra space. I don't really ever use those bottom 3 panels, but I use the \"Files\" list a lot.\r\n\r\n**Describe alternatives you've considered**\r\nMaybe allow the panels to be resized, but that sounds more advanced and less helpful.\r\n\r\n**Additional context**\r\nThere is already an option to hide the command log.\r\n\r\n```yml\r\ngui:\r\n  showCommandLog: false\r\n```\r\n\r\nI just want a similar option for the other panels too.\n", "patch": "diff --git a/docs/Config.md b/docs/Config.md\nindex b14ee993ae0..5662a4d7378 100644\n--- a/docs/Config.md\n+++ b/docs/Config.md\n@@ -68,6 +68,10 @@ gui:\n   # If true, increase the height of the focused side window; creating an accordion effect.\n   expandFocusedSidePanel: false\n \n+  # The weight of the expanded side panel, relative to the other panels. 2 means\n+  # twice as tall as the other panels. Only relevant if `expandFocusedSidePanel` is true.\n+  expandedSidePanelWeight: 2\n+\n   # Sometimes the main window is split in two (e.g. when the selected file has both staged and unstaged changes). This setting controls how the two sections are split.\n   # Options are:\n   # - 'horizontal': split the window horizontally\ndiff --git a/pkg/config/user_config.go b/pkg/config/user_config.go\nindex a5f65165d7b..41d3dfe1072 100644\n--- a/pkg/config/user_config.go\n+++ b/pkg/config/user_config.go\n@@ -77,6 +77,9 @@ type GuiConfig struct {\n \tSidePanelWidth float64 `yaml:\"sidePanelWidth\" jsonschema:\"maximum=1,minimum=0\"`\n \t// If true, increase the height of the focused side window; creating an accordion effect.\n \tExpandFocusedSidePanel bool `yaml:\"expandFocusedSidePanel\"`\n+\t// The weight of the expanded side panel, relative to the other panels. 2 means\n+\t// twice as tall as the other panels. Only relevant if `expandFocusedSidePanel` is true.\n+\tExpandedSidePanelWeight int `yaml:\"expandedSidePanelWeight\"`\n \t// Sometimes the main window is split in two (e.g. when the selected file has both staged and unstaged changes). This setting controls how the two sections are split.\n \t// Options are:\n \t// - 'horizontal': split the window horizontally\n@@ -651,6 +654,7 @@ func GetDefaultConfig() *UserConfig {\n \t\t\tSkipStashWarning:         false,\n \t\t\tSidePanelWidth:           0.3333,\n \t\t\tExpandFocusedSidePanel:   false,\n+\t\t\tExpandedSidePanelWeight:  2,\n \t\t\tMainPanelSplitMode:       \"flexible\",\n \t\t\tEnlargedSideViewLocation: \"left\",\n \t\t\tLanguage:                 \"auto\",\ndiff --git a/pkg/gui/controllers/helpers/window_arrangement_helper.go b/pkg/gui/controllers/helpers/window_arrangement_helper.go\nindex 8615769dc1b..5c17083f7f4 100644\n--- a/pkg/gui/controllers/helpers/window_arrangement_helper.go\n+++ b/pkg/gui/controllers/helpers/window_arrangement_helper.go\n@@ -443,7 +443,7 @@ func sidePanelChildren(args WindowArrangementArgs) func(width int, height int) [\n \t\t\t\tif accordionMode && defaultBox.Window == args.CurrentSideWindow {\n \t\t\t\t\treturn &boxlayout.Box{\n \t\t\t\t\t\tWindow: defaultBox.Window,\n-\t\t\t\t\t\tWeight: 2,\n+\t\t\t\t\t\tWeight: args.UserConfig.Gui.ExpandedSidePanelWeight,\n \t\t\t\t\t}\n \t\t\t\t}\n \ndiff --git a/schema/config.json b/schema/config.json\nindex a5c704db0cb..f5f7bab86ef 100644\n--- a/schema/config.json\n+++ b/schema/config.json\n@@ -76,6 +76,11 @@\n           \"description\": \"If true, increase the height of the focused side window; creating an accordion effect.\",\n           \"default\": false\n         },\n+        \"expandedSidePanelWeight\": {\n+          \"type\": \"integer\",\n+          \"description\": \"The weight of the expanded side panel, relative to the other panels. 2 means\\ntwice as tall as the other panels. Only relevant if `expandFocusedSidePanel` is true.\",\n+          \"default\": 2\n+        },\n         \"mainPanelSplitMode\": {\n           \"type\": \"string\",\n           \"enum\": [\n", "instance_id": "jesseduffield__lazygit-3623", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the feature request to hide certain panels in a GUI to allow more space for the \"Files - Submodules\" panel. The goal is evident: provide configuration options to hide specific panels, similar to an existing option for hiding the command log. The user also provides context about their usage patterns and an alternative solution (resizing panels), which adds clarity to their intent. However, the statement lacks specific details about which panels should be configurable, whether all should be hideable, or if there are any constraints or side effects to consider (e.g., minimum number of visible panels, impact on usability). Additionally, there are no explicit examples of the desired configuration format beyond referencing the existing `showCommandLog` option. Due to these minor ambiguities, I assign a clarity score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes are not directly related to hiding panels as described in the problem statement. Instead, they pertain to adjusting the weight of an expanded side panel when `expandFocusedSidePanel` is enabled. This discrepancy suggests that the code changes might be incomplete or misaligned with the problem statement. However, assuming the intent is to add configuration options for hiding panels (as per the problem description), the scope would likely involve adding new configuration fields in the `GuiConfig` struct, updating the default configuration, modifying the GUI layout logic to respect these settings, and updating documentation. This would span multiple files (e.g., config, GUI logic, schema, and docs), but the changes would be relatively straightforward additions without deep architectural impact. The provided diff, which modifies the `ExpandedSidePanelWeight`, involves minimal changes across four files and is a simple adjustment of a value with no complex logic.\n\n2. **Number of Technical Concepts:** Solving the actual problem (hiding panels) would require understanding basic GUI layout management, configuration parsing (YAML/JSON), and potentially some framework-specific knowledge if the GUI is built with a library like `tview` or similar (common in terminal-based GUIs). The concepts are not particularly advanced\u2014mostly struct modifications, conditional rendering logic, and schema updates. The provided code changes require even less: just understanding a single configuration parameter and its application in a layout calculation.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention specific edge cases, but implementing panel hiding could introduce considerations like ensuring at least one panel remains visible, handling invalid configurations, or adjusting layout dynamically when panels are hidden/shown. These are not overly complex but require some thought. The provided code changes for `ExpandedSidePanelWeight` introduce minimal edge case concerns (e.g., ensuring the weight is positive), and no explicit error handling is added or needed in the diff.\n\n4. **Overall Complexity:** The provided code changes are very simple, involving a single parameter addition and its usage in one place, with no significant logic or impact on the broader system. However, addressing the actual problem of hiding panels would require slightly more effort, though still within the realm of adding configuration toggles and conditional rendering\u2014tasks that are routine for a developer familiar with the codebase. There\u2019s no indication of deep architectural changes, complex algorithms, or performance-critical modifications.\n\nGiven the simplicity of the provided diff and the moderate but straightforward effort required for the actual feature (hiding panels), I assign a difficulty score of 0.35, placing it in the Easy category. It requires understanding some code logic and making modifications across a few files, but the concepts and changes are not particularly challenging.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`cmd/evm` ignores the `baseFeePerGas` setting in `genesis.json` when executing opcode `BASEFEE`\n#### System information\r\n\r\nGeth version: `evm version 1.14.5-stable-0dd173a7 ` and ` evm version 1.14.8-unstable-978041fe `\r\nCL client & version: Nope\r\nOS & Version: Ubuntu 20.04\r\nCommit hash : Nope\r\n\r\n#### Expected behaviour\r\n\r\nHello developers,\r\n\r\nI'm currently testing EVM implementations and have encountered a peculiar issue. When executing the `BASEFEE` opcode (0x48), `evm` will return `0x3b9aca00(1000000000)` instead of the value provided in `genesis.json` .\r\n\r\n#### Steps to reproduce the behaviour\r\n\r\nI used the latest version of geth image pulled from dockerhub(digest bb0eb0bd6) to reproduce this bug. \r\n\r\n```\r\ndocker pull ethereum/client-go:alltools-latest\r\ndocker run ethereum/client-go:alltools-latest tail -f /dev/null\r\ndocker exec -it containerid /bin/sh\r\n```\r\n\r\nThis is the `genesis.json` file for `--prestate`, which sets the baseFee to `0xffff`.\r\n\r\n```json\r\n{\r\n  \"config\": {\r\n    \"chainId\": 9599,\r\n    \"homesteadBlock\": 0,\r\n    \"eip150Block\": 0,\r\n    \"eip155Block\": 0,\r\n    \"eip158Block\": 0,\r\n    \"byzantiumBlock\": 0,\r\n    \"constantinopleBlock\": 0,\r\n    \"petersburgBlock\": 0,\r\n    \"istanbulBlock\": 0,\r\n    \"berlinBlock\": 0,\r\n    \"londonBlock\": 0,\r\n    \"shanghaiBlock\": 0,\r\n    \"shanghaiTime\":0\r\n  },\r\n  \"alloc\": {\r\n    \"0x1c7cd2d37ffd63856a5bd56a9af1643f2bcf545f\": {\r\n     \"balance\": \"0x1234\"\r\n    }\r\n  },\r\n  \"coinbase\": \"0x000000000000000000000000000000000000abcd\",\r\n  \"difficulty\": \"0x0\",\r\n  \"extraData\": \"\",\r\n  \"gasLimit\": \"0xffffff\",\r\n  \"nonce\": \"0x0000000000000042\",\r\n  \"mixhash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\r\n  \"parentHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\r\n  \"timestamp\": \"0x00\",\r\n  \"baseFeePerGas\": \"0xffff\"\r\n}\r\n```\r\n\r\nUse `evm` to run the bytecode `4860005260406000f3` .\r\n\r\n```shell\r\nevm --debug --gas 0xffffff --nomemory=false --json --code 4860005260406000f3 --prestate ./genesis.json run\r\n```\r\n\r\nThe mnemonics of the bytecode are as follows.\r\n\r\n```\r\nBASEFEE\r\nPUSH1 00\r\nMSTORE\r\nPUSH1 40\r\nPUSH1 00\r\nRETURN \r\n```\r\n\r\nThe result from evm's output, the stack output of `BASEFEE` is `0x3b9aca00`.\r\n\r\n```json\r\n{\"pc\":0,\"op\":72,\"gas\":\"0xffffff\",\"gasCost\":\"0x2\",\"memSize\":0,\"stack\":[],\"depth\":1,\"refund\":0,\"opName\":\"BASEFEE\"}\r\n{\"pc\":1,\"op\":96,\"gas\":\"0xfffffd\",\"gasCost\":\"0x3\",\"memSize\":0,\"stack\":[\"0x3b9aca00\"],\"depth\":1,\"refund\":0,\"opName\":\"PUSH1\"}\r\n{\"pc\":3,\"op\":82,\"gas\":\"0xfffffa\",\"gasCost\":\"0x6\",\"memSize\":0,\"stack\":[\"0x3b9aca00\",\"0x0\"],\"depth\":1,\"refund\":0,\"opName\":\"MSTORE\"}\r\n{\"pc\":4,\"op\":96,\"gas\":\"0xfffff4\",\"gasCost\":\"0x3\",\"memory\":\"0x000000000000000000000000000000000000000000000000000000003b9aca00\",\"memSize\":32,\"stack\":[],\"depth\":1,\"refund\":0,\"opName\":\"PUSH1\"}\r\n{\"pc\":6,\"op\":96,\"gas\":\"0xfffff1\",\"gasCost\":\"0x3\",\"memory\":\"0x000000000000000000000000000000000000000000000000000000003b9aca00\",\"memSize\":32,\"stack\":[\"0x40\"],\"depth\":1,\"refund\":0,\"opName\":\"PUSH1\"}\r\n{\"pc\":8,\"op\":243,\"gas\":\"0xffffee\",\"gasCost\":\"0x3\",\"memory\":\"0x000000000000000000000000000000000000000000000000000000003b9aca00\",\"memSize\":32,\"stack\":[\"0x40\",\"0x0\"],\"depth\":1,\"refund\":0,\"opName\":\"RETURN\"}\r\n{\"output\":\"000000000000000000000000000000000000000000000000000000003b9aca000000000000000000000000000000000000000000000000000000000000000000\",\"gasUsed\":\"0x14\"}\r\n```\r\n\r\n#### Backtrace\r\n\r\nI checked in `evm version 1.14.8-unstable-978041fe` and `evm version 1.14.5-stable-0dd173a7`, the problem occured in both versions. \r\n\r\nIt seems that the `baseFeePerGas` in `genesis.json` is just ignored.\r\n\r\nhttps://github.com/ethereum/go-ethereum/blob/b37ac5c1027847de2eb65162a7251b37b9466793/cmd/evm/runner.go#L214-L228\r\n\r\nThanks for your time!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n", "patch": "diff --git a/cmd/evm/runner.go b/cmd/evm/runner.go\nindex d06f85ed5965..c02f9f059085 100644\n--- a/cmd/evm/runner.go\n+++ b/cmd/evm/runner.go\n@@ -221,6 +221,7 @@ func runCmd(ctx *cli.Context) error {\n \t\tTime:        genesisConfig.Timestamp,\n \t\tCoinbase:    genesisConfig.Coinbase,\n \t\tBlockNumber: new(big.Int).SetUint64(genesisConfig.Number),\n+\t\tBaseFee:     genesisConfig.BaseFee,\n \t\tBlobHashes:  blobHashes,\n \t\tBlobBaseFee: blobBaseFee,\n \t\tEVMConfig: vm.Config{\n", "instance_id": "ethereum__go-ethereum-30281", "clarity": 3, "difficulty": 0.2, "clarity_explanation": "The problem statement is comprehensive and well-detailed. The goal is clearly defined: the `cmd/evm` tool ignores the `baseFeePerGas` value specified in the `genesis.json` file when executing the `BASEFEE` opcode, returning a hardcoded value instead. The input (a specific `genesis.json` configuration and bytecode), expected output (the `baseFeePerGas` value from the configuration), and actual output (a hardcoded value) are explicitly provided. The steps to reproduce the issue are thorough, including Docker commands, the exact `genesis.json` content, and the bytecode to run. Additionally, the problem statement includes a reference to the relevant code section in the repository, along with version information for reproducibility. There are no significant ambiguities, and the inclusion of detailed JSON outputs and mnemonics further clarifies the issue. While edge cases are not explicitly discussed, the problem's focus is narrow and specific enough that this does not detract from its clarity.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.20) based on the provided factors. The scope of the code change is minimal, involving a single line addition in the `runner.go` file to pass the `BaseFee` from the `genesisConfig` to the EVM configuration. This change is localized to a specific function and does not require understanding complex interactions across multiple modules or impacting the broader system architecture. The technical concepts involved are straightforward: basic understanding of Go structs, pointer handling, and how configuration values are passed to the EVM runtime. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Ethereum Virtual Machine (EVM) concepts are required. The problem does not mention specific edge cases or error handling requirements, and the provided code change does not introduce new error handling logic. Overall, this is a simple bug fix that requires minimal effort and a basic understanding of the codebase, fitting within the 0.2-0.4 range but leaning toward the lower end due to the simplicity of the fix.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "lru cache throws a warning because the ttlAutopurge option is set to false in codemirror-promql\n### What did you do?\n\nImport  `@prometheus-io/codemirror-promql` in [Perses](https://github.com/perses/perses). \n\n### What did you expect to see?\n\nNo warning as the `ttlAutopurge` should be set to `true` when a `ttl` is set. According to the configuration docs https://github.com/isaacs/node-lru-cache/blob/65c9971e3fef123ff0f17d67add53b2d99b461b0/src/index.ts#L698\n\n### What did you see instead? Under which circumstances?\n\nWe see the following warning when running the react app:\n\n```\n[\"[LRU_CACHE_UNBOUNDED] UnboundedCacheWarning: TTL caching without ttlAutopurge, max, or maxSize can result in unbounded memory consumption.\"]\n```\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\n\n```\n\n### Prometheus configuration file\n\n```yaml\n\n```\n\n### Alertmanager version\n\n```text\n\n```\n\n### Alertmanager configuration file\n\n```yaml\n\n```\n\n### Logs\n\n```text\n\n```\n", "patch": "diff --git a/web/ui/module/codemirror-promql/src/client/prometheus.ts b/web/ui/module/codemirror-promql/src/client/prometheus.ts\nindex 72e34d4ec59..7fce7f1a178 100644\n--- a/web/ui/module/codemirror-promql/src/client/prometheus.ts\n+++ b/web/ui/module/codemirror-promql/src/client/prometheus.ts\n@@ -294,7 +294,7 @@ class Cache {\n   constructor(config?: CacheConfig) {\n     const maxAge = {\n       ttl: config && config.maxAge ? config.maxAge : 5 * 60 * 1000,\n-      ttlAutopurge: false,\n+      ttlAutopurge: true,\n     };\n     this.completeAssociation = new LRUCache<string, Map<string, Set<string>>>(maxAge);\n     this.metricMetadata = {};\n", "instance_id": "prometheus__prometheus-16024", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a warning is thrown due to the `ttlAutopurge` option being set to `false` in the LRU cache configuration when a `ttl` is specified. The goal is implied (to eliminate the warning by setting `ttlAutopurge` to `true`), and the context of the issue (using `@prometheus-io/codemirror-promql` in the Perses project) is provided. The expected behavior and the actual warning message are also included, which helps in understanding the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly state the desired solution (though it can be inferred), nor does it mention any potential side effects or constraints of changing `ttlAutopurge` to `true` (e.g., impact on cache behavior or performance). Additionally, edge cases or specific conditions under which the warning occurs are not detailed beyond the general usage in a React app. Overall, the statement is valid and mostly clear but lacks comprehensive details on constraints or potential impacts, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward fix with minimal code change. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes**: The modification is extremely localized, involving a single line change in one file (`prometheus.ts`) to toggle `ttlAutopurge` from `false` to `true`. There is no impact on the broader codebase architecture, and no interaction with other modules or components is required. The amount of code change is trivial.\n\n2. **Number of Technical Concepts**: The solution requires only a basic understanding of the LRU cache configuration options, specifically the `ttl` and `ttlAutopurge` settings. No advanced language features, algorithms, design patterns, or domain-specific knowledge are needed beyond basic familiarity with TypeScript/JavaScript and the library's documentation. The concept itself is simple and well-documented.\n\n3. **Edge Cases and Error Handling**: The problem statement does not mention any specific edge cases or error conditions to handle, and the code change does not introduce or modify any error handling logic. While enabling `ttlAutopurge` might have implications for cache behavior (e.g., automatic purging of expired entries), these are not discussed or required to be addressed in the provided context, and the fix itself does not necessitate additional logic for edge cases.\n\nGiven these points, the task is a very easy fix that requires only a basic code modification with no significant complexity or broader impact. A difficulty score of 0.1 reflects this as a very easy problem, akin to changing a configuration setting or fixing a minor oversight.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Prometheus logging lots of `superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation...` errors\n### What did you do?\n\nBuild and run Prometheus from current `main` with any config (a completely empty file will do).\n\n### What did you expect to see?\n\nNo errors being logged.\n\n### What did you see instead? Under which circumstances?\n\nThe terminal is being flooded with lots of errors like:\r\n\r\n```\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\nts=2024-09-07T19:25:28.219Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:78\" msg=)\r\n```\r\n\r\nFrom a `git bisect`, it seems like this issue was introduced in an automatic dependency upgrade in https://github.com/prometheus/prometheus/pull/14834#issuecomment-2334789149\n\n### System information\n\nLinux 6.10.7-arch1-1 x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.54.1 (branch: tarball, revision: 2.54.1)\r\n  build user:       someone@builder\r\n  build date:       20240827-21:37:11\r\n  go version:       go1.23.0\r\n  platform:         linux/amd64\r\n  tags:             unknown\n```\n\n\n### Prometheus configuration file\n\n```yaml\n(doesn't matter, happens with any config, even empty ones)\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nSee above.\n```\n\n", "patch": "diff --git a/go.mod b/go.mod\nindex 845e3277b84..4a2dd1c779b 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -64,7 +64,7 @@ require (\n \tgithub.com/vultr/govultr/v2 v2.17.2\n \tgo.opentelemetry.io/collector/pdata v1.14.1\n \tgo.opentelemetry.io/collector/semconv v0.108.1\n-\tgo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0\n+\tgo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0\n \tgo.opentelemetry.io/otel v1.29.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.29.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.29.0\n@@ -81,7 +81,7 @@ require (\n \tgolang.org/x/text v0.18.0\n \tgolang.org/x/time v0.6.0\n \tgolang.org/x/tools v0.24.0\n-\tgoogle.golang.org/api v0.196.0\n+\tgoogle.golang.org/api v0.195.0\n \tgoogle.golang.org/genproto/googleapis/api v0.0.0-20240827150818-7e3bb234dfed\n \tgoogle.golang.org/grpc v1.66.0\n \tgoogle.golang.org/protobuf v1.34.2\ndiff --git a/go.sum b/go.sum\nindex edb5b650bd4..4fc4f93bd81 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -736,8 +736,8 @@ go.opentelemetry.io/collector/pdata v1.14.1 h1:wXZjtQA7Vy5HFqco+yA95ENyMQU5heBB1\n go.opentelemetry.io/collector/pdata v1.14.1/go.mod h1:z1dTjwwtcoXxZx2/nkHysjxMeaxe9pEmYTEr4SMNIx8=\n go.opentelemetry.io/collector/semconv v0.108.1 h1:Txk9tauUnamZaxS5vlf1O0uZ4VD6nioRBR0nX8L/fU4=\n go.opentelemetry.io/collector/semconv v0.108.1/go.mod h1:zCJ5njhWpejR+A40kiEoeFm1xq1uzyZwMnRNX6/D82A=\n-go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0 h1:TT4fX+nBOA/+LUkobKGW1ydGcn+G3vRw9+g5HwCphpk=\n-go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0/go.mod h1:L7UH0GbB0p47T4Rri3uHjbpCFYrVrwc1I25QhNPiGK8=\n+go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0 h1:4K4tsIXefpVJtvA/8srF4V4y0akAoPHkIslgAkjixJA=\n+go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0/go.mod h1:jjdQuTGVsXV4vSs+CJ2qYDeDPf9yIJV23qlIzBm73Vg=\n go.opentelemetry.io/otel v1.29.0 h1:PdomN/Al4q/lN6iBJEN3AwPvUiHPMlt93c8bqTG5Llw=\n go.opentelemetry.io/otel v1.29.0/go.mod h1:N/WtXPs1CNCUEx+Agz5uouwCba+i+bJGFicT8SR4NP8=\n go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.29.0 h1:dIIDULZJpgdiHz5tXrTgKIMLkus6jEFa7x5SOKcyR7E=\n@@ -1056,8 +1056,8 @@ google.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/\n google.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\n google.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\n google.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\n-google.golang.org/api v0.196.0 h1:k/RafYqebaIJBO3+SMnfEGtFVlvp5vSgqTUF54UN/zg=\n-google.golang.org/api v0.196.0/go.mod h1:g9IL21uGkYgvQ5BZg6BAtoGJQIm8r6EgaAbpNey5wBE=\n+google.golang.org/api v0.195.0 h1:Ude4N8FvTKnnQJHU48RFI40jOBgIrL8Zqr3/QeST6yU=\n+google.golang.org/api v0.195.0/go.mod h1:DOGRWuv3P8TU8Lnz7uQc4hyNqrBpMtD9ppW3wBJurgc=\n google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\n google.golang.org/appengine v1.2.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n", "instance_id": "prometheus__prometheus-14884", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Prometheus is logging numerous errors related to \"superfluous response.WriteHeader call\" after a dependency upgrade. The goal (eliminate these errors) and the context (happens with any configuration, introduced by a specific PR) are provided, along with logs and system information. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly suggest a root cause or hypothesize why the dependency upgrade caused this issue, leaving some investigative work to the solver. Additionally, edge cases or specific scenarios that might exacerbate the issue (e.g., certain HTTP request patterns) are not mentioned. Overall, while the issue is well-documented, it lacks some depth in analysis or guidance on where to look for a solution, making it \"Mostly Clear.\"", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) for several reasons. First, the scope of code changes appears minimal, as the provided diff only involves downgrading a dependency version in the `go.mod` file (from `otelhttp v0.54.0` to `v0.53.0`), suggesting a straightforward fix by reverting or adjusting dependency versions. This does not require deep modifications to the codebase or architectural changes, nor does it impact multiple modules beyond the dependency configuration. Second, the technical concepts involved are relatively basic for a Go developer: understanding dependency management with `go.mod` and potentially debugging interactions with the `otelhttp` library. However, it does require some familiarity with OpenTelemetry instrumentation and HTTP middleware behavior to confirm the root cause, which adds a slight layer of complexity. Third, the problem does not explicitly mention edge cases or complex error handling requirements beyond suppressing the error logs, though understanding the HTTP response lifecycle might be necessary to ensure the fix does not introduce other issues. Overall, this problem requires moderate investigation and understanding of dependency-induced bugs but does not demand deep architectural changes or advanced technical knowledge, justifying a score of 0.30.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Retention size flag affects expected config file location\n### What did you do?\r\n\r\nThis works as expected:\r\n```\r\n$ docker run --network=host \\\r\n    -p 9090:9090 \\\r\n    -v `pwd`/prometheus.yml:/etc/prometheus/prometheus.yml \\\r\n    prom/prometheus\r\n...\r\nts=2023-03-08T00:39:38.453Z caller=main.go:978 level=info msg=\"Server is ready to receive web requests.\"\r\n```\r\n\r\nBut adding the `--storage.tsdb.retention.size` flag and changing nothing else causes prometheus to no longer start:\r\n\r\n```\r\n$ docker run --network=host \\\r\n    -p 9090:9090 \\\r\n    -v `pwd`/prometheus.yml:/etc/prometheus/prometheus.yml \\\r\n    prom/prometheus --storage.tsdb.retention.size=1GB\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nI expected prometheus to run just like the first command, but now with a retention size set.\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nWhen I add `--storage.tsdb.retention.size=1GB`, prometheus can't find the configs any more:\r\n```\r\n$ docker run --network=host \\\r\n    -p 9090:9090 \\\r\n    -v `pwd`/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus \\\r\n    --storage.tsdb.retention.size=1GB \r\nWARNING: Published ports are discarded when using host network mode\r\nts=2023-03-08T00:40:53.052Z caller=main.go:468 level=error msg=\"Error loading config (--config.file=prometheus.yml)\" file=/prometheus/prometheus.yml err=\"open prometheus.yml: no such file or directory\"\r\n```\r\n\r\nThis can be fixed by explicitly setting the config file location, but that's unexpected:\r\n```\r\n$ docker run --network=host \\\r\n    -p 9090:9090 \\\r\n    -v `pwd`/prometheus.yml:/etc/prometheus/prometheus.yml \\\r\n    prom/prometheus --storage.tsdb.retention.size=1GB --config.file=/etc/prometheus/prometheus.yml\r\n```\r\n\r\n### System information\r\n\r\nLinux 6.1.12 x86_64\r\n\r\n### Prometheus version\r\n\r\n```text\r\nversion=2.42.0, branch=HEAD, revision=225c61122d88b01d1f0eaaee0e05b6f3e0567ac0\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```yaml\r\nglobal:\r\n  scrape_interval: 15s\r\n  external_labels:\r\n    monitor: 'tamarack-monitoring'\r\nscrape_configs:\r\n  - job_name: 'prometheus'\r\n    static_configs:\r\n      - targets: ['localhost:9090']\r\n  - job_name: 'tamarack-api'\r\n    metrics_path: \"/actuator/prometheus\"\r\n    static_configs:\r\n      - targets: ['localhost:8081']\r\n```\r\n\r\n\r\n### Alertmanager version\r\n\r\n```text\r\nN/A\r\n```\r\n\r\n\r\n### Alertmanager configuration file\r\n\r\n```yaml\r\nN/A\r\n```\r\n\r\n\r\n### Logs\r\n\r\n```text\r\nN/A\r\n```\r\n\n", "patch": "diff --git a/docs/installation.md b/docs/installation.md\nindex 28f64c0f959..c8e359e7809 100644\n--- a/docs/installation.md\n+++ b/docs/installation.md\n@@ -31,11 +31,19 @@ production deployments it is highly recommended to use a\n [named volume](https://docs.docker.com/storage/volumes/)\n to ease managing the data on Prometheus upgrades.\n \n-To provide your own configuration, there are several options. Here are\n-two examples.\n+### Setting command line parameters\n+\n+The Docker image is started with a number of default command line parameters, which\n+can be found in the [Dockerfile](https://github.com/prometheus/prometheus/blob/main/Dockerfile) (adjust the link to correspond with the version in use).\n+\n+If you want to add extra command line parameters to the `docker run` command,\n+you will need to re-add these yourself as they will be overwritten.\n \n ### Volumes & bind-mount\n \n+To provide your own configuration, there are several options. Here are\n+two examples.\n+\n Bind-mount your `prometheus.yml` from the host by running:\n \n ```bash\n", "instance_id": "prometheus__prometheus-14194", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when the `--storage.tsdb.retention.size` flag is added to the Docker run command for Prometheus, the configuration file is no longer found, leading to a failure to start. The statement includes detailed examples of commands, expected behavior, actual behavior, and a workaround (explicitly setting the config file location). It also provides system information, Prometheus version, and configuration details, which are helpful for context. However, there are minor ambiguities: the root cause of why adding the flag affects the config file location is not explained in the problem statement, and there are no explicit mentions of edge cases or constraints related to other flags or environments. Additionally, the problem statement does not clarify whether this is a bug in Prometheus, a Docker configuration issue, or expected behavior that needs better documentation. Despite these minor gaps, the issue is well-documented with reproducible steps, earning it a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as the code change provided is a simple documentation update in `installation.md` to inform users about the behavior of command line parameters in the Docker image. Let's break it down by the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modification is limited to a single documentation file (`installation.md`) and involves adding a small section (about 8 lines) to explain that default command line parameters are overwritten when custom parameters are added via `docker run`. There is no impact on the actual codebase or system architecture, as this is purely a documentation fix. The change is minimal and isolated.\n\n2. **Number of Technical Concepts:** The solution requires only basic knowledge of Docker and command line parameter handling. No advanced programming language features, libraries, algorithms, or design patterns are involved. The concept is straightforward\u2014understanding that Docker command line arguments can override defaults, which is a basic operational detail rather than a complex technical challenge.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement and code change do not address any specific edge cases or error handling requirements. The documentation update is purely informational and does not involve modifying or adding error handling logic in the codebase. There are no complex edge cases to consider in the provided solution.\n\n4. **Overall Assessment:** Given the simplicity of the change (a documentation update), the lack of impact on the actual codebase, and the minimal technical knowledge required, this task falls into the \"Very Easy\" category. It does not involve debugging, coding, or deep understanding of Prometheus internals\u2014just a clarification for users. Therefore, I assign a difficulty score of 0.15, reflecting a very easy task that requires only basic modifications to documentation.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Youtube shortcode now disables fullscreen by default\nThe changes introduced to the `youtube` shortcode via\n\n- #13532 \n\ntoggled the default for `fullscreen` from true (up until v0.145.x) to false in the latest release.\n\nIf you build the attached minimal sample project this is the diff (over the prettified) HTML output of a page using the shortcode:\n\n```diff\n5c5\n<   <meta name=\"generator\" content=\"Hugo 0.145.0\">\n---\n>   <meta name=\"generator\" content=\"Hugo 0.146.7\">\n14d13\n<         allowfullscreen=\"allowfullscreen\"\n```\n\nNotice that the `allowfullscreen` attribute is gone in v0.146. We should be seeing the addition of `allow=\"fullscreen; ...\"`. Can we recover the old default?\n\n\ud83d\udcce Test project: [hugo-youtube-demo.zip](https://github.com/user-attachments/files/19894972/hugo-youtube-demo.zip) (updated)\n", "patch": "diff --git a/tpl/tplimpl/embedded/templates/_shortcodes/vimeo.html b/tpl/tplimpl/embedded/templates/_shortcodes/vimeo.html\nindex 3ce470c6e6d..2588ac86cc1 100644\n--- a/tpl/tplimpl/embedded/templates/_shortcodes/vimeo.html\n+++ b/tpl/tplimpl/embedded/templates/_shortcodes/vimeo.html\n@@ -4,11 +4,11 @@\n Accepts named or positional arguments. If positional, order is id, class,\n title, then loading.\n \n-@param {string} [id] The video id. Optional if the id is provided as first positional argument.\n+@param {bool} [allowFullScreen=true] Whether the iframe element can activate full screen mode.\n @param {string} [class] The class attribute of the wrapping div element. When specified, removes the style attributes from the iframe element and its wrapping div element.\n+@param {string} [id] The video id. Optional if the id is the first and only positional argument.\n @param {string} [loading=eager] The loading attribute of the iframe element.\n @param {string} [title=Vimeo video] The title attribute of the iframe element.\n-@param {bool} [allowFullScreen=true] Whether the iframe element can activate full screen mode.\n \n @returns {template.HTML}\n \n@@ -22,16 +22,16 @@\n   {{- else }}\n     {{- $dnt := cond $pc.EnableDNT 1 0 }}\n \n-    {{- $id := or (.Get \"id\") (.Get 0) \"\" }}\n-    {{- $class := or (.Get \"class\") (.Get 1) \"\" }}\n-    {{- $title := or (.Get \"title\") (.Get 2) \"Vimeo video\" }}\n-    {{- $loading := or (.Get \"loading\") (.Get 3) \"eager\" }}\n-    {{- $allowFullScreen := or (.Get \"allowFullScreen\") (.Get 4) true }}\n+    {{- $allowFullScreen := true }}\n+    {{- $class := or (.Get \"class\") }}\n+    {{- $id := or (.Get \"id\") (.Get 0) }}\n+    {{- $loading := or (.Get \"loading\") }}\n+    {{- $title := or (.Get \"title\") }}\n \n-    {{- if in (slice \"false\" false 0) ($.Get \"allowFullScreen\") }}\n-      {{- $allowFullScreen = false }}\n-    {{- else if in (slice \"true\" true 1) ($.Get \"allowFullScreen\") }}\n+    {{- if in (slice \"true\" true 1) (.Get \"allowFullScreen\") }}\n       {{- $allowFullScreen = true }}\n+    {{- else if in (slice \"false\" false 0) (.Get \"allowFullScreen\") }}\n+      {{- $allowFullScreen = false }}\n     {{- end }}\n \n     {{- $iframeAllowList := \"\" }}\ndiff --git a/tpl/tplimpl/embedded/templates/_shortcodes/youtube.html b/tpl/tplimpl/embedded/templates/_shortcodes/youtube.html\nindex cebe50626ca..18b0869445f 100644\n--- a/tpl/tplimpl/embedded/templates/_shortcodes/youtube.html\n+++ b/tpl/tplimpl/embedded/templates/_shortcodes/youtube.html\n@@ -6,7 +6,7 @@\n @param {string} [class] The class attribute of the wrapping div element. When specified, removes the style attributes from the iframe element and its wrapping div element.\n @param {bool} [controls=true] Whether to display the video controls.\n @param {int} [end] The time, measured in seconds from the start of the video, when the player should stop playing the video.\n-@param {string} [id] The video id. Optional if the id is provided as first positional argument.\n+@param {string} [id] The video id. Optional if the id is the first and only positional argument.\n @param {string} [loading=eager] The loading attribute of the iframe element.\n @param {bool} [loop=false] Whether to indefinitely repeat the video. Ignores the start and end arguments after the first play.\n @param {bool} [mute=false] Whether to mute the video. Always true when autoplay is true.\n@@ -41,27 +41,29 @@\n \n     {{- /* Get arguments. */}}\n     {{- if in (slice \"true\" true 1) ($.Get \"allowFullScreen\") }}\n-      {{- $iframeAllowList = printf \"%s; fullscreen\" $iframeAllowList }}\n+      {{- $allowFullScreen = true }}\n+    {{- else if in (slice \"false\" false 0) ($.Get \"allowFullScreen\") }}\n+      {{- $allowFullScreen = false }}\n     {{- end }}\n-    {{- if in (slice \"false\" false 0) ($.Get \"autoplay\") }}\n-      {{- $autoplay = 0 }}\n-    {{- else if in (slice \"true\" true 1) ($.Get \"autoplay\") }}\n+    {{- if in (slice \"true\" true 1) ($.Get \"autoplay\") }}\n       {{- $autoplay = 1 }}\n+    {{- else if in (slice \"false\" false 0) ($.Get \"autoplay\") }}\n+      {{- $autoplay = 0 }}\n     {{- end }}\n-    {{- if in (slice \"false\" false 0) ($.Get \"controls\") }}\n-      {{- $controls = 0 }}\n-    {{- else if in (slice \"true\" true 1) ($.Get \"controls\") }}\n+    {{- if in (slice \"true\" true 1) ($.Get \"controls\") }}\n       {{- $controls = 1 }}\n+    {{- else if in (slice \"false\" false 0) ($.Get \"controls\") }}\n+      {{- $controls = 0 }}\n     {{- end }}\n-    {{- if in (slice \"false\" false 0) ($.Get \"loop\") }}\n-      {{- $loop = 0 }}\n-    {{- else if in (slice \"true\" true 1) ($.Get \"loop\") }}\n+    {{- if in (slice \"true\" true 1) ($.Get \"loop\") }}\n       {{- $loop = 1 }}\n+    {{- else if in (slice \"false\" false 0) ($.Get \"loop\") }}\n+      {{- $loop = 0 }}\n     {{- end }}\n-    {{- if in (slice \"false\" false 0) ($.Get \"mute\") }}\n-      {{- $mute = 0 }}\n-    {{- else if or (in (slice \"true\" true 1) ($.Get \"mute\")) $autoplay }}\n+    {{- if or (in (slice \"true\" true 1) ($.Get \"mute\")) $autoplay }}\n       {{- $mute = 1 }}\n+    {{- else if in (slice \"false\" false 0) ($.Get \"mute\") }}\n+      {{- $mute = 0 }}\n     {{- end }}\n     {{- $class := or ($.Get \"class\") $class }}\n     {{- $end := or ($.Get \"end\") $end }}\n@@ -69,6 +71,11 @@\n     {{- $start := or ($.Get \"start\") $start }}\n     {{- $title := or ($.Get \"title\") $title }}\n \n+    {{- /* Adjust iframeAllowList. */}}\n+    {{- if $allowFullScreen }}\n+      {{- $iframeAllowList = printf \"%s; fullscreen\" $iframeAllowList }}\n+    {{- end }}\n+\n     {{- /* Define src attribute. */}}\n     {{- $host := cond $pc.PrivacyEnhanced \"www.youtube-nocookie.com\" \"www.youtube.com\" }}\n     {{- $src := printf \"https://%s/embed/%s\" $host $id }}\n", "instance_id": "gohugoio__hugo-13652", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the default behavior of the `fullscreen` attribute in the `youtube` shortcode has changed from `true` to `false` in a recent version of Hugo, and the goal is to recover the old default behavior. It provides a diff of the HTML output to illustrate the issue and references a specific pull request (#13532) for context. Additionally, a test project is attached for reproduction. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify whether the solution should involve changing the default back to `true` globally or providing a way for users to override it. It also lacks mention of potential compatibility issues or constraints when reverting the behavior. Edge cases, such as how this change might interact with other shortcode parameters or privacy settings, are not addressed. Overall, while the core issue is clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is relatively small, primarily affecting two template files (`youtube.html` and `vimeo.html`) within the Hugo codebase. The modifications involve reordering parameter handling logic and adjusting how the `allowFullScreen` attribute is processed and applied to the `iframe` element. The changes do not impact the broader system architecture or require deep refactoring. Second, the technical concepts involved are straightforward: understanding Hugo's shortcode template syntax, basic conditional logic, and HTML `iframe` attributes. No advanced algorithms, design patterns, or domain-specific knowledge are required. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest minimal complexity in error handling\u2014mostly ensuring correct parsing of boolean inputs for `allowFullScreen` and related parameters. The primary challenge lies in understanding the intent of the original change (disabling fullscreen by default) and ensuring the fix aligns with user expectations without introducing regressions. Overall, this task requires moderate familiarity with the Hugo templating system and careful attention to detail, but it remains a relatively simple bug fix or feature adjustment.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Enabling authentication causes noisy logs for every /readyz call\n### Bug report criteria\n\n- [X] This bug report is not security related, security issues should be disclosed privately via [etcd maintainers](mailto:etcd-maintainers@googlegroups.com).\n- [X] This is not a support request or question, support requests or questions should be raised in the etcd [discussion forums](https://github.com/etcd-io/etcd/discussions).\n- [X] You have read the etcd [bug reporting guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md).\n- [X] Existing open issues along with etcd [frequently asked questions](https://etcd.io/docs/latest/faq) have been checked and this is not a duplicate.\n\n### What happened?\n\nWe run etcd with authentication enabled, we have a service discovery system that frequently polls the `/readyz` HTTP endpoint on etcd server. For every `/readyz` request, etcd prints two log statements  saying `\"deleted a simple token\"`. Example output:\r\n\r\n```\r\n{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:48.638643-0700\",\r\n  \"caller\":\"auth/simple_token.go:170\",\r\n  \"msg\":\"deleted a simple token\",\r\n  \"user-name\":\"root\",\"token\":\"swUCgfCSXzbNbVvM.0\"}\r\n...\r\n```\r\n\r\nCombined with the fact that our service discovery system polls this endpoint every 5 seconds, practically this statement single handedly emits most of the logs we get out of etcd.\n\n### What did you expect to happen?\n\nNo logs at the default level for something inconsequential like this.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Run etcd server on a macOS terminal\r\n\r\n       etcd --auth-token-ttl=5\r\n\r\n2. In another terminal window, add a user, then enable authentication\r\n\r\n       etcdctl user add root:root\r\n       etcdctl auth enable\r\n\r\n3. Make a few successive /readyz queries back to back\r\n\r\n       curl 127.0.0.1:2379/readyz \r\n\r\n4. Wait 5 seconds, and observe logs are flooded with this:\r\n\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:48.638643-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"swUCgfCSXzbNbVvM.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:48.638762-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"HbaByRCspoGGJDSQ.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638493-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"JHPWIPccneJuHHVo.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638675-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"bPwzKSxHgcHAqDWY.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638696-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"vHLnwqtzzYivgdjW.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638828-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"JorAHnUcTharTcGT.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638845-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"fEcjpFDKzbpmdymM.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638861-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"nqjiEIoiBHwIMEIz.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638875-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"AdmAVtAzjwEdYnmV.0\"}\r\n\t\t{\"level\":\"info\",\"ts\":\"2024-06-28T11:55:50.638888-0700\",\"caller\":\"auth/simple_token.go:170\",\"msg\":\"deleted a simple token\",\"user-name\":\"root\",\"token\":\"lzlBlKmLtPrTxzrh.0\"}\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Etcd version (please run commands below)\n\n<details>\r\n\r\n```console\r\n$ etcd --version\r\netcd Version: 3.5.13\r\nGit SHA: c9063a0dc\r\nGo Version: go1.22.1\r\nGo OS/Arch: darwin/arm64\r\n\r\n$ etcdctl version\r\netcdctl version: 3.5.13\r\nAPI version: 3.5\r\n```\r\n\r\n</details>\r\n\n\n### Etcd configuration (command line flags or environment variables)\n\n```\r\netcd --auth-token-ttl=5\r\n```\r\n(shortened the TTL to illustrate the problem).\n\n### Etcd debug information (please run commands below, feel free to obfuscate the IP address or FQDN in the output)\n\nNot applicable.\n\n### Relevant log output\n\n_No response_\n", "patch": "diff --git a/server/auth/simple_token.go b/server/auth/simple_token.go\nindex be706522a53..a657d9daefc 100644\n--- a/server/auth/simple_token.go\n+++ b/server/auth/simple_token.go\n@@ -167,7 +167,7 @@ func (t *tokenSimple) enable() {\n \n \tdelf := func(tk string) {\n \t\tif username, ok := t.simpleTokens[tk]; ok {\n-\t\t\tt.lg.Info(\n+\t\t\tt.lg.Debug(\n \t\t\t\t\"deleted a simple token\",\n \t\t\t\tzap.String(\"user-name\", username),\n \t\t\t\tzap.String(\"token\", tk),\n", "instance_id": "etcd-io__etcd-18265", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue of noisy logs generated by the `/readyz` endpoint when authentication is enabled in etcd, including detailed reproduction steps, expected behavior, and actual behavior with log output examples. The goal is explicit: reduce log noise at the default logging level for inconsequential actions like token deletion. The input (enabling authentication and polling `/readyz`), output (excessive log messages), and constraints (default log level behavior) are well-defined. Additionally, the context of the issue, including the etcd version, configuration, and environment, is provided, leaving no significant ambiguities. The inclusion of a bug report checklist and adherence to guidelines further enhances clarity.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is straightforward: excessive logging at the INFO level for token deletion events triggered by frequent `/readyz` calls. The provided code change is minimal, involving a single-line modification in `server/auth/simple_token.go` to switch the log level from `Info` to `Debug`, which effectively reduces log noise at the default logging level. This change does not require deep understanding of the codebase or complex logic, as it is a simple adjustment to the logging behavior. No new features, algorithms, or architectural changes are involved, and the scope is limited to a single function in one file. There are no edge cases or error handling requirements mentioned or needed for this fix, as it is purely a logging level adjustment. The technical concepts involved are basic\u2014understanding logging levels and their impact\u2014which are trivial for even a junior developer. The only minor complexity is ensuring that changing the log level does not hide critical information, but since the problem statement and logs indicate this is inconsequential, the risk is negligible.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
