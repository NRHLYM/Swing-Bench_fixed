{"repo": "ashvardanian/StringZilla", "instance_id": "ashvardanian__StringZilla-202", "base_commit": "645539b468f3c2902061425684d9b002c43a14f7", "patch": "diff --git a/include/stringzilla/stringzilla.hpp b/include/stringzilla/stringzilla.hpp\nindex a80da804..43869f08 100644\n--- a/include/stringzilla/stringzilla.hpp\n+++ b/include/stringzilla/stringzilla.hpp\n@@ -283,7 +283,7 @@ class basic_charset {\n     template <std::size_t count_characters>\n     explicit basic_charset(char_type const (&chars)[count_characters]) noexcept : basic_charset() {\n         static_assert(count_characters > 0, \"Character array cannot be empty\");\n-        for (std::size_t i = 0; i < count_characters - 1; ++i) { // count_characters - 1 to exclude the null terminator\n+        for (std::size_t i = 0; i != count_characters; ++i) {\n             char_type c = chars[i];\n             bitset_._u64s[sz_bitcast(sz_u8_t, c) >> 6] |= (1ull << (sz_bitcast(sz_u8_t, c) & 63u));\n         }\n@@ -292,7 +292,7 @@ class basic_charset {\n     template <std::size_t count_characters>\n     explicit basic_charset(std::array<char_type, count_characters> const &chars) noexcept : basic_charset() {\n         static_assert(count_characters > 0, \"Character array cannot be empty\");\n-        for (std::size_t i = 0; i < count_characters - 1; ++i) { // count_characters - 1 to exclude the null terminator\n+        for (std::size_t i = 0; i != count_characters; ++i) {\n             char_type c = chars[i];\n             bitset_._u64s[sz_bitcast(sz_u8_t, c) >> 6] |= (1ull << (sz_bitcast(sz_u8_t, c) & 63u));\n         }\n", "test_patch": "diff --git a/scripts/test.cpp b/scripts/test.cpp\nindex eecc97f0..72379f78 100644\n--- a/scripts/test.cpp\n+++ b/scripts/test.cpp\n@@ -137,6 +137,49 @@ static void test_arithmetical_utilities() {\n                    (static_cast<sz_u8_t>(number) / static_cast<sz_u8_t>(divisor)));\n }\n \n+/**\n+ *  @brief  Tests various ASCII-based methods (e.g., `is_alpha`, `is_digit`)\n+ *          provided by `sz::string` and `sz::string_view`.\n+ */\n+template <typename string_type>\n+static void test_ascii_utilities() {\n+\n+    using str = string_type;\n+\n+    assert(!str(\"\").is_alpha());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\").is_alpha());\n+    assert(!str(\"abc9\").is_alpha());\n+\n+    assert(!str(\"\").is_alnum());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\").is_alnum());\n+    assert(!str(\"abc!\").is_alnum());\n+\n+    assert(str(\"\").is_ascii());\n+    assert(str(\"\\x00x7F\").is_ascii());\n+    assert(!str(\"abc123\ud83d\udd25\").is_ascii());\n+\n+    assert(!str(\"\").is_digit());\n+    assert(str(\"0123456789\").is_digit());\n+    assert(!str(\"012a\").is_digit());\n+\n+    assert(!str(\"\").is_lower());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyz\").is_lower());\n+    assert(!str(\"abcA\").is_lower());\n+    assert(!str(\"abc\\n\").is_lower());\n+\n+    assert(!str(\"\").is_space());\n+    assert(str(\" \\t\\n\\r\\f\\v\").is_space());\n+    assert(!str(\" \\t\\r\\na\").is_space());\n+\n+    assert(!str(\"\").is_upper());\n+    assert(str(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\").is_upper());\n+    assert(!str(\"ABCa\").is_upper());\n+\n+    assert(str(\"\").is_printable());\n+    assert(str(\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!@#$%^&*()_+\").is_printable());\n+    assert(!str(\"012\ud83d\udd25\").is_printable());\n+}\n+\n inline void expect_equality(char const *a, char const *b, std::size_t size) {\n     if (std::memcmp(a, b, size) == 0) return;\n     std::size_t mismatch_position = 0;\n@@ -1583,6 +1626,8 @@ int main(int argc, char const **argv) {\n \n     // Basic utilities\n     test_arithmetical_utilities();\n+    test_ascii_utilities<sz::string>();\n+    test_ascii_utilities<sz::string_view>();\n     test_memory_utilities();\n     test_replacements();\n \n", "problem_statement": "Bug: Last elements in `basic_charset` initialization are discarded\n### Describe the bug\r\n\r\nWhen initializing a `basic_charset` with character arrays such as those returned by `digits()` and others, the last element of the array is incorrectly discarded. \r\n\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L192-L195\r\n\r\nThis is due to the use of `count_characters - 1` in the loop that populates the `basic_charset` bitset. \r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L283-L289\r\n\r\nWhile this prevents including a null terminator for string literals like `sz::char_set(\"x\")`, it causes incorrect behavior when handling character arrays that do not have a null terminator, resulting in the exclusion of the final character.\r\n\r\nI believe this local block is the full extent of the affected code.\r\n\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L330-L341\r\n\r\n### Steps to reproduce\r\n\r\n```cpp\r\n#include \"stringzilla/stringzilla.hpp\"\r\n\r\nnamespace sz = ashvardanian::stringzilla;\r\n\r\nint main() {\r\n        sz::string haystack = \"239\";\r\n\r\n        // Test with null-terminated string\r\n        assert(haystack.contains_only(sz::char_set(\"0123456789\")));\r\n        // Passes: null terminator is correctly discarded\r\n\r\n        // Test with initializer list\r\n        static std::initializer_list all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n        assert (haystack.contains_only(sz::char_set {all}));\r\n        // Passes: constructor for initializer list is called\r\n\r\n        // Test with carray\r\n        sz::carray<10> all_digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n        assert(haystack.contains_only(sz::char_set {all_digits}));\r\n        assert(haystack.is_digit());\r\n        // Fails: '9' is incorrectly discarded\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo asserts\r\n\r\n### StringZilla version\r\n\r\nv3.11.0\r\n\r\n### Operating System\r\n\r\nUbuntu 22.04.5\r\n\r\n### Hardware architecture\r\n\r\nx86\r\n\r\n### Which interface are you using?\r\n\r\nC++ bindings\r\n\r\n### Contact Details\r\n\r\n_No response_\r\n\r\n### Are you open to being tagged as a contributor?\r\n\r\n- [X] I am open to being mentioned in the project `.git` history as a contributor\r\n\r\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's Code of Conduct\n", "hints_text": "I will be able to write fix and add tests tomorrow\r\n\r\n**Proposed Fix**\r\n\r\n1. **Using `carray` with Null Terminator**\r\n\r\n```cpp\r\ninline carray<11> const &digits() noexcept { \r\n    static carray<11> const all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\0'}; \r\n    return all;\r\n}\r\n```\r\n\r\n2. **Using `std::initializer_list<char>`**\r\n\r\n```cpp\r\ninline auto const &digits() noexcept {\r\n    static std::initializer_list all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n    return all;\r\n}\r\n```\r\n\r\nThen the constructor for the initializer list will be called here:\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L335-L335\r\n\r\n\nThank you @alexbarev! Let's start with tests, and later merge a patch.", "created_at": "2024-12-07 18:17:07", "merge_commit_sha": "002e433c0265f4bb7628988f1b6b29d45ba53e03", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Update Version', '.github/workflows/prerelease.yml']", "['Swift on Linux', '.github/workflows/prerelease.yml']"], ["['Ubuntu (Clang 16)', '.github/workflows/prerelease.yml']", "['Cross Compilation (amd64, x86_64-linux-gnu)', '.github/workflows/prerelease.yml']"], ["['Windows', '.github/workflows/prerelease.yml']", "['Alpine Linux', '.github/workflows/prerelease.yml']"]]}
{"repo": "duesee/imap-codec", "instance_id": "duesee__imap-codec-561", "base_commit": "6f3db33569653c100e12332241c39bd37232d9ff", "patch": "diff --git a/bindings/imap-codec-python/imap_codec.pyi b/bindings/imap-codec-python/imap_codec.pyi\nindex 89874e45..33e0544a 100644\n--- a/bindings/imap-codec-python/imap_codec.pyi\n+++ b/bindings/imap-codec-python/imap_codec.pyi\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import Tuple\n+from typing import Tuple, Union\n \n class DecodeError(Exception):\n     \"\"\"\n@@ -25,6 +25,72 @@ class DecodeLiteralFound(DecodeError):\n     The decoder stopped at the beginning of literal data.\n     \"\"\"\n \n+class LiteralMode:\n+    \"\"\"\n+    Literal mode, i.e., sync or non-sync.\n+\n+    - Sync: A synchronizing literal, i.e., `{<n>}\\r\\n<data>`.\n+    - NonSync: A non-synchronizing literal according to RFC 7888, i.e., `{<n>+}\\r\\n<data>`.\n+\n+    Warning: The non-sync literal extension must only be used when the server advertised support\n+             for it sending the LITERAL+ or LITERAL- capability.\n+    \"\"\"\n+\n+    Sync: LiteralMode\n+    NonSync: LiteralMode\n+\n+class LineFragment:\n+    \"\"\"\n+    Fragment of a line that is ready to be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes):\n+        \"\"\"\n+        Create a line fragment from data bytes\n+\n+        :param data: Data bytes of fragment\n+        :raises TypeError: `data` is not byte-like\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get line fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+class LiteralFragment:\n+    \"\"\"\n+    Fragment of a literal that may require an action before it should be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes, mode: LiteralMode):\n+        \"\"\"\n+        Create a literal fragment from data bytes and literal mode\n+\n+        :param data: Data bytes of fragment\n+        :param mode: Literal mode\n+        :raises TypeError: `data` is not byte-like\n+        :raises TypeError: `mode` is invalid\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get literal fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+    @property\n+    def mode(self) -> LiteralMode:\n+        \"\"\"\n+        Get literal fragment literal mode\n+\n+        :return: Literal mode\n+        \"\"\"\n+\n class Encoded:\n     \"\"\"\n     An encoded message.\n@@ -36,7 +102,7 @@ class Encoded:\n     \"\"\"\n \n     def __iter__(self) -> Encoded: ...\n-    def __next__(self) -> dict: ...\n+    def __next__(self) -> Union[LineFragment, LiteralFragment]: ...\n     def dump(self) -> bytes:\n         \"\"\"\n         Dump the (remaining) encoded data without being guided by fragments.\ndiff --git a/bindings/imap-codec-python/src/encoded.rs b/bindings/imap-codec-python/src/encoded.rs\nnew file mode 100644\nindex 00000000..d896933e\n--- /dev/null\n+++ b/bindings/imap-codec-python/src/encoded.rs\n@@ -0,0 +1,150 @@\n+use imap_codec::{\n+    encode::{Encoded, Fragment},\n+    imap_types::core::LiteralMode,\n+};\n+use pyo3::{prelude::*, types::PyBytes};\n+\n+/// Python class representing a literal mode\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+#[pyclass(name = \"LiteralMode\", eq)]\n+pub(crate) enum PyLiteralMode {\n+    Sync,\n+    NonSync,\n+}\n+\n+impl std::fmt::Display for PyLiteralMode {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match self {\n+            PyLiteralMode::Sync => f.write_str(\"LiteralMode.Sync\"),\n+            PyLiteralMode::NonSync => f.write_str(\"LiteralMode.NonSync\"),\n+        }\n+    }\n+}\n+\n+impl From<LiteralMode> for PyLiteralMode {\n+    fn from(value: LiteralMode) -> Self {\n+        match value {\n+            LiteralMode::Sync => PyLiteralMode::Sync,\n+            LiteralMode::NonSync => PyLiteralMode::NonSync,\n+        }\n+    }\n+}\n+\n+/// Python class representing a line fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LineFragment\", eq)]\n+pub(crate) struct PyLineFragment {\n+    data: Vec<u8>,\n+}\n+\n+#[pymethods]\n+impl PyLineFragment {\n+    /// Create a new line fragment from data\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn new(data: Vec<u8>) -> Self {\n+        Self { data }\n+    }\n+\n+    /// Retrieve the data from the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// String representation of the fragment, e.g. `b'hello'`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        self.data(py).to_string()\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LineFragment(b'hello')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LineFragment({})\", self.__str__(py))\n+    }\n+}\n+\n+/// Python class representing a literal fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LiteralFragment\", eq)]\n+pub(crate) struct PyLiteralFragment {\n+    data: Vec<u8>,\n+    mode: PyLiteralMode,\n+}\n+\n+#[pymethods]\n+impl PyLiteralFragment {\n+    /// Create a new literal fragment from data and mode\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn try_new(data: Vec<u8>, mode: PyLiteralMode) -> PyResult<Self> {\n+        Ok(Self { data, mode })\n+    }\n+\n+    /// Retrieve the data of the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// Retrieve the mode of the fragment\n+    #[getter]\n+    pub(crate) fn mode(&self) -> PyLiteralMode {\n+        self.mode\n+    }\n+\n+    /// String representation of the fragment, e.g. `(b'hello', 'Sync')`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        format!(\"({}, {})\", self.data(py), self.mode)\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LiteralFragment(b'hello', 'Sync')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LiteralFragment{}\", self.__str__(py))\n+    }\n+}\n+\n+/// Python wrapper classes for `Encoded`\n+///\n+/// This implements a Python iterator over the containing fragments.\n+#[derive(Debug, Clone)]\n+#[pyclass(name = \"Encoded\")]\n+pub(crate) struct PyEncoded(pub(crate) Option<Encoded>);\n+\n+#[pymethods]\n+impl PyEncoded {\n+    /// Initialize iterator\n+    pub(crate) fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n+        slf\n+    }\n+\n+    /// Return next fragment\n+    pub(crate) fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<PyObject>> {\n+        // Try to get next `Fragment` from `Encoded` iterator\n+        let Some(fragment) = slf.0.as_mut().and_then(|encoded| encoded.next()) else {\n+            return Ok(None);\n+        };\n+\n+        // Return instance of `PyLineFragment` or `PyLiteralFragment` as a generic `PyObject`.\n+        Ok(Some(match fragment {\n+            Fragment::Line { data } => {\n+                Bound::new(slf.py(), PyLineFragment::new(data))?.to_object(slf.py())\n+            }\n+            Fragment::Literal { data, mode } => {\n+                Bound::new(slf.py(), PyLiteralFragment::try_new(data, mode.into())?)?\n+                    .to_object(slf.py())\n+            }\n+        }))\n+    }\n+\n+    /// Dump remaining fragment data\n+    pub(crate) fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n+        let encoded = slf.0.take();\n+        let dump = match encoded {\n+            Some(encoded) => encoded.dump(),\n+            None => Vec::new(),\n+        };\n+        Ok(PyBytes::new_bound(slf.py(), &dump))\n+    }\n+}\ndiff --git a/bindings/imap-codec-python/src/lib.rs b/bindings/imap-codec-python/src/lib.rs\nindex 23ee0717..da568fb4 100644\n--- a/bindings/imap-codec-python/src/lib.rs\n+++ b/bindings/imap-codec-python/src/lib.rs\n@@ -1,6 +1,9 @@\n+mod encoded;\n+\n+use encoded::PyEncoded;\n use imap_codec::{\n     decode::{self, Decoder},\n-    encode::{Encoded, Encoder},\n+    encode::Encoder,\n     AuthenticateDataCodec, CommandCodec, GreetingCodec, IdleDoneCodec, ResponseCodec,\n };\n use pyo3::{create_exception, exceptions::PyException, prelude::*, types::PyBytes};\n@@ -11,46 +14,10 @@ create_exception!(imap_codec, DecodeFailed, DecodeError);\n create_exception!(imap_codec, DecodeIncomplete, DecodeError);\n create_exception!(imap_codec, DecodeLiteralFound, DecodeError);\n \n-/// Wrapper for `Encoded`\n-///\n-/// This implements a Python iterator over the containing fragments.\n-#[derive(Debug, Clone)]\n-#[pyclass(name = \"Encoded\")]\n-struct PyEncoded(Option<Encoded>);\n-\n-#[pymethods]\n-impl PyEncoded {\n-    /// Initialize iterator\n-    fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n-        slf\n-    }\n-\n-    /// Return next fragment\n-    fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<Bound<PyAny>>> {\n-        let Some(encoded) = &mut slf.0 else {\n-            return Ok(None);\n-        };\n-        Ok(encoded\n-            .next()\n-            .map(|value| serde_pyobject::to_pyobject(slf.py(), &value))\n-            .transpose()?)\n-    }\n-\n-    /// Dump remaining fragment data\n-    fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n-        let encoded = slf.0.take();\n-        let dump = match encoded {\n-            Some(encoded) => encoded.dump(),\n-            None => Vec::new(),\n-        };\n-        Ok(PyBytes::new_bound(slf.py(), &dump))\n-    }\n-}\n-\n-/// Wrapper for `GreetingCodec`\n+/// Python class for using `GreetingCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"GreetingCodec\")]\n-struct PyGreetingCodec(GreetingCodec);\n+struct PyGreetingCodec;\n \n #[pymethods]\n impl PyGreetingCodec {\n@@ -80,10 +47,10 @@ impl PyGreetingCodec {\n     }\n }\n \n-/// Wrapper for `CommandCodec`\n+/// Python class for using `CommandCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"CommandCodec\")]\n-struct PyCommandCodec(CommandCodec);\n+struct PyCommandCodec;\n \n #[pymethods]\n impl PyCommandCodec {\n@@ -119,10 +86,10 @@ impl PyCommandCodec {\n     }\n }\n \n-/// Wrapper for `AuthenticateDataCodec`\n+/// Python class for using `AuthenticateDataCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"AuthenticateDataCodec\")]\n-struct PyAuthenticateDataCodec(AuthenticateDataCodec);\n+struct PyAuthenticateDataCodec;\n \n #[pymethods]\n impl PyAuthenticateDataCodec {\n@@ -151,10 +118,10 @@ impl PyAuthenticateDataCodec {\n     }\n }\n \n-/// Wrapper for `ResponseCodec`\n+/// Python class for using `ResponseCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"ResponseCodec\")]\n-struct PyResponseCodec(ResponseCodec);\n+struct PyResponseCodec;\n \n #[pymethods]\n impl PyResponseCodec {\n@@ -188,10 +155,10 @@ impl PyResponseCodec {\n     }\n }\n \n-/// Wrapper for `IdleDoneCodec`\n+/// Python class for using `IdleDoneCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"IdleDoneCodec\")]\n-struct PyIdleDoneCodec(IdleDoneCodec);\n+struct PyIdleDoneCodec;\n \n #[pymethods]\n impl PyIdleDoneCodec {\n@@ -233,6 +200,9 @@ fn imap_codec_python(m: &Bound<'_, PyModule>) -> PyResult<()> {\n         \"DecodeLiteralFound\",\n         m.py().get_type_bound::<DecodeLiteralFound>(),\n     )?;\n+    m.add_class::<encoded::PyLiteralMode>()?;\n+    m.add_class::<encoded::PyLineFragment>()?;\n+    m.add_class::<encoded::PyLiteralFragment>()?;\n     m.add_class::<PyEncoded>()?;\n     m.add_class::<PyGreetingCodec>()?;\n     m.add_class::<PyCommandCodec>()?;\n", "test_patch": "diff --git a/bindings/imap-codec-python/tests/test_authenticate_data_encode.py b/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\nindex 5ea8a4d5..d21ff726 100644\n--- a/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\n+++ b/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import AuthenticateDataCodec, Encoded\n+from imap_codec import AuthenticateDataCodec, Encoded, LineFragment\n \n \n class TestAuthenticateDataEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_authenticate_data(self):\n         encoded = AuthenticateDataCodec.encode(authenticate_data)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"VGVzdA==\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"VGVzdA==\\r\\n\")])\n \n     def test_authenticate_data_dump(self):\n         authenticate_data = {\"Continue\": list(b\"Test\")}\ndiff --git a/bindings/imap-codec-python/tests/test_command_encode.py b/bindings/imap-codec-python/tests/test_command_encode.py\nindex a61eef37..c51d7e61 100644\n--- a/bindings/imap-codec-python/tests/test_command_encode.py\n+++ b/bindings/imap-codec-python/tests/test_command_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import CommandCodec, Encoded\n+from imap_codec import CommandCodec, Encoded, LineFragment, LiteralFragment, LiteralMode\n \n \n class TestCommandEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_simple_command(self):\n         encoded = CommandCodec.encode(command)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"a NOOP\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"a NOOP\\r\\n\")])\n \n     def test_simple_command_dump(self):\n         command = {\"tag\": \"a\", \"body\": \"Noop\"}\n@@ -36,9 +36,9 @@ def test_multi_fragment_command(self):\n         self.assertEqual(\n             fragments,\n             [\n-                {\"Line\": {\"data\": list(b\"A LOGIN alice {2}\\r\\n\")}},\n-                {\"Literal\": {\"data\": list(b\"\\xCA\\xFE\"), \"mode\": \"Sync\"}},\n-                {\"Line\": {\"data\": list(b\"\\r\\n\")}},\n+                LineFragment(b\"A LOGIN alice {2}\\r\\n\"),\n+                LiteralFragment(b\"\\xCA\\xFE\", LiteralMode.Sync),\n+                LineFragment(b\"\\r\\n\"),\n             ],\n         )\n \n@@ -50,7 +50,5 @@ def test_multi_fragment_command_dump(self):\n     def test_multi_fragment_command_dump_remaining(self):\n         encoded = CommandCodec.encode(self._MULTI_FRAGMENT_COMMAND)\n         self.assertIsInstance(encoded, Encoded)\n-        self.assertEqual(\n-            next(encoded), {\"Line\": {\"data\": list(b\"A LOGIN alice {2}\\r\\n\")}}\n-        )\n+        self.assertEqual(next(encoded), LineFragment(b\"A LOGIN alice {2}\\r\\n\"))\n         self.assertEqual(encoded.dump(), b\"\\xCA\\xFE\\r\\n\")\ndiff --git a/bindings/imap-codec-python/tests/test_fragments.py b/bindings/imap-codec-python/tests/test_fragments.py\nnew file mode 100644\nindex 00000000..2803048f\n--- /dev/null\n+++ b/bindings/imap-codec-python/tests/test_fragments.py\n@@ -0,0 +1,101 @@\n+import unittest\n+\n+from imap_codec import LineFragment, LiteralFragment, LiteralMode\n+\n+\n+class TestLineFragment(unittest.TestCase):\n+    def test_data(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(fragment.data, data)\n+\n+    def test_repr(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(repr(fragment), f\"LineFragment({data})\")\n+\n+    def test_str(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(str(fragment), str(data))\n+\n+    def test_eq(self):\n+        fragment1 = LineFragment(b\"a NOOP\\r\\n\")\n+        fragment2 = LineFragment(b\"a NOOP\\r\\n\")\n+        fragment3 = LineFragment(b\"a LOGIN alice pass\\r\\n\")\n+\n+        self.assertEqual(fragment1, fragment1)\n+        self.assertEqual(fragment1, fragment2)\n+        self.assertNotEqual(fragment1, fragment3)\n+\n+        self.assertEqual(fragment2, fragment1)\n+        self.assertEqual(fragment2, fragment2)\n+        self.assertNotEqual(fragment2, fragment3)\n+\n+        self.assertNotEqual(fragment3, fragment1)\n+        self.assertNotEqual(fragment3, fragment2)\n+        self.assertEqual(fragment3, fragment3)\n+\n+\n+class TestLiteralFragment(unittest.TestCase):\n+    def test_data(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+        fragment = LiteralFragment(data, LiteralMode.Sync)\n+        self.assertEqual(fragment.data, data)\n+\n+    def test_mode(self):\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", mode)\n+        self.assertEqual(fragment.mode, mode)\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", mode)\n+        self.assertEqual(fragment.mode, mode)\n+\n+    def test_repr(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(repr(fragment), f\"LiteralFragment({data}, {mode})\")\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(repr(fragment), f\"LiteralFragment({data}, {mode})\")\n+\n+    def test_str(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(str(fragment), f\"({data}, {mode})\")\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(str(fragment), f\"({data}, {mode})\")\n+\n+    def test_eq(self):\n+        fragment1 = LiteralFragment(b\"data\", LiteralMode.Sync)\n+        fragment2 = LiteralFragment(b\"data\", LiteralMode.Sync)\n+        fragment3 = LiteralFragment(b\"data\", LiteralMode.NonSync)\n+        fragment4 = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", LiteralMode.NonSync)\n+\n+        self.assertEqual(fragment1, fragment1)\n+        self.assertEqual(fragment1, fragment2)\n+        self.assertNotEqual(fragment1, fragment3)\n+        self.assertNotEqual(fragment1, fragment4)\n+\n+        self.assertEqual(fragment2, fragment1)\n+        self.assertEqual(fragment2, fragment2)\n+        self.assertNotEqual(fragment2, fragment3)\n+        self.assertNotEqual(fragment2, fragment4)\n+\n+        self.assertNotEqual(fragment3, fragment1)\n+        self.assertNotEqual(fragment3, fragment2)\n+        self.assertEqual(fragment3, fragment3)\n+        self.assertNotEqual(fragment3, fragment4)\n+\n+        self.assertNotEqual(fragment4, fragment1)\n+        self.assertNotEqual(fragment4, fragment2)\n+        self.assertNotEqual(fragment4, fragment3)\n+        self.assertEqual(fragment4, fragment4)\ndiff --git a/bindings/imap-codec-python/tests/test_greeting_encode.py b/bindings/imap-codec-python/tests/test_greeting_encode.py\nindex e9b2077d..b64716fb 100644\n--- a/bindings/imap-codec-python/tests/test_greeting_encode.py\n+++ b/bindings/imap-codec-python/tests/test_greeting_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import Encoded, GreetingCodec\n+from imap_codec import Encoded, GreetingCodec, LineFragment\n \n \n class TestGreetingEncode(unittest.TestCase):\n@@ -9,9 +9,7 @@ def test_simple_greeting(self):\n         encoded = GreetingCodec.encode(greeting)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(\n-            fragments, [{\"Line\": {\"data\": list(b\"* OK Hello, World!\\r\\n\")}}]\n-        )\n+        self.assertEqual(fragments, [LineFragment(b\"* OK Hello, World!\\r\\n\")])\n \n     def test_simple_greeting_dump(self):\n         greeting = {\"code\": None, \"kind\": \"Ok\", \"text\": \"Hello, World!\"}\ndiff --git a/bindings/imap-codec-python/tests/test_idle_done_encode.py b/bindings/imap-codec-python/tests/test_idle_done_encode.py\nindex 5208d89a..2e15ced3 100644\n--- a/bindings/imap-codec-python/tests/test_idle_done_encode.py\n+++ b/bindings/imap-codec-python/tests/test_idle_done_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import Encoded, IdleDoneCodec\n+from imap_codec import Encoded, IdleDoneCodec, LineFragment\n \n \n class TestIdleDoneEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_idle_done(self):\n         encoded = IdleDoneCodec.encode(idle_done)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"DONE\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"DONE\\r\\n\")])\n \n     def test_idle_done_dump(self):\n         idle_done = ()\ndiff --git a/bindings/imap-codec-python/tests/test_response_encode.py b/bindings/imap-codec-python/tests/test_response_encode.py\nindex 94149197..7e30c141 100644\n--- a/bindings/imap-codec-python/tests/test_response_encode.py\n+++ b/bindings/imap-codec-python/tests/test_response_encode.py\n@@ -1,6 +1,12 @@\n import unittest\n \n-from imap_codec import Encoded, ResponseCodec\n+from imap_codec import (\n+    Encoded,\n+    LineFragment,\n+    LiteralFragment,\n+    LiteralMode,\n+    ResponseCodec,\n+)\n \n \n class TestResponseEncode(unittest.TestCase):\n@@ -9,7 +15,7 @@ def test_simple_response(self):\n         encoded = ResponseCodec.encode(response)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"* SEARCH 1\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"* SEARCH 1\\r\\n\")])\n \n     def test_simple_response_dump(self):\n         response = {\"Data\": {\"Search\": [1]}}\n@@ -46,9 +52,9 @@ def test_multi_fragment_response(self):\n         self.assertEqual(\n             fragments,\n             [\n-                {\"Line\": {\"data\": list(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\")}},\n-                {\"Literal\": {\"data\": list(b\"ABCDE\"), \"mode\": \"NonSync\"}},\n-                {\"Line\": {\"data\": list(b\")\\r\\n\")}},\n+                LineFragment(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\"),\n+                LiteralFragment(b\"ABCDE\", LiteralMode.NonSync),\n+                LineFragment(b\")\\r\\n\"),\n             ],\n         )\n \n@@ -63,9 +69,7 @@ def test_multi_fragment_response_dump(self):\n     def test_multi_fragment_response_dump_remaining(self):\n         encoded = ResponseCodec.encode(self._MULTI_FRAGMENT_RESPONSE)\n         self.assertIsInstance(encoded, Encoded)\n-        self.assertEqual(\n-            next(encoded), {\"Line\": {\"data\": list(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\")}}\n-        )\n+        self.assertEqual(next(encoded), LineFragment(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\"))\n         self.assertEqual(\n             encoded.dump(),\n             b\"ABCDE)\\r\\n\",\n", "problem_statement": "bindings(Python): Use Python classes for `Fragment` variants \nCurrently, the fragments of `Encoded` (Rust enum variants) are deserializes into variant dictionaries, e.g. `{\"Line\": {\"data\": list(b\"a NOOP\\r\\n\")}}`.\r\n\r\nBy using individual classes for each variant (`LineFragment`, `LiteralFragment`), handling these in the Python code would be greatly improved.\r\n\r\nRelated: This would remove the need for `Fragment` to implement `Serialize`, see #555.\n", "hints_text": "", "created_at": "2024-07-17 11:16:58", "merge_commit_sha": "32266eb6d06bfa0f16414ed6c7c69cabd9bb43d3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['fuzz', '.github/workflows/main.yml']", "['test (ubuntu-latest)', '.github/workflows/main.yml']"], ["['test (macos-latest)', '.github/workflows/main.yml']", "['minimal_versions', '.github/workflows/main.yml']"]]}
{"repo": "lalrpop/lalrpop", "instance_id": "lalrpop__lalrpop-910", "base_commit": "f79a1a920bd2244287804f61f07ad79362cc5105", "patch": "diff --git a/lalrpop-util/src/lib.rs b/lalrpop-util/src/lib.rs\nindex 28c8500ca..648a5fe19 100644\n--- a/lalrpop-util/src/lib.rs\n+++ b/lalrpop-util/src/lib.rs\n@@ -182,6 +182,9 @@ pub struct ErrorRecovery<L, T, E> {\n ///\n /// // define a public module\n /// lalrpop_mod!(pub parser);\n+///\n+/// // specify attributes for the generated module\n+/// lalrpop_mod!(#[allow(clippy::ptr_arg)]#[rustfmt::skip] parser);\n /// ```\n \n #[macro_export]\n", "test_patch": "", "problem_statement": "Document  `$attr`  in `lalrpop_mod!`\n\r\nThe rust doc for `lalrpop_mod!` does not show an example in which the `$attr` is used.\r\n\r\nA relevant example might be something like\r\n\r\n```rust\r\nlalrpop_mod!(#[allow(clippy::vec_box)] pub calculator);\r\n```\r\n\r\nI do understand why this is left out of the tutorial, but more fully documenting `lalrpop_mod!` will help those who do run clippy on the tutorial.\r\n\r\n\n", "hints_text": "", "created_at": "2024-06-18 19:29:30", "merge_commit_sha": "d7daf3ff1f1fe14727e5a034a30f368ec8c7f244", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build (1.70, true)', '.github/workflows/test.yaml']", "['lint', '.github/workflows/test.yaml']"], ["['min_version', '.github/workflows/test.yaml']", "['feature_powerset', '.github/workflows/test.yaml']"]]}
{"repo": "iced-rs/iced", "instance_id": "iced-rs__iced-2601", "base_commit": "88a2fac1f9171f162ecfe2a033cba5ae62e23231", "patch": "diff --git a/wgpu/src/triangle.rs b/wgpu/src/triangle.rs\nindex 3d0869e717..fb858c1088 100644\n--- a/wgpu/src/triangle.rs\n+++ b/wgpu/src/triangle.rs\n@@ -505,6 +505,14 @@ impl Layer {\n                 .intersection(&(mesh.clip_bounds() * transformation))\n                 .and_then(Rectangle::snap)\n             else {\n+                match mesh {\n+                    Mesh::Solid { .. } => {\n+                        num_solids += 1;\n+                    }\n+                    Mesh::Gradient { .. } => {\n+                        num_gradients += 1;\n+                    }\n+                }\n                 continue;\n             };\n \n", "test_patch": "", "problem_statement": "Rendering of `Mesh`es seems to break depending on the order they are added\n### Is your issue REALLY a bug?\r\n\r\n- [X] My issue is indeed a bug!\r\n- [X] I am not crazy! I will not fill out this form just to ask a question or request a feature. Pinky promise.\r\n\r\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues.\r\n\r\n### Is this issue related to iced?\r\n\r\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\r\n\r\n### What happened?\r\n\r\nIf a `Mesh` that has a clip bound with a dimension below 1 pixel is drawn before another `Mesh` with a larger clip bound, the second is not drawn.\r\n\r\nMinimum example:\r\n```rs\r\nuse iced::{\r\n    advanced::{\r\n        graphics::{\r\n            color,\r\n            mesh::{self, Renderer as _, SolidVertex2D},\r\n            Mesh,\r\n        },\r\n        layout::{Limits, Node},\r\n        renderer::Style,\r\n        widget::Tree,\r\n        Layout, Widget,\r\n    },\r\n    mouse::Cursor,\r\n    Length, Rectangle, Renderer, Size, Theme, Transformation,\r\n};\r\n\r\nfn main() {\r\n    iced::application(\"\", App::update, App::view).run().unwrap();\r\n}\r\n\r\nstruct App;\r\n\r\n#[derive(Debug)]\r\nenum Message {}\r\n\r\nimpl App {\r\n    fn new() -> Self {\r\n        Self\r\n    }\r\n\r\n    fn update(&mut self, _: Message) {}\r\n\r\n    fn view(&self) -> iced::Element<'_, Message> {\r\n        iced::Element::new(Test)\r\n    }\r\n}\r\n\r\nimpl Default for App {\r\n    fn default() -> Self {\r\n        Self::new()\r\n    }\r\n}\r\n\r\nstruct Test;\r\n\r\nimpl Widget<Message, Theme, Renderer> for Test {\r\n    fn size(&self) -> Size<Length> {\r\n        Size::new(Length::Fill, Length::Fill)\r\n    }\r\n\r\n    fn layout(&self, _tree: &mut Tree, _renderer: &Renderer, limits: &Limits) -> Node {\r\n        Node::new(Size::new(limits.max().width, limits.max().height))\r\n    }\r\n\r\n    fn draw(\r\n        &self,\r\n        _tree: &Tree,\r\n        renderer: &mut Renderer,\r\n        theme: &Theme,\r\n        _style: &Style,\r\n        _layout: Layout<'_>,\r\n        _cursor: Cursor,\r\n        _viewport: &Rectangle,\r\n    ) {\r\n        let mesh1 = Mesh::Solid {\r\n            buffers: mesh::Indexed {\r\n                vertices: vec![\r\n                    SolidVertex2D {\r\n                        position: [0.0, 0.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                    SolidVertex2D {\r\n                        position: [0.0, 100.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                    SolidVertex2D {\r\n                        position: [100.0, 100.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                ],\r\n                indices: vec![0, 1, 2],\r\n            },\r\n            transformation: Transformation::IDENTITY,\r\n            clip_bounds: Rectangle {\r\n                x: 0.0,\r\n                y: 0.0,\r\n                width: 100.0,\r\n                height: 0.9999999,\r\n            },\r\n        };\r\n\r\n        let mesh2 = Mesh::Solid {\r\n            buffers: mesh::Indexed {\r\n                vertices: vec![\r\n                    SolidVertex2D {\r\n                        position: [0.0, 100.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                    SolidVertex2D {\r\n                        position: [0.0, 200.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                    SolidVertex2D {\r\n                        position: [100.0, 200.0],\r\n                        color: color::pack(theme.extended_palette().secondary.base.text),\r\n                    },\r\n                ],\r\n                indices: vec![0, 1, 2],\r\n            },\r\n            transformation: Transformation::IDENTITY,\r\n            clip_bounds: Rectangle {\r\n                x: 0.0,\r\n                y: 100.0,\r\n                width: 100.0,\r\n                height: 100.0,\r\n            },\r\n        };\r\n\r\n        renderer.draw_mesh(mesh1);\r\n        renderer.draw_mesh(mesh2);\r\n    }\r\n}\r\n```\r\n`mesh2` doesn't render here. Swapping the two `draw_mesh` calls makes `mesh2` render correctly.\r\n\r\nThis affects both this release and the master branch.\r\n\r\n### What is the expected behavior?\r\n\r\nThe mesh with the large clip bound is rendered.\r\n\r\n### Version\r\n\r\ncrates.io release\r\n\r\n### Operating System\r\n\r\nLinux\r\n\r\n### Do you have any log output?\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2024-09-22 10:23:43", "merge_commit_sha": "faff6fb8e011be920f014471a93e4c72d543e7f2", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['vulnerabilities', '.github/workflows/audit.yml']", "['all (ubuntu-latest, beta)', '.github/workflows/test.yml']"], ["['all (macOS-latest, 1.80)', '.github/workflows/test.yml']", "['all (windows-latest, stable)', '.github/workflows/test.yml']"], ["['all', '.github/workflows/lint.yml']", "['all', '.github/workflows/format.yml']"], ["['wasm', '.github/workflows/check.yml']", "['all (macOS-latest, stable)', '.github/workflows/test.yml']"]]}
{"repo": "iced-rs/iced", "instance_id": "iced-rs__iced-2514", "base_commit": "295aae4faad7c151fa9d96c9c352264d14a9fcfe", "patch": "diff --git a/widget/src/text_editor.rs b/widget/src/text_editor.rs\nindex 5b565c3966..1df97962a2 100644\n--- a/widget/src/text_editor.rs\n+++ b/widget/src/text_editor.rs\n@@ -981,7 +981,9 @@ impl<Message> Binding<Message> {\n             keyboard::Key::Named(key::Named::Backspace) => {\n                 Some(Self::Backspace)\n             }\n-            keyboard::Key::Named(key::Named::Delete) => Some(Self::Delete),\n+            keyboard::Key::Named(key::Named::Delete) if text.is_none() => {\n+                Some(Self::Delete)\n+            }\n             keyboard::Key::Named(key::Named::Escape) => Some(Self::Unfocus),\n             keyboard::Key::Character(\"c\") if modifiers.command() => {\n                 Some(Self::Copy)\n", "test_patch": "", "problem_statement": "Editor widget does not handle '.'/'del' key on numeric keypad correctly.\n### Is your issue REALLY a bug?\n\n- [X] My issue is indeed a bug!\n- [X] I am not crazy! I will not fill out this form just to ask a question or request a feature. Pinky promise.\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nIf you have an Editor widget, then the \"decimal point\" on the numeric keyboard acts as the \"del\" key whether numlock is on or off.\r\n\r\nYou can see this in the Editor example from https://github.com/iced-rs/iced/tree/master/examples/editor.\r\n\r\nRun:  cargo run --package=editor and type anywhere using the numeric keypad to see this behaviour. This also occurs in my application.\n\n### What is the expected behavior?\n\nThe \"decimal point\" key on the numeric keypad should enter a \".\" when numlock is on.\n\n### Version\n\nmaster\n\n### Operating System\n\nLinux\n\n### Do you have any log output?\n\n_No response_\n", "hints_text": "Text_input does NOT show this behaviour. Tested with https://github.com/iced-rs/iced/tree/master/examples/todos", "created_at": "2024-07-23 03:46:21", "merge_commit_sha": "74bb93513e70c2ef3c8a744d5b2224e75e4cecc4", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['vulnerabilities', '.github/workflows/audit.yml']", "['all (ubuntu-latest, beta)', '.github/workflows/test.yml']"], ["['all (windows-latest, stable)', '.github/workflows/test.yml']", "['all', '.github/workflows/lint.yml']"], ["['wasm', '.github/workflows/check.yml']", "['all', '.github/workflows/format.yml']"]]}
{"repo": "iced-rs/iced", "instance_id": "iced-rs__iced-2752", "base_commit": "3428a3d2afb766c264453a58e15d33953d438238", "patch": "diff --git a/widget/src/image.rs b/widget/src/image.rs\nindex c8f2a620f6..f5a9c7f385 100644\n--- a/widget/src/image.rs\n+++ b/widget/src/image.rs\n@@ -167,6 +167,7 @@ where\n pub fn draw<Renderer, Handle>(\n     renderer: &mut Renderer,\n     layout: Layout<'_>,\n+    viewport: &Rectangle,\n     handle: &Handle,\n     content_fit: ContentFit,\n     filter_method: FilterMethod,\n@@ -218,7 +219,9 @@ pub fn draw<Renderer, Handle>(\n \n     if adjusted_fit.width > bounds.width || adjusted_fit.height > bounds.height\n     {\n-        renderer.with_layer(bounds, render);\n+        if let Some(bounds) = bounds.intersection(viewport) {\n+            renderer.with_layer(bounds, render);\n+        }\n     } else {\n         render(renderer);\n     }\n@@ -262,11 +265,12 @@ where\n         _style: &renderer::Style,\n         layout: Layout<'_>,\n         _cursor: mouse::Cursor,\n-        _viewport: &Rectangle,\n+        viewport: &Rectangle,\n     ) {\n         draw(\n             renderer,\n             layout,\n+            viewport,\n             &self.handle,\n             self.content_fit,\n             self.filter_method,\n", "test_patch": "", "problem_statement": "Image escaping parent bounds when ContentFIt is Cover\n### Is your issue REALLY a bug?\n\n- [X] My issue is indeed a bug!\n- [X] I am not crazy! I will not fill out this form just to ask a question or request a feature. Pinky promise.\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nWhen wrapping a series of images in a scrollable(container(image)), if the image has ContentFit Cover and both Width and Height set to Fill it escapes the scrollable bounds, rendering it outside of it.\r\n\r\n\r\nhttps://github.com/user-attachments/assets/bc2dd853-0dea-4dfc-a542-0ff2d14addc6\r\n\r\nThis example is running under Linux (Wayland, AMD) but the same happens on Windows.\r\n\r\n[SSCCE](https://github.com/Stay1444/iced-image-overflow-sscce/)\r\n\n\n### What is the expected behavior?\n\nThe images should not overflow the scrollable area and render on top of the TOP BAR\n\n### Version\n\nmaster\n\n### Operating System\n\nLinux\n\n### Do you have any log output?\n\n```shell\n2024-11-10T21:20:15.879368Z  INFO wgpu_hal::vulkan::instance: Debug utils not enabled: debug_utils_user_data not passed to Instance::from_raw    \r\n2024-11-10T21:20:15.885882Z  INFO wgpu_hal::gles::egl: Using Wayland platform    \r\n2024-11-10T21:20:15.921017Z  INFO iced_wgpu::window::compositor: Settings {\r\n    present_mode: AutoVsync,\r\n    backends: Backends(\r\n        VULKAN | GL | METAL | DX12 | BROWSER_WEBGPU,\r\n    ),\r\n    default_font: Font {\r\n        family: SansSerif,\r\n        weight: Normal,\r\n        stretch: Normal,\r\n        style: Normal,\r\n    },\r\n    default_text_size: Pixels(\r\n        16.0,\r\n    ),\r\n    antialiasing: None,\r\n}    \r\n2024-11-10T21:20:15.931879Z  INFO wgpu_core::instance: Adapter AdapterInfo { name: \"AMD Radeon RX 6600 XT (RADV NAVI23)\", vendor: 4098, device: 29695, device_type: DiscreteGpu, driver: \"radv\", driver_info: \"Mesa 24.2.6-arch1.1\", backend: Vulkan }    \r\n2024-11-10T21:20:15.933769Z  INFO wgpu_core::instance: Adapter AdapterInfo { name: \"AMD Radeon RX 6600 XT (radeonsi, navi23, LLVM 18.1.8, DRM 3.59, 6.11.6-zen1-1-zen)\", vendor: 4098, device: 0, device_type: Other, driver: \"\", driver_info: \"4.6 (Core Pr\r\nofile) Mesa 24.2.6-arch1.1\", backend: Gl }    \r\n2024-11-10T21:20:15.933885Z  INFO iced_wgpu::window::compositor: Available adapters: [\r\n    AdapterInfo {\r\n        name: \"AMD Radeon RX 6600 XT (RADV NAVI23)\",\r\n        vendor: 4098,\r\n        device: 29695,\r\n        device_type: DiscreteGpu,\r\n        driver: \"radv\",\r\n        driver_info: \"Mesa 24.2.6-arch1.1\",\r\n        backend: Vulkan,\r\n    },\r\n    AdapterInfo {\r\n        name: \"AMD Radeon RX 6600 XT (radeonsi, navi23, LLVM 18.1.8, DRM 3.59, 6.11.6-zen1-1-zen)\",\r\n        vendor: 4098,\r\n        device: 0,\r\n        device_type: Other,\r\n        driver: \"\",\r\n        driver_info: \"4.6 (Core Profile) Mesa 24.2.6-arch1.1\",\r\n        backend: Gl,\r\n    },\r\n]    \r\n2024-11-10T21:20:15.934015Z  WARN wgpu_hal::gles::egl: Re-initializing Gles context due to Wayland window    \r\n2024-11-10T21:20:15.964327Z  INFO wgpu_core::instance: Adapter AdapterInfo { name: \"AMD Radeon RX 6600 XT (RADV NAVI23)\", vendor: 4098, device: 29695, device_type: DiscreteGpu, driver: \"radv\", driver_info: \"Mesa 24.2.6-arch1.1\", backend: Vulkan }    \r\n2024-11-10T21:20:15.964445Z  INFO iced_wgpu::window::compositor: Selected: AdapterInfo {\r\n    name: \"AMD Radeon RX 6600 XT (RADV NAVI23)\",\r\n    vendor: 4098,\r\n    device: 29695,\r\n    device_type: DiscreteGpu,\r\n    driver: \"radv\",\r\n    driver_info: \"Mesa 24.2.6-arch1.1\",\r\n    backend: Vulkan,\r\n}    \r\n2024-11-10T21:20:15.965690Z  INFO iced_wgpu::window::compositor: Available formats: Copied {\r\n    it: Iter(\r\n        [\r\n            Bgra8UnormSrgb,\r\n            Rgba8UnormSrgb,\r\n            Bgra8Unorm,\r\n            Rgba8Unorm,\r\n        ],\r\n    ),\r\n}    \r\n2024-11-10T21:20:15.966018Z  INFO iced_wgpu::window::compositor: Available alpha modes: [\r\n    Opaque,\r\n    PreMultiplied,\r\n]    \r\n2024-11-10T21:20:15.966081Z  INFO iced_wgpu::window::compositor: Selected format: Bgra8UnormSrgb with alpha mode: PreMultiplied    \r\n2024-11-10T21:20:16.035760Z  INFO iced_winit::program: Window attributes for id `Id(\r\n    1,\r\n)`: WindowAttributes {\r\n    inner_size: Some(\r\n        Logical(\r\n            LogicalSize {\r\n                width: 1024.0,\r\n                height: 768.0,\r\n            },\r\n        ),\r\n    ),\r\n    min_inner_size: None,\r\n    max_inner_size: None,\r\n    position: None,\r\n    resizable: true,\r\n    enabled_buttons: WindowButtons(\r\n        CLOSE | MINIMIZE | MAXIMIZE,\r\n    ),\r\n    title: \"Iced Image Overflow\",\r\n    maximized: false,\r\n    visible: false,\r\n    transparent: false,\r\n    blur: false,\r\n    decorations: true,\r\n    window_icon: None,\r\n    preferred_theme: None,\r\n    resize_increments: None,\r\n    content_protected: false,\r\n    window_level: Normal,\r\n    active: true,\r\n    cursor: Icon(\r\n        Default,\r\n    ),\r\n    parent_window: None,\r\n    fullscreen: None,\r\n    platform_specific: PlatformSpecificWindowAttributes {\r\n        name: Some(\r\n            ApplicationName {\r\n                general: \"\",\r\n                instance: \"\",\r\n            },\r\n        ),\r\n        activation_token: None,\r\n        x11: X11WindowAttributes {\r\n            visual_id: None,\r\n            screen_id: None,\r\n            base_size: None,\r\n            override_redirect: false,\r\n            x11_window_types: [\r\n                Normal,\r\n            ],\r\n            embed_window: None,\r\n        },\r\n    },\r\n}\n```\n\n", "hints_text": "Have same problem with scrollable list of images. Image bleeds over into the above rows and does not disappear until the bottom of image reaches the top of the scroll. Circumvention? or is it being addressed?\nProblem seems to be caused by an incorrect `scissor_rect` that is passed into `image_pipeline::render`. Scissor rect is transformed by scrolling. At least that's how far I was able to dig as of now.", "created_at": "2025-01-26 03:28:49", "merge_commit_sha": "d034bd73d8fb176ef9edfc76a6198711354a97f1", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['vulnerabilities', '.github/workflows/audit.yml']", "['all (ubuntu-latest, beta)', '.github/workflows/test.yml']"], ["['all (windows-latest, stable)', '.github/workflows/test.yml']", "['all (macOS-latest, 1.81)', '.github/workflows/test.yml']"], ["['all', '.github/workflows/lint.yml']", "['all', '.github/workflows/format.yml']"], ["['wasm', '.github/workflows/check.yml']", "['all (macOS-latest, stable)', '.github/workflows/test.yml']"]]}
{"repo": "pulldown-cmark/pulldown-cmark", "instance_id": "pulldown-cmark__pulldown-cmark-1023", "base_commit": "fb01f7fd843485c0fb54b3d8a59eeb83ae3ec656", "patch": "diff --git a/pulldown-cmark/specs/super_sub.txt b/pulldown-cmark/specs/super_sub.txt\nindex f60da2e9..9cd7b9fb 100644\n--- a/pulldown-cmark/specs/super_sub.txt\n+++ b/pulldown-cmark/specs/super_sub.txt\n@@ -25,13 +25,19 @@ Backslash escapes:\n ```````````````````````````````` example_super_sub\n ~This~is~nothing~\n .\n-<p><sub>This~is~nothing</sub></p>\n+<p><sub>This</sub>is<sub>nothing</sub></p>\n ````````````````````````````````\n \n ```````````````````````````````` example_super_sub\n-~This ~~is stricken.~\n+~This ~~is not stricken.~\n .\n-<p><sub>This ~~is stricken.</sub></p>\n+<p><sub>This ~~is not stricken.</sub></p>\n+````````````````````````````````\n+\n+```````````````````````````````` example_super_sub\n+~~This ~is~~ stricken.~\n+.\n+<p><del>This ~is</del> stricken.~</p>\n ````````````````````````````````\n \n The first one wins.\n@@ -41,3 +47,15 @@ The first one wins.\n .\n <p><sub>This ~~is stricken</sub> but this is not~~</p>\n ````````````````````````````````\n+\n+Though strikethrough requires left and right flanking, subscript does not.\n+Neither does superscript.\n+\n+```````````````````````````````` example_super_sub\n+H~2~O\n+\n+y=x^2^a+xb+c\n+.\n+<p>H<sub>2</sub>O</p>\n+<p>y=x<sup>2</sup>a+xb+c</p>\n+````````````````````````````````\ndiff --git a/pulldown-cmark/src/firstpass.rs b/pulldown-cmark/src/firstpass.rs\nindex 8994a272..31eae05a 100644\n--- a/pulldown-cmark/src/firstpass.rs\n+++ b/pulldown-cmark/src/firstpass.rs\n@@ -925,6 +925,7 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                         count,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n                     let can_close = delim_run_can_close(\n                         &self.text[start..],\n@@ -932,6 +933,7 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                         count,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n                     let is_valid_seq = (c != b'~' || count <= 2) || (c == b'~' && count == 2);\n \n@@ -1176,14 +1178,21 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                 }\n                 c @ b'\\'' | c @ b'\"' => {\n                     let string_suffix = &self.text[ix..];\n-                    let can_open =\n-                        delim_run_can_open(&self.text[start..], string_suffix, 1, ix - start, mode);\n+                    let can_open = delim_run_can_open(\n+                        &self.text[start..],\n+                        string_suffix,\n+                        1,\n+                        ix - start,\n+                        mode,\n+                        self.options,\n+                    );\n                     let can_close = delim_run_can_close(\n                         &self.text[start..],\n                         string_suffix,\n                         1,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n \n                     self.tree.append_text(begin_text, ix, backslash_escaped);\n@@ -2258,6 +2267,7 @@ fn delim_run_can_open(\n     run_len: usize,\n     ix: usize,\n     mode: TableParseMode,\n+    options: Options,\n ) -> bool {\n     let next_char = if let Some(c) = suffix[run_len..].chars().next() {\n         c\n@@ -2279,15 +2289,18 @@ fn delim_run_can_open(\n         }\n     }\n     let delim = suffix.bytes().next().unwrap();\n-    // `*` and `~~` can be intraword, `_` and `~` cannot\n-    if delim == b'*' && !is_punctuation(next_char) {\n+    // `*`, `~~`, and `^` can be intraword, `~` can only be interword if it's subscript, `_` cannot\n+    if (delim == b'*' || delim == b'^') && !is_punctuation(next_char) {\n         return true;\n     }\n     if delim == b'~' && run_len > 1 {\n         return true;\n     }\n     let prev_char = s[..ix].chars().last().unwrap();\n-    if delim == b'~' && prev_char == '~' && !is_punctuation(next_char) {\n+    if delim == b'~'\n+        && (prev_char == '~' || options.contains(Options::ENABLE_SUBSCRIPT))\n+        && !is_punctuation(next_char)\n+    {\n         return true;\n     }\n \n@@ -2304,6 +2317,7 @@ fn delim_run_can_close(\n     run_len: usize,\n     ix: usize,\n     mode: TableParseMode,\n+    options: Options,\n ) -> bool {\n     if ix == 0 {\n         return false;\n@@ -2326,11 +2340,13 @@ fn delim_run_can_close(\n         }\n     }\n     let delim = suffix.bytes().next().unwrap();\n-    // `*` and `~~` can be intraword, `_` and `~` cannot\n-    if (delim == b'*' || (delim == b'~' && run_len > 1)) && !is_punctuation(prev_char) {\n+    // `*`, `~~`, and `^` can be intraword, `~` can only be interword if it's subscript, `_` cannot\n+    if (delim == b'*' || delim == b'^' || (delim == b'~' && run_len > 1))\n+        && !is_punctuation(prev_char)\n+    {\n         return true;\n     }\n-    if delim == b'~' && prev_char == '~' {\n+    if delim == b'~' && (prev_char == '~' || options.contains(Options::ENABLE_SUBSCRIPT)) {\n         return true;\n     }\n \n", "test_patch": "diff --git a/pulldown-cmark/tests/suite/super_sub.rs b/pulldown-cmark/tests/suite/super_sub.rs\nindex a9c0c84a..6c2492bf 100644\n--- a/pulldown-cmark/tests/suite/super_sub.rs\n+++ b/pulldown-cmark/tests/suite/super_sub.rs\n@@ -37,7 +37,7 @@ fn super_sub_test_3() {\n fn super_sub_test_4() {\n     let original = r##\"~This~is~nothing~\n \"##;\n-    let expected = r##\"<p><sub>This~is~nothing</sub></p>\n+    let expected = r##\"<p><sub>This</sub>is<sub>nothing</sub></p>\n \"##;\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n@@ -45,9 +45,9 @@ fn super_sub_test_4() {\n \n #[test]\n fn super_sub_test_5() {\n-    let original = r##\"~This ~~is stricken.~\n+    let original = r##\"~This ~~is not stricken.~\n \"##;\n-    let expected = r##\"<p><sub>This ~~is stricken.</sub></p>\n+    let expected = r##\"<p><sub>This ~~is not stricken.</sub></p>\n \"##;\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n@@ -55,6 +55,16 @@ fn super_sub_test_5() {\n \n #[test]\n fn super_sub_test_6() {\n+    let original = r##\"~~This ~is~~ stricken.~\n+\"##;\n+    let expected = r##\"<p><del>This ~is</del> stricken.~</p>\n+\"##;\n+\n+    test_markdown_html(original, expected, false, false, false, true, false, false);\n+}\n+\n+#[test]\n+fn super_sub_test_7() {\n     let original = r##\"~This ~~is stricken~ but this is not~~\n \"##;\n     let expected = r##\"<p><sub>This ~~is stricken</sub> but this is not~~</p>\n@@ -62,3 +72,16 @@ fn super_sub_test_6() {\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n }\n+\n+#[test]\n+fn super_sub_test_8() {\n+    let original = r##\"H~2~O\n+\n+y=x^2^a+xb+c\n+\"##;\n+    let expected = r##\"<p>H<sub>2</sub>O</p>\n+<p>y=x<sup>2</sup>a+xb+c</p>\n+\"##;\n+\n+    test_markdown_html(original, expected, false, false, false, true, false, false);\n+}\n", "problem_statement": "Superscript and subscript parsing inconsistent with Pandoc and markdown-it\n`pulldown-cmark` requires whitespace around superscripted and subscripted text whereas [Pandoc](https://pandoc.org/try/?params=%7B%22text%22%3A%22H%7E2%7E0%22%2C%22to%22%3A%22html5%22%2C%22from%22%3A%22commonmark_x%22%2C%22standalone%22%3Afalse%2C%22embed-resources%22%3Afalse%2C%22table-of-contents%22%3Afalse%2C%22number-sections%22%3Afalse%2C%22citeproc%22%3Afalse%2C%22html-math-method%22%3A%22plain%22%2C%22wrap%22%3A%22auto%22%2C%22highlight-style%22%3Anull%2C%22files%22%3A%7B%7D%2C%22template%22%3Anull%7D) and [markdown-it](https://markdown-it.github.io/#md3=%7B%22source%22%3A%22H~2~0%22%2C%22defaults%22%3A%7B%22html%22%3Afalse%2C%22xhtmlOut%22%3Afalse%2C%22breaks%22%3Afalse%2C%22langPrefix%22%3A%22language-%22%2C%22linkify%22%3Atrue%2C%22typographer%22%3Atrue%2C%22_highlight%22%3Atrue%2C%22_strict%22%3Afalse%2C%22_view%22%3A%22html%22%7D%7D) do not.\n\n```shell\n$ echo 'H~2~0' | cargo r --quiet -- --enable-subscript -e\n0..6: Start(Paragraph)\n0..5: Text(Borrowed(\"H~2~0\"))\n0..6: End(Paragraph)\nEOF\n```\n\n```shell\n$ echo 'H ~2~ 0' | cargo r --quiet -- --enable-subscript -e\n0..8: Start(Paragraph)\n0..2: Text(Borrowed(\"H \"))\n2..5: Start(Subscript)\n3..4: Text(Borrowed(\"2\"))\n2..5: End(Subscript)\n5..7: Text(Borrowed(\" 0\"))\n0..8: End(Paragraph)\nEOF\n```\n", "hints_text": "", "created_at": "2025-02-18 00:17:17", "merge_commit_sha": "5d05c3c61abf24d1644f16cb60f6f35bc049a4d3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['pulldown-cmark (stable)', '.github/workflows/rust.yml']", "['pulldown-cmark (1.71.1)', '.github/workflows/rust.yml']"], ["['build', '.github/workflows/book.yml']", "['pulldown-cmark (nightly)', '.github/workflows/rust.yml']"]]}
{"repo": "nextest-rs/nextest", "instance_id": "nextest-rs__nextest-2130", "base_commit": "1391ad43a6790fa92bd329344682ea9333064936", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 2c4372414de..ffa8c6f0406 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -373,6 +373,7 @@ dependencies = [\n  \"duct\",\n  \"enable-ansi-support\",\n  \"guppy\",\n+ \"indent_write\",\n  \"itertools 0.14.0\",\n  \"miette\",\n  \"nextest-filtering\",\n@@ -872,16 +873,6 @@ version = \"1.0.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5\"\n \n-[[package]]\n-name = \"erased-serde\"\n-version = \"0.4.5\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"24e2389d65ab4fab27dc2a5de7b191e1f6617d1f1c8855c0dc569c94a4cbb18d\"\n-dependencies = [\n- \"serde\",\n- \"typeid\",\n-]\n-\n [[package]]\n name = \"errno\"\n version = \"0.3.9\"\n@@ -945,9 +936,9 @@ dependencies = [\n \n [[package]]\n name = \"fixedbitset\"\n-version = \"0.4.2\"\n+version = \"0.5.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0ce7134b9999ecaf8bcd65542e436736ef32ddca1b3e06094cb6ec5755203b80\"\n+checksum = \"1d674e81391d1e1ab681a28d99df07927c6d4aa5b027d7da16ba32d1d21ecd99\"\n \n [[package]]\n name = \"fixture-data\"\n@@ -1149,9 +1140,9 @@ dependencies = [\n \n [[package]]\n name = \"guppy\"\n-version = \"0.17.12\"\n+version = \"0.17.13\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5a70d0754232d1c2fd4819914085dc06cccbe2f2377905cdd2e668ca77df6e72\"\n+checksum = \"54c43505e290c5dc9ca6fc7117927ae46685ae6ea23f988996d622a6394ca99b\"\n dependencies = [\n  \"ahash\",\n  \"camino\",\n@@ -1161,7 +1152,7 @@ dependencies = [\n  \"fixedbitset\",\n  \"guppy-workspace-hack\",\n  \"indexmap 2.7.1\",\n- \"itertools 0.13.0\",\n+ \"itertools 0.14.0\",\n  \"nested\",\n  \"once_cell\",\n  \"pathdiff\",\n@@ -1668,12 +1659,9 @@ dependencies = [\n \n [[package]]\n name = \"log\"\n-version = \"0.4.24\"\n+version = \"0.4.25\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3d6ea2a48c204030ee31a7d7fc72c93294c92fe87ecb1789881c9543516e1a0d\"\n-dependencies = [\n- \"value-bag\",\n-]\n+checksum = \"04cbf5b083de1c7e0222a7a51dbfdba1cbe1c6ab0b15e29fff3f6c077fd9cd9f\"\n \n [[package]]\n name = \"maplit\"\n@@ -2045,9 +2033,9 @@ dependencies = [\n \n [[package]]\n name = \"once_cell\"\n-version = \"1.20.2\"\n+version = \"1.20.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775\"\n+checksum = \"945462a4b81e43c4e3ba96bd7b49d834c6f61198356aa858733bc4acf3cbe62e\"\n \n [[package]]\n name = \"openssl\"\n@@ -2155,9 +2143,9 @@ checksum = \"e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e\"\n \n [[package]]\n name = \"petgraph\"\n-version = \"0.6.5\"\n+version = \"0.7.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b4c5cc86750666a3ed20bdaf5ca2a0344f9c67674cae0515bec2da16fbaa47db\"\n+checksum = \"3672b37090dbd86368a4145bc067582552b29c27377cad4e0a306c97f9bd7772\"\n dependencies = [\n  \"fixedbitset\",\n  \"indexmap 2.7.1\",\n@@ -2651,9 +2639,9 @@ dependencies = [\n \n [[package]]\n name = \"ryu\"\n-version = \"1.0.18\"\n+version = \"1.0.19\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f3cb5ba0dc43242ce17de99c180e96db90b235b8a9fdc9543c96d2209116bd9f\"\n+checksum = \"6ea1a2d0a644769cc99faa24c3ad26b379b786fe7c36fd3c546254801650e6dd\"\n \n [[package]]\n name = \"same-file\"\n@@ -2765,15 +2753,6 @@ dependencies = [\n  \"syn\",\n ]\n \n-[[package]]\n-name = \"serde_fmt\"\n-version = \"1.0.3\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e1d4ddca14104cd60529e8c7f7ba71a2c8acd8f7f5cfcdc2faf97eeb7c3010a4\"\n-dependencies = [\n- \"serde\",\n-]\n-\n [[package]]\n name = \"serde_ignored\"\n version = \"0.1.10\"\n@@ -3037,84 +3016,6 @@ version = \"3.0.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"b7401a30af6cb5818bb64852270bb722533397edcfc7344954a38f420819ece2\"\n \n-[[package]]\n-name = \"sval\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f6dc0f9830c49db20e73273ffae9b5240f63c42e515af1da1fceefb69fceafd8\"\n-\n-[[package]]\n-name = \"sval_buffer\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"429922f7ad43c0ef8fd7309e14d750e38899e32eb7e8da656ea169dd28ee212f\"\n-dependencies = [\n- \"sval\",\n- \"sval_ref\",\n-]\n-\n-[[package]]\n-name = \"sval_dynamic\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"68f16ff5d839396c11a30019b659b0976348f3803db0626f736764c473b50ff4\"\n-dependencies = [\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_fmt\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c01c27a80b6151b0557f9ccbe89c11db571dc5f68113690c1e028d7e974bae94\"\n-dependencies = [\n- \"itoa\",\n- \"ryu\",\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_json\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0deef63c70da622b2a8069d8600cf4b05396459e665862e7bdb290fd6cf3f155\"\n-dependencies = [\n- \"itoa\",\n- \"ryu\",\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_nested\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a39ce5976ae1feb814c35d290cf7cf8cd4f045782fe1548d6bc32e21f6156e9f\"\n-dependencies = [\n- \"sval\",\n- \"sval_buffer\",\n- \"sval_ref\",\n-]\n-\n-[[package]]\n-name = \"sval_ref\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bb7c6ee3751795a728bc9316a092023529ffea1783499afbc5c66f5fabebb1fa\"\n-dependencies = [\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_serde\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2a5572d0321b68109a343634e3a5d576bf131b82180c6c442dee06349dfc652a\"\n-dependencies = [\n- \"serde\",\n- \"sval\",\n- \"sval_nested\",\n-]\n-\n [[package]]\n name = \"swrite\"\n version = \"0.1.0\"\n@@ -3123,9 +3024,9 @@ checksum = \"7f3fece30b2dc06d65ecbca97b602db15bf75f932711d60cc604534f1f8b7a03\"\n \n [[package]]\n name = \"syn\"\n-version = \"2.0.96\"\n+version = \"2.0.98\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d5d0adab1ae378d7f53bdebc67a39f1f151407ef230f0ce2883572f5d8985c80\"\n+checksum = \"36147f1a48ae0ec2b5b3bc5b537d267457555a10dc06f3dbc8cb11ba3006d3b1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -3166,9 +3067,9 @@ checksum = \"61c41af27dd6d1e27b1b16b489db798443478cef1f06a660c96db617ba5de3b1\"\n \n [[package]]\n name = \"target-spec\"\n-version = \"3.3.1\"\n+version = \"3.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"da00bc76fdc8b7ed7163b7cf728179277b26985c177510b69cc320f30b5a517f\"\n+checksum = \"d9e2827c8368f9860f6d263f872175b7dd1d62628fc7518335fb1a660e477d6a\"\n dependencies = [\n  \"cfg-expr\",\n  \"guppy-workspace-hack\",\n@@ -3593,12 +3494,6 @@ version = \"0.2.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"e421abadd41a4225275504ea4d6566923418b7f05506fbc9c0fe86ba7396114b\"\n \n-[[package]]\n-name = \"typeid\"\n-version = \"1.0.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0e13db2e0ccd5e14a544e8a246ba2312cd25223f616442d7f2cb0e3db614236e\"\n-\n [[package]]\n name = \"typenum\"\n version = \"1.17.0\"\n@@ -3700,42 +3595,6 @@ version = \"0.1.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d\"\n \n-[[package]]\n-name = \"value-bag\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3ef4c4aa54d5d05a279399bfa921ec387b7aba77caf7a682ae8d86785b8fdad2\"\n-dependencies = [\n- \"value-bag-serde1\",\n- \"value-bag-sval2\",\n-]\n-\n-[[package]]\n-name = \"value-bag-serde1\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"4bb773bd36fd59c7ca6e336c94454d9c66386416734817927ac93d81cb3c5b0b\"\n-dependencies = [\n- \"erased-serde\",\n- \"serde\",\n- \"serde_fmt\",\n-]\n-\n-[[package]]\n-name = \"value-bag-sval2\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"53a916a702cac43a88694c97657d449775667bcd14b70419441d05b7fea4a83a\"\n-dependencies = [\n- \"sval\",\n- \"sval_buffer\",\n- \"sval_dynamic\",\n- \"sval_fmt\",\n- \"sval_json\",\n- \"sval_ref\",\n- \"sval_serde\",\n-]\n-\n [[package]]\n name = \"vcpkg\"\n version = \"0.2.15\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 8e07550909a..844ecfb39e5 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -61,7 +61,7 @@ fs-err = \"3.1.0\"\n future-queue = \"0.3.0\"\n futures = \"0.3.31\"\n globset = \"0.4.15\"\n-guppy = \"0.17.12\"\n+guppy = \"0.17.13\"\n hex = \"0.4.3\"\n home = \"0.5.11\"\n http = \"1.2.0\"\n@@ -76,7 +76,7 @@ is_ci = \"1.2.0\"\n itertools = \"0.14.0\"\n libc = \"0.2.169\"\n libtest-mimic = \"0.8.1\"\n-log = \"0.4.24\"\n+log = \"0.4.25\"\n maplit = \"1.0.2\"\n miette = \"7.5.0\"\n mukti-metadata = \"0.3.0\"\n@@ -116,7 +116,7 @@ supports-color = \"3.0.2\"\n supports-unicode = \"3.0.0\"\n swrite = \"0.1.0\"\n tar = \"0.4.43\"\n-target-spec = { version = \"3.3.1\", features = [\"custom\", \"summaries\"] }\n+target-spec = { version = \"3.4.0\", features = [\"custom\", \"summaries\"] }\n target-spec-miette = \"0.4.4\"\n test-case = \"3.3.1\"\n test-strategy = \"0.4.0\"\ndiff --git a/workspace-hack/Cargo.toml b/workspace-hack/Cargo.toml\nindex 5e0f0b15996..5c93155daa2 100644\n--- a/workspace-hack/Cargo.toml\n+++ b/workspace-hack/Cargo.toml\n@@ -26,7 +26,7 @@ console = { version = \"0.15.8\" }\n either = { version = \"1.13.0\" }\n getrandom = { version = \"0.2.15\", default-features = false, features = [\"std\"] }\n indexmap = { version = \"2.7.1\", features = [\"serde\"] }\n-log = { version = \"0.4.24\", default-features = false, features = [\"std\"] }\n+log = { version = \"0.4.25\", default-features = false, features = [\"std\"] }\n memchr = { version = \"2.7.4\" }\n miette = { version = \"7.5.0\", features = [\"fancy\"] }\n num-traits = { version = \"0.2.19\", default-features = false, features = [\"std\"] }\n@@ -36,7 +36,7 @@ regex-syntax = { version = \"0.8.5\" }\n serde = { version = \"1.0.217\", features = [\"alloc\", \"derive\"] }\n serde_json = { version = \"1.0.138\", features = [\"unbounded_depth\"] }\n smallvec = { version = \"1.13.2\", default-features = false, features = [\"const_generics\"] }\n-target-spec = { version = \"3.3.1\", default-features = false, features = [\"custom\", \"summaries\"] }\n+target-spec = { version = \"3.4.0\", default-features = false, features = [\"custom\", \"summaries\"] }\n target-spec-miette = { version = \"0.4.4\", default-features = false, features = [\"fixtures\"] }\n tokio = { version = \"1.43.0\", features = [\"fs\", \"io-std\", \"io-util\", \"macros\", \"process\", \"rt-multi-thread\", \"signal\", \"sync\", \"time\", \"tracing\"] }\n tracing-core = { version = \"0.1.33\" }\n@@ -50,7 +50,7 @@ cc = { version = \"1.2.5\", default-features = false, features = [\"parallel\"] }\n proc-macro2 = { version = \"1.0.93\" }\n quote = { version = \"1.0.38\" }\n serde = { version = \"1.0.217\", features = [\"alloc\", \"derive\"] }\n-syn = { version = \"2.0.96\", features = [\"extra-traits\", \"full\", \"visit\", \"visit-mut\"] }\n+syn = { version = \"2.0.98\", features = [\"extra-traits\", \"full\", \"visit\", \"visit-mut\"] }\n \n [target.x86_64-unknown-linux-gnu.dependencies]\n futures-channel = { version = \"0.3.31\", features = [\"sink\"] }\n", "test_patch": "diff --git a/cargo-nextest/Cargo.toml b/cargo-nextest/Cargo.toml\nindex 0842abf4aef..efbe3f0aa21 100644\n--- a/cargo-nextest/Cargo.toml\n+++ b/cargo-nextest/Cargo.toml\n@@ -20,6 +20,7 @@ dialoguer.workspace = true\n duct.workspace = true\n enable-ansi-support.workspace = true\n guppy.workspace = true\n+indent_write.workspace = true\n itertools.workspace = true\n miette = { workspace = true, features = [\"fancy\"] }\n # Instead of using workspace dependencies which have floating versions, we pin exact versions here\ndiff --git a/cargo-nextest/src/dispatch.rs b/cargo-nextest/src/dispatch.rs\nindex 9afba3e467b..8628261d96c 100644\n--- a/cargo-nextest/src/dispatch.rs\n+++ b/cargo-nextest/src/dispatch.rs\n@@ -1155,22 +1155,7 @@ impl BaseApp {\n         // Next, read the build platforms.\n         let build_platforms = match reuse_build.binaries_metadata() {\n             Some(kind) => kind.binary_list.rust_build_meta.build_platforms.clone(),\n-            None => {\n-                let host = HostPlatform::current(PlatformLibdir::from_rustc_stdout(\n-                    RustcCli::print_host_libdir().read(),\n-                ))?;\n-\n-                let triple_info =\n-                    discover_target_triple(&cargo_configs, cargo_opts.target.as_deref())?;\n-                let target = triple_info.map(|triple| {\n-                    let libdir = PlatformLibdir::from_rustc_stdout(\n-                        RustcCli::print_target_libdir(&triple).read(),\n-                    );\n-                    TargetPlatform::new(triple, libdir)\n-                });\n-\n-                BuildPlatforms { host, target }\n-            }\n+            None => detect_build_platforms(&cargo_configs, cargo_opts.target.as_deref())?,\n         };\n \n         // Read the Cargo metadata.\n@@ -2103,8 +2088,8 @@ enum DebugCommand {\n     /// Print the current executable path.\n     CurrentExe,\n \n-    /// Show the target platform that nextest would use.\n-    ShowTarget {\n+    /// Show the build platforms that nextest would use.\n+    BuildPlatforms {\n         /// The target triple to use.\n         #[arg(long)]\n         target: Option<String>,\n@@ -2112,6 +2097,10 @@ enum DebugCommand {\n         /// Override a Cargo configuration value.\n         #[arg(long, value_name = \"KEY=VALUE\")]\n         config: Vec<String>,\n+\n+        /// Output format.\n+        #[arg(long, value_enum, default_value_t)]\n+        output_format: BuildPlatformsOutputFormat,\n     },\n }\n \n@@ -2173,13 +2162,31 @@ impl DebugCommand {\n                     .map_err(|err| ExpectedError::GetCurrentExeFailed { err })?;\n                 println!(\"{}\", exe.display());\n             }\n-            DebugCommand::ShowTarget { target, config } => {\n+            DebugCommand::BuildPlatforms {\n+                target,\n+                config,\n+                output_format,\n+            } => {\n                 let cargo_configs = CargoConfigs::new(&config).map_err(Box::new)?;\n-                let target = discover_target_triple(&cargo_configs, target.as_deref())?;\n-                if let Some(target) = target {\n-                    println!(\"{:#?}\", target);\n-                } else {\n-                    println!(\"no target triple found\");\n+                let build_platforms = detect_build_platforms(&cargo_configs, target.as_deref())?;\n+                match output_format {\n+                    BuildPlatformsOutputFormat::Debug => {\n+                        println!(\"{:#?}\", build_platforms);\n+                    }\n+                    BuildPlatformsOutputFormat::Triple => {\n+                        println!(\n+                            \"host triple: {}\",\n+                            build_platforms.host.platform.triple().as_str()\n+                        );\n+                        if let Some(target) = &build_platforms.target {\n+                            println!(\n+                                \"target triple: {}\",\n+                                target.triple.platform.triple().as_str()\n+                            );\n+                        } else {\n+                            println!(\"target triple: (none)\");\n+                        }\n+                    }\n                 }\n             }\n         }\n@@ -2262,6 +2269,17 @@ impl fmt::Display for ExtractOutputFormat {\n     }\n }\n \n+/// Output format for `nextest debug build-platforms`.\n+#[derive(Clone, Copy, Debug, Default, ValueEnum)]\n+pub enum BuildPlatformsOutputFormat {\n+    /// Show Debug output.\n+    #[default]\n+    Debug,\n+\n+    /// Show just the triple.\n+    Triple,\n+}\n+\n fn acquire_graph_data(\n     manifest_path: Option<&Utf8Path>,\n     target_dir: Option<&Utf8Path>,\n@@ -2303,6 +2321,22 @@ fn acquire_graph_data(\n     Ok(json)\n }\n \n+fn detect_build_platforms(\n+    cargo_configs: &CargoConfigs,\n+    target_cli_option: Option<&str>,\n+) -> Result<BuildPlatforms, ExpectedError> {\n+    let host = HostPlatform::detect(PlatformLibdir::from_rustc_stdout(\n+        RustcCli::print_host_libdir().read(),\n+    ))?;\n+    let triple_info = discover_target_triple(cargo_configs, target_cli_option)?;\n+    let target = triple_info.map(|triple| {\n+        let libdir =\n+            PlatformLibdir::from_rustc_stdout(RustcCli::print_target_libdir(&triple).read());\n+        TargetPlatform::new(triple, libdir)\n+    });\n+    Ok(BuildPlatforms { host, target })\n+}\n+\n fn discover_target_triple(\n     cargo_configs: &CargoConfigs,\n     target_cli_option: Option<&str>,\ndiff --git a/cargo-nextest/src/errors.rs b/cargo-nextest/src/errors.rs\nindex fc1372f0c69..2e39c3b9d1a 100644\n--- a/cargo-nextest/src/errors.rs\n+++ b/cargo-nextest/src/errors.rs\n@@ -3,6 +3,7 @@\n \n use crate::{output::StderrStyles, ExtractOutputFormat};\n use camino::Utf8PathBuf;\n+use indent_write::indentable::Indented;\n use itertools::Itertools;\n use nextest_filtering::errors::FiltersetParseErrors;\n use nextest_metadata::NextestExitCode;\n@@ -90,9 +91,9 @@ pub enum ExpectedError {\n         err: TestFilterBuilderError,\n     },\n     #[error(\"unknown host platform\")]\n-    UnknownHostPlatform {\n+    HostPlatformDetectError {\n         #[from]\n-        err: UnknownHostPlatform,\n+        err: HostPlatformDetectError,\n     },\n     #[error(\"target triple error\")]\n     TargetTripleError {\n@@ -398,7 +399,7 @@ impl ExpectedError {\n             | Self::RootManifestNotFound { .. }\n             | Self::CargoConfigError { .. }\n             | Self::TestFilterBuilderError { .. }\n-            | Self::UnknownHostPlatform { .. }\n+            | Self::HostPlatformDetectError { .. }\n             | Self::TargetTripleError { .. }\n             | Self::MetadataMaterializeError { .. }\n             | Self::UnknownArchiveFormat { .. }\n@@ -652,8 +653,8 @@ impl ExpectedError {\n                 error!(\"{err}\");\n                 err.source()\n             }\n-            Self::UnknownHostPlatform { err } => {\n-                error!(\"the host platform was unknown to nextest\");\n+            Self::HostPlatformDetectError { err } => {\n+                error!(\"the host platform could not be detected\");\n                 Some(err as &dyn Error)\n             }\n             Self::TargetTripleError { err } => {\n@@ -882,7 +883,11 @@ impl ExpectedError {\n         };\n \n         while let Some(err) = next_error {\n-            error!(target: \"cargo_nextest::no_heading\", \"\\nCaused by:\\n  {}\", err);\n+            error!(\n+                target: \"cargo_nextest::no_heading\",\n+                \"\\nCaused by:\\n{}\",\n+                Indented { item: err, indent: \"  \" },\n+            );\n             next_error = err.source();\n         }\n     }\ndiff --git a/integration-tests/Cargo.toml b/integration-tests/Cargo.toml\nindex 90f94af4d86..a0299db871b 100644\n--- a/integration-tests/Cargo.toml\n+++ b/integration-tests/Cargo.toml\n@@ -13,6 +13,10 @@ path = \"test-helpers/cargo-nextest-dup.rs\"\n name = \"build-seed-archive\"\n path = \"test-helpers/build-seed-archive.rs\"\n \n+[[bin]]\n+name = \"rustc-shim\"\n+path = \"test-helpers/rustc-shim.rs\"\n+\n [[test]]\n name = \"custom-harness\"\n harness = false\ndiff --git a/integration-tests/src/nextest_cli.rs b/integration-tests/src/nextest_cli.rs\nindex a4d448cca4a..a9ca2a4cc50 100644\n--- a/integration-tests/src/nextest_cli.rs\n+++ b/integration-tests/src/nextest_cli.rs\n@@ -153,6 +153,24 @@ impl CargoNextestOutput {\n     pub fn decode_test_list_json(&self) -> Result<TestListSummary> {\n         Ok(serde_json::from_slice(&self.stdout)?)\n     }\n+\n+    /// Returns the output as a (hopefully) platform-independent snapshot that\n+    /// can be checked in and compared.\n+    pub fn to_snapshot(&self) -> String {\n+        // Don't include the command as its representation is\n+        // platform-dependent.\n+        let output = format!(\n+            \"exit code: {:?}\\n\\\n+            --- stdout ---\\n{}\\n\\n--- stderr ---\\n{}\\n\",\n+            self.exit_status.code(),\n+            String::from_utf8_lossy(&self.stdout),\n+            String::from_utf8_lossy(&self.stderr),\n+        );\n+\n+        // Turn \"exit status\" and \"exit code\" into \"exit status|code\"\n+        let output = output.replace(\"exit status: \", \"exit status|code: \");\n+        output.replace(\"exit code: \", \"exit status|code: \")\n+    }\n }\n \n impl fmt::Display for CargoNextestOutput {\ndiff --git a/integration-tests/test-helpers/rustc-shim.rs b/integration-tests/test-helpers/rustc-shim.rs\nnew file mode 100644\nindex 00000000000..e53550fd6c5\n--- /dev/null\n+++ b/integration-tests/test-helpers/rustc-shim.rs\n@@ -0,0 +1,82 @@\n+// Copyright (c) The nextest Contributors\n+// SPDX-License-Identifier: MIT OR Apache-2.0\n+\n+//! A shim for rustc that is used for injecting errors.\n+\n+use std::{\n+    env::args,\n+    process::{Command, ExitCode},\n+};\n+\n+fn main() -> ExitCode {\n+    // Currently, `--version --verbose` is supported. (This is a bit\n+    // overdetermined, but it's okay.)\n+    let args = args().collect::<Vec<_>>();\n+\n+    if &args[1] == \"--version\" && &args[2] == \"--verbose\" {\n+        match version_verbose_error() {\n+            Some(VersionVerboseError::NonZeroExitCode) => {\n+                println!(\"failure output to stdout\");\n+                eprintln!(\"failure output to stderr\");\n+                return ExitCode::FAILURE;\n+            }\n+            Some(VersionVerboseError::InvalidStdout) => {\n+                println!(\"invalid output to stdout\");\n+                eprintln!(\"invalid output to stderr\");\n+                return ExitCode::SUCCESS;\n+            }\n+            Some(VersionVerboseError::InvalidTriple) => {\n+                println!(\n+                    \"rustc 1.84.1 (e71f9a9a9 2025-01-27)\\n\\\n+                     binary: rustc\\n\\\n+                     commit-hash: e71f9a9a98b0faf423844bf0ba7438f29dc27d58\\n\\\n+                     commit-date: 2025-01-27\\n\\\n+                     host: invalid-triple\\n\\\n+                     release: 1.84.1\\n\\\n+                     LLVM version: 19.1.5\\n\\\n+                    \"\n+                );\n+                return ExitCode::SUCCESS;\n+            }\n+            None => {\n+                // Pass through to the real rustc.\n+            }\n+        }\n+    }\n+\n+    let code = Command::new(rustc_path())\n+        .args(&args[1..])\n+        .status()\n+        .expect(\"rustc executed successfully\")\n+        .code();\n+\n+    std::process::exit(code.unwrap_or(1))\n+}\n+\n+#[derive(Clone, Copy, Debug)]\n+enum VersionVerboseError {\n+    NonZeroExitCode,\n+    InvalidStdout,\n+    InvalidTriple,\n+}\n+\n+fn version_verbose_error() -> Option<VersionVerboseError> {\n+    const VAR: &str = \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\";\n+    match std::env::var(VAR) {\n+        Ok(s) => match s.as_str() {\n+            \"non-zero\" => Some(VersionVerboseError::NonZeroExitCode),\n+            \"invalid-stdout\" => Some(VersionVerboseError::InvalidStdout),\n+            \"invalid-triple\" => Some(VersionVerboseError::InvalidTriple),\n+            _ => panic!(\"unrecognized value for {}: {}\", VAR, s),\n+        },\n+        Err(_) => None,\n+    }\n+}\n+\n+fn rustc_path() -> String {\n+    const VAR: &str = \"__NEXTEST_RUSTC_SHIM_RUSTC\";\n+    match std::env::var(VAR) {\n+        Ok(s) => s,\n+        Err(_) => panic!(\"{VAR} not set\"),\n+    }\n+}\ndiff --git a/integration-tests/tests/datatest/custom_target.rs b/integration-tests/tests/datatest/custom_target.rs\nindex 4106850dbe6..31078af31c2 100644\n--- a/integration-tests/tests/datatest/custom_target.rs\n+++ b/integration-tests/tests/datatest/custom_target.rs\n@@ -24,7 +24,7 @@ pub(crate) fn custom_invalid(path: &Utf8Path, contents: String) -> datatest_stab\n             \"--color\",\n             \"always\",\n             \"debug\",\n-            \"show-target\",\n+            \"build-platforms\",\n             \"--target\",\n             json_path.as_str(),\n         ])\ndiff --git a/integration-tests/tests/integration/main.rs b/integration-tests/tests/integration/main.rs\nindex 25ef476ef1d..3660d39dff0 100644\n--- a/integration-tests/tests/integration/main.rs\n+++ b/integration-tests/tests/integration/main.rs\n@@ -29,7 +29,7 @@ use integration_tests::{\n };\n use nextest_metadata::{BuildPlatform, NextestExitCode, TestListSummary};\n use std::{borrow::Cow, fs::File, io::Write};\n-use target_spec::Platform;\n+use target_spec::{summaries::TargetFeaturesSummary, Platform};\n \n mod fixtures;\n mod stuck_signal;\n@@ -1382,8 +1382,9 @@ fn test_target_arg() {\n     set_env_vars();\n     let p = TempProject::new().unwrap();\n \n-    let host_platform = Platform::current().expect(\"should detect the host target successfully\");\n-    let host_triple = host_platform.triple_str();\n+    let build_target_platform =\n+        Platform::build_target().expect(\"should detect the host target successfully\");\n+    let host_triple = build_target_platform.triple_str();\n     let output = CargoNextestCli::for_test()\n         .args([\n             \"--manifest-path\",\n@@ -1400,10 +1401,110 @@ fn test_target_arg() {\n         .rust_build_meta\n         .platforms\n         .expect(\"should have the platforms field\");\n-    assert_eq!(build_platforms.host.platform, host_platform.to_summary());\n+\n+    // Target features get reset to unknown, unfortunately, so we can't compare\n+    // the full platform.\n+    let mut summary = build_target_platform.to_summary();\n+    summary.target_features = TargetFeaturesSummary::Unknown;\n+    assert_eq!(build_platforms.host.platform, summary);\n+\n     assert_eq!(build_platforms.targets[0].platform.triple, host_triple);\n     assert_eq!(\n         build_platforms.targets[0].libdir,\n         build_platforms.host.libdir\n     );\n }\n+\n+#[test]\n+fn test_rustc_version_verbose_errors() {\n+    set_env_vars();\n+\n+    // Set RUSTC to the shim.\n+    let shim_rustc = std::env::var(\"NEXTEST_BIN_EXE_rustc-shim\").unwrap();\n+\n+    let mut command = CargoNextestCli::for_test();\n+    command\n+        .args([\"debug\", \"build-platforms\", \"--output-format\", \"triple\"])\n+        .env(\"RUSTC\", &shim_rustc);\n+\n+    // --- Error cases ---\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\", \"non-zero\")\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_non_zero\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-stdout\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_stdout\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-triple\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_triple\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    // --- Warning cases ---\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\", \"non-zero\")\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_non_zero_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-stdout\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_stdout_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-triple\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_triple_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+}\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap\nnew file mode 100644\nindex 00000000000..8f9e591dff3\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap\n@@ -0,0 +1,28 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  parsing `rustc -vV` output failed, and detecting the build target failed as well\n+  - host platform error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        invalid output to stdout\n+        ---\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap\nnew file mode 100644\nindex 00000000000..073e5e071c9\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap\n@@ -0,0 +1,20 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, parsing `rustc -vV` failed; falling back to build target `x86_64-unknown-linux-gnu`\n+- host platform error:\n+  error parsing `rustc -vV` output\n+    caused by:\n+    - output from `rustc -vV` did not contain a `host: ` line; output:\n+      ---\n+      invalid output to stdout\n+      ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap\nnew file mode 100644\nindex 00000000000..de75aeb40b7\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap\n@@ -0,0 +1,27 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  parsing `rustc -vV` output failed, and detecting the build target failed as well\n+  - host platform error:\n+    unknown platform triple\n+      caused by:\n+      - unknown triple string: invalid-triple\n+      - triple not in builtin platforms and heuristic parsing failed\n+      - Unrecognized architecture: invalid\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap\nnew file mode 100644\nindex 00000000000..20c5245a038\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap\n@@ -0,0 +1,19 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, parsing `rustc -vV` failed; falling back to build target `x86_64-unknown-linux-gnu`\n+- host platform error:\n+  unknown platform triple\n+    caused by:\n+    - unknown triple string: invalid-triple\n+    - triple not in builtin platforms and heuristic parsing failed\n+    - Unrecognized architecture: invalid\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap\nnew file mode 100644\nindex 00000000000..86b27f9b240\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap\n@@ -0,0 +1,27 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  `rustc -vV` failed with exit status|code: 1, and detecting the build target failed as well\n+  - `rustc -vV` stdout:\n+    failure output to stdout\n+\n+  - `rustc -vV` stderr:\n+    failure output to stderr\n+\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap\nnew file mode 100644\nindex 00000000000..2d58e99b904\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap\n@@ -0,0 +1,18 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, `rustc -vV` failed with exit status|code: 1; falling back to build target `x86_64-unknown-linux-gnu`\n+- `rustc -vV` stdout:\n+  failure output to stdout\n+\n+- `rustc -vV` stderr:\n+  failure output to stderr\ndiff --git a/nextest-runner/src/errors.rs b/nextest-runner/src/errors.rs\nindex 93000ae1db0..6c477c139d0 100644\n--- a/nextest-runner/src/errors.rs\n+++ b/nextest-runner/src/errors.rs\n@@ -13,7 +13,7 @@ use crate::{\n };\n use camino::{FromPathBufError, Utf8Path, Utf8PathBuf};\n use config::ConfigError;\n-use indent_write::fmt::IndentWriter;\n+use indent_write::{fmt::IndentWriter, indentable::Indented};\n use itertools::{Either, Itertools};\n use nextest_filtering::errors::FiltersetParseErrors;\n use nextest_metadata::RustBinaryId;\n@@ -385,7 +385,7 @@ impl<T: std::error::Error> fmt::Display for ErrorList<T> {\n         )?;\n         for error in &self.inner {\n             let mut indent = IndentWriter::new_skip_initial(\"  \", f);\n-            writeln!(indent, \"* {}\", DisplayErrorChain(error))?;\n+            writeln!(indent, \"* {}\", DisplayErrorChain::new(error))?;\n             f = indent.into_inner();\n         }\n         Ok(())\n@@ -408,11 +408,24 @@ impl<T: std::error::Error> std::error::Error for ErrorList<T> {\n ///\n /// This is similar to the display-error-chain crate, but uses IndentWriter\n /// internally to ensure that subsequent lines are also nested.\n-pub(crate) struct DisplayErrorChain<E>(E);\n+pub(crate) struct DisplayErrorChain<E> {\n+    error: E,\n+    initial_indent: &'static str,\n+}\n \n impl<E: std::error::Error> DisplayErrorChain<E> {\n     pub(crate) fn new(error: E) -> Self {\n-        Self(error)\n+        Self {\n+            error,\n+            initial_indent: \"\",\n+        }\n+    }\n+\n+    pub(crate) fn new_with_initial_indent(initial_indent: &'static str, error: E) -> Self {\n+        Self {\n+            error,\n+            initial_indent,\n+        }\n     }\n }\n \n@@ -421,23 +434,26 @@ where\n     E: std::error::Error,\n {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        write!(f, \"{}\", self.0)?;\n+        let mut writer = IndentWriter::new(self.initial_indent, f);\n+        write!(writer, \"{}\", self.error)?;\n \n-        let Some(mut cause) = self.0.source() else {\n+        let Some(mut cause) = self.error.source() else {\n             return Ok(());\n         };\n \n-        write!(f, \"\\n  caused by:\")?;\n+        write!(writer, \"\\n  caused by:\")?;\n \n-        let mut indent = IndentWriter::new_skip_initial(\"  \", f);\n         loop {\n-            write!(indent, \"\\n- {}\", cause)?;\n+            writeln!(writer)?;\n+            let mut indent = IndentWriter::new_skip_initial(\"    \", writer);\n+            write!(indent, \"  - {}\", cause)?;\n \n             let Some(next_cause) = cause.source() else {\n                 break Ok(());\n             };\n \n             cause = next_cause;\n+            writer = indent.into_inner();\n         }\n     }\n }\n@@ -762,7 +778,7 @@ pub enum RustBuildMetaParseError {\n \n     /// The host platform could not be determined.\n     #[error(\"the host platform could not be determined\")]\n-    UnknownHostPlatform(#[source] target_spec::Error),\n+    DetectBuildTargetError(#[source] target_spec::Error),\n \n     /// The build metadata includes features unsupported.\n     #[error(\"unsupported features in the build metadata: {message}\")]\n@@ -1459,12 +1475,80 @@ pub enum InvalidCargoCliConfigReason {\n     DoesntProvideValue,\n }\n \n-/// The host platform could not be determined.\n+/// The host platform couldn't be detected.\n #[derive(Debug, Error)]\n-#[error(\"the host platform could not be determined\")]\n-pub struct UnknownHostPlatform {\n-    #[source]\n-    pub(crate) error: target_spec::Error,\n+pub enum HostPlatformDetectError {\n+    /// Spawning `rustc -vV` failed, and detecting the build target failed as\n+    /// well.\n+    #[error(\n+        \"error spawning `rustc -vV`, and detecting the build \\\n+         target failed as well\\n\\\n+         - rustc spawn error: {}\\n\\\n+         - build target error: {}\\n\",\n+        DisplayErrorChain::new_with_initial_indent(\"  \", error),\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    RustcVvSpawnError {\n+        /// The error.\n+        error: std::io::Error,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// `rustc -vV` exited with a non-zero code, and detecting the build target\n+    /// failed as well.\n+    #[error(\n+        \"`rustc -vV` failed with {}, and detecting the \\\n+         build target failed as well\\n\\\n+         - `rustc -vV` stdout:\\n{}\\n\\\n+         - `rustc -vV` stderr:\\n{}\\n\\\n+         - build target error:\\n{}\\n\",\n+        status,\n+        Indented { item: String::from_utf8_lossy(stdout), indent: \"  \" },\n+        Indented { item: String::from_utf8_lossy(stderr), indent: \"  \" },\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    RustcVvFailed {\n+        /// The status.\n+        status: ExitStatus,\n+\n+        /// The standard output from `rustc -vV`.\n+        stdout: Vec<u8>,\n+\n+        /// The standard error from `rustc -vV`.\n+        stderr: Vec<u8>,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// Parsing the host platform failed, and detecting the build target failed\n+    /// as well.\n+    #[error(\n+        \"parsing `rustc -vV` output failed, and detecting the build target \\\n+         failed as well\\n\\\n+         - host platform error:\\n{}\\n\\\n+         - build target error:\\n{}\\n\",\n+        DisplayErrorChain::new_with_initial_indent(\"  \", host_platform_error),\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    HostPlatformParseError {\n+        /// The error that occurred while parsing the host platform.\n+        host_platform_error: Box<target_spec::Error>,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// Test-only code: `rustc -vV` was not queried, and detecting the build\n+    /// target failed as well.\n+    #[error(\"test-only code, so `rustc -vV` was not called; failed to detect build target\")]\n+    BuildTargetError {\n+        /// The error that occurred while detecting the build target.\n+        #[source]\n+        build_target_error: Box<target_spec::Error>,\n+    },\n }\n \n /// An error occurred while determining the cross-compiling target triple.\ndiff --git a/nextest-runner/src/list/rust_build_meta.rs b/nextest-runner/src/list/rust_build_meta.rs\nindex 9cdce9df0bf..b7ed426b524 100644\n--- a/nextest-runner/src/list/rust_build_meta.rs\n+++ b/nextest-runner/src/list/rust_build_meta.rs\n@@ -215,7 +215,7 @@ mod tests {\n         BuildPlatformsSummary, HostPlatformSummary, PlatformLibdirSummary,\n         PlatformLibdirUnavailable,\n     };\n-    use target_spec::summaries::PlatformSummary;\n+    use target_spec::{summaries::PlatformSummary, Platform};\n     use test_case::test_case;\n \n     impl Default for RustBuildMeta<BinaryListState> {\n@@ -235,15 +235,19 @@ mod tests {\n     }\n \n     fn host_current() -> HostPlatform {\n-        HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .expect(\"should detect the host platform successfully\")\n+        HostPlatform {\n+            platform: Platform::build_target()\n+                .expect(\"should detect the build target successfully\"),\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        }\n     }\n \n     fn host_current_with_libdir(libdir: &str) -> HostPlatform {\n-        HostPlatform::current(PlatformLibdir::Available(libdir.into()))\n-            .expect(\"should detect the host platform successfully\")\n+        HostPlatform {\n+            platform: Platform::build_target()\n+                .expect(\"should detect the build target successfully\"),\n+            libdir: PlatformLibdir::Available(libdir.into()),\n+        }\n     }\n \n     fn host_not_current_with_libdir(libdir: &str) -> HostPlatform {\n@@ -427,8 +431,7 @@ mod tests {\n \n         let rust_build_meta = RustBuildMeta {\n             build_platforms: BuildPlatforms {\n-                host: HostPlatform::current(PlatformLibdir::Available(host_libdir.clone()))\n-                    .expect(\"should detect the host platform successfully\"),\n+                host: host_current_with_libdir(host_libdir.as_ref()),\n                 target: Some(TargetPlatform::new(\n                     TargetTriple::x86_64_unknown_linux_gnu(),\n                     PlatformLibdir::Available(target_libdir.clone()),\n@@ -467,8 +470,7 @@ mod tests {\n             linked_paths: [(Utf8PathBuf::from(tmpdir_dirname), Default::default())].into(),\n             base_output_directories: [Utf8PathBuf::from(tmpdir_dirname)].into(),\n             build_platforms: BuildPlatforms {\n-                host: HostPlatform::current(PlatformLibdir::Available(host_libdir.clone()))\n-                    .expect(\"should detect the host platform successfully\"),\n+                host: host_current_with_libdir(host_libdir.as_ref()),\n                 target: Some(TargetPlatform::new(\n                     TargetTriple::x86_64_unknown_linux_gnu(),\n                     PlatformLibdir::Available(target_libdir.clone()),\ndiff --git a/nextest-runner/src/platform.rs b/nextest-runner/src/platform.rs\nindex a842a082c86..c32d556a316 100644\n--- a/nextest-runner/src/platform.rs\n+++ b/nextest-runner/src/platform.rs\n@@ -5,17 +5,23 @@\n \n use crate::{\n     cargo_config::{CargoTargetArg, TargetTriple},\n-    errors::{RustBuildMetaParseError, TargetTripleError, UnknownHostPlatform},\n+    errors::{\n+        DisplayErrorChain, HostPlatformDetectError, RustBuildMetaParseError, TargetTripleError,\n+    },\n     reuse_build::{LibdirMapper, PlatformLibdirMapper},\n+    RustcCli,\n };\n use camino::{Utf8Path, Utf8PathBuf};\n+use indent_write::indentable::Indented;\n use nextest_metadata::{\n     BuildPlatformsSummary, HostPlatformSummary, PlatformLibdirSummary, PlatformLibdirUnavailable,\n     TargetPlatformSummary,\n };\n-use target_spec::summaries::PlatformSummary;\n pub use target_spec::Platform;\n-use tracing::debug;\n+use target_spec::{\n+    errors::RustcVersionVerboseParseError, summaries::PlatformSummary, TargetFeatures,\n+};\n+use tracing::{debug, warn};\n \n /// A representation of host and target platform.\n #[derive(Clone, Debug, Eq, PartialEq)]\n@@ -33,11 +39,18 @@ impl BuildPlatforms {\n     /// Creates a new `BuildPlatforms` with no libdirs or targets.\n     ///\n     /// Used for testing.\n-    pub fn new_with_no_target() -> Result<Self, UnknownHostPlatform> {\n+    pub fn new_with_no_target() -> Result<Self, HostPlatformDetectError> {\n         Ok(Self {\n-            host: HostPlatform::current(PlatformLibdir::Unavailable(\n-                PlatformLibdirUnavailable::new_const(\"test\"),\n-            ))?,\n+            host: HostPlatform {\n+                // Because this is for testing, we just use the build target\n+                // rather than `rustc -vV` output.\n+                platform: Platform::build_target().map_err(|build_target_error| {\n+                    HostPlatformDetectError::BuildTargetError {\n+                        build_target_error: Box::new(build_target_error),\n+                    }\n+                })?,\n+                libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::new_const(\"test\")),\n+            },\n             target: None,\n         })\n     }\n@@ -127,10 +140,13 @@ impl BuildPlatforms {\n         // * the host might be serialized as the target platform as well (we can't detect this case\n         //   reliably, so we treat it as the target platform as well, which isn't a problem in\n         //   practice).\n-        let host = HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .map_err(|error| RustBuildMetaParseError::UnknownHostPlatform(error.error))?;\n+        let host = HostPlatform {\n+            // We don't necessarily have `rustc` available, so we use the build\n+            // target instead.\n+            platform: Platform::build_target()\n+                .map_err(RustBuildMetaParseError::DetectBuildTargetError)?,\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        };\n \n         let target = TargetTriple::deserialize(Some(summary))?.map(|triple| {\n             TargetPlatform::new(\n@@ -153,10 +169,13 @@ impl BuildPlatforms {\n         // * the host might be serialized as the target platform as well (we can't detect this case\n         //   reliably, so we treat it as the target platform as well, which isn't a problem in\n         //   practice).\n-        let host = HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .map_err(|error| RustBuildMetaParseError::UnknownHostPlatform(error.error))?;\n+        let host = HostPlatform {\n+            // We don't necessarily have `rustc` available, so we use the build\n+            // target instead.\n+            platform: Platform::build_target()\n+                .map_err(RustBuildMetaParseError::DetectBuildTargetError)?,\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        };\n \n         let target = TargetTriple::deserialize_str(summary)?.map(|triple| {\n             TargetPlatform::new(\n@@ -180,9 +199,12 @@ pub struct HostPlatform {\n }\n \n impl HostPlatform {\n-    /// Creates a new `HostPlatform` representing the current platform.\n-    pub fn current(libdir: PlatformLibdir) -> Result<Self, UnknownHostPlatform> {\n-        let platform = Platform::current().map_err(|error| UnknownHostPlatform { error })?;\n+    /// Creates a new `HostPlatform` representing the current platform by\n+    /// querying rustc.\n+    ///\n+    /// This may fall back to the build target if `rustc -vV` fails.\n+    pub fn detect(libdir: PlatformLibdir) -> Result<Self, HostPlatformDetectError> {\n+        let platform = detect_host_platform()?;\n         Ok(Self { platform, libdir })\n     }\n \n@@ -214,6 +236,137 @@ impl HostPlatform {\n     }\n }\n \n+/// Detect the host platform by using `rustc -vV`, and falling back to the build\n+/// target.\n+///\n+/// Returns an error if both of those methods fail, and produces a warning if\n+/// `rustc -vV` fails.\n+fn detect_host_platform() -> Result<Platform, HostPlatformDetectError> {\n+    // A test-only environment variable to always make the build target a fixed\n+    // value, or to error out.\n+    const FORCE_BUILD_TARGET_VAR: &str = \"__NEXTEST_FORCE_BUILD_TARGET\";\n+\n+    enum ForceBuildTarget {\n+        Triple(String),\n+        Error,\n+    }\n+\n+    let force_build_target = match std::env::var(FORCE_BUILD_TARGET_VAR).as_deref() {\n+        Ok(\"error\") => Some(ForceBuildTarget::Error),\n+        Ok(triple) => Some(ForceBuildTarget::Triple(triple.to_owned())),\n+        Err(_) => None,\n+    };\n+\n+    let build_target = match force_build_target {\n+        Some(ForceBuildTarget::Triple(triple)) => Platform::new(triple, TargetFeatures::Unknown),\n+        Some(ForceBuildTarget::Error) => Err(target_spec::Error::RustcVersionVerboseParse(\n+            RustcVersionVerboseParseError::MissingHostLine {\n+                output: format!(\n+                    \"({} set to \\\"error\\\", forcibly failing build target detection)\\n\",\n+                    FORCE_BUILD_TARGET_VAR\n+                ),\n+            },\n+        )),\n+        None => Platform::build_target(),\n+    };\n+\n+    let rustc_vv = RustcCli::version_verbose()\n+        .to_expression()\n+        .stdout_capture()\n+        .stderr_capture()\n+        .unchecked();\n+    match rustc_vv.run() {\n+        Ok(output) => {\n+            if output.status.success() {\n+                // Neither `rustc` nor `cargo` tell us what target features are\n+                // enabled for the host, so we must use\n+                // `TargetFeatures::Unknown`.\n+                match Platform::from_rustc_version_verbose(output.stdout, TargetFeatures::Unknown) {\n+                    Ok(platform) => Ok(platform),\n+                    Err(host_platform_error) => {\n+                        match build_target {\n+                            Ok(build_target) => {\n+                                warn!(\n+                                    \"for host platform, parsing `rustc -vV` failed; \\\n+                                     falling back to build target `{}`\\n\\\n+                                     - host platform error:\\n{}\",\n+                                    build_target.triple().as_str(),\n+                                    DisplayErrorChain::new_with_initial_indent(\n+                                        \"  \",\n+                                        host_platform_error\n+                                    ),\n+                                );\n+                                Ok(build_target)\n+                            }\n+                            Err(build_target_error) => {\n+                                // In this case, we can't do anything.\n+                                Err(HostPlatformDetectError::HostPlatformParseError {\n+                                    host_platform_error: Box::new(host_platform_error),\n+                                    build_target_error: Box::new(build_target_error),\n+                                })\n+                            }\n+                        }\n+                    }\n+                }\n+            } else {\n+                match build_target {\n+                    Ok(build_target) => {\n+                        warn!(\n+                            \"for host platform, `rustc -vV` failed with {}; \\\n+                             falling back to build target `{}`\\n\\\n+                             - `rustc -vV` stdout:\\n{}\\n\\\n+                             - `rustc -vV` stderr:\\n{}\",\n+                            output.status,\n+                            build_target.triple().as_str(),\n+                            Indented {\n+                                item: String::from_utf8_lossy(&output.stdout),\n+                                indent: \"  \"\n+                            },\n+                            Indented {\n+                                item: String::from_utf8_lossy(&output.stderr),\n+                                indent: \"  \"\n+                            },\n+                        );\n+                        Ok(build_target)\n+                    }\n+                    Err(build_target_error) => {\n+                        // If the build target isn't available either, we\n+                        // can't do anything.\n+                        Err(HostPlatformDetectError::RustcVvFailed {\n+                            status: output.status,\n+                            stdout: output.stdout,\n+                            stderr: output.stderr,\n+                            build_target_error: Box::new(build_target_error),\n+                        })\n+                    }\n+                }\n+            }\n+        }\n+        Err(error) => {\n+            match build_target {\n+                Ok(build_target) => {\n+                    warn!(\n+                        \"for host platform, failed to spawn `rustc -vV`; \\\n+                         falling back to build target `{}`\\n\\\n+                         - host platform error:\\n{}\",\n+                        build_target.triple().as_str(),\n+                        DisplayErrorChain::new_with_initial_indent(\"  \", error),\n+                    );\n+                    Ok(build_target)\n+                }\n+                Err(build_target_error) => {\n+                    // If the build target isn't available either, we\n+                    // can't do anything.\n+                    Err(HostPlatformDetectError::RustcVvSpawnError {\n+                        error,\n+                        build_target_error: Box::new(build_target_error),\n+                    })\n+                }\n+            }\n+        }\n+    }\n+}\n+\n /// The target platform.\n #[derive(Clone, Debug, Eq, PartialEq)]\n pub struct TargetPlatform {\ndiff --git a/nextest-runner/src/rustc_cli.rs b/nextest-runner/src/rustc_cli.rs\nindex bc0241367d3..4b50c095b4c 100644\n--- a/nextest-runner/src/rustc_cli.rs\n+++ b/nextest-runner/src/rustc_cli.rs\n@@ -14,6 +14,13 @@ pub struct RustcCli<'a> {\n }\n \n impl<'a> RustcCli<'a> {\n+    /// Create a rustc CLI call: `rustc --version --verbose`.\n+    pub fn version_verbose() -> Self {\n+        let mut cli = Self::default();\n+        cli.add_arg(\"--version\").add_arg(\"--verbose\");\n+        cli\n+    }\n+\n     /// Create a rustc CLI call: `rustc --print target-libdir`.\n     pub fn print_host_libdir() -> Self {\n         let mut cli = Self::default();\n@@ -36,7 +43,8 @@ impl<'a> RustcCli<'a> {\n         self\n     }\n \n-    fn to_expression(&self) -> duct::Expression {\n+    /// Convert the command to a [`duct::Expression`].\n+    pub fn to_expression(&self) -> duct::Expression {\n         duct::cmd(\n             self.rustc_path.as_str(),\n             self.args.iter().map(|arg| arg.as_ref()),\ndiff --git a/nextest-runner/src/update.rs b/nextest-runner/src/update.rs\nindex ddfc08fe424..9cb61bab551 100644\n--- a/nextest-runner/src/update.rs\n+++ b/nextest-runner/src/update.rs\n@@ -222,7 +222,10 @@ impl NextestReleases {\n     }\n \n     fn target_triple(&self) -> String {\n-        let current = Platform::current().expect(\"current platform could not be detected\");\n+        // In this case, use the build target, *not* `rustc -vV` output. This\n+        // ensures that e.g. musl binary updates continue to use the musl\n+        // target.\n+        let current = Platform::build_target().expect(\"build target could not be detected\");\n         let triple_str = current.triple_str();\n         if triple_str.ends_with(\"-apple-darwin\") {\n             // Nextest builds a universal binary for Mac.\ndiff --git a/nextest-runner/tests/integration/target_runner.rs b/nextest-runner/tests/integration/target_runner.rs\nindex 746d295cdab..f7a79530b89 100644\n--- a/nextest-runner/tests/integration/target_runner.rs\n+++ b/nextest-runner/tests/integration/target_runner.rs\n@@ -31,7 +31,7 @@ fn runner_for_target(triple: Option<&str>) -> Result<(BuildPlatforms, TargetRunn\n     .unwrap();\n \n     let build_platforms = {\n-        let host = HostPlatform::current(PlatformLibdir::from_rustc_stdout(\n+        let host = HostPlatform::detect(PlatformLibdir::from_rustc_stdout(\n             RustcCli::print_host_libdir().read(),\n         ))?;\n         let target = if let Some(triple) = TargetTriple::find(&configs, triple)? {\n@@ -170,7 +170,7 @@ fn passthrough_path() -> &'static Utf8Path {\n \n fn current_runner_env_var() -> String {\n     PlatformRunner::runner_env_var(\n-        &Platform::current().expect(\"current platform is known to target-spec\"),\n+        &Platform::build_target().expect(\"current platform is known to target-spec\"),\n     )\n }\n \n", "problem_statement": "Bug: default deduced target triple is wrong\n### Description of the issue\n\n**Description:**\n\nYou use `target_spec::Platform::current()` to deduce the host target triple in many locations ([e.g.](https://github.com/nextest-rs/nextest/blob/c872a3b8654cc1a8444d87589f969885c8ab0764/nextest-runner/src/platform.rs#L185)). This is wrong; `Platform::current()` is the triple used at *build time*, causing nextest to think, for example, that the host target triple is `x86_64-unknown-linux-musl` if you obtained a binary distribution of nextest that was built using musl. You should deduce the host target from the stdout of `rustc -vV` instead.\n\n**Steps to reproduce:**\n1. On a system with the `x86_64-unknown-linux-gnu` toolchain installed, obtain the binary distribution of nextest that is built with `x86_64-unknown-linux-musl`\n2. Run `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`\n4. Run `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`\n\n\n### Expected outcome\n\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`: nextest detects the runner.\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`: nextest does not detect the runner.\n\n### Actual result\n\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`: nextest does not detect the runner.\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`: nextest erroneously detect the runner.\n\n\n### Nextest version\n\n```text\n0.9.88\n```\n\n### Additional context\n\n_No response_\n", "hints_text": "", "created_at": "2025-02-09 06:06:33", "merge_commit_sha": "02424f4552c4fa87724dfa962eafa8e940f7fbfc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build and test (ubuntu-latest, stable)', '.github/workflows/ci.yml']", "['Build documentation (ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Build and test (windows-latest, 1.81)', '.github/workflows/ci.yml']", "['Test archives with a runner (destination) (ubuntu-24.04, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Test archives with a runner (destination) (windows-latest, windows-latest)', '.github/workflows/ci.yml']", "['Build and deploy documentation', '.github/workflows/docs.yml']"], ["['Collect test coverage (ubuntu-latest)', '.github/workflows/coverage.yml']", "['Build and test (macos-14, 1.81)', '.github/workflows/ci.yml']"]]}
{"repo": "biomejs/biome", "instance_id": "biomejs__biome-5008", "base_commit": "320a1c469d7b83d24f09a5b31262d790f25f074f", "patch": "diff --git a/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md b/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md\nnew file mode 100644\nindex 000000000000..698d73ca98aa\n--- /dev/null\n+++ b/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md\n@@ -0,0 +1,4 @@\n+---\n+---\n+\n+# Fix [#5007](https://github.com/biomejs/biome/issues/5007) `noMissingVarFunction` false positives for `container-name`\ndiff --git a/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs b/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\nindex fdaf757a7cd8..6a20a39fee17 100644\n--- a/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\n+++ b/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\n@@ -123,9 +123,10 @@ declare_lint_rule! {\n     }\n }\n \n-pub const IGNORED_PROPERTIES: [&str; 17] = [\n+pub const IGNORED_PROPERTIES: [&str; 18] = [\n     \"animation\",\n     \"animation-name\",\n+    \"container-name\",\n     \"counter-increment\",\n     \"counter-reset\",\n     \"counter-set\",\n", "test_patch": "diff --git a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\nindex 8ed3b50ccd2e..fa4d80e37ce0 100644\n--- a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\n+++ b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\n@@ -60,6 +60,11 @@ a {\n     view-transition-name: --bbb;\n }\n \n+@property --baz {}\n+a {\n+\tcontainer-name: --baz;\n+}\n+\n .parent {\n     color: --foo;\n     .child {\ndiff --git a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\nindex 9eb37f42e50b..78aea938c017 100644\n--- a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\n+++ b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\n@@ -66,6 +66,11 @@ a {\n     view-transition-name: --bbb;\n }\n \n+@property --baz {}\n+a {\n+\tcontainer-name: --baz;\n+}\n+\n .parent {\n     color: --foo;\n     .child {\n", "problem_statement": "\ud83d\udc85 false positive error for `no-missing-var-function` with property `container-name`\n### Environment information\n\n```bash\nCLI:\n  Version:                      1.9.4\n  Color support:                true\n\nPlatform:\n  CPU Architecture:             aarch64\n  OS:                           macos\n\nEnvironment:\n  BIOME_LOG_PATH:               unset\n  BIOME_LOG_PREFIX_NAME:        unset\n  BIOME_CONFIG_PATH:            unset\n  NO_COLOR:                     unset\n  TERM:                         \"xterm-256color\"\n  JS_RUNTIME_VERSION:           \"v23.5.0\"\n  JS_RUNTIME_NAME:              \"node\"\n  NODE_PACKAGE_MANAGER:         \"bun/1.2.1\"\n\nBiome Configuration:\n  Status:                       Loaded successfully\n  Formatter disabled:           false\n  Linter disabled:              false\n  Organize imports disabled:    false\n  VCS disabled:                 true\n\nLinter:\n  JavaScript enabled:           true\n  JSON enabled:                 true\n  CSS enabled:                  true\n  GraphQL enabled:              false\n  Recommended:                  true\n  All:                          false\n  Enabled rules:\n  performance/noDelete\n  suspicious/noCatchAssign\n  suspicious/noUnsafeNegation\n  complexity/useLiteralKeys\n  style/useImportType\n  complexity/noMultipleSpacesInRegularExpressionLiterals\n  a11y/useValidLang\n  complexity/noUselessEmptyExport\n  suspicious/useNamespaceKeyword\n  suspicious/useValidTypeof\n  a11y/useValidAriaRole\n  correctness/noConstantCondition\n  a11y/useAriaActivedescendantWithTabindex\n  suspicious/noAssignInExpressions\n  style/useDefaultParameterLast\n  complexity/noEmptyTypeParameters\n  correctness/noConstructorReturn\n  style/useSelfClosingElements\n  suspicious/noDuplicateParameters\n  suspicious/noDuplicateSelectorsKeyframeBlock\n  style/useTemplate\n  correctness/noUnusedLabels\n  complexity/noUselessTernary\n  correctness/noUnreachableSuper\n  suspicious/noCompareNegZero\n  suspicious/noExplicitAny\n  correctness/noSwitchDeclarations\n  a11y/noAutofocus\n  correctness/noUnsafeOptionalChaining\n  correctness/noConstAssign\n  suspicious/noControlCharactersInRegex\n  complexity/noUselessTypeConstraint\n  style/noVar\n  suspicious/noDoubleEquals\n  suspicious/noRedundantUseStrict\n  style/useLiteralEnumMembers\n  suspicious/noGlobalIsNan\n  suspicious/noEmptyInterface\n  suspicious/noConstEnum\n  suspicious/noMisleadingCharacterClass\n  correctness/noPrecisionLoss\n  a11y/noLabelWithoutControl\n  suspicious/noRedeclare\n  correctness/noStringCaseMismatch\n  correctness/noSetterReturn\n  correctness/noInvalidConstructorSuper\n  suspicious/noImplicitAnyLet\n  suspicious/noFallthroughSwitchClause\n  suspicious/noUnsafeDeclarationMerging\n  correctness/noUnreachable\n  a11y/useKeyWithClickEvents\n  suspicious/noDuplicateObjectKeys\n  complexity/noUselessThisAlias\n  complexity/noThisInStatic\n  complexity/useOptionalChain\n  correctness/noInnerDeclarations\n  style/noParameterAssign\n  suspicious/noDuplicateCase\n  a11y/useValidAnchor\n  complexity/useRegexLiterals\n  correctness/noSelfAssign\n  correctness/noInvalidBuiltinInstantiation\n  style/noUselessElse\n  style/useShorthandFunctionType\n  suspicious/noShadowRestrictedNames\n  correctness/noInvalidDirectionInLinearGradient\n  nursery/noMissingVarFunction\n  a11y/useMediaCaption\n  complexity/noUselessLabel\n  complexity/noUselessCatch\n  suspicious/noImportantInKeyframe\n  correctness/noUnsafeFinally\n  a11y/useAriaPropsForRole\n  correctness/noNonoctalDecimalEscape\n  style/useEnumInitializers\n  a11y/useHtmlLang\n  suspicious/noDuplicateTestHooks\n  complexity/noStaticOnlyClass\n  style/useWhile\n  complexity/useArrowFunction\n  style/noInferrableTypes\n  a11y/noNoninteractiveTabindex\n  complexity/useSimpleNumberKeys\n  correctness/useYield\n  a11y/noInteractiveElementToNoninteractiveRole\n  style/useNumericLiterals\n  correctness/noUnnecessaryContinue\n  suspicious/noApproximativeNumericConstant\n  suspicious/noImportAssign\n  suspicious/noLabelVar\n  correctness/noGlobalObjectCalls\n  suspicious/useDefaultSwitchClauseLast\n  a11y/useAltText\n  correctness/noEmptyCharacterClassInRegex\n  correctness/noUnknownUnit\n  suspicious/noSparseArray\n  a11y/useIframeTitle\n  complexity/noBannedTypes\n  a11y/noSvgWithoutTitle\n  correctness/noVoidElementsWithChildren\n  style/useAsConstAssertion\n  correctness/useJsxKeyInIterable\n  style/useExportType\n  complexity/noUselessLoneBlockStatements\n  style/noArguments\n  a11y/useValidAriaValues\n  suspicious/noDebugger\n  suspicious/noCommentText\n  a11y/useFocusableInteractive\n  correctness/noUnmatchableAnbSelector\n  suspicious/noGlobalAssign\n  suspicious/noDuplicateJsxProps\n  suspicious/noMisleadingInstantiator\n  a11y/noPositiveTabindex\n  correctness/noEmptyPattern\n  complexity/noExcessiveNestedTestSuites\n  security/noDangerouslySetInnerHtmlWithChildren\n  a11y/useKeyWithMouseEvents\n  suspicious/noExtraNonNullAssertion\n  suspicious/noShorthandPropertyOverrides\n  correctness/noRenderReturnValue\n  correctness/useExhaustiveDependencies\n  security/noGlobalEval\n  style/noNonNullAssertion\n  a11y/noRedundantRoles\n  complexity/useFlatMap\n  correctness/useIsNan\n  style/useConst\n  suspicious/noGlobalIsFinite\n  suspicious/noSelfCompare\n  suspicious/noThenProperty\n  suspicious/noAsyncPromiseExecutor\n  suspicious/noDuplicateFontNames\n  suspicious/useGetterReturn\n  security/noDangerouslySetInnerHtml\n  style/useNodejsImportProtocol\n  a11y/noDistractingElements\n  suspicious/noArrayIndexKey\n  complexity/noWith\n  suspicious/noDuplicateClassMembers\n  complexity/noExtraBooleanCast\n  performance/noAccumulatingSpread\n  a11y/useValidAriaProps\n  a11y/noRedundantAlt\n  correctness/noChildrenProp\n  correctness/noUnknownFunction\n  correctness/noInvalidPositionAtImportRule\n  suspicious/noConfusingLabels\n  suspicious/noPrototypeBuiltins\n  suspicious/noConfusingVoidType\n  suspicious/noFocusedTests\n  a11y/useButtonType\n  a11y/useSemanticElements\n  a11y/noAriaUnsupportedElements\n  correctness/noInvalidGridAreas\n  correctness/noFlatMapIdentity\n  suspicious/noSuspiciousSemicolonInJsx\n  a11y/noBlankTarget\n  a11y/useHeadingContent\n  correctness/useValidForDirection\n  correctness/noVoidTypeReturn\n  correctness/noInvalidUseBeforeDeclaration\n  a11y/noAriaHiddenOnFocusable\n  a11y/useGenericFontNames\n  correctness/noUnknownMediaFeatureName\n  a11y/useAnchorContent\n  complexity/noUselessRename\n  style/useNumberNamespace\n  complexity/noUselessConstructor\n  a11y/noAccessKey\n  style/useExponentiationOperator\n  style/noUnusedTemplateLiteral\n  complexity/noUselessSwitchCase\n  style/useSingleVarDeclarator\n  suspicious/noExportsInTest\n  a11y/noNoninteractiveElementToInteractiveRole\n  style/noCommaOperator\n  suspicious/noDuplicateAtImportRules\n  suspicious/useIsArray\n  a11y/noHeaderScope\n  complexity/noUselessFragments\n  suspicious/noMisrefactoredShorthandAssign\n  complexity/noForEach\n  suspicious/noClassAssign\n  suspicious/noEmptyBlock\n  suspicious/noFunctionAssign\n\nWorkspace:\n  Open Documents:               0\n```\n\n### Rule name\n\nnursery/noMissingVarFunction\n\n### Playground link\n\nhttps://biomejs.dev/playground/?bracketSameLine=true&lintRules=all&unsafeParameterDecoratorsEnabled=false&allowComments=false&files.main.css=QABwAHIAbwBwAGUAcgB0AHkAIAAtAC0AZgBvAG8AIAB7AAoAIAAgAHMAeQBuAHQAYQB4ADoAIAAiADwAcwB0AHIAaQBuAGcAPgAiADsACgAgACAAaQBuAGgAZQByAGkAdABzADoAIAB0AHIAdQBlADsACgAgACAAaQBuAGkAdABpAGEAbAAtAHYAYQBsAHUAZQA6ACAAIgBjAHUAcwB0AG8AbQAiADsACgB9AAoACgBhACAAewAKACAAIABjAG8AbgB0AGEAaQBuAGUAcgAtAG4AYQBtAGUAOgAgAC0ALQBmAG8AbwA7AAoAfQA%3D\n\n### Expected result\n\ncss analyzer should not report for property `container-name` in `no-missing-var-function` rule.\n\n### Code of Conduct\n\n- [x] I agree to follow Biome's Code of Conduct\n", "hints_text": "", "created_at": "2025-02-01 08:12:36", "merge_commit_sha": "99f27a2b31fc15825a3175d342e2403e9764d22c", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test (windows-latest)', '.github/workflows/pull_request.yml']", "['Test (ubuntu-latest)', '.github/workflows/pull_request.yml']"], ["['Test node.js API', '.github/workflows/pull_request.yml']", "['Validate PR title', '.github/workflows/pull_request_title_lint.yaml']"], ["['Lint project', '.github/workflows/pull_request.yml']", "['Format project', '.github/workflows/pull_request.yml']"]]}
{"repo": "tensorzero/tensorzero", "instance_id": "tensorzero__tensorzero-1076", "base_commit": "cfa78d01fb2fa825d85adf9164e525bb6e0b8144", "patch": "diff --git a/clients/python-pyo3/src/lib.rs b/clients/python-pyo3/src/lib.rs\nindex c331682f4..3f8f33369 100644\n--- a/clients/python-pyo3/src/lib.rs\n+++ b/clients/python-pyo3/src/lib.rs\n@@ -13,6 +13,7 @@ use std::{collections::HashMap, future::Future, path::PathBuf, sync::Arc, time::\n use futures::StreamExt;\n use pyo3::{\n     exceptions::{PyStopAsyncIteration, PyStopIteration, PyValueError},\n+    ffi::c_str,\n     marker::Ungil,\n     prelude::*,\n     sync::GILOnceCell,\n@@ -413,6 +414,7 @@ impl TensorZeroGateway {\n         config_path: Option<&str>,\n         clickhouse_url: Option<String>,\n     ) -> PyResult<Py<TensorZeroGateway>> {\n+        warn_no_config(cls.py(), config_path)?;\n         let client_fut = ClientBuilder::new(ClientBuilderMode::EmbeddedGateway {\n             config_path: config_path.map(PathBuf::from),\n             clickhouse_url,\n@@ -662,6 +664,7 @@ impl AsyncTensorZeroGateway {\n         config_path: Option<&str>,\n         clickhouse_url: Option<String>,\n     ) -> PyResult<Bound<'a, PyAny>> {\n+        warn_no_config(cls.py(), config_path)?;\n         let client_fut = ClientBuilder::new(ClientBuilderMode::EmbeddedGateway {\n             config_path: config_path.map(PathBuf::from),\n             clickhouse_url,\n@@ -866,3 +869,15 @@ fn tensorzero_internal_error(py: Python<'_>, msg: &str) -> PyResult<PyErr> {\n     })?;\n     Ok(PyErr::from_value(err.bind(py).call1((msg,))?))\n }\n+\n+fn warn_no_config(py: Python<'_>, config: Option<&str>) -> PyResult<()> {\n+    if config.is_none() {\n+        let user_warning = py.get_type::<pyo3::exceptions::PyUserWarning>();\n+        PyErr::warn(\n+            py,\n+            &user_warning,\n+            c_str!(\"No config file provided, so only default functions will be available. Use `config_file=\\\"path/to/tensorzero.toml\\\"` to specify a config file.\"), 0\n+        )?;\n+    }\n+    Ok(())\n+}\ndiff --git a/clients/python-pyo3/uv.lock b/clients/python-pyo3/uv.lock\nindex 66e17641a..dde325977 100644\n--- a/clients/python-pyo3/uv.lock\n+++ b/clients/python-pyo3/uv.lock\n@@ -91,7 +91,6 @@ wheels = [\n \n [[package]]\n name = \"tensorzero\"\n-version = \"0.1.0\"\n source = { editable = \".\" }\n dependencies = [\n     { name = \"httpx\" },\n", "test_patch": "diff --git a/clients/python-pyo3/tests/test_client.py b/clients/python-pyo3/tests/test_client.py\nindex e5ca70bb5..b03f9e024 100644\n--- a/clients/python-pyo3/tests/test_client.py\n+++ b/clients/python-pyo3/tests/test_client.py\n@@ -70,9 +70,10 @@ async def async_client(request):\n \n \n def test_sync_embedded_gateway_no_config():\n+    with pytest.warns(UserWarning, match=\"No config file provided\"):\n+        client = TensorZeroGateway.build_embedded()\n     with pytest.raises(TensorZeroError) as exc_info:\n-        with TensorZeroGateway.build_embedded() as client:\n-            client.inference(function_name=\"my_missing_func\", input={})\n+        client.inference(function_name=\"my_missing_func\", input={})\n \n     assert exc_info.value.status_code == 404\n     assert exc_info.value.text == '{\"error\":\"Unknown function: my_missing_func\"}'\n@@ -80,9 +81,10 @@ def test_sync_embedded_gateway_no_config():\n \n @pytest.mark.asyncio\n async def test_async_embedded_gateway_no_config():\n+    with pytest.warns(UserWarning, match=\"No config file provided\"):\n+        client = await AsyncTensorZeroGateway.build_embedded()\n     with pytest.raises(TensorZeroError) as exc_info:\n-        async with await AsyncTensorZeroGateway.build_embedded() as client:\n-            await client.inference(function_name=\"my_missing_func\", input={})\n+        await client.inference(function_name=\"my_missing_func\", input={})\n \n     assert exc_info.value.status_code == 404\n     assert exc_info.value.text == '{\"error\":\"Unknown function: my_missing_func\"}'\n", "problem_statement": "Log a warning when you call `build_embedded` but don't provide a config\nLike we do with the gateway:\n\n```\n        tracing::warn!(\"No config file provided, so only default functions will be available. Use `--config-file path/to/tensorzero.toml` to specify a config file.\");\n```\n\nSo:\n\n```\nNo config file provided, so only default functions will be available. Use `config_file=\"path/to/tensorzero.toml\"` to specify a config file.\n```\n", "hints_text": "", "created_at": "2025-02-19 14:39:20", "merge_commit_sha": "73bdb3a8f5b08ad974b32735eff0283e579391ff", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['check-pyo3-build (ubuntu-22.04-arm, aarch64)', '.github/workflows/general.yml']", "['check-pyo3-build (macos-13, x86_64)', '.github/workflows/general.yml']"], ["['check-pyo3-build (windows-latest, x64)', '.github/workflows/general.yml']", "['check-docker-compose', '.github/workflows/general.yml']"]]}
{"repo": "randombit/botan-rs", "instance_id": "randombit__botan-rs-139", "base_commit": "1c07a514add373f5f1c52349294bf1bb94d94778", "patch": "diff --git a/botan-src/src/lib.rs b/botan-src/src/lib.rs\nindex ce3e68f..515768f 100644\n--- a/botan-src/src/lib.rs\n+++ b/botan-src/src/lib.rs\n@@ -27,6 +27,13 @@ fn configure(build_dir: &str) {\n     configure.arg(\"--without-documentation\");\n     configure.arg(\"--no-install-python-module\");\n     configure.arg(\"--distribution-info=https://crates.io/crates/botan-src\");\n+\n+    configure.arg(format!(\n+        \"--cpu={}\",\n+        env::var(\"CARGO_CFG_TARGET_ARCH\").unwrap()\n+    ));\n+    configure.arg(format!(\"--os={}\", env::var(\"CARGO_CFG_TARGET_OS\").unwrap()));\n+\n     #[cfg(debug_assertions)]\n     configure.arg(\"--with-debug-info\");\n \n@@ -36,11 +43,8 @@ fn configure(build_dir: &str) {\n     configure.arg(\"--amalgamation\");\n \n     let args = [\n-        \"--os\",\n-        \"--cpu\",\n         \"--compiler-cache\",\n         \"--cc\",\n-        \"--cc-min-version\",\n         \"--cc-bin\",\n         \"--cc-abi-flags\",\n         \"--cxxflags\",\n@@ -49,28 +53,15 @@ fn configure(build_dir: &str) {\n         \"--ar-command\",\n         \"--ar-options\",\n         \"--msvc-runtime\",\n-        \"--with-endian\",\n-        \"--with-os-features\",\n-        \"--without-os-features\",\n         \"--system-cert-bundle\",\n-        \"--with-local-config\",\n-        \"--boost-library-name\",\n         \"--module-policy\",\n         \"--enable-modules\",\n         \"--disable-modules\",\n-        \"--library-suffix\",\n-        \"--prefix\",\n-        \"--libdir\",\n-        \"--includedir\",\n-        \"--link-method\",\n     ];\n \n     let flags = [\n         \"--optimize-for-size\",\n-        \"--no-optimizations\",\n         \"--amalgamation\",\n-        \"--minimized-build\",\n-        \"--with-openssl\",\n         \"--with-commoncrypto\",\n         \"--with-sqlite3\",\n     ];\n", "test_patch": "", "problem_statement": "botan-src does not respect cargo's target triple when invoking configure.py\nI'm trying to build a binary on my Apple Silicon mac mini which depends on botan-rs with enabled features `vendored` and `botan3`. Since we want to create a universal binary with `lipo`, we have to build targets `aarch64-apple-darwin` and `x86_64-apple-darwin`. \r\n\r\n\r\nWhen `botan-src` builds the C library, it calls `configure.py` but the target initially passed to `cargo` is completely ignored.\r\nAlthough there is a list of options that can be overridden with `BOTAN_CONFIGURE_` env vars, I think `--os` and `--cpu` should be set depending on `cargo --target` and not via `BOTAN_CONFIGURE_CPU` and `BOTAN_CONFIGURE_OS`. I cannot think of any situation where setting CPU and OS to something other than what has been passed in `--target` would make sense.\r\n\r\n\r\nDumping the environment in `pub fn build() -> (String, String)` shows\r\n```\r\nCARGO_CFG_TARGET_ARCH=aarch64\r\nCARGO_CFG_TARGET_ENDIAN=little\r\nCARGO_CFG_TARGET_ENV=\r\nCARGO_CFG_TARGET_FAMILY=unix\r\nCARGO_CFG_TARGET_OS=macos\r\nCARGO_CFG_TARGET_POINTER_WIDTH=64\r\nCARGO_CFG_TARGET_VENDOR=apple\r\nCARGO_CFG_UNIX=\r\n...\r\nTARGET=aarch64-apple-darwin\r\n```\r\nSo the build script definitely knows what should be built, but doesn't pass the information to `configure.py`.\r\n\r\n\r\n- ### What I did:\r\n```\r\n$ cargo build -p botan --features vendored,botan3 --target x86_64-apple-darwin\r\n```\r\n\r\n\r\n- ### What I expected:\r\nThe library is built for x86_64\r\n\r\n\r\n- ### What actually happened:\r\nThe library was built for aarch64 according to `ld` and the log file\r\n```\r\n   INFO: configure.py invoked with options \"--with-build-dir=/Users/username/src/work/botan-rs/target/debug/build/botan-sys-b2a7d959a11e9080/out/botan --build-targets=static --without-documentation --no-install-python-module --distribution-info=https://crates.io/crates/botan-src --with-debug-info\"\r\n   INFO: Configuring to build Botan 3.5.0 (revision git:2e5c7d3eb0dcb63d087fc3e7c92010aa82509bbf)\r\n   INFO: Python version: \"3.12.5 (main, Aug  6 2024, 19:08:49) [Clang 15.0.0 (clang-1500.3.9.4)]\"\r\n   INFO: Autodetected platform information: OS=\"Darwin\" machine=\"arm64\" proc=\"arm\"\r\n   INFO: Guessing target OS is darwin (use --os to set)\r\n   INFO: Guessing to use compiler xcode (use --cc or CXX to set)\r\n   INFO: Guessing target processor is a arm64 (use --cpu to set)\r\n   INFO: Using /etc/ssl/cert.pem as system certificate store\r\n   INFO: Auto-detected compiler version xcode 15.3\r\n   INFO: Auto-detected compiler arch arm64\r\n   INFO: Target is xcode:15.3-macos-arm64\r\n   INFO: Assuming target arm64 is little endian\r\n```\n", "hints_text": "", "created_at": "2025-02-24 23:03:34", "merge_commit_sha": "4673060f1d5affcba0cab5fc42259f196ec3ef54", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['ci (1.64.0, no-std)', '.github/workflows/ci.yml']", "['ci (stable, vendored)', '.github/workflows/ci.yml']"], ["['ci_mac (stable, botan3)', '.github/workflows/ci.yml']", "['ci (nightly)', '.github/workflows/ci.yml']"], ["['ci_mac (stable, vendored)', '.github/workflows/ci.yml']", "['ci_mac (nightly, botan3)', '.github/workflows/ci.yml']"], ["['rustfmt', '.github/workflows/ci.yml']", "['clippy', '.github/workflows/ci.yml']"]]}
{"repo": "wasmerio/wasmer", "instance_id": "wasmerio__wasmer-5162", "base_commit": "dd09e78f04c90f9fad174db7a3007e3f8e471e4b", "patch": "diff --git a/lib/wasix/src/syscalls/wasi/fd_write.rs b/lib/wasix/src/syscalls/wasi/fd_write.rs\nindex 04954d6770e..7793b72a550 100644\n--- a/lib/wasix/src/syscalls/wasi/fd_write.rs\n+++ b/lib/wasix/src/syscalls/wasi/fd_write.rs\n@@ -163,7 +163,7 @@ pub(crate) fn fd_write_internal<M: MemorySize>(\n                                 if !is_stdio {\n                                     if fd_entry.flags.contains(Fdflags::APPEND) {\n                                         // `fdflags::append` means we need to seek to the end before writing.\n-                                        offset = handle.size();\n+                                        offset = fd_entry.inode.stat.read().unwrap().st_size;\n                                         fd_entry.offset.store(offset, Ordering::Release);\n                                     }\n \ndiff --git a/lib/wasix/src/syscalls/wasi/path_open.rs b/lib/wasix/src/syscalls/wasi/path_open.rs\nindex 9c8d3c81655..4f8c00d57dd 100644\n--- a/lib/wasix/src/syscalls/wasi/path_open.rs\n+++ b/lib/wasix/src/syscalls/wasi/path_open.rs\n@@ -237,7 +237,7 @@ pub(crate) fn path_open_internal(\n                 let open_options = open_options\n                     .write(minimum_rights.write)\n                     .create(minimum_rights.create)\n-                    .append(minimum_rights.append)\n+                    .append(false)\n                     .truncate(minimum_rights.truncate);\n \n                 if minimum_rights.read {\n", "test_patch": "diff --git a/tests/wasi-fyi/fs_open_append_offset.rs b/tests/wasi-fyi/fs_open_append_offset.rs\nnew file mode 100644\nindex 00000000000..fef83616f53\n--- /dev/null\n+++ b/tests/wasi-fyi/fs_open_append_offset.rs\n@@ -0,0 +1,31 @@\n+use std::{\n+    fs,\n+    io::{Seek, SeekFrom, Write},\n+};\n+\n+fn main() {\n+    let file = \"fyi/fs_open_append_offset.dir/file\";\n+    let mut f0 = fs::OpenOptions::new()\n+        .create(true)\n+        .truncate(true)\n+        .write(true)\n+        .open(file)\n+        .unwrap();\n+\n+    f0.write_all(b\"abc\").unwrap();\n+    f0.seek(SeekFrom::Start(1)).unwrap();\n+\n+    assert_eq!(fs::read_to_string(file).unwrap(), \"abc\");\n+\n+    // This open with append should not affect the offset of f0.\n+    let _f1 = fs::OpenOptions::new()\n+        .create(true)\n+        .write(true)\n+        .append(true)\n+        .open(file)\n+        .unwrap();\n+\n+    f0.write_all(b\"d\").unwrap();\n+\n+    assert_eq!(fs::read_to_string(file).unwrap(), \"adc\");\n+}\ndiff --git a/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore b/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore\nnew file mode 100644\nindex 00000000000..f73f3093ff8\n--- /dev/null\n+++ b/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore\n@@ -0,0 +1,1 @@\n+file\n", "problem_statement": "Opening a file with append should not change existing fd offset\n### Describe the bug\r\n\r\nIf file has been opened with a descriptor `fd0`, opening the same file with the append flag&mdash;producing `fd1`&mdash;should not change the offset of `fd0`.  `fd0` and `fd1` should have independent offsets.\r\n\r\nThis behavior is consistent across Linux and other Wasm runtimes (Wasmtime, WAMR, WasmEdge, Wazero), but Wasmer changes the offset of the original descriptor.\r\n\r\n```sh\r\nwasmer -vV; rustc -vV\r\nwasmer 4.4.0 (8b97cfe 2024-10-18)\r\nbinary: wasmer-cli\r\ncommit-hash: 8b97cfe3992ae3c2f0002f9955fcb23057f666a9\r\ncommit-date: 2024-10-18\r\nhost: x86_64-unknown-linux-gnu\r\ncompiler: singlepass,cranelift\r\nc_api backend: \r\nrustc 1.81.0 (eeb90cda1 2024-09-04)\r\nbinary: rustc\r\ncommit-hash: eeb90cda1969383f56a2637cbd3037bdf598841c\r\ncommit-date: 2024-09-04\r\nhost: x86_64-unknown-linux-gnu\r\nrelease: 1.81.0\r\nLLVM version: 18.1.7\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nCompile and run this snippet, mounting some diretory:\r\n\r\n```\r\ncargo build --target wasm32-wasip1\r\nwasmer run --mapdir /dir:dir target/wasm32-wasip1/debug/repro.wasm\r\n```\r\n\r\n```rust\r\nuse std::{\r\n    fs,\r\n    io::{Seek, SeekFrom, Write},\r\n};\r\n\r\nfn main() {\r\n    let mut f0 = fs::OpenOptions::new()\r\n        .create(true)\r\n        .write(true)\r\n        .open(\"file\")\r\n        .unwrap();\r\n\r\n    f0.write_all(b\"abc\").unwrap();\r\n    f0.seek(SeekFrom::Start(1)).unwrap();\r\n\r\n    assert_eq!(fs::read_to_string(\"file\").unwrap(), \"abc\");\r\n\r\n    // This open with append should not affect the offset of f0.\r\n    let _f1 = fs::OpenOptions::new()\r\n        .create(true)\r\n        .write(true)\r\n        .append(true)\r\n        .open(\"file\")\r\n        .unwrap();\r\n\r\n    f0.write_all(b\"d\").unwrap();\r\n\r\n    assert_eq!(fs::read_to_string(\"file\").unwrap(), \"adc\");\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nRunning this snippet should not panic.\r\n\r\n### Actual behavior\r\n\r\nWasmer panics, producing a file with the content `abcd` instead of `adc`.\r\n\r\n\n", "hints_text": "", "created_at": "2024-10-20 04:05:51", "merge_commit_sha": "64726330c2139b2c0a2b9ab0ee1705cf5ecaba67", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test API for v8 feature on windows-x64', '.github/workflows/test.yaml']", "['Build wasmer-cli on macos-arm', '.github/workflows/test.yaml']"], ["['Unit-test packages on std on macos-arm', '.github/workflows/test.yaml']", "['Build and test C-API on linux-musl', '.github/workflows/test.yaml']"], ["['Build wasmer-cli on macos-x64', '.github/workflows/test.yaml']", "['Run wast test suite for all compilers on macos-arm', '.github/workflows/test.yaml']"], ["['Actions', '.github/workflows/cifuzz.yaml']", "['Unit-test cranelift on no-std on macos-x64', '.github/workflows/test.yaml']"], ["['Security', '.github/workflows/cifuzz.yaml']", "['Unit-test examples on linux-musl', '.github/workflows/test.yaml']"], ["['Unit-test wasmer-cli on macos-arm', '.github/workflows/test.yaml']", "['Build C-API on linux-riscv64', '.github/workflows/test.yaml']"], ["['Test API for wasmi feature on macos-arm', '.github/workflows/test.yaml']", "['Build wasmer-cli on linux-aarch64', '.github/workflows/test.yaml']"], ["['Code lint', '.github/workflows/test.yaml']", "['Code', '.github/workflows/cifuzz.yaml']"], ["['Unit-test wasmer-cli on macos-x64', '.github/workflows/test.yaml']", "['test-wasmer-integration-tests', '.github/workflows/test.yaml']"], ["['Run wast test suite for all compilers on linux-x64', '.github/workflows/test.yaml']", "['Workflow file', '.github/workflows/cifuzz.yaml']"], ["['Unit-test examples on linux-x64', '.github/workflows/test.yaml']", "['Test API for wasmi feature on linux-x64', '.github/workflows/test.yaml']"], ["['Build and test C-API on linux-x64', '.github/workflows/test.yaml']", "['Unit-test singlepass on no-std on linux-x64', '.github/workflows/test.yaml']"], ["['Unit-test wasmer-cli on linux-x64', '.github/workflows/test.yaml']", "['Build and test C-API on windows-gnu', '.github/workflows/test.yaml']"], ["['Unit-test examples on windows-x64', '.github/workflows/test.yaml']", "['Build and test C-API on macos-x64', '.github/workflows/test.yaml']"], ["['Unit-test cranelift on no-std on linux-x64', '.github/workflows/test.yaml']", "['Unit-test wasmer-cli on windows-x64', '.github/workflows/test.yaml']"], ["['Discussions', '.github/workflows/cifuzz.yaml']", "['Unit-test packages on std on linux-x64', '.github/workflows/test.yaml']"], ["['Run wast test suite for all compilers on linux-musl', '.github/workflows/test.yaml']", "['Unit-test packages on std on linux-musl', '.github/workflows/test.yaml']"], ["['Unit-test cranelift on no-std on linux-musl', '.github/workflows/test.yaml']", "['Build wasmer-cli on windows-x64', '.github/workflows/test.yaml']"], ["['Projects', '.github/workflows/cifuzz.yaml']", "['Unit-test cranelift on no-std on macos-arm', '.github/workflows/test.yaml']"], ["['Build C-API on linux-aarch64', '.github/workflows/test.yaml']", "['Test WASIX', '.github/workflows/test.yaml']"]]}
{"repo": "tauri-apps/wry", "instance_id": "tauri-apps__wry-1200", "base_commit": "3ec2ea9d4724843c879a365aea94eae1e58a7b12", "patch": "diff --git a/.changes/32bit-compilation.md b/.changes/32bit-compilation.md\nnew file mode 100644\nindex 000000000..c18e035c9\n--- /dev/null\n+++ b/.changes/32bit-compilation.md\n@@ -0,0 +1,5 @@\n+---\n+wry: patch\n+---\n+\n+Fixes compilation for 32bit Linux targets.\ndiff --git a/src/webkitgtk/mod.rs b/src/webkitgtk/mod.rs\nindex a0f343f93..39966bf7a 100644\n--- a/src/webkitgtk/mod.rs\n+++ b/src/webkitgtk/mod.rs\n@@ -17,7 +17,10 @@ use javascriptcore::ValueExt;\n use raw_window_handle::{HasWindowHandle, RawWindowHandle};\n #[cfg(any(debug_assertions, feature = \"devtools\"))]\n use std::sync::atomic::{AtomicBool, Ordering};\n-use std::sync::{Arc, Mutex};\n+use std::{\n+  ffi::c_ulong,\n+  sync::{Arc, Mutex},\n+};\n #[cfg(any(debug_assertions, feature = \"devtools\"))]\n use webkit2gtk::WebInspectorExt;\n use webkit2gtk::{\n@@ -51,7 +54,7 @@ struct X11Data {\n   is_child: bool,\n   xlib: Xlib,\n   x11_display: *mut std::ffi::c_void,\n-  x11_window: u64,\n+  x11_window: c_ulong,\n   gtk_window: gtk::Window,\n }\n \n@@ -152,9 +155,9 @@ impl InnerWebView {\n   fn create_container_x11_window(\n     xlib: &Xlib,\n     display: *mut _XDisplay,\n-    parent: u64,\n+    parent: c_ulong,\n     attributes: &WebViewAttributes,\n-  ) -> u64 {\n+  ) -> c_ulong {\n     let window = unsafe {\n       (xlib.XCreateSimpleWindow)(\n         display,\n@@ -179,7 +182,10 @@ impl InnerWebView {\n     window\n   }\n \n-  pub fn create_gtk_window(raw: *mut GdkX11Display, x11_window: u64) -> (gtk::Window, gtk::Box) {\n+  pub fn create_gtk_window(\n+    raw: *mut GdkX11Display,\n+    x11_window: c_ulong,\n+  ) -> (gtk::Window, gtk::Box) {\n     // Gdk.Window\n     let gdk_window = unsafe { gdk_x11_window_foreign_new_for_display(raw, x11_window) };\n     let gdk_window = unsafe { gdk::Window::from_glib_full(gdk_window) };\n", "test_patch": "", "problem_statement": "[wry-0.37.0] Build failed for Linux i686 and armv7 32-bit targets\n**Describe the bug**\r\n\r\nWhen building a Tauri v2.0-beta application (wry-0.37.0) on Ubuntu 22, the build succeeds for x86_64 and aarch64 architectures but fails for i686 and armv7 32-bit targets with the following error:\r\n\r\n```shell\r\nerror[E0308]: mismatched types\r\n  --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:66:72\r\n   |\r\n66 |         unsafe { (xlib.XDestroyWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap()) };\r\n   |                  ---------------------                                 ^^^^^^^^^^^^^^^^^^^^^^^^ expected `u32`, found `u64`\r\n   |                  |\r\n   |                  arguments to this function are incorrect\r\n   |\r\nhelp: you can convert a `u64` to a `u32` and panic if the converted value doesn't fit\r\n```\r\n\r\nThe cross-compilation environment has been correctly configured for the `i686-unknown-linux-gnu` and `armv7-unknown-linux-gnueabihf` targets, following the official documentation.\r\n\r\n**Steps To Reproduce**\r\n\r\nOn Ubuntu 22 (x86_64), build the latest Tauri v2.0.0-beta example app targeting `i686-unknown-linux-gnu` or `armv7-unknown-linux-gnueabihf`.\r\n\r\n**Expected behavior**\r\n\r\nThe i686 and armv7 versions of the app should build successfully.\r\n\r\n**Screenshots**\r\n\r\n<img width=\"1497\" alt=\"\u56fe\u7247\" src=\"https://github.com/tauri-apps/wry/assets/35302658/f7473ef6-97e8-48ae-9531-013e50da372d\">\r\n\r\n**Platform and Versions (please complete the following information):**\r\n\r\nPlatform: Github Actions (ubuntu-latest)\r\nBuild Targets: `i686-unknown-linux-gnu`, `armv7-unknown-linux-gnueabihf`\r\n\r\n**Additional context**\r\n\r\nGithub Action logs: https://github.com/idootop/feiyu-player/actions/runs/8398912429/job/23004365678\r\n\r\n<details>\r\n<summary>\ud83d\udc49 Full error logs</summary>\r\n\r\n```shell\r\nerror[E0308]: mismatched types\r\n  --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:66:72\r\n   |\r\n66 |         unsafe { (xlib.XDestroyWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap()) };\r\n   |                  ---------------------                                 ^^^^^^^^^^^^^^^^^^^^^^^^ expected `u32`, found `u64`\r\n   |                  |\r\n   |                  arguments to this function are incorrect\r\n   |\r\nhelp: you can convert a `u64` to a `u32` and panic if the converted value doesn't fit\r\n   |\r\n66 |         unsafe { (xlib.XDestroyWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap().try_into().unwrap()) };\r\n   |                                                                                                ++++++++++++++++++++\r\n\r\nerror[E0308]: mismatched types\r\n   --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:168:27\r\n    |\r\n168 |       w.x11_window = Some(window);\r\n    |                      ---- ^^^^^^ expected `u64`, found `u32`\r\n    |                      |\r\n    |                      arguments to this enum variant are incorrect\r\n    |\r\nhelp: the type constructed contains `u32` due to the type of the argument passed\r\n   --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:168:22\r\n    |\r\n168 |       w.x11_window = Some(window);\r\n    |                      ^^^^^------^\r\n    |                           |\r\n    |                           this argument influences the type of `Some`\r\nnote: tuple variant defined here\r\n   --> /rustc/aedd173a2c086e558c2b66d3743b344f977621a7/library/core/src/option.rs:578:5\r\nhelp: you can convert a `u32` to a `u64`\r\n    |\r\n168 |       w.x11_window = Some(window.into());\r\n    |                                 +++++++\r\n\r\nerror[E0308]: mismatched types\r\n   --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:635:60\r\n    |\r\n635 |         let ok = (xlib.XGetWindowAttributes)(display as _, window_handle, &mut attributes);\r\n    |                  ---------------------------               ^^^^^^^^^^^^^ expected `u32`, found `u64`\r\n    |                  |\r\n    |                  arguments to this function are incorrect\r\n    |\r\nhelp: you can convert a `u64` to a `u32` and panic if the converted value doesn't fit\r\n    |\r\n635 |         let ok = (xlib.XGetWindowAttributes)(display as _, window_handle.try_into().unwrap(), &mut attributes);\r\n    |                                                                         ++++++++++++++++++++\r\n\r\nerror[E0308]: mismatched types\r\n   --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:697:68\r\n    |\r\n697 |         unsafe { (xlib.XMapWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap()) };\r\n    |                  -----------------                                 ^^^^^^^^^^^^^^^^^^^^^^^^ expected `u32`, found `u64`\r\n    |                  |\r\n    |                  arguments to this function are incorrect\r\n    |\r\nhelp: you can convert a `u64` to a `u32` and panic if the converted value doesn't fit\r\n    |\r\n697 |         unsafe { (xlib.XMapWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap().try_into().unwrap()) };\r\n    |                                                                                            ++++++++++++++++++++\r\n\r\nerror[E0308]: mismatched types\r\n   --> /home/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wry-0.37.0/src/webkitgtk/mod.rs:699:70\r\n    |\r\n699 |         unsafe { (xlib.XUnmapWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap()) };\r\n    |                  -------------------                                 ^^^^^^^^^^^^^^^^^^^^^^^^ expected `u32`, found `u64`\r\n    |                  |\r\n    |                  arguments to this function are incorrect\r\n    |\r\nhelp: you can convert a `u64` to a `u32` and panic if the converted value doesn't fit\r\n    |\r\n699 |         unsafe { (xlib.XUnmapWindow)(self.x11_display.unwrap() as _, self.x11_window.unwrap().try_into().unwrap()) };\r\n    |                                                                                              ++++++++++++++++++++\r\n\r\nFor more information about this error, try `rustc --explain E0308`.\r\nerror: could not compile `wry` (lib) due to 5 previous errors\r\nwarning: build failed, waiting for other jobs to finish...\r\n    Error failed to build app: failed to build app\r\n```\r\n\r\n</details>\r\n\n", "hints_text": "", "created_at": "2024-03-25 12:08:43", "merge_commit_sha": "3e84a0e276dfac0b28fb01f42460f9367fff9f22", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test (x86_64-pc-windows-msvc, windows-latest)', '.github/workflows/build.yml']", "['test (x86_64-apple-darwin, macos-latest)', '.github/workflows/build.yml']"], ["['fmt', '.github/workflows/clippy-fmt.yml']", "['covector', '.github/workflows/covector-status.yml']"], ["['test (aarch64-apple-ios, macos-latest)', '.github/workflows/build.yml']", "['test (aarch64-linux-android, ubuntu-latest)', '.github/workflows/build.yml']"]]}
{"repo": "seanmonstar/reqwest", "instance_id": "seanmonstar__reqwest-2464", "base_commit": "3f28983659ebff48bef8b2bd4af12ba9f9a15a55", "patch": "diff --git a/src/blocking/client.rs b/src/blocking/client.rs\nindex fe7574689..a8c5319b4 100644\n--- a/src/blocking/client.rs\n+++ b/src/blocking/client.rs\n@@ -625,7 +625,7 @@ impl ClientBuilder {\n     /// This requires the `rustls-tls(-...)` Cargo feature enabled.\n     #[cfg(feature = \"__rustls\")]\n     #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\n-    pub fn add_crl(mut self, crl: CertificateRevocationList) -> ClientBuilder {\n+    pub fn add_crl(self, crl: CertificateRevocationList) -> ClientBuilder {\n         self.with_inner(move |inner| inner.add_crl(crl))\n     }\n \n@@ -638,7 +638,7 @@ impl ClientBuilder {\n     #[cfg(feature = \"__rustls\")]\n     #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\n     pub fn add_crls(\n-        mut self,\n+        self,\n         crls: impl IntoIterator<Item = CertificateRevocationList>,\n     ) -> ClientBuilder {\n         self.with_inner(move |inner| inner.add_crls(crls))\n", "test_patch": "", "problem_statement": "Tests fail when run with `--all-features`\nOn current master (`797df2b96a88ee49a636337f87beebe87f6212fe`) with Rust 1.82 (`rustc 1.82.0 (f6e511eec 2024-10-15)`, `cargo 1.82.0 (8f40fc59f 2024-08-21)`), although a regular run of `cargo test` works fine, attempting to run `RUSTFLAGS='--cfg reqwest_unstable' cargo test --all-features --tests` fails with the following errors (which would have just warnings if not escalated by `deny` directives):\r\n\r\n<details><summary>Errors</summary>\r\n\r\n```\r\nwarning: variable does not need to be mutable\r\n   --> src/blocking/client.rs:619:20\r\n    |\r\n619 |     pub fn add_crl(mut self, crl: CertificateRevocationList) -> ClientBuilder {\r\n    |                    ----^^^^\r\n    |                    |\r\n    |                    help: remove this `mut`\r\n    |\r\n    = note: `#[warn(unused_mut)]` on by default\r\n\r\nwarning: variable does not need to be mutable\r\n   --> src/blocking/client.rs:632:9\r\n    |\r\n632 |         mut self,\r\n    |         ----^^^^\r\n    |         |\r\n    |         help: remove this `mut`\r\n\r\nerror: variable does not need to be mutable\r\n   --> src/blocking/client.rs:619:20\r\n    |\r\n619 |     pub fn add_crl(mut self, crl: CertificateRevocationList) -> ClientBuilder {\r\n    |                    ----^^^^\r\n    |                    |\r\n    |                    help: remove this `mut`\r\n    |\r\nnote: the lint level is defined here\r\n   --> src/lib.rs:4:24\r\n    |\r\n4   | #![cfg_attr(test, deny(warnings))]\r\n    |                        ^^^^^^^^\r\n    = note: `#[deny(unused_mut)]` implied by `#[deny(warnings)]`\r\n\r\nerror: variable does not need to be mutable\r\n   --> src/blocking/client.rs:632:9\r\n    |\r\n632 |         mut self,\r\n    |         ----^^^^\r\n    |         |\r\n    |         help: remove this `mut`\r\n\r\nerror: could not compile `reqwest` (lib test) due to 2 previous errors\r\n```\r\n\r\n</details>\r\n\r\nThose errors can be fixed with the following diff:\r\n\r\n<details><summary>Diff</summary>\r\n\r\n```diff\r\ndiff --git a/src/blocking/client.rs b/src/blocking/client.rs\r\nindex 7b5caff..4db9e42 100644\r\n--- a/src/blocking/client.rs\r\n+++ b/src/blocking/client.rs\r\n@@ -616,7 +616,7 @@ impl ClientBuilder {\r\n     /// This requires the `rustls-tls(-...)` Cargo feature enabled.\r\n     #[cfg(feature = \"__rustls\")]\r\n     #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\r\n-    pub fn add_crl(mut self, crl: CertificateRevocationList) -> ClientBuilder {\r\n+    pub fn add_crl(self, crl: CertificateRevocationList) -> ClientBuilder {\r\n         self.with_inner(move |inner| inner.add_crl(crl))\r\n     }\r\n \r\n@@ -629,7 +629,7 @@ impl ClientBuilder {\r\n     #[cfg(feature = \"__rustls\")]\r\n     #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\r\n     pub fn add_crls(\r\n-        mut self,\r\n+        self,\r\n         crls: impl IntoIterator<Item = CertificateRevocationList>,\r\n     ) -> ClientBuilder {\r\n         self.with_inner(move |inner| inner.add_crls(crls))\r\n```\r\n\r\n</details>\r\n\r\nHowever, this still results in a failure of the `async_impl_file_part` test:\r\n\r\n```\r\n---- async_impl_file_part stdout ----\r\nthread 'test(async_impl_file_part)-support-server' panicked at tests/multipart.rs:211:37:\r\nno entry found for key \"transfer-encoding\"\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\nthread 'async_impl_file_part' panicked at tests/multipart.rs:228:10:\r\ncalled `Result::unwrap()` on an `Err` value: reqwest::Error { kind: Request, url: \"http://127.0.0.1:43241/multipart/3\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(Io, Os { code: 104, kind: ConnectionReset, message: \"Connection reset by peer\" })) }\r\n```\r\n\r\nThat test was added in [`cc3dd510c0cb50ee03be217ea864d84e744658d1`](https://github.com/gretchenfrage/reqwest/commit/cc3dd510c0cb50ee03be217ea864d84e744658d1) (\"Add file function to async::multipart (#2106)\", Aug/31/2024), which is currently `master~14`. In that commit, running `RUSTFLAGS='--cfg reqwest_unstable' cargo test --all-features --tests` does not error.\r\n\r\nIt is at commit [`aba01ff7df33a3f29e3f7fdbd24ee90390276335`](https://github.com/seanmonstar/reqwest/commit/aba01ff7df33a3f29e3f7fdbd24ee90390276335) (\"feat: Add support for Certificate Revocation Lists (#2433)\", Oct/18/2024), which is currently `master~4`, that the _compilation_ error appears when running the tests. However, as of that commit, if the compilation error is patched, the tests do not error.\r\n\r\nIt it as commit [`598f8574cb428bb03b487e0abfd8169a594dd2db`](https://github.com/seanmonstar/reqwest/commit/598f8574cb428bb03b487e0abfd8169a594dd2db) (\"Add content length to async_impl::multipart file streams (#2459)\", Oct/27/2024), which is currently `master~2`, that the test failure appears even with the patched test.\r\n\r\n\n", "hints_text": "", "created_at": "2024-10-29 05:42:42", "merge_commit_sha": "8ed106529776805c7c2e6c626e3f248ff758de19", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['windows / stable-i686-msvc', '.github/workflows/ci.yml']", "['feat.: zstd', '.github/workflows/ci.yml']"], ["['Check Style', '.github/workflows/ci.yml']", "['feat.: rustls-tls-native-roots', '.github/workflows/ci.yml']"], ["['windows / stable-x86_64-gnu', '.github/workflows/ci.yml']", "['feat.: socks/rustls-tls', '.github/workflows/ci.yml']"], ["['feat.: blocking', '.github/workflows/ci.yml']", "['feat.: hickory-dns', '.github/workflows/ci.yml']"], ["['feat.: deflate', '.github/workflows/ci.yml']", "['feat.: multipart', '.github/workflows/ci.yml']"], ["['windows / stable-i686-gnu', '.github/workflows/ci.yml']", "['windows / stable-x86_64-msvc', '.github/workflows/ci.yml']"], ["['feat.: rustls-tls-no-provider', '.github/workflows/ci.yml']", "['feat.: brotli', '.github/workflows/ci.yml']"], ["['linux / nightly', '.github/workflows/ci.yml']", "['linux / beta', '.github/workflows/ci.yml']"], ["['WASM', '.github/workflows/ci.yml']", "['feat.: default-tls and rustls-tls', '.github/workflows/ci.yml']"]]}
{"repo": "paritytech/polkadot-sdk", "instance_id": "paritytech__polkadot-sdk-7377", "base_commit": "8834a9bf7dcb1e5a7498b7148787c544136609b2", "patch": "diff --git a/prdoc/pr_7377.prdoc b/prdoc/pr_7377.prdoc\nnew file mode 100644\nindex 0000000000000..1dfa88099a16e\n--- /dev/null\n+++ b/prdoc/pr_7377.prdoc\n@@ -0,0 +1,20 @@\n+# Schema: Polkadot SDK PRDoc Schema (prdoc) v1.0.0\n+# See doc at https://raw.githubusercontent.com/paritytech/polkadot-sdk/master/prdoc/schema_user.json\n+\n+title: Add missing events to nomination pool extrinsics\n+\n+doc:\n+  - audience: [Runtime Dev, Runtime User]\n+    description: |\n+      Introduces events to extrinsics from `pallet_nomination_pools` that previously had none:\n+        - `set_metadata`\n+        - `nominate`\n+        - `chill`\n+        - `set_configs`\n+        - `set_claim_permission`\n+\n+crates:\n+- name: pallet-nomination-pools\n+  bump: major\n+- name: pallet-staking\n+  bump: none\n\\ No newline at end of file\ndiff --git a/substrate/frame/nomination-pools/src/lib.rs b/substrate/frame/nomination-pools/src/lib.rs\nindex dc82bf3a37c6e..04736e6c1aad1 100644\n--- a/substrate/frame/nomination-pools/src/lib.rs\n+++ b/substrate/frame/nomination-pools/src/lib.rs\n@@ -18,7 +18,7 @@\n //! # Nomination Pools for Staking Delegation\n //!\n //! A pallet that allows members to delegate their stake to nominating pools. A nomination pool acts\n-//! as nominator and nominates validators on the members behalf.\n+//! as nominator and nominates validators on the members' behalf.\n //!\n //! # Index\n //!\n@@ -178,7 +178,7 @@\n //!\n //! ### Pool Members\n //!\n-//! * In general, whenever a pool member changes their total point, the chain will automatically\n+//! * In general, whenever a pool member changes their total points, the chain will automatically\n //!   claim all their pending rewards for them. This is not optional, and MUST happen for the reward\n //!   calculation to remain correct (see the documentation of `bond` as an example). So, make sure\n //!   you are warning your users about it. They might be surprised if they see that they bonded an\n@@ -1865,6 +1865,24 @@ pub mod pallet {\n \t\tMinBalanceDeficitAdjusted { pool_id: PoolId, amount: BalanceOf<T> },\n \t\t/// Claimed excess frozen ED of af the reward pool.\n \t\tMinBalanceExcessAdjusted { pool_id: PoolId, amount: BalanceOf<T> },\n+\t\t/// A pool member's claim permission has been updated.\n+\t\tMemberClaimPermissionUpdated { member: T::AccountId, permission: ClaimPermission },\n+\t\t/// A pool's metadata was updated.\n+\t\tMetadataUpdated { pool_id: PoolId, caller: T::AccountId },\n+\t\t/// A pool's nominating account (or the pool's root account) has nominated a validator set\n+\t\t/// on behalf of the pool.\n+\t\tPoolNominationMade { pool_id: PoolId, caller: T::AccountId },\n+\t\t/// The pool is chilled i.e. no longer nominating.\n+\t\tPoolNominatorChilled { pool_id: PoolId, caller: T::AccountId },\n+\t\t/// Global parameters regulating nomination pools have been updated.\n+\t\tGlobalParamsUpdated {\n+\t\t\tmin_join_bond: BalanceOf<T>,\n+\t\t\tmin_create_bond: BalanceOf<T>,\n+\t\t\tmax_pools: Option<u32>,\n+\t\t\tmax_members: Option<u32>,\n+\t\t\tmax_members_per_pool: Option<u32>,\n+\t\t\tglobal_max_commission: Option<Perbill>,\n+\t\t},\n \t}\n \n \t#[pallet::error]\n@@ -2509,13 +2527,13 @@ pub mod pallet {\n \t\t/// The dispatch origin of this call must be signed by the pool nominator or the pool\n \t\t/// root role.\n \t\t///\n-\t\t/// This directly forward the call to the staking pallet, on behalf of the pool bonded\n-\t\t/// account.\n+\t\t/// This directly forwards the call to an implementation of `StakingInterface` (e.g.,\n+\t\t/// `pallet-staking`) through [`Config::StakeAdapter`], on behalf of the bonded pool.\n \t\t///\n \t\t/// # Note\n \t\t///\n-\t\t/// In addition to a `root` or `nominator` role of `origin`, pool's depositor needs to have\n-\t\t/// at least `depositor_min_bond` in the pool to start nominating.\n+\t\t/// In addition to a `root` or `nominator` role of `origin`, the pool's depositor needs to\n+\t\t/// have at least `depositor_min_bond` in the pool to start nominating.\n \t\t#[pallet::call_index(8)]\n \t\t#[pallet::weight(T::WeightInfo::nominate(validators.len() as u32))]\n \t\tpub fn nominate(\n@@ -2538,7 +2556,9 @@ pub mod pallet {\n \t\t\t\tError::<T>::MinimumBondNotMet\n \t\t\t);\n \n-\t\t\tT::StakeAdapter::nominate(Pool::from(bonded_pool.bonded_account()), validators)\n+\t\t\tT::StakeAdapter::nominate(Pool::from(bonded_pool.bonded_account()), validators).map(\n+\t\t\t\t|_| Self::deposit_event(Event::<T>::PoolNominationMade { pool_id, caller: who }),\n+\t\t\t)\n \t\t}\n \n \t\t/// Set a new state for the pool.\n@@ -2603,6 +2623,8 @@ pub mod pallet {\n \n \t\t\tMetadata::<T>::mutate(pool_id, |pool_meta| *pool_meta = metadata);\n \n+\t\t\tSelf::deposit_event(Event::<T>::MetadataUpdated { pool_id, caller: who });\n+\n \t\t\tOk(())\n \t\t}\n \n@@ -2646,6 +2668,16 @@ pub mod pallet {\n \t\t\tconfig_op_exp!(MaxPoolMembers::<T>, max_members);\n \t\t\tconfig_op_exp!(MaxPoolMembersPerPool::<T>, max_members_per_pool);\n \t\t\tconfig_op_exp!(GlobalMaxCommission::<T>, global_max_commission);\n+\n+\t\t\tSelf::deposit_event(Event::<T>::GlobalParamsUpdated {\n+\t\t\t\tmin_join_bond: MinJoinBond::<T>::get(),\n+\t\t\t\tmin_create_bond: MinCreateBond::<T>::get(),\n+\t\t\t\tmax_pools: MaxPools::<T>::get(),\n+\t\t\t\tmax_members: MaxPoolMembers::<T>::get(),\n+\t\t\t\tmax_members_per_pool: MaxPoolMembersPerPool::<T>::get(),\n+\t\t\t\tglobal_max_commission: GlobalMaxCommission::<T>::get(),\n+\t\t\t});\n+\n \t\t\tOk(())\n \t\t}\n \n@@ -2710,17 +2742,18 @@ pub mod pallet {\n \t\t/// The dispatch origin of this call can be signed by the pool nominator or the pool\n \t\t/// root role, same as [`Pallet::nominate`].\n \t\t///\n+\t\t/// This directly forwards the call to an implementation of `StakingInterface` (e.g.,\n+\t\t/// `pallet-staking`) through [`Config::StakeAdapter`], on behalf of the bonded pool.\n+\t\t///\n \t\t/// Under certain conditions, this call can be dispatched permissionlessly (i.e. by any\n \t\t/// account).\n \t\t///\n \t\t/// # Conditions for a permissionless dispatch:\n-\t\t/// * When pool depositor has less than `MinNominatorBond` staked, otherwise  pool members\n+\t\t/// * When pool depositor has less than `MinNominatorBond` staked, otherwise pool members\n \t\t///   are unable to unbond.\n \t\t///\n \t\t/// # Conditions for permissioned dispatch:\n-\t\t/// * The caller has a nominator or root role of the pool.\n-\t\t/// This directly forward the call to the staking pallet, on behalf of the pool bonded\n-\t\t/// account.\n+\t\t/// * The caller is the pool's nominator or root.\n \t\t#[pallet::call_index(13)]\n \t\t#[pallet::weight(T::WeightInfo::chill())]\n \t\tpub fn chill(origin: OriginFor<T>, pool_id: PoolId) -> DispatchResult {\n@@ -2739,7 +2772,9 @@ pub mod pallet {\n \t\t\t\tensure!(bonded_pool.can_nominate(&who), Error::<T>::NotNominator);\n \t\t\t}\n \n-\t\t\tT::StakeAdapter::chill(Pool::from(bonded_pool.bonded_account()))\n+\t\t\tT::StakeAdapter::chill(Pool::from(bonded_pool.bonded_account())).map(|_| {\n+\t\t\t\tSelf::deposit_event(Event::<T>::PoolNominatorChilled { pool_id, caller: who })\n+\t\t\t})\n \t\t}\n \n \t\t/// `origin` bonds funds from `extra` for some pool member `member` into their respective\n@@ -2794,10 +2829,15 @@ pub mod pallet {\n \t\t\t\tError::<T>::NotMigrated\n \t\t\t);\n \n-\t\t\tClaimPermissions::<T>::mutate(who, |source| {\n+\t\t\tClaimPermissions::<T>::mutate(who.clone(), |source| {\n \t\t\t\t*source = permission;\n \t\t\t});\n \n+\t\t\tSelf::deposit_event(Event::<T>::MemberClaimPermissionUpdated {\n+\t\t\t\tmember: who,\n+\t\t\t\tpermission,\n+\t\t\t});\n+\n \t\t\tOk(())\n \t\t}\n \n@@ -2913,9 +2953,20 @@ pub mod pallet {\n \n \t\t/// Claim pending commission.\n \t\t///\n-\t\t/// The dispatch origin of this call must be signed by the `root` role of the pool. Pending\n-\t\t/// commission is paid out and added to total claimed commission`. Total pending commission\n-\t\t/// is reset to zero. the current.\n+\t\t/// The `root` role of the pool is _always_ allowed to claim the pool's commission.\n+\t\t///\n+\t\t/// If the pool has set `CommissionClaimPermission::Permissionless`, then any account can\n+\t\t/// trigger the process of claiming the pool's commission.\n+\t\t///\n+\t\t/// If the pool has set its `CommissionClaimPermission` to `Account(acc)`, then only\n+\t\t/// accounts\n+\t\t/// * `acc`, and\n+\t\t/// * the pool's root account\n+\t\t///\n+\t\t/// may call this extrinsic on behalf of the pool.\n+\t\t///\n+\t\t/// Pending commissions are paid out and added to the total claimed commission.\n+\t\t/// The total pending commission is reset to zero.\n \t\t#[pallet::call_index(20)]\n \t\t#[pallet::weight(T::WeightInfo::claim_commission())]\n \t\tpub fn claim_commission(origin: OriginFor<T>, pool_id: PoolId) -> DispatchResult {\ndiff --git a/substrate/frame/staking/src/pallet/mod.rs b/substrate/frame/staking/src/pallet/mod.rs\nindex 7d5da9ea0c497..9d8914627397f 100644\n--- a/substrate/frame/staking/src/pallet/mod.rs\n+++ b/substrate/frame/staking/src/pallet/mod.rs\n@@ -1552,7 +1552,7 @@ pub mod pallet {\n \n \t\t\tlet _ = ledger\n \t\t\t\t.set_payee(payee)\n-\t\t\t\t.defensive_proof(\"ledger was retrieved from storage, thus its bonded; qed.\")?;\n+\t\t\t\t.defensive_proof(\"ledger was retrieved from storage, thus it's bonded; qed.\")?;\n \n \t\t\tOk(())\n \t\t}\n", "test_patch": "diff --git a/substrate/frame/nomination-pools/src/tests.rs b/substrate/frame/nomination-pools/src/tests.rs\nindex c46638d2f8f7b..e2922e22fa989 100644\n--- a/substrate/frame/nomination-pools/src/tests.rs\n+++ b/substrate/frame/nomination-pools/src/tests.rs\n@@ -17,7 +17,7 @@\n \n use super::*;\n use crate::{mock::*, Event};\n-use frame_support::{assert_err, assert_noop, assert_ok, assert_storage_noop};\n+use frame_support::{assert_err, assert_noop, assert_ok};\n use pallet_balances::Event as BEvent;\n use sp_runtime::{\n \tbounded_btree_map,\n@@ -661,6 +661,7 @@ mod join {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 11, pool_id: 1, bonded: 2, joined: true },\n \t\t\t\t]\n \t\t\t);\n@@ -817,6 +818,7 @@ mod join {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 101, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 102, pool_id: 1, bonded: 100, joined: true }\n \t\t\t\t]\n@@ -1089,6 +1091,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 11, pool_id: 1, bonded: 11, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 11, pool_id: 1, points: 11, balance: 11, era: 3 }\n \t\t\t\t]\n@@ -1121,6 +1124,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(75), 2))\n@@ -1184,6 +1188,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 5 }\n \t\t\t\t]\n \t\t\t);\n@@ -1259,6 +1264,7 @@ mod claim_payout {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 40, pool_id: 1, bonded: 40, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 50, pool_id: 1, bonded: 50, joined: true }\n \t\t\t\t\t]\n@@ -1514,6 +1520,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 20 },\n \t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 10 },\n@@ -1557,6 +1564,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 3 + 3 },\n \t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 3 },\n@@ -1620,6 +1628,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 30, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 30 + 100 / 2 + 60 / 3 },\n@@ -1721,6 +1730,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 30, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 10 },\n@@ -1770,6 +1780,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 10 },\n \t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 20 }\n@@ -1818,6 +1829,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 30, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 10 },\n@@ -1884,6 +1896,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 10 }\n \t\t\t\t]\n@@ -1983,6 +1996,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Created { depositor: 20, pool_id: 2 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 2, bonded: 10, joined: true },\n \t\t\t\t\tEvent::Created { depositor: 30, pool_id: 3 },\n@@ -2052,6 +2066,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 30, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 40, pool_id: 1, bonded: 10, joined: true }\n@@ -2132,6 +2147,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: false },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 15 },\n@@ -2277,6 +2293,7 @@ mod claim_payout {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 30, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, balance: 10, points: 10, era: 3 },\n@@ -2315,6 +2332,7 @@ mod claim_payout {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 13 },\n \t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 26 }\n@@ -2385,6 +2403,7 @@ mod claim_payout {\n \t\t\t\t\t\t\tbonded: 1000000000000000,\n \t\t\t\t\t\t\tjoined: true\n \t\t\t\t\t\t},\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded {\n \t\t\t\t\t\t\tmember: 20,\n \t\t\t\t\t\t\tpool_id: 1,\n@@ -2584,6 +2603,7 @@ mod unbond {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 6 },\n \t\t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, balance: 20, points: 20, era: 3 }\n@@ -2833,6 +2853,7 @@ mod unbond {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 40, pool_id: 1, bonded: 40, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 550, pool_id: 1, bonded: 550, joined: true },\n \t\t\t\t\t\tEvent::PoolSlashed { pool_id: 1, balance: 100 },\n@@ -2975,6 +2996,7 @@ mod unbond {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 10, balance: 10, era: 9 }\n \t\t\t\t]\n \t\t\t);\n@@ -3009,6 +3031,7 @@ mod unbond {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 100, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 200, pool_id: 1, bonded: 200, joined: true },\n \t\t\t\t\t\tEvent::Unbonded {\n@@ -3102,6 +3125,7 @@ mod unbond {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 100, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 100, pool_id: 1, points: 100, balance: 100, era: 3 }\n \t\t\t\t]\n@@ -3258,6 +3282,7 @@ mod unbond {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 1, balance: 1, era: 3 }\n \t\t\t\t]\n \t\t\t);\n@@ -3390,6 +3415,7 @@ mod unbond {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, points: 2, balance: 2, era: 3 },\n \t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, points: 3, balance: 3, era: 4 },\n@@ -3426,6 +3452,7 @@ mod unbond {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 3, balance: 3, era: 3 }\n \t\t\t\t]\n \t\t\t);\n@@ -3467,6 +3494,7 @@ mod unbond {\n \t\t\t\t\t// 2/3 of ed, which is 20's share.\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 20, pool_id: 1, payout: 10 },\n \t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, balance: 2, points: 2, era: 3 }\n@@ -3641,6 +3669,7 @@ mod withdraw_unbonded {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 40, pool_id: 1, bonded: 40, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 550, pool_id: 1, bonded: 550, joined: true },\n \t\t\t\t\t\tEvent::Unbonded {\n@@ -3750,6 +3779,7 @@ mod withdraw_unbonded {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 40, pool_id: 1, bonded: 40, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 550, pool_id: 1, bonded: 550, joined: true },\n \t\t\t\t\t\tEvent::PoolSlashed { pool_id: 1, balance: 300 },\n@@ -3827,6 +3857,7 @@ mod withdraw_unbonded {\n \t\t\t\t\tpool_events_since_last_call(),\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 5, balance: 5, era: 6 },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Withdrawn { member: 10, pool_id: 1, points: 5, balance: 5 },\n \t\t\t\t\t\tEvent::MemberRemoved { pool_id: 1, member: 10, released_balance: 0 },\n \t\t\t\t\t\tEvent::Destroyed { pool_id: 1 }\n@@ -3915,6 +3946,7 @@ mod withdraw_unbonded {\n \t\t\t\t\tvec![\n \t\t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t\tEvent::Bonded { member: 100, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\t\tEvent::Bonded { member: 200, pool_id: 1, bonded: 200, joined: true },\n \t\t\t\t\t\tEvent::Unbonded {\n@@ -4008,6 +4040,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 100, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 100, pool_id: 1, points: 100, balance: 100, era: 3 },\n \t\t\t\t\tEvent::Withdrawn { member: 100, pool_id: 1, points: 100, balance: 100 },\n@@ -4048,6 +4081,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: false },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 6, balance: 6, era: 3 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, points: 1, balance: 1, era: 4 }\n@@ -4135,6 +4169,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 11, pool_id: 1, bonded: 10, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 11, pool_id: 1, points: 6, balance: 6, era: 3 },\n \t\t\t\t\tEvent::Unbonded { member: 11, pool_id: 1, points: 1, balance: 1, era: 4 }\n@@ -4231,6 +4266,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 100, pool_id: 1, bonded: 100, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 100, pool_id: 1, points: 75, balance: 75, era: 3 },\n \t\t\t\t\tEvent::Unbonded { member: 100, pool_id: 1, points: 25, balance: 25, era: 4 },\n@@ -4469,6 +4505,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: false },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, balance: 7, points: 7, era: 3 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, balance: 3, points: 3, era: 4 },\n@@ -4515,6 +4552,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::Unbonded { member: 20, pool_id: 1, balance: 20, points: 20, era: 4 },\n \t\t\t\t]\n@@ -4555,6 +4593,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, balance: 10, points: 10, era: 4 },\n \t\t\t\t]\n \t\t\t);\n@@ -4597,6 +4636,7 @@ mod withdraw_unbonded {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Unbonded { member: 10, pool_id: 1, balance: 10, points: 10, era: 4 },\n \t\t\t\t]\n \t\t\t);\n@@ -4699,6 +4739,7 @@ mod create {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Created { depositor: 11, pool_id: 2 },\n \t\t\t\t\tEvent::Bonded { member: 11, pool_id: 2, bonded: 10, joined: true }\n \t\t\t\t]\n@@ -4829,6 +4870,7 @@ fn set_claim_permission_works() {\n \t\t\tvec![\n \t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\tEvent::Bonded { member: 11, pool_id: 1, bonded: 2, joined: true },\n \t\t\t]\n \t\t);\n@@ -4844,6 +4886,14 @@ fn set_claim_permission_works() {\n \t\t\tClaimPermission::Permissioned\n \t\t));\n \n+\t\tassert_eq!(\n+\t\t\tpool_events_since_last_call(),\n+\t\t\tvec![Event::MemberClaimPermissionUpdated {\n+\t\t\t\tmember: 11,\n+\t\t\t\tpermission: ClaimPermission::Permissioned\n+\t\t\t},]\n+\t\t);\n+\n \t\t// then\n \t\tassert_eq!(ClaimPermissions::<Runtime>::get(11), ClaimPermission::Permissioned);\n \t});\n@@ -4883,10 +4933,21 @@ mod nominate {\n \t\t\tassert_ok!(Pools::nominate(RuntimeOrigin::signed(900), 1, vec![21]));\n \t\t\tassert_eq!(Nominations::get().unwrap(), vec![21]);\n \n+\t\t\t// Check event\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominationMade {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 900,\n+\t\t\t}));\n+\n \t\t\t// Nominator can nominate\n \t\t\tassert_ok!(Pools::nominate(RuntimeOrigin::signed(901), 1, vec![31]));\n \t\t\tassert_eq!(Nominations::get().unwrap(), vec![31]);\n \n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominationMade {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 901,\n+\t\t\t}));\n+\n \t\t\t// Can't nominate for a pool that doesn't exist\n \t\t\tassert_noop!(\n \t\t\t\tPools::nominate(RuntimeOrigin::signed(902), 123, vec![21]),\n@@ -4923,6 +4984,7 @@ mod set_state {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::StateChanged { pool_id: 1, new_state: PoolState::Blocked }\n \t\t\t\t]\n \t\t\t);\n@@ -4999,10 +5061,20 @@ mod set_metadata {\n \t\t\tassert_ok!(Pools::set_metadata(RuntimeOrigin::signed(900), 1, vec![1, 1]));\n \t\t\tassert_eq!(Metadata::<Runtime>::get(1), vec![1, 1]);\n \n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::MetadataUpdated {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 900,\n+\t\t\t}));\n+\n \t\t\t// bouncer can set metadata\n \t\t\tassert_ok!(Pools::set_metadata(RuntimeOrigin::signed(902), 1, vec![2, 2]));\n \t\t\tassert_eq!(Metadata::<Runtime>::get(1), vec![2, 2]);\n \n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::MetadataUpdated {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 902,\n+\t\t\t}));\n+\n \t\t\t// Depositor can't set metadata\n \t\t\tassert_noop!(\n \t\t\t\tPools::set_metadata(RuntimeOrigin::signed(10), 1, vec![3, 3]),\n@@ -5061,8 +5133,18 @@ mod set_configs {\n \t\t\tassert_eq!(MaxPoolMembersPerPool::<Runtime>::get(), Some(5));\n \t\t\tassert_eq!(GlobalMaxCommission::<Runtime>::get(), Some(Perbill::from_percent(6)));\n \n+\t\t\t// Check events\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::GlobalParamsUpdated {\n+\t\t\t\tmin_join_bond: 1,\n+\t\t\t\tmin_create_bond: 2,\n+\t\t\t\tmax_pools: Some(3),\n+\t\t\t\tmax_members: Some(4),\n+\t\t\t\tmax_members_per_pool: Some(5),\n+\t\t\t\tglobal_max_commission: Some(Perbill::from_percent(6)),\n+\t\t\t}));\n+\n \t\t\t// Noop does nothing\n-\t\t\tassert_storage_noop!(assert_ok!(Pools::set_configs(\n+\t\t\tassert_ok!(Pools::set_configs(\n \t\t\t\tRuntimeOrigin::signed(42),\n \t\t\t\tConfigOp::Noop,\n \t\t\t\tConfigOp::Noop,\n@@ -5070,7 +5152,23 @@ mod set_configs {\n \t\t\t\tConfigOp::Noop,\n \t\t\t\tConfigOp::Noop,\n \t\t\t\tConfigOp::Noop,\n-\t\t\t)));\n+\t\t\t));\n+\n+\t\t\tassert_eq!(MinJoinBond::<Runtime>::get(), 1);\n+\t\t\tassert_eq!(MinCreateBond::<Runtime>::get(), 2);\n+\t\t\tassert_eq!(MaxPools::<Runtime>::get(), Some(3));\n+\t\t\tassert_eq!(MaxPoolMembers::<Runtime>::get(), Some(4));\n+\t\t\tassert_eq!(MaxPoolMembersPerPool::<Runtime>::get(), Some(5));\n+\t\t\tassert_eq!(GlobalMaxCommission::<Runtime>::get(), Some(Perbill::from_percent(6)));\n+\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::GlobalParamsUpdated {\n+\t\t\t\tmin_join_bond: 1,\n+\t\t\t\tmin_create_bond: 2,\n+\t\t\t\tmax_pools: Some(3),\n+\t\t\t\tmax_members: Some(4),\n+\t\t\t\tmax_members_per_pool: Some(5),\n+\t\t\t\tglobal_max_commission: Some(Perbill::from_percent(6)),\n+\t\t\t}));\n \n \t\t\t// Removing works\n \t\t\tassert_ok!(Pools::set_configs(\n@@ -5088,6 +5186,15 @@ mod set_configs {\n \t\t\tassert_eq!(MaxPoolMembers::<Runtime>::get(), None);\n \t\t\tassert_eq!(MaxPoolMembersPerPool::<Runtime>::get(), None);\n \t\t\tassert_eq!(GlobalMaxCommission::<Runtime>::get(), None);\n+\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::GlobalParamsUpdated {\n+\t\t\t\tmin_join_bond: 0,\n+\t\t\t\tmin_create_bond: 0,\n+\t\t\t\tmax_pools: None,\n+\t\t\t\tmax_members: None,\n+\t\t\t\tmax_members_per_pool: None,\n+\t\t\t\tglobal_max_commission: None,\n+\t\t\t}));\n \t\t});\n \t}\n }\n@@ -5120,6 +5227,7 @@ mod bond_extra {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: false }\n \t\t\t\t]\n \t\t\t);\n@@ -5168,6 +5276,7 @@ mod bond_extra {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: claimable_reward },\n \t\t\t\t\tEvent::Bonded {\n \t\t\t\t\t\tmember: 10,\n@@ -5229,6 +5338,7 @@ mod bond_extra {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 20, pool_id: 1, bonded: 20, joined: true },\n \t\t\t\t\tEvent::PaidOut { member: 10, pool_id: 1, payout: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 1, joined: false },\n@@ -5372,6 +5482,7 @@ mod update_roles {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::RolesUpdated { root: Some(5), bouncer: Some(7), nominator: Some(6) }\n \t\t\t\t]\n \t\t\t);\n@@ -5485,7 +5596,8 @@ mod reward_counter_precision {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tbonded: 1173908528796953165005,\n \t\t\t\t\t\tjoined: true,\n-\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -5518,7 +5630,8 @@ mod reward_counter_precision {\n \t\t\t\tpool_events_since_last_call(),\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n-\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10000000000000, joined: true }\n+\t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10000000000000, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -5557,7 +5670,8 @@ mod reward_counter_precision {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tbonded: 12_968_712_300_500_000_000,\n \t\t\t\t\t\tjoined: true,\n-\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -5623,7 +5737,8 @@ mod reward_counter_precision {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tbonded: 12_968_712_300_500_000_000,\n \t\t\t\t\t\tjoined: true,\n-\t\t\t\t\t}\n+\t\t\t\t\t},\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -5658,7 +5773,8 @@ mod reward_counter_precision {\n \t\t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\t\tbonded: 2500000000000000000,\n \t\t\t\t\t\t\tjoined: true,\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t]\n \t\t\t\t);\n \n@@ -5726,7 +5842,8 @@ mod reward_counter_precision {\n \t\t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\t\tbonded: 2500000000000000000,\n \t\t\t\t\t\t\tjoined: true,\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\t},\n+\t\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\t]\n \t\t\t\t);\n \n@@ -5794,6 +5911,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(50), root))\n@@ -6087,6 +6205,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(10), 900))\n@@ -6109,6 +6228,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -6396,6 +6516,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolMaxCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tmax_commission: Perbill::from_percent(80)\n@@ -6463,6 +6584,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionChangeRateUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tchange_rate: CommissionChangeRate {\n@@ -6623,6 +6745,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(10), 900))\n@@ -6759,6 +6882,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionChangeRateUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tchange_rate: CommissionChangeRate {\n@@ -6792,6 +6916,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(33), 2))\n@@ -6882,6 +7007,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(10), 2))\n@@ -6948,6 +7074,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(10), 2))\n@@ -7007,6 +7134,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id: 1,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(100), 2))\n@@ -7052,6 +7180,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -7112,6 +7241,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::PoolCommissionUpdated {\n \t\t\t\t\t\tpool_id,\n \t\t\t\t\t\tcurrent: Some((Perbill::from_percent(50), 900))\n@@ -7294,6 +7424,7 @@ mod commission {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t]\n \t\t\t);\n \n@@ -7349,6 +7480,7 @@ mod slash {\n \t\t\t\tvec![\n \t\t\t\t\tEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\t\tEvent::Bonded { member: 10, pool_id: 1, bonded: 10, joined: true },\n+\t\t\t\t\tEvent::MetadataUpdated { pool_id: 1, caller: 900 },\n \t\t\t\t\tEvent::Bonded { member: 11, pool_id: 1, bonded: 2, joined: true },\n \t\t\t\t]\n \t\t\t);\n@@ -7404,11 +7536,28 @@ mod chill {\n \n \t\t\t// root can chill and re-nominate\n \t\t\tassert_ok!(Pools::chill(RuntimeOrigin::signed(900), 1));\n+\t\t\t// Check that chill now emits an event\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominatorChilled {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 900,\n+\t\t\t}));\n \t\t\tassert_ok!(Pools::nominate(RuntimeOrigin::signed(900), 1, vec![31]));\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominationMade {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 900,\n+\t\t\t}));\n \n \t\t\t// nominator can chill and re-nominate\n \t\t\tassert_ok!(Pools::chill(RuntimeOrigin::signed(901), 1));\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominatorChilled {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 901,\n+\t\t\t}));\n \t\t\tassert_ok!(Pools::nominate(RuntimeOrigin::signed(901), 1, vec![31]));\n+\t\t\tSystem::assert_last_event(tests::RuntimeEvent::Pools(Event::PoolNominationMade {\n+\t\t\t\tpool_id: 1,\n+\t\t\t\tcaller: 901,\n+\t\t\t}));\n \n \t\t\t// if `depositor` stake is less than the `MinimumNominatorBond`, then this call\n \t\t\t// becomes permissionless;\ndiff --git a/substrate/frame/nomination-pools/test-delegate-stake/src/lib.rs b/substrate/frame/nomination-pools/test-delegate-stake/src/lib.rs\nindex 54783332aa3ef..b43a41cd0f980 100644\n--- a/substrate/frame/nomination-pools/test-delegate-stake/src/lib.rs\n+++ b/substrate/frame/nomination-pools/test-delegate-stake/src/lib.rs\n@@ -62,6 +62,7 @@ fn pool_lifecycle_e2e() {\n \t\t\tvec![\n \t\t\t\tPoolsEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\tPoolsEvent::Bonded { member: 10, pool_id: 1, bonded: 50, joined: true },\n+\t\t\t\tPoolsEvent::PoolNominationMade { pool_id: 1, caller: 10 },\n \t\t\t]\n \t\t);\n \n@@ -180,7 +181,10 @@ fn pool_lifecycle_e2e() {\n \t\t);\n \t\tassert_eq!(\n \t\t\tpool_events_since_last_call(),\n-\t\t\tvec![PoolsEvent::Unbonded { member: 10, pool_id: 1, points: 50, balance: 50, era: 6 }]\n+\t\t\tvec![\n+\t\t\t\tPoolsEvent::PoolNominatorChilled { pool_id: 1, caller: 10 },\n+\t\t\t\tPoolsEvent::Unbonded { member: 10, pool_id: 1, points: 50, balance: 50, era: 6 }\n+\t\t\t]\n \t\t);\n \n \t\t// waiting another bonding duration:\n@@ -225,6 +229,7 @@ fn pool_chill_e2e() {\n \t\t\tvec![\n \t\t\t\tPoolsEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\tPoolsEvent::Bonded { member: 10, pool_id: 1, bonded: 50, joined: true },\n+\t\t\t\tPoolsEvent::PoolNominationMade { pool_id: 1, caller: 10 },\n \t\t\t]\n \t\t);\n \n@@ -968,6 +973,7 @@ fn pool_migration_e2e() {\n \t\t\tvec![\n \t\t\t\tPoolsEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\tPoolsEvent::Bonded { member: 10, pool_id: 1, bonded: 50, joined: true },\n+\t\t\t\tPoolsEvent::PoolNominationMade { pool_id: 1, caller: 10 }\n \t\t\t]\n \t\t);\n \n@@ -1252,6 +1258,7 @@ fn disable_pool_operations_on_non_migrated() {\n \t\t\tvec![\n \t\t\t\tPoolsEvent::Created { depositor: 10, pool_id: 1 },\n \t\t\t\tPoolsEvent::Bonded { member: 10, pool_id: 1, bonded: 50, joined: true },\n+\t\t\t\tPoolsEvent::PoolNominationMade { pool_id: 1, caller: 10 }\n \t\t\t]\n \t\t);\n \n", "problem_statement": "Extrinsics in nomination pool pallet are missing events\nFrom `pallet_nomination_pool`, the extrinsics\n\n* `set_configs`\n* `chill`\n* `set_claim_permission`\n* `set_metadata`\n\ndo not currently emit events.\n", "hints_text": "", "created_at": "2025-01-29 01:01:52", "merge_commit_sha": "a883475944b26c24704df7f0ff329121e396a6bb", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['preflight', '.github/workflows/build-publish-images.yml']", "['preflight', '.github/workflows/build-misc.yml']"], ["['cargo-check-each-crate (1)', '.github/workflows/tests-misc.yml']", "['check-markdown', '.github/workflows/checks-quick.yml']"], ["['build-linux-substrate', '.github/workflows/build-publish-images.yml']", "['build-malus', '.github/workflows/build-publish-images.yml']"], ["['prepare-bridges-zombienet-artifacts', '.github/workflows/build-publish-images.yml']", "['build-push-image-substrate-pr', '.github/workflows/build-publish-images.yml']"], ["['check-semver', '.github/workflows/check-semver.yml']", "['check-umbrella', '.github/workflows/checks-quick.yml']"], ["['build-push-image-test-parachain', '.github/workflows/build-publish-images.yml']", "['ci-versions', '.github/workflows/checks.yml']"], ["['run-frame-omni-bencher (dev, kitchensink-runtime, substrate/frame, substrate/HEADER-APACHE2, subs...', '.github/workflows/check-frame-omni-bencher.yml']", "['build-runtimes-polkavm', '.github/workflows/build-misc.yml']"], ["['cargo-check-all-crate-macos', '.github/workflows/tests-misc.yml']", "['cargo-clippy', '.github/workflows/checks.yml']"], ["['test-doc', '.github/workflows/docs.yml']", "['test-node-metrics', '.github/workflows/tests-misc.yml']"], ["['Backport pull request', '.github/workflows/command-backport.yml']", "['build (polkadot-availability-distribution, availability-distribution-regression-bench)', '.github/workflows/benchmarks-subsystem.yml']"], ["['ci-versions', '.github/workflows/benchmarks-subsystem.yml']", "['Extract runtimes from matrix', '.github/workflows/check-frame-omni-bencher.yml']"], ["['ci-versions', '.github/workflows/tests.yml']", "['set-variables', '.github/workflows/build-publish-eth-rpc.yml']"], ["['check-runtime-migration (bridge-hub-westend)', '.github/workflows/check-runtime-migration.yml']", "['All docs jobs passed', '.github/workflows/docs.yml']"], ["['test-linux-stable-runtime-benchmarks', '.github/workflows/tests-linux-stable.yml']", "['link-checker', '.github/workflows/check-links.yml']"], ["['preflight', '.github/workflows/checks-quick.yml']", "['preflight', '.github/workflows/check-semver.yml']"], ["['run-frame-omni-bencher (collectives-westend, collectives-westend-runtime, cumulus/parachains/runt...', '.github/workflows/check-frame-omni-bencher.yml']", "['build-push-image-malus', '.github/workflows/build-publish-images.yml']"], ["['check-runtime-migration (coretime-westend)', '.github/workflows/check-runtime-migration.yml']", "['publish-rustdoc', '.github/workflows/docs.yml']"], ["['cargo-check-each-crate (6)', '.github/workflows/tests-misc.yml']", "['check-metadata-hash', '.github/workflows/tests-misc.yml']"], ["['All runtime migrations passed', '.github/workflows/check-runtime-migration.yml']", "['test-linux-stable-coverage', '.github/workflows/tests-linux-stable-coverage.yml']"], ["['build-rustdoc', '.github/workflows/docs.yml']", "['build-test-parachain', '.github/workflows/build-publish-images.yml']"], ["['preflight', '.github/workflows/tests-misc.yml']", "['test-linux-stable-int', '.github/workflows/tests-linux-stable.yml']"], ["['test-linux-stable (1/3, parity-oldlinux)', '.github/workflows/tests-linux-stable.yml']", "['build-push-image-polkadot-parachain-debug', '.github/workflows/build-publish-images.yml']"], ["['test-rust-features', '.github/workflows/checks-quick.yml']", "['build-linux-stable', '.github/workflows/build-publish-images.yml']"], ["['test-linux-stable-no-try-runtime (2/2)', '.github/workflows/tests-linux-stable.yml']", "['build (polkadot-availability-recovery, availability-recovery-regression-bench)', '.github/workflows/benchmarks-subsystem.yml']"], ["['publish-benchmarks', '.github/workflows/benchmarks-subsystem.yml']", "['build-push-image-colander', '.github/workflows/build-publish-images.yml']"], ["['check-publish', '.github/workflows/publish-check-crates.yml']", "['check-workspace', '.github/workflows/checks-quick.yml']"], ["['cargo-check-benches (master)', '.github/workflows/tests-misc.yml']", "['check-publish-compile', '.github/workflows/publish-check-compile.yml']"], ["['test-linux-stable (1/3, parity-large)', '.github/workflows/tests-linux-stable.yml']", "['build-push-image-polkadot-debug', '.github/workflows/build-publish-images.yml']"], ["['ci-versions', '.github/workflows/publish-check-compile.yml']", "['cargo-check-all-benches', '.github/workflows/tests.yml']"], ["['test-deterministic-wasm', '.github/workflows/tests-misc.yml']", "['All benchmarks passed', '.github/workflows/check-frame-omni-bencher.yml']"], ["['node-bench-regression-guard', '.github/workflows/tests-misc.yml']", "['run-frame-omni-bencher (asset-hub-westend, asset-hub-westend-runtime, cumulus/parachains/runtimes...', '.github/workflows/check-frame-omni-bencher.yml']"], ["['test-linux-stable (2/3, parity-large)', '.github/workflows/tests-linux-stable.yml']", "['preflight', '.github/workflows/checks.yml']"], ["['cargo-check-each-crate (7)', '.github/workflows/tests-misc.yml']", "['check-tracing', '.github/workflows/tests-misc.yml']"], ["['check-product-references', '.github/workflows/check-licenses.yml']", "['All build misc jobs passed', '.github/workflows/build-misc.yml']"], ["['check-core-crypto-features', '.github/workflows/checks.yml']", "['check-fail-ci', '.github/workflows/checks-quick.yml']"], ["['upload-reports', '.github/workflows/tests-linux-stable-coverage.yml']", "['check-readme', '.github/workflows/checks-quick.yml']"]]}
{"repo": "CycloneDX/cyclonedx-rust-cargo", "instance_id": "CycloneDX__cyclonedx-rust-cargo-761", "base_commit": "76d0b094277d0135a8004de7a80415e814f1c244", "patch": "diff --git a/cyclonedx-bom/src/xml.rs b/cyclonedx-bom/src/xml.rs\nindex 20541279..2cd8f193 100644\n--- a/cyclonedx-bom/src/xml.rs\n+++ b/cyclonedx-bom/src/xml.rs\n@@ -442,6 +442,17 @@ impl FromXmlType for f32 {\n     }\n }\n \n+/// Reads a simple String tag.\n+///\n+/// ```xml\n+/// <description>Content</description>\n+/// ```\n+/// &\n+/// ```xml\n+/// <description />\n+/// ```\n+///\n+/// are valid XML tags. The first returns the string \"Content\", the latter is an empty string.\n pub(crate) fn read_simple_tag<R: Read>(\n     event_reader: &mut EventReader<R>,\n     element: &OwnedName,\n@@ -449,13 +460,19 @@ pub(crate) fn read_simple_tag<R: Read>(\n     let element_display = element.to_string();\n     let content = event_reader\n         .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(inner_text_or_error(&element_display))?;\n-\n-    event_reader\n-        .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(closing_tag_or_error(element))?;\n+        .map_err(to_xml_read_error(&element_display))?;\n+\n+    let content = match content {\n+        reader::XmlEvent::EndElement { .. } => String::new(),\n+        reader::XmlEvent::Characters(content) | reader::XmlEvent::CData(content) => {\n+            event_reader\n+                .next()\n+                .map_err(to_xml_read_error(&element_display))\n+                .and_then(closing_tag_or_error(element))?;\n+            content\n+        }\n+        unexpected => return Err(unexpected_element_error(element, unexpected)),\n+    };\n \n     Ok(content)\n }\n", "test_patch": "diff --git a/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml b/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\nnew file mode 100644\nindex 00000000..2ff484b9\n--- /dev/null\n+++ b/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\n@@ -0,0 +1,48 @@\n+<?xml version=\"1.0\"?>\n+<bom serialNumber=\"urn:uuid:3e671687-395b-41f5-a30f-a58921a69b79\" version=\"1\" xmlns=\"http://cyclonedx.org/schema/bom/1.3\">\n+    <components>\n+        <component type=\"library\" bom-ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+            <group>com.acme</group>\n+            <name>stock-java-client</name>\n+            <version>1.0.12</version>\n+            <hashes>\n+                <hash alg=\"SHA-1\">e6b1000b94e835ffd37f4c6dcbdad43f4b48a02a</hash>\n+            </hashes>\n+            <licenses>\n+                <license>\n+                    <id>Apache-2.0</id>\n+                </license>\n+            </licenses>\n+            <purl>pkg:maven/com.acme/stock-java-client@1.0.12</purl>\n+        </component>\n+    </components>\n+    <services>\n+        <service bom-ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\">\n+            <provider>\n+                <name>Partner Org</name>\n+                <url>https://partner.org</url>\n+                <contact>\n+                    <name>Support</name>\n+                    <email>support@partner</email>\n+                    <phone>800-555-1212</phone>\n+                </contact>\n+            </provider>\n+            <group>org.partner</group>\n+            <name>Stock ticker service</name>\n+            <version>2020-Q2</version>\n+            <description />\n+            <authenticated>true</authenticated>\n+            <x-trust-boundary>true</x-trust-boundary>\n+            <licenses>\n+                <license>\n+                    <name>Partner license</name>\n+                </license>\n+            </licenses>\n+        </service>\n+    </services>\n+    <dependencies>\n+        <dependency ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+            <dependency ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\"/>\n+        </dependency>\n+    </dependencies>\n+</bom>\ndiff --git a/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap b/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap\nnew file mode 100644\nindex 00000000..dc323228\n--- /dev/null\n+++ b/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap\n@@ -0,0 +1,54 @@\n+---\n+source: cyclonedx-bom/tests/specification_tests_v1_3.rs\n+assertion_line: 27\n+expression: bom_output\n+input_file: cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\n+---\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<bom xmlns=\"http://cyclonedx.org/schema/bom/1.3\" serialNumber=\"urn:uuid:3e671687-395b-41f5-a30f-a58921a69b79\" version=\"1\">\n+  <components>\n+    <component type=\"library\" bom-ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+      <group>com.acme</group>\n+      <name>stock-java-client</name>\n+      <version>1.0.12</version>\n+      <hashes>\n+        <hash alg=\"SHA-1\">e6b1000b94e835ffd37f4c6dcbdad43f4b48a02a</hash>\n+      </hashes>\n+      <licenses>\n+        <license>\n+          <id>Apache-2.0</id>\n+        </license>\n+      </licenses>\n+      <purl>pkg:maven/com.acme/stock-java-client@1.0.12</purl>\n+    </component>\n+  </components>\n+  <services>\n+    <service bom-ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\">\n+      <provider>\n+        <name>Partner Org</name>\n+        <url>https://partner.org</url>\n+        <contact>\n+          <name>Support</name>\n+          <email>support@partner</email>\n+          <phone>800-555-1212</phone>\n+        </contact>\n+      </provider>\n+      <group>org.partner</group>\n+      <name>Stock ticker service</name>\n+      <version>2020-Q2</version>\n+      <description></description>\n+      <authenticated>true</authenticated>\n+      <x-trust-boundary>true</x-trust-boundary>\n+      <licenses>\n+        <license>\n+          <name>Partner license</name>\n+        </license>\n+      </licenses>\n+    </service>\n+  </services>\n+  <dependencies>\n+    <dependency ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+      <dependency ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\" />\n+    </dependency>\n+  </dependencies>\n+</bom>\n", "problem_statement": "XML deserialization of empty tags is incorrect\nAs a working example, I am going to reference the following area of code, but I believe it is possible to run into this in any area that the library is parsing an `Option<String>` field: https://github.com/CycloneDX/cyclonedx-rust-cargo/blob/2911287b2520a7ddab1782b48c35112279b1be17/cyclonedx-bom/src/specs/common/component.rs#L513-L517\r\n\r\nSetting context, I was attempting to swap out an ad-hoc CycloneDX parser with `cyclonedx-bom`. One of the tests for my ad-hoc parser was to ensure that the keycloak BOM found here loads: https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/keycloak-10.0.2/bom.xml. Now, it obviously doesn't because it is a v1.2 spec and `cyclonedx-bom` only supports v1.3-v1.5. But, looking through the spec and looking at the BOM, it seemed like it should load as a v1.3, so I changed the version locally. That seemed a reasonable test given that a diff between https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.2.xml and https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.3.xml only added some properties and changed the spec version. \r\n\r\nHowever, when I attempted to load the modified keycloak BOM, I got the following error:\r\n\r\n```\r\n    Got unexpected XML element when reading {http://cyclonedx.org/schema/bom/1.3}description: Got unexpected element EndElement({http://cyclonedx.org/schema/bom/1.3}description)\r\n```\r\n\r\nAfter digging into the issue, what I discovered was that the keycloak BOM has several empty description tags in it:\r\n\r\n```xml\r\n<description />\r\n```\r\n\r\nA quick modification of the `laravel-7.12.0/bom.1.3.xml` file to modify one of the description tags the same way caused the same error to occur. I also attempted to just modify one of the description like the following and received the same error:\r\n\r\n```xml\r\n<description></description>\r\n```\r\n\r\nThe [v1.3 (XML)](https://cyclonedx.org/docs/1.3/xml/#type_component) and [v1.2 (XML)](https://cyclonedx.org/docs/1.2/xml/#type_component) specs \r\nboth define the description field as `<bom:description> xs:normalizedString </bom:description> [0..1]` and an `xs:normalizedString` _can_ be empty.\r\n\r\nAgain, there are multiple places just in the `Component::read_xml_element` where this could be an issue. My guess is that, since the `pom.xml` file that is used to generate the CycloneDX Maven BOMs can have empty strings for the description, that is why it is getting forwarded on to the BOM that way instead of stripping out the description tag completely. \r\n\r\nI could make an argument for handling it either by making an empty string become a `None` for the `Option<String>` OR by literally making it an `Some(String::new())`. I think that the latter is closer to the behavior that would happen with the JSON implementation of the spec, since an equivalent there would be:\r\n\r\n```json\r\n\"description\": \"\"\r\n```\r\n\r\nand that would be parsed as `Some(String::new())` by `serde`.\r\n\r\n\n", "hints_text": "Thank you for reporting this!\r\n\r\nTreating this as `Some(String::new())` for consistency with JSON sounds good to me. I'm not familiar with XML parsing myself, but I'd be happy to merge a PR with this change.\nHello,\r\n\r\nI asked on the CycloneDX Slack to clarify how to handle the `description` tag. It's not totally clear to me if `<description />` is a valid entry or if it needs to be either absent or contain text. I'm uncertain, because the XML sample files in the [specification](https://github.com/CycloneDX/specification/) repository don't contain any empty `<description />` tag. We used these sample files as test data for this repository, but they may not cover all cases.\n**Update**: The answer on Slack was the `<specification />` tag is valid, it should result in an empty string. The current XML parse logic is therefore wrong, at least for these types of fields.", "created_at": "2024-08-07 14:22:17", "merge_commit_sha": "7596551f4caeb368e500c501b5a28f80d5347aa0", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test Suite Runs - Nightly', '.github/workflows/rust.yml']", "[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']"], ["['plan', '.github/workflows/release.yml']", "['build-global-artifacts', '.github/workflows/release.yml']"], ["['Test Suite Runs - Minimum Supported Rust Version', '.github/workflows/rust.yml']", "['Check Dependencies', '.github/workflows/rust.yml']"]]}
{"repo": "atuinsh/atuin", "instance_id": "atuinsh__atuin-2154", "base_commit": "55840bddf5caec1137380f70ef9c9b556fd483e6", "patch": "diff --git a/crates/atuin-client/src/settings.rs b/crates/atuin-client/src/settings.rs\nindex 0f1b443a986..c0ead292fac 100644\n--- a/crates/atuin-client/src/settings.rs\n+++ b/crates/atuin-client/src/settings.rs\n@@ -553,6 +553,10 @@ impl Settings {\n             return Ok(false);\n         }\n \n+        if self.sync_frequency == \"0\" {\n+            return Ok(true);\n+        }\n+\n         match parse_duration(self.sync_frequency.as_str()) {\n             Ok(d) => {\n                 let d = time::Duration::try_from(d).unwrap();\n", "test_patch": "", "problem_statement": "[Bug]: sync_frequency = \"0\" never syncs\n### What did you expect to happen?\n\nAtuin syncs after every command.\n\n### What happened?\n\nAtuin never syncs. A manual sync using `atuin sync` still works, as does using `\"1s\"` as the frequency.\r\n\r\nI suspect this broke in 18.3.0.\n\n### Atuin doctor output\n\n```yaml\n{\r\n  \"atuin\": {\r\n    \"version\": \"18.3.0\",\r\n    \"sync\": {\r\n      \"cloud\": false,\r\n      \"records\": true,\r\n      \"auto_sync\": true,\r\n      \"last_sync\": \"2024-06-17 6:41:34.445861124 +00:00:00\"\r\n    },\r\n    \"sqlite_version\": \"3.44.0\"\r\n  },\r\n  \"shell\": {\r\n    \"name\": \"zsh\",\r\n    \"default\": \"zsh\",\r\n    \"plugins\": [\r\n      \"atuin\"\r\n    ],\r\n    \"preexec\": \"built-in\"\r\n  },\r\n  \"system\": {\r\n    \"os\": \"Arch Linux\",\r\n    \"arch\": \"x86_64\",\r\n    \"version\": \"rolling\",\r\n    \"disks\": [\r\n      {\r\n        \"name\": \"/dev/mapper/cornicle-root\",\r\n        \"filesystem\": \"btrfs\"\r\n      },\r\n      {\r\n        \"name\": \"/dev/nvme0n1p1\",\r\n        \"filesystem\": \"vfat\"\r\n      }\r\n    ]\r\n  }\r\n}\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n", "hints_text": "Could you also share your config file please?\n```toml\r\nupdate_check = false\r\nsync_address = \"http://<redacted>\"\r\nsync_frequency = \"0\"\r\nstyle = \"compact\"\r\ninline_height = 15\r\nscroll_context_lines = 2\r\nsecrets_filter = false\r\nenter_accept = false\r\n[stats]\r\n[keys]\r\n[sync]\r\nrecords = true\r\n[preview]\r\n[daemon]\r\n```\nThe regression was introduced here: https://github.com/atuinsh/atuin/pull/2074\r\n\r\nError:\r\n\r\n```\r\nError: failed to check sync: time unit needed, for example 0sec or 0ms\r\n\r\nLocation:\r\n    crates/atuin-client/src/settings.rs:561:27\r\n```\r\n\r\nSettings\r\n\r\n```\r\nsync_frequency = \"0s\"\r\n```\r\n\r\nshould resolve it\r\n\r\nI'll push a patch to make the unitless version work, though really consistent units make sense", "created_at": "2024-06-17 07:37:31", "merge_commit_sha": "0a6bfbba3ea1d44220115779b232a8d7d2e823c9", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['unit-test (macos-14)', '.github/workflows/rust.yml']", "['build-test', '.github/workflows/nix.yml']"], ["['check (ubuntu-latest)', '.github/workflows/rust.yml']", "['build (macos-14)', '.github/workflows/rust.yml']"], ["['cross-compile (x86_64-unknown-illumos)', '.github/workflows/rust.yml']", "['format', '.github/workflows/rust.yml']"], ["['check', '.github/workflows/nix.yml']", "['Check for spelling errors', '.github/workflows/codespell.yml']"], ["['shellcheck', '.github/workflows/shellcheck.yml']", "['plan', '.github/workflows/release.yml']"]]}
{"repo": "dathere/qsv", "instance_id": "dathere__qsv-2362", "base_commit": "9c0c1a7a63ef3773e599f6fa91e6fa3b734936df", "patch": "diff --git a/src/cmd/luau.rs b/src/cmd/luau.rs\nindex e0dcba7b2..e0da289b6 100644\n--- a/src/cmd/luau.rs\n+++ b/src/cmd/luau.rs\n@@ -238,6 +238,8 @@ Common options:\n \"#;\n \n use std::{\n+    cell::RefCell,\n+    collections::HashMap,\n     env, fs, io,\n     io::Write,\n     path::Path,\n@@ -1762,6 +1764,71 @@ fn setup_helpers(\n     )?;\n     luau.globals().set(\"qsv_loadcsv\", qsv_loadcsv)?;\n \n+    // this is a helper function to load a JSON file into a Luau table.\n+    //\n+    //   qsv_loadjson(table_name: string, filepath: string)\n+    //      table_name: the name of the Luau table to load the JSON data into.\n+    //        filepath: the path of the JSON file to load\n+    //         returns: true if successful.\n+    //                  A Luau runtime error if the filepath is invalid or JSON parsing fails.\n+    //\n+    let qsv_loadjson =\n+        luau.create_function(move |luau, (table_name, filepath): (String, String)| {\n+            if filepath.is_empty() {\n+                return helper_err!(\"qsv_loadjson\", \"filepath cannot be empty.\");\n+            }\n+\n+            let path = Path::new(&filepath);\n+            if !path.exists() {\n+                return helper_err!(\"qsv_loadjson\", \"\\\"{}\\\" does not exist.\", path.display());\n+            }\n+\n+            // Read the JSON file\n+            let json_str = match fs::read_to_string(path) {\n+                Ok(content) => content,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to read JSON file \\\"{}\\\": {e}\",\n+                        path.display()\n+                    );\n+                },\n+            };\n+\n+            // Parse the JSON string\n+            let json_value: serde_json::Value = match serde_json::from_str(&json_str) {\n+                Ok(v) => v,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to parse JSON from \\\"{}\\\": {e}\",\n+                        path.display()\n+                    );\n+                },\n+            };\n+\n+            // Convert JSON value to Luau value and store it in the global table\n+            let luau_value = match luau.to_value(&json_value) {\n+                Ok(v) => v,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to convert JSON to Luau value: {e}\"\n+                    );\n+                },\n+            };\n+\n+            luau.globals().raw_set(table_name.clone(), luau_value)?;\n+\n+            info!(\n+                \"{} successfully loaded JSON into table '{}'.\",\n+                filepath, table_name\n+            );\n+\n+            Ok(true)\n+        })?;\n+    luau.globals().set(\"qsv_loadjson\", qsv_loadjson)?;\n+\n     // this is a helper function that can be called from the BEGIN, MAIN & END scripts to write to\n     // a file. The file will be created if it does not exist. The file will be appended to if it\n     // already exists. The filename will be sanitized and will be written to the current working\n@@ -2030,6 +2097,271 @@ fn setup_helpers(\n     })?;\n     luau.globals().set(\"qsv_shellcmd\", qsv_shellcmd)?;\n \n+    // this is a helper function that calculates the cumulative sum of a numeric column.\n+    // if the input cannot be converted to a number, it returns 0 for that row.\n+    //\n+    //   qsv_cumsum(name, value)\n+    //          name: identifier for this cumulative sum (allows multiple sums to run in parallel)\n+    //         value: the numeric value to add to the cumulative sum\n+    //       returns: the cumulative sum up to the current row for the named sum\n+    //\n+    let qsv_cumsum = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        // Get static cumulative sums using thread_local storage\n+        thread_local! {\n+            static CUMSUMS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        // Convert input value to number, defaulting to 0.0 if conversion fails\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or_default(),\n+            _ => 0.0,\n+        };\n+\n+        // Update cumulative sum for this name\n+        CUMSUMS.with(|cs| {\n+            let mut sums = cs.borrow_mut();\n+            let sum = sums.entry(name).or_insert(0.0);\n+            *sum += num;\n+            Ok(*sum)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumsum\", qsv_cumsum)?;\n+\n+    // this is a helper function that calculates the cumulative product of a numeric column.\n+    // if the input cannot be converted to a number, it returns 1 for that row.\n+    //\n+    //   qsv_cumprod(name, value)\n+    //          name: identifier for this cumulative product (allows multiple products to run in\n+    // parallel)         value: the numeric value to multiply with the cumulative product\n+    //       returns: the cumulative product up to the current row for the named product\n+    //\n+    let qsv_cumprod = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMPRODS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(1.0),\n+            _ => 1.0,\n+        };\n+\n+        CUMPRODS.with(|cp| {\n+            let mut prods = cp.borrow_mut();\n+            let prod = prods.entry(name).or_insert(1.0);\n+            *prod *= num;\n+            Ok(*prod)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumprod\", qsv_cumprod)?;\n+\n+    // this is a helper function that calculates the cumulative maximum of a numeric column.\n+    // if the input cannot be converted to a number, it returns negative infinity for that row.\n+    //\n+    //   qsv_cummax(name, value)\n+    //          name: identifier for this cumulative maximum (allows multiple maximums to run in\n+    // parallel)         value: the numeric value to compare with the cumulative maximum\n+    //       returns: the cumulative maximum up to the current row for the named maximum\n+    //\n+    let qsv_cummax = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMMAXS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s\n+                .to_string_lossy()\n+                .parse::<f64>()\n+                .unwrap_or(f64::NEG_INFINITY),\n+            _ => f64::NEG_INFINITY,\n+        };\n+\n+        CUMMAXS.with(|cm| {\n+            let mut maxs = cm.borrow_mut();\n+            let max = maxs.entry(name).or_insert(f64::NEG_INFINITY);\n+            *max = max.max(num);\n+            Ok(*max)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cummax\", qsv_cummax)?;\n+\n+    // this is a helper function that calculates the cumulative minimum of a numeric column.\n+    // if the input cannot be converted to a number, it returns positive infinity for that row.\n+    //\n+    //   qsv_cummin(name, value)\n+    //          name: identifier for this cumulative minimum (allows multiple minimums to run in\n+    // parallel)         value: the numeric value to compare with the cumulative minimum\n+    //       returns: the cumulative minimum up to the current row for the named minimum\n+    //\n+    let qsv_cummin = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMMINS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(f64::INFINITY),\n+            _ => f64::INFINITY,\n+        };\n+\n+        CUMMINS.with(|cm| {\n+            let mut mins = cm.borrow_mut();\n+            let min = mins.entry(name).or_insert(f64::INFINITY);\n+            *min = min.min(num);\n+            Ok(*min)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cummin\", qsv_cummin)?;\n+\n+    // qsv_lag - returns lagged value with optional default\n+    //\n+    //   qsv_lag(name, value, lag, default)\n+    //          name: identifier for this lag (allows multiple lags to run in parallel)\n+    //         value: the value to lag\n+    //           lag: (optional) number of rows to lag by (default: 1)\n+    //       default: (optional) value to return for rows before lag is available (default: \"0\")\n+    //       returns: the value from 'lag' rows ago, or default if not enough rows seen yet\n+    let qsv_lag = luau.create_function(|luau, (name, value, lag, default): (String, mlua::Value, Option<i64>, Option<mlua::Value>)| {\n+        thread_local! {\n+            static LAGS: RefCell<HashMap<String, Vec<String>>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let lag = lag.unwrap_or(1);\n+        let key = format!(\"{name}_{lag}\");\n+\n+        // Convert the value to a string to store it\n+        let value_str = match &value {\n+            Value::String(s) => s.to_string_lossy(),\n+            Value::Number(n) => n.to_string(),\n+            Value::Integer(i) => i.to_string(),\n+            Value::Boolean(b) => b.to_string(),\n+            Value::Nil => String::new(),\n+            _ => value.to_string().unwrap_or_default(),\n+        };\n+\n+        LAGS.with(|l| {\n+            let mut lags = l.borrow_mut();\n+            let values = lags.entry(key).or_default();\n+            values.push(value_str);\n+\n+            if values.len() as i64 <= lag {\n+                // Return the default value when not enough history\n+                Ok(default.unwrap_or_else(|| {\n+                    mlua::Value::String(luau.create_string(\"0\").unwrap())\n+                }))\n+            } else {\n+                let lagged_value = &values[values.len() - 1 - lag as usize];\n+                Ok(mlua::Value::String(luau.create_string(lagged_value)?))\n+            }\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_lag\", qsv_lag)?;\n+\n+    // qsv_cumany - returns true if any value so far has been truthy\n+    //\n+    //   qsv_cumany(name, value)\n+    //          name: identifier for this cumulative any (allows multiple cumany's to run in\n+    // parallel)         value: the value to check for truthiness\n+    //       returns: true if any value seen so far has been truthy, false otherwise\n+    let qsv_cumany = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMANYS: RefCell<HashMap<String, bool>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let is_truthy = match value {\n+            Value::Boolean(b) => b,\n+            Value::Number(n) => n != 0.0,\n+            Value::Integer(i) => i != 0,\n+            Value::String(s) => !s.to_string_lossy().is_empty(),\n+            Value::Nil => false,\n+            _ => true, // Tables, functions, etc. are considered truthy\n+        };\n+\n+        CUMANYS.with(|ca| {\n+            let mut anys = ca.borrow_mut();\n+            let any = anys.entry(name).or_insert(false);\n+            *any = *any || is_truthy;\n+            Ok(*any)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumany\", qsv_cumany)?;\n+\n+    // qsv_cumall - returns true if all values so far have been truthy\n+    //\n+    //   qsv_cumall(name, value)\n+    //          name: identifier for this cumulative all (allows multiple cumall's to run in\n+    // parallel)         value: the value to check for truthiness\n+    //       returns: true if all values seen so far have been truthy, false otherwise\n+    let qsv_cumall = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMALLS: RefCell<HashMap<String, bool>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let is_truthy = match value {\n+            Value::Boolean(b) => b,\n+            Value::Number(n) => n != 0.0,\n+            Value::Integer(i) => i != 0,\n+            Value::String(s) => !s.to_string_lossy().is_empty(),\n+            Value::Nil => false,\n+            _ => true, // Tables, functions, etc. are considered truthy\n+        };\n+\n+        CUMALLS.with(|ca| {\n+            let mut all_vals = ca.borrow_mut();\n+            let all = all_vals.entry(name).or_insert(true);\n+            *all = *all && is_truthy;\n+            Ok(*all)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumall\", qsv_cumall)?;\n+\n+    // qsv_diff - returns difference between current and previous value\n+    //\n+    //   qsv_diff(name, value[, periods])\n+    //          name: identifier for this diff (allows multiple diffs to run in parallel)\n+    //         value: the value to calculate difference for\n+    //       periods: optional number of periods to look back (default: 1)\n+    //       returns: difference between current value and value 'periods' rows back\n+    //               returns 0 if not enough history available yet\n+    let qsv_diff = luau.create_function(\n+        |_, (name, value, periods): (String, mlua::Value, Option<i64>)| {\n+            thread_local! {\n+                static DIFFS: RefCell<HashMap<String, Vec<f64>>> = RefCell::new(HashMap::new());\n+            }\n+\n+            let periods = periods.unwrap_or(1);\n+            // Create a unique key that includes both the name and periods\n+            let key = format!(\"{name}_{periods}\");\n+\n+            let num = match value {\n+                Value::Number(n) => n,\n+                Value::Integer(i) => i as f64,\n+                Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(0.0),\n+                _ => 0.0,\n+            };\n+\n+            DIFFS.with(|d| {\n+                let mut diffs = d.borrow_mut();\n+                let values = diffs.entry(key).or_default();\n+                values.push(num);\n+\n+                if values.len() as i64 <= periods {\n+                    Ok(0.0) // Return 0 when not enough history\n+                } else {\n+                    let prev_value = values[values.len() - 1 - periods as usize];\n+                    Ok(num - prev_value)\n+                }\n+            })\n+        },\n+    )?;\n+    luau.globals().set(\"qsv_diff\", qsv_diff)?;\n+\n     // this is a helper function that can be called from the BEGIN script to register\n     // and load a lookup table. It expects two arguments - the lookup_name & the\n     // lookup_table_uri - the URI of the CSV to use as a lookup table.\n", "test_patch": "diff --git a/tests/test_luau.rs b/tests/test_luau.rs\nindex a926845ac..599d065c2 100644\n--- a/tests/test_luau.rs\n+++ b/tests/test_luau.rs\n@@ -2313,3 +2313,621 @@ return tonumber(number) > tonumber(limit)\n     ];\n     assert_eq!(got, expected);\n }\n+\n+#[test]\n+fn luau_loadjson() {\n+    let wrk = Workdir::new(\"luau_loadjson\");\n+\n+    // Create a test JSON file\n+    wrk.create_from_string(\n+        \"test.json\",\n+        r#\"{\n+            \"string\": \"hello\",\n+            \"number\": 42,\n+            \"boolean\": true,\n+            \"null\": null,\n+            \"array\": [1, 2, 3],\n+            \"object\": {\n+                \"nested\": \"value\",\n+                \"numbers\": [4, 5, 6]\n+            }\n+        }\"#,\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(r#\"\n+BEGIN {\n+    qsv_loadjson(\"config_tbl\", \"test.json\");\n+    qsv_log(\"debug\", \"Loaded JSON table:\", config_tbl)\n+    for k,v in pairs(config_tbl) do\n+        qsv_log(\"debug\", \"Key:\", k, \"Value:\", v)\n+    end\n+}!\n+\n+return config_tbl[\"string\"] .. \" \" .. config_tbl[\"object\"][\"nested\"] .. \" \" .. tostring(config_tbl[\"number\"])\n+\"#)\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"], svec![\"b\"], svec![\"c\"]]);\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"a\", \"hello value 42\"],\n+        svec![\"b\", \"hello value 42\"],\n+        svec![\"c\", \"hello value 42\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_loadjson_array() {\n+    let wrk = Workdir::new(\"luau_loadjson_array\");\n+\n+    // Create a test JSON file with an array\n+    wrk.create_from_string(\n+        \"test.json\",\n+        r#\"[\n+            {\"id\": 1, \"name\": \"Alice\"},\n+            {\"id\": 2, \"name\": \"Bob\"},\n+            {\"id\": 3, \"name\": \"Charlie\"}\n+        ]\"#,\n+    );\n+\n+    // Create input CSV with index references\n+    wrk.create(\n+        \"data.csv\",\n+        vec![svec![\"x\"], svec![\"1\"], svec![\"2\"], svec![\"3\"]],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"name\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"users\", \"test.json\");\n+}!\n+\n+local idx = tonumber(x)\n+return users[idx][\"name\"]\n+\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"x\", \"name\"],\n+        svec![\"1\", \"Alice\"],\n+        svec![\"2\", \"Bob\"],\n+        svec![\"3\", \"Charlie\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_loadjson_error_missing_file() {\n+    let wrk = Workdir::new(\"luau_loadjson_error\");\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"config\", \"nonexistent.json\");\n+}!\n+\n+return \"should not get here\"\n+\"#,\n+        )\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"]]);\n+\n+    wrk.assert_err(&mut cmd);\n+}\n+\n+#[test]\n+fn luau_loadjson_error_invalid_json() {\n+    let wrk = Workdir::new(\"luau_loadjson_invalid\");\n+\n+    // Create an invalid JSON file\n+    wrk.create_from_string(\n+        \"invalid.json\",\n+        r#\"{\n+            \"unclosed\": \"object\"\n+        \"#, // Missing closing brace\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"config\", \"invalid.json\");\n+}!\n+\n+return \"should not get here\"\n+\"#,\n+        )\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"]]);\n+\n+    wrk.assert_err(&mut cmd);\n+}\n+\n+#[test]\n+fn luau_cumsum_single() {\n+    let wrk = Workdir::new(\"luau_cumsum_single\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"amount\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+            svec![\"50\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_total\")\n+        .arg(r#\"qsv_cumsum(\"total\", amount)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"amount\", \"running_total\"],\n+        svec![\"10\", \"10\"],\n+        svec![\"20\", \"30\"],\n+        svec![\"30\", \"60\"],\n+        svec![\"40\", \"100\"],\n+        svec![\"50\", \"150\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumsum_multiple() {\n+    let wrk = Workdir::new(\"luau_cumsum_multiple\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"revenue\", \"expenses\"],\n+            svec![\"100\", \"50\"],\n+            svec![\"200\", \"75\"],\n+            svec![\"150\", \"80\"],\n+            svec![\"300\", \"100\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"revenue_total,expenses_total,profit_total\")\n+        .arg(\n+            r#\"{\n+            qsv_cumsum(\"revenue\", revenue),\n+            qsv_cumsum(\"expenses\", expenses),\n+            qsv_cumsum(\"profit\", tonumber(revenue) - tonumber(expenses))\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\n+            \"revenue\",\n+            \"expenses\",\n+            \"revenue_total\",\n+            \"expenses_total\",\n+            \"profit_total\"\n+        ],\n+        svec![\"100\", \"50\", \"100\", \"50\", \"50\"],\n+        svec![\"200\", \"75\", \"300\", \"125\", \"175\"],\n+        svec![\"150\", \"80\", \"450\", \"205\", \"245\"],\n+        svec![\"300\", \"100\", \"750\", \"305\", \"445\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumsum_invalid_input() {\n+    let wrk = Workdir::new(\"luau_cumsum_invalid\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"invalid\"],\n+            svec![\"20\"],\n+            svec![\"bad\"],\n+            svec![\"30\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"total\")\n+        .arg(r#\"qsv_cumsum(\"sum\", value)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"total\"],\n+        svec![\"10\", \"10\"],\n+        svec![\"invalid\", \"10\"], // Invalid input treated as 0\n+        svec![\"20\", \"30\"],\n+        svec![\"bad\", \"30\"], // Invalid input treated as 0\n+        svec![\"30\", \"60\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumprod() {\n+    let wrk = Workdir::new(\"luau_cumprod\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"2\"],\n+            svec![\"3\"],\n+            svec![\"4\"],\n+            svec![\"5\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_product\")\n+        .arg(r#\"qsv_cumprod(\"prod\", value)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_product\"],\n+        svec![\"2\", \"2\"],\n+        svec![\"3\", \"6\"],\n+        svec![\"4\", \"24\"],\n+        svec![\"5\", \"120\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cummax_cummin() {\n+    let wrk = Workdir::new(\"luau_cummax_cummin\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"5\"],\n+            svec![\"15\"],\n+            svec![\"3\"],\n+            svec![\"12\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_max,running_min\")\n+        .arg(r#\"{qsv_cummax(\"max\", value), qsv_cummin(\"min\", value)}\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_max\", \"running_min\"],\n+        svec![\"10\", \"10\", \"10\"],\n+        svec![\"5\", \"10\", \"5\"],\n+        svec![\"15\", \"15\", \"5\"],\n+        svec![\"3\", \"15\", \"3\"],\n+        svec![\"12\", \"15\", \"3\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_lag() {\n+    let wrk = Workdir::new(\"luau_lag\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+            svec![\"50\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"lag1,lag2\")\n+        .arg(\n+            r#\"{\n+            qsv_lag(\"val\", value, 1, \"0\"),\n+            qsv_lag(\"val\", value, 2, \"0\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"lag1\", \"lag2\"],\n+        svec![\"10\", \"0\", \"0\"],\n+        svec![\"20\", \"10\", \"0\"],\n+        svec![\"30\", \"20\", \"10\"],\n+        svec![\"40\", \"30\", \"20\"],\n+        svec![\"50\", \"40\", \"30\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumany_cumall() {\n+    let wrk = Workdir::new(\"luau_cumany_cumall\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"0\"],\n+            svec![\"15\"],\n+            svec![\"0\"],\n+            svec![\"20\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"has_zero,all_positive\")\n+        .arg(\n+            r#\"{\n+            qsv_cumany(\"zero\", value == \"0\"),\n+            qsv_cumall(\"pos\", tonumber(value) > 0)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"has_zero\", \"all_positive\"],\n+        svec![\"10\", \"false\", \"true\"],\n+        svec![\"0\", \"true\", \"false\"],\n+        svec![\"15\", \"true\", \"false\"],\n+        svec![\"0\", \"true\", \"false\"],\n+        svec![\"20\", \"true\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_diff() {\n+    let wrk = Workdir::new(\"luau_diff\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"15\"],\n+            svec![\"13\"],\n+            svec![\"20\"],\n+            svec![\"18\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"diff1,diff2\")\n+        .arg(\n+            r#\"{\n+            qsv_diff(\"val\", value),\n+            qsv_diff(\"val\", value, 2)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"diff1\", \"diff2\"],\n+        svec![\"10\", \"0\", \"0\"],\n+        svec![\"15\", \"5\", \"0\"],\n+        svec![\"13\", \"-2\", \"3\"],\n+        svec![\"20\", \"7\", \"5\"],\n+        svec![\"18\", \"-2\", \"5\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumany_cumall_with_strings() {\n+    let wrk = Workdir::new(\"luau_cumany_cumall_strings\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"hello\"],\n+            svec![\"\"],\n+            svec![\"world\"],\n+            svec![\"\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"has_empty,all_nonempty\")\n+        .arg(\n+            r#\"{\n+            qsv_cumany(\"empty\", value == \"\"),\n+            qsv_cumall(\"nonempty\", value ~= \"\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"has_empty\", \"all_nonempty\"],\n+        svec![\"hello\", \"false\", \"true\"],\n+        svec![\"\", \"true\", \"false\"],\n+        svec![\"world\", \"true\", \"false\"],\n+        svec![\"\", \"true\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumall_with_numbers() {\n+    let wrk = Workdir::new(\"luau_cumall_numbers\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"0\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"all_positive\")\n+        .arg(r#\"qsv_cumall(\"pos\", tonumber(value) > 0)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"all_positive\"],\n+        svec![\"10\", \"true\"],\n+        svec![\"0\", \"false\"],\n+        svec![\"20\", \"false\"],\n+        svec![\"30\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_lag_with_default() {\n+    let wrk = Workdir::new(\"luau_lag_default\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"prev,prev2\")\n+        .arg(\n+            r#\"{\n+            qsv_lag(\"lag1\", value, 1, \"N/A\"),\n+            qsv_lag(\"lag2\", value, 2, \"N/A\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"prev\", \"prev2\"],\n+        svec![\"10\", \"N/A\", \"N/A\"],\n+        svec![\"20\", \"10\", \"N/A\"],\n+        svec![\"30\", \"20\", \"10\"],\n+        svec![\"40\", \"30\", \"20\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_diff_with_lag() {\n+    let wrk = Workdir::new(\"luau_diff_lag\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"100\"],\n+            svec![\"120\"],\n+            svec![\"115\"],\n+            svec![\"140\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"diff1,diff2,pct_change\")\n+        .arg(\n+            r#\"{\n+            qsv_diff(\"d1\", value),\n+            qsv_diff(\"d2\", value, 2),\n+            string.format(\"%.1f%%\", ((tonumber(value) / tonumber(qsv_lag(\"pct\", value))) - 1.0) * 100)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"diff1\", \"diff2\", \"pct_change\"],\n+        svec![\"100\", \"0\", \"0\", \"inf%\"], // division by zero\n+        svec![\"120\", \"20\", \"0\", \"20.0%\"],\n+        svec![\"115\", \"-5\", \"15\", \"-4.2%\"],\n+        svec![\"140\", \"25\", \"20\", \"21.7%\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumulative_stats() {\n+    let wrk = Workdir::new(\"luau_cumulative_stats\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"15\"],\n+            svec![\"10\"],\n+            svec![\"25\"],\n+            svec![\"5\"],\n+            svec![\"20\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_max,running_min,running_sum\")\n+        .arg(\n+            r#\"{\n+            qsv_cummax(\"max\", value),\n+            qsv_cummin(\"min\", value),\n+            qsv_cumsum(\"sum\", value)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_max\", \"running_min\", \"running_sum\"],\n+        svec![\"15\", \"15\", \"15\", \"15\"],\n+        svec![\"10\", \"15\", \"10\", \"25\"],\n+        svec![\"25\", \"25\", \"10\", \"50\"],\n+        svec![\"5\", \"25\", \"5\", \"55\"],\n+        svec![\"20\", \"25\", \"5\", \"75\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n", "problem_statement": "`luau`: additional helper functions\n@ggrothendieck came up with an extensive list of helper functions to add to `luau` as qsv's DSL.\r\n\r\n---\r\n\r\nIf you are implementing cumsum there are a number of other related functions that have proven to be useful in other languages which\u00a0follow a similar pattern and so could be readily implemented at the same time.\r\n\r\n**cumprod, cumany, cumall, cummax, cummin**\r\nThese are like cumsum but in place of + they use *, or, and, max amd min. cummean is also useful but does not fit exactly into the same pattern.\r\n\r\n**accumulate**\r\nHas three arguments. A column, a function of two arguments and an optional initial value. If\u00a0\u00a0 y = accum(x, f, init)then y[1] = init and for i >1 we have y[i] = f(y[i-1], x[i]). The default for init is x[1]. Note that if f is +, *, or, and, max or min we get the above cum... functions.\r\n\r\n**Other**\r\n\r\nThe following are also useful and are related in so far as they also involve storing the previous value.\r\n\r\n**lag**\r\nIt has three arguments. The column, how many positions to lag (default is 1) and in the case that the lag is off the front of the column use default. If y = lag(x, k, default) then\u00a0y[i] = x[i-k] if i > k and default otherwise. Negative k could be considered too if not too hard to implement.\r\nRecall we discussed enum by group with the shortest solution being the following where Name is the column to group by:\r\n```\r\nqsv luau map seq \"x = (Name == prev and 1 or 0) * (x or 0) + 1; prev = Name; return x\" file1.csv\r\n```\r\nWith lag we could omit setting prev\r\n```\r\nqsv luau map seq \"x = (Name == lag(Name, 1) and 1 or 0) * (x or 0) + 1; return x\" file1.csv\r\n```\r\n\r\n**diff**\r\nSame args as lag. Defined as x - lag(x, k, default)\r\n\r\n_Originally posted by @ggrothendieck in https://github.com/jqnatividad/qsv/discussions/1760#discussioncomment-9262555_\n", "hints_text": "Do this once mlua 0.10.0 is released\r\nhttps://github.com/mlua-rs/mlua/blob/main/docs/release_notes/v0.10.md", "created_at": "2024-12-21 14:45:24", "merge_commit_sha": "e87f3e9a957c59cf084aab7ed94656e8d37bfdcd", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build', '.github/workflows/rust-windows.yml']", "['build', '.github/workflows/rust-macos-polars.yml']"], ["['DevSkim', '.github/workflows/devskim.yml']", "['build', '.github/workflows/rust.yml']"], ["['build', '.github/workflows/rust-qsvdp.yml']", "['build', '.github/workflows/rust-qsvlite.yml']"]]}
{"repo": "flxzt/rnote", "instance_id": "flxzt__rnote-1268", "base_commit": "70e5e0b5f90ece76a384bf7c8b7361d18eb1cbc7", "patch": "diff --git a/crates/rnote-engine/src/engine/mod.rs b/crates/rnote-engine/src/engine/mod.rs\nindex 6a3c661753..0a3d54252c 100644\n--- a/crates/rnote-engine/src/engine/mod.rs\n+++ b/crates/rnote-engine/src/engine/mod.rs\n@@ -845,6 +845,18 @@ impl Engine {\n         widget_flags\n     }\n \n+    pub fn text_select_closest_word(&mut self) {\n+        if let Pen::Typewriter(typewriter) = self.penholder.current_pen_mut() {\n+            typewriter.select_closest_word(&mut engine_view_mut!(self))\n+        }\n+    }\n+\n+    pub fn text_select_closest_line(&mut self) {\n+        if let Pen::Typewriter(typewriter) = self.penholder.current_pen_mut() {\n+            typewriter.select_closest_line(&mut engine_view_mut!(self))\n+        }\n+    }\n+\n     pub fn text_selection_toggle_attribute(\n         &mut self,\n         text_attribute: TextAttribute,\ndiff --git a/crates/rnote-engine/src/pens/typewriter/mod.rs b/crates/rnote-engine/src/pens/typewriter/mod.rs\nindex f4bd028e7e..9eb7207b78 100644\n--- a/crates/rnote-engine/src/pens/typewriter/mod.rs\n+++ b/crates/rnote-engine/src/pens/typewriter/mod.rs\n@@ -24,12 +24,27 @@ use std::time::{Duration, Instant};\n use tracing::error;\n use unicode_segmentation::GraphemeCursor;\n \n+#[derive(Debug, Clone)]\n+pub(super) enum SelectionMode {\n+    /// Select individual characters.\n+    Caret,\n+    /// Select whole words.\n+    ///\n+    /// The values represent the start and end of the initially selected word.\n+    Word(usize, usize),\n+    /// Select whole lines.\n+    ///\n+    /// The values represent the start and end of the initially selected line.\n+    Line(usize, usize),\n+}\n+\n #[derive(Debug, Clone)]\n pub(super) enum ModifyState {\n     Up,\n     Hover(na::Vector2<f64>),\n     Selecting {\n         selection_cursor: GraphemeCursor,\n+        mode: SelectionMode,\n         /// Whether selecting is finished.\n         ///\n         /// If true, the state will get reset on the next click.\ndiff --git a/crates/rnote-engine/src/pens/typewriter/penevents.rs b/crates/rnote-engine/src/pens/typewriter/penevents.rs\nindex 3fc6de8b4f..379f6ebea0 100644\n--- a/crates/rnote-engine/src/pens/typewriter/penevents.rs\n+++ b/crates/rnote-engine/src/pens/typewriter/penevents.rs\n@@ -1,5 +1,5 @@\n // Imports\n-use super::{ModifyState, Typewriter, TypewriterState};\n+use super::{ModifyState, SelectionMode, Typewriter, TypewriterState};\n use crate::engine::EngineViewMut;\n use crate::pens::PenBehaviour;\n use crate::strokes::{Stroke, TextStroke};\n@@ -150,6 +150,7 @@ impl Typewriter {\n                                             self.state = TypewriterState::Modifying {\n                                                 modify_state: ModifyState::Selecting {\n                                                     selection_cursor: cursor.clone(),\n+                                                    mode: SelectionMode::Caret,\n                                                     finished: false,\n                                                 },\n                                                 stroke_key: *stroke_key,\n@@ -176,7 +177,11 @@ impl Typewriter {\n                             progress,\n                         }\n                     }\n-                    ModifyState::Selecting { finished, .. } => {\n+                    ModifyState::Selecting {\n+                        selection_cursor,\n+                        mode,\n+                        finished,\n+                    } => {\n                         let mut progress = PenProgress::InProgress;\n \n                         if let Some(typewriter_bounds) = typewriter_bounds {\n@@ -214,8 +219,48 @@ impl Typewriter {\n                                         if let Ok(new_cursor) =\n                                             textstroke.get_cursor_for_global_coord(element.pos)\n                                         {\n+                                            let previous_cursor_position = cursor.cur_cursor();\n                                             *cursor = new_cursor;\n-                                            self.reset_blink();\n+\n+                                            match mode {\n+                                                SelectionMode::Word(start, end) => {\n+                                                    let mouse_position = cursor.cur_cursor();\n+\n+                                                    if mouse_position <= *start {\n+                                                        selection_cursor.set_cursor(*end);\n+                                                        textstroke\n+                                                            .move_cursor_word_boundary_back(cursor);\n+                                                    } else if mouse_position >= *end {\n+                                                        selection_cursor.set_cursor(*start);\n+                                                        textstroke\n+                                                            .move_cursor_word_boundary_forward(\n+                                                                cursor,\n+                                                            );\n+                                                    } else {\n+                                                        selection_cursor.set_cursor(*start);\n+                                                        cursor.set_cursor(*end);\n+                                                    }\n+                                                }\n+                                                SelectionMode::Line(start, end) => {\n+                                                    let mouse_position = cursor.cur_cursor();\n+\n+                                                    if mouse_position < *start {\n+                                                        selection_cursor.set_cursor(*end);\n+                                                        textstroke.move_cursor_line_start(cursor);\n+                                                    } else if mouse_position > *end {\n+                                                        selection_cursor.set_cursor(*start);\n+                                                        textstroke.move_cursor_line_end(cursor);\n+                                                    } else {\n+                                                        selection_cursor.set_cursor(*start);\n+                                                        cursor.set_cursor(*end);\n+                                                    }\n+                                                }\n+                                                SelectionMode::Caret => {}\n+                                            }\n+\n+                                            if previous_cursor_position != cursor.cur_cursor() {\n+                                                self.reset_blink();\n+                                            }\n                                         }\n                                     }\n                                 }\n@@ -590,6 +635,7 @@ impl Typewriter {\n                                                 textstroke.text.len(),\n                                                 true,\n                                             ),\n+                                            mode: SelectionMode::Caret,\n                                             finished: true,\n                                         };\n                                     } else {\n@@ -665,6 +711,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         }\n                                     } else {\n@@ -693,6 +740,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         };\n                                     } else {\n@@ -717,6 +765,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         };\n                                     } else {\n@@ -736,6 +785,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         };\n                                     } else {\n@@ -759,6 +809,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         };\n                                     } else {\n@@ -787,6 +838,7 @@ impl Typewriter {\n \n                                         *modify_state = ModifyState::Selecting {\n                                             selection_cursor: old_cursor,\n+                                            mode: SelectionMode::Caret,\n                                             finished: false,\n                                         };\n                                     } else {\n@@ -821,6 +873,7 @@ impl Typewriter {\n                     ModifyState::Selecting {\n                         selection_cursor,\n                         finished,\n+                        ..\n                     } => {\n                         super::play_sound(Some(keyboard_key), engine_view.audioplayer);\n \n@@ -1150,6 +1203,7 @@ impl Typewriter {\n                     ModifyState::Selecting {\n                         selection_cursor,\n                         finished,\n+                        ..\n                     } => {\n                         super::play_sound(None, engine_view.audioplayer);\n \n@@ -1229,4 +1283,64 @@ impl Typewriter {\n \n         (event_result, widget_flags)\n     }\n+\n+    pub fn select_closest_word(&mut self, engine_view: &mut EngineViewMut) {\n+        match &mut self.state {\n+            TypewriterState::Modifying {\n+                modify_state,\n+                stroke_key,\n+                cursor,\n+                pen_down: _,\n+            } => {\n+                if let Some(Stroke::TextStroke(ref mut textstroke)) =\n+                    engine_view.store.get_stroke_mut(*stroke_key)\n+                {\n+                    textstroke.move_cursor_word_boundary_forward(cursor);\n+\n+                    let mut selection_cursor = cursor.clone();\n+                    textstroke.move_cursor_word_boundary_back(&mut selection_cursor);\n+\n+                    *modify_state = ModifyState::Selecting {\n+                        mode: SelectionMode::Word(\n+                            selection_cursor.cur_cursor(),\n+                            cursor.cur_cursor(),\n+                        ),\n+                        selection_cursor,\n+                        finished: false,\n+                    };\n+                }\n+            }\n+            _ => {}\n+        }\n+    }\n+\n+    pub fn select_closest_line(&mut self, engine_view: &mut EngineViewMut) {\n+        match &mut self.state {\n+            TypewriterState::Modifying {\n+                modify_state,\n+                stroke_key,\n+                cursor,\n+                pen_down: _,\n+            } => {\n+                if let Some(Stroke::TextStroke(ref mut textstroke)) =\n+                    engine_view.store.get_stroke_mut(*stroke_key)\n+                {\n+                    textstroke.move_cursor_line_end(cursor);\n+\n+                    let mut selection_cursor = cursor.clone();\n+                    textstroke.move_cursor_line_start(&mut selection_cursor);\n+\n+                    *modify_state = ModifyState::Selecting {\n+                        mode: SelectionMode::Line(\n+                            selection_cursor.cur_cursor(),\n+                            cursor.cur_cursor(),\n+                        ),\n+                        selection_cursor,\n+                        finished: false,\n+                    };\n+                }\n+            }\n+            _ => {}\n+        }\n+    }\n }\ndiff --git a/crates/rnote-engine/src/strokes/textstroke.rs b/crates/rnote-engine/src/strokes/textstroke.rs\nindex 83816854d5..09d1e910ad 100644\n--- a/crates/rnote-engine/src/strokes/textstroke.rs\n+++ b/crates/rnote-engine/src/strokes/textstroke.rs\n@@ -840,6 +840,22 @@ impl TextStroke {\n         current_char_index\n     }\n \n+    fn get_prev_word_boundary_index(&self, current_char_index: usize) -> usize {\n+        for (start_index, word) in self.text.unicode_word_indices().rev() {\n+            let end_index = start_index + word.len();\n+\n+            if end_index < current_char_index {\n+                return end_index;\n+            }\n+\n+            if start_index < current_char_index {\n+                return start_index;\n+            }\n+        }\n+\n+        current_char_index\n+    }\n+\n     fn get_next_word_end_index(&self, current_char_index: usize) -> usize {\n         for (start_index, word) in self.text.unicode_word_indices() {\n             let end_index = start_index + word.len();\n@@ -852,6 +868,22 @@ impl TextStroke {\n         current_char_index\n     }\n \n+    fn get_next_word_boundary_index(&self, current_char_index: usize) -> usize {\n+        for (start_index, word) in self.text.unicode_word_indices() {\n+            if start_index >= current_char_index {\n+                return start_index;\n+            }\n+\n+            let end_index = start_index + word.len();\n+\n+            if end_index >= current_char_index {\n+                return end_index;\n+            }\n+        }\n+\n+        current_char_index\n+    }\n+\n     pub fn move_cursor_back(&self, cursor: &mut GraphemeCursor) {\n         // Cant fail, we are providing the entire text\n         cursor.prev_boundary(&self.text, 0).unwrap();\n@@ -866,10 +898,18 @@ impl TextStroke {\n         cursor.set_cursor(self.get_prev_word_start_index(cursor.cur_cursor()));\n     }\n \n+    pub fn move_cursor_word_boundary_back(&self, cursor: &mut GraphemeCursor) {\n+        cursor.set_cursor(self.get_prev_word_boundary_index(cursor.cur_cursor()));\n+    }\n+\n     pub fn move_cursor_word_forward(&self, cursor: &mut GraphemeCursor) {\n         cursor.set_cursor(self.get_next_word_end_index(cursor.cur_cursor()));\n     }\n \n+    pub fn move_cursor_word_boundary_forward(&self, cursor: &mut GraphemeCursor) {\n+        cursor.set_cursor(self.get_next_word_boundary_index(cursor.cur_cursor()));\n+    }\n+\n     pub fn move_cursor_text_start(&self, cursor: &mut GraphemeCursor) {\n         cursor.set_cursor(0);\n     }\ndiff --git a/crates/rnote-ui/src/canvas/input.rs b/crates/rnote-ui/src/canvas/input.rs\nindex 8775ec852b..897fe2e6db 100644\n--- a/crates/rnote-ui/src/canvas/input.rs\n+++ b/crates/rnote-ui/src/canvas/input.rs\n@@ -300,7 +300,7 @@ fn trace_gdk_event(event: &gdk::Event) {\n }\n \n /// Returns true if input should be rejected\n-fn reject_pointer_input(event: &gdk::Event, touch_drawing: bool) -> bool {\n+pub(crate) fn reject_pointer_input(event: &gdk::Event, touch_drawing: bool) -> bool {\n     if touch_drawing {\n         if event.device().unwrap().num_touches() > 1 {\n             return true;\ndiff --git a/crates/rnote-ui/src/canvas/mod.rs b/crates/rnote-ui/src/canvas/mod.rs\nindex 11bee39b2b..a6f6054b1e 100644\n--- a/crates/rnote-ui/src/canvas/mod.rs\n+++ b/crates/rnote-ui/src/canvas/mod.rs\n@@ -6,6 +6,7 @@ mod widgetflagsboxed;\n \n // Re-exports\n pub(crate) use canvaslayout::RnCanvasLayout;\n+pub(crate) use input::reject_pointer_input;\n pub(crate) use widgetflagsboxed::WidgetFlagsBoxed;\n \n // Imports\ndiff --git a/crates/rnote-ui/src/canvaswrapper.rs b/crates/rnote-ui/src/canvaswrapper.rs\nindex e94ad1bdba..3a3532f325 100644\n--- a/crates/rnote-ui/src/canvaswrapper.rs\n+++ b/crates/rnote-ui/src/canvaswrapper.rs\n@@ -1,9 +1,9 @@\n // Imports\n-use crate::{RnAppWindow, RnCanvas, RnContextMenu};\n+use crate::{canvas::reject_pointer_input, RnAppWindow, RnCanvas, RnContextMenu};\n use gtk4::{\n     gdk, glib, glib::clone, graphene, prelude::*, subclass::prelude::*, CompositeTemplate,\n     CornerType, EventControllerMotion, EventControllerScroll, EventControllerScrollFlags,\n-    EventSequenceState, GestureDrag, GestureLongPress, GestureZoom, PropagationPhase,\n+    EventSequenceState, GestureClick, GestureDrag, GestureLongPress, GestureZoom, PropagationPhase,\n     ScrolledWindow, Widget,\n };\n use once_cell::sync::Lazy;\n@@ -39,6 +39,7 @@ mod imp {\n         pub(crate) pointer_motion_controller: EventControllerMotion,\n         pub(crate) canvas_drag_gesture: GestureDrag,\n         pub(crate) canvas_zoom_gesture: GestureZoom,\n+        pub(crate) canvas_multi_press_gesture: GestureClick,\n         pub(crate) canvas_zoom_scroll_controller: EventControllerScroll,\n         pub(crate) canvas_mouse_drag_middle_gesture: GestureDrag,\n         pub(crate) canvas_alt_drag_gesture: GestureDrag,\n@@ -75,6 +76,13 @@ mod imp {\n                 .propagation_phase(PropagationPhase::Capture)\n                 .build();\n \n+            let canvas_multi_press_gesture = GestureClick::builder()\n+                .name(\"canvas_multi_press_gesture\")\n+                .button(gdk::BUTTON_PRIMARY)\n+                .exclusive(true)\n+                .propagation_phase(PropagationPhase::Capture)\n+                .build();\n+\n             let canvas_zoom_scroll_controller = EventControllerScroll::builder()\n                 .name(\"canvas_zoom_scroll_controller\")\n                 .propagation_phase(PropagationPhase::Bubble)\n@@ -130,6 +138,7 @@ mod imp {\n                 pointer_motion_controller,\n                 canvas_drag_gesture,\n                 canvas_zoom_gesture,\n+                canvas_multi_press_gesture,\n                 canvas_zoom_scroll_controller,\n                 canvas_mouse_drag_middle_gesture,\n                 canvas_alt_drag_gesture,\n@@ -171,6 +180,8 @@ mod imp {\n                 .add_controller(self.canvas_drag_gesture.clone());\n             self.scroller\n                 .add_controller(self.canvas_zoom_gesture.clone());\n+            self.scroller\n+                .add_controller(self.canvas_multi_press_gesture.clone());\n             self.scroller\n                 .add_controller(self.canvas_zoom_scroll_controller.clone());\n             self.scroller\n@@ -642,6 +653,40 @@ mod imp {\n                 ));\n             }\n \n+            // Double press to select word, triple press to select line\n+            {\n+                self.canvas_multi_press_gesture.connect_pressed(clone!(\n+                    #[weak(rename_to=canvaswrapper)]\n+                    obj,\n+                    move |signal, n_press, _, _| {\n+                        // cycle through 0, 1, 2 - single, double, triple press\n+                        let action = (n_press - 1) % 3;\n+\n+                        if action <= 0 {\n+                            // Single press or invalid press count\n+                            return;\n+                        }\n+\n+                        let canvas = canvaswrapper.canvas();\n+\n+                        if signal.current_event().is_none_or(|event| {\n+                            reject_pointer_input(&event, canvas.touch_drawing())\n+                        }) {\n+                            // Reject certain kinds of input (same behavior as canvas)\n+                            return;\n+                        }\n+\n+                        match action {\n+                            // Double press\n+                            1 => canvas.engine_mut().text_select_closest_word(),\n+                            // Triple press\n+                            2 => canvas.engine_mut().text_select_closest_line(),\n+                            _ => unreachable!(),\n+                        }\n+                    }\n+                ));\n+            }\n+\n             // Zoom with alt + shift + drag\n             {\n                 let zoom_begin = Rc::new(Cell::new(1_f64));\n", "test_patch": "", "problem_statement": "Double click to select word and sentences in the text?\n**Please submit only one feature request in one issue!**  \r\nIf it is UX / UI related, don't post how a single aspect of the UI should be changed, rather in which use-case you felt the UI is lacking or should be improved.\r\n\r\n**Is your feature request related to a problem?**\r\nWhen selecting the texts typed in the note, it is not possible to use double click/triple click to select the entire word or all texts in a sentence, like other programs in Windows. \r\n\r\n**Describe the solution you'd like**  \r\nDouble clicking/triple clicking would select the word or sentence where the cursor is located. \r\n\r\n**Describe alternatives you've considered**  \r\nHolding down the left mouse button and move the mouse, but this is not as convenient.\r\n\r\n**Additional context**  \r\nThanks for making this note-taking tool! It would be much more easy to type with quick selection implemented.\r\n\n", "hints_text": "", "created_at": "2024-10-29 19:11:06", "merge_commit_sha": "58087a1909b09b8f7f904e179de08956080c94fe", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Tests', '.github/workflows/ci.yml']", "['check', '.github/workflows/ci.yml']"]]}
{"repo": "databendlabs/databend", "instance_id": "databendlabs__databend-17076", "base_commit": "27eeef86ecd1a6f78434b9a3aacc386692c21ed3", "patch": "diff --git a/src/query/ast/src/ast/statements/table.rs b/src/query/ast/src/ast/statements/table.rs\nindex 7efc87fa6b8fa..34a89d4a83d09 100644\n--- a/src/query/ast/src/ast/statements/table.rs\n+++ b/src/query/ast/src/ast/statements/table.rs\n@@ -745,6 +745,21 @@ impl Display for Engine {\n     }\n }\n \n+impl From<&str> for Engine {\n+    fn from(s: &str) -> Self {\n+        match s.to_lowercase().as_str() {\n+            \"null\" => Engine::Null,\n+            \"memory\" => Engine::Memory,\n+            \"fuse\" => Engine::Fuse,\n+            \"view\" => Engine::View,\n+            \"random\" => Engine::Random,\n+            \"iceberg\" => Engine::Iceberg,\n+            \"delta\" => Engine::Delta,\n+            _ => unreachable!(\"invalid engine: {}\", s),\n+        }\n+    }\n+}\n+\n #[derive(Debug, Clone, PartialEq, Eq, Drive, DriveMut)]\n pub enum CompactTarget {\n     Block,\ndiff --git a/src/query/service/src/interpreters/common/table_option_validation.rs b/src/query/service/src/interpreters/common/table_option_validation.rs\nindex a36a217ac8b0e..b16132635d8c0 100644\n--- a/src/query/service/src/interpreters/common/table_option_validation.rs\n+++ b/src/query/service/src/interpreters/common/table_option_validation.rs\n@@ -17,6 +17,7 @@ use std::collections::HashSet;\n use std::sync::LazyLock;\n \n use chrono::Duration;\n+use databend_common_ast::ast::Engine;\n use databend_common_exception::ErrorCode;\n use databend_common_expression::TableSchemaRef;\n use databend_common_io::constants::DEFAULT_BLOCK_MAX_ROWS;\n@@ -45,7 +46,7 @@ use databend_storages_common_table_meta::table::OPT_KEY_TEMP_PREFIX;\n use log::error;\n \n /// Table option keys that can occur in 'create table statement'.\n-pub static CREATE_TABLE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+pub static CREATE_FUSE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n     let mut r = HashSet::new();\n     r.insert(FUSE_OPT_KEY_ROW_PER_PAGE);\n     r.insert(FUSE_OPT_KEY_BLOCK_PER_SEGMENT);\n@@ -64,12 +65,32 @@ pub static CREATE_TABLE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new\n \n     r.insert(OPT_KEY_ENGINE);\n \n+    r.insert(OPT_KEY_CONNECTION_NAME);\n+\n+    r.insert(\"transient\");\n+    r.insert(OPT_KEY_TEMP_PREFIX);\n+    r\n+});\n+\n+pub static CREATE_LAKE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n     r.insert(OPT_KEY_LOCATION);\n     r.insert(OPT_KEY_CONNECTION_NAME);\n+    r\n+});\n \n+pub static CREATE_RANDOM_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n     r.insert(OPT_KEY_RANDOM_SEED);\n+    r\n+});\n \n-    r.insert(\"transient\");\n+pub static CREATE_MEMORY_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n+    r.insert(OPT_KEY_DATABASE_ID);\n     r.insert(OPT_KEY_TEMP_PREFIX);\n     r\n });\n@@ -85,8 +106,16 @@ pub static UNSET_TABLE_OPTIONS_WHITE_LIST: LazyLock<HashSet<&'static str>> = Laz\n     r\n });\n \n-pub fn is_valid_create_opt<S: AsRef<str>>(opt_key: S) -> bool {\n-    CREATE_TABLE_OPTIONS.contains(opt_key.as_ref().to_lowercase().as_str())\n+pub fn is_valid_create_opt<S: AsRef<str>>(opt_key: S, engine: &Engine) -> bool {\n+    let opt_key = opt_key.as_ref().to_lowercase();\n+    let opt_key = opt_key.as_str();\n+    match engine {\n+        Engine::Fuse => CREATE_FUSE_OPTIONS.contains(opt_key),\n+        Engine::Iceberg | Engine::Delta => CREATE_LAKE_OPTIONS.contains(&opt_key),\n+        Engine::Random => CREATE_RANDOM_OPTIONS.contains(&opt_key),\n+        Engine::Memory => CREATE_MEMORY_OPTIONS.contains(&opt_key),\n+        Engine::Null | Engine::View => opt_key == OPT_KEY_ENGINE,\n+    }\n }\n \n pub fn is_valid_block_per_segment(\ndiff --git a/src/query/service/src/interpreters/interpreter_table_create.rs b/src/query/service/src/interpreters/interpreter_table_create.rs\nindex f15937f892ccd..173d891b5d0cc 100644\n--- a/src/query/service/src/interpreters/interpreter_table_create.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_create.rs\n@@ -402,7 +402,7 @@ impl CreateTableInterpreter {\n \n         for table_option in table_meta.options.iter() {\n             let key = table_option.0.to_lowercase();\n-            if !is_valid_create_opt(&key) {\n+            if !is_valid_create_opt(&key, &self.plan.engine) {\n                 error!(\"invalid opt for fuse table in create table statement\");\n                 return Err(ErrorCode::TableOptionInvalid(format!(\n                     \"table option {key} is invalid for create table statement\",\ndiff --git a/src/query/service/src/interpreters/interpreter_table_set_options.rs b/src/query/service/src/interpreters/interpreter_table_set_options.rs\nindex 9aa7e26449fc4..2147a607ae162 100644\n--- a/src/query/service/src/interpreters/interpreter_table_set_options.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_set_options.rs\n@@ -15,6 +15,7 @@\n use std::collections::HashMap;\n use std::sync::Arc;\n \n+use databend_common_ast::ast::Engine;\n use databend_common_catalog::table::TableExt;\n use databend_common_exception::ErrorCode;\n use databend_common_exception::Result;\n@@ -101,9 +102,17 @@ impl Interpreter for SetOptionsInterpreter {\n                 OPT_KEY_CLUSTER_TYPE\n             )));\n         }\n+        let catalog = self.ctx.get_catalog(self.plan.catalog.as_str()).await?;\n+        let database = self.plan.database.as_str();\n+        let table_name = self.plan.table.as_str();\n+        let table = catalog\n+            .get_table(&self.ctx.get_tenant(), database, table_name)\n+            .await?;\n+\n         for table_option in self.plan.set_options.iter() {\n             let key = table_option.0.to_lowercase();\n-            if !is_valid_create_opt(&key) {\n+            let engine = Engine::from(table.engine());\n+            if !is_valid_create_opt(&key, &engine) {\n                 error!(\"{}\", &error_str);\n                 return Err(ErrorCode::TableOptionInvalid(format!(\n                     \"table option {key} is invalid for alter table statement\",\n@@ -111,12 +120,6 @@ impl Interpreter for SetOptionsInterpreter {\n             }\n             options_map.insert(key, Some(table_option.1.clone()));\n         }\n-        let catalog = self.ctx.get_catalog(self.plan.catalog.as_str()).await?;\n-        let database = self.plan.database.as_str();\n-        let table_name = self.plan.table.as_str();\n-        let table = catalog\n-            .get_table(&self.ctx.get_tenant(), database, table_name)\n-            .await?;\n \n         let table_version = table.get_table_info().ident.seq;\n         if let Some(value) = self.plan.set_options.get(OPT_KEY_CHANGE_TRACKING) {\ndiff --git a/src/query/service/src/interpreters/interpreter_table_show_create.rs b/src/query/service/src/interpreters/interpreter_table_show_create.rs\nindex 8021e1d6f2ac9..1b69d3ef4f815 100644\n--- a/src/query/service/src/interpreters/interpreter_table_show_create.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_show_create.rs\n@@ -281,7 +281,7 @@ impl ShowCreateTableInterpreter {\n \n         if engine != \"ICEBERG\" && engine != \"DELTA\" {\n             if let Some(sp) = &table_info.meta.storage_params {\n-                table_create_sql.push_str(format!(\" LOCATION = '{}'\", sp).as_str());\n+                table_create_sql.push_str(format!(\" '{}' \", sp).as_str());\n             }\n         }\n \n", "test_patch": "diff --git a/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test b/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\nindex 01e23ba86fae3..fa0b044115b69 100644\n--- a/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\n+++ b/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\n@@ -303,6 +303,15 @@ create table t(a int) x=x\n statement error 1301\n create table t(a int) external_location='xxx'\n \n+statement error 1301\n+create table t(a int) location='xxx'\n+\n+statement error 1301\n+create table t(a int) seed='123'\n+\n+statement ok\n+create table t_random(a int) engine=RANDOM seed='123'\n+\n statement error 1301\n create table t(a int) snapshot_loc='xxx'\n \ndiff --git a/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test b/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\nindex 2020b3db6d849..558d5f33b593b 100644\n--- a/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\n+++ b/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\n@@ -53,7 +53,7 @@ CREATE TABLE test.d (a int not null) ENGINE=FUSE 'fs:///tmp/load/files/' CONNECT\n query TT\n SHOW CREATE TABLE `test`.`d`\n ----\n-d CREATE TABLE d ( a INT NOT NULL ) ENGINE=FUSE CLUSTER BY linear(a, a % 3) CLUSTER_TYPE='linear' COMPRESSION='lz4' STORAGE_FORMAT='parquet' LOCATION = 'fs | root=/tmp/load/files/'\n+d CREATE TABLE d ( a INT NOT NULL ) ENGINE=FUSE CLUSTER BY linear(a, a % 3) CLUSTER_TYPE='linear' COMPRESSION='lz4' STORAGE_FORMAT='parquet' 'fs | root=/tmp/load/files/'\n \n query TT\n SHOW CREATE TABLE `test`.`c`\n", "problem_statement": "bug:  `LOCATION` of `CREATE TABLE` does NOT work as expected \n**Summary**\r\n\r\n\r\nNot sure if this is a documentation bug or a implementation bug:\r\n\r\nwhen creating a table with external location, following the official doc:\r\n\r\nhttps://docs.databend.com/sql/sql-commands/ddl/table/ddl-create-table-external-location#syntax\r\n\r\n> CREATE TABLE [IF NOT EXISTS] [db.]table_name (\r\n>     <column_name> <data_type> [NOT NULL | NULL] [{ DEFAULT <expr> }],\r\n>     <column_name> <data_type> [NOT NULL | NULL] [{ DEFAULT <expr> }],\r\n>     ...\r\n> )\r\n> LOCATION = 's3://<bucket>/[<path>]'\r\n> CONNECTION = (\r\n>     ENDPOINT_URL = 'https://<endpoint-URL>'\r\n>     ACCESS_KEY_ID = '<your-access-key-ID>'\r\n>     SECRET_ACCESS_KEY = '<your-secret-access-key>'\r\n>     REGION = '<region-name>'\r\n>     ENABLE_VIRTUAL_HOST_STYLE = 'true' | 'false'\r\n> )\r\n> |\r\n> CONNECTION = (\r\n>     CONNECTION_NAME = '<your-connection-name>'\r\n> );\r\n\r\nA table of invalid location could be created successfully, mutation and query are all work,\r\n\r\n***But the table data is NOT stored at the specified location***.\r\n\r\n\r\nAlso: In section `Create a Table with External Location`\r\nhttps://docs.databend.com/sql/sql-commands/ddl/table/ddl-create-table-external-location#create-a-table-with-external-location\r\n\r\nA different syntax is used:\r\n\r\n~~~\r\n-- Create a table named `mytable` and specify the location `s3://testbucket/admin/data/` for the data storage\r\nCREATE TABLE mytable (\r\n  a INT\r\n)\r\n's3://testbucket/admin/data/'\r\nCONNECTION = (\r\n  ACCESS_KEY_ID = '<your_aws_key_id>',\r\n  SECRET_ACCESS_KEY = '<your_aws_secret_key>',\r\n  ENDPOINT_URL = 'https://s3.amazonaws.com'\r\n);\r\n~~~\r\n\r\n\r\n\r\n\r\n**Reporduce:**\r\n\r\nfor example, a table of with invalid s3 location can be created (incorrectly):\r\n\r\n~~~\r\n\r\nmysql> select version();\r\n+----------------------------------------------------------------------------------------+\r\n| version()                                                                              |\r\n+----------------------------------------------------------------------------------------+\r\n| 8.0.26-v1.2.673-nightly-f777d195be(rust-1.85.0-nightly-2024-12-18T05:01:23.526505965Z) |\r\n+----------------------------------------------------------------------------------------+\r\n1 row in set (0.01 sec)\r\nRead 1 rows, 1.00 B in 0.007 sec., 153.02 rows/sec., 153.02 B/sec.\r\n\r\nmysql> select * from system.configs where \"group\" = 'storage' and name = 'type';\r\n+---------+------+-------+-------------+\r\n| group   | name | value | description |\r\n+---------+------+-------+-------------+\r\n| storage | type | fs    |             |\r\n+---------+------+-------+-------------+\r\n1 row in set (0.03 sec)\r\nRead 184 rows, 12.05 KiB in 0.008 sec., 21.89 thousand rows/sec., 1.40 MiB/sec.\r\n\r\n\r\nmysql> create table t_location (c int) LOCATION = 's3 | bucket=non-exist-bucket,root=/,endpoint=https://s3.amazonaws.com';\r\nQuery OK, 0 rows affected (0.05 sec)\r\n\r\nmysql> insert into t_location values(1);\r\n+-------------------------+\r\n| number of rows inserted |\r\n+-------------------------+\r\n|                       1 |\r\n+-------------------------+\r\n1 row in set (0.21 sec)\r\nRead 1 rows, 5.00 B in 0.144 sec., 6.96 rows/sec., 34.81 B/sec.\r\n\r\n\r\nmysql> select * from t_location;\r\n+------+\r\n| c    |\r\n+------+\r\n|    1 |\r\n+------+\r\n1 row in set (0.11 sec)\r\nRead 1 rows, 5.00 B in 0.025 sec., 40.12 rows/sec., 200.61 B/sec.\r\n\r\n~~~\n", "hints_text": "", "created_at": "2024-12-18 14:24:43", "merge_commit_sha": "19012a581ec0283d9a2d253199424758672d4828", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['management_mode', '.github/workflows/dev.yml']", "['cluster (query, 4c16g, mysql)', '.github/workflows/dev.yml']"], ["['check', '.github/workflows/dev.yml']", "['ee (native)', '.github/workflows/dev.yml']"], ["['description', '.github/workflows/pr.yml']", "['cluster (tpch, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['test_compat_fuse', '.github/workflows/dev.yml']", "['cluster (crdb, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_metactl', '.github/workflows/dev.yml']", "['standalone_minio (query, mysql, parquet)', '.github/workflows/dev.yml']"], ["['standalone (tpcds, 2c8g, mysql)', '.github/workflows/dev.yml']", "['standalone (standalone, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['cluster (cluster, 2c8g, mysql)', '.github/workflows/dev.yml']", "['cluster (cluster, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_stateful_iceberg_rest', '.github/workflows/dev.yml']", "['standalone_udf_server', '.github/workflows/dev.yml']"], ["['cluster (base, 2c8g, mysql)', '.github/workflows/dev.yml']", "['test_stateless_cluster', '.github/workflows/dev.yml']"], ["['test_stateless_standalone', '.github/workflows/dev.yml']", "['standalone (standalone, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_ee_standalone_background', '.github/workflows/dev.yml']", "['test_logs', '.github/workflows/dev.yml']"], ["['standalone (query, 4c16g, mysql)', '.github/workflows/dev.yml']", "['cluster (base, 2c8g, http)', '.github/workflows/dev.yml']"], ["['standalone (duckdb, 4c16g, http)', '.github/workflows/dev.yml']", "['cluster (crdb, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['cluster (tpcds, 2c8g, mysql)', '.github/workflows/dev.yml']", "['standalone (tpcds, 2c8g, http)', '.github/workflows/dev.yml']"], ["['standalone (query, 4c16g, http)', '.github/workflows/dev.yml']", "['standalone (tpch, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['title', '.github/workflows/pr.yml']", "['ee (parquet)', '.github/workflows/dev.yml']"], ["['test_compat_meta_meta', '.github/workflows/dev.yml']", "['test_stateful_hive_standalone', '.github/workflows/dev.yml']"], ["['standalone (tpch, 2c8g, http)', '.github/workflows/dev.yml']", "['standalone_no_table_meta_cache (no_table_meta_cache, http)', '.github/workflows/dev.yml']"]]}
{"repo": "libbpf/libbpf-rs", "instance_id": "libbpf__libbpf-rs-890", "base_commit": "6fbd2961cf40596b0b8308b939e153a67830ad05", "patch": "diff --git a/libbpf-rs/src/program.rs b/libbpf-rs/src/program.rs\nindex 700e1ac8..def7b9a9 100644\n--- a/libbpf-rs/src/program.rs\n+++ b/libbpf-rs/src/program.rs\n@@ -104,7 +104,6 @@ impl From<TracepointOpts> for libbpf_sys::bpf_tracepoint_opts {\n     }\n }\n \n-\n /// An immutable parsed but not yet loaded BPF program.\n pub type OpenProgram<'obj> = OpenProgramImpl<'obj>;\n /// A mutable parsed but not yet loaded BPF program.\n@@ -506,7 +505,7 @@ impl From<u32> for ProgramAttachType {\n #[derive(Debug, Default)]\n pub struct Input<'dat> {\n     /// The input context to provide.\n-    pub context_in: Option<&'dat [u8]>,\n+    pub context_in: Option<&'dat mut [u8]>,\n     /// The output context buffer provided to the program.\n     pub context_out: Option<&'dat mut [u8]>,\n     /// Additional data to provide to the program.\n@@ -544,7 +543,6 @@ pub type Program<'obj> = ProgramImpl<'obj>;\n /// A mutable loaded BPF program.\n pub type ProgramMut<'obj> = ProgramImpl<'obj, Mut>;\n \n-\n /// Represents a loaded [`Program`].\n ///\n /// This struct is not safe to clone because the underlying libbpf resource cannot currently\n@@ -1091,6 +1089,7 @@ impl<'obj> ProgramMut<'obj> {\n         let mut opts = unsafe { mem::zeroed::<libbpf_sys::bpf_test_run_opts>() };\n         opts.sz = size_of_val(&opts) as _;\n         opts.ctx_in = context_in\n+            .as_ref()\n             .map(|data| data.as_ptr().cast())\n             .unwrap_or_else(ptr::null);\n         opts.ctx_size_in = context_in.map(|data| data.len() as _).unwrap_or(0);\n", "test_patch": "diff --git a/libbpf-rs/tests/test.rs b/libbpf-rs/tests/test.rs\nindex cd547d4a..97813325 100644\n--- a/libbpf-rs/tests/test.rs\n+++ b/libbpf-rs/tests/test.rs\n@@ -13,6 +13,7 @@ use std::hint;\n use std::io;\n use std::io::Read;\n use std::mem::size_of;\n+use std::mem::size_of_val;\n use std::os::unix::io::AsFd;\n use std::path::Path;\n use std::path::PathBuf;\n@@ -55,7 +56,6 @@ use crate::common::get_test_object;\n use crate::common::get_test_object_path;\n use crate::common::open_test_object;\n \n-\n /// A helper function for instantiating a `RingBuffer` with a callback meant to\n /// be invoked when `action` is executed and that is intended to trigger a write\n /// to said `RingBuffer` from kernel space, which then reads a single `i32` from\n@@ -1994,9 +1994,11 @@ fn test_run_prog_success() {\n \n     let value = 42;\n     let state = bpf_dummy_ops_state { val: value };\n-    let args = [addr_of!(state) as u64];\n+    let mut args = [addr_of!(state) as u64];\n     let input = ProgramInput {\n-        context_in: Some(unsafe { plain::as_bytes(&args) }),\n+        context_in: Some(unsafe {\n+            slice::from_raw_parts_mut(&mut args as *mut _ as *mut u8, size_of_val(&args))\n+        }),\n         ..Default::default()\n     };\n     let output = prog.test_run(input).unwrap();\n", "problem_statement": "Possible mutation of `context_in` constant when using `test_run` in `syscall` methods\nThe field `context_in` in `ProgramInput` structure now looks like this::\r\n```rust\r\nstruct ProgramInput<'dat> {\r\n  context_in: Option<&'dat [u8]>,\r\n  // ...\r\n}\r\n```\r\n\r\nBut this field can be overwritten by kernel when calling `syscall` method from bpf. [File `net/bpf/test_run.c` at line 1567](https://github.com/torvalds/linux/blob/defaf1a2113a22b00dfa1abc0fd2014820eaf065/net/bpf/test_run.c#L1567): kernel is copy back `context_in` to user space after calling `syscall`. And this is the only way to return some data from `syscall` (something bigger than `i32`).\r\n\r\nPerhaps the `context_in` field should have the signature `Option<&'dat mut [u8]>` or special attention should be added to the comments of `test_run` method and `context_in` field.\r\n\r\nExample of valid bpf code:\r\n```c++\r\nSEC(\"syscall\")\r\nint mySyscall(int *ctx) {\r\n  *ctx += *ctx;\r\n  return 0;\r\n}\r\n```\r\nAnd rust with `libbpf-rs`:\r\n```rust\r\nlet value: i32 = 21;\r\nlet mut value = value.to_ne_bytes();\r\n\r\nlet mut input = ProgramInput::default();\r\ninput.context_in = Some(&mut value);\r\n\r\nmySyscall.test_run(input).unwrap();\r\n\r\nlet value = i32::from_ne_bytes(value);\r\nassert_eq!(value, 42);\r\n```\n", "hints_text": "Hi @x1b6e6, yes, that's a fair point.\r\n\r\n> Perhaps the context_in field should have the signature Option<&'dat mut [u8]>\r\n\r\nDo you plan to send a pull request?\n@danielocfb\n\n> Do you plan to send a pull request?\n\nIf changing field signature is an acceptable solution then I can make this PR.", "created_at": "2024-08-06 07:33:39", "merge_commit_sha": "64f558d008db95aa24b311c065668755ea099fb3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build for aarch32', '.github/workflows/test.yml']", "['Build using minimum versions of dependencies', '.github/workflows/test.yml']"], ["['Test [stable, release]', '.github/workflows/test.yml']", "['Run tcp_ca example', '.github/workflows/test.yml']"], ["[\"Build [features = ['static']]\", '.github/workflows/test.yml']", "['Test [stable, dev]', '.github/workflows/test.yml']"]]}
{"repo": "jamesmunns/postcard", "instance_id": "jamesmunns__postcard-188", "base_commit": "7544011da2f80de911cf3d633db131ffa1888d64", "patch": "diff --git a/ci.sh b/ci.sh\nindex 7ca57a7..5b723b1 100755\n--- a/ci.sh\n+++ b/ci.sh\n@@ -29,7 +29,7 @@ cargo_test() {\n   cargo test --all \"$@\"\n }\n \n-cargo_test --features=alloc,experimental-derive,use-std,use-crc,derive\n+cargo_test --features=alloc,experimental-derive,use-std,use-crc,derive,nalgebra-v0_33\n \n # NOTE: we exclude postcard-dyn for these checks because it is std-only\n \ndiff --git a/source/postcard-schema/Cargo.toml b/source/postcard-schema/Cargo.toml\nindex 4a8e7ef..72d1f53 100644\n--- a/source/postcard-schema/Cargo.toml\n+++ b/source/postcard-schema/Cargo.toml\n@@ -68,6 +68,12 @@ path = \"../postcard\"\n version = \"1.0\"\n features = [\"use-std\"]\n \n+[dev-dependencies.nalgebra_v0_33]\n+package = \"nalgebra\"\n+version = \"0.33.0\"\n+default-features = false\n+features = [\"serde-serialize-no-std\"]\n+\n [features]\n default = []\n use-std = [\"serde/std\"]\ndiff --git a/source/postcard-schema/src/impls/nalgebra_v0_33.rs b/source/postcard-schema/src/impls/nalgebra_v0_33.rs\nindex 61de71e..3f9ceb7 100644\n--- a/source/postcard-schema/src/impls/nalgebra_v0_33.rs\n+++ b/source/postcard-schema/src/impls/nalgebra_v0_33.rs\n@@ -18,6 +18,31 @@ where\n {\n     const SCHEMA: &'static NamedType = &NamedType {\n         name: \"nalgebra::Matrix<T, R, C, ArrayStorage<T, R, C>>\",\n-        ty: &DataModelType::Seq(T::SCHEMA),\n+        ty: &DataModelType::Tuple(flatten(&[[T::SCHEMA; R]; C])),\n     };\n }\n+\n+/// Const version of the const-unstable [`<[[T; N]]>::as_flattened()`]\n+const fn flatten<T, const N: usize>(slice: &[[T; N]]) -> &[T] {\n+    const {\n+        assert!(size_of::<T>() != 0);\n+    }\n+    // SAFETY: `self.len() * N` cannot overflow because `self` is\n+    // already in the address space.\n+    let len = unsafe { slice.len().unchecked_mul(N) };\n+    // SAFETY: `[T]` is layout-identical to `[T; N]`\n+    unsafe { core::slice::from_raw_parts(slice.as_ptr().cast(), len) }\n+}\n+\n+#[test]\n+fn flattened() {\n+    type T = nalgebra_v0_33::SMatrix<u8, 3, 3>;\n+    assert_eq!(T::SCHEMA.ty, <[u8; 9]>::SCHEMA.ty);\n+}\n+\n+#[test]\n+fn smoke() {\n+    let x = nalgebra_v0_33::SMatrix::<u8, 3, 3>::new(1, 2, 3, 4, 5, 6, 7, 8, 9);\n+    let y = postcard::to_stdvec(&x).unwrap();\n+    assert_eq!(&[1, 4, 7, 2, 5, 8, 3, 6, 9], y.as_slice());\n+}\n", "test_patch": "", "problem_statement": "postcard-schema: `nalgebra` schema is incorrect\nHey @avsaase - I was wondering how you chose `Seq(T)` for the schema of `nalgebra::Matrix`. A `Seq` is a length prefixed item, so a 3x3 matrix of `u8`s should be 10 bytes. However this test fails:\r\n\r\n```rust\r\nuse nalgebra_v0_33::{Const, Matrix};\r\n#[test]\r\nfn smoke() {\r\n    let x = nalgebra_v0_33::Matrix::<u8, Const<3>, Const<3>, _>::new(\r\n        1, 2, 3,\r\n        4, 5, 6,\r\n        7, 8, 9,\r\n    );\r\n    let y = postcard::to_stdvec(&x).unwrap();\r\n    assert_eq!(&[9, 1, 4, 7, 2, 5, 8, 3, 6, 9], y.as_slice());\r\n}\r\n```\r\n\r\n```txt\r\ntest impls::nalgebra_v0_33::test::smoke ... FAILED\r\n\r\nfailures:\r\n\r\n---- impls::nalgebra_v0_33::test::smoke stdout ----\r\nthread 'impls::nalgebra_v0_33::test::smoke' panicked at source/postcard-schema/src/impls/nalgebra_v0_33.rs:32:9:\r\nassertion `left == right` failed\r\n  left: [9, 1, 4, 7, 2, 5, 8, 3, 6, 9]\r\n right: [1, 4, 7, 2, 5, 8, 3, 6, 9]\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n```\r\n\r\nWas the choice of `Seq(T)` a guess? Or were you basing this on something else? Just making sure I understand the original choice before I change it.\r\n\r\nIt appears to me that the actual serialized form is actually `[u8; 9]`, which is *not* a length prefixed, but is actually modeled as a tuple of size N, which avoids serializing a fixed size len:\r\n\r\n```rust\r\nimpl<T: Schema, const N: usize> Schema for [T; N] {\r\n    const SCHEMA: &'static NamedType = &NamedType {\r\n        name: \"[T; N]\",\r\n        ty: &DataModelType::Tuple(&[T::SCHEMA; N]),\r\n    };\r\n}\r\n```\n", "hints_text": "Hi, I don't recall exactly why I picked a `Seq(T)`. I guess I thought this would be the correct format but I should have added this test myself to confirm. Sorry about that!\nSo it turns out this is very frustrating to fix, this is what I've come up with:\r\n\r\n```rust\r\n#[cfg_attr(docsrs, doc(cfg(feature = \"nalgebra-v0_33\")))]\r\nimpl<T, const R: usize, const C: usize> Schema\r\n    for nalgebra_v0_33::Matrix<\r\n        T,\r\n        nalgebra_v0_33::Const<R>,\r\n        nalgebra_v0_33::Const<C>,\r\n        nalgebra_v0_33::ArrayStorage<T, R, C>,\r\n    >\r\nwhere\r\n    T: Schema + nalgebra_v0_33::Scalar,\r\n{\r\n    /// Warning! This is not TECHNICALLY correct. nalgebra actually serializes the\r\n    /// ArrayStorage as `[T; R * C]`, however there is no way to express this below,\r\n    /// as we cannot use generics from the outer context in const context to do\r\n    /// what we really want here, which is `<[T; R * C]>::SCHEMA.ty`. For the\r\n    /// purposes of postcard, these two are actually equivalent with respect to\r\n    /// size and other meaning, but crates like `postcard-dyn` may make this\r\n    /// difference visible, and give the user the nested arrays instead of the\r\n    /// flat array.\r\n    const SCHEMA: &'static NamedType = &NamedType {\r\n        name: \"nalgebra::Matrix<T, R, C, ArrayStorage<T, R, C>>\",\r\n        ty: <[[T; R]; C]>::SCHEMA.ty,\r\n    };\r\n}\r\n```\r\n\r\nThis will need to be a breaking change to `postcard-schema`, since changing the schema will break users of postcard-rpc (if they update the HOST but not the MCU, then they will end up unable to use endpoints that contain nalgebra types).\r\n\r\nI definitely need to raise the bar for required testing to add schema types!\r\n\r\nAlso no worries @avsaase, I made the same mistake with `Uuid` before I released, it's pretty annoying that it is necessary to do this manually.\r\n\r\nSo many things to learn :)\n> ```rust\r\n>     /// Warning! This is not TECHNICALLY correct. nalgebra actually serializes the\r\n>     /// ArrayStorage as `[T; R * C]`, however there is no way to express this below,\r\n>     /// as we cannot use generics from the outer context in const context to do\r\n>     /// what we really want here, which is `<[T; R * C]>::SCHEMA.ty`. For the\r\n>     /// purposes of postcard, these two are actually equivalent with respect to\r\n>     /// size and other meaning, but crates like `postcard-dyn` may make this\r\n>     /// difference visible, and give the user the nested arrays instead of the\r\n>     /// flat array.\r\n>     const SCHEMA: &'static NamedType = &NamedType {\r\n>         name: \"nalgebra::Matrix<T, R, C, ArrayStorage<T, R, C>>\",\r\n>         ty: <[[T; R]; C]>::SCHEMA.ty,\r\n>     };\r\n> ```\r\n\r\nThis gets to a question about what `Schema` should mean:\r\n- Should `Schema` promise to mirror the exact sequence of serializer calls made by the `Serialize` implementation of a type (call this a \"perfect schema\")? In this case, the implementation should use `[T; R * C]` once this is possible to compile.\r\n- Or, should `Schema` only promise that the schema is equivalent to the type's \"perfect schema\" **given postcard's serialization format**. In this case, `[[T; R]; C]` is a correct implementation, and may be more intuitive because it mirrors the user-facing structure of `ArrayStorage<T, R, C>`.\r\n  \r\nFor use-cases involving `postcard-dyn`, I could see it being easier to work with matrices in the array of columns form rather than the flattened form. Round-trips from bytes through `postcard-dyn` back to bytes would still work, but the looser contract for Schema would mean that paths like [`T` -> bytes -> `postcard-dyn` -> json -> `T`] wouldn't work", "created_at": "2024-11-24 11:57:53", "merge_commit_sha": "fae27b3d114cd214bea6f83295ab880c541cb8d9", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": []}
{"repo": "rustls/rustls", "instance_id": "rustls__rustls-2097", "base_commit": "cc403427b0dccdb46e6557d0c762e508112d705c", "patch": "diff --git a/rustls/src/msgs/persist.rs b/rustls/src/msgs/persist.rs\nindex 775123b7462..5a8998fb551 100644\n--- a/rustls/src/msgs/persist.rs\n+++ b/rustls/src/msgs/persist.rs\n@@ -117,6 +117,12 @@ impl Tls13ClientSessionValue {\n         self.common.epoch -= delta as u64;\n     }\n \n+    #[doc(hidden)]\n+    /// Test only: replace `max_early_data_size` with `new`\n+    pub fn _private_set_max_early_data_size(&mut self, new: u32) {\n+        self.max_early_data_size = new;\n+    }\n+\n     pub fn set_quic_params(&mut self, quic_params: &[u8]) {\n         self.quic_params = PayloadU16(quic_params.to_vec());\n     }\ndiff --git a/rustls/src/server/server_conn.rs b/rustls/src/server/server_conn.rs\nindex 515b312d6b6..027768d5948 100644\n--- a/rustls/src/server/server_conn.rs\n+++ b/rustls/src/server/server_conn.rs\n@@ -988,7 +988,10 @@ impl State<ServerConnectionData> for Accepting {\n \n pub(super) enum EarlyDataState {\n     New,\n-    Accepted(ChunkVecBuffer),\n+    Accepted {\n+        received: ChunkVecBuffer,\n+        left: usize,\n+    },\n     Rejected,\n }\n \n@@ -1004,12 +1007,15 @@ impl EarlyDataState {\n     }\n \n     pub(super) fn accept(&mut self, max_size: usize) {\n-        *self = Self::Accepted(ChunkVecBuffer::new(Some(max_size)));\n+        *self = Self::Accepted {\n+            received: ChunkVecBuffer::new(Some(max_size)),\n+            left: max_size,\n+        };\n     }\n \n     #[cfg(feature = \"std\")]\n     fn was_accepted(&self) -> bool {\n-        matches!(self, Self::Accepted(_))\n+        matches!(self, Self::Accepted { .. })\n     }\n \n     pub(super) fn was_rejected(&self) -> bool {\n@@ -1018,7 +1024,9 @@ impl EarlyDataState {\n \n     fn pop(&mut self) -> Option<Vec<u8>> {\n         match self {\n-            Self::Accepted(ref mut received) => received.pop(),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.pop(),\n             _ => None,\n         }\n     }\n@@ -1026,7 +1034,9 @@ impl EarlyDataState {\n     #[cfg(feature = \"std\")]\n     fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read(buf),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read(buf),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1034,7 +1044,9 @@ impl EarlyDataState {\n     #[cfg(read_buf)]\n     fn read_buf(&mut self, cursor: core::io::BorrowedCursor<'_>) -> io::Result<()> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read_buf(cursor),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read_buf(cursor),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1042,8 +1054,12 @@ impl EarlyDataState {\n     pub(super) fn take_received_plaintext(&mut self, bytes: Payload<'_>) -> bool {\n         let available = bytes.bytes().len();\n         match self {\n-            Self::Accepted(ref mut received) if received.apply_limit(available) == available => {\n+            Self::Accepted {\n+                ref mut received,\n+                ref mut left,\n+            } if received.apply_limit(available) == available && available <= *left => {\n                 received.append(bytes.into_vec());\n+                *left -= available;\n                 true\n             }\n             _ => false,\n@@ -1055,7 +1071,12 @@ impl Debug for EarlyDataState {\n     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {\n         match self {\n             Self::New => write!(f, \"EarlyDataState::New\"),\n-            Self::Accepted(buf) => write!(f, \"EarlyDataState::Accepted({})\", buf.len()),\n+            Self::Accepted { received, left } => write!(\n+                f,\n+                \"EarlyDataState::Accepted {{ received: {}, left: {} }}\",\n+                received.len(),\n+                left\n+            ),\n             Self::Rejected => write!(f, \"EarlyDataState::Rejected\"),\n         }\n     }\n", "test_patch": "diff --git a/rustls/tests/api.rs b/rustls/tests/api.rs\nindex d012226b009..7a439f2c06f 100644\n--- a/rustls/tests/api.rs\n+++ b/rustls/tests/api.rs\n@@ -3877,6 +3877,7 @@ enum ClientStorageOp {\n struct ClientStorage {\n     storage: Arc<dyn rustls::client::ClientSessionStore>,\n     ops: Mutex<Vec<ClientStorageOp>>,\n+    alter_max_early_data_size: Option<(u32, u32)>,\n }\n \n impl ClientStorage {\n@@ -3884,9 +3885,14 @@ impl ClientStorage {\n         Self {\n             storage: Arc::new(rustls::client::ClientSessionMemoryCache::new(1024)),\n             ops: Mutex::new(Vec::new()),\n+            alter_max_early_data_size: None,\n         }\n     }\n \n+    fn alter_max_early_data_size(&mut self, expected: u32, altered: u32) {\n+        self.alter_max_early_data_size = Some((expected, altered));\n+    }\n+\n     #[cfg(feature = \"tls12\")]\n     fn ops(&self) -> Vec<ClientStorageOp> {\n         self.ops.lock().unwrap().clone()\n@@ -3963,8 +3969,13 @@ impl rustls::client::ClientSessionStore for ClientStorage {\n     fn insert_tls13_ticket(\n         &self,\n         server_name: ServerName<'static>,\n-        value: rustls::client::Tls13ClientSessionValue,\n+        mut value: rustls::client::Tls13ClientSessionValue,\n     ) {\n+        if let Some((expected, desired)) = self.alter_max_early_data_size {\n+            assert_eq!(value.max_early_data_size(), expected);\n+            value._private_set_max_early_data_size(desired);\n+        }\n+\n         self.ops\n             .lock()\n             .unwrap()\n@@ -4215,6 +4226,159 @@ fn early_data_can_be_rejected_by_server() {\n     assert!(!client.is_early_data_accepted());\n }\n \n+#[test]\n+fn early_data_is_limited_on_client() {\n+    let (client_config, server_config) = early_data_configs();\n+\n+    // warm up\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    do_handshake(&mut client, &mut server);\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        1234\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 1234 + 1])\n+            .unwrap(),\n+        1234\n+    );\n+    do_handshake(&mut client, &mut server);\n+\n+    let mut received_early_data = [0u8; 1234];\n+    assert_eq!(\n+        server\n+            .early_data()\n+            .expect(\"early_data didn't happen\")\n+            .read(&mut received_early_data)\n+            .expect(\"early_data failed unexpectedly\"),\n+        1234\n+    );\n+    assert_eq!(&received_early_data[..], [0xaa; 1234]);\n+}\n+\n+fn early_data_configs_allowing_client_to_send_excess_data() -> (Arc<ClientConfig>, Arc<ServerConfig>)\n+{\n+    let (client_config, server_config) = early_data_configs();\n+\n+    // adjust client session storage to corrupt received max_early_data_size\n+    let mut client_config = Arc::into_inner(client_config).unwrap();\n+    let mut storage = ClientStorage::new();\n+    storage.alter_max_early_data_size(1234, 2024);\n+    client_config.resumption = Resumption::store(Arc::new(storage));\n+    let client_config = Arc::new(client_config);\n+\n+    // warm up\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    do_handshake(&mut client, &mut server);\n+    (client_config, server_config)\n+}\n+\n+#[test]\n+fn server_detects_excess_early_data() {\n+    let (client_config, server_config) = early_data_configs_allowing_client_to_send_excess_data();\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        2024\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 2024])\n+            .unwrap(),\n+        2024\n+    );\n+    assert_eq!(\n+        do_handshake_until_error(&mut client, &mut server),\n+        Err(ErrorFromPeer::Server(Error::PeerMisbehaved(\n+            PeerMisbehaved::TooMuchEarlyDataReceived\n+        ))),\n+    );\n+}\n+\n+// regression test for https://github.com/rustls/rustls/issues/2096\n+#[test]\n+fn server_detects_excess_streamed_early_data() {\n+    let (client_config, server_config) = early_data_configs_allowing_client_to_send_excess_data();\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        2024\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 1024])\n+            .unwrap(),\n+        1024\n+    );\n+    transfer(&mut client, &mut server);\n+    server.process_new_packets().unwrap();\n+\n+    let mut received_early_data = [0u8; 1024];\n+    assert_eq!(\n+        server\n+            .early_data()\n+            .expect(\"early_data didn't happen\")\n+            .read(&mut received_early_data)\n+            .expect(\"early_data failed unexpectedly\"),\n+        1024\n+    );\n+    assert_eq!(&received_early_data[..], [0xaa; 1024]);\n+\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xbb; 1000])\n+            .unwrap(),\n+        1000\n+    );\n+    transfer(&mut client, &mut server);\n+    assert_eq!(\n+        server.process_new_packets(),\n+        Err(Error::PeerMisbehaved(\n+            PeerMisbehaved::TooMuchEarlyDataReceived\n+        ))\n+    );\n+}\n+\n mod test_quic {\n     use rustls::quic::{self, ConnectionCommon};\n \n", "problem_statement": "client can send more TLS1.3 early data than maximum if early data is read\nReported originally by @tahmid-23 :\r\n\r\n---\r\n\r\n### Summary\r\nReading early data on the server reduces the tracked received early data, which allows the client to send more early data than specified by `max_early_data_size`.\r\n\r\n### Details\r\n`ChunkVecBuffer` pops/shrinks chunks as it is read. In `EarlyDataState::take_received_plaintext`, it uses the total length of the `ChunkVecBuffer` to determine whether or not more early data can be accepted. However, if the server reads the early data, the `ChunkVecBuffer` will pop the data, thus reducing the perceived amount of early data that can be read.\r\n\r\n### PoC\r\nAdding this test to `rustls/tests/api.rs` demonstrates that this is possible.\r\nIn order for this to work, the client must behave maliciously. This can be achieved by making `rustls::client::client_conn::EarlyData::check_write_opt()` always return all requested bytes.\r\n```rs\r\n#[test]\r\nfn malicious_early_data_client() {\r\n    let (client_config, server_config) = early_data_configs();\r\n\r\n    // first connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    do_handshake(&mut client, &mut server);\r\n\r\n    // second connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    assert!(client.early_data().is_some());\r\n    assert_eq!(\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .bytes_left(),\r\n        1234\r\n    );\r\n\r\n    // send early data on connection\r\n    client\r\n        .early_data()\r\n        .unwrap()\r\n        .write(&[b'Z'; 1234]) // maximum 1234 bytes\r\n        .unwrap();\r\n\r\n    transfer(&mut client, &mut server);\r\n    server.process_new_packets().unwrap();\r\n\r\n    // server begins to respond, but...\r\n    transfer(&mut server, &mut client);\r\n    // ...don't process anything on the client so that it remains in early state\r\n\r\n    let mut received_early_data = [0u8; 1234];\r\n    assert_eq!(\r\n        server\r\n            .early_data()\r\n            .expect(\"early_data didn't happen\")\r\n            .read(&mut received_early_data)\r\n            .expect(\"early_data failed unexpectedly\"),\r\n        1234\r\n    );\r\n\r\n    // Send as much early data as you'd like!\r\n    // client sends early data, waits until server processes it, and then client sends more early data\r\n    // an actual implementation over IO would require special timing\r\n    for _ in 0..100 {\r\n        // just keep sending under max early data size limit each time\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .write(&[b'Z'; 1234])\r\n            .unwrap();\r\n        transfer(&mut client, &mut server);\r\n        // this should panic, too much early data was sent\r\n        server.process_new_packets().unwrap();\r\n\r\n        // server keeps responding...\r\n        transfer(&mut server, &mut client);\r\n        // client keeps itself in early state\r\n\r\n        let mut received_early_data = [0u8; 1234];\r\n        // fails here, all early data should be read at this point\r\n        assert_eq!(\r\n            server\r\n                .early_data()\r\n                .expect(\"early_data didn't happen\")\r\n                .read(&mut received_early_data)\r\n                .expect(\"early_data failed unexpectedly\"),\r\n            0\r\n        );\r\n    }\r\n}\r\n```\r\n\r\n### Impact\r\nAny server that attempts to read early data before waiting for the handshake to complete is vulnerable.\r\n\r\n---\r\n\r\nIn private discussion, we were sure that this was a bug, but unsure if this was a security bug, given RFC8446 doesn't actually require a server to limit anything to `max_early_data_size` (it's a \"SHOULD\"). However, we shall certainly fix this.\n", "hints_text": "", "created_at": "2024-08-28 15:07:36", "merge_commit_sha": "a2fedecc900a1887d1c8d2ad65ea340576782597", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check semver compatibility', '.github/workflows/build.yml']", "['Check minimum versions of direct dependencies', '.github/workflows/build.yml']"], ["['Validate external types appearing in public API', '.github/workflows/build.yml']", "['Features', '.github/workflows/build.yml']"], ["['MSRV', '.github/workflows/build.yml']", "['Build+test (nightly, ubuntu-latest)', '.github/workflows/build.yml']"], ["['BoGo test suite', '.github/workflows/build.yml']", "['Measure coverage', '.github/workflows/build.yml']"], ["['Build+test (stable, ubuntu-latest)', '.github/workflows/build.yml']", "['Fuzzing', '.github/workflows/cifuzz.yml']"], ["['Run openssl-tests', '.github/workflows/build.yml']", "['Check cross compilation targets', '.github/workflows/build.yml']"]]}
{"repo": "SeaQL/sea-query", "instance_id": "SeaQL__sea-query-835", "base_commit": "b91ba14f680e11a2e813880b775299a44d279c66", "patch": "diff --git a/src/backend/query_builder.rs b/src/backend/query_builder.rs\nindex 7bf29b55..ae7358b1 100644\n--- a/src/backend/query_builder.rs\n+++ b/src/backend/query_builder.rs\n@@ -777,15 +777,6 @@ pub trait QueryBuilder:\n             \"Cannot build a with query that has no common table expression!\"\n         );\n \n-        if with_clause.recursive {\n-            assert_eq!(\n-                with_clause.cte_expressions.len(),\n-                1,\n-                \"Cannot build a recursive query with more than one common table! \\\n-                A recursive with query must have a single cte inside it that has a union query of \\\n-                two queries!\"\n-            );\n-        }\n         for cte in &with_clause.cte_expressions {\n             if !cte_first {\n                 write!(sql, \", \").unwrap();\n", "test_patch": "diff --git a/tests/sqlite/query.rs b/tests/sqlite/query.rs\nindex 9a9c8fcf..48a6f90b 100644\n--- a/tests/sqlite/query.rs\n+++ b/tests/sqlite/query.rs\n@@ -1792,3 +1792,47 @@ fn sub_query_with_fn() {\n         r#\"SELECT ARRAY((SELECT * FROM \"character\"))\"#\n     );\n }\n+\n+#[test]\n+fn recursive_with_multiple_ctes() {\n+    let sub_select1 = Query::select()\n+        .column(Asterisk)\n+        .from(Char::Table)\n+        .to_owned();\n+    let sub_select1_name = SeaRc::new(Alias::new(\"sub1\"));\n+    let mut sub_select1_cte = CommonTableExpression::new();\n+    sub_select1_cte.table_name(sub_select1_name.clone());\n+    sub_select1_cte.column(SeaRc::new(Alias::new(\"a\")));\n+    sub_select1_cte.query(sub_select1);\n+    let sub_select2 = Query::select()\n+        .column(Asterisk)\n+        .from(Char::Table)\n+        .to_owned();\n+    let sub_select2_name = SeaRc::new(Alias::new(\"sub2\"));\n+    let mut sub_select2_cte = CommonTableExpression::new();\n+    sub_select2_cte.table_name(sub_select2_name.clone());\n+    sub_select2_cte.column(SeaRc::new(Alias::new(\"b\")));\n+    sub_select2_cte.query(sub_select2);\n+\n+    let mut with = WithClause::new();\n+    with.recursive(true)\n+        .cte(sub_select1_cte)\n+        .cte(sub_select2_cte);\n+\n+    let mut main_sel2 = Query::select();\n+    main_sel2\n+        .expr(Expr::col(Asterisk))\n+        .from(TableRef::Table(sub_select2_name));\n+    let mut main_sel1 = Query::select();\n+    main_sel1\n+        .expr(Expr::col(Asterisk))\n+        .from(TableRef::Table(sub_select1_name))\n+        .union(UnionType::All, main_sel2);\n+\n+    let query = with.query(main_sel1);\n+\n+    assert_eq!(\n+        query.to_string(SqliteQueryBuilder),\n+        r#\"WITH RECURSIVE \"sub1\" (\"a\") AS (SELECT * FROM \"character\") , \"sub2\" (\"b\") AS (SELECT * FROM \"character\") SELECT * FROM \"sub1\" UNION ALL SELECT * FROM \"sub2\"\"#\n+    );\n+}\n", "problem_statement": "Multiple CTEs should be allowed in recursive WithQuery\n## Description\r\n\r\nPer https://github.com/SeaQL/sea-query/blob/master/src/backend/query_builder.rs#L780 when you do a recursive with you are only allowed to have one CTE.\r\n\r\nIn sqlite at least (and I'd expect other dbs as well) you can have any number of CTEs in the with query, per https://sqlite.org/lang_with.html .  Especially see example 3.5 which has 4 recursive CTEs and one non-recursive CTE (`a`).\r\n\r\nI'm not sure what the impetus for that line was, but would dropping it be reasonable? Are there other parts of the code that rely on it having only one CTE?\nMultiple CTEs should be allowed in recursive WithQuery\n## Description\r\n\r\nPer https://github.com/SeaQL/sea-query/blob/master/src/backend/query_builder.rs#L780 when you do a recursive with you are only allowed to have one CTE.\r\n\r\nIn sqlite at least (and I'd expect other dbs as well) you can have any number of CTEs in the with query, per https://sqlite.org/lang_with.html .  Especially see example 3.5 which has 4 recursive CTEs and one non-recursive CTE (`a`).\r\n\r\nI'm not sure what the impetus for that line was, but would dropping it be reasonable? Are there other parts of the code that rely on it having only one CTE?\n", "hints_text": "\n", "created_at": "2024-11-08 17:52:09", "merge_commit_sha": "67713263e4ddc4efc3629bce2425af6a50c481db", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build `sea-query-postgres`', '.github/workflows/rust.yml']", "['PostgreSQL (14.4, sqlx_postgres)', '.github/workflows/rust.yml']"], ["['SQLite (sqlx_sqlite)', '.github/workflows/rust.yml']", "['PostgreSQL (13.7, postgres)', '.github/workflows/rust.yml']"], ["['PostgreSQL (12.11, sqlx_postgres)', '.github/workflows/rust.yml']", "['Derive Tests', '.github/workflows/rust.yml']"], ["['Clippy', '.github/workflows/rust.yml']", "['PostgreSQL (13.7, sqlx_postgres)', '.github/workflows/rust.yml']"], ["['MySQL (8, sqlx_mysql)', '.github/workflows/rust.yml']", "['PostgreSQL (14.4, postgres)', '.github/workflows/rust.yml']"]]}
{"repo": "duesee/imap-codec", "instance_id": "duesee__imap-codec-561", "base_commit": "6f3db33569653c100e12332241c39bd37232d9ff", "patch": "diff --git a/bindings/imap-codec-python/imap_codec.pyi b/bindings/imap-codec-python/imap_codec.pyi\nindex 89874e45..33e0544a 100644\n--- a/bindings/imap-codec-python/imap_codec.pyi\n+++ b/bindings/imap-codec-python/imap_codec.pyi\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import Tuple\n+from typing import Tuple, Union\n \n class DecodeError(Exception):\n     \"\"\"\n@@ -25,6 +25,72 @@ class DecodeLiteralFound(DecodeError):\n     The decoder stopped at the beginning of literal data.\n     \"\"\"\n \n+class LiteralMode:\n+    \"\"\"\n+    Literal mode, i.e., sync or non-sync.\n+\n+    - Sync: A synchronizing literal, i.e., `{<n>}\\r\\n<data>`.\n+    - NonSync: A non-synchronizing literal according to RFC 7888, i.e., `{<n>+}\\r\\n<data>`.\n+\n+    Warning: The non-sync literal extension must only be used when the server advertised support\n+             for it sending the LITERAL+ or LITERAL- capability.\n+    \"\"\"\n+\n+    Sync: LiteralMode\n+    NonSync: LiteralMode\n+\n+class LineFragment:\n+    \"\"\"\n+    Fragment of a line that is ready to be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes):\n+        \"\"\"\n+        Create a line fragment from data bytes\n+\n+        :param data: Data bytes of fragment\n+        :raises TypeError: `data` is not byte-like\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get line fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+class LiteralFragment:\n+    \"\"\"\n+    Fragment of a literal that may require an action before it should be send.\n+    \"\"\"\n+\n+    def __init__(self, data: bytes, mode: LiteralMode):\n+        \"\"\"\n+        Create a literal fragment from data bytes and literal mode\n+\n+        :param data: Data bytes of fragment\n+        :param mode: Literal mode\n+        :raises TypeError: `data` is not byte-like\n+        :raises TypeError: `mode` is invalid\n+        \"\"\"\n+\n+    @property\n+    def data(self) -> bytes:\n+        \"\"\"\n+        Get literal fragment data bytes\n+\n+        :return: Data bytes of fragment\n+        \"\"\"\n+\n+    @property\n+    def mode(self) -> LiteralMode:\n+        \"\"\"\n+        Get literal fragment literal mode\n+\n+        :return: Literal mode\n+        \"\"\"\n+\n class Encoded:\n     \"\"\"\n     An encoded message.\n@@ -36,7 +102,7 @@ class Encoded:\n     \"\"\"\n \n     def __iter__(self) -> Encoded: ...\n-    def __next__(self) -> dict: ...\n+    def __next__(self) -> Union[LineFragment, LiteralFragment]: ...\n     def dump(self) -> bytes:\n         \"\"\"\n         Dump the (remaining) encoded data without being guided by fragments.\ndiff --git a/bindings/imap-codec-python/src/encoded.rs b/bindings/imap-codec-python/src/encoded.rs\nnew file mode 100644\nindex 00000000..d896933e\n--- /dev/null\n+++ b/bindings/imap-codec-python/src/encoded.rs\n@@ -0,0 +1,150 @@\n+use imap_codec::{\n+    encode::{Encoded, Fragment},\n+    imap_types::core::LiteralMode,\n+};\n+use pyo3::{prelude::*, types::PyBytes};\n+\n+/// Python class representing a literal mode\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+#[pyclass(name = \"LiteralMode\", eq)]\n+pub(crate) enum PyLiteralMode {\n+    Sync,\n+    NonSync,\n+}\n+\n+impl std::fmt::Display for PyLiteralMode {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match self {\n+            PyLiteralMode::Sync => f.write_str(\"LiteralMode.Sync\"),\n+            PyLiteralMode::NonSync => f.write_str(\"LiteralMode.NonSync\"),\n+        }\n+    }\n+}\n+\n+impl From<LiteralMode> for PyLiteralMode {\n+    fn from(value: LiteralMode) -> Self {\n+        match value {\n+            LiteralMode::Sync => PyLiteralMode::Sync,\n+            LiteralMode::NonSync => PyLiteralMode::NonSync,\n+        }\n+    }\n+}\n+\n+/// Python class representing a line fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LineFragment\", eq)]\n+pub(crate) struct PyLineFragment {\n+    data: Vec<u8>,\n+}\n+\n+#[pymethods]\n+impl PyLineFragment {\n+    /// Create a new line fragment from data\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn new(data: Vec<u8>) -> Self {\n+        Self { data }\n+    }\n+\n+    /// Retrieve the data from the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// String representation of the fragment, e.g. `b'hello'`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        self.data(py).to_string()\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LineFragment(b'hello')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LineFragment({})\", self.__str__(py))\n+    }\n+}\n+\n+/// Python class representing a literal fragment\n+#[derive(Debug, Clone, PartialEq)]\n+#[pyclass(name = \"LiteralFragment\", eq)]\n+pub(crate) struct PyLiteralFragment {\n+    data: Vec<u8>,\n+    mode: PyLiteralMode,\n+}\n+\n+#[pymethods]\n+impl PyLiteralFragment {\n+    /// Create a new literal fragment from data and mode\n+    ///\n+    /// `data` can be anything that can be extracted to `Vec`, e.g. Python `bytes`\n+    #[new]\n+    pub(crate) fn try_new(data: Vec<u8>, mode: PyLiteralMode) -> PyResult<Self> {\n+        Ok(Self { data, mode })\n+    }\n+\n+    /// Retrieve the data of the fragment as Python `bytes`\n+    #[getter]\n+    pub(crate) fn data<'py>(&self, py: Python<'py>) -> Bound<'py, PyBytes> {\n+        PyBytes::new_bound(py, self.data.as_slice())\n+    }\n+\n+    /// Retrieve the mode of the fragment\n+    #[getter]\n+    pub(crate) fn mode(&self) -> PyLiteralMode {\n+        self.mode\n+    }\n+\n+    /// String representation of the fragment, e.g. `(b'hello', 'Sync')`\n+    pub(crate) fn __str__(&self, py: Python) -> String {\n+        format!(\"({}, {})\", self.data(py), self.mode)\n+    }\n+\n+    /// Printable representation of the fragment, e.g. `LiteralFragment(b'hello', 'Sync')`\n+    pub(crate) fn __repr__(&self, py: Python) -> String {\n+        format!(\"LiteralFragment{}\", self.__str__(py))\n+    }\n+}\n+\n+/// Python wrapper classes for `Encoded`\n+///\n+/// This implements a Python iterator over the containing fragments.\n+#[derive(Debug, Clone)]\n+#[pyclass(name = \"Encoded\")]\n+pub(crate) struct PyEncoded(pub(crate) Option<Encoded>);\n+\n+#[pymethods]\n+impl PyEncoded {\n+    /// Initialize iterator\n+    pub(crate) fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n+        slf\n+    }\n+\n+    /// Return next fragment\n+    pub(crate) fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<PyObject>> {\n+        // Try to get next `Fragment` from `Encoded` iterator\n+        let Some(fragment) = slf.0.as_mut().and_then(|encoded| encoded.next()) else {\n+            return Ok(None);\n+        };\n+\n+        // Return instance of `PyLineFragment` or `PyLiteralFragment` as a generic `PyObject`.\n+        Ok(Some(match fragment {\n+            Fragment::Line { data } => {\n+                Bound::new(slf.py(), PyLineFragment::new(data))?.to_object(slf.py())\n+            }\n+            Fragment::Literal { data, mode } => {\n+                Bound::new(slf.py(), PyLiteralFragment::try_new(data, mode.into())?)?\n+                    .to_object(slf.py())\n+            }\n+        }))\n+    }\n+\n+    /// Dump remaining fragment data\n+    pub(crate) fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n+        let encoded = slf.0.take();\n+        let dump = match encoded {\n+            Some(encoded) => encoded.dump(),\n+            None => Vec::new(),\n+        };\n+        Ok(PyBytes::new_bound(slf.py(), &dump))\n+    }\n+}\ndiff --git a/bindings/imap-codec-python/src/lib.rs b/bindings/imap-codec-python/src/lib.rs\nindex 23ee0717..da568fb4 100644\n--- a/bindings/imap-codec-python/src/lib.rs\n+++ b/bindings/imap-codec-python/src/lib.rs\n@@ -1,6 +1,9 @@\n+mod encoded;\n+\n+use encoded::PyEncoded;\n use imap_codec::{\n     decode::{self, Decoder},\n-    encode::{Encoded, Encoder},\n+    encode::Encoder,\n     AuthenticateDataCodec, CommandCodec, GreetingCodec, IdleDoneCodec, ResponseCodec,\n };\n use pyo3::{create_exception, exceptions::PyException, prelude::*, types::PyBytes};\n@@ -11,46 +14,10 @@ create_exception!(imap_codec, DecodeFailed, DecodeError);\n create_exception!(imap_codec, DecodeIncomplete, DecodeError);\n create_exception!(imap_codec, DecodeLiteralFound, DecodeError);\n \n-/// Wrapper for `Encoded`\n-///\n-/// This implements a Python iterator over the containing fragments.\n-#[derive(Debug, Clone)]\n-#[pyclass(name = \"Encoded\")]\n-struct PyEncoded(Option<Encoded>);\n-\n-#[pymethods]\n-impl PyEncoded {\n-    /// Initialize iterator\n-    fn __iter__(slf: PyRef<'_, Self>) -> PyRef<'_, Self> {\n-        slf\n-    }\n-\n-    /// Return next fragment\n-    fn __next__(mut slf: PyRefMut<'_, Self>) -> PyResult<Option<Bound<PyAny>>> {\n-        let Some(encoded) = &mut slf.0 else {\n-            return Ok(None);\n-        };\n-        Ok(encoded\n-            .next()\n-            .map(|value| serde_pyobject::to_pyobject(slf.py(), &value))\n-            .transpose()?)\n-    }\n-\n-    /// Dump remaining fragment data\n-    fn dump(mut slf: PyRefMut<'_, Self>) -> PyResult<Bound<PyBytes>> {\n-        let encoded = slf.0.take();\n-        let dump = match encoded {\n-            Some(encoded) => encoded.dump(),\n-            None => Vec::new(),\n-        };\n-        Ok(PyBytes::new_bound(slf.py(), &dump))\n-    }\n-}\n-\n-/// Wrapper for `GreetingCodec`\n+/// Python class for using `GreetingCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"GreetingCodec\")]\n-struct PyGreetingCodec(GreetingCodec);\n+struct PyGreetingCodec;\n \n #[pymethods]\n impl PyGreetingCodec {\n@@ -80,10 +47,10 @@ impl PyGreetingCodec {\n     }\n }\n \n-/// Wrapper for `CommandCodec`\n+/// Python class for using `CommandCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"CommandCodec\")]\n-struct PyCommandCodec(CommandCodec);\n+struct PyCommandCodec;\n \n #[pymethods]\n impl PyCommandCodec {\n@@ -119,10 +86,10 @@ impl PyCommandCodec {\n     }\n }\n \n-/// Wrapper for `AuthenticateDataCodec`\n+/// Python class for using `AuthenticateDataCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"AuthenticateDataCodec\")]\n-struct PyAuthenticateDataCodec(AuthenticateDataCodec);\n+struct PyAuthenticateDataCodec;\n \n #[pymethods]\n impl PyAuthenticateDataCodec {\n@@ -151,10 +118,10 @@ impl PyAuthenticateDataCodec {\n     }\n }\n \n-/// Wrapper for `ResponseCodec`\n+/// Python class for using `ResponseCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"ResponseCodec\")]\n-struct PyResponseCodec(ResponseCodec);\n+struct PyResponseCodec;\n \n #[pymethods]\n impl PyResponseCodec {\n@@ -188,10 +155,10 @@ impl PyResponseCodec {\n     }\n }\n \n-/// Wrapper for `IdleDoneCodec`\n+/// Python class for using `IdleDoneCodec`\n #[derive(Debug, Clone, PartialEq)]\n #[pyclass(name = \"IdleDoneCodec\")]\n-struct PyIdleDoneCodec(IdleDoneCodec);\n+struct PyIdleDoneCodec;\n \n #[pymethods]\n impl PyIdleDoneCodec {\n@@ -233,6 +200,9 @@ fn imap_codec_python(m: &Bound<'_, PyModule>) -> PyResult<()> {\n         \"DecodeLiteralFound\",\n         m.py().get_type_bound::<DecodeLiteralFound>(),\n     )?;\n+    m.add_class::<encoded::PyLiteralMode>()?;\n+    m.add_class::<encoded::PyLineFragment>()?;\n+    m.add_class::<encoded::PyLiteralFragment>()?;\n     m.add_class::<PyEncoded>()?;\n     m.add_class::<PyGreetingCodec>()?;\n     m.add_class::<PyCommandCodec>()?;\n", "test_patch": "diff --git a/bindings/imap-codec-python/tests/test_authenticate_data_encode.py b/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\nindex 5ea8a4d5..d21ff726 100644\n--- a/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\n+++ b/bindings/imap-codec-python/tests/test_authenticate_data_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import AuthenticateDataCodec, Encoded\n+from imap_codec import AuthenticateDataCodec, Encoded, LineFragment\n \n \n class TestAuthenticateDataEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_authenticate_data(self):\n         encoded = AuthenticateDataCodec.encode(authenticate_data)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"VGVzdA==\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"VGVzdA==\\r\\n\")])\n \n     def test_authenticate_data_dump(self):\n         authenticate_data = {\"Continue\": list(b\"Test\")}\ndiff --git a/bindings/imap-codec-python/tests/test_command_encode.py b/bindings/imap-codec-python/tests/test_command_encode.py\nindex a61eef37..c51d7e61 100644\n--- a/bindings/imap-codec-python/tests/test_command_encode.py\n+++ b/bindings/imap-codec-python/tests/test_command_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import CommandCodec, Encoded\n+from imap_codec import CommandCodec, Encoded, LineFragment, LiteralFragment, LiteralMode\n \n \n class TestCommandEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_simple_command(self):\n         encoded = CommandCodec.encode(command)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"a NOOP\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"a NOOP\\r\\n\")])\n \n     def test_simple_command_dump(self):\n         command = {\"tag\": \"a\", \"body\": \"Noop\"}\n@@ -36,9 +36,9 @@ def test_multi_fragment_command(self):\n         self.assertEqual(\n             fragments,\n             [\n-                {\"Line\": {\"data\": list(b\"A LOGIN alice {2}\\r\\n\")}},\n-                {\"Literal\": {\"data\": list(b\"\\xCA\\xFE\"), \"mode\": \"Sync\"}},\n-                {\"Line\": {\"data\": list(b\"\\r\\n\")}},\n+                LineFragment(b\"A LOGIN alice {2}\\r\\n\"),\n+                LiteralFragment(b\"\\xCA\\xFE\", LiteralMode.Sync),\n+                LineFragment(b\"\\r\\n\"),\n             ],\n         )\n \n@@ -50,7 +50,5 @@ def test_multi_fragment_command_dump(self):\n     def test_multi_fragment_command_dump_remaining(self):\n         encoded = CommandCodec.encode(self._MULTI_FRAGMENT_COMMAND)\n         self.assertIsInstance(encoded, Encoded)\n-        self.assertEqual(\n-            next(encoded), {\"Line\": {\"data\": list(b\"A LOGIN alice {2}\\r\\n\")}}\n-        )\n+        self.assertEqual(next(encoded), LineFragment(b\"A LOGIN alice {2}\\r\\n\"))\n         self.assertEqual(encoded.dump(), b\"\\xCA\\xFE\\r\\n\")\ndiff --git a/bindings/imap-codec-python/tests/test_fragments.py b/bindings/imap-codec-python/tests/test_fragments.py\nnew file mode 100644\nindex 00000000..2803048f\n--- /dev/null\n+++ b/bindings/imap-codec-python/tests/test_fragments.py\n@@ -0,0 +1,101 @@\n+import unittest\n+\n+from imap_codec import LineFragment, LiteralFragment, LiteralMode\n+\n+\n+class TestLineFragment(unittest.TestCase):\n+    def test_data(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(fragment.data, data)\n+\n+    def test_repr(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(repr(fragment), f\"LineFragment({data})\")\n+\n+    def test_str(self):\n+        data = b\"a NOOP\\r\\n\"\n+        fragment = LineFragment(data)\n+        self.assertEqual(str(fragment), str(data))\n+\n+    def test_eq(self):\n+        fragment1 = LineFragment(b\"a NOOP\\r\\n\")\n+        fragment2 = LineFragment(b\"a NOOP\\r\\n\")\n+        fragment3 = LineFragment(b\"a LOGIN alice pass\\r\\n\")\n+\n+        self.assertEqual(fragment1, fragment1)\n+        self.assertEqual(fragment1, fragment2)\n+        self.assertNotEqual(fragment1, fragment3)\n+\n+        self.assertEqual(fragment2, fragment1)\n+        self.assertEqual(fragment2, fragment2)\n+        self.assertNotEqual(fragment2, fragment3)\n+\n+        self.assertNotEqual(fragment3, fragment1)\n+        self.assertNotEqual(fragment3, fragment2)\n+        self.assertEqual(fragment3, fragment3)\n+\n+\n+class TestLiteralFragment(unittest.TestCase):\n+    def test_data(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+        fragment = LiteralFragment(data, LiteralMode.Sync)\n+        self.assertEqual(fragment.data, data)\n+\n+    def test_mode(self):\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", mode)\n+        self.assertEqual(fragment.mode, mode)\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", mode)\n+        self.assertEqual(fragment.mode, mode)\n+\n+    def test_repr(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(repr(fragment), f\"LiteralFragment({data}, {mode})\")\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(repr(fragment), f\"LiteralFragment({data}, {mode})\")\n+\n+    def test_str(self):\n+        data = b\"\\x01\\x02\\x03\\x04\"\n+\n+        mode = LiteralMode.Sync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(str(fragment), f\"({data}, {mode})\")\n+\n+        mode = LiteralMode.NonSync\n+        fragment = LiteralFragment(data, mode)\n+        self.assertEqual(str(fragment), f\"({data}, {mode})\")\n+\n+    def test_eq(self):\n+        fragment1 = LiteralFragment(b\"data\", LiteralMode.Sync)\n+        fragment2 = LiteralFragment(b\"data\", LiteralMode.Sync)\n+        fragment3 = LiteralFragment(b\"data\", LiteralMode.NonSync)\n+        fragment4 = LiteralFragment(b\"\\x01\\x02\\x03\\x04\", LiteralMode.NonSync)\n+\n+        self.assertEqual(fragment1, fragment1)\n+        self.assertEqual(fragment1, fragment2)\n+        self.assertNotEqual(fragment1, fragment3)\n+        self.assertNotEqual(fragment1, fragment4)\n+\n+        self.assertEqual(fragment2, fragment1)\n+        self.assertEqual(fragment2, fragment2)\n+        self.assertNotEqual(fragment2, fragment3)\n+        self.assertNotEqual(fragment2, fragment4)\n+\n+        self.assertNotEqual(fragment3, fragment1)\n+        self.assertNotEqual(fragment3, fragment2)\n+        self.assertEqual(fragment3, fragment3)\n+        self.assertNotEqual(fragment3, fragment4)\n+\n+        self.assertNotEqual(fragment4, fragment1)\n+        self.assertNotEqual(fragment4, fragment2)\n+        self.assertNotEqual(fragment4, fragment3)\n+        self.assertEqual(fragment4, fragment4)\ndiff --git a/bindings/imap-codec-python/tests/test_greeting_encode.py b/bindings/imap-codec-python/tests/test_greeting_encode.py\nindex e9b2077d..b64716fb 100644\n--- a/bindings/imap-codec-python/tests/test_greeting_encode.py\n+++ b/bindings/imap-codec-python/tests/test_greeting_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import Encoded, GreetingCodec\n+from imap_codec import Encoded, GreetingCodec, LineFragment\n \n \n class TestGreetingEncode(unittest.TestCase):\n@@ -9,9 +9,7 @@ def test_simple_greeting(self):\n         encoded = GreetingCodec.encode(greeting)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(\n-            fragments, [{\"Line\": {\"data\": list(b\"* OK Hello, World!\\r\\n\")}}]\n-        )\n+        self.assertEqual(fragments, [LineFragment(b\"* OK Hello, World!\\r\\n\")])\n \n     def test_simple_greeting_dump(self):\n         greeting = {\"code\": None, \"kind\": \"Ok\", \"text\": \"Hello, World!\"}\ndiff --git a/bindings/imap-codec-python/tests/test_idle_done_encode.py b/bindings/imap-codec-python/tests/test_idle_done_encode.py\nindex 5208d89a..2e15ced3 100644\n--- a/bindings/imap-codec-python/tests/test_idle_done_encode.py\n+++ b/bindings/imap-codec-python/tests/test_idle_done_encode.py\n@@ -1,6 +1,6 @@\n import unittest\n \n-from imap_codec import Encoded, IdleDoneCodec\n+from imap_codec import Encoded, IdleDoneCodec, LineFragment\n \n \n class TestIdleDoneEncode(unittest.TestCase):\n@@ -9,7 +9,7 @@ def test_idle_done(self):\n         encoded = IdleDoneCodec.encode(idle_done)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"DONE\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"DONE\\r\\n\")])\n \n     def test_idle_done_dump(self):\n         idle_done = ()\ndiff --git a/bindings/imap-codec-python/tests/test_response_encode.py b/bindings/imap-codec-python/tests/test_response_encode.py\nindex 94149197..7e30c141 100644\n--- a/bindings/imap-codec-python/tests/test_response_encode.py\n+++ b/bindings/imap-codec-python/tests/test_response_encode.py\n@@ -1,6 +1,12 @@\n import unittest\n \n-from imap_codec import Encoded, ResponseCodec\n+from imap_codec import (\n+    Encoded,\n+    LineFragment,\n+    LiteralFragment,\n+    LiteralMode,\n+    ResponseCodec,\n+)\n \n \n class TestResponseEncode(unittest.TestCase):\n@@ -9,7 +15,7 @@ def test_simple_response(self):\n         encoded = ResponseCodec.encode(response)\n         self.assertIsInstance(encoded, Encoded)\n         fragments = list(encoded)\n-        self.assertEqual(fragments, [{\"Line\": {\"data\": list(b\"* SEARCH 1\\r\\n\")}}])\n+        self.assertEqual(fragments, [LineFragment(b\"* SEARCH 1\\r\\n\")])\n \n     def test_simple_response_dump(self):\n         response = {\"Data\": {\"Search\": [1]}}\n@@ -46,9 +52,9 @@ def test_multi_fragment_response(self):\n         self.assertEqual(\n             fragments,\n             [\n-                {\"Line\": {\"data\": list(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\")}},\n-                {\"Literal\": {\"data\": list(b\"ABCDE\"), \"mode\": \"NonSync\"}},\n-                {\"Line\": {\"data\": list(b\")\\r\\n\")}},\n+                LineFragment(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\"),\n+                LiteralFragment(b\"ABCDE\", LiteralMode.NonSync),\n+                LineFragment(b\")\\r\\n\"),\n             ],\n         )\n \n@@ -63,9 +69,7 @@ def test_multi_fragment_response_dump(self):\n     def test_multi_fragment_response_dump_remaining(self):\n         encoded = ResponseCodec.encode(self._MULTI_FRAGMENT_RESPONSE)\n         self.assertIsInstance(encoded, Encoded)\n-        self.assertEqual(\n-            next(encoded), {\"Line\": {\"data\": list(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\")}}\n-        )\n+        self.assertEqual(next(encoded), LineFragment(b\"* 12345 FETCH (BODY[] {5+}\\r\\n\"))\n         self.assertEqual(\n             encoded.dump(),\n             b\"ABCDE)\\r\\n\",\n", "problem_statement": "bindings(Python): Use Python classes for `Fragment` variants \nCurrently, the fragments of `Encoded` (Rust enum variants) are deserializes into variant dictionaries, e.g. `{\"Line\": {\"data\": list(b\"a NOOP\\r\\n\")}}`.\r\n\r\nBy using individual classes for each variant (`LineFragment`, `LiteralFragment`), handling these in the Python code would be greatly improved.\r\n\r\nRelated: This would remove the need for `Fragment` to implement `Serialize`, see #555.\n", "hints_text": "", "created_at": "2024-07-17 11:16:58", "merge_commit_sha": "32266eb6d06bfa0f16414ed6c7c69cabd9bb43d3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['fuzz', '.github/workflows/main.yml']", "['test (ubuntu-latest)', '.github/workflows/main.yml']"], ["['test (macos-latest)', '.github/workflows/main.yml']", "['minimal_versions', '.github/workflows/main.yml']"]]}
{"repo": "alexpasmantier/television", "instance_id": "alexpasmantier__television-315", "base_commit": "5271b507a04af992f49ef04871abc8edeb5e0b81", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 2d5835a..9701bcf 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -343,8 +343,10 @@ version = \"4.5.26\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"96b01801b5fc6a0a232407abc821660c9c6d25a1cafc0d4f85f29fb8d9afc121\"\n dependencies = [\n+ \"anstream\",\n  \"anstyle\",\n  \"clap_lex\",\n+ \"strsim\",\n ]\n \n [[package]]\n@@ -1870,7 +1872,7 @@ dependencies = [\n \n [[package]]\n name = \"television\"\n-version = \"0.10.0\"\n+version = \"0.10.1\"\n dependencies = [\n  \"anyhow\",\n  \"bat\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 6e8206f..e09f89a 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"television\"\n-version = \"0.10.0\"\n+version = \"0.10.1\"\n edition = \"2021\"\n description = \"The revolution will be televised.\"\n license = \"MIT\"\n@@ -40,12 +40,7 @@ tracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n rustc-hash = \"2.1\"\n syntect = { version = \"5.2\", default-features = false }\n unicode-width = \"0.2\"\n-clap = { version = \"4.5\", default-features = false, features = [\n-  \"std\",\n-  \"derive\",\n-  \"cargo\",\n-  \"string\",\n-] }\n+clap = { version = \"4.5\", features = [\"derive\", \"cargo\", \"string\"] }\n serde = { version = \"1.0\", features = [\"derive\"] }\n ratatui = { version = \"0.29\", features = [\"serde\", \"macros\"] }\n better-panic = \"0.3\"\n", "test_patch": "", "problem_statement": "Flag --help missing\n**Description**\n\nIn release 0.10.0, the -h and --help command line flag does not work. It prints an error about unexpected argument. In the previous release this worked.\n\n**Example**\n```bash\n$ ./tv -h\nerror: unexpected argument found\n\n$ ./tv --help\nerror: unexpected argument found\n\n$ ./tv --version\ntelevision 0.10.0\n```\n\n**Expected behavior**\n\nTV prints command line help with arguments and flags.\n\n**Environment**\n - OS: Linux x86_64\n - Project version: 0.10.0\n\n", "hints_text": "", "created_at": "2025-01-26 08:47:55", "merge_commit_sha": "82f471d0aa01285ce82dfb19ab5c81b4b9d1f562", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test Suite', '.github/workflows/ci.yml']", "['Rustfmt', '.github/workflows/ci.yml']"]]}
{"repo": "bytecodealliance/rustix", "instance_id": "bytecodealliance__rustix-1335", "base_commit": "cc673a10b6a44acb9800599e6cbfa072011f85c9", "patch": "diff --git a/CHANGES.md b/CHANGES.md\nindex 8df8801cf..c8a1de780 100644\n--- a/CHANGES.md\n+++ b/CHANGES.md\n@@ -223,5 +223,12 @@ specific socket types using `From`/`Into`/`TryFrom`/`TryInto` conversions.\n \n [`SocketAddrAny`]: https://docs.rs/rustix/1.0.0/rustix/net/struct.SocketAddrAny.html\n \n+The `len` parameter to [`rustix::fs::fadvise`] has changed from `u64` to\n+`Option<NonZeroU64>`, to reflect that zero is a special case meaning the\n+advice applies to the end of the file. To convert an arbitrary `u64` value to\n+`Option<NonZeroU64>`, use `NonZeroU64::new`.\n+\n+[`rustix::fs::fadvise`]: https://docs.rs/rustix/1.0.0/rustix/fs/fn.fadvise.html\n+\n All explicitly deprecated functions and types have been removed. Their\n deprecation messages will have identified alternatives.\ndiff --git a/src/backend/libc/fs/syscalls.rs b/src/backend/libc/fs/syscalls.rs\nindex 468b9dfed..74108a7bb 100644\n--- a/src/backend/libc/fs/syscalls.rs\n+++ b/src/backend/libc/fs/syscalls.rs\n@@ -10,17 +10,6 @@ use crate::ffi::CString;\n use crate::ffi::{self, CStr};\n #[cfg(not(any(target_os = \"espidf\", target_os = \"vita\")))]\n use crate::fs::Access;\n-#[cfg(not(any(\n-    apple,\n-    netbsdlike,\n-    target_os = \"solaris\",\n-    target_os = \"dragonfly\",\n-    target_os = \"espidf\",\n-    target_os = \"haiku\",\n-    target_os = \"redox\",\n-    target_os = \"vita\",\n-)))]\n-use crate::fs::Advice;\n #[cfg(not(any(target_os = \"espidf\", target_os = \"redox\")))]\n use crate::fs::AtFlags;\n #[cfg(not(any(\n@@ -75,6 +64,17 @@ use {\n     crate::backend::conv::nonnegative_ret,\n     crate::fs::{copyfile_state_t, CloneFlags, CopyfileFlags},\n };\n+#[cfg(not(any(\n+    apple,\n+    netbsdlike,\n+    target_os = \"solaris\",\n+    target_os = \"dragonfly\",\n+    target_os = \"espidf\",\n+    target_os = \"haiku\",\n+    target_os = \"redox\",\n+    target_os = \"vita\",\n+)))]\n+use {crate::fs::Advice, core::num::NonZeroU64};\n #[cfg(any(apple, linux_kernel, target_os = \"hurd\"))]\n use {crate::fs::XattrFlags, core::mem::size_of, core::ptr::null_mut};\n #[cfg(linux_kernel)]\n@@ -1213,9 +1213,17 @@ pub(crate) fn copy_file_range(\n     target_os = \"redox\",\n     target_os = \"vita\",\n )))]\n-pub(crate) fn fadvise(fd: BorrowedFd<'_>, offset: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub(crate) fn fadvise(\n+    fd: BorrowedFd<'_>,\n+    offset: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n     let offset = offset as i64;\n-    let len = len as i64;\n+    let len = match len {\n+        None => 0,\n+        Some(len) => len.get() as i64,\n+    };\n \n     // Our public API uses `u64` following the [Rust convention], but the\n     // underlying host APIs use a signed `off_t`. Converting these values may\ndiff --git a/src/backend/linux_raw/fs/syscalls.rs b/src/backend/linux_raw/fs/syscalls.rs\nindex 882a47724..7dd19ab09 100644\n--- a/src/backend/linux_raw/fs/syscalls.rs\n+++ b/src/backend/linux_raw/fs/syscalls.rs\n@@ -41,6 +41,7 @@ use crate::fs::{\n };\n use crate::io;\n use core::mem::MaybeUninit;\n+use core::num::NonZeroU64;\n #[cfg(any(target_arch = \"mips64\", target_arch = \"mips64r6\"))]\n use linux_raw_sys::general::stat as linux_stat64;\n use linux_raw_sys::general::{\n@@ -364,7 +365,17 @@ pub(crate) fn fallocate(\n }\n \n #[inline]\n-pub(crate) fn fadvise(fd: BorrowedFd<'_>, pos: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub(crate) fn fadvise(\n+    fd: BorrowedFd<'_>,\n+    pos: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n+    let len = match len {\n+        None => 0,\n+        Some(len) => len.get(),\n+    };\n+\n     // On ARM, the arguments are reordered so that the `len` and `pos` argument\n     // pairs are aligned. And ARM has a custom syscall code for this.\n     #[cfg(target_arch = \"arm\")]\ndiff --git a/src/fs/fadvise.rs b/src/fs/fadvise.rs\nindex f4732d55f..93d8e0810 100644\n--- a/src/fs/fadvise.rs\n+++ b/src/fs/fadvise.rs\n@@ -1,10 +1,13 @@\n use crate::{backend, io};\n use backend::fd::AsFd;\n use backend::fs::types::Advice;\n+use core::num::NonZeroU64;\n \n /// `posix_fadvise(fd, offset, len, advice)`\u2014Declares an expected access\n /// pattern for a file.\n ///\n+/// If `len` is `None`, the advice extends to the end of the file.\n+///\n /// # References\n ///  - [POSIX]\n ///  - [Linux]\n@@ -15,6 +18,11 @@ use backend::fs::types::Advice;\n /// [FreeBSD]: https://man.freebsd.org/cgi/man.cgi?query=posix_fadvise&sektion=2\n #[inline]\n #[doc(alias = \"posix_fadvise\")]\n-pub fn fadvise<Fd: AsFd>(fd: Fd, offset: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub fn fadvise<Fd: AsFd>(\n+    fd: Fd,\n+    offset: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n     backend::fs::syscalls::fadvise(fd.as_fd(), offset, len, advice)\n }\ndiff --git a/src/net/addr.rs b/src/net/addr.rs\nindex 8b2aa177c..015997e45 100644\n--- a/src/net/addr.rs\n+++ b/src/net/addr.rs\n@@ -155,7 +155,7 @@ mod tests {\n     fn test_layouts() {\n         assert_eq_size!(SocketAddrLen, c::socklen_t);\n \n-        #[cfg(not(windows))]\n+        #[cfg(not(any(windows, target_os = \"redox\")))]\n         assert_eq!(\n             memoffset::span_of!(c::msghdr, msg_namelen).len(),\n             size_of::<SocketAddrLen>()\n", "test_patch": "diff --git a/tests/fs/file.rs b/tests/fs/file.rs\nindex 9aa6216df..1c0b11cb9 100644\n--- a/tests/fs/file.rs\n+++ b/tests/fs/file.rs\n@@ -1,3 +1,14 @@\n+#[cfg(not(any(\n+    apple,\n+    netbsdlike,\n+    target_os = \"solaris\",\n+    target_os = \"dragonfly\",\n+    target_os = \"espidf\",\n+    target_os = \"haiku\",\n+    target_os = \"redox\",\n+)))]\n+use core::num::NonZeroU64;\n+\n #[cfg(not(target_os = \"redox\"))]\n #[test]\n fn test_file() {\n@@ -88,9 +99,8 @@ fn test_file() {\n         target_os = \"dragonfly\",\n         target_os = \"espidf\",\n         target_os = \"haiku\",\n-        target_os = \"redox\",\n     )))]\n-    rustix::fs::fadvise(&file, 0, 10, rustix::fs::Advice::Normal).unwrap();\n+    rustix::fs::fadvise(&file, 0, NonZeroU64::new(10), rustix::fs::Advice::Normal).unwrap();\n \n     rustix::fs::fsync(&file).unwrap();\n \n@@ -99,7 +109,6 @@ fn test_file() {\n         target_os = \"dragonfly\",\n         target_os = \"espidf\",\n         target_os = \"haiku\",\n-        target_os = \"redox\",\n     )))]\n     rustix::fs::fdatasync(&file).unwrap();\n \n@@ -134,7 +143,6 @@ fn test_file() {\n         solarish,\n         target_os = \"haiku\",\n         target_os = \"netbsd\",\n-        target_os = \"redox\",\n         target_os = \"wasi\",\n     )))]\n     {\ndiff --git a/tests/fs/invalid_offset.rs b/tests/fs/invalid_offset.rs\nindex 85b964c79..75dfb2ec1 100644\n--- a/tests/fs/invalid_offset.rs\n+++ b/tests/fs/invalid_offset.rs\n@@ -67,6 +67,7 @@ fn invalid_offset_fallocate() {\n )))]\n #[test]\n fn invalid_offset_fadvise() {\n+    use core::num::NonZeroU64;\n     use rustix::fs::{fadvise, openat, Advice, Mode, OFlags, CWD};\n     let tmp = tempfile::tempdir().unwrap();\n     let dir = openat(CWD, tmp.path(), OFlags::RDONLY, Mode::empty()).unwrap();\n@@ -79,21 +80,63 @@ fn invalid_offset_fadvise() {\n     .unwrap();\n \n     // `fadvise` never fails on invalid offsets.\n-    fadvise(&file, i64::MAX as u64, i64::MAX as u64, Advice::Normal).unwrap();\n-    fadvise(&file, u64::MAX, 0, Advice::Normal).unwrap();\n-    fadvise(&file, i64::MAX as u64, 1, Advice::Normal).unwrap();\n-    fadvise(&file, 1, i64::MAX as u64, Advice::Normal).unwrap();\n-    fadvise(&file, i64::MAX as u64 + 1, 0, Advice::Normal).unwrap();\n-    fadvise(&file, u64::MAX, i64::MAX as u64, Advice::Normal).unwrap();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(i64::MAX as u64),\n+        Advice::Normal,\n+    )\n+    .unwrap();\n+    fadvise(&file, u64::MAX, None, Advice::Normal).unwrap();\n+    fadvise(&file, i64::MAX as u64, NonZeroU64::new(1), Advice::Normal).unwrap();\n+    fadvise(&file, 1, NonZeroU64::new(i64::MAX as u64), Advice::Normal).unwrap();\n+    fadvise(&file, i64::MAX as u64 + 1, None, Advice::Normal).unwrap();\n+    fadvise(\n+        &file,\n+        u64::MAX,\n+        NonZeroU64::new(i64::MAX as u64),\n+        Advice::Normal,\n+    )\n+    .unwrap();\n \n     // `fadvise` fails on invalid lengths.\n-    fadvise(&file, u64::MAX, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, 0, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, u64::MAX, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64 + 1, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n-    fadvise(&file, 0, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n+    fadvise(&file, u64::MAX, NonZeroU64::new(u64::MAX), Advice::Normal).unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(u64::MAX),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(&file, 0, NonZeroU64::new(u64::MAX), Advice::Normal).unwrap_err();\n+    fadvise(\n+        &file,\n+        u64::MAX,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64 + 1,\n+        NonZeroU64::new(u64::MAX),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        0,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n }\n \n #[test]\ndiff --git a/tests/fs/negative_timestamp.rs b/tests/fs/negative_timestamp.rs\nindex 84d74c1db..63e2e81b1 100644\n--- a/tests/fs/negative_timestamp.rs\n+++ b/tests/fs/negative_timestamp.rs\n@@ -1,3 +1,4 @@\n+#[cfg(not(target_os = \"redox\"))]\n #[test]\n fn negative_file_timetamp() {\n     use rustix::fs::{\n", "problem_statement": "Consider replacing `len: u64` with `len: Option<NonZeroU64>` in `fs::fadvise`\nAt the moment the [`rustix::fs::fadvise`](https://docs.rs/rustix/latest/rustix/fs/fn.fadvise.html) function uses a `u64` as the type of it's `len` parameter. POSIX, however, specifies for zero to have the special value, meaning the whole file instead of a specific length.\n\n> If len is zero, all data from offset to the largest possible value of the file offset for that file shall be specified.\n> -- <cite>[POSIX](https://pubs.opengroup.org/onlinepubs/9799919799/functions/posix_fadvise.html)</cite>\n\n> [...] extending for len bytes (or until the end of the file if len is 0) within the file referred to by fd.\n> -- <cite>[Linux](https://man7.org/linux/man-pages/man2/posix_fadvise.2.html)\n\nI'm not sure whether rustix usually abstracts over things like that or whether other similar apis simply use a `u64` as a parameter. I would think a `Option<NonZeroU64>`  would make this difference between passing `0` and `5` for example more clear to the reader, without providing a performance degradation.\n\nOtherwise, at least hinting at the possibility of passing `0` for `len` in the documentation would probably be enough for most cases as well.\n\n", "hints_text": "", "created_at": "2025-02-15 22:58:04", "merge_commit_sha": "76c657f5534e83fbb8009bc1591a5671d6485b86", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test use-libc (riscv64-linux)', '.github/workflows/main.yml']", "['Test (i686-linux-stable)', '.github/workflows/main.yml']"], ["['Test use-libc (powerpc64le-linux)', '.github/workflows/main.yml']", "['Test (s390x-linux-1.63)', '.github/workflows/main.yml']"], ["['Test (ubuntu-1.63)', '.github/workflows/main.yml']", "['Test (s390x-linux)', '.github/workflows/main.yml']"], ["['Test (powerpc64le-linux-1.63)', '.github/workflows/main.yml']", "['Test use-libc (ubuntu)', '.github/workflows/main.yml']"], ["['Test (riscv64-linux)', '.github/workflows/main.yml']", "['Test (arm-linux-stable)', '.github/workflows/main.yml']"], ["['Test use-libc (aarch64-linux)', '.github/workflows/main.yml']", "['Check selected Tier 3 platforms (nightly)', '.github/workflows/main.yml']"], ["['Test (ubuntu-stable)', '.github/workflows/main.yml']", "['Test rustix_use_experimental_asm (s390x-linux)', '.github/workflows/main.yml']"], ["['Test (i686-linux)', '.github/workflows/main.yml']", "['Rustfmt', '.github/workflows/main.yml']"], ["['Check (nightly)', '.github/workflows/main.yml']", "['Test rustix_use_experimental_asm (powerpc64le-linux)', '.github/workflows/main.yml']"], ["['Check --no-default-features (nightly)', '.github/workflows/main.yml']", "['Test (arm-linux-1.63)', '.github/workflows/main.yml']"], ["['Test (powerpc64le-linux)', '.github/workflows/main.yml']", "['Test (aarch64-linux-stable)', '.github/workflows/main.yml']"]]}
{"repo": "pulldown-cmark/pulldown-cmark", "instance_id": "pulldown-cmark__pulldown-cmark-1023", "base_commit": "fb01f7fd843485c0fb54b3d8a59eeb83ae3ec656", "patch": "diff --git a/pulldown-cmark/specs/super_sub.txt b/pulldown-cmark/specs/super_sub.txt\nindex f60da2e9..9cd7b9fb 100644\n--- a/pulldown-cmark/specs/super_sub.txt\n+++ b/pulldown-cmark/specs/super_sub.txt\n@@ -25,13 +25,19 @@ Backslash escapes:\n ```````````````````````````````` example_super_sub\n ~This~is~nothing~\n .\n-<p><sub>This~is~nothing</sub></p>\n+<p><sub>This</sub>is<sub>nothing</sub></p>\n ````````````````````````````````\n \n ```````````````````````````````` example_super_sub\n-~This ~~is stricken.~\n+~This ~~is not stricken.~\n .\n-<p><sub>This ~~is stricken.</sub></p>\n+<p><sub>This ~~is not stricken.</sub></p>\n+````````````````````````````````\n+\n+```````````````````````````````` example_super_sub\n+~~This ~is~~ stricken.~\n+.\n+<p><del>This ~is</del> stricken.~</p>\n ````````````````````````````````\n \n The first one wins.\n@@ -41,3 +47,15 @@ The first one wins.\n .\n <p><sub>This ~~is stricken</sub> but this is not~~</p>\n ````````````````````````````````\n+\n+Though strikethrough requires left and right flanking, subscript does not.\n+Neither does superscript.\n+\n+```````````````````````````````` example_super_sub\n+H~2~O\n+\n+y=x^2^a+xb+c\n+.\n+<p>H<sub>2</sub>O</p>\n+<p>y=x<sup>2</sup>a+xb+c</p>\n+````````````````````````````````\ndiff --git a/pulldown-cmark/src/firstpass.rs b/pulldown-cmark/src/firstpass.rs\nindex 8994a272..31eae05a 100644\n--- a/pulldown-cmark/src/firstpass.rs\n+++ b/pulldown-cmark/src/firstpass.rs\n@@ -925,6 +925,7 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                         count,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n                     let can_close = delim_run_can_close(\n                         &self.text[start..],\n@@ -932,6 +933,7 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                         count,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n                     let is_valid_seq = (c != b'~' || count <= 2) || (c == b'~' && count == 2);\n \n@@ -1176,14 +1178,21 @@ impl<'a, 'b> FirstPass<'a, 'b> {\n                 }\n                 c @ b'\\'' | c @ b'\"' => {\n                     let string_suffix = &self.text[ix..];\n-                    let can_open =\n-                        delim_run_can_open(&self.text[start..], string_suffix, 1, ix - start, mode);\n+                    let can_open = delim_run_can_open(\n+                        &self.text[start..],\n+                        string_suffix,\n+                        1,\n+                        ix - start,\n+                        mode,\n+                        self.options,\n+                    );\n                     let can_close = delim_run_can_close(\n                         &self.text[start..],\n                         string_suffix,\n                         1,\n                         ix - start,\n                         mode,\n+                        self.options,\n                     );\n \n                     self.tree.append_text(begin_text, ix, backslash_escaped);\n@@ -2258,6 +2267,7 @@ fn delim_run_can_open(\n     run_len: usize,\n     ix: usize,\n     mode: TableParseMode,\n+    options: Options,\n ) -> bool {\n     let next_char = if let Some(c) = suffix[run_len..].chars().next() {\n         c\n@@ -2279,15 +2289,18 @@ fn delim_run_can_open(\n         }\n     }\n     let delim = suffix.bytes().next().unwrap();\n-    // `*` and `~~` can be intraword, `_` and `~` cannot\n-    if delim == b'*' && !is_punctuation(next_char) {\n+    // `*`, `~~`, and `^` can be intraword, `~` can only be interword if it's subscript, `_` cannot\n+    if (delim == b'*' || delim == b'^') && !is_punctuation(next_char) {\n         return true;\n     }\n     if delim == b'~' && run_len > 1 {\n         return true;\n     }\n     let prev_char = s[..ix].chars().last().unwrap();\n-    if delim == b'~' && prev_char == '~' && !is_punctuation(next_char) {\n+    if delim == b'~'\n+        && (prev_char == '~' || options.contains(Options::ENABLE_SUBSCRIPT))\n+        && !is_punctuation(next_char)\n+    {\n         return true;\n     }\n \n@@ -2304,6 +2317,7 @@ fn delim_run_can_close(\n     run_len: usize,\n     ix: usize,\n     mode: TableParseMode,\n+    options: Options,\n ) -> bool {\n     if ix == 0 {\n         return false;\n@@ -2326,11 +2340,13 @@ fn delim_run_can_close(\n         }\n     }\n     let delim = suffix.bytes().next().unwrap();\n-    // `*` and `~~` can be intraword, `_` and `~` cannot\n-    if (delim == b'*' || (delim == b'~' && run_len > 1)) && !is_punctuation(prev_char) {\n+    // `*`, `~~`, and `^` can be intraword, `~` can only be interword if it's subscript, `_` cannot\n+    if (delim == b'*' || delim == b'^' || (delim == b'~' && run_len > 1))\n+        && !is_punctuation(prev_char)\n+    {\n         return true;\n     }\n-    if delim == b'~' && prev_char == '~' {\n+    if delim == b'~' && (prev_char == '~' || options.contains(Options::ENABLE_SUBSCRIPT)) {\n         return true;\n     }\n \n", "test_patch": "diff --git a/pulldown-cmark/tests/suite/super_sub.rs b/pulldown-cmark/tests/suite/super_sub.rs\nindex a9c0c84a..6c2492bf 100644\n--- a/pulldown-cmark/tests/suite/super_sub.rs\n+++ b/pulldown-cmark/tests/suite/super_sub.rs\n@@ -37,7 +37,7 @@ fn super_sub_test_3() {\n fn super_sub_test_4() {\n     let original = r##\"~This~is~nothing~\n \"##;\n-    let expected = r##\"<p><sub>This~is~nothing</sub></p>\n+    let expected = r##\"<p><sub>This</sub>is<sub>nothing</sub></p>\n \"##;\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n@@ -45,9 +45,9 @@ fn super_sub_test_4() {\n \n #[test]\n fn super_sub_test_5() {\n-    let original = r##\"~This ~~is stricken.~\n+    let original = r##\"~This ~~is not stricken.~\n \"##;\n-    let expected = r##\"<p><sub>This ~~is stricken.</sub></p>\n+    let expected = r##\"<p><sub>This ~~is not stricken.</sub></p>\n \"##;\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n@@ -55,6 +55,16 @@ fn super_sub_test_5() {\n \n #[test]\n fn super_sub_test_6() {\n+    let original = r##\"~~This ~is~~ stricken.~\n+\"##;\n+    let expected = r##\"<p><del>This ~is</del> stricken.~</p>\n+\"##;\n+\n+    test_markdown_html(original, expected, false, false, false, true, false, false);\n+}\n+\n+#[test]\n+fn super_sub_test_7() {\n     let original = r##\"~This ~~is stricken~ but this is not~~\n \"##;\n     let expected = r##\"<p><sub>This ~~is stricken</sub> but this is not~~</p>\n@@ -62,3 +72,16 @@ fn super_sub_test_6() {\n \n     test_markdown_html(original, expected, false, false, false, true, false, false);\n }\n+\n+#[test]\n+fn super_sub_test_8() {\n+    let original = r##\"H~2~O\n+\n+y=x^2^a+xb+c\n+\"##;\n+    let expected = r##\"<p>H<sub>2</sub>O</p>\n+<p>y=x<sup>2</sup>a+xb+c</p>\n+\"##;\n+\n+    test_markdown_html(original, expected, false, false, false, true, false, false);\n+}\n", "problem_statement": "Superscript and subscript parsing inconsistent with Pandoc and markdown-it\n`pulldown-cmark` requires whitespace around superscripted and subscripted text whereas [Pandoc](https://pandoc.org/try/?params=%7B%22text%22%3A%22H%7E2%7E0%22%2C%22to%22%3A%22html5%22%2C%22from%22%3A%22commonmark_x%22%2C%22standalone%22%3Afalse%2C%22embed-resources%22%3Afalse%2C%22table-of-contents%22%3Afalse%2C%22number-sections%22%3Afalse%2C%22citeproc%22%3Afalse%2C%22html-math-method%22%3A%22plain%22%2C%22wrap%22%3A%22auto%22%2C%22highlight-style%22%3Anull%2C%22files%22%3A%7B%7D%2C%22template%22%3Anull%7D) and [markdown-it](https://markdown-it.github.io/#md3=%7B%22source%22%3A%22H~2~0%22%2C%22defaults%22%3A%7B%22html%22%3Afalse%2C%22xhtmlOut%22%3Afalse%2C%22breaks%22%3Afalse%2C%22langPrefix%22%3A%22language-%22%2C%22linkify%22%3Atrue%2C%22typographer%22%3Atrue%2C%22_highlight%22%3Atrue%2C%22_strict%22%3Afalse%2C%22_view%22%3A%22html%22%7D%7D) do not.\n\n```shell\n$ echo 'H~2~0' | cargo r --quiet -- --enable-subscript -e\n0..6: Start(Paragraph)\n0..5: Text(Borrowed(\"H~2~0\"))\n0..6: End(Paragraph)\nEOF\n```\n\n```shell\n$ echo 'H ~2~ 0' | cargo r --quiet -- --enable-subscript -e\n0..8: Start(Paragraph)\n0..2: Text(Borrowed(\"H \"))\n2..5: Start(Subscript)\n3..4: Text(Borrowed(\"2\"))\n2..5: End(Subscript)\n5..7: Text(Borrowed(\" 0\"))\n0..8: End(Paragraph)\nEOF\n```\n", "hints_text": "", "created_at": "2025-02-18 00:17:17", "merge_commit_sha": "5d05c3c61abf24d1644f16cb60f6f35bc049a4d3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['pulldown-cmark (stable)', '.github/workflows/rust.yml']", "['pulldown-cmark (1.71.1)', '.github/workflows/rust.yml']"], ["['build', '.github/workflows/book.yml']", "['pulldown-cmark (nightly)', '.github/workflows/rust.yml']"]]}
{"repo": "slint-ui/slint", "instance_id": "slint-ui__slint-7327", "base_commit": "9504a4f09005d257bfd8f5c0d3adf2bd2845c112", "patch": "diff --git a/internal/backends/qt/qt_window.rs b/internal/backends/qt/qt_window.rs\nindex dc6e52d5424..08e8ffe4c3b 100644\n--- a/internal/backends/qt/qt_window.rs\n+++ b/internal/backends/qt/qt_window.rs\n@@ -158,17 +158,23 @@ cpp! {{\n             isMouseButtonDown = false;\n \n             void *parent_of_popup_to_close = nullptr;\n+            int popup_id_to_close = 0;\n             if (auto p = dynamic_cast<const SlintWidget*>(parent())) {\n                 while (auto pp = dynamic_cast<const SlintWidget*>(p->parent())) {\n                     p = pp;\n                 }\n                 void *parent_window = p->rust_window;\n                 bool inside = rect().contains(event->pos());\n-                bool close_on_click = rust!(Slint_mouseReleaseEventPopup [parent_window: &QtWindow as \"void*\", inside: bool as \"bool\"] -> bool as \"bool\" {\n-                    let close_policy = parent_window.top_close_policy();\n-                    close_policy == PopupClosePolicy::CloseOnClick || (close_policy == PopupClosePolicy::CloseOnClickOutside && !inside)\n+                popup_id_to_close = rust!(Slint_mouseReleaseEventPopup [parent_window: &QtWindow as \"void*\", inside: bool as \"bool\"] -> u32 as \"int\" {\n+                    let active_popups = WindowInner::from_pub(&parent_window.window).active_popups();\n+                    if let Some(popup) = active_popups.last() {\n+                        if popup.close_policy == PopupClosePolicy::CloseOnClick || (popup.close_policy == PopupClosePolicy::CloseOnClickOutside && !inside) {\n+                            return popup.popup_id.get();\n+                        }\n+                    }\n+                    0\n                 });\n-                if (close_on_click) {\n+                if (popup_id_to_close) {\n                     parent_of_popup_to_close = parent_window;\n                 }\n             }\n@@ -180,9 +186,9 @@ cpp! {{\n                 let button = from_qt_button(button);\n                 rust_window.mouse_event(MouseEvent::Released{ position, button, click_count: 0 })\n             });\n-            if (parent_of_popup_to_close) {\n-                rust!(Slint_mouseReleaseEventClosePopup [parent_of_popup_to_close: &QtWindow as \"void*\"] {\n-                    parent_of_popup_to_close.close_top_popup();\n+            if (popup_id_to_close) {\n+                rust!(Slint_mouseReleaseEventClosePopup [parent_of_popup_to_close: &QtWindow as \"void*\", popup_id_to_close: std::num::NonZeroU32 as \"int\"] {\n+                    WindowInner::from_pub(&parent_of_popup_to_close.window).close_popup(popup_id_to_close);\n                 });\n             }\n         }\n@@ -1750,14 +1756,6 @@ impl QtWindow {\n         timer_event();\n     }\n \n-    fn close_top_popup(&self) {\n-        WindowInner::from_pub(&self.window).close_top_popup();\n-    }\n-\n-    fn top_close_policy(&self) -> PopupClosePolicy {\n-        WindowInner::from_pub(&self.window).top_close_policy()\n-    }\n-\n     fn window_state_event(&self) {\n         let widget_ptr = self.widget_ptr();\n \ndiff --git a/internal/core/window.rs b/internal/core/window.rs\nindex e25a80076e9..2ce23d83c84 100644\n--- a/internal/core/window.rs\n+++ b/internal/core/window.rs\n@@ -393,13 +393,13 @@ pub enum PopupWindowLocation {\n #[derive(Clone)]\n pub struct PopupWindow {\n     /// The ID of the associated popup.\n-    popup_id: NonZeroU32,\n+    pub popup_id: NonZeroU32,\n     /// The location defines where the pop up is rendered.\n     pub location: PopupWindowLocation,\n     /// The component that is responsible for providing the popup content.\n     pub component: ItemTreeRc,\n     /// Defines the close behaviour of the popup.\n-    close_policy: PopupClosePolicy,\n+    pub close_policy: PopupClosePolicy,\n     /// the item that had the focus in the parent window when the popup was opened\n     focus_item_in_parent: ItemWeak,\n     /// The item from where the Popup was invoked from\n@@ -596,8 +596,30 @@ impl WindowInner {\n             self.had_popup_on_press.set(!self.active_popups.borrow().is_empty());\n         }\n \n-        let close_policy = self.top_close_policy();\n-        let mut mouse_inside_popup = false;\n+        let popup_to_close = self.active_popups.borrow().last().and_then(|popup| {\n+            let mouse_inside_popup = || {\n+                if let PopupWindowLocation::ChildWindow(coordinates) = &popup.location {\n+                    event.position().map_or(true, |pos| {\n+                        ItemTreeRc::borrow_pin(&popup.component)\n+                            .as_ref()\n+                            .item_geometry(0)\n+                            .contains(pos - coordinates.to_vector())\n+                    })\n+                } else {\n+                    false\n+                }\n+            };\n+            match popup.close_policy {\n+                PopupClosePolicy::CloseOnClick => {\n+                    let mouse_inside_popup = mouse_inside_popup();\n+                    (mouse_inside_popup && released_event && self.had_popup_on_press.get())\n+                        || (!mouse_inside_popup && pressed_event)\n+                }\n+                PopupClosePolicy::CloseOnClickOutside => !mouse_inside_popup() && pressed_event,\n+                PopupClosePolicy::NoAutoClose => false,\n+            }\n+            .then_some(popup.popup_id)\n+        });\n \n         mouse_input_state = if let Some(mut event) =\n             crate::input::handle_mouse_grab(event, &window_adapter, &mut mouse_input_state)\n@@ -610,7 +632,7 @@ impl WindowInner {\n             {\n                 let geom = ItemTreeRc::borrow_pin(component).as_ref().item_geometry(0);\n \n-                mouse_inside_popup = event\n+                let mouse_inside_popup = event\n                     .position()\n                     .map_or(true, |pos| geom.contains(pos - coordinates.to_vector()));\n \n@@ -655,21 +677,9 @@ impl WindowInner {\n \n         self.mouse_input_state.set(mouse_input_state);\n \n-        match close_policy {\n-            PopupClosePolicy::CloseOnClick => {\n-                if (mouse_inside_popup && released_event && self.had_popup_on_press.get())\n-                    || (!mouse_inside_popup && pressed_event)\n-                {\n-                    self.close_top_popup();\n-                }\n-            }\n-            PopupClosePolicy::CloseOnClickOutside => {\n-                if !mouse_inside_popup && pressed_event {\n-                    self.close_top_popup();\n-                }\n-            }\n-            PopupClosePolicy::NoAutoClose => {}\n-        };\n+        if let Some(popup_id) = popup_to_close {\n+            self.close_popup(popup_id);\n+        }\n \n         crate::properties::ChangeTracker::run_change_handlers();\n     }\n@@ -1203,14 +1213,6 @@ impl WindowInner {\n         }\n     }\n \n-    /// Returns the close policy of the top-most popup. PopupClosePolicy::NoAutoClose if there is no active popup.\n-    pub fn top_close_policy(&self) -> PopupClosePolicy {\n-        self.active_popups\n-            .borrow()\n-            .last()\n-            .map_or(PopupClosePolicy::NoAutoClose, |popup| popup.close_policy)\n-    }\n-\n     /// Returns the scale factor set on the window, as provided by the windowing system.\n     pub fn scale_factor(&self) -> f32 {\n         self.pinned_fields.as_ref().project_ref().scale_factor.get()\n", "test_patch": "diff --git a/tests/cases/elements/popupwindow_nested_close-on-click.slint b/tests/cases/elements/popupwindow_nested_close-on-click.slint\nnew file mode 100644\nindex 00000000000..69a3a60ff3f\n--- /dev/null\n+++ b/tests/cases/elements/popupwindow_nested_close-on-click.slint\n@@ -0,0 +1,115 @@\n+// Copyright \u00a9 SixtyFPS GmbH <info@slint.dev>\n+// SPDX-License-Identifier: GPL-3.0-only OR LicenseRef-Slint-Royalty-free-2.0 OR LicenseRef-Slint-Software-3.0\n+\n+// https://github.com/slint-ui/slint/issues/7322\n+// A PopupWindow with the default close-on-click policy should close when clicked, even if the click open a nested popup\n+\n+export component TestCase {\n+    width: 300px;\n+    height: 300px;\n+\n+    in-out property <int> popup1-clicked;\n+    in-out property <int> popup2-clicked;\n+    in-out property <int> root-clicked;\n+\n+    popup1 := PopupWindow {\n+        Rectangle {\n+            background: yellow;\n+        }\n+\n+        x: 10px;\n+        y: 10px;\n+        height: 50px;\n+        width: 50px;\n+\n+        TouchArea {\n+            clicked => {\n+                popup1-clicked += 1;\n+                popup2.show();\n+            }\n+        }\n+    }\n+\n+    popup2 := PopupWindow {\n+        Rectangle {\n+            background: red;\n+        }\n+\n+        x: 40px;\n+        y: 40px;\n+        height: 50px;\n+        width: 50px;\n+\n+        TouchArea {\n+            clicked => {\n+                popup2-clicked += 1;\n+            }\n+        }\n+    }\n+\n+    TouchArea {\n+        clicked => {\n+            root-clicked += 1;\n+            popup1.show();\n+        }\n+    }\n+}\n+\n+/*\n+\n+```rust\n+\n+let instance = TestCase::new().unwrap();\n+\n+\n+slint_testing::send_mouse_click(&instance, 15., 15.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 0);\n+assert_eq!(instance.get_popup2_clicked(), 0);\n+\n+// popup1 is open\n+\n+slint_testing::send_mouse_click(&instance, 15., 15.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 0);\n+\n+// popup2 is open, popup1 is closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// all popup closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// popup1 is open\n+// click where popup2 will be\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// popup2 is open, popup1 is closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 2);\n+\n+// all popup closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 3);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 2);\n+\n+```\n+\n+*/\n\\ No newline at end of file\n", "problem_statement": "Qt backend: Nested PopupWindow is not shown\n### Bug Description\n\nOpening a `PopupWindow` from within another `PopupWindow` does work on WASM and winit, but not with the Qt backend. With the Qt backend, only the first popup is opened. When calling `show()` on the second one, nothing happens.\r\n\r\nSide note: With the given example, clicking into the yellow popup does not work in the bottom right corner (where the second popup is placed). This is independent of the renderer (reproducible in SlintPad) so I'm not sure if this is expected behavior or not. I'd assume the second popup should not intercept click events in its area while it is not shown...\n\n### Reproducible Code (if applicable)\n\n```slint\nexport component AppWindow inherits Window {\r\n    width: 100px;\r\n    height: 100px;\r\n\r\n    popup1 := PopupWindow {\r\n        Rectangle {\r\n            background: yellow;\r\n        }\r\n\r\n        x: 10px;\r\n        y: 10px;\r\n        height: 50px;\r\n        width: 50px;\r\n\r\n        TouchArea {\r\n            clicked => {\r\n                popup2.show();\r\n            }\r\n        }\r\n    }\r\n\r\n    popup2 := PopupWindow {\r\n        Rectangle {\r\n            background: red;\r\n        }\r\n\r\n        x: 40px;\r\n        y: 40px;\r\n        height: 50px;\r\n        width: 50px;\r\n    }\r\n\r\n    TouchArea {\r\n        clicked => {\r\n            popup1.show();\r\n        }\r\n    }\r\n}\n```\n\n\n### Environment Details\n\n- Slint Version: 1.9.1\r\n- Platform/OS: Linux\r\n- Backend/Renderer: Qt\r\n\n\n### Product Impact\n\n_No response_\n", "hints_text": "Another issue with `PopupWindow` with Qt backend:\r\n- On Wayland, popups stay at their absolute screen position even when moving the application window around. With winit the popup keeps its relative window position, i.e. moves with the application window as desired.\r\n- On X11, a popup is somehow globally modal, not just application modal. While a popup is open, no other application receives click events anymore. Instead, any click event outside the application window will just close the popup.\r\n\r\nSlint 1.9.1 on Ubuntu 22.04.\nThe problem is that the yellow PopupWindow's close policy is set to close on click, so the release event is supposed to close the yellow popup.\r\nBut the thing is that the release event also open the red popup.\r\n\r\nSo while delivering the release event this happens\r\n 1. check that there is currently a popup (yes, the yellow popup)\r\n 2. clicked callback open the yellow popup\r\n 3. attempt to close the yellow popup by closing the top popup. Ooops, this closes the red popup instead\r\n\r\nThe thing is that with winit, there is an additional check that the mouse cursor is inside the popup in step 3, and this fails because it uses the red instead of the yellow. Which is why your testcase \"works\" when not clicking in the bottom-right corner.\r\n\r\nSo two possible fix: either\r\n - Fix the step 3 to close the right popup always (the yellow popup, which will also result of the closing of the red one) \r\n - Or if a popup is getting open while processing the event, do not close any popup (which i think is what you expect, but is probably technically wrong.\r\n\r\n\n> - Fix the step 3 to close the right popup always (the yellow popup, which will also result of the closing of the red one)\r\n> - Or if a popup is getting open while processing the event, do not close any popup (which i think is what you expect, but is probably technically wrong.\r\n\r\nActually in my particular case I expect the yellow popup to close when clicked, leaving only the red one open. Not sure if everyone would expect this behavior but at least the yellow popups close policy is set to close if there's a click. That the click also opens a new popup is a quite specific case but IMHO (by default) it should not have an impact on the close behavior of the yellow popup.\r\n\r\nEDIT: Btw my yellow popup is in the real use-case a menu item and the red popup a normal popup (like a message box). This is a very common use-case and it is expected that the menu closes when an item is triggered, no matter what action it executes.", "created_at": "2025-01-10 18:00:41", "merge_commit_sha": "e34c19325c82ab2323da6264a573a0a794d0fd67", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build_and_test (ubuntu-22.04, nightly)', '.github/workflows/ci.yaml']", "['miri', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (macos-13)', '.github/workflows/ci.yaml']", "['wasm', '.github/workflows/ci.yaml']"], ["['tree-sitter-tests', '.github/workflows/ci.yaml']", "['cpp_cmake (macos-14, 1.77)', '.github/workflows/ci.yaml']"], ["['build_and_test (ubuntu-22.04, 1.77)', '.github/workflows/ci.yaml']", "['build_and_test (windows-2022, beta, --exclude ffmpeg --exclude gstreamer-player)', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (windows-2022)', '.github/workflows/ci.yaml']", "['ffi_32bit_build', '.github/workflows/ci.yaml']"], ["['build_and_test (windows-2022, 1.77)', '.github/workflows/ci.yaml']", "['mcu_esp', '.github/workflows/ci.yaml']"], ["['build_and_test (windows-2022, stable)', '.github/workflows/ci.yaml']", "['build-slintpad', '.github/workflows/ci.yaml']"], ["['esp-idf-quick', '.github/workflows/ci.yaml']", "['python_test (macos-14)', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (ubuntu-22.04)', '.github/workflows/ci.yaml']", "['cpp_cmake (windows-2022, nightly)', '.github/workflows/ci.yaml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-489", "base_commit": "b4eda6979d3a44ef376adda377b385d10ab85003", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 8fdfd5e6..2dae9a17 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,10 @@\n \n All notable changes to MiniJinja are documented here.\n \n+## 1.0.20\n+\n+- Added support for implicit string concatenation for Jinja2 compatibility.  #489\n+\n ## 1.0.19\n \n - Deprecated `Value::from_iterator` and introduced replacement\ndiff --git a/minijinja/src/compiler/parser.rs b/minijinja/src/compiler/parser.rs\nindex 4236cf56..c6f6720c 100644\n--- a/minijinja/src/compiler/parser.rs\n+++ b/minijinja/src/compiler/parser.rs\n@@ -534,7 +534,7 @@ impl<'a> Parser<'a> {\n         let (token, span) = expect_token!(self, \"expression\");\n         macro_rules! const_val {\n             ($expr:expr) => {\n-                make_const(Value::from($expr), span)\n+                make_const(Value::from($expr), self.stream.expand_span(span))\n             };\n         }\n \n@@ -543,7 +543,18 @@ impl<'a> Parser<'a> {\n             Token::Ident(\"false\" | \"False\") => Ok(const_val!(false)),\n             Token::Ident(\"none\" | \"None\") => Ok(const_val!(())),\n             Token::Ident(name) => Ok(ast::Expr::Var(Spanned::new(ast::Var { id: name }, span))),\n-            Token::Str(val) => Ok(const_val!(val)),\n+            Token::Str(val) => {\n+                if matches_token!(self, Token::Str(_)) {\n+                    let mut buf = String::from(val);\n+                    while let Some((Token::Str(s), _)) = ok!(self.stream.current()) {\n+                        buf.push_str(s);\n+                        ok!(self.stream.next());\n+                    }\n+                    Ok(const_val!(buf))\n+                } else {\n+                    Ok(const_val!(val))\n+                }\n+            }\n             Token::String(val) => Ok(const_val!(val)),\n             Token::Int(val) => Ok(const_val!(val)),\n             Token::Int128(val) => Ok(const_val!(val)),\n", "test_patch": "diff --git a/minijinja/tests/parser-inputs/string-implicit-concat.txt b/minijinja/tests/parser-inputs/string-implicit-concat.txt\nnew file mode 100644\nindex 00000000..a0c6285a\n--- /dev/null\n+++ b/minijinja/tests/parser-inputs/string-implicit-concat.txt\n@@ -0,0 +1,3 @@\n+{{ \"foo\" }}\n+{{ \"foo\" \"bar\" }}\n+{{ \"foo\" \"bar\" \"baz\" }}\ndiff --git a/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap b/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap\nnew file mode 100644\nindex 00000000..231267f9\n--- /dev/null\n+++ b/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap\n@@ -0,0 +1,32 @@\n+---\n+source: minijinja/tests/test_parser.rs\n+description: \"{{ \\\"foo\\\" }}\\n{{ \\\"foo\\\" \\\"bar\\\" }}\\n{{ \\\"foo\\\" \\\"bar\\\" \\\"baz\\\" }}\"\n+input_file: minijinja/tests/parser-inputs/string-implicit-concat.txt\n+---\n+Ok(\n+    Template {\n+        children: [\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foo\",\n+                } @ 1:3-1:8,\n+            } @ 1:0-1:8,\n+            EmitRaw {\n+                raw: \"\\n\",\n+            } @ 1:11-2:0,\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foobar\",\n+                } @ 2:3-2:14,\n+            } @ 2:0-2:14,\n+            EmitRaw {\n+                raw: \"\\n\",\n+            } @ 2:17-3:0,\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foobarbaz\",\n+                } @ 3:3-3:20,\n+            } @ 3:0-3:20,\n+        ],\n+    } @ 0:0-3:23,\n+)\n", "problem_statement": "String concatenation without \"+\" operator\nJinja allowed Python strings as `\"abc\" \"def\"` for concatenation at \"compile time\". For compatibility, it would be helpful if minijinja did as well.\n", "hints_text": "Oh well. I completely forgot that this is a thing. Apparently there since 2008. https://github.com/pallets/jinja/commit/4778bda8312875a943e3f16df3efc5f6ffb1e586", "created_at": "2024-04-10 18:30:20", "merge_commit_sha": "1b0dd3a7c5cdd8a985e63dc2bec7f2da504287ff", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check on 1.61.0 (32bit)', '.github/workflows/tests.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['build', '.github/workflows/clippy.yml']"], ["['Test on nightly', '.github/workflows/tests.yml']", "['Test on 1.61.0', '.github/workflows/tests.yml']"]]}
{"repo": "Shopify/bluejay", "instance_id": "Shopify__bluejay-34", "base_commit": "536a324711e11949e7da67109f97f0b6bf7f9bd6", "patch": "diff --git a/bluejay-parser/src/ast/definition/enum_value_definition.rs b/bluejay-parser/src/ast/definition/enum_value_definition.rs\nindex a9ef7b7..761f933 100644\n--- a/bluejay-parser/src/ast/definition/enum_value_definition.rs\n+++ b/bluejay-parser/src/ast/definition/enum_value_definition.rs\n@@ -1,8 +1,11 @@\n-use crate::ast::{\n-    definition::{Context, Directives},\n-    ConstDirectives, FromTokens, ParseError, Tokens, TryFromTokens,\n-};\n use crate::lexical_token::{Name, StringValue};\n+use crate::{\n+    ast::{\n+        definition::{Context, Directives},\n+        ConstDirectives, FromTokens, ParseError, Tokens, TryFromTokens,\n+    },\n+    HasSpan,\n+};\n use bluejay_core::definition::{EnumValueDefinition as CoreEnumValueDefinition, HasDirectives};\n \n #[derive(Debug)]\n@@ -32,6 +35,13 @@ impl<'a, C: Context> FromTokens<'a> for EnumValueDefinition<'a, C> {\n     fn from_tokens(tokens: &mut impl Tokens<'a>) -> Result<Self, ParseError> {\n         let description = tokens.next_if_string_value();\n         let name = tokens.expect_name()?;\n+        if matches!(name.as_str(), \"null\" | \"true\" | \"false\") {\n+            return Err(ParseError::InvalidEnumValue {\n+                span: name.span().clone(),\n+                value: name.as_str().to_string(),\n+            });\n+        }\n+\n         let directives = ConstDirectives::try_from_tokens(tokens).transpose()?;\n         Ok(Self {\n             description,\ndiff --git a/bluejay-parser/src/ast/parse_error.rs b/bluejay-parser/src/ast/parse_error.rs\nindex dd826cc..880142f 100644\n--- a/bluejay-parser/src/ast/parse_error.rs\n+++ b/bluejay-parser/src/ast/parse_error.rs\n@@ -3,6 +3,10 @@ use crate::Span;\n \n #[derive(Debug)]\n pub enum ParseError {\n+    InvalidEnumValue {\n+        span: Span,\n+        value: String,\n+    },\n     ExpectedOneOf {\n         span: Span,\n         values: &'static [&'static str],\n@@ -26,6 +30,14 @@ pub enum ParseError {\n impl From<ParseError> for Error {\n     fn from(val: ParseError) -> Self {\n         match val {\n+            ParseError::InvalidEnumValue { span, value } => Self::new(\n+                \"Parse error\",\n+                Some(Annotation::new(\n+                    format!(\"{value} is not an allowed enum value\"),\n+                    span,\n+                )),\n+                Vec::new(),\n+            ),\n             ParseError::ExpectedOneOf { span, values } => Self::new(\n                 \"Parse error\",\n                 Some(Annotation::new(\n", "test_patch": "diff --git a/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap b/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap\nnew file mode 100644\nindex 0000000..294fe82\n--- /dev/null\n+++ b/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap\n@@ -0,0 +1,12 @@\n+---\n+source: bluejay-parser/tests/schema_definition_integration_test.rs\n+expression: formatted_errors\n+input_file: bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\n+---\n+Error: Parse error\n+   \u256d\u2500[<unknown>:2:3]\n+   \u2502\n+ 2 \u2502   true\n+   \u2502   \u2500\u2500\u252c\u2500  \n+   \u2502     \u2570\u2500\u2500\u2500 true is not an allowed enum value\n+\u2500\u2500\u2500\u256f\ndiff --git a/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql b/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\nnew file mode 100644\nindex 0000000..ede8c0c\n--- /dev/null\n+++ b/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\n@@ -0,0 +1,7 @@\n+enum Test {\n+  true\n+}\n+\n+schema {\n+  query: Test\n+}\n", "problem_statement": "Parse error for enum value definitions `true`, `false`, and `null`\nPer the [spec](https://spec.graphql.org/draft/#sel-HAFdTDCAEAAAAEBCAE8oE) enum values must be:\r\n> _Name_ but not `true` or `false` or `null`\r\n\r\nWe should return a parse error when parsing enum value definitions where the name is invalid\n", "hints_text": "", "created_at": "2024-06-28 14:58:46", "merge_commit_sha": "c823823cb1819a4c40af2f05dce83bb68bfe84d7", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['lint', '.github/workflows/rust.yml']", "['test', '.github/workflows/rust.yml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-502", "base_commit": "f2ee26297403bcfa571d773b17a467970663efcf", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 01384460..978b0bb5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,11 @@\n \n All notable changes to MiniJinja are documented here.\n \n+## 1.0.21\n+\n+- Fixed an issue where `lstrip_blocks` unintentionally also applied to\n+  variable expression blocks.  #502\n+\n ## 1.0.20\n \n - Added support for implicit string concatenation for Jinja2 compatibility.  #489\ndiff --git a/minijinja/src/compiler/lexer.rs b/minijinja/src/compiler/lexer.rs\nindex e535527b..cb413689 100644\n--- a/minijinja/src/compiler/lexer.rs\n+++ b/minijinja/src/compiler/lexer.rs\n@@ -491,6 +491,7 @@ impl<'s> Tokenizer<'s> {\n         }\n     }\n \n+    syntax_token_getter!(variable_start, \"{{\");\n     syntax_token_getter!(variable_end, \"}}\");\n     syntax_token_getter!(block_start, \"{%\");\n     syntax_token_getter!(block_end, \"%}\");\n@@ -506,7 +507,11 @@ impl<'s> Tokenizer<'s> {\n         }\n         let old_loc = self.loc();\n         let (lead, span) = match find_start_marker(self.rest, &self.syntax_config) {\n-            Some((start, Whitespace::Default)) if self.ws_config.lstrip_blocks => {\n+            Some((start, Whitespace::Default))\n+                if self.ws_config.lstrip_blocks\n+                    && self.rest.get(start..start + self.variable_start().len())\n+                        != Some(self.variable_start()) =>\n+            {\n                 let peeked = &self.rest[..start];\n                 let trimmed = lstrip_block(peeked);\n                 let lead = self.advance(trimmed.len());\n", "test_patch": "diff --git a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\nindex 6e88b380..8c3446c0 100644\n--- a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\n+++ b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\n@@ -47,8 +47,8 @@ Ident(\"seq\")\n   \"seq\"\n BlockEnd\n   \"+%}\"\n-TemplateData(\"\\n\")\n-  \"\\n\"\n+TemplateData(\"\\n    \")\n+  \"\\n    \"\n VariableStart\n   \"{{\"\n Ident(\"item\")\n@@ -65,4 +65,3 @@ BlockEnd\n   \"+%}\"\n TemplateData(\"\\n</ul>\")\n   \"\\n</ul>\"\n-\ndiff --git a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\nindex f5241b4f..84f43eb3 100644\n--- a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\n+++ b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\n@@ -47,6 +47,8 @@ Ident(\"seq\")\n   \"seq\"\n BlockEnd\n   \"%}\"\n+TemplateData(\"    \")\n+  \"    \"\n VariableStart\n   \"{{\"\n Ident(\"item\")\n@@ -63,4 +65,3 @@ BlockEnd\n   \"%}\"\n TemplateData(\"</ul>\")\n   \"</ul>\"\n-\n", "problem_statement": "set_lstrip_blocks should not apply to expressions, only to blocks.\n## Description\r\n\r\nI've been using the newly added support for set_lstrip_blocks. Thank you for that! I've noticed, though, that i think it is also stripping leading spaces from expressions -- {{ }}. Those aren't blocks, right? If an expression is positioned with leading whitespace in the file, it should stay that way, I think.\r\n\r\nHere's the way it works in python:\r\n\r\n```\r\n[16:52:32][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ python3 script.py\r\n    one\r\n    two\r\n\r\n(venv)\r\n[16:52:33][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ cat script.py\r\nimport jinja2\r\ntemplateLoader = jinja2.FileSystemLoader(searchpath=\".\")\r\ntemplateEnv = jinja2.Environment(\r\n  loader=templateLoader,\r\n  lstrip_blocks=True,\r\n  trim_blocks=True\r\n)\r\ntemplate = templateEnv.get_template('tmpl.tmp')\r\ntemplateVars = {\r\n  \"things\": [\r\n    \"one\",\r\n    \"two\",\r\n  ]\r\n}\r\n\r\nprint(template.render(templateVars))\r\n(venv)\r\n[16:52:48][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ cat tmpl.tmp\r\n{% for thing in things %}\r\n    {{ thing }}\r\n{% endfor %}\r\n```\r\n\r\n## Reproduction steps\r\n\r\nDo the above, in rust:\r\n\r\n```\r\nuse minijinja::{context, path_loader, Environment};\r\n\r\nfn main() {\r\n    let mut env = Environment::new();\r\n    env.set_trim_blocks(true);\r\n    env.set_lstrip_blocks(true);\r\n    env.set_loader(path_loader(\".\"));\r\n    let tmpl = env.get_template(\"tmpl.tmp\").unwrap();\r\n    println!(\r\n        \"{}\",\r\n        tmpl.render(context! {things => vec![\"one\", \"two\"]})\r\n            .unwrap()\r\n    )\r\n}\r\n\r\n```\r\n\r\nand get this output:\r\n\r\n```\r\n$ cargo run --bin testtmpl\r\n   Compiling rules v0.1.0 (/Users/kylederr/Code/perpetualsystems/simba/rules)\r\n    Finished dev [unoptimized + debuginfo] target(s) in 0.17s\r\n     Running `/Users/kylederr/Code/perpetualsystems/simba/target/debug/testtmpl`\r\none\r\ntwo\r\n```\r\n\r\n(note they are not indented)\r\n\r\nAdditional helpful information:\r\n\r\n- Version of minijinja: 1.0.20\r\n- Version of rustc: rustc 1.76.0 (07dca489a 2024-02-04)\r\n- Operating system and version: macos sonoma 14.2.1\r\n\r\n## What did you expect\r\n\r\nFor the blocks to be indented, as per above.\r\n\r\n\r\n\n", "hints_text": "", "created_at": "2024-04-24 07:40:08", "merge_commit_sha": "4639d099792aabe9f0d764f4b162897e8e76b6ac", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check on 1.61.0 (32bit)', '.github/workflows/tests.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['build', '.github/workflows/clippy.yml']"], ["[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']", "['Test on nightly', '.github/workflows/tests.yml']"], ["['Test on 1.61.0', '.github/workflows/tests.yml']", "['Test on Latest Stable', '.github/workflows/tests.yml']"]]}
{"repo": "denoland/deno", "instance_id": "denoland__deno-27871", "base_commit": "b7456fed703b148ed627b80afc3ec4c01e1eafef", "patch": "diff --git a/ext/node/ops/http.rs b/ext/node/ops/http.rs\nindex 57bcf69a47cf3c..1da630f97edebd 100644\n--- a/ext/node/ops/http.rs\n+++ b/ext/node/ops/http.rs\n@@ -113,6 +113,9 @@ pub enum ConnError {\n   #[error(\"Invalid URL {0}\")]\n   InvalidUrl(Url),\n   #[class(type)]\n+  #[error(\"Invalid Path {0}\")]\n+  InvalidPath(String),\n+  #[class(type)]\n   #[error(transparent)]\n   InvalidHeaderName(#[from] http::header::InvalidHeaderName),\n   #[class(type)]\n@@ -150,6 +153,7 @@ pub async fn op_node_http_request_with_conn<P>(\n   state: Rc<RefCell<OpState>>,\n   #[serde] method: ByteString,\n   #[string] url: String,\n+  #[string] request_path: Option<String>,\n   #[serde] headers: Vec<(ByteString, ByteString)>,\n   #[smi] body: Option<ResourceId>,\n   #[smi] conn_rid: ResourceId,\n@@ -247,11 +251,17 @@ where\n   *request.method_mut() = method.clone();\n   let path = url_parsed.path();\n   let query = url_parsed.query();\n-  *request.uri_mut() = query\n-    .map(|q| format!(\"{}?{}\", path, q))\n-    .unwrap_or_else(|| path.to_string())\n-    .parse()\n-    .map_err(|_| ConnError::InvalidUrl(url_parsed.clone()))?;\n+  if let Some(request_path) = request_path {\n+    *request.uri_mut() = request_path\n+      .parse()\n+      .map_err(|_| ConnError::InvalidPath(request_path.clone()))?;\n+  } else {\n+    *request.uri_mut() = query\n+      .map(|q| format!(\"{}?{}\", path, q))\n+      .unwrap_or_else(|| path.to_string())\n+      .parse()\n+      .map_err(|_| ConnError::InvalidUrl(url_parsed.clone()))?;\n+  }\n   *request.headers_mut() = header_map;\n \n   if let Some((username, password)) = maybe_authority {\ndiff --git a/ext/node/polyfills/http.ts b/ext/node/polyfills/http.ts\nindex dd94c9d025fd95..0438f9af22e6c7 100644\n--- a/ext/node/polyfills/http.ts\n+++ b/ext/node/polyfills/http.ts\n@@ -479,6 +479,7 @@ class ClientRequest extends OutgoingMessage {\n         this._req = await op_node_http_request_with_conn(\n           this.method,\n           url,\n+          this._createRequestPath(),\n           headers,\n           this._bodyWriteRid,\n           baseConnRid,\n@@ -817,6 +818,15 @@ class ClientRequest extends OutgoingMessage {\n     return url.href;\n   }\n \n+  _createRequestPath(): string | undefined {\n+    // If the path starts with protocol, pass this to op_node_http_request_with_conn\n+    // This will be used as Request.uri in hyper for supporting http proxy\n+    if (this.path?.startsWith(\"http://\") || this.path?.startsWith(\"https://\")) {\n+      return this.path;\n+    }\n+    return undefined;\n+  }\n+\n   setTimeout(msecs: number, callback?: () => void) {\n     if (msecs === 0) {\n       if (this._timeout) {\n", "test_patch": "diff --git a/tests/unit_node/http_test.ts b/tests/unit_node/http_test.ts\nindex b4f0d260aa0906..54803ab995f511 100644\n--- a/tests/unit_node/http_test.ts\n+++ b/tests/unit_node/http_test.ts\n@@ -1892,3 +1892,27 @@ Deno.test(\"[node/http] an error with DNS propagates to request object\", async ()\n   });\n   await promise;\n });\n+\n+Deno.test(\"[node/http] supports proxy http request\", async () => {\n+  const { promise, resolve } = Promise.withResolvers<void>();\n+  const server = Deno.serve({ port: 0, onListen }, (req) => {\n+    console.log(\"server received\", req.url);\n+    assertEquals(req.url, \"http://example.com/\");\n+    return new Response(\"ok\");\n+  });\n+\n+  function onListen({ port }: { port: number }) {\n+    http.request({\n+      host: \"localhost\",\n+      port,\n+      path: \"http://example.com\",\n+    }, async (res) => {\n+      assertEquals(res.statusCode, 200);\n+      assertEquals(await text(res), \"ok\");\n+      resolve();\n+      server.shutdown();\n+    }).end();\n+  }\n+  await promise;\n+  await server.finished;\n+});\n", "problem_statement": "Wrong path when using HTTP proxy in Node API `http`\nVersion: Deno 2.1.5\n\nTLDR: when `http.request`'s opinions include proxy settings and `options.path` is a full URL (e.g., `http://httpbin.org/200`), the website that proxy server accesses should be `http://httpbin.org/200`, but in Deno it is `http://httpbin.org/http://httpbin.org/200`.\n\nPOC:\n```js\nimport axios from \"axios\";\nconst instance = axios.create({\n  baseURL: \"http://httpbin.org/\",\n  proxy: {\n    protocol: \"http\",\n    host: \"127.0.0.1\",\n    port: 8892,\n  },\n});\ntry {\n  const resp = await instance.get(\"/get\");\n  console.log(\"success\", resp.config);\n} catch (e) {\n  console.log(e);\n}\n```\n\nFor convenience, I use Axios as demo, and related code is [here](https://github.com/axios/axios/blob/bad6d8b97b52c0c15311c92dd596fc0bff122651/lib/adapters/http.js#L464C11-L464C28). When I add a patch `options.path = new URL(options.path).pathname;`, it becomes correct.\nThis demo requires a http proxy running at port 8892 (such as Fiddler or mitmproxy).\n\nIn Node.JS, it can get `http://httpbin.org/get` correctly. In Deno, it gets a 404, while Fiddler shows it is requesting `http://httpbin.org/http://httpbin.org/get`\n", "hints_text": "", "created_at": "2025-01-30 06:50:01", "merge_commit_sha": "1cecc0a8b0229ec922727ea7ce6f71872a11bc51", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test release windows-x86_64', '.github/workflows/ci.yml']", "['test debug windows-x86_64', '.github/workflows/ci.yml']"], ["['lint debug macos-x86_64', '.github/workflows/ci.yml']", "['lint debug windows-x86_64', '.github/workflows/ci.yml']"], ["['test debug macos-x86_64', '.github/workflows/ci.yml']", "['publish canary', '.github/workflows/ci.yml']"], ["['test release macos-aarch64', '.github/workflows/ci.yml']", "['test release macos-x86_64', '.github/workflows/ci.yml']"]]}
{"repo": "denoland/deno", "instance_id": "denoland__deno-28102", "base_commit": "4921411bb2d010458b1193c25f98c5fab2925c02", "patch": "diff --git a/cli/args/flags.rs b/cli/args/flags.rs\nindex e00af3a83e5a5e..6cf8d684ef65ed 100644\n--- a/cli/args/flags.rs\n+++ b/cli/args/flags.rs\n@@ -1772,7 +1772,7 @@ If you specify a directory instead of a file, the path is expanded to all contai\n }\n \n fn bundle_subcommand() -> Command {\n-  command(\"bundle\",  \"\u26a0\ufe0f `deno bundle` was removed in Deno 2.\n+  command(\"bundle\",  \"`deno bundle` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\", UnstableArgsConfig::ResolutionOnly)\n     .hide(true)\n@@ -3321,7 +3321,7 @@ different location, use the <c>--output</> flag:\n \n fn vendor_subcommand() -> Command {\n   command(\"vendor\",\n-      \"\u26a0\ufe0f `deno vendor` was removed in Deno 2.\n+      \"`deno vendor` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\",\n       UnstableArgsConfig::ResolutionOnly\n", "test_patch": "diff --git a/tests/specs/bundle/removed/bundle_help.out b/tests/specs/bundle/removed/bundle_help.out\nindex 349043baaf43d2..e1f43dfea61891 100644\n--- a/tests/specs/bundle/removed/bundle_help.out\n+++ b/tests/specs/bundle/removed/bundle_help.out\n@@ -1,4 +1,4 @@\n-\u26a0\ufe0f `deno bundle` was removed in Deno 2.\n+`deno bundle` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\n \ndiff --git a/tests/specs/vendor/removed/vendor_help.out b/tests/specs/vendor/removed/vendor_help.out\nindex aeda5176ff0fb9..98aca359d76ced 100644\n--- a/tests/specs/vendor/removed/vendor_help.out\n+++ b/tests/specs/vendor/removed/vendor_help.out\n@@ -1,4 +1,4 @@\n-\u26a0\ufe0f `deno vendor` was removed in Deno 2.\n+`deno vendor` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\n \n", "problem_statement": "CLI completions output for PowerShell mangled on Windows\nVersion: deno 2.0.2 (stable, release, x86_64-pc-windows-msvc)\n\nShell: PowerShell (pwsh) 7.4.6\nTerminal: Windows Terminal Preview 1.23.10353.0\n\nI'm initialising deno completions in PowerShell profile using the example from documentation:\n```PowerShell\nif (Get-Command -Name \"deno\" -ErrorAction SilentlyContinue) {\n    deno completions powershell | Out-String | Invoke-Expression\n}\n```\n\nWhich results in PowerShell error when starting new terminal instance:\n```pwsh\n\nInvoke-Expression: C:\\Users\\andis\\Documents\\PowerShell\\Completions\\deno.ps1:2\nLine |\n   2 |      deno completions powershell | Out-String | Invoke-Expression\n     |                                                 ~~~~~~~~~~~~~~~~~\n     | Missing ')' in method call.\n```\n\n\nI looked at the output using redirection to a file `deno completions powershell | Out-File deno-completions.tmp.ps1` and noticed a strange string of 6 characters on line 114 of the output: `\u014c\u00dc\u0100\u2019\u0116\u00c5`.\n```pwsh\n[CompletionResult]::new('bundle', 'bundle', [CompletionResultType]::ParameterValue, '\u014c\u00dc\u0100\u2019\u0116\u00c5 `deno bundle` was removed in Deno 2.  See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations')\n```\n\nCompare the the output from `deno completions powershell` without involving PowerShell by running the command in cmd shell and redirecting output to a file:\n```pwsh\n[CompletionResult]::new('bundle', 'bundle', [CompletionResultType]::ParameterValue, '\u26a0\ufe0f `deno bundle` was removed in Deno 2.  See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations')\n```\n\n\n\nIt seems that PowerShell redirection is mangling the UTF-8 character (\u26a0\ufe0f) even though it shouldn't because terminal is using UTF-8 and all involved files are UTF-8 encoded.\n\nJust to make sure it's not my environment or some Windows charset translation shenanigans I tried to execute the unmangled script as written by `deno completions powershell > deno-comletions.tmp2.ps1` and it works as expected.\n\n\n## Possible workaround\nSet charset to UTF-8 explicitly in PowerShell profile file before loading completions:\n```pwsh\n$OutputEncoding = [Console]::OutputEncoding = [Text.UTF8Encoding]::UTF8\n```\n", "hints_text": "", "created_at": "2025-02-13 10:08:24", "merge_commit_sha": "d7203c9522a91582e7e361053d0cf16e2f934273", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test release windows-x86_64', '.github/workflows/ci.yml']", "['test debug windows-x86_64', '.github/workflows/ci.yml']"], ["['lint debug macos-x86_64', '.github/workflows/ci.yml']", "['lint debug windows-x86_64', '.github/workflows/ci.yml']"], ["['test debug macos-x86_64', '.github/workflows/ci.yml']", "['publish canary', '.github/workflows/ci.yml']"], ["['test release macos-aarch64', '.github/workflows/ci.yml']", "['test release macos-x86_64', '.github/workflows/ci.yml']"]]}
{"repo": "greyblake/nutype", "instance_id": "greyblake__nutype-184", "base_commit": "265c0ddbd49ca1c44f707f9c305161e56a7d30f9", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 0c7a8b5..48c0f27 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -329,6 +329,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"regex\",\n+ \"rustc_version\",\n  \"syn 2.0.66\",\n  \"urlencoding\",\n ]\n@@ -426,6 +427,15 @@ dependencies = [\n  \"serde_derive\",\n ]\n \n+[[package]]\n+name = \"rustc_version\"\n+version = \"0.4.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"cfcb3a22ef46e85b45de6ee7e79d063319ebb6594faafcf1c225ea92ab6e9b92\"\n+dependencies = [\n+ \"semver\",\n+]\n+\n [[package]]\n name = \"ryu\"\n version = \"1.0.13\"\n@@ -456,6 +466,12 @@ dependencies = [\n  \"syn 1.0.109\",\n ]\n \n+[[package]]\n+name = \"semver\"\n+version = \"1.0.23\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"61697e0a1c7e512e84a621326239844a24d8207b4669b41bc18b32ea5cbf988b\"\n+\n [[package]]\n name = \"serde\"\n version = \"1.0.203\"\ndiff --git a/nutype_macros/Cargo.toml b/nutype_macros/Cargo.toml\nindex fd57dcc..6b0ce3f 100644\n--- a/nutype_macros/Cargo.toml\n+++ b/nutype_macros/Cargo.toml\n@@ -26,6 +26,12 @@ cfg-if = \"1.0\"\n kinded = \"0.3.0\"\n urlencoding = \"2.0\"\n \n+[build-dependencies]\n+rustc_version = \"0.4.1\"\n+\n+[lints.rust]\n+unexpected_cfgs = { level = \"warn\", check-cfg = ['cfg(ERROR_IN_CORE)'] }\n+\n [lib]\n proc-macro = true\n \ndiff --git a/nutype_macros/build.rs b/nutype_macros/build.rs\nnew file mode 100644\nindex 0000000..8041087\n--- /dev/null\n+++ b/nutype_macros/build.rs\n@@ -0,0 +1,20 @@\n+use rustc_version::{version, version_meta, Channel};\n+\n+fn main() {\n+    let version = version().expect(\"Couldn't get Rust version\");\n+    let version_meta = version_meta().expect(\"Couldn't get Rust channel\");\n+\n+    // Assert we haven't travelled back in time\n+    assert!(\n+        version.major >= 1,\n+        \"How did you get a version before 1.0.0?\"\n+    );\n+\n+    // Generic setting\n+    println!(\"cargo:rerun-if-changed=build.rs\");\n+\n+    // feature `error-in-core` landed in rust 1.81.0\n+    if matches!(version_meta.channel, Channel::Nightly) || version.minor >= 81 {\n+        println!(\"cargo:rustc-cfg=ERROR_IN_CORE\");\n+    }\n+}\ndiff --git a/nutype_macros/src/common/gen/error.rs b/nutype_macros/src/common/gen/error.rs\nindex 64fb8e7..af9e8cf 100644\n--- a/nutype_macros/src/common/gen/error.rs\n+++ b/nutype_macros/src/common/gen/error.rs\n@@ -11,15 +11,22 @@ pub fn gen_error_type_name(type_name: &TypeName) -> ErrorTypePath {\n     ErrorTypePath::new(ident)\n }\n \n-// NOTE: There is no `::core::error::Error` yet in stable Rust.\n-// So for `no_std` we just don't implement `Error` trait.\n+// NOTE: `::core::error::Error` is stable only for rust >= 1.81.0.\n #[allow(unused_variables)]\n pub fn gen_impl_error_trait(error_type_path: &ErrorTypePath) -> TokenStream {\n     cfg_if! {\n-        if #[cfg(feature = \"std\")] {\n+        if #[cfg(any(ERROR_IN_CORE, feature = \"std\"))] {\n+            cfg_if! {\n+                if #[cfg(ERROR_IN_CORE)] {\n+                    let error = quote! { ::core::error::Error };\n+                } else {\n+                    let error = quote! { ::std::error::Error };\n+                }\n+            };\n+\n             quote! {\n-                impl ::std::error::Error for #error_type_path {\n-                    fn source(&self) -> Option<&(dyn ::std::error::Error + 'static)> {\n+                impl #error for #error_type_path {\n+                    fn source(&self) -> Option<&(dyn #error + 'static)> {\n                         None\n                     }\n                 }\ndiff --git a/nutype_macros/src/common/gen/parse_error.rs b/nutype_macros/src/common/gen/parse_error.rs\nindex 2aa8e87..b2d07d6 100644\n--- a/nutype_macros/src/common/gen/parse_error.rs\n+++ b/nutype_macros/src/common/gen/parse_error.rs\n@@ -68,27 +68,33 @@ pub fn gen_def_parse_error(\n     };\n \n     cfg_if! {\n-        if #[cfg(feature = \"std\")] {\n+        if #[cfg(any(ERROR_IN_CORE, feature = \"std\"))] {\n+            cfg_if! {\n+                if #[cfg(ERROR_IN_CORE)] {\n+                    let error = quote! { ::core::error::Error };\n+                } else {\n+                    let error = quote! { ::std::error::Error };\n+                }\n+            };\n             let generics_with_fromstr_and_debug_bounds = add_bound_to_all_type_params(\n                 &generics_with_fromstr_bound,\n                 syn::parse_quote!(::core::fmt::Debug),\n             );\n-            let impl_std_error = quote! {\n-                impl #generics_with_fromstr_and_debug_bounds ::std::error::Error for #parse_error_type_name #generics_without_bounds {\n-                    fn source(&self) -> Option<&(dyn ::std::error::Error + 'static)> {\n+            let impl_error = quote! {\n+                impl #generics_with_fromstr_and_debug_bounds #error for #parse_error_type_name #generics_without_bounds {\n+                    fn source(&self) -> Option<&(dyn #error + 'static)> {\n                         None\n                     }\n                 }\n             };\n         } else {\n-            // NOTE: There is no `::core::error::Error` yet in stable Rust.\n-            // So for `no_std` we just don't implement `Error` trait.\n-            let impl_std_error = quote! {};\n+            // NOTE: `::core::error::Error` is stable only for rust >= 1.81.0.\n+            let impl_error = quote! {};\n         }\n     };\n \n     quote! {\n         #definition\n-        #impl_std_error\n+        #impl_error\n     }\n }\n", "test_patch": "", "problem_statement": "Generate core::error::Error for no_std\nRust  1.81.0 stabilizes `core::error::Error`:  https://blog.rust-lang.org/2024/09/05/Rust-1.81.0.html#whats-in-1810-stable\r\n\r\nWith that we should able to remove the condition here:\r\nhttps://github.com/greyblake/nutype/blob/master/nutype_macros/src/common/gen/error.rs#L14-L31\r\n\r\nAnd generate `core::error::Error` instead of `::std::error::Error`.\r\n\r\nSince it's a breaking (the older versions of Rust will not be properly supported), add a `[BREAKING]` note change to the CHANGELOG.\n", "hints_text": "", "created_at": "2024-09-13 09:10:12", "merge_commit_sha": "d398ce8057eef97ba8ce0646a7db790c9acc7994", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Unit Tests', '.github/workflows/ci.yml']", "['Rustfmt', '.github/workflows/ci.yml']"], ["['Clippy', '.github/workflows/ci.yml']", "['Doctests', '.github/workflows/ci.yml']"]]}
{"repo": "denoland/deno", "instance_id": "denoland__deno-27871", "base_commit": "b7456fed703b148ed627b80afc3ec4c01e1eafef", "patch": "diff --git a/ext/node/ops/http.rs b/ext/node/ops/http.rs\nindex 57bcf69a47cf3c..1da630f97edebd 100644\n--- a/ext/node/ops/http.rs\n+++ b/ext/node/ops/http.rs\n@@ -113,6 +113,9 @@ pub enum ConnError {\n   #[error(\"Invalid URL {0}\")]\n   InvalidUrl(Url),\n   #[class(type)]\n+  #[error(\"Invalid Path {0}\")]\n+  InvalidPath(String),\n+  #[class(type)]\n   #[error(transparent)]\n   InvalidHeaderName(#[from] http::header::InvalidHeaderName),\n   #[class(type)]\n@@ -150,6 +153,7 @@ pub async fn op_node_http_request_with_conn<P>(\n   state: Rc<RefCell<OpState>>,\n   #[serde] method: ByteString,\n   #[string] url: String,\n+  #[string] request_path: Option<String>,\n   #[serde] headers: Vec<(ByteString, ByteString)>,\n   #[smi] body: Option<ResourceId>,\n   #[smi] conn_rid: ResourceId,\n@@ -247,11 +251,17 @@ where\n   *request.method_mut() = method.clone();\n   let path = url_parsed.path();\n   let query = url_parsed.query();\n-  *request.uri_mut() = query\n-    .map(|q| format!(\"{}?{}\", path, q))\n-    .unwrap_or_else(|| path.to_string())\n-    .parse()\n-    .map_err(|_| ConnError::InvalidUrl(url_parsed.clone()))?;\n+  if let Some(request_path) = request_path {\n+    *request.uri_mut() = request_path\n+      .parse()\n+      .map_err(|_| ConnError::InvalidPath(request_path.clone()))?;\n+  } else {\n+    *request.uri_mut() = query\n+      .map(|q| format!(\"{}?{}\", path, q))\n+      .unwrap_or_else(|| path.to_string())\n+      .parse()\n+      .map_err(|_| ConnError::InvalidUrl(url_parsed.clone()))?;\n+  }\n   *request.headers_mut() = header_map;\n \n   if let Some((username, password)) = maybe_authority {\ndiff --git a/ext/node/polyfills/http.ts b/ext/node/polyfills/http.ts\nindex dd94c9d025fd95..0438f9af22e6c7 100644\n--- a/ext/node/polyfills/http.ts\n+++ b/ext/node/polyfills/http.ts\n@@ -479,6 +479,7 @@ class ClientRequest extends OutgoingMessage {\n         this._req = await op_node_http_request_with_conn(\n           this.method,\n           url,\n+          this._createRequestPath(),\n           headers,\n           this._bodyWriteRid,\n           baseConnRid,\n@@ -817,6 +818,15 @@ class ClientRequest extends OutgoingMessage {\n     return url.href;\n   }\n \n+  _createRequestPath(): string | undefined {\n+    // If the path starts with protocol, pass this to op_node_http_request_with_conn\n+    // This will be used as Request.uri in hyper for supporting http proxy\n+    if (this.path?.startsWith(\"http://\") || this.path?.startsWith(\"https://\")) {\n+      return this.path;\n+    }\n+    return undefined;\n+  }\n+\n   setTimeout(msecs: number, callback?: () => void) {\n     if (msecs === 0) {\n       if (this._timeout) {\n", "test_patch": "diff --git a/tests/unit_node/http_test.ts b/tests/unit_node/http_test.ts\nindex b4f0d260aa0906..54803ab995f511 100644\n--- a/tests/unit_node/http_test.ts\n+++ b/tests/unit_node/http_test.ts\n@@ -1892,3 +1892,27 @@ Deno.test(\"[node/http] an error with DNS propagates to request object\", async ()\n   });\n   await promise;\n });\n+\n+Deno.test(\"[node/http] supports proxy http request\", async () => {\n+  const { promise, resolve } = Promise.withResolvers<void>();\n+  const server = Deno.serve({ port: 0, onListen }, (req) => {\n+    console.log(\"server received\", req.url);\n+    assertEquals(req.url, \"http://example.com/\");\n+    return new Response(\"ok\");\n+  });\n+\n+  function onListen({ port }: { port: number }) {\n+    http.request({\n+      host: \"localhost\",\n+      port,\n+      path: \"http://example.com\",\n+    }, async (res) => {\n+      assertEquals(res.statusCode, 200);\n+      assertEquals(await text(res), \"ok\");\n+      resolve();\n+      server.shutdown();\n+    }).end();\n+  }\n+  await promise;\n+  await server.finished;\n+});\n", "problem_statement": "Wrong path when using HTTP proxy in Node API `http`\nVersion: Deno 2.1.5\n\nTLDR: when `http.request`'s opinions include proxy settings and `options.path` is a full URL (e.g., `http://httpbin.org/200`), the website that proxy server accesses should be `http://httpbin.org/200`, but in Deno it is `http://httpbin.org/http://httpbin.org/200`.\n\nPOC:\n```js\nimport axios from \"axios\";\nconst instance = axios.create({\n  baseURL: \"http://httpbin.org/\",\n  proxy: {\n    protocol: \"http\",\n    host: \"127.0.0.1\",\n    port: 8892,\n  },\n});\ntry {\n  const resp = await instance.get(\"/get\");\n  console.log(\"success\", resp.config);\n} catch (e) {\n  console.log(e);\n}\n```\n\nFor convenience, I use Axios as demo, and related code is [here](https://github.com/axios/axios/blob/bad6d8b97b52c0c15311c92dd596fc0bff122651/lib/adapters/http.js#L464C11-L464C28). When I add a patch `options.path = new URL(options.path).pathname;`, it becomes correct.\nThis demo requires a http proxy running at port 8892 (such as Fiddler or mitmproxy).\n\nIn Node.JS, it can get `http://httpbin.org/get` correctly. In Deno, it gets a 404, while Fiddler shows it is requesting `http://httpbin.org/http://httpbin.org/get`\n", "hints_text": "", "created_at": "2025-01-30 06:50:01", "merge_commit_sha": "1cecc0a8b0229ec922727ea7ce6f71872a11bc51", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test release windows-x86_64', '.github/workflows/ci.yml']", "['test debug windows-x86_64', '.github/workflows/ci.yml']"], ["['lint debug macos-x86_64', '.github/workflows/ci.yml']", "['lint debug windows-x86_64', '.github/workflows/ci.yml']"], ["['test debug macos-x86_64', '.github/workflows/ci.yml']", "['publish canary', '.github/workflows/ci.yml']"], ["['test release macos-aarch64', '.github/workflows/ci.yml']", "['test release macos-x86_64', '.github/workflows/ci.yml']"]]}
{"repo": "kevinmehall/rust-peg", "instance_id": "kevinmehall__rust-peg-385", "base_commit": "fc7d27f0a5c52666f645bec6ec9be001f171b411", "patch": "diff --git a/peg-macros/grammar.rs b/peg-macros/grammar.rs\nindex f7debea..0ea43e8 100644\n--- a/peg-macros/grammar.rs\n+++ b/peg-macros/grammar.rs\n@@ -1388,7 +1388,7 @@ pub mod peg {\n                                                     ::peg::RuleResult::Failed => break,\n                                                 }\n                                             };\n-                                            let __step_res = match __parse_rust_type(\n+                                            let __step_res = match __parse_rust_ty_param_bound(\n                                                 __input,\n                                                 __state,\n                                                 __err_state,\n@@ -1456,19 +1456,24 @@ pub mod peg {\n                                                                 ::peg::RuleResult::Failed => break,\n                                                             }\n                                                         };\n-                                                        let __step_res = match __parse_rust_type(\n-                                                            __input,\n-                                                            __state,\n-                                                            __err_state,\n-                                                            __pos,\n-                                                        ) {\n-                                                            ::peg::RuleResult::Matched(pos, _) => {\n-                                                                ::peg::RuleResult::Matched(pos, ())\n-                                                            }\n-                                                            ::peg::RuleResult::Failed => {\n-                                                                ::peg::RuleResult::Failed\n-                                                            }\n-                                                        };\n+                                                        let __step_res =\n+                                                            match __parse_rust_ty_param_bound(\n+                                                                __input,\n+                                                                __state,\n+                                                                __err_state,\n+                                                                __pos,\n+                                                            ) {\n+                                                                ::peg::RuleResult::Matched(\n+                                                                    pos,\n+                                                                    _,\n+                                                                ) => ::peg::RuleResult::Matched(\n+                                                                    pos,\n+                                                                    (),\n+                                                                ),\n+                                                                ::peg::RuleResult::Failed => {\n+                                                                    ::peg::RuleResult::Failed\n+                                                                }\n+                                                            };\n                                                         match __step_res {\n                                                             ::peg::RuleResult::Matched(\n                                                                 __newpos,\n@@ -2036,45 +2041,24 @@ pub mod peg {\n                                     ::peg::RuleResult::Matched(__pos, __value)\n                                 }\n                                 ::peg::RuleResult::Failed => {\n-                                    let __seq_res =\n-                                        match match ::peg::ParseLiteral::parse_string_literal(\n-                                            __input, __pos, \"for\",\n-                                        ) {\n-                                            ::peg::RuleResult::Matched(__pos, __val) => {\n-                                                let __seq_res = match __parse_rust_ty_params(\n-                                                    __input,\n-                                                    __state,\n-                                                    __err_state,\n-                                                    __pos,\n-                                                ) {\n-                                                    ::peg::RuleResult::Matched(pos, _) => {\n-                                                        ::peg::RuleResult::Matched(pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                };\n-                                                match __seq_res {\n-                                                    ::peg::RuleResult::Matched(__pos, _) => {\n-                                                        ::peg::RuleResult::Matched(__pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                }\n-                                            }\n-                                            ::peg::RuleResult::Failed => {\n-                                                __err_state.mark_failure(__pos, \"\\\"for\\\"\");\n-                                                ::peg::RuleResult::Failed\n-                                            }\n-                                        } {\n-                                            ::peg::RuleResult::Matched(__newpos, _) => {\n-                                                ::peg::RuleResult::Matched(__newpos, ())\n-                                            }\n-                                            ::peg::RuleResult::Failed => {\n-                                                ::peg::RuleResult::Matched(__pos, ())\n-                                            }\n-                                        };\n+                                    let __seq_res = match match __parse_rust_for_lifetimes(\n+                                        __input,\n+                                        __state,\n+                                        __err_state,\n+                                        __pos,\n+                                    ) {\n+                                        ::peg::RuleResult::Matched(pos, _) => {\n+                                            ::peg::RuleResult::Matched(pos, ())\n+                                        }\n+                                        ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                    } {\n+                                        ::peg::RuleResult::Matched(__newpos, _) => {\n+                                            ::peg::RuleResult::Matched(__newpos, ())\n+                                        }\n+                                        ::peg::RuleResult::Failed => {\n+                                            ::peg::RuleResult::Matched(__pos, ())\n+                                        }\n+                                    };\n                                     match __seq_res {\n                                         ::peg::RuleResult::Matched(__pos, _) => {\n                                             let __seq_res = match __parse_rust_type(\n@@ -2112,10 +2096,7 @@ pub mod peg {\n                                                                         let __sep_res = match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"+\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"+\\\"\") ; :: peg :: RuleResult :: Failed } } ;\n                                                                         match __sep_res { :: peg :: RuleResult :: Matched (__newpos , _) => { __newpos } , :: peg :: RuleResult :: Failed => break , }\n                                                                     };\n-                                                                    let __step_res = {\n-                                                                        let __choice_res = match __parse_LIFETIME (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n-                                                                        match __choice_res { :: peg :: RuleResult :: Matched (__pos , __value) => :: peg :: RuleResult :: Matched (__pos , __value) , :: peg :: RuleResult :: Failed => { let __seq_res = match match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"?\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"?\\\"\") ; :: peg :: RuleResult :: Failed } } { :: peg :: RuleResult :: Matched (__newpos , _) => { :: peg :: RuleResult :: Matched (__newpos , ()) } , :: peg :: RuleResult :: Failed => { :: peg :: RuleResult :: Matched (__pos , ()) } , } ; match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { { let __seq_res = match __parse_rust_ty_path (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ; match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { :: peg :: RuleResult :: Matched (__pos , ()) } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } } } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } } }\n-                                                                    };\n+                                                                    let __step_res = match __parse_rust_ty_param_bound (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n                                                                     match __step_res { :: peg :: RuleResult :: Matched (__newpos , __value) => { __repeat_pos = __newpos ; __repeat_value . push (__value) ; } , :: peg :: RuleResult :: Failed => { break ; } }\n                                                                 }\n                                                                 if __repeat_value.len() >= 1 {\n@@ -2347,52 +2328,17 @@ pub mod peg {\n                                                     ::peg::RuleResult::Failed => break,\n                                                 }\n                                             };\n-                                            let __step_res = {\n-                                                let __choice_res = match __parse_LIFETIME(\n-                                                    __input,\n-                                                    __state,\n-                                                    __err_state,\n-                                                    __pos,\n-                                                ) {\n-                                                    ::peg::RuleResult::Matched(pos, _) => {\n-                                                        ::peg::RuleResult::Matched(pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                };\n-                                                match __choice_res {\n-                                                    ::peg::RuleResult::Matched(__pos, __value) => {\n-                                                        ::peg::RuleResult::Matched(__pos, __value)\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        let __seq_res = match match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"?\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"?\\\"\") ; :: peg :: RuleResult :: Failed } } { :: peg :: RuleResult :: Matched (__newpos , _) => { :: peg :: RuleResult :: Matched (__newpos , ()) } , :: peg :: RuleResult :: Failed => { :: peg :: RuleResult :: Matched (__pos , ()) } , } ;\n-                                                        match __seq_res {\n-                                                            ::peg::RuleResult::Matched(\n-                                                                __pos,\n-                                                                _,\n-                                                            ) => {\n-                                                                let __seq_res = match __parse_rust_ty_path (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n-                                                                match __seq_res {\n-                                                                    ::peg::RuleResult::Matched(\n-                                                                        __pos,\n-                                                                        _,\n-                                                                    ) => {\n-                                                                        ::peg::RuleResult::Matched(\n-                                                                            __pos,\n-                                                                            (),\n-                                                                        )\n-                                                                    }\n-                                                                    ::peg::RuleResult::Failed => {\n-                                                                        ::peg::RuleResult::Failed\n-                                                                    }\n-                                                                }\n-                                                            }\n-                                                            ::peg::RuleResult::Failed => {\n-                                                                ::peg::RuleResult::Failed\n-                                                            }\n-                                                        }\n-                                                    }\n+                                            let __step_res = match __parse_rust_ty_param_bound(\n+                                                __input,\n+                                                __state,\n+                                                __err_state,\n+                                                __pos,\n+                                            ) {\n+                                                ::peg::RuleResult::Matched(pos, _) => {\n+                                                    ::peg::RuleResult::Matched(pos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n                                                 }\n                                             };\n                                             match __step_res {\n@@ -2441,6 +2387,194 @@ pub mod peg {\n             }\n         }\n     }\n+    fn __parse_rust_for_lifetimes<'input>(\n+        __input: &'input Input,\n+        __state: &mut ParseState<'input>,\n+        __err_state: &mut ::peg::error::ErrorState,\n+        __pos: usize,\n+    ) -> ::peg::RuleResult<()> {\n+        #![allow(non_snake_case, unused, clippy::redundant_closure_call)]\n+        match ::peg::ParseLiteral::parse_string_literal(__input, __pos, \"for\") {\n+            ::peg::RuleResult::Matched(__pos, __val) => {\n+                let __seq_res = match __parse_rust_ty_params(__input, __state, __err_state, __pos) {\n+                    ::peg::RuleResult::Matched(pos, _) => ::peg::RuleResult::Matched(pos, ()),\n+                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                };\n+                match __seq_res {\n+                    ::peg::RuleResult::Matched(__pos, _) => ::peg::RuleResult::Matched(__pos, ()),\n+                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                }\n+            }\n+            ::peg::RuleResult::Failed => {\n+                __err_state.mark_failure(__pos, \"\\\"for\\\"\");\n+                ::peg::RuleResult::Failed\n+            }\n+        }\n+    }\n+    fn __parse_rust_ty_param_bound<'input>(\n+        __input: &'input Input,\n+        __state: &mut ParseState<'input>,\n+        __err_state: &mut ::peg::error::ErrorState,\n+        __pos: usize,\n+    ) -> ::peg::RuleResult<()> {\n+        #![allow(non_snake_case, unused, clippy::redundant_closure_call)]\n+        {\n+            let __choice_res = match __parse_LIFETIME(__input, __state, __err_state, __pos) {\n+                ::peg::RuleResult::Matched(pos, _) => ::peg::RuleResult::Matched(pos, ()),\n+                ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+            };\n+            match __choice_res {\n+                ::peg::RuleResult::Matched(__pos, __value) => {\n+                    ::peg::RuleResult::Matched(__pos, __value)\n+                }\n+                ::peg::RuleResult::Failed => {\n+                    let __choice_res = {\n+                        let __seq_res = match match ::peg::ParseLiteral::parse_string_literal(\n+                            __input, __pos, \"?\",\n+                        ) {\n+                            ::peg::RuleResult::Matched(__pos, __val) => {\n+                                ::peg::RuleResult::Matched(__pos, __val)\n+                            }\n+                            ::peg::RuleResult::Failed => {\n+                                __err_state.mark_failure(__pos, \"\\\"?\\\"\");\n+                                ::peg::RuleResult::Failed\n+                            }\n+                        } {\n+                            ::peg::RuleResult::Matched(__newpos, _) => {\n+                                ::peg::RuleResult::Matched(__newpos, ())\n+                            }\n+                            ::peg::RuleResult::Failed => ::peg::RuleResult::Matched(__pos, ()),\n+                        };\n+                        match __seq_res {\n+                            ::peg::RuleResult::Matched(__pos, _) => {\n+                                let __seq_res = match match __parse_rust_for_lifetimes(\n+                                    __input,\n+                                    __state,\n+                                    __err_state,\n+                                    __pos,\n+                                ) {\n+                                    ::peg::RuleResult::Matched(pos, _) => {\n+                                        ::peg::RuleResult::Matched(pos, ())\n+                                    }\n+                                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                } {\n+                                    ::peg::RuleResult::Matched(__newpos, _) => {\n+                                        ::peg::RuleResult::Matched(__newpos, ())\n+                                    }\n+                                    ::peg::RuleResult::Failed => {\n+                                        ::peg::RuleResult::Matched(__pos, ())\n+                                    }\n+                                };\n+                                match __seq_res {\n+                                    ::peg::RuleResult::Matched(__pos, _) => {\n+                                        let __seq_res = match __parse_rust_ty_path(\n+                                            __input,\n+                                            __state,\n+                                            __err_state,\n+                                            __pos,\n+                                        ) {\n+                                            ::peg::RuleResult::Matched(pos, _) => {\n+                                                ::peg::RuleResult::Matched(pos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                        };\n+                                        match __seq_res {\n+                                            ::peg::RuleResult::Matched(__pos, _) => {\n+                                                ::peg::RuleResult::Matched(__pos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                        }\n+                                    }\n+                                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                }\n+                            }\n+                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                        }\n+                    };\n+                    match __choice_res {\n+                        ::peg::RuleResult::Matched(__pos, __value) => {\n+                            ::peg::RuleResult::Matched(__pos, __value)\n+                        }\n+                        ::peg::RuleResult::Failed => {\n+                            match ::peg::ParseLiteral::parse_string_literal(__input, __pos, \"(\") {\n+                                ::peg::RuleResult::Matched(__pos, __val) => {\n+                                    let __seq_res =\n+                                        match match ::peg::ParseLiteral::parse_string_literal(\n+                                            __input, __pos, \"?\",\n+                                        ) {\n+                                            ::peg::RuleResult::Matched(__pos, __val) => {\n+                                                ::peg::RuleResult::Matched(__pos, __val)\n+                                            }\n+                                            ::peg::RuleResult::Failed => {\n+                                                __err_state.mark_failure(__pos, \"\\\"?\\\"\");\n+                                                ::peg::RuleResult::Failed\n+                                            }\n+                                        } {\n+                                            ::peg::RuleResult::Matched(__newpos, _) => {\n+                                                ::peg::RuleResult::Matched(__newpos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => {\n+                                                ::peg::RuleResult::Matched(__pos, ())\n+                                            }\n+                                        };\n+                                    match __seq_res {\n+                                        ::peg::RuleResult::Matched(__pos, _) => {\n+                                            let __seq_res = match match __parse_rust_for_lifetimes(\n+                                                __input,\n+                                                __state,\n+                                                __err_state,\n+                                                __pos,\n+                                            ) {\n+                                                ::peg::RuleResult::Matched(pos, _) => {\n+                                                    ::peg::RuleResult::Matched(pos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n+                                                }\n+                                            } {\n+                                                ::peg::RuleResult::Matched(__newpos, _) => {\n+                                                    ::peg::RuleResult::Matched(__newpos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Matched(__pos, ())\n+                                                }\n+                                            };\n+                                            match __seq_res {\n+                                                ::peg::RuleResult::Matched(__pos, _) => {\n+                                                    let __seq_res = match __parse_rust_ty_path(\n+                                                        __input,\n+                                                        __state,\n+                                                        __err_state,\n+                                                        __pos,\n+                                                    ) {\n+                                                        ::peg::RuleResult::Matched(pos, _) => {\n+                                                            ::peg::RuleResult::Matched(pos, ())\n+                                                        }\n+                                                        ::peg::RuleResult::Failed => {\n+                                                            ::peg::RuleResult::Failed\n+                                                        }\n+                                                    };\n+                                                    match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \")\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , ()) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\")\\\"\") ; :: peg :: RuleResult :: Failed } } } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , }\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n+                                                }\n+                                            }\n+                                        }\n+                                        ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                    }\n+                                }\n+                                ::peg::RuleResult::Failed => {\n+                                    __err_state.mark_failure(__pos, \"\\\"(\\\"\");\n+                                    ::peg::RuleResult::Failed\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n     fn __parse_expression<'input>(\n         __input: &'input Input,\n         __state: &mut ParseState<'input>,\ndiff --git a/peg-macros/grammar.rustpeg b/peg-macros/grammar.rustpeg\nindex 7fdafdb..a534d04 100644\n--- a/peg-macros/grammar.rustpeg\n+++ b/peg-macros/grammar.rustpeg\n@@ -59,8 +59,8 @@ rule rust_path()\n rule rust_type()\n     = BRACKET_GROUP()\n     / \"&\" LIFETIME()? \"mut\"? rust_type()\n-    / \"dyn\" rust_type() ++ \"+\"\n-    / \"impl\" rust_type() ++ \"+\"\n+    / \"dyn\" rust_ty_param_bound() ++ \"+\"\n+    / \"impl\" rust_ty_param_bound() ++ \"+\"\n     / \"(\" (rust_type() ++ \",\" \",\"?)? \")\"\n     / (\"<\" rust_type() (\"as\" rust_ty_path())? \">\")? rust_ty_path()\n \n@@ -73,12 +73,20 @@ rule rust_ty_params() -> Vec<TokenStream>\n rule rust_where_clause()\n     = \"where\" (\n         LIFETIME() (\":\" LIFETIME() ++ \"+\")?\n-        / (\"for\" rust_ty_params())? rust_type() \":\" (LIFETIME() / \"?\"? rust_ty_path()) ++ \"+\"\n+        / rust_for_lifetimes()? rust_type() \":\" rust_ty_param_bound() ++ \"+\"\n     ) ** \",\" \",\"?\n \n rule rust_generic_param()\n     = LIFETIME() (\":\" LIFETIME() ++ \"+\")?\n-    / IDENT() (\":\"(LIFETIME() / \"?\"? rust_ty_path()) ++ \"+\")?\n+    / IDENT() (\":\" rust_ty_param_bound() ++ \"+\")?\n+\n+rule rust_for_lifetimes()\n+    = \"for\" rust_ty_params()\n+\n+rule rust_ty_param_bound()\n+    = LIFETIME() \n+    / \"?\"? rust_for_lifetimes()? rust_ty_path()\n+    / \"(\" \"?\"? rust_for_lifetimes()? rust_ty_path() \")\"\n \n rule expression() -> SpannedExpr = choice()\n \n", "test_patch": "diff --git a/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs b/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs\nnew file mode 100644\nindex 0000000..8116666\n--- /dev/null\n+++ b/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs\n@@ -0,0 +1,122 @@\n+extern crate peg;\n+use peg::parser;\n+\n+// C++ in Rust\n+trait Operation<'a>: std::fmt::Debug {}\n+trait Operand<'a>: std::fmt::Debug + AsDynOperand<'a> {}\n+trait Location<'a>: Operand<'a> {}\n+impl<'a, T: ?Sized + Location<'a>> Operand<'a> for T {}\n+\n+// Thanks to quinedot for their comprehensive write-up on dyn Traits.\n+// https://quinedot.github.io/rust-learning/dyn-trait-combining.html#manual-supertrait-upcasting\n+trait AsDynOperand<'a> {\n+    fn as_dyn_operand(self: Box<Self>) -> Box<dyn Operand<'a> + 'a>;\n+}\n+\n+impl<'a, T: /* Sized + */ Operand<'a> + 'a> AsDynOperand<'a> for T {\n+    fn as_dyn_operand(self: Box<Self>) -> Box<dyn Operand<'a> + 'a> {\n+        self\n+    }\n+}\n+\n+\n+\n+#[derive(Debug)]\n+pub struct Program<'a>(Vec<Box<dyn Operation<'a> + 'a>>);\n+\n+#[derive(Debug)]\n+struct Add<'a> {\n+    result: Box<dyn Location<'a> + 'a>,\n+    lhs: Box<dyn Operand<'a> + 'a>,\n+    rhs: Box<dyn Operand<'a> + 'a>,\n+}\n+impl<'a> Operation<'a> for Add<'a> {}\n+\n+#[derive(Debug)]\n+struct Sub<'a> {\n+    result: Box<dyn Location<'a> + 'a>,\n+    lhs: Box<dyn Operand<'a> + 'a>,\n+    rhs: Box<dyn Operand<'a> + 'a>,\n+}\n+impl<'a> Operation<'a> for Sub<'a> {}\n+\n+#[derive(Debug)]\n+struct Register<'a>(&'a str);\n+impl<'a> Location<'a> for Register<'a> {}\n+\n+#[derive(Debug)]\n+struct Global<'a>(&'a str);\n+impl<'a> Location<'a> for Global<'a> {}\n+\n+#[derive(Debug)]\n+struct Literal(i32);\n+impl<'a> Operand<'a> for Literal {}\n+\n+parser!{\n+grammar assembly() for str {\n+    pub rule program() -> Program<'input>\n+        = op:operation() ** \"\\n\" { Program(op) }\n+\n+    rule _ = [' ']*\n+\n+    rule operation() -> Box<dyn Operation<'input> + 'input>\n+        = a:add() {a} / s:sub() {s}\n+\n+    rule add() -> Box<Add<'input>>\n+        = result:location() _ \"=\" _ \"add\" _ lhs:operand() _ rhs:operand() { Box::new(Add{ result, lhs, rhs }) }\n+\n+    rule sub() -> Box<Sub<'input>>\n+        = result:location() _ \"=\" _ \"sub\" _ lhs:operand() _ rhs:operand() { Box::new(Sub{ result, lhs, rhs }) }\n+\n+    rule location() -> Box<dyn Location<'input> + 'input>\n+        = r:register() {r} / g:global() {g}\n+\n+    rule register() -> Box<Register<'input>>\n+        = \"%\" _ id:identifier() { Box::new(Register(id)) }\n+\n+    rule global() -> Box<Global<'input>>\n+        = \"@\" _ id:identifier() { Box::new(Global(id)) }\n+\n+    rule identifier() -> &'input str\n+        = $(['a'..='z' | 'A'..='Z']+)\n+\n+    rule operand() -> Box<dyn Operand<'input> + 'input>\n+        = l:location() {l.as_dyn_operand()} / l:literal() {Box::new(l)}\n+    \n+    rule literal() -> Literal\n+        = n:$(['0'..='9']+) {?\n+            let n = n.parse::<i32>().map_err(|_| \"invalid int literal\")?;\n+            Ok(Literal(n))\n+        }\n+\n+}}\n+\n+fn main() {\n+    let parsed = assembly::program(\"%apple = add 1 @g\n+@b = add 2 %a\n+%c = sub 82 @b\n+@dog = sub @b 12\").unwrap();\n+    let expected = Program(vec![\n+        Box::new(Add{\n+            result: Box::new(Register(\"apple\")),\n+            lhs: Box::new(Literal(1)),\n+            rhs: Box::new(Global(\"g\"))\n+        }),\n+        Box::new(Add{\n+            result: Box::new(Global(\"b\")),\n+            lhs: Box::new(Literal(2)),\n+            rhs: Box::new(Register(\"a\"))\n+        }),\n+        Box::new(Sub{\n+            result: Box::new(Register(\"c\")),\n+            lhs: Box::new(Literal(82)),\n+            rhs: Box::new(Global(\"b\"))\n+        }),\n+        Box::new(Sub{\n+            result: Box::new(Global(\"dog\")),\n+            lhs: Box::new(Global(\"b\")),\n+            rhs: Box::new(Literal(12))\n+        }),\n+    ]);\n+    assert_eq!(format!(\"{parsed:?}\"), format!(\"{expected:?}\"));\n+}\n", "problem_statement": "Bounds on trait objects and impl traits not parsing\nThe following will not parse:\r\n```rust\r\nrule operand() -> Box<dyn Operand + 'input>\r\n```\r\n\r\nThe error:\r\n```\r\nerror: expected one of \"&\", \"(\", \"::\", \"<\", \"dyn\", \"impl\"\r\n  --> src\\parse.rs:41:45\r\n   |\r\n41 |         rule operand() -> Box<dyn Operand + 'input>\r\n   |                                             ^^^^^^\r\n```\r\n\r\nI believe the issue is in the `rust_type()` rule of grammar.rustpeg:\r\n\r\n```\r\nrule rust_type()\r\n    = BRACKET_GROUP()\r\n    / \"&\" LIFETIME()? \"mut\"? rust_type()\r\n    / \"dyn\" rust_type() ++ \"+\" // here\r\n    / \"impl\" rust_type() ++ \"+\" // and here\r\n    / \"(\" (rust_type() ++ \",\" \",\"?)? \")\"\r\n    / (\"<\" rust_type() (\"as\" rust_ty_path())? \">\")? rust_ty_path()\r\n```\r\n\r\n`rust_type()`s shouldn't follow \"dyn\" and \"impl\"; instead it should be something that parses type parameter bounds (which include rust types AND lifetimes) ([ref](https://doc.rust-lang.org/reference/types/trait-object.html)) ([ref](https://doc.rust-lang.org/reference/types/impl-trait.html)). I think the rules to parse that might already be written as part of `rust_where_clause()` and could perhaps be factored out, but I don't have time at the moment to verify that.\n", "hints_text": "", "created_at": "2024-10-09 21:02:40", "merge_commit_sha": "4928a1e6748b981ce1d5bf72c2d35d1516d9b4ec", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Rust 1.68.0', '.github/workflows/rust.yml']", "['Rust stable', '.github/workflows/rust.yml']"]]}
{"repo": "denoland/deno", "instance_id": "denoland__deno-28102", "base_commit": "4921411bb2d010458b1193c25f98c5fab2925c02", "patch": "diff --git a/cli/args/flags.rs b/cli/args/flags.rs\nindex e00af3a83e5a5e..6cf8d684ef65ed 100644\n--- a/cli/args/flags.rs\n+++ b/cli/args/flags.rs\n@@ -1772,7 +1772,7 @@ If you specify a directory instead of a file, the path is expanded to all contai\n }\n \n fn bundle_subcommand() -> Command {\n-  command(\"bundle\",  \"\u26a0\ufe0f `deno bundle` was removed in Deno 2.\n+  command(\"bundle\",  \"`deno bundle` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\", UnstableArgsConfig::ResolutionOnly)\n     .hide(true)\n@@ -3321,7 +3321,7 @@ different location, use the <c>--output</> flag:\n \n fn vendor_subcommand() -> Command {\n   command(\"vendor\",\n-      \"\u26a0\ufe0f `deno vendor` was removed in Deno 2.\n+      \"`deno vendor` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\",\n       UnstableArgsConfig::ResolutionOnly\n", "test_patch": "diff --git a/tests/specs/bundle/removed/bundle_help.out b/tests/specs/bundle/removed/bundle_help.out\nindex 349043baaf43d2..e1f43dfea61891 100644\n--- a/tests/specs/bundle/removed/bundle_help.out\n+++ b/tests/specs/bundle/removed/bundle_help.out\n@@ -1,4 +1,4 @@\n-\u26a0\ufe0f `deno bundle` was removed in Deno 2.\n+`deno bundle` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\n \ndiff --git a/tests/specs/vendor/removed/vendor_help.out b/tests/specs/vendor/removed/vendor_help.out\nindex aeda5176ff0fb9..98aca359d76ced 100644\n--- a/tests/specs/vendor/removed/vendor_help.out\n+++ b/tests/specs/vendor/removed/vendor_help.out\n@@ -1,4 +1,4 @@\n-\u26a0\ufe0f `deno vendor` was removed in Deno 2.\n+`deno vendor` was removed in Deno 2.\n \n See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations\n \n", "problem_statement": "CLI completions output for PowerShell mangled on Windows\nVersion: deno 2.0.2 (stable, release, x86_64-pc-windows-msvc)\n\nShell: PowerShell (pwsh) 7.4.6\nTerminal: Windows Terminal Preview 1.23.10353.0\n\nI'm initialising deno completions in PowerShell profile using the example from documentation:\n```PowerShell\nif (Get-Command -Name \"deno\" -ErrorAction SilentlyContinue) {\n    deno completions powershell | Out-String | Invoke-Expression\n}\n```\n\nWhich results in PowerShell error when starting new terminal instance:\n```pwsh\n\nInvoke-Expression: C:\\Users\\andis\\Documents\\PowerShell\\Completions\\deno.ps1:2\nLine |\n   2 |      deno completions powershell | Out-String | Invoke-Expression\n     |                                                 ~~~~~~~~~~~~~~~~~\n     | Missing ')' in method call.\n```\n\n\nI looked at the output using redirection to a file `deno completions powershell | Out-File deno-completions.tmp.ps1` and noticed a strange string of 6 characters on line 114 of the output: `\u014c\u00dc\u0100\u2019\u0116\u00c5`.\n```pwsh\n[CompletionResult]::new('bundle', 'bundle', [CompletionResultType]::ParameterValue, '\u014c\u00dc\u0100\u2019\u0116\u00c5 `deno bundle` was removed in Deno 2.  See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations')\n```\n\nCompare the the output from `deno completions powershell` without involving PowerShell by running the command in cmd shell and redirecting output to a file:\n```pwsh\n[CompletionResult]::new('bundle', 'bundle', [CompletionResultType]::ParameterValue, '\u26a0\ufe0f `deno bundle` was removed in Deno 2.  See the Deno 1.x to 2.x Migration Guide for migration instructions: https://docs.deno.com/runtime/manual/advanced/migrate_deprecations')\n```\n\n\n\nIt seems that PowerShell redirection is mangling the UTF-8 character (\u26a0\ufe0f) even though it shouldn't because terminal is using UTF-8 and all involved files are UTF-8 encoded.\n\nJust to make sure it's not my environment or some Windows charset translation shenanigans I tried to execute the unmangled script as written by `deno completions powershell > deno-comletions.tmp2.ps1` and it works as expected.\n\n\n## Possible workaround\nSet charset to UTF-8 explicitly in PowerShell profile file before loading completions:\n```pwsh\n$OutputEncoding = [Console]::OutputEncoding = [Text.UTF8Encoding]::UTF8\n```\n", "hints_text": "", "created_at": "2025-02-13 10:08:24", "merge_commit_sha": "d7203c9522a91582e7e361053d0cf16e2f934273", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test release windows-x86_64', '.github/workflows/ci.yml']", "['test debug windows-x86_64', '.github/workflows/ci.yml']"], ["['lint debug macos-x86_64', '.github/workflows/ci.yml']", "['lint debug windows-x86_64', '.github/workflows/ci.yml']"], ["['test debug macos-x86_64', '.github/workflows/ci.yml']", "['publish canary', '.github/workflows/ci.yml']"], ["['test release macos-aarch64', '.github/workflows/ci.yml']", "['test release macos-x86_64', '.github/workflows/ci.yml']"]]}
{"repo": "nathanbabcock/ffmpeg-sidecar", "instance_id": "nathanbabcock__ffmpeg-sidecar-71", "base_commit": "2272b10ec4582c8fd64fcd76d74d7caf697f2ff4", "patch": "diff --git a/src/child.rs b/src/child.rs\nindex f252ded..a4c3b7d 100644\n--- a/src/child.rs\n+++ b/src/child.rs\n@@ -1,14 +1,12 @@\n //! Wrapper around `std::process::Child` containing a spawned FFmpeg command.\n \n+use crate::iter::FfmpegIterator;\n+use anyhow::Context;\n use std::{\n-  io::{self, Write},\n+  io::{self, copy, sink, Write},\n   process::{Child, ChildStderr, ChildStdin, ChildStdout, ExitStatus},\n };\n \n-use anyhow::Context;\n-\n-use crate::iter::FfmpegIterator;\n-\n /// A wrapper around [`std::process::Child`] containing a spawned FFmpeg command.\n /// Provides interfaces for reading parsed metadata, progress updates, warnings and errors, and\n /// piped output frames if applicable.\n@@ -100,6 +98,12 @@ impl FfmpegChild {\n   ///\n   /// Identical to `wait` in [`std::process::Child`].\n   pub fn wait(&mut self) -> io::Result<ExitStatus> {\n+    // If stderr hasn't already been consumed by a method like `iter()`,\n+    // we need to run it to completion to avoid a deadlock.\n+    if let Some(mut stderr) = self.take_stderr() {\n+      copy(&mut stderr, &mut sink())?;\n+    };\n+\n     self.inner.wait()\n   }\n \n", "test_patch": "diff --git a/src/test.rs b/src/test.rs\nindex 2cd591e..a9ea3f3 100644\n--- a/src/test.rs\n+++ b/src/test.rs\n@@ -40,6 +40,35 @@ fn spawn_with_timeout(command: &mut FfmpegCommand, timeout: u64) -> anyhow::Resu\n   }\r\n }\r\n \r\n+/// Returns `Err` if the timeout thread finishes before the FFmpeg process\r\n+/// Note: this variant leaves behind a hung FFmpeg child process + thread until\r\n+/// the test suite exits.\r\n+fn wait_with_timeout(command: &mut FfmpegCommand, timeout: u64) -> anyhow::Result<()> {\r\n+  let (sender, receiver) = mpsc::channel();\r\n+\r\n+  // Thread 1: Waits for 1000ms and sends a message\r\n+  let timeout_sender = sender.clone();\r\n+  thread::spawn(move || {\r\n+    thread::sleep(Duration::from_millis(timeout));\r\n+    timeout_sender.send(\"timeout\").ok();\r\n+  });\r\n+\r\n+  // Thread 2: Wait for the child to exit in another thread\r\n+  let mut ffmpeg_child = command.spawn()?;\r\n+  thread::spawn(move || {\r\n+    ffmpeg_child.wait().unwrap();\r\n+    sender.send(\"ffmpeg\").ok();\r\n+  });\r\n+\r\n+  // Race the two threads\r\n+  let finished_first = receiver.recv()?;\r\n+  match finished_first {\r\n+    \"timeout\" => anyhow::bail!(\"Timeout thread expired before FFmpeg\"),\r\n+    \"ffmpeg\" => Ok(()),\r\n+    _ => anyhow::bail!(\"Unknown message received\"),\r\n+  }\r\n+}\r\n+\r\n #[test]\r\n fn test_installed() {\r\n   assert!(ffmpeg_is_installed());\r\n@@ -715,3 +744,30 @@ fn test_no_empty_events() -> anyhow::Result<()> {\n \r\n   Ok(())\r\n }\r\n+\r\n+/// This command generates an warning on every frame, e.g.:\r\n+///\r\n+/// ```txt\r\n+/// [Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n+///```\r\n+///\r\n+/// When used in combination with `.wait()`, these error messages can completely\r\n+/// fill the stderr buffer and cause a deadlock. The solution is to\r\n+/// automatically drop the stderr channel when `.wait()` is called.\r\n+///\r\n+/// <https://github.com/nathanbabcock/ffmpeg-sidecar/issues/70>\r\n+#[test]\r\n+fn test_wait() -> anyhow::Result<()> {\r\n+  let mut command = FfmpegCommand::new();\r\n+  command\r\n+    .args(\"-color_primaries 1\".split(' '))\r\n+    .args(\"-color_trc 1\".split(' '))\r\n+    .args(\"-colorspace 1\".split(' '))\r\n+    .format(\"lavfi\")\r\n+    .input(\"yuvtestsrc=size=64x64:rate=60:duration=60\")\r\n+    .args(\"-vf palettegen=max_colors=164\".split(' '))\r\n+    .codec_video(\"gif\")\r\n+    .format(\"null\")\r\n+    .output(\"-\");\r\n+  wait_with_timeout(&mut command, 5000)\r\n+}\r\n", "problem_statement": "FFmpeg's nearly unlimited output causes ffmpeg-sidecar to hang without a timeout\nI'm converting a video(with some strange .ts extension) to a 3-seconds gif using ffmpeg-sidecar. Command like\r\n\r\n```\r\nffmpeg -loglevel level+warning -hide_banner -i \"/path/to/bipbop-gear1-all.ts\" -t 3 -an -vf \"fps=10,scale=-1:200:flags=lanczos,split[s0][s1];[s0]palettegen=max_colors=164:stats_mode=diff[p];[s1][p]paletteuse=dither=floyd_steinberg:diff_mode=rectangle\" -compression_level 9 -quality 85 -c:v gif -y \"/path/to/thumbnail.gif\"\r\n```\r\n\r\nHowever, ffmpeg cli output nearly unlimited lines of warning, like \r\n\r\n```\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n...(tons of lines...)\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n```\r\n\r\nbut, ffmpeg cli just **exits(not sure exit 0 or not 0) after about ~7 seconds**, which I think reasonable.\r\n\r\nWhen it comes to ffmpeg-sidecar in Rust\r\n\r\n```\r\n    let exit_status = FfmpegCommand::new()\r\n        .args([\r\n            \"-loglevel\", \"level+warning\",\r\n            \"-hide_banner\",\r\n            \"-i\", filepath,\r\n            \"-t\", \"3\",\r\n            \"-an\",\r\n            \"-vf\", \"fps=10,scale=-1:200:flags=lanczos,split[s0][s1];[s0]palettegen=max_colors=164:stats_mode=diff[p];[s1][p]paletteuse=dither=floyd_steinberg:diff_mode=rectangle\",\r\n            \"-compression_level\", \"9\",\r\n            \"-quality\", \"85\",\r\n            \"-c:v\", \"gif\",\r\n            \"-y\",\r\n            &thumbnail_path_buf.to_string_lossy()\r\n        ])\r\n        .spawn()?\r\n        .wait()?;\r\n\r\n    if exit_status.success() {\r\n        Ok(...)\r\n    } else {\r\n        Err(...)\r\n    }\r\n```\r\n\r\nFfmpegCommand(FfmpegChild maybe) thread just hangs at .wait()?\r\n\r\nMaybe adding timeout parameter would be better? like the ffmpeg cli do internally? Or, I have to set a timer, and invoke handle FfmpegChild.quit/kill(maybe) when timeout, not very convenient.\r\n\r\nYou can download the test video at [http://www.live555.com/liveMedia/public/h264-in-mp2t/bipbop-gear1-all.ts](http://www.live555.com/liveMedia/public/h264-in-mp2t/bipbop-gear1-all.ts)\n", "hints_text": "Thanks for providing a test video, I will take a look.", "created_at": "2024-12-12 21:25:55", "merge_commit_sha": "5fb98043c3680fe3945cdfef6806cc8bc42b4a43", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build', '.github/workflows/mac-m1.yml']", "['build', '.github/workflows/mac.yml']"]]}
{"repo": "static-web-server/static-web-server", "instance_id": "static-web-server__static-web-server-495", "base_commit": "cd11bd62b4be6803a22d8ff274f0f6cf7adf0691", "patch": "diff --git a/docs/content/configuration/command-line-arguments.md b/docs/content/configuration/command-line-arguments.md\nindex dffd5b5d..515dfd40 100644\n--- a/docs/content/configuration/command-line-arguments.md\n+++ b/docs/content/configuration/command-line-arguments.md\n@@ -85,6 +85,10 @@ Options:\n           Server TOML configuration file path [env: SERVER_CONFIG_FILE=] [default: ./config.toml]\n       --log-remote-address [<LOG_REMOTE_ADDRESS>]\n           Log incoming requests information along with its remote address if available using the `info` log level [env: SERVER_LOG_REMOTE_ADDRESS=] [default: false] [possible values: true, false]\n+      --log-forwarded-for [<LOG_FORWARDED_FOR>]\n+          Log real IP from X-Forwarded-For header [env: SERVER_LOG_FORWARDED_FOR] [default: false] [possible values: true, false]\n+      --trusted-proxies <TRUSTED_PROXIES>\n+          A comma separated list of IP addresses to accept the X-Forwarded-For header from. Empty means trust all IPs [env: SERVER_TRUSTED_PROXIES] [default: \"\"]\n       --redirect-trailing-slash [<REDIRECT_TRAILING_SLASH>]\n           Check for a trailing slash in the requested directory URI and redirect permanently (308) to the same path with a trailing slash suffix if it is missing [env: SERVER_REDIRECT_TRAILING_SLASH=] [default: true] [possible values: true, false]\n       --ignore-hidden-files [<IGNORE_HIDDEN_FILES>]\ndiff --git a/docs/content/configuration/config-file.md b/docs/content/configuration/config-file.md\nindex 78a13597..d6058962 100644\n--- a/docs/content/configuration/config-file.md\n+++ b/docs/content/configuration/config-file.md\n@@ -75,6 +75,12 @@ grace-period = 0\n #### Log request Remote Address if available\n log-remote-address = false\n \n+#### Log real IP from X-Forwarded-For header if available\n+log-forwarded-for = false\n+\n+#### IPs to accept the X-Forwarded-For header from. Empty means all\n+trusted-proxies = []\n+\n #### Redirect to trailing slash in the requested directory uri\n redirect-trailing-slash = true\n \ndiff --git a/docs/content/configuration/environment-variables.md b/docs/content/configuration/environment-variables.md\nindex 046bbf49..605fa080 100644\n--- a/docs/content/configuration/environment-variables.md\n+++ b/docs/content/configuration/environment-variables.md\n@@ -30,6 +30,12 @@ Specify a logging level in lowercase. Possible values are `error`, `warn`, `info\n ### SERVER_LOG_REMOTE_ADDRESS\n Log incoming request information along with its Remote Address (IP) if available using the `info` log level. Default `false`.\n \n+### SERVER_LOG_FORWARDED_FOR\n+Log real IP from X-Forwarded-For header if available using the `info` log level. Default `false`\n+\n+### SERVER_TRUSTED_PROXIES\n+A comma separated list of IP addresses to accept the X-Forwarded-For header from. An empty string means trust all IPs. Default `\"\"`\n+\n ### SERVER_ERROR_PAGE_404\n HTML file path for 404 errors. If the path is not specified or simply doesn't exist then the server will use a generic HTML error message.\n If a relative path is used then it will be resolved under the root directory. Default `./404.html`.\ndiff --git a/docs/content/features/logging.md b/docs/content/features/logging.md\nindex 335688dd..dd100ab7 100644\n--- a/docs/content/features/logging.md\n+++ b/docs/content/features/logging.md\n@@ -27,55 +27,85 @@ Log entry example:\n 2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n ```\n \n-Below is an example of how to enable Remote Address (IP) logging. Note the last two entries.\n+Below is an example of how to enable Remote Address (IP) logging.\n \n ```sh\n static-web-server -a \"0.0.0.0\" -p 8080 -d docker/public/ -g info --log-remote-address=true\n-# 2022-05-23T22:24:44.523057Z  INFO static_web_server::logger: logging level: info\n-# 2022-05-23T22:24:44.523856Z  INFO static_web_server::server: server bound to TCP socket 0.0.0.0:8080\n-# 2022-05-23T22:24:44.523962Z  INFO static_web_server::server: runtime worker threads: 4\n-# 2022-05-23T22:24:44.523989Z  INFO static_web_server::server: security headers: enabled=false\n-# 2022-05-23T22:24:44.524006Z  INFO static_web_server::server: auto compression: enabled=true\n-# 2022-05-23T22:24:44.524061Z  INFO static_web_server::server: directory listing: enabled=false\n-# 2022-05-23T22:24:44.524097Z  INFO static_web_server::server: directory listing order code: 6\n-# 2022-05-23T22:24:44.524133Z  INFO static_web_server::server: cache control headers: enabled=true\n-# 2022-05-23T22:24:44.524191Z  INFO static_web_server::server: basic authentication: enabled=false\n-# 2022-05-23T22:24:44.524210Z  INFO static_web_server::server: grace period before graceful shutdown: 0s\n-# 2022-05-23T22:24:44.524527Z  INFO Server::start_server{addr_str=\"0.0.0.0:8080\" threads=4}: static_web_server::server: close time.busy=0.00ns time.idle=10.6\u00b5s\n-# 2022-05-23T22:24:44.524585Z  INFO static_web_server::server: listening on http://0.0.0.0:8080\n-# 2022-05-23T22:24:44.524614Z  INFO static_web_server::server: press ctrl+c to shut down the server\n-# 2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n-# 2022-05-23T22:25:26.516841Z  INFO static_web_server::handler: incoming request: method=GET uri=/favicon.ico remote_addr=192.168.1.126:57625\n+```\n+\n+The relevant log output:\n+```log\n+INFO static_web_server::logger: logging level: info\n+<...>\n+INFO static_web_server::info: log requests with remote IP addresses: enabled=true\n+<...>\n+INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n+INFO static_web_server::handler: incoming request: method=GET uri=/favicon.ico remote_addr=192.168.1.126:57625\n ```\n ## Log Real Remote IP\n \n-When used behind reverse proxy, reported `remote_addr` indicate proxy internal IP address and port, and not client real remote IP.\n-Proxy server can be configured to provide [X-Forwarded-For header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For), containing comma-separated list of IP addresses, starting with *client real remote IP*, and all following intermediate proxies (if any).\n+When used behind a reverse proxy the reported `remote_addr` indicates the proxies IP address and port, not the clients real IP.\n+The Proxy server can be configured to provide the [X-Forwarded-For header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For), containing a comma-separated list of IP addresses, starting with the *real remote client IP*, and all following intermediate proxies (if any).\n+\n+\n+To enable logging of the real remote IP, enable the `--log-forwarded-for` option or the equivalent [SERVER_LOG_FORWARDED_FOR](/docs/content/configuration/environment-variables.md#serverlogforwardedfor) env. By default this will log all requests which have a correctly formatted `X-Forwarded-For` header. \n+\n+Since the content of the `X-Forwarded-For` header can be changed by all proxies in the chain, the remote IP address reported may not be trusted.\n \n-When *Remote Address (IP) logging* [is enabled](#log-remote-addresses), and `X-Forwarded-For` header is present and correctly formatted, then log entries for requests will contain a `real_remote_ip` section with IP of remote client, **as reported by this header**. \n+To restrict the logging to only trusted proxy IPs, you can use the `--trusted-proxies` option, or the equivalent [SERVER_TRUSTED_PROXIES](/docs/content/configuration/environment-variables.md#servertrustedproxies) env. This should be a list of IPs, separated by commas. An empty list (the default) indicates that all IPs should be trusted.\n+\n+Command used for the following examples:\n+```sh\n+static-web-server -a \"::\" --log-forwarded-for=true --trusted-proxies=\"::1\" -p 8080 -d docker/public/ -g info\n+```\n+\n+Look for these lines in the log output:\n+```log\n+<...>\n+INFO static_web_server::info: log level: info\n+INFO static_web_server::info: log requests with remote IP addresses: enabled=false\n+INFO static_web_server::info: log X-Forwarded-For real remote IP addresses: enabled=true\n+INFO static_web_server::info: trusted IPs for X-Forwarded-For: [::1]\n+<...>\n+```\n \n We can simulate request as from behind reverse proxy with additional intermediate-proxy with following command:\n \n ```sh\n-curl --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\" http://0.0.0.0:8080\n+curl \"http://[::1]:8080\" --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\"\n ```\n \n-Log entry for such case will look like:\n+Log entry for this request will look like:\n \n ```log\n-2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625 real_remote_ip=203.0.113.195\n+INFO static_web_server::handler: incoming request: method=GET uri=/ real_remote_ip=203.0.113.195\n ```\n \n-**`SWS`** will parse `X-Forwarded-For` header, and if format of provided IP is invalid - it will be ignored to prevent log poisoning attacks. In such case `real_remote_ip` section will not be added.\n+---\n+\n+If we send the request from `127.0.0.1` instead:\n+```sh\n+curl \"http://127.0.0.1:8080\" --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\"\n+```\n+\n+we get the following log output:\n+```log\n+INFO static_web_server::handler: incoming request: method=GET uri=/\n+```\n+`127.0.0.1` is not in the `trusted_proxies`, so we dont get a `real_remote_address` in the log.\n+\n+Note the absence of the proxies remote address in these examples. If you want to log the remote address and the real remote address, you need to specify both `--log-remote-address` and `--log-forwarded-for`.\n+\n+---\n+\n+**`SWS`** will parse the `X-Forwarded-For` header and if the provided client IP is invalid, it will be ignored to prevent log poisoning attacks. In such cases the `real_remote_ip` section will not be added.\n \n Example from above, but with invalid header:\n \n ```sh\n-curl --header \"X-Forwarded-For: <iframe src=//malware.attack>\" http://0.0.0.0:8080\n+curl \"http://[::1]:8080\" --header \"X-Forwarded-For: <iframe src=//malware.attack>\"\n ```\n \n ```log\n-2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n+2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/\n ```\n-\n-Be aware, that contents of `X-Forwarded-For` header can be augumented by all proxies in the chain, and as such - remote IP address reported by it may not be trusted.\n\\ No newline at end of file\ndiff --git a/src/handler.rs b/src/handler.rs\nindex 7e0216be..79b1e09b 100644\n--- a/src/handler.rs\n+++ b/src/handler.rs\n@@ -7,7 +7,12 @@\n //!\n \n use hyper::{Body, Request, Response, StatusCode};\n-use std::{future::Future, net::SocketAddr, path::PathBuf, sync::Arc};\n+use std::{\n+    future::Future,\n+    net::{IpAddr, SocketAddr},\n+    path::PathBuf,\n+    sync::Arc,\n+};\n \n #[cfg(any(\n     feature = \"compression\",\n@@ -97,6 +102,10 @@ pub struct RequestHandlerOpts {\n     pub index_files: Vec<String>,\n     /// Log remote address feature.\n     pub log_remote_address: bool,\n+    /// Log the X-Forwarded-For header.\n+    pub log_forwarded_for: bool,\n+    /// Trusted IPs for remote addresses.\n+    pub trusted_proxies: Vec<IpAddr>,\n     /// Redirect trailing slash feature.\n     pub redirect_trailing_slash: bool,\n     /// Ignore hidden files feature.\n@@ -152,6 +161,8 @@ impl Default for RequestHandlerOpts {\n             basic_auth: String::new(),\n             index_files: vec![\"index.html\".into()],\n             log_remote_address: false,\n+            log_forwarded_for: false,\n+            trusted_proxies: Vec::new(),\n             redirect_trailing_slash: true,\n             ignore_hidden_files: false,\n             disable_symlinks: false,\ndiff --git a/src/log_addr.rs b/src/log_addr.rs\nindex c0a98e14..27174c92 100644\n--- a/src/log_addr.rs\n+++ b/src/log_addr.rs\n@@ -14,7 +14,18 @@ use crate::{handler::RequestHandlerOpts, health};\n /// Initializes the log address module.\n pub(crate) fn init(enabled: bool, handler_opts: &mut RequestHandlerOpts) {\n     handler_opts.log_remote_address = enabled;\n-    server_info!(\"log requests with remote and real IP addresses: enabled={enabled}\");\n+    let trusted = if handler_opts.trusted_proxies.is_empty() {\n+        \"all\".to_owned()\n+    } else {\n+        format!(\"{:?}\", handler_opts.trusted_proxies)\n+    };\n+\n+    server_info!(\"log requests with remote IP addresses: enabled={enabled}\");\n+    server_info!(\n+        \"log X-Forwarded-For real remote IP addresses: enabled={}\",\n+        handler_opts.log_forwarded_for\n+    );\n+    server_info!(\"trusted IPs for X-Forwarded-For: {trusted}\");\n }\n \n /// It logs remote and real IP addresses if available.\n@@ -23,23 +34,27 @@ pub(crate) fn pre_process<T>(\n     req: &Request<T>,\n     remote_addr: Option<SocketAddr>,\n ) {\n-    let remote_addrs = if opts.log_remote_address {\n-        // Add a Remote IP if available\n-        let remote_addr = remote_addr.map_or(\"\".to_owned(), |ip| format!(\" remote_addr={ip}\"));\n+    let mut remote_addrs = String::new();\n \n-        // Add also a Real Remote IP if available\n-        let real_remote_addr = req\n+    if opts.log_remote_address {\n+        if let Some(addr) = remote_addr {\n+            remote_addrs.push_str(format!(\" remote_addr={addr}\").as_str());\n+        }\n+    }\n+    if opts.log_forwarded_for\n+        && (opts.trusted_proxies.is_empty()\n+            || remote_addr.is_some_and(|addr| opts.trusted_proxies.contains(&addr.ip())))\n+    {\n+        if let Some(real_ip) = req\n             .headers()\n             .get(\"X-Forwarded-For\")\n             .and_then(|h| h.to_str().ok())\n             .and_then(|s| s.split(',').next())\n             .and_then(|s| s.trim().parse::<IpAddr>().ok())\n-            .map_or(\"\".to_owned(), |ip| format!(\" real_remote_ip={ip}\"));\n-\n-        [remote_addr, real_remote_addr].concat()\n-    } else {\n-        String::new()\n-    };\n+        {\n+            remote_addrs.push_str(format!(\" real_remote_ip={real_ip}\").as_str());\n+        }\n+    }\n \n     // Log incoming requests in debug mode only if the health option is enabled\n     if opts.health && health::is_health_endpoint(req) {\ndiff --git a/src/server.rs b/src/server.rs\nindex 8df52d3b..209375ee 100644\n--- a/src/server.rs\n+++ b/src/server.rs\n@@ -225,6 +225,12 @@ impl Server {\n         // Log remote address option\n         let log_remote_address = general.log_remote_address;\n \n+        // Log the X-Forwarded-For header.\n+        let log_forwarded_for = general.log_forwarded_for;\n+\n+        // Trusted IPs for remote addresses.\n+        let trusted_proxies = general.trusted_proxies;\n+\n         // Log redirect trailing slash option\n         let redirect_trailing_slash = general.redirect_trailing_slash;\n         server_info!(\n@@ -261,6 +267,8 @@ impl Server {\n             page404: page404.clone(),\n             page50x: page50x.clone(),\n             log_remote_address,\n+            log_forwarded_for,\n+            trusted_proxies,\n             redirect_trailing_slash,\n             ignore_hidden_files,\n             disable_symlinks,\ndiff --git a/src/settings/cli.rs b/src/settings/cli.rs\nindex 12b36106..80cc9df3 100644\n--- a/src/settings/cli.rs\n+++ b/src/settings/cli.rs\n@@ -7,7 +7,7 @@\n \n use clap::Parser;\n use hyper::StatusCode;\n-use std::path::PathBuf;\n+use std::{net::IpAddr, path::PathBuf};\n \n #[cfg(feature = \"directory-listing\")]\n use crate::directory_listing::DirListFmt;\n@@ -414,6 +414,28 @@ pub struct General {\n     /// Log incoming requests information along with its remote address if available using the `info` log level.\n     pub log_remote_address: bool,\n \n+    #[arg(\n+        long,\n+        default_value = \"false\",\n+        default_missing_value(\"true\"),\n+        num_args(0..=1),\n+        require_equals(false),\n+        action = clap::ArgAction::Set,\n+        env = \"SERVER_LOG_FORWARDED_FOR\",\n+    )]\n+    /// Log the X-Forwarded-For header for remote IP information\n+    pub log_forwarded_for: bool,\n+\n+    #[arg(\n+        long,\n+        require_equals(false),\n+        value_delimiter(','),\n+        action = clap::ArgAction::Set,\n+        env = \"SERVER_TRUSTED_PROXIES\",\n+    )]\n+    /// List of IPs to use X-Forwarded-For from. The default is to trust all\n+    pub trusted_proxies: Vec<IpAddr>,\n+\n     #[arg(\n         long,\n         default_value = \"true\",\ndiff --git a/src/settings/file.rs b/src/settings/file.rs\nindex cdd45994..98f0154e 100644\n--- a/src/settings/file.rs\n+++ b/src/settings/file.rs\n@@ -8,6 +8,7 @@\n use headers::HeaderMap;\n use serde::Deserialize;\n use serde_repr::{Deserialize_repr, Serialize_repr};\n+use std::net::IpAddr;\n use std::path::Path;\n use std::{collections::BTreeSet, path::PathBuf};\n \n@@ -356,6 +357,12 @@ pub struct General {\n     /// Log remote address feature.\n     pub log_remote_address: Option<bool>,\n \n+    /// Log the X-Forwarded-For header.\n+    pub log_forwarded_for: Option<bool>,\n+\n+    /// Trusted IPs for remote addresses.\n+    pub trusted_proxies: Option<Vec<IpAddr>>,\n+\n     /// Redirect trailing slash feature.\n     pub redirect_trailing_slash: Option<bool>,\n \ndiff --git a/src/settings/mod.rs b/src/settings/mod.rs\nindex 5cd58b54..46a0f67f 100644\n--- a/src/settings/mod.rs\n+++ b/src/settings/mod.rs\n@@ -197,6 +197,8 @@ impl Settings {\n         let mut page_fallback = opts.page_fallback;\n \n         let mut log_remote_address = opts.log_remote_address;\n+        let mut log_forwarded_for = opts.log_forwarded_for;\n+        let mut trusted_proxies = opts.trusted_proxies;\n         let mut redirect_trailing_slash = opts.redirect_trailing_slash;\n         let mut ignore_hidden_files = opts.ignore_hidden_files;\n         let mut disable_symlinks = opts.disable_symlinks;\n@@ -363,6 +365,12 @@ impl Settings {\n                 if let Some(v) = general.log_remote_address {\n                     log_remote_address = v\n                 }\n+                if let Some(v) = general.log_forwarded_for {\n+                    log_forwarded_for = v\n+                }\n+                if let Some(v) = general.trusted_proxies {\n+                    trusted_proxies = v\n+                }\n                 if let Some(v) = general.redirect_trailing_slash {\n                     redirect_trailing_slash = v\n                 }\n@@ -651,6 +659,8 @@ impl Settings {\n                 #[cfg(feature = \"fallback-page\")]\n                 page_fallback,\n                 log_remote_address,\n+                log_forwarded_for,\n+                trusted_proxies,\n                 redirect_trailing_slash,\n                 ignore_hidden_files,\n                 disable_symlinks,\n", "test_patch": "diff --git a/src/testing.rs b/src/testing.rs\nindex bac712a7..77b2751c 100644\n--- a/src/testing.rs\n+++ b/src/testing.rs\n@@ -94,6 +94,8 @@ pub mod fixtures {\n             #[cfg(feature = \"basic-auth\")]\n             basic_auth: general.basic_auth,\n             log_remote_address: general.log_remote_address,\n+            log_forwarded_for: general.log_forwarded_for,\n+            trusted_proxies: general.trusted_proxies,\n             redirect_trailing_slash: general.redirect_trailing_slash,\n             ignore_hidden_files: general.ignore_hidden_files,\n             disable_symlinks: general.disable_symlinks,\n", "problem_statement": "Add trusted_proxies option\n### Search for duplicate feature request\n\n- [X] I already searched, and this feature request or improvement is not a duplicate.\n\n### Feature scope\n\nConfiguration (e.g. TOML) or CLI/env option\n\n### Feature request related to a problem\n\nThe `log_remote_address` option currently always logs the contents of the `X-Forwarded-For` header. This is problematic when static-web-server is used without a trusted proxy in front, because a client could send this header and \"poison\" the log. More details can be found [in this MDN document](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For#security_and_privacy_concerns).\n\n### Describe the solution you'd like\n\nI would like a `trusted_proxies` option which would allow specifying from which IP addresses the `X-Forwarded-For` header is allowed to be read. I have actually already implemented this in https://github.com/static-web-server/static-web-server/commit/4cfa56978cda6b3bb3cbf88d8a2b72499db55c0f.\n\n### Describe alternatives you've considered\n\nAn alternative would be to have a boolean `trust_x_forwarded`, which toggles the option for all IPs. This is not as fine grained though.\n\n### Build target\n\nAll targets\n\n### Additional context\n\nMy original Plan was to just submit a draft PR with the changes i made. Before doing that i looked for a Contributing.md file. When opening the PR i was a bit surprised to see the policy that PRs will only be accepted with a related issue, so it would be awesome if a contributing.md file is created :)\n", "hints_text": "The idea sounds fine to me. Feel free to open a draft.\r\n\r\nYes, we need a `CONTRIBUTING.md` file. I will create one.", "created_at": "2024-11-02 16:47:38", "merge_commit_sha": "13e3f3861f11e4c516e224784f44476c6e553332", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test (pinned)', '.github/workflows/devel.yml']", "['test (linux-musl-armv7)', '.github/workflows/devel.yml']"], ["['merge-artifacts', '.github/workflows/perfcheck.yml']", "['checks', '.github/workflows/devel.yml']"], ["['test (macos)', '.github/workflows/devel.yml']", "['test (linux-gnu-i686)', '.github/workflows/devel.yml']"], ["['test (macos-arm64)', '.github/workflows/devel.yml']", "['test (linux-android-arm64)', '.github/workflows/devel.yml']"], ["['build', '.github/workflows/release.build.manual.yml']", "['test (linux-arm-gnueabihf)', '.github/workflows/devel.yml']"], ["['test (linux-musl-i686)', '.github/workflows/devel.yml']", "['test (linux-gnu)', '.github/workflows/devel.yml']"], ["['test (linux-musl-arm64)', '.github/workflows/devel.yml']", "['test (windows-msvc-i686)', '.github/workflows/devel.yml']"], ["['test (windows-msvc-arm64)', '.github/workflows/devel.yml']", "['benchmark (Small file (pre-compressed), http://localhost:8080/small_precompressed.txt, 1000/s)', '.github/workflows/perfcheck.yml']"], ["['benchmark (Directory listing (empty directory), http://localhost:8080/emptydir/, 1000/s)', '.github/workflows/perfcheck.yml']", "['test (linux-gnu-arm64)', '.github/workflows/devel.yml']"]]}
{"repo": "clarkmcc/cel-rust", "instance_id": "clarkmcc__cel-rust-109", "base_commit": "5b02b0817ced05c7cfc1c72bab03bc97bbfa2dea", "patch": "diff --git a/example/Cargo.toml b/example/Cargo.toml\nindex a0720ef..5751eee 100644\n--- a/example/Cargo.toml\n+++ b/example/Cargo.toml\n@@ -3,48 +3,58 @@ name = \"example\"\n version = \"0.1.0\"\n edition = \"2021\"\n \n+[features]\n+axum = [\"dep:axum\", \"dep:tokio\", \"dep:thiserror\"]\n+json = [\"dep:serde_json\", \"cel-interpreter/json\"]\n+chrono = [\"dep:chrono\", \"cel-interpreter/chrono\"]\n+\n [dependencies]\n+cel-interpreter = { path = \"../interpreter\", default-features = false }\n+\n+chrono = { version = \"0.4\", optional = true }\n+\n+serde = { version = \"1.0\", features = [\"derive\"] }\n+serde_json = { version = \"1.0\", optional = true }\n+\n axum = { version = \"0.7.5\", default-features = false, features = [\n     \"http1\",\n     \"json\",\n     \"tokio\",\n-] }\n-cel-interpreter = { path = \"../interpreter\", features = [\"json\", \"chrono\", \"regex\"] }\n-chrono = \"0.4.26\"\n-serde = { version = \"1.0.196\", features = [\"derive\"] }\n-serde_json = \"1.0.124\"\n-thiserror = { version = \"1.0.61\", default-features = false }\n+], optional = true }\n tokio = { version = \"1.38.0\", default-features = false, features = [\n     \"macros\",\n     \"net\",\n     \"rt-multi-thread\",\n-] }\n+], optional = true }\n+thiserror = { version = \"1.0\", optional = true }\n \n [[bin]]\n-name = \"simple\"\n+name = \"example-simple\"\n path = \"src/simple.rs\"\n \n [[bin]]\n-name = \"variables\"\n+name = \"example-variables\"\n path = \"src/variables.rs\"\n \n [[bin]]\n-name = \"functions\"\n+name = \"example-functions\"\n path = \"src/functions.rs\"\n+required-features = [\"chrono\"]\n \n [[bin]]\n-name = \"threads\"\n+name = \"example-threads\"\n path = \"src/threads.rs\"\n \n [[bin]]\n-name = \"serde\"\n+name = \"example-serde\"\n path = \"src/serde.rs\"\n \n [[bin]]\n-name = \"axum\"\n+name = \"example-axum\"\n path = \"src/axum.rs\"\n+required-features = [\"axum\"]\n \n [[bin]]\n-name = \"json\"\n+name = \"example-json\"\n path = \"src/json.rs\"\n-\n+required-features = [\"json\"]\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\nindex fa70cce..45a1379 100644\n--- a/interpreter/Cargo.toml\n+++ b/interpreter/Cargo.toml\n@@ -10,15 +10,18 @@ categories = [\"compilers\"]\n \n [dependencies]\n cel-parser = { path = \"../parser\", version = \"0.8.0\" }\n-thiserror = \"1.0.40\"\n-chrono = { version = \"0.4.26\", default-features = false, features = [\"alloc\"], optional = true }\n+\n nom = \"7.1.3\"\n-paste = \"1.0.14\"\n-serde = \"1.0.196\"\n+\n+chrono = { version = \"0.4\", default-features = false, features = [\"alloc\"], optional = true }\n regex = { version = \"1.10.5\", optional = true }\n-serde_json = { version = \"1.0.124\", optional = true }\n+serde = \"1.0\"\n+serde_json = { version = \"1.0\", optional = true }\n base64 = { version = \"0.22.1\", optional = true }\n \n+thiserror = \"1.0\"\n+paste = \"1.0\"\n+\n [dev-dependencies]\n criterion = { version = \"0.5.1\", features = [\"html_reports\"] }\n serde_bytes = \"0.11.14\"\n@@ -28,6 +31,7 @@ name = \"runtime\"\n harness = false\n \n [features]\n-json = [\"dep:base64\", \"dep:serde_json\"]\n+default = [\"regex\", \"chrono\"]\n+json = [\"dep:serde_json\", \"dep:base64\"]\n regex = [\"dep:regex\"]\n chrono = [\"dep:chrono\"]\ndiff --git a/interpreter/src/functions.rs b/interpreter/src/functions.rs\nindex a63cb38..457010f 100644\n--- a/interpreter/src/functions.rs\n+++ b/interpreter/src/functions.rs\n@@ -645,10 +645,7 @@ pub fn max(Arguments(args): Arguments) -> Result<Value> {\n #[cfg(test)]\n mod tests {\n     use crate::context::Context;\n-    use crate::testing::test_script;\n-    #[cfg(feature = \"regex\")]\n-    use crate::ExecutionError::FunctionError;\n-    use std::collections::HashMap;\n+    use crate::tests::test_script;\n \n     fn assert_script(input: &(&str, &str)) {\n         assert_eq!(test_script(input.1, None), Ok(true.into()), \"{}\", input.0);\n@@ -679,7 +676,7 @@ mod tests {\n \n         for (name, script) in tests {\n             let mut ctx = Context::default();\n-            ctx.add_variable_from_value(\"foo\", HashMap::from([(\"bar\", 1)]));\n+            ctx.add_variable_from_value(\"foo\", std::collections::HashMap::from([(\"bar\", 1)]));\n             assert_eq!(test_script(script, Some(ctx)), Ok(true.into()), \"{}\", name);\n         }\n     }\n@@ -943,7 +940,7 @@ mod tests {\n             test_script(\n                 \"'foobar'.matches('(foo') == true\", None),\n             Err(\n-                FunctionError {\n+                crate::ExecutionError::FunctionError {\n                     function: \"matches\".to_string(),\n                     message: \"'(foo' not a valid regex:\\nregex parse error:\\n    (foo\\n    ^\\nerror: unclosed group\".to_string()\n                 }\ndiff --git a/interpreter/src/lib.rs b/interpreter/src/lib.rs\nindex e307f55..b11ff47 100644\n--- a/interpreter/src/lib.rs\n+++ b/interpreter/src/lib.rs\n@@ -13,19 +13,19 @@ pub use cel_parser::Expression;\n pub use context::Context;\n pub use functions::FunctionContext;\n pub use objects::{ResolveResult, Value};\n-#[cfg(feature = \"chrono\")]\n-mod duration;\n pub mod functions;\n mod magic;\n pub mod objects;\n mod resolvers;\n+\n+#[cfg(feature = \"chrono\")]\n+mod duration;\n+\n mod ser;\n pub use ser::to_value;\n \n #[cfg(feature = \"json\")]\n mod json;\n-#[cfg(test)]\n-mod testing;\n \n use magic::FromContext;\n \n@@ -173,11 +173,16 @@ impl TryFrom<&str> for Program {\n mod tests {\n     use crate::context::Context;\n     use crate::objects::{ResolveResult, Value};\n-    use crate::testing::test_script;\n     use crate::{ExecutionError, Program};\n     use std::collections::HashMap;\n     use std::convert::TryInto;\n \n+    /// Tests the provided script and returns the result. An optional context can be provided.\n+    pub(crate) fn test_script(script: &str, ctx: Option<Context>) -> ResolveResult {\n+        let program = Program::compile(script).unwrap();\n+        program.execute(&ctx.unwrap_or_default())\n+    }\n+\n     #[test]\n     fn parse() {\n         Program::compile(\"1 + 1\").unwrap();\ndiff --git a/interpreter/src/objects.rs b/interpreter/src/objects.rs\nindex 50708d8..6c8ccc2 100644\n--- a/interpreter/src/objects.rs\n+++ b/interpreter/src/objects.rs\n@@ -1,15 +1,12 @@\n use crate::context::Context;\n use crate::functions::FunctionContext;\n-use crate::ser::SerializationError;\n-use crate::ExecutionError::NoSuchKey;\n-use crate::{to_value, ExecutionError};\n-use cel_parser::{ArithmeticOp, Atom, Expression, Member, RelationOp, UnaryOp};\n-use core::ops;\n-use serde::{Serialize, Serializer};\n+use crate::ExecutionError;\n+use cel_parser::ast::*;\n use std::cmp::Ordering;\n use std::collections::HashMap;\n use std::convert::{Infallible, TryFrom, TryInto};\n use std::fmt::{Display, Formatter};\n+use std::ops;\n use std::sync::Arc;\n \n #[derive(Debug, PartialEq, Clone)]\n@@ -84,10 +81,10 @@ impl From<u64> for Key {\n     }\n }\n \n-impl Serialize for Key {\n+impl serde::Serialize for Key {\n     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n     where\n-        S: Serializer,\n+        S: serde::Serializer,\n     {\n         match self {\n             Key::Int(v) => v.serialize(serializer),\n@@ -143,13 +140,12 @@ pub trait TryIntoValue {\n     fn try_into_value(self) -> Result<Value, Self::Error>;\n }\n \n-impl<T: Serialize> TryIntoValue for T {\n-    type Error = SerializationError;\n+impl<T: serde::Serialize> TryIntoValue for T {\n+    type Error = crate::ser::SerializationError;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n-        to_value(self)\n+        crate::ser::to_value(self)\n     }\n }\n-\n impl TryIntoValue for Value {\n     type Error = Infallible;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n@@ -629,7 +625,7 @@ impl<'a> Value {\n                 // give priority to the property. Maybe we can implement lookahead\n                 // to see if the next token is a function call?\n                 match (child, ctx.has_function(&***name)) {\n-                    (None, false) => NoSuchKey(name.clone()).into(),\n+                    (None, false) => ExecutionError::NoSuchKey(name.clone()).into(),\n                     (Some(child), _) => child.into(),\n                     (None, true) => Value::Function(name.clone(), Some(self.into())).into(),\n                 }\n@@ -960,4 +956,20 @@ mod tests {\n         let result = program.execute(&context);\n         assert_eq!(result.unwrap(), Value::Null);\n     }\n+\n+    #[test]\n+    fn reference_to_value() {\n+        let test = \"example\".to_string();\n+        let direct: Value = test.as_str().into();\n+        assert_eq!(direct, Value::String(Arc::new(String::from(\"example\"))));\n+\n+        let vec = vec![test.as_str()];\n+        let indirect: Value = vec.into();\n+        assert_eq!(\n+            indirect,\n+            Value::List(Arc::new(vec![Value::String(Arc::new(String::from(\n+                \"example\"\n+            )))]))\n+        );\n+    }\n }\n", "test_patch": "diff --git a/interpreter/src/testing.rs b/interpreter/src/testing.rs\ndeleted file mode 100644\nindex f814652..0000000\n--- a/interpreter/src/testing.rs\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-use crate::context::Context;\n-use crate::objects::ResolveResult;\n-use crate::Program;\n-\n-/// Tests the provided script and returns the result. An optional context can be provided.\n-pub(crate) fn test_script(script: &str, ctx: Option<Context>) -> ResolveResult {\n-    let program = Program::compile(script).unwrap();\n-    program.execute(&ctx.unwrap_or_default())\n-}\n", "problem_statement": "Defaulting to features for a compliant CEL interpreter? \nWith the fix of #90, some \"standard\" features of CEL are now disabled by default. While I agree with the reason and think that providing this fine grained control over dependencies et al is real great to have in any crate, I wonder if such functionality now behind a feature flag, should be `default` and be an opt-out rather an opt-in. So that anyone grabbing the crate, gets a \"std compatible experience\" wrt CEL... more so here, as this would be a \"slow killer\", i.e. only becoming visible at runtime when actually executing some otherwise perfectly fine `Expression`. wdyt? cc/ @Caellian\r\n\r\n<details>\r\n\r\ni.e.\r\n```diff\r\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\r\nindex fa70cce..c9fb30e 100644\r\n--- a/interpreter/Cargo.toml\r\n+++ b/interpreter/Cargo.toml\r\n@@ -28,6 +28,7 @@ name = \"runtime\"\r\n harness = false\r\n \r\n [features]\r\n+default = [\"regex\", \"chrono\"]\r\n json = [\"dep:base64\", \"dep:serde_json\"]\r\n regex = [\"dep:regex\"]\r\n chrono = [\"dep:chrono\"]\r\n```\r\n\r\n</details>\n", "hints_text": "Also, unsure about `json`, unlike `regex` & `chrono`, I _think_ this would become visible during development using the crate... so less opinionated whether it should be opt-in or -out... Tho technically JSON (& protobuf) interop are meant to be standard based on my reading of the spec... ", "created_at": "2024-11-05 15:26:16", "merge_commit_sha": "564e3fca2fe8c9975aa6605cace1f9ffcd8d0055", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": []}
{"repo": "ast-grep/ast-grep", "instance_id": "ast-grep__ast-grep-1634", "base_commit": "b87dad753fb2ce87cae17d488bac4da3fd62a5a7", "patch": "diff --git a/crates/core/src/node.rs b/crates/core/src/node.rs\nindex a0953e7bab..bd3737f6cd 100644\n--- a/crates/core/src/node.rs\n+++ b/crates/core/src/node.rs\n@@ -19,21 +19,25 @@ pub struct Position {\n   row: usize,\n   /// zero-based BYTE offset instead of character offset\n   byte_column: usize,\n+  /// byte offset of this position\n+  byte_offset: usize,\n }\n \n impl Position {\n-  fn new(row: u32, byte_column: u32) -> Self {\n+  fn new(row: u32, byte_column: u32, byte_offset: u32) -> Self {\n     Self {\n       row: row as usize,\n       byte_column: byte_column as usize,\n+      byte_offset: byte_offset as usize,\n     }\n   }\n   pub fn row(&self) -> usize {\n     self.row\n   }\n   /// TODO: return unicode character offset\n-  pub fn column<D: Doc>(&self, _node: &Node<D>) -> usize {\n-    self.byte_column\n+  pub fn column<D: Doc>(&self, node: &Node<D>) -> usize {\n+    let source = node.root.doc.get_source();\n+    source.get_char_column(self.byte_column, self.byte_offset)\n   }\n   /// Convert to tree-sitter's Point\n   pub fn ts_point(&self) -> tree_sitter::Point {\n@@ -218,13 +222,15 @@ impl<'r, D: Doc> Node<'r, D> {\n   /// Nodes' start position in terms of zero-based rows and columns.\n   pub fn start_pos(&self) -> Position {\n     let pos = self.inner.start_position();\n-    Position::new(pos.row(), pos.column())\n+    let byte = self.inner.start_byte();\n+    Position::new(pos.row(), pos.column(), byte)\n   }\n \n   /// Nodes' end position in terms of rows and columns.\n   pub fn end_pos(&self) -> Position {\n     let pos = self.inner.end_position();\n-    Position::new(pos.row(), pos.column())\n+    let byte = self.inner.end_byte();\n+    Position::new(pos.row(), pos.column(), byte)\n   }\n \n   pub fn text(&self) -> Cow<'r, str> {\n@@ -742,7 +748,6 @@ if (a) {\n   }\n \n   #[test]\n-  #[ignore = \"TODO: fix column to be unicode character\"]\n   fn test_unicode_pos() {\n     let root = Tsx.ast_grep(\"\ud83e\udd80\");\n     let root = root.root();\n@@ -751,5 +756,12 @@ if (a) {\n     assert_eq!(node.start_pos().column(&node), 0);\n     assert_eq!(node.end_pos().row(), 0);\n     assert_eq!(node.end_pos().column(&node), 1);\n+    let root = Tsx.ast_grep(\"\\n  \ud83e\udd80\ud83e\udd80\");\n+    let root = root.root();\n+    let node = root.find(\"$A\").expect(\"should exist\");\n+    assert_eq!(node.start_pos().row(), 1);\n+    assert_eq!(node.start_pos().column(&node), 2);\n+    assert_eq!(node.end_pos().row(), 1);\n+    assert_eq!(node.end_pos().column(&node), 4);\n   }\n }\ndiff --git a/crates/core/src/source.rs b/crates/core/src/source.rs\nindex 28fc6d2a55..e6b8dad6a7 100644\n--- a/crates/core/src/source.rs\n+++ b/crates/core/src/source.rs\n@@ -146,6 +146,8 @@ pub trait Content: Sized {\n   /// Used for string replacement. We need this for\n   /// transformation.\n   fn encode_bytes(bytes: &[Self::Underlying]) -> Cow<str>;\n+  /// Get the character column at the given position\n+  fn get_char_column(&self, column: usize, offset: usize) -> usize;\n }\n \n impl Content for String {\n@@ -189,6 +191,25 @@ impl Content for String {\n   fn encode_bytes(bytes: &[Self::Underlying]) -> Cow<str> {\n     String::from_utf8_lossy(bytes)\n   }\n+\n+  /// This is an O(n) operation. We assume the col will not be a\n+  /// huge number in reality. This may be problematic for special\n+  /// files like compressed js\n+  fn get_char_column(&self, _col: usize, offset: usize) -> usize {\n+    let src = self.as_bytes();\n+    let mut col = 0;\n+    // TODO: is it possible to use SIMD here???\n+    for &b in src[..offset].iter().rev() {\n+      if b == b'\\n' {\n+        break;\n+      }\n+      // https://en.wikipedia.org/wiki/UTF-8#Description\n+      if b & 0b1100_0000 != 0b1000_0000 {\n+        col += 1;\n+      }\n+    }\n+    col\n+  }\n }\n \n #[cfg(test)]\ndiff --git a/crates/napi/src/doc.rs b/crates/napi/src/doc.rs\nindex 99225d9210..eed952b813 100644\n--- a/crates/napi/src/doc.rs\n+++ b/crates/napi/src/doc.rs\n@@ -107,6 +107,10 @@ impl Content for Wrapper {\n     let s = String::from_utf16_lossy(bytes);\n     Cow::Owned(s)\n   }\n+  fn get_char_column(&self, column: usize, _offset: usize) -> usize {\n+    // utf-16 is 2 bytes per character, this is O(1) operation!\n+    column / 2\n+  }\n }\n \n fn pos_for_byte_offset(input: &[u16], byte_offset: usize) -> Point {\ndiff --git a/crates/napi/src/sg_node.rs b/crates/napi/src/sg_node.rs\nindex 59a9a89200..fb752621e6 100644\n--- a/crates/napi/src/sg_node.rs\n+++ b/crates/napi/src/sg_node.rs\n@@ -43,8 +43,7 @@ impl SgNode {\n   fn to_pos(&self, pos: Position, offset: usize) -> Pos {\n     Pos {\n       line: pos.row() as u32,\n-      // TODO: remove the division by 2 hack\n-      column: pos.column(&self.inner) as u32 / 2,\n+      column: pos.column(&self.inner) as u32,\n       index: offset as u32 / 2,\n     }\n   }\n", "test_patch": "diff --git a/crates/pyo3/tests/test_range.py b/crates/pyo3/tests/test_range.py\nindex 9b72a9b3b4..037fb3517d 100644\n--- a/crates/pyo3/tests/test_range.py\n+++ b/crates/pyo3/tests/test_range.py\n@@ -48,5 +48,4 @@ def test_unicode():\n     assert node is not None\n     assert node.range().start.index == 5\n     assert node.range().start.line == 0\n-    # TODO: Fix this, it should be 5 in character\n-    # assert node.range().start.column == 5\n\\ No newline at end of file\n+    assert node.range().start.column == 5\n\\ No newline at end of file\n", "problem_statement": "[bug] ast-grep scan: special characters lead to different end columns\n### Please read the FAQ for the bug you encountered.\n\n- [X] I have read the existing FAQ\n\n### \u23ef Playground Link\n\nhttps://ast-grep.github.io/playground.html#eyJtb2RlIjoiQ29uZmlnIiwibGFuZyI6ImNwcCIsInF1ZXJ5IjoiY29uc29sZS5sb2coJE1BVENIKSIsInJld3JpdGUiOiJsb2dnZXIubG9nKCRNQVRDSCkiLCJzdHJpY3RuZXNzIjoic21hcnQiLCJzZWxlY3RvciI6IiIsImNvbmZpZyI6IiMgWUFNTCBSdWxlIGlzIG1vcmUgcG93ZXJmdWwhXG4jIGh0dHBzOi8vYXN0LWdyZXAuZ2l0aHViLmlvL2d1aWRlL3J1bGUtY29uZmlnLmh0bWwjcnVsZVxucnVsZTpcbiAga2luZDogY29tbWVudFxuICBwYXR0ZXJuOiAkQ09NTUVOVFxuICBhbGw6XG4gICAgLSByZWdleDogXCJUT0RPW146XXxGSVhNRVteOl1cIlxudHJhbnNmb3JtOlxuICBORVdfQ09NTUVOVDpcbiAgICByZXBsYWNlOlxuICAgICAgc291cmNlOiAkQ09NTUVOVFxuICAgICAgcmVwbGFjZTogKD88RklYPlRPRE98RklYTUUpXG4gICAgICBieTogXCIkRklYOlwiXG5maXg6ICRORVdfQ09NTUVOVCIsInNvdXJjZSI6IiAgLy8gIFRPRE8gdGVzdCBzdHVmZi4uLi4uXG4gIC8vICBUT0RPIHRlc3RlIMOcYmVyZ8OkbmdlIn0=\n\n### \ud83d\udcbb Code\n\n_No response_\n\n### \ud83d\ude41 Actual behavior\n\nThe playground example has two comments that are matched. In the playground, you can see that the end column is at 26 for both. When I run the rule with json output I get the following:\r\n```\r\n[\r\n{\r\n  \"text\": \"//  TODO test stuff.....\",\r\n  \"range\": {\r\n    \"byteOffset\": {\r\n      \"start\": 2,\r\n      \"end\": 26\r\n    },\r\n    \"start\": {\r\n      \"line\": 0,\r\n      \"column\": 2\r\n    },\r\n    \"end\": {\r\n      \"line\": 0,\r\n      \"column\": 26\r\n    }\r\n  },\r\n  \"file\": \"comment.cpp\",\r\n  \"lines\": \"  //  TODO test stuff.....\",\r\n  \"charCount\": {\r\n    \"leading\": 2,\r\n    \"trailing\": 0\r\n  },\r\n  \"replacement\": \"//  TODO: test stuff.....\",\r\n  \"replacementOffsets\": {\r\n    \"start\": 2,\r\n    \"end\": 26\r\n  },\r\n  \"language\": \"Cpp\",\r\n  \"metaVariables\": {\r\n    \"single\": {\r\n      \"COMMENT\": {\r\n        \"text\": \"//  TODO test stuff.....\",\r\n        \"range\": {\r\n          \"byteOffset\": {\r\n            \"start\": 2,\r\n            \"end\": 26\r\n          },\r\n          \"start\": {\r\n            \"line\": 0,\r\n            \"column\": 2\r\n          },\r\n          \"end\": {\r\n            \"line\": 0,\r\n            \"column\": 26\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"multi\": {},\r\n    \"transformed\": {\r\n      \"NEW_COMMENT\": \"//  TODO: test stuff.....\"\r\n    }\r\n  },\r\n  \"ruleId\": \"comment\",\r\n  \"severity\": \"hint\",\r\n  \"note\": null,\r\n  \"message\": \"\"\r\n},\r\n{\r\n  \"text\": \"//  TODO teste \u00dcberg\u00e4nge\",\r\n  \"range\": {\r\n    \"byteOffset\": {\r\n      \"start\": 29,\r\n      \"end\": 55\r\n    },\r\n    \"start\": {\r\n      \"line\": 1,\r\n      \"column\": 2\r\n    },\r\n    \"end\": {\r\n      \"line\": 1,\r\n      \"column\": 28\r\n    }\r\n  },\r\n  \"file\": \"comment.cpp\",\r\n  \"lines\": \"  //  TODO teste \u00dcberg\u00e4nge\",\r\n  \"charCount\": {\r\n    \"leading\": 2,\r\n    \"trailing\": 0\r\n  },\r\n  \"replacement\": \"//  TODO: teste \u00dcberg\u00e4nge\",\r\n  \"replacementOffsets\": {\r\n    \"start\": 29,\r\n    \"end\": 55\r\n  },\r\n  \"language\": \"Cpp\",\r\n  \"metaVariables\": {\r\n    \"single\": {\r\n      \"COMMENT\": {\r\n        \"text\": \"//  TODO teste \u00dcberg\u00e4nge\",\r\n        \"range\": {\r\n          \"byteOffset\": {\r\n            \"start\": 29,\r\n            \"end\": 55\r\n          },\r\n          \"start\": {\r\n            \"line\": 1,\r\n            \"column\": 2\r\n          },\r\n          \"end\": {\r\n            \"line\": 1,\r\n            \"column\": 28\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"multi\": {},\r\n    \"transformed\": {\r\n      \"NEW_COMMENT\": \"//  TODO: teste \u00dcberg\u00e4nge\"\r\n    }\r\n  },\r\n  \"ruleId\": \"comment\",\r\n  \"severity\": \"hint\",\r\n  \"note\": null,\r\n  \"message\": \"\"\r\n}\r\n]\r\n```\r\n\r\nAlthough both comment matches have the end column 26, ast-grep scan reports end column 28 for the comment with special characters.\n\n### \ud83d\ude42 Expected behavior\n\nAst-grep scan should report the actual end column.\n\n### Additional information about the issue\n\n_No response_\n", "hints_text": "This is because `ts_node_start_point` returns zero-based byte offset column.\r\n\r\nUsing tree-sitter-json as an example, the test case\r\n\r\n```json\r\n\"\u00dcberg\u00e4nge\"\r\n```\r\n\r\nreturns the `string_content` ending at 12\r\n\r\n<img width=\"502\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fa2625e0-3b47-4eea-a2b9-0e43cea89299\">\r\n\r\n\r\nIn contrast, the tree-sitter web returns ending column 10\r\n\r\n<img width=\"434\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e3ac1275-38e8-4efe-be5f-b190ba4407b4\">\r\n\r\n\r\nAlso see \r\n\r\nhttps://github.com/tree-sitter/tree-sitter/issues/1727\r\nhttps://github.com/tree-sitter/tree-sitter/pull/1581#issuecomment-1010512449\r\nhttps://github.com/tree-sitter/tree-sitter/issues/2946\nI don't think upstream tree-sitter will fix this in the core lib in the near future. See\r\n\r\nhttps://github.com/tree-sitter/tree-sitter/pull/3610/files", "created_at": "2024-12-01 03:40:21", "merge_commit_sha": "1266fc7533894ff276f1d244eea96ae540c71d13", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['coverage', '.github/workflows/coverage.yaml']", "['Lint/Format Check', '.github/workflows/coverage.yaml']"]]}
{"repo": "uutils/coreutils", "instance_id": "uutils__coreutils-7115", "base_commit": "2dbc941b69917df058c57746dae0a18dcc01b57a", "patch": "diff --git a/src/uu/stat/src/stat.rs b/src/uu/stat/src/stat.rs\nindex 5e617e7a31a..a6220267314 100644\n--- a/src/uu/stat/src/stat.rs\n+++ b/src/uu/stat/src/stat.rs\n@@ -94,6 +94,7 @@ pub enum OutputType {\n     Unsigned(u64),\n     UnsignedHex(u64),\n     UnsignedOct(u32),\n+    Float(f64),\n     Unknown,\n }\n \n@@ -120,6 +121,13 @@ impl std::str::FromStr for QuotingStyle {\n     }\n }\n \n+#[derive(Debug, PartialEq, Eq, Clone, Copy)]\n+enum Precision {\n+    NotSpecified,\n+    NoNumber,\n+    Number(usize),\n+}\n+\n #[derive(Debug, PartialEq, Eq)]\n enum Token {\n     Char(char),\n@@ -127,7 +135,7 @@ enum Token {\n     Directive {\n         flag: Flags,\n         width: usize,\n-        precision: Option<usize>,\n+        precision: Precision,\n         format: char,\n     },\n }\n@@ -238,10 +246,10 @@ struct Stater {\n /// * `output` - A reference to the OutputType enum containing the value to be printed.\n /// * `flags` - A Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed output.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n ///\n /// This function delegates the printing process to more specialized functions depending on the output type.\n-fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<usize>) {\n+fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Precision) {\n     // If the precision is given as just '.', the precision is taken to be zero.\n     // A negative precision is taken as if the precision were omitted.\n     // This gives the minimum number of digits to appear for d, i, o, u, x, and X conversions,\n@@ -271,7 +279,7 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n     // A sign (+ or -) should always be placed before a number produced by a signed conversion.\n     // By default, a sign  is  used only for negative numbers.\n     // A + overrides a space if both are used.\n-    let padding_char = determine_padding_char(&flags, &precision);\n+    let padding_char = determine_padding_char(&flags);\n \n     match output {\n         OutputType::Str(s) => print_str(s, &flags, width, precision),\n@@ -283,6 +291,9 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n         OutputType::UnsignedHex(num) => {\n             print_unsigned_hex(*num, &flags, width, precision, padding_char);\n         }\n+        OutputType::Float(num) => {\n+            print_float(*num, &flags, width, precision, padding_char);\n+        }\n         OutputType::Unknown => print!(\"?\"),\n     }\n }\n@@ -292,13 +303,12 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n /// # Arguments\n ///\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n-/// * `precision` - An Option containing the precision value.\n ///\n /// # Returns\n ///\n /// * Padding - An instance of the Padding enum representing the padding character.\n-fn determine_padding_char(flags: &Flags, precision: &Option<usize>) -> Padding {\n-    if flags.zero && !flags.left && precision.is_none() {\n+fn determine_padding_char(flags: &Flags) -> Padding {\n+    if flags.zero && !flags.left {\n         Padding::Zero\n     } else {\n         Padding::Space\n@@ -312,10 +322,10 @@ fn determine_padding_char(flags: &Flags, precision: &Option<usize>) -> Padding {\n /// * `s` - The string to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed string.\n-/// * `precision` - An Option containing the precision value.\n-fn print_str(s: &str, flags: &Flags, width: usize, precision: Option<usize>) {\n+/// * `precision` - How many digits of precision, if any.\n+fn print_str(s: &str, flags: &Flags, width: usize, precision: Precision) {\n     let s = match precision {\n-        Some(p) if p < s.len() => &s[..p],\n+        Precision::Number(p) if p < s.len() => &s[..p],\n         _ => s,\n     };\n     pad_and_print(s, flags.left, width, Padding::Space);\n@@ -415,13 +425,13 @@ fn process_token_filesystem(t: &Token, meta: StatFs, display_name: &str) {\n /// * `num` - The integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_integer(\n     num: i64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let num = num.to_string();\n@@ -437,13 +447,66 @@ fn print_integer(\n     } else {\n         \"\"\n     };\n-    let extended = format!(\n-        \"{prefix}{arg:0>precision$}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let extended = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{arg}\"),\n+        Precision::NoNumber => format!(\"{prefix}{arg}\"),\n+        Precision::Number(p) => format!(\"{prefix}{arg:0>precision$}\", precision = p),\n+    };\n     pad_and_print(&extended, flags.left, width, padding_char);\n }\n \n+/// Truncate a float to the given number of digits after the decimal point.\n+fn precision_trunc(num: f64, precision: Precision) -> String {\n+    // GNU `stat` doesn't round, it just seems to truncate to the\n+    // given precision:\n+    //\n+    //     $ stat -c \"%.5Y\" /dev/pts/ptmx\n+    //     1736344012.76399\n+    //     $ stat -c \"%.4Y\" /dev/pts/ptmx\n+    //     1736344012.7639\n+    //     $ stat -c \"%.3Y\" /dev/pts/ptmx\n+    //     1736344012.763\n+    //\n+    // Contrast this with `printf`, which seems to round the\n+    // numbers:\n+    //\n+    //     $ printf \"%.5f\\n\" 1736344012.76399\n+    //     1736344012.76399\n+    //     $ printf \"%.4f\\n\" 1736344012.76399\n+    //     1736344012.7640\n+    //     $ printf \"%.3f\\n\" 1736344012.76399\n+    //     1736344012.764\n+    //\n+    let num_str = num.to_string();\n+    let n = num_str.len();\n+    match (num_str.find('.'), precision) {\n+        (None, Precision::NotSpecified) => num_str,\n+        (None, Precision::NoNumber) => num_str,\n+        (None, Precision::Number(0)) => num_str,\n+        (None, Precision::Number(p)) => format!(\"{num_str}.{zeros}\", zeros = \"0\".repeat(p)),\n+        (Some(i), Precision::NotSpecified) => num_str[..i].to_string(),\n+        (Some(_), Precision::NoNumber) => num_str,\n+        (Some(i), Precision::Number(0)) => num_str[..i].to_string(),\n+        (Some(i), Precision::Number(p)) if p < n - i => num_str[..i + 1 + p].to_string(),\n+        (Some(i), Precision::Number(p)) => {\n+            format!(\"{num_str}{zeros}\", zeros = \"0\".repeat(p - (n - i - 1)))\n+        }\n+    }\n+}\n+\n+fn print_float(num: f64, flags: &Flags, width: usize, precision: Precision, padding_char: Padding) {\n+    let prefix = if flags.sign {\n+        \"+\"\n+    } else if flags.space {\n+        \" \"\n+    } else {\n+        \"\"\n+    };\n+    let num_str = precision_trunc(num, precision);\n+    let extended = format!(\"{prefix}{num_str}\");\n+    pad_and_print(&extended, flags.left, width, padding_char)\n+}\n+\n /// Prints an unsigned integer value based on the provided flags, width, and precision.\n ///\n /// # Arguments\n@@ -451,13 +514,13 @@ fn print_integer(\n /// * `num` - The unsigned integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned(\n     num: u64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let num = num.to_string();\n@@ -466,7 +529,11 @@ fn print_unsigned(\n     } else {\n         Cow::Borrowed(num.as_str())\n     };\n-    let s = format!(\"{s:0>precision$}\", precision = precision.unwrap_or(0));\n+    let s = match precision {\n+        Precision::NotSpecified => s,\n+        Precision::NoNumber => s,\n+        Precision::Number(p) => format!(\"{s:0>precision$}\", precision = p).into(),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -477,20 +544,21 @@ fn print_unsigned(\n /// * `num` - The unsigned octal integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned octal integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned_oct(\n     num: u32,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let prefix = if flags.alter { \"0\" } else { \"\" };\n-    let s = format!(\n-        \"{prefix}{num:0>precision$o}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let s = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{num:o}\"),\n+        Precision::NoNumber => format!(\"{prefix}{num:o}\"),\n+        Precision::Number(p) => format!(\"{prefix}{num:0>precision$o}\", precision = p),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -501,20 +569,21 @@ fn print_unsigned_oct(\n /// * `num` - The unsigned hexadecimal integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned hexadecimal integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned_hex(\n     num: u64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let prefix = if flags.alter { \"0x\" } else { \"\" };\n-    let s = format!(\n-        \"{prefix}{num:0>precision$x}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let s = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{num:x}\"),\n+        Precision::NoNumber => format!(\"{prefix}{num:x}\"),\n+        Precision::Number(p) => format!(\"{prefix}{num:0>precision$x}\", precision = p),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -530,6 +599,10 @@ impl Stater {\n                 '0' => flag.zero = true,\n                 '-' => flag.left = true,\n                 ' ' => flag.space = true,\n+                // This is not documented but the behavior seems to be\n+                // the same as a space. For example `stat -c \"%I5s\" f`\n+                // prints \"    0\".\n+                'I' => flag.space = true,\n                 '+' => flag.sign = true,\n                 '\\'' => flag.group = true,\n                 _ => break,\n@@ -560,7 +633,7 @@ impl Stater {\n         Self::process_flags(chars, i, bound, &mut flag);\n \n         let mut width = 0;\n-        let mut precision = None;\n+        let mut precision = Precision::NotSpecified;\n         let mut j = *i;\n \n         if let Some((field_width, offset)) = format_str[j..].scan_num::<usize>() {\n@@ -585,11 +658,11 @@ impl Stater {\n             match format_str[j..].scan_num::<i32>() {\n                 Some((value, offset)) => {\n                     if value >= 0 {\n-                        precision = Some(value as usize);\n+                        precision = Precision::Number(value as usize);\n                     }\n                     j += offset;\n                 }\n-                None => precision = Some(0),\n+                None => precision = Precision::NoNumber,\n             }\n             check_bound(format_str, bound, old, j)?;\n         }\n@@ -898,7 +971,24 @@ impl Stater {\n                     // time of last data modification, human-readable\n                     'y' => OutputType::Str(pretty_time(meta.mtime(), meta.mtime_nsec())),\n                     // time of last data modification, seconds since Epoch\n-                    'Y' => OutputType::Integer(meta.mtime()),\n+                    'Y' => {\n+                        let sec = meta.mtime();\n+                        let nsec = meta.mtime_nsec();\n+                        let tm =\n+                            chrono::DateTime::from_timestamp(sec, nsec as u32).unwrap_or_default();\n+                        let tm: DateTime<Local> = tm.into();\n+                        match tm.timestamp_nanos_opt() {\n+                            None => {\n+                                let micros = tm.timestamp_micros();\n+                                let secs = micros as f64 / 1_000_000.0;\n+                                OutputType::Float(secs)\n+                            }\n+                            Some(ns) => {\n+                                let secs = ns as f64 / 1_000_000_000.0;\n+                                OutputType::Float(secs)\n+                            }\n+                        }\n+                    }\n                     // time of last status change, human-readable\n                     'z' => OutputType::Str(pretty_time(meta.ctime(), meta.ctime_nsec())),\n                     // time of last status change, seconds since Epoch\n@@ -1107,7 +1197,7 @@ fn pretty_time(sec: i64, nsec: i64) -> String {\n \n #[cfg(test)]\n mod tests {\n-    use super::{group_num, Flags, ScanUtil, Stater, Token};\n+    use super::{group_num, precision_trunc, Flags, Precision, ScanUtil, Stater, Token};\n \n     #[test]\n     fn test_scanners() {\n@@ -1155,7 +1245,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 10,\n-                precision: Some(2),\n+                precision: Precision::Number(2),\n                 format: 'a',\n             },\n             Token::Char('c'),\n@@ -1166,7 +1256,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 5,\n-                precision: Some(0),\n+                precision: Precision::NoNumber,\n                 format: 'w',\n             },\n             Token::Char('\\n'),\n@@ -1186,7 +1276,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 15,\n-                precision: None,\n+                precision: Precision::NotSpecified,\n                 format: 'a',\n             },\n             Token::Byte(b'\\t'),\n@@ -1205,7 +1295,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 20,\n-                precision: None,\n+                precision: Precision::NotSpecified,\n                 format: 'w',\n             },\n             Token::Byte(b'\\x12'),\n@@ -1216,4 +1306,16 @@ mod tests {\n         ];\n         assert_eq!(&expected, &Stater::generate_tokens(s, true).unwrap());\n     }\n+\n+    #[test]\n+    fn test_precision_trunc() {\n+        assert_eq!(precision_trunc(123.456, Precision::NotSpecified), \"123\");\n+        assert_eq!(precision_trunc(123.456, Precision::NoNumber), \"123.456\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(0)), \"123\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(1)), \"123.4\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(2)), \"123.45\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(3)), \"123.456\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(4)), \"123.4560\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(5)), \"123.45600\");\n+    }\n }\n", "test_patch": "diff --git a/tests/by-util/test_stat.rs b/tests/by-util/test_stat.rs\nindex cbd36832f48..cd74767283a 100644\n--- a/tests/by-util/test_stat.rs\n+++ b/tests/by-util/test_stat.rs\n@@ -184,9 +184,74 @@ fn test_char() {\n     ];\n     let ts = TestScenario::new(util_name!());\n     let expected_stdout = unwrap_or_return!(expected_result(&ts, &args)).stdout_move_str();\n+    eprintln!(\"{expected_stdout}\");\n     ts.ucmd().args(&args).succeeds().stdout_is(expected_stdout);\n }\n \n+#[cfg(target_os = \"linux\")]\n+#[test]\n+fn test_printf_mtime_precision() {\n+    // TODO Higher precision numbers (`%.3Y`, `%.4Y`, etc.) are\n+    // formatted correctly, but we are not precise enough when we do\n+    // some `mtime` computations, so we get `.7640` instead of\n+    // `.7639`. This can be fixed by being more careful when\n+    // transforming the number from `Metadata::mtime_nsec()` to the form\n+    // used in rendering.\n+    let args = [\"-c\", \"%.0Y %.1Y %.2Y\", \"/dev/pts/ptmx\"];\n+    let ts = TestScenario::new(util_name!());\n+    let expected_stdout = unwrap_or_return!(expected_result(&ts, &args)).stdout_move_str();\n+    eprintln!(\"{expected_stdout}\");\n+    ts.ucmd().args(&args).succeeds().stdout_is(expected_stdout);\n+}\n+\n+#[cfg(feature = \"touch\")]\n+#[test]\n+fn test_timestamp_format() {\n+    let ts = TestScenario::new(util_name!());\n+\n+    // Create a file with a specific timestamp for testing\n+    ts.ccmd(\"touch\")\n+        .args(&[\"-d\", \"1970-01-01 18:43:33.023456789\", \"k\"])\n+        .succeeds()\n+        .no_stderr();\n+\n+    let test_cases = vec![\n+        // Basic timestamp formats\n+        (\"%Y\", \"67413\"),\n+        (\"%.Y\", \"67413.023456789\"),\n+        (\"%.1Y\", \"67413.0\"),\n+        (\"%.3Y\", \"67413.023\"),\n+        (\"%.6Y\", \"67413.023456\"),\n+        (\"%.9Y\", \"67413.023456789\"),\n+        // Width and padding tests\n+        (\"%13.6Y\", \" 67413.023456\"),\n+        (\"%013.6Y\", \"067413.023456\"),\n+        (\"%-13.6Y\", \"67413.023456 \"),\n+        // Longer width/precision combinations\n+        (\"%18.10Y\", \"  67413.0234567890\"),\n+        (\"%I18.10Y\", \"  67413.0234567890\"),\n+        (\"%018.10Y\", \"0067413.0234567890\"),\n+        (\"%-18.10Y\", \"67413.0234567890  \"),\n+    ];\n+\n+    for (format_str, expected) in test_cases {\n+        let result = ts\n+            .ucmd()\n+            .args(&[\"-c\", format_str, \"k\"])\n+            .succeeds()\n+            .stdout_move_str();\n+\n+        assert_eq!(\n+            result,\n+            format!(\"{expected}\\n\"),\n+            \"Format '{}' failed.\\nExpected: '{}'\\nGot: '{}'\",\n+            format_str,\n+            expected,\n+            result,\n+        );\n+    }\n+}\n+\n #[cfg(any(target_os = \"linux\", target_os = \"android\", target_vendor = \"apple\"))]\n #[test]\n fn test_date() {\n", "problem_statement": "stat: --printf option does not support precision in certain format strings\nThe `--printf` argument to `stat` does not seem to correctly support precision specifiers in format strings.\n\nFor example, after creating a file with `touch f`...\n\nGNU stat:\n```\n$ stat --printf='%.1Y\\n' f\n1646795086.4\n```\n\nuutils stat:\n```\n$ ./target/release/stat --printf='%.1Y\\n' f\n1\n```\n\nFrom the documentation\n> The \u2018%W\u2019, \u2018%X\u2019, \u2018%Y\u2019, and \u2018%Z\u2019 formats accept a precision preceded by a period to specify the number of digits to print after the decimal point. For example, \u2018%.3X\u2019 outputs the access timestamp to millisecond precision. If a period is given but no precision, stat uses 9 digits, so \u2018%.X\u2019 is equivalent to \u2018%.9X\u2019. When discarding excess precision, timestamps are truncated toward minus infinity. \n\n-- https://www.gnu.org/software/coreutils/manual/html_node/stat-invocation.html\n\n*Edit:* this is causing a test failure in the GNU test file `tests/stat/stat-nanoseconds.sh`.\n", "hints_text": "@jfinkels, err\u2026 Where did you find `stat` in this package?\r\nThat's what I see on Windows 7 x64 upon launch of _coreutils 0.0.16_\r\n```console\r\n$ coreutils.exe\r\ncoreutils 0.0.16 (multi-call binary)\r\n\r\nUsage: coreutils [function [arguments...]]                                                      \r\n                                                                                                \r\nCurrently defined functions:                                                                    \r\n                                                                                                \r\n    [, arch, b2sum, b3sum, base32, base64, basename, basenc, cat, cksum, comm, cp, csplit, cut, \r\n    date, dd, df, dir, dircolors, dirname, du, echo, env, expand, expr, factor, false, fmt,     \r\n    fold, hashsum, head, hostname, join, link, ln, ls, md5sum, mkdir, mktemp, more, mv, nl,     \r\n    nproc, numfmt, od, paste, pr, printenv, printf, ptx, pwd, readlink, realpath, relpath, rm,  \r\n    rmdir, seq, sha1sum, sha224sum, sha256sum, sha3-224sum, sha3-256sum, sha3-384sum, sha3-     \r\n    512sum, sha384sum, sha3sum, sha512sum, shake128sum, shake256sum, shred, shuf, sleep, sort,  \r\n    split, sum, sync, tac, tail, tee, test, touch, tr, true, truncate, tsort, unexpand, uniq,   \r\n    unlink, vdir, wc, whoami, yes\r\n\r\n$ coreutils.exe stat\r\nstat: function/utility not found                                                             \r\n```\n@sergeevabc It's not available on Windows. If you're on unix, you can compile with `--features unix` or `--features stat` to access it.\nEven though it was supposed to be only a refactor, https://github.com/uutils/coreutils/pull/4150 solves part of this:\r\n```shell\r\n$ cargo run --quiet -- stat --printf='%.1Y\\n' f\r\n1668645806\r\n```\r\nAt least it shows some useful info and not just `1` \ud83d\ude04 I think the problem is that the precision is misinterpreted as the length of the string instead of the digits after the decimal point or something like that.\nI've dug into this issue and found that the implementation of `stat` does not care about type `float`. Furthermore, the function used to get meta info of files `fs::symlink_metadata` or `fs::metadata` returns values no more than type `integer` or `unsigned` in the first place. So I suppose fixing this would require another dependency such as filetime for higher precision. @tertsdiepraam", "created_at": "2025-01-11 02:34:56", "merge_commit_sha": "988cc4eae36a4098b07ed5447f4f26de436459e5", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build/stable (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']", "['Build (windows-latest, aarch64-pc-windows-msvc, feat_os_windows, use-cross, true)', '.github/workflows/CICD.yml']"], ["['Separate Builds (macos-latest)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_env, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_seq, false)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_wc, false)', '.github/workflows/fuzzing.yml']"], ["['Style/format (ubuntu-latest, feat_os_unix)', '.github/workflows/code-quality.yml']", "['Style and Lint (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']"], ["['Tests (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']", "['Run the fuzzers (fuzz_split, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_echo, true)', '.github/workflows/fuzzing.yml']", "['Binary sizes (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Build/stable (windows-latest, feat_os_windows)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (macos-latest, aarch64-apple-darwin, feat_os_macos)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, i686-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Tests/Toybox test suite (ubuntu-latest)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, i686-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']", "['Update/format (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']"], ["['Run the fuzzers (fuzz_test, true)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_expr, true)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_parse_size, true)', '.github/workflows/fuzzing.yml']", "['Dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_seq_parse_number, true)', '.github/workflows/fuzzing.yml']", "['Test all features separately (macos-latest)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_cksum, false)', '.github/workflows/fuzzing.yml']", "['Run GNU tests', '.github/workflows/GnuTests.yml']"], ["['Build the fuzzers', '.github/workflows/fuzzing.yml']", "['Update/dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']"], ["['Run the fuzzers (fuzz_sort, false)', '.github/workflows/fuzzing.yml']", "['Build (windows-latest, x86_64-pc-windows-msvc, feat_os_windows)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, arm-unknown-linux-gnueabihf, feat_os_unix_gnueabihf, use-cross, true)', '.github/workflows/CICD.yml']", "['Build (windows-latest, x86_64-pc-windows-gnu, feat_os_windows)', '.github/workflows/CICD.yml']"]]}
{"repo": "GuillaumeGomez/sysinfo", "instance_id": "GuillaumeGomez__sysinfo-1387", "base_commit": "5721f848f7e9b5d883e4ff7f88bd0b297d4ee05c", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex a60eb441c..2a9fc5935 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -103,7 +103,7 @@ ntapi = { version = \"0.4\", optional = true }\n windows = { version = \">=0.54, <=0.57\", optional = true }\n \n [target.'cfg(not(any(target_os = \"unknown\", target_arch = \"wasm32\")))'.dependencies]\n-libc = \"^0.2.165\"\n+libc = \"^0.2.164\"\n \n [target.'cfg(any(target_os = \"macos\", target_os = \"ios\"))'.dependencies]\n core-foundation-sys = \"0.8.7\"\n@@ -115,6 +115,7 @@ tempfile = \"3.9\"\n serde_json = \"1.0\" # Used in documentation tests.\n bstr = \"1.9.0\"\n tempfile = \"3.9\"\n+itertools = \"0.13.0\"\n \n [[example]]\n name = \"simple\"\ndiff --git a/src/common/disk.rs b/src/common/disk.rs\nindex 1fe004f11..90d2f307a 100644\n--- a/src/common/disk.rs\n+++ b/src/common/disk.rs\n@@ -4,6 +4,7 @@ use std::ffi::OsStr;\n use std::fmt;\n use std::path::Path;\n \n+use crate::common::impl_get_set::impl_get_set;\n use crate::DiskUsage;\n \n /// Struct containing a disk information.\n@@ -133,7 +134,9 @@ impl Disk {\n         self.inner.is_read_only()\n     }\n \n-    /// Updates the disk' information.\n+    /// Updates the disk' information with everything loaded.\n+    ///\n+    /// Equivalent to <code>[Disk::refresh_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n     ///\n     /// ```no_run\n     /// use sysinfo::Disks;\n@@ -144,7 +147,21 @@ impl Disk {\n     /// }\n     /// ```\n     pub fn refresh(&mut self) -> bool {\n-        self.inner.refresh()\n+        self.refresh_specifics(DiskRefreshKind::everything())\n+    }\n+\n+    /// Updates the disk's information corresponding to the given [`DiskRefreshKind`].\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n+    ///\n+    /// let mut disks = Disks::new_with_refreshed_list();\n+    /// for disk in disks.list_mut() {\n+    ///     disk.refresh_specifics(DiskRefreshKind::new());\n+    /// }\n+    /// ```\n+    pub fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) -> bool {\n+        self.inner.refresh_specifics(refreshes)\n     }\n \n     /// Returns number of bytes read and written by the disk\n@@ -244,7 +261,8 @@ impl Disks {\n     }\n \n     /// Creates a new [`Disks`][crate::Disks] type with the disk list loaded.\n-    /// It is a combination of [`Disks::new`] and [`Disks::refresh_list`].\n+    ///\n+    /// Equivalent to <code>[Disks::new_with_refreshed_list_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n     ///\n     /// ```no_run\n     /// use sysinfo::Disks;\n@@ -255,8 +273,24 @@ impl Disks {\n     /// }\n     /// ```\n     pub fn new_with_refreshed_list() -> Self {\n+        Self::new_with_refreshed_list_specifics(DiskRefreshKind::everything())\n+    }\n+\n+    /// Creates a new [`Disks`][crate::Disks] type with the disk list loaded\n+    /// and refreshed according to the given [`DiskRefreshKind`]. It is a combination of\n+    /// [`Disks::new`] and [`Disks::refresh_list_specifics`].\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n+    ///\n+    /// let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::new());\n+    /// for disk in disks.list() {\n+    ///     println!(\"{disk:?}\");\n+    /// }\n+    /// ```\n+    pub fn new_with_refreshed_list_specifics(refreshes: DiskRefreshKind) -> Self {\n         let mut disks = Self::new();\n-        disks.refresh_list();\n+        disks.refresh_list_specifics(refreshes);\n         disks\n     }\n \n@@ -291,6 +325,13 @@ impl Disks {\n \n     /// Refreshes the listed disks' information.\n     ///\n+    /// Equivalent to <code>[Disks::refresh_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n+    pub fn refresh(&mut self) {\n+        self.refresh_specifics(DiskRefreshKind::everything());\n+    }\n+\n+    /// Refreshes the listed disks' information according to the given [`DiskRefreshKind`].\n+    ///\n     /// \u26a0\ufe0f If a disk is added or removed, this method won't take it into account. Use\n     /// [`Disks::refresh_list`] instead.\n     ///\n@@ -304,30 +345,45 @@ impl Disks {\n     /// // We wait some time...?\n     /// disks.refresh();\n     /// ```\n-    pub fn refresh(&mut self) {\n-        self.inner.refresh();\n+    pub fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) {\n+        self.inner.refresh_specifics(refreshes);\n     }\n \n     /// The disk list will be emptied then completely recomputed.\n     ///\n+    /// Equivalent to <code>[Disks::refresh_list_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::Disks;\n+    ///\n+    /// let mut disks = Disks::new();\n+    /// disks.refresh_list();\n+    /// ```\n+    pub fn refresh_list(&mut self) {\n+        self.refresh_list_specifics(DiskRefreshKind::everything());\n+    }\n+\n+    /// The disk list will be emptied then completely recomputed according to the given\n+    /// [`DiskRefreshKind`].\n+    ///\n     /// ## Linux\n     ///\n     /// \u26a0\ufe0f On Linux, the [NFS](https://en.wikipedia.org/wiki/Network_File_System) file\n     /// systems are ignored and the information of a mounted NFS **cannot** be obtained\n-    /// via [`Disks::refresh_list`]. This is due to the fact that I/O function\n-    /// `statvfs` used by [`Disks::refresh_list`] is blocking and\n+    /// via [`Disks::refresh_list_specifics`]. This is due to the fact that I/O function\n+    /// `statvfs` used by [`Disks::refresh_list_specifics`] is blocking and\n     /// [may hang](https://github.com/GuillaumeGomez/sysinfo/pull/876) in some cases,\n     /// requiring to call `systemctl stop` to terminate the NFS service from the remote\n     /// server in some cases.\n     ///\n     /// ```no_run\n-    /// use sysinfo::Disks;\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n     ///\n     /// let mut disks = Disks::new();\n-    /// disks.refresh_list();\n+    /// disks.refresh_list_specifics(DiskRefreshKind::new());\n     /// ```\n-    pub fn refresh_list(&mut self) {\n-        self.inner.refresh_list();\n+    pub fn refresh_list_specifics(&mut self, refreshes: DiskRefreshKind) {\n+        self.inner.refresh_list_specifics(refreshes);\n     }\n }\n \n@@ -376,3 +432,61 @@ impl fmt::Display for DiskKind {\n         })\n     }\n }\n+\n+/// Used to determine what you want to refresh specifically on the [`Disk`] type.\n+///\n+/// ```no_run\n+/// use sysinfo::{Disks, DiskRefreshKind};\n+///\n+/// let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::everything());\n+///\n+/// for disk in disks.list() {\n+///     assert_eq!(disk.total_space(), 0);\n+/// }\n+/// ```\n+#[derive(Clone, Copy, Debug, Default)]\n+pub struct DiskRefreshKind {\n+    kind: bool,\n+    details: bool,\n+    io_usage: bool,\n+}\n+\n+impl DiskRefreshKind {\n+    /// Creates a new `DiskRefreshKind` with every refresh set to false.\n+    ///\n+    /// ```\n+    /// use sysinfo::DiskRefreshKind;\n+    ///\n+    /// let r = DiskRefreshKind::new();\n+    ///\n+    /// assert_eq!(r.kind(), false);\n+    /// assert_eq!(r.details(), false);\n+    /// assert_eq!(r.io_usage(), false);\n+    /// ```\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Creates a new `DiskRefreshKind` with every refresh set to true.\n+    ///\n+    /// ```\n+    /// use sysinfo::DiskRefreshKind;\n+    ///\n+    /// let r = DiskRefreshKind::everything();\n+    ///\n+    /// assert_eq!(r.kind(), true);\n+    /// assert_eq!(r.details(), true);\n+    /// assert_eq!(r.io_usage(), true);\n+    /// ```\n+    pub fn everything() -> Self {\n+        Self {\n+            kind: true,\n+            details: true,\n+            io_usage: true,\n+        }\n+    }\n+\n+    impl_get_set!(DiskRefreshKind, kind, with_kind, without_kind);\n+    impl_get_set!(DiskRefreshKind, details, with_details, without_details,);\n+    impl_get_set!(DiskRefreshKind, io_usage, with_io_usage, without_io_usage);\n+}\ndiff --git a/src/common/impl_get_set.rs b/src/common/impl_get_set.rs\nnew file mode 100644\nindex 000000000..2df3616db\n--- /dev/null\n+++ b/src/common/impl_get_set.rs\n@@ -0,0 +1,173 @@\n+// Take a look at the license at the top of the repository in the LICENSE file.\n+\n+macro_rules! impl_get_set {\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident $(, $extra_doc:literal)? $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n+        $(#[doc = concat!(\"\n+\", $extra_doc, \"\n+\")])?\n+        #[doc = concat!(\"\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::new();\n+\n+let r = r.with_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), false);\n+```\")]\n+        pub fn $name(&self) -> bool {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `true`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::new();\n+\n+let r = r.with_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self) -> Self {\n+            self.$name = true;\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `false`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), false);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = false;\n+            self\n+        }\n+    };\n+\n+    // To handle `UpdateKind`.\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident, UpdateKind $(, $extra_doc:literal)? $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n+        $(#[doc = concat!(\"\n+\", $extra_doc, \"\n+\")])?\n+        #[doc = concat!(\"\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+\n+let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+```\")]\n+        pub fn $name(&self) -> UpdateKind {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+\n+let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self, kind: UpdateKind) -> Self {\n+            self.$name = kind;\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `UpdateKind::Never`.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = UpdateKind::Never;\n+            self\n+        }\n+    };\n+\n+    // To handle `*RefreshKind`.\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident, $typ:ty $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+\n+let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+```\")]\n+        pub fn $name(&self) -> Option<$typ> {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `Some(...)`.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+\n+let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self, kind: $typ) -> Self {\n+            self.$name = Some(kind);\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `None`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = None;\n+            self\n+        }\n+    };\n+}\n+\n+pub(crate) use impl_get_set;\ndiff --git a/src/common/mod.rs b/src/common/mod.rs\nindex cbbf52131..b59ba48f3 100644\n--- a/src/common/mod.rs\n+++ b/src/common/mod.rs\n@@ -4,6 +4,8 @@\n pub(crate) mod component;\n #[cfg(feature = \"disk\")]\n pub(crate) mod disk;\n+#[cfg(any(feature = \"system\", feature = \"disk\"))]\n+pub(crate) mod impl_get_set;\n #[cfg(feature = \"network\")]\n pub(crate) mod network;\n #[cfg(feature = \"system\")]\ndiff --git a/src/common/system.rs b/src/common/system.rs\nindex 71c71e5b1..73fd41078 100644\n--- a/src/common/system.rs\n+++ b/src/common/system.rs\n@@ -6,6 +6,7 @@ use std::fmt;\n use std::path::Path;\n use std::str::FromStr;\n \n+use crate::common::impl_get_set::impl_get_set;\n use crate::common::DiskUsage;\n use crate::{CpuInner, Gid, ProcessInner, SystemInner, Uid};\n \n@@ -1686,178 +1687,6 @@ cfg_if! {\n     }\n }\n \n-macro_rules! impl_get_set {\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident $(, $extra_doc:literal)? $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n-        $(#[doc = concat!(\"\n-\", $extra_doc, \"\n-\")])?\n-        #[doc = concat!(\"\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-\n-let r = r.with_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-```\")]\n-        pub fn $name(&self) -> bool {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `true`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-\n-let r = r.with_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self) -> Self {\n-            self.$name = true;\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `false`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = false;\n-            self\n-        }\n-    };\n-\n-    // To handle `UpdateKind`.\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident, UpdateKind $(, $extra_doc:literal)? $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n-        $(#[doc = concat!(\"\n-\", $extra_doc, \"\n-\")])?\n-        #[doc = concat!(\"\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-\n-let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-```\")]\n-        pub fn $name(&self) -> UpdateKind {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-\n-let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self, kind: UpdateKind) -> Self {\n-            self.$name = kind;\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `UpdateKind::Never`.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = UpdateKind::Never;\n-            self\n-        }\n-    };\n-\n-    // To handle `*RefreshKind`.\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident, $typ:ty $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-\n-let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-```\")]\n-        pub fn $name(&self) -> Option<$typ> {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `Some(...)`.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-\n-let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self, kind: $typ) -> Self {\n-            self.$name = Some(kind);\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `None`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = None;\n-            self\n-        }\n-    };\n-}\n-\n /// This enum allows you to specify when you want the related information to be updated.\n ///\n /// For example if you only want the [`Process::exe()`] information to be refreshed only if it's not\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 1a1e411c3..c04c3be1a 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -71,7 +71,7 @@ cfg_if! {\n #[cfg(feature = \"component\")]\n pub use crate::common::component::{Component, Components};\n #[cfg(feature = \"disk\")]\n-pub use crate::common::disk::{Disk, DiskKind, Disks};\n+pub use crate::common::disk::{Disk, DiskKind, DiskRefreshKind, Disks};\n #[cfg(feature = \"network\")]\n pub use crate::common::network::{IpNetwork, MacAddr, NetworkData, Networks};\n #[cfg(feature = \"system\")]\ndiff --git a/src/unix/apple/disk.rs b/src/unix/apple/disk.rs\nindex 2ac36ac17..e2f5e28cf 100644\n--- a/src/unix/apple/disk.rs\n+++ b/src/unix/apple/disk.rs\n@@ -7,7 +7,7 @@ use crate::{\n     },\n     DiskUsage,\n };\n-use crate::{Disk, DiskKind};\n+use crate::{Disk, DiskKind, DiskRefreshKind};\n \n use core_foundation_sys::array::CFArrayCreate;\n use core_foundation_sys::base::kCFAllocatorDefault;\n@@ -72,44 +72,70 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        #[cfg(target_os = \"macos\")]\n-        let Some((read_bytes, written_bytes)) = self\n-            .bsd_name\n-            .as_ref()\n-            .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n-        else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n-        #[cfg(not(target_os = \"macos\"))]\n-        let (read_bytes, written_bytes) = (0, 0);\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        if refresh_kind.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            let type_ = {\n+                #[cfg(target_os = \"macos\")]\n+                {\n+                    self.bsd_name\n+                        .as_ref()\n+                        .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n+                        .unwrap_or(DiskKind::Unknown(-1))\n+                }\n+                #[cfg(not(target_os = \"macos\"))]\n+                DiskKind::SSD\n+            };\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n+            self.type_ = type_;\n+        }\n \n-        unsafe {\n-            if let Some(requested_properties) = build_requested_properties(&[\n-                ffi::kCFURLVolumeAvailableCapacityKey,\n-                ffi::kCFURLVolumeAvailableCapacityForImportantUsageKey,\n-            ]) {\n-                match get_disk_properties(&self.volume_url, &requested_properties) {\n-                    Some(disk_props) => {\n-                        self.available_space = get_available_volume_space(&disk_props);\n-                        true\n-                    }\n-                    None => {\n-                        sysinfo_debug!(\"Failed to get disk properties\");\n-                        false\n+        if refresh_kind.io_usage() {\n+            #[cfg(target_os = \"macos\")]\n+            match self\n+                .bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n+            {\n+                Some((read_bytes, written_bytes)) => {\n+                    self.old_read_bytes = self.read_bytes;\n+                    self.old_written_bytes = self.written_bytes;\n+                    self.read_bytes = read_bytes;\n+                    self.written_bytes = written_bytes;\n+                }\n+                None => {\n+                    sysinfo_debug!(\"Failed to update disk i/o stats\");\n+                }\n+            }\n+        }\n+\n+        if refresh_kind.details() {\n+            unsafe {\n+                if let Some(requested_properties) = build_requested_properties(&[\n+                    ffi::kCFURLVolumeTotalCapacityKey,\n+                    ffi::kCFURLVolumeAvailableCapacityKey,\n+                    ffi::kCFURLVolumeAvailableCapacityForImportantUsageKey,\n+                ]) {\n+                    match get_disk_properties(&self.volume_url, &requested_properties) {\n+                        Some(disk_props) => {\n+                            self.total_space = get_int_value(\n+                                disk_props.inner(),\n+                                DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n+                            )\n+                            .unwrap_or_default()\n+                                as u64;\n+                            self.available_space = get_available_volume_space(&disk_props);\n+                        }\n+                        None => {\n+                            sysinfo_debug!(\"Failed to get disk properties\");\n+                        }\n                     }\n+                } else {\n+                    sysinfo_debug!(\"failed to create volume key list, skipping refresh\");\n                 }\n-            } else {\n-                sysinfo_debug!(\"failed to create volume key list, skipping refresh\");\n-                false\n             }\n         }\n+\n+        true\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -129,19 +155,19 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         unsafe {\n             // SAFETY: We don't keep any Objective-C objects around because we\n             // don't make any direct Objective-C calls in this code.\n             with_autorelease(|| {\n-                get_list(&mut self.disks);\n+                get_list(&mut self.disks, refresh_kind);\n             })\n         }\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         for disk in self.list_mut() {\n-            disk.refresh();\n+            disk.refresh_specifics(refresh_kind);\n         }\n     }\n \n@@ -154,7 +180,7 @@ impl crate::DisksInner {\n     }\n }\n \n-unsafe fn get_list(container: &mut Vec<Disk>) {\n+unsafe fn get_list(container: &mut Vec<Disk>, refresh_kind: DiskRefreshKind) {\n     container.clear();\n \n     let raw_disks = {\n@@ -252,7 +278,7 @@ unsafe fn get_list(container: &mut Vec<Disk>) {\n             CStr::from_ptr(c_disk.f_mntonname.as_ptr()).to_bytes(),\n         ));\n \n-        if let Some(disk) = new_disk(mount_point, volume_url, c_disk, &prop_dict) {\n+        if let Some(disk) = new_disk(mount_point, volume_url, c_disk, &prop_dict, refresh_kind) {\n             container.push(disk);\n         }\n     }\n@@ -398,6 +424,7 @@ unsafe fn new_disk(\n     volume_url: RetainedCFURL,\n     c_disk: libc::statfs,\n     disk_props: &RetainedCFDictionary,\n+    refresh_kind: DiskRefreshKind,\n ) -> Option<Disk> {\n     let bsd_name = get_bsd_name(&c_disk);\n \n@@ -406,21 +433,33 @@ unsafe fn new_disk(\n     // so we just assume the disk type is an SSD and set disk i/o stats to 0 until Rust has a way to conditionally link to\n     // IOKit in more recent deployment versions.\n \n-    #[cfg(target_os = \"macos\")]\n-    let type_ = bsd_name\n-        .as_ref()\n-        .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n-        .unwrap_or(DiskKind::Unknown(-1));\n-    #[cfg(not(target_os = \"macos\"))]\n-    let type_ = DiskKind::SSD;\n+    let type_ = if refresh_kind.kind() {\n+        #[cfg(target_os = \"macos\")]\n+        {\n+            bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n+                .unwrap_or(DiskKind::Unknown(-1))\n+        }\n+        #[cfg(not(target_os = \"macos\"))]\n+        DiskKind::SSD\n+    } else {\n+        DiskKind::Unknown(-1)\n+    };\n \n-    #[cfg(target_os = \"macos\")]\n-    let (read_bytes, written_bytes) = bsd_name\n-        .as_ref()\n-        .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n-        .unwrap_or_default();\n-    #[cfg(not(target_os = \"macos\"))]\n-    let (read_bytes, written_bytes) = (0, 0);\n+    let (read_bytes, written_bytes) = if refresh_kind.io_usage() {\n+        #[cfg(target_os = \"macos\")]\n+        {\n+            bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n+                .unwrap_or_default()\n+        }\n+        #[cfg(not(target_os = \"macos\"))]\n+        (0, 0)\n+    } else {\n+        (0, 0)\n+    };\n \n     // Note: Since we requested these properties from the system, we don't expect\n     // these property retrievals to fail.\n@@ -431,7 +470,7 @@ unsafe fn new_disk(\n     )\n     .map(OsString::from)?;\n \n-    let is_removable = {\n+    let is_removable = if refresh_kind.details() {\n         let ejectable = get_bool_value(\n             disk_props.inner(),\n             DictKey::Extern(ffi::kCFURLVolumeIsEjectableKey),\n@@ -459,14 +498,25 @@ unsafe fn new_disk(\n \n             !internal\n         }\n+    } else {\n+        false\n     };\n \n-    let total_space = get_int_value(\n-        disk_props.inner(),\n-        DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n-    )? as u64;\n+    let total_space = if refresh_kind.details() {\n+        get_int_value(\n+            disk_props.inner(),\n+            DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n+        )\n+        .unwrap_or_default() as u64\n+    } else {\n+        0\n+    };\n \n-    let available_space = get_available_volume_space(disk_props);\n+    let available_space = if refresh_kind.details() {\n+        get_available_volume_space(disk_props)\n+    } else {\n+        0\n+    };\n \n     let file_system = {\n         let len = c_disk\n@@ -482,7 +532,7 @@ unsafe fn new_disk(\n         )\n     };\n \n-    let is_read_only = (c_disk.f_flags & libc::MNT_RDONLY as u32) != 0;\n+    let is_read_only = refresh_kind.details() && (c_disk.f_flags & libc::MNT_RDONLY as u32) != 0;\n \n     Some(Disk {\n         inner: DiskInner {\ndiff --git a/src/unix/freebsd/disk.rs b/src/unix/freebsd/disk.rs\nindex dff7cafec..b273ce937 100644\n--- a/src/unix/freebsd/disk.rs\n+++ b/src/unix/freebsd/disk.rs\n@@ -16,7 +16,7 @@ use super::ffi::{\n     DEVSTAT_WRITE,\n };\n use super::utils::{c_buf_to_utf8_str, get_sys_value_str_by_name};\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n #[derive(Debug)]\n pub(crate) struct DiskInner {\n@@ -69,8 +69,8 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        refresh_disk(self)\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        refresh_disk(self, refresh_kind)\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -90,10 +90,8 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n-        unsafe {\n-            get_all_list(&mut self.disks, true);\n-        }\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n+        unsafe { get_all_list(&mut self.disks, true, refresh_kind) }\n     }\n \n     pub(crate) fn list(&self) -> &[Disk] {\n@@ -104,9 +102,9 @@ impl crate::DisksInner {\n         &mut self.disks\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         unsafe {\n-            get_all_list(&mut self.disks, false);\n+            get_all_list(&mut self.disks, false, refresh_kind);\n         }\n     }\n }\n@@ -164,19 +162,27 @@ impl GetValues for DiskInner {\n     }\n }\n \n-fn refresh_disk(disk: &mut DiskInner) -> bool {\n-    unsafe {\n-        let mut vfs: libc::statvfs = std::mem::zeroed();\n-        if libc::statvfs(disk.c_mount_point.as_ptr() as *const _, &mut vfs as *mut _) < 0 {\n-            return false;\n+fn refresh_disk(disk: &mut DiskInner, refresh_kind: DiskRefreshKind) -> bool {\n+    if refresh_kind.details() {\n+        unsafe {\n+            let mut vfs: libc::statvfs = std::mem::zeroed();\n+            if libc::statvfs(disk.c_mount_point.as_ptr() as *const _, &mut vfs as *mut _) < 0 {\n+                sysinfo_debug!(\"statvfs failed\");\n+            } else {\n+                let block_size: u64 = vfs.f_frsize as _;\n+                disk.total_space = vfs.f_blocks.saturating_mul(block_size);\n+                disk.available_space = vfs.f_favail.saturating_mul(block_size);\n+            }\n         }\n-        let block_size: u64 = vfs.f_frsize as _;\n+    }\n \n-        disk.total_space = vfs.f_blocks.saturating_mul(block_size);\n-        disk.available_space = vfs.f_favail.saturating_mul(block_size);\n-        refresh_disk_io(&mut [disk]);\n-        true\n+    if refresh_kind.io_usage() {\n+        unsafe {\n+            refresh_disk_io(&mut [disk]);\n+        }\n     }\n+\n+    true\n }\n \n unsafe fn initialize_geom() -> Result<(), ()> {\n@@ -291,7 +297,11 @@ fn get_disks_mapping() -> HashMap<String, String> {\n     disk_mapping\n }\n \n-pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n+pub unsafe fn get_all_list(\n+    container: &mut Vec<Disk>,\n+    add_new_disks: bool,\n+    refresh_kind: DiskRefreshKind,\n+) {\n     let mut fs_infos: *mut libc::statfs = null_mut();\n \n     let count = libc::getmntinfo(&mut fs_infos, libc::MNT_WAIT);\n@@ -349,15 +359,21 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             OsString::from(mount_point)\n         };\n \n-        if libc::statvfs(fs_info.f_mntonname.as_ptr(), &mut vfs) != 0 {\n-            continue;\n-        }\n-\n-        let f_frsize: u64 = vfs.f_frsize as _;\n-\n-        let is_read_only = (vfs.f_flag & libc::ST_RDONLY) != 0;\n-        let total_space = vfs.f_blocks.saturating_mul(f_frsize);\n-        let available_space = vfs.f_favail.saturating_mul(f_frsize);\n+        let (is_read_only, total_space, available_space) = if refresh_kind.details() {\n+            if libc::statvfs(fs_info.f_mntonname.as_ptr(), &mut vfs) != 0 {\n+                (false, 0, 0)\n+            } else {\n+                let f_frsize: u64 = vfs.f_frsize as _;\n+\n+                (\n+                    ((vfs.f_flag & libc::ST_RDONLY) != 0),\n+                    vfs.f_blocks.saturating_mul(f_frsize),\n+                    vfs.f_favail.saturating_mul(f_frsize),\n+                )\n+            }\n+        } else {\n+            (false, 0, 0)\n+        };\n \n         if let Some(disk) = container.iter_mut().find(|d| d.inner.name == name) {\n             disk.inner.updated = true;\n@@ -367,8 +383,12 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             let dev_mount_point = c_buf_to_utf8_str(&fs_info.f_mntfromname).unwrap_or(\"\");\n \n             // USB keys and CDs are removable.\n-            let is_removable = [b\"USB\", b\"usb\"].iter().any(|b| *b == &fs_type[..])\n-                || fs_type.starts_with(b\"/dev/cd\");\n+            let is_removable = if refresh_kind.details() {\n+                [b\"USB\", b\"usb\"].iter().any(|b| *b == &fs_type[..])\n+                    || fs_type.starts_with(b\"/dev/cd\")\n+            } else {\n+                false\n+            };\n \n             container.push(Disk {\n                 inner: DiskInner {\n@@ -376,8 +396,8 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n                     c_mount_point: fs_info.f_mntonname.to_vec(),\n                     mount_point: PathBuf::from(mount_point),\n                     dev_id: disk_mapping.get(dev_mount_point).map(ToString::to_string),\n-                    total_space: vfs.f_blocks.saturating_mul(f_frsize),\n-                    available_space: vfs.f_favail.saturating_mul(f_frsize),\n+                    total_space,\n+                    available_space,\n                     file_system: OsString::from_vec(fs_type),\n                     is_removable,\n                     is_read_only,\n@@ -404,7 +424,9 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             c.inner.updated = false;\n         }\n     }\n-    refresh_disk_io(container.as_mut_slice());\n+    if refresh_kind.io_usage() {\n+        refresh_disk_io(container.as_mut_slice());\n+    }\n }\n \n // struct DevInfoWrapper {\ndiff --git a/src/unix/linux/disk.rs b/src/unix/linux/disk.rs\nindex 8d1d41fe9..f07b56a96 100644\n--- a/src/unix/linux/disk.rs\n+++ b/src/unix/linux/disk.rs\n@@ -1,7 +1,7 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n use crate::sys::utils::{get_all_utf8_data, to_cpath};\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use libc::statvfs;\n use std::collections::HashMap;\n@@ -37,7 +37,7 @@ macro_rules! cast {\n pub(crate) struct DiskInner {\n     type_: DiskKind,\n     device_name: OsString,\n-    actual_device_name: String,\n+    actual_device_name: Option<String>,\n     file_system: OsString,\n     mount_point: PathBuf,\n     total_space: u64,\n@@ -83,39 +83,52 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        self.efficient_refresh(&disk_stats())\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        self.efficient_refresh(refresh_kind, &disk_stats(&refresh_kind))\n     }\n \n-    fn efficient_refresh(&mut self, procfs_disk_stats: &HashMap<String, DiskStat>) -> bool {\n-        let Some((read_bytes, written_bytes)) =\n-            procfs_disk_stats.get(&self.actual_device_name).map(|stat| {\n-                (\n-                    stat.sectors_read * SECTOR_SIZE,\n-                    stat.sectors_written * SECTOR_SIZE,\n-                )\n-            })\n-        else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n+    fn efficient_refresh(\n+        &mut self,\n+        refresh_kind: DiskRefreshKind,\n+        procfs_disk_stats: &HashMap<String, DiskStat>,\n+    ) -> bool {\n+        if refresh_kind.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            self.type_ = find_type_for_device_name(&self.device_name);\n+        }\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n-\n-        unsafe {\n-            let mut stat: statvfs = mem::zeroed();\n-            let mount_point_cpath = to_cpath(&self.mount_point);\n-            if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n-                let tmp = cast!(stat.f_bsize).saturating_mul(cast!(stat.f_bavail));\n-                self.available_space = cast!(tmp);\n-                true\n+        if refresh_kind.io_usage() {\n+            if self.actual_device_name.is_none() {\n+                self.actual_device_name = Some(get_actual_device_name(&self.device_name));\n+            }\n+            if let Some((read_bytes, written_bytes)) = procfs_disk_stats\n+                .get(self.actual_device_name.as_ref().unwrap())\n+                .map(|stat| {\n+                    (\n+                        stat.sectors_read * SECTOR_SIZE,\n+                        stat.sectors_written * SECTOR_SIZE,\n+                    )\n+                })\n+            {\n+                self.old_read_bytes = self.read_bytes;\n+                self.old_written_bytes = self.written_bytes;\n+                self.read_bytes = read_bytes;\n+                self.written_bytes = written_bytes;\n             } else {\n-                false\n+                sysinfo_debug!(\"Failed to update disk i/o stats\");\n+            }\n+        }\n+\n+        if refresh_kind.details() {\n+            if let Some((total_space, available_space, is_read_only)) =\n+                unsafe { load_statvfs_values(&self.mount_point) }\n+            {\n+                self.total_space = total_space;\n+                self.available_space = available_space;\n+                self.is_read_only = is_read_only;\n             }\n         }\n+\n+        true\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -135,17 +148,19 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         get_all_list(\n             &mut self.disks,\n             &get_all_utf8_data(\"/proc/mounts\", 16_385).unwrap_or_default(),\n+            refresh_kind,\n         )\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n-        let procfs_disk_stats = disk_stats();\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n+        let procfs_disk_stats = disk_stats(&refresh_kind);\n         for disk in self.list_mut() {\n-            disk.inner.efficient_refresh(&procfs_disk_stats);\n+            disk.inner\n+                .efficient_refresh(refresh_kind, &procfs_disk_stats);\n         }\n     }\n \n@@ -177,38 +192,53 @@ fn get_actual_device_name(device: &OsStr) -> String {\n         .unwrap_or_default()\n }\n \n+unsafe fn load_statvfs_values(mount_point: &Path) -> Option<(u64, u64, bool)> {\n+    let mount_point_cpath = to_cpath(mount_point);\n+    let mut stat: statvfs = mem::zeroed();\n+    if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n+        let bsize = cast!(stat.f_bsize);\n+        let blocks = cast!(stat.f_blocks);\n+        let bavail = cast!(stat.f_bavail);\n+        let total = bsize.saturating_mul(blocks);\n+        if total == 0 {\n+            return None;\n+        }\n+        let available = bsize.saturating_mul(bavail);\n+        let is_read_only = (stat.f_flag & libc::ST_RDONLY) != 0;\n+\n+        Some((total, available, is_read_only))\n+    } else {\n+        None\n+    }\n+}\n+\n fn new_disk(\n     device_name: &OsStr,\n     mount_point: &Path,\n     file_system: &OsStr,\n     removable_entries: &[PathBuf],\n     procfs_disk_stats: &HashMap<String, DiskStat>,\n-) -> Option<Disk> {\n-    let mount_point_cpath = to_cpath(mount_point);\n-    let type_ = find_type_for_device_name(device_name);\n-    let mut total = 0;\n-    let mut available = 0;\n-    let mut is_read_only = false;\n-    unsafe {\n-        let mut stat: statvfs = mem::zeroed();\n-        if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n-            let bsize = cast!(stat.f_bsize);\n-            let blocks = cast!(stat.f_blocks);\n-            let bavail = cast!(stat.f_bavail);\n-            total = bsize.saturating_mul(blocks);\n-            available = bsize.saturating_mul(bavail);\n-            is_read_only = (stat.f_flag & libc::ST_RDONLY) != 0;\n-        }\n-        if total == 0 {\n-            return None;\n-        }\n-        let mount_point = mount_point.to_owned();\n-        let is_removable = removable_entries\n+    refresh_kind: DiskRefreshKind,\n+) -> Disk {\n+    let type_ = if refresh_kind.kind() {\n+        find_type_for_device_name(device_name)\n+    } else {\n+        DiskKind::Unknown(-1)\n+    };\n+\n+    let (total_space, available_space, is_read_only) = if refresh_kind.details() {\n+        unsafe { load_statvfs_values(mount_point).unwrap_or((0, 0, false)) }\n+    } else {\n+        (0, 0, false)\n+    };\n+\n+    let is_removable = refresh_kind.details()\n+        && removable_entries\n             .iter()\n             .any(|e| e.as_os_str() == device_name);\n \n+    let (actual_device_name, read_bytes, written_bytes) = if refresh_kind.io_usage() {\n         let actual_device_name = get_actual_device_name(device_name);\n-\n         let (read_bytes, written_bytes) = procfs_disk_stats\n             .get(&actual_device_name)\n             .map(|stat| {\n@@ -218,24 +248,27 @@ fn new_disk(\n                 )\n             })\n             .unwrap_or_default();\n+        (Some(actual_device_name), read_bytes, written_bytes)\n+    } else {\n+        (None, 0, 0)\n+    };\n \n-        Some(Disk {\n-            inner: DiskInner {\n-                type_,\n-                device_name: device_name.to_owned(),\n-                actual_device_name,\n-                file_system: file_system.to_owned(),\n-                mount_point,\n-                total_space: cast!(total),\n-                available_space: cast!(available),\n-                is_removable,\n-                is_read_only,\n-                old_read_bytes: 0,\n-                old_written_bytes: 0,\n-                read_bytes,\n-                written_bytes,\n-            },\n-        })\n+    Disk {\n+        inner: DiskInner {\n+            type_,\n+            device_name: device_name.to_owned(),\n+            actual_device_name,\n+            file_system: file_system.to_owned(),\n+            mount_point: mount_point.to_owned(),\n+            total_space,\n+            available_space,\n+            is_removable,\n+            is_read_only,\n+            old_read_bytes: 0,\n+            old_written_bytes: 0,\n+            read_bytes,\n+            written_bytes,\n+        },\n     }\n }\n \n@@ -310,7 +343,7 @@ fn find_type_for_device_name(device_name: &OsStr) -> DiskKind {\n     }\n }\n \n-fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n+fn get_all_list(container: &mut Vec<Disk>, content: &str, refresh_kind: DiskRefreshKind) {\n     container.clear();\n     // The goal of this array is to list all removable devices (the ones whose name starts with\n     // \"usb-\").\n@@ -331,7 +364,7 @@ fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n         _ => Vec::new(),\n     };\n \n-    let procfs_disk_stats = disk_stats();\n+    let procfs_disk_stats = disk_stats(&refresh_kind);\n \n     for disk in content\n         .lines()\n@@ -378,13 +411,14 @@ fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n                (fs_file.starts_with(\"/run\") && !fs_file.starts_with(\"/run/media\")) ||\n                fs_spec.starts_with(\"sunrpc\"))\n         })\n-        .filter_map(|(fs_spec, fs_file, fs_vfstype)| {\n+        .map(|(fs_spec, fs_file, fs_vfstype)| {\n             new_disk(\n                 fs_spec.as_ref(),\n                 Path::new(&fs_file),\n                 fs_vfstype.as_ref(),\n                 &removable_entries,\n                 &procfs_disk_stats,\n+                refresh_kind,\n             )\n         })\n     {\n@@ -444,14 +478,18 @@ impl DiskStat {\n     }\n }\n \n-fn disk_stats() -> HashMap<String, DiskStat> {\n-    let path = \"/proc/diskstats\";\n-    match fs::read_to_string(path) {\n-        Ok(content) => disk_stats_inner(&content),\n-        Err(_error) => {\n-            sysinfo_debug!(\"failed to read {path:?}: {_error:?}\");\n-            HashMap::new()\n+fn disk_stats(refresh_kind: &DiskRefreshKind) -> HashMap<String, DiskStat> {\n+    if refresh_kind.io_usage() {\n+        let path = \"/proc/diskstats\";\n+        match fs::read_to_string(path) {\n+            Ok(content) => disk_stats_inner(&content),\n+            Err(_error) => {\n+                sysinfo_debug!(\"failed to read {path:?}: {_error:?}\");\n+                HashMap::new()\n+            }\n         }\n+    } else {\n+        Default::default()\n     }\n }\n \ndiff --git a/src/unknown/disk.rs b/src/unknown/disk.rs\nindex 7ab253b47..7a4b97d54 100644\n--- a/src/unknown/disk.rs\n+++ b/src/unknown/disk.rs\n@@ -1,6 +1,6 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use std::{ffi::OsStr, path::Path};\n \n@@ -39,7 +39,7 @@ impl DiskInner {\n         false\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n+    pub(crate) fn refresh_specifics(&mut self, _refreshes: DiskRefreshKind) -> bool {\n         true\n     }\n \n@@ -65,11 +65,11 @@ impl DisksInner {\n         self.disks\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, _refreshes: DiskRefreshKind) {\n         // Does nothing.\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, _refreshes: DiskRefreshKind) {\n         // Does nothing.\n     }\n \ndiff --git a/src/windows/disk.rs b/src/windows/disk.rs\nindex 5d635c396..f9efc6dfb 100644\n--- a/src/windows/disk.rs\n+++ b/src/windows/disk.rs\n@@ -1,7 +1,7 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n use crate::sys::utils::HandleWrapper;\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use std::ffi::{c_void, OsStr, OsString};\n use std::mem::size_of;\n@@ -167,25 +167,28 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        let Some((read_bytes, written_bytes)) = get_disk_io(&self.device_path, None) else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n+    pub(crate) fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) -> bool {\n+        if refreshes.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            self.type_ = get_disk_kind(&self.device_path, None);\n+        }\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n-\n-        if self.total_space != 0 {\n-            unsafe {\n-                let mut tmp = 0;\n-                let lpdirectoryname = PCWSTR::from_raw(self.mount_point.as_ptr());\n-                if GetDiskFreeSpaceExW(lpdirectoryname, None, None, Some(&mut tmp)).is_ok() {\n-                    self.available_space = tmp;\n-                    return true;\n-                }\n+        if refreshes.io_usage() {\n+            if let Some((read_bytes, written_bytes)) = get_disk_io(&self.device_path, None) {\n+                self.old_read_bytes = self.read_bytes;\n+                self.old_written_bytes = self.written_bytes;\n+                self.read_bytes = read_bytes;\n+                self.written_bytes = written_bytes;\n+            } else {\n+                sysinfo_debug!(\"Failed to update disk i/o stats\");\n+            }\n+        }\n+\n+        if refreshes.details() {\n+            if let Some((total_space, available_space)) =\n+                unsafe { get_drive_size(&self.mount_point) }\n+            {\n+                self.total_space = total_space;\n+                self.available_space = available_space;\n             }\n         }\n         false\n@@ -220,15 +223,15 @@ impl DisksInner {\n         self.disks\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refreshes: DiskRefreshKind) {\n         unsafe {\n-            self.disks = get_list();\n+            self.disks = get_list(refreshes);\n         }\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) {\n         for disk in self.list_mut() {\n-            disk.refresh();\n+            disk.refresh_specifics(refreshes);\n         }\n     }\n \n@@ -259,7 +262,7 @@ unsafe fn get_drive_size(mount_point: &[u16]) -> Option<(u64, u64)> {\n     }\n }\n \n-pub(crate) unsafe fn get_list() -> Vec<Disk> {\n+pub(crate) unsafe fn get_list(refreshes: DiskRefreshKind) -> Vec<Disk> {\n     #[cfg(feature = \"multithread\")]\n     use rayon::iter::ParallelIterator;\n \n@@ -268,7 +271,7 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n             let raw_volume_name = PCWSTR::from_raw(volume_name.as_ptr());\n             let drive_type = GetDriveTypeW(raw_volume_name);\n \n-            let is_removable = drive_type == DRIVE_REMOVABLE;\n+            let is_removable = refreshes.details() && drive_type == DRIVE_REMOVABLE;\n \n             if drive_type != DRIVE_FIXED && drive_type != DRIVE_REMOVABLE {\n                 return Vec::new();\n@@ -292,7 +295,7 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n                 );\n                 return Vec::new();\n             }\n-            let is_read_only = (flags & FILE_READ_ONLY_VOLUME) != 0;\n+            let is_read_only = refreshes.details() && (flags & FILE_READ_ONLY_VOLUME) != 0;\n \n             let mount_paths = get_volume_path_names_for_volume_name(&volume_name[..]);\n             if mount_paths.is_empty() {\n@@ -305,51 +308,25 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n                 .copied()\n                 .chain([0])\n                 .collect::<Vec<_>>();\n-            let Some(handle) = HandleWrapper::new_from_file(&device_path[..], Default::default())\n-            else {\n-                return Vec::new();\n-            };\n-            let Some((total_space, available_space)) = get_drive_size(&mount_paths[0][..]) else {\n-                return Vec::new();\n-            };\n-            if total_space == 0 {\n-                sysinfo_debug!(\"total_space == 0\");\n-                return Vec::new();\n-            }\n-            let spq_trim = STORAGE_PROPERTY_QUERY {\n-                PropertyId: StorageDeviceSeekPenaltyProperty,\n-                QueryType: PropertyStandardQuery,\n-                AdditionalParameters: [0],\n+            let handle = HandleWrapper::new_from_file(&device_path[..], Default::default());\n+\n+            let (total_space, available_space) = if refreshes.details() {\n+                get_drive_size(&mount_paths[0][..]).unwrap_or_default()\n+            } else {\n+                (0, 0)\n             };\n-            let mut result: DEVICE_SEEK_PENALTY_DESCRIPTOR = std::mem::zeroed();\n-\n-            let mut dw_size = 0;\n-            let device_io_control = DeviceIoControl(\n-                handle.0,\n-                IOCTL_STORAGE_QUERY_PROPERTY,\n-                Some(&spq_trim as *const STORAGE_PROPERTY_QUERY as *const c_void),\n-                size_of::<STORAGE_PROPERTY_QUERY>() as u32,\n-                Some(&mut result as *mut DEVICE_SEEK_PENALTY_DESCRIPTOR as *mut c_void),\n-                size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32,\n-                Some(&mut dw_size),\n-                None,\n-            )\n-            .is_ok();\n-            let type_ = if !device_io_control\n-                || dw_size != size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32\n-            {\n-                DiskKind::Unknown(-1)\n+\n+            let type_ = if refreshes.kind() {\n+                get_disk_kind(&device_path, handle.as_ref())\n             } else {\n-                let is_hdd = result.IncursSeekPenalty.as_bool();\n-                if is_hdd {\n-                    DiskKind::HDD\n-                } else {\n-                    DiskKind::SSD\n-                }\n+                DiskKind::Unknown(-1)\n             };\n \n-            let (read_bytes, written_bytes) =\n-                get_disk_io(&device_path, Some(handle)).unwrap_or_default();\n+            let (read_bytes, written_bytes) = if refreshes.io_usage() {\n+                get_disk_io(&device_path, handle).unwrap_or_default()\n+            } else {\n+                (0, 0)\n+            };\n \n             let name = os_string_from_zero_terminated(&name);\n             let file_system = os_string_from_zero_terminated(&file_system);\n@@ -383,6 +360,63 @@ fn os_string_from_zero_terminated(name: &[u16]) -> OsString {\n     OsString::from_wide(&name[..len])\n }\n \n+fn get_disk_kind(device_path: &[u16], borrowed_handle: Option<&HandleWrapper>) -> DiskKind {\n+    let binding = (\n+        borrowed_handle,\n+        if borrowed_handle.is_none() {\n+            unsafe { HandleWrapper::new_from_file(device_path, Default::default()) }\n+        } else {\n+            None\n+        },\n+    );\n+    let handle = match binding {\n+        (Some(handle), _) => handle,\n+        (_, Some(ref handle)) => handle,\n+        (None, None) => return DiskKind::Unknown(-1),\n+    };\n+\n+    if handle.is_invalid() {\n+        sysinfo_debug!(\n+            \"Expected handle to {:?} to be valid\",\n+            String::from_utf16_lossy(device_path)\n+        );\n+        return DiskKind::Unknown(-1);\n+    }\n+\n+    let spq_trim = STORAGE_PROPERTY_QUERY {\n+        PropertyId: StorageDeviceSeekPenaltyProperty,\n+        QueryType: PropertyStandardQuery,\n+        AdditionalParameters: [0],\n+    };\n+    let mut result: DEVICE_SEEK_PENALTY_DESCRIPTOR = unsafe { std::mem::zeroed() };\n+\n+    let mut dw_size = 0;\n+    let device_io_control = unsafe {\n+        DeviceIoControl(\n+            handle.0,\n+            IOCTL_STORAGE_QUERY_PROPERTY,\n+            Some(&spq_trim as *const STORAGE_PROPERTY_QUERY as *const c_void),\n+            size_of::<STORAGE_PROPERTY_QUERY>() as u32,\n+            Some(&mut result as *mut DEVICE_SEEK_PENALTY_DESCRIPTOR as *mut c_void),\n+            size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32,\n+            Some(&mut dw_size),\n+            None,\n+        )\n+        .is_ok()\n+    };\n+\n+    if !device_io_control || dw_size != size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32 {\n+        DiskKind::Unknown(-1)\n+    } else {\n+        let is_hdd = result.IncursSeekPenalty.as_bool();\n+        if is_hdd {\n+            DiskKind::HDD\n+        } else {\n+            DiskKind::SSD\n+        }\n+    }\n+}\n+\n /// Returns a tuple consisting of the total number of bytes read and written by the volume with the specified device path\n fn get_disk_io(device_path: &[u16], handle: Option<HandleWrapper>) -> Option<(u64, u64)> {\n     let handle =\n@@ -390,7 +424,7 @@ fn get_disk_io(device_path: &[u16], handle: Option<HandleWrapper>) -> Option<(u6\n \n     if handle.is_invalid() {\n         sysinfo_debug!(\n-            \"Expected handle to '{:?}' to be valid\",\n+            \"Expected handle to {:?} to be valid\",\n             String::from_utf16_lossy(device_path)\n         );\n         return None;\n", "test_patch": "diff --git a/tests/disk.rs b/tests/disk.rs\nindex 4210fcac7..2fbab2ba3 100644\n--- a/tests/disk.rs\n+++ b/tests/disk.rs\n@@ -1,34 +1,158 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n+fn should_skip() -> bool {\n+    if !sysinfo::IS_SUPPORTED_SYSTEM {\n+        return true;\n+    }\n+\n+    let s = sysinfo::System::new_all();\n+\n+    // If we don't have any physical core present, it's very likely that we're inside a VM...\n+    s.physical_core_count().unwrap_or_default() == 0\n+}\n+\n #[test]\n #[cfg(all(feature = \"system\", feature = \"disk\"))]\n fn test_disks() {\n-    if sysinfo::IS_SUPPORTED_SYSTEM {\n-        let s = sysinfo::System::new_all();\n-        // If we don't have any physical core present, it's very likely that we're inside a VM...\n-        if s.physical_core_count().unwrap_or_default() > 0 {\n-            let mut disks = sysinfo::Disks::new();\n-            assert!(disks.list().is_empty());\n-            disks.refresh_list();\n-            assert!(!disks.list().is_empty());\n+    if should_skip() {\n+        return;\n+    }\n+\n+    let mut disks = sysinfo::Disks::new();\n+    assert!(disks.list().is_empty());\n+    disks.refresh_list();\n+    assert!(!disks.list().is_empty());\n+}\n+\n+#[test]\n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n+fn test_disk_refresh_kind() {\n+    use itertools::Itertools;\n+\n+    use sysinfo::{DiskKind, DiskRefreshKind, Disks};\n+\n+    if should_skip() {\n+        return;\n+    }\n+\n+    for fs in [\n+        DiskRefreshKind::with_kind,\n+        DiskRefreshKind::without_kind,\n+        DiskRefreshKind::with_details,\n+        DiskRefreshKind::without_details,\n+        DiskRefreshKind::with_io_usage,\n+        DiskRefreshKind::without_io_usage,\n+    ]\n+    .iter()\n+    .powerset()\n+    {\n+        let mut refreshes = DiskRefreshKind::new();\n+        for f in fs {\n+            refreshes = f(refreshes);\n         }\n+\n+        let assertions = |name: &'static str, disks: &Disks| {\n+            if refreshes.kind() {\n+                // This would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                #[cfg(not(any(target_os = \"freebsd\", target_os = \"windows\")))]\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.kind() != DiskKind::Unknown(-1)),\n+                    \"{name}: disk.kind should be refreshed\"\n+                );\n+            } else {\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.kind() == DiskKind::Unknown(-1)),\n+                    \"{name}: disk.kind should not be refreshed\"\n+                );\n+            }\n+\n+            if refreshes.details() {\n+                // These would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.available_space() != Default::default()),\n+                    \"{name}: disk.available_space should be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.total_space() != Default::default()),\n+                    \"{name}: disk.total_space should be refreshed\"\n+                );\n+                // We can't assert anything about booleans, since false is indistinguishable from\n+                // not-refreshed\n+            } else {\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.available_space() == Default::default()),\n+                    \"{name}: disk.available_space should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.total_space() == Default::default()),\n+                    \"{name}: disk.total_space should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.is_read_only() == <bool as Default>::default()),\n+                    \"{name}: disk.is_read_only should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.is_removable() == <bool as Default>::default()),\n+                    \"{name}: disk.is_removable should not be refreshed\"\n+                );\n+            }\n+\n+            if refreshes.io_usage() {\n+                // This would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                assert!(\n+                    disks.iter().any(|disk| disk.usage() != Default::default()),\n+                    \"{name}: disk.usage should be refreshed\"\n+                );\n+            } else {\n+                assert!(\n+                    disks.iter().all(|disk| disk.usage() == Default::default()),\n+                    \"{name}: disk.usage should not be refreshed\"\n+                );\n+            }\n+        };\n+\n+        // load and refresh with the desired details should work\n+        let disks = Disks::new_with_refreshed_list_specifics(refreshes);\n+        assertions(\"full\", &disks);\n+\n+        // load with minimal `DiskRefreshKind`, then refresh for added detail should also work!\n+        let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::new());\n+        disks.refresh_specifics(refreshes);\n+        assertions(\"incremental\", &disks);\n     }\n }\n \n #[test]\n-#[cfg(feature = \"disk\")]\n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n fn test_disks_usage() {\n     use std::fs::{remove_file, File};\n     use std::io::Write;\n     use std::path::{Path, PathBuf};\n     use std::thread::sleep;\n \n-    use sysinfo::{CpuRefreshKind, Disks, RefreshKind, System};\n-\n-    let s = System::new_with_specifics(RefreshKind::new().with_cpu(CpuRefreshKind::new()));\n+    use sysinfo::Disks;\n \n-    // Skip the tests on unsupported platforms and on systems with no physical cores (likely a VM)\n-    if !sysinfo::IS_SUPPORTED_SYSTEM || s.physical_core_count().unwrap_or_default() == 0 {\n+    if should_skip() {\n         return;\n     }\n \n", "problem_statement": "Selectively refreshing disks\nWhen running `Disks::new_with_refreshed_list()` on android devices, I often see warnings (from SELinux?) along the lines of:\r\n\r\n```\r\ntype=1400 audit(0.0:339351): avc:  denied  { search } for  name=\"block\" dev=\"tmpfs\" ino=12\r\n```\r\n\r\nI've done some spelunking, and it appears they are a side-effect of [this `canonicalize` call](https://github.com/GuillaumeGomez/sysinfo/blob/master/src/unix/linux/disk.rs#L157), on certain disks' mount points.\r\n\r\n(N.b. the dev=\"tmpfs\" in the warning doesn't refer to the type of the filesystem being refreshed. I'm not using `linux-tmpfs`, so they're filtered out. In debugging, I've seen the warning for \"ext4\" and \"fuse\" filesystems.)\r\n\r\nI know these warnings are harmless! ...but I've had customers who were alarmed about them, so I'd prefer to avoid generating them. Given my use-case (I'm only reporting disk usage over time for *one* disk), I'd love some sort of mechanism for *selectively* refreshing `Disks`. There are lots of options.\r\n\r\nMy first suggestion would be a separate `Disks` constructor (`new_with_list()` or `new_with_unrefreshed_list()`?) that loads the *list* of disks from /proc/mounts, but defers the *stat collection* until a later `refresh`. On linux, at least, I think the separation makes sense, and it gives complete control over *what is refreshed* to the library user, without adding much complexity to the interface. On other platforms, I haven't looked, but it seems like the worst-case would simply be that `new_with_list()` calls `new_with_refreshed_list()` on platforms where there's no distinction between listing and refreshing.\r\n\r\nI'd be happy to do the work. If my proposal sounds reasonable, I'll submit a PR next week and we can hash out the specifics. If not, I'd be open to other ways of solving this... just let me know :)\n", "hints_text": "Seems a bit weird to want to have disks but not their path or even their kind. I think the best course would be to instead detect when the `canonicalize` call is actually needed and only call it if so.\n> Seems a bit weird to want to have disks but not their path or even their kind.\r\n\r\nI was thinking those *would* be present in the `new_with_list` return. Only the information obtained from `statvfs` would be deferred.\r\n\r\n> I think the best course would be to instead detect when the canonicalize call is actually needed and only call it if so.\r\n\r\nHummm. Canonicalize (on linux) IIUC is just `realpath`. How would one decide if it were necessary? (I think you'd end up reimplementing almost all of `realpath`, right?)\nThe kind is retrieved from the function using `canonicalize`, so both functions would get a call to it.\r\n\r\n> Hummm. Canonicalize (on linux) IIUC is just `realpath`. How would one decide if it were necessary? (I think you'd end up reimplementing almost all of `realpath`, right?)\r\n\r\nMostly checking if it's a link, if not, checking if it doesn't start with `/` or contains `..` (still not great to check that ourselves).\r\n\r\nAny clue why `statvfs` is emitting this warning?\n> The kind is retrieved from the function using canonicalize, so both functions would get a call to it.\r\n\r\nAh, I see. I was thinking about `vfstype`, not `DiskKind`. You're right that `DiskKind` requires the `canonicalize`.\r\n\r\nSo in my suggestion, I guess the populated fields would be `device_name`, `file_system` and `mount_point`; the other fields would require a `refresh`.\r\n\r\n> Any clue why statvfs is emitting this warning?\r\n\r\nI believe it happens during the `canonicalize`, not the `statvfs`. And it's coming from SELinux or apparmor or some similar system for permissions above and beyond POSIX. The fact that it emits these audit warnings *in addition to failing the syscall* is super annoying, at least in this instance :/\nI think `DiskKind` is part of the fundamental information for a `Disk`, so I'm not super open to the idea of not having it by default.\r\n\r\nHowever the `RefreshKind` approach for `Disks` sounds pretty good, just not sure there are enough different fields to make it relevant currently.\nI'm not sure what you mean about `RefreshKind`, can you elaborate?\nSure. Like for [`refresh_processes_specifics`](https://docs.rs/sysinfo/latest/sysinfo/struct.System.html#method.refresh_processes_specifics): you can pick what you want to refresh with `ProcessRefreshKind`.\nAhh, I see. That's perfect! I'd be happy to extend that pattern over `Disks`. So I think I would:\r\n * add a `DiskRefreshKind`\r\n * add a `Disks::new_with_specifics()` constructor\r\n * add a `Disks::refresh_specifics()` refresher\r\n * add a `Disk::refresh_specifics()` refresher\r\n * make (at least) the linux impl respect the specific refresh kind(s)\r\n\r\n... sound about right? That would work for me :)\nYes but that wouldn't solve your problem since I still want `DiskKind` to be retrieved for all `Disk` unconditionally. ^^'\nWhat if you always get `DiskKind` unless you specifically ask *not* to get it?\nThat could work. Only question, what value do you put into `DiskKind::Unknown`? It's supposed to mean something to the OS.\nI'm not sure. I don't use the `DiskKind`, personally. The idiomatic Rust thing would be to make it `Option<DiskKind>`, but I'm guessing that's exactly what you want to avoid. Could we add another enum variant (`DiskKind::NeedsReload`)?\nIndeed, I don't want an `Option`. I'm still wondering if we're not trying to go around a technical issue that could be fixed instead...\nFWIW: even if it weren't for the annoying warnings, I would still use this functionality, since I'm currently loading way more information than I actually need :)\nWe can do it in two passes then: a PR to implement the selective refresh and another where we can debate what's best for `DiskKind`.\nLook for a PR early next week!\nThanks in advance!", "created_at": "2024-11-22 21:12:44", "merge_commit_sha": "08dee51f02d246e9b5ea40f31721c1f396d8d016", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check stable / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Check nightly / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check stable / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-apple-ios', '.github/workflows/CI.yml']", "['Check nightly / i686-linux-android', '.github/workflows/CI.yml']"], ["['Check stable / armv7-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['Test ubuntu-latest (rust stable)', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']", "['Check nightly / arm-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Test ubuntu-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / aarch64-unknown-linux-musl', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust stable)', '.github/workflows/CI.yml']", "['Check 1.74.0 / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']", "['Check 1.74.0 / armv7-linux-androideabi', '.github/workflows/CI.yml']"], ["['c_interface', '.github/workflows/CI.yml']", "['Check 1.74.0 / x86_64-apple-ios', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check nightly / x86_64-linux-android', '.github/workflows/CI.yml']"], ["['clippy (macos-latest)', '.github/workflows/CI.yml']", "['clippy (ubuntu-latest)', '.github/workflows/CI.yml']"], ["['Check nightly / arm-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['unknown-targets (1.74.0)', '.github/workflows/CI.yml']"], ["['clippy (windows-latest)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check nightly / i686-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Test windows-latest (rust nightly)', '.github/workflows/CI.yml']"], ["['rustfmt', '.github/workflows/CI.yml']", "['Check stable / x86_64-apple-darwin', '.github/workflows/CI.yml']"]]}
{"repo": "GuillaumeGomez/sysinfo", "instance_id": "GuillaumeGomez__sysinfo-1324", "base_commit": "296bbf02359b11eef28fc344832760b654215a22", "patch": "diff --git a/src/common/system.rs b/src/common/system.rs\nindex 219d9881d..5fd0d0548 100644\n--- a/src/common/system.rs\n+++ b/src/common/system.rs\n@@ -367,6 +367,9 @@ impl System {\n     /// exist (it will **NOT** be removed from the processes if it doesn't exist anymore). If it\n     /// isn't listed yet, it'll be added.\n     ///\n+    /// \u26a0\ufe0f If you need to refresh multiple processes at once, use [`refresh_pids`] instead! It has\n+    /// much better performance.\n+    ///\n     /// It is the same as calling:\n     ///\n     /// ```no_run\n@@ -394,6 +397,8 @@ impl System {\n     /// let mut s = System::new_all();\n     /// s.refresh_process(Pid::from(1337));\n     /// ```\n+    ///\n+    /// [`refresh_pids`]: #method.refresh_pids\n     pub fn refresh_process(&mut self, pid: Pid) -> bool {\n         self.refresh_process_specifics(\n             pid,\n@@ -409,6 +414,9 @@ impl System {\n     /// exist (it will **NOT** be removed from the processes if it doesn't exist anymore). If it\n     /// isn't listed yet, it'll be added.\n     ///\n+    /// \u26a0\ufe0f If you need to refresh multiple processes at once, use [`refresh_pids_specifics`]\n+    /// instead! It has much better performance.\n+    ///\n     /// \u26a0\ufe0f On Linux, `sysinfo` keeps the `stat` files open by default. You can change this behaviour\n     /// by using [`set_open_files_limit`][crate::set_open_files_limit].\n     ///\n@@ -418,6 +426,8 @@ impl System {\n     /// let mut s = System::new_all();\n     /// s.refresh_process_specifics(Pid::from(1337), ProcessRefreshKind::new());\n     /// ```\n+    ///\n+    /// [`refresh_pids_specifics`]: #method.refresh_pids_specifics\n     pub fn refresh_process_specifics(\n         &mut self,\n         pid: Pid,\ndiff --git a/src/unix/apple/macos/system.rs b/src/unix/apple/macos/system.rs\nindex d0191eb2e..6c182ae1f 100644\n--- a/src/unix/apple/macos/system.rs\n+++ b/src/unix/apple/macos/system.rs\n@@ -8,6 +8,7 @@ use libc::{\n     processor_cpu_load_info_t, sysconf, vm_page_size, PROCESSOR_CPU_LOAD_INFO, _SC_CLK_TCK,\n };\n use std::ptr::null_mut;\n+use std::time::Instant;\n \n struct ProcessorCpuLoadInfo {\n     cpu_load: processor_cpu_load_info_t,\n@@ -55,6 +56,8 @@ pub(crate) struct SystemTimeInfo {\n     timebase_to_ns: f64,\n     clock_per_sec: f64,\n     old_cpu_info: ProcessorCpuLoadInfo,\n+    last_update: Option<Instant>,\n+    prev_time_interval: f64,\n }\n \n unsafe impl Send for SystemTimeInfo {}\n@@ -97,40 +100,52 @@ impl SystemTimeInfo {\n                 timebase_to_ns: info.numer as f64 / info.denom as f64,\n                 clock_per_sec: nano_per_seconds / clock_ticks_per_sec as f64,\n                 old_cpu_info,\n+                last_update: None,\n+                prev_time_interval: 0.,\n             })\n         }\n     }\n \n     pub fn get_time_interval(&mut self, port: mach_port_t) -> f64 {\n-        let mut total = 0;\n-        let new_cpu_info = match ProcessorCpuLoadInfo::new(port) {\n-            Some(cpu_info) => cpu_info,\n-            None => return 0.,\n-        };\n-        let cpu_count = std::cmp::min(self.old_cpu_info.cpu_count, new_cpu_info.cpu_count);\n-        unsafe {\n-            for i in 0..cpu_count {\n-                let new_load: &processor_cpu_load_info = &*new_cpu_info.cpu_load.offset(i as _);\n-                let old_load: &processor_cpu_load_info =\n-                    &*self.old_cpu_info.cpu_load.offset(i as _);\n-                for (new, old) in new_load.cpu_ticks.iter().zip(old_load.cpu_ticks.iter()) {\n-                    if new > old {\n-                        total += new.saturating_sub(*old);\n+        let need_cpu_usage_update = self\n+            .last_update\n+            .map(|last_update| last_update.elapsed() > crate::MINIMUM_CPU_UPDATE_INTERVAL)\n+            .unwrap_or(true);\n+        if need_cpu_usage_update {\n+            let mut total = 0;\n+            let new_cpu_info = match ProcessorCpuLoadInfo::new(port) {\n+                Some(cpu_info) => cpu_info,\n+                None => return 0.,\n+            };\n+            let cpu_count = std::cmp::min(self.old_cpu_info.cpu_count, new_cpu_info.cpu_count);\n+            unsafe {\n+                for i in 0..cpu_count {\n+                    let new_load: &processor_cpu_load_info = &*new_cpu_info.cpu_load.offset(i as _);\n+                    let old_load: &processor_cpu_load_info =\n+                        &*self.old_cpu_info.cpu_load.offset(i as _);\n+                    for (new, old) in new_load.cpu_ticks.iter().zip(old_load.cpu_ticks.iter()) {\n+                        if new > old {\n+                            total += new.saturating_sub(*old);\n+                        }\n                     }\n                 }\n             }\n \n             self.old_cpu_info = new_cpu_info;\n+            self.last_update = Some(Instant::now());\n \n             // Now we convert the ticks to nanoseconds (if the interval is less than\n             // `MINIMUM_CPU_UPDATE_INTERVAL`, we replace it with it instead):\n             let base_interval = total as f64 / cpu_count as f64 * self.clock_per_sec;\n             let smallest = crate::MINIMUM_CPU_UPDATE_INTERVAL.as_secs_f64() * 1_000_000_000.0;\n-            if base_interval < smallest {\n+            self.prev_time_interval = if base_interval < smallest {\n                 smallest\n             } else {\n                 base_interval / self.timebase_to_ns\n-            }\n+            };\n+            self.prev_time_interval\n+        } else {\n+            self.prev_time_interval\n         }\n     }\n }\n", "test_patch": "diff --git a/tests/process.rs b/tests/process.rs\nindex f6f841a6f..e0963de56 100644\n--- a/tests/process.rs\n+++ b/tests/process.rs\n@@ -813,3 +813,47 @@ fn test_parent_change() {\n     // We kill the child to clean up.\n     child.kill();\n }\n+\n+// We want to ensure that if `System::refresh_process*` methods are called\n+// one after the other, it won't impact the CPU usage computation badly.\n+#[test]\n+fn test_multiple_single_process_refresh() {\n+    if !sysinfo::IS_SUPPORTED_SYSTEM || cfg!(feature = \"apple-sandbox\") || cfg!(windows) {\n+        // Windows never updates its parent PID so no need to check anything.\n+        return;\n+    }\n+\n+    let file_name = \"target/test_binary3\";\n+    build_test_binary(file_name);\n+    let mut p_a = std::process::Command::new(format!(\"./{file_name}\"))\n+        .arg(\"1\")\n+        .spawn()\n+        .unwrap();\n+    let mut p_b = std::process::Command::new(format!(\"./{file_name}\"))\n+        .arg(\"1\")\n+        .spawn()\n+        .unwrap();\n+\n+    let pid_a = Pid::from_u32(p_a.id() as _);\n+    let pid_b = Pid::from_u32(p_b.id() as _);\n+\n+    let mut s = System::new();\n+    let process_refresh_kind = ProcessRefreshKind::new().with_cpu();\n+    s.refresh_process_specifics(pid_a, process_refresh_kind);\n+    s.refresh_process_specifics(pid_b, process_refresh_kind);\n+\n+    std::thread::sleep(std::time::Duration::from_secs(1));\n+    s.refresh_process_specifics(pid_a, process_refresh_kind);\n+    s.refresh_process_specifics(pid_b, process_refresh_kind);\n+\n+    let cpu_a = s.process(pid_a).unwrap().cpu_usage();\n+    let cpu_b = s.process(pid_b).unwrap().cpu_usage();\n+\n+    p_a.kill().expect(\"failed to kill process a\");\n+    p_b.kill().expect(\"failed to kill process b\");\n+\n+    let _ = p_a.wait();\n+    let _ = p_b.wait();\n+\n+    assert!(cpu_b - 5. < cpu_a && cpu_b + 5. > cpu_a);\n+}\n", "problem_statement": "Refreshing multiple processes on macos leads to invalid cpu usage\nI have a piece of code that looks like this:\r\n\r\n```rust\r\nlet mut sys = sysinfo::System::new();\r\nlet process_refresh_kind = ProcessRefreshKind::everything().without_disk_usage().without_environ();\r\nsys.refresh_process_specifics(pid_a, process_refresh_kind);\r\nsys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\nloop {\r\n    // Wait 1 minute\r\n    std::thread::sleep(std::time::Duration::from_secs(60));\r\n\r\n    // Refresh CPU measurement\r\n    sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n    sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n    // Get CPU usage of the two processes\r\n    let cpu_a = sys.process(pid_a).map(|p| p.cpu_usage());\r\n    let cpu_b = sys.process(pid_b).map(|p| p.cpu_usage());\r\n    println!(\"cpu a: {:?}, cpu b: {:?}\", cpu_a, cpu_b);\r\n}\r\n```\r\n\r\nWith this code, the cpu for process A seems to be valid, but the one for process B is way too high, it can even get higher than the max value.\r\n\r\nLooking at the code, I see that the function `compute_cpu_usage` uses a `time_interval` variable. This value seems to be right on the first call to refresh (for pid A), but not on the second call (for pid B). I suppose this value is reset after each call to \"refresh_process\", leading to the second call computing a CPU load with a time interval that is way too small.\r\n\r\nHere is a reproducer:\r\n\r\n```rust\r\nfn main() {\r\n    let mut sys = sysinfo::System::new();\r\n\r\n    let pid_a = sysinfo::Pid::from_u32(std::process::id());\r\n    let pid_b = sysinfo::Pid::from_u32(1);\r\n\r\n    let process_refresh_kind = sysinfo::ProcessRefreshKind::everything()\r\n        .without_disk_usage()\r\n        .without_environ();\r\n    sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n    sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n    loop {\r\n        std::thread::sleep(std::time::Duration::from_secs(10));\r\n\r\n        sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n        sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n        let cpu_a = sys.process(pid_a).map(|p| p.cpu_usage());\r\n        let cpu_b = sys.process(pid_b).map(|p| p.cpu_usage());\r\n        println!(\"cpu a: {:?}, cpu b: {:?}\", cpu_a, cpu_b);\r\n    }\r\n}\r\n```\r\n\r\nReversing the two PIDs reverses the one that is reported with a very high CPU usage, showing the bug.\r\n\r\nThis was reproduced on 0.30.12 on macos x64.\n", "hints_text": "For this kind of operations, you likely want to use [refresh_pids_specifics](https://docs.rs/sysinfo/latest/sysinfo/struct.System.html#method.refresh_pids_specifics). Although, in this case it should still work.\nIndeed, I didn't know about this API, and it fixes the issue. I'm leaving this issue open since there is still a bug afaict, but i'm no longer blocked by it, I let you decide what to do with it. Thanks!\nIt definitely needs to be fixed. Gonna try to take a look in the next days.", "created_at": "2024-07-24 20:46:11", "merge_commit_sha": "222ed97ab06e898767e8194658a9f6d0bd755ac6", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check stable / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Check nightly / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check stable / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-apple-ios', '.github/workflows/CI.yml']", "['Check nightly / i686-linux-android', '.github/workflows/CI.yml']"], ["['Check stable / armv7-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['Test ubuntu-latest (rust stable)', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']", "['Check nightly / arm-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Test ubuntu-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / aarch64-unknown-linux-musl', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust stable)', '.github/workflows/CI.yml']", "['Check 1.74.0 / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']", "['Check 1.74.0 / armv7-linux-androideabi', '.github/workflows/CI.yml']"], ["['c_interface', '.github/workflows/CI.yml']", "['Check 1.74.0 / x86_64-apple-ios', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check nightly / x86_64-linux-android', '.github/workflows/CI.yml']"], ["['clippy (macos-latest)', '.github/workflows/CI.yml']", "['clippy (ubuntu-latest)', '.github/workflows/CI.yml']"], ["['Check nightly / arm-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['unknown-targets (1.74.0)', '.github/workflows/CI.yml']"], ["['clippy (windows-latest)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check nightly / i686-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Test windows-latest (rust nightly)', '.github/workflows/CI.yml']"], ["['rustfmt', '.github/workflows/CI.yml']", "['Check stable / x86_64-apple-darwin', '.github/workflows/CI.yml']"]]}
{"repo": "uutils/coreutils", "instance_id": "uutils__coreutils-7154", "base_commit": "64dad0c3ab704be4c6c4f4fb802afa026b983a5e", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 1b2a67c13aa..29b14b9a820 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -2871,9 +2871,11 @@ version = \"0.0.29\"\n dependencies = [\n  \"ansi-width\",\n  \"chrono\",\n+ \"chrono-tz\",\n  \"clap\",\n  \"glob\",\n  \"hostname\",\n+ \"iana-time-zone\",\n  \"lscolors\",\n  \"number_prefix\",\n  \"once_cell\",\ndiff --git a/src/uu/ls/Cargo.toml b/src/uu/ls/Cargo.toml\nindex 17cef9b8aa4..0b60009e65b 100644\n--- a/src/uu/ls/Cargo.toml\n+++ b/src/uu/ls/Cargo.toml\n@@ -18,13 +18,17 @@ path = \"src/ls.rs\"\n \n [dependencies]\n ansi-width = { workspace = true }\n-clap = { workspace = true, features = [\"env\"] }\n chrono = { workspace = true }\n-number_prefix = { workspace = true }\n-uutils_term_grid = { workspace = true }\n-terminal_size = { workspace = true }\n+chrono-tz = { workspace = true }\n+clap = { workspace = true, features = [\"env\"] }\n glob = { workspace = true }\n+hostname = { workspace = true }\n+iana-time-zone = { workspace = true }\n lscolors = { workspace = true }\n+number_prefix = { workspace = true }\n+once_cell = { workspace = true }\n+selinux = { workspace = true, optional = true }\n+terminal_size = { workspace = true }\n uucore = { workspace = true, features = [\n   \"colors\",\n   \"entries\",\n@@ -34,9 +38,7 @@ uucore = { workspace = true, features = [\n   \"quoting-style\",\n   \"version-cmp\",\n ] }\n-once_cell = { workspace = true }\n-selinux = { workspace = true, optional = true }\n-hostname = { workspace = true }\n+uutils_term_grid = { workspace = true }\n \n [[bin]]\n name = \"ls\"\ndiff --git a/src/uu/ls/src/ls.rs b/src/uu/ls/src/ls.rs\nindex 994eabc21b6..9aaa0d0a4e3 100644\n--- a/src/uu/ls/src/ls.rs\n+++ b/src/uu/ls/src/ls.rs\n@@ -5,19 +5,9 @@\n \n // spell-checker:ignore (ToDO) somegroup nlink tabsize dired subdired dtype colorterm stringly\n \n-use clap::{\n-    builder::{NonEmptyStringValueParser, PossibleValue, ValueParser},\n-    crate_version, Arg, ArgAction, Command,\n-};\n-use glob::{MatchOptions, Pattern};\n-use lscolors::LsColors;\n-\n-use ansi_width::ansi_width;\n-use std::{cell::OnceCell, num::IntErrorKind};\n-use std::{collections::HashSet, io::IsTerminal};\n-\n #[cfg(windows)]\n use std::os::windows::fs::MetadataExt;\n+use std::{cell::OnceCell, num::IntErrorKind};\n use std::{\n     cmp::Reverse,\n     error::Error,\n@@ -34,7 +24,20 @@ use std::{\n     os::unix::fs::{FileTypeExt, MetadataExt},\n     time::Duration,\n };\n+use std::{collections::HashSet, io::IsTerminal};\n+\n+use ansi_width::ansi_width;\n+use chrono::{DateTime, Local, TimeDelta, TimeZone, Utc};\n+use chrono_tz::{OffsetName, Tz};\n+use clap::{\n+    builder::{NonEmptyStringValueParser, PossibleValue, ValueParser},\n+    crate_version, Arg, ArgAction, Command,\n+};\n+use glob::{MatchOptions, Pattern};\n+use iana_time_zone::get_timezone;\n+use lscolors::LsColors;\n use term_grid::{Direction, Filling, Grid, GridOptions};\n+\n use uucore::error::USimpleError;\n use uucore::format::human::{human_readable, SizeFormat};\n #[cfg(all(unix, not(any(target_os = \"android\", target_os = \"macos\"))))]\n@@ -67,10 +70,12 @@ use uucore::{\n     version_cmp::version_cmp,\n };\n use uucore::{help_about, help_section, help_usage, parse_glob, show, show_error, show_warning};\n+\n mod dired;\n use dired::{is_dired_arg_present, DiredOutput};\n mod colors;\n use colors::{color_name, StyleManager};\n+\n #[cfg(not(feature = \"selinux\"))]\n static CONTEXT_HELP_TEXT: &str = \"print any security context of each file (not enabled)\";\n #[cfg(feature = \"selinux\")]\n@@ -334,6 +339,58 @@ enum TimeStyle {\n     Format(String),\n }\n \n+/// Whether the given date is considered recent (i.e., in the last 6 months).\n+fn is_recent(time: DateTime<Local>) -> bool {\n+    // According to GNU a Gregorian year has 365.2425 * 24 * 60 * 60 == 31556952 seconds on the average.\n+    time + TimeDelta::try_seconds(31_556_952 / 2).unwrap() > Local::now()\n+}\n+\n+/// Get the alphabetic abbreviation of the current timezone.\n+///\n+/// For example, \"UTC\" or \"CET\" or \"PDT\".\n+fn timezone_abbrev() -> String {\n+    let tz = match std::env::var(\"TZ\") {\n+        // TODO Support other time zones...\n+        Ok(s) if s == \"UTC0\" || s.is_empty() => Tz::Etc__UTC,\n+        _ => match get_timezone() {\n+            Ok(tz_str) => tz_str.parse().unwrap(),\n+            Err(_) => Tz::Etc__UTC,\n+        },\n+    };\n+    let offset = tz.offset_from_utc_date(&Utc::now().date_naive());\n+    offset.abbreviation().unwrap_or(\"UTC\").to_string()\n+}\n+\n+/// Format the given time according to a custom format string.\n+fn custom_time_format(fmt: &str, time: DateTime<Local>) -> String {\n+    // TODO Refactor the common code from `ls` and `date` for rendering dates.\n+    // TODO - Revisit when chrono 0.5 is released. https://github.com/chronotope/chrono/issues/970\n+    // GNU `date` uses `%N` for nano seconds, however the `chrono` crate uses `%f`.\n+    let fmt = fmt.replace(\"%N\", \"%f\").replace(\"%Z\", &timezone_abbrev());\n+    time.format(&fmt).to_string()\n+}\n+\n+impl TimeStyle {\n+    /// Format the given time according to this time format style.\n+    fn format(&self, time: DateTime<Local>) -> String {\n+        let recent = is_recent(time);\n+        match (self, recent) {\n+            (Self::FullIso, _) => time.format(\"%Y-%m-%d %H:%M:%S.%f %z\").to_string(),\n+            (Self::LongIso, _) => time.format(\"%Y-%m-%d %H:%M\").to_string(),\n+            (Self::Iso, true) => time.format(\"%m-%d %H:%M\").to_string(),\n+            (Self::Iso, false) => time.format(\"%Y-%m-%d \").to_string(),\n+            // spell-checker:ignore (word) datetime\n+            //In this version of chrono translating can be done\n+            //The function is chrono::datetime::DateTime::format_localized\n+            //However it's currently still hard to get the current pure-rust-locale\n+            //So it's not yet implemented\n+            (Self::Locale, true) => time.format(\"%b %e %H:%M\").to_string(),\n+            (Self::Locale, false) => time.format(\"%b %e  %Y\").to_string(),\n+            (Self::Format(e), _) => custom_time_format(e, time),\n+        }\n+    }\n+}\n+\n fn parse_time_style(options: &clap::ArgMatches) -> Result<TimeStyle, LsError> {\n     let possible_time_styles = vec![\n         \"full-iso\".to_string(),\n@@ -3115,31 +3172,7 @@ fn get_time(md: &Metadata, config: &Config) -> Option<chrono::DateTime<chrono::L\n \n fn display_date(metadata: &Metadata, config: &Config) -> String {\n     match get_time(metadata, config) {\n-        Some(time) => {\n-            //Date is recent if from past 6 months\n-            //According to GNU a Gregorian year has 365.2425 * 24 * 60 * 60 == 31556952 seconds on the average.\n-            let recent = time + chrono::TimeDelta::try_seconds(31_556_952 / 2).unwrap()\n-                > chrono::Local::now();\n-\n-            match &config.time_style {\n-                TimeStyle::FullIso => time.format(\"%Y-%m-%d %H:%M:%S.%f %z\"),\n-                TimeStyle::LongIso => time.format(\"%Y-%m-%d %H:%M\"),\n-                TimeStyle::Iso => time.format(if recent { \"%m-%d %H:%M\" } else { \"%Y-%m-%d \" }),\n-                TimeStyle::Locale => {\n-                    let fmt = if recent { \"%b %e %H:%M\" } else { \"%b %e  %Y\" };\n-\n-                    // spell-checker:ignore (word) datetime\n-                    //In this version of chrono translating can be done\n-                    //The function is chrono::datetime::DateTime::format_localized\n-                    //However it's currently still hard to get the current pure-rust-locale\n-                    //So it's not yet implemented\n-\n-                    time.format(fmt)\n-                }\n-                TimeStyle::Format(e) => time.format(e),\n-            }\n-            .to_string()\n-        }\n+        Some(time) => config.time_style.format(time),\n         None => \"???\".into(),\n     }\n }\n", "test_patch": "diff --git a/tests/by-util/test_ls.rs b/tests/by-util/test_ls.rs\nindex 6ef7ac93a2e..715f18a1eaf 100644\n--- a/tests/by-util/test_ls.rs\n+++ b/tests/by-util/test_ls.rs\n@@ -5628,3 +5628,14 @@ fn test_non_unicode_names() {\n         .succeeds()\n         .stdout_is_bytes(b\"\\xC0.dir\\n\\xC0.file\\n\");\n }\n+\n+#[test]\n+fn test_time_style_timezone_name() {\n+    let re_custom_format = Regex::new(r\"[a-z-]* \\d* [\\w.]* [\\w.]* \\d* UTC f\\n\").unwrap();\n+    let (at, mut ucmd) = at_and_ucmd!();\n+    at.touch(\"f\");\n+    ucmd.env(\"TZ\", \"UTC0\")\n+        .args(&[\"-l\", \"--time-style=+%Z\"])\n+        .succeeds()\n+        .stdout_matches(&re_custom_format);\n+}\n", "problem_statement": "ls: --time-style with format string %Z gives timezone offset instead of name\nEnvironment: Ubuntu 20.04, uutils `main` branch (git commit 00d186606035876ad47afb82ad7d109a1868201b), GNU coreutils v8.30\n\nSteps to reproduce:\n```\nTZ=UTC0 touch -d '1970-07-08 09:10:11' f\nTZ=UTC0 ls -l --time-style=\"+%Z\" f\n```\n\nWhat happens now: uutils `ls` shows a numeric offset for the timezone\n```\n-rw-rw-r-- 1 jeffrey jeffrey 0 +00:00 f\n```\n\nWhat I expected to happen: GNU `ls` shows the timezone name\n```\n-rw-rw-r-- 1 jeffrey jeffrey 0 UTC f\n```\n\nNotes: This causes a failure in the GNU test file `tests/ls/time-style.sh`.\n\nWe use the `chrono` package for time formatting. It seems that they have an open issue about this here https://github.com/chronotope/chrono/issues/288\n", "hints_text": "Related to #5164", "created_at": "2025-01-18 14:38:19", "merge_commit_sha": "79f4b8976c2ceb9c401bf30ecbf029810d42e25d", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build/stable (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']", "['Build (windows-latest, aarch64-pc-windows-msvc, feat_os_windows, use-cross, true)', '.github/workflows/CICD.yml']"], ["['Separate Builds (macos-latest)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_env, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_seq, false)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_wc, false)', '.github/workflows/fuzzing.yml']"], ["['Style/format (ubuntu-latest, feat_os_unix)', '.github/workflows/code-quality.yml']", "['Style and Lint (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']"], ["['Tests (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']", "['Run the fuzzers (fuzz_split, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_echo, true)', '.github/workflows/fuzzing.yml']", "['Binary sizes (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Build/stable (windows-latest, feat_os_windows)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (macos-latest, aarch64-apple-darwin, feat_os_macos)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, i686-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Tests/Toybox test suite (ubuntu-latest)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, i686-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_parse_size, true)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_test, true)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_expr, true)', '.github/workflows/fuzzing.yml']"], ["['Update/format (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']", "['Dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_seq_parse_number, true)', '.github/workflows/fuzzing.yml']", "['Test all features separately (macos-latest)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_cksum, false)', '.github/workflows/fuzzing.yml']", "['Run GNU tests', '.github/workflows/GnuTests.yml']"], ["['Build the fuzzers', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_sort, false)', '.github/workflows/fuzzing.yml']"], ["['Update/dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']", "['Build (windows-latest, x86_64-pc-windows-msvc, feat_os_windows)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, arm-unknown-linux-gnueabihf, feat_os_unix_gnueabihf, use-cross, true)', '.github/workflows/CICD.yml']", "['Build (windows-latest, x86_64-pc-windows-gnu, feat_os_windows)', '.github/workflows/CICD.yml']"]]}
{"repo": "rustls/rustls", "instance_id": "rustls__rustls-2097", "base_commit": "cc403427b0dccdb46e6557d0c762e508112d705c", "patch": "diff --git a/rustls/src/msgs/persist.rs b/rustls/src/msgs/persist.rs\nindex 775123b7462..5a8998fb551 100644\n--- a/rustls/src/msgs/persist.rs\n+++ b/rustls/src/msgs/persist.rs\n@@ -117,6 +117,12 @@ impl Tls13ClientSessionValue {\n         self.common.epoch -= delta as u64;\n     }\n \n+    #[doc(hidden)]\n+    /// Test only: replace `max_early_data_size` with `new`\n+    pub fn _private_set_max_early_data_size(&mut self, new: u32) {\n+        self.max_early_data_size = new;\n+    }\n+\n     pub fn set_quic_params(&mut self, quic_params: &[u8]) {\n         self.quic_params = PayloadU16(quic_params.to_vec());\n     }\ndiff --git a/rustls/src/server/server_conn.rs b/rustls/src/server/server_conn.rs\nindex 515b312d6b6..027768d5948 100644\n--- a/rustls/src/server/server_conn.rs\n+++ b/rustls/src/server/server_conn.rs\n@@ -988,7 +988,10 @@ impl State<ServerConnectionData> for Accepting {\n \n pub(super) enum EarlyDataState {\n     New,\n-    Accepted(ChunkVecBuffer),\n+    Accepted {\n+        received: ChunkVecBuffer,\n+        left: usize,\n+    },\n     Rejected,\n }\n \n@@ -1004,12 +1007,15 @@ impl EarlyDataState {\n     }\n \n     pub(super) fn accept(&mut self, max_size: usize) {\n-        *self = Self::Accepted(ChunkVecBuffer::new(Some(max_size)));\n+        *self = Self::Accepted {\n+            received: ChunkVecBuffer::new(Some(max_size)),\n+            left: max_size,\n+        };\n     }\n \n     #[cfg(feature = \"std\")]\n     fn was_accepted(&self) -> bool {\n-        matches!(self, Self::Accepted(_))\n+        matches!(self, Self::Accepted { .. })\n     }\n \n     pub(super) fn was_rejected(&self) -> bool {\n@@ -1018,7 +1024,9 @@ impl EarlyDataState {\n \n     fn pop(&mut self) -> Option<Vec<u8>> {\n         match self {\n-            Self::Accepted(ref mut received) => received.pop(),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.pop(),\n             _ => None,\n         }\n     }\n@@ -1026,7 +1034,9 @@ impl EarlyDataState {\n     #[cfg(feature = \"std\")]\n     fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read(buf),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read(buf),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1034,7 +1044,9 @@ impl EarlyDataState {\n     #[cfg(read_buf)]\n     fn read_buf(&mut self, cursor: core::io::BorrowedCursor<'_>) -> io::Result<()> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read_buf(cursor),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read_buf(cursor),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1042,8 +1054,12 @@ impl EarlyDataState {\n     pub(super) fn take_received_plaintext(&mut self, bytes: Payload<'_>) -> bool {\n         let available = bytes.bytes().len();\n         match self {\n-            Self::Accepted(ref mut received) if received.apply_limit(available) == available => {\n+            Self::Accepted {\n+                ref mut received,\n+                ref mut left,\n+            } if received.apply_limit(available) == available && available <= *left => {\n                 received.append(bytes.into_vec());\n+                *left -= available;\n                 true\n             }\n             _ => false,\n@@ -1055,7 +1071,12 @@ impl Debug for EarlyDataState {\n     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {\n         match self {\n             Self::New => write!(f, \"EarlyDataState::New\"),\n-            Self::Accepted(buf) => write!(f, \"EarlyDataState::Accepted({})\", buf.len()),\n+            Self::Accepted { received, left } => write!(\n+                f,\n+                \"EarlyDataState::Accepted {{ received: {}, left: {} }}\",\n+                received.len(),\n+                left\n+            ),\n             Self::Rejected => write!(f, \"EarlyDataState::Rejected\"),\n         }\n     }\n", "test_patch": "diff --git a/rustls/tests/api.rs b/rustls/tests/api.rs\nindex d012226b009..7a439f2c06f 100644\n--- a/rustls/tests/api.rs\n+++ b/rustls/tests/api.rs\n@@ -3877,6 +3877,7 @@ enum ClientStorageOp {\n struct ClientStorage {\n     storage: Arc<dyn rustls::client::ClientSessionStore>,\n     ops: Mutex<Vec<ClientStorageOp>>,\n+    alter_max_early_data_size: Option<(u32, u32)>,\n }\n \n impl ClientStorage {\n@@ -3884,9 +3885,14 @@ impl ClientStorage {\n         Self {\n             storage: Arc::new(rustls::client::ClientSessionMemoryCache::new(1024)),\n             ops: Mutex::new(Vec::new()),\n+            alter_max_early_data_size: None,\n         }\n     }\n \n+    fn alter_max_early_data_size(&mut self, expected: u32, altered: u32) {\n+        self.alter_max_early_data_size = Some((expected, altered));\n+    }\n+\n     #[cfg(feature = \"tls12\")]\n     fn ops(&self) -> Vec<ClientStorageOp> {\n         self.ops.lock().unwrap().clone()\n@@ -3963,8 +3969,13 @@ impl rustls::client::ClientSessionStore for ClientStorage {\n     fn insert_tls13_ticket(\n         &self,\n         server_name: ServerName<'static>,\n-        value: rustls::client::Tls13ClientSessionValue,\n+        mut value: rustls::client::Tls13ClientSessionValue,\n     ) {\n+        if let Some((expected, desired)) = self.alter_max_early_data_size {\n+            assert_eq!(value.max_early_data_size(), expected);\n+            value._private_set_max_early_data_size(desired);\n+        }\n+\n         self.ops\n             .lock()\n             .unwrap()\n@@ -4215,6 +4226,159 @@ fn early_data_can_be_rejected_by_server() {\n     assert!(!client.is_early_data_accepted());\n }\n \n+#[test]\n+fn early_data_is_limited_on_client() {\n+    let (client_config, server_config) = early_data_configs();\n+\n+    // warm up\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    do_handshake(&mut client, &mut server);\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        1234\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 1234 + 1])\n+            .unwrap(),\n+        1234\n+    );\n+    do_handshake(&mut client, &mut server);\n+\n+    let mut received_early_data = [0u8; 1234];\n+    assert_eq!(\n+        server\n+            .early_data()\n+            .expect(\"early_data didn't happen\")\n+            .read(&mut received_early_data)\n+            .expect(\"early_data failed unexpectedly\"),\n+        1234\n+    );\n+    assert_eq!(&received_early_data[..], [0xaa; 1234]);\n+}\n+\n+fn early_data_configs_allowing_client_to_send_excess_data() -> (Arc<ClientConfig>, Arc<ServerConfig>)\n+{\n+    let (client_config, server_config) = early_data_configs();\n+\n+    // adjust client session storage to corrupt received max_early_data_size\n+    let mut client_config = Arc::into_inner(client_config).unwrap();\n+    let mut storage = ClientStorage::new();\n+    storage.alter_max_early_data_size(1234, 2024);\n+    client_config.resumption = Resumption::store(Arc::new(storage));\n+    let client_config = Arc::new(client_config);\n+\n+    // warm up\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    do_handshake(&mut client, &mut server);\n+    (client_config, server_config)\n+}\n+\n+#[test]\n+fn server_detects_excess_early_data() {\n+    let (client_config, server_config) = early_data_configs_allowing_client_to_send_excess_data();\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        2024\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 2024])\n+            .unwrap(),\n+        2024\n+    );\n+    assert_eq!(\n+        do_handshake_until_error(&mut client, &mut server),\n+        Err(ErrorFromPeer::Server(Error::PeerMisbehaved(\n+            PeerMisbehaved::TooMuchEarlyDataReceived\n+        ))),\n+    );\n+}\n+\n+// regression test for https://github.com/rustls/rustls/issues/2096\n+#[test]\n+fn server_detects_excess_streamed_early_data() {\n+    let (client_config, server_config) = early_data_configs_allowing_client_to_send_excess_data();\n+\n+    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\n+    assert!(client.early_data().is_some());\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .bytes_left(),\n+        2024\n+    );\n+    client\n+        .early_data()\n+        .unwrap()\n+        .flush()\n+        .unwrap();\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xaa; 1024])\n+            .unwrap(),\n+        1024\n+    );\n+    transfer(&mut client, &mut server);\n+    server.process_new_packets().unwrap();\n+\n+    let mut received_early_data = [0u8; 1024];\n+    assert_eq!(\n+        server\n+            .early_data()\n+            .expect(\"early_data didn't happen\")\n+            .read(&mut received_early_data)\n+            .expect(\"early_data failed unexpectedly\"),\n+        1024\n+    );\n+    assert_eq!(&received_early_data[..], [0xaa; 1024]);\n+\n+    assert_eq!(\n+        client\n+            .early_data()\n+            .unwrap()\n+            .write(&[0xbb; 1000])\n+            .unwrap(),\n+        1000\n+    );\n+    transfer(&mut client, &mut server);\n+    assert_eq!(\n+        server.process_new_packets(),\n+        Err(Error::PeerMisbehaved(\n+            PeerMisbehaved::TooMuchEarlyDataReceived\n+        ))\n+    );\n+}\n+\n mod test_quic {\n     use rustls::quic::{self, ConnectionCommon};\n \n", "problem_statement": "client can send more TLS1.3 early data than maximum if early data is read\nReported originally by @tahmid-23 :\r\n\r\n---\r\n\r\n### Summary\r\nReading early data on the server reduces the tracked received early data, which allows the client to send more early data than specified by `max_early_data_size`.\r\n\r\n### Details\r\n`ChunkVecBuffer` pops/shrinks chunks as it is read. In `EarlyDataState::take_received_plaintext`, it uses the total length of the `ChunkVecBuffer` to determine whether or not more early data can be accepted. However, if the server reads the early data, the `ChunkVecBuffer` will pop the data, thus reducing the perceived amount of early data that can be read.\r\n\r\n### PoC\r\nAdding this test to `rustls/tests/api.rs` demonstrates that this is possible.\r\nIn order for this to work, the client must behave maliciously. This can be achieved by making `rustls::client::client_conn::EarlyData::check_write_opt()` always return all requested bytes.\r\n```rs\r\n#[test]\r\nfn malicious_early_data_client() {\r\n    let (client_config, server_config) = early_data_configs();\r\n\r\n    // first connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    do_handshake(&mut client, &mut server);\r\n\r\n    // second connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    assert!(client.early_data().is_some());\r\n    assert_eq!(\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .bytes_left(),\r\n        1234\r\n    );\r\n\r\n    // send early data on connection\r\n    client\r\n        .early_data()\r\n        .unwrap()\r\n        .write(&[b'Z'; 1234]) // maximum 1234 bytes\r\n        .unwrap();\r\n\r\n    transfer(&mut client, &mut server);\r\n    server.process_new_packets().unwrap();\r\n\r\n    // server begins to respond, but...\r\n    transfer(&mut server, &mut client);\r\n    // ...don't process anything on the client so that it remains in early state\r\n\r\n    let mut received_early_data = [0u8; 1234];\r\n    assert_eq!(\r\n        server\r\n            .early_data()\r\n            .expect(\"early_data didn't happen\")\r\n            .read(&mut received_early_data)\r\n            .expect(\"early_data failed unexpectedly\"),\r\n        1234\r\n    );\r\n\r\n    // Send as much early data as you'd like!\r\n    // client sends early data, waits until server processes it, and then client sends more early data\r\n    // an actual implementation over IO would require special timing\r\n    for _ in 0..100 {\r\n        // just keep sending under max early data size limit each time\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .write(&[b'Z'; 1234])\r\n            .unwrap();\r\n        transfer(&mut client, &mut server);\r\n        // this should panic, too much early data was sent\r\n        server.process_new_packets().unwrap();\r\n\r\n        // server keeps responding...\r\n        transfer(&mut server, &mut client);\r\n        // client keeps itself in early state\r\n\r\n        let mut received_early_data = [0u8; 1234];\r\n        // fails here, all early data should be read at this point\r\n        assert_eq!(\r\n            server\r\n                .early_data()\r\n                .expect(\"early_data didn't happen\")\r\n                .read(&mut received_early_data)\r\n                .expect(\"early_data failed unexpectedly\"),\r\n            0\r\n        );\r\n    }\r\n}\r\n```\r\n\r\n### Impact\r\nAny server that attempts to read early data before waiting for the handshake to complete is vulnerable.\r\n\r\n---\r\n\r\nIn private discussion, we were sure that this was a bug, but unsure if this was a security bug, given RFC8446 doesn't actually require a server to limit anything to `max_early_data_size` (it's a \"SHOULD\"). However, we shall certainly fix this.\n", "hints_text": "", "created_at": "2024-08-28 15:07:36", "merge_commit_sha": "a2fedecc900a1887d1c8d2ad65ea340576782597", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check semver compatibility', '.github/workflows/build.yml']", "['Check minimum versions of direct dependencies', '.github/workflows/build.yml']"], ["['Validate external types appearing in public API', '.github/workflows/build.yml']", "['Features', '.github/workflows/build.yml']"], ["['MSRV', '.github/workflows/build.yml']", "['Build+test (nightly, ubuntu-latest)', '.github/workflows/build.yml']"], ["['BoGo test suite', '.github/workflows/build.yml']", "['Measure coverage', '.github/workflows/build.yml']"], ["['Build+test (stable, ubuntu-latest)', '.github/workflows/build.yml']", "['Fuzzing', '.github/workflows/cifuzz.yml']"], ["['Run openssl-tests', '.github/workflows/build.yml']", "['Check cross compilation targets', '.github/workflows/build.yml']"]]}
{"repo": "ynqa/jnv", "instance_id": "ynqa__jnv-33", "base_commit": "c3f53a3107dc602845a2ce151bf39a0713e91eb7", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 6c14d02..c15dcd2 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -67,9 +67,9 @@ checksum = \"0952808a6c2afd1aa8947271f3a60f1a6763c7b912d210184c5149b5cf147247\"\n \n [[package]]\n name = \"autocfg\"\n-version = \"1.1.0\"\n+version = \"1.2.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d468802bab17cbc0cc575e9b053f41e72aa36bfa6b7f55e3529ffa43161b97fa\"\n+checksum = \"f1fdabc7756949593fe60f30ec81974b613357de856987752631dea1e3394c80\"\n \n [[package]]\n name = \"autotools\"\n@@ -149,9 +149,9 @@ dependencies = [\n \n [[package]]\n name = \"clap\"\n-version = \"4.5.3\"\n+version = \"4.5.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"949626d00e063efc93b6dca932419ceb5432f99769911c0b995f7e884c778813\"\n+checksum = \"90bc066a67923782aa8515dbaea16946c5bcc5addbd668bb80af688e53e548a0\"\n dependencies = [\n  \"clap_builder\",\n  \"clap_derive\",\n@@ -171,9 +171,9 @@ dependencies = [\n \n [[package]]\n name = \"clap_derive\"\n-version = \"4.5.3\"\n+version = \"4.5.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"90239a040c80f5e14809ca132ddc4176ab33d5e17e49691793296e3fcb34d72f\"\n+checksum = \"528131438037fd55894f62d6e9f068b8f45ac57ffa77517819645d10aed04f64\"\n dependencies = [\n  \"heck\",\n  \"proc-macro2\",\n@@ -322,9 +322,9 @@ dependencies = [\n \n [[package]]\n name = \"itoa\"\n-version = \"1.0.10\"\n+version = \"1.0.11\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b1a46d1a171d865aa5f83f92695765caa047a9b4cbae2cbf37dbd613a793fd4c\"\n+checksum = \"49f1f14873335454500d59611f1cf4a4b0f786f9ac11f4312a78e4cf2566695b\"\n \n [[package]]\n name = \"j9\"\n@@ -350,7 +350,7 @@ dependencies = [\n \n [[package]]\n name = \"jnv\"\n-version = \"0.2.0\"\n+version = \"0.2.1\"\n dependencies = [\n  \"anyhow\",\n  \"clap\",\n@@ -503,9 +503,9 @@ dependencies = [\n \n [[package]]\n name = \"promkit\"\n-version = \"0.3.2\"\n+version = \"0.3.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"dfa31233c1a91cf9b5e8753a57b59a3bcca559f45f38da508857fb866635ab92\"\n+checksum = \"a5d06099a0a47b6bd7414d6692596b754a12ec4537fc46b72c7363a88fee66d9\"\n dependencies = [\n  \"anyhow\",\n  \"crossterm\",\n@@ -570,9 +570,9 @@ dependencies = [\n \n [[package]]\n name = \"regex-syntax\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c08c74e62047bb2de4ff487b251e4a92e24f48745648451635cec7d591162d9f\"\n+checksum = \"adad44e29e4c806119491a7f06f03de4d1af22c3a680dd47f1e6e179439d1f56\"\n \n [[package]]\n name = \"rustc-hash\"\n@@ -636,9 +636,9 @@ dependencies = [\n \n [[package]]\n name = \"serde_json\"\n-version = \"1.0.114\"\n+version = \"1.0.115\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c5f09b1bd632ef549eaa9f60a1f8de742bdbc698e6cee2095fc84dde5f549ae0\"\n+checksum = \"12dc5c46daa8e9fdf4f5e71b6cf9a53f2487da0e86e55808e2d35539666497dd\"\n dependencies = [\n  \"indexmap\",\n  \"itoa\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 4139a9e..2b39a57 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"jnv\"\n-version = \"0.2.0\"\n+version = \"0.2.1\"\n authors = [\"ynqa <un.pensiero.vano@gmail.com>\"]\n edition = \"2021\"\n description = \"JSON navigator and interactive filter leveraging jq\"\n@@ -10,10 +10,10 @@ readme = \"README.md\"\n \n [dependencies]\n anyhow = \"1.0.80\"\n-clap = { version = \"4.5.1\", features = [\"derive\"] }\n+clap = { version = \"4.5.4\", features = [\"derive\"] }\n gag = \"1.0.0\"\n j9 = \"0.1.3\"\n-promkit = \"0.3.2\"\n+promkit = \"0.3.3\"\n radix_trie = \"0.2.1\"\n \n # The profile that 'cargo dist' will build with\ndiff --git a/README.md b/README.md\nindex 143c6bb..1270956 100644\n--- a/README.md\n+++ b/README.md\n@@ -1,5 +1,7 @@\n # jnv\n \n+[![ci](https://github.com/ynqa/jnv/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/ynqa/jnv/actions/workflows/ci.yml)\n+\n *jnv* is designed for navigating JSON,\n offering an interactive JSON viewer and `jq` filter editor.\n \n@@ -87,6 +89,10 @@ jnv data.json\n | <kbd>Enter</kbd>     | Toggle expand/collapse in JSON viewer\n | <kbd>Ctrl + P</kbd>  | Expand all folds in JSON viewer\n | <kbd>Ctrl + N</kbd>  | Collapse all folds in JSON viewer\n+| <kbd>Alt + B</kbd>   | Move the cursor to the previous nearest character within set(`.`,`\\|`,`(`,`)`,`[`,`]`)\n+| <kbd>Alt + F</kbd>   | Move the cursor to the next nearest character within set(`.`,`\\|`,`(`,`)`,`[`,`]`)\n+| <kbd>Ctrl + W</kbd>  | Erase to the previous nearest character within set(`.`,`\\|`,`(`,`)`,`[`,`]`)\n+| <kbd>Alt + D</kbd>   | Erase to the next nearest character within set(`.`,`\\|`,`(`,`)`,`[`,`]`)\n \n ## Usage\n \ndiff --git a/src/jnv.rs b/src/jnv.rs\nindex 19a4497..86a3353 100644\n--- a/src/jnv.rs\n+++ b/src/jnv.rs\n@@ -1,4 +1,4 @@\n-use std::{cell::RefCell, rc::Rc};\n+use std::{cell::RefCell, collections::HashSet, rc::Rc};\n \n use anyhow::Result;\n use gag::Gag;\n@@ -83,6 +83,7 @@ impl Jnv {\n                 active_char_style: StyleBuilder::new().bgc(Color::Magenta).build(),\n                 inactive_char_style: StyleBuilder::new().build(),\n                 edit_mode,\n+                word_break_chars: HashSet::from(['.', '|', '(', ')', '[', ']']),\n                 lines: Default::default(),\n             },\n             hint_message_renderer: text::Renderer {\ndiff --git a/src/jnv/keymap.rs b/src/jnv/keymap.rs\nindex cc48bab..34d3f61 100644\n--- a/src/jnv/keymap.rs\n+++ b/src/jnv/keymap.rs\n@@ -69,6 +69,24 @@ pub fn default(event: &Event, renderer: &mut crate::jnv::render::Renderer) -> Re\n             state: KeyEventState::NONE,\n         }) => query_editor_after_mut.texteditor.move_to_tail(),\n \n+        Event::Key(KeyEvent {\n+            code: KeyCode::Char('b'),\n+            modifiers: KeyModifiers::ALT,\n+            kind: KeyEventKind::Press,\n+            state: KeyEventState::NONE,\n+        }) => query_editor_after_mut\n+            .texteditor\n+            .move_to_previous_nearest(&query_editor_after_mut.word_break_chars),\n+\n+        Event::Key(KeyEvent {\n+            code: KeyCode::Char('f'),\n+            modifiers: KeyModifiers::ALT,\n+            kind: KeyEventKind::Press,\n+            state: KeyEventState::NONE,\n+        }) => query_editor_after_mut\n+            .texteditor\n+            .move_to_next_nearest(&query_editor_after_mut.word_break_chars),\n+\n         // Erase char(s).\n         Event::Key(KeyEvent {\n             code: KeyCode::Backspace,\n@@ -83,6 +101,25 @@ pub fn default(event: &Event, renderer: &mut crate::jnv::render::Renderer) -> Re\n             state: KeyEventState::NONE,\n         }) => query_editor_after_mut.texteditor.erase_all(),\n \n+        // Erase to the nearest character.\n+        Event::Key(KeyEvent {\n+            code: KeyCode::Char('w'),\n+            modifiers: KeyModifiers::CONTROL,\n+            kind: KeyEventKind::Press,\n+            state: KeyEventState::NONE,\n+        }) => query_editor_after_mut\n+            .texteditor\n+            .erase_to_previous_nearest(&query_editor_after_mut.word_break_chars),\n+\n+        Event::Key(KeyEvent {\n+            code: KeyCode::Char('d'),\n+            modifiers: KeyModifiers::ALT,\n+            kind: KeyEventKind::Press,\n+            state: KeyEventState::NONE,\n+        }) => query_editor_after_mut\n+            .texteditor\n+            .erase_to_next_nearest(&query_editor_after_mut.word_break_chars),\n+\n         // Move up.\n         Event::Key(KeyEvent {\n             code: KeyCode::Up,\n", "test_patch": "", "problem_statement": "Native keyboard shortcuts for quickly navigating the query don't work\nOn macOS, I frequently use Option+Arrow Keys to jump through text inputs one \"word\" at a time. This works in the native shell as well as most text inputs. Would it be possible to support this in jnv's query input?\r\n\r\nThanks for making this! It's a neat tool that I've enjoyed with the minimal usage I've had so far, and I hope it's been a fun project to work on.\r\n\n", "hints_text": "@hu0p Thanks for your suggestions \ud83d\udcdd \r\n\r\nI'm currently investigating the cause, but I've discovered that pressing Option+Arrow is evaluated as Esc by crossterm (my environment is MacOS using Iterm2).\r\n\r\nOn the other hand, the combination of Option+{b, f} is recognized correctly, so I'm planning to provide equivalent functionality with shortcuts for Option+{b, f}.\nI also use iTerm 2. I forgot that I changed something in my configuration to make this work the same way. Sorry about that! Your approach sounds good to me. ", "created_at": "2024-03-27 10:08:51", "merge_commit_sha": "933da6237af7eced04de93bf61ad61bc5b9dbd9e", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['publish-homebrew-formula', '.github/workflows/release.yml']", "[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']"], ["['plan', '.github/workflows/release.yml']", "['test', '.github/workflows/ci.yml']"]]}
{"repo": "GuillaumeGomez/sysinfo", "instance_id": "GuillaumeGomez__sysinfo-1387", "base_commit": "5721f848f7e9b5d883e4ff7f88bd0b297d4ee05c", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex a60eb441c..2a9fc5935 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -103,7 +103,7 @@ ntapi = { version = \"0.4\", optional = true }\n windows = { version = \">=0.54, <=0.57\", optional = true }\n \n [target.'cfg(not(any(target_os = \"unknown\", target_arch = \"wasm32\")))'.dependencies]\n-libc = \"^0.2.165\"\n+libc = \"^0.2.164\"\n \n [target.'cfg(any(target_os = \"macos\", target_os = \"ios\"))'.dependencies]\n core-foundation-sys = \"0.8.7\"\n@@ -115,6 +115,7 @@ tempfile = \"3.9\"\n serde_json = \"1.0\" # Used in documentation tests.\n bstr = \"1.9.0\"\n tempfile = \"3.9\"\n+itertools = \"0.13.0\"\n \n [[example]]\n name = \"simple\"\ndiff --git a/src/common/disk.rs b/src/common/disk.rs\nindex 1fe004f11..90d2f307a 100644\n--- a/src/common/disk.rs\n+++ b/src/common/disk.rs\n@@ -4,6 +4,7 @@ use std::ffi::OsStr;\n use std::fmt;\n use std::path::Path;\n \n+use crate::common::impl_get_set::impl_get_set;\n use crate::DiskUsage;\n \n /// Struct containing a disk information.\n@@ -133,7 +134,9 @@ impl Disk {\n         self.inner.is_read_only()\n     }\n \n-    /// Updates the disk' information.\n+    /// Updates the disk' information with everything loaded.\n+    ///\n+    /// Equivalent to <code>[Disk::refresh_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n     ///\n     /// ```no_run\n     /// use sysinfo::Disks;\n@@ -144,7 +147,21 @@ impl Disk {\n     /// }\n     /// ```\n     pub fn refresh(&mut self) -> bool {\n-        self.inner.refresh()\n+        self.refresh_specifics(DiskRefreshKind::everything())\n+    }\n+\n+    /// Updates the disk's information corresponding to the given [`DiskRefreshKind`].\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n+    ///\n+    /// let mut disks = Disks::new_with_refreshed_list();\n+    /// for disk in disks.list_mut() {\n+    ///     disk.refresh_specifics(DiskRefreshKind::new());\n+    /// }\n+    /// ```\n+    pub fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) -> bool {\n+        self.inner.refresh_specifics(refreshes)\n     }\n \n     /// Returns number of bytes read and written by the disk\n@@ -244,7 +261,8 @@ impl Disks {\n     }\n \n     /// Creates a new [`Disks`][crate::Disks] type with the disk list loaded.\n-    /// It is a combination of [`Disks::new`] and [`Disks::refresh_list`].\n+    ///\n+    /// Equivalent to <code>[Disks::new_with_refreshed_list_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n     ///\n     /// ```no_run\n     /// use sysinfo::Disks;\n@@ -255,8 +273,24 @@ impl Disks {\n     /// }\n     /// ```\n     pub fn new_with_refreshed_list() -> Self {\n+        Self::new_with_refreshed_list_specifics(DiskRefreshKind::everything())\n+    }\n+\n+    /// Creates a new [`Disks`][crate::Disks] type with the disk list loaded\n+    /// and refreshed according to the given [`DiskRefreshKind`]. It is a combination of\n+    /// [`Disks::new`] and [`Disks::refresh_list_specifics`].\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n+    ///\n+    /// let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::new());\n+    /// for disk in disks.list() {\n+    ///     println!(\"{disk:?}\");\n+    /// }\n+    /// ```\n+    pub fn new_with_refreshed_list_specifics(refreshes: DiskRefreshKind) -> Self {\n         let mut disks = Self::new();\n-        disks.refresh_list();\n+        disks.refresh_list_specifics(refreshes);\n         disks\n     }\n \n@@ -291,6 +325,13 @@ impl Disks {\n \n     /// Refreshes the listed disks' information.\n     ///\n+    /// Equivalent to <code>[Disks::refresh_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n+    pub fn refresh(&mut self) {\n+        self.refresh_specifics(DiskRefreshKind::everything());\n+    }\n+\n+    /// Refreshes the listed disks' information according to the given [`DiskRefreshKind`].\n+    ///\n     /// \u26a0\ufe0f If a disk is added or removed, this method won't take it into account. Use\n     /// [`Disks::refresh_list`] instead.\n     ///\n@@ -304,30 +345,45 @@ impl Disks {\n     /// // We wait some time...?\n     /// disks.refresh();\n     /// ```\n-    pub fn refresh(&mut self) {\n-        self.inner.refresh();\n+    pub fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) {\n+        self.inner.refresh_specifics(refreshes);\n     }\n \n     /// The disk list will be emptied then completely recomputed.\n     ///\n+    /// Equivalent to <code>[Disks::refresh_list_specifics]\\([DiskRefreshKind::everything]\\())</code>.\n+    ///\n+    /// ```no_run\n+    /// use sysinfo::Disks;\n+    ///\n+    /// let mut disks = Disks::new();\n+    /// disks.refresh_list();\n+    /// ```\n+    pub fn refresh_list(&mut self) {\n+        self.refresh_list_specifics(DiskRefreshKind::everything());\n+    }\n+\n+    /// The disk list will be emptied then completely recomputed according to the given\n+    /// [`DiskRefreshKind`].\n+    ///\n     /// ## Linux\n     ///\n     /// \u26a0\ufe0f On Linux, the [NFS](https://en.wikipedia.org/wiki/Network_File_System) file\n     /// systems are ignored and the information of a mounted NFS **cannot** be obtained\n-    /// via [`Disks::refresh_list`]. This is due to the fact that I/O function\n-    /// `statvfs` used by [`Disks::refresh_list`] is blocking and\n+    /// via [`Disks::refresh_list_specifics`]. This is due to the fact that I/O function\n+    /// `statvfs` used by [`Disks::refresh_list_specifics`] is blocking and\n     /// [may hang](https://github.com/GuillaumeGomez/sysinfo/pull/876) in some cases,\n     /// requiring to call `systemctl stop` to terminate the NFS service from the remote\n     /// server in some cases.\n     ///\n     /// ```no_run\n-    /// use sysinfo::Disks;\n+    /// use sysinfo::{Disks, DiskRefreshKind};\n     ///\n     /// let mut disks = Disks::new();\n-    /// disks.refresh_list();\n+    /// disks.refresh_list_specifics(DiskRefreshKind::new());\n     /// ```\n-    pub fn refresh_list(&mut self) {\n-        self.inner.refresh_list();\n+    pub fn refresh_list_specifics(&mut self, refreshes: DiskRefreshKind) {\n+        self.inner.refresh_list_specifics(refreshes);\n     }\n }\n \n@@ -376,3 +432,61 @@ impl fmt::Display for DiskKind {\n         })\n     }\n }\n+\n+/// Used to determine what you want to refresh specifically on the [`Disk`] type.\n+///\n+/// ```no_run\n+/// use sysinfo::{Disks, DiskRefreshKind};\n+///\n+/// let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::everything());\n+///\n+/// for disk in disks.list() {\n+///     assert_eq!(disk.total_space(), 0);\n+/// }\n+/// ```\n+#[derive(Clone, Copy, Debug, Default)]\n+pub struct DiskRefreshKind {\n+    kind: bool,\n+    details: bool,\n+    io_usage: bool,\n+}\n+\n+impl DiskRefreshKind {\n+    /// Creates a new `DiskRefreshKind` with every refresh set to false.\n+    ///\n+    /// ```\n+    /// use sysinfo::DiskRefreshKind;\n+    ///\n+    /// let r = DiskRefreshKind::new();\n+    ///\n+    /// assert_eq!(r.kind(), false);\n+    /// assert_eq!(r.details(), false);\n+    /// assert_eq!(r.io_usage(), false);\n+    /// ```\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Creates a new `DiskRefreshKind` with every refresh set to true.\n+    ///\n+    /// ```\n+    /// use sysinfo::DiskRefreshKind;\n+    ///\n+    /// let r = DiskRefreshKind::everything();\n+    ///\n+    /// assert_eq!(r.kind(), true);\n+    /// assert_eq!(r.details(), true);\n+    /// assert_eq!(r.io_usage(), true);\n+    /// ```\n+    pub fn everything() -> Self {\n+        Self {\n+            kind: true,\n+            details: true,\n+            io_usage: true,\n+        }\n+    }\n+\n+    impl_get_set!(DiskRefreshKind, kind, with_kind, without_kind);\n+    impl_get_set!(DiskRefreshKind, details, with_details, without_details,);\n+    impl_get_set!(DiskRefreshKind, io_usage, with_io_usage, without_io_usage);\n+}\ndiff --git a/src/common/impl_get_set.rs b/src/common/impl_get_set.rs\nnew file mode 100644\nindex 000000000..2df3616db\n--- /dev/null\n+++ b/src/common/impl_get_set.rs\n@@ -0,0 +1,173 @@\n+// Take a look at the license at the top of the repository in the LICENSE file.\n+\n+macro_rules! impl_get_set {\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident $(, $extra_doc:literal)? $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n+        $(#[doc = concat!(\"\n+\", $extra_doc, \"\n+\")])?\n+        #[doc = concat!(\"\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::new();\n+\n+let r = r.with_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), false);\n+```\")]\n+        pub fn $name(&self) -> bool {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `true`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::new();\n+\n+let r = r.with_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self) -> Self {\n+            self.$name = true;\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `false`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), false);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = false;\n+            self\n+        }\n+    };\n+\n+    // To handle `UpdateKind`.\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident, UpdateKind $(, $extra_doc:literal)? $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n+        $(#[doc = concat!(\"\n+\", $extra_doc, \"\n+\")])?\n+        #[doc = concat!(\"\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+\n+let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+```\")]\n+        pub fn $name(&self) -> UpdateKind {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+\n+let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self, kind: UpdateKind) -> Self {\n+            self.$name = kind;\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `UpdateKind::Never`.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = UpdateKind::Never;\n+            self\n+        }\n+    };\n+\n+    // To handle `*RefreshKind`.\n+    ($ty_name:ident, $name:ident, $with:ident, $without:ident, $typ:ty $(,)?) => {\n+        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+\n+let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+```\")]\n+        pub fn $name(&self) -> Option<$typ> {\n+            self.$name\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `Some(...)`.\n+\n+```\n+use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n+\n+let r = \", stringify!($ty_name), \"::new();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+\n+let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+```\")]\n+        #[must_use]\n+        pub fn $with(mut self, kind: $typ) -> Self {\n+            self.$name = Some(kind);\n+            self\n+        }\n+\n+        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `None`.\n+\n+```\n+use sysinfo::\", stringify!($ty_name), \";\n+\n+let r = \", stringify!($ty_name), \"::everything();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n+\n+let r = r.without_\", stringify!($name), \"();\n+assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n+```\")]\n+        #[must_use]\n+        pub fn $without(mut self) -> Self {\n+            self.$name = None;\n+            self\n+        }\n+    };\n+}\n+\n+pub(crate) use impl_get_set;\ndiff --git a/src/common/mod.rs b/src/common/mod.rs\nindex cbbf52131..b59ba48f3 100644\n--- a/src/common/mod.rs\n+++ b/src/common/mod.rs\n@@ -4,6 +4,8 @@\n pub(crate) mod component;\n #[cfg(feature = \"disk\")]\n pub(crate) mod disk;\n+#[cfg(any(feature = \"system\", feature = \"disk\"))]\n+pub(crate) mod impl_get_set;\n #[cfg(feature = \"network\")]\n pub(crate) mod network;\n #[cfg(feature = \"system\")]\ndiff --git a/src/common/system.rs b/src/common/system.rs\nindex 71c71e5b1..73fd41078 100644\n--- a/src/common/system.rs\n+++ b/src/common/system.rs\n@@ -6,6 +6,7 @@ use std::fmt;\n use std::path::Path;\n use std::str::FromStr;\n \n+use crate::common::impl_get_set::impl_get_set;\n use crate::common::DiskUsage;\n use crate::{CpuInner, Gid, ProcessInner, SystemInner, Uid};\n \n@@ -1686,178 +1687,6 @@ cfg_if! {\n     }\n }\n \n-macro_rules! impl_get_set {\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident $(, $extra_doc:literal)? $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n-        $(#[doc = concat!(\"\n-\", $extra_doc, \"\n-\")])?\n-        #[doc = concat!(\"\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-\n-let r = r.with_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-```\")]\n-        pub fn $name(&self) -> bool {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `true`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-\n-let r = r.with_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self) -> Self {\n-            self.$name = true;\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `false`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), false);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = false;\n-            self\n-        }\n-    };\n-\n-    // To handle `UpdateKind`.\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident, UpdateKind $(, $extra_doc:literal)? $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\")]\n-        $(#[doc = concat!(\"\n-\", $extra_doc, \"\n-\")])?\n-        #[doc = concat!(\"\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-\n-let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-```\")]\n-        pub fn $name(&self) -> UpdateKind {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-\n-let r = r.with_\", stringify!($name), \"(UpdateKind::OnlyIfNotSet);\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self, kind: UpdateKind) -> Self {\n-            self.$name = kind;\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `UpdateKind::Never`.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", UpdateKind};\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::OnlyIfNotSet);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"(), UpdateKind::Never);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = UpdateKind::Never;\n-            self\n-        }\n-    };\n-\n-    // To handle `*RefreshKind`.\n-    ($ty_name:ident, $name:ident, $with:ident, $without:ident, $typ:ty $(,)?) => {\n-        #[doc = concat!(\"Returns the value of the \\\"\", stringify!($name), \"\\\" refresh kind.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-\n-let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-```\")]\n-        pub fn $name(&self) -> Option<$typ> {\n-            self.$name\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `Some(...)`.\n-\n-```\n-use sysinfo::{\", stringify!($ty_name), \", \", stringify!($typ), \"};\n-\n-let r = \", stringify!($ty_name), \"::new();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-\n-let r = r.with_\", stringify!($name), \"(\", stringify!($typ), \"::everything());\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-```\")]\n-        #[must_use]\n-        pub fn $with(mut self, kind: $typ) -> Self {\n-            self.$name = Some(kind);\n-            self\n-        }\n-\n-        #[doc = concat!(\"Sets the value of the \\\"\", stringify!($name), \"\\\" refresh kind to `None`.\n-\n-```\n-use sysinfo::\", stringify!($ty_name), \";\n-\n-let r = \", stringify!($ty_name), \"::everything();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), true);\n-\n-let r = r.without_\", stringify!($name), \"();\n-assert_eq!(r.\", stringify!($name), \"().is_some(), false);\n-```\")]\n-        #[must_use]\n-        pub fn $without(mut self) -> Self {\n-            self.$name = None;\n-            self\n-        }\n-    };\n-}\n-\n /// This enum allows you to specify when you want the related information to be updated.\n ///\n /// For example if you only want the [`Process::exe()`] information to be refreshed only if it's not\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 1a1e411c3..c04c3be1a 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -71,7 +71,7 @@ cfg_if! {\n #[cfg(feature = \"component\")]\n pub use crate::common::component::{Component, Components};\n #[cfg(feature = \"disk\")]\n-pub use crate::common::disk::{Disk, DiskKind, Disks};\n+pub use crate::common::disk::{Disk, DiskKind, DiskRefreshKind, Disks};\n #[cfg(feature = \"network\")]\n pub use crate::common::network::{IpNetwork, MacAddr, NetworkData, Networks};\n #[cfg(feature = \"system\")]\ndiff --git a/src/unix/apple/disk.rs b/src/unix/apple/disk.rs\nindex 2ac36ac17..e2f5e28cf 100644\n--- a/src/unix/apple/disk.rs\n+++ b/src/unix/apple/disk.rs\n@@ -7,7 +7,7 @@ use crate::{\n     },\n     DiskUsage,\n };\n-use crate::{Disk, DiskKind};\n+use crate::{Disk, DiskKind, DiskRefreshKind};\n \n use core_foundation_sys::array::CFArrayCreate;\n use core_foundation_sys::base::kCFAllocatorDefault;\n@@ -72,44 +72,70 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        #[cfg(target_os = \"macos\")]\n-        let Some((read_bytes, written_bytes)) = self\n-            .bsd_name\n-            .as_ref()\n-            .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n-        else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n-        #[cfg(not(target_os = \"macos\"))]\n-        let (read_bytes, written_bytes) = (0, 0);\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        if refresh_kind.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            let type_ = {\n+                #[cfg(target_os = \"macos\")]\n+                {\n+                    self.bsd_name\n+                        .as_ref()\n+                        .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n+                        .unwrap_or(DiskKind::Unknown(-1))\n+                }\n+                #[cfg(not(target_os = \"macos\"))]\n+                DiskKind::SSD\n+            };\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n+            self.type_ = type_;\n+        }\n \n-        unsafe {\n-            if let Some(requested_properties) = build_requested_properties(&[\n-                ffi::kCFURLVolumeAvailableCapacityKey,\n-                ffi::kCFURLVolumeAvailableCapacityForImportantUsageKey,\n-            ]) {\n-                match get_disk_properties(&self.volume_url, &requested_properties) {\n-                    Some(disk_props) => {\n-                        self.available_space = get_available_volume_space(&disk_props);\n-                        true\n-                    }\n-                    None => {\n-                        sysinfo_debug!(\"Failed to get disk properties\");\n-                        false\n+        if refresh_kind.io_usage() {\n+            #[cfg(target_os = \"macos\")]\n+            match self\n+                .bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n+            {\n+                Some((read_bytes, written_bytes)) => {\n+                    self.old_read_bytes = self.read_bytes;\n+                    self.old_written_bytes = self.written_bytes;\n+                    self.read_bytes = read_bytes;\n+                    self.written_bytes = written_bytes;\n+                }\n+                None => {\n+                    sysinfo_debug!(\"Failed to update disk i/o stats\");\n+                }\n+            }\n+        }\n+\n+        if refresh_kind.details() {\n+            unsafe {\n+                if let Some(requested_properties) = build_requested_properties(&[\n+                    ffi::kCFURLVolumeTotalCapacityKey,\n+                    ffi::kCFURLVolumeAvailableCapacityKey,\n+                    ffi::kCFURLVolumeAvailableCapacityForImportantUsageKey,\n+                ]) {\n+                    match get_disk_properties(&self.volume_url, &requested_properties) {\n+                        Some(disk_props) => {\n+                            self.total_space = get_int_value(\n+                                disk_props.inner(),\n+                                DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n+                            )\n+                            .unwrap_or_default()\n+                                as u64;\n+                            self.available_space = get_available_volume_space(&disk_props);\n+                        }\n+                        None => {\n+                            sysinfo_debug!(\"Failed to get disk properties\");\n+                        }\n                     }\n+                } else {\n+                    sysinfo_debug!(\"failed to create volume key list, skipping refresh\");\n                 }\n-            } else {\n-                sysinfo_debug!(\"failed to create volume key list, skipping refresh\");\n-                false\n             }\n         }\n+\n+        true\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -129,19 +155,19 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         unsafe {\n             // SAFETY: We don't keep any Objective-C objects around because we\n             // don't make any direct Objective-C calls in this code.\n             with_autorelease(|| {\n-                get_list(&mut self.disks);\n+                get_list(&mut self.disks, refresh_kind);\n             })\n         }\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         for disk in self.list_mut() {\n-            disk.refresh();\n+            disk.refresh_specifics(refresh_kind);\n         }\n     }\n \n@@ -154,7 +180,7 @@ impl crate::DisksInner {\n     }\n }\n \n-unsafe fn get_list(container: &mut Vec<Disk>) {\n+unsafe fn get_list(container: &mut Vec<Disk>, refresh_kind: DiskRefreshKind) {\n     container.clear();\n \n     let raw_disks = {\n@@ -252,7 +278,7 @@ unsafe fn get_list(container: &mut Vec<Disk>) {\n             CStr::from_ptr(c_disk.f_mntonname.as_ptr()).to_bytes(),\n         ));\n \n-        if let Some(disk) = new_disk(mount_point, volume_url, c_disk, &prop_dict) {\n+        if let Some(disk) = new_disk(mount_point, volume_url, c_disk, &prop_dict, refresh_kind) {\n             container.push(disk);\n         }\n     }\n@@ -398,6 +424,7 @@ unsafe fn new_disk(\n     volume_url: RetainedCFURL,\n     c_disk: libc::statfs,\n     disk_props: &RetainedCFDictionary,\n+    refresh_kind: DiskRefreshKind,\n ) -> Option<Disk> {\n     let bsd_name = get_bsd_name(&c_disk);\n \n@@ -406,21 +433,33 @@ unsafe fn new_disk(\n     // so we just assume the disk type is an SSD and set disk i/o stats to 0 until Rust has a way to conditionally link to\n     // IOKit in more recent deployment versions.\n \n-    #[cfg(target_os = \"macos\")]\n-    let type_ = bsd_name\n-        .as_ref()\n-        .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n-        .unwrap_or(DiskKind::Unknown(-1));\n-    #[cfg(not(target_os = \"macos\"))]\n-    let type_ = DiskKind::SSD;\n+    let type_ = if refresh_kind.kind() {\n+        #[cfg(target_os = \"macos\")]\n+        {\n+            bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_type(name))\n+                .unwrap_or(DiskKind::Unknown(-1))\n+        }\n+        #[cfg(not(target_os = \"macos\"))]\n+        DiskKind::SSD\n+    } else {\n+        DiskKind::Unknown(-1)\n+    };\n \n-    #[cfg(target_os = \"macos\")]\n-    let (read_bytes, written_bytes) = bsd_name\n-        .as_ref()\n-        .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n-        .unwrap_or_default();\n-    #[cfg(not(target_os = \"macos\"))]\n-    let (read_bytes, written_bytes) = (0, 0);\n+    let (read_bytes, written_bytes) = if refresh_kind.io_usage() {\n+        #[cfg(target_os = \"macos\")]\n+        {\n+            bsd_name\n+                .as_ref()\n+                .and_then(|name| crate::sys::inner::disk::get_disk_io(name))\n+                .unwrap_or_default()\n+        }\n+        #[cfg(not(target_os = \"macos\"))]\n+        (0, 0)\n+    } else {\n+        (0, 0)\n+    };\n \n     // Note: Since we requested these properties from the system, we don't expect\n     // these property retrievals to fail.\n@@ -431,7 +470,7 @@ unsafe fn new_disk(\n     )\n     .map(OsString::from)?;\n \n-    let is_removable = {\n+    let is_removable = if refresh_kind.details() {\n         let ejectable = get_bool_value(\n             disk_props.inner(),\n             DictKey::Extern(ffi::kCFURLVolumeIsEjectableKey),\n@@ -459,14 +498,25 @@ unsafe fn new_disk(\n \n             !internal\n         }\n+    } else {\n+        false\n     };\n \n-    let total_space = get_int_value(\n-        disk_props.inner(),\n-        DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n-    )? as u64;\n+    let total_space = if refresh_kind.details() {\n+        get_int_value(\n+            disk_props.inner(),\n+            DictKey::Extern(ffi::kCFURLVolumeTotalCapacityKey),\n+        )\n+        .unwrap_or_default() as u64\n+    } else {\n+        0\n+    };\n \n-    let available_space = get_available_volume_space(disk_props);\n+    let available_space = if refresh_kind.details() {\n+        get_available_volume_space(disk_props)\n+    } else {\n+        0\n+    };\n \n     let file_system = {\n         let len = c_disk\n@@ -482,7 +532,7 @@ unsafe fn new_disk(\n         )\n     };\n \n-    let is_read_only = (c_disk.f_flags & libc::MNT_RDONLY as u32) != 0;\n+    let is_read_only = refresh_kind.details() && (c_disk.f_flags & libc::MNT_RDONLY as u32) != 0;\n \n     Some(Disk {\n         inner: DiskInner {\ndiff --git a/src/unix/freebsd/disk.rs b/src/unix/freebsd/disk.rs\nindex dff7cafec..b273ce937 100644\n--- a/src/unix/freebsd/disk.rs\n+++ b/src/unix/freebsd/disk.rs\n@@ -16,7 +16,7 @@ use super::ffi::{\n     DEVSTAT_WRITE,\n };\n use super::utils::{c_buf_to_utf8_str, get_sys_value_str_by_name};\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n #[derive(Debug)]\n pub(crate) struct DiskInner {\n@@ -69,8 +69,8 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        refresh_disk(self)\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        refresh_disk(self, refresh_kind)\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -90,10 +90,8 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n-        unsafe {\n-            get_all_list(&mut self.disks, true);\n-        }\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n+        unsafe { get_all_list(&mut self.disks, true, refresh_kind) }\n     }\n \n     pub(crate) fn list(&self) -> &[Disk] {\n@@ -104,9 +102,9 @@ impl crate::DisksInner {\n         &mut self.disks\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         unsafe {\n-            get_all_list(&mut self.disks, false);\n+            get_all_list(&mut self.disks, false, refresh_kind);\n         }\n     }\n }\n@@ -164,19 +162,27 @@ impl GetValues for DiskInner {\n     }\n }\n \n-fn refresh_disk(disk: &mut DiskInner) -> bool {\n-    unsafe {\n-        let mut vfs: libc::statvfs = std::mem::zeroed();\n-        if libc::statvfs(disk.c_mount_point.as_ptr() as *const _, &mut vfs as *mut _) < 0 {\n-            return false;\n+fn refresh_disk(disk: &mut DiskInner, refresh_kind: DiskRefreshKind) -> bool {\n+    if refresh_kind.details() {\n+        unsafe {\n+            let mut vfs: libc::statvfs = std::mem::zeroed();\n+            if libc::statvfs(disk.c_mount_point.as_ptr() as *const _, &mut vfs as *mut _) < 0 {\n+                sysinfo_debug!(\"statvfs failed\");\n+            } else {\n+                let block_size: u64 = vfs.f_frsize as _;\n+                disk.total_space = vfs.f_blocks.saturating_mul(block_size);\n+                disk.available_space = vfs.f_favail.saturating_mul(block_size);\n+            }\n         }\n-        let block_size: u64 = vfs.f_frsize as _;\n+    }\n \n-        disk.total_space = vfs.f_blocks.saturating_mul(block_size);\n-        disk.available_space = vfs.f_favail.saturating_mul(block_size);\n-        refresh_disk_io(&mut [disk]);\n-        true\n+    if refresh_kind.io_usage() {\n+        unsafe {\n+            refresh_disk_io(&mut [disk]);\n+        }\n     }\n+\n+    true\n }\n \n unsafe fn initialize_geom() -> Result<(), ()> {\n@@ -291,7 +297,11 @@ fn get_disks_mapping() -> HashMap<String, String> {\n     disk_mapping\n }\n \n-pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n+pub unsafe fn get_all_list(\n+    container: &mut Vec<Disk>,\n+    add_new_disks: bool,\n+    refresh_kind: DiskRefreshKind,\n+) {\n     let mut fs_infos: *mut libc::statfs = null_mut();\n \n     let count = libc::getmntinfo(&mut fs_infos, libc::MNT_WAIT);\n@@ -349,15 +359,21 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             OsString::from(mount_point)\n         };\n \n-        if libc::statvfs(fs_info.f_mntonname.as_ptr(), &mut vfs) != 0 {\n-            continue;\n-        }\n-\n-        let f_frsize: u64 = vfs.f_frsize as _;\n-\n-        let is_read_only = (vfs.f_flag & libc::ST_RDONLY) != 0;\n-        let total_space = vfs.f_blocks.saturating_mul(f_frsize);\n-        let available_space = vfs.f_favail.saturating_mul(f_frsize);\n+        let (is_read_only, total_space, available_space) = if refresh_kind.details() {\n+            if libc::statvfs(fs_info.f_mntonname.as_ptr(), &mut vfs) != 0 {\n+                (false, 0, 0)\n+            } else {\n+                let f_frsize: u64 = vfs.f_frsize as _;\n+\n+                (\n+                    ((vfs.f_flag & libc::ST_RDONLY) != 0),\n+                    vfs.f_blocks.saturating_mul(f_frsize),\n+                    vfs.f_favail.saturating_mul(f_frsize),\n+                )\n+            }\n+        } else {\n+            (false, 0, 0)\n+        };\n \n         if let Some(disk) = container.iter_mut().find(|d| d.inner.name == name) {\n             disk.inner.updated = true;\n@@ -367,8 +383,12 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             let dev_mount_point = c_buf_to_utf8_str(&fs_info.f_mntfromname).unwrap_or(\"\");\n \n             // USB keys and CDs are removable.\n-            let is_removable = [b\"USB\", b\"usb\"].iter().any(|b| *b == &fs_type[..])\n-                || fs_type.starts_with(b\"/dev/cd\");\n+            let is_removable = if refresh_kind.details() {\n+                [b\"USB\", b\"usb\"].iter().any(|b| *b == &fs_type[..])\n+                    || fs_type.starts_with(b\"/dev/cd\")\n+            } else {\n+                false\n+            };\n \n             container.push(Disk {\n                 inner: DiskInner {\n@@ -376,8 +396,8 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n                     c_mount_point: fs_info.f_mntonname.to_vec(),\n                     mount_point: PathBuf::from(mount_point),\n                     dev_id: disk_mapping.get(dev_mount_point).map(ToString::to_string),\n-                    total_space: vfs.f_blocks.saturating_mul(f_frsize),\n-                    available_space: vfs.f_favail.saturating_mul(f_frsize),\n+                    total_space,\n+                    available_space,\n                     file_system: OsString::from_vec(fs_type),\n                     is_removable,\n                     is_read_only,\n@@ -404,7 +424,9 @@ pub unsafe fn get_all_list(container: &mut Vec<Disk>, add_new_disks: bool) {\n             c.inner.updated = false;\n         }\n     }\n-    refresh_disk_io(container.as_mut_slice());\n+    if refresh_kind.io_usage() {\n+        refresh_disk_io(container.as_mut_slice());\n+    }\n }\n \n // struct DevInfoWrapper {\ndiff --git a/src/unix/linux/disk.rs b/src/unix/linux/disk.rs\nindex 8d1d41fe9..f07b56a96 100644\n--- a/src/unix/linux/disk.rs\n+++ b/src/unix/linux/disk.rs\n@@ -1,7 +1,7 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n use crate::sys::utils::{get_all_utf8_data, to_cpath};\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use libc::statvfs;\n use std::collections::HashMap;\n@@ -37,7 +37,7 @@ macro_rules! cast {\n pub(crate) struct DiskInner {\n     type_: DiskKind,\n     device_name: OsString,\n-    actual_device_name: String,\n+    actual_device_name: Option<String>,\n     file_system: OsString,\n     mount_point: PathBuf,\n     total_space: u64,\n@@ -83,39 +83,52 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        self.efficient_refresh(&disk_stats())\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) -> bool {\n+        self.efficient_refresh(refresh_kind, &disk_stats(&refresh_kind))\n     }\n \n-    fn efficient_refresh(&mut self, procfs_disk_stats: &HashMap<String, DiskStat>) -> bool {\n-        let Some((read_bytes, written_bytes)) =\n-            procfs_disk_stats.get(&self.actual_device_name).map(|stat| {\n-                (\n-                    stat.sectors_read * SECTOR_SIZE,\n-                    stat.sectors_written * SECTOR_SIZE,\n-                )\n-            })\n-        else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n+    fn efficient_refresh(\n+        &mut self,\n+        refresh_kind: DiskRefreshKind,\n+        procfs_disk_stats: &HashMap<String, DiskStat>,\n+    ) -> bool {\n+        if refresh_kind.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            self.type_ = find_type_for_device_name(&self.device_name);\n+        }\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n-\n-        unsafe {\n-            let mut stat: statvfs = mem::zeroed();\n-            let mount_point_cpath = to_cpath(&self.mount_point);\n-            if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n-                let tmp = cast!(stat.f_bsize).saturating_mul(cast!(stat.f_bavail));\n-                self.available_space = cast!(tmp);\n-                true\n+        if refresh_kind.io_usage() {\n+            if self.actual_device_name.is_none() {\n+                self.actual_device_name = Some(get_actual_device_name(&self.device_name));\n+            }\n+            if let Some((read_bytes, written_bytes)) = procfs_disk_stats\n+                .get(self.actual_device_name.as_ref().unwrap())\n+                .map(|stat| {\n+                    (\n+                        stat.sectors_read * SECTOR_SIZE,\n+                        stat.sectors_written * SECTOR_SIZE,\n+                    )\n+                })\n+            {\n+                self.old_read_bytes = self.read_bytes;\n+                self.old_written_bytes = self.written_bytes;\n+                self.read_bytes = read_bytes;\n+                self.written_bytes = written_bytes;\n             } else {\n-                false\n+                sysinfo_debug!(\"Failed to update disk i/o stats\");\n+            }\n+        }\n+\n+        if refresh_kind.details() {\n+            if let Some((total_space, available_space, is_read_only)) =\n+                unsafe { load_statvfs_values(&self.mount_point) }\n+            {\n+                self.total_space = total_space;\n+                self.available_space = available_space;\n+                self.is_read_only = is_read_only;\n             }\n         }\n+\n+        true\n     }\n \n     pub(crate) fn usage(&self) -> DiskUsage {\n@@ -135,17 +148,19 @@ impl crate::DisksInner {\n         }\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n         get_all_list(\n             &mut self.disks,\n             &get_all_utf8_data(\"/proc/mounts\", 16_385).unwrap_or_default(),\n+            refresh_kind,\n         )\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n-        let procfs_disk_stats = disk_stats();\n+    pub(crate) fn refresh_specifics(&mut self, refresh_kind: DiskRefreshKind) {\n+        let procfs_disk_stats = disk_stats(&refresh_kind);\n         for disk in self.list_mut() {\n-            disk.inner.efficient_refresh(&procfs_disk_stats);\n+            disk.inner\n+                .efficient_refresh(refresh_kind, &procfs_disk_stats);\n         }\n     }\n \n@@ -177,38 +192,53 @@ fn get_actual_device_name(device: &OsStr) -> String {\n         .unwrap_or_default()\n }\n \n+unsafe fn load_statvfs_values(mount_point: &Path) -> Option<(u64, u64, bool)> {\n+    let mount_point_cpath = to_cpath(mount_point);\n+    let mut stat: statvfs = mem::zeroed();\n+    if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n+        let bsize = cast!(stat.f_bsize);\n+        let blocks = cast!(stat.f_blocks);\n+        let bavail = cast!(stat.f_bavail);\n+        let total = bsize.saturating_mul(blocks);\n+        if total == 0 {\n+            return None;\n+        }\n+        let available = bsize.saturating_mul(bavail);\n+        let is_read_only = (stat.f_flag & libc::ST_RDONLY) != 0;\n+\n+        Some((total, available, is_read_only))\n+    } else {\n+        None\n+    }\n+}\n+\n fn new_disk(\n     device_name: &OsStr,\n     mount_point: &Path,\n     file_system: &OsStr,\n     removable_entries: &[PathBuf],\n     procfs_disk_stats: &HashMap<String, DiskStat>,\n-) -> Option<Disk> {\n-    let mount_point_cpath = to_cpath(mount_point);\n-    let type_ = find_type_for_device_name(device_name);\n-    let mut total = 0;\n-    let mut available = 0;\n-    let mut is_read_only = false;\n-    unsafe {\n-        let mut stat: statvfs = mem::zeroed();\n-        if retry_eintr!(statvfs(mount_point_cpath.as_ptr() as *const _, &mut stat)) == 0 {\n-            let bsize = cast!(stat.f_bsize);\n-            let blocks = cast!(stat.f_blocks);\n-            let bavail = cast!(stat.f_bavail);\n-            total = bsize.saturating_mul(blocks);\n-            available = bsize.saturating_mul(bavail);\n-            is_read_only = (stat.f_flag & libc::ST_RDONLY) != 0;\n-        }\n-        if total == 0 {\n-            return None;\n-        }\n-        let mount_point = mount_point.to_owned();\n-        let is_removable = removable_entries\n+    refresh_kind: DiskRefreshKind,\n+) -> Disk {\n+    let type_ = if refresh_kind.kind() {\n+        find_type_for_device_name(device_name)\n+    } else {\n+        DiskKind::Unknown(-1)\n+    };\n+\n+    let (total_space, available_space, is_read_only) = if refresh_kind.details() {\n+        unsafe { load_statvfs_values(mount_point).unwrap_or((0, 0, false)) }\n+    } else {\n+        (0, 0, false)\n+    };\n+\n+    let is_removable = refresh_kind.details()\n+        && removable_entries\n             .iter()\n             .any(|e| e.as_os_str() == device_name);\n \n+    let (actual_device_name, read_bytes, written_bytes) = if refresh_kind.io_usage() {\n         let actual_device_name = get_actual_device_name(device_name);\n-\n         let (read_bytes, written_bytes) = procfs_disk_stats\n             .get(&actual_device_name)\n             .map(|stat| {\n@@ -218,24 +248,27 @@ fn new_disk(\n                 )\n             })\n             .unwrap_or_default();\n+        (Some(actual_device_name), read_bytes, written_bytes)\n+    } else {\n+        (None, 0, 0)\n+    };\n \n-        Some(Disk {\n-            inner: DiskInner {\n-                type_,\n-                device_name: device_name.to_owned(),\n-                actual_device_name,\n-                file_system: file_system.to_owned(),\n-                mount_point,\n-                total_space: cast!(total),\n-                available_space: cast!(available),\n-                is_removable,\n-                is_read_only,\n-                old_read_bytes: 0,\n-                old_written_bytes: 0,\n-                read_bytes,\n-                written_bytes,\n-            },\n-        })\n+    Disk {\n+        inner: DiskInner {\n+            type_,\n+            device_name: device_name.to_owned(),\n+            actual_device_name,\n+            file_system: file_system.to_owned(),\n+            mount_point: mount_point.to_owned(),\n+            total_space,\n+            available_space,\n+            is_removable,\n+            is_read_only,\n+            old_read_bytes: 0,\n+            old_written_bytes: 0,\n+            read_bytes,\n+            written_bytes,\n+        },\n     }\n }\n \n@@ -310,7 +343,7 @@ fn find_type_for_device_name(device_name: &OsStr) -> DiskKind {\n     }\n }\n \n-fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n+fn get_all_list(container: &mut Vec<Disk>, content: &str, refresh_kind: DiskRefreshKind) {\n     container.clear();\n     // The goal of this array is to list all removable devices (the ones whose name starts with\n     // \"usb-\").\n@@ -331,7 +364,7 @@ fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n         _ => Vec::new(),\n     };\n \n-    let procfs_disk_stats = disk_stats();\n+    let procfs_disk_stats = disk_stats(&refresh_kind);\n \n     for disk in content\n         .lines()\n@@ -378,13 +411,14 @@ fn get_all_list(container: &mut Vec<Disk>, content: &str) {\n                (fs_file.starts_with(\"/run\") && !fs_file.starts_with(\"/run/media\")) ||\n                fs_spec.starts_with(\"sunrpc\"))\n         })\n-        .filter_map(|(fs_spec, fs_file, fs_vfstype)| {\n+        .map(|(fs_spec, fs_file, fs_vfstype)| {\n             new_disk(\n                 fs_spec.as_ref(),\n                 Path::new(&fs_file),\n                 fs_vfstype.as_ref(),\n                 &removable_entries,\n                 &procfs_disk_stats,\n+                refresh_kind,\n             )\n         })\n     {\n@@ -444,14 +478,18 @@ impl DiskStat {\n     }\n }\n \n-fn disk_stats() -> HashMap<String, DiskStat> {\n-    let path = \"/proc/diskstats\";\n-    match fs::read_to_string(path) {\n-        Ok(content) => disk_stats_inner(&content),\n-        Err(_error) => {\n-            sysinfo_debug!(\"failed to read {path:?}: {_error:?}\");\n-            HashMap::new()\n+fn disk_stats(refresh_kind: &DiskRefreshKind) -> HashMap<String, DiskStat> {\n+    if refresh_kind.io_usage() {\n+        let path = \"/proc/diskstats\";\n+        match fs::read_to_string(path) {\n+            Ok(content) => disk_stats_inner(&content),\n+            Err(_error) => {\n+                sysinfo_debug!(\"failed to read {path:?}: {_error:?}\");\n+                HashMap::new()\n+            }\n         }\n+    } else {\n+        Default::default()\n     }\n }\n \ndiff --git a/src/unknown/disk.rs b/src/unknown/disk.rs\nindex 7ab253b47..7a4b97d54 100644\n--- a/src/unknown/disk.rs\n+++ b/src/unknown/disk.rs\n@@ -1,6 +1,6 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use std::{ffi::OsStr, path::Path};\n \n@@ -39,7 +39,7 @@ impl DiskInner {\n         false\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n+    pub(crate) fn refresh_specifics(&mut self, _refreshes: DiskRefreshKind) -> bool {\n         true\n     }\n \n@@ -65,11 +65,11 @@ impl DisksInner {\n         self.disks\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, _refreshes: DiskRefreshKind) {\n         // Does nothing.\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, _refreshes: DiskRefreshKind) {\n         // Does nothing.\n     }\n \ndiff --git a/src/windows/disk.rs b/src/windows/disk.rs\nindex 5d635c396..f9efc6dfb 100644\n--- a/src/windows/disk.rs\n+++ b/src/windows/disk.rs\n@@ -1,7 +1,7 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n use crate::sys::utils::HandleWrapper;\n-use crate::{Disk, DiskKind, DiskUsage};\n+use crate::{Disk, DiskKind, DiskRefreshKind, DiskUsage};\n \n use std::ffi::{c_void, OsStr, OsString};\n use std::mem::size_of;\n@@ -167,25 +167,28 @@ impl DiskInner {\n         self.is_read_only\n     }\n \n-    pub(crate) fn refresh(&mut self) -> bool {\n-        let Some((read_bytes, written_bytes)) = get_disk_io(&self.device_path, None) else {\n-            sysinfo_debug!(\"Failed to update disk i/o stats\");\n-            return false;\n-        };\n+    pub(crate) fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) -> bool {\n+        if refreshes.kind() && self.type_ == DiskKind::Unknown(-1) {\n+            self.type_ = get_disk_kind(&self.device_path, None);\n+        }\n \n-        self.old_read_bytes = self.read_bytes;\n-        self.old_written_bytes = self.written_bytes;\n-        self.read_bytes = read_bytes;\n-        self.written_bytes = written_bytes;\n-\n-        if self.total_space != 0 {\n-            unsafe {\n-                let mut tmp = 0;\n-                let lpdirectoryname = PCWSTR::from_raw(self.mount_point.as_ptr());\n-                if GetDiskFreeSpaceExW(lpdirectoryname, None, None, Some(&mut tmp)).is_ok() {\n-                    self.available_space = tmp;\n-                    return true;\n-                }\n+        if refreshes.io_usage() {\n+            if let Some((read_bytes, written_bytes)) = get_disk_io(&self.device_path, None) {\n+                self.old_read_bytes = self.read_bytes;\n+                self.old_written_bytes = self.written_bytes;\n+                self.read_bytes = read_bytes;\n+                self.written_bytes = written_bytes;\n+            } else {\n+                sysinfo_debug!(\"Failed to update disk i/o stats\");\n+            }\n+        }\n+\n+        if refreshes.details() {\n+            if let Some((total_space, available_space)) =\n+                unsafe { get_drive_size(&self.mount_point) }\n+            {\n+                self.total_space = total_space;\n+                self.available_space = available_space;\n             }\n         }\n         false\n@@ -220,15 +223,15 @@ impl DisksInner {\n         self.disks\n     }\n \n-    pub(crate) fn refresh_list(&mut self) {\n+    pub(crate) fn refresh_list_specifics(&mut self, refreshes: DiskRefreshKind) {\n         unsafe {\n-            self.disks = get_list();\n+            self.disks = get_list(refreshes);\n         }\n     }\n \n-    pub(crate) fn refresh(&mut self) {\n+    pub(crate) fn refresh_specifics(&mut self, refreshes: DiskRefreshKind) {\n         for disk in self.list_mut() {\n-            disk.refresh();\n+            disk.refresh_specifics(refreshes);\n         }\n     }\n \n@@ -259,7 +262,7 @@ unsafe fn get_drive_size(mount_point: &[u16]) -> Option<(u64, u64)> {\n     }\n }\n \n-pub(crate) unsafe fn get_list() -> Vec<Disk> {\n+pub(crate) unsafe fn get_list(refreshes: DiskRefreshKind) -> Vec<Disk> {\n     #[cfg(feature = \"multithread\")]\n     use rayon::iter::ParallelIterator;\n \n@@ -268,7 +271,7 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n             let raw_volume_name = PCWSTR::from_raw(volume_name.as_ptr());\n             let drive_type = GetDriveTypeW(raw_volume_name);\n \n-            let is_removable = drive_type == DRIVE_REMOVABLE;\n+            let is_removable = refreshes.details() && drive_type == DRIVE_REMOVABLE;\n \n             if drive_type != DRIVE_FIXED && drive_type != DRIVE_REMOVABLE {\n                 return Vec::new();\n@@ -292,7 +295,7 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n                 );\n                 return Vec::new();\n             }\n-            let is_read_only = (flags & FILE_READ_ONLY_VOLUME) != 0;\n+            let is_read_only = refreshes.details() && (flags & FILE_READ_ONLY_VOLUME) != 0;\n \n             let mount_paths = get_volume_path_names_for_volume_name(&volume_name[..]);\n             if mount_paths.is_empty() {\n@@ -305,51 +308,25 @@ pub(crate) unsafe fn get_list() -> Vec<Disk> {\n                 .copied()\n                 .chain([0])\n                 .collect::<Vec<_>>();\n-            let Some(handle) = HandleWrapper::new_from_file(&device_path[..], Default::default())\n-            else {\n-                return Vec::new();\n-            };\n-            let Some((total_space, available_space)) = get_drive_size(&mount_paths[0][..]) else {\n-                return Vec::new();\n-            };\n-            if total_space == 0 {\n-                sysinfo_debug!(\"total_space == 0\");\n-                return Vec::new();\n-            }\n-            let spq_trim = STORAGE_PROPERTY_QUERY {\n-                PropertyId: StorageDeviceSeekPenaltyProperty,\n-                QueryType: PropertyStandardQuery,\n-                AdditionalParameters: [0],\n+            let handle = HandleWrapper::new_from_file(&device_path[..], Default::default());\n+\n+            let (total_space, available_space) = if refreshes.details() {\n+                get_drive_size(&mount_paths[0][..]).unwrap_or_default()\n+            } else {\n+                (0, 0)\n             };\n-            let mut result: DEVICE_SEEK_PENALTY_DESCRIPTOR = std::mem::zeroed();\n-\n-            let mut dw_size = 0;\n-            let device_io_control = DeviceIoControl(\n-                handle.0,\n-                IOCTL_STORAGE_QUERY_PROPERTY,\n-                Some(&spq_trim as *const STORAGE_PROPERTY_QUERY as *const c_void),\n-                size_of::<STORAGE_PROPERTY_QUERY>() as u32,\n-                Some(&mut result as *mut DEVICE_SEEK_PENALTY_DESCRIPTOR as *mut c_void),\n-                size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32,\n-                Some(&mut dw_size),\n-                None,\n-            )\n-            .is_ok();\n-            let type_ = if !device_io_control\n-                || dw_size != size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32\n-            {\n-                DiskKind::Unknown(-1)\n+\n+            let type_ = if refreshes.kind() {\n+                get_disk_kind(&device_path, handle.as_ref())\n             } else {\n-                let is_hdd = result.IncursSeekPenalty.as_bool();\n-                if is_hdd {\n-                    DiskKind::HDD\n-                } else {\n-                    DiskKind::SSD\n-                }\n+                DiskKind::Unknown(-1)\n             };\n \n-            let (read_bytes, written_bytes) =\n-                get_disk_io(&device_path, Some(handle)).unwrap_or_default();\n+            let (read_bytes, written_bytes) = if refreshes.io_usage() {\n+                get_disk_io(&device_path, handle).unwrap_or_default()\n+            } else {\n+                (0, 0)\n+            };\n \n             let name = os_string_from_zero_terminated(&name);\n             let file_system = os_string_from_zero_terminated(&file_system);\n@@ -383,6 +360,63 @@ fn os_string_from_zero_terminated(name: &[u16]) -> OsString {\n     OsString::from_wide(&name[..len])\n }\n \n+fn get_disk_kind(device_path: &[u16], borrowed_handle: Option<&HandleWrapper>) -> DiskKind {\n+    let binding = (\n+        borrowed_handle,\n+        if borrowed_handle.is_none() {\n+            unsafe { HandleWrapper::new_from_file(device_path, Default::default()) }\n+        } else {\n+            None\n+        },\n+    );\n+    let handle = match binding {\n+        (Some(handle), _) => handle,\n+        (_, Some(ref handle)) => handle,\n+        (None, None) => return DiskKind::Unknown(-1),\n+    };\n+\n+    if handle.is_invalid() {\n+        sysinfo_debug!(\n+            \"Expected handle to {:?} to be valid\",\n+            String::from_utf16_lossy(device_path)\n+        );\n+        return DiskKind::Unknown(-1);\n+    }\n+\n+    let spq_trim = STORAGE_PROPERTY_QUERY {\n+        PropertyId: StorageDeviceSeekPenaltyProperty,\n+        QueryType: PropertyStandardQuery,\n+        AdditionalParameters: [0],\n+    };\n+    let mut result: DEVICE_SEEK_PENALTY_DESCRIPTOR = unsafe { std::mem::zeroed() };\n+\n+    let mut dw_size = 0;\n+    let device_io_control = unsafe {\n+        DeviceIoControl(\n+            handle.0,\n+            IOCTL_STORAGE_QUERY_PROPERTY,\n+            Some(&spq_trim as *const STORAGE_PROPERTY_QUERY as *const c_void),\n+            size_of::<STORAGE_PROPERTY_QUERY>() as u32,\n+            Some(&mut result as *mut DEVICE_SEEK_PENALTY_DESCRIPTOR as *mut c_void),\n+            size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32,\n+            Some(&mut dw_size),\n+            None,\n+        )\n+        .is_ok()\n+    };\n+\n+    if !device_io_control || dw_size != size_of::<DEVICE_SEEK_PENALTY_DESCRIPTOR>() as u32 {\n+        DiskKind::Unknown(-1)\n+    } else {\n+        let is_hdd = result.IncursSeekPenalty.as_bool();\n+        if is_hdd {\n+            DiskKind::HDD\n+        } else {\n+            DiskKind::SSD\n+        }\n+    }\n+}\n+\n /// Returns a tuple consisting of the total number of bytes read and written by the volume with the specified device path\n fn get_disk_io(device_path: &[u16], handle: Option<HandleWrapper>) -> Option<(u64, u64)> {\n     let handle =\n@@ -390,7 +424,7 @@ fn get_disk_io(device_path: &[u16], handle: Option<HandleWrapper>) -> Option<(u6\n \n     if handle.is_invalid() {\n         sysinfo_debug!(\n-            \"Expected handle to '{:?}' to be valid\",\n+            \"Expected handle to {:?} to be valid\",\n             String::from_utf16_lossy(device_path)\n         );\n         return None;\n", "test_patch": "diff --git a/tests/disk.rs b/tests/disk.rs\nindex 4210fcac7..2fbab2ba3 100644\n--- a/tests/disk.rs\n+++ b/tests/disk.rs\n@@ -1,34 +1,158 @@\n // Take a look at the license at the top of the repository in the LICENSE file.\n \n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n+fn should_skip() -> bool {\n+    if !sysinfo::IS_SUPPORTED_SYSTEM {\n+        return true;\n+    }\n+\n+    let s = sysinfo::System::new_all();\n+\n+    // If we don't have any physical core present, it's very likely that we're inside a VM...\n+    s.physical_core_count().unwrap_or_default() == 0\n+}\n+\n #[test]\n #[cfg(all(feature = \"system\", feature = \"disk\"))]\n fn test_disks() {\n-    if sysinfo::IS_SUPPORTED_SYSTEM {\n-        let s = sysinfo::System::new_all();\n-        // If we don't have any physical core present, it's very likely that we're inside a VM...\n-        if s.physical_core_count().unwrap_or_default() > 0 {\n-            let mut disks = sysinfo::Disks::new();\n-            assert!(disks.list().is_empty());\n-            disks.refresh_list();\n-            assert!(!disks.list().is_empty());\n+    if should_skip() {\n+        return;\n+    }\n+\n+    let mut disks = sysinfo::Disks::new();\n+    assert!(disks.list().is_empty());\n+    disks.refresh_list();\n+    assert!(!disks.list().is_empty());\n+}\n+\n+#[test]\n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n+fn test_disk_refresh_kind() {\n+    use itertools::Itertools;\n+\n+    use sysinfo::{DiskKind, DiskRefreshKind, Disks};\n+\n+    if should_skip() {\n+        return;\n+    }\n+\n+    for fs in [\n+        DiskRefreshKind::with_kind,\n+        DiskRefreshKind::without_kind,\n+        DiskRefreshKind::with_details,\n+        DiskRefreshKind::without_details,\n+        DiskRefreshKind::with_io_usage,\n+        DiskRefreshKind::without_io_usage,\n+    ]\n+    .iter()\n+    .powerset()\n+    {\n+        let mut refreshes = DiskRefreshKind::new();\n+        for f in fs {\n+            refreshes = f(refreshes);\n         }\n+\n+        let assertions = |name: &'static str, disks: &Disks| {\n+            if refreshes.kind() {\n+                // This would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                #[cfg(not(any(target_os = \"freebsd\", target_os = \"windows\")))]\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.kind() != DiskKind::Unknown(-1)),\n+                    \"{name}: disk.kind should be refreshed\"\n+                );\n+            } else {\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.kind() == DiskKind::Unknown(-1)),\n+                    \"{name}: disk.kind should not be refreshed\"\n+                );\n+            }\n+\n+            if refreshes.details() {\n+                // These would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.available_space() != Default::default()),\n+                    \"{name}: disk.available_space should be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .any(|disk| disk.total_space() != Default::default()),\n+                    \"{name}: disk.total_space should be refreshed\"\n+                );\n+                // We can't assert anything about booleans, since false is indistinguishable from\n+                // not-refreshed\n+            } else {\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.available_space() == Default::default()),\n+                    \"{name}: disk.available_space should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.total_space() == Default::default()),\n+                    \"{name}: disk.total_space should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.is_read_only() == <bool as Default>::default()),\n+                    \"{name}: disk.is_read_only should not be refreshed\"\n+                );\n+                assert!(\n+                    disks\n+                        .iter()\n+                        .all(|disk| disk.is_removable() == <bool as Default>::default()),\n+                    \"{name}: disk.is_removable should not be refreshed\"\n+                );\n+            }\n+\n+            if refreshes.io_usage() {\n+                // This would ideally assert that *all* are refreshed, but we settle for a weaker\n+                // assertion because failures can't be distinguished from \"not refreshed\" values.\n+                assert!(\n+                    disks.iter().any(|disk| disk.usage() != Default::default()),\n+                    \"{name}: disk.usage should be refreshed\"\n+                );\n+            } else {\n+                assert!(\n+                    disks.iter().all(|disk| disk.usage() == Default::default()),\n+                    \"{name}: disk.usage should not be refreshed\"\n+                );\n+            }\n+        };\n+\n+        // load and refresh with the desired details should work\n+        let disks = Disks::new_with_refreshed_list_specifics(refreshes);\n+        assertions(\"full\", &disks);\n+\n+        // load with minimal `DiskRefreshKind`, then refresh for added detail should also work!\n+        let mut disks = Disks::new_with_refreshed_list_specifics(DiskRefreshKind::new());\n+        disks.refresh_specifics(refreshes);\n+        assertions(\"incremental\", &disks);\n     }\n }\n \n #[test]\n-#[cfg(feature = \"disk\")]\n+#[cfg(all(feature = \"system\", feature = \"disk\"))]\n fn test_disks_usage() {\n     use std::fs::{remove_file, File};\n     use std::io::Write;\n     use std::path::{Path, PathBuf};\n     use std::thread::sleep;\n \n-    use sysinfo::{CpuRefreshKind, Disks, RefreshKind, System};\n-\n-    let s = System::new_with_specifics(RefreshKind::new().with_cpu(CpuRefreshKind::new()));\n+    use sysinfo::Disks;\n \n-    // Skip the tests on unsupported platforms and on systems with no physical cores (likely a VM)\n-    if !sysinfo::IS_SUPPORTED_SYSTEM || s.physical_core_count().unwrap_or_default() == 0 {\n+    if should_skip() {\n         return;\n     }\n \n", "problem_statement": "Selectively refreshing disks\nWhen running `Disks::new_with_refreshed_list()` on android devices, I often see warnings (from SELinux?) along the lines of:\r\n\r\n```\r\ntype=1400 audit(0.0:339351): avc:  denied  { search } for  name=\"block\" dev=\"tmpfs\" ino=12\r\n```\r\n\r\nI've done some spelunking, and it appears they are a side-effect of [this `canonicalize` call](https://github.com/GuillaumeGomez/sysinfo/blob/master/src/unix/linux/disk.rs#L157), on certain disks' mount points.\r\n\r\n(N.b. the dev=\"tmpfs\" in the warning doesn't refer to the type of the filesystem being refreshed. I'm not using `linux-tmpfs`, so they're filtered out. In debugging, I've seen the warning for \"ext4\" and \"fuse\" filesystems.)\r\n\r\nI know these warnings are harmless! ...but I've had customers who were alarmed about them, so I'd prefer to avoid generating them. Given my use-case (I'm only reporting disk usage over time for *one* disk), I'd love some sort of mechanism for *selectively* refreshing `Disks`. There are lots of options.\r\n\r\nMy first suggestion would be a separate `Disks` constructor (`new_with_list()` or `new_with_unrefreshed_list()`?) that loads the *list* of disks from /proc/mounts, but defers the *stat collection* until a later `refresh`. On linux, at least, I think the separation makes sense, and it gives complete control over *what is refreshed* to the library user, without adding much complexity to the interface. On other platforms, I haven't looked, but it seems like the worst-case would simply be that `new_with_list()` calls `new_with_refreshed_list()` on platforms where there's no distinction between listing and refreshing.\r\n\r\nI'd be happy to do the work. If my proposal sounds reasonable, I'll submit a PR next week and we can hash out the specifics. If not, I'd be open to other ways of solving this... just let me know :)\n", "hints_text": "Seems a bit weird to want to have disks but not their path or even their kind. I think the best course would be to instead detect when the `canonicalize` call is actually needed and only call it if so.\n> Seems a bit weird to want to have disks but not their path or even their kind.\r\n\r\nI was thinking those *would* be present in the `new_with_list` return. Only the information obtained from `statvfs` would be deferred.\r\n\r\n> I think the best course would be to instead detect when the canonicalize call is actually needed and only call it if so.\r\n\r\nHummm. Canonicalize (on linux) IIUC is just `realpath`. How would one decide if it were necessary? (I think you'd end up reimplementing almost all of `realpath`, right?)\nThe kind is retrieved from the function using `canonicalize`, so both functions would get a call to it.\r\n\r\n> Hummm. Canonicalize (on linux) IIUC is just `realpath`. How would one decide if it were necessary? (I think you'd end up reimplementing almost all of `realpath`, right?)\r\n\r\nMostly checking if it's a link, if not, checking if it doesn't start with `/` or contains `..` (still not great to check that ourselves).\r\n\r\nAny clue why `statvfs` is emitting this warning?\n> The kind is retrieved from the function using canonicalize, so both functions would get a call to it.\r\n\r\nAh, I see. I was thinking about `vfstype`, not `DiskKind`. You're right that `DiskKind` requires the `canonicalize`.\r\n\r\nSo in my suggestion, I guess the populated fields would be `device_name`, `file_system` and `mount_point`; the other fields would require a `refresh`.\r\n\r\n> Any clue why statvfs is emitting this warning?\r\n\r\nI believe it happens during the `canonicalize`, not the `statvfs`. And it's coming from SELinux or apparmor or some similar system for permissions above and beyond POSIX. The fact that it emits these audit warnings *in addition to failing the syscall* is super annoying, at least in this instance :/\nI think `DiskKind` is part of the fundamental information for a `Disk`, so I'm not super open to the idea of not having it by default.\r\n\r\nHowever the `RefreshKind` approach for `Disks` sounds pretty good, just not sure there are enough different fields to make it relevant currently.\nI'm not sure what you mean about `RefreshKind`, can you elaborate?\nSure. Like for [`refresh_processes_specifics`](https://docs.rs/sysinfo/latest/sysinfo/struct.System.html#method.refresh_processes_specifics): you can pick what you want to refresh with `ProcessRefreshKind`.\nAhh, I see. That's perfect! I'd be happy to extend that pattern over `Disks`. So I think I would:\r\n * add a `DiskRefreshKind`\r\n * add a `Disks::new_with_specifics()` constructor\r\n * add a `Disks::refresh_specifics()` refresher\r\n * add a `Disk::refresh_specifics()` refresher\r\n * make (at least) the linux impl respect the specific refresh kind(s)\r\n\r\n... sound about right? That would work for me :)\nYes but that wouldn't solve your problem since I still want `DiskKind` to be retrieved for all `Disk` unconditionally. ^^'\nWhat if you always get `DiskKind` unless you specifically ask *not* to get it?\nThat could work. Only question, what value do you put into `DiskKind::Unknown`? It's supposed to mean something to the OS.\nI'm not sure. I don't use the `DiskKind`, personally. The idiomatic Rust thing would be to make it `Option<DiskKind>`, but I'm guessing that's exactly what you want to avoid. Could we add another enum variant (`DiskKind::NeedsReload`)?\nIndeed, I don't want an `Option`. I'm still wondering if we're not trying to go around a technical issue that could be fixed instead...\nFWIW: even if it weren't for the annoying warnings, I would still use this functionality, since I'm currently loading way more information than I actually need :)\nWe can do it in two passes then: a PR to implement the selective refresh and another where we can debate what's best for `DiskKind`.\nLook for a PR early next week!\nThanks in advance!", "created_at": "2024-11-22 21:12:44", "merge_commit_sha": "08dee51f02d246e9b5ea40f31721c1f396d8d016", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check stable / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Check nightly / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check stable / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-apple-ios', '.github/workflows/CI.yml']", "['Check nightly / i686-linux-android', '.github/workflows/CI.yml']"], ["['Check stable / armv7-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['Test ubuntu-latest (rust stable)', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']", "['Check nightly / arm-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Test ubuntu-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / aarch64-unknown-linux-musl', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust stable)', '.github/workflows/CI.yml']", "['Check 1.74.0 / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']", "['Check 1.74.0 / armv7-linux-androideabi', '.github/workflows/CI.yml']"], ["['c_interface', '.github/workflows/CI.yml']", "['Check 1.74.0 / x86_64-apple-ios', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check nightly / x86_64-linux-android', '.github/workflows/CI.yml']"], ["['clippy (macos-latest)', '.github/workflows/CI.yml']", "['clippy (ubuntu-latest)', '.github/workflows/CI.yml']"], ["['Check nightly / arm-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['unknown-targets (1.74.0)', '.github/workflows/CI.yml']"], ["['clippy (windows-latest)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check nightly / i686-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Test windows-latest (rust nightly)', '.github/workflows/CI.yml']"], ["['rustfmt', '.github/workflows/CI.yml']", "['Check stable / x86_64-apple-darwin', '.github/workflows/CI.yml']"]]}
{"repo": "GuillaumeGomez/sysinfo", "instance_id": "GuillaumeGomez__sysinfo-1324", "base_commit": "296bbf02359b11eef28fc344832760b654215a22", "patch": "diff --git a/src/common/system.rs b/src/common/system.rs\nindex 219d9881d..5fd0d0548 100644\n--- a/src/common/system.rs\n+++ b/src/common/system.rs\n@@ -367,6 +367,9 @@ impl System {\n     /// exist (it will **NOT** be removed from the processes if it doesn't exist anymore). If it\n     /// isn't listed yet, it'll be added.\n     ///\n+    /// \u26a0\ufe0f If you need to refresh multiple processes at once, use [`refresh_pids`] instead! It has\n+    /// much better performance.\n+    ///\n     /// It is the same as calling:\n     ///\n     /// ```no_run\n@@ -394,6 +397,8 @@ impl System {\n     /// let mut s = System::new_all();\n     /// s.refresh_process(Pid::from(1337));\n     /// ```\n+    ///\n+    /// [`refresh_pids`]: #method.refresh_pids\n     pub fn refresh_process(&mut self, pid: Pid) -> bool {\n         self.refresh_process_specifics(\n             pid,\n@@ -409,6 +414,9 @@ impl System {\n     /// exist (it will **NOT** be removed from the processes if it doesn't exist anymore). If it\n     /// isn't listed yet, it'll be added.\n     ///\n+    /// \u26a0\ufe0f If you need to refresh multiple processes at once, use [`refresh_pids_specifics`]\n+    /// instead! It has much better performance.\n+    ///\n     /// \u26a0\ufe0f On Linux, `sysinfo` keeps the `stat` files open by default. You can change this behaviour\n     /// by using [`set_open_files_limit`][crate::set_open_files_limit].\n     ///\n@@ -418,6 +426,8 @@ impl System {\n     /// let mut s = System::new_all();\n     /// s.refresh_process_specifics(Pid::from(1337), ProcessRefreshKind::new());\n     /// ```\n+    ///\n+    /// [`refresh_pids_specifics`]: #method.refresh_pids_specifics\n     pub fn refresh_process_specifics(\n         &mut self,\n         pid: Pid,\ndiff --git a/src/unix/apple/macos/system.rs b/src/unix/apple/macos/system.rs\nindex d0191eb2e..6c182ae1f 100644\n--- a/src/unix/apple/macos/system.rs\n+++ b/src/unix/apple/macos/system.rs\n@@ -8,6 +8,7 @@ use libc::{\n     processor_cpu_load_info_t, sysconf, vm_page_size, PROCESSOR_CPU_LOAD_INFO, _SC_CLK_TCK,\n };\n use std::ptr::null_mut;\n+use std::time::Instant;\n \n struct ProcessorCpuLoadInfo {\n     cpu_load: processor_cpu_load_info_t,\n@@ -55,6 +56,8 @@ pub(crate) struct SystemTimeInfo {\n     timebase_to_ns: f64,\n     clock_per_sec: f64,\n     old_cpu_info: ProcessorCpuLoadInfo,\n+    last_update: Option<Instant>,\n+    prev_time_interval: f64,\n }\n \n unsafe impl Send for SystemTimeInfo {}\n@@ -97,40 +100,52 @@ impl SystemTimeInfo {\n                 timebase_to_ns: info.numer as f64 / info.denom as f64,\n                 clock_per_sec: nano_per_seconds / clock_ticks_per_sec as f64,\n                 old_cpu_info,\n+                last_update: None,\n+                prev_time_interval: 0.,\n             })\n         }\n     }\n \n     pub fn get_time_interval(&mut self, port: mach_port_t) -> f64 {\n-        let mut total = 0;\n-        let new_cpu_info = match ProcessorCpuLoadInfo::new(port) {\n-            Some(cpu_info) => cpu_info,\n-            None => return 0.,\n-        };\n-        let cpu_count = std::cmp::min(self.old_cpu_info.cpu_count, new_cpu_info.cpu_count);\n-        unsafe {\n-            for i in 0..cpu_count {\n-                let new_load: &processor_cpu_load_info = &*new_cpu_info.cpu_load.offset(i as _);\n-                let old_load: &processor_cpu_load_info =\n-                    &*self.old_cpu_info.cpu_load.offset(i as _);\n-                for (new, old) in new_load.cpu_ticks.iter().zip(old_load.cpu_ticks.iter()) {\n-                    if new > old {\n-                        total += new.saturating_sub(*old);\n+        let need_cpu_usage_update = self\n+            .last_update\n+            .map(|last_update| last_update.elapsed() > crate::MINIMUM_CPU_UPDATE_INTERVAL)\n+            .unwrap_or(true);\n+        if need_cpu_usage_update {\n+            let mut total = 0;\n+            let new_cpu_info = match ProcessorCpuLoadInfo::new(port) {\n+                Some(cpu_info) => cpu_info,\n+                None => return 0.,\n+            };\n+            let cpu_count = std::cmp::min(self.old_cpu_info.cpu_count, new_cpu_info.cpu_count);\n+            unsafe {\n+                for i in 0..cpu_count {\n+                    let new_load: &processor_cpu_load_info = &*new_cpu_info.cpu_load.offset(i as _);\n+                    let old_load: &processor_cpu_load_info =\n+                        &*self.old_cpu_info.cpu_load.offset(i as _);\n+                    for (new, old) in new_load.cpu_ticks.iter().zip(old_load.cpu_ticks.iter()) {\n+                        if new > old {\n+                            total += new.saturating_sub(*old);\n+                        }\n                     }\n                 }\n             }\n \n             self.old_cpu_info = new_cpu_info;\n+            self.last_update = Some(Instant::now());\n \n             // Now we convert the ticks to nanoseconds (if the interval is less than\n             // `MINIMUM_CPU_UPDATE_INTERVAL`, we replace it with it instead):\n             let base_interval = total as f64 / cpu_count as f64 * self.clock_per_sec;\n             let smallest = crate::MINIMUM_CPU_UPDATE_INTERVAL.as_secs_f64() * 1_000_000_000.0;\n-            if base_interval < smallest {\n+            self.prev_time_interval = if base_interval < smallest {\n                 smallest\n             } else {\n                 base_interval / self.timebase_to_ns\n-            }\n+            };\n+            self.prev_time_interval\n+        } else {\n+            self.prev_time_interval\n         }\n     }\n }\n", "test_patch": "diff --git a/tests/process.rs b/tests/process.rs\nindex f6f841a6f..e0963de56 100644\n--- a/tests/process.rs\n+++ b/tests/process.rs\n@@ -813,3 +813,47 @@ fn test_parent_change() {\n     // We kill the child to clean up.\n     child.kill();\n }\n+\n+// We want to ensure that if `System::refresh_process*` methods are called\n+// one after the other, it won't impact the CPU usage computation badly.\n+#[test]\n+fn test_multiple_single_process_refresh() {\n+    if !sysinfo::IS_SUPPORTED_SYSTEM || cfg!(feature = \"apple-sandbox\") || cfg!(windows) {\n+        // Windows never updates its parent PID so no need to check anything.\n+        return;\n+    }\n+\n+    let file_name = \"target/test_binary3\";\n+    build_test_binary(file_name);\n+    let mut p_a = std::process::Command::new(format!(\"./{file_name}\"))\n+        .arg(\"1\")\n+        .spawn()\n+        .unwrap();\n+    let mut p_b = std::process::Command::new(format!(\"./{file_name}\"))\n+        .arg(\"1\")\n+        .spawn()\n+        .unwrap();\n+\n+    let pid_a = Pid::from_u32(p_a.id() as _);\n+    let pid_b = Pid::from_u32(p_b.id() as _);\n+\n+    let mut s = System::new();\n+    let process_refresh_kind = ProcessRefreshKind::new().with_cpu();\n+    s.refresh_process_specifics(pid_a, process_refresh_kind);\n+    s.refresh_process_specifics(pid_b, process_refresh_kind);\n+\n+    std::thread::sleep(std::time::Duration::from_secs(1));\n+    s.refresh_process_specifics(pid_a, process_refresh_kind);\n+    s.refresh_process_specifics(pid_b, process_refresh_kind);\n+\n+    let cpu_a = s.process(pid_a).unwrap().cpu_usage();\n+    let cpu_b = s.process(pid_b).unwrap().cpu_usage();\n+\n+    p_a.kill().expect(\"failed to kill process a\");\n+    p_b.kill().expect(\"failed to kill process b\");\n+\n+    let _ = p_a.wait();\n+    let _ = p_b.wait();\n+\n+    assert!(cpu_b - 5. < cpu_a && cpu_b + 5. > cpu_a);\n+}\n", "problem_statement": "Refreshing multiple processes on macos leads to invalid cpu usage\nI have a piece of code that looks like this:\r\n\r\n```rust\r\nlet mut sys = sysinfo::System::new();\r\nlet process_refresh_kind = ProcessRefreshKind::everything().without_disk_usage().without_environ();\r\nsys.refresh_process_specifics(pid_a, process_refresh_kind);\r\nsys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\nloop {\r\n    // Wait 1 minute\r\n    std::thread::sleep(std::time::Duration::from_secs(60));\r\n\r\n    // Refresh CPU measurement\r\n    sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n    sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n    // Get CPU usage of the two processes\r\n    let cpu_a = sys.process(pid_a).map(|p| p.cpu_usage());\r\n    let cpu_b = sys.process(pid_b).map(|p| p.cpu_usage());\r\n    println!(\"cpu a: {:?}, cpu b: {:?}\", cpu_a, cpu_b);\r\n}\r\n```\r\n\r\nWith this code, the cpu for process A seems to be valid, but the one for process B is way too high, it can even get higher than the max value.\r\n\r\nLooking at the code, I see that the function `compute_cpu_usage` uses a `time_interval` variable. This value seems to be right on the first call to refresh (for pid A), but not on the second call (for pid B). I suppose this value is reset after each call to \"refresh_process\", leading to the second call computing a CPU load with a time interval that is way too small.\r\n\r\nHere is a reproducer:\r\n\r\n```rust\r\nfn main() {\r\n    let mut sys = sysinfo::System::new();\r\n\r\n    let pid_a = sysinfo::Pid::from_u32(std::process::id());\r\n    let pid_b = sysinfo::Pid::from_u32(1);\r\n\r\n    let process_refresh_kind = sysinfo::ProcessRefreshKind::everything()\r\n        .without_disk_usage()\r\n        .without_environ();\r\n    sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n    sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n    loop {\r\n        std::thread::sleep(std::time::Duration::from_secs(10));\r\n\r\n        sys.refresh_process_specifics(pid_a, process_refresh_kind);\r\n        sys.refresh_process_specifics(pid_b, process_refresh_kind);\r\n\r\n        let cpu_a = sys.process(pid_a).map(|p| p.cpu_usage());\r\n        let cpu_b = sys.process(pid_b).map(|p| p.cpu_usage());\r\n        println!(\"cpu a: {:?}, cpu b: {:?}\", cpu_a, cpu_b);\r\n    }\r\n}\r\n```\r\n\r\nReversing the two PIDs reverses the one that is reported with a very high CPU usage, showing the bug.\r\n\r\nThis was reproduced on 0.30.12 on macos x64.\n", "hints_text": "For this kind of operations, you likely want to use [refresh_pids_specifics](https://docs.rs/sysinfo/latest/sysinfo/struct.System.html#method.refresh_pids_specifics). Although, in this case it should still work.\nIndeed, I didn't know about this API, and it fixes the issue. I'm leaving this issue open since there is still a bug afaict, but i'm no longer blocked by it, I let you decide what to do with it. Thanks!\nIt definitely needs to be fixed. Gonna try to take a look in the next days.", "created_at": "2024-07-24 20:46:11", "merge_commit_sha": "222ed97ab06e898767e8194658a9f6d0bd755ac6", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check stable / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Check nightly / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check stable / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-apple-ios', '.github/workflows/CI.yml']", "['Check nightly / i686-linux-android', '.github/workflows/CI.yml']"], ["['Check stable / armv7-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['Test ubuntu-latest (rust stable)', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']", "['Check nightly / arm-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Test ubuntu-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / aarch64-unknown-linux-musl', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust stable)', '.github/workflows/CI.yml']", "['Check 1.74.0 / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']", "['Check 1.74.0 / armv7-linux-androideabi', '.github/workflows/CI.yml']"], ["['c_interface', '.github/workflows/CI.yml']", "['Check 1.74.0 / x86_64-apple-ios', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check nightly / x86_64-linux-android', '.github/workflows/CI.yml']"], ["['clippy (macos-latest)', '.github/workflows/CI.yml']", "['clippy (ubuntu-latest)', '.github/workflows/CI.yml']"], ["['Check nightly / arm-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['unknown-targets (1.74.0)', '.github/workflows/CI.yml']"], ["['clippy (windows-latest)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check nightly / i686-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Test windows-latest (rust nightly)', '.github/workflows/CI.yml']"], ["['rustfmt', '.github/workflows/CI.yml']", "['Check stable / x86_64-apple-darwin', '.github/workflows/CI.yml']"]]}
{"repo": "GuillaumeGomez/sysinfo", "instance_id": "GuillaumeGomez__sysinfo-1373", "base_commit": "dc79f96e6744bff575df8b2fa7706573ab94ee18", "patch": "diff --git a/src/common/system.rs b/src/common/system.rs\nindex 5fb82956f..1409becd1 100644\n--- a/src/common/system.rs\n+++ b/src/common/system.rs\n@@ -942,6 +942,10 @@ pub struct CGroupLimits {\n ///\n /// It is returned by [`Process::disk_usage`][crate::Process::disk_usage].\n ///\n+/// \u26a0\ufe0f Files might be cached in memory by your OS, meaning that reading/writing them might not\n+/// increase the `read_bytes`/`written_bytes` values. You can find more information about it\n+/// in the `proc_pid_io` manual (`man proc_pid_io` on unix platforms).\n+///\n /// ```no_run\n /// use sysinfo::System;\n ///\n@@ -1440,6 +1444,10 @@ impl Process {\n     /// \u26a0\ufe0f On Windows, this method actually returns **ALL** I/O read and\n     /// written bytes.\n     ///\n+    /// \u26a0\ufe0f Files might be cached in memory by your OS, meaning that reading/writing them might not\n+    /// increase the `read_bytes`/`written_bytes` values. You can find more information about it\n+    /// in the `proc_pid_io` manual (`man proc_pid_io` on unix platforms).\n+    ///\n     /// ```no_run\n     /// use sysinfo::{Pid, System};\n     ///\n", "test_patch": "", "problem_statement": "read_bytes always return 0 on nixos and macos\nI have a timer reading the values every 5 second, and i then do random read/writes.\r\nOn nixos and macos and windows, written_bytes works, but read_bytes is always 0 except on windows.\r\nAm I missing a function call to refresh the information or does read only work on windows ?\r\n\r\n**bare bone example, to reproduce**\r\nCargo.toml\r\n```\r\nsysinfo = \"0.29\"\r\n```\r\nexample code\r\n```\r\n    let pid = get_current_pid()?;\r\n    let sys = System::new_all();\r\n    println!(\"System name:             {:?}\", sys.name());\r\n    println!(\"System kernel version:   {:?}\", sys.kernel_version());\r\n    println!(\"System OS version:       {:?}\", sys.os_version());\r\n    println!(\"System host name:        {:?}\", sys.host_name());\r\n    let sys = Arc::new(Mutex::new(sys));\r\n    std::thread::spawn(move || {\r\n        loop {\r\n            {\r\n                let mut file = std::fs::File::create(\"testfile.txt\").unwrap();\r\n                let _ = file.write_all(\"Hello world\".as_bytes());\r\n                file.sync_all().unwrap();\r\n            }\r\n            {\r\n                let mut file = std::fs::File::open(\"testfile.txt\").unwrap();\r\n                let mut contents = String::new();\r\n                file.read_to_string(&mut contents).unwrap();\r\n            }\r\n\r\n            std::thread::sleep(std::time::Duration::from_secs(5));\r\n            let mut sys = sys.lock().unwrap();\r\n            sys.refresh_process(pid);\r\n            if let Some(process) = sys.process(pid) {\r\n                let disk_io = process.disk_usage();\r\n                println!(\"read: {:?} write: {:?} total read: {:?} total write: {:?}\", disk_io.read_bytes, disk_io.written_bytes, disk_io.total_read_bytes, disk_io.total_written_bytes);\r\n            }\r\n        }\r\n    });\r\n```\r\nOutput on nixos\r\n```\r\nSystem name:             Some(\"NixOS\")\r\nSystem kernel version:   Some(\"6.1.96\")\r\nSystem OS version:       Some(\"23.11\")\r\nSystem host name:        Some(\"nixos\")\r\nread: 0 write: 0 total read: 0 total write: 4096\r\nread: 0 write: 0 total read: 0 total write: 8192\r\nread: 0 write: 0 total read: 0 total write: 12288\r\nread: 0 write: 0 total read: 0 total write: 16384\r\nread: 0 write: 0 total read: 0 total write: 20480\r\nread: 0 write: 0 total read: 0 total write: 24576\r\n```\r\nOutput on macos\r\n```\r\nSystem name:             Some(\"Darwin\")\r\nSystem kernel version:   Some(\"23.1.0\")\r\nSystem OS version:       Some(\"14.1.2\")\r\nSystem host name:        Some(\"Allans-Mac-mini.local\")\r\nread: 417792 write: 61440 total read: 417792 total write: 61440\r\nread: 0 write: 4096 total read: 417792 total write: 65536\r\nread: 0 write: 4096 total read: 417792 total write: 69632\r\nread: 0 write: 4096 total read: 417792 total write: 73728\r\nread: 0 write: 4096 total read: 417792 total write: 77824\r\nread: 0 write: 4096 total read: 417792 total write: 81920\r\n```\r\nOutput on windows 10\r\n```\r\nSystem name:             Some(\"Windows\")\r\nSystem kernel version:   Some(\"19045\")\r\nSystem OS version:       Some(\"10 (19045)\")\r\nSystem host name:        Some(\"mac-windows\")\r\nread: 11 write: 11 total read: 1090997 total write: 11\r\nread: 11 write: 11 total read: 1091008 total write: 22\r\nread: 11 write: 11 total read: 1091019 total write: 33\r\nread: 11 write: 11 total read: 1091030 total write: 44\r\nread: 11 write: 11 total read: 1091041 total write: 55\r\nread: 11 write: 11 total read: 1091052 total write: 66\r\nread: 11 write: 11 total read: 1091063 total write: 77\r\n```\r\n\n", "hints_text": "On macOS, I confirmed that `read` doesn't change all the time. However, interestingly enough, `Activity Monitor` gives the same read value. So I suppose it's a mac bug or mac is not reading file content if it's the same.", "created_at": "2024-11-01 00:08:35", "merge_commit_sha": "6812e46e35090b7ebb0b3b992cb7cf2133b0b1e0", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check stable / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Check nightly / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check stable / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check stable / aarch64-apple-ios', '.github/workflows/CI.yml']", "['Check nightly / i686-linux-android', '.github/workflows/CI.yml']"], ["['Check stable / armv7-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['Test ubuntu-latest (rust stable)', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']", "['Check nightly / arm-unknown-linux-musleabihf', '.github/workflows/CI.yml']"], ["['Test ubuntu-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / aarch64-unknown-linux-musl', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust stable)', '.github/workflows/CI.yml']", "['Check 1.74.0 / aarch64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / armv7-unknown-linux-musleabihf', '.github/workflows/CI.yml']", "['Check 1.74.0 / armv7-linux-androideabi', '.github/workflows/CI.yml']"], ["['c_interface', '.github/workflows/CI.yml']", "['Check 1.74.0 / x86_64-apple-ios', '.github/workflows/CI.yml']"], ["['Test macos-latest (rust 1.74.0)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-pc-windows-msvc', '.github/workflows/CI.yml']"], ["['Check 1.74.0 / aarch64-linux-android', '.github/workflows/CI.yml']", "['Check nightly / x86_64-linux-android', '.github/workflows/CI.yml']"], ["['clippy (macos-latest)', '.github/workflows/CI.yml']", "['clippy (ubuntu-latest)', '.github/workflows/CI.yml']"], ["['Check nightly / arm-unknown-linux-gnueabihf', '.github/workflows/CI.yml']", "['unknown-targets (1.74.0)', '.github/workflows/CI.yml']"], ["['clippy (windows-latest)', '.github/workflows/CI.yml']", "['Check nightly / x86_64-unknown-linux-gnu', '.github/workflows/CI.yml']"], ["['Check nightly / i686-unknown-linux-gnu', '.github/workflows/CI.yml']", "['Test windows-latest (rust nightly)', '.github/workflows/CI.yml']"], ["['rustfmt', '.github/workflows/CI.yml']", "['Check stable / x86_64-apple-darwin', '.github/workflows/CI.yml']"]]}
{"repo": "CycloneDX/cyclonedx-rust-cargo", "instance_id": "CycloneDX__cyclonedx-rust-cargo-761", "base_commit": "76d0b094277d0135a8004de7a80415e814f1c244", "patch": "diff --git a/cyclonedx-bom/src/xml.rs b/cyclonedx-bom/src/xml.rs\nindex 20541279..2cd8f193 100644\n--- a/cyclonedx-bom/src/xml.rs\n+++ b/cyclonedx-bom/src/xml.rs\n@@ -442,6 +442,17 @@ impl FromXmlType for f32 {\n     }\n }\n \n+/// Reads a simple String tag.\n+///\n+/// ```xml\n+/// <description>Content</description>\n+/// ```\n+/// &\n+/// ```xml\n+/// <description />\n+/// ```\n+///\n+/// are valid XML tags. The first returns the string \"Content\", the latter is an empty string.\n pub(crate) fn read_simple_tag<R: Read>(\n     event_reader: &mut EventReader<R>,\n     element: &OwnedName,\n@@ -449,13 +460,19 @@ pub(crate) fn read_simple_tag<R: Read>(\n     let element_display = element.to_string();\n     let content = event_reader\n         .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(inner_text_or_error(&element_display))?;\n-\n-    event_reader\n-        .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(closing_tag_or_error(element))?;\n+        .map_err(to_xml_read_error(&element_display))?;\n+\n+    let content = match content {\n+        reader::XmlEvent::EndElement { .. } => String::new(),\n+        reader::XmlEvent::Characters(content) | reader::XmlEvent::CData(content) => {\n+            event_reader\n+                .next()\n+                .map_err(to_xml_read_error(&element_display))\n+                .and_then(closing_tag_or_error(element))?;\n+            content\n+        }\n+        unexpected => return Err(unexpected_element_error(element, unexpected)),\n+    };\n \n     Ok(content)\n }\n", "test_patch": "diff --git a/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml b/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\nnew file mode 100644\nindex 00000000..2ff484b9\n--- /dev/null\n+++ b/cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\n@@ -0,0 +1,48 @@\n+<?xml version=\"1.0\"?>\n+<bom serialNumber=\"urn:uuid:3e671687-395b-41f5-a30f-a58921a69b79\" version=\"1\" xmlns=\"http://cyclonedx.org/schema/bom/1.3\">\n+    <components>\n+        <component type=\"library\" bom-ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+            <group>com.acme</group>\n+            <name>stock-java-client</name>\n+            <version>1.0.12</version>\n+            <hashes>\n+                <hash alg=\"SHA-1\">e6b1000b94e835ffd37f4c6dcbdad43f4b48a02a</hash>\n+            </hashes>\n+            <licenses>\n+                <license>\n+                    <id>Apache-2.0</id>\n+                </license>\n+            </licenses>\n+            <purl>pkg:maven/com.acme/stock-java-client@1.0.12</purl>\n+        </component>\n+    </components>\n+    <services>\n+        <service bom-ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\">\n+            <provider>\n+                <name>Partner Org</name>\n+                <url>https://partner.org</url>\n+                <contact>\n+                    <name>Support</name>\n+                    <email>support@partner</email>\n+                    <phone>800-555-1212</phone>\n+                </contact>\n+            </provider>\n+            <group>org.partner</group>\n+            <name>Stock ticker service</name>\n+            <version>2020-Q2</version>\n+            <description />\n+            <authenticated>true</authenticated>\n+            <x-trust-boundary>true</x-trust-boundary>\n+            <licenses>\n+                <license>\n+                    <name>Partner license</name>\n+                </license>\n+            </licenses>\n+        </service>\n+    </services>\n+    <dependencies>\n+        <dependency ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+            <dependency ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\"/>\n+        </dependency>\n+    </dependencies>\n+</bom>\ndiff --git a/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap b/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap\nnew file mode 100644\nindex 00000000..dc323228\n--- /dev/null\n+++ b/cyclonedx-bom/tests/spec/snapshots/1.3/it_should_parse_all_of_the_valid_xml_specifications@valid-service-empty-description-1.3.xml.snap\n@@ -0,0 +1,54 @@\n+---\n+source: cyclonedx-bom/tests/specification_tests_v1_3.rs\n+assertion_line: 27\n+expression: bom_output\n+input_file: cyclonedx-bom/tests/spec/1.3/valid-service-empty-description-1.3.xml\n+---\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<bom xmlns=\"http://cyclonedx.org/schema/bom/1.3\" serialNumber=\"urn:uuid:3e671687-395b-41f5-a30f-a58921a69b79\" version=\"1\">\n+  <components>\n+    <component type=\"library\" bom-ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+      <group>com.acme</group>\n+      <name>stock-java-client</name>\n+      <version>1.0.12</version>\n+      <hashes>\n+        <hash alg=\"SHA-1\">e6b1000b94e835ffd37f4c6dcbdad43f4b48a02a</hash>\n+      </hashes>\n+      <licenses>\n+        <license>\n+          <id>Apache-2.0</id>\n+        </license>\n+      </licenses>\n+      <purl>pkg:maven/com.acme/stock-java-client@1.0.12</purl>\n+    </component>\n+  </components>\n+  <services>\n+    <service bom-ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\">\n+      <provider>\n+        <name>Partner Org</name>\n+        <url>https://partner.org</url>\n+        <contact>\n+          <name>Support</name>\n+          <email>support@partner</email>\n+          <phone>800-555-1212</phone>\n+        </contact>\n+      </provider>\n+      <group>org.partner</group>\n+      <name>Stock ticker service</name>\n+      <version>2020-Q2</version>\n+      <description></description>\n+      <authenticated>true</authenticated>\n+      <x-trust-boundary>true</x-trust-boundary>\n+      <licenses>\n+        <license>\n+          <name>Partner license</name>\n+        </license>\n+      </licenses>\n+    </service>\n+  </services>\n+  <dependencies>\n+    <dependency ref=\"pkg:maven/com.acme/stock-java-client@1.0.12\">\n+      <dependency ref=\"b2a46a4b-8367-4bae-9820-95557cfe03a8\" />\n+    </dependency>\n+  </dependencies>\n+</bom>\n", "problem_statement": "XML deserialization of empty tags is incorrect\nAs a working example, I am going to reference the following area of code, but I believe it is possible to run into this in any area that the library is parsing an `Option<String>` field: https://github.com/CycloneDX/cyclonedx-rust-cargo/blob/2911287b2520a7ddab1782b48c35112279b1be17/cyclonedx-bom/src/specs/common/component.rs#L513-L517\r\n\r\nSetting context, I was attempting to swap out an ad-hoc CycloneDX parser with `cyclonedx-bom`. One of the tests for my ad-hoc parser was to ensure that the keycloak BOM found here loads: https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/keycloak-10.0.2/bom.xml. Now, it obviously doesn't because it is a v1.2 spec and `cyclonedx-bom` only supports v1.3-v1.5. But, looking through the spec and looking at the BOM, it seemed like it should load as a v1.3, so I changed the version locally. That seemed a reasonable test given that a diff between https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.2.xml and https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.3.xml only added some properties and changed the spec version. \r\n\r\nHowever, when I attempted to load the modified keycloak BOM, I got the following error:\r\n\r\n```\r\n    Got unexpected XML element when reading {http://cyclonedx.org/schema/bom/1.3}description: Got unexpected element EndElement({http://cyclonedx.org/schema/bom/1.3}description)\r\n```\r\n\r\nAfter digging into the issue, what I discovered was that the keycloak BOM has several empty description tags in it:\r\n\r\n```xml\r\n<description />\r\n```\r\n\r\nA quick modification of the `laravel-7.12.0/bom.1.3.xml` file to modify one of the description tags the same way caused the same error to occur. I also attempted to just modify one of the description like the following and received the same error:\r\n\r\n```xml\r\n<description></description>\r\n```\r\n\r\nThe [v1.3 (XML)](https://cyclonedx.org/docs/1.3/xml/#type_component) and [v1.2 (XML)](https://cyclonedx.org/docs/1.2/xml/#type_component) specs \r\nboth define the description field as `<bom:description> xs:normalizedString </bom:description> [0..1]` and an `xs:normalizedString` _can_ be empty.\r\n\r\nAgain, there are multiple places just in the `Component::read_xml_element` where this could be an issue. My guess is that, since the `pom.xml` file that is used to generate the CycloneDX Maven BOMs can have empty strings for the description, that is why it is getting forwarded on to the BOM that way instead of stripping out the description tag completely. \r\n\r\nI could make an argument for handling it either by making an empty string become a `None` for the `Option<String>` OR by literally making it an `Some(String::new())`. I think that the latter is closer to the behavior that would happen with the JSON implementation of the spec, since an equivalent there would be:\r\n\r\n```json\r\n\"description\": \"\"\r\n```\r\n\r\nand that would be parsed as `Some(String::new())` by `serde`.\r\n\r\n\n", "hints_text": "Thank you for reporting this!\r\n\r\nTreating this as `Some(String::new())` for consistency with JSON sounds good to me. I'm not familiar with XML parsing myself, but I'd be happy to merge a PR with this change.\nHello,\r\n\r\nI asked on the CycloneDX Slack to clarify how to handle the `description` tag. It's not totally clear to me if `<description />` is a valid entry or if it needs to be either absent or contain text. I'm uncertain, because the XML sample files in the [specification](https://github.com/CycloneDX/specification/) repository don't contain any empty `<description />` tag. We used these sample files as test data for this repository, but they may not cover all cases.\n**Update**: The answer on Slack was the `<specification />` tag is valid, it should result in an empty string. The current XML parse logic is therefore wrong, at least for these types of fields.", "created_at": "2024-08-07 14:22:17", "merge_commit_sha": "7596551f4caeb368e500c501b5a28f80d5347aa0", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test Suite Runs - Nightly', '.github/workflows/rust.yml']", "[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']"], ["['plan', '.github/workflows/release.yml']", "['build-global-artifacts', '.github/workflows/release.yml']"], ["['Test Suite Runs - Minimum Supported Rust Version', '.github/workflows/rust.yml']", "['Check Dependencies', '.github/workflows/rust.yml']"]]}
{"repo": "libbpf/libbpf-rs", "instance_id": "libbpf__libbpf-rs-890", "base_commit": "6fbd2961cf40596b0b8308b939e153a67830ad05", "patch": "diff --git a/libbpf-rs/src/program.rs b/libbpf-rs/src/program.rs\nindex 700e1ac8..def7b9a9 100644\n--- a/libbpf-rs/src/program.rs\n+++ b/libbpf-rs/src/program.rs\n@@ -104,7 +104,6 @@ impl From<TracepointOpts> for libbpf_sys::bpf_tracepoint_opts {\n     }\n }\n \n-\n /// An immutable parsed but not yet loaded BPF program.\n pub type OpenProgram<'obj> = OpenProgramImpl<'obj>;\n /// A mutable parsed but not yet loaded BPF program.\n@@ -506,7 +505,7 @@ impl From<u32> for ProgramAttachType {\n #[derive(Debug, Default)]\n pub struct Input<'dat> {\n     /// The input context to provide.\n-    pub context_in: Option<&'dat [u8]>,\n+    pub context_in: Option<&'dat mut [u8]>,\n     /// The output context buffer provided to the program.\n     pub context_out: Option<&'dat mut [u8]>,\n     /// Additional data to provide to the program.\n@@ -544,7 +543,6 @@ pub type Program<'obj> = ProgramImpl<'obj>;\n /// A mutable loaded BPF program.\n pub type ProgramMut<'obj> = ProgramImpl<'obj, Mut>;\n \n-\n /// Represents a loaded [`Program`].\n ///\n /// This struct is not safe to clone because the underlying libbpf resource cannot currently\n@@ -1091,6 +1089,7 @@ impl<'obj> ProgramMut<'obj> {\n         let mut opts = unsafe { mem::zeroed::<libbpf_sys::bpf_test_run_opts>() };\n         opts.sz = size_of_val(&opts) as _;\n         opts.ctx_in = context_in\n+            .as_ref()\n             .map(|data| data.as_ptr().cast())\n             .unwrap_or_else(ptr::null);\n         opts.ctx_size_in = context_in.map(|data| data.len() as _).unwrap_or(0);\n", "test_patch": "diff --git a/libbpf-rs/tests/test.rs b/libbpf-rs/tests/test.rs\nindex cd547d4a..97813325 100644\n--- a/libbpf-rs/tests/test.rs\n+++ b/libbpf-rs/tests/test.rs\n@@ -13,6 +13,7 @@ use std::hint;\n use std::io;\n use std::io::Read;\n use std::mem::size_of;\n+use std::mem::size_of_val;\n use std::os::unix::io::AsFd;\n use std::path::Path;\n use std::path::PathBuf;\n@@ -55,7 +56,6 @@ use crate::common::get_test_object;\n use crate::common::get_test_object_path;\n use crate::common::open_test_object;\n \n-\n /// A helper function for instantiating a `RingBuffer` with a callback meant to\n /// be invoked when `action` is executed and that is intended to trigger a write\n /// to said `RingBuffer` from kernel space, which then reads a single `i32` from\n@@ -1994,9 +1994,11 @@ fn test_run_prog_success() {\n \n     let value = 42;\n     let state = bpf_dummy_ops_state { val: value };\n-    let args = [addr_of!(state) as u64];\n+    let mut args = [addr_of!(state) as u64];\n     let input = ProgramInput {\n-        context_in: Some(unsafe { plain::as_bytes(&args) }),\n+        context_in: Some(unsafe {\n+            slice::from_raw_parts_mut(&mut args as *mut _ as *mut u8, size_of_val(&args))\n+        }),\n         ..Default::default()\n     };\n     let output = prog.test_run(input).unwrap();\n", "problem_statement": "Possible mutation of `context_in` constant when using `test_run` in `syscall` methods\nThe field `context_in` in `ProgramInput` structure now looks like this::\r\n```rust\r\nstruct ProgramInput<'dat> {\r\n  context_in: Option<&'dat [u8]>,\r\n  // ...\r\n}\r\n```\r\n\r\nBut this field can be overwritten by kernel when calling `syscall` method from bpf. [File `net/bpf/test_run.c` at line 1567](https://github.com/torvalds/linux/blob/defaf1a2113a22b00dfa1abc0fd2014820eaf065/net/bpf/test_run.c#L1567): kernel is copy back `context_in` to user space after calling `syscall`. And this is the only way to return some data from `syscall` (something bigger than `i32`).\r\n\r\nPerhaps the `context_in` field should have the signature `Option<&'dat mut [u8]>` or special attention should be added to the comments of `test_run` method and `context_in` field.\r\n\r\nExample of valid bpf code:\r\n```c++\r\nSEC(\"syscall\")\r\nint mySyscall(int *ctx) {\r\n  *ctx += *ctx;\r\n  return 0;\r\n}\r\n```\r\nAnd rust with `libbpf-rs`:\r\n```rust\r\nlet value: i32 = 21;\r\nlet mut value = value.to_ne_bytes();\r\n\r\nlet mut input = ProgramInput::default();\r\ninput.context_in = Some(&mut value);\r\n\r\nmySyscall.test_run(input).unwrap();\r\n\r\nlet value = i32::from_ne_bytes(value);\r\nassert_eq!(value, 42);\r\n```\n", "hints_text": "Hi @x1b6e6, yes, that's a fair point.\r\n\r\n> Perhaps the context_in field should have the signature Option<&'dat mut [u8]>\r\n\r\nDo you plan to send a pull request?\n@danielocfb\n\n> Do you plan to send a pull request?\n\nIf changing field signature is an acceptable solution then I can make this PR.", "created_at": "2024-08-06 07:33:39", "merge_commit_sha": "64f558d008db95aa24b311c065668755ea099fb3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build for aarch32', '.github/workflows/test.yml']", "['Build using minimum versions of dependencies', '.github/workflows/test.yml']"], ["['Test [stable, release]', '.github/workflows/test.yml']", "['Run tcp_ca example', '.github/workflows/test.yml']"], ["[\"Build [features = ['static']]\", '.github/workflows/test.yml']", "['Test [stable, dev]', '.github/workflows/test.yml']"]]}
{"repo": "wezterm/wezterm", "instance_id": "wezterm__wezterm-6342", "base_commit": "f847bd284db6a38e7f5920421ba23e3aa688f8fd", "patch": "diff --git a/window/Cargo.toml b/window/Cargo.toml\nindex 2ad3ac6b4c2..06f32a49902 100644\n--- a/window/Cargo.toml\n+++ b/window/Cargo.toml\n@@ -15,7 +15,7 @@ k9 = \"0.12.0\"\n gl_generator = \"0.14\"\n \n [features]\n-wayland = [\"wayland-client\", \"smithay-client-toolkit\", \"wayland-egl\", \"wayland-protocols\"]\n+wayland = [\"wayland-client\", \"smithay-client-toolkit\", \"wayland-egl\", \"wayland-protocols\", \"wayland-backend\"]\n \n [dependencies]\n async-channel = \"2.3\"\n@@ -82,7 +82,7 @@ zbus = \"4.2\"\n zvariant = \"4.0\"\n \n smithay-client-toolkit = {version = \"0.19\", default-features=false, optional=true}\n-wayland-backend = {version=\"0.3.5\", features=[\"client_system\", \"rwh_06\"]}\n+wayland-backend = {version=\"0.3.5\", features=[\"client_system\", \"rwh_06\"], optional=true}\n wayland-protocols = {version=\"0.32\", optional=true}\n wayland-client = {version=\"0.31\", optional=true}\n wayland-egl = {version=\"0.32\", optional=true}\n", "test_patch": "", "problem_statement": "wgpu depends upon wayland even when building wezterm without the wayland feature\nWhen doing `cargo build --release --no-default-features --features vendored-fonts` I get an error building `wayland-sys`, which shouldn't happen.\r\n\r\nI am seeing this issue on the current git main 9ddca7bde92090792dbcdc65c1e9897c362196d7.\r\n\r\nI have tested the last public release 20240203-110809-5046fc22 and I am able to build from source just fine.\r\n\r\nBased on some crude `git bisect`ing, it looks like the issue appeared around 09ac8c53777ac6de61b757292f5dc4da80322bbd.\r\n\r\n24702de74a958460e3b9af3fbf21327b7ba4a3e3 is the last version I can build currently.\r\n\r\n## Build Environment (please complete the following information):\r\n\r\n - OS: Linux X11\r\n - Linux: `Linux name 6.10.13-304.current #1 SMP PREEMPT_DYNAMIC 2024-10-06 x86_64 GNU/Linux`\r\n - Compiler: clang 18.1.8 (not sure if this is what you were looking for)\r\n - Rust version: 1.79\r\n\r\n```\r\nLSB Version:    1.4\r\nDistributor ID: Solus\r\nDescription:    Solus\r\nRelease:        4.6\r\nCodename:       convergence\r\n```\r\n\r\n```\r\nDefault host: x86_64-unknown-linux-gnu\r\nrustup home:  /home/user/.rustup\r\n\r\ninstalled toolchains\r\n--------------------\r\n\r\nstable-x86_64-unknown-linux-gnu\r\n1.79.0-x86_64-unknown-linux-gnu (default)\r\n\r\nactive toolchain\r\n----------------\r\n\r\n1.79.0-x86_64-unknown-linux-gnu (default)\r\nrustc 1.79.0 (129f3b996 2024-06-10)\r\n```\r\n\r\n## Dependencies\r\n\r\nDid you run the `get-deps` script to install required system dependencies? yes\r\nWas it successful? yes\r\n\r\nIf building from the git repo, did you update the submodules?  yes\r\n\r\n## The build output\r\n\r\n```sh\r\ngit submodule update --init --recursive\r\ncargo clean\r\ncargo build --release --no-default-features --features vendored-fonts\r\n```\r\n\r\n```\r\nerror: failed to run custom build command for `wayland-sys v0.31.5`\r\n\r\nCaused by:\r\n  process didn't exit successfully: `/home/user/Source/Clones/wezterm/target/debug/build/wayland-sys-54d8d2a2b5738154/build-script-build` (exit status: 101)\r\n  --- stdout\r\n  cargo:rerun-if-env-changed=WAYLAND_CLIENT_NO_PKG_CONFIG\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_x86_64-unknown-linux-gnu\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_x86_64_unknown_linux_gnu\r\n  cargo:rerun-if-env-changed=HOST_PKG_CONFIG\r\n  cargo:rerun-if-env-changed=PKG_CONFIG\r\n  cargo:rerun-if-env-changed=WAYLAND_CLIENT_STATIC\r\n  cargo:rerun-if-env-changed=WAYLAND_CLIENT_DYNAMIC\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_ALL_STATIC\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_ALL_DYNAMIC\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_PATH_x86_64-unknown-linux-gnu\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_PATH_x86_64_unknown_linux_gnu\r\n  cargo:rerun-if-env-changed=HOST_PKG_CONFIG_PATH\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_PATH\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_LIBDIR_x86_64-unknown-linux-gnu\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_LIBDIR_x86_64_unknown_linux_gnu\r\n  cargo:rerun-if-env-changed=HOST_PKG_CONFIG_LIBDIR\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_LIBDIR\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_SYSROOT_DIR_x86_64-unknown-linux-gnu\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_SYSROOT_DIR_x86_64_unknown_linux_gnu\r\n  cargo:rerun-if-env-changed=HOST_PKG_CONFIG_SYSROOT_DIR\r\n  cargo:rerun-if-env-changed=PKG_CONFIG_SYSROOT_DIR\r\n\r\n  --- stderr\r\n  thread 'main' panicked at /home/user/.cargo/registry/src/index.crates.io-6f17d22bba15001f/wayland-sys-0.31.5/build.rs:10:47:\r\n  called `Result::unwrap()` on an `Err` value:\r\n  pkg-config exited with status code 1\r\n  > PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 pkg-config --libs --cflags wayland-client\r\n\r\n  The system library `wayland-client` required by crate `wayland-sys` was not found.\r\n  The file `wayland-client.pc` needs to be installed and the PKG_CONFIG_PATH environment variable must contain its parent directory.\r\n  The PKG_CONFIG_PATH environment variable is not set.\r\n\r\n  HINT: if you have installed the library, try setting PKG_CONFIG_PATH to the directory containing `wayland-client.pc`.\r\n\r\n  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\nwarning: build failed, waiting for other jobs to finish...\r\n```\r\n\r\nseems unrelated to this Nix issue: https://github.com/wez/wezterm/issues/5367\n", "hints_text": "You need to install some wayland client lib, see your error log. On other distros this is probably included in the get-deps script file.\r\n```\r\nThe system library `wayland-client` required by crate `wayland-sys` was not found.\r\n  The file `wayland-client.pc` needs to be installed and the PKG_CONFIG_PATH environment variable must contain its parent directory.\r\n  The PKG_CONFIG_PATH environment variable is not set.\r\n```\nI'm not using Wayland and no part of it is installed. `--no-default-features` is documented as disabling Wayland support. Prior to 09ac8c53777ac6de61b757292f5dc4da80322bbd it was possible to fully disable Wayland. But it doesn't seem to fully do that anymore.\nI think you will need to find out how to relay/interpret the feature selection down to the wgpu crate (by looking at its available set of features) to tell it not to import the wayland client libraries if you want to build wezterm without them.\nI don't think it's `wgpu`, I don't see a hard dependency on anything Wayland there. It's `wayland-backend`, which has the `wayland-sys` dependency.\r\n\r\nhttps://github.com/wez/wezterm/commit/09ac8c53777ac6de61b757292f5dc4da80322bbd#diff-8b7cc7bbcc91b1d0b3bb4323aeab9b4999f5e6a78a5d6d62183c28c44e8601bfR85\r\n\r\nI have a fix for this I just tested, will submit a PR soon.\r\n", "created_at": "2024-10-31 16:31:03", "merge_commit_sha": "0983ae90d6dfb45c5f99058e97de73a70ca9dd36", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build', '.github/workflows/gen_centos9.yml']", "['build', '.github/workflows/gen_windows.yml']"], ["['build', '.github/workflows/gen_fedora40.yml']", "['build', '.github/workflows/gen_debian12.yml']"], ["['build', '.github/workflows/gen_ubuntu22.04.yml']", "['build', '.github/workflows/gen_debian11.yml']"]]}
{"repo": "elastio/bon", "instance_id": "elastio__bon-222", "base_commit": "00d9a3ee614f76f477b9f0f8a4f3e1210cbadc57", "patch": "diff --git a/bon-macros/Cargo.toml b/bon-macros/Cargo.toml\nindex 98101b18..3c5570ed 100644\n--- a/bon-macros/Cargo.toml\n+++ b/bon-macros/Cargo.toml\n@@ -63,6 +63,9 @@ rustversion  = \"1.0\"\n [features]\n default = []\n \n+# See the docs on this feature in the `bon`'s crate `Cargo.toml`.\n+experimental-getter = []\n+\n # See the docs on this feature in the `bon`'s crate `Cargo.toml`\n experimental-overwritable = []\n \ndiff --git a/bon-macros/src/builder/builder_gen/getter.rs b/bon-macros/src/builder/builder_gen/getter.rs\nnew file mode 100644\nindex 00000000..291a9249\n--- /dev/null\n+++ b/bon-macros/src/builder/builder_gen/getter.rs\n@@ -0,0 +1,92 @@\n+use proc_macro2::TokenStream;\n+use quote::quote;\n+\n+use super::{BuilderGenCtx, IdentExt, NamedMember};\n+\n+pub(crate) struct GetterCtx<'a> {\n+    base: &'a BuilderGenCtx,\n+    member: &'a NamedMember,\n+}\n+\n+struct GetterItem {\n+    name: syn::Ident,\n+    vis: syn::Visibility,\n+    docs: Vec<syn::Attribute>,\n+}\n+\n+impl<'a> GetterCtx<'a> {\n+    pub(crate) fn new(base: &'a BuilderGenCtx, member: &'a NamedMember) -> Self {\n+        Self { base, member }\n+    }\n+\n+    pub(crate) fn getter_method(&self) -> TokenStream {\n+        let Some(GetterItem { name, vis, docs }) = GetterItem::new(self) else {\n+            return quote! {};\n+        };\n+\n+        let index = &self.member.index;\n+        let ty = self.member.underlying_norm_ty();\n+\n+        let (return_type, body) = if self.member.is_required() {\n+            (\n+                quote! { &#ty },\n+                quote! { unsafe { ::std::option::Option::unwrap_unchecked(self.__unsafe_private_named.#index.as_ref()) } },\n+            )\n+        } else {\n+            (\n+                quote! { ::core::option::Option<&#ty> },\n+                quote! { self.__unsafe_private_named.#index.as_ref() },\n+            )\n+        };\n+\n+        let state_var = &self.base.state_var;\n+        let member_pascal = &self.member.name.pascal;\n+        let state_mod = &self.base.state_mod.ident;\n+\n+        quote! {\n+            #( #docs )*\n+            #[allow(\n+                // This is intentional. We want the builder syntax to compile away\n+                clippy::inline_always,\n+                clippy::missing_const_for_fn,\n+            )]\n+            #[inline(always)]\n+            #vis fn #name(&self) -> #return_type\n+            where #state_var::#member_pascal: #state_mod::IsSet,\n+            {\n+                #body\n+            }\n+        }\n+    }\n+}\n+\n+impl GetterItem {\n+    fn new(ctx: &GetterCtx<'_>) -> Option<Self> {\n+        let GetterCtx { member, base } = ctx;\n+\n+        let spanned_keyed_config = member.config.getter.as_ref()?;\n+\n+        let common_name = spanned_keyed_config.name();\n+        let common_vis = spanned_keyed_config.vis();\n+        let common_docs = spanned_keyed_config.docs();\n+\n+        Some(Self {\n+            name: common_name.cloned().unwrap_or_else(|| {\n+                syn::Ident::new(\n+                    &format!(\"get_{}\", member.name.snake.raw_name()),\n+                    member.name.snake.span(),\n+                )\n+            }),\n+            vis: common_vis.unwrap_or(&base.builder_type.vis).clone(),\n+            docs: common_docs\n+                .map(<[syn::Attribute]>::to_vec)\n+                .unwrap_or_else(|| {\n+                    const HEADER: &str = \"_**Getter.**_\\n\\n\";\n+\n+                    std::iter::once(syn::parse_quote!(#[doc = #HEADER]))\n+                        .chain(member.docs.iter().cloned())\n+                        .collect()\n+                }),\n+        })\n+    }\n+}\ndiff --git a/bon-macros/src/builder/builder_gen/member/config/getter.rs b/bon-macros/src/builder/builder_gen/member/config/getter.rs\nnew file mode 100644\nindex 00000000..1e95ea56\n--- /dev/null\n+++ b/bon-macros/src/builder/builder_gen/member/config/getter.rs\n@@ -0,0 +1,54 @@\n+use darling::FromMeta;\n+\n+use super::{Result, SpannedKey};\n+\n+#[derive(Debug, Default)]\n+pub(crate) struct GetterConfig {\n+    name: Option<SpannedKey<syn::Ident>>,\n+    vis: Option<SpannedKey<syn::Visibility>>,\n+\n+    docs: Option<SpannedKey<Vec<syn::Attribute>>>,\n+}\n+\n+impl FromMeta for GetterConfig {\n+    fn from_meta(meta: &syn::Meta) -> Result<Self> {\n+        if let syn::Meta::Path(_) = meta {\n+            return Ok(Self::default());\n+        }\n+\n+        // Reject empty parens such as `#[builder(getter())]`\n+        crate::parsing::require_non_empty_paren_meta_list_or_name_value(meta)?;\n+\n+        // Nested `Parsed` struct used as a helper for parsing the verbose form\n+        #[derive(FromMeta)]\n+        struct Parsed {\n+            name: Option<SpannedKey<syn::Ident>>,\n+            vis: Option<SpannedKey<syn::Visibility>>,\n+\n+            #[darling(rename = \"doc\", default, with = parse_docs, map = Some)]\n+            docs: Option<SpannedKey<Vec<syn::Attribute>>>,\n+        }\n+\n+        let Parsed { name, vis, docs } = Parsed::from_meta(meta)?;\n+\n+        Ok(Self { name, vis, docs })\n+    }\n+}\n+\n+impl GetterConfig {\n+    pub(crate) fn name(&self) -> Option<&syn::Ident> {\n+        self.name.as_ref().map(|n| &n.value)\n+    }\n+\n+    pub(crate) fn vis(&self) -> Option<&syn::Visibility> {\n+        self.vis.as_ref().map(|v| &v.value)\n+    }\n+\n+    pub(crate) fn docs(&self) -> Option<&[syn::Attribute]> {\n+        self.docs.as_ref().map(|a| &a.value).map(|a| &**a)\n+    }\n+}\n+\n+fn parse_docs(meta: &syn::Meta) -> Result<SpannedKey<Vec<syn::Attribute>>> {\n+    crate::parsing::parse_docs_without_self_mentions(\"builder struct's impl block\", meta)\n+}\ndiff --git a/bon-macros/src/builder/builder_gen/member/config/mod.rs b/bon-macros/src/builder/builder_gen/member/config/mod.rs\nindex 02bfe590..a6e5606e 100644\n--- a/bon-macros/src/builder/builder_gen/member/config/mod.rs\n+++ b/bon-macros/src/builder/builder_gen/member/config/mod.rs\n@@ -1,8 +1,10 @@\n mod blanket;\n+mod getter;\n mod setters;\n mod with;\n \n pub(crate) use blanket::*;\n+pub(crate) use getter::*;\n pub(crate) use setters::*;\n pub(crate) use with::*;\n \n@@ -33,6 +35,12 @@ pub(crate) struct MemberConfig {\n     #[darling(with = parse_optional_expr, map = Some)]\n     pub(crate) field: Option<SpannedKey<Option<syn::Expr>>>,\n \n+    /// Make the member gettable by reference.\n+    ///\n+    /// This takes the same attributes as the setter fns; `name`, `vis`, and `doc`\n+    /// and produces a getter method that returns `&T` for the member.\n+    pub(crate) getter: Option<SpannedKey<GetterConfig>>,\n+\n     /// Accept the value for the member in the finishing function parameters.\n     pub(crate) finish_fn: darling::util::Flag,\n \n@@ -82,6 +90,7 @@ pub(crate) struct MemberConfig {\n enum ParamName {\n     Default,\n     Field,\n+    Getter,\n     FinishFn,\n     Into,\n     Name,\n@@ -98,6 +107,7 @@ impl fmt::Display for ParamName {\n         let str = match self {\n             Self::Default => \"default\",\n             Self::Field => \"field\",\n+            Self::Getter => \"getter\",\n             Self::FinishFn => \"finish_fn\",\n             Self::Into => \"into\",\n             Self::Name => \"name\",\n@@ -162,6 +172,7 @@ impl MemberConfig {\n         let Self {\n             default,\n             field,\n+            getter,\n             finish_fn,\n             into,\n             name,\n@@ -176,6 +187,7 @@ impl MemberConfig {\n         let attrs = [\n             (default.is_some(), ParamName::Default),\n             (field.is_some(), ParamName::Field),\n+            (getter.is_some(), ParamName::Getter),\n             (finish_fn.is_present(), ParamName::FinishFn),\n             (into.is_present(), ParamName::Into),\n             (name.is_some(), ParamName::Name),\n@@ -212,7 +224,7 @@ impl MemberConfig {\n             self.validate_mutually_allowed(\n                 ParamName::StartFn,\n                 self.start_fn.span(),\n-                &[ParamName::Into],\n+                &[ParamName::Into, ParamName::Getter],\n             )?;\n         }\n \n@@ -220,7 +232,34 @@ impl MemberConfig {\n             self.validate_mutually_allowed(\n                 ParamName::FinishFn,\n                 self.finish_fn.span(),\n-                &[ParamName::Into],\n+                &[ParamName::Into, ParamName::Getter],\n+            )?;\n+        }\n+\n+        if let Some(getter) = &self.getter {\n+            if !cfg!(feature = \"experimental-getter\") {\n+                bail!(\n+                    &getter.key.span(),\n+                    \"`getter` attribute is experimental and requires \\\n+                    \\\"experimental-getter\\\" cargo feature to be enabled; \\\n+                    we would be glad to make this attribute stable if you find it useful; \\\n+                    please leave a \ud83d\udc4d reaction under the issue https://github.com/elastio/bon/issues/221 \\\n+                    to help us measure the impact on this feature. If you have \\\n+                    a use case for this attribute, then open an issue/discussion on \\\n+                    https://github.com/elastio/bon/issues.\",\n+                );\n+            }\n+\n+            self.validate_mutually_allowed(\n+                ParamName::Getter,\n+                getter.key.span(),\n+                &[\n+                    ParamName::With,\n+                    ParamName::Into,\n+                    ParamName::Name,\n+                    ParamName::Setters,\n+                    ParamName::Required,\n+                ],\n             )?;\n         }\n \ndiff --git a/bon-macros/src/builder/builder_gen/mod.rs b/bon-macros/src/builder/builder_gen/mod.rs\nindex 9196c498..325fbd38 100644\n--- a/bon-macros/src/builder/builder_gen/mod.rs\n+++ b/bon-macros/src/builder/builder_gen/mod.rs\n@@ -1,6 +1,7 @@\n mod builder_decl;\n mod builder_derives;\n mod finish_fn;\n+mod getter;\n mod member;\n mod models;\n mod setters;\n@@ -11,6 +12,7 @@ mod top_level_config;\n pub(crate) mod input_fn;\n pub(crate) mod input_struct;\n \n+use getter::GetterCtx;\n pub(crate) use top_level_config::TopLevelConfig;\n \n use crate::util::prelude::*;\n@@ -107,6 +109,10 @@ impl BuilderGenCtx {\n             .map(|member| SettersCtx::new(self, member).setter_methods())\n             .collect::<Result<Vec<_>>>()?;\n \n+        let getter_methods = self\n+            .named_members()\n+            .map(|member| GetterCtx::new(self, member).getter_method());\n+\n         let generics_decl = &self.generics.decl_without_defaults;\n         let generic_args = &self.generics.args;\n         let where_clause = &self.generics.where_clause;\n@@ -128,6 +134,7 @@ impl BuilderGenCtx {\n             {\n                 #finish_fn\n                 #(#setter_methods)*\n+                #(#getter_methods)*\n             }\n         })\n     }\ndiff --git a/bon-sandbox/Cargo.toml b/bon-sandbox/Cargo.toml\nindex 12f0dc0f..8c5e56ca 100644\n--- a/bon-sandbox/Cargo.toml\n+++ b/bon-sandbox/Cargo.toml\n@@ -29,7 +29,7 @@ targets = [\"x86_64-unknown-linux-gnu\"]\n workspace = true\n \n [dependencies]\n-bon            = { path = \"../bon\", version = \"=3.1.1\", features = [\"experimental-overwritable\"] }\n+bon            = { path = \"../bon\", version = \"=3.1.1\", features = [\"experimental-overwritable\", \"experimental-getter\"] }\n buildstructor  = \"0.5\"\n derive_builder = \"0.20\"\n typed-builder  = \"0.20\"\ndiff --git a/bon-sandbox/src/getter.rs b/bon-sandbox/src/getter.rs\nnew file mode 100644\nindex 00000000..287fd91e\n--- /dev/null\n+++ b/bon-sandbox/src/getter.rs\n@@ -0,0 +1,17 @@\n+use bon::{builder, Builder};\n+\n+#[builder]\n+pub fn full_name_fn(#[builder(getter)] first_name: &str, last_name: &str) -> String {\n+    format!(\"{first_name} {last_name}\")\n+}\n+\n+#[derive(Builder)]\n+pub struct FullName {\n+    #[builder(getter)]\n+    pub first_name: String,\n+    #[builder(getter(name = get_the_last_name, vis = \"pub(crate)\", doc {\n+        /// Docs on the getter\n+    }))]\n+    pub last_name: String,\n+    pub no_getter: String,\n+}\ndiff --git a/bon-sandbox/src/lib.rs b/bon-sandbox/src/lib.rs\nindex 9cd816e8..49d95423 100644\n--- a/bon-sandbox/src/lib.rs\n+++ b/bon-sandbox/src/lib.rs\n@@ -9,6 +9,7 @@ pub mod attr_default;\n pub mod attr_with;\n pub mod docs_comparison;\n pub mod functions;\n+pub mod getter;\n pub mod macro_rules_wrapper_test;\n pub mod missing_docs_test;\n pub mod overrides;\ndiff --git a/bon/Cargo.toml b/bon/Cargo.toml\nindex 38eeff6a..fca6039e 100644\n--- a/bon/Cargo.toml\n+++ b/bon/Cargo.toml\n@@ -82,3 +82,17 @@ implied-bounds = [\"bon-macros/implied-bounds\"]\n # this attribute. It would also be cool if you could leave a comment under that issue\n # describing your use case for it.\n experimental-overwritable = [\"bon-macros/experimental-overwritable\"]\n+\n+# \ud83d\udd2c Experimental! There may be breaking changes to this feature between *minor* releases,\n+# however, compatibility within patch releases is guaranteed though.\n+#\n+# This feature enables the #[builder(getter)] attribute that can be used to\n+# allow getting references to already set fields in the builder.\n+#\n+# See more info at https://bon-rs.com/reference/builder/member/getter.\n+#\n+# We are considering stabilizing this attribute if you have a use for it. Please leave\n+# a \ud83d\udc4d reaction under the issue https://github.com/elastio/bon/issues/221 if you need\n+# this attribute. It would also be cool if you could leave a comment under that issue\n+# describing your use case for it.\n+experimental-getter = [\"bon-macros/experimental-getter\"]\ndiff --git a/rust-toolchain b/rust-toolchain\ndeleted file mode 100644\nindex 71fae54f..00000000\n--- a/rust-toolchain\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-1.82.0\ndiff --git a/rust-toolchain.toml b/rust-toolchain.toml\nnew file mode 100644\nindex 00000000..66788910\n--- /dev/null\n+++ b/rust-toolchain.toml\n@@ -0,0 +1,3 @@\n+[toolchain]\n+channel    = \"1.82\"\n+components = [\"cargo\"]\ndiff --git a/scripts/init.sh b/scripts/init.sh\nindex 6df82a0b..bd9c3f9b 100755\n--- a/scripts/init.sh\n+++ b/scripts/init.sh\n@@ -6,5 +6,8 @@ set -euxo pipefail\n # Install prettier\n npm ci\n \n+# Install taplo\n+cargo install taplo-cli --locked\n+\n # Install the pre-commit hook\n ln -s ../../.githooks/pre-commit .git/hooks/pre-commit\ndiff --git a/website/.vitepress/config.mts b/website/.vitepress/config.mts\nindex ffc757bf..3a6f3ae1 100644\n--- a/website/.vitepress/config.mts\n+++ b/website/.vitepress/config.mts\n@@ -303,6 +303,10 @@ export default defineConfig({\n                                     text: \"finish_fn\",\n                                     link: \"/reference/builder/member/finish_fn\",\n                                 },\n+                                {\n+                                    text: \"getters \ud83d\udd2c\",\n+                                    link: \"/reference/builder/member/getters\",\n+                                },\n                                 {\n                                     text: \"into\",\n                                     link: \"/reference/builder/member/into\",\ndiff --git a/website/src/guide/contributing.md b/website/src/guide/contributing.md\nindex 6d9e9dce..ba31c906 100644\n--- a/website/src/guide/contributing.md\n+++ b/website/src/guide/contributing.md\n@@ -14,6 +14,8 @@ However, even though desirable, creating an issue before making a pull request i\n \n This repository is a regular [`cargo` workspace](https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html). Just fork it and do the usual `cargo` business.\n \n+Additionally, an init script is provided: `./scripts/init.sh`. This will install required dependencies and set up the commit hooks for our CI.\n+\n ## Testing\n \n Test your changes with `cargo test`. You may add new tests to the `bon/tests/integration` folder.\ndiff --git a/website/src/reference/builder.md b/website/src/reference/builder.md\nindex 9f439428..742e8712 100644\n--- a/website/src/reference/builder.md\n+++ b/website/src/reference/builder.md\n@@ -25,6 +25,7 @@ These attributes are placed on a `struct` field or `fn` argument.\n | [`default`](./builder/member/default)              | Makes the member optional with a default value                   |\n | [`field`](./builder/member/field)                  | Defines a private field on the builder without setters           |\n | [`finish_fn`](./builder/member/finish_fn)          | Makes the member a positional argument on the finishing function |\n+| [`getter`](./builder/member/getter)                | Makes the member have getter method for `&T`                     |\n | [`into`](./builder/member/into)                    | Changes the signature of the setters to accept `impl Into<T>`    |\n | [`name`](./builder/member/name)                    | Overrides the name of the member used in the builder's API       |\n | [`overwritable` \ud83d\udd2c](./builder/member/overwritable) | Allows calling setters for the same member repeatedly            |\ndiff --git a/website/src/reference/builder/member/getter.md b/website/src/reference/builder/member/getter.md\nnew file mode 100644\nindex 00000000..57081ef3\n--- /dev/null\n+++ b/website/src/reference/builder/member/getter.md\n@@ -0,0 +1,77 @@\n+# `getter` :microscope:\n+\n+**Applies to:** <Badge type=\"warning\" text=\"struct fields\"/> <Badge type=\"warning\" text=\"function arguments\"/> <Badge type=\"warning\" text=\"method arguments\"/>\n+\n+Allows getting a reference to an already set member.\n+\n+::: danger \ud83d\udd2c **Experimental**\n+\n+This attribute is available under the cargo feature `experimental-getter`. Breaking changes may occur between **minor** releases but not between patch releases.\n+\n+The fate of this feature depends on your feedback in the tracking issue [#149](https://github.com/elastio/bon/issues/221). Please, let us know if you have a use case for this attribute!\n+\n+:::\n+\n+This attribute now makes the following possible:\n+\n+```rust\n+#[derive(bon::Builder)]\n+struct Example {\n+    #[builder(getter)] // [!code highlight]\n+    x: u32,\n+}\n+\n+let builder = Example::builder().x(1);\n+\n+let x_ref = builder.get_x(); // [!code highlight]\n+\n+builder.build();\n+```\n+\n+## Config\n+\n+`getter` is configured quite similarly to [`setters`](./setters).\n+\n+You can specify any combination of a `name`, `vis`, and `doc` for the getter, or you can omit all of them and use it as a flag to inherit default values for your getters.\n+\n+### Example\n+\n+```rust\n+#[derive(bon::Builder)]\n+struct Example {\n+    #[builder(getter(name = get_my_member, vis = \"pub(crate)\", doc { // [!code highlight]\n+        /// `get_my_member` gets a ref to the member // [!code highlight]\n+    }))] // [!code highlight]\n+    member: u32, // [!code highlight]\n+}\n+\n+// Name of `some_fn` that accepts the non-None value was overridden\n+let builder = Example::builder().member(2);\n+\n+let my_member = builder.get_my_member(); // [!code highlight]\n+```\n+\n+## `name`\n+\n+The default name for getters are chosen according to the following rules:\n+\n+| Member type | Default        |\n+| ----------- | -------------- |\n+| Required    | `get_{member}` |\n+| Optional    | `get_{member}` |\n+\n+## `vis`\n+\n+The visibility must be enclosed with quotes. Use `\"\"` or [`\"pub(self)\"`](https://doc.rust-lang.org/reference/visibility-and-privacy.html#pubin-path-pubcrate-pubsuper-and-pubself) for private visibility.\n+\n+The default visibility is the same as the visibility of the [`builder_type`](../top-level/builder_type#vis), which in turn, defaults to the visibility of the underlying `struct` or `fn`.\n+\n+## `doc`\n+\n+Simple documentation is generated by default. The syntax of this attribute expects a block with doc comments.\n+\n+```attr\n+doc {\n+    /// Doc comments\n+}\n+```\n", "test_patch": "diff --git a/bon/tests/integration/builder/attr_getter.rs b/bon/tests/integration/builder/attr_getter.rs\nnew file mode 100644\nindex 00000000..90c8d995\n--- /dev/null\n+++ b/bon/tests/integration/builder/attr_getter.rs\n@@ -0,0 +1,67 @@\n+use crate::prelude::*;\n+\n+#[test]\n+fn test_struct() {\n+    #[derive(Debug, Builder)]\n+    #[builder(derive(Debug, Clone))]\n+    #[allow(dead_code)]\n+    struct Sut<T> {\n+        #[builder(start_fn)]\n+        x1: u32,\n+\n+        #[builder(getter(name = x2_with_custom_name))]\n+        x2: String,\n+\n+        #[builder(getter(vis = \"pub(crate)\", doc {\n+            /// Docs on the getter\n+        }))]\n+        x3: u32,\n+\n+        #[builder(into, getter(name = x5, vis = \"pub(crate)\", doc {\n+            /// The name is a lie\n+        }))]\n+        x4_but_its_actually_5: String,\n+\n+        not_a_getter: u32,\n+\n+        #[builder(getter)]\n+        generic_option_getter: Option<T>,\n+\n+        x6: (),\n+    }\n+\n+    #[allow(clippy::redundant_clone)]\n+    let sut = Sut::<()>::builder(0u32).clone();\n+\n+    let actual = sut.x2(\"2\".to_owned()).x3(3);\n+\n+    let actual = actual.x4_but_its_actually_5(\"4\".to_owned());\n+    let x5 = actual.x5();\n+    assert_eq!(x5, \"4\");\n+\n+    let actual = actual.not_a_getter(5).x6(());\n+\n+    let x2 = actual.x2_with_custom_name();\n+    assert_eq!(x2, \"2\");\n+\n+    let x3 = actual.get_x3();\n+    assert_eq!(x3, &3);\n+\n+    let actual = actual.maybe_generic_option_getter(None);\n+\n+    let gen_opt_get = actual.get_generic_option_getter();\n+    assert_eq!(gen_opt_get, None);\n+\n+    assert_debug_eq(\n+        &actual,\n+        expect![[r#\"\n+            SutBuilder {\n+                x1: 0,\n+                x2: \"2\",\n+                x3: 3,\n+                x4_but_its_actually_5: \"4\",\n+                not_a_getter: 5,\n+                x6: (),\n+            }\"#]],\n+    );\n+}\ndiff --git a/bon/tests/integration/builder/mod.rs b/bon/tests/integration/builder/mod.rs\nindex 8965f559..aa15c51c 100644\n--- a/bon/tests/integration/builder/mod.rs\n+++ b/bon/tests/integration/builder/mod.rs\n@@ -3,6 +3,7 @@ mod attr_crate;\n mod attr_default;\n mod attr_derive;\n mod attr_field;\n+mod attr_getter;\n mod attr_into;\n mod attr_on;\n mod attr_overwritable;\n", "problem_statement": "Support getters for already set members\nHi, Bon is my favorite crate and I'm using it as the backbone for a complex pipeline internal framework.\r\n\r\nIn essence, my pipeline works like this:\r\n\r\n```rust\r\n        pipeline\r\n            .pass(the_first_step)\r\n            .pass(another_step)\r\n            .pass(some_other_step)\r\n            .pass(the_last_step)\r\n```\r\n\r\nEach step of the pipeline is (simplified) implemented like this:\r\n\r\n```rust\r\npub async fn the_first_step<S>(\r\n    Payload(id): Payload,\r\n    PgConn(pg_conn): PgConn,\r\n    State(builder): State<NewDbRowBuilder<S>>,\r\n) -> Result<NewDbRowBuilder<SetTheFirstStepValue<S>>, crate::Error>\r\nwhere\r\n    S: new_db_row_builder::State,\r\n    S::TheFirstStepValue: IsUnset,\r\n{\r\n   // some work that connects to api, db, etc, and produces a value\r\n\r\n    Ok(builder.the_first_step_value(my_awesome_value_i_fetched))\r\n}\r\n```\r\n\r\nThe types of the pipeline enforce that the state produced by a prior pass must be fed into the pass under it, so you can't accidentally order anything wrong. I _love_ using Bon to back this pipeline system and allow me to make each pass specify what the conditions of its state should be. \r\n\r\nWhere I am struggling is that sometimes I run into a weird, limiting situation.\r\n\r\nI have one step in my pipeline that scrapes some information off of a user in my DB, processes it, then sets it in the builder (because this value must go in the row that's being built). Then, in another pass of the pipeline, I must read processed data out and use it to compute another value that also goes on my row. Given the current implementation of Bon (while following the rules and not directly implementing unsafe methods on generated code), I have three solutions:\r\n\r\n1. Take this processed value, clone it and set it into the state, then return a tuple of the state and the cloned value. This is bad to me because the passes of my pipeline become much less generic as they now must take a tuple of the built state and any previously obtained values and drill them through the entire way\r\n2. Do what I'm doing currently, and maintain a cache that passes can use to set values and retrieve them later. This works (and is fairly generic), but I lose the ability to _safely_ do the guards like `IsSet` because this cache has to be completely detached from `bon`\r\n3. Use `builder(field)` on these fields I want to cache. This also works, but it has the same problem as before where I lose the ability to do guards on the passes which is what I love so much about bon.\r\n\r\nI noticed in 3.1.1 you removed the non-determinism of generated privates. After seeing this, being able to add methods that take in `&self` and return a reference to a value in the builder state would be incredible and make my pipeline work exactly how I want it to. This long winded context finally leads to my question (the one in the title)...\r\n\r\nHow fundamentally unsafe is this? Taking a look at the expanded macro from `builder` everything I can see indicates that adding read only accessor methods that do not move out of the tuple or clone the data would be completely safe, but I figured it would be well worth opening an issue for this because this will be running in production code where a mistake like this will not be acceptable. \r\n\r\nThank you!\r\n\r\n<!-- Please keep this note for the community at the end of the issue -->\r\n\r\n### A note for the community from the maintainers\r\n\r\nPlease vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to help the maintainers with prioritizing it. You may add a comment describing your real use case related to this issue for us to better understand the problem domain.\r\n\n", "hints_text": "Hi, thank you for creating the issue! Accessing the private fields of the builder (except for `#[builder(field)]`) is definitely wrong, since those internals may change between patch versions. The only unsafe code generated by builder macros at the moment is `unwrap_unchecked` for required members in the finishing function. So the only way this may break is if you set the required field inside the builder to `None` by directly accessing its internals. Vending a readonly reference to the internal field is completely fine, however, there isn't an official API to do that in the current version of `bon` (3.1.1).\r\n\r\nBut.. I was actually planning to add an ability to generate getters for already set members as the next feature after `#[builder(field)]`. I imagine it like this. There will be a `#[builder(getter)]` annotation that will generate a simple by-readonly-reference getter with no magic, that has the name `get_{member}`. It will be possible to override its visibility, docs and name with the usual attributes `#[builder(getter(name = ..., vis = ..., doc { ... })]`. The default visibility will be the same as it would be for setters.\r\n\r\nIn future increments, I plan to provide the ability to generate getters for mutable references, the ability to do `AsRef` and `Deref` coercions, and custom conversions similar to `#[builder(with)]`, but it'll be `#[builder(getter(with))]`.\nFantastic to hear! Thank you. I am fairly familiar with `bon`'s internals, and if you haven't started I'd be happy to take on and implement this.\nYeah, I've been procrastinating on this a bit given the Stalker 2 release \ud83d\ude33. I haven't started with this feature yet, so feel free to take a stab at this \ud83d\udc4d \nHaha, have fun. Feel free to assign me this. Will get started now.\nAt a high level, how do you feel about having the getters return an `impl Borrow<T>` instead of just `&T`? The main reason I'm suggesting this is because the ergonomics could be a bit better for `String`s and `Vec`s which would then return `&str` and `&[T]` accordingly.", "created_at": "2024-11-28 21:23:45", "merge_commit_sha": "4ea25abf13dab36628deb2772d00fda57a790729", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['prettier', '.github/workflows/ci.yml']", "['test-unstable (beta)', '.github/workflows/ci.yml']"], ["['runtime-benchmarks (args_5)', '.github/workflows/ci.yml']", "['runtime-benchmarks (args_10)', '.github/workflows/ci.yml']"], ["['test-unstable (nightly)', '.github/workflows/ci.yml']", "['cargo-doc (ubuntu)', '.github/workflows/ci.yml']"], ["['test-msrv (windows)', '.github/workflows/ci.yml']", "['cargo-machete', '.github/workflows/ci.yml']"], ["['test-stable (ubuntu)', '.github/workflows/ci.yml']", "['taplo-fmt', '.github/workflows/ci.yml']"], ["['test-stable (macos)', '.github/workflows/ci.yml']", "['test-stable (windows)', '.github/workflows/ci.yml']"], ["['test-msrv (macos)', '.github/workflows/ci.yml']", "['runtime-benchmarks (args_10_structs)', '.github/workflows/ci.yml']"]]}
{"repo": "zng-ui/zng", "instance_id": "zng-ui__zng-198", "base_commit": "7332b56a3930841b59093793589846526e3d8513", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a052af0c7..830cc82be 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -12,6 +12,14 @@\n * Deprecate `NilAnimationObserver`, use `()` now.\n * Add `ForceAnimationController` to force important animations to run when animations are disabled on the system.\n * Fix crash handler passing app name twice as command line arguments.\n+* **Breaking** Implemented new syntax for the localization scrapper to separate standalone notes per file:\n+    - `// l10n-file-### {note}` only adds the note to the `template/file.ftl`.\n+    - `// l10n-*-### {note}` adds the note to all files that match the glob pattern (`template/*.ftl`).\n+    - The old syntax `// l10n-### {note}` is still supported, but now it is equivalent to `// l10n--###` that\n+      matches the default `template.ftl` file only.\n+    - Note that this is only a breaking change for dependents of `zng-l10n-scraper`. Normal users (cargo install)\n+      must update the tool to scrap using the new syntax, comments with the new file pattern matcher are ignored\n+      by older scrappers.\n \n # 0.5.0\n \ndiff --git a/crates/zng-ext-l10n/src/lib.rs b/crates/zng-ext-l10n/src/lib.rs\nindex 95b5b29d5..7265e0c46 100644\n--- a/crates/zng-ext-l10n/src/lib.rs\n+++ b/crates/zng-ext-l10n/src/lib.rs\n@@ -115,9 +115,11 @@ impl AppExtension for L10nManager {\n /// matches this macro name and the two first input literals, avoid renaming this macro to support scrapping, otherwise you will\n /// have to declare the template file manually.\n ///\n-/// The scrapper also has some support for comments, if the previous code line from a [`l10n!`] call is a comment starting with\n+/// The scrapper can also scrap comments, if the previous code line from a [`l10n!`] call is a comment starting with\n /// prefix `l10n-# ` the text the follows is collected, same for a comment in the same line of the [`l10n!`] call. Sections\n-/// can be declared using `l10n-##` and standalone notes can be added to the top of the template file from anywhere using `l10n-###`.\n+/// can be declared using `l10n-##` and standalone notes can be added to the top of the template file from anywhere using\n+/// `l10n-{file_pattern}-###`, file pattern can be omitted, `l10n-###` is equivalent to `l10n--###` that matches the localization\n+/// template used when no file is specified.\n ///\n /// ```\n /// # use zng_ext_l10n::*;\ndiff --git a/crates/zng-l10n-scraper/src/main.rs b/crates/zng-l10n-scraper/src/main.rs\nindex 4fac38ef4..56d092062 100644\n--- a/crates/zng-l10n-scraper/src/main.rs\n+++ b/crates/zng-l10n-scraper/src/main.rs\n@@ -21,7 +21,7 @@ struct Args {\n     #[arg(short, long)]\n     output: PathBuf,\n \n-    /// Custom macro names, comma separated\n+    /// Custom l10n macro names, comma separated\n     #[arg(short, long, default_value = \"\")]\n     macros: String,\n \ndiff --git a/crates/zng-l10n-scraper/src/scraper.rs b/crates/zng-l10n-scraper/src/scraper.rs\nindex ebf77f44b..d4ef24db7 100644\n--- a/crates/zng-l10n-scraper/src/scraper.rs\n+++ b/crates/zng-l10n-scraper/src/scraper.rs\n@@ -37,7 +37,7 @@ fn scrape_files(buf: &mut Vec<PathBuf>, custom_macro_names: &[&str]) -> io::Resu\n     buf.par_drain(..).map(|f| scrape_file(f, custom_macro_names)).reduce(\n         || {\n             Ok(FluentTemplate {\n-                notes: String::new(),\n+                notes: vec![],\n                 entries: vec![],\n             })\n         },\n@@ -62,8 +62,7 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n         s = &s[i..];\n     }\n \n-    let mut l10n_notes = String::new();\n-    let mut last_note_line = 0;\n+    let mut l10n_notes = vec![];\n     let mut l10n_section = Arc::new(String::new());\n \n     let mut output: Vec<FluentEntry> = vec![];\n@@ -97,36 +96,39 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n             Expect::CommentOrMacroName => match token.kind {\n                 rustc_lexer::TokenKind::LineComment => {\n                     let c = s[..token.len].trim().trim_start_matches('/').trim_start();\n-                    if let Some(c) = c.strip_prefix(\"l10n-###\") {\n-                        if !l10n_notes.is_empty() && (line - last_note_line) > 1 {\n-                            l10n_notes.push('\\n');\n-                        }\n-                        l10n_notes.push_str(c.trim());\n-                        l10n_notes.push('\\n');\n-\n-                        last_note_line = line;\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-##\") {\n-                        l10n_section = Arc::new(c.trim().to_owned())\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-#\") {\n-                        let c = c.trim_start();\n-\n-                        // comment still on the last already inserted entry lines\n-                        if last_entry_line == line && !output.is_empty() {\n-                            let last = output.len() - 1;\n-                            if !output[last].comments.is_empty() {\n-                                output[last].comments.push('\\n');\n-                            }\n-                            output[last].comments.push_str(c);\n-                        } else {\n-                            if !entry.comments.is_empty() {\n-                                if (line - last_comment_line) > 1 {\n-                                    entry.comments.clear();\n-                                } else {\n-                                    entry.comments.push('\\n');\n+\n+                    if let Some(c) = c.strip_prefix(\"l10n-\") {\n+                        if let Some(i) = c.find(\"###\") {\n+                            let file_name = c[..i].trim_end_matches('-');\n+                            let c = &c[i + \"###\".len()..];\n+\n+                            l10n_notes.push(FluentNote {\n+                                file: file_name.to_owned(),\n+                                note: c.trim().to_owned(),\n+                            });\n+                        } else if let Some(c) = c.strip_prefix(\"##\") {\n+                            l10n_section = Arc::new(c.trim().to_owned())\n+                        } else if let Some(c) = c.strip_prefix('#') {\n+                            let c = c.trim_start();\n+\n+                            // comment still on the last already inserted entry lines\n+                            if last_entry_line == line && !output.is_empty() {\n+                                let last = output.len() - 1;\n+                                if !output[last].comments.is_empty() {\n+                                    output[last].comments.push('\\n');\n                                 }\n+                                output[last].comments.push_str(c);\n+                            } else {\n+                                if !entry.comments.is_empty() {\n+                                    if (line - last_comment_line) > 1 {\n+                                        entry.comments.clear();\n+                                    } else {\n+                                        entry.comments.push('\\n');\n+                                    }\n+                                }\n+                                entry.comments.push_str(c);\n+                                last_comment_line = line;\n                             }\n-                            entry.comments.push_str(c);\n-                            last_comment_line = line;\n                         }\n                     }\n                 }\n@@ -185,24 +187,26 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n                     // comment inside macro\n \n                     let c = s[..token.len].trim().trim_start_matches('/').trim_start();\n-                    if let Some(c) = c.strip_prefix(\"l10n-###\") {\n-                        if !l10n_notes.is_empty() && (line - last_note_line) > 1 {\n-                            l10n_notes.push('\\n');\n-                        }\n-                        l10n_notes.push_str(c.trim());\n-                        l10n_notes.push('\\n');\n+                    if let Some(c) = c.strip_prefix(\"l10n-\") {\n+                        if let Some(i) = c.find(\"###\") {\n+                            let file_name = c[..i].trim_end_matches('-');\n+                            let c = &c[i + \"###\".len()..];\n+\n+                            l10n_notes.push(FluentNote {\n+                                file: file_name.to_owned(),\n+                                note: c.trim().to_owned(),\n+                            });\n+                        } else if let Some(c) = c.strip_prefix(\"##\") {\n+                            l10n_section = Arc::new(c.trim().to_owned())\n+                        } else if let Some(c) = c.strip_prefix('#') {\n+                            let c = c.trim_start();\n \n-                        last_note_line = line;\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-##\") {\n-                        l10n_section = Arc::new(c.trim().to_owned())\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-#\") {\n-                        let c = c.trim_start();\n-\n-                        if !entry.comments.is_empty() {\n-                            entry.comments.push('\\n');\n+                            if !entry.comments.is_empty() {\n+                                entry.comments.push('\\n');\n+                            }\n+                            entry.comments.push_str(c);\n+                            last_comment_line = line;\n                         }\n-                        entry.comments.push_str(c);\n-                        last_comment_line = line;\n                     }\n                 }\n                 rustc_lexer::TokenKind::Whitespace => {}\n@@ -219,24 +223,26 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n                     // comment inside macro\n \n                     let c = s[..token.len].trim().trim_start_matches('/').trim_start();\n-                    if let Some(c) = c.strip_prefix(\"l10n-###\") {\n-                        if !l10n_notes.is_empty() && (line - last_note_line) > 1 {\n-                            l10n_notes.push('\\n');\n-                        }\n-                        l10n_notes.push_str(c.trim());\n-                        l10n_notes.push('\\n');\n-\n-                        last_note_line = line;\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-##\") {\n-                        l10n_section = Arc::new(c.trim().to_owned())\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-#\") {\n-                        let c = c.trim_start();\n+                    if let Some(c) = c.strip_prefix(\"l10n-\") {\n+                        if let Some(i) = c.find(\"###\") {\n+                            let file_name = c[..i].trim_end_matches('-');\n+                            let c = &c[i + \"###\".len()..];\n+\n+                            l10n_notes.push(FluentNote {\n+                                file: file_name.to_owned(),\n+                                note: c.trim().to_owned(),\n+                            });\n+                        } else if let Some(c) = c.strip_prefix(\"##\") {\n+                            l10n_section = Arc::new(c.trim().to_owned())\n+                        } else if let Some(c) = c.strip_prefix('#') {\n+                            let c = c.trim_start();\n \n-                        if !entry.comments.is_empty() {\n-                            entry.comments.push('\\n');\n+                            if !entry.comments.is_empty() {\n+                                entry.comments.push('\\n');\n+                            }\n+                            entry.comments.push_str(c);\n+                            last_comment_line = line;\n                         }\n-                        entry.comments.push_str(c);\n-                        last_comment_line = line;\n                     }\n                 }\n                 rustc_lexer::TokenKind::Whitespace => {}\n@@ -284,24 +290,26 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n                     // comment inside macro\n \n                     let c = s[..token.len].trim().trim_start_matches('/').trim_start();\n-                    if let Some(c) = c.strip_prefix(\"l10n-###\") {\n-                        if !l10n_notes.is_empty() && (line - last_note_line) > 1 {\n-                            l10n_notes.push('\\n');\n-                        }\n-                        l10n_notes.push_str(c.trim());\n-                        l10n_notes.push('\\n');\n+                    if let Some(c) = c.strip_prefix(\"l10n-\") {\n+                        if let Some(i) = c.find(\"###\") {\n+                            let file_name = c[..i].trim_end_matches('-');\n+                            let c = &c[i + \"###\".len()..];\n+\n+                            l10n_notes.push(FluentNote {\n+                                file: file_name.to_owned(),\n+                                note: c.trim().to_owned(),\n+                            });\n+                        } else if let Some(c) = c.strip_prefix(\"##\") {\n+                            l10n_section = Arc::new(c.trim().to_owned())\n+                        } else if let Some(c) = c.strip_prefix('#') {\n+                            let c = c.trim_start();\n \n-                        last_note_line = line;\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-##\") {\n-                        l10n_section = Arc::new(c.trim().to_owned())\n-                    } else if let Some(c) = c.strip_prefix(\"l10n-#\") {\n-                        let c = c.trim_start();\n-\n-                        if !entry.comments.is_empty() {\n-                            entry.comments.push('\\n');\n+                            if !entry.comments.is_empty() {\n+                                entry.comments.push('\\n');\n+                            }\n+                            entry.comments.push_str(c);\n+                            last_comment_line = line;\n                         }\n-                        entry.comments.push_str(c);\n-                        last_comment_line = line;\n                     }\n                 }\n                 rustc_lexer::TokenKind::Whitespace => {}\n@@ -323,6 +331,16 @@ fn scrape_file(file: PathBuf, custom_macro_names: &[&str]) -> io::Result<FluentT\n     })\n }\n \n+/// Represents a standalone note, declared using `// l10n-{file}-### {note}` or `l10n-### {note}`.\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct FluentNote {\n+    /// Localization file name pattern where the note must be added.\n+    pub file: String,\n+\n+    /// The note.\n+    pub note: String,\n+}\n+\n /// Represents one call to `l10n!` or similar macro in a Rust code file.\n ///\n /// Use [`scrape_fluent_text`] to collect entries.\n@@ -350,8 +368,8 @@ pub struct FluentEntry {\n /// Use [`scrape_fluent_text`] to collect entries.\n #[derive(Default)]\n pub struct FluentTemplate {\n-    /// Scraped note comments `// l10n-### `.\n-    pub notes: String,\n+    /// Scraped standalone note comments.\n+    pub notes: Vec<FluentNote>,\n \n     /// Scraped entries.\n     ///\n@@ -361,12 +379,7 @@ pub struct FluentTemplate {\n impl FluentTemplate {\n     /// Append `other` to `self`.\n     pub fn extend(&mut self, other: Self) {\n-        if self.notes.is_empty() {\n-            self.notes = other.notes;\n-        } else if !other.notes.is_empty() {\n-            self.notes.push('\\n');\n-            self.notes.push_str(&other.notes);\n-        }\n+        self.notes.extend(other.notes);\n         self.entries.extend(other.entries);\n     }\n \n@@ -489,8 +502,19 @@ impl FluentTemplate {\n                 let mut out = select_l10n_file(&entry.file)?;\n \n                 if !self.notes.is_empty() {\n-                    for line in self.notes.lines() {\n-                        out.write_fmt(format_args!(\"### {line}\\n\"))?;\n+                    for n in &self.notes {\n+                        let matches_file = if n.file.contains('*') {\n+                            match glob::Pattern::new(&n.file) {\n+                                Ok(b) => b.matches(&entry.file),\n+                                Err(e) => return Err(io::Error::new(io::ErrorKind::InvalidInput, e)),\n+                            }\n+                        } else {\n+                            n.file == entry.file\n+                        };\n+\n+                        if matches_file {\n+                            out.write_fmt(format_args!(\"### {}\\n\", n.note))?;\n+                        }\n                     }\n                     out.write_all(\"\\n\".as_bytes())?;\n                 }\ndiff --git a/crates/zng/src/l10n.rs b/crates/zng/src/l10n.rs\nindex b3cfd7ca4..271434863 100644\n--- a/crates/zng/src/l10n.rs\n+++ b/crates/zng/src/l10n.rs\n@@ -68,7 +68,7 @@\n //! use zng::prelude::*;\n //! # let _scope = APP.defaults();\n //!\n-//! // l10n-### This standalone comment is added to all scraped template files.\n+//! // l10n-### This standalone comment is added to the scraped template file.\n //!\n //! let click_count = var(0u32);\n //! # let _ =\ndiff --git a/examples/localize.rs b/examples/localize.rs\nindex 0f646dc3a..db2236d90 100644\n--- a/examples/localize.rs\n+++ b/examples/localize.rs\n@@ -15,8 +15,10 @@ use zng::{\n     widget::node::presenter,\n };\n \n-// l10n-### Localize Example\n-// l10n-### This standalone comment is added to all scraped template files.\n+// l10n-*-### Localize Example\n+// l10n-*-### This standalone comment is added to all scraped template files.\n+// l10n-### This standalone comment is only added to the default file.\n+// l10n-msg-### This standalone comment is only added to the `msg` file.\n \n // Run this command to scrap template:\n // cargo run -p zng-l10n-scraper -- -i\"examples/localize*\" -o\"examples/res/localize\"\n@@ -167,4 +169,4 @@ fn locale_menu() -> impl UiNode {\n     }\n }\n \n-// l10n-### Another standalone comment, also added to the top of all template files.\n+// l10n--### Another standalone comment, also added to the top of the template file.\ndiff --git a/examples/res/localize/pseudo-mirr.ftl b/examples/res/localize/pseudo-mirr.ftl\nindex c70f08bca..952c7d6a8 100644\n--- a/examples/res/localize/pseudo-mirr.ftl\n+++ b/examples/res/localize/pseudo-mirr.ftl\n@@ -1,7 +1,7 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the default file.\n+### Another standalone comment, also added to the top of the template file.\n \n # icon:\n #     first syllable of \"Localize\"\ndiff --git a/examples/res/localize/pseudo-mirr/msg.ftl b/examples/res/localize/pseudo-mirr/msg.ftl\nindex 1ebe9d2cf..ce5ec7937 100644\n--- a/examples/res/localize/pseudo-mirr/msg.ftl\n+++ b/examples/res/localize/pseudo-mirr/msg.ftl\n@@ -1,6 +1,5 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the `msg` file.\n \n click-count = \u2183\u0285\u0131\u0254\u029e\u01ddp {$n} \u0287\u0131\u026f\u01dds\ndiff --git a/examples/res/localize/pseudo-wide.ftl b/examples/res/localize/pseudo-wide.ftl\nindex 5aeef6688..40d4f56ab 100644\n--- a/examples/res/localize/pseudo-wide.ftl\n+++ b/examples/res/localize/pseudo-wide.ftl\n@@ -1,7 +1,7 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the default file.\n+### Another standalone comment, also added to the top of the template file.\n \n # icon:\n #     first syllable of \"Localize\"\ndiff --git a/examples/res/localize/pseudo-wide/msg.ftl b/examples/res/localize/pseudo-wide/msg.ftl\nindex df5f57eb2..992406c17 100644\n--- a/examples/res/localize/pseudo-wide/msg.ftl\n+++ b/examples/res/localize/pseudo-wide/msg.ftl\n@@ -1,6 +1,5 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the `msg` file.\n \n click-count = \u0187\u0140i\u0188\u0137ee\u1e13 {$n} \u0167i\u1e3fee\u015f\ndiff --git a/examples/res/localize/pseudo.ftl b/examples/res/localize/pseudo.ftl\nindex a10c2982a..2faca8463 100644\n--- a/examples/res/localize/pseudo.ftl\n+++ b/examples/res/localize/pseudo.ftl\n@@ -1,7 +1,7 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the default file.\n+### Another standalone comment, also added to the top of the template file.\n \n # icon:\n #     first syllable of \"Localize\"\ndiff --git a/examples/res/localize/pseudo/msg.ftl b/examples/res/localize/pseudo/msg.ftl\nindex 13770df54..a1ca683e5 100644\n--- a/examples/res/localize/pseudo/msg.ftl\n+++ b/examples/res/localize/pseudo/msg.ftl\n@@ -1,6 +1,5 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the `msg` file.\n \n click-count = \u0187\u0140i\u0188\u0137e\u1e13 {$n} \u0167i\u1e3fe\u015f\ndiff --git a/examples/res/localize/template.ftl b/examples/res/localize/template.ftl\nindex c227ec164..edaa60d0d 100644\n--- a/examples/res/localize/template.ftl\n+++ b/examples/res/localize/template.ftl\n@@ -1,7 +1,7 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the default file.\n+### Another standalone comment, also added to the top of the template file.\n \n # icon:\n #     first syllable of \"Localize\"\ndiff --git a/examples/res/localize/template/msg.ftl b/examples/res/localize/template/msg.ftl\nindex a91c2b2e1..f2236ef63 100644\n--- a/examples/res/localize/template/msg.ftl\n+++ b/examples/res/localize/template/msg.ftl\n@@ -1,6 +1,5 @@\n ### Localize Example\n ### This standalone comment is added to all scraped template files.\n-### \n-### Another standalone comment, also added to the top of all template files.\n+### This standalone comment is only added to the `msg` file.\n \n click-count = Clicked {$n} times\n", "test_patch": "", "problem_statement": "Implement scrapping of standalone notes for a single localization file\nRight now the code below will generate a copy of \"### Standalone Note\" in both \"foo.ftl\" and \"bar.ftl\" templates, this is documented, but it is not very useful.\r\n\r\n```rust\r\nfn foo() {\r\n    let _ = l10n!(\"foo/id\", \"foo\");\r\n}\r\n\r\nfn bar() {\r\n   // l10n-### Standalone Note\r\n    let _ = l10n!(\"bar/id\", \"bar\");\r\n}\r\n```\r\n\r\nImplement `// l10n-{file}-###` syntax to fix this, `// l10n-*-###` copies the comment to all files, `// l10n-###` copies the comment to the *unamed* template, that is `template.ftl`.\n", "hints_text": "", "created_at": "2024-05-12 03:52:03", "merge_commit_sha": "f709d5176fc5e48f571ba1cb8992243b0774fd4a", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['check-release', '.github/workflows/ci.yml']", "['check', '.github/workflows/ci.yml']"], ["['test', '.github/workflows/ci.yml']", "['test-all', '.github/workflows/ci.yml']"]]}
{"repo": "zng-ui/zng", "instance_id": "zng-ui__zng-154", "base_commit": "a5c20758c51a9a6a226ca6188331865e28f4cdba", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex ecec97283..797b852ae 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -12,6 +12,34 @@\n * Add `OPEN_TITLE_BAR_CONTEXT_MENU_CMD` to window API.\n * Fix `WIDGET.border().offsets()` not including the innermost border offset.\n * Add `WindowVars::enabled_buttons` to window API.\n+* **Breaking** Fix when/property assign expansion order.\n+    - When blocks now expand in the same declaration order, before they always expanded after all property assigns.\n+```rust\n+// code like this incorrectly builds in v0.4:\n+fn version_0_4() -> impl UiNode {\n+    let can_move = var(true);\n+    Container! {\n+        when *#{can_move} {\n+            mouse::cursor = mouse::CursorIcon::Move;\n+        }\n+        mouse::on_mouse_down = hn!(can_move, |_| {\n+            let _use = &can_move;\n+        });\n+    }\n+}\n+// now in v0.5 the value must be cloned before the last move:\n+fn version_0_5() -> impl UiNode {\n+    let can_move = var(true);\n+    Container! {\n+        when *#{can_move.clone()} {\n+            mouse::cursor = mouse::CursorIcon::Move;\n+        }\n+        mouse::on_mouse_down = hn!(|_| {\n+            let _use = &can_move;\n+        });\n+    }\n+}\n+```\n \n # 0.4.0\n \ndiff --git a/crates/zng-app-proc-macros/src/widget.rs b/crates/zng-app-proc-macros/src/widget.rs\nindex 73236684f..648164534 100644\n--- a/crates/zng-app-proc-macros/src/widget.rs\n+++ b/crates/zng-app-proc-macros/src/widget.rs\n@@ -4,7 +4,7 @@ use syn::{ext::IdentExt, parse::Parse, spanned::Spanned, *};\n \n use crate::{\n     util::{self, parse_outer_attrs, ErrorRecoverable, Errors},\n-    widget_util::{self, WgtProperty, WgtWhen},\n+    widget_util::{self, WgtItem, WgtProperty, WgtWhen},\n };\n \n lazy_static! {\n@@ -373,14 +373,12 @@ impl Parse for Args {\n \n struct Properties {\n     errors: Errors,\n-    properties: Vec<WgtProperty>,\n-    whens: Vec<WgtWhen>,\n+    items: Vec<WgtItem>,\n }\n impl Parse for Properties {\n     fn parse(input: parse::ParseStream) -> Result<Self> {\n         let mut errors = Errors::default();\n-        let mut properties = vec![];\n-        let mut whens = vec![];\n+        let mut items = vec![];\n \n         while !input.is_empty() {\n             let attrs = parse_outer_attrs(input, &mut errors);\n@@ -388,7 +386,7 @@ impl Parse for Properties {\n             if input.peek(widget_util::keyword::when) {\n                 if let Some(mut when) = WgtWhen::parse(input, &mut errors) {\n                     when.attrs = util::Attributes::new(attrs);\n-                    whens.push(when);\n+                    items.push(WgtItem::When(when));\n                 }\n             } else if input.peek(Token![pub])\n                 || input.peek(Ident::peek_any)\n@@ -413,7 +411,7 @@ impl Parse for Properties {\n                                 let _ = input.parse::<TokenTree>();\n                             }\n                         }\n-                        properties.push(p);\n+                        items.push(WgtItem::Property(p));\n                     }\n                     Err(e) => {\n                         let (recoverable, e) = e.recoverable();\n@@ -432,7 +430,7 @@ impl Parse for Properties {\n             }\n         }\n \n-        Ok(Properties { errors, properties, whens })\n+        Ok(Properties { errors, items })\n     }\n }\n \n@@ -457,114 +455,117 @@ pub fn expand_new(args: proc_macro::TokenStream) -> proc_macro::TokenStream {\n \n     let core = util::crate_core();\n \n-    let mut set_props = quote!();\n-    for prop in &p.properties {\n-        set_props.extend(prop_assign(prop, &mut p.errors, false));\n-    }\n+    let mut items = quote!();\n \n-    let mut set_whens = quote!();\n-    for when in &p.whens {\n-        let when_expr = match syn::parse2::<widget_util::WhenExpr>(when.condition_expr.clone()) {\n-            Ok(w) => w,\n-            Err(e) => {\n-                p.errors.push_syn(e);\n-                continue;\n+    for item in &p.items {\n+        match item {\n+            WgtItem::Property(prop) => {\n+                items.extend(prop_assign(prop, &mut p.errors, false));\n             }\n-        };\n-\n-        let mut when_expr_vars = quote!();\n-        let mut inputs = quote!();\n-        for ((property, member), var) in when_expr.inputs {\n-            let (property, generics) = widget_util::split_path_generics(property).unwrap();\n-            let p_ident = &property.segments.last().unwrap().ident;\n-            let p_meta = ident_spanned!(p_ident.span()=> \"{p_ident}_\");\n-            let var_input = ident!(\"{var}_in__\");\n-            let member_ident = ident_spanned!(property.span()=> \"__w_{member}__\");\n-\n-            let member = match member {\n-                widget_util::WhenInputMember::Named(ident) => {\n-                    let ident_str = ident.to_string();\n-                    quote! {\n-                        Named(#ident_str)\n+            WgtItem::When(when) => {\n+                let when_expr = match syn::parse2::<widget_util::WhenExpr>(when.condition_expr.clone()) {\n+                    Ok(w) => w,\n+                    Err(e) => {\n+                        p.errors.push_syn(e);\n+                        continue;\n                     }\n-                }\n-                widget_util::WhenInputMember::Index(i) => quote! {\n-                    Index(#i)\n-                },\n-            };\n+                };\n \n-            macro_rules! quote_call {\n-                (#$mtd:ident ( $($args:tt)* )) => {\n-                    if property.get_ident().is_some() {\n-                        quote! {\n-                            wgt__.#$mtd #generics($($args)*);\n+                let mut when_expr_vars = quote!();\n+                let mut inputs = quote!();\n+                for ((property, member), var) in when_expr.inputs {\n+                    let (property, generics) = widget_util::split_path_generics(property).unwrap();\n+                    let p_ident = &property.segments.last().unwrap().ident;\n+                    let p_meta = ident_spanned!(p_ident.span()=> \"{p_ident}_\");\n+                    let var_input = ident!(\"{var}_in__\");\n+                    let member_ident = ident_spanned!(property.span()=> \"__w_{member}__\");\n+\n+                    let member = match member {\n+                        widget_util::WhenInputMember::Named(ident) => {\n+                            let ident_str = ident.to_string();\n+                            quote! {\n+                                Named(#ident_str)\n+                            }\n                         }\n-                    } else {\n-                        quote! {\n-                            #property::#$mtd #generics(#core::widget::base::WidgetImpl::base(&mut *wgt__), $($args)*);\n+                        widget_util::WhenInputMember::Index(i) => quote! {\n+                            Index(#i)\n+                        },\n+                    };\n+\n+                    macro_rules! quote_call {\n+                        (#$mtd:ident ( $($args:tt)* )) => {\n+                            if property.get_ident().is_some() {\n+                                quote! {\n+                                    wgt__.#$mtd #generics($($args)*);\n+                                }\n+                            } else {\n+                                quote! {\n+                                    #property::#$mtd #generics(#core::widget::base::WidgetImpl::base(&mut *wgt__), $($args)*);\n+                                }\n+                            }\n                         }\n                     }\n+\n+                    let get_meta = quote_call!(#p_meta());\n+\n+                    when_expr_vars.extend(quote! {\n+                        let (#var_input, #var) = {\n+                            let meta__ = #get_meta\n+                            meta__.allowed_in_when_expr();\n+                            meta__.inputs #generics().#member_ident()\n+                        };\n+                    });\n+\n+                    inputs.extend(quote! {\n+                        {\n+                            let meta__ = #get_meta\n+                            #core::widget::builder::WhenInput {\n+                                property: meta__.id(),\n+                                member: #core::widget::builder::WhenInputMember::#member,\n+                                var: #var_input,\n+                                property_default: meta__ .default_fn #generics(),\n+                            }\n+                        },\n+                    });\n                 }\n-            }\n \n-            let get_meta = quote_call!(#p_meta());\n+                let mut assigns = quote!();\n+                for prop in &when.assigns {\n+                    assigns.extend(prop_assign(prop, &mut p.errors, true));\n+                }\n \n-            when_expr_vars.extend(quote! {\n-                let (#var_input, #var) = {\n-                    let meta__ = #get_meta\n-                    meta__.allowed_in_when_expr();\n-                    meta__.inputs #generics().#member_ident()\n-                };\n-            });\n+                let attrs = when.attrs.cfg_and_lints();\n+                let expr = when_expr.expr;\n+                let expr_str = &when.condition_expr_str;\n \n-            inputs.extend(quote! {\n-                {\n-                    let meta__ = #get_meta\n-                    #core::widget::builder::WhenInput {\n-                        property: meta__.id(),\n-                        member: #core::widget::builder::WhenInputMember::#member,\n-                        var: #var_input,\n-                        property_default: meta__ .default_fn #generics(),\n+                let box_expr = quote_spanned! {expr.span()=>\n+                    {\n+                        let expr_var = #core::var::expr_var!{#expr};\n+                        #core::widget::builder::when_condition_expr_var(expr_var)\n                     }\n-                },\n-            });\n-        }\n-\n-        let mut assigns = quote!();\n-        for prop in &when.assigns {\n-            assigns.extend(prop_assign(prop, &mut p.errors, true));\n-        }\n-\n-        let attrs = when.attrs.cfg_and_lints();\n-        let expr = when_expr.expr;\n-        let expr_str = &when.condition_expr_str;\n-\n-        let box_expr = quote_spanned! {expr.span()=>\n-            {\n-                let expr_var = #core::var::expr_var!{#expr};\n-                #core::widget::builder::when_condition_expr_var(expr_var)\n-            }\n-        };\n+                };\n \n-        let source_location = widget_util::source_location(&core, Span::call_site());\n-        set_whens.extend(quote! {\n-            #attrs {\n-                #when_expr_vars\n-                let inputs__ = std::boxed::Box::new([\n-                    #inputs\n-                ]);\n-                #core::widget::base::WidgetImpl::base(&mut *wgt__).start_when_block(\n-                    inputs__,\n-                    #box_expr,\n-                    #expr_str,\n-                    #source_location,\n-                );\n-\n-                #assigns\n-\n-                #core::widget::base::WidgetImpl::base(&mut *wgt__).end_when_block();\n+                let source_location = widget_util::source_location(&core, Span::call_site());\n+                items.extend(quote! {\n+                    #attrs {\n+                        #when_expr_vars\n+                        let inputs__ = std::boxed::Box::new([\n+                            #inputs\n+                        ]);\n+                        #core::widget::base::WidgetImpl::base(&mut *wgt__).start_when_block(\n+                            inputs__,\n+                            #box_expr,\n+                            #expr_str,\n+                            #source_location,\n+                        );\n+\n+                        #assigns\n+\n+                        #core::widget::base::WidgetImpl::base(&mut *wgt__).end_when_block();\n+                    }\n+                });\n             }\n-        });\n+        }\n     }\n \n     let errors = p.errors;\n@@ -574,8 +575,7 @@ pub fn expand_new(args: proc_macro::TokenStream) -> proc_macro::TokenStream {\n             #errors\n \n             #start\n-            #set_props\n-            #set_whens\n+            #items\n             #end\n         }\n     };\ndiff --git a/crates/zng-app-proc-macros/src/widget_util.rs b/crates/zng-app-proc-macros/src/widget_util.rs\nindex 9eab0216e..0c5d08293 100644\n--- a/crates/zng-app-proc-macros/src/widget_util.rs\n+++ b/crates/zng-app-proc-macros/src/widget_util.rs\n@@ -15,6 +15,12 @@ use crate::{\n     wgt_property_attrs::PropertyAttrData,\n };\n \n+/// Represents a property assign or when block.\n+pub enum WgtItem {\n+    Property(WgtProperty),\n+    When(WgtWhen),\n+}\n+\n /// Represents a property assign.\n pub struct WgtProperty {\n     /// Attributes.\ndiff --git a/crates/zng-wgt-markdown/src/resolvers.rs b/crates/zng-wgt-markdown/src/resolvers.rs\nindex fa4ab92c8..cf1f10365 100644\n--- a/crates/zng-wgt-markdown/src/resolvers.rs\n+++ b/crates/zng-wgt-markdown/src/resolvers.rs\n@@ -313,7 +313,7 @@ pub fn try_open_link(args: &LinkArgs) -> bool {\n             opacity = 100.pct();\n             offset = (0, 0);\n         }\n-        when *#{status} == Status::Err {\n+        when *#{status.clone()} == Status::Err {\n             background_color = color_scheme_map(web_colors::DARK_RED.with_alpha(90.pct()), web_colors::PINK.with_alpha(90.pct()));\n         }\n \ndiff --git a/crates/zng/src/lib.rs b/crates/zng/src/lib.rs\nindex 4fef9cbb3..a16fbe963 100644\n--- a/crates/zng/src/lib.rs\n+++ b/crates/zng/src/lib.rs\n@@ -118,7 +118,7 @@\n //! feature of widgets, they declare conditional property values. The easing attribute can be set in any property with transitionable\n //! values to smoothly animate between changes.\n //!\n-//! The [`widget`] module documentation provides an in-depth explanation of how widgets and properties work.\n+//! The [`widget`](mod@widget) module documentation provides an in-depth explanation of how widgets and properties work.\n //!\n //! [`Button!`]: struct@button::Button\n //! [`Window!`]: struct@window::Window\n@@ -457,7 +457,7 @@\n //!\n //! This crate level documentation only gives an overview required to start making apps using existing widgets and properties.\n //! All top-level modules in this crate contains in-depth documentation about their subject, of particular importance the\n-//! [`app`], [`widget`], [`layout`] and [`render`] modules should give you a solid understanding of how everything works.\n+//! [`app`], [`widget`](mod@widget), [`layout`] and [`render`] modules should give you a solid understanding of how everything works.\n //!\n //! ## Cargo Features\n //!\ndiff --git a/examples/button.rs b/examples/button.rs\nindex d90c8bb94..7dc0a73db 100644\n--- a/examples/button.rs\n+++ b/examples/button.rs\n@@ -344,7 +344,7 @@ fn dyn_button(content: char, remove: impl Fn() + Send + Sync + 'static) -> impl\n             margin = 0;\n         }\n \n-        when *#{removing} {\n+        when *#{removing.clone()} {\n             widget::interactive = false;\n             opacity = 0.pct();\n             margin = (0, 0, -30, 0);\ndiff --git a/examples/window.rs b/examples/window.rs\nindex b1bb8a91a..22efe179a 100644\n--- a/examples/window.rs\n+++ b/examples/window.rs\n@@ -432,14 +432,14 @@ fn custom_chrome(title: impl Var<Txt>) -> impl UiNode {\n         padding = 4;\n         corner_radius = (0, 0, 5, 5);\n \n-        mouse::on_mouse_down = hn!(can_move, |args: &mouse::MouseInputArgs| {\n+        when *#{can_move.clone()} {\n+            mouse::cursor = mouse::CursorIcon::Move;\n+        }\n+        mouse::on_mouse_down = hn!(|args: &mouse::MouseInputArgs| {\n             if args.is_primary() && can_move.get() {\n                 window::cmd::DRAG_MOVE_RESIZE_CMD.scoped(WINDOW.id()).notify();\n             }\n         });\n-        when *#{can_move} {\n-            mouse::cursor = mouse::CursorIcon::Move;\n-        }\n \n         gesture::on_context_click = hn!(|args: &gesture::ClickArgs| {\n             if matches!(WINDOW.vars().state().get(), WindowState::Normal | WindowState::Maximized) {\n", "test_patch": "", "problem_statement": "When expression \"borrow of moved value\"\n<!--\r\nPlease, make sure:\r\n\r\n- The issue happens in the latest crate release or newer (master branch).\r\n- The issue happens after `cargo update`.\r\n-->\r\n\r\nI tried this code:\r\n\r\n```rust\r\nlet can_move = var(true);\r\n\r\nContainer! {\r\n        when *#{can_move.clone()} {\r\n            mouse::cursor = mouse::CursorIcon::Move;\r\n        }\r\n        mouse::on_mouse_down = hn!( |_| {\r\n            let _use = &can_move;\r\n        });\r\n\r\n}\r\n```\r\n\r\nI saw this happen:\r\n\r\nerror[E0382]: borrow of moved value: `can_move`\r\n\r\nI expected this to happen:\r\n\r\nIt should compile because the when expression is declared before the handler, so`#{can_move.clone()}` should eval before the `hn!` capture move.\r\n\r\nError log or stack trace:\r\n\r\n<details>\r\n  <summary>stderr</summary>\r\n\r\n```\r\nRunning: do \"run\" [\"window\"]\r\n    Blocking waiting for file lock on package cache\r\n    Blocking waiting for file lock on build directory\r\n   Compiling examples v0.0.0 (C:\\code\\zng\\examples)\r\nerror[E0382]: borrow of moved value: `can_move`\r\n   --> examples\\window.rs:435:17\r\n    |\r\n427 |       let can_move = vars.state().map(|s| matches!(s, WindowState::Normal | WindowState:... \r\n    |           -------- move occurs because `can_move` has type `ReadOnlyVar<bool, ArcVar<bool>>`, which does not implement the `Copy` trait\r\n...\r\n435 |           when *#{can_move.clone()} {\r\n    |                   ^^^^^^^^ value borrowed here after move\r\n...\r\n438 |           mouse::on_mouse_down = hn!(|args: &mouse::MouseInputArgs| {\r\n    |  ________________________________-\r\n439 | |             if args.is_primary() && can_move.get() {\r\n    | |                                     -------- variable moved due to use in closure\r\n440 | |                 window::cmd::DRAG_MOVE_RESIZE_CMD.scoped(WINDOW.id()).notify();\r\n441 | |             }\r\n442 | |         });\r\n    | |__________- value moved into closure here\r\n    |\r\nhelp: consider cloning the value if the performance cost is acceptable\r\n   --> C:\\code\\zng\\crates\\zng-clone-move\\src\\lib.rs:140:17\r\n    |\r\n140 |             move.clone() | $($rest)+\r\n    |                 ++++++++\r\n\r\nFor more information about this error, try `rustc --explain E0382`.\r\nerror: could not compile `examples` (example \"window\") due to 1 previous error\r\nerror: task \"run\" failed with exit code: 101\r\n```\r\n\r\n</details>\n", "hints_text": "When expressions are expanded after all assigns to support later assigns back when the WidgetBuilder did not exist:\r\n\r\n```rust\r\nContainer! {\r\n    when #widget::background_color.0.red == 0.0 {\r\n        mouse::cursor = mouse::CursorIcon::Move;\r\n    }\r\n    widget::background_color = colors::BLUE;\r\n}\r\n```\r\n\r\nCurrent WidgetBuilder can handle delayed assigns, even from a later dynamic source like a Style. Time to go mess with the proc-macros again...", "created_at": "2024-04-28 04:15:58", "merge_commit_sha": "bea5323bc55adf6718bbd16102cf9f488bf798ac", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['check-release', '.github/workflows/ci.yml']", "['check', '.github/workflows/ci.yml']"], ["['test', '.github/workflows/ci.yml']", "['test-all', '.github/workflows/ci.yml']"]]}
{"repo": "diesel-rs/diesel", "instance_id": "diesel-rs__diesel-4151", "base_commit": "029a8d41b85739803272ce6aca8460a42f11f74e", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 6b3435b9b926..53e7eb9026e6 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -13,6 +13,7 @@ Increasing the minimal supported Rust version will always be coupled at least wi\n ### Added\n \n * Support for libsqlite3-sys 0.29.0\n+* Add support for built-in PostgreSQL range operators and functions\n * Support for postgres multirange type\n \n ## [2.2.0] 2024-05-31\ndiff --git a/diesel/src/pg/expression/expression_methods.rs b/diesel/src/pg/expression/expression_methods.rs\nindex 76136d3563d2..5aaca4773a15 100644\n--- a/diesel/src/pg/expression/expression_methods.rs\n+++ b/diesel/src/pg/expression/expression_methods.rs\n@@ -11,7 +11,7 @@ use crate::expression::grouped::Grouped;\n use crate::expression::operators::{Asc, Concat, Desc, Like, NotLike};\n use crate::expression::{AsExpression, Expression, IntoSql, TypedExpressionType};\n use crate::pg::expression::expression_methods::private::BinaryOrNullableBinary;\n-use crate::sql_types::{Array, Inet, Integer, SqlType, Text, VarChar};\n+use crate::sql_types::{Array, Inet, Integer, Range, SqlType, Text, VarChar};\n use crate::EscapeExpressionMethods;\n \n /// PostgreSQL specific methods which are present on all expressions.\n@@ -72,6 +72,89 @@ pub trait PgExpressionMethods: Expression + Sized {\n     {\n         Grouped(IsDistinctFrom::new(self, other.as_expression()))\n     }\n+\n+    /// Creates a PostgreSQL `<@` expression.\n+    ///\n+    /// This operator returns true whether a element is contained by a range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:     |\n+    /// other:  [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  |\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       |\n+    /// other:  [----]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:           |\n+    /// other:  [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  |\n+    /// other:   [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       |\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use std::collections::Bound;\n+    /// #     use diesel::dsl::int4range;\n+    /// #     use diesel::sql_types::Integer;\n+    /// #\n+    /// #     let conn = &mut establish_connection();\n+    /// #\n+    ///       let my_range = int4range(1, 5, diesel::sql_types::RangeBound::LowerBoundInclusiveUpperBoundExclusive);\n+    ///\n+    ///       let (first, second) = diesel::select((\n+    ///           4.into_sql::<Integer>().is_contained_by_range(my_range),\n+    ///           10.into_sql::<Integer>().is_contained_by_range(my_range)\n+    ///       )).get_result::<(bool, bool)>(conn)?;\n+    ///\n+    ///       assert_eq!(first, true);\n+    ///       assert_eq!(second, false);\n+    /// #\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    #[allow(clippy::wrong_self_convention)] // This is named after the sql operator\n+    fn is_contained_by_range<T>(self, other: T) -> dsl::IsContainedByRange<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Range<Self::SqlType>>,\n+    {\n+        Grouped(IsContainedBy::new(self, other.as_expression()))\n+    }\n }\n \n impl<T: Expression> PgExpressionMethods for T {}\n@@ -706,7 +789,41 @@ impl<T, U> EscapeExpressionMethods for Grouped<NotSimilarTo<T, U>> {\n pub trait PgRangeExpressionMethods: Expression + Sized {\n     /// Creates a PostgreSQL `@>` expression.\n     ///\n-    /// This operator returns whether a range contains an specific element\n+    /// This operator returns true whether a range contains an specific element\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:    |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other: |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:      |\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:          |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [----]\n+    /// other: |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:      |\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -758,7 +875,36 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `@>` expression.\n     ///\n-    /// This operator returns whether a range contains an specific element\n+    /// This operator returns true whether a range contains another range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-------]\n+    /// other:     [--]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: [------]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:     [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [----]\n+    /// other: [--------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other: [----]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -809,7 +955,36 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `<@` expression.\n     ///\n-    /// This operator returns whether a range is contained by a specific element\n+    /// This operator returns true whether a range is contained by another range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other: [-------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [------]\n+    /// other:   [---]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other: [-----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:   [----]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -861,7 +1036,46 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `&&` expression.\n     ///\n-    /// This operator returns whether two ranges overlap.\n+    /// This operator returns true whether two ranges overlap.\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:    [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [----]\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [----]\n+    /// other:  [-------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other:  [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other:  [----)\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:          [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       [----]\n+    /// other: [--]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -916,7 +1130,44 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `&<` expression.\n     ///\n-    /// This operator returns whether a range doesn't extend to the right of another\n+    /// This operator returns true whether the argument range extend to the right of the current range\n+    ///\n+    /// Postgresql defines \"extends\" as does not have a lower bound smaller than the lower bound of the\n+    /// self range. That means the right hand side range can overlap parts of the left hand side\n+    /// range or be on the right side of the left hand side range\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:    [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:         [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------)\n+    /// other:    [------)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:             [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:            [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: [------)\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -945,30 +1196,150 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///     .values(versions.eq((Bound::Included(1), Bound::Excluded(20))))\n     ///     .execute(conn)?;\n     ///\n-    /// let cool_posts = posts.select(id)\n-    ///     .filter(versions.range_not_extends_right_to((Bound::Included(18), Bound::Excluded(20))))\n-    ///     .load::<i32>(conn)?;\n-    /// assert_eq!(vec![1], cool_posts);\n+    /// let cool_posts = posts.select(versions.range_extends_right_to((Bound::Included(18), Bound::Excluded(20))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n     ///\n-    /// let amazing_posts = posts.select(id)\n-    ///     .filter(versions.contains_range((Bound::Included(20), Bound::Excluded(25))))\n-    ///     .load::<i32>(conn)?;\n-    /// assert!(amazing_posts.is_empty());\n+    /// let cool_posts = posts.select(versions.range_extends_right_to((Bound::Included(25), Bound::Excluded(30))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let amazing_posts = posts.select(versions.range_extends_right_to((Bound::Included(-10), Bound::Excluded(0))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(false, amazing_posts);\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    fn range_extends_right_to<T>(self, other: T) -> dsl::RangeExtendsRightTo<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Self::SqlType>,\n+    {\n+        Grouped(ExtendsRightTo::new(self, other.as_expression()))\n+    }\n+\n+    /// Creates a PostgreSQL `&>` expression.\n+    ///\n+    /// This operator returns true whether a range does extend to the left of another\n+    ///\n+    /// Postgresql defines \"extends\" as does not have a upper bound greater than the upper bound of the\n+    /// self range. That means the right hand side range can overlap parts of the left hand side\n+    /// range or be on the left side of the left hand side range\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:        [------)\n+    /// other:    [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:          [----)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------)\n+    /// other:        [------)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:   [--------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:          [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: (------]\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use self::posts::dsl::*;\n+    /// #     use std::collections::Bound;\n+    /// #     let conn = &mut establish_connection();\n+    /// #     diesel::sql_query(\"DROP TABLE IF EXISTS posts\").execute(conn).unwrap();\n+    /// #     diesel::sql_query(\"CREATE TABLE posts (id SERIAL PRIMARY KEY, versions INT4RANGE NOT NULL)\").execute(conn).unwrap();\n+    /// #\n+    /// diesel::insert_into(posts)\n+    ///     .values(versions.eq((Bound::Included(1), Bound::Excluded(20))))\n+    ///     .execute(conn)?;\n+    ///\n+    /// let cool_posts = posts.select(versions.range_extends_left_to((Bound::Included(-10), Bound::Excluded(5))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let cool_posts = posts.select(versions.range_extends_left_to((Bound::Included(-10), Bound::Excluded(-5))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let amazing_posts = posts.select(versions.range_extends_left_to((Bound::Included(25), Bound::Excluded(30))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(false, amazing_posts);\n     /// #     Ok(())\n     /// # }\n     /// ```\n-    fn range_not_extends_right_to<T>(self, other: T) -> dsl::RangeNotExtendsRightTo<Self, T>\n+    fn range_extends_left_to<T>(self, other: T) -> dsl::RangeExtendsLeftTo<Self, T>\n     where\n         Self::SqlType: SqlType,\n         T: AsExpression<Self::SqlType>,\n     {\n-        Grouped(NotExtendsRightTo::new(self, other.as_expression()))\n+        Grouped(ExtendsLeftTo::new(self, other.as_expression()))\n     }\n \n     /// Creates a PostgreSQL `<<` expression.\n     ///\n     /// Is the first range strictly left of the second?\n     ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:            [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:      [----)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:             [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:        [------)\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1024,6 +1395,34 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///\n     /// Is the first range strictly right of the second?\n     ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:          [------)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:        [----)\n+    /// other:  [----)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:          [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:         [------]\n+    /// other: [------]\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1075,10 +1474,120 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n         Grouped(ContainsNet::new(self, other.as_expression()))\n     }\n \n+    /// Creates a PostgreSQL ` -|- ` expression.\n+    ///\n+    /// This operator evaluates to true if the two ranges are adjacent\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:         [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       [----)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:        [----)\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:       [----]\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:        [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:        [------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:           [------]\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use self::posts::dsl::*;\n+    /// #     use std::collections::Bound;\n+    /// #     let conn = &mut establish_connection();\n+    /// #     diesel::sql_query(\"DROP TABLE IF EXISTS posts\").execute(conn).unwrap();\n+    /// #     diesel::sql_query(\"CREATE TABLE posts (id SERIAL PRIMARY KEY, versions INT4RANGE NOT NULL)\").execute(conn).unwrap();\n+    /// #\n+    /// diesel::insert_into(posts)\n+    ///     .values(&vec![\n+    ///         (versions.eq((Bound::Included(1), Bound::Excluded(2)))),\n+    ///         (versions.eq((Bound::Included(4), Bound::Excluded(7))))\n+    ///     ])\n+    ///     .execute(conn)?;\n+    ///\n+    /// let data = posts.select(versions.range_adjacent((Bound::Included(2),Bound::Included(6))))\n+    ///     .load::<bool>(conn)?;\n+    /// let expected = vec![true, false];\n+    /// assert_eq!(expected, data);\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    fn range_adjacent<T>(self, other: T) -> dsl::RangeAdjacent<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Self::SqlType>,\n+    {\n+        Grouped(RangeAdjacent::new(self, other.as_expression()))\n+    }\n+\n     /// Creates a PostgreSQL ` + ` expression.\n     ///\n     /// This operator unions two ranges and returns the union.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result: [--------)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:          [----)\n+    /// other: [----)\n+    /// result: error\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other: [----]\n+    /// result [--------)\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1130,6 +1639,24 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     /// The second range must not be contained in the first in such a way that the\n     /// difference would not be a single range.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result: [---)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other:  [----)\n+    /// result:      [--)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [--------)\n+    /// other:       [----]\n+    /// result: error\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1178,6 +1705,30 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///\n     /// This operator takes two ranges and returns the intersection.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result:     [--)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other:  [----)\n+    /// result:    [-)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [--------)\n+    /// other:     [----]\n+    /// result:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [--------)\n+    /// other:               [----]\n+    /// result: empty range\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\ndiff --git a/diesel/src/pg/expression/functions.rs b/diesel/src/pg/expression/functions.rs\nindex 313958927be8..0b7bfa9eb989 100644\n--- a/diesel/src/pg/expression/functions.rs\n+++ b/diesel/src/pg/expression/functions.rs\n@@ -66,8 +66,10 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the lower bound of the range.\n-    /// if the range is empty or has no lower bound, it returns NULL.\n+    /// Returns the lower bound of the range\n+    ///\n+    /// If the range is empty or has no lower bound, it returns NULL.\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -109,8 +111,10 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the upper bound of the range.\n-    /// if the range is empty or has no upper bound, it returns NULL.\n+    /// Returns the upper bound of the range\n+    ///\n+    /// If the range is empty or has no upper bound, it returns NULL.\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -152,7 +156,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range is empty.\n+    /// Returns true if the range is empty\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -194,7 +199,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's lower bound is inclusive.\n+    /// Returns true if the range's lower bound is inclusive\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -236,7 +242,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's upper bound is inclusive.\n+    /// Returns true if the range's upper bound is inclusive\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -277,7 +284,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's lower bound is unbounded.\n+    /// Returns true if the range's lower bound is unbounded\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -319,7 +327,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's upper bound is unbounded.\n+    /// Returns true if the range's upper bound is unbounded\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -361,7 +370,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the smallest range which includes both of the given ranges.\n+    /// Returns the smallest range which includes both of the given ranges\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -400,11 +410,12 @@ define_sql_function! {\n     /// # }\n     /// ```\n     #[cfg(feature = \"postgres_backend\")]\n-    fn range_merge<T: RangeHelper>(lhs: T, rhs: T) -> Range<<T as RangeHelper>::Inner>;\n+    fn range_merge<T1: RangeHelper, T2: RangeHelper<Inner = T1::Inner>>(lhs: T1, rhs: T2) -> Range<T1::Inner>;\n }\n \n define_sql_function! {\n-    /// Returns range of integer.\n+    /// Returns range of integer\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -451,7 +462,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of integer.\n+    /// Returns range of big ints\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -498,7 +510,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of number.\n+    /// Returns range of numeric values\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -548,7 +561,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of timestamp without timezone.\n+    /// Returns range of timestamps without timezone\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -598,7 +612,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of timestamp with timezone.\n+    /// Returns range of timestamps with timezone\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -646,7 +661,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of date.\n+    /// Returns range of dates\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -697,7 +713,8 @@ define_sql_function! {\n \n #[cfg(feature = \"postgres_backend\")]\n define_sql_function! {\n-    /// Append an element to the end of an array.\n+    /// Append an element to the end of an array\n+    ///\n     /// # Example\n     ///\n     /// ```rust\ndiff --git a/diesel/src/pg/expression/helper_types.rs b/diesel/src/pg/expression/helper_types.rs\nindex b38e4bc9a68a..5bcaadfc3e98 100644\n--- a/diesel/src/pg/expression/helper_types.rs\n+++ b/diesel/src/pg/expression/helper_types.rs\n@@ -60,11 +60,17 @@ pub type RangeContains<Lhs, Rhs> = Grouped<\n     >,\n >;\n \n-/// The return type of [`lhs.range_not_extends_right_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_not_extends_right_to)\n+/// The return type of [`lhs.range_extends_right_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_extends_right_to)\n /// for range expressions\n #[cfg(feature = \"postgres_backend\")]\n-pub type RangeNotExtendsRightTo<Lhs, Rhs> =\n-    Grouped<super::operators::NotExtendsRightTo<Lhs, AsExpr<Rhs, Lhs>>>;\n+pub type RangeExtendsRightTo<Lhs, Rhs> =\n+    Grouped<super::operators::ExtendsRightTo<Lhs, AsExpr<Rhs, Lhs>>>;\n+\n+/// The return type of [`lhs.range_extends_left_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_extends_left_to)\n+/// for range expressions\n+#[cfg(feature = \"postgres_backend\")]\n+pub type RangeExtendsLeftTo<Lhs, Rhs> =\n+    Grouped<super::operators::ExtendsLeftTo<Lhs, AsExpr<Rhs, Lhs>>>;\n \n /// The return type of [`lhs.contains_range(rhs)`](super::expression_methods::PgRangeExpressionMethods::contains_range)\n /// for range expressions\n@@ -76,6 +82,12 @@ pub type ContainsRange<Lhs, Rhs> = Contains<Lhs, Rhs>;\n #[cfg(feature = \"postgres_backend\")]\n pub type IsContainedBy<Lhs, Rhs> = Grouped<super::operators::IsContainedBy<Lhs, AsExpr<Rhs, Lhs>>>;\n \n+/// The return type of [`lhs.is_contained_by_range(rhs)`](super::expression_methods::PgExpressionMethods::is_contained_by_range)\n+#[cfg(feature = \"postgres_backend\")]\n+pub type IsContainedByRange<Lhs, Rhs> = Grouped<\n+    super::operators::IsContainedBy<Lhs, AsExprOf<Rhs, diesel::sql_types::Range<SqlTypeOf<Lhs>>>>,\n+>;\n+\n /// The return type of [`lhs.range_is_contained_by(rhs)`](super::expression_methods::PgRangeExpressionMethods::lesser_than)\n #[cfg(feature = \"postgres_backend\")]\n pub type LesserThanRange<Lhs, Rhs> =\n@@ -102,6 +114,10 @@ pub type Difference<Lhs, Rhs> = Grouped<super::operators::DifferenceRange<Lhs, A\n #[doc(hidden)] // used by `#[auto_type]`\n pub type DifferenceRange<Lhs, Rhs> = Difference<Lhs, Rhs>;\n \n+/// The return type of [`lhs.range_adjacent(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_adjacent)\n+#[cfg(feature = \"postgres_backend\")]\n+pub type RangeAdjacent<Lhs, Rhs> = Grouped<super::operators::RangeAdjacent<Lhs, AsExpr<Rhs, Lhs>>>;\n+\n /// The return type of [`lhs.intersection_range(rhs)`](super::expression_methods::PgRangeExpressionMethods::intersection_range)\n #[cfg(feature = \"postgres_backend\")]\n pub type Intersection<Lhs, Rhs> =\n@@ -291,3 +307,48 @@ pub type NotLikeBinary<Lhs, Rhs> = crate::dsl::NotLike<Lhs, Rhs>;\n #[doc(hidden)]\n #[deprecated(note = \"Use `dsl::Concat` instead\")]\n pub type ConcatArray<Lhs, Rhs> = crate::dsl::Concat<Lhs, Rhs>;\n+\n+/// Return type of [`lower(range)`](super::functions::lower())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower<R> = super::functions::lower<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper(range)`](super::functions::upper())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper<R> = super::functions::upper<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`isempty(range)`](super::functions::isempty())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type isempty<R> = super::functions::isempty<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`lower_inc(range)`](super::functions::lower_inc())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower_inc<R> = super::functions::lower_inc<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper_inc(range)`](super::functions::upper_inc())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper_inc<R> = super::functions::upper_inc<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`lower_inf(range)`](super::functions::lower_inf())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower_inf<R> = super::functions::lower_inf<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper_inf(range)`](super::functions::upper_inf())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper_inf<R> = super::functions::upper_inf<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`range_merge(range_a, range_b)`](super::functions::range_merge())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type range_merge<R1, R2> = super::functions::range_merge<SqlTypeOf<R1>, SqlTypeOf<R2>, R1, R2>;\n+\n+/// Return type of [`array_append(array, element)`](super::functions::array_append())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type array_append<A, E> = super::functions::array_append<SqlTypeOf<A>, SqlTypeOf<E>, A, E>;\ndiff --git a/diesel/src/pg/expression/operators.rs b/diesel/src/pg/expression/operators.rs\nindex 0fa057e6b329..dc6100694b98 100644\n--- a/diesel/src/pg/expression/operators.rs\n+++ b/diesel/src/pg/expression/operators.rs\n@@ -15,7 +15,8 @@ infix_operator!(OverlapsWith, \" && \", backend: Pg);\n infix_operator!(Contains, \" @> \", backend: Pg);\n infix_operator!(IsContainedBy, \" <@ \", backend: Pg);\n infix_operator!(ILike, \" ILIKE \", backend: Pg);\n-infix_operator!(NotExtendsRightTo, \" &< \", backend: Pg);\n+infix_operator!(ExtendsRightTo, \" &< \", backend: Pg);\n+infix_operator!(ExtendsLeftTo, \" &> \", backend: Pg);\n infix_operator!(NotILike, \" NOT ILIKE \", backend: Pg);\n infix_operator!(SimilarTo, \" SIMILAR TO \", backend: Pg);\n infix_operator!(NotSimilarTo, \" NOT SIMILAR TO \", backend: Pg);\n@@ -31,6 +32,7 @@ infix_operator!(DifferenceNet, \" - \", Bigint, backend: Pg);\n infix_operator!(HasKeyJsonb, \" ? \", backend: Pg);\n infix_operator!(HasAnyKeyJsonb, \" ?| \", backend: Pg);\n infix_operator!(HasAllKeysJsonb, \" ?& \", backend: Pg);\n+infix_operator!(RangeAdjacent, \" -|- \", backend: Pg);\n infix_operator!(RemoveFromJsonb, \" - \", Jsonb, backend: Pg);\n __diesel_infix_operator!(\n     RetrieveAsObjectJson,\n", "test_patch": "diff --git a/diesel_derives/tests/auto_type.rs b/diesel_derives/tests/auto_type.rs\nindex 223c68dce163..dc2c91c50c11 100644\n--- a/diesel_derives/tests/auto_type.rs\n+++ b/diesel_derives/tests/auto_type.rs\n@@ -11,6 +11,9 @@ table! {\n         id -> Integer,\n         name -> Text,\n         time -> Timestamp,\n+        bigint -> BigInt,\n+        numeric -> Numeric,\n+        date -> Date,\n     }\n }\n \n@@ -46,6 +49,7 @@ table! {\n         blob -> Binary,\n         timestamp -> Timestamp,\n         range -> Range<Integer>,\n+        timestamptz -> Timestamptz,\n     }\n }\n \n@@ -287,7 +291,9 @@ fn test_pg_range_expression_methods() -> _ {\n         .and(pg_extras::range.overlaps_with(my_range))\n         .and(pg_extras::range.lesser_than(my_range))\n         .and(pg_extras::range.greater_than(my_range))\n-        .and(pg_extras::range.range_not_extends_right_to(my_range))\n+        .and(pg_extras::range.range_extends_right_to(my_range))\n+        .and(pg_extras::range.range_extends_left_to(my_range))\n+        .and(pg_extras::id.is_contained_by_range(my_range))\n         .and(\n             pg_extras::range\n                 .union_range(pg_extras::range)\n@@ -309,12 +315,6 @@ fn test_pg_range_expression_methods() -> _ {\n     // function. We could likely support it by\n     // renaming the function to `.range_contains()` (or something similar)\n     // .contains(42_i32)\n-    //.select(\n-    // this kind of free standing functions is not supported by auto_type yet\n-    //lower(pg_extras::range),\n-    // The auto_trait also didn't like this one\n-    //int4range(None, Some(5i32), RangeBound::LowerBoundInclusiveUpperBoundInclusive),\n-    //)\n }\n \n #[cfg(feature = \"postgres\")]\n@@ -386,6 +386,34 @@ fn test_normal_functions() -> _ {\n     ))\n }\n \n+#[cfg(feature = \"postgres\")]\n+#[auto_type]\n+fn postgres_functions() -> _ {\n+    let bound: sql_types::RangeBound =\n+        sql_types::RangeBound::LowerBoundExclusiveUpperBoundExclusive;\n+    (\n+        lower(pg_extras::range),\n+        upper(pg_extras::range),\n+        isempty(pg_extras::range),\n+        lower_inc(pg_extras::range),\n+        upper_inc(pg_extras::range),\n+        lower_inf(pg_extras::range),\n+        upper_inf(pg_extras::range),\n+        range_merge(pg_extras::range, pg_extras::range),\n+        int4range(users::id.nullable(), users::id.nullable(), bound),\n+        int8range(users::bigint.nullable(), users::bigint.nullable(), bound),\n+        numrange(users::numeric.nullable(), users::numeric.nullable(), bound),\n+        daterange(users::date.nullable(), users::date.nullable(), bound),\n+        tsrange(users::time.nullable(), users::time.nullable(), bound),\n+        tstzrange(\n+            pg_extras::timestamptz.nullable(),\n+            pg_extras::timestamptz.nullable(),\n+            bound,\n+        ),\n+        array_append(pg_extras::array, pg_extras::id),\n+    )\n+}\n+\n #[auto_type]\n fn with_lifetime<'a>(name: &'a str) -> _ {\n     users::table.filter(users::name.eq(name))\n", "problem_statement": "Add support for currently unsupported range operators and methods\nDiesel currently supports the postgres range types. We do not provide built-in support for [various operators and methods](https://www.postgresql.org/docs/current/functions-range.html) available for these types. This is a tracking issue for adding support for these operators. \r\n\r\nThe general strategy for adding support for new operators is as following:\r\n\r\n1. Define the operator via [`infix_operator!()`](https://docs.diesel.rs/master/diesel/macro.infix_operator.html). The RangeExtends operator  would be defined as following: `infix_operator!(RangeExtends, \" &> \", backend: Pg);` These operators can be defined [here](https://github.com/diesel-rs/diesel/blob/dd5f41dc93c574077646185393cd064c7103000b/diesel/src/pg/expression/operators.rs). If there is already an existing definition, this step could be skipped.\r\n2. Define a new helper type [here](https://github.com/diesel-rs/diesel/blob/master/diesel/src/pg/expression/helper_types.rs#L30). Helper types are a tool for diesel users to simplify type definitions. Again if there is already a matching type definition this could be skipped. \r\n3. Add a new method to the [`PgRangeExpressionMethods` trait](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/pg/expression/expression_methods.rs#L706-L758). This definition should:\r\n    * Use the previously defined helper type\r\n    * Contain a documentation with an example\r\n4. If possible add a test case for the auto_type attribute [here](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel_derives/tests/auto_type.rs#L277-L288). (If not possible add a documentation entry there)\r\n5. Check if compile test output is up to date by running `TRYBUILD=overwrite cargo test` inside of `diesel_compile_tests`. (This is required if you've added a new operator based on `infix_operator!`\r\n5. Submit a PR with the change\r\n\r\nThe general strategy for adding support for new methods is as following:\r\n\r\n1. Define the operator via [`define_sql_function!()`](https://docs.diesel.rs/master/diesel/expression/functions/macro.define_sql_function.html). The lower method would be defined as following: `define_sql_function!(fn lower<T: RangeHelper>(range: T) -> T::Inner);` These operators can be defined [here](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/pg/expression/functions.rs). If there is already an existing definition, this step could be skipped. This function should have a short documentation snippet with en example\r\n6. Check if compile test output is up to date by running `TRYBUILD=overwrite cargo test` inside of `diesel_compile_tests`. (This is required if you've added a new operator based on `infix_operator!`\r\n5. Submit a PR with the change\r\n\r\n\r\n\r\nOperator list:\r\n\r\n- [x] `anyrange @> anyrange -> bool` (Does the first range contain the second?)\r\n- [x] `anyrange @> anyelement -> boolean` (Does the range contain the element?) * \r\n- [x] `anyrange <@ anyrange -> boolean` (Is the first range contained by the second?)  *\r\n- [x] `anyelement <@ anyrange -> boolean` (Is the element contained in the range?) *\r\n- [x] `anyrange && anyrange -> boolean` (Do the ranges overlap, that is, have any elements in common?) *\r\n- [x] `anyrange << anyrange -> boolean` (Is the first range strictly left of the second?) *\r\n- [x] `anyrange >> anyrange -> boolean` (Is the first range strictly right of the second?) *\r\n- [x] `anyrange &< anyrange -> boolean` (Does the first range not extend to the right of the second?) *\r\n- [x] `anyrange &> anyrange -> boolean` (Does the first range not extend to the left of the second?) *\r\n- [x] `anyrange -|- anyrange -> boolean` (Are the ranges adjacent?) *\r\n- [x] `anyrange + anyrange -> anyrange` (Computes the union of the ranges. The ranges must overlap or be adjacent, so that the union is a single range (but see range_merge()).) **\r\n- [x] `anyrange * anyrange -> anyrange` (Computes the intersection of the ranges.) **\r\n- [x] `anyrange - anyrange -> anyrange` (Computes the difference of the ranges. The second range must not be contained in the first in such a way that the difference would not be a single range.) **\r\n\r\nMethod list:\r\n\r\n- [x] `lower ( anyrange ) -> anyelement` (Extracts the lower bound of the range (NULL if the range is empty or has no lower bound).) *\r\n- [x] `upper ( anyrange ) -> anyelement` (Extracts the upper bound of the range (NULL if the range is empty or has no upper bound).) *\r\n- [x] `isempty ( anyrange ) -> boolean` (Is the range empty?) *\r\n- [x] `lower_inc ( anyrange ) -> boolean` (Is the range's lower bound inclusive?) *\r\n- [x] `upper_inc ( anyrange ) -> boolean` (Is the range's upper bound inclusive?) *\r\n- [x] `lower_inf ( anyrange ) -> boolean` (Does the range have no lower bound? (A lower bound of -Infinity returns false.) *\r\n- [x] `upper_inf ( anyrange ) -> boolean` (Does the range have no upper bound? (An upper bound of Infinity returns false.)) *\r\n- [x] `range_merge ( anyrange, anyrange ) -> anyrange` (Computes the smallest range that includes both of the given ranges.) *\r\n- [x] `int4range(Nullable<Integer>, Nullable<Integer>, RangeBoundEnum) -> int4range` ***\r\n- [x] `int8range(Nullable<BigInteger>, Nullable<BigInteger>, RangeBoundEnum) -> int8range` ***\r\n- [x] `numrange(Nullable<Numeric>, Nullable<Numeric>, RangeBoundEnum) -> numrange` ***\r\n- [x] `tsrange(Nullable<Timestamp>, Nullable<Timestamp>, RangeBoundEnum) -> tsrange` ***\r\n- [x] `tstzrange(Nullable<Timestamptz>, Nullable<Timestamptz>, RangeBoundEnum) -> tstzrange` ***\r\n- [x] `daterange(Nullable<Date>, Nullable<Date>, RangeBoundEnum) -> daterange` ***\r\n\r\n\r\nItems marked with one `*` can follow the instructions directly. For items marked with ** it might be useful to look into how to use the [existing](https://github.com/diesel-rs/diesel/tree/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/expression/ops) rust side operator implementations as well. For items marked with *** we need to introduce an `enum RangeBoundEnum` type (or similar named) that specifies which bound kinds should be used. That likely requires a custom sql type + implementing the relevant type mapping.\r\n\r\nPlease add a comment to this issue if you plan to work on a specific operator/method.\r\n\r\nIf there is anything unclear about how to add a support for a specific operator/method just ask and we will try to answer your questions.\r\n\n", "hints_text": "I would like to help out with this issue. I haven't worked on diesel before so I'm currently working on building the repo locally\nHi! I'm also interested in helping with this issue. \r\nIt's many small issues so we can split I think, or maybe work together to discover how to do it!\r\n \nHi! I would like to help out with this issue as well. \nI can start working on the `anyrange && anyrange -> boolean`. \nHi! Started working on `anyelement <@ anyrange -> boolean`\n> Hi! Started working on `anyelement <@ anyrange -> boolean`\r\n\r\nOops, It's already implemented.  I see `anyrange &< anyrange -> boolean` isn't yet so I'm working on that instead.\nHi, I can work on `anyrange << anyrange -> boolean` and `anyrange >> anyrange -> boolean` next.\nI can work on `anyrange + anyrange -> anyrange`, `anyrange * anyrange -> anyrange` and`anyrange - anyrange -> anyrange` next.\r\n\r\n\r\n", "created_at": "2024-08-08 06:42:24", "merge_commit_sha": "025282054b9f84e830b6b583d3ab4ff9e44ccafc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check (nightly, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check sqlite bundled + Sqlite with asan', '.github/workflows/ci.yml']"], ["['Check (beta, postgres, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, postgres, windows-2019)', '.github/workflows/ci.yml']"], ["['Check mysql bundled + Mysql with asan', '.github/workflows/ci.yml']", "['Check (stable, mysql, windows-2019)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (stable, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check rustfmt style && run clippy', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, windows-2019)', '.github/workflows/ci.yml']", "['Check (nightly, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (beta, mysql, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Check (stable, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-13)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (beta, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Compiletests', '.github/workflows/ci.yml']", "['Check (beta, mysql, macos-14)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-14)', '.github/workflows/ci.yml']"]]}
{"repo": "diesel-rs/diesel", "instance_id": "diesel-rs__diesel-4132", "base_commit": "4763e7c9c968e20d7dff7e23ba027fd1de30c37d", "patch": "diff --git a/diesel_derives/src/queryable_by_name.rs b/diesel_derives/src/queryable_by_name.rs\nindex 16763059a70b..7848e20ed58c 100644\n--- a/diesel_derives/src/queryable_by_name.rs\n+++ b/diesel_derives/src/queryable_by_name.rs\n@@ -23,12 +23,13 @@ pub fn derive(item: DeriveInput) -> Result<TokenStream> {\n             if f.embed() {\n                 Ok(quote!(<#field_ty as QueryableByName<__DB>>::build(row)?))\n             } else {\n+                let st = sql_type(f, &model)?;\n                 let deserialize_ty = f.ty_for_deserialize();\n                 let name = f.column_name()?;\n                 let name = LitStr::new(&name.to_string(), name.span());\n                 Ok(quote!(\n                    {\n-                       let field = diesel::row::NamedRow::get(row, #name)?;\n+                       let field = diesel::row::NamedRow::get::<#st, #deserialize_ty>(row, #name)?;\n                        <#deserialize_ty as Into<#field_ty>>::into(field)\n                    }\n                 ))\n", "test_patch": "diff --git a/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr b/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\nindex 2dd72fafbb74..f46370568073 100644\n--- a/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\n+++ b/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\n@@ -6,49 +6,6 @@ error: All fields of tuple structs must be annotated with `#[diesel(column_name)\n    |\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n \n-error[E0277]: cannot deserialize a value of the database type `_` as `i32`\n- --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:4:10\n-  |\n-4 | #[derive(QueryableByName)]\n-  |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `i32`\n-  |\n-  = note: double check your type mappings via the documentation of `_`\n-  = help: the following other types implement trait `FromSql<A, DB>`:\n-            <i32 as FromSql<diesel::sql_types::Integer, Mysql>>\n-            <i32 as FromSql<diesel::sql_types::Integer, Pg>>\n-            <i32 as FromSql<diesel::sql_types::Integer, Sqlite>>\n-note: required by a bound in `diesel::row::NamedRow::get`\n- --> $DIESEL/src/row.rs\n-  |\n-  |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-  |        --- required by a bound in this associated function\n-  |     where\n-  |         T: FromSql<ST, DB>;\n-  |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-  = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n-\n-error[E0277]: cannot deserialize a value of the database type `_` as `*const str`\n- --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:4:10\n-  |\n-4 | #[derive(QueryableByName)]\n-  |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `*const str`, which is required by `std::string::String: FromSql<_, __DB>`\n-  |\n-  = note: double check your type mappings via the documentation of `_`\n-  = help: the following other types implement trait `FromSql<A, DB>`:\n-            <*const str as FromSql<diesel::sql_types::Text, Mysql>>\n-            <*const str as FromSql<diesel::sql_types::Text, Pg>>\n-            <*const str as FromSql<diesel::sql_types::Text, Sqlite>>\n-  = note: required for `std::string::String` to implement `FromSql<_, __DB>`\n-note: required by a bound in `diesel::row::NamedRow::get`\n- --> $DIESEL/src/row.rs\n-  |\n-  |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-  |        --- required by a bound in this associated function\n-  |     where\n-  |         T: FromSql<ST, DB>;\n-  |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-  = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n-\n error[E0433]: failed to resolve: use of undeclared crate or module `foos`\n  --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:5:8\n   |\ndiff --git a/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr b/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\nindex 5b5592b5bf6e..94c66d44a278 100644\n--- a/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\n+++ b/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\n@@ -59,24 +59,3 @@ error[E0412]: cannot find type `foo` in this scope\n    |\n 27 |     #[sql_type = \"foo\"]\n    |                  ^^^^^ not found in this scope\n-\n-error[E0277]: cannot deserialize a value of the database type `_` as `i32`\n-  --> tests/fail/derive_deprecated/deprecated_sql_type.rs:25:10\n-   |\n-25 | #[derive(QueryableByName)]\n-   |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `i32`\n-   |\n-   = note: double check your type mappings via the documentation of `_`\n-   = help: the following other types implement trait `FromSql<A, DB>`:\n-             <i32 as FromSql<diesel::sql_types::Integer, Mysql>>\n-             <i32 as FromSql<diesel::sql_types::Integer, Pg>>\n-             <i32 as FromSql<diesel::sql_types::Integer, Sqlite>>\n-note: required by a bound in `diesel::row::NamedRow::get`\n-  --> $DIESEL/src/row.rs\n-   |\n-   |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-   |        --- required by a bound in this associated function\n-   |     where\n-   |         T: FromSql<ST, DB>;\n-   |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-   = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\ndiff --git a/diesel_derives/tests/queryable_by_name.rs b/diesel_derives/tests/queryable_by_name.rs\nindex d865ebba8804..4135935b4b2f 100644\n--- a/diesel_derives/tests/queryable_by_name.rs\n+++ b/diesel_derives/tests/queryable_by_name.rs\n@@ -12,6 +12,15 @@ table! {\n     }\n }\n \n+#[cfg(feature = \"sqlite\")]\n+table! {\n+    multiple_sql_types_for_text {\n+        id -> Integer,\n+        string -> Text,\n+        time -> Timestamp,\n+    }\n+}\n+\n #[test]\n fn named_struct_definition() {\n     #[derive(Debug, Clone, Copy, PartialEq, Eq, QueryableByName)]\n@@ -86,8 +95,50 @@ fn struct_with_path_in_name() {\n     );\n }\n \n-// FIXME: Test usage with renamed columns\n+#[cfg(feature = \"sqlite\")]\n+#[test]\n+fn struct_with_multiple_sql_types_for_text() {\n+    #[derive(Debug, PartialEq, QueryableByName)]\n+    struct MultipleSqlTypesForText {\n+        #[diesel(sql_type = diesel::sql_types::Text)]\n+        string: String,\n+        #[diesel(sql_type = diesel::sql_types::Timestamp)]\n+        time: String,\n+    }\n+\n+    let conn = &mut connection();\n+    let data = sql_query(\"SELECT 'name' AS string, '2024-07-31T21:09:00' AS time\").get_result(conn);\n+    assert_eq!(\n+        Ok(MultipleSqlTypesForText {\n+            string: \"name\".into(),\n+            time: \"2024-07-31T21:09:00\".into()\n+        }),\n+        data\n+    );\n+}\n \n+#[cfg(feature = \"sqlite\")]\n+#[test]\n+fn struct_with_multiple_sql_types_for_text_from_table() {\n+    #[derive(Debug, PartialEq, QueryableByName)]\n+    #[diesel(table_name = multiple_sql_types_for_text)]\n+    struct MultipleSqlTypesForText {\n+        string: String,\n+        time: String,\n+    }\n+\n+    let conn = &mut connection();\n+    let data = sql_query(\"SELECT 'name' AS string, '2024-07-31T21:09:00' AS time\").get_result(conn);\n+    assert_eq!(\n+        Ok(MultipleSqlTypesForText {\n+            string: \"name\".into(),\n+            time: \"2024-07-31T21:09:00\".into()\n+        }),\n+        data\n+    );\n+}\n+\n+// FIXME: Test usage with renamed columns\n #[test]\n fn struct_with_no_table() {\n     #[derive(Debug, Clone, Copy, PartialEq, Eq, QueryableByName)]\n", "problem_statement": "Ambiguous traits in QueryableByName derivation when two fields have same concrete type but different SQL types\n<!--\r\nIf you want to report a bug, we added some points below which help us track down the problem faster.\r\n-->\r\n\r\n## Setup\r\n\r\n### Versions\r\n\r\n- **Rust:** 1.80.0\r\n- **Diesel:** 2.2.2\r\n- **Database:** PostgreSQL\r\n- **Operating System: ** MacOS\r\n\r\n### Feature Flags\r\n\r\n- **diesel:** `postgres`\r\n\r\n## Problem Description\r\n\r\n\r\n### What are you trying to accomplish?\r\n\r\nAttempting to use `QueryableByName` + `sql_query`. In this case, two fields have the same type in the struct, but different SQL types.\r\n\r\nExample:\r\n```rust\r\n#[derive(QueryableByName)]\r\n#[diesel(check_for_backend(Pg))]\r\nstruct StructWithTextAndCitext {\r\n    #[diesel(sql_type = sql_types::Text)]\r\n    name: String,\r\n    #[diesel(sql_type = sql_types::Citext)]\r\n    email: String,\r\n}\r\n```\r\n\r\nAlternative example (this also requires the `serde_json` feature of diesel):\r\n```rust\r\n#[derive(QueryableByName)]\r\n#[diesel(check_for_backend(Pg))]\r\nstruct StructWithJsonAndJsonb {\r\n    #[diesel(sql_type = sql_types::Json)]\r\n    v1: serde_json::Value,\r\n    #[diesel(sql_type = sql_types::Jsonb)]\r\n    v2: serde_json::Value,\r\n}\r\n```\r\n\r\n### What is the expected output?\r\n\r\nThe code compiles, and creates a useful `QueryableByName` trait implementation.\r\n\r\n### What is the actual output?\r\n\r\nIn the first case above, the error is:\r\n\r\n```\r\nerror[E0283]: type annotations needed\r\n   --> src/main.rs:11:10\r\n    |\r\n11  | #[derive(QueryableByName)]\r\n    |          ^^^^^^^^^^^^^^^ cannot infer type\r\n    |\r\n    = note: cannot satisfy `String: FromSql<_, __DB>`\r\n    = help: the following types implement trait `FromSql<A, DB>`:\r\n              <String as FromSql<Citext, Pg>>\r\n              <String as FromSql<ST, DB>>\r\nnote: required by a bound in `diesel::row::NamedRow::get`\r\n   --> /xxx/.cargo/registry/src/index.crates.io-6f17d22bba15001f/diesel-2.2.2/src/row.rs:133:12\r\n    |\r\n131 |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\r\n    |        --- required by a bound in this associated function\r\n132 |     where\r\n133 |         T: FromSql<ST, DB>;\r\n    |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\r\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\r\n```\r\n\r\nIn the second case, the error is:\r\n\r\n```\r\nerror[E0283]: type annotations needed\r\n   --> src/main.rs:11:10\r\n    |\r\n11  | #[derive(QueryableByName)]\r\n    |          ^^^^^^^^^^^^^^^ cannot infer type\r\n    |\r\n    = note: cannot satisfy `Value: FromSql<_, __DB>`\r\n    = help: the following types implement trait `FromSql<A, DB>`:\r\n              <Value as FromSql<Json, Pg>>\r\n              <Value as FromSql<Jsonb, Pg>>\r\nnote: required by a bound in `diesel::row::NamedRow::get`\r\n   --> /xxx/.cargo/registry/src/index.crates.io-6f17d22bba15001f/diesel-2.2.2/src/row.rs:133:12\r\n    |\r\n131 |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\r\n    |        --- required by a bound in this associated function\r\n132 |     where\r\n133 |         T: FromSql<ST, DB>;\r\n    |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\r\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\r\n```\r\n\r\n### Steps to reproduce\r\n\r\nWrite the code above, with `diesel` dependency, and features `postgres` (and `serde_json` in the second case). If added to `src/main.rs`, you should see the above compile errors. \r\n\r\n## Checklist\r\n\r\n- [x] I have already looked over the [issue tracker](https://github.com/diesel-rs/diesel/issues) and the [discussion forum](https://github.com/diesel-rs/diesel/discussions) for similar possible closed issues.\r\n- [x] This issue can be reproduced on Rust's stable channel. (Your issue will be\r\n  closed if this is not the case)\r\n- [x] This issue can be reproduced without requiring a third party crate\n", "hints_text": "", "created_at": "2024-07-26 09:31:51", "merge_commit_sha": "e21dfb95db3bccaaae9a53a8842cc999856f883b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check (nightly, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check sqlite bundled + Sqlite with asan', '.github/workflows/ci.yml']"], ["['Check (beta, postgres, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, postgres, windows-2019)', '.github/workflows/ci.yml']"], ["['Check mysql bundled + Mysql with asan', '.github/workflows/ci.yml']", "['Check (stable, mysql, windows-2019)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (stable, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check rustfmt style && run clippy', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, windows-2019)', '.github/workflows/ci.yml']", "['Check (nightly, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (beta, mysql, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Check (stable, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-13)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (beta, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Compiletests', '.github/workflows/ci.yml']", "['Check (beta, mysql, macos-14)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-14)', '.github/workflows/ci.yml']"]]}
{"repo": "cloudflare/pingora", "instance_id": "cloudflare__pingora-257", "base_commit": "479d9badbfdfe29f1d810299057477250c69c98d", "patch": "diff --git a/.bleep b/.bleep\nindex 5b3f05fd5..fa13307f1 100644\n--- a/.bleep\n+++ b/.bleep\n@@ -1,1 +1,1 @@\n-719b4dcafd471ecfa9e4ade3abfb217929afddc6\n\\ No newline at end of file\n+946e1733bc009bf3e73c635ad1b8bac0079dc680\n\\ No newline at end of file\ndiff --git a/pingora-core/src/apps/http_app.rs b/pingora-core/src/apps/http_app.rs\nindex 4dc905939..c758a1494 100644\n--- a/pingora-core/src/apps/http_app.rs\n+++ b/pingora-core/src/apps/http_app.rs\n@@ -161,13 +161,16 @@ where\n         }\n         let mut module_ctx = self.modules.build_ctx();\n         let req = http.req_header_mut();\n-        module_ctx.request_header_filter(req).ok()?;\n+        module_ctx.request_header_filter(req).await.ok()?;\n         let new_response = self.app.response(&mut http).await;\n         let (parts, body) = new_response.into_parts();\n-        let resp_header: ResponseHeader = parts.into();\n-        let mut task = HttpTask::Header(Box::new(resp_header), body.is_empty());\n-        module_ctx.response_filter(&mut task).ok()?;\n+        let mut resp_header: ResponseHeader = parts.into();\n+        module_ctx\n+            .response_header_filter(&mut resp_header, body.is_empty())\n+            .await\n+            .ok()?;\n \n+        let task = HttpTask::Header(Box::new(resp_header), body.is_empty());\n         trace!(\"{task:?}\");\n \n         match http.response_duplex_vec(vec![task]).await {\n@@ -181,15 +184,13 @@ where\n                 );\n             }\n         }\n-        let mut task = if !body.is_empty() {\n-            HttpTask::Body(Some(body.into()), true)\n-        } else {\n-            HttpTask::Body(None, true)\n-        };\n \n-        trace!(\"{task:?}\");\n+        let mut body = Some(body.into());\n+        module_ctx.response_body_filter(&mut body, true).ok()?;\n \n-        module_ctx.response_filter(&mut task).ok()?;\n+        let task = HttpTask::Body(body, true);\n+\n+        trace!(\"{task:?}\");\n \n         // TODO: check if chunked encoding is needed\n         match http.response_duplex_vec(vec![task]).await {\ndiff --git a/pingora-core/src/modules/http/compression.rs b/pingora-core/src/modules/http/compression.rs\nindex b07e1c730..134f94fe3 100644\n--- a/pingora-core/src/modules/http/compression.rs\n+++ b/pingora-core/src/modules/http/compression.rs\n@@ -20,6 +20,7 @@ use crate::protocols::http::compression::ResponseCompressionCtx;\n /// HTTP response compression module\n pub struct ResponseCompression(ResponseCompressionCtx);\n \n+#[async_trait]\n impl HttpModule for ResponseCompression {\n     fn as_any(&self) -> &dyn std::any::Any {\n         self\n@@ -28,13 +29,30 @@ impl HttpModule for ResponseCompression {\n         self\n     }\n \n-    fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+    async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n         self.0.request_filter(req);\n         Ok(())\n     }\n \n-    fn response_filter(&mut self, t: &mut HttpTask) -> Result<()> {\n-        self.0.response_filter(t);\n+    async fn response_header_filter(\n+        &mut self,\n+        resp: &mut ResponseHeader,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        self.0.response_header_filter(resp, end_of_stream);\n+        Ok(())\n+    }\n+\n+    fn response_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        if !self.0.is_enabled() {\n+            return Ok(());\n+        }\n+        let compressed = self.0.response_body_filter(body.as_ref(), end_of_stream);\n+        *body = compressed;\n         Ok(())\n     }\n }\ndiff --git a/pingora-core/src/modules/http/mod.rs b/pingora-core/src/modules/http/mod.rs\nindex 0f0401b22..31c5f9a21 100644\n--- a/pingora-core/src/modules/http/mod.rs\n+++ b/pingora-core/src/modules/http/mod.rs\n@@ -20,28 +20,44 @@\n \n pub mod compression;\n \n-use crate::protocols::http::HttpTask;\n+use async_trait::async_trait;\n use bytes::Bytes;\n use once_cell::sync::OnceCell;\n use pingora_error::Result;\n-use pingora_http::RequestHeader;\n+use pingora_http::{RequestHeader, ResponseHeader};\n use std::any::Any;\n use std::any::TypeId;\n use std::collections::HashMap;\n use std::sync::Arc;\n \n /// The trait an HTTP traffic module needs to implement\n-// TODO: * async filters for, e.g., 3rd party auth server; * access the connection for, e.g., GeoIP\n+#[async_trait]\n pub trait HttpModule {\n-    fn request_header_filter(&mut self, _req: &mut RequestHeader) -> Result<()> {\n+    async fn request_header_filter(&mut self, _req: &mut RequestHeader) -> Result<()> {\n         Ok(())\n     }\n \n-    fn request_body_filter(&mut self, body: Option<Bytes>) -> Result<Option<Bytes>> {\n-        Ok(body)\n+    async fn request_body_filter(\n+        &mut self,\n+        _body: &mut Option<Bytes>,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n+        Ok(())\n+    }\n+\n+    async fn response_header_filter(\n+        &mut self,\n+        _resp: &mut ResponseHeader,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n+        Ok(())\n     }\n \n-    fn response_filter(&mut self, _t: &mut HttpTask) -> Result<()> {\n+    fn response_body_filter(\n+        &mut self,\n+        _body: &mut Option<Bytes>,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n         Ok(())\n     }\n \n@@ -168,25 +184,45 @@ impl HttpModuleCtx {\n     }\n \n     /// Run the `request_header_filter` for all the modules according to their orders.\n-    pub fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+    pub async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            filter.request_header_filter(req)?;\n+            filter.request_header_filter(req).await?;\n         }\n         Ok(())\n     }\n \n     /// Run the `request_body_filter` for all the modules according to their orders.\n-    pub fn request_body_filter(&mut self, mut body: Option<Bytes>) -> Result<Option<Bytes>> {\n+    pub async fn request_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            body = filter.request_body_filter(body)?;\n+            filter.request_body_filter(body, end_of_stream).await?;\n         }\n-        Ok(body)\n+        Ok(())\n     }\n \n-    /// Run the `response_filter` for all the modules according to their orders.\n-    pub fn response_filter(&mut self, t: &mut HttpTask) -> Result<()> {\n+    /// Run the `response_header_filter` for all the modules according to their orders.\n+    pub async fn response_header_filter(\n+        &mut self,\n+        req: &mut ResponseHeader,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            filter.response_filter(t)?;\n+            filter.response_header_filter(req, end_of_stream).await?;\n+        }\n+        Ok(())\n+    }\n+\n+    /// Run the `response_body_filter` for all the modules according to their orders.\n+    pub fn response_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        for filter in self.module_ctx.iter_mut() {\n+            filter.response_body_filter(body, end_of_stream)?;\n         }\n         Ok(())\n     }\n@@ -197,6 +233,7 @@ mod tests {\n     use super::*;\n \n     struct MyModule;\n+    #[async_trait]\n     impl HttpModule for MyModule {\n         fn as_any(&self) -> &dyn Any {\n             self\n@@ -204,7 +241,7 @@ mod tests {\n         fn as_any_mut(&mut self) -> &mut dyn Any {\n             self\n         }\n-        fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+        async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n             req.insert_header(\"my-filter\", \"1\")\n         }\n     }\n@@ -220,6 +257,7 @@ mod tests {\n     }\n \n     struct MyOtherModule;\n+    #[async_trait]\n     impl HttpModule for MyOtherModule {\n         fn as_any(&self) -> &dyn Any {\n             self\n@@ -227,7 +265,7 @@ mod tests {\n         fn as_any_mut(&mut self) -> &mut dyn Any {\n             self\n         }\n-        fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+        async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n             if req.headers.get(\"my-filter\").is_some() {\n                 // if this MyOtherModule runs after MyModule\n                 req.insert_header(\"my-filter\", \"2\")\n@@ -262,14 +300,14 @@ mod tests {\n         assert!(ctx.get_mut::<usize>().is_none());\n     }\n \n-    #[test]\n-    fn test_module_filter() {\n+    #[tokio::test]\n+    async fn test_module_filter() {\n         let mut http_module = HttpModules::new();\n         http_module.add_module(Box::new(MyOtherModuleBuilder));\n         http_module.add_module(Box::new(MyModuleBuilder));\n         let mut ctx = http_module.build_ctx();\n         let mut req = RequestHeader::build(\"Get\", b\"/\", None).unwrap();\n-        ctx.request_header_filter(&mut req).unwrap();\n+        ctx.request_header_filter(&mut req).await.unwrap();\n         // MyModule runs before MyOtherModule\n         assert_eq!(req.headers.get(\"my-filter\").unwrap(), \"2\");\n         assert!(req.headers.get(\"my-other-filter\").is_none());\ndiff --git a/pingora-core/src/protocols/digest.rs b/pingora-core/src/protocols/digest.rs\nindex 594dbba74..e080a89ab 100644\n--- a/pingora-core/src/protocols/digest.rs\n+++ b/pingora-core/src/protocols/digest.rs\n@@ -15,10 +15,11 @@\n //! Extra information about the connection\n \n use std::sync::Arc;\n-use std::time::SystemTime;\n+use std::time::{Duration, SystemTime};\n \n use once_cell::sync::OnceCell;\n \n+use super::l4::ext::{get_recv_buf, get_tcp_info, TCP_INFO};\n use super::l4::socket::SocketAddr;\n use super::raw_connect::ProxyDigest;\n use super::ssl::digest::SslDigest;\n@@ -88,12 +89,38 @@ impl SocketDigest {\n             .get_or_init(|| SocketAddr::from_raw_fd(self.raw_fd, false))\n             .as_ref()\n     }\n+\n+    fn is_inet(&self) -> bool {\n+        self.local_addr().and_then(|p| p.as_inet()).is_some()\n+    }\n+\n+    pub fn tcp_info(&self) -> Option<TCP_INFO> {\n+        if self.is_inet() {\n+            get_tcp_info(self.raw_fd).ok()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    pub fn get_recv_buf(&self) -> Option<usize> {\n+        if self.is_inet() {\n+            get_recv_buf(self.raw_fd).ok()\n+        } else {\n+            None\n+        }\n+    }\n }\n \n /// The interface to return timing information\n pub trait GetTimingDigest {\n     /// Return the timing for each layer from the lowest layer to upper\n     fn get_timing_digest(&self) -> Vec<Option<TimingDigest>>;\n+    fn get_read_pending_time(&self) -> Duration {\n+        Duration::ZERO\n+    }\n+    fn get_write_pending_time(&self) -> Duration {\n+        Duration::ZERO\n+    }\n }\n \n /// The interface to set or return proxy information\ndiff --git a/pingora-core/src/protocols/http/client.rs b/pingora-core/src/protocols/http/client.rs\nindex 83a7618f6..5c44b6f2d 100644\n--- a/pingora-core/src/protocols/http/client.rs\n+++ b/pingora-core/src/protocols/http/client.rs\n@@ -19,7 +19,7 @@ use std::time::Duration;\n \n use super::v1::client::HttpSession as Http1Session;\n use super::v2::client::Http2Session;\n-use crate::protocols::{Digest, SocketAddr};\n+use crate::protocols::{Digest, SocketAddr, Stream};\n \n /// A type for Http client session. It can be either an Http1 connection or an Http2 stream.\n pub enum HttpSession {\n@@ -174,4 +174,13 @@ impl HttpSession {\n             Self::H2(s) => s.client_addr(),\n         }\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP/1 session is operating upon.\n+    /// None if the HTTP session is over H2\n+    pub fn stream(&self) -> Option<&Stream> {\n+        match self {\n+            Self::H1(s) => Some(s.stream()),\n+            Self::H2(_) => None,\n+        }\n+    }\n }\ndiff --git a/pingora-core/src/protocols/http/compression/mod.rs b/pingora-core/src/protocols/http/compression/mod.rs\nindex 26d4fb8e0..6cbd7388d 100644\n--- a/pingora-core/src/protocols/http/compression/mod.rs\n+++ b/pingora-core/src/protocols/http/compression/mod.rs\n@@ -159,7 +159,11 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n-    fn response_header_filter(&mut self, resp: &mut ResponseHeader, end: bool) {\n+    /// Feed the response header into this ctx\n+    pub fn response_header_filter(&mut self, resp: &mut ResponseHeader, end: bool) {\n+        if !self.is_enabled() {\n+            return;\n+        }\n         match &self.0 {\n             CtxInner::HeaderPhase {\n                 compression_level,\n@@ -195,7 +199,8 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n-    fn response_body_filter(&mut self, data: Option<&Bytes>, end: bool) -> Option<Bytes> {\n+    /// Stream the response body chunks into this ctx. The return value will be the compressed data\n+    pub fn response_body_filter(&mut self, data: Option<&Bytes>, end: bool) -> Option<Bytes> {\n         match &mut self.0 {\n             CtxInner::HeaderPhase {\n                 compression_level: _,\n@@ -222,6 +227,7 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n+    // TODO: retire this function, replace it with the two functions above\n     /// Feed the response into this ctx.\n     /// This filter will mutate the response accordingly if encoding is needed.\n     pub fn response_filter(&mut self, t: &mut HttpTask) {\ndiff --git a/pingora-core/src/protocols/http/server.rs b/pingora-core/src/protocols/http/server.rs\nindex a9619a88b..8dfb103d6 100644\n--- a/pingora-core/src/protocols/http/server.rs\n+++ b/pingora-core/src/protocols/http/server.rs\n@@ -370,4 +370,13 @@ impl Session {\n             Self::H2(s) => s.server_addr(),\n         }\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP/1 session is operating upon.\n+    /// None if the HTTP session is over H2\n+    pub fn stream(&self) -> Option<&Stream> {\n+        match self {\n+            Self::H1(s) => Some(s.stream()),\n+            Self::H2(_) => None,\n+        }\n+    }\n }\ndiff --git a/pingora-core/src/protocols/http/v1/body.rs b/pingora-core/src/protocols/http/v1/body.rs\nindex c317bb728..0ddc90a28 100644\n--- a/pingora-core/src/protocols/http/v1/body.rs\n+++ b/pingora-core/src/protocols/http/v1/body.rs\n@@ -538,20 +538,10 @@ impl BodyWriter {\n \n                 let chuck_size_buf = format!(\"{:X}\\r\\n\", chunk_size);\n                 let mut output_buf = Bytes::from(chuck_size_buf).chain(buf).chain(&b\"\\r\\n\"[..]);\n-\n-                while output_buf.has_remaining() {\n-                    let res = stream.write_vec(&mut output_buf).await;\n-                    match res {\n-                        Ok(n) => {\n-                            if n == 0 {\n-                                return Error::e_explain(ConnectionClosed, \"while writing body\");\n-                            }\n-                        }\n-                        Err(e) => {\n-                            return Error::e_because(WriteError, \"while writing body\", e);\n-                        }\n-                    }\n-                }\n+                stream\n+                    .write_vec_all(&mut output_buf)\n+                    .await\n+                    .or_err(WriteError, \"while writing body\")?;\n                 stream.flush().await.or_err(WriteError, \"flushing body\")?;\n                 self.body_mode = BM::ChunkedEncoding(written + chunk_size);\n                 Ok(Some(chunk_size))\ndiff --git a/pingora-core/src/protocols/http/v1/client.rs b/pingora-core/src/protocols/http/v1/client.rs\nindex fa7710f4d..41cf5901f 100644\n--- a/pingora-core/src/protocols/http/v1/client.rs\n+++ b/pingora-core/src/protocols/http/v1/client.rs\n@@ -641,6 +641,11 @@ impl HttpSession {\n             .as_ref()\n             .map(|d| d.local_addr())?\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP session is operating upon.\n+    pub fn stream(&self) -> &Stream {\n+        &self.underlying_stream\n+    }\n }\n \n #[inline]\ndiff --git a/pingora-core/src/protocols/http/v1/server.rs b/pingora-core/src/protocols/http/v1/server.rs\nindex 6f8c96ce2..5168456d4 100644\n--- a/pingora-core/src/protocols/http/v1/server.rs\n+++ b/pingora-core/src/protocols/http/v1/server.rs\n@@ -926,6 +926,11 @@ impl HttpSession {\n         }\n         Ok(end_stream)\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP session is operating upon.\n+    pub fn stream(&self) -> &Stream {\n+        &self.underlying_stream\n+    }\n }\n \n // Regex to parse request line that has illegal chars in it\ndiff --git a/pingora-core/src/protocols/l4/ext.rs b/pingora-core/src/protocols/l4/ext.rs\nindex be81f5563..aefd36c52 100644\n--- a/pingora-core/src/protocols/l4/ext.rs\n+++ b/pingora-core/src/protocols/l4/ext.rs\n@@ -56,13 +56,12 @@ pub struct TCP_INFO {\n     tcpi_rcv_ssthresh: u32,\n     pub tcpi_rtt: u32,\n     tcpi_rttvar: u32,\n-    /* uncomment these field if needed\n     tcpi_snd_ssthresh: u32,\n     tcpi_snd_cwnd: u32,\n     tcpi_advmss: u32,\n     tcpi_reordering: u32,\n     tcpi_rcv_rtt: u32,\n-    tcpi_rcv_space: u32,\n+    pub tcpi_rcv_space: u32,\n     tcpi_total_retrans: u32,\n     tcpi_pacing_rate: u64,\n     tcpi_max_pacing_rate: u64,\n@@ -75,8 +74,19 @@ pub struct TCP_INFO {\n     tcpi_data_segs_in: u32,\n     tcpi_data_segs_out: u32,\n     tcpi_delivery_rate: u64,\n-    */\n-    /* and more, see include/linux/tcp.h */\n+    tcpi_busy_time: u64,\n+    tcpi_rwnd_limited: u64,\n+    tcpi_sndbuf_limited: u64,\n+    tcpi_delivered: u32,\n+    tcpi_delivered_ce: u32,\n+    tcpi_bytes_sent: u64,\n+    tcpi_bytes_retrans: u64,\n+    tcpi_dsack_dups: u32,\n+    tcpi_reord_seen: u32,\n+    tcpi_rcv_ooopack: u32,\n+    tcpi_snd_wnd: u32,\n+    pub tcpi_rcv_wnd: u32,\n+    // and more, see include/linux/tcp.h\n }\n \n impl TCP_INFO {\n@@ -223,6 +233,25 @@ pub fn set_recv_buf(_fd: RawFd, _: usize) -> Result<()> {\n     Ok(())\n }\n \n+#[cfg(target_os = \"linux\")]\n+pub fn get_recv_buf(fd: RawFd) -> io::Result<usize> {\n+    let mut recv_size: c_int = 0;\n+    let mut size = std::mem::size_of::<c_int>() as u32;\n+    get_opt(\n+        fd,\n+        libc::SOL_SOCKET,\n+        libc::SO_RCVBUF,\n+        &mut recv_size,\n+        &mut size,\n+    )?;\n+    Ok(recv_size as usize)\n+}\n+\n+#[cfg(not(target_os = \"linux\"))]\n+pub fn get_recv_buf(_fd: RawFd) -> Result<usize> {\n+    Ok(0)\n+}\n+\n /// Enable client side TCP fast open.\n #[cfg(target_os = \"linux\")]\n pub fn set_tcp_fastopen_connect(fd: RawFd) -> Result<()> {\n@@ -358,18 +387,8 @@ mod test {\n \n         #[cfg(target_os = \"linux\")]\n         {\n-            let mut recv_size: c_int = 0;\n-            let mut size = std::mem::size_of::<c_int>() as u32;\n-            get_opt(\n-                socket.as_raw_fd(),\n-                libc::SOL_SOCKET,\n-                libc::SO_RCVBUF,\n-                &mut recv_size,\n-                &mut size,\n-            )\n-            .unwrap();\n             // kernel doubles whatever is set\n-            assert_eq!(recv_size, 102400 * 2);\n+            assert_eq!(get_recv_buf(socket.as_raw_fd()).unwrap(), 102400 * 2);\n         }\n     }\n \ndiff --git a/pingora-core/src/protocols/l4/stream.rs b/pingora-core/src/protocols/l4/stream.rs\nindex 6169d3115..6a13c6588 100644\n--- a/pingora-core/src/protocols/l4/stream.rs\n+++ b/pingora-core/src/protocols/l4/stream.rs\n@@ -22,7 +22,7 @@ use std::os::unix::io::AsRawFd;\n use std::pin::Pin;\n use std::sync::Arc;\n use std::task::{Context, Poll};\n-use std::time::SystemTime;\n+use std::time::{Duration, Instant, SystemTime};\n use tokio::io::{self, AsyncRead, AsyncWrite, AsyncWriteExt, BufStream, ReadBuf};\n use tokio::net::{TcpStream, UnixStream};\n \n@@ -141,6 +141,8 @@ pub struct Stream {\n     pub established_ts: SystemTime,\n     /// The distributed tracing object for this stream\n     pub tracer: Option<Tracer>,\n+    read_pending_time: AccumulatedDuration,\n+    write_pending_time: AccumulatedDuration,\n }\n \n impl Stream {\n@@ -172,6 +174,8 @@ impl From<TcpStream> for Stream {\n             proxy_digest: None,\n             socket_digest: None,\n             tracer: None,\n+            read_pending_time: AccumulatedDuration::new(),\n+            write_pending_time: AccumulatedDuration::new(),\n         }\n     }\n }\n@@ -185,6 +189,8 @@ impl From<UnixStream> for Stream {\n             proxy_digest: None,\n             socket_digest: None,\n             tracer: None,\n+            read_pending_time: AccumulatedDuration::new(),\n+            write_pending_time: AccumulatedDuration::new(),\n         }\n     }\n }\n@@ -220,6 +226,14 @@ impl GetTimingDigest for Stream {\n         }));\n         digest\n     }\n+\n+    fn get_read_pending_time(&self) -> Duration {\n+        self.read_pending_time.total\n+    }\n+\n+    fn get_write_pending_time(&self) -> Duration {\n+        self.write_pending_time.total\n+    }\n }\n \n impl GetProxyDigest for Stream {\n@@ -282,7 +296,9 @@ impl AsyncRead for Stream {\n         cx: &mut Context<'_>,\n         buf: &mut ReadBuf<'_>,\n     ) -> Poll<io::Result<()>> {\n-        Pin::new(&mut self.stream).poll_read(cx, buf)\n+        let result = Pin::new(&mut self.stream).poll_read(cx, buf);\n+        self.read_pending_time.poll_time(&result);\n+        result\n     }\n }\n \n@@ -292,15 +308,19 @@ impl AsyncWrite for Stream {\n         cx: &mut Context,\n         buf: &[u8],\n     ) -> Poll<io::Result<usize>> {\n-        if self.buffer_write {\n+        let result = if self.buffer_write {\n             Pin::new(&mut self.stream).poll_write(cx, buf)\n         } else {\n             Pin::new(&mut self.stream.get_mut()).poll_write(cx, buf)\n-        }\n+        };\n+        self.write_pending_time.poll_write_time(&result, buf.len());\n+        result\n     }\n \n     fn poll_flush(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<io::Result<()>> {\n-        Pin::new(&mut self.stream).poll_flush(cx)\n+        let result = Pin::new(&mut self.stream).poll_flush(cx);\n+        self.write_pending_time.poll_time(&result);\n+        result\n     }\n \n     fn poll_shutdown(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<io::Result<()>> {\n@@ -312,11 +332,16 @@ impl AsyncWrite for Stream {\n         cx: &mut Context<'_>,\n         bufs: &[std::io::IoSlice<'_>],\n     ) -> Poll<io::Result<usize>> {\n-        if self.buffer_write {\n+        let total_size = bufs.iter().fold(0, |acc, s| acc + s.len());\n+\n+        let result = if self.buffer_write {\n             Pin::new(&mut self.stream).poll_write_vectored(cx, bufs)\n         } else {\n             Pin::new(&mut self.stream.get_mut()).poll_write_vectored(cx, bufs)\n-        }\n+        };\n+\n+        self.write_pending_time.poll_write_time(&result, total_size);\n+        result\n     }\n \n     fn is_write_vectored(&self) -> bool {\n@@ -350,6 +375,12 @@ pub mod async_write_vec {\n         buf: &'a mut B,\n     }\n \n+    #[must_use = \"futures do nothing unless you `.await` or poll them\"]\n+    pub struct WriteVecAll<'a, W, B> {\n+        writer: &'a mut W,\n+        buf: &'a mut B,\n+    }\n+\n     pub trait AsyncWriteVec {\n         fn poll_write_vec<B: Buf>(\n             self: Pin<&mut Self>,\n@@ -367,6 +398,17 @@ pub mod async_write_vec {\n                 buf: src,\n             }\n         }\n+\n+        fn write_vec_all<'a, B>(&'a mut self, src: &'a mut B) -> WriteVecAll<'a, Self, B>\n+        where\n+            Self: Sized,\n+            B: Buf,\n+        {\n+            WriteVecAll {\n+                writer: self,\n+                buf: src,\n+            }\n+        }\n     }\n \n     impl<W, B> Future for WriteVec<'_, W, B>\n@@ -382,6 +424,25 @@ pub mod async_write_vec {\n         }\n     }\n \n+    impl<W, B> Future for WriteVecAll<'_, W, B>\n+    where\n+        W: AsyncWriteVec + Unpin,\n+        B: Buf,\n+    {\n+        type Output = io::Result<()>;\n+\n+        fn poll(mut self: Pin<&mut Self>, ctx: &mut Context<'_>) -> Poll<io::Result<()>> {\n+            let me = &mut *self;\n+            while me.buf.has_remaining() {\n+                let n = ready!(Pin::new(&mut *me.writer).poll_write_vec(ctx, me.buf))?;\n+                if n == 0 {\n+                    return Poll::Ready(Err(io::ErrorKind::WriteZero.into()));\n+                }\n+            }\n+            Poll::Ready(Ok(()))\n+        }\n+    }\n+\n     /* from https://github.com/tokio-rs/tokio/blob/master/tokio-util/src/lib.rs#L177 */\n     impl<T> AsyncWriteVec for T\n     where\n@@ -414,3 +475,55 @@ pub mod async_write_vec {\n }\n \n pub use async_write_vec::AsyncWriteVec;\n+\n+#[derive(Debug)]\n+struct AccumulatedDuration {\n+    total: Duration,\n+    last_start: Option<Instant>,\n+}\n+\n+impl AccumulatedDuration {\n+    fn new() -> Self {\n+        AccumulatedDuration {\n+            total: Duration::ZERO,\n+            last_start: None,\n+        }\n+    }\n+\n+    fn start(&mut self) {\n+        if self.last_start.is_none() {\n+            self.last_start = Some(Instant::now());\n+        }\n+    }\n+\n+    fn stop(&mut self) {\n+        if let Some(start) = self.last_start.take() {\n+            self.total += start.elapsed();\n+        }\n+    }\n+\n+    fn poll_write_time(&mut self, result: &Poll<io::Result<usize>>, buf_size: usize) {\n+        match result {\n+            Poll::Ready(Ok(n)) => {\n+                if *n == buf_size {\n+                    self.stop();\n+                } else {\n+                    // partial write\n+                    self.start();\n+                }\n+            }\n+            // Pending: start the timer, Ready(Err()): does not matter\n+            _ => self.start(),\n+        }\n+    }\n+\n+    fn poll_time(&mut self, result: &Poll<io::Result<()>>) {\n+        match result {\n+            Poll::Ready(Ok(())) => {\n+                self.stop();\n+            }\n+            // Pending: start the timer, Ready(Err()): does not matter\n+            _ => self.start(),\n+        }\n+    }\n+}\ndiff --git a/pingora-core/src/protocols/ssl/client.rs b/pingora-core/src/protocols/ssl/client.rs\nindex 07811dda3..9edf29b2a 100644\n--- a/pingora-core/src/protocols/ssl/client.rs\n+++ b/pingora-core/src/protocols/ssl/client.rs\n@@ -23,6 +23,7 @@ use crate::tls::{ssl, ssl::ConnectConfiguration, ssl_sys::X509_V_ERR_INVALID_CAL\n \n use pingora_error::{Error, ErrorType::*, OrErr, Result};\n use std::sync::Arc;\n+use std::time::Duration;\n \n /// Perform the TLS handshake for the given connection with the given configuration\n pub async fn handshake<S: IO>(\n@@ -82,6 +83,13 @@ where\n         ts_vec.push(Some(self.timing.clone()));\n         ts_vec\n     }\n+    fn get_read_pending_time(&self) -> Duration {\n+        self.get_ref().get_read_pending_time()\n+    }\n+\n+    fn get_write_pending_time(&self) -> Duration {\n+        self.get_ref().get_write_pending_time()\n+    }\n }\n \n impl<S> GetProxyDigest for SslStream<S>\ndiff --git a/pingora-core/src/server/configuration/mod.rs b/pingora-core/src/server/configuration/mod.rs\nindex 5b7609dcb..a4b5cc732 100644\n--- a/pingora-core/src/server/configuration/mod.rs\n+++ b/pingora-core/src/server/configuration/mod.rs\n@@ -119,17 +119,18 @@ impl Default for ServerConf {\n ///\n /// Call `Opt::from_args()` to build this object from the process's command line arguments.\n #[derive(Parser, Debug)]\n-#[clap(name = \"basic\")]\n+#[clap(name = \"basic\", long_about = None)]\n pub struct Opt {\n     /// Whether this server should try to upgrade from a running old server\n     #[clap(\n         short,\n         long,\n-        help = \"This is the base set of command line arguments for a pingora-based service\"\n+        help = \"This is the base set of command line arguments for a pingora-based service\",\n+        long_help = None\n     )]\n     pub upgrade: bool,\n \n-    /// Whether should run this server in the background\n+    /// Whether this server should run in the background\n     #[clap(short, long)]\n     pub daemon: bool,\n \n@@ -149,14 +150,15 @@ pub struct Opt {\n         long,\n         help = \"This flag is useful for upgrading service where the user wants \\\n                 to make sure the new service can start before shutting down \\\n-                the old server process.\"\n+                the old server process.\",\n+        long_help = None\n     )]\n     pub test: bool,\n \n     /// The path to the configuration file.\n     ///\n     /// See [`ServerConf`] for more details of the configuration file.\n-    #[clap(short, long, help = \"The path to the configuration file.\")]\n+    #[clap(short, long, help = \"The path to the configuration file.\", long_help = None)]\n     pub conf: Option<String>,\n }\n \ndiff --git a/pingora-core/src/server/transfer_fd/mod.rs b/pingora-core/src/server/transfer_fd/mod.rs\nindex 969d9cc31..ee63ac02e 100644\n--- a/pingora-core/src/server/transfer_fd/mod.rs\n+++ b/pingora-core/src/server/transfer_fd/mod.rs\n@@ -77,7 +77,7 @@ impl Fds {\n     {\n         let mut de_buf: [u8; 2048] = [0; 2048];\n         let (fds, bytes) = get_fds_from(path, &mut de_buf)?;\n-        let keys = deserialize_vec_string(&de_buf[..bytes]);\n+        let keys = deserialize_vec_string(&de_buf[..bytes])?;\n         self.deserialize(keys, fds);\n         Ok(())\n     }\n@@ -91,13 +91,9 @@ fn serialize_vec_string(vec_string: &[String], mut buf: &mut [u8]) -> usize {\n     buf.write(joined.as_bytes()).unwrap()\n }\n \n-fn deserialize_vec_string(buf: &[u8]) -> Vec<String> {\n-    let joined = std::str::from_utf8(buf).unwrap(); // TODO: handle error\n-    let mut results: Vec<String> = Vec::new();\n-    for iter in joined.split_ascii_whitespace() {\n-        results.push(String::from(iter));\n-    }\n-    results\n+fn deserialize_vec_string(buf: &[u8]) -> Result<Vec<String>, Error> {\n+    let joined = std::str::from_utf8(buf).map_err(|_| Error::EINVAL)?;\n+    Ok(joined.split_ascii_whitespace().map(String::from).collect())\n }\n \n #[cfg(target_os = \"linux\")]\n@@ -369,7 +365,7 @@ mod tests {\n         let vec_str: Vec<String> = vec![\"aaaa\".to_string(), \"bbb\".to_string()];\n         let mut ser_buf: [u8; 1024] = [0; 1024];\n         let size = serialize_vec_string(&vec_str, &mut ser_buf);\n-        let de_vec_string = deserialize_vec_string(&ser_buf[..size]);\n+        let de_vec_string = deserialize_vec_string(&ser_buf[..size]).unwrap();\n         assert_eq!(de_vec_string.len(), 2);\n         assert_eq!(de_vec_string[0], \"aaaa\");\n         assert_eq!(de_vec_string[1], \"bbb\");\ndiff --git a/pingora-core/src/services/listening.rs b/pingora-core/src/services/listening.rs\nindex 54373230d..ea81799b9 100644\n--- a/pingora-core/src/services/listening.rs\n+++ b/pingora-core/src/services/listening.rs\n@@ -35,29 +35,29 @@ use std::sync::Arc;\n pub struct Service<A> {\n     name: String,\n     listeners: Listeners,\n-    app_logic: Arc<A>,\n+    app_logic: Option<A>,\n     /// The number of preferred threads. `None` to follow global setting.\n     pub threads: Option<usize>,\n }\n \n impl<A> Service<A> {\n     /// Create a new [`Service`] with the given application (see [`crate::apps`]).\n-    pub fn new(name: String, app_logic: Arc<A>) -> Self {\n+    pub fn new(name: String, app_logic: A) -> Self {\n         Service {\n             name,\n             listeners: Listeners::new(),\n-            app_logic,\n+            app_logic: Some(app_logic),\n             threads: None,\n         }\n     }\n \n     /// Create a new [`Service`] with the given application (see [`crate::apps`]) and the given\n     /// [`Listeners`].\n-    pub fn with_listeners(name: String, listeners: Listeners, app_logic: Arc<A>) -> Self {\n+    pub fn with_listeners(name: String, listeners: Listeners, app_logic: A) -> Self {\n         Service {\n             name,\n             listeners,\n-            app_logic,\n+            app_logic: Some(app_logic),\n             threads: None,\n         }\n     }\n@@ -107,6 +107,16 @@ impl<A> Service<A> {\n     pub fn add_address(&mut self, addr: ServerAddress) {\n         self.listeners.add_address(addr);\n     }\n+\n+    /// Get a reference to the application inside this service\n+    pub fn app_logic(&self) -> Option<&A> {\n+        self.app_logic.as_ref()\n+    }\n+\n+    /// Get a mutable reference to the application inside this service\n+    pub fn app_logic_mut(&mut self) -> Option<&mut A> {\n+        self.app_logic.as_mut()\n+    }\n }\n \n impl<A: ServerApp + Send + Sync + 'static> Service<A> {\n@@ -194,18 +204,23 @@ impl<A: ServerApp + Send + Sync + 'static> ServiceTrait for Service<A> {\n     async fn start_service(&mut self, fds: Option<ListenFds>, shutdown: ShutdownWatch) {\n         let runtime = current_handle();\n         let endpoints = self.listeners.build(fds);\n+        let app_logic = self\n+            .app_logic\n+            .take()\n+            .expect(\"can only start_service() once\");\n+        let app_logic = Arc::new(app_logic);\n \n         let handlers = endpoints.into_iter().map(|endpoint| {\n-            let app_logic = self.app_logic.clone();\n             let shutdown = shutdown.clone();\n+            let my_app_logic = app_logic.clone();\n             runtime.spawn(async move {\n-                Self::run_endpoint(app_logic, endpoint, shutdown).await;\n+                Self::run_endpoint(my_app_logic, endpoint, shutdown).await;\n             })\n         });\n \n         futures::future::join_all(handlers).await;\n         self.listeners.cleanup();\n-        self.app_logic.cleanup().await;\n+        app_logic.cleanup().await;\n     }\n \n     fn name(&self) -> &str {\n@@ -226,7 +241,7 @@ impl Service<PrometheusServer> {\n     pub fn prometheus_http_service() -> Self {\n         Service::new(\n             \"Prometheus metric HTTP\".to_string(),\n-            Arc::new(PrometheusServer::new()),\n+            PrometheusServer::new(),\n         )\n     }\n }\ndiff --git a/pingora-proxy/src/lib.rs b/pingora-proxy/src/lib.rs\nindex a60e6a7fb..eaf31d52b 100644\n--- a/pingora-proxy/src/lib.rs\n+++ b/pingora-proxy/src/lib.rs\n@@ -96,12 +96,12 @@ pub struct HttpProxy<SV> {\n }\n \n impl<SV> HttpProxy<SV> {\n-    fn new(inner: SV, conf: Arc<ServerConf>) -> Arc<Self> {\n-        Arc::new(HttpProxy {\n+    fn new(inner: SV, conf: Arc<ServerConf>) -> Self {\n+        HttpProxy {\n             inner,\n             client_upstream: Connector::new(Some(ConnectorOptions::from_server_conf(&conf))),\n             shutdown: Notify::new(),\n-        })\n+        }\n     }\n \n     async fn handle_new_request(\ndiff --git a/pingora/examples/app/echo.rs b/pingora/examples/app/echo.rs\nindex 61f94e5c6..09725c48d 100644\n--- a/pingora/examples/app/echo.rs\n+++ b/pingora/examples/app/echo.rs\n@@ -86,13 +86,3 @@ impl ServeHttp for HttpEchoApp {\n             .unwrap()\n     }\n }\n-\n-impl EchoApp {\n-    pub fn new() -> Arc<Self> {\n-        Arc::new(EchoApp {})\n-    }\n-}\n-\n-pub fn new_http_echo_app() -> Arc<HttpEchoApp> {\n-    Arc::new(HttpEchoApp {})\n-}\ndiff --git a/pingora/examples/service/echo.rs b/pingora/examples/service/echo.rs\nindex 8126088a7..af07da217 100644\n--- a/pingora/examples/service/echo.rs\n+++ b/pingora/examples/service/echo.rs\n@@ -12,13 +12,13 @@\n // See the License for the specific language governing permissions and\n // limitations under the License.\n \n-use crate::app::echo::{new_http_echo_app, EchoApp, HttpEchoApp};\n+use crate::app::echo::{EchoApp, HttpEchoApp};\n use pingora::services::listening::Service;\n \n pub fn echo_service() -> Service<EchoApp> {\n-    Service::new(\"Echo Service\".to_string(), EchoApp::new())\n+    Service::new(\"Echo Service\".to_string(), EchoApp)\n }\n \n pub fn echo_service_http() -> Service<HttpEchoApp> {\n-    Service::new(\"Echo Service HTTP\".to_string(), new_http_echo_app())\n+    Service::new(\"Echo Service HTTP\".to_string(), HttpEchoApp)\n }\ndiff --git a/pingora/examples/service/proxy.rs b/pingora/examples/service/proxy.rs\nindex a103b4776..49c24ca02 100644\n--- a/pingora/examples/service/proxy.rs\n+++ b/pingora/examples/service/proxy.rs\n@@ -16,7 +16,6 @@ use crate::app::proxy::ProxyApp;\n use pingora_core::listeners::Listeners;\n use pingora_core::services::listening::Service;\n use pingora_core::upstreams::peer::BasicPeer;\n-use std::sync::Arc;\n \n pub fn proxy_service(addr: &str, proxy_addr: &str) -> Service<ProxyApp> {\n     let proxy_to = BasicPeer::new(proxy_addr);\n@@ -24,7 +23,7 @@ pub fn proxy_service(addr: &str, proxy_addr: &str) -> Service<ProxyApp> {\n     Service::with_listeners(\n         \"Proxy Service\".to_string(),\n         Listeners::tcp(addr),\n-        Arc::new(ProxyApp::new(proxy_to)),\n+        ProxyApp::new(proxy_to),\n     )\n }\n \n@@ -41,6 +40,6 @@ pub fn proxy_service_tls(\n     Service::with_listeners(\n         \"Proxy Service TLS\".to_string(),\n         Listeners::tls(addr, cert_path, key_path).unwrap(),\n-        Arc::new(ProxyApp::new(proxy_to)),\n+        ProxyApp::new(proxy_to),\n     )\n }\ndiff --git a/tinyufo/Cargo.toml b/tinyufo/Cargo.toml\nindex 0326fe540..68c4236b5 100644\n--- a/tinyufo/Cargo.toml\n+++ b/tinyufo/Cargo.toml\n@@ -29,6 +29,7 @@ zipf = \"7\"\n moka = { version = \"0\", features = [\"sync\"] }\n dhat = \"0\"\n quick_cache = \"0.4\"\n+triomphe = \"<=0.1.11\" # 0.1.12 requires Rust 1.76\n \n [[bench]]\n name = \"bench_perf\"\n", "test_patch": "diff --git a/pingora-core/tests/utils/mod.rs b/pingora-core/tests/utils/mod.rs\nindex af5de8544..d254e9469 100644\n--- a/pingora-core/tests/utils/mod.rs\n+++ b/pingora-core/tests/utils/mod.rs\n@@ -25,7 +25,6 @@ use async_trait::async_trait;\n use bytes::Bytes;\n use http::{Response, StatusCode};\n use pingora_timeout::timeout;\n-use std::sync::Arc;\n use std::time::Duration;\n \n use pingora_core::apps::http_app::ServeHttp;\n@@ -63,10 +62,6 @@ impl ServeHttp for EchoApp {\n     }\n }\n \n-pub fn new_http_echo_app() -> Arc<EchoApp> {\n-    Arc::new(EchoApp {})\n-}\n-\n pub struct MyServer {\n     pub handle: thread::JoinHandle<()>,\n }\n@@ -88,11 +83,8 @@ fn entry_point(opt: Option<Opt>) {\n     tls_settings.enable_h2();\n     listeners.add_tls_with_settings(\"0.0.0.0:6146\", None, tls_settings);\n \n-    let echo_service_http = Service::with_listeners(\n-        \"Echo Service HTTP\".to_string(),\n-        listeners,\n-        new_http_echo_app(),\n-    );\n+    let echo_service_http =\n+        Service::with_listeners(\"Echo Service HTTP\".to_string(), listeners, EchoApp);\n \n     my_server.add_service(echo_service_http);\n     my_server.run_forever();\n", "problem_statement": "Raw Markdown displayed in command line help\n## Describe the bug\r\n\r\nRunning a Pingora server with `--help` command line flag will produce descriptions including raw Markdown. Also, the names of command line flags unnecessarily. These are meant for online documentation and often make little sense as a command line help.\r\n\r\n## Pingora info\r\n\r\nPlease include the following information about your environment:\r\n\r\n**Pingora version**: https://github.com/cloudflare/pingora/commit/2501d4adb038d93613c0edbd7c1e3b3de9b415b1\r\n**Rust version**: 1.79.0-nightly (28e7b2bc0 2024-04-05)\r\n**Operating system version**: Fedora 39\r\n\r\n## Steps to reproduce\r\n\r\nRun `cargo run --example gateway -- --help` from the command line.\r\n\r\n## Expected results\r\n\r\nA human-readable output without any duplicated information.\r\n\r\n## Observed results\r\n\r\nRaw Markdown and duplicated information on command line flags, pointless references to code (the command description in particular is meaningless) and references to code (`ServerConf`) that should have really been URLs:\r\n\r\n```\r\nbasic 0.1.0\r\nCommand-line options\r\n\r\nCall `Opt::from_args()` to build this object from the process's command line arguments.\r\n\r\nUSAGE:\r\n    gateway [FLAGS] [OPTIONS]\r\n\r\nFLAGS:\r\n    -d, --daemon       \r\n            Whether should run this server in the background\r\n            \r\n            `-d` or `--daemon` can be used\r\n    -h, --help         \r\n            Prints help information\r\n\r\n        --nocapture    \r\n            Not actually used. This flag is there so that the server is not upset seeing this flag passed from `cargo\r\n            test` sometimes\r\n    -t, --test         \r\n            Test the configuration and exit\r\n            \r\n            When this flag is set, calling `server.bootstrap()` will exit the process without errors\r\n            \r\n            This flag is useful for upgrading service where the user wants to make sure the new service can start before\r\n            shutting down the old server process.\r\n            \r\n            `-t` or `--test` can be used\r\n    -u, --upgrade      \r\n            Whether this server should try to upgrade from a running old server\r\n            \r\n            `-u` or `--upgrade` can be used\r\n    -V, --version      \r\n            Prints version information\r\n\r\n\r\nOPTIONS:\r\n    -c, --conf <conf>    \r\n            The path to the configuration file.\r\n            \r\n            See [`ServerConf`] for more details of the configuration file.\r\n            \r\n            `-c` or `--conf` can be used\r\n```\r\n\r\n## Additional context\r\n\r\nThe docstrings that this info is compiled from should really be adjusted. While an application extending the default command line options will overwrite the application description, adjusting the description of individual options is non-trivial.\r\n\r\nAlternatively, the docstrings can be left unchanged and [structopt help messages](https://docs.rs/structopt/latest/structopt/#help-messages) can be specified separately.\n", "hints_text": "", "created_at": "2024-05-31 16:33:39", "merge_commit_sha": "216d8e9d92c75d46cba78da9b167fb41dec91ad7", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['pingora', '.github/workflows/build.yml']", "['pingora (stable)', '.github/workflows/build.yml']"]]}
{"repo": "dusk-network/rusk", "instance_id": "dusk-network__rusk-3441", "base_commit": "2db27ecdd9614605ca2fd83a5a7370a0d0900706", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex bdd54ead73..76783d94b6 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -12,6 +12,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n ### Changed\n \n - Update Transactions list design [#1922]\n+- Auto-focus text field (Unlock page) [#3420]\n \n ### Removed\n \n@@ -563,6 +564,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3362]: https://github.com/dusk-network/rusk/issues/3362\n [#3381]: https://github.com/dusk-network/rusk/issues/3381\n [#3387]: https://github.com/dusk-network/rusk/issues/3387\n+[#3420]: https://github.com/dusk-network/rusk/issues/3420\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/routes/(welcome)/unlock/+page.svelte b/web-wallet/src/routes/(welcome)/unlock/+page.svelte\nindex 2734f30249..57912e4448 100644\n--- a/web-wallet/src/routes/(welcome)/unlock/+page.svelte\n+++ b/web-wallet/src/routes/(welcome)/unlock/+page.svelte\n@@ -111,11 +111,13 @@\n         <Textbox\n           bind:this={fldSecret}\n           bind:value={secretText}\n-          name={loginInfo ? \"password\" : \"mnemonic\"}\n+          name={modeLabel}\n+          aria-label={modeLabel}\n           placeholder={modeLabel}\n           required\n           type=\"password\"\n           autocomplete=\"current-password\"\n+          autofocus\n         />\n         {#if error instanceof InvalidMnemonicError}\n           <Banner title=\"Invalid mnemonic phrase\" variant=\"error\">\n", "test_patch": "diff --git a/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap b/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\nindex ee4d23a961..ac5d1ee29a 100644\n--- a/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\n+++ b/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\n@@ -67,9 +67,11 @@ exports[`Unlock Wallet > Mnemonic phrase workflow > should render the Unlock Wal\n           class=\"login__form svelte-1e8gc2s\"\n         >\n           <input\n+            aria-label=\"Mnemonic phrase\"\n             autocomplete=\"current-password\"\n+            autofocus=\"\"\n             class=\"dusk-textbox dusk-textbox-password\"\n-            name=\"mnemonic\"\n+            name=\"Mnemonic phrase\"\n             placeholder=\"Mnemonic phrase\"\n             required=\"\"\n             type=\"password\"\n@@ -193,9 +195,11 @@ exports[`Unlock Wallet > Password workflow > should show the password field and\n           class=\"login__form svelte-1e8gc2s\"\n         >\n           <input\n+            aria-label=\"Password\"\n             autocomplete=\"current-password\"\n+            autofocus=\"\"\n             class=\"dusk-textbox dusk-textbox-password\"\n-            name=\"password\"\n+            name=\"Password\"\n             placeholder=\"Password\"\n             required=\"\"\n             type=\"password\"\n", "problem_statement": "After clicking \"Unlock Wallet,\" the focus should automatically be placed on the password input field\nSummary\n\ud83d\udca1 After clicking \"Unlock Wallet,\" the focus should automatically be placed on the password input field. This small improvement aligns with good UX practices by eliminating an unnecessary click and streamlining the user flow.\n\nPossible solution design or implementation\n\ud83d\udca1 Set focus on the password input field programmatically after the \"Unlock Wallet\" button is clicked. For example:\n\nUse JavaScript to call the .focus() method on the password input field once the modal or relevant section is displayed.\nEnsure this behavior is applied across all views or scenarios where the \"Unlock Wallet\" action is available.\nAlternative: Evaluate if screen readers also benefit from this behavior for accessibility compliance.\n\nAdditional context\n\ud83d\udca1 This is a quick win with minimal implementation complexity but a notable impact on user experience. Removing small friction points like an additional click improves the overall perception and usability of the application.\n\n", "hints_text": "Good point :) Thanks!", "created_at": "2025-01-29 11:55:06", "merge_commit_sha": "539cf2b47363495b74ff860791b62cfcebf46029", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['changes', '.github/workflows/ruskwallet_ci.yml']", "['[Windows] Nightly tests', '.github/workflows/ruskwallet_ci.yml']"], ["['Node 20.x', '.github/workflows/explorer_ci.yml']", "['changes', '.github/workflows/w3sperjs_ci.yml']"], ["['[Mac arm64] Nightly checks', '.github/workflows/ruskwallet_ci.yml']", "['Node 20.x', '.github/workflows/webwallet_ci.yml']"], ["['Clippy check release', '.github/workflows/rusk_ci.yml']", "['changes', '.github/workflows/webwallet_ci.yml']"]]}
{"repo": "cloudflare/pingora", "instance_id": "cloudflare__pingora-257", "base_commit": "479d9badbfdfe29f1d810299057477250c69c98d", "patch": "diff --git a/.bleep b/.bleep\nindex 5b3f05fd5..fa13307f1 100644\n--- a/.bleep\n+++ b/.bleep\n@@ -1,1 +1,1 @@\n-719b4dcafd471ecfa9e4ade3abfb217929afddc6\n\\ No newline at end of file\n+946e1733bc009bf3e73c635ad1b8bac0079dc680\n\\ No newline at end of file\ndiff --git a/pingora-core/src/apps/http_app.rs b/pingora-core/src/apps/http_app.rs\nindex 4dc905939..c758a1494 100644\n--- a/pingora-core/src/apps/http_app.rs\n+++ b/pingora-core/src/apps/http_app.rs\n@@ -161,13 +161,16 @@ where\n         }\n         let mut module_ctx = self.modules.build_ctx();\n         let req = http.req_header_mut();\n-        module_ctx.request_header_filter(req).ok()?;\n+        module_ctx.request_header_filter(req).await.ok()?;\n         let new_response = self.app.response(&mut http).await;\n         let (parts, body) = new_response.into_parts();\n-        let resp_header: ResponseHeader = parts.into();\n-        let mut task = HttpTask::Header(Box::new(resp_header), body.is_empty());\n-        module_ctx.response_filter(&mut task).ok()?;\n+        let mut resp_header: ResponseHeader = parts.into();\n+        module_ctx\n+            .response_header_filter(&mut resp_header, body.is_empty())\n+            .await\n+            .ok()?;\n \n+        let task = HttpTask::Header(Box::new(resp_header), body.is_empty());\n         trace!(\"{task:?}\");\n \n         match http.response_duplex_vec(vec![task]).await {\n@@ -181,15 +184,13 @@ where\n                 );\n             }\n         }\n-        let mut task = if !body.is_empty() {\n-            HttpTask::Body(Some(body.into()), true)\n-        } else {\n-            HttpTask::Body(None, true)\n-        };\n \n-        trace!(\"{task:?}\");\n+        let mut body = Some(body.into());\n+        module_ctx.response_body_filter(&mut body, true).ok()?;\n \n-        module_ctx.response_filter(&mut task).ok()?;\n+        let task = HttpTask::Body(body, true);\n+\n+        trace!(\"{task:?}\");\n \n         // TODO: check if chunked encoding is needed\n         match http.response_duplex_vec(vec![task]).await {\ndiff --git a/pingora-core/src/modules/http/compression.rs b/pingora-core/src/modules/http/compression.rs\nindex b07e1c730..134f94fe3 100644\n--- a/pingora-core/src/modules/http/compression.rs\n+++ b/pingora-core/src/modules/http/compression.rs\n@@ -20,6 +20,7 @@ use crate::protocols::http::compression::ResponseCompressionCtx;\n /// HTTP response compression module\n pub struct ResponseCompression(ResponseCompressionCtx);\n \n+#[async_trait]\n impl HttpModule for ResponseCompression {\n     fn as_any(&self) -> &dyn std::any::Any {\n         self\n@@ -28,13 +29,30 @@ impl HttpModule for ResponseCompression {\n         self\n     }\n \n-    fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+    async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n         self.0.request_filter(req);\n         Ok(())\n     }\n \n-    fn response_filter(&mut self, t: &mut HttpTask) -> Result<()> {\n-        self.0.response_filter(t);\n+    async fn response_header_filter(\n+        &mut self,\n+        resp: &mut ResponseHeader,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        self.0.response_header_filter(resp, end_of_stream);\n+        Ok(())\n+    }\n+\n+    fn response_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        if !self.0.is_enabled() {\n+            return Ok(());\n+        }\n+        let compressed = self.0.response_body_filter(body.as_ref(), end_of_stream);\n+        *body = compressed;\n         Ok(())\n     }\n }\ndiff --git a/pingora-core/src/modules/http/mod.rs b/pingora-core/src/modules/http/mod.rs\nindex 0f0401b22..31c5f9a21 100644\n--- a/pingora-core/src/modules/http/mod.rs\n+++ b/pingora-core/src/modules/http/mod.rs\n@@ -20,28 +20,44 @@\n \n pub mod compression;\n \n-use crate::protocols::http::HttpTask;\n+use async_trait::async_trait;\n use bytes::Bytes;\n use once_cell::sync::OnceCell;\n use pingora_error::Result;\n-use pingora_http::RequestHeader;\n+use pingora_http::{RequestHeader, ResponseHeader};\n use std::any::Any;\n use std::any::TypeId;\n use std::collections::HashMap;\n use std::sync::Arc;\n \n /// The trait an HTTP traffic module needs to implement\n-// TODO: * async filters for, e.g., 3rd party auth server; * access the connection for, e.g., GeoIP\n+#[async_trait]\n pub trait HttpModule {\n-    fn request_header_filter(&mut self, _req: &mut RequestHeader) -> Result<()> {\n+    async fn request_header_filter(&mut self, _req: &mut RequestHeader) -> Result<()> {\n         Ok(())\n     }\n \n-    fn request_body_filter(&mut self, body: Option<Bytes>) -> Result<Option<Bytes>> {\n-        Ok(body)\n+    async fn request_body_filter(\n+        &mut self,\n+        _body: &mut Option<Bytes>,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n+        Ok(())\n+    }\n+\n+    async fn response_header_filter(\n+        &mut self,\n+        _resp: &mut ResponseHeader,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n+        Ok(())\n     }\n \n-    fn response_filter(&mut self, _t: &mut HttpTask) -> Result<()> {\n+    fn response_body_filter(\n+        &mut self,\n+        _body: &mut Option<Bytes>,\n+        _end_of_stream: bool,\n+    ) -> Result<()> {\n         Ok(())\n     }\n \n@@ -168,25 +184,45 @@ impl HttpModuleCtx {\n     }\n \n     /// Run the `request_header_filter` for all the modules according to their orders.\n-    pub fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+    pub async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            filter.request_header_filter(req)?;\n+            filter.request_header_filter(req).await?;\n         }\n         Ok(())\n     }\n \n     /// Run the `request_body_filter` for all the modules according to their orders.\n-    pub fn request_body_filter(&mut self, mut body: Option<Bytes>) -> Result<Option<Bytes>> {\n+    pub async fn request_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            body = filter.request_body_filter(body)?;\n+            filter.request_body_filter(body, end_of_stream).await?;\n         }\n-        Ok(body)\n+        Ok(())\n     }\n \n-    /// Run the `response_filter` for all the modules according to their orders.\n-    pub fn response_filter(&mut self, t: &mut HttpTask) -> Result<()> {\n+    /// Run the `response_header_filter` for all the modules according to their orders.\n+    pub async fn response_header_filter(\n+        &mut self,\n+        req: &mut ResponseHeader,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n         for filter in self.module_ctx.iter_mut() {\n-            filter.response_filter(t)?;\n+            filter.response_header_filter(req, end_of_stream).await?;\n+        }\n+        Ok(())\n+    }\n+\n+    /// Run the `response_body_filter` for all the modules according to their orders.\n+    pub fn response_body_filter(\n+        &mut self,\n+        body: &mut Option<Bytes>,\n+        end_of_stream: bool,\n+    ) -> Result<()> {\n+        for filter in self.module_ctx.iter_mut() {\n+            filter.response_body_filter(body, end_of_stream)?;\n         }\n         Ok(())\n     }\n@@ -197,6 +233,7 @@ mod tests {\n     use super::*;\n \n     struct MyModule;\n+    #[async_trait]\n     impl HttpModule for MyModule {\n         fn as_any(&self) -> &dyn Any {\n             self\n@@ -204,7 +241,7 @@ mod tests {\n         fn as_any_mut(&mut self) -> &mut dyn Any {\n             self\n         }\n-        fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+        async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n             req.insert_header(\"my-filter\", \"1\")\n         }\n     }\n@@ -220,6 +257,7 @@ mod tests {\n     }\n \n     struct MyOtherModule;\n+    #[async_trait]\n     impl HttpModule for MyOtherModule {\n         fn as_any(&self) -> &dyn Any {\n             self\n@@ -227,7 +265,7 @@ mod tests {\n         fn as_any_mut(&mut self) -> &mut dyn Any {\n             self\n         }\n-        fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n+        async fn request_header_filter(&mut self, req: &mut RequestHeader) -> Result<()> {\n             if req.headers.get(\"my-filter\").is_some() {\n                 // if this MyOtherModule runs after MyModule\n                 req.insert_header(\"my-filter\", \"2\")\n@@ -262,14 +300,14 @@ mod tests {\n         assert!(ctx.get_mut::<usize>().is_none());\n     }\n \n-    #[test]\n-    fn test_module_filter() {\n+    #[tokio::test]\n+    async fn test_module_filter() {\n         let mut http_module = HttpModules::new();\n         http_module.add_module(Box::new(MyOtherModuleBuilder));\n         http_module.add_module(Box::new(MyModuleBuilder));\n         let mut ctx = http_module.build_ctx();\n         let mut req = RequestHeader::build(\"Get\", b\"/\", None).unwrap();\n-        ctx.request_header_filter(&mut req).unwrap();\n+        ctx.request_header_filter(&mut req).await.unwrap();\n         // MyModule runs before MyOtherModule\n         assert_eq!(req.headers.get(\"my-filter\").unwrap(), \"2\");\n         assert!(req.headers.get(\"my-other-filter\").is_none());\ndiff --git a/pingora-core/src/protocols/digest.rs b/pingora-core/src/protocols/digest.rs\nindex 594dbba74..e080a89ab 100644\n--- a/pingora-core/src/protocols/digest.rs\n+++ b/pingora-core/src/protocols/digest.rs\n@@ -15,10 +15,11 @@\n //! Extra information about the connection\n \n use std::sync::Arc;\n-use std::time::SystemTime;\n+use std::time::{Duration, SystemTime};\n \n use once_cell::sync::OnceCell;\n \n+use super::l4::ext::{get_recv_buf, get_tcp_info, TCP_INFO};\n use super::l4::socket::SocketAddr;\n use super::raw_connect::ProxyDigest;\n use super::ssl::digest::SslDigest;\n@@ -88,12 +89,38 @@ impl SocketDigest {\n             .get_or_init(|| SocketAddr::from_raw_fd(self.raw_fd, false))\n             .as_ref()\n     }\n+\n+    fn is_inet(&self) -> bool {\n+        self.local_addr().and_then(|p| p.as_inet()).is_some()\n+    }\n+\n+    pub fn tcp_info(&self) -> Option<TCP_INFO> {\n+        if self.is_inet() {\n+            get_tcp_info(self.raw_fd).ok()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    pub fn get_recv_buf(&self) -> Option<usize> {\n+        if self.is_inet() {\n+            get_recv_buf(self.raw_fd).ok()\n+        } else {\n+            None\n+        }\n+    }\n }\n \n /// The interface to return timing information\n pub trait GetTimingDigest {\n     /// Return the timing for each layer from the lowest layer to upper\n     fn get_timing_digest(&self) -> Vec<Option<TimingDigest>>;\n+    fn get_read_pending_time(&self) -> Duration {\n+        Duration::ZERO\n+    }\n+    fn get_write_pending_time(&self) -> Duration {\n+        Duration::ZERO\n+    }\n }\n \n /// The interface to set or return proxy information\ndiff --git a/pingora-core/src/protocols/http/client.rs b/pingora-core/src/protocols/http/client.rs\nindex 83a7618f6..5c44b6f2d 100644\n--- a/pingora-core/src/protocols/http/client.rs\n+++ b/pingora-core/src/protocols/http/client.rs\n@@ -19,7 +19,7 @@ use std::time::Duration;\n \n use super::v1::client::HttpSession as Http1Session;\n use super::v2::client::Http2Session;\n-use crate::protocols::{Digest, SocketAddr};\n+use crate::protocols::{Digest, SocketAddr, Stream};\n \n /// A type for Http client session. It can be either an Http1 connection or an Http2 stream.\n pub enum HttpSession {\n@@ -174,4 +174,13 @@ impl HttpSession {\n             Self::H2(s) => s.client_addr(),\n         }\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP/1 session is operating upon.\n+    /// None if the HTTP session is over H2\n+    pub fn stream(&self) -> Option<&Stream> {\n+        match self {\n+            Self::H1(s) => Some(s.stream()),\n+            Self::H2(_) => None,\n+        }\n+    }\n }\ndiff --git a/pingora-core/src/protocols/http/compression/mod.rs b/pingora-core/src/protocols/http/compression/mod.rs\nindex 26d4fb8e0..6cbd7388d 100644\n--- a/pingora-core/src/protocols/http/compression/mod.rs\n+++ b/pingora-core/src/protocols/http/compression/mod.rs\n@@ -159,7 +159,11 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n-    fn response_header_filter(&mut self, resp: &mut ResponseHeader, end: bool) {\n+    /// Feed the response header into this ctx\n+    pub fn response_header_filter(&mut self, resp: &mut ResponseHeader, end: bool) {\n+        if !self.is_enabled() {\n+            return;\n+        }\n         match &self.0 {\n             CtxInner::HeaderPhase {\n                 compression_level,\n@@ -195,7 +199,8 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n-    fn response_body_filter(&mut self, data: Option<&Bytes>, end: bool) -> Option<Bytes> {\n+    /// Stream the response body chunks into this ctx. The return value will be the compressed data\n+    pub fn response_body_filter(&mut self, data: Option<&Bytes>, end: bool) -> Option<Bytes> {\n         match &mut self.0 {\n             CtxInner::HeaderPhase {\n                 compression_level: _,\n@@ -222,6 +227,7 @@ impl ResponseCompressionCtx {\n         }\n     }\n \n+    // TODO: retire this function, replace it with the two functions above\n     /// Feed the response into this ctx.\n     /// This filter will mutate the response accordingly if encoding is needed.\n     pub fn response_filter(&mut self, t: &mut HttpTask) {\ndiff --git a/pingora-core/src/protocols/http/server.rs b/pingora-core/src/protocols/http/server.rs\nindex a9619a88b..8dfb103d6 100644\n--- a/pingora-core/src/protocols/http/server.rs\n+++ b/pingora-core/src/protocols/http/server.rs\n@@ -370,4 +370,13 @@ impl Session {\n             Self::H2(s) => s.server_addr(),\n         }\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP/1 session is operating upon.\n+    /// None if the HTTP session is over H2\n+    pub fn stream(&self) -> Option<&Stream> {\n+        match self {\n+            Self::H1(s) => Some(s.stream()),\n+            Self::H2(_) => None,\n+        }\n+    }\n }\ndiff --git a/pingora-core/src/protocols/http/v1/body.rs b/pingora-core/src/protocols/http/v1/body.rs\nindex c317bb728..0ddc90a28 100644\n--- a/pingora-core/src/protocols/http/v1/body.rs\n+++ b/pingora-core/src/protocols/http/v1/body.rs\n@@ -538,20 +538,10 @@ impl BodyWriter {\n \n                 let chuck_size_buf = format!(\"{:X}\\r\\n\", chunk_size);\n                 let mut output_buf = Bytes::from(chuck_size_buf).chain(buf).chain(&b\"\\r\\n\"[..]);\n-\n-                while output_buf.has_remaining() {\n-                    let res = stream.write_vec(&mut output_buf).await;\n-                    match res {\n-                        Ok(n) => {\n-                            if n == 0 {\n-                                return Error::e_explain(ConnectionClosed, \"while writing body\");\n-                            }\n-                        }\n-                        Err(e) => {\n-                            return Error::e_because(WriteError, \"while writing body\", e);\n-                        }\n-                    }\n-                }\n+                stream\n+                    .write_vec_all(&mut output_buf)\n+                    .await\n+                    .or_err(WriteError, \"while writing body\")?;\n                 stream.flush().await.or_err(WriteError, \"flushing body\")?;\n                 self.body_mode = BM::ChunkedEncoding(written + chunk_size);\n                 Ok(Some(chunk_size))\ndiff --git a/pingora-core/src/protocols/http/v1/client.rs b/pingora-core/src/protocols/http/v1/client.rs\nindex fa7710f4d..41cf5901f 100644\n--- a/pingora-core/src/protocols/http/v1/client.rs\n+++ b/pingora-core/src/protocols/http/v1/client.rs\n@@ -641,6 +641,11 @@ impl HttpSession {\n             .as_ref()\n             .map(|d| d.local_addr())?\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP session is operating upon.\n+    pub fn stream(&self) -> &Stream {\n+        &self.underlying_stream\n+    }\n }\n \n #[inline]\ndiff --git a/pingora-core/src/protocols/http/v1/server.rs b/pingora-core/src/protocols/http/v1/server.rs\nindex 6f8c96ce2..5168456d4 100644\n--- a/pingora-core/src/protocols/http/v1/server.rs\n+++ b/pingora-core/src/protocols/http/v1/server.rs\n@@ -926,6 +926,11 @@ impl HttpSession {\n         }\n         Ok(end_stream)\n     }\n+\n+    /// Get the reference of the [Stream] that this HTTP session is operating upon.\n+    pub fn stream(&self) -> &Stream {\n+        &self.underlying_stream\n+    }\n }\n \n // Regex to parse request line that has illegal chars in it\ndiff --git a/pingora-core/src/protocols/l4/ext.rs b/pingora-core/src/protocols/l4/ext.rs\nindex be81f5563..aefd36c52 100644\n--- a/pingora-core/src/protocols/l4/ext.rs\n+++ b/pingora-core/src/protocols/l4/ext.rs\n@@ -56,13 +56,12 @@ pub struct TCP_INFO {\n     tcpi_rcv_ssthresh: u32,\n     pub tcpi_rtt: u32,\n     tcpi_rttvar: u32,\n-    /* uncomment these field if needed\n     tcpi_snd_ssthresh: u32,\n     tcpi_snd_cwnd: u32,\n     tcpi_advmss: u32,\n     tcpi_reordering: u32,\n     tcpi_rcv_rtt: u32,\n-    tcpi_rcv_space: u32,\n+    pub tcpi_rcv_space: u32,\n     tcpi_total_retrans: u32,\n     tcpi_pacing_rate: u64,\n     tcpi_max_pacing_rate: u64,\n@@ -75,8 +74,19 @@ pub struct TCP_INFO {\n     tcpi_data_segs_in: u32,\n     tcpi_data_segs_out: u32,\n     tcpi_delivery_rate: u64,\n-    */\n-    /* and more, see include/linux/tcp.h */\n+    tcpi_busy_time: u64,\n+    tcpi_rwnd_limited: u64,\n+    tcpi_sndbuf_limited: u64,\n+    tcpi_delivered: u32,\n+    tcpi_delivered_ce: u32,\n+    tcpi_bytes_sent: u64,\n+    tcpi_bytes_retrans: u64,\n+    tcpi_dsack_dups: u32,\n+    tcpi_reord_seen: u32,\n+    tcpi_rcv_ooopack: u32,\n+    tcpi_snd_wnd: u32,\n+    pub tcpi_rcv_wnd: u32,\n+    // and more, see include/linux/tcp.h\n }\n \n impl TCP_INFO {\n@@ -223,6 +233,25 @@ pub fn set_recv_buf(_fd: RawFd, _: usize) -> Result<()> {\n     Ok(())\n }\n \n+#[cfg(target_os = \"linux\")]\n+pub fn get_recv_buf(fd: RawFd) -> io::Result<usize> {\n+    let mut recv_size: c_int = 0;\n+    let mut size = std::mem::size_of::<c_int>() as u32;\n+    get_opt(\n+        fd,\n+        libc::SOL_SOCKET,\n+        libc::SO_RCVBUF,\n+        &mut recv_size,\n+        &mut size,\n+    )?;\n+    Ok(recv_size as usize)\n+}\n+\n+#[cfg(not(target_os = \"linux\"))]\n+pub fn get_recv_buf(_fd: RawFd) -> Result<usize> {\n+    Ok(0)\n+}\n+\n /// Enable client side TCP fast open.\n #[cfg(target_os = \"linux\")]\n pub fn set_tcp_fastopen_connect(fd: RawFd) -> Result<()> {\n@@ -358,18 +387,8 @@ mod test {\n \n         #[cfg(target_os = \"linux\")]\n         {\n-            let mut recv_size: c_int = 0;\n-            let mut size = std::mem::size_of::<c_int>() as u32;\n-            get_opt(\n-                socket.as_raw_fd(),\n-                libc::SOL_SOCKET,\n-                libc::SO_RCVBUF,\n-                &mut recv_size,\n-                &mut size,\n-            )\n-            .unwrap();\n             // kernel doubles whatever is set\n-            assert_eq!(recv_size, 102400 * 2);\n+            assert_eq!(get_recv_buf(socket.as_raw_fd()).unwrap(), 102400 * 2);\n         }\n     }\n \ndiff --git a/pingora-core/src/protocols/l4/stream.rs b/pingora-core/src/protocols/l4/stream.rs\nindex 6169d3115..6a13c6588 100644\n--- a/pingora-core/src/protocols/l4/stream.rs\n+++ b/pingora-core/src/protocols/l4/stream.rs\n@@ -22,7 +22,7 @@ use std::os::unix::io::AsRawFd;\n use std::pin::Pin;\n use std::sync::Arc;\n use std::task::{Context, Poll};\n-use std::time::SystemTime;\n+use std::time::{Duration, Instant, SystemTime};\n use tokio::io::{self, AsyncRead, AsyncWrite, AsyncWriteExt, BufStream, ReadBuf};\n use tokio::net::{TcpStream, UnixStream};\n \n@@ -141,6 +141,8 @@ pub struct Stream {\n     pub established_ts: SystemTime,\n     /// The distributed tracing object for this stream\n     pub tracer: Option<Tracer>,\n+    read_pending_time: AccumulatedDuration,\n+    write_pending_time: AccumulatedDuration,\n }\n \n impl Stream {\n@@ -172,6 +174,8 @@ impl From<TcpStream> for Stream {\n             proxy_digest: None,\n             socket_digest: None,\n             tracer: None,\n+            read_pending_time: AccumulatedDuration::new(),\n+            write_pending_time: AccumulatedDuration::new(),\n         }\n     }\n }\n@@ -185,6 +189,8 @@ impl From<UnixStream> for Stream {\n             proxy_digest: None,\n             socket_digest: None,\n             tracer: None,\n+            read_pending_time: AccumulatedDuration::new(),\n+            write_pending_time: AccumulatedDuration::new(),\n         }\n     }\n }\n@@ -220,6 +226,14 @@ impl GetTimingDigest for Stream {\n         }));\n         digest\n     }\n+\n+    fn get_read_pending_time(&self) -> Duration {\n+        self.read_pending_time.total\n+    }\n+\n+    fn get_write_pending_time(&self) -> Duration {\n+        self.write_pending_time.total\n+    }\n }\n \n impl GetProxyDigest for Stream {\n@@ -282,7 +296,9 @@ impl AsyncRead for Stream {\n         cx: &mut Context<'_>,\n         buf: &mut ReadBuf<'_>,\n     ) -> Poll<io::Result<()>> {\n-        Pin::new(&mut self.stream).poll_read(cx, buf)\n+        let result = Pin::new(&mut self.stream).poll_read(cx, buf);\n+        self.read_pending_time.poll_time(&result);\n+        result\n     }\n }\n \n@@ -292,15 +308,19 @@ impl AsyncWrite for Stream {\n         cx: &mut Context,\n         buf: &[u8],\n     ) -> Poll<io::Result<usize>> {\n-        if self.buffer_write {\n+        let result = if self.buffer_write {\n             Pin::new(&mut self.stream).poll_write(cx, buf)\n         } else {\n             Pin::new(&mut self.stream.get_mut()).poll_write(cx, buf)\n-        }\n+        };\n+        self.write_pending_time.poll_write_time(&result, buf.len());\n+        result\n     }\n \n     fn poll_flush(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<io::Result<()>> {\n-        Pin::new(&mut self.stream).poll_flush(cx)\n+        let result = Pin::new(&mut self.stream).poll_flush(cx);\n+        self.write_pending_time.poll_time(&result);\n+        result\n     }\n \n     fn poll_shutdown(mut self: Pin<&mut Self>, cx: &mut Context) -> Poll<io::Result<()>> {\n@@ -312,11 +332,16 @@ impl AsyncWrite for Stream {\n         cx: &mut Context<'_>,\n         bufs: &[std::io::IoSlice<'_>],\n     ) -> Poll<io::Result<usize>> {\n-        if self.buffer_write {\n+        let total_size = bufs.iter().fold(0, |acc, s| acc + s.len());\n+\n+        let result = if self.buffer_write {\n             Pin::new(&mut self.stream).poll_write_vectored(cx, bufs)\n         } else {\n             Pin::new(&mut self.stream.get_mut()).poll_write_vectored(cx, bufs)\n-        }\n+        };\n+\n+        self.write_pending_time.poll_write_time(&result, total_size);\n+        result\n     }\n \n     fn is_write_vectored(&self) -> bool {\n@@ -350,6 +375,12 @@ pub mod async_write_vec {\n         buf: &'a mut B,\n     }\n \n+    #[must_use = \"futures do nothing unless you `.await` or poll them\"]\n+    pub struct WriteVecAll<'a, W, B> {\n+        writer: &'a mut W,\n+        buf: &'a mut B,\n+    }\n+\n     pub trait AsyncWriteVec {\n         fn poll_write_vec<B: Buf>(\n             self: Pin<&mut Self>,\n@@ -367,6 +398,17 @@ pub mod async_write_vec {\n                 buf: src,\n             }\n         }\n+\n+        fn write_vec_all<'a, B>(&'a mut self, src: &'a mut B) -> WriteVecAll<'a, Self, B>\n+        where\n+            Self: Sized,\n+            B: Buf,\n+        {\n+            WriteVecAll {\n+                writer: self,\n+                buf: src,\n+            }\n+        }\n     }\n \n     impl<W, B> Future for WriteVec<'_, W, B>\n@@ -382,6 +424,25 @@ pub mod async_write_vec {\n         }\n     }\n \n+    impl<W, B> Future for WriteVecAll<'_, W, B>\n+    where\n+        W: AsyncWriteVec + Unpin,\n+        B: Buf,\n+    {\n+        type Output = io::Result<()>;\n+\n+        fn poll(mut self: Pin<&mut Self>, ctx: &mut Context<'_>) -> Poll<io::Result<()>> {\n+            let me = &mut *self;\n+            while me.buf.has_remaining() {\n+                let n = ready!(Pin::new(&mut *me.writer).poll_write_vec(ctx, me.buf))?;\n+                if n == 0 {\n+                    return Poll::Ready(Err(io::ErrorKind::WriteZero.into()));\n+                }\n+            }\n+            Poll::Ready(Ok(()))\n+        }\n+    }\n+\n     /* from https://github.com/tokio-rs/tokio/blob/master/tokio-util/src/lib.rs#L177 */\n     impl<T> AsyncWriteVec for T\n     where\n@@ -414,3 +475,55 @@ pub mod async_write_vec {\n }\n \n pub use async_write_vec::AsyncWriteVec;\n+\n+#[derive(Debug)]\n+struct AccumulatedDuration {\n+    total: Duration,\n+    last_start: Option<Instant>,\n+}\n+\n+impl AccumulatedDuration {\n+    fn new() -> Self {\n+        AccumulatedDuration {\n+            total: Duration::ZERO,\n+            last_start: None,\n+        }\n+    }\n+\n+    fn start(&mut self) {\n+        if self.last_start.is_none() {\n+            self.last_start = Some(Instant::now());\n+        }\n+    }\n+\n+    fn stop(&mut self) {\n+        if let Some(start) = self.last_start.take() {\n+            self.total += start.elapsed();\n+        }\n+    }\n+\n+    fn poll_write_time(&mut self, result: &Poll<io::Result<usize>>, buf_size: usize) {\n+        match result {\n+            Poll::Ready(Ok(n)) => {\n+                if *n == buf_size {\n+                    self.stop();\n+                } else {\n+                    // partial write\n+                    self.start();\n+                }\n+            }\n+            // Pending: start the timer, Ready(Err()): does not matter\n+            _ => self.start(),\n+        }\n+    }\n+\n+    fn poll_time(&mut self, result: &Poll<io::Result<()>>) {\n+        match result {\n+            Poll::Ready(Ok(())) => {\n+                self.stop();\n+            }\n+            // Pending: start the timer, Ready(Err()): does not matter\n+            _ => self.start(),\n+        }\n+    }\n+}\ndiff --git a/pingora-core/src/protocols/ssl/client.rs b/pingora-core/src/protocols/ssl/client.rs\nindex 07811dda3..9edf29b2a 100644\n--- a/pingora-core/src/protocols/ssl/client.rs\n+++ b/pingora-core/src/protocols/ssl/client.rs\n@@ -23,6 +23,7 @@ use crate::tls::{ssl, ssl::ConnectConfiguration, ssl_sys::X509_V_ERR_INVALID_CAL\n \n use pingora_error::{Error, ErrorType::*, OrErr, Result};\n use std::sync::Arc;\n+use std::time::Duration;\n \n /// Perform the TLS handshake for the given connection with the given configuration\n pub async fn handshake<S: IO>(\n@@ -82,6 +83,13 @@ where\n         ts_vec.push(Some(self.timing.clone()));\n         ts_vec\n     }\n+    fn get_read_pending_time(&self) -> Duration {\n+        self.get_ref().get_read_pending_time()\n+    }\n+\n+    fn get_write_pending_time(&self) -> Duration {\n+        self.get_ref().get_write_pending_time()\n+    }\n }\n \n impl<S> GetProxyDigest for SslStream<S>\ndiff --git a/pingora-core/src/server/configuration/mod.rs b/pingora-core/src/server/configuration/mod.rs\nindex 5b7609dcb..a4b5cc732 100644\n--- a/pingora-core/src/server/configuration/mod.rs\n+++ b/pingora-core/src/server/configuration/mod.rs\n@@ -119,17 +119,18 @@ impl Default for ServerConf {\n ///\n /// Call `Opt::from_args()` to build this object from the process's command line arguments.\n #[derive(Parser, Debug)]\n-#[clap(name = \"basic\")]\n+#[clap(name = \"basic\", long_about = None)]\n pub struct Opt {\n     /// Whether this server should try to upgrade from a running old server\n     #[clap(\n         short,\n         long,\n-        help = \"This is the base set of command line arguments for a pingora-based service\"\n+        help = \"This is the base set of command line arguments for a pingora-based service\",\n+        long_help = None\n     )]\n     pub upgrade: bool,\n \n-    /// Whether should run this server in the background\n+    /// Whether this server should run in the background\n     #[clap(short, long)]\n     pub daemon: bool,\n \n@@ -149,14 +150,15 @@ pub struct Opt {\n         long,\n         help = \"This flag is useful for upgrading service where the user wants \\\n                 to make sure the new service can start before shutting down \\\n-                the old server process.\"\n+                the old server process.\",\n+        long_help = None\n     )]\n     pub test: bool,\n \n     /// The path to the configuration file.\n     ///\n     /// See [`ServerConf`] for more details of the configuration file.\n-    #[clap(short, long, help = \"The path to the configuration file.\")]\n+    #[clap(short, long, help = \"The path to the configuration file.\", long_help = None)]\n     pub conf: Option<String>,\n }\n \ndiff --git a/pingora-core/src/server/transfer_fd/mod.rs b/pingora-core/src/server/transfer_fd/mod.rs\nindex 969d9cc31..ee63ac02e 100644\n--- a/pingora-core/src/server/transfer_fd/mod.rs\n+++ b/pingora-core/src/server/transfer_fd/mod.rs\n@@ -77,7 +77,7 @@ impl Fds {\n     {\n         let mut de_buf: [u8; 2048] = [0; 2048];\n         let (fds, bytes) = get_fds_from(path, &mut de_buf)?;\n-        let keys = deserialize_vec_string(&de_buf[..bytes]);\n+        let keys = deserialize_vec_string(&de_buf[..bytes])?;\n         self.deserialize(keys, fds);\n         Ok(())\n     }\n@@ -91,13 +91,9 @@ fn serialize_vec_string(vec_string: &[String], mut buf: &mut [u8]) -> usize {\n     buf.write(joined.as_bytes()).unwrap()\n }\n \n-fn deserialize_vec_string(buf: &[u8]) -> Vec<String> {\n-    let joined = std::str::from_utf8(buf).unwrap(); // TODO: handle error\n-    let mut results: Vec<String> = Vec::new();\n-    for iter in joined.split_ascii_whitespace() {\n-        results.push(String::from(iter));\n-    }\n-    results\n+fn deserialize_vec_string(buf: &[u8]) -> Result<Vec<String>, Error> {\n+    let joined = std::str::from_utf8(buf).map_err(|_| Error::EINVAL)?;\n+    Ok(joined.split_ascii_whitespace().map(String::from).collect())\n }\n \n #[cfg(target_os = \"linux\")]\n@@ -369,7 +365,7 @@ mod tests {\n         let vec_str: Vec<String> = vec![\"aaaa\".to_string(), \"bbb\".to_string()];\n         let mut ser_buf: [u8; 1024] = [0; 1024];\n         let size = serialize_vec_string(&vec_str, &mut ser_buf);\n-        let de_vec_string = deserialize_vec_string(&ser_buf[..size]);\n+        let de_vec_string = deserialize_vec_string(&ser_buf[..size]).unwrap();\n         assert_eq!(de_vec_string.len(), 2);\n         assert_eq!(de_vec_string[0], \"aaaa\");\n         assert_eq!(de_vec_string[1], \"bbb\");\ndiff --git a/pingora-core/src/services/listening.rs b/pingora-core/src/services/listening.rs\nindex 54373230d..ea81799b9 100644\n--- a/pingora-core/src/services/listening.rs\n+++ b/pingora-core/src/services/listening.rs\n@@ -35,29 +35,29 @@ use std::sync::Arc;\n pub struct Service<A> {\n     name: String,\n     listeners: Listeners,\n-    app_logic: Arc<A>,\n+    app_logic: Option<A>,\n     /// The number of preferred threads. `None` to follow global setting.\n     pub threads: Option<usize>,\n }\n \n impl<A> Service<A> {\n     /// Create a new [`Service`] with the given application (see [`crate::apps`]).\n-    pub fn new(name: String, app_logic: Arc<A>) -> Self {\n+    pub fn new(name: String, app_logic: A) -> Self {\n         Service {\n             name,\n             listeners: Listeners::new(),\n-            app_logic,\n+            app_logic: Some(app_logic),\n             threads: None,\n         }\n     }\n \n     /// Create a new [`Service`] with the given application (see [`crate::apps`]) and the given\n     /// [`Listeners`].\n-    pub fn with_listeners(name: String, listeners: Listeners, app_logic: Arc<A>) -> Self {\n+    pub fn with_listeners(name: String, listeners: Listeners, app_logic: A) -> Self {\n         Service {\n             name,\n             listeners,\n-            app_logic,\n+            app_logic: Some(app_logic),\n             threads: None,\n         }\n     }\n@@ -107,6 +107,16 @@ impl<A> Service<A> {\n     pub fn add_address(&mut self, addr: ServerAddress) {\n         self.listeners.add_address(addr);\n     }\n+\n+    /// Get a reference to the application inside this service\n+    pub fn app_logic(&self) -> Option<&A> {\n+        self.app_logic.as_ref()\n+    }\n+\n+    /// Get a mutable reference to the application inside this service\n+    pub fn app_logic_mut(&mut self) -> Option<&mut A> {\n+        self.app_logic.as_mut()\n+    }\n }\n \n impl<A: ServerApp + Send + Sync + 'static> Service<A> {\n@@ -194,18 +204,23 @@ impl<A: ServerApp + Send + Sync + 'static> ServiceTrait for Service<A> {\n     async fn start_service(&mut self, fds: Option<ListenFds>, shutdown: ShutdownWatch) {\n         let runtime = current_handle();\n         let endpoints = self.listeners.build(fds);\n+        let app_logic = self\n+            .app_logic\n+            .take()\n+            .expect(\"can only start_service() once\");\n+        let app_logic = Arc::new(app_logic);\n \n         let handlers = endpoints.into_iter().map(|endpoint| {\n-            let app_logic = self.app_logic.clone();\n             let shutdown = shutdown.clone();\n+            let my_app_logic = app_logic.clone();\n             runtime.spawn(async move {\n-                Self::run_endpoint(app_logic, endpoint, shutdown).await;\n+                Self::run_endpoint(my_app_logic, endpoint, shutdown).await;\n             })\n         });\n \n         futures::future::join_all(handlers).await;\n         self.listeners.cleanup();\n-        self.app_logic.cleanup().await;\n+        app_logic.cleanup().await;\n     }\n \n     fn name(&self) -> &str {\n@@ -226,7 +241,7 @@ impl Service<PrometheusServer> {\n     pub fn prometheus_http_service() -> Self {\n         Service::new(\n             \"Prometheus metric HTTP\".to_string(),\n-            Arc::new(PrometheusServer::new()),\n+            PrometheusServer::new(),\n         )\n     }\n }\ndiff --git a/pingora-proxy/src/lib.rs b/pingora-proxy/src/lib.rs\nindex a60e6a7fb..eaf31d52b 100644\n--- a/pingora-proxy/src/lib.rs\n+++ b/pingora-proxy/src/lib.rs\n@@ -96,12 +96,12 @@ pub struct HttpProxy<SV> {\n }\n \n impl<SV> HttpProxy<SV> {\n-    fn new(inner: SV, conf: Arc<ServerConf>) -> Arc<Self> {\n-        Arc::new(HttpProxy {\n+    fn new(inner: SV, conf: Arc<ServerConf>) -> Self {\n+        HttpProxy {\n             inner,\n             client_upstream: Connector::new(Some(ConnectorOptions::from_server_conf(&conf))),\n             shutdown: Notify::new(),\n-        })\n+        }\n     }\n \n     async fn handle_new_request(\ndiff --git a/pingora/examples/app/echo.rs b/pingora/examples/app/echo.rs\nindex 61f94e5c6..09725c48d 100644\n--- a/pingora/examples/app/echo.rs\n+++ b/pingora/examples/app/echo.rs\n@@ -86,13 +86,3 @@ impl ServeHttp for HttpEchoApp {\n             .unwrap()\n     }\n }\n-\n-impl EchoApp {\n-    pub fn new() -> Arc<Self> {\n-        Arc::new(EchoApp {})\n-    }\n-}\n-\n-pub fn new_http_echo_app() -> Arc<HttpEchoApp> {\n-    Arc::new(HttpEchoApp {})\n-}\ndiff --git a/pingora/examples/service/echo.rs b/pingora/examples/service/echo.rs\nindex 8126088a7..af07da217 100644\n--- a/pingora/examples/service/echo.rs\n+++ b/pingora/examples/service/echo.rs\n@@ -12,13 +12,13 @@\n // See the License for the specific language governing permissions and\n // limitations under the License.\n \n-use crate::app::echo::{new_http_echo_app, EchoApp, HttpEchoApp};\n+use crate::app::echo::{EchoApp, HttpEchoApp};\n use pingora::services::listening::Service;\n \n pub fn echo_service() -> Service<EchoApp> {\n-    Service::new(\"Echo Service\".to_string(), EchoApp::new())\n+    Service::new(\"Echo Service\".to_string(), EchoApp)\n }\n \n pub fn echo_service_http() -> Service<HttpEchoApp> {\n-    Service::new(\"Echo Service HTTP\".to_string(), new_http_echo_app())\n+    Service::new(\"Echo Service HTTP\".to_string(), HttpEchoApp)\n }\ndiff --git a/pingora/examples/service/proxy.rs b/pingora/examples/service/proxy.rs\nindex a103b4776..49c24ca02 100644\n--- a/pingora/examples/service/proxy.rs\n+++ b/pingora/examples/service/proxy.rs\n@@ -16,7 +16,6 @@ use crate::app::proxy::ProxyApp;\n use pingora_core::listeners::Listeners;\n use pingora_core::services::listening::Service;\n use pingora_core::upstreams::peer::BasicPeer;\n-use std::sync::Arc;\n \n pub fn proxy_service(addr: &str, proxy_addr: &str) -> Service<ProxyApp> {\n     let proxy_to = BasicPeer::new(proxy_addr);\n@@ -24,7 +23,7 @@ pub fn proxy_service(addr: &str, proxy_addr: &str) -> Service<ProxyApp> {\n     Service::with_listeners(\n         \"Proxy Service\".to_string(),\n         Listeners::tcp(addr),\n-        Arc::new(ProxyApp::new(proxy_to)),\n+        ProxyApp::new(proxy_to),\n     )\n }\n \n@@ -41,6 +40,6 @@ pub fn proxy_service_tls(\n     Service::with_listeners(\n         \"Proxy Service TLS\".to_string(),\n         Listeners::tls(addr, cert_path, key_path).unwrap(),\n-        Arc::new(ProxyApp::new(proxy_to)),\n+        ProxyApp::new(proxy_to),\n     )\n }\ndiff --git a/tinyufo/Cargo.toml b/tinyufo/Cargo.toml\nindex 0326fe540..68c4236b5 100644\n--- a/tinyufo/Cargo.toml\n+++ b/tinyufo/Cargo.toml\n@@ -29,6 +29,7 @@ zipf = \"7\"\n moka = { version = \"0\", features = [\"sync\"] }\n dhat = \"0\"\n quick_cache = \"0.4\"\n+triomphe = \"<=0.1.11\" # 0.1.12 requires Rust 1.76\n \n [[bench]]\n name = \"bench_perf\"\n", "test_patch": "diff --git a/pingora-core/tests/utils/mod.rs b/pingora-core/tests/utils/mod.rs\nindex af5de8544..d254e9469 100644\n--- a/pingora-core/tests/utils/mod.rs\n+++ b/pingora-core/tests/utils/mod.rs\n@@ -25,7 +25,6 @@ use async_trait::async_trait;\n use bytes::Bytes;\n use http::{Response, StatusCode};\n use pingora_timeout::timeout;\n-use std::sync::Arc;\n use std::time::Duration;\n \n use pingora_core::apps::http_app::ServeHttp;\n@@ -63,10 +62,6 @@ impl ServeHttp for EchoApp {\n     }\n }\n \n-pub fn new_http_echo_app() -> Arc<EchoApp> {\n-    Arc::new(EchoApp {})\n-}\n-\n pub struct MyServer {\n     pub handle: thread::JoinHandle<()>,\n }\n@@ -88,11 +83,8 @@ fn entry_point(opt: Option<Opt>) {\n     tls_settings.enable_h2();\n     listeners.add_tls_with_settings(\"0.0.0.0:6146\", None, tls_settings);\n \n-    let echo_service_http = Service::with_listeners(\n-        \"Echo Service HTTP\".to_string(),\n-        listeners,\n-        new_http_echo_app(),\n-    );\n+    let echo_service_http =\n+        Service::with_listeners(\"Echo Service HTTP\".to_string(), listeners, EchoApp);\n \n     my_server.add_service(echo_service_http);\n     my_server.run_forever();\n", "problem_statement": "Raw Markdown displayed in command line help\n## Describe the bug\r\n\r\nRunning a Pingora server with `--help` command line flag will produce descriptions including raw Markdown. Also, the names of command line flags unnecessarily. These are meant for online documentation and often make little sense as a command line help.\r\n\r\n## Pingora info\r\n\r\nPlease include the following information about your environment:\r\n\r\n**Pingora version**: https://github.com/cloudflare/pingora/commit/2501d4adb038d93613c0edbd7c1e3b3de9b415b1\r\n**Rust version**: 1.79.0-nightly (28e7b2bc0 2024-04-05)\r\n**Operating system version**: Fedora 39\r\n\r\n## Steps to reproduce\r\n\r\nRun `cargo run --example gateway -- --help` from the command line.\r\n\r\n## Expected results\r\n\r\nA human-readable output without any duplicated information.\r\n\r\n## Observed results\r\n\r\nRaw Markdown and duplicated information on command line flags, pointless references to code (the command description in particular is meaningless) and references to code (`ServerConf`) that should have really been URLs:\r\n\r\n```\r\nbasic 0.1.0\r\nCommand-line options\r\n\r\nCall `Opt::from_args()` to build this object from the process's command line arguments.\r\n\r\nUSAGE:\r\n    gateway [FLAGS] [OPTIONS]\r\n\r\nFLAGS:\r\n    -d, --daemon       \r\n            Whether should run this server in the background\r\n            \r\n            `-d` or `--daemon` can be used\r\n    -h, --help         \r\n            Prints help information\r\n\r\n        --nocapture    \r\n            Not actually used. This flag is there so that the server is not upset seeing this flag passed from `cargo\r\n            test` sometimes\r\n    -t, --test         \r\n            Test the configuration and exit\r\n            \r\n            When this flag is set, calling `server.bootstrap()` will exit the process without errors\r\n            \r\n            This flag is useful for upgrading service where the user wants to make sure the new service can start before\r\n            shutting down the old server process.\r\n            \r\n            `-t` or `--test` can be used\r\n    -u, --upgrade      \r\n            Whether this server should try to upgrade from a running old server\r\n            \r\n            `-u` or `--upgrade` can be used\r\n    -V, --version      \r\n            Prints version information\r\n\r\n\r\nOPTIONS:\r\n    -c, --conf <conf>    \r\n            The path to the configuration file.\r\n            \r\n            See [`ServerConf`] for more details of the configuration file.\r\n            \r\n            `-c` or `--conf` can be used\r\n```\r\n\r\n## Additional context\r\n\r\nThe docstrings that this info is compiled from should really be adjusted. While an application extending the default command line options will overwrite the application description, adjusting the description of individual options is non-trivial.\r\n\r\nAlternatively, the docstrings can be left unchanged and [structopt help messages](https://docs.rs/structopt/latest/structopt/#help-messages) can be specified separately.\n", "hints_text": "", "created_at": "2024-05-31 16:33:39", "merge_commit_sha": "216d8e9d92c75d46cba78da9b167fb41dec91ad7", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['pingora', '.github/workflows/build.yml']", "['pingora (stable)', '.github/workflows/build.yml']"]]}
{"repo": "dusk-network/rusk", "instance_id": "dusk-network__rusk-3213", "base_commit": "341b58e6232afc417d4876c88e7d75df218c8ae0", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex 73e800f99b..c2fe6b4748 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -17,6 +17,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Fixed\n \n+- Fix gas settings reset and validation in preferences [#3212]\n+\n ## [0.10.0] - 2024-12-17\n \n ### Added\n@@ -455,6 +457,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3178]: https://github.com/dusk-network/rusk/issues/3178\n [#3179]: https://github.com/dusk-network/rusk/issues/3179\n [#3203]: https://github.com/dusk-network/rusk/issues/3203\n+[#3212]: https://github.com/dusk-network/rusk/issues/3212\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/lib/components/BigIntInput/BigIntInput.svelte b/web-wallet/src/lib/components/BigIntInput/BigIntInput.svelte\nindex 55e5ec05b0..a4ac58e4e1 100644\n--- a/web-wallet/src/lib/components/BigIntInput/BigIntInput.svelte\n+++ b/web-wallet/src/lib/components/BigIntInput/BigIntInput.svelte\n@@ -20,7 +20,8 @@\n   /** @type {bigint} */\n   export let value = 0n;\n \n-  let internalValue = value.toString();\n+  /** @type {string} */\n+  let internalValue;\n \n   /** @type {(v: bigint, min: bigint, max: bigint) => boolean} */\n   const isInvalidInput = (v, min, max) => !!(min > v || v > max);\ndiff --git a/web-wallet/src/routes/(app)/settings/+page.svelte b/web-wallet/src/routes/(app)/settings/+page.svelte\nindex c75de6a9a2..bfe6f698c6 100644\n--- a/web-wallet/src/routes/(app)/settings/+page.svelte\n+++ b/web-wallet/src/routes/(app)/settings/+page.svelte\n@@ -45,6 +45,7 @@\n     // eslint-disable-next-line no-alert\n     if (confirm(confirmResetGasMessage)) {\n       settingsStore.resetGasSettings();\n+      isGasValid = true;\n     }\n   }\n \n@@ -88,13 +89,11 @@\n \n             isGasValid = areValidGasSettings(price, limit);\n \n-            if (isGasValid) {\n-              settingsStore.update((store) => ({\n-                ...store,\n-                gasLimit: limit,\n-                gasPrice: price,\n-              }));\n-            }\n+            settingsStore.update((store) => ({\n+              ...store,\n+              gasLimit: limit,\n+              gasPrice: price,\n+            }));\n           }}\n           limit={gasLimit}\n           limitLower={gasLimitLower}\n", "test_patch": "", "problem_statement": "Fix gas settings reset and validation in preferences\n**Describe the bug**\r\nWe have two wrong behaviours after introducing the \"reset to defaults\" button for gas settings.\r\n\r\n1. the \"back\" button of the preferences screen stays disabled after entering an invalid value and using the \"reset to defaults\" button, even if the values are now correct\r\n2. after inserting an invalid value for either price or limit the \"reset\" button works only under special conditions.\r\n\r\nFor example if the default value for gas limit is `100_000_000`, and I add a zero to make it `1_000_000_000` (also valid). In this case the valid value is written in the settings and it's different from the default. In this situation the reset button works, because Svelte's reactivity is triggered (the value written in the settings is different from the default).\r\n\r\nOn the other hand if I start from the default value of `100_000_000` and I add, say, a nine at the end (`1_000_000_009`) the value becomes immediately invalid and the value written in the store, which is the last valid value, is equal to the default: in this case the reset button does nothing as Svelte's reactivity is not triggered.\r\n\r\n**Proposed solution**\r\nFor the first behaviour the solution is quite straightforward: update the validation flag when the reset is clicked.\r\n\r\nFor the second behaviour the most immediate solution is to allow storing values out of limits: this way the reactivity works properly. Only drawback is that if the user closes the tab he will restart with invalid values in the settings and he has to fix them before being allowed to do any operation.\n", "hints_text": "", "created_at": "2024-12-18 08:19:29", "merge_commit_sha": "22e3b015b83f538ad70358e5123476780b4e3d1b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['changes', '.github/workflows/ruskwallet_ci.yml']", "['[Windows] Nightly tests', '.github/workflows/ruskwallet_ci.yml']"], ["['Node 20.x', '.github/workflows/explorer_ci.yml']", "['changes', '.github/workflows/w3sperjs_ci.yml']"], ["['[Mac arm64] Nightly checks', '.github/workflows/ruskwallet_ci.yml']", "['Clippy check release', '.github/workflows/rusk_ci.yml']"], ["['Node 20.x', '.github/workflows/webwallet_ci.yml']", "['Dusk Analyzer', '.github/workflows/rusk_ci.yml']"]]}
{"repo": "dusk-network/rusk", "instance_id": "dusk-network__rusk-3232", "base_commit": "9c0c8b02bb24056b553da8f9ba43a94d1d7002fe", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex 3118dcbfd4..3b99844675 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -17,6 +17,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Fixed\n \n+- Fix wrong error shown in the login screen [#3226] [#3097]\n+\n ## [0.10.1] - 2024-12-18\n \n ### Fixed\n@@ -452,6 +454,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3073]: https://github.com/dusk-network/rusk/issues/3073\n [#3076]: https://github.com/dusk-network/rusk/issues/3076\n [#3081]: https://github.com/dusk-network/rusk/issues/3081\n+[#3097]: https://github.com/dusk-network/rusk/issues/3097\n [#3098]: https://github.com/dusk-network/rusk/issues/3098\n [#3099]: https://github.com/dusk-network/rusk/issues/3099\n [#3113]: https://github.com/dusk-network/rusk/issues/3113\n@@ -462,6 +465,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3179]: https://github.com/dusk-network/rusk/issues/3179\n [#3203]: https://github.com/dusk-network/rusk/issues/3203\n [#3212]: https://github.com/dusk-network/rusk/issues/3212\n+[#3226]: https://github.com/dusk-network/rusk/issues/3226\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/lib/errors/index.js b/web-wallet/src/lib/errors/index.js\nnew file mode 100644\nindex 0000000000..3659355455\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/index.js\n@@ -0,0 +1,3 @@\n+export { default as InvalidMnemonicError } from \"./invalid-mnemonic-error\";\n+export { default as InvalidPasswordError } from \"./invalid-password-error\";\n+export { default as MismatchedWalletError } from \"./mismatched-wallet-error\";\ndiff --git a/web-wallet/src/lib/errors/invalid-mnemonic-error.js b/web-wallet/src/lib/errors/invalid-mnemonic-error.js\nnew file mode 100644\nindex 0000000000..e9c3fef503\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/invalid-mnemonic-error.js\n@@ -0,0 +1,8 @@\n+class InvalidMnemonicError extends Error {\n+  constructor() {\n+    super(\"Invalid mnemonic\");\n+    this.name = \"InvalidMnemonicError\";\n+  }\n+}\n+\n+export default InvalidMnemonicError;\ndiff --git a/web-wallet/src/lib/errors/invalid-password-error.js b/web-wallet/src/lib/errors/invalid-password-error.js\nnew file mode 100644\nindex 0000000000..f6683b066d\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/invalid-password-error.js\n@@ -0,0 +1,8 @@\n+class InvalidPasswordError extends Error {\n+  constructor() {\n+    super(\"Wrong password\");\n+    this.name = \"InvalidPasswordError\";\n+  }\n+}\n+\n+export default InvalidPasswordError;\ndiff --git a/web-wallet/src/lib/errors/mismatched-wallet-error.js b/web-wallet/src/lib/errors/mismatched-wallet-error.js\nnew file mode 100644\nindex 0000000000..23d43510b1\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/mismatched-wallet-error.js\n@@ -0,0 +1,8 @@\n+class MismatchedWalletError extends Error {\n+  constructor() {\n+    super(\"Mismatched wallet address or no existing wallet\");\n+    this.name = \"MismatchedWalletError\";\n+  }\n+}\n+\n+export default MismatchedWalletError;\ndiff --git a/web-wallet/src/routes/(welcome)/login/+page.svelte b/web-wallet/src/routes/(welcome)/login/+page.svelte\nindex 23085dd43f..c592b0fdfb 100644\n--- a/web-wallet/src/routes/(welcome)/login/+page.svelte\n+++ b/web-wallet/src/routes/(welcome)/login/+page.svelte\n@@ -4,7 +4,14 @@\n   import { mdiArrowLeft, mdiKeyOutline } from \"@mdi/js\";\n   import { validateMnemonic } from \"bip39\";\n \n+  import { getErrorFrom } from \"$lib/dusk/error\";\n   import { Button, Textbox } from \"$lib/dusk/components\";\n+  import {\n+    InvalidMnemonicError,\n+    InvalidPasswordError,\n+    MismatchedWalletError,\n+  } from \"$lib/errors\";\n+\n   import { AppAnchor, AppAnchorButton, Banner } from \"$lib/components\";\n   import { IconHeadingCard } from \"$lib/containers/Cards\";\n   import { goto } from \"$lib/navigation\";\n@@ -20,9 +27,6 @@\n   } from \"$lib/wallet\";\n   import loginInfoStorage from \"$lib/services/loginInfoStorage\";\n \n-  const localDataCheckErrorMsg =\n-    \"Mismatched wallet address or no existing wallet\";\n-\n   /** @type {(seed: Uint8Array) => Promise<import(\"$lib/vendor/w3sper.js/src/mod\").ProfileGenerator>} */\n   async function checkLocalData(seed) {\n     const profileGenerator = profileGeneratorFrom(seed);\n@@ -30,7 +34,7 @@\n     const currentAddress = $settingsStore.userId;\n \n     if (!currentAddress || currentAddress !== defaultAddress) {\n-      throw new Error(localDataCheckErrorMsg);\n+      throw new MismatchedWalletError();\n     }\n \n     return profileGenerator;\n@@ -40,12 +44,12 @@\n   const getSeedFromMnemonicAsync = async (mnemonic) =>\n     validateMnemonic(mnemonic)\n       ? getSeedFromMnemonic(mnemonic)\n-      : Promise.reject(new Error(\"Invalid mnemonic\"));\n+      : Promise.reject(new InvalidMnemonicError());\n \n   /** @type {(loginInfo: MnemonicEncryptInfo) => (pwd: string) => Promise<Uint8Array>} */\n   const getSeedFromInfo = (loginInfo) => (pwd) =>\n     decryptMnemonic(loginInfo, pwd).then(getSeedFromMnemonic, () =>\n-      Promise.reject(new Error(\"Wrong password\"))\n+      Promise.reject(new InvalidPasswordError())\n     );\n \n   const loginInfo = loginInfoStorage.get();\n@@ -57,8 +61,8 @@\n   /** @type {string} */\n   let secretText = \"\";\n \n-  /** @type {null|\"invalid-password\"|\"invalid-mnemonic\"} */\n-  let error = null;\n+  /** @type {Error} */\n+  let error;\n \n   /** @type {import(\"svelte/elements\").FormEventHandler<HTMLFormElement>} */\n   function handleUnlockWalletSubmit() {\n@@ -72,16 +76,16 @@\n       .then((profileGenerator) => walletStore.init(profileGenerator))\n       .then(() => goto(\"/dashboard\"))\n       .catch((err) => {\n-        if (err.message === localDataCheckErrorMsg) {\n+        if (err instanceof MismatchedWalletError) {\n           const enteredMnemonicPhrase = secretText.split(\" \");\n           mnemonicPhraseResetStore.set(enteredMnemonicPhrase);\n           goto(\"/setup/restore\");\n+\n           return;\n+        } else {\n+          error = err instanceof Error ? err : getErrorFrom(err);\n         }\n-        error =\n-          err.message === \"Wrong password\"\n-            ? \"invalid-password\"\n-            : \"invalid-mnemonic\";\n+\n         fldSecret.focus();\n         fldSecret.select();\n       });\n@@ -113,15 +117,14 @@\n           type=\"password\"\n           autocomplete=\"current-password\"\n         />\n-        {#if error === \"invalid-mnemonic\"}\n+        {#if error instanceof InvalidMnemonicError}\n           <Banner title=\"Invalid mnemonic phrase\" variant=\"error\">\n             <p>\n               Please ensure you have entered your 12-word mnemonic phrase, with\n               a space separating each word.\n             </p>\n           </Banner>\n-        {/if}\n-        {#if error === \"invalid-password\"}\n+        {:else if error instanceof InvalidPasswordError}\n           <Banner title=\"Invalid password\" variant=\"error\">\n             <p>\n               Please ensure the password entered matches the one you have set up\n@@ -129,6 +132,13 @@\n               you can <AppAnchor href=\"/setup/restore\">restore</AppAnchor> your wallet.\n             </p>\n           </Banner>\n+        {:else if error}\n+          <Banner\n+            title={error.name.replace(/(\\w)([A-Z])/g, \"$1 $2\")}\n+            variant=\"error\"\n+          >\n+            <p>{error.message}</p>\n+          </Banner>\n         {/if}\n         <Button text=\"Unlock Wallet\" type=\"submit\" />\n         {#if modeLabel === \"Password\"}\n", "test_patch": "diff --git a/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap b/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\nindex 836f724ab0..f06439819a 100644\n--- a/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\n+++ b/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\n@@ -77,7 +77,6 @@ exports[`Login > Mnemonic phrase workflow > should render the login page and sho\n           \n            \n            \n-           \n           <button\n             class=\"dusk-button dusk-button--type--submit dusk-button--variant--primary dusk-button--size--default\"\n             type=\"submit\"\n@@ -204,7 +203,6 @@ exports[`Login > Password workflow > should show the password field and the link\n           \n            \n            \n-           \n           <button\n             class=\"dusk-button dusk-button--type--submit dusk-button--variant--primary dusk-button--size--default\"\n             type=\"submit\"\n", "problem_statement": "web-wallet: Misleading Error Message When Unlocking Wallet Without Internet Connection\n### Description\r\nCurrently, when a user attempts to unlock their Wallet without an internet connection, they are presented with an **\"invalid mnemonic\" error**. This is misleading and may cause users to believe there is an issue with the mnemonic they entered, when the actual problem is the lack of internet connectivity.\r\n\r\nAs the Web Wallet uses a service worker, it is possible for the application to remain loaded in the browser even when there is no active internet connection. While this behavior is handled correctly in some areas (e.g., sync failures are displayed with appropriate status messages on the dashboard), it is not yet addressed in the pre-unlocked views.\r\n\r\n**Affected Areas**\r\n- Unlock screen\r\n- Create wallet flow\r\n- Restore wallet flow\r\n\r\n### Expected Behavior\r\nThe application should properly handle scenarios where the user's internet connection is unavailable:\r\n- Detect lack of internet connectivity.\r\n- Display a clear and specific error message to inform the user about the connectivity issue.\r\n- Prevent misleading error messages like \"invalid mnemonic.\"\r\n\r\n### Steps to Reproduce\r\n- Load the Web Wallet while online.\r\n- Disconnect from the internet.\r\n- Attempt to unlock the wallet with a valid mnemonic.\r\n\r\n### Actual Behavior\r\nThe user sees an **\"invalid mnemonic\" error**, despite entering a correct mnemonic.\r\n\r\n### Proposed Solution\r\n- Implement connectivity checks for the pre-unlocked views.\r\n- Update error messaging to accurately reflect the connectivity status when applicable.\r\n\r\n### Additional Context\r\nThis issue is particularly relevant as users may frequently encounter this scenario when working with service worker-based applications.\n", "hints_text": "", "created_at": "2024-12-19 10:47:33", "merge_commit_sha": "379af862d8041b076059a045a3472442e7879702", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['changes', '.github/workflows/ruskwallet_ci.yml']", "['[Windows] Nightly tests', '.github/workflows/ruskwallet_ci.yml']"], ["['Node 20.x', '.github/workflows/explorer_ci.yml']", "['changes', '.github/workflows/w3sperjs_ci.yml']"], ["['[Mac arm64] Nightly checks', '.github/workflows/ruskwallet_ci.yml']", "['Clippy check release', '.github/workflows/rusk_ci.yml']"], ["['Node 20.x', '.github/workflows/webwallet_ci.yml']", "['Dusk Analyzer', '.github/workflows/rusk_ci.yml']"]]}
{"repo": "mozilla/cbindgen", "instance_id": "mozilla__cbindgen-916", "base_commit": "80c50c643a453bd396331b4a8eb17a47f7c6c6b1", "patch": "diff --git a/src/bindgen/bindings.rs b/src/bindgen/bindings.rs\nindex 029cfc66b..7ef85baba 100644\n--- a/src/bindgen/bindings.rs\n+++ b/src/bindgen/bindings.rs\n@@ -7,7 +7,7 @@ use std::cell::RefCell;\n use std::collections::HashMap;\n use std::fs;\n use std::fs::File;\n-use std::io::{Read, Write};\n+use std::io::{BufWriter, Read, Write};\n use std::path;\n use std::rc::Rc;\n \n@@ -129,6 +129,27 @@ impl Bindings {\n         fields\n     }\n \n+    /// Lists the exported symbols that can be dynamically linked, i.e. globals and functions.\n+    pub fn dynamic_symbols_names(&self) -> impl Iterator<Item = &str> {\n+        use crate::bindgen::ir::Item;\n+\n+        let function_names = self.functions.iter().map(|f| f.path().name());\n+        let global_names = self.globals.iter().map(|g| g.export_name());\n+        function_names.chain(global_names)\n+    }\n+\n+    pub fn generate_symfile<P: AsRef<path::Path>>(&self, symfile_path: P) {\n+        if let Some(dir) = symfile_path.as_ref().parent() {\n+            std::fs::create_dir_all(dir).unwrap();\n+        }\n+        let mut writer = BufWriter::new(File::create(symfile_path).unwrap());\n+        write!(&mut writer, \"{{\\n\").expect(\"writing symbol file header failed\");\n+        for symbol in self.dynamic_symbols_names() {\n+            write!(&mut writer, \"{};\\n\", symbol).expect(\"writing symbol failed\");\n+        }\n+        write!(&mut writer, \"}};\").expect(\"writing symbol file footer failed\");\n+    }\n+\n     pub fn generate_depfile<P: AsRef<path::Path>>(&self, header_path: P, depfile_path: P) {\n         if let Some(dir) = depfile_path.as_ref().parent() {\n             if !dir.exists() {\ndiff --git a/src/main.rs b/src/main.rs\nindex a6a1852c2..f8d9c437a 100644\n--- a/src/main.rs\n+++ b/src/main.rs\n@@ -298,6 +298,17 @@ fn main() {\n                     This option is ignored if `--out` is missing.\"\n                 )\n         )\n+        .arg(\n+            Arg::new(\"symfile\")\n+                .value_name(\"PATH\")\n+                .long(\"symfile\")\n+                .num_args(1)\n+                .required(false)\n+                .help(\"Generate a list of symbols at the given Path. This list can be \\\n+                    given to a linker in order to compile an application that exposes \\\n+                    dynamic symbols. Useful when creating a plugin system with a C interface.\"\n+                )\n+        )\n         .get_matches();\n \n     if matches.get_flag(\"verify\") && !matches.contains_id(\"out\") {\n@@ -343,7 +354,10 @@ fn main() {\n                 std::process::exit(2);\n             }\n             if let Some(depfile) = matches.get_one(\"depfile\") {\n-                bindings.generate_depfile(file, depfile)\n+                bindings.generate_depfile(file, depfile);\n+            }\n+            if let Some(symfile) = matches.get_one::<String>(\"symfile\") {\n+                bindings.generate_symfile(symfile);\n             }\n         }\n         _ => {\n", "test_patch": "diff --git a/tests/expectations-symbols/abi_string.c.sym b/tests/expectations-symbols/abi_string.c.sym\nnew file mode 100644\nindex 000000000..19e73069e\n--- /dev/null\n+++ b/tests/expectations-symbols/abi_string.c.sym\n@@ -0,0 +1,4 @@\n+{\n+c;\n+c_unwind;\n+};\ndiff --git a/tests/expectations-symbols/alias.c.sym b/tests/expectations-symbols/alias.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/alias.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/annotation.c.sym b/tests/expectations-symbols/annotation.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/annotation.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/array.c.sym b/tests/expectations-symbols/array.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/array.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/asserted_cast.c.sym b/tests/expectations-symbols/asserted_cast.c.sym\nnew file mode 100644\nindex 000000000..57c772c4a\n--- /dev/null\n+++ b/tests/expectations-symbols/asserted_cast.c.sym\n@@ -0,0 +1,3 @@\n+{\n+foo;\n+};\ndiff --git a/tests/expectations-symbols/assoc_constant.c.sym b/tests/expectations-symbols/assoc_constant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/assoc_constant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/associated_in_body.c.sym b/tests/expectations-symbols/associated_in_body.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/associated_in_body.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/bitfield.c.sym b/tests/expectations-symbols/bitfield.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/bitfield.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/bitflags.c.sym b/tests/expectations-symbols/bitflags.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/bitflags.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/body.c.sym b/tests/expectations-symbols/body.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/body.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/box.c.sym b/tests/expectations-symbols/box.c.sym\nnew file mode 100644\nindex 000000000..79a114122\n--- /dev/null\n+++ b/tests/expectations-symbols/box.c.sym\n@@ -0,0 +1,5 @@\n+{\n+root;\n+drop_box;\n+drop_box_opt;\n+};\ndiff --git a/tests/expectations-symbols/cdecl.c.sym b/tests/expectations-symbols/cdecl.c.sym\nnew file mode 100644\nindex 000000000..b3f50ef4a\n--- /dev/null\n+++ b/tests/expectations-symbols/cdecl.c.sym\n@@ -0,0 +1,4 @@\n+{\n+O;\n+root;\n+};\ndiff --git a/tests/expectations-symbols/cell.c.sym b/tests/expectations-symbols/cell.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/cell.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/cfg.c.sym b/tests/expectations-symbols/cfg.c.sym\nnew file mode 100644\nindex 000000000..732af1661\n--- /dev/null\n+++ b/tests/expectations-symbols/cfg.c.sym\n@@ -0,0 +1,5 @@\n+{\n+root;\n+root;\n+cond;\n+};\ndiff --git a/tests/expectations-symbols/cfg_2.c.sym b/tests/expectations-symbols/cfg_2.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/cfg_2.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/char.c.sym b/tests/expectations-symbols/char.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/char.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/const_generics.c.sym b/tests/expectations-symbols/const_generics.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_arrayvec.c.sym b/tests/expectations-symbols/const_generics_arrayvec.c.sym\nnew file mode 100644\nindex 000000000..f6b2cd601\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_arrayvec.c.sym\n@@ -0,0 +1,3 @@\n+{\n+push;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_bool.c.sym b/tests/expectations-symbols/const_generics_bool.c.sym\nnew file mode 100644\nindex 000000000..1bed6d7ae\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_bool.c.sym\n@@ -0,0 +1,6 @@\n+{\n+new_set;\n+set_for_each;\n+new_map;\n+map_for_each;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_byte.c.sym b/tests/expectations-symbols/const_generics_byte.c.sym\nnew file mode 100644\nindex 000000000..6a1843f1f\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_byte.c.sym\n@@ -0,0 +1,5 @@\n+{\n+init_parens_parser;\n+destroy_parens_parser;\n+init_braces_parser;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_char.c.sym b/tests/expectations-symbols/const_generics_char.c.sym\nnew file mode 100644\nindex 000000000..5888deefd\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_char.c.sym\n@@ -0,0 +1,3 @@\n+{\n+until_nul;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_constant.c.sym b/tests/expectations-symbols/const_generics_constant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_constant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/const_generics_thru.c.sym b/tests/expectations-symbols/const_generics_thru.c.sym\nnew file mode 100644\nindex 000000000..59344f322\n--- /dev/null\n+++ b/tests/expectations-symbols/const_generics_thru.c.sym\n@@ -0,0 +1,4 @@\n+{\n+one;\n+two;\n+};\ndiff --git a/tests/expectations-symbols/constant.c.sym b/tests/expectations-symbols/constant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/constant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/constant_sort_name.c.sym b/tests/expectations-symbols/constant_sort_name.c.sym\nnew file mode 100644\nindex 000000000..3f7f900f8\n--- /dev/null\n+++ b/tests/expectations-symbols/constant_sort_name.c.sym\n@@ -0,0 +1,4 @@\n+{\n+C;\n+D;\n+};\ndiff --git a/tests/expectations-symbols/constant_sort_none.c.sym b/tests/expectations-symbols/constant_sort_none.c.sym\nnew file mode 100644\nindex 000000000..85430bed2\n--- /dev/null\n+++ b/tests/expectations-symbols/constant_sort_none.c.sym\n@@ -0,0 +1,4 @@\n+{\n+D;\n+C;\n+};\ndiff --git a/tests/expectations-symbols/custom_header.c.sym b/tests/expectations-symbols/custom_header.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/custom_header.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/decl_name_conflicting.c.sym b/tests/expectations-symbols/decl_name_conflicting.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/decl_name_conflicting.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/dep_v2.c.sym b/tests/expectations-symbols/dep_v2.c.sym\nnew file mode 100644\nindex 000000000..1f4414931\n--- /dev/null\n+++ b/tests/expectations-symbols/dep_v2.c.sym\n@@ -0,0 +1,3 @@\n+{\n+get_x;\n+};\ndiff --git a/tests/expectations-symbols/deprecated.c.sym b/tests/expectations-symbols/deprecated.c.sym\nnew file mode 100644\nindex 000000000..159198dd4\n--- /dev/null\n+++ b/tests/expectations-symbols/deprecated.c.sym\n@@ -0,0 +1,8 @@\n+{\n+deprecated_without_note;\n+deprecated_without_bracket;\n+deprecated_with_note;\n+deprecated_with_note_and_since;\n+deprecated_with_note_which_requires_to_be_escaped;\n+dummy;\n+};\ndiff --git a/tests/expectations-symbols/derive_eq.c.sym b/tests/expectations-symbols/derive_eq.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/derive_eq.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/derive_ostream.c.sym b/tests/expectations-symbols/derive_ostream.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/derive_ostream.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/destructor_and_copy_ctor.c.sym b/tests/expectations-symbols/destructor_and_copy_ctor.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/destructor_and_copy_ctor.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/display_list.c.sym b/tests/expectations-symbols/display_list.c.sym\nnew file mode 100644\nindex 000000000..c39f6f8b3\n--- /dev/null\n+++ b/tests/expectations-symbols/display_list.c.sym\n@@ -0,0 +1,3 @@\n+{\n+push_item;\n+};\ndiff --git a/tests/expectations-symbols/doclength_short.c.sym b/tests/expectations-symbols/doclength_short.c.sym\nnew file mode 100644\nindex 000000000..ea28bf2be\n--- /dev/null\n+++ b/tests/expectations-symbols/doclength_short.c.sym\n@@ -0,0 +1,4 @@\n+{\n+root;\n+trunk;\n+};\ndiff --git a/tests/expectations-symbols/docstyle_auto.c.sym b/tests/expectations-symbols/docstyle_auto.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/docstyle_auto.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/docstyle_c99.c.sym b/tests/expectations-symbols/docstyle_c99.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/docstyle_c99.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/docstyle_doxy.c.sym b/tests/expectations-symbols/docstyle_doxy.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/docstyle_doxy.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/documentation.c.sym b/tests/expectations-symbols/documentation.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/documentation.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/documentation_attr.c.sym b/tests/expectations-symbols/documentation_attr.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/documentation_attr.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/enum.c.sym b/tests/expectations-symbols/enum.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/enum.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/enum_discriminant.c.sym b/tests/expectations-symbols/enum_discriminant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/enum_discriminant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/enum_self.c.sym b/tests/expectations-symbols/enum_self.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/enum_self.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/euclid.c.sym b/tests/expectations-symbols/euclid.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/euclid.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/exclude_generic_monomorph.c.sym b/tests/expectations-symbols/exclude_generic_monomorph.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/exclude_generic_monomorph.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/expand.c.sym b/tests/expectations-symbols/expand.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/expand.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/expand_default_features.c.sym b/tests/expectations-symbols/expand_default_features.c.sym\nnew file mode 100644\nindex 000000000..67cb8bbe7\n--- /dev/null\n+++ b/tests/expectations-symbols/expand_default_features.c.sym\n@@ -0,0 +1,4 @@\n+{\n+extra_debug_fn;\n+root;\n+};\ndiff --git a/tests/expectations-symbols/expand_dep.c.sym b/tests/expectations-symbols/expand_dep.c.sym\nnew file mode 100644\nindex 000000000..1f4414931\n--- /dev/null\n+++ b/tests/expectations-symbols/expand_dep.c.sym\n@@ -0,0 +1,3 @@\n+{\n+get_x;\n+};\ndiff --git a/tests/expectations-symbols/expand_dep_v2.c.sym b/tests/expectations-symbols/expand_dep_v2.c.sym\nnew file mode 100644\nindex 000000000..1f4414931\n--- /dev/null\n+++ b/tests/expectations-symbols/expand_dep_v2.c.sym\n@@ -0,0 +1,3 @@\n+{\n+get_x;\n+};\ndiff --git a/tests/expectations-symbols/expand_features.c.sym b/tests/expectations-symbols/expand_features.c.sym\nnew file mode 100644\nindex 000000000..70a323c35\n--- /dev/null\n+++ b/tests/expectations-symbols/expand_features.c.sym\n@@ -0,0 +1,5 @@\n+{\n+extra_debug_fn;\n+cbindgen;\n+root;\n+};\ndiff --git a/tests/expectations-symbols/expand_no_default_features.c.sym b/tests/expectations-symbols/expand_no_default_features.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/expand_no_default_features.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/export_name.c.sym b/tests/expectations-symbols/export_name.c.sym\nnew file mode 100644\nindex 000000000..3698cd3d3\n--- /dev/null\n+++ b/tests/expectations-symbols/export_name.c.sym\n@@ -0,0 +1,3 @@\n+{\n+do_the_thing_with_export_name;\n+};\ndiff --git a/tests/expectations-symbols/extern.c.sym b/tests/expectations-symbols/extern.c.sym\nnew file mode 100644\nindex 000000000..eb1bbe714\n--- /dev/null\n+++ b/tests/expectations-symbols/extern.c.sym\n@@ -0,0 +1,4 @@\n+{\n+foo;\n+bar;\n+};\ndiff --git a/tests/expectations-symbols/extern_2.c.sym b/tests/expectations-symbols/extern_2.c.sym\nnew file mode 100644\nindex 000000000..8638cd5a5\n--- /dev/null\n+++ b/tests/expectations-symbols/extern_2.c.sym\n@@ -0,0 +1,4 @@\n+{\n+first;\n+second;\n+};\ndiff --git a/tests/expectations-symbols/external_workspace_child.c.sym b/tests/expectations-symbols/external_workspace_child.c.sym\nnew file mode 100644\nindex 000000000..0cb41d3fe\n--- /dev/null\n+++ b/tests/expectations-symbols/external_workspace_child.c.sym\n@@ -0,0 +1,3 @@\n+{\n+consume_ext;\n+};\ndiff --git a/tests/expectations-symbols/fns.c.sym b/tests/expectations-symbols/fns.c.sym\nnew file mode 100644\nindex 000000000..f299d02d3\n--- /dev/null\n+++ b/tests/expectations-symbols/fns.c.sym\n@@ -0,0 +1,4 @@\n+{\n+root;\n+no_return;\n+};\ndiff --git a/tests/expectations-symbols/forward_declaration.c.sym b/tests/expectations-symbols/forward_declaration.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/forward_declaration.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/function_args.c.sym b/tests/expectations-symbols/function_args.c.sym\nnew file mode 100644\nindex 000000000..361aca4ad\n--- /dev/null\n+++ b/tests/expectations-symbols/function_args.c.sym\n@@ -0,0 +1,5 @@\n+{\n+unnamed;\n+pointer_test;\n+print_from_rust;\n+};\ndiff --git a/tests/expectations-symbols/function_noreturn.c.sym b/tests/expectations-symbols/function_noreturn.c.sym\nnew file mode 100644\nindex 000000000..c38fe7d9e\n--- /dev/null\n+++ b/tests/expectations-symbols/function_noreturn.c.sym\n@@ -0,0 +1,4 @@\n+{\n+loop_forever;\n+normal_return;\n+};\ndiff --git a/tests/expectations-symbols/function_ptr.c.sym b/tests/expectations-symbols/function_ptr.c.sym\nnew file mode 100644\nindex 000000000..25aa6ab49\n--- /dev/null\n+++ b/tests/expectations-symbols/function_ptr.c.sym\n@@ -0,0 +1,3 @@\n+{\n+my_function;\n+};\ndiff --git a/tests/expectations-symbols/function_sort_name.c.sym b/tests/expectations-symbols/function_sort_name.c.sym\nnew file mode 100644\nindex 000000000..b4badb861\n--- /dev/null\n+++ b/tests/expectations-symbols/function_sort_name.c.sym\n@@ -0,0 +1,6 @@\n+{\n+A;\n+B;\n+C;\n+D;\n+};\ndiff --git a/tests/expectations-symbols/function_sort_none.c.sym b/tests/expectations-symbols/function_sort_none.c.sym\nnew file mode 100644\nindex 000000000..38ad212b2\n--- /dev/null\n+++ b/tests/expectations-symbols/function_sort_none.c.sym\n@@ -0,0 +1,6 @@\n+{\n+C;\n+B;\n+D;\n+A;\n+};\ndiff --git a/tests/expectations-symbols/generic_pointer.c.sym b/tests/expectations-symbols/generic_pointer.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/generic_pointer.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/global_variable.c.sym b/tests/expectations-symbols/global_variable.c.sym\nnew file mode 100644\nindex 000000000..b64e8969c\n--- /dev/null\n+++ b/tests/expectations-symbols/global_variable.c.sym\n@@ -0,0 +1,4 @@\n+{\n+MUT_GLOBAL_ARRAY;\n+CONST_GLOBAL_ARRAY;\n+};\ndiff --git a/tests/expectations-symbols/ignore.c.sym b/tests/expectations-symbols/ignore.c.sym\nnew file mode 100644\nindex 000000000..dff03068a\n--- /dev/null\n+++ b/tests/expectations-symbols/ignore.c.sym\n@@ -0,0 +1,3 @@\n+{\n+no_ignore_root;\n+};\ndiff --git a/tests/expectations-symbols/include_guard.c.sym b/tests/expectations-symbols/include_guard.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/include_guard.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/inner_mod.c.sym b/tests/expectations-symbols/inner_mod.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/inner_mod.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/item_types.c.sym b/tests/expectations-symbols/item_types.c.sym\nnew file mode 100644\nindex 000000000..f6fcdc99f\n--- /dev/null\n+++ b/tests/expectations-symbols/item_types.c.sym\n@@ -0,0 +1,3 @@\n+{\n+;\n+};\ndiff --git a/tests/expectations-symbols/item_types_renamed.c.sym b/tests/expectations-symbols/item_types_renamed.c.sym\nnew file mode 100644\nindex 000000000..f6fcdc99f\n--- /dev/null\n+++ b/tests/expectations-symbols/item_types_renamed.c.sym\n@@ -0,0 +1,3 @@\n+{\n+;\n+};\ndiff --git a/tests/expectations-symbols/lifetime_arg.c.sym b/tests/expectations-symbols/lifetime_arg.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/lifetime_arg.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/linestyle_cr.c.sym b/tests/expectations-symbols/linestyle_cr.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/linestyle_cr.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/linestyle_crlf.c.sym b/tests/expectations-symbols/linestyle_crlf.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/linestyle_crlf.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/linestyle_lf.c.sym b/tests/expectations-symbols/linestyle_lf.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/linestyle_lf.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/mangle.c.sym b/tests/expectations-symbols/mangle.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/mangle.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/manuallydrop.c.sym b/tests/expectations-symbols/manuallydrop.c.sym\nnew file mode 100644\nindex 000000000..2d62f6ada\n--- /dev/null\n+++ b/tests/expectations-symbols/manuallydrop.c.sym\n@@ -0,0 +1,4 @@\n+{\n+root;\n+take;\n+};\ndiff --git a/tests/expectations-symbols/maybeuninit.c.sym b/tests/expectations-symbols/maybeuninit.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/maybeuninit.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/mod_2015.c.sym b/tests/expectations-symbols/mod_2015.c.sym\nnew file mode 100644\nindex 000000000..bbc2e6756\n--- /dev/null\n+++ b/tests/expectations-symbols/mod_2015.c.sym\n@@ -0,0 +1,4 @@\n+{\n+export_me;\n+from_really_nested_mod;\n+};\ndiff --git a/tests/expectations-symbols/mod_2018.c.sym b/tests/expectations-symbols/mod_2018.c.sym\nnew file mode 100644\nindex 000000000..814cea8ee\n--- /dev/null\n+++ b/tests/expectations-symbols/mod_2018.c.sym\n@@ -0,0 +1,5 @@\n+{\n+export_me;\n+export_me_2;\n+from_really_nested_mod;\n+};\ndiff --git a/tests/expectations-symbols/mod_attr.c.sym b/tests/expectations-symbols/mod_attr.c.sym\nnew file mode 100644\nindex 000000000..eb1bbe714\n--- /dev/null\n+++ b/tests/expectations-symbols/mod_attr.c.sym\n@@ -0,0 +1,4 @@\n+{\n+foo;\n+bar;\n+};\ndiff --git a/tests/expectations-symbols/mod_path.c.sym b/tests/expectations-symbols/mod_path.c.sym\nnew file mode 100644\nindex 000000000..441241ba2\n--- /dev/null\n+++ b/tests/expectations-symbols/mod_path.c.sym\n@@ -0,0 +1,3 @@\n+{\n+export_me;\n+};\ndiff --git a/tests/expectations-symbols/monomorph_1.c.sym b/tests/expectations-symbols/monomorph_1.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/monomorph_1.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/monomorph_2.c.sym b/tests/expectations-symbols/monomorph_2.c.sym\nnew file mode 100644\nindex 000000000..eb1bbe714\n--- /dev/null\n+++ b/tests/expectations-symbols/monomorph_2.c.sym\n@@ -0,0 +1,4 @@\n+{\n+foo;\n+bar;\n+};\ndiff --git a/tests/expectations-symbols/monomorph_3.c.sym b/tests/expectations-symbols/monomorph_3.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/monomorph_3.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/must_use.c.sym b/tests/expectations-symbols/must_use.c.sym\nnew file mode 100644\nindex 000000000..2b1a7af14\n--- /dev/null\n+++ b/tests/expectations-symbols/must_use.c.sym\n@@ -0,0 +1,3 @@\n+{\n+maybe_consume;\n+};\ndiff --git a/tests/expectations-symbols/namespace_constant.c.sym b/tests/expectations-symbols/namespace_constant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/namespace_constant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/namespaces_constant.c.sym b/tests/expectations-symbols/namespaces_constant.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/namespaces_constant.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/no_includes.c.sym b/tests/expectations-symbols/no_includes.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/no_includes.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/non_pub_extern.c.sym b/tests/expectations-symbols/non_pub_extern.c.sym\nnew file mode 100644\nindex 000000000..ce9b8673b\n--- /dev/null\n+++ b/tests/expectations-symbols/non_pub_extern.c.sym\n@@ -0,0 +1,6 @@\n+{\n+first;\n+renamed;\n+FIRST;\n+RENAMED;\n+};\ndiff --git a/tests/expectations-symbols/nonnull.c.sym b/tests/expectations-symbols/nonnull.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/nonnull.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/nonnull_attribute.c.sym b/tests/expectations-symbols/nonnull_attribute.c.sym\nnew file mode 100644\nindex 000000000..78a53d315\n--- /dev/null\n+++ b/tests/expectations-symbols/nonnull_attribute.c.sym\n@@ -0,0 +1,10 @@\n+{\n+value_arg;\n+mutltiple_args;\n+ref_arg;\n+mut_ref_arg;\n+optional_ref_arg;\n+optional_mut_ref_arg;\n+nullable_const_ptr;\n+nullable_mut_ptr;\n+};\ndiff --git a/tests/expectations-symbols/nonzero.c.sym b/tests/expectations-symbols/nonzero.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/nonzero.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/opaque.c.sym b/tests/expectations-symbols/opaque.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/opaque.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/pin.c.sym b/tests/expectations-symbols/pin.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/pin.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/pragma_once.c.sym b/tests/expectations-symbols/pragma_once.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/pragma_once.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/prefix.c.sym b/tests/expectations-symbols/prefix.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/prefix.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/prefixed_struct_literal.c.sym b/tests/expectations-symbols/prefixed_struct_literal.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/prefixed_struct_literal.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/prefixed_struct_literal_deep.c.sym b/tests/expectations-symbols/prefixed_struct_literal_deep.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/prefixed_struct_literal_deep.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/ptrs_as_arrays.c.sym b/tests/expectations-symbols/ptrs_as_arrays.c.sym\nnew file mode 100644\nindex 000000000..de7efd434\n--- /dev/null\n+++ b/tests/expectations-symbols/ptrs_as_arrays.c.sym\n@@ -0,0 +1,7 @@\n+{\n+ptr_as_array;\n+ptr_as_array1;\n+ptr_as_array2;\n+ptr_as_array_wrong_syntax;\n+ptr_as_array_unnamed;\n+};\ndiff --git a/tests/expectations-symbols/raw_ident.c.sym b/tests/expectations-symbols/raw_ident.c.sym\nnew file mode 100644\nindex 000000000..ea76f34b7\n--- /dev/null\n+++ b/tests/expectations-symbols/raw_ident.c.sym\n@@ -0,0 +1,4 @@\n+{\n+fn;\n+STATIC;\n+};\ndiff --git a/tests/expectations-symbols/raw_lines.c.sym b/tests/expectations-symbols/raw_lines.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/raw_lines.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/rename.c.sym b/tests/expectations-symbols/rename.c.sym\nnew file mode 100644\nindex 000000000..8a247ae6f\n--- /dev/null\n+++ b/tests/expectations-symbols/rename.c.sym\n@@ -0,0 +1,4 @@\n+{\n+root;\n+G;\n+};\ndiff --git a/tests/expectations-symbols/rename_case.c.sym b/tests/expectations-symbols/rename_case.c.sym\nnew file mode 100644\nindex 000000000..1bacd14c0\n--- /dev/null\n+++ b/tests/expectations-symbols/rename_case.c.sym\n@@ -0,0 +1,7 @@\n+{\n+test_camel_case;\n+test_pascal_case;\n+test_snake_case;\n+test_screaming_snake_case;\n+test_gecko_case;\n+};\ndiff --git a/tests/expectations-symbols/rename_crate.c.sym b/tests/expectations-symbols/rename_crate.c.sym\nnew file mode 100644\nindex 000000000..256a2894a\n--- /dev/null\n+++ b/tests/expectations-symbols/rename_crate.c.sym\n@@ -0,0 +1,5 @@\n+{\n+root;\n+renamed_func;\n+no_extern_func;\n+};\ndiff --git a/tests/expectations-symbols/renaming_overrides_prefixing.c.sym b/tests/expectations-symbols/renaming_overrides_prefixing.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/renaming_overrides_prefixing.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/reserved.c.sym b/tests/expectations-symbols/reserved.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/reserved.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/sentinel.c.sym b/tests/expectations-symbols/sentinel.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/sentinel.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/simplify_option_ptr.c.sym b/tests/expectations-symbols/simplify_option_ptr.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/simplify_option_ptr.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/size_types.c.sym b/tests/expectations-symbols/size_types.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/size_types.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/static.c.sym b/tests/expectations-symbols/static.c.sym\nnew file mode 100644\nindex 000000000..53daac495\n--- /dev/null\n+++ b/tests/expectations-symbols/static.c.sym\n@@ -0,0 +1,6 @@\n+{\n+root;\n+NUMBER;\n+FOO;\n+BAR;\n+};\ndiff --git a/tests/expectations-symbols/std_lib.c.sym b/tests/expectations-symbols/std_lib.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/std_lib.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/struct.c.sym b/tests/expectations-symbols/struct.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/struct.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/struct_literal.c.sym b/tests/expectations-symbols/struct_literal.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/struct_literal.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/struct_literal_order.c.sym b/tests/expectations-symbols/struct_literal_order.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/struct_literal_order.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/struct_self.c.sym b/tests/expectations-symbols/struct_self.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/struct_self.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/swift_name.c.sym b/tests/expectations-symbols/swift_name.c.sym\nnew file mode 100644\nindex 000000000..5110fbbf8\n--- /dev/null\n+++ b/tests/expectations-symbols/swift_name.c.sym\n@@ -0,0 +1,21 @@\n+{\n+rust_print_hello_world;\n+SelfTypeTestStruct_should_exist_ref;\n+SelfTypeTestStruct_should_exist_ref_mut;\n+SelfTypeTestStruct_should_not_exist_box;\n+SelfTypeTestStruct_should_not_exist_return_box;\n+SelfTypeTestStruct_should_exist_annotated_self;\n+SelfTypeTestStruct_should_exist_annotated_mut_self;\n+SelfTypeTestStruct_should_exist_annotated_by_name;\n+SelfTypeTestStruct_should_exist_annotated_mut_by_name;\n+SelfTypeTestStruct_should_exist_unannotated;\n+SelfTypeTestStruct_should_exist_mut_unannotated;\n+free_function_should_exist_ref;\n+free_function_should_exist_ref_mut;\n+unnamed_argument;\n+free_function_should_not_exist_box;\n+free_function_should_exist_annotated_by_name;\n+free_function_should_exist_annotated_mut_by_name;\n+PointerToOpaque_create;\n+PointerToOpaque_sayHello;\n+};\ndiff --git a/tests/expectations-symbols/transform_op.c.sym b/tests/expectations-symbols/transform_op.c.sym\nnew file mode 100644\nindex 000000000..57c772c4a\n--- /dev/null\n+++ b/tests/expectations-symbols/transform_op.c.sym\n@@ -0,0 +1,3 @@\n+{\n+foo;\n+};\ndiff --git a/tests/expectations-symbols/transparent.c.sym b/tests/expectations-symbols/transparent.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/transparent.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/typedef.c.sym b/tests/expectations-symbols/typedef.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/typedef.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/union.c.sym b/tests/expectations-symbols/union.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/union.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/union_self.c.sym b/tests/expectations-symbols/union_self.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/union_self.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/using_namespaces.c.sym b/tests/expectations-symbols/using_namespaces.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/using_namespaces.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/expectations-symbols/va_list.c.sym b/tests/expectations-symbols/va_list.c.sym\nnew file mode 100644\nindex 000000000..715658c35\n--- /dev/null\n+++ b/tests/expectations-symbols/va_list.c.sym\n@@ -0,0 +1,4 @@\n+{\n+va_list_test;\n+va_list_test2;\n+};\ndiff --git a/tests/expectations-symbols/workspace.c.sym b/tests/expectations-symbols/workspace.c.sym\nnew file mode 100644\nindex 000000000..0cb41d3fe\n--- /dev/null\n+++ b/tests/expectations-symbols/workspace.c.sym\n@@ -0,0 +1,3 @@\n+{\n+consume_ext;\n+};\ndiff --git a/tests/expectations-symbols/zst.c.sym b/tests/expectations-symbols/zst.c.sym\nnew file mode 100644\nindex 000000000..8c89234f5\n--- /dev/null\n+++ b/tests/expectations-symbols/zst.c.sym\n@@ -0,0 +1,3 @@\n+{\n+root;\n+};\ndiff --git a/tests/tests.rs b/tests/tests.rs\nindex eebabefbe..56fce8b39 100644\n--- a/tests/tests.rs\n+++ b/tests/tests.rs\n@@ -1,6 +1,7 @@\n extern crate cbindgen;\n \n use cbindgen::*;\n+use tempfile::NamedTempFile;\n use std::collections::HashSet;\n use std::fs::File;\n use std::io::Read;\n@@ -21,6 +22,12 @@ fn style_str(style: Style) -> &'static str {\n     }\n }\n \n+struct CBindgenOutput {\n+    bindings_content: Vec<u8>,\n+    depfile_content: Option<String>,\n+    symfile_content: Option<String>,\n+}\n+\n fn run_cbindgen(\n     path: &Path,\n     output: Option<&Path>,\n@@ -29,20 +36,30 @@ fn run_cbindgen(\n     style: Option<Style>,\n     generate_depfile: bool,\n     package_version: bool,\n-) -> (Vec<u8>, Option<String>) {\n+    generate_symfile: bool,\n+) -> CBindgenOutput {\n     assert!(\n-        !(output.is_none() && generate_depfile),\n-        \"generating a depfile requires outputting to a path\"\n+        !output.is_none() || !(generate_depfile || generate_symfile),\n+        \"generating a depfile or symfile requires outputting to a path\"\n     );\n     let program = Path::new(CBINDGEN_PATH);\n     let mut command = Command::new(program);\n     if let Some(output) = output {\n         command.arg(\"--output\").arg(output);\n     }\n-    let cbindgen_depfile = if generate_depfile {\n-        let depfile = tempfile::NamedTempFile::new().unwrap();\n-        command.arg(\"--depfile\").arg(depfile.path());\n-        Some(depfile)\n+    \n+    let depfile = if generate_depfile {\n+        let tmp = tempfile::NamedTempFile::new().unwrap();\n+        command.arg(\"--depfile\").arg(tmp.path());\n+        Some(tmp)\n+    } else {\n+        None\n+    };\n+    \n+    let symfile = if generate_symfile {\n+        let tmp = tempfile::NamedTempFile::new().unwrap();\n+        command.arg(\"--symfile\").arg(tmp.path());\n+        Some(tmp)\n     } else {\n         None\n     };\n@@ -86,7 +103,7 @@ fn run_cbindgen(\n         str::from_utf8(&cbindgen_output.stderr).unwrap_or_default()\n     );\n \n-    let bindings = if let Some(output_path) = output {\n+    let bindings_content = if let Some(output_path) = output {\n         let mut bindings = Vec::new();\n         // Ignore errors here, we have assertions on the expected output later.\n         let _ = File::open(output_path).map(|mut file| {\n@@ -97,18 +114,17 @@ fn run_cbindgen(\n         cbindgen_output.stdout\n     };\n \n-    let depfile_contents = if let Some(mut depfile) = cbindgen_depfile {\n-        let mut raw = Vec::new();\n-        depfile.read_to_end(&mut raw).unwrap();\n-        Some(\n-            str::from_utf8(raw.as_slice())\n-                .expect(\"Invalid encoding encountered in depfile\")\n-                .into(),\n-        )\n-    } else {\n-        None\n-    };\n-    (bindings, depfile_contents)\n+    fn read_to_string(f: NamedTempFile) -> String {\n+        std::fs::read_to_string(&f).expect(&format!(\"Failed to read file as String: {:?}\", f))\n+    }\n+    let depfile_content = depfile.map(read_to_string);\n+    let symfile_content = symfile.map(read_to_string);\n+\n+    CBindgenOutput {\n+        bindings_content,\n+        depfile_content,\n+        symfile_content,\n+    }\n }\n \n fn compile(\n@@ -208,11 +224,17 @@ fn run_compile_test(\n     style: Option<Style>,\n     cbindgen_outputs: &mut HashSet<Vec<u8>>,\n     package_version: bool,\n+    generate_symfile: bool\n ) {\n     let crate_dir = env::var(\"CARGO_MANIFEST_DIR\").unwrap();\n     let tests_path = Path::new(&crate_dir).join(\"tests\");\n     let mut generated_file = tests_path.join(\"expectations\");\n+    let mut generated_symfile = tests_path.join(\"expectations-symbols\");\n     fs::create_dir_all(&generated_file).unwrap();\n+    fs::create_dir_all(&generated_symfile).unwrap();\n+    \n+    let verify = env::var_os(\"CBINDGEN_TEST_VERIFY\").is_some();\n+    let no_compile = env::var_os(\"CBINDGEN_TEST_NO_COMPILE\").is_some();\n \n     let style_ext = style\n         // Cython is sensitive to dots, so we can't include any dots.\n@@ -236,10 +258,12 @@ fn run_compile_test(\n \n     let source_file =\n         format!(\"{}{}{}\", name, style_ext, lang_ext).replace(SKIP_WARNING_AS_ERROR_SUFFIX, \"\");\n+    let symbols_file = format!(\"{source_file}.sym\");\n \n     generated_file.push(source_file);\n+    generated_symfile.push(symbols_file);\n \n-    let (output_file, generate_depfile) = if env::var_os(\"CBINDGEN_TEST_VERIFY\").is_some() {\n+    let (output_file, generate_depfile) = if verify {\n         (None, false)\n     } else {\n         (\n@@ -249,7 +273,7 @@ fn run_compile_test(\n         )\n     };\n \n-    let (cbindgen_output, depfile_contents) = run_cbindgen(\n+    let CBindgenOutput { bindings_content, depfile_content, symfile_content } = run_cbindgen(\n         path,\n         output_file,\n         language,\n@@ -257,9 +281,10 @@ fn run_compile_test(\n         style,\n         generate_depfile,\n         package_version,\n+        true\n     );\n     if generate_depfile {\n-        let depfile = depfile_contents.expect(\"No depfile generated\");\n+        let depfile = depfile_content.expect(\"No depfile generated\");\n         assert!(!depfile.is_empty());\n         let mut rules = depfile.split(':');\n         let target = rules.next().expect(\"No target found\");\n@@ -275,27 +300,36 @@ fn run_compile_test(\n         assert_eq!(rules.count(), 0, \"More than 1 rule in the depfile\");\n     }\n \n-    if cbindgen_outputs.contains(&cbindgen_output) {\n+    if cbindgen_outputs.contains(&bindings_content) {\n         // We already generated an identical file previously.\n-        if env::var_os(\"CBINDGEN_TEST_VERIFY\").is_some() {\n+        if verify {\n             assert!(!generated_file.exists());\n         } else if generated_file.exists() {\n             fs::remove_file(&generated_file).unwrap();\n         }\n     } else {\n-        if env::var_os(\"CBINDGEN_TEST_VERIFY\").is_some() {\n-            use std::str::from_utf8;\n-            let prev_cbindgen_output = fs::read(&generated_file).unwrap();\n-            let cbindgen_output = from_utf8(&cbindgen_output).unwrap();\n-            let prev_cbindgen_output = from_utf8(&prev_cbindgen_output).unwrap();\n-            assert_eq!(prev_cbindgen_output, cbindgen_output);\n+        if verify {\n+            // Compare cbindgen output to expected (existing on disk) output.\n+            let prev_cbindgen_bindings = fs::read(&generated_file).unwrap();\n+            assert_eq!(bindings_content, prev_cbindgen_bindings);\n+            \n+            if generate_symfile {\n+                let symbols = symfile_content.expect(\"No symfile generated\");\n+                let prev_cbindgen_symbols = fs::read(&generated_symfile).unwrap();\n+                let prev_symbols = String::from_utf8(prev_cbindgen_symbols).unwrap();\n+                assert_eq!(symbols, prev_symbols);\n+            }\n         } else {\n-            fs::write(&generated_file, &cbindgen_output).unwrap();\n+            fs::write(&generated_file, &bindings_content).expect(\"Failed to write generated bindings.\");\n+            if generate_symfile {\n+                let symbols = symfile_content.expect(\"No symfile generated\");\n+                fs::write(&generated_symfile, &symbols).expect(\"Failed to write generated symbols.\");\n+            }\n         }\n \n-        cbindgen_outputs.insert(cbindgen_output);\n+        cbindgen_outputs.insert(bindings_content);\n \n-        if env::var_os(\"CBINDGEN_TEST_NO_COMPILE\").is_some() {\n+        if no_compile {\n             return;\n         }\n \n@@ -333,6 +367,9 @@ fn test_file(name: &'static str, filename: &'static str) {\n     let mut cbindgen_outputs = HashSet::new();\n     for cpp_compat in &[true, false] {\n         for style in &[Style::Type, Style::Tag, Style::Both] {\n+             // We only need to generate the symfile once,\n+             // it should not change with the different options.\n+            let generate_symfile = !cpp_compat && *style == Style::Type;\n             run_compile_test(\n                 name,\n                 test,\n@@ -342,6 +379,7 @@ fn test_file(name: &'static str, filename: &'static str) {\n                 Some(*style),\n                 &mut cbindgen_outputs,\n                 false,\n+                generate_symfile,\n             );\n         }\n     }\n@@ -355,6 +393,7 @@ fn test_file(name: &'static str, filename: &'static str) {\n         None,\n         &mut HashSet::new(),\n         false,\n+        /* generate_symfile = */ false\n     );\n \n     // `Style::Both` should be identical to `Style::Tag` for Cython.\n@@ -369,6 +408,7 @@ fn test_file(name: &'static str, filename: &'static str) {\n             Some(*style),\n             &mut cbindgen_outputs,\n             false,\n+            /* generate_symfile = */ false\n         );\n     }\n }\n", "problem_statement": "Allow access to bindings symbols names\nWhat I'm trying to do:\r\n1. write a Rust library with a C-compatible interface (extern functions, etc)\r\n2. use `cbindgen` to generate a C header, to use the library from C\r\n3. compile a Rust program that depends on the library and links with `-C link-args=-Wl,--export-dynamic-symbol=<regex of symbols to export>` (or another [ldd option](https://man.archlinux.org/man/extra/lld/ld.lld.1.en)), in order to load plugins (written in C) that use the library symbols\r\n\r\nFor step 3, I need the list of symbols in the C header. I can parse the header or fill this information by hand, but `cbindgen` already has the list! What is missing is a way to access it.\r\n\r\nI can see two ways of solving this problem (both may be useful):\r\n1. Modify the `Bindings` impl: add a public API to read the `items` of the bindings (with methods to get the exported name of an item of any type), for instance:\r\n```rs\r\nlet bindings = cbindgen::Builder::new().with_crate(crate_dir).with_language(C).generate().unwrap(); // usual generation\r\nlet symbols_names: Vec<String> = bindings.items.iter().map(|i| i.export_name()).collect(); // would be awesome if possible\r\n```\r\n2. Add a way to export the symbols in the [linker format](https://stackoverflow.com/a/70555660), which looks like\r\n```\r\n{\r\n   symbol_1;\r\n   symbol_2;\r\n};\r\n```\r\nI will then be able to use `--export-dynamic-symbol-list=<exported file>` in order to export the symbols properly.\r\n\r\nIf someone guides me a little bit, I could try to implement one of the aforementioned features (or both, who knows?) :slightly_smiling_face:.\n", "hints_text": "", "created_at": "2024-01-08 16:08:40", "merge_commit_sha": "87afbf9e01ae8d9190638d4b347676ff7e282d4f", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build (amd64)', '.github/workflows/cbindgen.yml']", "['build (arm64)', '.github/workflows/cbindgen.yml']"]]}
{"repo": "m4b/goblin", "instance_id": "m4b__goblin-397", "base_commit": "ff2208954608fc163f54ad0abcf3aafe02fdcdb5", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex ba11f32fb..1b0f1ba3f 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -19,7 +19,7 @@ include = [\n     \"LICENSE\",\n     \"README.md\",\n ]\n-keywords = [\"binary\", \"elf\", \"mach\", \"pe\", \"archive\"]\n+keywords = [\"binary\", \"elf\", \"mach\", \"pe\", \"te\", \"archive\"]\n license = \"MIT\"\n readme = \"README.md\"\n repository = \"https://github.com/m4b/goblin\"\n@@ -38,7 +38,7 @@ version = \"0.12\"\n default_features = false\n \n [features]\n-default = [\"std\", \"elf32\", \"elf64\", \"mach32\", \"mach64\", \"pe32\", \"pe64\", \"archive\", \"endian_fd\"]\n+default = [\"std\", \"elf32\", \"elf64\", \"mach32\", \"mach64\", \"pe32\", \"pe64\", \"te\", \"archive\", \"endian_fd\"]\n std = [\"alloc\", \"scroll/std\"]\n alloc = [\"scroll/derive\", \"log\"]\n endian_fd = [\"alloc\"]\n@@ -49,6 +49,7 @@ mach32 = [\"alloc\", \"endian_fd\", \"archive\"]\n mach64 = [\"alloc\", \"endian_fd\", \"archive\"]\n pe32 = [\"alloc\", \"endian_fd\"]\n pe64 = [\"alloc\", \"endian_fd\"]\n+te = [\"alloc\", \"endian_fd\"]\n archive = [\"alloc\"]\n \n [badges.travis-ci]\ndiff --git a/README.md b/README.md\nindex 30d30f3c9..e4680c223 100644\n--- a/README.md\n+++ b/README.md\n@@ -97,6 +97,7 @@ Here are some things you could do with this crate (or help to implement so they\n * mach32 - 32-bit mach-o `repr(C)` struct defs\n * pe32 - 32-bit PE `repr(C)` struct defs\n * pe64 - 64-bit PE `repr(C)` struct defs\n++ te - Terse Executable (TE) `repr(C)` struct defs\n * archive - a Unix Archive parser\n * endian_fd - parses according to the endianness in the binary\n * std - to allow `no_std` environments\ndiff --git a/src/lib.rs b/src/lib.rs\nindex ec77f93d5..e811de1ea 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -229,6 +229,7 @@ pub enum Hint {\n     Mach(HintData),\n     MachFat(usize),\n     PE,\n+    TE,\n     COFF,\n     Archive,\n     Unknown(u64),\n@@ -236,7 +237,7 @@ pub enum Hint {\n \n macro_rules! if_everything {\n     ($($i:item)*) => ($(\n-        #[cfg(all(feature = \"endian_fd\", feature = \"elf64\", feature = \"elf32\", feature = \"pe64\", feature = \"pe32\", feature = \"mach64\", feature = \"mach32\", feature = \"archive\"))]\n+        #[cfg(all(feature = \"endian_fd\", feature = \"elf64\", feature = \"elf32\", feature = \"pe64\", feature = \"pe32\", feature = \"te\", feature = \"mach64\", feature = \"mach32\", feature = \"archive\"))]\n         $i\n     )*)\n }\n@@ -262,6 +263,7 @@ if_everything! {\n         } else {\n             match *&bytes[0..2].pread_with::<u16>(0, LE)? {\n                 pe::header::DOS_MAGIC => Ok(Hint::PE),\n+                pe::header::TE_MAGIC => Ok(Hint::TE),\n                 pe::header::COFF_MACHINE_X86 |\n                 pe::header::COFF_MACHINE_X86_64 |\n                 pe::header::COFF_MACHINE_ARM64 => Ok(Hint::COFF),\n@@ -290,6 +292,8 @@ if_everything! {\n         Elf(elf::Elf<'a>),\n         /// A PE32/PE32+!\n         PE(pe::PE<'a>),\n+        /// A TE!\n+        TE(pe::TE<'a>),\n         /// A COFF\n         COFF(pe::Coff<'a>),\n         /// A 32/64-bit Mach-o binary _OR_ it is a multi-architecture binary container!\n@@ -309,6 +313,7 @@ if_everything! {\n                     Hint::Mach(_) | Hint::MachFat(_) => Ok(Object::Mach(mach::Mach::parse(bytes)?)),\n                     Hint::Archive => Ok(Object::Archive(archive::Archive::parse(bytes)?)),\n                     Hint::PE => Ok(Object::PE(pe::PE::parse(bytes)?)),\n+                    Hint::TE => Ok(Object::TE(pe::TE::parse(bytes)?)),\n                     Hint::COFF => Ok(Object::COFF(pe::Coff::parse(bytes)?)),\n                     Hint::Unknown(magic) => Ok(Object::Unknown(magic)),\n                 }\ndiff --git a/src/pe/debug.rs b/src/pe/debug.rs\nindex 311bc632c..cae77b4be 100644\n--- a/src/pe/debug.rs\n+++ b/src/pe/debug.rs\n@@ -92,7 +92,7 @@ impl ImageDebugDirectory {\n         )\n     }\n \n-    fn parse_with_opts(\n+    pub(crate) fn parse_with_opts(\n         bytes: &[u8],\n         dd: data_directories::DataDirectory,\n         sections: &[section_table::SectionTable],\ndiff --git a/src/pe/header.rs b/src/pe/header.rs\nindex cb3b08faa..62bfe1906 100644\n--- a/src/pe/header.rs\n+++ b/src/pe/header.rs\n@@ -1,5 +1,5 @@\n use crate::error;\n-use crate::pe::{optional_header, section_table, symbol};\n+use crate::pe::{data_directories, optional_header, section_table, symbol};\n use crate::strtab;\n use alloc::vec::Vec;\n use log::debug;\n@@ -837,6 +837,147 @@ impl ctx::TryIntoCtx<scroll::Endian> for Header {\n     }\n }\n \n+/// The TE header is a reduced PE32/PE32+ header containing only fields\n+/// required for execution in the Platform Initialization\n+/// ([PI](https://uefi.org/specs/PI/1.8/V1_Introduction.html)) architecture.\n+/// The TE header is described in this specification:\n+/// <https://uefi.org/specs/PI/1.8/V1_TE_Image.html#te-header>\n+#[cfg(feature = \"te\")]\n+#[repr(C)]\n+#[derive(Debug, Default, PartialEq, Copy, Clone, Pread, Pwrite)]\n+pub struct TeHeader {\n+    /// Te signature, always [TE_MAGIC]\n+    pub signature: u16,\n+    /// The machine type\n+    pub machine: u16,\n+    /// The number of sections\n+    pub number_of_sections: u8,\n+    /// The subsystem\n+    pub subsystem: u8,\n+    /// the amount of bytes stripped from the header when converting from a\n+    /// PE32/PE32+ header to a TE header. Used to resolve addresses\n+    pub stripped_size: u16,\n+    /// The entry point of the binary\n+    pub entry_point: u32,\n+    /// The base of the code section\n+    pub base_of_code: u32,\n+    /// The image base\n+    pub image_base: u64,\n+    /// The size and address of the relocation directory\n+    pub reloc_dir: data_directories::DataDirectory,\n+    /// The size and address of the debug directory\n+    pub debug_dir: data_directories::DataDirectory,\n+}\n+\n+#[cfg(feature = \"te\")]\n+#[doc(alias(\"IMAGE_TE_SIGNATURE\"))]\n+pub const TE_MAGIC: u16 = 0x5a56;\n+\n+#[cfg(feature = \"te\")]\n+impl TeHeader {\n+    /// Parse the TE header from the given bytes.\n+    pub fn parse(bytes: &[u8], offset: &mut usize) -> error::Result<Self> {\n+        let mut header: TeHeader = bytes.gread_with(offset, scroll::LE)?;\n+        let adj_offset = header.stripped_size as u32 - core::mem::size_of::<TeHeader>() as u32;\n+        header.fixup_header(adj_offset);\n+        Ok(header)\n+    }\n+\n+    /// Parse the sections from the TE header.\n+    pub fn sections(\n+        &self,\n+        bytes: &[u8],\n+        offset: &mut usize,\n+    ) -> error::Result<Vec<section_table::SectionTable>> {\n+        let adj_offset = self.stripped_size as u32 - core::mem::size_of::<TeHeader>() as u32;\n+        let nsections = self.number_of_sections as usize;\n+\n+        // a section table is at least 40 bytes\n+        if nsections > bytes.len() / 40 {\n+            return Err(error::Error::BufferTooShort(nsections, \"sections\"));\n+        }\n+\n+        let mut sections = Vec::with_capacity(nsections);\n+        for i in 0..nsections {\n+            let mut section = section_table::SectionTable::parse(bytes, offset, 0)?;\n+            TeHeader::fixup_section(&mut section, adj_offset);\n+            debug!(\"({}) {:#?}\", i, section);\n+            sections.push(section);\n+        }\n+        Ok(sections)\n+    }\n+\n+    // Adjust addresses in the header to account for the stripped size\n+    fn fixup_header(&mut self, adj_offset: u32) {\n+        debug!(\n+            \"Entry point fixed up from: 0x{:x} to 0x{:X}\",\n+            self.entry_point,\n+            self.entry_point.wrapping_sub(adj_offset)\n+        );\n+        self.entry_point = self.entry_point.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Base of code fixed up from: 0x{:x} to 0x{:X}\",\n+            self.base_of_code,\n+            self.base_of_code.wrapping_sub(adj_offset)\n+        );\n+        self.base_of_code = self.base_of_code.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Relocation Directory fixed up from: 0x{:x} to 0x{:X}\",\n+            self.reloc_dir.virtual_address,\n+            self.reloc_dir.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        self.reloc_dir.virtual_address = self.reloc_dir.virtual_address.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Debug Directory fixed up from: 0x{:x} to 0x{:X}\",\n+            self.debug_dir.virtual_address,\n+            self.debug_dir.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        self.debug_dir.virtual_address = self.debug_dir.virtual_address.wrapping_sub(adj_offset);\n+    }\n+\n+    // Adjust addresses in the section to account for the stripped size\n+    fn fixup_section(section: &mut section_table::SectionTable, adj_offset: u32) {\n+        debug!(\n+            \"Section virtual address fixed up from: 0x{:X} to 0x{:X}\",\n+            section.virtual_address,\n+            section.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        section.virtual_address = section.virtual_address.wrapping_sub(adj_offset);\n+\n+        if section.pointer_to_linenumbers > 0 {\n+            debug!(\n+                \"Section pointer to line numbers fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_linenumbers,\n+                section.pointer_to_linenumbers.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_linenumbers =\n+                section.pointer_to_linenumbers.wrapping_sub(adj_offset);\n+        }\n+\n+        if section.pointer_to_raw_data > 0 {\n+            debug!(\n+                \"Section pointer to raw data fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_raw_data,\n+                section.pointer_to_raw_data.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_raw_data = section.pointer_to_raw_data.wrapping_sub(adj_offset);\n+        }\n+\n+        if section.pointer_to_relocations > 0 {\n+            debug!(\n+                \"Section pointer to relocations fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_relocations,\n+                section.pointer_to_relocations.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_relocations =\n+                section.pointer_to_relocations.wrapping_sub(adj_offset);\n+        }\n+    }\n+}\n+\n /// Convert machine to str representation. Any case of \"COFF_UNKNOWN\"\n /// should be expected to change to a more specific value.\n pub fn machine_to_str(machine: u16) -> &'static str {\ndiff --git a/src/pe/mod.rs b/src/pe/mod.rs\nindex dd9c2c5df..1f2bac7a9 100644\n--- a/src/pe/mod.rs\n+++ b/src/pe/mod.rs\n@@ -467,6 +467,98 @@ impl<'a> ctx::TryIntoCtx<scroll::Endian> for PE<'a> {\n     }\n }\n \n+/// An analyzed TE binary\n+///\n+/// A TE binary is a PE/PE32+ binary that has had it's header stripped and\n+/// re-formatted to the TE specification. This presents a challenge for\n+/// parsing, as all relative addresses (RVAs) are not updated to take this into\n+/// account, and are thus incorrect. The parsing of a TE must take this into\n+/// account by using the [header::TeHeader::stripped_size`] field of the TE\n+/// header to adjust the RVAs during parsing.\n+#[cfg(feature = \"te\")]\n+#[derive(Debug)]\n+pub struct TE<'a> {\n+    /// The TE header\n+    pub header: header::TeHeader,\n+    /// A list of the sections in this TE binary\n+    pub sections: Vec<section_table::SectionTable>,\n+    /// Debug information, contained in the PE header\n+    pub debug_data: debug::DebugData<'a>,\n+    /// The offset to apply to addresses not parsed by the TE parser\n+    /// itself: [header::TeHeader::stripped_size] - size_of::<[header::TeHeader]>()\n+    pub rva_offset: usize,\n+}\n+\n+#[cfg(feature = \"te\")]\n+impl<'a> TE<'a> {\n+    /// Reads a TE binary from the underlying `bytes`\n+    pub fn parse(bytes: &'a [u8]) -> error::Result<Self> {\n+        let opts = &options::ParseOptions {\n+            resolve_rva: false,\n+            parse_attribute_certificates: false,\n+        };\n+\n+        let mut offset = 0;\n+\n+        // Parse the TE header and adjust the offsets\n+        let header = header::TeHeader::parse(bytes, &mut offset)?;\n+        let rva_offset = header.stripped_size as usize - core::mem::size_of::<header::TeHeader>();\n+\n+        // Parse the sections and adjust the offsets\n+        let sections = header.sections(bytes, &mut offset)?;\n+\n+        // Parse the debug data. Must adjust offsets before parsing the image_debug_directory\n+        let mut debug_data = debug::DebugData::default();\n+        debug_data.image_debug_directory = debug::ImageDebugDirectory::parse_with_opts(\n+            bytes,\n+            header.debug_dir,\n+            &sections,\n+            0,\n+            opts,\n+        )?;\n+        TE::fixup_debug_data(&mut debug_data, rva_offset as u32);\n+        debug_data.codeview_pdb70_debug_info = debug::CodeviewPDB70DebugInfo::parse_with_opts(\n+            bytes,\n+            &debug_data.image_debug_directory,\n+            opts,\n+        )?;\n+\n+        Ok(TE {\n+            header,\n+            sections,\n+            debug_data,\n+            rva_offset,\n+        })\n+    }\n+\n+    /// Adjust all addresses in the TE binary debug data.\n+    fn fixup_debug_data(dd: &mut debug::DebugData, rva_offset: u32) {\n+        debug!(\n+            \"ImageDebugDirectory address of raw data fixed up from: 0x{:X} to 0x{:X}\",\n+            dd.image_debug_directory.address_of_raw_data,\n+            dd.image_debug_directory\n+                .address_of_raw_data\n+                .wrapping_sub(rva_offset),\n+        );\n+        dd.image_debug_directory.address_of_raw_data = dd\n+            .image_debug_directory\n+            .address_of_raw_data\n+            .wrapping_sub(rva_offset);\n+\n+        debug!(\n+            \"ImageDebugDirectory pointer to raw data fixed up from: 0x{:X} to 0x{:X}\",\n+            dd.image_debug_directory.pointer_to_raw_data,\n+            dd.image_debug_directory\n+                .pointer_to_raw_data\n+                .wrapping_sub(rva_offset),\n+        );\n+        dd.image_debug_directory.pointer_to_raw_data = dd\n+            .image_debug_directory\n+            .pointer_to_raw_data\n+            .wrapping_sub(rva_offset);\n+    }\n+}\n+\n /// An analyzed COFF object\n #[derive(Debug)]\n pub struct Coff<'a> {\n", "test_patch": "diff --git a/tests/bins/te/README.md b/tests/bins/te/README.md\nnew file mode 100644\nindex 000000000..c60f26333\n--- /dev/null\n+++ b/tests/bins/te/README.md\n@@ -0,0 +1,24 @@\n+# TE binaries\r\n+\r\n+Binaries located in this directory are precompiled PE32/PE32+ binaries using a\r\n+terse executable (TE) header as defined in the Platform Initialization (PI)\r\n+specification: [TE](https://uefi.org/specs/PI/1.8/V1_TE_Image.html#te-header).\r\n+These binaries were compiled using the\r\n+[EDK2](https://github.com/tianocore/edk2) build system.\r\n+\r\n+## test_image.te\r\n+\r\n+This binary is a simple Terse executable binary\r\n+\r\n+## test_image_loaded.bin\r\n+\r\n+This binary is the same as `test_image.te`, but it has been loaded by a loader,\r\n+meaning the sections have been placed in the expected address. Please note that\r\n+this particular binary has not been relocated, so no relocations have been\r\n+applied\r\n+\r\n+## test_image_relocated.bin\r\n+\r\n+This binary is the same as `test_image.te`, but it has been loaded by a loader,\r\n+meaning the sections have been placed in the expected address, and any any\r\n+relocations have been applied.\r\ndiff --git a/tests/bins/te/test_image.te b/tests/bins/te/test_image.te\nnew file mode 100644\nindex 000000000..8ac7c77b1\nBinary files /dev/null and b/tests/bins/te/test_image.te differ\ndiff --git a/tests/bins/te/test_image_loaded.bin b/tests/bins/te/test_image_loaded.bin\nnew file mode 100644\nindex 000000000..0463f7bf5\nBinary files /dev/null and b/tests/bins/te/test_image_loaded.bin differ\ndiff --git a/tests/bins/te/test_image_relocated.bin b/tests/bins/te/test_image_relocated.bin\nnew file mode 100644\nindex 000000000..123f95d1c\nBinary files /dev/null and b/tests/bins/te/test_image_relocated.bin differ\ndiff --git a/tests/te.rs b/tests/te.rs\nnew file mode 100644\nindex 000000000..347131f26\n--- /dev/null\n+++ b/tests/te.rs\n@@ -0,0 +1,109 @@\n+#[cfg(test)]\r\n+mod te_tests {\r\n+    use goblin::pe;\r\n+    use goblin::pe::header::machine_to_str;\r\n+    use goblin::pe::section_table::*;\r\n+\r\n+    // https://learn.microsoft.com/en-us/windows/win32/debug/pe-format#windows-subsystem\r\n+    const IMAGE_SUBSYSTEM_EFI_BOOT_SERVICE_DRIVER: u8 = 11;\r\n+\r\n+    #[test]\r\n+    fn parse_unloaded_te() {\r\n+        let image = include_bytes!(\"bins/te/test_image.te\");\r\n+        let te = pe::TE::parse(image).expect(\"Failed to parse TE\");\r\n+\r\n+        assert_eq!(machine_to_str(te.header.machine), \"X86_64\");\r\n+        assert_eq!(te.header.number_of_sections, 5);\r\n+        assert_eq!(te.header.subsystem, IMAGE_SUBSYSTEM_EFI_BOOT_SERVICE_DRIVER);\r\n+\r\n+        // Pre-determined field values to be correct for this specific binary\r\n+        assert_eq!(te.header.stripped_size, 0x1c8);\r\n+        assert_eq!(te.header.entry_point, 0x10a8);\r\n+        assert_eq!(te.header.base_of_code, 0x0e60);\r\n+        assert_eq!(te.header.image_base, 0x0);\r\n+        assert_eq!(te.header.reloc_dir.virtual_address, 0x6e58);\r\n+        assert_eq!(te.header.reloc_dir.size, 0x0);\r\n+        assert_eq!(te.header.debug_dir.virtual_address, 0x3a64);\r\n+        assert_eq!(te.header.debug_dir.size, 0x54);\r\n+\r\n+        // Verify section information is correct - with pre-determined values\r\n+        // known to be correct. For brevity sake, check first and last entries.\r\n+        assert_eq!(String::from_utf8_lossy(&te.sections[0].name), \".text\\0\\0\\0\");\r\n+        assert_eq!(te.sections[0].virtual_address, 0xe60);\r\n+        assert_eq!(te.sections[0].virtual_size, 0x17db);\r\n+        assert_eq!(te.sections[0].pointer_to_linenumbers, 0);\r\n+        assert_eq!(te.sections[0].pointer_to_raw_data, 0xe60);\r\n+        assert_eq!(te.sections[0].pointer_to_relocations, 0);\r\n+        assert_eq!(\r\n+            te.sections[0].characteristics,\r\n+            IMAGE_SCN_MEM_EXECUTE\r\n+                | IMAGE_SCN_MEM_READ\r\n+                | IMAGE_SCN_MEM_NOT_PAGED\r\n+                | IMAGE_SCN_CNT_CODE\r\n+        );\r\n+\r\n+        assert_eq!(String::from_utf8_lossy(&te.sections[4].name), \".xdata\\0\\0\");\r\n+        assert_eq!(te.sections[4].virtual_address, 0x5e60);\r\n+        assert_eq!(te.sections[4].virtual_size, 0x98);\r\n+        assert_eq!(\r\n+            te.sections[4].characteristics,\r\n+            IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_DISCARDABLE | IMAGE_SCN_CNT_INITIALIZED_DATA\r\n+        );\r\n+        assert_eq!(te.sections[4].pointer_to_linenumbers, 0);\r\n+        assert_eq!(te.sections[4].pointer_to_raw_data, 0x5e60);\r\n+        assert_eq!(te.sections[4].pointer_to_relocations, 0);\r\n+\r\n+        // Verify the debug directory is correct\r\n+        assert_eq!(te.debug_data.image_debug_directory.size_of_data, 0xab);\r\n+        assert_eq!(\r\n+            te.debug_data.image_debug_directory.address_of_raw_data,\r\n+            0x3b54\r\n+        );\r\n+        assert_eq!(\r\n+            te.debug_data.image_debug_directory.pointer_to_raw_data,\r\n+            0x3b54\r\n+        );\r\n+        let debug_info = te.debug_data.codeview_pdb70_debug_info.unwrap();\r\n+        assert_eq!(\r\n+            debug_info.signature,\r\n+            [\r\n+                0x70, 0xfb, 0xb5, 0x4b, 0xcf, 0x68, 0x15, 0x42, 0xa1, 0x2b, 0xa5, 0xc5, 0x51, 0x95,\r\n+                0x0a, 0x4a\r\n+            ]\r\n+        );\r\n+        assert_eq!(String::from_utf8_lossy(debug_info.filename), String::from(\"c:\\\\src\\\\mu_tiano_platforms\\\\Build\\\\QemuQ35Pkg\\\\DEBUG_VS2022\\\\X64\\\\QemuQ35Pkg\\\\RustTerseImageTestDxe\\\\RustTerseImageTestDxe\\\\DEBUG\\\\RustTerseImageTestDxe.pdb\\0\"));\r\n+\r\n+        // Misc matches\r\n+        assert_eq!(te.header.base_of_code, te.sections[0].virtual_address);\r\n+    }\r\n+\r\n+    /// Verify that parsing of a loaded TE image works.\r\n+    #[test]\r\n+    fn parse_loaded_te() {\r\n+        let image = include_bytes!(\"bins/te/test_image.te\");\r\n+        let te = pe::TE::parse(image).expect(\"Failed to parse TE\");\r\n+\r\n+        let loaded_image = include_bytes!(\"bins/te/test_image_loaded.bin\");\r\n+        let te_loaded = pe::TE::parse(loaded_image).expect(\"Failed to parse TE\");\r\n+\r\n+        assert_eq!(te.header, te_loaded.header);\r\n+        assert_eq!(te.sections, te_loaded.sections);\r\n+        assert_eq!(te.debug_data, te_loaded.debug_data);\r\n+    }\r\n+\r\n+    /// Verify that parsing of a relocated TE image works. Raw data should be different due to\r\n+    /// the relocations being applied, but that is outside the scope of goblin.\r\n+    #[test]\r\n+    fn parse_relocated_te() {\r\n+        let loaded_image = include_bytes!(\"bins/te/test_image_loaded.bin\");\r\n+        let te_loaded = pe::TE::parse(loaded_image).expect(\"Failed to parse TE\");\r\n+\r\n+        let relocated_image = include_bytes!(\"bins/te/test_image_relocated.bin\");\r\n+        let te_relocated = pe::TE::parse(relocated_image).expect(\"Failed to parse TE\");\r\n+\r\n+        // Only the image base should be different in the section headers.\r\n+        assert_ne!(te_loaded.header.image_base, te_relocated.header.image_base);\r\n+        assert_eq!(te_loaded.sections, te_relocated.sections);\r\n+        assert_eq!(te_loaded.debug_data, te_relocated.debug_data);\r\n+    }\r\n+}\r\n", "problem_statement": "Add Terse Executable (TE) support to goblin\nI would like to request the addition of Terse Executable (TE) support to goblin's PE module. a terse executable is simply a PE/PE32+ binary with the header replaced with a terse executable header (as defined in the PI specification: [TE}(https://uefi.org/specs/PI/1.8/V1_TE_Image.html). The TE header is a subset of values from the PE/PE32+ header and are the only fields necessary for the binary to be properly executed by a PI architecture compliant loader and executor. Terse executables are most commonly used by UEFI compliant firmware to reduce the overall size of the binary in firmware.\r\n\r\nIf willing to add this support, I have  created #397 for you to review and provide feedback on. I am open to changing implementation (such as having it be it's own module rather than a subset of the PE module (like `PeCoff`). As the maintainers, I look to you for details such as these.\r\n\r\nThanks for your time :)\r\n\r\n\n", "hints_text": "", "created_at": "2024-03-14 16:41:07", "merge_commit_sha": "47ee850af2beaf81ed5d99b5546de75db8e33b75", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test (win64)', '.github/workflows/main.yml']", "['Test (win32)', '.github/workflows/main.yml']"], ["['Test (macos)', '.github/workflows/main.yml']", "['Test (MSRV)', '.github/workflows/main.yml']"]]}
{"repo": "dusk-network/rusk", "instance_id": "dusk-network__rusk-3232", "base_commit": "9c0c8b02bb24056b553da8f9ba43a94d1d7002fe", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex 3118dcbfd4..3b99844675 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -17,6 +17,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Fixed\n \n+- Fix wrong error shown in the login screen [#3226] [#3097]\n+\n ## [0.10.1] - 2024-12-18\n \n ### Fixed\n@@ -452,6 +454,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3073]: https://github.com/dusk-network/rusk/issues/3073\n [#3076]: https://github.com/dusk-network/rusk/issues/3076\n [#3081]: https://github.com/dusk-network/rusk/issues/3081\n+[#3097]: https://github.com/dusk-network/rusk/issues/3097\n [#3098]: https://github.com/dusk-network/rusk/issues/3098\n [#3099]: https://github.com/dusk-network/rusk/issues/3099\n [#3113]: https://github.com/dusk-network/rusk/issues/3113\n@@ -462,6 +465,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3179]: https://github.com/dusk-network/rusk/issues/3179\n [#3203]: https://github.com/dusk-network/rusk/issues/3203\n [#3212]: https://github.com/dusk-network/rusk/issues/3212\n+[#3226]: https://github.com/dusk-network/rusk/issues/3226\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/lib/errors/index.js b/web-wallet/src/lib/errors/index.js\nnew file mode 100644\nindex 0000000000..3659355455\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/index.js\n@@ -0,0 +1,3 @@\n+export { default as InvalidMnemonicError } from \"./invalid-mnemonic-error\";\n+export { default as InvalidPasswordError } from \"./invalid-password-error\";\n+export { default as MismatchedWalletError } from \"./mismatched-wallet-error\";\ndiff --git a/web-wallet/src/lib/errors/invalid-mnemonic-error.js b/web-wallet/src/lib/errors/invalid-mnemonic-error.js\nnew file mode 100644\nindex 0000000000..e9c3fef503\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/invalid-mnemonic-error.js\n@@ -0,0 +1,8 @@\n+class InvalidMnemonicError extends Error {\n+  constructor() {\n+    super(\"Invalid mnemonic\");\n+    this.name = \"InvalidMnemonicError\";\n+  }\n+}\n+\n+export default InvalidMnemonicError;\ndiff --git a/web-wallet/src/lib/errors/invalid-password-error.js b/web-wallet/src/lib/errors/invalid-password-error.js\nnew file mode 100644\nindex 0000000000..f6683b066d\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/invalid-password-error.js\n@@ -0,0 +1,8 @@\n+class InvalidPasswordError extends Error {\n+  constructor() {\n+    super(\"Wrong password\");\n+    this.name = \"InvalidPasswordError\";\n+  }\n+}\n+\n+export default InvalidPasswordError;\ndiff --git a/web-wallet/src/lib/errors/mismatched-wallet-error.js b/web-wallet/src/lib/errors/mismatched-wallet-error.js\nnew file mode 100644\nindex 0000000000..23d43510b1\n--- /dev/null\n+++ b/web-wallet/src/lib/errors/mismatched-wallet-error.js\n@@ -0,0 +1,8 @@\n+class MismatchedWalletError extends Error {\n+  constructor() {\n+    super(\"Mismatched wallet address or no existing wallet\");\n+    this.name = \"MismatchedWalletError\";\n+  }\n+}\n+\n+export default MismatchedWalletError;\ndiff --git a/web-wallet/src/routes/(welcome)/login/+page.svelte b/web-wallet/src/routes/(welcome)/login/+page.svelte\nindex 23085dd43f..c592b0fdfb 100644\n--- a/web-wallet/src/routes/(welcome)/login/+page.svelte\n+++ b/web-wallet/src/routes/(welcome)/login/+page.svelte\n@@ -4,7 +4,14 @@\n   import { mdiArrowLeft, mdiKeyOutline } from \"@mdi/js\";\n   import { validateMnemonic } from \"bip39\";\n \n+  import { getErrorFrom } from \"$lib/dusk/error\";\n   import { Button, Textbox } from \"$lib/dusk/components\";\n+  import {\n+    InvalidMnemonicError,\n+    InvalidPasswordError,\n+    MismatchedWalletError,\n+  } from \"$lib/errors\";\n+\n   import { AppAnchor, AppAnchorButton, Banner } from \"$lib/components\";\n   import { IconHeadingCard } from \"$lib/containers/Cards\";\n   import { goto } from \"$lib/navigation\";\n@@ -20,9 +27,6 @@\n   } from \"$lib/wallet\";\n   import loginInfoStorage from \"$lib/services/loginInfoStorage\";\n \n-  const localDataCheckErrorMsg =\n-    \"Mismatched wallet address or no existing wallet\";\n-\n   /** @type {(seed: Uint8Array) => Promise<import(\"$lib/vendor/w3sper.js/src/mod\").ProfileGenerator>} */\n   async function checkLocalData(seed) {\n     const profileGenerator = profileGeneratorFrom(seed);\n@@ -30,7 +34,7 @@\n     const currentAddress = $settingsStore.userId;\n \n     if (!currentAddress || currentAddress !== defaultAddress) {\n-      throw new Error(localDataCheckErrorMsg);\n+      throw new MismatchedWalletError();\n     }\n \n     return profileGenerator;\n@@ -40,12 +44,12 @@\n   const getSeedFromMnemonicAsync = async (mnemonic) =>\n     validateMnemonic(mnemonic)\n       ? getSeedFromMnemonic(mnemonic)\n-      : Promise.reject(new Error(\"Invalid mnemonic\"));\n+      : Promise.reject(new InvalidMnemonicError());\n \n   /** @type {(loginInfo: MnemonicEncryptInfo) => (pwd: string) => Promise<Uint8Array>} */\n   const getSeedFromInfo = (loginInfo) => (pwd) =>\n     decryptMnemonic(loginInfo, pwd).then(getSeedFromMnemonic, () =>\n-      Promise.reject(new Error(\"Wrong password\"))\n+      Promise.reject(new InvalidPasswordError())\n     );\n \n   const loginInfo = loginInfoStorage.get();\n@@ -57,8 +61,8 @@\n   /** @type {string} */\n   let secretText = \"\";\n \n-  /** @type {null|\"invalid-password\"|\"invalid-mnemonic\"} */\n-  let error = null;\n+  /** @type {Error} */\n+  let error;\n \n   /** @type {import(\"svelte/elements\").FormEventHandler<HTMLFormElement>} */\n   function handleUnlockWalletSubmit() {\n@@ -72,16 +76,16 @@\n       .then((profileGenerator) => walletStore.init(profileGenerator))\n       .then(() => goto(\"/dashboard\"))\n       .catch((err) => {\n-        if (err.message === localDataCheckErrorMsg) {\n+        if (err instanceof MismatchedWalletError) {\n           const enteredMnemonicPhrase = secretText.split(\" \");\n           mnemonicPhraseResetStore.set(enteredMnemonicPhrase);\n           goto(\"/setup/restore\");\n+\n           return;\n+        } else {\n+          error = err instanceof Error ? err : getErrorFrom(err);\n         }\n-        error =\n-          err.message === \"Wrong password\"\n-            ? \"invalid-password\"\n-            : \"invalid-mnemonic\";\n+\n         fldSecret.focus();\n         fldSecret.select();\n       });\n@@ -113,15 +117,14 @@\n           type=\"password\"\n           autocomplete=\"current-password\"\n         />\n-        {#if error === \"invalid-mnemonic\"}\n+        {#if error instanceof InvalidMnemonicError}\n           <Banner title=\"Invalid mnemonic phrase\" variant=\"error\">\n             <p>\n               Please ensure you have entered your 12-word mnemonic phrase, with\n               a space separating each word.\n             </p>\n           </Banner>\n-        {/if}\n-        {#if error === \"invalid-password\"}\n+        {:else if error instanceof InvalidPasswordError}\n           <Banner title=\"Invalid password\" variant=\"error\">\n             <p>\n               Please ensure the password entered matches the one you have set up\n@@ -129,6 +132,13 @@\n               you can <AppAnchor href=\"/setup/restore\">restore</AppAnchor> your wallet.\n             </p>\n           </Banner>\n+        {:else if error}\n+          <Banner\n+            title={error.name.replace(/(\\w)([A-Z])/g, \"$1 $2\")}\n+            variant=\"error\"\n+          >\n+            <p>{error.message}</p>\n+          </Banner>\n         {/if}\n         <Button text=\"Unlock Wallet\" type=\"submit\" />\n         {#if modeLabel === \"Password\"}\n", "test_patch": "diff --git a/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap b/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\nindex 836f724ab0..f06439819a 100644\n--- a/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\n+++ b/web-wallet/src/routes/(welcome)/login/__tests__/__snapshots__/page.spec.js.snap\n@@ -77,7 +77,6 @@ exports[`Login > Mnemonic phrase workflow > should render the login page and sho\n           \n            \n            \n-           \n           <button\n             class=\"dusk-button dusk-button--type--submit dusk-button--variant--primary dusk-button--size--default\"\n             type=\"submit\"\n@@ -204,7 +203,6 @@ exports[`Login > Password workflow > should show the password field and the link\n           \n            \n            \n-           \n           <button\n             class=\"dusk-button dusk-button--type--submit dusk-button--variant--primary dusk-button--size--default\"\n             type=\"submit\"\n", "problem_statement": "web-wallet: Misleading Error Message When Unlocking Wallet Without Internet Connection\n### Description\r\nCurrently, when a user attempts to unlock their Wallet without an internet connection, they are presented with an **\"invalid mnemonic\" error**. This is misleading and may cause users to believe there is an issue with the mnemonic they entered, when the actual problem is the lack of internet connectivity.\r\n\r\nAs the Web Wallet uses a service worker, it is possible for the application to remain loaded in the browser even when there is no active internet connection. While this behavior is handled correctly in some areas (e.g., sync failures are displayed with appropriate status messages on the dashboard), it is not yet addressed in the pre-unlocked views.\r\n\r\n**Affected Areas**\r\n- Unlock screen\r\n- Create wallet flow\r\n- Restore wallet flow\r\n\r\n### Expected Behavior\r\nThe application should properly handle scenarios where the user's internet connection is unavailable:\r\n- Detect lack of internet connectivity.\r\n- Display a clear and specific error message to inform the user about the connectivity issue.\r\n- Prevent misleading error messages like \"invalid mnemonic.\"\r\n\r\n### Steps to Reproduce\r\n- Load the Web Wallet while online.\r\n- Disconnect from the internet.\r\n- Attempt to unlock the wallet with a valid mnemonic.\r\n\r\n### Actual Behavior\r\nThe user sees an **\"invalid mnemonic\" error**, despite entering a correct mnemonic.\r\n\r\n### Proposed Solution\r\n- Implement connectivity checks for the pre-unlocked views.\r\n- Update error messaging to accurately reflect the connectivity status when applicable.\r\n\r\n### Additional Context\r\nThis issue is particularly relevant as users may frequently encounter this scenario when working with service worker-based applications.\n", "hints_text": "", "created_at": "2024-12-19 10:47:33", "merge_commit_sha": "379af862d8041b076059a045a3472442e7879702", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['changes', '.github/workflows/ruskwallet_ci.yml']", "['[Windows] Nightly tests', '.github/workflows/ruskwallet_ci.yml']"], ["['Node 20.x', '.github/workflows/explorer_ci.yml']", "['changes', '.github/workflows/w3sperjs_ci.yml']"], ["['[Mac arm64] Nightly checks', '.github/workflows/ruskwallet_ci.yml']", "['Clippy check release', '.github/workflows/rusk_ci.yml']"], ["['Node 20.x', '.github/workflows/webwallet_ci.yml']", "['Dusk Analyzer', '.github/workflows/rusk_ci.yml']"]]}
{"repo": "dusk-network/rusk", "instance_id": "dusk-network__rusk-3441", "base_commit": "2db27ecdd9614605ca2fd83a5a7370a0d0900706", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex bdd54ead73..76783d94b6 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -12,6 +12,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n ### Changed\n \n - Update Transactions list design [#1922]\n+- Auto-focus text field (Unlock page) [#3420]\n \n ### Removed\n \n@@ -563,6 +564,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3362]: https://github.com/dusk-network/rusk/issues/3362\n [#3381]: https://github.com/dusk-network/rusk/issues/3381\n [#3387]: https://github.com/dusk-network/rusk/issues/3387\n+[#3420]: https://github.com/dusk-network/rusk/issues/3420\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/routes/(welcome)/unlock/+page.svelte b/web-wallet/src/routes/(welcome)/unlock/+page.svelte\nindex 2734f30249..57912e4448 100644\n--- a/web-wallet/src/routes/(welcome)/unlock/+page.svelte\n+++ b/web-wallet/src/routes/(welcome)/unlock/+page.svelte\n@@ -111,11 +111,13 @@\n         <Textbox\n           bind:this={fldSecret}\n           bind:value={secretText}\n-          name={loginInfo ? \"password\" : \"mnemonic\"}\n+          name={modeLabel}\n+          aria-label={modeLabel}\n           placeholder={modeLabel}\n           required\n           type=\"password\"\n           autocomplete=\"current-password\"\n+          autofocus\n         />\n         {#if error instanceof InvalidMnemonicError}\n           <Banner title=\"Invalid mnemonic phrase\" variant=\"error\">\n", "test_patch": "diff --git a/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap b/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\nindex ee4d23a961..ac5d1ee29a 100644\n--- a/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\n+++ b/web-wallet/src/routes/(welcome)/unlock/__tests__/__snapshots__/page.spec.js.snap\n@@ -67,9 +67,11 @@ exports[`Unlock Wallet > Mnemonic phrase workflow > should render the Unlock Wal\n           class=\"login__form svelte-1e8gc2s\"\n         >\n           <input\n+            aria-label=\"Mnemonic phrase\"\n             autocomplete=\"current-password\"\n+            autofocus=\"\"\n             class=\"dusk-textbox dusk-textbox-password\"\n-            name=\"mnemonic\"\n+            name=\"Mnemonic phrase\"\n             placeholder=\"Mnemonic phrase\"\n             required=\"\"\n             type=\"password\"\n@@ -193,9 +195,11 @@ exports[`Unlock Wallet > Password workflow > should show the password field and\n           class=\"login__form svelte-1e8gc2s\"\n         >\n           <input\n+            aria-label=\"Password\"\n             autocomplete=\"current-password\"\n+            autofocus=\"\"\n             class=\"dusk-textbox dusk-textbox-password\"\n-            name=\"password\"\n+            name=\"Password\"\n             placeholder=\"Password\"\n             required=\"\"\n             type=\"password\"\n", "problem_statement": "After clicking \"Unlock Wallet,\" the focus should automatically be placed on the password input field\nSummary\n\ud83d\udca1 After clicking \"Unlock Wallet,\" the focus should automatically be placed on the password input field. This small improvement aligns with good UX practices by eliminating an unnecessary click and streamlining the user flow.\n\nPossible solution design or implementation\n\ud83d\udca1 Set focus on the password input field programmatically after the \"Unlock Wallet\" button is clicked. For example:\n\nUse JavaScript to call the .focus() method on the password input field once the modal or relevant section is displayed.\nEnsure this behavior is applied across all views or scenarios where the \"Unlock Wallet\" action is available.\nAlternative: Evaluate if screen readers also benefit from this behavior for accessibility compliance.\n\nAdditional context\n\ud83d\udca1 This is a quick win with minimal implementation complexity but a notable impact on user experience. Removing small friction points like an additional click improves the overall perception and usability of the application.\n\n", "hints_text": "Good point :) Thanks!", "created_at": "2025-01-29 11:55:06", "merge_commit_sha": "539cf2b47363495b74ff860791b62cfcebf46029", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['changes', '.github/workflows/ruskwallet_ci.yml']", "['[Windows] Nightly tests', '.github/workflows/ruskwallet_ci.yml']"], ["['Node 20.x', '.github/workflows/explorer_ci.yml']", "['changes', '.github/workflows/w3sperjs_ci.yml']"], ["['[Mac arm64] Nightly checks', '.github/workflows/ruskwallet_ci.yml']", "['Node 20.x', '.github/workflows/webwallet_ci.yml']"], ["['Clippy check release', '.github/workflows/rusk_ci.yml']", "['changes', '.github/workflows/webwallet_ci.yml']"]]}
{"repo": "databendlabs/databend", "instance_id": "databendlabs__databend-17076", "base_commit": "27eeef86ecd1a6f78434b9a3aacc386692c21ed3", "patch": "diff --git a/src/query/ast/src/ast/statements/table.rs b/src/query/ast/src/ast/statements/table.rs\nindex 7efc87fa6b8fa..34a89d4a83d09 100644\n--- a/src/query/ast/src/ast/statements/table.rs\n+++ b/src/query/ast/src/ast/statements/table.rs\n@@ -745,6 +745,21 @@ impl Display for Engine {\n     }\n }\n \n+impl From<&str> for Engine {\n+    fn from(s: &str) -> Self {\n+        match s.to_lowercase().as_str() {\n+            \"null\" => Engine::Null,\n+            \"memory\" => Engine::Memory,\n+            \"fuse\" => Engine::Fuse,\n+            \"view\" => Engine::View,\n+            \"random\" => Engine::Random,\n+            \"iceberg\" => Engine::Iceberg,\n+            \"delta\" => Engine::Delta,\n+            _ => unreachable!(\"invalid engine: {}\", s),\n+        }\n+    }\n+}\n+\n #[derive(Debug, Clone, PartialEq, Eq, Drive, DriveMut)]\n pub enum CompactTarget {\n     Block,\ndiff --git a/src/query/service/src/interpreters/common/table_option_validation.rs b/src/query/service/src/interpreters/common/table_option_validation.rs\nindex a36a217ac8b0e..b16132635d8c0 100644\n--- a/src/query/service/src/interpreters/common/table_option_validation.rs\n+++ b/src/query/service/src/interpreters/common/table_option_validation.rs\n@@ -17,6 +17,7 @@ use std::collections::HashSet;\n use std::sync::LazyLock;\n \n use chrono::Duration;\n+use databend_common_ast::ast::Engine;\n use databend_common_exception::ErrorCode;\n use databend_common_expression::TableSchemaRef;\n use databend_common_io::constants::DEFAULT_BLOCK_MAX_ROWS;\n@@ -45,7 +46,7 @@ use databend_storages_common_table_meta::table::OPT_KEY_TEMP_PREFIX;\n use log::error;\n \n /// Table option keys that can occur in 'create table statement'.\n-pub static CREATE_TABLE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+pub static CREATE_FUSE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n     let mut r = HashSet::new();\n     r.insert(FUSE_OPT_KEY_ROW_PER_PAGE);\n     r.insert(FUSE_OPT_KEY_BLOCK_PER_SEGMENT);\n@@ -64,12 +65,32 @@ pub static CREATE_TABLE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new\n \n     r.insert(OPT_KEY_ENGINE);\n \n+    r.insert(OPT_KEY_CONNECTION_NAME);\n+\n+    r.insert(\"transient\");\n+    r.insert(OPT_KEY_TEMP_PREFIX);\n+    r\n+});\n+\n+pub static CREATE_LAKE_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n     r.insert(OPT_KEY_LOCATION);\n     r.insert(OPT_KEY_CONNECTION_NAME);\n+    r\n+});\n \n+pub static CREATE_RANDOM_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n     r.insert(OPT_KEY_RANDOM_SEED);\n+    r\n+});\n \n-    r.insert(\"transient\");\n+pub static CREATE_MEMORY_OPTIONS: LazyLock<HashSet<&'static str>> = LazyLock::new(|| {\n+    let mut r = HashSet::new();\n+    r.insert(OPT_KEY_ENGINE);\n+    r.insert(OPT_KEY_DATABASE_ID);\n     r.insert(OPT_KEY_TEMP_PREFIX);\n     r\n });\n@@ -85,8 +106,16 @@ pub static UNSET_TABLE_OPTIONS_WHITE_LIST: LazyLock<HashSet<&'static str>> = Laz\n     r\n });\n \n-pub fn is_valid_create_opt<S: AsRef<str>>(opt_key: S) -> bool {\n-    CREATE_TABLE_OPTIONS.contains(opt_key.as_ref().to_lowercase().as_str())\n+pub fn is_valid_create_opt<S: AsRef<str>>(opt_key: S, engine: &Engine) -> bool {\n+    let opt_key = opt_key.as_ref().to_lowercase();\n+    let opt_key = opt_key.as_str();\n+    match engine {\n+        Engine::Fuse => CREATE_FUSE_OPTIONS.contains(opt_key),\n+        Engine::Iceberg | Engine::Delta => CREATE_LAKE_OPTIONS.contains(&opt_key),\n+        Engine::Random => CREATE_RANDOM_OPTIONS.contains(&opt_key),\n+        Engine::Memory => CREATE_MEMORY_OPTIONS.contains(&opt_key),\n+        Engine::Null | Engine::View => opt_key == OPT_KEY_ENGINE,\n+    }\n }\n \n pub fn is_valid_block_per_segment(\ndiff --git a/src/query/service/src/interpreters/interpreter_table_create.rs b/src/query/service/src/interpreters/interpreter_table_create.rs\nindex f15937f892ccd..173d891b5d0cc 100644\n--- a/src/query/service/src/interpreters/interpreter_table_create.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_create.rs\n@@ -402,7 +402,7 @@ impl CreateTableInterpreter {\n \n         for table_option in table_meta.options.iter() {\n             let key = table_option.0.to_lowercase();\n-            if !is_valid_create_opt(&key) {\n+            if !is_valid_create_opt(&key, &self.plan.engine) {\n                 error!(\"invalid opt for fuse table in create table statement\");\n                 return Err(ErrorCode::TableOptionInvalid(format!(\n                     \"table option {key} is invalid for create table statement\",\ndiff --git a/src/query/service/src/interpreters/interpreter_table_set_options.rs b/src/query/service/src/interpreters/interpreter_table_set_options.rs\nindex 9aa7e26449fc4..2147a607ae162 100644\n--- a/src/query/service/src/interpreters/interpreter_table_set_options.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_set_options.rs\n@@ -15,6 +15,7 @@\n use std::collections::HashMap;\n use std::sync::Arc;\n \n+use databend_common_ast::ast::Engine;\n use databend_common_catalog::table::TableExt;\n use databend_common_exception::ErrorCode;\n use databend_common_exception::Result;\n@@ -101,9 +102,17 @@ impl Interpreter for SetOptionsInterpreter {\n                 OPT_KEY_CLUSTER_TYPE\n             )));\n         }\n+        let catalog = self.ctx.get_catalog(self.plan.catalog.as_str()).await?;\n+        let database = self.plan.database.as_str();\n+        let table_name = self.plan.table.as_str();\n+        let table = catalog\n+            .get_table(&self.ctx.get_tenant(), database, table_name)\n+            .await?;\n+\n         for table_option in self.plan.set_options.iter() {\n             let key = table_option.0.to_lowercase();\n-            if !is_valid_create_opt(&key) {\n+            let engine = Engine::from(table.engine());\n+            if !is_valid_create_opt(&key, &engine) {\n                 error!(\"{}\", &error_str);\n                 return Err(ErrorCode::TableOptionInvalid(format!(\n                     \"table option {key} is invalid for alter table statement\",\n@@ -111,12 +120,6 @@ impl Interpreter for SetOptionsInterpreter {\n             }\n             options_map.insert(key, Some(table_option.1.clone()));\n         }\n-        let catalog = self.ctx.get_catalog(self.plan.catalog.as_str()).await?;\n-        let database = self.plan.database.as_str();\n-        let table_name = self.plan.table.as_str();\n-        let table = catalog\n-            .get_table(&self.ctx.get_tenant(), database, table_name)\n-            .await?;\n \n         let table_version = table.get_table_info().ident.seq;\n         if let Some(value) = self.plan.set_options.get(OPT_KEY_CHANGE_TRACKING) {\ndiff --git a/src/query/service/src/interpreters/interpreter_table_show_create.rs b/src/query/service/src/interpreters/interpreter_table_show_create.rs\nindex 8021e1d6f2ac9..1b69d3ef4f815 100644\n--- a/src/query/service/src/interpreters/interpreter_table_show_create.rs\n+++ b/src/query/service/src/interpreters/interpreter_table_show_create.rs\n@@ -281,7 +281,7 @@ impl ShowCreateTableInterpreter {\n \n         if engine != \"ICEBERG\" && engine != \"DELTA\" {\n             if let Some(sp) = &table_info.meta.storage_params {\n-                table_create_sql.push_str(format!(\" LOCATION = '{}'\", sp).as_str());\n+                table_create_sql.push_str(format!(\" '{}' \", sp).as_str());\n             }\n         }\n \n", "test_patch": "diff --git a/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test b/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\nindex 01e23ba86fae3..fa0b044115b69 100644\n--- a/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\n+++ b/tests/sqllogictests/suites/base/05_ddl/05_0000_ddl_create_tables.test\n@@ -303,6 +303,15 @@ create table t(a int) x=x\n statement error 1301\n create table t(a int) external_location='xxx'\n \n+statement error 1301\n+create table t(a int) location='xxx'\n+\n+statement error 1301\n+create table t(a int) seed='123'\n+\n+statement ok\n+create table t_random(a int) engine=RANDOM seed='123'\n+\n statement error 1301\n create table t(a int) snapshot_loc='xxx'\n \ndiff --git a/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test b/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\nindex 2020b3db6d849..558d5f33b593b 100644\n--- a/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\n+++ b/tests/sqllogictests/suites/base/06_show/06_0001_show_create_table.test\n@@ -53,7 +53,7 @@ CREATE TABLE test.d (a int not null) ENGINE=FUSE 'fs:///tmp/load/files/' CONNECT\n query TT\n SHOW CREATE TABLE `test`.`d`\n ----\n-d CREATE TABLE d ( a INT NOT NULL ) ENGINE=FUSE CLUSTER BY linear(a, a % 3) CLUSTER_TYPE='linear' COMPRESSION='lz4' STORAGE_FORMAT='parquet' LOCATION = 'fs | root=/tmp/load/files/'\n+d CREATE TABLE d ( a INT NOT NULL ) ENGINE=FUSE CLUSTER BY linear(a, a % 3) CLUSTER_TYPE='linear' COMPRESSION='lz4' STORAGE_FORMAT='parquet' 'fs | root=/tmp/load/files/'\n \n query TT\n SHOW CREATE TABLE `test`.`c`\n", "problem_statement": "bug:  `LOCATION` of `CREATE TABLE` does NOT work as expected \n**Summary**\r\n\r\n\r\nNot sure if this is a documentation bug or a implementation bug:\r\n\r\nwhen creating a table with external location, following the official doc:\r\n\r\nhttps://docs.databend.com/sql/sql-commands/ddl/table/ddl-create-table-external-location#syntax\r\n\r\n> CREATE TABLE [IF NOT EXISTS] [db.]table_name (\r\n>     <column_name> <data_type> [NOT NULL | NULL] [{ DEFAULT <expr> }],\r\n>     <column_name> <data_type> [NOT NULL | NULL] [{ DEFAULT <expr> }],\r\n>     ...\r\n> )\r\n> LOCATION = 's3://<bucket>/[<path>]'\r\n> CONNECTION = (\r\n>     ENDPOINT_URL = 'https://<endpoint-URL>'\r\n>     ACCESS_KEY_ID = '<your-access-key-ID>'\r\n>     SECRET_ACCESS_KEY = '<your-secret-access-key>'\r\n>     REGION = '<region-name>'\r\n>     ENABLE_VIRTUAL_HOST_STYLE = 'true' | 'false'\r\n> )\r\n> |\r\n> CONNECTION = (\r\n>     CONNECTION_NAME = '<your-connection-name>'\r\n> );\r\n\r\nA table of invalid location could be created successfully, mutation and query are all work,\r\n\r\n***But the table data is NOT stored at the specified location***.\r\n\r\n\r\nAlso: In section `Create a Table with External Location`\r\nhttps://docs.databend.com/sql/sql-commands/ddl/table/ddl-create-table-external-location#create-a-table-with-external-location\r\n\r\nA different syntax is used:\r\n\r\n~~~\r\n-- Create a table named `mytable` and specify the location `s3://testbucket/admin/data/` for the data storage\r\nCREATE TABLE mytable (\r\n  a INT\r\n)\r\n's3://testbucket/admin/data/'\r\nCONNECTION = (\r\n  ACCESS_KEY_ID = '<your_aws_key_id>',\r\n  SECRET_ACCESS_KEY = '<your_aws_secret_key>',\r\n  ENDPOINT_URL = 'https://s3.amazonaws.com'\r\n);\r\n~~~\r\n\r\n\r\n\r\n\r\n**Reporduce:**\r\n\r\nfor example, a table of with invalid s3 location can be created (incorrectly):\r\n\r\n~~~\r\n\r\nmysql> select version();\r\n+----------------------------------------------------------------------------------------+\r\n| version()                                                                              |\r\n+----------------------------------------------------------------------------------------+\r\n| 8.0.26-v1.2.673-nightly-f777d195be(rust-1.85.0-nightly-2024-12-18T05:01:23.526505965Z) |\r\n+----------------------------------------------------------------------------------------+\r\n1 row in set (0.01 sec)\r\nRead 1 rows, 1.00 B in 0.007 sec., 153.02 rows/sec., 153.02 B/sec.\r\n\r\nmysql> select * from system.configs where \"group\" = 'storage' and name = 'type';\r\n+---------+------+-------+-------------+\r\n| group   | name | value | description |\r\n+---------+------+-------+-------------+\r\n| storage | type | fs    |             |\r\n+---------+------+-------+-------------+\r\n1 row in set (0.03 sec)\r\nRead 184 rows, 12.05 KiB in 0.008 sec., 21.89 thousand rows/sec., 1.40 MiB/sec.\r\n\r\n\r\nmysql> create table t_location (c int) LOCATION = 's3 | bucket=non-exist-bucket,root=/,endpoint=https://s3.amazonaws.com';\r\nQuery OK, 0 rows affected (0.05 sec)\r\n\r\nmysql> insert into t_location values(1);\r\n+-------------------------+\r\n| number of rows inserted |\r\n+-------------------------+\r\n|                       1 |\r\n+-------------------------+\r\n1 row in set (0.21 sec)\r\nRead 1 rows, 5.00 B in 0.144 sec., 6.96 rows/sec., 34.81 B/sec.\r\n\r\n\r\nmysql> select * from t_location;\r\n+------+\r\n| c    |\r\n+------+\r\n|    1 |\r\n+------+\r\n1 row in set (0.11 sec)\r\nRead 1 rows, 5.00 B in 0.025 sec., 40.12 rows/sec., 200.61 B/sec.\r\n\r\n~~~\n", "hints_text": "", "created_at": "2024-12-18 14:24:43", "merge_commit_sha": "19012a581ec0283d9a2d253199424758672d4828", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['management_mode', '.github/workflows/dev.yml']", "['cluster (query, 4c16g, mysql)', '.github/workflows/dev.yml']"], ["['check', '.github/workflows/dev.yml']", "['ee (native)', '.github/workflows/dev.yml']"], ["['description', '.github/workflows/pr.yml']", "['cluster (tpch, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['test_compat_fuse', '.github/workflows/dev.yml']", "['cluster (crdb, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_metactl', '.github/workflows/dev.yml']", "['standalone_minio (query, mysql, parquet)', '.github/workflows/dev.yml']"], ["['standalone (tpcds, 2c8g, mysql)', '.github/workflows/dev.yml']", "['standalone (standalone, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['cluster (cluster, 2c8g, mysql)', '.github/workflows/dev.yml']", "['cluster (cluster, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_stateful_iceberg_rest', '.github/workflows/dev.yml']", "['standalone_udf_server', '.github/workflows/dev.yml']"], ["['cluster (base, 2c8g, mysql)', '.github/workflows/dev.yml']", "['test_stateless_cluster', '.github/workflows/dev.yml']"], ["['test_stateless_standalone', '.github/workflows/dev.yml']", "['standalone (standalone, 2c8g, http)', '.github/workflows/dev.yml']"], ["['test_ee_standalone_background', '.github/workflows/dev.yml']", "['test_logs', '.github/workflows/dev.yml']"], ["['standalone (query, 4c16g, mysql)', '.github/workflows/dev.yml']", "['cluster (base, 2c8g, http)', '.github/workflows/dev.yml']"], ["['standalone (duckdb, 4c16g, http)', '.github/workflows/dev.yml']", "['cluster (crdb, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['cluster (tpcds, 2c8g, mysql)', '.github/workflows/dev.yml']", "['standalone (tpcds, 2c8g, http)', '.github/workflows/dev.yml']"], ["['standalone (query, 4c16g, http)', '.github/workflows/dev.yml']", "['standalone (tpch, 2c8g, mysql)', '.github/workflows/dev.yml']"], ["['title', '.github/workflows/pr.yml']", "['ee (parquet)', '.github/workflows/dev.yml']"], ["['test_compat_meta_meta', '.github/workflows/dev.yml']", "['test_stateful_hive_standalone', '.github/workflows/dev.yml']"], ["['standalone (tpch, 2c8g, http)', '.github/workflows/dev.yml']", "['standalone_no_table_meta_cache (no_table_meta_cache, http)', '.github/workflows/dev.yml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-723", "base_commit": "56af803e89ce0ff6054b491ca6d8086c95c84bad", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex fbe6c50c..8ca4b601 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -23,6 +23,8 @@ All notable changes to MiniJinja are documented here.\n   rendering from the same environment at once.  #717\n - The Python bindings handle `__bool__` correctly now for custom\n   objects in if-conditions and filters.  #719\n+- Fixed a bug where `}}` caused a syntax error in expressions with\n+  open parentheses, braces or brackets.  #723\n \n ## 2.8.0\n \ndiff --git a/minijinja/src/compiler/lexer.rs b/minijinja/src/compiler/lexer.rs\nindex 268477ab..d7aa89b7 100644\n--- a/minijinja/src/compiler/lexer.rs\n+++ b/minijinja/src/compiler/lexer.rs\n@@ -24,7 +24,6 @@ pub struct Tokenizer<'s> {\n     current_offset: usize,\n     trim_leading_whitespace: bool,\n     pending_start_marker: Option<(StartMarker, usize)>,\n-    #[cfg(feature = \"custom_syntax\")]\n     paren_balance: isize,\n     syntax_config: SyntaxConfig,\n     ws_config: WhitespaceConfig,\n@@ -319,7 +318,6 @@ impl<'s> Tokenizer<'s> {\n             current_line: 1,\n             current_col: 0,\n             current_offset: 0,\n-            #[cfg(feature = \"custom_syntax\")]\n             paren_balance: 0,\n             trim_leading_whitespace: false,\n             pending_start_marker: None,\n@@ -801,50 +799,52 @@ impl<'s> Tokenizer<'s> {\n         }\n \n         // look out for the end of blocks\n-        match sentinel {\n-            BlockSentinel::Block => {\n-                if matches!(rest.get(..1), Some(\"-\" | \"+\"))\n-                    && rest[1..].starts_with(self.block_end())\n-                {\n-                    self.stack.pop();\n-                    let was_minus = &rest[..1] == \"-\";\n-                    self.advance(self.block_end().len() + 1);\n-                    let span = self.span(old_loc);\n-                    if was_minus {\n-                        self.trim_leading_whitespace = true;\n+        if self.paren_balance == 0 {\n+            match sentinel {\n+                BlockSentinel::Block => {\n+                    if matches!(rest.get(..1), Some(\"-\" | \"+\"))\n+                        && rest[1..].starts_with(self.block_end())\n+                    {\n+                        self.stack.pop();\n+                        let was_minus = &rest[..1] == \"-\";\n+                        self.advance(self.block_end().len() + 1);\n+                        let span = self.span(old_loc);\n+                        if was_minus {\n+                            self.trim_leading_whitespace = true;\n+                        }\n+                        return Ok(ControlFlow::Break((Token::BlockEnd, span)));\n                     }\n-                    return Ok(ControlFlow::Break((Token::BlockEnd, span)));\n-                }\n-                if rest.starts_with(self.block_end()) {\n-                    self.stack.pop();\n-                    self.advance(self.block_end().len());\n-                    let span = self.span(old_loc);\n-                    self.skip_newline_if_trim_blocks();\n-                    return Ok(ControlFlow::Break((Token::BlockEnd, span)));\n-                }\n-            }\n-            BlockSentinel::Variable => {\n-                if matches!(rest.get(..1), Some(\"-\" | \"+\"))\n-                    && rest[1..].starts_with(self.variable_end())\n-                {\n-                    self.stack.pop();\n-                    let was_minus = &rest[..1] == \"-\";\n-                    self.advance(self.variable_end().len() + 1);\n-                    let span = self.span(old_loc);\n-                    if was_minus {\n-                        self.trim_leading_whitespace = true;\n+                    if rest.starts_with(self.block_end()) {\n+                        self.stack.pop();\n+                        self.advance(self.block_end().len());\n+                        let span = self.span(old_loc);\n+                        self.skip_newline_if_trim_blocks();\n+                        return Ok(ControlFlow::Break((Token::BlockEnd, span)));\n                     }\n-                    return Ok(ControlFlow::Break((Token::VariableEnd, span)));\n                 }\n-                if rest.starts_with(self.variable_end()) {\n-                    self.stack.pop();\n-                    self.advance(self.variable_end().len());\n-                    return Ok(ControlFlow::Break((Token::VariableEnd, self.span(old_loc))));\n+                BlockSentinel::Variable => {\n+                    if matches!(rest.get(..1), Some(\"-\" | \"+\"))\n+                        && rest[1..].starts_with(self.variable_end())\n+                    {\n+                        self.stack.pop();\n+                        let was_minus = &rest[..1] == \"-\";\n+                        self.advance(self.variable_end().len() + 1);\n+                        let span = self.span(old_loc);\n+                        if was_minus {\n+                            self.trim_leading_whitespace = true;\n+                        }\n+                        return Ok(ControlFlow::Break((Token::VariableEnd, span)));\n+                    }\n+                    if rest.starts_with(self.variable_end()) {\n+                        self.stack.pop();\n+                        self.advance(self.variable_end().len());\n+                        return Ok(ControlFlow::Break((Token::VariableEnd, self.span(old_loc))));\n+                    }\n                 }\n+                // line statements are handled above\n+                #[cfg(feature = \"custom_syntax\")]\n+                BlockSentinel::LineStatement => {}\n             }\n-            // line statements are handled above\n-            #[cfg(feature = \"custom_syntax\")]\n-            BlockSentinel::LineStatement => {}\n         }\n \n         // two character operators\n@@ -864,10 +864,7 @@ impl<'s> Tokenizer<'s> {\n \n         macro_rules! with_paren_balance {\n             ($delta:expr, $tok:expr) => {{\n-                #[cfg(feature = \"custom_syntax\")]\n-                {\n-                    self.paren_balance += $delta;\n-                }\n+                self.paren_balance += $delta;\n                 Some($tok)\n             }};\n         }\n", "test_patch": "diff --git a/minijinja/tests/lexer-inputs/parent-balance.txt b/minijinja/tests/lexer-inputs/parent-balance.txt\nnew file mode 100644\nindex 00000000..900d2be0\n--- /dev/null\n+++ b/minijinja/tests/lexer-inputs/parent-balance.txt\n@@ -0,0 +1,3 @@\n+{}\n+---\n+{{ {\"a\": {\"b\": 1}} }}\n\\ No newline at end of file\ndiff --git a/minijinja/tests/snapshots/test_lexer__lexer@parent-balance.txt.snap b/minijinja/tests/snapshots/test_lexer__lexer@parent-balance.txt.snap\nnew file mode 100644\nindex 00000000..3328d00f\n--- /dev/null\n+++ b/minijinja/tests/snapshots/test_lexer__lexer@parent-balance.txt.snap\n@@ -0,0 +1,27 @@\n+---\n+source: minijinja/tests/test_lexer.rs\n+description: \"{{ {\\\"a\\\": {\\\"b\\\": 1}} }}\"\n+input_file: minijinja/tests/lexer-inputs/parent-balance.txt\n+---\n+VariableStart\n+  \"{{\"\n+BraceOpen\n+  \"{\"\n+Str(\"a\")\n+  \"\\\"a\\\"\"\n+Colon\n+  \":\"\n+BraceOpen\n+  \"{\"\n+Str(\"b\")\n+  \"\\\"b\\\"\"\n+Colon\n+  \":\"\n+Int(1)\n+  \"1\"\n+BraceClose\n+  \"}\"\n+BraceClose\n+  \"}\"\n+VariableEnd\n+  \"}}\"\n", "problem_statement": "Syntax error on a valid expression\n## Description\n\nThe following expression results in a syntax error, despite being a legal Jinja expression (at least according to the official library of Jinja2).\n\n## Reproduction steps\n\n```jinja2\n{{ {'a': {'b': 2}} }}\n```\n\n```\nError: syntax error: unexpected end of variable block, expected `,` (in template.html:1)\n-------------------------------- template.html --------------------------------\n   1 > {{ {'a': {'b': 2}} }}\n     i                 ^^ syntax error\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNo referenced variables\n-------------------------------------------------------------------------------\n```\n\nAdditional helpful information:\n\nA workaround is to put a space between the closing curly braces if the dictionary:\n```jinja2\n{{ {'a': {'b': 2} } }}\n```\n\n- Version of minijinja: 2.7.0\n\n## What did you expect\n\nThe expression is expected to render without issue.\n\n\n\n", "hints_text": "", "created_at": "2025-02-12 17:47:27", "merge_commit_sha": "6610548ff3d02aabc3aee25d1d88c958be6ab2fe", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test on Latest Stable', '.github/workflows/tests.yml']", "['Check on 1.70.0 (fuel feature)', '.github/workflows/tests.yml']"], ["['Test on 1.70.0', '.github/workflows/tests.yml']", "['Test on WASI', '.github/workflows/tests.yml']"], ["['Test on nightly', '.github/workflows/tests.yml']", "['Check on 1.70.0 (32bit)', '.github/workflows/tests.yml']"], ["['Pyright', '.github/workflows/pyright.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['plan', '.github/workflows/release.yml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-500", "base_commit": "2293ae43f8e5ed67ff352e520c64b6b0f2f483d1", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 89825428..463acbff 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -56,6 +56,8 @@ For upgrade instructions read the [UPDATING](UPDATING.md) guide.\n   was raised to 100000. #493\n - `Value::from` is now implemented for `Error` as public API to create invalid values.\n   Previously this behavior was hidden internally in the serde support. #495\n+- `UndefinedBehavior::Strict` now acts more delayed.  This means that now `value.key is defined`\n+  will no longer error when `value.key` is `undefined`. #500\n \n ## 1.0.20\n \ndiff --git a/minijinja/src/utils.rs b/minijinja/src/utils.rs\nindex 4dcd4b96..4efd8725 100644\n--- a/minijinja/src/utils.rs\n+++ b/minijinja/src/utils.rs\n@@ -147,15 +147,27 @@ impl UndefinedBehavior {\n     /// looking up a missing attribute on a defined value.\n     pub(crate) fn handle_undefined(self, parent_was_undefined: bool) -> Result<Value, Error> {\n         match (self, parent_was_undefined) {\n-            (UndefinedBehavior::Lenient, false) | (UndefinedBehavior::Chainable, _) => {\n-                Ok(Value::UNDEFINED)\n-            }\n-            (UndefinedBehavior::Lenient, true) | (UndefinedBehavior::Strict, _) => {\n+            (UndefinedBehavior::Lenient, false)\n+            | (UndefinedBehavior::Strict, false)\n+            | (UndefinedBehavior::Chainable, _) => Ok(Value::UNDEFINED),\n+            (UndefinedBehavior::Lenient, true) | (UndefinedBehavior::Strict, true) => {\n                 Err(Error::from(ErrorKind::UndefinedError))\n             }\n         }\n     }\n \n+    /// Utility method to check if something is true.\n+    ///\n+    /// This fails only for strict undefined values.\n+    #[inline]\n+    pub(crate) fn is_true(self, value: &Value) -> Result<bool, Error> {\n+        if matches!(self, UndefinedBehavior::Strict) && value.is_undefined() {\n+            Err(Error::from(ErrorKind::UndefinedError))\n+        } else {\n+            Ok(value.is_true())\n+        }\n+    }\n+\n     /// Tries to iterate over a value while handling the undefined value.\n     ///\n     /// If the value is undefined, then iteration fails if the behavior is set to strict,\ndiff --git a/minijinja/src/vm/mod.rs b/minijinja/src/vm/mod.rs\nindex 24af90f0..807faecd 100644\n--- a/minijinja/src/vm/mod.rs\n+++ b/minijinja/src/vm/mod.rs\n@@ -235,8 +235,7 @@ impl<'env> Vm<'env> {\n \n             // if we only have two arguments that we pull from the stack, we\n             // can assign them to a and b.  This slightly reduces the amount of\n-            // code bloat generated here.  Do the same for a potential error\n-            // that needs processing.\n+            // code bloat generated here.\n             let a;\n             let b;\n             let mut err;\n@@ -483,13 +482,13 @@ impl<'env> Vm<'env> {\n                 }\n                 Instruction::JumpIfFalse(jump_target) => {\n                     a = stack.pop();\n-                    if !a.is_true() {\n+                    if !ok!(undefined_behavior.is_true(&a)) {\n                         pc = *jump_target;\n                         continue;\n                     }\n                 }\n                 Instruction::JumpIfFalseOrPop(jump_target) => {\n-                    if !stack.peek().is_true() {\n+                    if !ok!(undefined_behavior.is_true(stack.peek())) {\n                         pc = *jump_target;\n                         continue;\n                     } else {\n@@ -497,7 +496,7 @@ impl<'env> Vm<'env> {\n                     }\n                 }\n                 Instruction::JumpIfTrueOrPop(jump_target) => {\n-                    if stack.peek().is_true() {\n+                    if ok!(undefined_behavior.is_true(stack.peek())) {\n                         pc = *jump_target;\n                         continue;\n                     } else {\n", "test_patch": "diff --git a/minijinja/src/tests.rs b/minijinja/src/tests.rs\nindex 2aeb1e5a..5ec1d88c 100644\n--- a/minijinja/src/tests.rs\n+++ b/minijinja/src/tests.rs\n@@ -474,10 +474,11 @@ mod builtins {\n     /// This is useful when combined with [`select`](crate::filters::select).\n     #[cfg_attr(docsrs, doc(cfg(feature = \"builtins\")))]\n     #[cfg(feature = \"builtins\")]\n-    pub fn is_in(value: &Value, other: &Value) -> bool {\n-        crate::value::ops::contains(other, value)\n+    pub fn is_in(state: &State, value: &Value, other: &Value) -> Result<bool, Error> {\n+        ok!(state.undefined_behavior().assert_iterable(value));\n+        Ok(crate::value::ops::contains(other, value)\n             .map(|value| value.is_true())\n-            .unwrap_or(false)\n+            .unwrap_or(false))\n     }\n \n     /// Checks if a value is `true`.\ndiff --git a/minijinja/tests/test_undefined.rs b/minijinja/tests/test_undefined.rs\nindex 0fb70332..0ebca355 100644\n--- a/minijinja/tests/test_undefined.rs\n+++ b/minijinja/tests/test_undefined.rs\n@@ -1,5 +1,7 @@\n #![cfg(feature = \"builtins\")]\n-use minijinja::{render, Environment, ErrorKind, State, UndefinedBehavior};\n+use std::collections::HashMap;\n+\n+use minijinja::{context, render, Environment, ErrorKind, State, UndefinedBehavior, Value};\n \n use similar_asserts::assert_eq;\n \n@@ -24,8 +26,13 @@ fn test_lenient_undefined() {\n         render!(in env, \"<{% for x in undefined %}...{% endfor %}>\"),\n         \"<>\"\n     );\n+    assert_eq!(render!(in env, \"{{ 'foo' is in(undefined) }}\"), \"false\");\n     assert_eq!(render!(in env, \"<{{ undefined }}>\"), \"<>\");\n     assert_eq!(render!(in env, \"{{ undefined is undefined }}\"), \"true\");\n+    assert_eq!(\n+        render!(in env, \"{{ x.foo is undefined }}\", x => HashMap::<String, String>::new()),\n+        \"true\"\n+    );\n     assert_eq!(render!(in env, \"{{ undefined|list }}\"), \"[]\");\n     assert_eq!(render!(in env, \"<{{ undefined|test }}>\"), \"<>\");\n     assert_eq!(render!(in env, \"{{ 42 in undefined }}\"), \"false\");\n@@ -38,6 +45,9 @@ fn test_strict_undefined() {\n     env.add_filter(\"test\", |_state: &State, _value: String| -> String {\n         panic!(\"filter must not be called\");\n     });\n+    env.add_filter(\"test2\", |_state: &State, _value: Value| -> String {\n+        \"WAS CALLED\".into()\n+    });\n \n     assert_eq!(\n         env.render_str(\"{{ true.missing_attribute }}\", ())\n@@ -57,11 +67,30 @@ fn test_strict_undefined() {\n             .kind(),\n         ErrorKind::UndefinedError\n     );\n+    assert_eq!(\n+        env.render_str(\"{{ 'foo' is in(undefined) }}\", ())\n+            .unwrap_err()\n+            .kind(),\n+        ErrorKind::UndefinedError\n+    );\n     assert_eq!(\n         env.render_str(\"<{{ undefined }}>\", ()).unwrap_err().kind(),\n         ErrorKind::UndefinedError\n     );\n     assert_eq!(render!(in env, \"{{ undefined is undefined }}\"), \"true\");\n+    assert_eq!(\n+        render!(in env, \"{{ x.foo is undefined }}\", x => HashMap::<String, String>::new()),\n+        \"true\"\n+    );\n+    assert_eq!(\n+        env.render_str(\n+            \"{% if x.foo %}...{% endif %}\",\n+            context! { x => HashMap::<String, String>::new() }\n+        )\n+        .unwrap_err()\n+        .kind(),\n+        ErrorKind::UndefinedError\n+    );\n     assert_eq!(\n         env.render_str(\"{{ undefined|list }}\", ())\n             .unwrap_err()\n@@ -74,6 +103,7 @@ fn test_strict_undefined() {\n             .kind(),\n         ErrorKind::UndefinedError\n     );\n+    assert_eq!(render!(in env, \"{{ undefined|test2 }}\"), \"WAS CALLED\");\n     assert_eq!(\n         env.render_str(\"{{ 42 in undefined }}\", ())\n             .unwrap_err()\n@@ -98,6 +128,11 @@ fn test_chainable_undefined() {\n         render!(in env, \"<{% for x in undefined %}...{% endfor %}>\"),\n         \"<>\"\n     );\n+    assert_eq!(\n+        render!(in env, \"{{ x.foo is undefined }}\", x => HashMap::<String, String>::new()),\n+        \"true\"\n+    );\n+    assert_eq!(render!(in env, \"{{ 'foo' is in(undefined) }}\"), \"false\");\n     assert_eq!(render!(in env, \"<{{ undefined }}>\"), \"<>\");\n     assert_eq!(render!(in env, \"{{ undefined is undefined }}\"), \"true\");\n     assert_eq!(render!(in env, \"{{ undefined|list }}\"), \"[]\");\n", "problem_statement": "UndefinedBehavior::Strict and undefined key in mapping\n## Description\r\n\r\nI can reproduce my problem with `minijinja-cli`, but it happens in my own code.\r\n\r\n## Reproduction steps\r\n\r\n1. with input `input.json`:\r\n```\r\n{ \"m\": {} }\r\n```\r\n2. and template `template.j2`:\r\n```\r\n{% if m.key is defined %}\r\n  {{ m.key }}\r\n{% endif %}\r\n```\r\n3. and running `minijinja-cli --strict template.j2 input.json` I get\r\n```\r\ntemplate error: undefined value (in [...]/template.j2:1)\r\n----------------------------------- test.j2 -----------------------------------\r\n   1 > {%- if m.key is defined %}\r\n     i        ^^^^^ undefined value\r\n   2 | {{ m.key }}\r\n   3 | {%- endif %}\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nReferenced variables: {\r\n    m: {},\r\n}\r\n-------------------------------------------------------------------------------\r\n```\r\n\r\nI would expect that the `is_defined` and `is_undefined` test would work.  I also did not find a test like `has_key` which would also solve that problem.\r\n\r\nIs that behavior expected and intended?\r\n\r\n## minijinja version\r\n`minijinja-cli 1.0.20`\n", "hints_text": "This is currently intentional but not ideal as it does not match the Jinja2 behavior for strict undefineds.  However you can solve this issue today:\r\n\r\n```jinja2\r\n{% if \"key\" in m %}\r\n  {{ m.key }}\r\n{% endif %}\r\n```\nAh, thank you.  I missed the `is_in` test.  Do you prefer to keep this issue open?\nYes. This likely should be changed.", "created_at": "2024-04-23 19:21:22", "merge_commit_sha": "7b3549f1f9e2371b8a53b2c49581d7b692c36d85", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check on 1.61.0 (32bit)', '.github/workflows/tests.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['build', '.github/workflows/clippy.yml']"], ["[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']", "['Test on nightly', '.github/workflows/tests.yml']"], ["['Test on 1.61.0', '.github/workflows/tests.yml']", "['Test on Latest Stable', '.github/workflows/tests.yml']"], ["['plan', '.github/workflows/release.yml']", "['build-global-artifacts', '.github/workflows/release.yml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-489", "base_commit": "b4eda6979d3a44ef376adda377b385d10ab85003", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 8fdfd5e6..2dae9a17 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,10 @@\n \n All notable changes to MiniJinja are documented here.\n \n+## 1.0.20\n+\n+- Added support for implicit string concatenation for Jinja2 compatibility.  #489\n+\n ## 1.0.19\n \n - Deprecated `Value::from_iterator` and introduced replacement\ndiff --git a/minijinja/src/compiler/parser.rs b/minijinja/src/compiler/parser.rs\nindex 4236cf56..c6f6720c 100644\n--- a/minijinja/src/compiler/parser.rs\n+++ b/minijinja/src/compiler/parser.rs\n@@ -534,7 +534,7 @@ impl<'a> Parser<'a> {\n         let (token, span) = expect_token!(self, \"expression\");\n         macro_rules! const_val {\n             ($expr:expr) => {\n-                make_const(Value::from($expr), span)\n+                make_const(Value::from($expr), self.stream.expand_span(span))\n             };\n         }\n \n@@ -543,7 +543,18 @@ impl<'a> Parser<'a> {\n             Token::Ident(\"false\" | \"False\") => Ok(const_val!(false)),\n             Token::Ident(\"none\" | \"None\") => Ok(const_val!(())),\n             Token::Ident(name) => Ok(ast::Expr::Var(Spanned::new(ast::Var { id: name }, span))),\n-            Token::Str(val) => Ok(const_val!(val)),\n+            Token::Str(val) => {\n+                if matches_token!(self, Token::Str(_)) {\n+                    let mut buf = String::from(val);\n+                    while let Some((Token::Str(s), _)) = ok!(self.stream.current()) {\n+                        buf.push_str(s);\n+                        ok!(self.stream.next());\n+                    }\n+                    Ok(const_val!(buf))\n+                } else {\n+                    Ok(const_val!(val))\n+                }\n+            }\n             Token::String(val) => Ok(const_val!(val)),\n             Token::Int(val) => Ok(const_val!(val)),\n             Token::Int128(val) => Ok(const_val!(val)),\n", "test_patch": "diff --git a/minijinja/tests/parser-inputs/string-implicit-concat.txt b/minijinja/tests/parser-inputs/string-implicit-concat.txt\nnew file mode 100644\nindex 00000000..a0c6285a\n--- /dev/null\n+++ b/minijinja/tests/parser-inputs/string-implicit-concat.txt\n@@ -0,0 +1,3 @@\n+{{ \"foo\" }}\n+{{ \"foo\" \"bar\" }}\n+{{ \"foo\" \"bar\" \"baz\" }}\ndiff --git a/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap b/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap\nnew file mode 100644\nindex 00000000..231267f9\n--- /dev/null\n+++ b/minijinja/tests/snapshots/test_parser__parser@string-implicit-concat.txt.snap\n@@ -0,0 +1,32 @@\n+---\n+source: minijinja/tests/test_parser.rs\n+description: \"{{ \\\"foo\\\" }}\\n{{ \\\"foo\\\" \\\"bar\\\" }}\\n{{ \\\"foo\\\" \\\"bar\\\" \\\"baz\\\" }}\"\n+input_file: minijinja/tests/parser-inputs/string-implicit-concat.txt\n+---\n+Ok(\n+    Template {\n+        children: [\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foo\",\n+                } @ 1:3-1:8,\n+            } @ 1:0-1:8,\n+            EmitRaw {\n+                raw: \"\\n\",\n+            } @ 1:11-2:0,\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foobar\",\n+                } @ 2:3-2:14,\n+            } @ 2:0-2:14,\n+            EmitRaw {\n+                raw: \"\\n\",\n+            } @ 2:17-3:0,\n+            EmitExpr {\n+                expr: Const {\n+                    value: \"foobarbaz\",\n+                } @ 3:3-3:20,\n+            } @ 3:0-3:20,\n+        ],\n+    } @ 0:0-3:23,\n+)\n", "problem_statement": "String concatenation without \"+\" operator\nJinja allowed Python strings as `\"abc\" \"def\"` for concatenation at \"compile time\". For compatibility, it would be helpful if minijinja did as well.\n", "hints_text": "Oh well. I completely forgot that this is a thing. Apparently there since 2008. https://github.com/pallets/jinja/commit/4778bda8312875a943e3f16df3efc5f6ffb1e586", "created_at": "2024-04-10 18:30:20", "merge_commit_sha": "1b0dd3a7c5cdd8a985e63dc2bec7f2da504287ff", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check on 1.61.0 (32bit)', '.github/workflows/tests.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['build', '.github/workflows/clippy.yml']"], ["['Test on nightly', '.github/workflows/tests.yml']", "['Test on 1.61.0', '.github/workflows/tests.yml']"]]}
{"repo": "static-web-server/static-web-server", "instance_id": "static-web-server__static-web-server-495", "base_commit": "cd11bd62b4be6803a22d8ff274f0f6cf7adf0691", "patch": "diff --git a/docs/content/configuration/command-line-arguments.md b/docs/content/configuration/command-line-arguments.md\nindex dffd5b5d..515dfd40 100644\n--- a/docs/content/configuration/command-line-arguments.md\n+++ b/docs/content/configuration/command-line-arguments.md\n@@ -85,6 +85,10 @@ Options:\n           Server TOML configuration file path [env: SERVER_CONFIG_FILE=] [default: ./config.toml]\n       --log-remote-address [<LOG_REMOTE_ADDRESS>]\n           Log incoming requests information along with its remote address if available using the `info` log level [env: SERVER_LOG_REMOTE_ADDRESS=] [default: false] [possible values: true, false]\n+      --log-forwarded-for [<LOG_FORWARDED_FOR>]\n+          Log real IP from X-Forwarded-For header [env: SERVER_LOG_FORWARDED_FOR] [default: false] [possible values: true, false]\n+      --trusted-proxies <TRUSTED_PROXIES>\n+          A comma separated list of IP addresses to accept the X-Forwarded-For header from. Empty means trust all IPs [env: SERVER_TRUSTED_PROXIES] [default: \"\"]\n       --redirect-trailing-slash [<REDIRECT_TRAILING_SLASH>]\n           Check for a trailing slash in the requested directory URI and redirect permanently (308) to the same path with a trailing slash suffix if it is missing [env: SERVER_REDIRECT_TRAILING_SLASH=] [default: true] [possible values: true, false]\n       --ignore-hidden-files [<IGNORE_HIDDEN_FILES>]\ndiff --git a/docs/content/configuration/config-file.md b/docs/content/configuration/config-file.md\nindex 78a13597..d6058962 100644\n--- a/docs/content/configuration/config-file.md\n+++ b/docs/content/configuration/config-file.md\n@@ -75,6 +75,12 @@ grace-period = 0\n #### Log request Remote Address if available\n log-remote-address = false\n \n+#### Log real IP from X-Forwarded-For header if available\n+log-forwarded-for = false\n+\n+#### IPs to accept the X-Forwarded-For header from. Empty means all\n+trusted-proxies = []\n+\n #### Redirect to trailing slash in the requested directory uri\n redirect-trailing-slash = true\n \ndiff --git a/docs/content/configuration/environment-variables.md b/docs/content/configuration/environment-variables.md\nindex 046bbf49..605fa080 100644\n--- a/docs/content/configuration/environment-variables.md\n+++ b/docs/content/configuration/environment-variables.md\n@@ -30,6 +30,12 @@ Specify a logging level in lowercase. Possible values are `error`, `warn`, `info\n ### SERVER_LOG_REMOTE_ADDRESS\n Log incoming request information along with its Remote Address (IP) if available using the `info` log level. Default `false`.\n \n+### SERVER_LOG_FORWARDED_FOR\n+Log real IP from X-Forwarded-For header if available using the `info` log level. Default `false`\n+\n+### SERVER_TRUSTED_PROXIES\n+A comma separated list of IP addresses to accept the X-Forwarded-For header from. An empty string means trust all IPs. Default `\"\"`\n+\n ### SERVER_ERROR_PAGE_404\n HTML file path for 404 errors. If the path is not specified or simply doesn't exist then the server will use a generic HTML error message.\n If a relative path is used then it will be resolved under the root directory. Default `./404.html`.\ndiff --git a/docs/content/features/logging.md b/docs/content/features/logging.md\nindex 335688dd..dd100ab7 100644\n--- a/docs/content/features/logging.md\n+++ b/docs/content/features/logging.md\n@@ -27,55 +27,85 @@ Log entry example:\n 2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n ```\n \n-Below is an example of how to enable Remote Address (IP) logging. Note the last two entries.\n+Below is an example of how to enable Remote Address (IP) logging.\n \n ```sh\n static-web-server -a \"0.0.0.0\" -p 8080 -d docker/public/ -g info --log-remote-address=true\n-# 2022-05-23T22:24:44.523057Z  INFO static_web_server::logger: logging level: info\n-# 2022-05-23T22:24:44.523856Z  INFO static_web_server::server: server bound to TCP socket 0.0.0.0:8080\n-# 2022-05-23T22:24:44.523962Z  INFO static_web_server::server: runtime worker threads: 4\n-# 2022-05-23T22:24:44.523989Z  INFO static_web_server::server: security headers: enabled=false\n-# 2022-05-23T22:24:44.524006Z  INFO static_web_server::server: auto compression: enabled=true\n-# 2022-05-23T22:24:44.524061Z  INFO static_web_server::server: directory listing: enabled=false\n-# 2022-05-23T22:24:44.524097Z  INFO static_web_server::server: directory listing order code: 6\n-# 2022-05-23T22:24:44.524133Z  INFO static_web_server::server: cache control headers: enabled=true\n-# 2022-05-23T22:24:44.524191Z  INFO static_web_server::server: basic authentication: enabled=false\n-# 2022-05-23T22:24:44.524210Z  INFO static_web_server::server: grace period before graceful shutdown: 0s\n-# 2022-05-23T22:24:44.524527Z  INFO Server::start_server{addr_str=\"0.0.0.0:8080\" threads=4}: static_web_server::server: close time.busy=0.00ns time.idle=10.6\u00b5s\n-# 2022-05-23T22:24:44.524585Z  INFO static_web_server::server: listening on http://0.0.0.0:8080\n-# 2022-05-23T22:24:44.524614Z  INFO static_web_server::server: press ctrl+c to shut down the server\n-# 2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n-# 2022-05-23T22:25:26.516841Z  INFO static_web_server::handler: incoming request: method=GET uri=/favicon.ico remote_addr=192.168.1.126:57625\n+```\n+\n+The relevant log output:\n+```log\n+INFO static_web_server::logger: logging level: info\n+<...>\n+INFO static_web_server::info: log requests with remote IP addresses: enabled=true\n+<...>\n+INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n+INFO static_web_server::handler: incoming request: method=GET uri=/favicon.ico remote_addr=192.168.1.126:57625\n ```\n ## Log Real Remote IP\n \n-When used behind reverse proxy, reported `remote_addr` indicate proxy internal IP address and port, and not client real remote IP.\n-Proxy server can be configured to provide [X-Forwarded-For header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For), containing comma-separated list of IP addresses, starting with *client real remote IP*, and all following intermediate proxies (if any).\n+When used behind a reverse proxy the reported `remote_addr` indicates the proxies IP address and port, not the clients real IP.\n+The Proxy server can be configured to provide the [X-Forwarded-For header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For), containing a comma-separated list of IP addresses, starting with the *real remote client IP*, and all following intermediate proxies (if any).\n+\n+\n+To enable logging of the real remote IP, enable the `--log-forwarded-for` option or the equivalent [SERVER_LOG_FORWARDED_FOR](/docs/content/configuration/environment-variables.md#serverlogforwardedfor) env. By default this will log all requests which have a correctly formatted `X-Forwarded-For` header. \n+\n+Since the content of the `X-Forwarded-For` header can be changed by all proxies in the chain, the remote IP address reported may not be trusted.\n \n-When *Remote Address (IP) logging* [is enabled](#log-remote-addresses), and `X-Forwarded-For` header is present and correctly formatted, then log entries for requests will contain a `real_remote_ip` section with IP of remote client, **as reported by this header**. \n+To restrict the logging to only trusted proxy IPs, you can use the `--trusted-proxies` option, or the equivalent [SERVER_TRUSTED_PROXIES](/docs/content/configuration/environment-variables.md#servertrustedproxies) env. This should be a list of IPs, separated by commas. An empty list (the default) indicates that all IPs should be trusted.\n+\n+Command used for the following examples:\n+```sh\n+static-web-server -a \"::\" --log-forwarded-for=true --trusted-proxies=\"::1\" -p 8080 -d docker/public/ -g info\n+```\n+\n+Look for these lines in the log output:\n+```log\n+<...>\n+INFO static_web_server::info: log level: info\n+INFO static_web_server::info: log requests with remote IP addresses: enabled=false\n+INFO static_web_server::info: log X-Forwarded-For real remote IP addresses: enabled=true\n+INFO static_web_server::info: trusted IPs for X-Forwarded-For: [::1]\n+<...>\n+```\n \n We can simulate request as from behind reverse proxy with additional intermediate-proxy with following command:\n \n ```sh\n-curl --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\" http://0.0.0.0:8080\n+curl \"http://[::1]:8080\" --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\"\n ```\n \n-Log entry for such case will look like:\n+Log entry for this request will look like:\n \n ```log\n-2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625 real_remote_ip=203.0.113.195\n+INFO static_web_server::handler: incoming request: method=GET uri=/ real_remote_ip=203.0.113.195\n ```\n \n-**`SWS`** will parse `X-Forwarded-For` header, and if format of provided IP is invalid - it will be ignored to prevent log poisoning attacks. In such case `real_remote_ip` section will not be added.\n+---\n+\n+If we send the request from `127.0.0.1` instead:\n+```sh\n+curl \"http://127.0.0.1:8080\" --header \"X-Forwarded-For: 203.0.113.195, 2001:db8:85a3:8d3:1319:8a2e:370:7348\"\n+```\n+\n+we get the following log output:\n+```log\n+INFO static_web_server::handler: incoming request: method=GET uri=/\n+```\n+`127.0.0.1` is not in the `trusted_proxies`, so we dont get a `real_remote_address` in the log.\n+\n+Note the absence of the proxies remote address in these examples. If you want to log the remote address and the real remote address, you need to specify both `--log-remote-address` and `--log-forwarded-for`.\n+\n+---\n+\n+**`SWS`** will parse the `X-Forwarded-For` header and if the provided client IP is invalid, it will be ignored to prevent log poisoning attacks. In such cases the `real_remote_ip` section will not be added.\n \n Example from above, but with invalid header:\n \n ```sh\n-curl --header \"X-Forwarded-For: <iframe src=//malware.attack>\" http://0.0.0.0:8080\n+curl \"http://[::1]:8080\" --header \"X-Forwarded-For: <iframe src=//malware.attack>\"\n ```\n \n ```log\n-2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/ remote_addr=192.168.1.126:57625\n+2022-05-23T22:24:50.519540Z  INFO static_web_server::handler: incoming request: method=GET uri=/\n ```\n-\n-Be aware, that contents of `X-Forwarded-For` header can be augumented by all proxies in the chain, and as such - remote IP address reported by it may not be trusted.\n\\ No newline at end of file\ndiff --git a/src/handler.rs b/src/handler.rs\nindex 7e0216be..79b1e09b 100644\n--- a/src/handler.rs\n+++ b/src/handler.rs\n@@ -7,7 +7,12 @@\n //!\n \n use hyper::{Body, Request, Response, StatusCode};\n-use std::{future::Future, net::SocketAddr, path::PathBuf, sync::Arc};\n+use std::{\n+    future::Future,\n+    net::{IpAddr, SocketAddr},\n+    path::PathBuf,\n+    sync::Arc,\n+};\n \n #[cfg(any(\n     feature = \"compression\",\n@@ -97,6 +102,10 @@ pub struct RequestHandlerOpts {\n     pub index_files: Vec<String>,\n     /// Log remote address feature.\n     pub log_remote_address: bool,\n+    /// Log the X-Forwarded-For header.\n+    pub log_forwarded_for: bool,\n+    /// Trusted IPs for remote addresses.\n+    pub trusted_proxies: Vec<IpAddr>,\n     /// Redirect trailing slash feature.\n     pub redirect_trailing_slash: bool,\n     /// Ignore hidden files feature.\n@@ -152,6 +161,8 @@ impl Default for RequestHandlerOpts {\n             basic_auth: String::new(),\n             index_files: vec![\"index.html\".into()],\n             log_remote_address: false,\n+            log_forwarded_for: false,\n+            trusted_proxies: Vec::new(),\n             redirect_trailing_slash: true,\n             ignore_hidden_files: false,\n             disable_symlinks: false,\ndiff --git a/src/log_addr.rs b/src/log_addr.rs\nindex c0a98e14..27174c92 100644\n--- a/src/log_addr.rs\n+++ b/src/log_addr.rs\n@@ -14,7 +14,18 @@ use crate::{handler::RequestHandlerOpts, health};\n /// Initializes the log address module.\n pub(crate) fn init(enabled: bool, handler_opts: &mut RequestHandlerOpts) {\n     handler_opts.log_remote_address = enabled;\n-    server_info!(\"log requests with remote and real IP addresses: enabled={enabled}\");\n+    let trusted = if handler_opts.trusted_proxies.is_empty() {\n+        \"all\".to_owned()\n+    } else {\n+        format!(\"{:?}\", handler_opts.trusted_proxies)\n+    };\n+\n+    server_info!(\"log requests with remote IP addresses: enabled={enabled}\");\n+    server_info!(\n+        \"log X-Forwarded-For real remote IP addresses: enabled={}\",\n+        handler_opts.log_forwarded_for\n+    );\n+    server_info!(\"trusted IPs for X-Forwarded-For: {trusted}\");\n }\n \n /// It logs remote and real IP addresses if available.\n@@ -23,23 +34,27 @@ pub(crate) fn pre_process<T>(\n     req: &Request<T>,\n     remote_addr: Option<SocketAddr>,\n ) {\n-    let remote_addrs = if opts.log_remote_address {\n-        // Add a Remote IP if available\n-        let remote_addr = remote_addr.map_or(\"\".to_owned(), |ip| format!(\" remote_addr={ip}\"));\n+    let mut remote_addrs = String::new();\n \n-        // Add also a Real Remote IP if available\n-        let real_remote_addr = req\n+    if opts.log_remote_address {\n+        if let Some(addr) = remote_addr {\n+            remote_addrs.push_str(format!(\" remote_addr={addr}\").as_str());\n+        }\n+    }\n+    if opts.log_forwarded_for\n+        && (opts.trusted_proxies.is_empty()\n+            || remote_addr.is_some_and(|addr| opts.trusted_proxies.contains(&addr.ip())))\n+    {\n+        if let Some(real_ip) = req\n             .headers()\n             .get(\"X-Forwarded-For\")\n             .and_then(|h| h.to_str().ok())\n             .and_then(|s| s.split(',').next())\n             .and_then(|s| s.trim().parse::<IpAddr>().ok())\n-            .map_or(\"\".to_owned(), |ip| format!(\" real_remote_ip={ip}\"));\n-\n-        [remote_addr, real_remote_addr].concat()\n-    } else {\n-        String::new()\n-    };\n+        {\n+            remote_addrs.push_str(format!(\" real_remote_ip={real_ip}\").as_str());\n+        }\n+    }\n \n     // Log incoming requests in debug mode only if the health option is enabled\n     if opts.health && health::is_health_endpoint(req) {\ndiff --git a/src/server.rs b/src/server.rs\nindex 8df52d3b..209375ee 100644\n--- a/src/server.rs\n+++ b/src/server.rs\n@@ -225,6 +225,12 @@ impl Server {\n         // Log remote address option\n         let log_remote_address = general.log_remote_address;\n \n+        // Log the X-Forwarded-For header.\n+        let log_forwarded_for = general.log_forwarded_for;\n+\n+        // Trusted IPs for remote addresses.\n+        let trusted_proxies = general.trusted_proxies;\n+\n         // Log redirect trailing slash option\n         let redirect_trailing_slash = general.redirect_trailing_slash;\n         server_info!(\n@@ -261,6 +267,8 @@ impl Server {\n             page404: page404.clone(),\n             page50x: page50x.clone(),\n             log_remote_address,\n+            log_forwarded_for,\n+            trusted_proxies,\n             redirect_trailing_slash,\n             ignore_hidden_files,\n             disable_symlinks,\ndiff --git a/src/settings/cli.rs b/src/settings/cli.rs\nindex 12b36106..80cc9df3 100644\n--- a/src/settings/cli.rs\n+++ b/src/settings/cli.rs\n@@ -7,7 +7,7 @@\n \n use clap::Parser;\n use hyper::StatusCode;\n-use std::path::PathBuf;\n+use std::{net::IpAddr, path::PathBuf};\n \n #[cfg(feature = \"directory-listing\")]\n use crate::directory_listing::DirListFmt;\n@@ -414,6 +414,28 @@ pub struct General {\n     /// Log incoming requests information along with its remote address if available using the `info` log level.\n     pub log_remote_address: bool,\n \n+    #[arg(\n+        long,\n+        default_value = \"false\",\n+        default_missing_value(\"true\"),\n+        num_args(0..=1),\n+        require_equals(false),\n+        action = clap::ArgAction::Set,\n+        env = \"SERVER_LOG_FORWARDED_FOR\",\n+    )]\n+    /// Log the X-Forwarded-For header for remote IP information\n+    pub log_forwarded_for: bool,\n+\n+    #[arg(\n+        long,\n+        require_equals(false),\n+        value_delimiter(','),\n+        action = clap::ArgAction::Set,\n+        env = \"SERVER_TRUSTED_PROXIES\",\n+    )]\n+    /// List of IPs to use X-Forwarded-For from. The default is to trust all\n+    pub trusted_proxies: Vec<IpAddr>,\n+\n     #[arg(\n         long,\n         default_value = \"true\",\ndiff --git a/src/settings/file.rs b/src/settings/file.rs\nindex cdd45994..98f0154e 100644\n--- a/src/settings/file.rs\n+++ b/src/settings/file.rs\n@@ -8,6 +8,7 @@\n use headers::HeaderMap;\n use serde::Deserialize;\n use serde_repr::{Deserialize_repr, Serialize_repr};\n+use std::net::IpAddr;\n use std::path::Path;\n use std::{collections::BTreeSet, path::PathBuf};\n \n@@ -356,6 +357,12 @@ pub struct General {\n     /// Log remote address feature.\n     pub log_remote_address: Option<bool>,\n \n+    /// Log the X-Forwarded-For header.\n+    pub log_forwarded_for: Option<bool>,\n+\n+    /// Trusted IPs for remote addresses.\n+    pub trusted_proxies: Option<Vec<IpAddr>>,\n+\n     /// Redirect trailing slash feature.\n     pub redirect_trailing_slash: Option<bool>,\n \ndiff --git a/src/settings/mod.rs b/src/settings/mod.rs\nindex 5cd58b54..46a0f67f 100644\n--- a/src/settings/mod.rs\n+++ b/src/settings/mod.rs\n@@ -197,6 +197,8 @@ impl Settings {\n         let mut page_fallback = opts.page_fallback;\n \n         let mut log_remote_address = opts.log_remote_address;\n+        let mut log_forwarded_for = opts.log_forwarded_for;\n+        let mut trusted_proxies = opts.trusted_proxies;\n         let mut redirect_trailing_slash = opts.redirect_trailing_slash;\n         let mut ignore_hidden_files = opts.ignore_hidden_files;\n         let mut disable_symlinks = opts.disable_symlinks;\n@@ -363,6 +365,12 @@ impl Settings {\n                 if let Some(v) = general.log_remote_address {\n                     log_remote_address = v\n                 }\n+                if let Some(v) = general.log_forwarded_for {\n+                    log_forwarded_for = v\n+                }\n+                if let Some(v) = general.trusted_proxies {\n+                    trusted_proxies = v\n+                }\n                 if let Some(v) = general.redirect_trailing_slash {\n                     redirect_trailing_slash = v\n                 }\n@@ -651,6 +659,8 @@ impl Settings {\n                 #[cfg(feature = \"fallback-page\")]\n                 page_fallback,\n                 log_remote_address,\n+                log_forwarded_for,\n+                trusted_proxies,\n                 redirect_trailing_slash,\n                 ignore_hidden_files,\n                 disable_symlinks,\n", "test_patch": "diff --git a/src/testing.rs b/src/testing.rs\nindex bac712a7..77b2751c 100644\n--- a/src/testing.rs\n+++ b/src/testing.rs\n@@ -94,6 +94,8 @@ pub mod fixtures {\n             #[cfg(feature = \"basic-auth\")]\n             basic_auth: general.basic_auth,\n             log_remote_address: general.log_remote_address,\n+            log_forwarded_for: general.log_forwarded_for,\n+            trusted_proxies: general.trusted_proxies,\n             redirect_trailing_slash: general.redirect_trailing_slash,\n             ignore_hidden_files: general.ignore_hidden_files,\n             disable_symlinks: general.disable_symlinks,\n", "problem_statement": "Add trusted_proxies option\n### Search for duplicate feature request\n\n- [X] I already searched, and this feature request or improvement is not a duplicate.\n\n### Feature scope\n\nConfiguration (e.g. TOML) or CLI/env option\n\n### Feature request related to a problem\n\nThe `log_remote_address` option currently always logs the contents of the `X-Forwarded-For` header. This is problematic when static-web-server is used without a trusted proxy in front, because a client could send this header and \"poison\" the log. More details can be found [in this MDN document](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For#security_and_privacy_concerns).\n\n### Describe the solution you'd like\n\nI would like a `trusted_proxies` option which would allow specifying from which IP addresses the `X-Forwarded-For` header is allowed to be read. I have actually already implemented this in https://github.com/static-web-server/static-web-server/commit/4cfa56978cda6b3bb3cbf88d8a2b72499db55c0f.\n\n### Describe alternatives you've considered\n\nAn alternative would be to have a boolean `trust_x_forwarded`, which toggles the option for all IPs. This is not as fine grained though.\n\n### Build target\n\nAll targets\n\n### Additional context\n\nMy original Plan was to just submit a draft PR with the changes i made. Before doing that i looked for a Contributing.md file. When opening the PR i was a bit surprised to see the policy that PRs will only be accepted with a related issue, so it would be awesome if a contributing.md file is created :)\n", "hints_text": "The idea sounds fine to me. Feel free to open a draft.\r\n\r\nYes, we need a `CONTRIBUTING.md` file. I will create one.", "created_at": "2024-11-02 16:47:38", "merge_commit_sha": "13e3f3861f11e4c516e224784f44476c6e553332", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test (pinned)', '.github/workflows/devel.yml']", "['test (linux-musl-armv7)', '.github/workflows/devel.yml']"], ["['merge-artifacts', '.github/workflows/perfcheck.yml']", "['checks', '.github/workflows/devel.yml']"], ["['test (macos)', '.github/workflows/devel.yml']", "['test (linux-gnu-i686)', '.github/workflows/devel.yml']"], ["['test (macos-arm64)', '.github/workflows/devel.yml']", "['test (linux-android-arm64)', '.github/workflows/devel.yml']"], ["['build', '.github/workflows/release.build.manual.yml']", "['test (linux-arm-gnueabihf)', '.github/workflows/devel.yml']"], ["['test (linux-musl-i686)', '.github/workflows/devel.yml']", "['test (linux-gnu)', '.github/workflows/devel.yml']"], ["['test (linux-musl-arm64)', '.github/workflows/devel.yml']", "['test (windows-msvc-i686)', '.github/workflows/devel.yml']"], ["['test (windows-msvc-arm64)', '.github/workflows/devel.yml']", "['benchmark (Small file (pre-compressed), http://localhost:8080/small_precompressed.txt, 1000/s)', '.github/workflows/perfcheck.yml']"], ["['benchmark (Directory listing (empty directory), http://localhost:8080/emptydir/, 1000/s)', '.github/workflows/perfcheck.yml']", "['test (linux-gnu-arm64)', '.github/workflows/devel.yml']"]]}
{"repo": "mitsuhiko/minijinja", "instance_id": "mitsuhiko__minijinja-502", "base_commit": "f2ee26297403bcfa571d773b17a467970663efcf", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 01384460..978b0bb5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,11 @@\n \n All notable changes to MiniJinja are documented here.\n \n+## 1.0.21\n+\n+- Fixed an issue where `lstrip_blocks` unintentionally also applied to\n+  variable expression blocks.  #502\n+\n ## 1.0.20\n \n - Added support for implicit string concatenation for Jinja2 compatibility.  #489\ndiff --git a/minijinja/src/compiler/lexer.rs b/minijinja/src/compiler/lexer.rs\nindex e535527b..cb413689 100644\n--- a/minijinja/src/compiler/lexer.rs\n+++ b/minijinja/src/compiler/lexer.rs\n@@ -491,6 +491,7 @@ impl<'s> Tokenizer<'s> {\n         }\n     }\n \n+    syntax_token_getter!(variable_start, \"{{\");\n     syntax_token_getter!(variable_end, \"}}\");\n     syntax_token_getter!(block_start, \"{%\");\n     syntax_token_getter!(block_end, \"%}\");\n@@ -506,7 +507,11 @@ impl<'s> Tokenizer<'s> {\n         }\n         let old_loc = self.loc();\n         let (lead, span) = match find_start_marker(self.rest, &self.syntax_config) {\n-            Some((start, Whitespace::Default)) if self.ws_config.lstrip_blocks => {\n+            Some((start, Whitespace::Default))\n+                if self.ws_config.lstrip_blocks\n+                    && self.rest.get(start..start + self.variable_start().len())\n+                        != Some(self.variable_start()) =>\n+            {\n                 let peeked = &self.rest[..start];\n                 let trimmed = lstrip_block(peeked);\n                 let lead = self.advance(trimmed.len());\n", "test_patch": "diff --git a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\nindex 6e88b380..8c3446c0 100644\n--- a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\n+++ b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks-preserve.txt.snap\n@@ -47,8 +47,8 @@ Ident(\"seq\")\n   \"seq\"\n BlockEnd\n   \"+%}\"\n-TemplateData(\"\\n\")\n-  \"\\n\"\n+TemplateData(\"\\n    \")\n+  \"\\n    \"\n VariableStart\n   \"{{\"\n Ident(\"item\")\n@@ -65,4 +65,3 @@ BlockEnd\n   \"+%}\"\n TemplateData(\"\\n</ul>\")\n   \"\\n</ul>\"\n-\ndiff --git a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\nindex f5241b4f..84f43eb3 100644\n--- a/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\n+++ b/minijinja/tests/snapshots/test_lexer__lexer@lstrip-blocks.txt.snap\n@@ -47,6 +47,8 @@ Ident(\"seq\")\n   \"seq\"\n BlockEnd\n   \"%}\"\n+TemplateData(\"    \")\n+  \"    \"\n VariableStart\n   \"{{\"\n Ident(\"item\")\n@@ -63,4 +65,3 @@ BlockEnd\n   \"%}\"\n TemplateData(\"</ul>\")\n   \"</ul>\"\n-\n", "problem_statement": "set_lstrip_blocks should not apply to expressions, only to blocks.\n## Description\r\n\r\nI've been using the newly added support for set_lstrip_blocks. Thank you for that! I've noticed, though, that i think it is also stripping leading spaces from expressions -- {{ }}. Those aren't blocks, right? If an expression is positioned with leading whitespace in the file, it should stay that way, I think.\r\n\r\nHere's the way it works in python:\r\n\r\n```\r\n[16:52:32][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ python3 script.py\r\n    one\r\n    two\r\n\r\n(venv)\r\n[16:52:33][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ cat script.py\r\nimport jinja2\r\ntemplateLoader = jinja2.FileSystemLoader(searchpath=\".\")\r\ntemplateEnv = jinja2.Environment(\r\n  loader=templateLoader,\r\n  lstrip_blocks=True,\r\n  trim_blocks=True\r\n)\r\ntemplate = templateEnv.get_template('tmpl.tmp')\r\ntemplateVars = {\r\n  \"things\": [\r\n    \"one\",\r\n    \"two\",\r\n  ]\r\n}\r\n\r\nprint(template.render(templateVars))\r\n(venv)\r\n[16:52:48][kylederr@KylesPenalMBPM3][~/Code/derrley/testing_jinja]\r\n$ cat tmpl.tmp\r\n{% for thing in things %}\r\n    {{ thing }}\r\n{% endfor %}\r\n```\r\n\r\n## Reproduction steps\r\n\r\nDo the above, in rust:\r\n\r\n```\r\nuse minijinja::{context, path_loader, Environment};\r\n\r\nfn main() {\r\n    let mut env = Environment::new();\r\n    env.set_trim_blocks(true);\r\n    env.set_lstrip_blocks(true);\r\n    env.set_loader(path_loader(\".\"));\r\n    let tmpl = env.get_template(\"tmpl.tmp\").unwrap();\r\n    println!(\r\n        \"{}\",\r\n        tmpl.render(context! {things => vec![\"one\", \"two\"]})\r\n            .unwrap()\r\n    )\r\n}\r\n\r\n```\r\n\r\nand get this output:\r\n\r\n```\r\n$ cargo run --bin testtmpl\r\n   Compiling rules v0.1.0 (/Users/kylederr/Code/perpetualsystems/simba/rules)\r\n    Finished dev [unoptimized + debuginfo] target(s) in 0.17s\r\n     Running `/Users/kylederr/Code/perpetualsystems/simba/target/debug/testtmpl`\r\none\r\ntwo\r\n```\r\n\r\n(note they are not indented)\r\n\r\nAdditional helpful information:\r\n\r\n- Version of minijinja: 1.0.20\r\n- Version of rustc: rustc 1.76.0 (07dca489a 2024-02-04)\r\n- Operating system and version: macos sonoma 14.2.1\r\n\r\n## What did you expect\r\n\r\nFor the blocks to be indented, as per above.\r\n\r\n\r\n\n", "hints_text": "", "created_at": "2024-04-24 07:40:08", "merge_commit_sha": "4639d099792aabe9f0d764f4b162897e8e76b6ac", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check on 1.61.0 (32bit)', '.github/workflows/tests.yml']", "['Test on Python binding', '.github/workflows/tests.yml']"], ["['Test on Latest (No Lock)', '.github/workflows/tests.yml']", "['build', '.github/workflows/clippy.yml']"], ["[\"build-local-artifacts (${{ join(matrix.targets, ', ') }})\", '.github/workflows/release.yml']", "['Test on nightly', '.github/workflows/tests.yml']"], ["['Test on 1.61.0', '.github/workflows/tests.yml']", "['Test on Latest Stable', '.github/workflows/tests.yml']"]]}
{"repo": "clarkmcc/cel-rust", "instance_id": "clarkmcc__cel-rust-109", "base_commit": "5b02b0817ced05c7cfc1c72bab03bc97bbfa2dea", "patch": "diff --git a/example/Cargo.toml b/example/Cargo.toml\nindex a0720ef..5751eee 100644\n--- a/example/Cargo.toml\n+++ b/example/Cargo.toml\n@@ -3,48 +3,58 @@ name = \"example\"\n version = \"0.1.0\"\n edition = \"2021\"\n \n+[features]\n+axum = [\"dep:axum\", \"dep:tokio\", \"dep:thiserror\"]\n+json = [\"dep:serde_json\", \"cel-interpreter/json\"]\n+chrono = [\"dep:chrono\", \"cel-interpreter/chrono\"]\n+\n [dependencies]\n+cel-interpreter = { path = \"../interpreter\", default-features = false }\n+\n+chrono = { version = \"0.4\", optional = true }\n+\n+serde = { version = \"1.0\", features = [\"derive\"] }\n+serde_json = { version = \"1.0\", optional = true }\n+\n axum = { version = \"0.7.5\", default-features = false, features = [\n     \"http1\",\n     \"json\",\n     \"tokio\",\n-] }\n-cel-interpreter = { path = \"../interpreter\", features = [\"json\", \"chrono\", \"regex\"] }\n-chrono = \"0.4.26\"\n-serde = { version = \"1.0.196\", features = [\"derive\"] }\n-serde_json = \"1.0.124\"\n-thiserror = { version = \"1.0.61\", default-features = false }\n+], optional = true }\n tokio = { version = \"1.38.0\", default-features = false, features = [\n     \"macros\",\n     \"net\",\n     \"rt-multi-thread\",\n-] }\n+], optional = true }\n+thiserror = { version = \"1.0\", optional = true }\n \n [[bin]]\n-name = \"simple\"\n+name = \"example-simple\"\n path = \"src/simple.rs\"\n \n [[bin]]\n-name = \"variables\"\n+name = \"example-variables\"\n path = \"src/variables.rs\"\n \n [[bin]]\n-name = \"functions\"\n+name = \"example-functions\"\n path = \"src/functions.rs\"\n+required-features = [\"chrono\"]\n \n [[bin]]\n-name = \"threads\"\n+name = \"example-threads\"\n path = \"src/threads.rs\"\n \n [[bin]]\n-name = \"serde\"\n+name = \"example-serde\"\n path = \"src/serde.rs\"\n \n [[bin]]\n-name = \"axum\"\n+name = \"example-axum\"\n path = \"src/axum.rs\"\n+required-features = [\"axum\"]\n \n [[bin]]\n-name = \"json\"\n+name = \"example-json\"\n path = \"src/json.rs\"\n-\n+required-features = [\"json\"]\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\nindex fa70cce..45a1379 100644\n--- a/interpreter/Cargo.toml\n+++ b/interpreter/Cargo.toml\n@@ -10,15 +10,18 @@ categories = [\"compilers\"]\n \n [dependencies]\n cel-parser = { path = \"../parser\", version = \"0.8.0\" }\n-thiserror = \"1.0.40\"\n-chrono = { version = \"0.4.26\", default-features = false, features = [\"alloc\"], optional = true }\n+\n nom = \"7.1.3\"\n-paste = \"1.0.14\"\n-serde = \"1.0.196\"\n+\n+chrono = { version = \"0.4\", default-features = false, features = [\"alloc\"], optional = true }\n regex = { version = \"1.10.5\", optional = true }\n-serde_json = { version = \"1.0.124\", optional = true }\n+serde = \"1.0\"\n+serde_json = { version = \"1.0\", optional = true }\n base64 = { version = \"0.22.1\", optional = true }\n \n+thiserror = \"1.0\"\n+paste = \"1.0\"\n+\n [dev-dependencies]\n criterion = { version = \"0.5.1\", features = [\"html_reports\"] }\n serde_bytes = \"0.11.14\"\n@@ -28,6 +31,7 @@ name = \"runtime\"\n harness = false\n \n [features]\n-json = [\"dep:base64\", \"dep:serde_json\"]\n+default = [\"regex\", \"chrono\"]\n+json = [\"dep:serde_json\", \"dep:base64\"]\n regex = [\"dep:regex\"]\n chrono = [\"dep:chrono\"]\ndiff --git a/interpreter/src/functions.rs b/interpreter/src/functions.rs\nindex a63cb38..457010f 100644\n--- a/interpreter/src/functions.rs\n+++ b/interpreter/src/functions.rs\n@@ -645,10 +645,7 @@ pub fn max(Arguments(args): Arguments) -> Result<Value> {\n #[cfg(test)]\n mod tests {\n     use crate::context::Context;\n-    use crate::testing::test_script;\n-    #[cfg(feature = \"regex\")]\n-    use crate::ExecutionError::FunctionError;\n-    use std::collections::HashMap;\n+    use crate::tests::test_script;\n \n     fn assert_script(input: &(&str, &str)) {\n         assert_eq!(test_script(input.1, None), Ok(true.into()), \"{}\", input.0);\n@@ -679,7 +676,7 @@ mod tests {\n \n         for (name, script) in tests {\n             let mut ctx = Context::default();\n-            ctx.add_variable_from_value(\"foo\", HashMap::from([(\"bar\", 1)]));\n+            ctx.add_variable_from_value(\"foo\", std::collections::HashMap::from([(\"bar\", 1)]));\n             assert_eq!(test_script(script, Some(ctx)), Ok(true.into()), \"{}\", name);\n         }\n     }\n@@ -943,7 +940,7 @@ mod tests {\n             test_script(\n                 \"'foobar'.matches('(foo') == true\", None),\n             Err(\n-                FunctionError {\n+                crate::ExecutionError::FunctionError {\n                     function: \"matches\".to_string(),\n                     message: \"'(foo' not a valid regex:\\nregex parse error:\\n    (foo\\n    ^\\nerror: unclosed group\".to_string()\n                 }\ndiff --git a/interpreter/src/lib.rs b/interpreter/src/lib.rs\nindex e307f55..b11ff47 100644\n--- a/interpreter/src/lib.rs\n+++ b/interpreter/src/lib.rs\n@@ -13,19 +13,19 @@ pub use cel_parser::Expression;\n pub use context::Context;\n pub use functions::FunctionContext;\n pub use objects::{ResolveResult, Value};\n-#[cfg(feature = \"chrono\")]\n-mod duration;\n pub mod functions;\n mod magic;\n pub mod objects;\n mod resolvers;\n+\n+#[cfg(feature = \"chrono\")]\n+mod duration;\n+\n mod ser;\n pub use ser::to_value;\n \n #[cfg(feature = \"json\")]\n mod json;\n-#[cfg(test)]\n-mod testing;\n \n use magic::FromContext;\n \n@@ -173,11 +173,16 @@ impl TryFrom<&str> for Program {\n mod tests {\n     use crate::context::Context;\n     use crate::objects::{ResolveResult, Value};\n-    use crate::testing::test_script;\n     use crate::{ExecutionError, Program};\n     use std::collections::HashMap;\n     use std::convert::TryInto;\n \n+    /// Tests the provided script and returns the result. An optional context can be provided.\n+    pub(crate) fn test_script(script: &str, ctx: Option<Context>) -> ResolveResult {\n+        let program = Program::compile(script).unwrap();\n+        program.execute(&ctx.unwrap_or_default())\n+    }\n+\n     #[test]\n     fn parse() {\n         Program::compile(\"1 + 1\").unwrap();\ndiff --git a/interpreter/src/objects.rs b/interpreter/src/objects.rs\nindex 50708d8..6c8ccc2 100644\n--- a/interpreter/src/objects.rs\n+++ b/interpreter/src/objects.rs\n@@ -1,15 +1,12 @@\n use crate::context::Context;\n use crate::functions::FunctionContext;\n-use crate::ser::SerializationError;\n-use crate::ExecutionError::NoSuchKey;\n-use crate::{to_value, ExecutionError};\n-use cel_parser::{ArithmeticOp, Atom, Expression, Member, RelationOp, UnaryOp};\n-use core::ops;\n-use serde::{Serialize, Serializer};\n+use crate::ExecutionError;\n+use cel_parser::ast::*;\n use std::cmp::Ordering;\n use std::collections::HashMap;\n use std::convert::{Infallible, TryFrom, TryInto};\n use std::fmt::{Display, Formatter};\n+use std::ops;\n use std::sync::Arc;\n \n #[derive(Debug, PartialEq, Clone)]\n@@ -84,10 +81,10 @@ impl From<u64> for Key {\n     }\n }\n \n-impl Serialize for Key {\n+impl serde::Serialize for Key {\n     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n     where\n-        S: Serializer,\n+        S: serde::Serializer,\n     {\n         match self {\n             Key::Int(v) => v.serialize(serializer),\n@@ -143,13 +140,12 @@ pub trait TryIntoValue {\n     fn try_into_value(self) -> Result<Value, Self::Error>;\n }\n \n-impl<T: Serialize> TryIntoValue for T {\n-    type Error = SerializationError;\n+impl<T: serde::Serialize> TryIntoValue for T {\n+    type Error = crate::ser::SerializationError;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n-        to_value(self)\n+        crate::ser::to_value(self)\n     }\n }\n-\n impl TryIntoValue for Value {\n     type Error = Infallible;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n@@ -629,7 +625,7 @@ impl<'a> Value {\n                 // give priority to the property. Maybe we can implement lookahead\n                 // to see if the next token is a function call?\n                 match (child, ctx.has_function(&***name)) {\n-                    (None, false) => NoSuchKey(name.clone()).into(),\n+                    (None, false) => ExecutionError::NoSuchKey(name.clone()).into(),\n                     (Some(child), _) => child.into(),\n                     (None, true) => Value::Function(name.clone(), Some(self.into())).into(),\n                 }\n@@ -960,4 +956,20 @@ mod tests {\n         let result = program.execute(&context);\n         assert_eq!(result.unwrap(), Value::Null);\n     }\n+\n+    #[test]\n+    fn reference_to_value() {\n+        let test = \"example\".to_string();\n+        let direct: Value = test.as_str().into();\n+        assert_eq!(direct, Value::String(Arc::new(String::from(\"example\"))));\n+\n+        let vec = vec![test.as_str()];\n+        let indirect: Value = vec.into();\n+        assert_eq!(\n+            indirect,\n+            Value::List(Arc::new(vec![Value::String(Arc::new(String::from(\n+                \"example\"\n+            )))]))\n+        );\n+    }\n }\n", "test_patch": "diff --git a/interpreter/src/testing.rs b/interpreter/src/testing.rs\ndeleted file mode 100644\nindex f814652..0000000\n--- a/interpreter/src/testing.rs\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-use crate::context::Context;\n-use crate::objects::ResolveResult;\n-use crate::Program;\n-\n-/// Tests the provided script and returns the result. An optional context can be provided.\n-pub(crate) fn test_script(script: &str, ctx: Option<Context>) -> ResolveResult {\n-    let program = Program::compile(script).unwrap();\n-    program.execute(&ctx.unwrap_or_default())\n-}\n", "problem_statement": "Defaulting to features for a compliant CEL interpreter? \nWith the fix of #90, some \"standard\" features of CEL are now disabled by default. While I agree with the reason and think that providing this fine grained control over dependencies et al is real great to have in any crate, I wonder if such functionality now behind a feature flag, should be `default` and be an opt-out rather an opt-in. So that anyone grabbing the crate, gets a \"std compatible experience\" wrt CEL... more so here, as this would be a \"slow killer\", i.e. only becoming visible at runtime when actually executing some otherwise perfectly fine `Expression`. wdyt? cc/ @Caellian\r\n\r\n<details>\r\n\r\ni.e.\r\n```diff\r\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\r\nindex fa70cce..c9fb30e 100644\r\n--- a/interpreter/Cargo.toml\r\n+++ b/interpreter/Cargo.toml\r\n@@ -28,6 +28,7 @@ name = \"runtime\"\r\n harness = false\r\n \r\n [features]\r\n+default = [\"regex\", \"chrono\"]\r\n json = [\"dep:base64\", \"dep:serde_json\"]\r\n regex = [\"dep:regex\"]\r\n chrono = [\"dep:chrono\"]\r\n```\r\n\r\n</details>\n", "hints_text": "Also, unsure about `json`, unlike `regex` & `chrono`, I _think_ this would become visible during development using the crate... so less opinionated whether it should be opt-in or -out... Tho technically JSON (& protobuf) interop are meant to be standard based on my reading of the spec... ", "created_at": "2024-11-05 15:26:16", "merge_commit_sha": "564e3fca2fe8c9975aa6605cace1f9ffcd8d0055", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": []}
{"repo": "biomejs/biome", "instance_id": "biomejs__biome-5008", "base_commit": "320a1c469d7b83d24f09a5b31262d790f25f074f", "patch": "diff --git a/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md b/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md\nnew file mode 100644\nindex 000000000000..698d73ca98aa\n--- /dev/null\n+++ b/.changeset/fix_nomissingvarfunction_false_positives_for_container_name.md\n@@ -0,0 +1,4 @@\n+---\n+---\n+\n+# Fix [#5007](https://github.com/biomejs/biome/issues/5007) `noMissingVarFunction` false positives for `container-name`\ndiff --git a/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs b/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\nindex fdaf757a7cd8..6a20a39fee17 100644\n--- a/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\n+++ b/crates/biome_css_analyze/src/lint/nursery/no_missing_var_function.rs\n@@ -123,9 +123,10 @@ declare_lint_rule! {\n     }\n }\n \n-pub const IGNORED_PROPERTIES: [&str; 17] = [\n+pub const IGNORED_PROPERTIES: [&str; 18] = [\n     \"animation\",\n     \"animation-name\",\n+    \"container-name\",\n     \"counter-increment\",\n     \"counter-reset\",\n     \"counter-set\",\n", "test_patch": "diff --git a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\nindex 8ed3b50ccd2e..fa4d80e37ce0 100644\n--- a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\n+++ b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css\n@@ -60,6 +60,11 @@ a {\n     view-transition-name: --bbb;\n }\n \n+@property --baz {}\n+a {\n+\tcontainer-name: --baz;\n+}\n+\n .parent {\n     color: --foo;\n     .child {\ndiff --git a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\nindex 9eb37f42e50b..78aea938c017 100644\n--- a/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\n+++ b/crates/biome_css_analyze/tests/specs/nursery/noMissingVarFunction/valid.css.snap\n@@ -66,6 +66,11 @@ a {\n     view-transition-name: --bbb;\n }\n \n+@property --baz {}\n+a {\n+\tcontainer-name: --baz;\n+}\n+\n .parent {\n     color: --foo;\n     .child {\n", "problem_statement": "\ud83d\udc85 false positive error for `no-missing-var-function` with property `container-name`\n### Environment information\n\n```bash\nCLI:\n  Version:                      1.9.4\n  Color support:                true\n\nPlatform:\n  CPU Architecture:             aarch64\n  OS:                           macos\n\nEnvironment:\n  BIOME_LOG_PATH:               unset\n  BIOME_LOG_PREFIX_NAME:        unset\n  BIOME_CONFIG_PATH:            unset\n  NO_COLOR:                     unset\n  TERM:                         \"xterm-256color\"\n  JS_RUNTIME_VERSION:           \"v23.5.0\"\n  JS_RUNTIME_NAME:              \"node\"\n  NODE_PACKAGE_MANAGER:         \"bun/1.2.1\"\n\nBiome Configuration:\n  Status:                       Loaded successfully\n  Formatter disabled:           false\n  Linter disabled:              false\n  Organize imports disabled:    false\n  VCS disabled:                 true\n\nLinter:\n  JavaScript enabled:           true\n  JSON enabled:                 true\n  CSS enabled:                  true\n  GraphQL enabled:              false\n  Recommended:                  true\n  All:                          false\n  Enabled rules:\n  performance/noDelete\n  suspicious/noCatchAssign\n  suspicious/noUnsafeNegation\n  complexity/useLiteralKeys\n  style/useImportType\n  complexity/noMultipleSpacesInRegularExpressionLiterals\n  a11y/useValidLang\n  complexity/noUselessEmptyExport\n  suspicious/useNamespaceKeyword\n  suspicious/useValidTypeof\n  a11y/useValidAriaRole\n  correctness/noConstantCondition\n  a11y/useAriaActivedescendantWithTabindex\n  suspicious/noAssignInExpressions\n  style/useDefaultParameterLast\n  complexity/noEmptyTypeParameters\n  correctness/noConstructorReturn\n  style/useSelfClosingElements\n  suspicious/noDuplicateParameters\n  suspicious/noDuplicateSelectorsKeyframeBlock\n  style/useTemplate\n  correctness/noUnusedLabels\n  complexity/noUselessTernary\n  correctness/noUnreachableSuper\n  suspicious/noCompareNegZero\n  suspicious/noExplicitAny\n  correctness/noSwitchDeclarations\n  a11y/noAutofocus\n  correctness/noUnsafeOptionalChaining\n  correctness/noConstAssign\n  suspicious/noControlCharactersInRegex\n  complexity/noUselessTypeConstraint\n  style/noVar\n  suspicious/noDoubleEquals\n  suspicious/noRedundantUseStrict\n  style/useLiteralEnumMembers\n  suspicious/noGlobalIsNan\n  suspicious/noEmptyInterface\n  suspicious/noConstEnum\n  suspicious/noMisleadingCharacterClass\n  correctness/noPrecisionLoss\n  a11y/noLabelWithoutControl\n  suspicious/noRedeclare\n  correctness/noStringCaseMismatch\n  correctness/noSetterReturn\n  correctness/noInvalidConstructorSuper\n  suspicious/noImplicitAnyLet\n  suspicious/noFallthroughSwitchClause\n  suspicious/noUnsafeDeclarationMerging\n  correctness/noUnreachable\n  a11y/useKeyWithClickEvents\n  suspicious/noDuplicateObjectKeys\n  complexity/noUselessThisAlias\n  complexity/noThisInStatic\n  complexity/useOptionalChain\n  correctness/noInnerDeclarations\n  style/noParameterAssign\n  suspicious/noDuplicateCase\n  a11y/useValidAnchor\n  complexity/useRegexLiterals\n  correctness/noSelfAssign\n  correctness/noInvalidBuiltinInstantiation\n  style/noUselessElse\n  style/useShorthandFunctionType\n  suspicious/noShadowRestrictedNames\n  correctness/noInvalidDirectionInLinearGradient\n  nursery/noMissingVarFunction\n  a11y/useMediaCaption\n  complexity/noUselessLabel\n  complexity/noUselessCatch\n  suspicious/noImportantInKeyframe\n  correctness/noUnsafeFinally\n  a11y/useAriaPropsForRole\n  correctness/noNonoctalDecimalEscape\n  style/useEnumInitializers\n  a11y/useHtmlLang\n  suspicious/noDuplicateTestHooks\n  complexity/noStaticOnlyClass\n  style/useWhile\n  complexity/useArrowFunction\n  style/noInferrableTypes\n  a11y/noNoninteractiveTabindex\n  complexity/useSimpleNumberKeys\n  correctness/useYield\n  a11y/noInteractiveElementToNoninteractiveRole\n  style/useNumericLiterals\n  correctness/noUnnecessaryContinue\n  suspicious/noApproximativeNumericConstant\n  suspicious/noImportAssign\n  suspicious/noLabelVar\n  correctness/noGlobalObjectCalls\n  suspicious/useDefaultSwitchClauseLast\n  a11y/useAltText\n  correctness/noEmptyCharacterClassInRegex\n  correctness/noUnknownUnit\n  suspicious/noSparseArray\n  a11y/useIframeTitle\n  complexity/noBannedTypes\n  a11y/noSvgWithoutTitle\n  correctness/noVoidElementsWithChildren\n  style/useAsConstAssertion\n  correctness/useJsxKeyInIterable\n  style/useExportType\n  complexity/noUselessLoneBlockStatements\n  style/noArguments\n  a11y/useValidAriaValues\n  suspicious/noDebugger\n  suspicious/noCommentText\n  a11y/useFocusableInteractive\n  correctness/noUnmatchableAnbSelector\n  suspicious/noGlobalAssign\n  suspicious/noDuplicateJsxProps\n  suspicious/noMisleadingInstantiator\n  a11y/noPositiveTabindex\n  correctness/noEmptyPattern\n  complexity/noExcessiveNestedTestSuites\n  security/noDangerouslySetInnerHtmlWithChildren\n  a11y/useKeyWithMouseEvents\n  suspicious/noExtraNonNullAssertion\n  suspicious/noShorthandPropertyOverrides\n  correctness/noRenderReturnValue\n  correctness/useExhaustiveDependencies\n  security/noGlobalEval\n  style/noNonNullAssertion\n  a11y/noRedundantRoles\n  complexity/useFlatMap\n  correctness/useIsNan\n  style/useConst\n  suspicious/noGlobalIsFinite\n  suspicious/noSelfCompare\n  suspicious/noThenProperty\n  suspicious/noAsyncPromiseExecutor\n  suspicious/noDuplicateFontNames\n  suspicious/useGetterReturn\n  security/noDangerouslySetInnerHtml\n  style/useNodejsImportProtocol\n  a11y/noDistractingElements\n  suspicious/noArrayIndexKey\n  complexity/noWith\n  suspicious/noDuplicateClassMembers\n  complexity/noExtraBooleanCast\n  performance/noAccumulatingSpread\n  a11y/useValidAriaProps\n  a11y/noRedundantAlt\n  correctness/noChildrenProp\n  correctness/noUnknownFunction\n  correctness/noInvalidPositionAtImportRule\n  suspicious/noConfusingLabels\n  suspicious/noPrototypeBuiltins\n  suspicious/noConfusingVoidType\n  suspicious/noFocusedTests\n  a11y/useButtonType\n  a11y/useSemanticElements\n  a11y/noAriaUnsupportedElements\n  correctness/noInvalidGridAreas\n  correctness/noFlatMapIdentity\n  suspicious/noSuspiciousSemicolonInJsx\n  a11y/noBlankTarget\n  a11y/useHeadingContent\n  correctness/useValidForDirection\n  correctness/noVoidTypeReturn\n  correctness/noInvalidUseBeforeDeclaration\n  a11y/noAriaHiddenOnFocusable\n  a11y/useGenericFontNames\n  correctness/noUnknownMediaFeatureName\n  a11y/useAnchorContent\n  complexity/noUselessRename\n  style/useNumberNamespace\n  complexity/noUselessConstructor\n  a11y/noAccessKey\n  style/useExponentiationOperator\n  style/noUnusedTemplateLiteral\n  complexity/noUselessSwitchCase\n  style/useSingleVarDeclarator\n  suspicious/noExportsInTest\n  a11y/noNoninteractiveElementToInteractiveRole\n  style/noCommaOperator\n  suspicious/noDuplicateAtImportRules\n  suspicious/useIsArray\n  a11y/noHeaderScope\n  complexity/noUselessFragments\n  suspicious/noMisrefactoredShorthandAssign\n  complexity/noForEach\n  suspicious/noClassAssign\n  suspicious/noEmptyBlock\n  suspicious/noFunctionAssign\n\nWorkspace:\n  Open Documents:               0\n```\n\n### Rule name\n\nnursery/noMissingVarFunction\n\n### Playground link\n\nhttps://biomejs.dev/playground/?bracketSameLine=true&lintRules=all&unsafeParameterDecoratorsEnabled=false&allowComments=false&files.main.css=QABwAHIAbwBwAGUAcgB0AHkAIAAtAC0AZgBvAG8AIAB7AAoAIAAgAHMAeQBuAHQAYQB4ADoAIAAiADwAcwB0AHIAaQBuAGcAPgAiADsACgAgACAAaQBuAGgAZQByAGkAdABzADoAIAB0AHIAdQBlADsACgAgACAAaQBuAGkAdABpAGEAbAAtAHYAYQBsAHUAZQA6ACAAIgBjAHUAcwB0AG8AbQAiADsACgB9AAoACgBhACAAewAKACAAIABjAG8AbgB0AGEAaQBuAGUAcgAtAG4AYQBtAGUAOgAgAC0ALQBmAG8AbwA7AAoAfQA%3D\n\n### Expected result\n\ncss analyzer should not report for property `container-name` in `no-missing-var-function` rule.\n\n### Code of Conduct\n\n- [x] I agree to follow Biome's Code of Conduct\n", "hints_text": "", "created_at": "2025-02-01 08:12:36", "merge_commit_sha": "99f27a2b31fc15825a3175d342e2403e9764d22c", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test (windows-latest)', '.github/workflows/pull_request.yml']", "['Test (ubuntu-latest)', '.github/workflows/pull_request.yml']"], ["['Test node.js API', '.github/workflows/pull_request.yml']", "['Validate PR title', '.github/workflows/pull_request_title_lint.yaml']"], ["['Lint project', '.github/workflows/pull_request.yml']", "['Format project', '.github/workflows/pull_request.yml']"]]}
{"repo": "foundry-rs/foundry", "instance_id": "foundry-rs__foundry-9836", "base_commit": "c4ae68826405e2ecb4ab4c27cbce5ac4e21bf1a3", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 06196f4cf276..df5e5a80a8a0 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -3756,9 +3756,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers\"\n-version = \"0.13.1\"\n+version = \"0.13.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"203e96bd596350ec8d9aedfca7eff573e9347e2b2fe50eedfbf78532dabe418e\"\n+checksum = \"de23802550de5204eec1a6297296d2fef6b86ea61f33b1667eebd886577caa34\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3793,9 +3793,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts\"\n-version = \"0.13.1\"\n+version = \"0.13.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"be8ba2ccf8a4bf7730b2ad2815984c1af8e5b8c492420f1cf1d26a8be29cc9e4\"\n+checksum = \"27f065a1c785b3ec556a7a49a27016fc723bd9a4d8623f521c326f8396df64a6\"\n dependencies = [\n  \"foundry-compilers-artifacts-solc\",\n  \"foundry-compilers-artifacts-vyper\",\n@@ -3803,9 +3803,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts-solc\"\n-version = \"0.13.1\"\n+version = \"0.13.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5889eeb7d6729afbbbb1313b22e8f58aa909fcf38aa6b2f9c9b2443ca0765c59\"\n+checksum = \"127e0e788765bc0103eb6b92067843c6ffe660090b2f6fbff1a1441a51939aa0\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3827,9 +3827,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts-vyper\"\n-version = \"0.13.1\"\n+version = \"0.13.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b01048f354b4e98bf5fb4810e0607c87d4b8cc7fe6305a42103ce192e33b2da7\"\n+checksum = \"8764f450332267305440ec399560919e01aa98f23af2c36ac952f9216449cdca\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3842,9 +3842,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-core\"\n-version = \"0.13.1\"\n+version = \"0.13.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"079b80a4f188af9c273b081547d1a95a0a33f782851a99e21aa4eba06e47fa34\"\n+checksum = \"c59caebbeaeb0e34564c1a404081478a0aadaa8999b57f0befd85965252e074a\"\n dependencies = [\n  \"alloy-primitives\",\n  \"cfg-if\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex f152898f64c2..9d14ed5925e3 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -187,7 +187,7 @@ foundry-linking = { path = \"crates/linking\" }\n \n # solc & compilation utilities\n foundry-block-explorers = { version = \"0.11.0\", default-features = false }\n-foundry-compilers = { version = \"0.13.0\", default-features = false }\n+foundry-compilers = { version = \"0.13.2\", default-features = false }\n foundry-fork-db = \"0.11.0\"\n solang-parser = \"=0.3.3\"\n solar-parse = { version = \"=0.1.1\", default-features = false }\n", "test_patch": "", "problem_statement": "forge flatten sort contracts alphabetically\n### Component\n\nForge\n\n### Describe the feature you would like\n\nWe rely on flatten quite heavily to diff code.\nOne huge pain point with this is that flattening two versions of a contract can result in dramatically different code as import order might have changed.\n\nTherefore it would be a huge help/improvement if flatten would sort contacts alphabetically, either as default or when passing a flag.\n\n### Additional context\n\n_No response_\nforge flatten sort contracts alphabetically\n### Component\n\nForge\n\n### Describe the feature you would like\n\nWe rely on flatten quite heavily to diff code.\nOne huge pain point with this is that flattening two versions of a contract can result in dramatically different code as import order might have changed.\n\nTherefore it would be a huge help/improvement if flatten would sort contacts alphabetically, either as default or when passing a flag.\n\n### Additional context\n\n_No response_\n", "hints_text": "per foundry compilers comment\n```\n/// We can't just sort files alphabetically as it might break compilation, because Solidity\n/// does not allow base class definitions to appear after derived contract\n/// definitions.\n///\n/// Instead, we sort files by the number of their dependencies (imports of any depth) in ascending\n/// order. If files have the same number of dependencies, we sort them alphabetically.\n/// Target file is always placed last.\n```\n\nsimple test yields solc failure with `Error (2449): Definition of base has to precede definition of derived contract`\n\n@klkvr could you please chime in?\n@grandizzy interesting, i was not aware.\n\nFor us this would still be insanely useful \ud83d\ude05 - we don't actually want to build/test or deploy the flattened code. We just want to compare it.\nThe realworld problem we have is for example [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/1/0x80f2c02224a2E548FC67c0bF705eBFA825dd5439.sol) vs [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/Collector.sol).\n\nIn the first `Address` comes first, in the second `Initializable` comes first. Both i think don't have dependencies.\n\n---\n\nPerhaps this overall approach is a bit suboptimal and there might be better ways to do this?\nWould be curious on your opinions, I guess contract diffing could be a quite useful feature on foundry itself as i guess everyone working with proxies would be interested in this.\n@sakulstra this was initially addressed in scope of https://github.com/foundry-rs/compilers/pull/52 to resolve https://github.com/foundry-rs/foundry/issues/6539\n\nSorting contracs alphabetically is not feasible because of the `base has to precede definition of derived contract` error. When working on this I was mostly optimizing for diff readability which I think is good enough now as order of files is pretty much deterministic as long as imported fileds not change. \n\n> The realworld problem we have is for example [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/1/0x80f2c02224a2E548FC67c0bF705eBFA825dd5439.sol) vs [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/Collector.sol).\n> \n> In the first `Address` comes first, in the second `Initializable` comes first. Both i think don't have dependencies.\n\nThis is happening because in the second file, `Initializable.sol` is coming from `lib/aave-v3-origin/..` which is lexicographically higher than `Address.sol` path\n> This is happening because in the second file, Initializable.sol is coming from lib/aave-v3-origin/.. which is lexicographically higher than Address.sol path\n\nSorry, i don't see it \ud83d\ude05 \n\nHow is Initializable lexicographically higher than Address? Or is the path considered for the sorting... which would be weird, no?\nAs in this case changing from e.g. submodules to soldeer or npm would completely change the output i guess?\n> Or is the path considered for the sorting... which would be weird, no?\n\nyeah path is considered right now\n\n> As in this case changing from e.g. submodules to soldeer or npm would completely change the output i guess?\n\nyeah tbh I'd say this is an edge case I just didn't consider at the time :) I guess we can use path as secondary sorting key, should be fixed in https://github.com/foundry-rs/compilers/pull/247\ngoing to reopen as a placeholder for bumping compiler version\nsending patch @grandizzy \npatch is out\nper foundry compilers comment\n```\n/// We can't just sort files alphabetically as it might break compilation, because Solidity\n/// does not allow base class definitions to appear after derived contract\n/// definitions.\n///\n/// Instead, we sort files by the number of their dependencies (imports of any depth) in ascending\n/// order. If files have the same number of dependencies, we sort them alphabetically.\n/// Target file is always placed last.\n```\n\nsimple test yields solc failure with `Error (2449): Definition of base has to precede definition of derived contract`\n\n@klkvr could you please chime in?\n@grandizzy interesting, i was not aware.\n\nFor us this would still be insanely useful \ud83d\ude05 - we don't actually want to build/test or deploy the flattened code. We just want to compare it.\nThe realworld problem we have is for example [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/1/0x80f2c02224a2E548FC67c0bF705eBFA825dd5439.sol) vs [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/Collector.sol).\n\nIn the first `Address` comes first, in the second `Initializable` comes first. Both i think don't have dependencies.\n\n---\n\nPerhaps this overall approach is a bit suboptimal and there might be better ways to do this?\nWould be curious on your opinions, I guess contract diffing could be a quite useful feature on foundry itself as i guess everyone working with proxies would be interested in this.\n@sakulstra this was initially addressed in scope of https://github.com/foundry-rs/compilers/pull/52 to resolve https://github.com/foundry-rs/foundry/issues/6539\n\nSorting contracs alphabetically is not feasible because of the `base has to precede definition of derived contract` error. When working on this I was mostly optimizing for diff readability which I think is good enough now as order of files is pretty much deterministic as long as imported fileds not change. \n\n> The realworld problem we have is for example [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/1/0x80f2c02224a2E548FC67c0bF705eBFA825dd5439.sol) vs [here](https://github.com/bgd-labs/collector-upgrade-rev6/blob/main/flattened/Collector.sol).\n> \n> In the first `Address` comes first, in the second `Initializable` comes first. Both i think don't have dependencies.\n\nThis is happening because in the second file, `Initializable.sol` is coming from `lib/aave-v3-origin/..` which is lexicographically higher than `Address.sol` path\n> This is happening because in the second file, Initializable.sol is coming from lib/aave-v3-origin/.. which is lexicographically higher than Address.sol path\n\nSorry, i don't see it \ud83d\ude05 \n\nHow is Initializable lexicographically higher than Address? Or is the path considered for the sorting... which would be weird, no?\nAs in this case changing from e.g. submodules to soldeer or npm would completely change the output i guess?\n> Or is the path considered for the sorting... which would be weird, no?\n\nyeah path is considered right now\n\n> As in this case changing from e.g. submodules to soldeer or npm would completely change the output i guess?\n\nyeah tbh I'd say this is an edge case I just didn't consider at the time :) I guess we can use path as secondary sorting key, should be fixed in https://github.com/foundry-rs/compilers/pull/247\ngoing to reopen as a placeholder for bumping compiler version\nsending patch @grandizzy \npatch is out", "created_at": "2025-02-06 11:35:47", "merge_commit_sha": "1d5fa644df2dd6b141db15bed37d42f8fb7600b3", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test integration (3/3) (x86_64-pc-windows-msvc)', '.github/workflows/test.yml']", "['test integration (1/3) (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']"], ["['doctest', '.github/workflows/test.yml']", "['test unit (x86_64-pc-windows-msvc)', '.github/workflows/test.yml']"], ["['issue-repros (2/2) (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']", "['external (1/2) (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']"], ["['test unit (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']", "['test integration (3/3) (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']"], ["['clippy', '.github/workflows/test.yml']", "['test integration (2/3) (x86_64-unknown-linux-gnu)', '.github/workflows/test.yml']"], ["['codespell', '.github/workflows/test.yml']", "['test integration (1/3) (x86_64-pc-windows-msvc)', '.github/workflows/test.yml']"]]}
{"repo": "diesel-rs/diesel", "instance_id": "diesel-rs__diesel-4076", "base_commit": "2bc0ad6d8c474fd873227bcf8c68ae82d9fe3cd2", "patch": "diff --git a/diesel/src/expression/operators.rs b/diesel/src/expression/operators.rs\nindex 836389908ae6..5cc9e5db83d5 100644\n--- a/diesel/src/expression/operators.rs\n+++ b/diesel/src/expression/operators.rs\n@@ -540,12 +540,12 @@ postfix_operator!(IsNull, \" IS NULL\");\n postfix_operator!(IsNotNull, \" IS NOT NULL\");\n postfix_operator!(\n     Asc,\n-    \" ASC \",\n+    \" ASC\",\n     crate::expression::expression_types::NotSelectable\n );\n postfix_operator!(\n     Desc,\n-    \" DESC \",\n+    \" DESC\",\n     crate::expression::expression_types::NotSelectable\n );\n \ndiff --git a/examples/mysql/all_about_inserts/src/lib.rs b/examples/mysql/all_about_inserts/src/lib.rs\nindex 39ba19f7cd89..e5e44cd95353 100644\n--- a/examples/mysql/all_about_inserts/src/lib.rs\n+++ b/examples/mysql/all_about_inserts/src/lib.rs\n@@ -320,7 +320,7 @@ fn examine_sql_from_insert_get_results_batch() {\n                     `users`.`hair_color`, `users`.`created_at`, \\\n                     `users`.`updated_at` \\\n                     FROM `users` \\\n-                    ORDER BY `users`.`id` DESC  \\\n+                    ORDER BY `users`.`id` DESC \\\n                     -- binds: []\";\n     assert_eq!(load_sql, debug_query::<Mysql, _>(&load_query).to_string());\n }\n@@ -372,7 +372,7 @@ fn examine_sql_from_insert_get_result() {\n                     `users`.`hair_color`, `users`.`created_at`, \\\n                     `users`.`updated_at` \\\n                     FROM `users` \\\n-                    ORDER BY `users`.`id` DESC  \\\n+                    ORDER BY `users`.`id` DESC \\\n                     -- binds: []\";\n     assert_eq!(load_sql, debug_query::<Mysql, _>(&load_query).to_string());\n }\n@@ -399,7 +399,7 @@ fn examine_sql_from_explicit_returning() {\n         debug_query::<Mysql, _>(&insert_query).to_string()\n     );\n     let load_query = users.select(id).order(id.desc());\n-    let load_sql = \"SELECT `users`.`id` FROM `users` ORDER BY `users`.`id` DESC  -- binds: []\";\n+    let load_sql = \"SELECT `users`.`id` FROM `users` ORDER BY `users`.`id` DESC -- binds: []\";\n     assert_eq!(load_sql, debug_query::<Mysql, _>(&load_query).to_string());\n }\n \n", "test_patch": "", "problem_statement": "ASC/DESC postfix operators include trailing whitespace\n## Setup\r\n\r\n### Versions\r\n\r\n- **Rust:** 1.79.0\r\n- **Diesel:** master\r\n- **Database:** SQLite, MySQL, Postgres\r\n- **Operating System**: macOS\r\n\r\n### Feature Flags\r\n\r\n- **diesel:** sqlite,mysql_backend,postgres_backend\r\n\r\n## Problem Description\r\n\r\nThe generated queries for `ORDER BY` have `ASC` and `DESC` clauses with additional whitespace. This applies to the SQLite, MySQL, and Postgres backends.\r\n\r\n### What is the expected output?\r\n\r\n```\r\nSQLite:\r\nSELECT `users`.`id`, `users`.`name` FROM `users` ORDER BY `users`.`name` ASC, `users`.`id` DESC -- binds: []\r\nMySQL:\r\nSELECT `users`.`id`, `users`.`name` FROM `users` ORDER BY `users`.`name` ASC, `users`.`id` DESC -- binds: []\r\nPostgres:\r\nSELECT \"users\".\"id\", \"users\".\"name\" FROM \"users\" ORDER BY \"users\".\"name\" ASC, \"users\".\"id\" DESC -- binds: []\r\n```\r\n\r\n### What is the actual output?\r\n\r\n```\r\nSQLite:\r\nSELECT `users`.`id`, `users`.`name` FROM `users` ORDER BY `users`.`name` ASC , `users`.`id` DESC  -- binds: []\r\nMySQL:\r\nSELECT `users`.`id`, `users`.`name` FROM `users` ORDER BY `users`.`name` ASC , `users`.`id` DESC  -- binds: []\r\nPostgres:\r\nSELECT \"users\".\"id\", \"users\".\"name\" FROM \"users\" ORDER BY \"users\".\"name\" ASC , \"users\".\"id\" DESC  -- binds: []\r\n```\r\n\r\n### Are you seeing any additional errors?\r\n\r\nNo.\r\n\r\n### Steps to reproduce\r\n\r\nHere is a full example that reproduces the output above:\r\n\r\n```rust\r\nuse diesel::{\r\n    backend::Backend, debug_query, mysql::Mysql, pg::Pg, prelude::*, query_builder::QueryFragment,\r\n    sqlite::Sqlite,\r\n};\r\n\r\ntable! {\r\n    users {\r\n        id -> Integer,\r\n        name -> VarChar,\r\n    }\r\n}\r\n\r\nfn main() {\r\n    use self::users::dsl::*;\r\n\r\n    let query = users.select((id, name)).order((name.asc(), id.desc()));\r\n\r\n    println!(\"SQLite:\\n{}\", debug_query::<Sqlite, _>(&query));\r\n    println!(\"MySQL:\\n{}\", debug_query::<Mysql, _>(&query));\r\n    println!(\"Postgres:\\n{}\", debug_query::<Pg, _>(&query));\r\n}\r\n```\r\n\r\n## Checklist\r\n\r\n- [x] I have already looked over the [issue tracker](https://github.com/diesel-rs/diesel/issues) and the [discussion forum](https://github.com/diesel-rs/diesel/discussions) for similar possible closed issues.\r\n- [x] This issue can be reproduced on Rust's stable channel. (Your issue will be\r\n  closed if this is not the case)\r\n- [x] This issue can be reproduced without requiring a third party crate\n", "hints_text": "", "created_at": "2024-06-19 13:36:21", "merge_commit_sha": "e9093a2a5ecb990a461b998ff2e1fca915c80a89", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check (nightly, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check sqlite bundled + Sqlite with asan', '.github/workflows/ci.yml']"], ["['Check (beta, postgres, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, postgres, windows-2019)', '.github/workflows/ci.yml']"], ["['Check mysql bundled + Mysql with asan', '.github/workflows/ci.yml']", "['Check (stable, mysql, windows-2019)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (stable, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check rustfmt style && run clippy', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, windows-2019)', '.github/workflows/ci.yml']", "['Check (nightly, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (beta, mysql, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Check (stable, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-13)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (beta, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Compiletests', '.github/workflows/ci.yml']", "['Check (beta, mysql, macos-14)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-14)', '.github/workflows/ci.yml']"]]}
{"repo": "diesel-rs/diesel", "instance_id": "diesel-rs__diesel-4151", "base_commit": "029a8d41b85739803272ce6aca8460a42f11f74e", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 6b3435b9b926..53e7eb9026e6 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -13,6 +13,7 @@ Increasing the minimal supported Rust version will always be coupled at least wi\n ### Added\n \n * Support for libsqlite3-sys 0.29.0\n+* Add support for built-in PostgreSQL range operators and functions\n * Support for postgres multirange type\n \n ## [2.2.0] 2024-05-31\ndiff --git a/diesel/src/pg/expression/expression_methods.rs b/diesel/src/pg/expression/expression_methods.rs\nindex 76136d3563d2..5aaca4773a15 100644\n--- a/diesel/src/pg/expression/expression_methods.rs\n+++ b/diesel/src/pg/expression/expression_methods.rs\n@@ -11,7 +11,7 @@ use crate::expression::grouped::Grouped;\n use crate::expression::operators::{Asc, Concat, Desc, Like, NotLike};\n use crate::expression::{AsExpression, Expression, IntoSql, TypedExpressionType};\n use crate::pg::expression::expression_methods::private::BinaryOrNullableBinary;\n-use crate::sql_types::{Array, Inet, Integer, SqlType, Text, VarChar};\n+use crate::sql_types::{Array, Inet, Integer, Range, SqlType, Text, VarChar};\n use crate::EscapeExpressionMethods;\n \n /// PostgreSQL specific methods which are present on all expressions.\n@@ -72,6 +72,89 @@ pub trait PgExpressionMethods: Expression + Sized {\n     {\n         Grouped(IsDistinctFrom::new(self, other.as_expression()))\n     }\n+\n+    /// Creates a PostgreSQL `<@` expression.\n+    ///\n+    /// This operator returns true whether a element is contained by a range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:     |\n+    /// other:  [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  |\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       |\n+    /// other:  [----]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:           |\n+    /// other:  [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  |\n+    /// other:   [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       |\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use std::collections::Bound;\n+    /// #     use diesel::dsl::int4range;\n+    /// #     use diesel::sql_types::Integer;\n+    /// #\n+    /// #     let conn = &mut establish_connection();\n+    /// #\n+    ///       let my_range = int4range(1, 5, diesel::sql_types::RangeBound::LowerBoundInclusiveUpperBoundExclusive);\n+    ///\n+    ///       let (first, second) = diesel::select((\n+    ///           4.into_sql::<Integer>().is_contained_by_range(my_range),\n+    ///           10.into_sql::<Integer>().is_contained_by_range(my_range)\n+    ///       )).get_result::<(bool, bool)>(conn)?;\n+    ///\n+    ///       assert_eq!(first, true);\n+    ///       assert_eq!(second, false);\n+    /// #\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    #[allow(clippy::wrong_self_convention)] // This is named after the sql operator\n+    fn is_contained_by_range<T>(self, other: T) -> dsl::IsContainedByRange<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Range<Self::SqlType>>,\n+    {\n+        Grouped(IsContainedBy::new(self, other.as_expression()))\n+    }\n }\n \n impl<T: Expression> PgExpressionMethods for T {}\n@@ -706,7 +789,41 @@ impl<T, U> EscapeExpressionMethods for Grouped<NotSimilarTo<T, U>> {\n pub trait PgRangeExpressionMethods: Expression + Sized {\n     /// Creates a PostgreSQL `@>` expression.\n     ///\n-    /// This operator returns whether a range contains an specific element\n+    /// This operator returns true whether a range contains an specific element\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:    |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other: |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:      |\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:          |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [----]\n+    /// other: |\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:      |\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -758,7 +875,36 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `@>` expression.\n     ///\n-    /// This operator returns whether a range contains an specific element\n+    /// This operator returns true whether a range contains another range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-------]\n+    /// other:     [--]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: [------]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:     [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [----]\n+    /// other: [--------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other: [----]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -809,7 +955,36 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `<@` expression.\n     ///\n-    /// This operator returns whether a range is contained by a specific element\n+    /// This operator returns true whether a range is contained by another range\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other: [-------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [------]\n+    /// other:   [---]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other: [-----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:   [----]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -861,7 +1036,46 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `&&` expression.\n     ///\n-    /// This operator returns whether two ranges overlap.\n+    /// This operator returns true whether two ranges overlap.\n+    ///\n+    /// This operator evaluates to true for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:    [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [----]\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [----]\n+    /// other:  [-------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other:  [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:   [----]\n+    /// other:  [----)\n+    /// ```\n+    ///\n+    /// This operator evaluates to false for the following cases:\n+    ///\n+    /// ```text\n+    /// self:   [-----]\n+    /// other:          [-----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       [----]\n+    /// other: [--]\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -916,7 +1130,44 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n \n     /// Creates a PostgreSQL `&<` expression.\n     ///\n-    /// This operator returns whether a range doesn't extend to the right of another\n+    /// This operator returns true whether the argument range extend to the right of the current range\n+    ///\n+    /// Postgresql defines \"extends\" as does not have a lower bound smaller than the lower bound of the\n+    /// self range. That means the right hand side range can overlap parts of the left hand side\n+    /// range or be on the right side of the left hand side range\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:    [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:         [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------)\n+    /// other:    [------)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:             [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:            [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: [------)\n+    /// ```\n     ///\n     /// # Example\n     ///\n@@ -945,30 +1196,150 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///     .values(versions.eq((Bound::Included(1), Bound::Excluded(20))))\n     ///     .execute(conn)?;\n     ///\n-    /// let cool_posts = posts.select(id)\n-    ///     .filter(versions.range_not_extends_right_to((Bound::Included(18), Bound::Excluded(20))))\n-    ///     .load::<i32>(conn)?;\n-    /// assert_eq!(vec![1], cool_posts);\n+    /// let cool_posts = posts.select(versions.range_extends_right_to((Bound::Included(18), Bound::Excluded(20))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n     ///\n-    /// let amazing_posts = posts.select(id)\n-    ///     .filter(versions.contains_range((Bound::Included(20), Bound::Excluded(25))))\n-    ///     .load::<i32>(conn)?;\n-    /// assert!(amazing_posts.is_empty());\n+    /// let cool_posts = posts.select(versions.range_extends_right_to((Bound::Included(25), Bound::Excluded(30))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let amazing_posts = posts.select(versions.range_extends_right_to((Bound::Included(-10), Bound::Excluded(0))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(false, amazing_posts);\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    fn range_extends_right_to<T>(self, other: T) -> dsl::RangeExtendsRightTo<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Self::SqlType>,\n+    {\n+        Grouped(ExtendsRightTo::new(self, other.as_expression()))\n+    }\n+\n+    /// Creates a PostgreSQL `&>` expression.\n+    ///\n+    /// This operator returns true whether a range does extend to the left of another\n+    ///\n+    /// Postgresql defines \"extends\" as does not have a upper bound greater than the upper bound of the\n+    /// self range. That means the right hand side range can overlap parts of the left hand side\n+    /// range or be on the left side of the left hand side range\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:        [------)\n+    /// other:    [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:          [----)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------)\n+    /// other:        [------)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:   [--------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:          [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other: (------]\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use self::posts::dsl::*;\n+    /// #     use std::collections::Bound;\n+    /// #     let conn = &mut establish_connection();\n+    /// #     diesel::sql_query(\"DROP TABLE IF EXISTS posts\").execute(conn).unwrap();\n+    /// #     diesel::sql_query(\"CREATE TABLE posts (id SERIAL PRIMARY KEY, versions INT4RANGE NOT NULL)\").execute(conn).unwrap();\n+    /// #\n+    /// diesel::insert_into(posts)\n+    ///     .values(versions.eq((Bound::Included(1), Bound::Excluded(20))))\n+    ///     .execute(conn)?;\n+    ///\n+    /// let cool_posts = posts.select(versions.range_extends_left_to((Bound::Included(-10), Bound::Excluded(5))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let cool_posts = posts.select(versions.range_extends_left_to((Bound::Included(-10), Bound::Excluded(-5))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(true, cool_posts);\n+    ///\n+    /// let amazing_posts = posts.select(versions.range_extends_left_to((Bound::Included(25), Bound::Excluded(30))))\n+    ///     .get_result::<bool>(conn)?;\n+    /// assert_eq!(false, amazing_posts);\n     /// #     Ok(())\n     /// # }\n     /// ```\n-    fn range_not_extends_right_to<T>(self, other: T) -> dsl::RangeNotExtendsRightTo<Self, T>\n+    fn range_extends_left_to<T>(self, other: T) -> dsl::RangeExtendsLeftTo<Self, T>\n     where\n         Self::SqlType: SqlType,\n         T: AsExpression<Self::SqlType>,\n     {\n-        Grouped(NotExtendsRightTo::new(self, other.as_expression()))\n+        Grouped(ExtendsLeftTo::new(self, other.as_expression()))\n     }\n \n     /// Creates a PostgreSQL `<<` expression.\n     ///\n     /// Is the first range strictly left of the second?\n     ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:            [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----)\n+    /// other:      [----)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:             [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:        [------)\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1024,6 +1395,34 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///\n     /// Is the first range strictly right of the second?\n     ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:          [------)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:        [----)\n+    /// other:  [----)\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:          [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:         [------]\n+    /// other: [------]\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1075,10 +1474,120 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n         Grouped(ContainsNet::new(self, other.as_expression()))\n     }\n \n+    /// Creates a PostgreSQL ` -|- ` expression.\n+    ///\n+    /// This operator evaluates to true if the two ranges are adjacent\n+    ///\n+    /// The following constelations evaluate to true:\n+    /// ```text\n+    /// self:   [------)\n+    /// other:         [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:       [----)\n+    /// other: [----)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:        [----)\n+    /// other: [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [----]\n+    /// other:       [----]\n+    /// ```\n+    ///\n+    /// The following constelations evaluate to false:\n+    ///\n+    /// ```text\n+    /// self:        [------]\n+    /// other:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:     [------]\n+    /// other:         [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:        [------]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:  [------]\n+    /// other:           [------]\n+    /// ```\n+    ///\n+    /// # Example\n+    ///\n+    /// ```rust\n+    /// # include!(\"../../doctest_setup.rs\");\n+    /// #\n+    /// # table! {\n+    /// #     posts {\n+    /// #         id -> Integer,\n+    /// #         versions -> Range<Integer>,\n+    /// #     }\n+    /// # }\n+    /// #\n+    /// # fn main() {\n+    /// #     run_test().unwrap();\n+    /// # }\n+    /// #\n+    /// # fn run_test() -> QueryResult<()> {\n+    /// #     use self::posts::dsl::*;\n+    /// #     use std::collections::Bound;\n+    /// #     let conn = &mut establish_connection();\n+    /// #     diesel::sql_query(\"DROP TABLE IF EXISTS posts\").execute(conn).unwrap();\n+    /// #     diesel::sql_query(\"CREATE TABLE posts (id SERIAL PRIMARY KEY, versions INT4RANGE NOT NULL)\").execute(conn).unwrap();\n+    /// #\n+    /// diesel::insert_into(posts)\n+    ///     .values(&vec![\n+    ///         (versions.eq((Bound::Included(1), Bound::Excluded(2)))),\n+    ///         (versions.eq((Bound::Included(4), Bound::Excluded(7))))\n+    ///     ])\n+    ///     .execute(conn)?;\n+    ///\n+    /// let data = posts.select(versions.range_adjacent((Bound::Included(2),Bound::Included(6))))\n+    ///     .load::<bool>(conn)?;\n+    /// let expected = vec![true, false];\n+    /// assert_eq!(expected, data);\n+    /// #     Ok(())\n+    /// # }\n+    /// ```\n+    fn range_adjacent<T>(self, other: T) -> dsl::RangeAdjacent<Self, T>\n+    where\n+        Self::SqlType: SqlType,\n+        T: AsExpression<Self::SqlType>,\n+    {\n+        Grouped(RangeAdjacent::new(self, other.as_expression()))\n+    }\n+\n     /// Creates a PostgreSQL ` + ` expression.\n     ///\n     /// This operator unions two ranges and returns the union.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result: [--------)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:          [----)\n+    /// other: [----)\n+    /// result: error\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other: [----]\n+    /// result [--------)\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1130,6 +1639,24 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     /// The second range must not be contained in the first in such a way that the\n     /// difference would not be a single range.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result: [---)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other:  [----)\n+    /// result:      [--)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [--------)\n+    /// other:       [----]\n+    /// result: error\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -1178,6 +1705,30 @@ pub trait PgRangeExpressionMethods: Expression + Sized {\n     ///\n     /// This operator takes two ranges and returns the intersection.\n     ///\n+    /// ```text\n+    /// self:   [------)\n+    /// other:      [----)\n+    /// result:     [--)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:      [----)\n+    /// other:  [----)\n+    /// result:    [-)\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [--------)\n+    /// other:     [----]\n+    /// result:    [----]\n+    /// ```\n+    ///\n+    /// ```text\n+    /// self:    [--------)\n+    /// other:               [----]\n+    /// result: empty range\n+    /// ```\n+    ///\n     /// # Example\n     ///\n     /// ```rust\ndiff --git a/diesel/src/pg/expression/functions.rs b/diesel/src/pg/expression/functions.rs\nindex 313958927be8..0b7bfa9eb989 100644\n--- a/diesel/src/pg/expression/functions.rs\n+++ b/diesel/src/pg/expression/functions.rs\n@@ -66,8 +66,10 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the lower bound of the range.\n-    /// if the range is empty or has no lower bound, it returns NULL.\n+    /// Returns the lower bound of the range\n+    ///\n+    /// If the range is empty or has no lower bound, it returns NULL.\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -109,8 +111,10 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the upper bound of the range.\n-    /// if the range is empty or has no upper bound, it returns NULL.\n+    /// Returns the upper bound of the range\n+    ///\n+    /// If the range is empty or has no upper bound, it returns NULL.\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -152,7 +156,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range is empty.\n+    /// Returns true if the range is empty\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -194,7 +199,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's lower bound is inclusive.\n+    /// Returns true if the range's lower bound is inclusive\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -236,7 +242,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's upper bound is inclusive.\n+    /// Returns true if the range's upper bound is inclusive\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -277,7 +284,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's lower bound is unbounded.\n+    /// Returns true if the range's lower bound is unbounded\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -319,7 +327,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns true if the range's upper bound is unbounded.\n+    /// Returns true if the range's upper bound is unbounded\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -361,7 +370,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns the smallest range which includes both of the given ranges.\n+    /// Returns the smallest range which includes both of the given ranges\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -400,11 +410,12 @@ define_sql_function! {\n     /// # }\n     /// ```\n     #[cfg(feature = \"postgres_backend\")]\n-    fn range_merge<T: RangeHelper>(lhs: T, rhs: T) -> Range<<T as RangeHelper>::Inner>;\n+    fn range_merge<T1: RangeHelper, T2: RangeHelper<Inner = T1::Inner>>(lhs: T1, rhs: T2) -> Range<T1::Inner>;\n }\n \n define_sql_function! {\n-    /// Returns range of integer.\n+    /// Returns range of integer\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -451,7 +462,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of integer.\n+    /// Returns range of big ints\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -498,7 +510,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of number.\n+    /// Returns range of numeric values\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -548,7 +561,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of timestamp without timezone.\n+    /// Returns range of timestamps without timezone\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -598,7 +612,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of timestamp with timezone.\n+    /// Returns range of timestamps with timezone\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -646,7 +661,8 @@ define_sql_function! {\n }\n \n define_sql_function! {\n-    /// Returns range of date.\n+    /// Returns range of dates\n+    ///\n     /// # Example\n     ///\n     /// ```rust\n@@ -697,7 +713,8 @@ define_sql_function! {\n \n #[cfg(feature = \"postgres_backend\")]\n define_sql_function! {\n-    /// Append an element to the end of an array.\n+    /// Append an element to the end of an array\n+    ///\n     /// # Example\n     ///\n     /// ```rust\ndiff --git a/diesel/src/pg/expression/helper_types.rs b/diesel/src/pg/expression/helper_types.rs\nindex b38e4bc9a68a..5bcaadfc3e98 100644\n--- a/diesel/src/pg/expression/helper_types.rs\n+++ b/diesel/src/pg/expression/helper_types.rs\n@@ -60,11 +60,17 @@ pub type RangeContains<Lhs, Rhs> = Grouped<\n     >,\n >;\n \n-/// The return type of [`lhs.range_not_extends_right_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_not_extends_right_to)\n+/// The return type of [`lhs.range_extends_right_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_extends_right_to)\n /// for range expressions\n #[cfg(feature = \"postgres_backend\")]\n-pub type RangeNotExtendsRightTo<Lhs, Rhs> =\n-    Grouped<super::operators::NotExtendsRightTo<Lhs, AsExpr<Rhs, Lhs>>>;\n+pub type RangeExtendsRightTo<Lhs, Rhs> =\n+    Grouped<super::operators::ExtendsRightTo<Lhs, AsExpr<Rhs, Lhs>>>;\n+\n+/// The return type of [`lhs.range_extends_left_to(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_extends_left_to)\n+/// for range expressions\n+#[cfg(feature = \"postgres_backend\")]\n+pub type RangeExtendsLeftTo<Lhs, Rhs> =\n+    Grouped<super::operators::ExtendsLeftTo<Lhs, AsExpr<Rhs, Lhs>>>;\n \n /// The return type of [`lhs.contains_range(rhs)`](super::expression_methods::PgRangeExpressionMethods::contains_range)\n /// for range expressions\n@@ -76,6 +82,12 @@ pub type ContainsRange<Lhs, Rhs> = Contains<Lhs, Rhs>;\n #[cfg(feature = \"postgres_backend\")]\n pub type IsContainedBy<Lhs, Rhs> = Grouped<super::operators::IsContainedBy<Lhs, AsExpr<Rhs, Lhs>>>;\n \n+/// The return type of [`lhs.is_contained_by_range(rhs)`](super::expression_methods::PgExpressionMethods::is_contained_by_range)\n+#[cfg(feature = \"postgres_backend\")]\n+pub type IsContainedByRange<Lhs, Rhs> = Grouped<\n+    super::operators::IsContainedBy<Lhs, AsExprOf<Rhs, diesel::sql_types::Range<SqlTypeOf<Lhs>>>>,\n+>;\n+\n /// The return type of [`lhs.range_is_contained_by(rhs)`](super::expression_methods::PgRangeExpressionMethods::lesser_than)\n #[cfg(feature = \"postgres_backend\")]\n pub type LesserThanRange<Lhs, Rhs> =\n@@ -102,6 +114,10 @@ pub type Difference<Lhs, Rhs> = Grouped<super::operators::DifferenceRange<Lhs, A\n #[doc(hidden)] // used by `#[auto_type]`\n pub type DifferenceRange<Lhs, Rhs> = Difference<Lhs, Rhs>;\n \n+/// The return type of [`lhs.range_adjacent(rhs)`](super::expression_methods::PgRangeExpressionMethods::range_adjacent)\n+#[cfg(feature = \"postgres_backend\")]\n+pub type RangeAdjacent<Lhs, Rhs> = Grouped<super::operators::RangeAdjacent<Lhs, AsExpr<Rhs, Lhs>>>;\n+\n /// The return type of [`lhs.intersection_range(rhs)`](super::expression_methods::PgRangeExpressionMethods::intersection_range)\n #[cfg(feature = \"postgres_backend\")]\n pub type Intersection<Lhs, Rhs> =\n@@ -291,3 +307,48 @@ pub type NotLikeBinary<Lhs, Rhs> = crate::dsl::NotLike<Lhs, Rhs>;\n #[doc(hidden)]\n #[deprecated(note = \"Use `dsl::Concat` instead\")]\n pub type ConcatArray<Lhs, Rhs> = crate::dsl::Concat<Lhs, Rhs>;\n+\n+/// Return type of [`lower(range)`](super::functions::lower())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower<R> = super::functions::lower<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper(range)`](super::functions::upper())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper<R> = super::functions::upper<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`isempty(range)`](super::functions::isempty())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type isempty<R> = super::functions::isempty<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`lower_inc(range)`](super::functions::lower_inc())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower_inc<R> = super::functions::lower_inc<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper_inc(range)`](super::functions::upper_inc())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper_inc<R> = super::functions::upper_inc<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`lower_inf(range)`](super::functions::lower_inf())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type lower_inf<R> = super::functions::lower_inf<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`upper_inf(range)`](super::functions::upper_inf())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type upper_inf<R> = super::functions::upper_inf<SqlTypeOf<R>, R>;\n+\n+/// Return type of [`range_merge(range_a, range_b)`](super::functions::range_merge())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type range_merge<R1, R2> = super::functions::range_merge<SqlTypeOf<R1>, SqlTypeOf<R2>, R1, R2>;\n+\n+/// Return type of [`array_append(array, element)`](super::functions::array_append())\n+#[allow(non_camel_case_types)]\n+#[cfg(feature = \"postgres_backend\")]\n+pub type array_append<A, E> = super::functions::array_append<SqlTypeOf<A>, SqlTypeOf<E>, A, E>;\ndiff --git a/diesel/src/pg/expression/operators.rs b/diesel/src/pg/expression/operators.rs\nindex 0fa057e6b329..dc6100694b98 100644\n--- a/diesel/src/pg/expression/operators.rs\n+++ b/diesel/src/pg/expression/operators.rs\n@@ -15,7 +15,8 @@ infix_operator!(OverlapsWith, \" && \", backend: Pg);\n infix_operator!(Contains, \" @> \", backend: Pg);\n infix_operator!(IsContainedBy, \" <@ \", backend: Pg);\n infix_operator!(ILike, \" ILIKE \", backend: Pg);\n-infix_operator!(NotExtendsRightTo, \" &< \", backend: Pg);\n+infix_operator!(ExtendsRightTo, \" &< \", backend: Pg);\n+infix_operator!(ExtendsLeftTo, \" &> \", backend: Pg);\n infix_operator!(NotILike, \" NOT ILIKE \", backend: Pg);\n infix_operator!(SimilarTo, \" SIMILAR TO \", backend: Pg);\n infix_operator!(NotSimilarTo, \" NOT SIMILAR TO \", backend: Pg);\n@@ -31,6 +32,7 @@ infix_operator!(DifferenceNet, \" - \", Bigint, backend: Pg);\n infix_operator!(HasKeyJsonb, \" ? \", backend: Pg);\n infix_operator!(HasAnyKeyJsonb, \" ?| \", backend: Pg);\n infix_operator!(HasAllKeysJsonb, \" ?& \", backend: Pg);\n+infix_operator!(RangeAdjacent, \" -|- \", backend: Pg);\n infix_operator!(RemoveFromJsonb, \" - \", Jsonb, backend: Pg);\n __diesel_infix_operator!(\n     RetrieveAsObjectJson,\n", "test_patch": "diff --git a/diesel_derives/tests/auto_type.rs b/diesel_derives/tests/auto_type.rs\nindex 223c68dce163..dc2c91c50c11 100644\n--- a/diesel_derives/tests/auto_type.rs\n+++ b/diesel_derives/tests/auto_type.rs\n@@ -11,6 +11,9 @@ table! {\n         id -> Integer,\n         name -> Text,\n         time -> Timestamp,\n+        bigint -> BigInt,\n+        numeric -> Numeric,\n+        date -> Date,\n     }\n }\n \n@@ -46,6 +49,7 @@ table! {\n         blob -> Binary,\n         timestamp -> Timestamp,\n         range -> Range<Integer>,\n+        timestamptz -> Timestamptz,\n     }\n }\n \n@@ -287,7 +291,9 @@ fn test_pg_range_expression_methods() -> _ {\n         .and(pg_extras::range.overlaps_with(my_range))\n         .and(pg_extras::range.lesser_than(my_range))\n         .and(pg_extras::range.greater_than(my_range))\n-        .and(pg_extras::range.range_not_extends_right_to(my_range))\n+        .and(pg_extras::range.range_extends_right_to(my_range))\n+        .and(pg_extras::range.range_extends_left_to(my_range))\n+        .and(pg_extras::id.is_contained_by_range(my_range))\n         .and(\n             pg_extras::range\n                 .union_range(pg_extras::range)\n@@ -309,12 +315,6 @@ fn test_pg_range_expression_methods() -> _ {\n     // function. We could likely support it by\n     // renaming the function to `.range_contains()` (or something similar)\n     // .contains(42_i32)\n-    //.select(\n-    // this kind of free standing functions is not supported by auto_type yet\n-    //lower(pg_extras::range),\n-    // The auto_trait also didn't like this one\n-    //int4range(None, Some(5i32), RangeBound::LowerBoundInclusiveUpperBoundInclusive),\n-    //)\n }\n \n #[cfg(feature = \"postgres\")]\n@@ -386,6 +386,34 @@ fn test_normal_functions() -> _ {\n     ))\n }\n \n+#[cfg(feature = \"postgres\")]\n+#[auto_type]\n+fn postgres_functions() -> _ {\n+    let bound: sql_types::RangeBound =\n+        sql_types::RangeBound::LowerBoundExclusiveUpperBoundExclusive;\n+    (\n+        lower(pg_extras::range),\n+        upper(pg_extras::range),\n+        isempty(pg_extras::range),\n+        lower_inc(pg_extras::range),\n+        upper_inc(pg_extras::range),\n+        lower_inf(pg_extras::range),\n+        upper_inf(pg_extras::range),\n+        range_merge(pg_extras::range, pg_extras::range),\n+        int4range(users::id.nullable(), users::id.nullable(), bound),\n+        int8range(users::bigint.nullable(), users::bigint.nullable(), bound),\n+        numrange(users::numeric.nullable(), users::numeric.nullable(), bound),\n+        daterange(users::date.nullable(), users::date.nullable(), bound),\n+        tsrange(users::time.nullable(), users::time.nullable(), bound),\n+        tstzrange(\n+            pg_extras::timestamptz.nullable(),\n+            pg_extras::timestamptz.nullable(),\n+            bound,\n+        ),\n+        array_append(pg_extras::array, pg_extras::id),\n+    )\n+}\n+\n #[auto_type]\n fn with_lifetime<'a>(name: &'a str) -> _ {\n     users::table.filter(users::name.eq(name))\n", "problem_statement": "Add support for currently unsupported range operators and methods\nDiesel currently supports the postgres range types. We do not provide built-in support for [various operators and methods](https://www.postgresql.org/docs/current/functions-range.html) available for these types. This is a tracking issue for adding support for these operators. \r\n\r\nThe general strategy for adding support for new operators is as following:\r\n\r\n1. Define the operator via [`infix_operator!()`](https://docs.diesel.rs/master/diesel/macro.infix_operator.html). The RangeExtends operator  would be defined as following: `infix_operator!(RangeExtends, \" &> \", backend: Pg);` These operators can be defined [here](https://github.com/diesel-rs/diesel/blob/dd5f41dc93c574077646185393cd064c7103000b/diesel/src/pg/expression/operators.rs). If there is already an existing definition, this step could be skipped.\r\n2. Define a new helper type [here](https://github.com/diesel-rs/diesel/blob/master/diesel/src/pg/expression/helper_types.rs#L30). Helper types are a tool for diesel users to simplify type definitions. Again if there is already a matching type definition this could be skipped. \r\n3. Add a new method to the [`PgRangeExpressionMethods` trait](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/pg/expression/expression_methods.rs#L706-L758). This definition should:\r\n    * Use the previously defined helper type\r\n    * Contain a documentation with an example\r\n4. If possible add a test case for the auto_type attribute [here](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel_derives/tests/auto_type.rs#L277-L288). (If not possible add a documentation entry there)\r\n5. Check if compile test output is up to date by running `TRYBUILD=overwrite cargo test` inside of `diesel_compile_tests`. (This is required if you've added a new operator based on `infix_operator!`\r\n5. Submit a PR with the change\r\n\r\nThe general strategy for adding support for new methods is as following:\r\n\r\n1. Define the operator via [`define_sql_function!()`](https://docs.diesel.rs/master/diesel/expression/functions/macro.define_sql_function.html). The lower method would be defined as following: `define_sql_function!(fn lower<T: RangeHelper>(range: T) -> T::Inner);` These operators can be defined [here](https://github.com/diesel-rs/diesel/blob/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/pg/expression/functions.rs). If there is already an existing definition, this step could be skipped. This function should have a short documentation snippet with en example\r\n6. Check if compile test output is up to date by running `TRYBUILD=overwrite cargo test` inside of `diesel_compile_tests`. (This is required if you've added a new operator based on `infix_operator!`\r\n5. Submit a PR with the change\r\n\r\n\r\n\r\nOperator list:\r\n\r\n- [x] `anyrange @> anyrange -> bool` (Does the first range contain the second?)\r\n- [x] `anyrange @> anyelement -> boolean` (Does the range contain the element?) * \r\n- [x] `anyrange <@ anyrange -> boolean` (Is the first range contained by the second?)  *\r\n- [x] `anyelement <@ anyrange -> boolean` (Is the element contained in the range?) *\r\n- [x] `anyrange && anyrange -> boolean` (Do the ranges overlap, that is, have any elements in common?) *\r\n- [x] `anyrange << anyrange -> boolean` (Is the first range strictly left of the second?) *\r\n- [x] `anyrange >> anyrange -> boolean` (Is the first range strictly right of the second?) *\r\n- [x] `anyrange &< anyrange -> boolean` (Does the first range not extend to the right of the second?) *\r\n- [x] `anyrange &> anyrange -> boolean` (Does the first range not extend to the left of the second?) *\r\n- [x] `anyrange -|- anyrange -> boolean` (Are the ranges adjacent?) *\r\n- [x] `anyrange + anyrange -> anyrange` (Computes the union of the ranges. The ranges must overlap or be adjacent, so that the union is a single range (but see range_merge()).) **\r\n- [x] `anyrange * anyrange -> anyrange` (Computes the intersection of the ranges.) **\r\n- [x] `anyrange - anyrange -> anyrange` (Computes the difference of the ranges. The second range must not be contained in the first in such a way that the difference would not be a single range.) **\r\n\r\nMethod list:\r\n\r\n- [x] `lower ( anyrange ) -> anyelement` (Extracts the lower bound of the range (NULL if the range is empty or has no lower bound).) *\r\n- [x] `upper ( anyrange ) -> anyelement` (Extracts the upper bound of the range (NULL if the range is empty or has no upper bound).) *\r\n- [x] `isempty ( anyrange ) -> boolean` (Is the range empty?) *\r\n- [x] `lower_inc ( anyrange ) -> boolean` (Is the range's lower bound inclusive?) *\r\n- [x] `upper_inc ( anyrange ) -> boolean` (Is the range's upper bound inclusive?) *\r\n- [x] `lower_inf ( anyrange ) -> boolean` (Does the range have no lower bound? (A lower bound of -Infinity returns false.) *\r\n- [x] `upper_inf ( anyrange ) -> boolean` (Does the range have no upper bound? (An upper bound of Infinity returns false.)) *\r\n- [x] `range_merge ( anyrange, anyrange ) -> anyrange` (Computes the smallest range that includes both of the given ranges.) *\r\n- [x] `int4range(Nullable<Integer>, Nullable<Integer>, RangeBoundEnum) -> int4range` ***\r\n- [x] `int8range(Nullable<BigInteger>, Nullable<BigInteger>, RangeBoundEnum) -> int8range` ***\r\n- [x] `numrange(Nullable<Numeric>, Nullable<Numeric>, RangeBoundEnum) -> numrange` ***\r\n- [x] `tsrange(Nullable<Timestamp>, Nullable<Timestamp>, RangeBoundEnum) -> tsrange` ***\r\n- [x] `tstzrange(Nullable<Timestamptz>, Nullable<Timestamptz>, RangeBoundEnum) -> tstzrange` ***\r\n- [x] `daterange(Nullable<Date>, Nullable<Date>, RangeBoundEnum) -> daterange` ***\r\n\r\n\r\nItems marked with one `*` can follow the instructions directly. For items marked with ** it might be useful to look into how to use the [existing](https://github.com/diesel-rs/diesel/tree/99ef8bbdee614b8eb6560b00c4c1ead88fb0e737/diesel/src/expression/ops) rust side operator implementations as well. For items marked with *** we need to introduce an `enum RangeBoundEnum` type (or similar named) that specifies which bound kinds should be used. That likely requires a custom sql type + implementing the relevant type mapping.\r\n\r\nPlease add a comment to this issue if you plan to work on a specific operator/method.\r\n\r\nIf there is anything unclear about how to add a support for a specific operator/method just ask and we will try to answer your questions.\r\n\n", "hints_text": "I would like to help out with this issue. I haven't worked on diesel before so I'm currently working on building the repo locally\nHi! I'm also interested in helping with this issue. \r\nIt's many small issues so we can split I think, or maybe work together to discover how to do it!\r\n \nHi! I would like to help out with this issue as well. \nI can start working on the `anyrange && anyrange -> boolean`. \nHi! Started working on `anyelement <@ anyrange -> boolean`\n> Hi! Started working on `anyelement <@ anyrange -> boolean`\r\n\r\nOops, It's already implemented.  I see `anyrange &< anyrange -> boolean` isn't yet so I'm working on that instead.\nHi, I can work on `anyrange << anyrange -> boolean` and `anyrange >> anyrange -> boolean` next.\nI can work on `anyrange + anyrange -> anyrange`, `anyrange * anyrange -> anyrange` and`anyrange - anyrange -> anyrange` next.\r\n\r\n\r\n", "created_at": "2024-08-08 06:42:24", "merge_commit_sha": "025282054b9f84e830b6b583d3ab4ff9e44ccafc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check (nightly, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check sqlite bundled + Sqlite with asan', '.github/workflows/ci.yml']"], ["['Check (beta, postgres, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, postgres, windows-2019)', '.github/workflows/ci.yml']"], ["['Check mysql bundled + Mysql with asan', '.github/workflows/ci.yml']", "['Check (stable, mysql, windows-2019)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (stable, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check rustfmt style && run clippy', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, windows-2019)', '.github/workflows/ci.yml']", "['Check (nightly, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (beta, mysql, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Check (stable, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-13)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (beta, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Compiletests', '.github/workflows/ci.yml']", "['Check (beta, mysql, macos-14)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-14)', '.github/workflows/ci.yml']"]]}
{"repo": "diesel-rs/diesel", "instance_id": "diesel-rs__diesel-4132", "base_commit": "4763e7c9c968e20d7dff7e23ba027fd1de30c37d", "patch": "diff --git a/diesel_derives/src/queryable_by_name.rs b/diesel_derives/src/queryable_by_name.rs\nindex 16763059a70b..7848e20ed58c 100644\n--- a/diesel_derives/src/queryable_by_name.rs\n+++ b/diesel_derives/src/queryable_by_name.rs\n@@ -23,12 +23,13 @@ pub fn derive(item: DeriveInput) -> Result<TokenStream> {\n             if f.embed() {\n                 Ok(quote!(<#field_ty as QueryableByName<__DB>>::build(row)?))\n             } else {\n+                let st = sql_type(f, &model)?;\n                 let deserialize_ty = f.ty_for_deserialize();\n                 let name = f.column_name()?;\n                 let name = LitStr::new(&name.to_string(), name.span());\n                 Ok(quote!(\n                    {\n-                       let field = diesel::row::NamedRow::get(row, #name)?;\n+                       let field = diesel::row::NamedRow::get::<#st, #deserialize_ty>(row, #name)?;\n                        <#deserialize_ty as Into<#field_ty>>::into(field)\n                    }\n                 ))\n", "test_patch": "diff --git a/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr b/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\nindex 2dd72fafbb74..f46370568073 100644\n--- a/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\n+++ b/diesel_compile_tests/tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.stderr\n@@ -6,49 +6,6 @@ error: All fields of tuple structs must be annotated with `#[diesel(column_name)\n    |\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n \n-error[E0277]: cannot deserialize a value of the database type `_` as `i32`\n- --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:4:10\n-  |\n-4 | #[derive(QueryableByName)]\n-  |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `i32`\n-  |\n-  = note: double check your type mappings via the documentation of `_`\n-  = help: the following other types implement trait `FromSql<A, DB>`:\n-            <i32 as FromSql<diesel::sql_types::Integer, Mysql>>\n-            <i32 as FromSql<diesel::sql_types::Integer, Pg>>\n-            <i32 as FromSql<diesel::sql_types::Integer, Sqlite>>\n-note: required by a bound in `diesel::row::NamedRow::get`\n- --> $DIESEL/src/row.rs\n-  |\n-  |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-  |        --- required by a bound in this associated function\n-  |     where\n-  |         T: FromSql<ST, DB>;\n-  |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-  = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n-\n-error[E0277]: cannot deserialize a value of the database type `_` as `*const str`\n- --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:4:10\n-  |\n-4 | #[derive(QueryableByName)]\n-  |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `*const str`, which is required by `std::string::String: FromSql<_, __DB>`\n-  |\n-  = note: double check your type mappings via the documentation of `_`\n-  = help: the following other types implement trait `FromSql<A, DB>`:\n-            <*const str as FromSql<diesel::sql_types::Text, Mysql>>\n-            <*const str as FromSql<diesel::sql_types::Text, Pg>>\n-            <*const str as FromSql<diesel::sql_types::Text, Sqlite>>\n-  = note: required for `std::string::String` to implement `FromSql<_, __DB>`\n-note: required by a bound in `diesel::row::NamedRow::get`\n- --> $DIESEL/src/row.rs\n-  |\n-  |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-  |        --- required by a bound in this associated function\n-  |     where\n-  |         T: FromSql<ST, DB>;\n-  |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-  = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\n-\n error[E0433]: failed to resolve: use of undeclared crate or module `foos`\n  --> tests/fail/derive/queryable_by_name_requires_table_name_or_sql_type_annotation.rs:5:8\n   |\ndiff --git a/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr b/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\nindex 5b5592b5bf6e..94c66d44a278 100644\n--- a/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\n+++ b/diesel_compile_tests/tests/fail/derive_deprecated/deprecated_sql_type.stderr\n@@ -59,24 +59,3 @@ error[E0412]: cannot find type `foo` in this scope\n    |\n 27 |     #[sql_type = \"foo\"]\n    |                  ^^^^^ not found in this scope\n-\n-error[E0277]: cannot deserialize a value of the database type `_` as `i32`\n-  --> tests/fail/derive_deprecated/deprecated_sql_type.rs:25:10\n-   |\n-25 | #[derive(QueryableByName)]\n-   |          ^^^^^^^^^^^^^^^ the trait `FromSql<_, __DB>` is not implemented for `i32`\n-   |\n-   = note: double check your type mappings via the documentation of `_`\n-   = help: the following other types implement trait `FromSql<A, DB>`:\n-             <i32 as FromSql<diesel::sql_types::Integer, Mysql>>\n-             <i32 as FromSql<diesel::sql_types::Integer, Pg>>\n-             <i32 as FromSql<diesel::sql_types::Integer, Sqlite>>\n-note: required by a bound in `diesel::row::NamedRow::get`\n-  --> $DIESEL/src/row.rs\n-   |\n-   |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\n-   |        --- required by a bound in this associated function\n-   |     where\n-   |         T: FromSql<ST, DB>;\n-   |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\n-   = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\ndiff --git a/diesel_derives/tests/queryable_by_name.rs b/diesel_derives/tests/queryable_by_name.rs\nindex d865ebba8804..4135935b4b2f 100644\n--- a/diesel_derives/tests/queryable_by_name.rs\n+++ b/diesel_derives/tests/queryable_by_name.rs\n@@ -12,6 +12,15 @@ table! {\n     }\n }\n \n+#[cfg(feature = \"sqlite\")]\n+table! {\n+    multiple_sql_types_for_text {\n+        id -> Integer,\n+        string -> Text,\n+        time -> Timestamp,\n+    }\n+}\n+\n #[test]\n fn named_struct_definition() {\n     #[derive(Debug, Clone, Copy, PartialEq, Eq, QueryableByName)]\n@@ -86,8 +95,50 @@ fn struct_with_path_in_name() {\n     );\n }\n \n-// FIXME: Test usage with renamed columns\n+#[cfg(feature = \"sqlite\")]\n+#[test]\n+fn struct_with_multiple_sql_types_for_text() {\n+    #[derive(Debug, PartialEq, QueryableByName)]\n+    struct MultipleSqlTypesForText {\n+        #[diesel(sql_type = diesel::sql_types::Text)]\n+        string: String,\n+        #[diesel(sql_type = diesel::sql_types::Timestamp)]\n+        time: String,\n+    }\n+\n+    let conn = &mut connection();\n+    let data = sql_query(\"SELECT 'name' AS string, '2024-07-31T21:09:00' AS time\").get_result(conn);\n+    assert_eq!(\n+        Ok(MultipleSqlTypesForText {\n+            string: \"name\".into(),\n+            time: \"2024-07-31T21:09:00\".into()\n+        }),\n+        data\n+    );\n+}\n \n+#[cfg(feature = \"sqlite\")]\n+#[test]\n+fn struct_with_multiple_sql_types_for_text_from_table() {\n+    #[derive(Debug, PartialEq, QueryableByName)]\n+    #[diesel(table_name = multiple_sql_types_for_text)]\n+    struct MultipleSqlTypesForText {\n+        string: String,\n+        time: String,\n+    }\n+\n+    let conn = &mut connection();\n+    let data = sql_query(\"SELECT 'name' AS string, '2024-07-31T21:09:00' AS time\").get_result(conn);\n+    assert_eq!(\n+        Ok(MultipleSqlTypesForText {\n+            string: \"name\".into(),\n+            time: \"2024-07-31T21:09:00\".into()\n+        }),\n+        data\n+    );\n+}\n+\n+// FIXME: Test usage with renamed columns\n #[test]\n fn struct_with_no_table() {\n     #[derive(Debug, Clone, Copy, PartialEq, Eq, QueryableByName)]\n", "problem_statement": "Ambiguous traits in QueryableByName derivation when two fields have same concrete type but different SQL types\n<!--\r\nIf you want to report a bug, we added some points below which help us track down the problem faster.\r\n-->\r\n\r\n## Setup\r\n\r\n### Versions\r\n\r\n- **Rust:** 1.80.0\r\n- **Diesel:** 2.2.2\r\n- **Database:** PostgreSQL\r\n- **Operating System: ** MacOS\r\n\r\n### Feature Flags\r\n\r\n- **diesel:** `postgres`\r\n\r\n## Problem Description\r\n\r\n\r\n### What are you trying to accomplish?\r\n\r\nAttempting to use `QueryableByName` + `sql_query`. In this case, two fields have the same type in the struct, but different SQL types.\r\n\r\nExample:\r\n```rust\r\n#[derive(QueryableByName)]\r\n#[diesel(check_for_backend(Pg))]\r\nstruct StructWithTextAndCitext {\r\n    #[diesel(sql_type = sql_types::Text)]\r\n    name: String,\r\n    #[diesel(sql_type = sql_types::Citext)]\r\n    email: String,\r\n}\r\n```\r\n\r\nAlternative example (this also requires the `serde_json` feature of diesel):\r\n```rust\r\n#[derive(QueryableByName)]\r\n#[diesel(check_for_backend(Pg))]\r\nstruct StructWithJsonAndJsonb {\r\n    #[diesel(sql_type = sql_types::Json)]\r\n    v1: serde_json::Value,\r\n    #[diesel(sql_type = sql_types::Jsonb)]\r\n    v2: serde_json::Value,\r\n}\r\n```\r\n\r\n### What is the expected output?\r\n\r\nThe code compiles, and creates a useful `QueryableByName` trait implementation.\r\n\r\n### What is the actual output?\r\n\r\nIn the first case above, the error is:\r\n\r\n```\r\nerror[E0283]: type annotations needed\r\n   --> src/main.rs:11:10\r\n    |\r\n11  | #[derive(QueryableByName)]\r\n    |          ^^^^^^^^^^^^^^^ cannot infer type\r\n    |\r\n    = note: cannot satisfy `String: FromSql<_, __DB>`\r\n    = help: the following types implement trait `FromSql<A, DB>`:\r\n              <String as FromSql<Citext, Pg>>\r\n              <String as FromSql<ST, DB>>\r\nnote: required by a bound in `diesel::row::NamedRow::get`\r\n   --> /xxx/.cargo/registry/src/index.crates.io-6f17d22bba15001f/diesel-2.2.2/src/row.rs:133:12\r\n    |\r\n131 |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\r\n    |        --- required by a bound in this associated function\r\n132 |     where\r\n133 |         T: FromSql<ST, DB>;\r\n    |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\r\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\r\n```\r\n\r\nIn the second case, the error is:\r\n\r\n```\r\nerror[E0283]: type annotations needed\r\n   --> src/main.rs:11:10\r\n    |\r\n11  | #[derive(QueryableByName)]\r\n    |          ^^^^^^^^^^^^^^^ cannot infer type\r\n    |\r\n    = note: cannot satisfy `Value: FromSql<_, __DB>`\r\n    = help: the following types implement trait `FromSql<A, DB>`:\r\n              <Value as FromSql<Json, Pg>>\r\n              <Value as FromSql<Jsonb, Pg>>\r\nnote: required by a bound in `diesel::row::NamedRow::get`\r\n   --> /xxx/.cargo/registry/src/index.crates.io-6f17d22bba15001f/diesel-2.2.2/src/row.rs:133:12\r\n    |\r\n131 |     fn get<ST, T>(&self, column_name: &str) -> deserialize::Result<T>\r\n    |        --- required by a bound in this associated function\r\n132 |     where\r\n133 |         T: FromSql<ST, DB>;\r\n    |            ^^^^^^^^^^^^^^^ required by this bound in `NamedRow::get`\r\n    = note: this error originates in the derive macro `QueryableByName` (in Nightly builds, run with -Z macro-backtrace for more info)\r\n```\r\n\r\n### Steps to reproduce\r\n\r\nWrite the code above, with `diesel` dependency, and features `postgres` (and `serde_json` in the second case). If added to `src/main.rs`, you should see the above compile errors. \r\n\r\n## Checklist\r\n\r\n- [x] I have already looked over the [issue tracker](https://github.com/diesel-rs/diesel/issues) and the [discussion forum](https://github.com/diesel-rs/diesel/discussions) for similar possible closed issues.\r\n- [x] This issue can be reproduced on Rust's stable channel. (Your issue will be\r\n  closed if this is not the case)\r\n- [x] This issue can be reproduced without requiring a third party crate\n", "hints_text": "", "created_at": "2024-07-26 09:31:51", "merge_commit_sha": "e21dfb95db3bccaaae9a53a8842cc999856f883b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Check (nightly, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check sqlite bundled + Sqlite with asan', '.github/workflows/ci.yml']"], ["['Check (beta, postgres, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, postgres, windows-2019)', '.github/workflows/ci.yml']"], ["['Check mysql bundled + Mysql with asan', '.github/workflows/ci.yml']", "['Check (stable, mysql, windows-2019)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (stable, mysql, macos-14)', '.github/workflows/ci.yml']", "['Check rustfmt style && run clippy', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, windows-2019)', '.github/workflows/ci.yml']", "['Check (nightly, mysql, macos-13)', '.github/workflows/ci.yml']"], ["['Check (beta, mysql, ubuntu-latest)', '.github/workflows/ci.yml']", "['Check (stable, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Check (stable, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-13)', '.github/workflows/ci.yml']"], ["['Check (nightly, sqlite, windows-2019)', '.github/workflows/ci.yml']", "['Check (beta, sqlite, macos-14)', '.github/workflows/ci.yml']"], ["['Compiletests', '.github/workflows/ci.yml']", "['Check (beta, mysql, macos-14)', '.github/workflows/ci.yml']"], ["['Check (nightly, postgres, macos-13)', '.github/workflows/ci.yml']", "['Check (stable, postgres, macos-14)', '.github/workflows/ci.yml']"]]}
{"repo": "tensorzero/tensorzero", "instance_id": "tensorzero__tensorzero-1076", "base_commit": "cfa78d01fb2fa825d85adf9164e525bb6e0b8144", "patch": "diff --git a/clients/python-pyo3/src/lib.rs b/clients/python-pyo3/src/lib.rs\nindex c331682f4..3f8f33369 100644\n--- a/clients/python-pyo3/src/lib.rs\n+++ b/clients/python-pyo3/src/lib.rs\n@@ -13,6 +13,7 @@ use std::{collections::HashMap, future::Future, path::PathBuf, sync::Arc, time::\n use futures::StreamExt;\n use pyo3::{\n     exceptions::{PyStopAsyncIteration, PyStopIteration, PyValueError},\n+    ffi::c_str,\n     marker::Ungil,\n     prelude::*,\n     sync::GILOnceCell,\n@@ -413,6 +414,7 @@ impl TensorZeroGateway {\n         config_path: Option<&str>,\n         clickhouse_url: Option<String>,\n     ) -> PyResult<Py<TensorZeroGateway>> {\n+        warn_no_config(cls.py(), config_path)?;\n         let client_fut = ClientBuilder::new(ClientBuilderMode::EmbeddedGateway {\n             config_path: config_path.map(PathBuf::from),\n             clickhouse_url,\n@@ -662,6 +664,7 @@ impl AsyncTensorZeroGateway {\n         config_path: Option<&str>,\n         clickhouse_url: Option<String>,\n     ) -> PyResult<Bound<'a, PyAny>> {\n+        warn_no_config(cls.py(), config_path)?;\n         let client_fut = ClientBuilder::new(ClientBuilderMode::EmbeddedGateway {\n             config_path: config_path.map(PathBuf::from),\n             clickhouse_url,\n@@ -866,3 +869,15 @@ fn tensorzero_internal_error(py: Python<'_>, msg: &str) -> PyResult<PyErr> {\n     })?;\n     Ok(PyErr::from_value(err.bind(py).call1((msg,))?))\n }\n+\n+fn warn_no_config(py: Python<'_>, config: Option<&str>) -> PyResult<()> {\n+    if config.is_none() {\n+        let user_warning = py.get_type::<pyo3::exceptions::PyUserWarning>();\n+        PyErr::warn(\n+            py,\n+            &user_warning,\n+            c_str!(\"No config file provided, so only default functions will be available. Use `config_file=\\\"path/to/tensorzero.toml\\\"` to specify a config file.\"), 0\n+        )?;\n+    }\n+    Ok(())\n+}\ndiff --git a/clients/python-pyo3/uv.lock b/clients/python-pyo3/uv.lock\nindex 66e17641a..dde325977 100644\n--- a/clients/python-pyo3/uv.lock\n+++ b/clients/python-pyo3/uv.lock\n@@ -91,7 +91,6 @@ wheels = [\n \n [[package]]\n name = \"tensorzero\"\n-version = \"0.1.0\"\n source = { editable = \".\" }\n dependencies = [\n     { name = \"httpx\" },\n", "test_patch": "diff --git a/clients/python-pyo3/tests/test_client.py b/clients/python-pyo3/tests/test_client.py\nindex e5ca70bb5..b03f9e024 100644\n--- a/clients/python-pyo3/tests/test_client.py\n+++ b/clients/python-pyo3/tests/test_client.py\n@@ -70,9 +70,10 @@ async def async_client(request):\n \n \n def test_sync_embedded_gateway_no_config():\n+    with pytest.warns(UserWarning, match=\"No config file provided\"):\n+        client = TensorZeroGateway.build_embedded()\n     with pytest.raises(TensorZeroError) as exc_info:\n-        with TensorZeroGateway.build_embedded() as client:\n-            client.inference(function_name=\"my_missing_func\", input={})\n+        client.inference(function_name=\"my_missing_func\", input={})\n \n     assert exc_info.value.status_code == 404\n     assert exc_info.value.text == '{\"error\":\"Unknown function: my_missing_func\"}'\n@@ -80,9 +81,10 @@ def test_sync_embedded_gateway_no_config():\n \n @pytest.mark.asyncio\n async def test_async_embedded_gateway_no_config():\n+    with pytest.warns(UserWarning, match=\"No config file provided\"):\n+        client = await AsyncTensorZeroGateway.build_embedded()\n     with pytest.raises(TensorZeroError) as exc_info:\n-        async with await AsyncTensorZeroGateway.build_embedded() as client:\n-            await client.inference(function_name=\"my_missing_func\", input={})\n+        await client.inference(function_name=\"my_missing_func\", input={})\n \n     assert exc_info.value.status_code == 404\n     assert exc_info.value.text == '{\"error\":\"Unknown function: my_missing_func\"}'\n", "problem_statement": "Log a warning when you call `build_embedded` but don't provide a config\nLike we do with the gateway:\n\n```\n        tracing::warn!(\"No config file provided, so only default functions will be available. Use `--config-file path/to/tensorzero.toml` to specify a config file.\");\n```\n\nSo:\n\n```\nNo config file provided, so only default functions will be available. Use `config_file=\"path/to/tensorzero.toml\"` to specify a config file.\n```\n", "hints_text": "", "created_at": "2025-02-19 14:39:20", "merge_commit_sha": "73bdb3a8f5b08ad974b32735eff0283e579391ff", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['check-pyo3-build (ubuntu-22.04-arm, aarch64)', '.github/workflows/general.yml']", "['check-pyo3-build (macos-13, x86_64)', '.github/workflows/general.yml']"], ["['check-pyo3-build (windows-latest, x64)', '.github/workflows/general.yml']", "['check-docker-compose', '.github/workflows/general.yml']"]]}
{"repo": "nathanbabcock/ffmpeg-sidecar", "instance_id": "nathanbabcock__ffmpeg-sidecar-71", "base_commit": "2272b10ec4582c8fd64fcd76d74d7caf697f2ff4", "patch": "diff --git a/src/child.rs b/src/child.rs\nindex f252ded..a4c3b7d 100644\n--- a/src/child.rs\n+++ b/src/child.rs\n@@ -1,14 +1,12 @@\n //! Wrapper around `std::process::Child` containing a spawned FFmpeg command.\n \n+use crate::iter::FfmpegIterator;\n+use anyhow::Context;\n use std::{\n-  io::{self, Write},\n+  io::{self, copy, sink, Write},\n   process::{Child, ChildStderr, ChildStdin, ChildStdout, ExitStatus},\n };\n \n-use anyhow::Context;\n-\n-use crate::iter::FfmpegIterator;\n-\n /// A wrapper around [`std::process::Child`] containing a spawned FFmpeg command.\n /// Provides interfaces for reading parsed metadata, progress updates, warnings and errors, and\n /// piped output frames if applicable.\n@@ -100,6 +98,12 @@ impl FfmpegChild {\n   ///\n   /// Identical to `wait` in [`std::process::Child`].\n   pub fn wait(&mut self) -> io::Result<ExitStatus> {\n+    // If stderr hasn't already been consumed by a method like `iter()`,\n+    // we need to run it to completion to avoid a deadlock.\n+    if let Some(mut stderr) = self.take_stderr() {\n+      copy(&mut stderr, &mut sink())?;\n+    };\n+\n     self.inner.wait()\n   }\n \n", "test_patch": "diff --git a/src/test.rs b/src/test.rs\nindex 2cd591e..a9ea3f3 100644\n--- a/src/test.rs\n+++ b/src/test.rs\n@@ -40,6 +40,35 @@ fn spawn_with_timeout(command: &mut FfmpegCommand, timeout: u64) -> anyhow::Resu\n   }\r\n }\r\n \r\n+/// Returns `Err` if the timeout thread finishes before the FFmpeg process\r\n+/// Note: this variant leaves behind a hung FFmpeg child process + thread until\r\n+/// the test suite exits.\r\n+fn wait_with_timeout(command: &mut FfmpegCommand, timeout: u64) -> anyhow::Result<()> {\r\n+  let (sender, receiver) = mpsc::channel();\r\n+\r\n+  // Thread 1: Waits for 1000ms and sends a message\r\n+  let timeout_sender = sender.clone();\r\n+  thread::spawn(move || {\r\n+    thread::sleep(Duration::from_millis(timeout));\r\n+    timeout_sender.send(\"timeout\").ok();\r\n+  });\r\n+\r\n+  // Thread 2: Wait for the child to exit in another thread\r\n+  let mut ffmpeg_child = command.spawn()?;\r\n+  thread::spawn(move || {\r\n+    ffmpeg_child.wait().unwrap();\r\n+    sender.send(\"ffmpeg\").ok();\r\n+  });\r\n+\r\n+  // Race the two threads\r\n+  let finished_first = receiver.recv()?;\r\n+  match finished_first {\r\n+    \"timeout\" => anyhow::bail!(\"Timeout thread expired before FFmpeg\"),\r\n+    \"ffmpeg\" => Ok(()),\r\n+    _ => anyhow::bail!(\"Unknown message received\"),\r\n+  }\r\n+}\r\n+\r\n #[test]\r\n fn test_installed() {\r\n   assert!(ffmpeg_is_installed());\r\n@@ -715,3 +744,30 @@ fn test_no_empty_events() -> anyhow::Result<()> {\n \r\n   Ok(())\r\n }\r\n+\r\n+/// This command generates an warning on every frame, e.g.:\r\n+///\r\n+/// ```txt\r\n+/// [Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n+///```\r\n+///\r\n+/// When used in combination with `.wait()`, these error messages can completely\r\n+/// fill the stderr buffer and cause a deadlock. The solution is to\r\n+/// automatically drop the stderr channel when `.wait()` is called.\r\n+///\r\n+/// <https://github.com/nathanbabcock/ffmpeg-sidecar/issues/70>\r\n+#[test]\r\n+fn test_wait() -> anyhow::Result<()> {\r\n+  let mut command = FfmpegCommand::new();\r\n+  command\r\n+    .args(\"-color_primaries 1\".split(' '))\r\n+    .args(\"-color_trc 1\".split(' '))\r\n+    .args(\"-colorspace 1\".split(' '))\r\n+    .format(\"lavfi\")\r\n+    .input(\"yuvtestsrc=size=64x64:rate=60:duration=60\")\r\n+    .args(\"-vf palettegen=max_colors=164\".split(' '))\r\n+    .codec_video(\"gif\")\r\n+    .format(\"null\")\r\n+    .output(\"-\");\r\n+  wait_with_timeout(&mut command, 5000)\r\n+}\r\n", "problem_statement": "FFmpeg's nearly unlimited output causes ffmpeg-sidecar to hang without a timeout\nI'm converting a video(with some strange .ts extension) to a 3-seconds gif using ffmpeg-sidecar. Command like\r\n\r\n```\r\nffmpeg -loglevel level+warning -hide_banner -i \"/path/to/bipbop-gear1-all.ts\" -t 3 -an -vf \"fps=10,scale=-1:200:flags=lanczos,split[s0][s1];[s0]palettegen=max_colors=164:stats_mode=diff[p];[s1][p]paletteuse=dither=floyd_steinberg:diff_mode=rectangle\" -compression_level 9 -quality 85 -c:v gif -y \"/path/to/thumbnail.gif\"\r\n```\r\n\r\nHowever, ffmpeg cli output nearly unlimited lines of warning, like \r\n\r\n```\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n...(tons of lines...)\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n[Parsed_palettegen_4 @ 0x600001574bb0] [warning] The input frame is not in sRGB, colors may be off\r\n```\r\n\r\nbut, ffmpeg cli just **exits(not sure exit 0 or not 0) after about ~7 seconds**, which I think reasonable.\r\n\r\nWhen it comes to ffmpeg-sidecar in Rust\r\n\r\n```\r\n    let exit_status = FfmpegCommand::new()\r\n        .args([\r\n            \"-loglevel\", \"level+warning\",\r\n            \"-hide_banner\",\r\n            \"-i\", filepath,\r\n            \"-t\", \"3\",\r\n            \"-an\",\r\n            \"-vf\", \"fps=10,scale=-1:200:flags=lanczos,split[s0][s1];[s0]palettegen=max_colors=164:stats_mode=diff[p];[s1][p]paletteuse=dither=floyd_steinberg:diff_mode=rectangle\",\r\n            \"-compression_level\", \"9\",\r\n            \"-quality\", \"85\",\r\n            \"-c:v\", \"gif\",\r\n            \"-y\",\r\n            &thumbnail_path_buf.to_string_lossy()\r\n        ])\r\n        .spawn()?\r\n        .wait()?;\r\n\r\n    if exit_status.success() {\r\n        Ok(...)\r\n    } else {\r\n        Err(...)\r\n    }\r\n```\r\n\r\nFfmpegCommand(FfmpegChild maybe) thread just hangs at .wait()?\r\n\r\nMaybe adding timeout parameter would be better? like the ffmpeg cli do internally? Or, I have to set a timer, and invoke handle FfmpegChild.quit/kill(maybe) when timeout, not very convenient.\r\n\r\nYou can download the test video at [http://www.live555.com/liveMedia/public/h264-in-mp2t/bipbop-gear1-all.ts](http://www.live555.com/liveMedia/public/h264-in-mp2t/bipbop-gear1-all.ts)\n", "hints_text": "Thanks for providing a test video, I will take a look.", "created_at": "2024-12-12 21:25:55", "merge_commit_sha": "5fb98043c3680fe3945cdfef6806cc8bc42b4a43", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build', '.github/workflows/mac-m1.yml']", "['build', '.github/workflows/mac.yml']"]]}
{"repo": "m4b/goblin", "instance_id": "m4b__goblin-397", "base_commit": "ff2208954608fc163f54ad0abcf3aafe02fdcdb5", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex ba11f32fb..1b0f1ba3f 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -19,7 +19,7 @@ include = [\n     \"LICENSE\",\n     \"README.md\",\n ]\n-keywords = [\"binary\", \"elf\", \"mach\", \"pe\", \"archive\"]\n+keywords = [\"binary\", \"elf\", \"mach\", \"pe\", \"te\", \"archive\"]\n license = \"MIT\"\n readme = \"README.md\"\n repository = \"https://github.com/m4b/goblin\"\n@@ -38,7 +38,7 @@ version = \"0.12\"\n default_features = false\n \n [features]\n-default = [\"std\", \"elf32\", \"elf64\", \"mach32\", \"mach64\", \"pe32\", \"pe64\", \"archive\", \"endian_fd\"]\n+default = [\"std\", \"elf32\", \"elf64\", \"mach32\", \"mach64\", \"pe32\", \"pe64\", \"te\", \"archive\", \"endian_fd\"]\n std = [\"alloc\", \"scroll/std\"]\n alloc = [\"scroll/derive\", \"log\"]\n endian_fd = [\"alloc\"]\n@@ -49,6 +49,7 @@ mach32 = [\"alloc\", \"endian_fd\", \"archive\"]\n mach64 = [\"alloc\", \"endian_fd\", \"archive\"]\n pe32 = [\"alloc\", \"endian_fd\"]\n pe64 = [\"alloc\", \"endian_fd\"]\n+te = [\"alloc\", \"endian_fd\"]\n archive = [\"alloc\"]\n \n [badges.travis-ci]\ndiff --git a/README.md b/README.md\nindex 30d30f3c9..e4680c223 100644\n--- a/README.md\n+++ b/README.md\n@@ -97,6 +97,7 @@ Here are some things you could do with this crate (or help to implement so they\n * mach32 - 32-bit mach-o `repr(C)` struct defs\n * pe32 - 32-bit PE `repr(C)` struct defs\n * pe64 - 64-bit PE `repr(C)` struct defs\n++ te - Terse Executable (TE) `repr(C)` struct defs\n * archive - a Unix Archive parser\n * endian_fd - parses according to the endianness in the binary\n * std - to allow `no_std` environments\ndiff --git a/src/lib.rs b/src/lib.rs\nindex ec77f93d5..e811de1ea 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -229,6 +229,7 @@ pub enum Hint {\n     Mach(HintData),\n     MachFat(usize),\n     PE,\n+    TE,\n     COFF,\n     Archive,\n     Unknown(u64),\n@@ -236,7 +237,7 @@ pub enum Hint {\n \n macro_rules! if_everything {\n     ($($i:item)*) => ($(\n-        #[cfg(all(feature = \"endian_fd\", feature = \"elf64\", feature = \"elf32\", feature = \"pe64\", feature = \"pe32\", feature = \"mach64\", feature = \"mach32\", feature = \"archive\"))]\n+        #[cfg(all(feature = \"endian_fd\", feature = \"elf64\", feature = \"elf32\", feature = \"pe64\", feature = \"pe32\", feature = \"te\", feature = \"mach64\", feature = \"mach32\", feature = \"archive\"))]\n         $i\n     )*)\n }\n@@ -262,6 +263,7 @@ if_everything! {\n         } else {\n             match *&bytes[0..2].pread_with::<u16>(0, LE)? {\n                 pe::header::DOS_MAGIC => Ok(Hint::PE),\n+                pe::header::TE_MAGIC => Ok(Hint::TE),\n                 pe::header::COFF_MACHINE_X86 |\n                 pe::header::COFF_MACHINE_X86_64 |\n                 pe::header::COFF_MACHINE_ARM64 => Ok(Hint::COFF),\n@@ -290,6 +292,8 @@ if_everything! {\n         Elf(elf::Elf<'a>),\n         /// A PE32/PE32+!\n         PE(pe::PE<'a>),\n+        /// A TE!\n+        TE(pe::TE<'a>),\n         /// A COFF\n         COFF(pe::Coff<'a>),\n         /// A 32/64-bit Mach-o binary _OR_ it is a multi-architecture binary container!\n@@ -309,6 +313,7 @@ if_everything! {\n                     Hint::Mach(_) | Hint::MachFat(_) => Ok(Object::Mach(mach::Mach::parse(bytes)?)),\n                     Hint::Archive => Ok(Object::Archive(archive::Archive::parse(bytes)?)),\n                     Hint::PE => Ok(Object::PE(pe::PE::parse(bytes)?)),\n+                    Hint::TE => Ok(Object::TE(pe::TE::parse(bytes)?)),\n                     Hint::COFF => Ok(Object::COFF(pe::Coff::parse(bytes)?)),\n                     Hint::Unknown(magic) => Ok(Object::Unknown(magic)),\n                 }\ndiff --git a/src/pe/debug.rs b/src/pe/debug.rs\nindex 311bc632c..cae77b4be 100644\n--- a/src/pe/debug.rs\n+++ b/src/pe/debug.rs\n@@ -92,7 +92,7 @@ impl ImageDebugDirectory {\n         )\n     }\n \n-    fn parse_with_opts(\n+    pub(crate) fn parse_with_opts(\n         bytes: &[u8],\n         dd: data_directories::DataDirectory,\n         sections: &[section_table::SectionTable],\ndiff --git a/src/pe/header.rs b/src/pe/header.rs\nindex cb3b08faa..62bfe1906 100644\n--- a/src/pe/header.rs\n+++ b/src/pe/header.rs\n@@ -1,5 +1,5 @@\n use crate::error;\n-use crate::pe::{optional_header, section_table, symbol};\n+use crate::pe::{data_directories, optional_header, section_table, symbol};\n use crate::strtab;\n use alloc::vec::Vec;\n use log::debug;\n@@ -837,6 +837,147 @@ impl ctx::TryIntoCtx<scroll::Endian> for Header {\n     }\n }\n \n+/// The TE header is a reduced PE32/PE32+ header containing only fields\n+/// required for execution in the Platform Initialization\n+/// ([PI](https://uefi.org/specs/PI/1.8/V1_Introduction.html)) architecture.\n+/// The TE header is described in this specification:\n+/// <https://uefi.org/specs/PI/1.8/V1_TE_Image.html#te-header>\n+#[cfg(feature = \"te\")]\n+#[repr(C)]\n+#[derive(Debug, Default, PartialEq, Copy, Clone, Pread, Pwrite)]\n+pub struct TeHeader {\n+    /// Te signature, always [TE_MAGIC]\n+    pub signature: u16,\n+    /// The machine type\n+    pub machine: u16,\n+    /// The number of sections\n+    pub number_of_sections: u8,\n+    /// The subsystem\n+    pub subsystem: u8,\n+    /// the amount of bytes stripped from the header when converting from a\n+    /// PE32/PE32+ header to a TE header. Used to resolve addresses\n+    pub stripped_size: u16,\n+    /// The entry point of the binary\n+    pub entry_point: u32,\n+    /// The base of the code section\n+    pub base_of_code: u32,\n+    /// The image base\n+    pub image_base: u64,\n+    /// The size and address of the relocation directory\n+    pub reloc_dir: data_directories::DataDirectory,\n+    /// The size and address of the debug directory\n+    pub debug_dir: data_directories::DataDirectory,\n+}\n+\n+#[cfg(feature = \"te\")]\n+#[doc(alias(\"IMAGE_TE_SIGNATURE\"))]\n+pub const TE_MAGIC: u16 = 0x5a56;\n+\n+#[cfg(feature = \"te\")]\n+impl TeHeader {\n+    /// Parse the TE header from the given bytes.\n+    pub fn parse(bytes: &[u8], offset: &mut usize) -> error::Result<Self> {\n+        let mut header: TeHeader = bytes.gread_with(offset, scroll::LE)?;\n+        let adj_offset = header.stripped_size as u32 - core::mem::size_of::<TeHeader>() as u32;\n+        header.fixup_header(adj_offset);\n+        Ok(header)\n+    }\n+\n+    /// Parse the sections from the TE header.\n+    pub fn sections(\n+        &self,\n+        bytes: &[u8],\n+        offset: &mut usize,\n+    ) -> error::Result<Vec<section_table::SectionTable>> {\n+        let adj_offset = self.stripped_size as u32 - core::mem::size_of::<TeHeader>() as u32;\n+        let nsections = self.number_of_sections as usize;\n+\n+        // a section table is at least 40 bytes\n+        if nsections > bytes.len() / 40 {\n+            return Err(error::Error::BufferTooShort(nsections, \"sections\"));\n+        }\n+\n+        let mut sections = Vec::with_capacity(nsections);\n+        for i in 0..nsections {\n+            let mut section = section_table::SectionTable::parse(bytes, offset, 0)?;\n+            TeHeader::fixup_section(&mut section, adj_offset);\n+            debug!(\"({}) {:#?}\", i, section);\n+            sections.push(section);\n+        }\n+        Ok(sections)\n+    }\n+\n+    // Adjust addresses in the header to account for the stripped size\n+    fn fixup_header(&mut self, adj_offset: u32) {\n+        debug!(\n+            \"Entry point fixed up from: 0x{:x} to 0x{:X}\",\n+            self.entry_point,\n+            self.entry_point.wrapping_sub(adj_offset)\n+        );\n+        self.entry_point = self.entry_point.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Base of code fixed up from: 0x{:x} to 0x{:X}\",\n+            self.base_of_code,\n+            self.base_of_code.wrapping_sub(adj_offset)\n+        );\n+        self.base_of_code = self.base_of_code.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Relocation Directory fixed up from: 0x{:x} to 0x{:X}\",\n+            self.reloc_dir.virtual_address,\n+            self.reloc_dir.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        self.reloc_dir.virtual_address = self.reloc_dir.virtual_address.wrapping_sub(adj_offset);\n+\n+        debug!(\n+            \"Debug Directory fixed up from: 0x{:x} to 0x{:X}\",\n+            self.debug_dir.virtual_address,\n+            self.debug_dir.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        self.debug_dir.virtual_address = self.debug_dir.virtual_address.wrapping_sub(adj_offset);\n+    }\n+\n+    // Adjust addresses in the section to account for the stripped size\n+    fn fixup_section(section: &mut section_table::SectionTable, adj_offset: u32) {\n+        debug!(\n+            \"Section virtual address fixed up from: 0x{:X} to 0x{:X}\",\n+            section.virtual_address,\n+            section.virtual_address.wrapping_sub(adj_offset)\n+        );\n+        section.virtual_address = section.virtual_address.wrapping_sub(adj_offset);\n+\n+        if section.pointer_to_linenumbers > 0 {\n+            debug!(\n+                \"Section pointer to line numbers fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_linenumbers,\n+                section.pointer_to_linenumbers.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_linenumbers =\n+                section.pointer_to_linenumbers.wrapping_sub(adj_offset);\n+        }\n+\n+        if section.pointer_to_raw_data > 0 {\n+            debug!(\n+                \"Section pointer to raw data fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_raw_data,\n+                section.pointer_to_raw_data.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_raw_data = section.pointer_to_raw_data.wrapping_sub(adj_offset);\n+        }\n+\n+        if section.pointer_to_relocations > 0 {\n+            debug!(\n+                \"Section pointer to relocations fixed up from: 0x{:X} to 0x{:X}\",\n+                section.pointer_to_relocations,\n+                section.pointer_to_relocations.wrapping_sub(adj_offset)\n+            );\n+            section.pointer_to_relocations =\n+                section.pointer_to_relocations.wrapping_sub(adj_offset);\n+        }\n+    }\n+}\n+\n /// Convert machine to str representation. Any case of \"COFF_UNKNOWN\"\n /// should be expected to change to a more specific value.\n pub fn machine_to_str(machine: u16) -> &'static str {\ndiff --git a/src/pe/mod.rs b/src/pe/mod.rs\nindex dd9c2c5df..1f2bac7a9 100644\n--- a/src/pe/mod.rs\n+++ b/src/pe/mod.rs\n@@ -467,6 +467,98 @@ impl<'a> ctx::TryIntoCtx<scroll::Endian> for PE<'a> {\n     }\n }\n \n+/// An analyzed TE binary\n+///\n+/// A TE binary is a PE/PE32+ binary that has had it's header stripped and\n+/// re-formatted to the TE specification. This presents a challenge for\n+/// parsing, as all relative addresses (RVAs) are not updated to take this into\n+/// account, and are thus incorrect. The parsing of a TE must take this into\n+/// account by using the [header::TeHeader::stripped_size`] field of the TE\n+/// header to adjust the RVAs during parsing.\n+#[cfg(feature = \"te\")]\n+#[derive(Debug)]\n+pub struct TE<'a> {\n+    /// The TE header\n+    pub header: header::TeHeader,\n+    /// A list of the sections in this TE binary\n+    pub sections: Vec<section_table::SectionTable>,\n+    /// Debug information, contained in the PE header\n+    pub debug_data: debug::DebugData<'a>,\n+    /// The offset to apply to addresses not parsed by the TE parser\n+    /// itself: [header::TeHeader::stripped_size] - size_of::<[header::TeHeader]>()\n+    pub rva_offset: usize,\n+}\n+\n+#[cfg(feature = \"te\")]\n+impl<'a> TE<'a> {\n+    /// Reads a TE binary from the underlying `bytes`\n+    pub fn parse(bytes: &'a [u8]) -> error::Result<Self> {\n+        let opts = &options::ParseOptions {\n+            resolve_rva: false,\n+            parse_attribute_certificates: false,\n+        };\n+\n+        let mut offset = 0;\n+\n+        // Parse the TE header and adjust the offsets\n+        let header = header::TeHeader::parse(bytes, &mut offset)?;\n+        let rva_offset = header.stripped_size as usize - core::mem::size_of::<header::TeHeader>();\n+\n+        // Parse the sections and adjust the offsets\n+        let sections = header.sections(bytes, &mut offset)?;\n+\n+        // Parse the debug data. Must adjust offsets before parsing the image_debug_directory\n+        let mut debug_data = debug::DebugData::default();\n+        debug_data.image_debug_directory = debug::ImageDebugDirectory::parse_with_opts(\n+            bytes,\n+            header.debug_dir,\n+            &sections,\n+            0,\n+            opts,\n+        )?;\n+        TE::fixup_debug_data(&mut debug_data, rva_offset as u32);\n+        debug_data.codeview_pdb70_debug_info = debug::CodeviewPDB70DebugInfo::parse_with_opts(\n+            bytes,\n+            &debug_data.image_debug_directory,\n+            opts,\n+        )?;\n+\n+        Ok(TE {\n+            header,\n+            sections,\n+            debug_data,\n+            rva_offset,\n+        })\n+    }\n+\n+    /// Adjust all addresses in the TE binary debug data.\n+    fn fixup_debug_data(dd: &mut debug::DebugData, rva_offset: u32) {\n+        debug!(\n+            \"ImageDebugDirectory address of raw data fixed up from: 0x{:X} to 0x{:X}\",\n+            dd.image_debug_directory.address_of_raw_data,\n+            dd.image_debug_directory\n+                .address_of_raw_data\n+                .wrapping_sub(rva_offset),\n+        );\n+        dd.image_debug_directory.address_of_raw_data = dd\n+            .image_debug_directory\n+            .address_of_raw_data\n+            .wrapping_sub(rva_offset);\n+\n+        debug!(\n+            \"ImageDebugDirectory pointer to raw data fixed up from: 0x{:X} to 0x{:X}\",\n+            dd.image_debug_directory.pointer_to_raw_data,\n+            dd.image_debug_directory\n+                .pointer_to_raw_data\n+                .wrapping_sub(rva_offset),\n+        );\n+        dd.image_debug_directory.pointer_to_raw_data = dd\n+            .image_debug_directory\n+            .pointer_to_raw_data\n+            .wrapping_sub(rva_offset);\n+    }\n+}\n+\n /// An analyzed COFF object\n #[derive(Debug)]\n pub struct Coff<'a> {\n", "test_patch": "diff --git a/tests/bins/te/README.md b/tests/bins/te/README.md\nnew file mode 100644\nindex 000000000..c60f26333\n--- /dev/null\n+++ b/tests/bins/te/README.md\n@@ -0,0 +1,24 @@\n+# TE binaries\r\n+\r\n+Binaries located in this directory are precompiled PE32/PE32+ binaries using a\r\n+terse executable (TE) header as defined in the Platform Initialization (PI)\r\n+specification: [TE](https://uefi.org/specs/PI/1.8/V1_TE_Image.html#te-header).\r\n+These binaries were compiled using the\r\n+[EDK2](https://github.com/tianocore/edk2) build system.\r\n+\r\n+## test_image.te\r\n+\r\n+This binary is a simple Terse executable binary\r\n+\r\n+## test_image_loaded.bin\r\n+\r\n+This binary is the same as `test_image.te`, but it has been loaded by a loader,\r\n+meaning the sections have been placed in the expected address. Please note that\r\n+this particular binary has not been relocated, so no relocations have been\r\n+applied\r\n+\r\n+## test_image_relocated.bin\r\n+\r\n+This binary is the same as `test_image.te`, but it has been loaded by a loader,\r\n+meaning the sections have been placed in the expected address, and any any\r\n+relocations have been applied.\r\ndiff --git a/tests/bins/te/test_image.te b/tests/bins/te/test_image.te\nnew file mode 100644\nindex 000000000..8ac7c77b1\nBinary files /dev/null and b/tests/bins/te/test_image.te differ\ndiff --git a/tests/bins/te/test_image_loaded.bin b/tests/bins/te/test_image_loaded.bin\nnew file mode 100644\nindex 000000000..0463f7bf5\nBinary files /dev/null and b/tests/bins/te/test_image_loaded.bin differ\ndiff --git a/tests/bins/te/test_image_relocated.bin b/tests/bins/te/test_image_relocated.bin\nnew file mode 100644\nindex 000000000..123f95d1c\nBinary files /dev/null and b/tests/bins/te/test_image_relocated.bin differ\ndiff --git a/tests/te.rs b/tests/te.rs\nnew file mode 100644\nindex 000000000..347131f26\n--- /dev/null\n+++ b/tests/te.rs\n@@ -0,0 +1,109 @@\n+#[cfg(test)]\r\n+mod te_tests {\r\n+    use goblin::pe;\r\n+    use goblin::pe::header::machine_to_str;\r\n+    use goblin::pe::section_table::*;\r\n+\r\n+    // https://learn.microsoft.com/en-us/windows/win32/debug/pe-format#windows-subsystem\r\n+    const IMAGE_SUBSYSTEM_EFI_BOOT_SERVICE_DRIVER: u8 = 11;\r\n+\r\n+    #[test]\r\n+    fn parse_unloaded_te() {\r\n+        let image = include_bytes!(\"bins/te/test_image.te\");\r\n+        let te = pe::TE::parse(image).expect(\"Failed to parse TE\");\r\n+\r\n+        assert_eq!(machine_to_str(te.header.machine), \"X86_64\");\r\n+        assert_eq!(te.header.number_of_sections, 5);\r\n+        assert_eq!(te.header.subsystem, IMAGE_SUBSYSTEM_EFI_BOOT_SERVICE_DRIVER);\r\n+\r\n+        // Pre-determined field values to be correct for this specific binary\r\n+        assert_eq!(te.header.stripped_size, 0x1c8);\r\n+        assert_eq!(te.header.entry_point, 0x10a8);\r\n+        assert_eq!(te.header.base_of_code, 0x0e60);\r\n+        assert_eq!(te.header.image_base, 0x0);\r\n+        assert_eq!(te.header.reloc_dir.virtual_address, 0x6e58);\r\n+        assert_eq!(te.header.reloc_dir.size, 0x0);\r\n+        assert_eq!(te.header.debug_dir.virtual_address, 0x3a64);\r\n+        assert_eq!(te.header.debug_dir.size, 0x54);\r\n+\r\n+        // Verify section information is correct - with pre-determined values\r\n+        // known to be correct. For brevity sake, check first and last entries.\r\n+        assert_eq!(String::from_utf8_lossy(&te.sections[0].name), \".text\\0\\0\\0\");\r\n+        assert_eq!(te.sections[0].virtual_address, 0xe60);\r\n+        assert_eq!(te.sections[0].virtual_size, 0x17db);\r\n+        assert_eq!(te.sections[0].pointer_to_linenumbers, 0);\r\n+        assert_eq!(te.sections[0].pointer_to_raw_data, 0xe60);\r\n+        assert_eq!(te.sections[0].pointer_to_relocations, 0);\r\n+        assert_eq!(\r\n+            te.sections[0].characteristics,\r\n+            IMAGE_SCN_MEM_EXECUTE\r\n+                | IMAGE_SCN_MEM_READ\r\n+                | IMAGE_SCN_MEM_NOT_PAGED\r\n+                | IMAGE_SCN_CNT_CODE\r\n+        );\r\n+\r\n+        assert_eq!(String::from_utf8_lossy(&te.sections[4].name), \".xdata\\0\\0\");\r\n+        assert_eq!(te.sections[4].virtual_address, 0x5e60);\r\n+        assert_eq!(te.sections[4].virtual_size, 0x98);\r\n+        assert_eq!(\r\n+            te.sections[4].characteristics,\r\n+            IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_DISCARDABLE | IMAGE_SCN_CNT_INITIALIZED_DATA\r\n+        );\r\n+        assert_eq!(te.sections[4].pointer_to_linenumbers, 0);\r\n+        assert_eq!(te.sections[4].pointer_to_raw_data, 0x5e60);\r\n+        assert_eq!(te.sections[4].pointer_to_relocations, 0);\r\n+\r\n+        // Verify the debug directory is correct\r\n+        assert_eq!(te.debug_data.image_debug_directory.size_of_data, 0xab);\r\n+        assert_eq!(\r\n+            te.debug_data.image_debug_directory.address_of_raw_data,\r\n+            0x3b54\r\n+        );\r\n+        assert_eq!(\r\n+            te.debug_data.image_debug_directory.pointer_to_raw_data,\r\n+            0x3b54\r\n+        );\r\n+        let debug_info = te.debug_data.codeview_pdb70_debug_info.unwrap();\r\n+        assert_eq!(\r\n+            debug_info.signature,\r\n+            [\r\n+                0x70, 0xfb, 0xb5, 0x4b, 0xcf, 0x68, 0x15, 0x42, 0xa1, 0x2b, 0xa5, 0xc5, 0x51, 0x95,\r\n+                0x0a, 0x4a\r\n+            ]\r\n+        );\r\n+        assert_eq!(String::from_utf8_lossy(debug_info.filename), String::from(\"c:\\\\src\\\\mu_tiano_platforms\\\\Build\\\\QemuQ35Pkg\\\\DEBUG_VS2022\\\\X64\\\\QemuQ35Pkg\\\\RustTerseImageTestDxe\\\\RustTerseImageTestDxe\\\\DEBUG\\\\RustTerseImageTestDxe.pdb\\0\"));\r\n+\r\n+        // Misc matches\r\n+        assert_eq!(te.header.base_of_code, te.sections[0].virtual_address);\r\n+    }\r\n+\r\n+    /// Verify that parsing of a loaded TE image works.\r\n+    #[test]\r\n+    fn parse_loaded_te() {\r\n+        let image = include_bytes!(\"bins/te/test_image.te\");\r\n+        let te = pe::TE::parse(image).expect(\"Failed to parse TE\");\r\n+\r\n+        let loaded_image = include_bytes!(\"bins/te/test_image_loaded.bin\");\r\n+        let te_loaded = pe::TE::parse(loaded_image).expect(\"Failed to parse TE\");\r\n+\r\n+        assert_eq!(te.header, te_loaded.header);\r\n+        assert_eq!(te.sections, te_loaded.sections);\r\n+        assert_eq!(te.debug_data, te_loaded.debug_data);\r\n+    }\r\n+\r\n+    /// Verify that parsing of a relocated TE image works. Raw data should be different due to\r\n+    /// the relocations being applied, but that is outside the scope of goblin.\r\n+    #[test]\r\n+    fn parse_relocated_te() {\r\n+        let loaded_image = include_bytes!(\"bins/te/test_image_loaded.bin\");\r\n+        let te_loaded = pe::TE::parse(loaded_image).expect(\"Failed to parse TE\");\r\n+\r\n+        let relocated_image = include_bytes!(\"bins/te/test_image_relocated.bin\");\r\n+        let te_relocated = pe::TE::parse(relocated_image).expect(\"Failed to parse TE\");\r\n+\r\n+        // Only the image base should be different in the section headers.\r\n+        assert_ne!(te_loaded.header.image_base, te_relocated.header.image_base);\r\n+        assert_eq!(te_loaded.sections, te_relocated.sections);\r\n+        assert_eq!(te_loaded.debug_data, te_relocated.debug_data);\r\n+    }\r\n+}\r\n", "problem_statement": "Add Terse Executable (TE) support to goblin\nI would like to request the addition of Terse Executable (TE) support to goblin's PE module. a terse executable is simply a PE/PE32+ binary with the header replaced with a terse executable header (as defined in the PI specification: [TE}(https://uefi.org/specs/PI/1.8/V1_TE_Image.html). The TE header is a subset of values from the PE/PE32+ header and are the only fields necessary for the binary to be properly executed by a PI architecture compliant loader and executor. Terse executables are most commonly used by UEFI compliant firmware to reduce the overall size of the binary in firmware.\r\n\r\nIf willing to add this support, I have  created #397 for you to review and provide feedback on. I am open to changing implementation (such as having it be it's own module rather than a subset of the PE module (like `PeCoff`). As the maintainers, I look to you for details such as these.\r\n\r\nThanks for your time :)\r\n\r\n\n", "hints_text": "", "created_at": "2024-03-14 16:41:07", "merge_commit_sha": "47ee850af2beaf81ed5d99b5546de75db8e33b75", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test (win64)', '.github/workflows/main.yml']", "['Test (win32)', '.github/workflows/main.yml']"], ["['Test (macos)', '.github/workflows/main.yml']", "['Test (MSRV)', '.github/workflows/main.yml']"]]}
{"repo": "wasmerio/wasmer", "instance_id": "wasmerio__wasmer-5162", "base_commit": "dd09e78f04c90f9fad174db7a3007e3f8e471e4b", "patch": "diff --git a/lib/wasix/src/syscalls/wasi/fd_write.rs b/lib/wasix/src/syscalls/wasi/fd_write.rs\nindex 04954d6770e..7793b72a550 100644\n--- a/lib/wasix/src/syscalls/wasi/fd_write.rs\n+++ b/lib/wasix/src/syscalls/wasi/fd_write.rs\n@@ -163,7 +163,7 @@ pub(crate) fn fd_write_internal<M: MemorySize>(\n                                 if !is_stdio {\n                                     if fd_entry.flags.contains(Fdflags::APPEND) {\n                                         // `fdflags::append` means we need to seek to the end before writing.\n-                                        offset = handle.size();\n+                                        offset = fd_entry.inode.stat.read().unwrap().st_size;\n                                         fd_entry.offset.store(offset, Ordering::Release);\n                                     }\n \ndiff --git a/lib/wasix/src/syscalls/wasi/path_open.rs b/lib/wasix/src/syscalls/wasi/path_open.rs\nindex 9c8d3c81655..4f8c00d57dd 100644\n--- a/lib/wasix/src/syscalls/wasi/path_open.rs\n+++ b/lib/wasix/src/syscalls/wasi/path_open.rs\n@@ -237,7 +237,7 @@ pub(crate) fn path_open_internal(\n                 let open_options = open_options\n                     .write(minimum_rights.write)\n                     .create(minimum_rights.create)\n-                    .append(minimum_rights.append)\n+                    .append(false)\n                     .truncate(minimum_rights.truncate);\n \n                 if minimum_rights.read {\n", "test_patch": "diff --git a/tests/wasi-fyi/fs_open_append_offset.rs b/tests/wasi-fyi/fs_open_append_offset.rs\nnew file mode 100644\nindex 00000000000..fef83616f53\n--- /dev/null\n+++ b/tests/wasi-fyi/fs_open_append_offset.rs\n@@ -0,0 +1,31 @@\n+use std::{\n+    fs,\n+    io::{Seek, SeekFrom, Write},\n+};\n+\n+fn main() {\n+    let file = \"fyi/fs_open_append_offset.dir/file\";\n+    let mut f0 = fs::OpenOptions::new()\n+        .create(true)\n+        .truncate(true)\n+        .write(true)\n+        .open(file)\n+        .unwrap();\n+\n+    f0.write_all(b\"abc\").unwrap();\n+    f0.seek(SeekFrom::Start(1)).unwrap();\n+\n+    assert_eq!(fs::read_to_string(file).unwrap(), \"abc\");\n+\n+    // This open with append should not affect the offset of f0.\n+    let _f1 = fs::OpenOptions::new()\n+        .create(true)\n+        .write(true)\n+        .append(true)\n+        .open(file)\n+        .unwrap();\n+\n+    f0.write_all(b\"d\").unwrap();\n+\n+    assert_eq!(fs::read_to_string(file).unwrap(), \"adc\");\n+}\ndiff --git a/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore b/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore\nnew file mode 100644\nindex 00000000000..f73f3093ff8\n--- /dev/null\n+++ b/tests/wasi-fyi/test_fs/fyi/fs_open_append_offset.dir/.gitignore\n@@ -0,0 +1,1 @@\n+file\n", "problem_statement": "Opening a file with append should not change existing fd offset\n### Describe the bug\r\n\r\nIf file has been opened with a descriptor `fd0`, opening the same file with the append flag&mdash;producing `fd1`&mdash;should not change the offset of `fd0`.  `fd0` and `fd1` should have independent offsets.\r\n\r\nThis behavior is consistent across Linux and other Wasm runtimes (Wasmtime, WAMR, WasmEdge, Wazero), but Wasmer changes the offset of the original descriptor.\r\n\r\n```sh\r\nwasmer -vV; rustc -vV\r\nwasmer 4.4.0 (8b97cfe 2024-10-18)\r\nbinary: wasmer-cli\r\ncommit-hash: 8b97cfe3992ae3c2f0002f9955fcb23057f666a9\r\ncommit-date: 2024-10-18\r\nhost: x86_64-unknown-linux-gnu\r\ncompiler: singlepass,cranelift\r\nc_api backend: \r\nrustc 1.81.0 (eeb90cda1 2024-09-04)\r\nbinary: rustc\r\ncommit-hash: eeb90cda1969383f56a2637cbd3037bdf598841c\r\ncommit-date: 2024-09-04\r\nhost: x86_64-unknown-linux-gnu\r\nrelease: 1.81.0\r\nLLVM version: 18.1.7\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nCompile and run this snippet, mounting some diretory:\r\n\r\n```\r\ncargo build --target wasm32-wasip1\r\nwasmer run --mapdir /dir:dir target/wasm32-wasip1/debug/repro.wasm\r\n```\r\n\r\n```rust\r\nuse std::{\r\n    fs,\r\n    io::{Seek, SeekFrom, Write},\r\n};\r\n\r\nfn main() {\r\n    let mut f0 = fs::OpenOptions::new()\r\n        .create(true)\r\n        .write(true)\r\n        .open(\"file\")\r\n        .unwrap();\r\n\r\n    f0.write_all(b\"abc\").unwrap();\r\n    f0.seek(SeekFrom::Start(1)).unwrap();\r\n\r\n    assert_eq!(fs::read_to_string(\"file\").unwrap(), \"abc\");\r\n\r\n    // This open with append should not affect the offset of f0.\r\n    let _f1 = fs::OpenOptions::new()\r\n        .create(true)\r\n        .write(true)\r\n        .append(true)\r\n        .open(\"file\")\r\n        .unwrap();\r\n\r\n    f0.write_all(b\"d\").unwrap();\r\n\r\n    assert_eq!(fs::read_to_string(\"file\").unwrap(), \"adc\");\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nRunning this snippet should not panic.\r\n\r\n### Actual behavior\r\n\r\nWasmer panics, producing a file with the content `abcd` instead of `adc`.\r\n\r\n\n", "hints_text": "", "created_at": "2024-10-20 04:05:51", "merge_commit_sha": "64726330c2139b2c0a2b9ab0ee1705cf5ecaba67", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test API for v8 feature on windows-x64', '.github/workflows/test.yaml']", "['Build wasmer-cli on macos-arm', '.github/workflows/test.yaml']"], ["['Unit-test packages on std on macos-arm', '.github/workflows/test.yaml']", "['Build and test C-API on linux-musl', '.github/workflows/test.yaml']"], ["['Build wasmer-cli on macos-x64', '.github/workflows/test.yaml']", "['Run wast test suite for all compilers on macos-arm', '.github/workflows/test.yaml']"], ["['Actions', '.github/workflows/cifuzz.yaml']", "['Unit-test cranelift on no-std on macos-x64', '.github/workflows/test.yaml']"], ["['Security', '.github/workflows/cifuzz.yaml']", "['Unit-test examples on linux-musl', '.github/workflows/test.yaml']"], ["['Unit-test wasmer-cli on macos-arm', '.github/workflows/test.yaml']", "['Build C-API on linux-riscv64', '.github/workflows/test.yaml']"], ["['Test API for wasmi feature on macos-arm', '.github/workflows/test.yaml']", "['Build wasmer-cli on linux-aarch64', '.github/workflows/test.yaml']"], ["['Code lint', '.github/workflows/test.yaml']", "['Code', '.github/workflows/cifuzz.yaml']"], ["['Unit-test wasmer-cli on macos-x64', '.github/workflows/test.yaml']", "['test-wasmer-integration-tests', '.github/workflows/test.yaml']"], ["['Run wast test suite for all compilers on linux-x64', '.github/workflows/test.yaml']", "['Workflow file', '.github/workflows/cifuzz.yaml']"], ["['Unit-test examples on linux-x64', '.github/workflows/test.yaml']", "['Test API for wasmi feature on linux-x64', '.github/workflows/test.yaml']"], ["['Build and test C-API on linux-x64', '.github/workflows/test.yaml']", "['Unit-test singlepass on no-std on linux-x64', '.github/workflows/test.yaml']"], ["['Unit-test wasmer-cli on linux-x64', '.github/workflows/test.yaml']", "['Build and test C-API on windows-gnu', '.github/workflows/test.yaml']"], ["['Unit-test examples on windows-x64', '.github/workflows/test.yaml']", "['Build and test C-API on macos-x64', '.github/workflows/test.yaml']"], ["['Unit-test cranelift on no-std on linux-x64', '.github/workflows/test.yaml']", "['Unit-test wasmer-cli on windows-x64', '.github/workflows/test.yaml']"], ["['Discussions', '.github/workflows/cifuzz.yaml']", "['Unit-test packages on std on linux-x64', '.github/workflows/test.yaml']"], ["['Run wast test suite for all compilers on linux-musl', '.github/workflows/test.yaml']", "['Unit-test packages on std on linux-musl', '.github/workflows/test.yaml']"], ["['Unit-test cranelift on no-std on linux-musl', '.github/workflows/test.yaml']", "['Build wasmer-cli on windows-x64', '.github/workflows/test.yaml']"], ["['Projects', '.github/workflows/cifuzz.yaml']", "['Unit-test cranelift on no-std on macos-arm', '.github/workflows/test.yaml']"], ["['Build C-API on linux-aarch64', '.github/workflows/test.yaml']", "['Test WASIX', '.github/workflows/test.yaml']"]]}
{"repo": "elastio/bon", "instance_id": "elastio__bon-222", "base_commit": "00d9a3ee614f76f477b9f0f8a4f3e1210cbadc57", "patch": "diff --git a/bon-macros/Cargo.toml b/bon-macros/Cargo.toml\nindex 98101b18..3c5570ed 100644\n--- a/bon-macros/Cargo.toml\n+++ b/bon-macros/Cargo.toml\n@@ -63,6 +63,9 @@ rustversion  = \"1.0\"\n [features]\n default = []\n \n+# See the docs on this feature in the `bon`'s crate `Cargo.toml`.\n+experimental-getter = []\n+\n # See the docs on this feature in the `bon`'s crate `Cargo.toml`\n experimental-overwritable = []\n \ndiff --git a/bon-macros/src/builder/builder_gen/getter.rs b/bon-macros/src/builder/builder_gen/getter.rs\nnew file mode 100644\nindex 00000000..291a9249\n--- /dev/null\n+++ b/bon-macros/src/builder/builder_gen/getter.rs\n@@ -0,0 +1,92 @@\n+use proc_macro2::TokenStream;\n+use quote::quote;\n+\n+use super::{BuilderGenCtx, IdentExt, NamedMember};\n+\n+pub(crate) struct GetterCtx<'a> {\n+    base: &'a BuilderGenCtx,\n+    member: &'a NamedMember,\n+}\n+\n+struct GetterItem {\n+    name: syn::Ident,\n+    vis: syn::Visibility,\n+    docs: Vec<syn::Attribute>,\n+}\n+\n+impl<'a> GetterCtx<'a> {\n+    pub(crate) fn new(base: &'a BuilderGenCtx, member: &'a NamedMember) -> Self {\n+        Self { base, member }\n+    }\n+\n+    pub(crate) fn getter_method(&self) -> TokenStream {\n+        let Some(GetterItem { name, vis, docs }) = GetterItem::new(self) else {\n+            return quote! {};\n+        };\n+\n+        let index = &self.member.index;\n+        let ty = self.member.underlying_norm_ty();\n+\n+        let (return_type, body) = if self.member.is_required() {\n+            (\n+                quote! { &#ty },\n+                quote! { unsafe { ::std::option::Option::unwrap_unchecked(self.__unsafe_private_named.#index.as_ref()) } },\n+            )\n+        } else {\n+            (\n+                quote! { ::core::option::Option<&#ty> },\n+                quote! { self.__unsafe_private_named.#index.as_ref() },\n+            )\n+        };\n+\n+        let state_var = &self.base.state_var;\n+        let member_pascal = &self.member.name.pascal;\n+        let state_mod = &self.base.state_mod.ident;\n+\n+        quote! {\n+            #( #docs )*\n+            #[allow(\n+                // This is intentional. We want the builder syntax to compile away\n+                clippy::inline_always,\n+                clippy::missing_const_for_fn,\n+            )]\n+            #[inline(always)]\n+            #vis fn #name(&self) -> #return_type\n+            where #state_var::#member_pascal: #state_mod::IsSet,\n+            {\n+                #body\n+            }\n+        }\n+    }\n+}\n+\n+impl GetterItem {\n+    fn new(ctx: &GetterCtx<'_>) -> Option<Self> {\n+        let GetterCtx { member, base } = ctx;\n+\n+        let spanned_keyed_config = member.config.getter.as_ref()?;\n+\n+        let common_name = spanned_keyed_config.name();\n+        let common_vis = spanned_keyed_config.vis();\n+        let common_docs = spanned_keyed_config.docs();\n+\n+        Some(Self {\n+            name: common_name.cloned().unwrap_or_else(|| {\n+                syn::Ident::new(\n+                    &format!(\"get_{}\", member.name.snake.raw_name()),\n+                    member.name.snake.span(),\n+                )\n+            }),\n+            vis: common_vis.unwrap_or(&base.builder_type.vis).clone(),\n+            docs: common_docs\n+                .map(<[syn::Attribute]>::to_vec)\n+                .unwrap_or_else(|| {\n+                    const HEADER: &str = \"_**Getter.**_\\n\\n\";\n+\n+                    std::iter::once(syn::parse_quote!(#[doc = #HEADER]))\n+                        .chain(member.docs.iter().cloned())\n+                        .collect()\n+                }),\n+        })\n+    }\n+}\ndiff --git a/bon-macros/src/builder/builder_gen/member/config/getter.rs b/bon-macros/src/builder/builder_gen/member/config/getter.rs\nnew file mode 100644\nindex 00000000..1e95ea56\n--- /dev/null\n+++ b/bon-macros/src/builder/builder_gen/member/config/getter.rs\n@@ -0,0 +1,54 @@\n+use darling::FromMeta;\n+\n+use super::{Result, SpannedKey};\n+\n+#[derive(Debug, Default)]\n+pub(crate) struct GetterConfig {\n+    name: Option<SpannedKey<syn::Ident>>,\n+    vis: Option<SpannedKey<syn::Visibility>>,\n+\n+    docs: Option<SpannedKey<Vec<syn::Attribute>>>,\n+}\n+\n+impl FromMeta for GetterConfig {\n+    fn from_meta(meta: &syn::Meta) -> Result<Self> {\n+        if let syn::Meta::Path(_) = meta {\n+            return Ok(Self::default());\n+        }\n+\n+        // Reject empty parens such as `#[builder(getter())]`\n+        crate::parsing::require_non_empty_paren_meta_list_or_name_value(meta)?;\n+\n+        // Nested `Parsed` struct used as a helper for parsing the verbose form\n+        #[derive(FromMeta)]\n+        struct Parsed {\n+            name: Option<SpannedKey<syn::Ident>>,\n+            vis: Option<SpannedKey<syn::Visibility>>,\n+\n+            #[darling(rename = \"doc\", default, with = parse_docs, map = Some)]\n+            docs: Option<SpannedKey<Vec<syn::Attribute>>>,\n+        }\n+\n+        let Parsed { name, vis, docs } = Parsed::from_meta(meta)?;\n+\n+        Ok(Self { name, vis, docs })\n+    }\n+}\n+\n+impl GetterConfig {\n+    pub(crate) fn name(&self) -> Option<&syn::Ident> {\n+        self.name.as_ref().map(|n| &n.value)\n+    }\n+\n+    pub(crate) fn vis(&self) -> Option<&syn::Visibility> {\n+        self.vis.as_ref().map(|v| &v.value)\n+    }\n+\n+    pub(crate) fn docs(&self) -> Option<&[syn::Attribute]> {\n+        self.docs.as_ref().map(|a| &a.value).map(|a| &**a)\n+    }\n+}\n+\n+fn parse_docs(meta: &syn::Meta) -> Result<SpannedKey<Vec<syn::Attribute>>> {\n+    crate::parsing::parse_docs_without_self_mentions(\"builder struct's impl block\", meta)\n+}\ndiff --git a/bon-macros/src/builder/builder_gen/member/config/mod.rs b/bon-macros/src/builder/builder_gen/member/config/mod.rs\nindex 02bfe590..a6e5606e 100644\n--- a/bon-macros/src/builder/builder_gen/member/config/mod.rs\n+++ b/bon-macros/src/builder/builder_gen/member/config/mod.rs\n@@ -1,8 +1,10 @@\n mod blanket;\n+mod getter;\n mod setters;\n mod with;\n \n pub(crate) use blanket::*;\n+pub(crate) use getter::*;\n pub(crate) use setters::*;\n pub(crate) use with::*;\n \n@@ -33,6 +35,12 @@ pub(crate) struct MemberConfig {\n     #[darling(with = parse_optional_expr, map = Some)]\n     pub(crate) field: Option<SpannedKey<Option<syn::Expr>>>,\n \n+    /// Make the member gettable by reference.\n+    ///\n+    /// This takes the same attributes as the setter fns; `name`, `vis`, and `doc`\n+    /// and produces a getter method that returns `&T` for the member.\n+    pub(crate) getter: Option<SpannedKey<GetterConfig>>,\n+\n     /// Accept the value for the member in the finishing function parameters.\n     pub(crate) finish_fn: darling::util::Flag,\n \n@@ -82,6 +90,7 @@ pub(crate) struct MemberConfig {\n enum ParamName {\n     Default,\n     Field,\n+    Getter,\n     FinishFn,\n     Into,\n     Name,\n@@ -98,6 +107,7 @@ impl fmt::Display for ParamName {\n         let str = match self {\n             Self::Default => \"default\",\n             Self::Field => \"field\",\n+            Self::Getter => \"getter\",\n             Self::FinishFn => \"finish_fn\",\n             Self::Into => \"into\",\n             Self::Name => \"name\",\n@@ -162,6 +172,7 @@ impl MemberConfig {\n         let Self {\n             default,\n             field,\n+            getter,\n             finish_fn,\n             into,\n             name,\n@@ -176,6 +187,7 @@ impl MemberConfig {\n         let attrs = [\n             (default.is_some(), ParamName::Default),\n             (field.is_some(), ParamName::Field),\n+            (getter.is_some(), ParamName::Getter),\n             (finish_fn.is_present(), ParamName::FinishFn),\n             (into.is_present(), ParamName::Into),\n             (name.is_some(), ParamName::Name),\n@@ -212,7 +224,7 @@ impl MemberConfig {\n             self.validate_mutually_allowed(\n                 ParamName::StartFn,\n                 self.start_fn.span(),\n-                &[ParamName::Into],\n+                &[ParamName::Into, ParamName::Getter],\n             )?;\n         }\n \n@@ -220,7 +232,34 @@ impl MemberConfig {\n             self.validate_mutually_allowed(\n                 ParamName::FinishFn,\n                 self.finish_fn.span(),\n-                &[ParamName::Into],\n+                &[ParamName::Into, ParamName::Getter],\n+            )?;\n+        }\n+\n+        if let Some(getter) = &self.getter {\n+            if !cfg!(feature = \"experimental-getter\") {\n+                bail!(\n+                    &getter.key.span(),\n+                    \"`getter` attribute is experimental and requires \\\n+                    \\\"experimental-getter\\\" cargo feature to be enabled; \\\n+                    we would be glad to make this attribute stable if you find it useful; \\\n+                    please leave a \ud83d\udc4d reaction under the issue https://github.com/elastio/bon/issues/221 \\\n+                    to help us measure the impact on this feature. If you have \\\n+                    a use case for this attribute, then open an issue/discussion on \\\n+                    https://github.com/elastio/bon/issues.\",\n+                );\n+            }\n+\n+            self.validate_mutually_allowed(\n+                ParamName::Getter,\n+                getter.key.span(),\n+                &[\n+                    ParamName::With,\n+                    ParamName::Into,\n+                    ParamName::Name,\n+                    ParamName::Setters,\n+                    ParamName::Required,\n+                ],\n             )?;\n         }\n \ndiff --git a/bon-macros/src/builder/builder_gen/mod.rs b/bon-macros/src/builder/builder_gen/mod.rs\nindex 9196c498..325fbd38 100644\n--- a/bon-macros/src/builder/builder_gen/mod.rs\n+++ b/bon-macros/src/builder/builder_gen/mod.rs\n@@ -1,6 +1,7 @@\n mod builder_decl;\n mod builder_derives;\n mod finish_fn;\n+mod getter;\n mod member;\n mod models;\n mod setters;\n@@ -11,6 +12,7 @@ mod top_level_config;\n pub(crate) mod input_fn;\n pub(crate) mod input_struct;\n \n+use getter::GetterCtx;\n pub(crate) use top_level_config::TopLevelConfig;\n \n use crate::util::prelude::*;\n@@ -107,6 +109,10 @@ impl BuilderGenCtx {\n             .map(|member| SettersCtx::new(self, member).setter_methods())\n             .collect::<Result<Vec<_>>>()?;\n \n+        let getter_methods = self\n+            .named_members()\n+            .map(|member| GetterCtx::new(self, member).getter_method());\n+\n         let generics_decl = &self.generics.decl_without_defaults;\n         let generic_args = &self.generics.args;\n         let where_clause = &self.generics.where_clause;\n@@ -128,6 +134,7 @@ impl BuilderGenCtx {\n             {\n                 #finish_fn\n                 #(#setter_methods)*\n+                #(#getter_methods)*\n             }\n         })\n     }\ndiff --git a/bon-sandbox/Cargo.toml b/bon-sandbox/Cargo.toml\nindex 12f0dc0f..8c5e56ca 100644\n--- a/bon-sandbox/Cargo.toml\n+++ b/bon-sandbox/Cargo.toml\n@@ -29,7 +29,7 @@ targets = [\"x86_64-unknown-linux-gnu\"]\n workspace = true\n \n [dependencies]\n-bon            = { path = \"../bon\", version = \"=3.1.1\", features = [\"experimental-overwritable\"] }\n+bon            = { path = \"../bon\", version = \"=3.1.1\", features = [\"experimental-overwritable\", \"experimental-getter\"] }\n buildstructor  = \"0.5\"\n derive_builder = \"0.20\"\n typed-builder  = \"0.20\"\ndiff --git a/bon-sandbox/src/getter.rs b/bon-sandbox/src/getter.rs\nnew file mode 100644\nindex 00000000..287fd91e\n--- /dev/null\n+++ b/bon-sandbox/src/getter.rs\n@@ -0,0 +1,17 @@\n+use bon::{builder, Builder};\n+\n+#[builder]\n+pub fn full_name_fn(#[builder(getter)] first_name: &str, last_name: &str) -> String {\n+    format!(\"{first_name} {last_name}\")\n+}\n+\n+#[derive(Builder)]\n+pub struct FullName {\n+    #[builder(getter)]\n+    pub first_name: String,\n+    #[builder(getter(name = get_the_last_name, vis = \"pub(crate)\", doc {\n+        /// Docs on the getter\n+    }))]\n+    pub last_name: String,\n+    pub no_getter: String,\n+}\ndiff --git a/bon-sandbox/src/lib.rs b/bon-sandbox/src/lib.rs\nindex 9cd816e8..49d95423 100644\n--- a/bon-sandbox/src/lib.rs\n+++ b/bon-sandbox/src/lib.rs\n@@ -9,6 +9,7 @@ pub mod attr_default;\n pub mod attr_with;\n pub mod docs_comparison;\n pub mod functions;\n+pub mod getter;\n pub mod macro_rules_wrapper_test;\n pub mod missing_docs_test;\n pub mod overrides;\ndiff --git a/bon/Cargo.toml b/bon/Cargo.toml\nindex 38eeff6a..fca6039e 100644\n--- a/bon/Cargo.toml\n+++ b/bon/Cargo.toml\n@@ -82,3 +82,17 @@ implied-bounds = [\"bon-macros/implied-bounds\"]\n # this attribute. It would also be cool if you could leave a comment under that issue\n # describing your use case for it.\n experimental-overwritable = [\"bon-macros/experimental-overwritable\"]\n+\n+# \ud83d\udd2c Experimental! There may be breaking changes to this feature between *minor* releases,\n+# however, compatibility within patch releases is guaranteed though.\n+#\n+# This feature enables the #[builder(getter)] attribute that can be used to\n+# allow getting references to already set fields in the builder.\n+#\n+# See more info at https://bon-rs.com/reference/builder/member/getter.\n+#\n+# We are considering stabilizing this attribute if you have a use for it. Please leave\n+# a \ud83d\udc4d reaction under the issue https://github.com/elastio/bon/issues/221 if you need\n+# this attribute. It would also be cool if you could leave a comment under that issue\n+# describing your use case for it.\n+experimental-getter = [\"bon-macros/experimental-getter\"]\ndiff --git a/rust-toolchain b/rust-toolchain\ndeleted file mode 100644\nindex 71fae54f..00000000\n--- a/rust-toolchain\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-1.82.0\ndiff --git a/rust-toolchain.toml b/rust-toolchain.toml\nnew file mode 100644\nindex 00000000..66788910\n--- /dev/null\n+++ b/rust-toolchain.toml\n@@ -0,0 +1,3 @@\n+[toolchain]\n+channel    = \"1.82\"\n+components = [\"cargo\"]\ndiff --git a/scripts/init.sh b/scripts/init.sh\nindex 6df82a0b..bd9c3f9b 100755\n--- a/scripts/init.sh\n+++ b/scripts/init.sh\n@@ -6,5 +6,8 @@ set -euxo pipefail\n # Install prettier\n npm ci\n \n+# Install taplo\n+cargo install taplo-cli --locked\n+\n # Install the pre-commit hook\n ln -s ../../.githooks/pre-commit .git/hooks/pre-commit\ndiff --git a/website/.vitepress/config.mts b/website/.vitepress/config.mts\nindex ffc757bf..3a6f3ae1 100644\n--- a/website/.vitepress/config.mts\n+++ b/website/.vitepress/config.mts\n@@ -303,6 +303,10 @@ export default defineConfig({\n                                     text: \"finish_fn\",\n                                     link: \"/reference/builder/member/finish_fn\",\n                                 },\n+                                {\n+                                    text: \"getters \ud83d\udd2c\",\n+                                    link: \"/reference/builder/member/getters\",\n+                                },\n                                 {\n                                     text: \"into\",\n                                     link: \"/reference/builder/member/into\",\ndiff --git a/website/src/guide/contributing.md b/website/src/guide/contributing.md\nindex 6d9e9dce..ba31c906 100644\n--- a/website/src/guide/contributing.md\n+++ b/website/src/guide/contributing.md\n@@ -14,6 +14,8 @@ However, even though desirable, creating an issue before making a pull request i\n \n This repository is a regular [`cargo` workspace](https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html). Just fork it and do the usual `cargo` business.\n \n+Additionally, an init script is provided: `./scripts/init.sh`. This will install required dependencies and set up the commit hooks for our CI.\n+\n ## Testing\n \n Test your changes with `cargo test`. You may add new tests to the `bon/tests/integration` folder.\ndiff --git a/website/src/reference/builder.md b/website/src/reference/builder.md\nindex 9f439428..742e8712 100644\n--- a/website/src/reference/builder.md\n+++ b/website/src/reference/builder.md\n@@ -25,6 +25,7 @@ These attributes are placed on a `struct` field or `fn` argument.\n | [`default`](./builder/member/default)              | Makes the member optional with a default value                   |\n | [`field`](./builder/member/field)                  | Defines a private field on the builder without setters           |\n | [`finish_fn`](./builder/member/finish_fn)          | Makes the member a positional argument on the finishing function |\n+| [`getter`](./builder/member/getter)                | Makes the member have getter method for `&T`                     |\n | [`into`](./builder/member/into)                    | Changes the signature of the setters to accept `impl Into<T>`    |\n | [`name`](./builder/member/name)                    | Overrides the name of the member used in the builder's API       |\n | [`overwritable` \ud83d\udd2c](./builder/member/overwritable) | Allows calling setters for the same member repeatedly            |\ndiff --git a/website/src/reference/builder/member/getter.md b/website/src/reference/builder/member/getter.md\nnew file mode 100644\nindex 00000000..57081ef3\n--- /dev/null\n+++ b/website/src/reference/builder/member/getter.md\n@@ -0,0 +1,77 @@\n+# `getter` :microscope:\n+\n+**Applies to:** <Badge type=\"warning\" text=\"struct fields\"/> <Badge type=\"warning\" text=\"function arguments\"/> <Badge type=\"warning\" text=\"method arguments\"/>\n+\n+Allows getting a reference to an already set member.\n+\n+::: danger \ud83d\udd2c **Experimental**\n+\n+This attribute is available under the cargo feature `experimental-getter`. Breaking changes may occur between **minor** releases but not between patch releases.\n+\n+The fate of this feature depends on your feedback in the tracking issue [#149](https://github.com/elastio/bon/issues/221). Please, let us know if you have a use case for this attribute!\n+\n+:::\n+\n+This attribute now makes the following possible:\n+\n+```rust\n+#[derive(bon::Builder)]\n+struct Example {\n+    #[builder(getter)] // [!code highlight]\n+    x: u32,\n+}\n+\n+let builder = Example::builder().x(1);\n+\n+let x_ref = builder.get_x(); // [!code highlight]\n+\n+builder.build();\n+```\n+\n+## Config\n+\n+`getter` is configured quite similarly to [`setters`](./setters).\n+\n+You can specify any combination of a `name`, `vis`, and `doc` for the getter, or you can omit all of them and use it as a flag to inherit default values for your getters.\n+\n+### Example\n+\n+```rust\n+#[derive(bon::Builder)]\n+struct Example {\n+    #[builder(getter(name = get_my_member, vis = \"pub(crate)\", doc { // [!code highlight]\n+        /// `get_my_member` gets a ref to the member // [!code highlight]\n+    }))] // [!code highlight]\n+    member: u32, // [!code highlight]\n+}\n+\n+// Name of `some_fn` that accepts the non-None value was overridden\n+let builder = Example::builder().member(2);\n+\n+let my_member = builder.get_my_member(); // [!code highlight]\n+```\n+\n+## `name`\n+\n+The default name for getters are chosen according to the following rules:\n+\n+| Member type | Default        |\n+| ----------- | -------------- |\n+| Required    | `get_{member}` |\n+| Optional    | `get_{member}` |\n+\n+## `vis`\n+\n+The visibility must be enclosed with quotes. Use `\"\"` or [`\"pub(self)\"`](https://doc.rust-lang.org/reference/visibility-and-privacy.html#pubin-path-pubcrate-pubsuper-and-pubself) for private visibility.\n+\n+The default visibility is the same as the visibility of the [`builder_type`](../top-level/builder_type#vis), which in turn, defaults to the visibility of the underlying `struct` or `fn`.\n+\n+## `doc`\n+\n+Simple documentation is generated by default. The syntax of this attribute expects a block with doc comments.\n+\n+```attr\n+doc {\n+    /// Doc comments\n+}\n+```\n", "test_patch": "diff --git a/bon/tests/integration/builder/attr_getter.rs b/bon/tests/integration/builder/attr_getter.rs\nnew file mode 100644\nindex 00000000..90c8d995\n--- /dev/null\n+++ b/bon/tests/integration/builder/attr_getter.rs\n@@ -0,0 +1,67 @@\n+use crate::prelude::*;\n+\n+#[test]\n+fn test_struct() {\n+    #[derive(Debug, Builder)]\n+    #[builder(derive(Debug, Clone))]\n+    #[allow(dead_code)]\n+    struct Sut<T> {\n+        #[builder(start_fn)]\n+        x1: u32,\n+\n+        #[builder(getter(name = x2_with_custom_name))]\n+        x2: String,\n+\n+        #[builder(getter(vis = \"pub(crate)\", doc {\n+            /// Docs on the getter\n+        }))]\n+        x3: u32,\n+\n+        #[builder(into, getter(name = x5, vis = \"pub(crate)\", doc {\n+            /// The name is a lie\n+        }))]\n+        x4_but_its_actually_5: String,\n+\n+        not_a_getter: u32,\n+\n+        #[builder(getter)]\n+        generic_option_getter: Option<T>,\n+\n+        x6: (),\n+    }\n+\n+    #[allow(clippy::redundant_clone)]\n+    let sut = Sut::<()>::builder(0u32).clone();\n+\n+    let actual = sut.x2(\"2\".to_owned()).x3(3);\n+\n+    let actual = actual.x4_but_its_actually_5(\"4\".to_owned());\n+    let x5 = actual.x5();\n+    assert_eq!(x5, \"4\");\n+\n+    let actual = actual.not_a_getter(5).x6(());\n+\n+    let x2 = actual.x2_with_custom_name();\n+    assert_eq!(x2, \"2\");\n+\n+    let x3 = actual.get_x3();\n+    assert_eq!(x3, &3);\n+\n+    let actual = actual.maybe_generic_option_getter(None);\n+\n+    let gen_opt_get = actual.get_generic_option_getter();\n+    assert_eq!(gen_opt_get, None);\n+\n+    assert_debug_eq(\n+        &actual,\n+        expect![[r#\"\n+            SutBuilder {\n+                x1: 0,\n+                x2: \"2\",\n+                x3: 3,\n+                x4_but_its_actually_5: \"4\",\n+                not_a_getter: 5,\n+                x6: (),\n+            }\"#]],\n+    );\n+}\ndiff --git a/bon/tests/integration/builder/mod.rs b/bon/tests/integration/builder/mod.rs\nindex 8965f559..aa15c51c 100644\n--- a/bon/tests/integration/builder/mod.rs\n+++ b/bon/tests/integration/builder/mod.rs\n@@ -3,6 +3,7 @@ mod attr_crate;\n mod attr_default;\n mod attr_derive;\n mod attr_field;\n+mod attr_getter;\n mod attr_into;\n mod attr_on;\n mod attr_overwritable;\n", "problem_statement": "Support getters for already set members\nHi, Bon is my favorite crate and I'm using it as the backbone for a complex pipeline internal framework.\r\n\r\nIn essence, my pipeline works like this:\r\n\r\n```rust\r\n        pipeline\r\n            .pass(the_first_step)\r\n            .pass(another_step)\r\n            .pass(some_other_step)\r\n            .pass(the_last_step)\r\n```\r\n\r\nEach step of the pipeline is (simplified) implemented like this:\r\n\r\n```rust\r\npub async fn the_first_step<S>(\r\n    Payload(id): Payload,\r\n    PgConn(pg_conn): PgConn,\r\n    State(builder): State<NewDbRowBuilder<S>>,\r\n) -> Result<NewDbRowBuilder<SetTheFirstStepValue<S>>, crate::Error>\r\nwhere\r\n    S: new_db_row_builder::State,\r\n    S::TheFirstStepValue: IsUnset,\r\n{\r\n   // some work that connects to api, db, etc, and produces a value\r\n\r\n    Ok(builder.the_first_step_value(my_awesome_value_i_fetched))\r\n}\r\n```\r\n\r\nThe types of the pipeline enforce that the state produced by a prior pass must be fed into the pass under it, so you can't accidentally order anything wrong. I _love_ using Bon to back this pipeline system and allow me to make each pass specify what the conditions of its state should be. \r\n\r\nWhere I am struggling is that sometimes I run into a weird, limiting situation.\r\n\r\nI have one step in my pipeline that scrapes some information off of a user in my DB, processes it, then sets it in the builder (because this value must go in the row that's being built). Then, in another pass of the pipeline, I must read processed data out and use it to compute another value that also goes on my row. Given the current implementation of Bon (while following the rules and not directly implementing unsafe methods on generated code), I have three solutions:\r\n\r\n1. Take this processed value, clone it and set it into the state, then return a tuple of the state and the cloned value. This is bad to me because the passes of my pipeline become much less generic as they now must take a tuple of the built state and any previously obtained values and drill them through the entire way\r\n2. Do what I'm doing currently, and maintain a cache that passes can use to set values and retrieve them later. This works (and is fairly generic), but I lose the ability to _safely_ do the guards like `IsSet` because this cache has to be completely detached from `bon`\r\n3. Use `builder(field)` on these fields I want to cache. This also works, but it has the same problem as before where I lose the ability to do guards on the passes which is what I love so much about bon.\r\n\r\nI noticed in 3.1.1 you removed the non-determinism of generated privates. After seeing this, being able to add methods that take in `&self` and return a reference to a value in the builder state would be incredible and make my pipeline work exactly how I want it to. This long winded context finally leads to my question (the one in the title)...\r\n\r\nHow fundamentally unsafe is this? Taking a look at the expanded macro from `builder` everything I can see indicates that adding read only accessor methods that do not move out of the tuple or clone the data would be completely safe, but I figured it would be well worth opening an issue for this because this will be running in production code where a mistake like this will not be acceptable. \r\n\r\nThank you!\r\n\r\n<!-- Please keep this note for the community at the end of the issue -->\r\n\r\n### A note for the community from the maintainers\r\n\r\nPlease vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to help the maintainers with prioritizing it. You may add a comment describing your real use case related to this issue for us to better understand the problem domain.\r\n\n", "hints_text": "Hi, thank you for creating the issue! Accessing the private fields of the builder (except for `#[builder(field)]`) is definitely wrong, since those internals may change between patch versions. The only unsafe code generated by builder macros at the moment is `unwrap_unchecked` for required members in the finishing function. So the only way this may break is if you set the required field inside the builder to `None` by directly accessing its internals. Vending a readonly reference to the internal field is completely fine, however, there isn't an official API to do that in the current version of `bon` (3.1.1).\r\n\r\nBut.. I was actually planning to add an ability to generate getters for already set members as the next feature after `#[builder(field)]`. I imagine it like this. There will be a `#[builder(getter)]` annotation that will generate a simple by-readonly-reference getter with no magic, that has the name `get_{member}`. It will be possible to override its visibility, docs and name with the usual attributes `#[builder(getter(name = ..., vis = ..., doc { ... })]`. The default visibility will be the same as it would be for setters.\r\n\r\nIn future increments, I plan to provide the ability to generate getters for mutable references, the ability to do `AsRef` and `Deref` coercions, and custom conversions similar to `#[builder(with)]`, but it'll be `#[builder(getter(with))]`.\nFantastic to hear! Thank you. I am fairly familiar with `bon`'s internals, and if you haven't started I'd be happy to take on and implement this.\nYeah, I've been procrastinating on this a bit given the Stalker 2 release \ud83d\ude33. I haven't started with this feature yet, so feel free to take a stab at this \ud83d\udc4d \nHaha, have fun. Feel free to assign me this. Will get started now.\nAt a high level, how do you feel about having the getters return an `impl Borrow<T>` instead of just `&T`? The main reason I'm suggesting this is because the ergonomics could be a bit better for `String`s and `Vec`s which would then return `&str` and `&[T]` accordingly.", "created_at": "2024-11-28 21:23:45", "merge_commit_sha": "4ea25abf13dab36628deb2772d00fda57a790729", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['prettier', '.github/workflows/ci.yml']", "['test-unstable (beta)', '.github/workflows/ci.yml']"], ["['runtime-benchmarks (args_5)', '.github/workflows/ci.yml']", "['runtime-benchmarks (args_10)', '.github/workflows/ci.yml']"], ["['test-unstable (nightly)', '.github/workflows/ci.yml']", "['cargo-doc (ubuntu)', '.github/workflows/ci.yml']"], ["['test-msrv (windows)', '.github/workflows/ci.yml']", "['cargo-machete', '.github/workflows/ci.yml']"], ["['test-stable (ubuntu)', '.github/workflows/ci.yml']", "['taplo-fmt', '.github/workflows/ci.yml']"], ["['test-stable (macos)', '.github/workflows/ci.yml']", "['test-stable (windows)', '.github/workflows/ci.yml']"], ["['test-msrv (macos)', '.github/workflows/ci.yml']", "['runtime-benchmarks (args_10_structs)', '.github/workflows/ci.yml']"]]}
{"repo": "sharkdp/bat", "instance_id": "sharkdp__bat-3075", "base_commit": "61c9f312c9d10103b33b7d8069401304ca938f06", "patch": "diff --git a/src/bin/bat/main.rs b/src/bin/bat/main.rs\nindex 4528a60beb..3b74ec7589 100644\n--- a/src/bin/bat/main.rs\n+++ b/src/bin/bat/main.rs\n@@ -202,7 +202,7 @@ pub fn list_themes(cfg: &Config, config_dir: &Path, cache_dir: &Path) -> Result<\n \n     let default_theme = HighlightingAssets::default_theme();\n     for theme in assets.themes() {\n-        let default_theme_info = if default_theme == theme {\n+        let default_theme_info = if !config.loop_through && default_theme == theme {\n             \" (default)\"\n         } else {\n             \"\"\n", "test_patch": "diff --git a/tests/integration_tests.rs b/tests/integration_tests.rs\nindex 8df4327cd7..c083a941ba 100644\n--- a/tests/integration_tests.rs\n+++ b/tests/integration_tests.rs\n@@ -300,6 +300,7 @@ fn list_themes_without_colors() {\n \n     bat()\n         .arg(\"--color=never\")\n+        .arg(\"--decorations=always\") // trick bat into setting `Config::loop_through` to false\n         .arg(\"--list-themes\")\n         .assert()\n         .success()\n@@ -307,6 +308,15 @@ fn list_themes_without_colors() {\n         .stdout(predicate::str::contains(default_theme_chunk).normalize());\n }\n \n+#[test]\n+fn list_themes_to_piped_output() {\n+    bat()\n+        .arg(\"--list-themes\")\n+        .assert()\n+        .success()\n+        .stdout(predicate::str::contains(\"(default)\").not());\n+}\n+\n #[test]\n #[cfg_attr(any(not(feature = \"git\"), target_os = \"windows\"), ignore)]\n fn short_help() {\n", "problem_statement": "Shell auto-completion for `--theme` list the default theme incorrectly\n*(not at all severe)*\r\n\r\nWhen using auto-complete to complete the default theme in any shell with supported completions (I've tested fish, zsh and bash) the default theme is completed as `Monokai Extended (default)` instead of `Monokai Extended`.\r\n\r\nThis is because the completion scripts use `bat --list-themes | cat` to get the list of themes.\r\n#2937 changed the basic output to include the `(default)` suffix which breaks the completions.\r\n\r\n---\r\n\r\n**What steps will reproduce the bug?**\r\n\r\n1. Start bash, fish or zsh with bat's completions installed. See #3072 \r\n3. Type `bat --theme Monokai\\ Extended` and trigger auto-complete (press tab)\r\n\r\n**What happens?**\r\n\r\nThe suggestions include `Monokai Extended (default)` (not a valid theme name)\r\n\r\n**What did you expect to happen instead?**\r\n\r\nThe suggestions should include `Monokai Extended` instead.\r\n\r\n**How did you install `bat`?**\r\n\r\nFrom source: https://github.com/sharkdp/bat/commit/b662fec214daaa77a67f08d8fbd2e81915d6e0bc\r\n\n", "hints_text": "", "created_at": "2024-08-25 09:40:42", "merge_commit_sha": "eca6b8a3768ec3931a41330c42b138107b1786b5", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['x86_64-unknown-linux-gnu (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['x86_64-apple-darwin (macos-12)', '.github/workflows/CICD.yml']"], ["['Run tests with system wide configuration', '.github/workflows/CICD.yml']", "['cargo audit', '.github/workflows/CICD.yml']"], ["['Run tests with updated syntaxes and themes', '.github/workflows/CICD.yml']", "['x86_64-pc-windows-msvc (windows-2019)', '.github/workflows/CICD.yml']"], ["['i686-unknown-linux-musl (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['aarch64-unknown-linux-musl (ubuntu-20.04)', '.github/workflows/CICD.yml']"], ["['all-jobs', '.github/workflows/CICD.yml']", "['Publish to Winget', '.github/workflows/CICD.yml']"], ["['i686-unknown-linux-gnu (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['Check for changelog entry', '.github/workflows/require-changelog-for-PRs.yml']"]]}
{"repo": "Nukesor/pueue", "instance_id": "Nukesor__pueue-583", "base_commit": "d43beefff66d976bf53cde3d8546ab6da6967460", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 03e141234..bd2722c30 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -81,6 +81,7 @@ TLDR: The new task state representation is more verbose but significantly cleane\n - Add `command` filter to `pueue status`. [#524](https://github.com/Nukesor/pueue/issues/524) [#560](https://github.com/Nukesor/pueue/pull/560)\n - Allow `pueue status` to order tasks by `enqueue_at`. [#554](https://github.com/Nukesor/pueue/issues/554)\n - Added Windows service on Windows to allow a true daemon experience. [#344](https://github.com/Nukesor/pueue/issues/344) [#567](https://github.com/Nukesor/pueue/pull/567)\n+- Add `queued_count` and `stashed_count` to callback template variables. This allows users to fire callbacks when whole groups are finished. [#578](https://github.com/Nukesor/pueue/issues/578)\n \n ### Fixed\n \ndiff --git a/pueue/src/daemon/callbacks.rs b/pueue/src/daemon/callbacks.rs\nindex 0e05513f7..0884488b0 100644\n--- a/pueue/src/daemon/callbacks.rs\n+++ b/pueue/src/daemon/callbacks.rs\n@@ -21,7 +21,7 @@ pub fn spawn_callback(settings: &Settings, state: &mut LockedState, task: &Task)\n     };\n \n     // Build the command to be called from the template string in the configuration file.\n-    let callback_command = match build_callback_command(settings, task, template_string) {\n+    let callback_command = match build_callback_command(settings, state, task, template_string) {\n         Ok(callback_command) => callback_command,\n         Err(err) => {\n             error!(\"Failed to create callback command from template with error: {err}\");\n@@ -49,6 +49,7 @@ pub fn spawn_callback(settings: &Settings, state: &mut LockedState, task: &Task)\n /// finished task.\n pub fn build_callback_command(\n     settings: &Settings,\n+    state: &mut LockedState,\n     task: &Task,\n     template_string: &str,\n ) -> Result<String, RenderError> {\n@@ -62,7 +63,20 @@ pub fn build_callback_command(\n     parameters.insert(\"id\", task.id.to_string());\n     parameters.insert(\"command\", task.command.clone());\n     parameters.insert(\"path\", (*task.path.to_string_lossy()).to_owned());\n+\n+    // Add group information to template\n+    // This includes how many stashed and queued tasks are left in the group.\n     parameters.insert(\"group\", task.group.clone());\n+    let queued_tasks = state\n+        .filter_tasks_of_group(|task| task.is_queued(), &task.group)\n+        .matching_ids\n+        .len();\n+    parameters.insert(\"queued_count\", queued_tasks.to_string());\n+    let stashed_tasks = state\n+        .filter_tasks_of_group(|task| task.is_stashed(), &task.group)\n+        .matching_ids\n+        .len();\n+    parameters.insert(\"stashed_count\", stashed_tasks.to_string());\n \n     // Result takes the TaskResult Enum strings, unless it didn't finish yet.\n     if let TaskStatus::Done { result, .. } = &task.status {\ndiff --git a/pueue/src/daemon/process_handler/finish.rs b/pueue/src/daemon/process_handler/finish.rs\nindex a124f3551..292a18a7c 100644\n--- a/pueue/src/daemon/process_handler/finish.rs\n+++ b/pueue/src/daemon/process_handler/finish.rs\n@@ -97,6 +97,8 @@ pub fn handle_finished_tasks(settings: &Settings, state: &mut LockedState) {\n             None => TaskResult::Killed,\n         };\n \n+        info!(\"Task {task_id} finished with result: {result:?}\");\n+\n         // Update the tasks's state and return a clone for callback handling.\n         let task = {\n             let task = state\n@@ -147,7 +149,6 @@ fn get_finished(state: &mut LockedState) -> Vec<((usize, String, usize), Option<\n                 // Child process did not exit yet\n                 Ok(None) => continue,\n                 Ok(_exit_status) => {\n-                    info!(\"Task {task_id} just finished\");\n                     finished.push(((*task_id, group.clone(), *worker_id), None));\n                 }\n             }\n", "test_patch": "diff --git a/pueue/tests/daemon/integration/callback.rs b/pueue/tests/daemon/integration/callback.rs\nnew file mode 100644\nindex 000000000..c5021d9ed\n--- /dev/null\n+++ b/pueue/tests/daemon/integration/callback.rs\n@@ -0,0 +1,43 @@\n+use std::fs::read_to_string;\n+\n+use anyhow::{Context, Result};\n+\n+use crate::helper::*;\n+\n+/// Make sure that callback commands are executed while variables are\n+/// templated into the command as expected.\n+#[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n+async fn test_callback_variables() -> Result<()> {\n+    let (mut settings, tempdir) = daemon_base_setup()?;\n+\n+    // Configure the daemon to use a callback command that echos some variables into a file\n+    // that's located in the temporary runtime directory of the daemon.\n+    let tempdir_path = tempdir.path().to_path_buf();\n+    let echo_command =\n+        \"echo '{{queued_count}}\\n{{stashed_count}}\\n{{command}}\\n{{id}}\\n{{result}}'\";\n+    settings.daemon.callback = Some(format!(\n+        \"{echo_command} > {}/testfile\",\n+        tempdir_path.to_string_lossy()\n+    ));\n+    settings\n+        .save(&Some(tempdir_path.join(\"pueue.yml\")))\n+        .context(\"Couldn't write pueue config to temporary directory\")?;\n+\n+    // Create the daemon with the changed settings.\n+    let daemon = daemon_with_settings(settings, tempdir).await?;\n+    let shared = &daemon.settings.shared;\n+\n+    // Create one stashed task.\n+    assert_success(create_stashed_task(shared, \"stashed\", None).await?);\n+    // Create a task that'll then trigger the callback\n+    assert_success(add_task(shared, \"ls\").await?);\n+\n+    // Give the callback command some time to be executed.\n+    sleep_ms(3000).await;\n+\n+    let callback_output = read_to_string(tempdir_path.join(\"testfile\"))?;\n+\n+    assert_eq!(callback_output, \"0\\n1\\nls\\n1\\nSuccess\\n\");\n+\n+    Ok(())\n+}\ndiff --git a/pueue/tests/daemon/integration/mod.rs b/pueue/tests/daemon/integration/mod.rs\nindex 65099ac7d..509d6b5da 100644\n--- a/pueue/tests/daemon/integration/mod.rs\n+++ b/pueue/tests/daemon/integration/mod.rs\n@@ -1,5 +1,6 @@\n mod add;\n mod aliases;\n+mod callback;\n mod clean;\n mod edit;\n mod environment_variables;\ndiff --git a/pueue/tests/daemon/integration/stashed.rs b/pueue/tests/daemon/integration/stashed.rs\nindex beee041a8..915e81d3a 100644\n--- a/pueue/tests/daemon/integration/stashed.rs\n+++ b/pueue/tests/daemon/integration/stashed.rs\n@@ -4,43 +4,22 @@ use pueue_lib::state::GroupStatus;\n use rstest::rstest;\n \n use pueue_lib::network::message::*;\n-use pueue_lib::settings::Shared;\n use pueue_lib::task::*;\n \n use crate::helper::*;\n \n-/// Helper to pause the whole daemon\n-pub async fn add_stashed_task(\n-    shared: &Shared,\n-    command: &str,\n-    stashed: bool,\n-    enqueue_at: Option<DateTime<Local>>,\n-) -> Result<Message> {\n-    let mut message = create_add_message(shared, command);\n-    message.stashed = stashed;\n-    message.enqueue_at = enqueue_at;\n-\n-    send_message(shared, message)\n-        .await\n-        .context(\"Failed to to add task message\")\n-}\n-\n /// Tasks can be stashed and scheduled for being enqueued at a specific point in time.\n ///\n /// Furthermore these stashed tasks can then be manually enqueued again.\n #[rstest]\n-#[case(true, None)]\n-#[case(true, Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n-#[case(false, Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n+#[case(None)]\n+#[case(Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n-async fn test_enqueued_tasks(\n-    #[case] stashed: bool,\n-    #[case] enqueue_at: Option<DateTime<Local>>,\n-) -> Result<()> {\n+async fn test_enqueued_tasks(#[case] enqueue_at: Option<DateTime<Local>>) -> Result<()> {\n     let daemon = daemon().await?;\n     let shared = &daemon.settings.shared;\n \n-    assert_success(add_stashed_task(shared, \"sleep 10\", stashed, enqueue_at).await?);\n+    assert_success(create_stashed_task(shared, \"sleep 10\", enqueue_at).await?);\n \n     // The task should be added in stashed state.\n     let task = wait_for_task_condition(shared, 0, |task| task.is_stashed()).await?;\n@@ -77,10 +56,9 @@ async fn test_delayed_tasks() -> Result<()> {\n     let shared = &daemon.settings.shared;\n \n     // The task will be stashed and automatically enqueued after about 1 second.\n-    let response = add_stashed_task(\n+    let response = create_stashed_task(\n         shared,\n         \"sleep 10\",\n-        true,\n         Some(Local::now() + TimeDelta::try_seconds(1).unwrap()),\n     )\n     .await?;\ndiff --git a/pueue/tests/helper/mod.rs b/pueue/tests/helper/mod.rs\nindex bc0a96ec4..85cfa0532 100644\n--- a/pueue/tests/helper/mod.rs\n+++ b/pueue/tests/helper/mod.rs\n@@ -1,5 +1,7 @@\n //! This module contains helper functions, which are used by both, the client and daemon tests.\n+use ::log::{warn, LevelFilter};\n use anyhow::Result;\n+use simplelog::{Config, ConfigBuilder, TermLogger, TerminalMode};\n use tokio::io::{self, AsyncWriteExt};\n \n pub use pueue_lib::state::PUEUE_DEFAULT_GROUP;\n@@ -27,6 +29,31 @@ pub use wait::*;\n // Global acceptable test timeout\n const TIMEOUT: u64 = 5000;\n \n+/// Use this function to enable log output for in-runtime daemon output.\n+#[allow(dead_code)]\n+pub fn enable_logger() {\n+    let level = LevelFilter::Debug;\n+\n+    // Try to initialize the logger with the timezone set to the Local time of the machine.\n+    let mut builder = ConfigBuilder::new();\n+    let logger_config = match builder.set_time_offset_to_local() {\n+        Err(_) => {\n+            warn!(\"Failed to determine the local time of this machine. Fallback to UTC.\");\n+            Config::default()\n+        }\n+        Ok(builder) => builder.build(),\n+    };\n+\n+    // Init a terminal logger\n+    TermLogger::init(\n+        level,\n+        logger_config.clone(),\n+        TerminalMode::Stderr,\n+        simplelog::ColorChoice::Auto,\n+    )\n+    .unwrap()\n+}\n+\n /// A helper function to sleep for ms time.\n /// Only used to avoid the biolerplate of importing the same stuff all over the place.\n pub async fn sleep_ms(ms: u64) {\ndiff --git a/pueue/tests/helper/task.rs b/pueue/tests/helper/task.rs\nindex b7e877357..4cca395a9 100644\n--- a/pueue/tests/helper/task.rs\n+++ b/pueue/tests/helper/task.rs\n@@ -3,6 +3,7 @@ use std::env::vars;\n \n use anyhow::{anyhow, Context, Result};\n \n+use chrono::{DateTime, Local};\n use pueue_lib::network::message::*;\n use pueue_lib::settings::*;\n use pueue_lib::task::{Task, TaskStatus};\n@@ -27,6 +28,21 @@ pub fn create_add_message(shared: &Shared, command: &str) -> AddMessage {\n     }\n }\n \n+/// Helper to create a stashed task\n+pub async fn create_stashed_task(\n+    shared: &Shared,\n+    command: &str,\n+    enqueue_at: Option<DateTime<Local>>,\n+) -> Result<Message> {\n+    let mut message = create_add_message(shared, command);\n+    message.stashed = true;\n+    message.enqueue_at = enqueue_at;\n+\n+    send_message(shared, message)\n+        .await\n+        .context(\"Failed to to add task message\")\n+}\n+\n /// Helper to either continue the daemon or start specific tasks\n pub async fn start_tasks(shared: &Shared, tasks: TaskSelection) -> Result<Message> {\n     let message = StartMessage { tasks };\n", "problem_statement": "Run an extra command whenever the queue is empty\n### A detailed description of the feature you would like to see added.\r\n\r\nHi thanks for the tool! My (summarized) use case is like this: Submit a few deep learning jobs to my workstation, and when all are done (i.e. GPU is finally idle), send me a Slack message such that I can come and use the idle GPU. For example, I will need the idle GPU to tentatively run some new commands, debug them, and when they look good, submit it to the queue.\r\n\r\nThus, it would be great to have a feature that, when the job is empty, run an extra command to notify users.\r\n\r\nEDIT: Looks like I can use the \"Callback\" feature to handle it. Seems need to query `pueue` to know whether the queue is empty now, thus it would be great if this information could be provided as a variable.\r\n\r\nEDIT: Looks like it can be done by e.g. https://github.com/Nukesor/pueue/blob/9a402d5f895fdf489e7b8006375946080ea5950c/pueue/src/daemon/callbacks.rs#L62 add a line here, giving `state.tasks.len()`.\r\n\r\n### Explain your usecase of the requested feature\r\n\r\n(see above)\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\n", "hints_text": "(The content in the root post is edited for more details)\nI see how that can be useful.\r\n\r\nHow about, we add a simple boolean to the template variables. Something like `group_finished`?\r\nI'm against just using `state.tasks.len()` as users will probably be interested in which group just finished. This will give them more control on what should happen based on the finished group.\r\n\r\nThe `group` is already part of the variables that're available.\nYou could then do stuff like\r\n\r\n```sh\r\nif [[ \"true\" = \"{{ group_finished }}\" ]]; then\r\n  if [[ \"default\" = \"{{ group }}\" ]]; then\r\n    # Do your stuff\r\n  fi\r\nfi\r\n```\n> we add a simple boolean to the template variables. Something like group_finished\r\n\r\nThanks, that looks pretty reasonable!\nOne more question.\r\n\r\nWhen should a queue be considered finished?\r\nI feel at least when there're no more `queued`, `running` or `paused` entries. But what about stashed entries with a delay? I guess a group shouldn't be considered \"finished\" if there's still a task that'll be queued sometime in the future.\r\n\r\n`stashed` entries without a delay oh the other hand should probably be ignored?\nHmm, then maybe provide fine-grained details, such as `{{ num_queued }}`, `{{ num_stashed }}`, etc?", "created_at": "2024-12-01 14:55:51", "merge_commit_sha": "8cc78935513920d86b593e848e0807c79570af73", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test on ubuntu-latest for arm-unknown-linux-musleabihf', '.github/workflows/test.yml']", "['Test on macos-latest for aarch64-apple-darwin', '.github/workflows/test.yml']"], ["['Lint on macos-latest for aarch64-apple-darwin', '.github/workflows/lint.yml']", "['Test on ubuntu-latest for aarch64-unknown-linux-musl', '.github/workflows/test.yml']"], ["['Lint on ubuntu-latest for x86_64-unknown-linux-gnu', '.github/workflows/lint.yml']", "['Lint on macos-latest for x86_64-apple-darwin', '.github/workflows/lint.yml']"], ["['Event File', '.github/workflows/test.yml']", "['Test on ubuntu-latest for x86_64-unknown-linux-gnu', '.github/workflows/test.yml']"]]}
{"repo": "tokio-rs/tokio", "instance_id": "tokio-rs__tokio-6975", "base_commit": "480c010b01cf32e36d6cba307ca436fabb6d95fa", "patch": "diff --git a/spellcheck.dic b/spellcheck.dic\nindex f368d2d7214..ad73ea17bdc 100644\n--- a/spellcheck.dic\n+++ b/spellcheck.dic\n@@ -1,4 +1,4 @@\n-292\n+298\n &\n +\n <\n@@ -30,6 +30,8 @@\n 450ms\n 50ms\n 8MB\n+ABI\n+accessors\n adaptor\n adaptors\n Adaptors\n@@ -75,8 +77,11 @@ datagrams\n deallocate\n deallocated\n Deallocates\n+debuginfo\n decrementing\n+demangled\n dequeued\n+dereferenced\n deregister\n deregistered\n deregistering\n@@ -133,6 +138,7 @@ implementers\n implementor\n implementors\n incrementing\n+inlining\n interoperate\n invariants\n Invariants\n@@ -291,4 +297,3 @@ Wakers\n wakeup\n wakeups\n workstealing\n-\ndiff --git a/tokio/src/runtime/dump.rs b/tokio/src/runtime/dump.rs\nindex aea2381127b..d262f3987cc 100644\n--- a/tokio/src/runtime/dump.rs\n+++ b/tokio/src/runtime/dump.rs\n@@ -1,13 +1,13 @@\n //! Snapshots of runtime state.\n //!\n-//! See [Handle::dump][crate::runtime::Handle::dump].\n+//! See [`Handle::dump`][crate::runtime::Handle::dump].\n \n use crate::task::Id;\n-use std::fmt;\n+use std::{fmt, path::Path};\n \n /// A snapshot of a runtime's state.\n ///\n-/// See [Handle::dump][crate::runtime::Handle::dump].\n+/// See [`Handle::dump`][crate::runtime::Handle::dump].\n #[derive(Debug)]\n pub struct Dump {\n     tasks: Tasks,\n@@ -15,7 +15,7 @@ pub struct Dump {\n \n /// Snapshots of tasks.\n ///\n-/// See [Handle::dump][crate::runtime::Handle::dump].\n+/// See [`Handle::dump`][crate::runtime::Handle::dump].\n #[derive(Debug)]\n pub struct Tasks {\n     tasks: Vec<Task>,\n@@ -23,21 +23,199 @@ pub struct Tasks {\n \n /// A snapshot of a task.\n ///\n-/// See [Handle::dump][crate::runtime::Handle::dump].\n+/// See [`Handle::dump`][crate::runtime::Handle::dump].\n #[derive(Debug)]\n pub struct Task {\n     id: Id,\n     trace: Trace,\n }\n \n+/// Represents an address that should not be dereferenced.\n+///\n+/// This type exists to get the auto traits correct, the public API\n+/// uses raw pointers to make life easier for users.\n+#[derive(Copy, Clone, Debug)]\n+struct Address(*mut std::ffi::c_void);\n+\n+// Safe since Address should not be dereferenced\n+unsafe impl Send for Address {}\n+unsafe impl Sync for Address {}\n+\n+/// A backtrace symbol.\n+///\n+/// This struct provides accessors for backtrace symbols, similar to [`backtrace::BacktraceSymbol`].\n+#[derive(Clone, Debug)]\n+pub struct BacktraceSymbol {\n+    name: Option<Box<[u8]>>,\n+    name_demangled: Option<Box<str>>,\n+    addr: Option<Address>,\n+    filename: Option<std::path::PathBuf>,\n+    lineno: Option<u32>,\n+    colno: Option<u32>,\n+}\n+\n+impl BacktraceSymbol {\n+    pub(crate) fn from_backtrace_symbol(sym: &backtrace::BacktraceSymbol) -> Self {\n+        let name = sym.name();\n+        Self {\n+            name: name.as_ref().map(|name| name.as_bytes().into()),\n+            name_demangled: name.map(|name| format!(\"{}\", name).into()),\n+            addr: sym.addr().map(Address),\n+            filename: sym.filename().map(From::from),\n+            lineno: sym.lineno(),\n+            colno: sym.colno(),\n+        }\n+    }\n+\n+    /// Return the raw name of the symbol.\n+    pub fn name_raw(&self) -> Option<&[u8]> {\n+        self.name.as_deref()\n+    }\n+\n+    /// Return the demangled name of the symbol.\n+    pub fn name_demangled(&self) -> Option<&str> {\n+        self.name_demangled.as_deref()\n+    }\n+\n+    /// Returns the starting address of this symbol.\n+    pub fn addr(&self) -> Option<*mut std::ffi::c_void> {\n+        self.addr.map(|addr| addr.0)\n+    }\n+\n+    /// Returns the file name where this function was defined. If debuginfo\n+    /// is missing, this is likely to return None.\n+    pub fn filename(&self) -> Option<&Path> {\n+        self.filename.as_deref()\n+    }\n+\n+    /// Returns the line number for where this symbol is currently executing.\n+    ///\n+    /// If debuginfo is missing, this is likely to return `None`.\n+    pub fn lineno(&self) -> Option<u32> {\n+        self.lineno\n+    }\n+\n+    /// Returns the column number for where this symbol is currently executing.\n+    ///\n+    /// If debuginfo is missing, this is likely to return `None`.\n+    pub fn colno(&self) -> Option<u32> {\n+        self.colno\n+    }\n+}\n+\n+/// A backtrace frame.\n+///\n+/// This struct represents one stack frame in a captured backtrace, similar to [`backtrace::BacktraceFrame`].\n+#[derive(Clone, Debug)]\n+pub struct BacktraceFrame {\n+    ip: Address,\n+    symbol_address: Address,\n+    symbols: Box<[BacktraceSymbol]>,\n+}\n+\n+impl BacktraceFrame {\n+    pub(crate) fn from_resolved_backtrace_frame(frame: &backtrace::BacktraceFrame) -> Self {\n+        Self {\n+            ip: Address(frame.ip()),\n+            symbol_address: Address(frame.symbol_address()),\n+            symbols: frame\n+                .symbols()\n+                .iter()\n+                .map(BacktraceSymbol::from_backtrace_symbol)\n+                .collect(),\n+        }\n+    }\n+\n+    /// Return the instruction pointer of this frame.\n+    ///\n+    /// See the ABI docs for your platform for the exact meaning.\n+    pub fn ip(&self) -> *mut std::ffi::c_void {\n+        self.ip.0\n+    }\n+\n+    /// Returns the starting symbol address of the frame of this function.\n+    pub fn symbol_address(&self) -> *mut std::ffi::c_void {\n+        self.symbol_address.0\n+    }\n+\n+    /// Return an iterator over the symbols of this backtrace frame.\n+    ///\n+    /// Due to inlining, it is possible for there to be multiple [`BacktraceSymbol`] items relating\n+    /// to a single frame. The first symbol listed is the \"innermost function\",\n+    /// whereas the last symbol is the outermost (last caller).\n+    pub fn symbols(&self) -> impl Iterator<Item = &BacktraceSymbol> {\n+        self.symbols.iter()\n+    }\n+}\n+\n+/// A captured backtrace.\n+///\n+/// This struct provides access to each backtrace frame, similar to [`backtrace::Backtrace`].\n+#[derive(Clone, Debug)]\n+pub struct Backtrace {\n+    frames: Box<[BacktraceFrame]>,\n+}\n+\n+impl Backtrace {\n+    /// Return the frames in this backtrace, innermost (in a task dump,\n+    /// likely to be a leaf future's poll function) first.\n+    pub fn frames(&self) -> impl Iterator<Item = &BacktraceFrame> {\n+        self.frames.iter()\n+    }\n+}\n+\n /// An execution trace of a task's last poll.\n ///\n-/// See [Handle::dump][crate::runtime::Handle::dump].\n+/// <div class=\"warning\">\n+///\n+/// Resolving a backtrace, either via the [`Display`][std::fmt::Display] impl or via\n+/// [`resolve_backtraces`][Trace::resolve_backtraces], parses debuginfo, which is\n+/// possibly a CPU-expensive operation that can take a platform-specific but\n+/// long time to run - often over 100 milliseconds, especially if the current\n+/// process's binary is big. In some cases, the platform might internally cache some of the\n+/// debuginfo, so successive calls to `resolve_backtraces` might be faster than\n+/// the first call, but all guarantees are platform-dependent.\n+///\n+/// To avoid blocking the runtime, it is recommended\n+/// that you resolve backtraces inside of a [`spawn_blocking()`][crate::task::spawn_blocking]\n+/// and to have some concurrency-limiting mechanism to avoid unexpected performance impact.\n+/// </div>\n+///\n+/// See [`Handle::dump`][crate::runtime::Handle::dump].\n #[derive(Debug)]\n pub struct Trace {\n     inner: super::task::trace::Trace,\n }\n \n+impl Trace {\n+    /// Resolve and return a list of backtraces that are involved in polls in this trace.\n+    ///\n+    /// The exact backtraces included here are unstable and might change in the future,\n+    /// but you can expect one [`Backtrace`] for every call to\n+    /// [`poll`] to a bottom-level Tokio future - so if something like [`join!`] is\n+    /// used, there will be a backtrace for each future in the join.\n+    ///\n+    /// [`poll`]: std::future::Future::poll\n+    /// [`join!`]: macro@join\n+    pub fn resolve_backtraces(&self) -> Vec<Backtrace> {\n+        self.inner\n+            .backtraces()\n+            .iter()\n+            .map(|backtrace| {\n+                let mut backtrace = backtrace::Backtrace::from(backtrace.clone());\n+                backtrace.resolve();\n+                Backtrace {\n+                    frames: backtrace\n+                        .frames()\n+                        .iter()\n+                        .map(BacktraceFrame::from_resolved_backtrace_frame)\n+                        .collect(),\n+                }\n+            })\n+            .collect()\n+    }\n+}\n+\n impl Dump {\n     pub(crate) fn new(tasks: Vec<Task>) -> Self {\n         Self {\ndiff --git a/tokio/src/runtime/task/trace/mod.rs b/tokio/src/runtime/task/trace/mod.rs\nindex bb411f42d72..851e96a73a1 100644\n--- a/tokio/src/runtime/task/trace/mod.rs\n+++ b/tokio/src/runtime/task/trace/mod.rs\n@@ -138,6 +138,10 @@ impl Trace {\n     pub(crate) fn root<F>(future: F) -> Root<F> {\n         Root { future }\n     }\n+\n+    pub(crate) fn backtraces(&self) -> &[Backtrace] {\n+        &self.backtraces\n+    }\n }\n \n /// If this is a sub-invocation of [`Trace::capture`], capture a backtrace.\n", "test_patch": "", "problem_statement": "Support an API to extract backtrace information from taskdump\n\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nCurrently, the only supported output for taskdump is logging, as shown in the examples [here](https://github.com/tokio-rs/tokio/blob/4846959e8a534cf2aff63f8613e5f69a7d2ce5f8/examples/dump.rs#L50-L54). However, I would like to add a public API that can extract this backtrace information. By adding such an API, it will become possible to handle taskdump data programmatically, making it useful for various purposes.\r\n\r\n**Describe the solution you'd like**\r\nI would like to add a public API something like this:\r\n\r\n```rust\r\nstruct Symbol {\r\n  // one call stack info\r\n}\r\n\r\npub struct TraceTree {\r\n  symbol: Symbol,\r\n  children: Vec<TraceTree>\r\n}\r\n\r\nimpl Task {\r\n  pub fn trace_tree(&self) -> TraceTree { ... }\r\n}\r\n\r\nasync fn run {\r\n  let handle = tokio::runtime::Handle::current();\r\n  let dump = handle.dump().await;\r\n  for task in dump.tasks().iter() {\r\n    // get a trace tree per one tokio task.\r\n    let root: TraceTree = task.trace_tree();\r\n\t\t\r\n    // some traverse operations...\r\n  }\r\n}\r\n```\r\n\r\nWe already have an internal struct named [Tree](https://github.com/tokio-rs/tokio/blob/4846959e8a534cf2aff63f8613e5f69a7d2ce5f8/tokio/src/runtime/task/trace/tree.rs#L11), so I believe it's not too hard to provide such an API.\r\n\r\n**Additional context**\r\nrelated issue: https://github.com/tokio-rs/tokio/issues/5638\r\n\n", "hints_text": "cc @jswrenn \nIt's not hard to provide an API here, but it might be hard to provide the *right* API. The current `Tree` type is an implementation detail used to aid printing task dumps. It's not currently designed to function well as a public API for traversing the tracing tree. Rust doesn't have a standard interface for tree traversal, so we'll need to decide for ourselves what interface to expose. We'll also need to ensure that this API doesn't lock us into sub-optimal representation decisions.\nThanks for clarifying the context!\r\nYeah, deciding on the right api seems difficult, but I believe we should eventually reach.\nIs there a strong reason we don't just export an API that is of the form:\r\n```rust\r\npub struct TokioBacktraceFrame {\r\n    name: Option<Vec<u8>>,\r\n    name_demangled: Option<String>,\r\n    addr: Option<*mut c_void>,\r\n    filename: Option<PathBuf>,\r\n    lineno: Option<u32>,\r\n    colno: Option<u32>,\r\n}\r\n\r\nimpl TokioBacktraceFrame {\r\n    pub fn name_raw(&self) -> Option<&[u8]> { /* .. */ }\r\n    pub fn name_demangled(&self) -> Option<&str> { /* ... */ }\r\n    pub fn addr(&self) -> Option<*mut c_void> { /* ... */ }\r\n    pub fn filename(&self) -> Option<&Path> { /* ... */ }\r\n    pub fn lineno(&self) -> Option<u32> { /* ... */ }\r\n    pub fn colno(&self) -> Option<u32> { /* ... */ }\r\n}\r\n\r\nimpl Trace {\r\n    pub fn dump(&self) -> Vec<Vec<TokioBacktraceFrame>>;\r\n}\r\n```\r\n\r\nI believe that this will work with every form of taking backtraces (especially given that all fields are options). Possibly it will require some inefficiency, but we can always expose a more efficient API later.\nTokio does not and will not depend on serde.\nK, so without the derive(Serialize) but with exposing accessors. People will have to do another map if they want to turn it to a JSON which will have some negative performance implications, but I think this is acceptable.\nWe can probably expose some accessors.\nSo are you positive for a function returning a `Vec<Vec<TokioBacktraceFrame>>` for a `TokioBacktraceFrame` that is supposed to be a plain old data type with accessors?\nYes, we can add it.", "created_at": "2024-11-14 16:48:09", "merge_commit_sha": "48e07a6d1011825d3c8d90fb6351ad68ea479dfc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['minimal-versions', '.github/workflows/ci.yml']", "['wasm32-wasip1', '.github/workflows/ci.yml']"], ["['triage', '.github/workflows/labeler.yml']", "['check-spelling', '.github/workflows/ci.yml']"], ["['loom ALT multi-thread scheduler (loom_multi_thread_alt::group_a)', '.github/workflows/loom.yml']", "['Test tokio --all-features on i686-unknown-linux-gnu without AtomicU64', '.github/workflows/ci.yml']"], ["['test tokio full --unstable (ubuntu-latest)', '.github/workflows/ci.yml']", "['test all crates in the workspace with all features and panic=abort (ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Stress Test (simple_echo_tcp)', '.github/workflows/stress-test.yml']", "['loom ALT multi-thread scheduler (loom_multi_thread_alt::group_d)', '.github/workflows/loom.yml']"], ["['loom multi-thread scheduler (loom_multi_thread::group_b)', '.github/workflows/loom.yml']", "['Check tokio --feature-powerset --depth 2 on i686-unknown-linux-gnu without AtomicU64', '.github/workflows/ci.yml']"], ["['docs (windows-latest)', '.github/workflows/ci.yml']", "['Test Quinn (ubuntu-latest)', '.github/workflows/ci.yml']"], ["['features --unstable', '.github/workflows/ci.yml']", "['Test hyper (macos-latest)', '.github/workflows/ci.yml']"], ["['build loom tests', '.github/workflows/ci.yml']", "['test all crates in the workspace with all features (ubuntu-latest)', '.github/workflows/ci.yml']"], ["['check-external-types (ubuntu-latest)', '.github/workflows/ci.yml']", "['valgrind', '.github/workflows/ci.yml']"], ["['cross-test-with-parking_lot (armv7-unknown-linux-gnueabihf)', '.github/workflows/ci.yml']", "['test tokio full --unstable (windows-latest)', '.github/workflows/ci.yml']"], ["['cross-check (powerpc-unknown-linux-gnu)', '.github/workflows/ci.yml']", "['all systems go', '.github/workflows/ci.yml']"], ["['asan', '.github/workflows/ci.yml']", "['check unexpected lints and cfgs', '.github/workflows/ci.yml']"], ["['cross-test-with-parking_lot (i686-unknown-linux-gnu, --cfg tokio_taskdump)', '.github/workflows/ci.yml']", "['loom tokio::sync', '.github/workflows/loom.yml']"], ["['test all crates in the workspace with all features and panic=abort (macos-latest)', '.github/workflows/ci.yml']", "['loom current-thread scheduler', '.github/workflows/loom.yml']"], ["['loom multi-thread scheduler (loom_multi_thread::group_c)', '.github/workflows/loom.yml']", "['test tokio for wasm32-unknown-unknown', '.github/workflows/ci.yml']"], ["['Test Quinn (macos-latest)', '.github/workflows/ci.yml']", "['cross-test-with-parking_lot (armv5te-unknown-linux-gnueabi)', '.github/workflows/ci.yml']"], ["['test all crates in the workspace with all features and panic=abort (windows-latest)', '.github/workflows/ci.yml']", "['loom ALT multi-thread scheduler (loom_multi_thread_alt::group_c)', '.github/workflows/loom.yml']"], ["['cross-check (arm-linux-androideabi)', '.github/workflows/ci.yml']", "['build tokio for redox-os', '.github/workflows/ci.yml']"], ["['Test Quinn (windows-latest)', '.github/workflows/ci.yml']", "['loom multi-thread scheduler (loom_multi_thread::group_d)', '.github/workflows/loom.yml']"]]}
{"repo": "bilelmoussaoui/oo7", "instance_id": "bilelmoussaoui__oo7-151", "base_commit": "768f6537b5ac088691e9abeff8e20b21cf0422c6", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 95ff10a05..d71293e67 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -1175,6 +1175,7 @@ dependencies = [\n  \"endi\",\n  \"futures-lite\",\n  \"futures-util\",\n+ \"getrandom\",\n  \"hkdf\",\n  \"hmac\",\n  \"md-5\",\n@@ -1228,7 +1229,6 @@ version = \"0.3.0\"\n dependencies = [\n  \"ashpd\",\n  \"clap\",\n- \"getrandom\",\n  \"oo7\",\n  \"tokio\",\n  \"tracing\",\ndiff --git a/cargo-credential/src/main.rs b/cargo-credential/src/main.rs\nindex bfc8eb899..c9e5491ab 100644\n--- a/cargo-credential/src/main.rs\n+++ b/cargo-credential/src/main.rs\n@@ -50,7 +50,7 @@ impl SecretServiceCredential {\n                 let token = cargo_credential::read_token(options, registry)?.expose();\n \n                 if let Some(item) = items.first() {\n-                    item.set_secret(token, \"text/utf8\")\n+                    item.set_secret(token)\n                         .await\n                         .map_err(|err| Error::Other(Box::new(err)))?;\n                 } else {\n@@ -60,7 +60,6 @@ impl SecretServiceCredential {\n                             &attributes,\n                             token,\n                             true,\n-                            \"text/utf8\",\n                             None,\n                         )\n                         .await\ndiff --git a/cli/src/main.rs b/cli/src/main.rs\nindex 479b50e7c..12e9601e6 100644\n--- a/cli/src/main.rs\n+++ b/cli/src/main.rs\n@@ -184,7 +184,7 @@ impl Commands {\n                 };\n \n                 collection\n-                    .create_item(&label, &attributes, &secret, true, \"text/plain\", None)\n+                    .create_item(&label, &attributes, secret, true, None)\n                     .await?;\n             }\n             Commands::Lock => collection.lock(None).await?,\n@@ -245,14 +245,15 @@ async fn print_item(\n     as_hex: bool,\n ) -> Result<(), Error> {\n     use std::fmt::Write;\n+    let secret = item.secret().await?;\n+    let bytes = secret.as_bytes();\n     if secret_only {\n-        let bytes = item.secret().await?;\n         let mut stdout = std::io::stdout().lock();\n         if as_hex {\n-            let hex = hex::encode(&bytes);\n+            let hex = hex::encode(bytes);\n             stdout.write_all(hex.as_bytes())?;\n         } else {\n-            stdout.write_all(&bytes)?;\n+            stdout.write_all(bytes)?;\n         }\n         // Add a new line if we are writing to a tty\n         if stdout.is_terminal() {\n@@ -260,7 +261,6 @@ async fn print_item(\n         }\n     } else {\n         let label = item.label().await?;\n-        let bytes = item.secret().await?;\n         let mut attributes = item.attributes().await?;\n         let created = item.created().await?;\n         let modified = item.modified().await?;\n@@ -277,15 +277,15 @@ async fn print_item(\n \n         // we still fallback to hex if it is not a string\n         if as_hex {\n-            let hex = hex::encode(&bytes);\n+            let hex = hex::encode(bytes);\n             writeln!(&mut result, \"secret = {hex}\").unwrap();\n         } else {\n-            match std::str::from_utf8(&bytes) {\n+            match std::str::from_utf8(bytes) {\n                 Ok(secret) => {\n                     writeln!(&mut result, \"secret = {secret}\").unwrap();\n                 }\n                 Err(_) => {\n-                    let hex = hex::encode(&bytes);\n+                    let hex = hex::encode(bytes);\n                     writeln!(&mut result, \"secret = {hex}\").unwrap();\n                 }\n             }\ndiff --git a/client/Cargo.toml b/client/Cargo.toml\nindex 81acaf989..5ab7ae4b8 100644\n--- a/client/Cargo.toml\n+++ b/client/Cargo.toml\n@@ -28,6 +28,7 @@ digest = { version = \"0.10\", optional = true }\n endi.workspace = true\n futures-lite = { workspace = true, optional = true }\n futures-util.workspace = true\n+getrandom = \"0.2\"\n hkdf = { version = \"0.12\", optional = true }\n hmac = { version = \"0.12\", optional = true }\n md-5 = { version = \"0.10\", optional = true }\ndiff --git a/client/examples/basic.rs b/client/examples/basic.rs\nindex 654720001..5de5bfe8e 100644\n--- a/client/examples/basic.rs\n+++ b/client/examples/basic.rs\n@@ -7,7 +7,7 @@ async fn main() -> oo7::Result<()> {\n     let keyring = Keyring::new().await?;\n     let attributes = HashMap::from([(\"attr\", \"value\")]);\n     keyring\n-        .create_item(\"Some Label\", &attributes, b\"secret\", true)\n+        .create_item(\"Some Label\", &attributes, \"secret\", true)\n         .await?;\n \n     let items = keyring.search_items(&attributes).await?;\ndiff --git a/client/examples/basic_2.rs b/client/examples/basic_2.rs\nindex e7ead7b94..6e4ab53d8 100644\n--- a/client/examples/basic_2.rs\n+++ b/client/examples/basic_2.rs\n@@ -14,7 +14,7 @@ async fn main() -> oo7::Result<()> {\n     KEYRING\n         .get()\n         .unwrap()\n-        .create_item(\"Some Label\", &attributes, b\"secret\", true)\n+        .create_item(\"Some Label\", &attributes, \"secret\", true)\n         .await?;\n \n     let items = KEYRING.get().unwrap().search_items(&attributes).await?;\ndiff --git a/client/src/dbus/api/collection.rs b/client/src/dbus/api/collection.rs\nindex d07d0a70f..ab397a01a 100644\n--- a/client/src/dbus/api/collection.rs\n+++ b/client/src/dbus/api/collection.rs\n@@ -5,7 +5,7 @@ use futures_util::{Stream, StreamExt};\n use serde::Serialize;\n use zbus::zvariant::{ObjectPath, OwnedObjectPath, Type};\n \n-use super::{Item, Prompt, Properties, Secret, Unlockable, DESTINATION};\n+use super::{DBusSecret, Item, Prompt, Properties, Unlockable, DESTINATION};\n use crate::{\n     dbus::{Error, ServiceError},\n     AsAttributes,\n@@ -165,7 +165,7 @@ impl<'a> Collection<'a> {\n         &self,\n         label: &str,\n         attributes: &impl AsAttributes,\n-        secret: &Secret<'_>,\n+        secret: &DBusSecret<'_>,\n         replace: bool,\n         window_id: Option<WindowIdentifier>,\n     ) -> Result<Item<'a>, Error> {\ndiff --git a/client/src/dbus/api/item.rs b/client/src/dbus/api/item.rs\nindex cb313439c..24679405e 100644\n--- a/client/src/dbus/api/item.rs\n+++ b/client/src/dbus/api/item.rs\n@@ -4,7 +4,7 @@ use ashpd::WindowIdentifier;\n use serde::Serialize;\n use zbus::zvariant::{ObjectPath, OwnedObjectPath, Type};\n \n-use super::{secret::SecretInner, Prompt, Secret, Session, Unlockable, DESTINATION};\n+use super::{DBusSecret, Prompt, Session, Unlockable, DESTINATION};\n use crate::{\n     dbus::{Error, ServiceError},\n     AsAttributes,\n@@ -121,19 +121,19 @@ impl<'a> Item<'a> {\n     }\n \n     #[doc(alias = \"GetSecret\")]\n-    pub async fn secret(&self, session: &Session<'_>) -> Result<Secret<'_>, Error> {\n+    pub async fn secret(&self, session: &Session<'_>) -> Result<DBusSecret<'_>, Error> {\n         let inner = self\n             .inner()\n             .call_method(\"GetSecret\", &(session))\n             .await\n             .map_err::<ServiceError, _>(From::from)?\n             .body()\n-            .deserialize::<SecretInner>()?;\n-        Secret::from_inner(self.inner().connection(), inner).await\n+            .deserialize::<super::secret::DBusSecretInner>()?;\n+        DBusSecret::from_inner(self.inner().connection(), inner).await\n     }\n \n     #[doc(alias = \"SetSecret\")]\n-    pub async fn set_secret(&self, secret: &Secret<'_>) -> Result<(), Error> {\n+    pub async fn set_secret(&self, secret: &DBusSecret<'_>) -> Result<(), Error> {\n         self.inner()\n             .call_method(\"SetSecret\", &(secret,))\n             .await\ndiff --git a/client/src/dbus/api/mod.rs b/client/src/dbus/api/mod.rs\nindex a546a430a..0236511cf 100644\n--- a/client/src/dbus/api/mod.rs\n+++ b/client/src/dbus/api/mod.rs\n@@ -29,9 +29,9 @@ pub(crate) use properties::Properties;\n #[cfg(feature = \"unstable\")]\n #[cfg_attr(docsrs, doc(cfg(feature = \"unstable\")))]\n pub use properties::Properties;\n-pub use secret::Secret;\n+pub use secret::DBusSecret;\n #[cfg(feature = \"unstable\")]\n #[cfg_attr(docsrs, doc(cfg(feature = \"unstable\")))]\n-pub use secret::SecretInner;\n+pub use secret::DBusSecretInner;\n pub use service::Service;\n pub use session::Session;\ndiff --git a/client/src/dbus/api/secret.rs b/client/src/dbus/api/secret.rs\nindex 87371c500..5df7e64ee 100644\n--- a/client/src/dbus/api/secret.rs\n+++ b/client/src/dbus/api/secret.rs\n@@ -5,15 +5,21 @@ use zbus::zvariant::{OwnedObjectPath, Type};\n use zeroize::{Zeroize, ZeroizeOnDrop};\n \n use super::Session;\n-use crate::{crypto, dbus::Error, Key};\n+use crate::{\n+    crypto,\n+    dbus::Error,\n+    secret::{BLOB_CONTENT_TYPE, TEXT_CONTENT_TYPE},\n+    Key, Secret,\n+};\n \n #[derive(Debug, Serialize, Deserialize, Type)]\n #[zvariant(signature = \"(oayays)\")]\n-pub struct SecretInner(pub OwnedObjectPath, pub Vec<u8>, pub Vec<u8>, pub String);\n+/// Same as [`DBusSecret`] without tying the session path to a [`Session`] type.\n+pub struct DBusSecretInner(pub OwnedObjectPath, pub Vec<u8>, pub Vec<u8>, pub String);\n \n #[derive(Debug, Type, Zeroize, ZeroizeOnDrop)]\n #[zvariant(signature = \"(oayays)\")]\n-pub struct Secret<'a> {\n+pub struct DBusSecret<'a> {\n     #[zeroize(skip)]\n     pub(crate) session: Arc<Session<'a>>,\n     pub(crate) parameters: Vec<u8>,\n@@ -22,47 +28,59 @@ pub struct Secret<'a> {\n     pub(crate) content_type: String,\n }\n \n-impl<'a> Secret<'a> {\n-    pub(crate) fn new(\n-        session: Arc<Session<'a>>,\n-        secret: impl AsRef<[u8]>,\n-        content_type: &str,\n-    ) -> Self {\n+impl<'a> DBusSecret<'a> {\n+    pub(crate) fn new(session: Arc<Session<'a>>, secret: impl Into<Secret>) -> Self {\n+        let secret = secret.into();\n         Self {\n             session,\n             parameters: vec![],\n-            value: secret.as_ref().to_vec(),\n-            content_type: content_type.to_owned(),\n+            value: secret.as_bytes().to_vec(),\n+            content_type: secret.content_type().to_owned(),\n         }\n     }\n \n     pub(crate) fn new_encrypted(\n         session: Arc<Session<'a>>,\n-        secret: impl AsRef<[u8]>,\n-        content_type: &str,\n+        secret: impl Into<Secret>,\n         aes_key: &Key,\n     ) -> Self {\n         let iv = crypto::generate_iv();\n-        let secret = crypto::encrypt(secret.as_ref(), aes_key, &iv);\n+        let secret = secret.into();\n         Self {\n             session,\n+            value: crypto::encrypt(secret.as_bytes(), aes_key, &iv),\n             parameters: iv,\n-            value: secret,\n-            content_type: content_type.to_owned(),\n+            content_type: secret.content_type().to_owned(),\n         }\n     }\n \n     pub(crate) async fn from_inner(\n         cnx: &zbus::Connection,\n-        inner: SecretInner,\n-    ) -> Result<Secret<'_>, Error> {\n-        let secret = Secret {\n+        inner: DBusSecretInner,\n+    ) -> Result<Self, Error> {\n+        Ok(Self {\n             session: Arc::new(Session::new(cnx, inner.0).await?),\n             parameters: inner.1,\n             value: inner.2,\n             content_type: inner.3,\n+        })\n+    }\n+\n+    pub(crate) fn decrypt(&self, key: Option<&Arc<Key>>) -> Result<Secret, Error> {\n+        let value = match key {\n+            Some(key) => &crypto::decrypt(&self.value, key, &self.parameters),\n+            None => &self.value,\n         };\n-        Ok(secret)\n+\n+        match self.content_type.as_str() {\n+            TEXT_CONTENT_TYPE => Ok(Secret::Text(String::from_utf8(value.to_vec())?)),\n+            BLOB_CONTENT_TYPE => Ok(Secret::blob(value)),\n+            e => {\n+                #[cfg(feature = \"tracing\")]\n+                tracing::warn!(\"Unsupported content-type {e}, falling back to blob\");\n+                Ok(Secret::blob(value))\n+            }\n+        }\n     }\n \n     /// Session used to encode the secret\n@@ -86,7 +104,7 @@ impl<'a> Secret<'a> {\n     }\n }\n \n-impl Serialize for Secret<'_> {\n+impl Serialize for DBusSecret<'_> {\n     fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>\n     where\n         S: serde::Serializer,\n@@ -106,6 +124,6 @@ mod tests {\n \n     #[test]\n     fn signature() {\n-        assert_eq!(Secret::SIGNATURE, \"(oayays)\");\n+        assert_eq!(DBusSecret::SIGNATURE, \"(oayays)\");\n     }\n }\ndiff --git a/client/src/dbus/api/service.rs b/client/src/dbus/api/service.rs\nindex d10815ed5..78aca075e 100644\n--- a/client/src/dbus/api/service.rs\n+++ b/client/src/dbus/api/service.rs\n@@ -5,8 +5,7 @@ use futures_util::{Stream, StreamExt};\n use zbus::zvariant::{ObjectPath, OwnedObjectPath, OwnedValue, Type, Value};\n \n use super::{\n-    secret::SecretInner, Collection, Item, Prompt, Properties, Secret, Session, Unlockable,\n-    DESTINATION, PATH,\n+    Collection, DBusSecret, Item, Prompt, Properties, Session, Unlockable, DESTINATION, PATH,\n };\n use crate::{\n     dbus::{Algorithm, Error, ServiceError},\n@@ -211,14 +210,14 @@ impl<'a> Service<'a> {\n         &self,\n         items: &[Item<'_>],\n         session: &Session<'_>,\n-    ) -> Result<HashMap<Item<'_>, Secret<'_>>, Error> {\n+    ) -> Result<HashMap<Item<'_>, DBusSecret<'_>>, Error> {\n         let secrets = self\n             .inner()\n             .call_method(\"GetSecrets\", &(items, session))\n             .await\n             .map_err::<ServiceError, _>(From::from)?\n             .body()\n-            .deserialize::<HashMap<OwnedObjectPath, SecretInner>>()?;\n+            .deserialize::<HashMap<OwnedObjectPath, super::secret::DBusSecretInner>>()?;\n \n         let cnx = self.inner().connection();\n         // Item's Hash implementation doesn't make use of any mutable internals\n@@ -227,7 +226,7 @@ impl<'a> Service<'a> {\n         for (path, secret_inner) in secrets {\n             output.insert(\n                 Item::new(cnx, path).await?,\n-                Secret::from_inner(cnx, secret_inner).await?,\n+                DBusSecret::from_inner(cnx, secret_inner).await?,\n             );\n         }\n \ndiff --git a/client/src/dbus/collection.rs b/client/src/dbus/collection.rs\nindex 64cfc1fd3..f1338a0ed 100644\n--- a/client/src/dbus/collection.rs\n+++ b/client/src/dbus/collection.rs\n@@ -9,7 +9,7 @@ use tokio::sync::RwLock;\n use zbus::zvariant::{ObjectPath, OwnedObjectPath};\n \n use super::{api, Algorithm, Error, Item};\n-use crate::{AsAttributes, Key};\n+use crate::{AsAttributes, Key, Secret};\n \n /// A collection allows to store and retrieve items.\n ///\n@@ -151,28 +151,22 @@ impl<'a> Collection<'a> {\n     /// * `secret` - The secret to store.\n     /// * `replace` - Whether to replace the value if the `attributes` matches\n     ///   an existing `secret`.\n-    /// * `content_type` - The content type of the secret, usually something\n-    ///   like `text/plain`.\n     pub async fn create_item(\n         &self,\n         label: &str,\n         attributes: &impl AsAttributes,\n-        secret: impl AsRef<[u8]>,\n+        secret: impl Into<Secret>,\n         replace: bool,\n-        content_type: &str,\n         window_id: Option<WindowIdentifier>,\n     ) -> Result<Item<'a>, Error> {\n         if !self.is_available().await {\n             Err(Error::Deleted)\n         } else {\n             let secret = match self.algorithm {\n-                Algorithm::Plain => {\n-                    api::Secret::new(Arc::clone(&self.session), secret, content_type)\n-                }\n-                Algorithm::Encrypted => api::Secret::new_encrypted(\n+                Algorithm::Plain => api::DBusSecret::new(Arc::clone(&self.session), secret),\n+                Algorithm::Encrypted => api::DBusSecret::new_encrypted(\n                     Arc::clone(&self.session),\n                     secret,\n-                    content_type,\n                     self.aes_key.as_ref().unwrap(),\n                 ),\n             };\n@@ -275,18 +269,18 @@ mod tests {\n             \"plain-type-test\"\n         };\n         attributes.insert(\"type\", value);\n-        let secret = \"a password\".as_bytes();\n+        let secret = crate::Secret::text(\"a password\");\n \n         let collection = service.default_collection().await.unwrap();\n         let n_items = collection.items().await.unwrap().len();\n         let n_search_items = collection.search_items(&attributes).await.unwrap().len();\n \n         let item = collection\n-            .create_item(\"A secret\", &attributes, secret, true, \"text/plain\", None)\n+            .create_item(\"A secret\", &attributes, secret.clone(), true, None)\n             .await\n             .unwrap();\n \n-        assert_eq!(*item.secret().await.unwrap(), secret);\n+        assert_eq!(item.secret().await.unwrap(), secret);\n         assert_eq!(item.attributes().await.unwrap()[\"type\"], value);\n \n         assert_eq!(collection.items().await.unwrap().len(), n_items + 1);\ndiff --git a/client/src/dbus/error.rs b/client/src/dbus/error.rs\nindex bc112883e..23890de33 100644\n--- a/client/src/dbus/error.rs\n+++ b/client/src/dbus/error.rs\n@@ -1,4 +1,4 @@\n-use std::fmt;\n+use std::{fmt, string::FromUtf8Error};\n \n /// DBus Secret Service specific errors.\n /// <https://specifications.freedesktop.org/secret-service-spec/latest/errors.html>\n@@ -31,6 +31,8 @@ pub enum Error {\n     NotFound(String),\n     /// Input/Output.\n     IO(std::io::Error),\n+    /// Secret to string conversion failure.\n+    Utf8(FromUtf8Error),\n }\n \n impl From<zbus::Error> for Error {\n@@ -63,6 +65,12 @@ impl From<std::io::Error> for Error {\n     }\n }\n \n+impl From<FromUtf8Error> for Error {\n+    fn from(value: FromUtf8Error) -> Self {\n+        Self::Utf8(value)\n+    }\n+}\n+\n impl std::error::Error for Error {}\n \n impl fmt::Display for Error {\n@@ -74,6 +82,7 @@ impl fmt::Display for Error {\n             Self::Deleted => write!(f, \"Item/Collection was deleted, can no longer be used\"),\n             Self::NotFound(name) => write!(f, \"The collection '{name}' doesn't exists\"),\n             Self::Dismissed => write!(f, \"Prompt was dismissed\"),\n+            Self::Utf8(e) => write!(f, \"Failed to convert a text/plain secret to string, {e}\"),\n         }\n     }\n }\ndiff --git a/client/src/dbus/item.rs b/client/src/dbus/item.rs\nindex f19258a71..8bf6a1b05 100644\n--- a/client/src/dbus/item.rs\n+++ b/client/src/dbus/item.rs\n@@ -6,10 +6,9 @@ use async_lock::RwLock;\n #[cfg(feature = \"tokio\")]\n use tokio::sync::RwLock;\n use zbus::zvariant::ObjectPath;\n-use zeroize::Zeroizing;\n \n use super::{api, Algorithm, Error};\n-use crate::{crypto, AsAttributes, Key};\n+use crate::{AsAttributes, Key, Secret};\n \n /// A secret with a label and attributes to identify it.\n ///\n@@ -134,23 +133,14 @@ impl<'a> Item<'a> {\n     }\n \n     /// Retrieve the currently stored secret.\n-    pub async fn secret(&self) -> Result<Zeroizing<Vec<u8>>, Error> {\n+    pub async fn secret(&self) -> Result<Secret, Error> {\n         if !self.is_available().await {\n             Err(Error::Deleted)\n         } else {\n-            let secret = self.inner.secret(&self.session).await?;\n-\n-            let value = match self.algorithm {\n-                Algorithm::Plain => Zeroizing::new(secret.value.to_owned()),\n-                Algorithm::Encrypted => {\n-                    let iv = &secret.parameters;\n-                    // Safe unwrap as it is encrypted\n-                    let aes_key = self.aes_key.as_ref().unwrap();\n-\n-                    crypto::decrypt(&secret.value, aes_key, iv)\n-                }\n-            };\n-            Ok(value)\n+            self.inner\n+                .secret(&self.session)\n+                .await?\n+                .decrypt(self.aes_key.as_ref())\n         }\n     }\n \n@@ -159,19 +149,13 @@ impl<'a> Item<'a> {\n     /// # Arguments\n     ///\n     /// * `secret` - The secret to store.\n-    /// * `content_type` - The content type of the secret, usually something\n-    ///   like `text/plain`.\n     #[doc(alias = \"SetSecret\")]\n-    pub async fn set_secret(\n-        &self,\n-        secret: impl AsRef<[u8]>,\n-        content_type: &str,\n-    ) -> Result<(), Error> {\n+    pub async fn set_secret(&self, secret: impl Into<Secret>) -> Result<(), Error> {\n         let secret = match self.algorithm {\n-            Algorithm::Plain => api::Secret::new(Arc::clone(&self.session), secret, content_type),\n+            Algorithm::Plain => api::DBusSecret::new(Arc::clone(&self.session), secret),\n             Algorithm::Encrypted => {\n                 let aes_key = self.aes_key.as_ref().unwrap();\n-                api::Secret::new_encrypted(Arc::clone(&self.session), secret, content_type, aes_key)\n+                api::DBusSecret::new_encrypted(Arc::clone(&self.session), secret, aes_key)\n             }\n         };\n         self.inner.set_secret(&secret).await?;\ndiff --git a/client/src/dbus/mod.rs b/client/src/dbus/mod.rs\nindex 1e4291729..071915a2c 100644\n--- a/client/src/dbus/mod.rs\n+++ b/client/src/dbus/mod.rs\n@@ -14,20 +14,13 @@\n //! let collection = service.default_collection().await?;\n //! // Store a secret\n //! collection\n-//!     .create_item(\n-//!         \"My App's secret\",\n-//!         &attributes,\n-//!         b\"password\",\n-//!         true,\n-//!         \"text/plain\",\n-//!         None,\n-//!     )\n+//!     .create_item(\"My App's secret\", &attributes, \"password\", true, None)\n //!     .await?;\n //!\n //! // Retrieve it later thanks to it attributes\n //! let items = collection.search_items(&attributes).await?;\n //! let item = items.first().unwrap();\n-//! assert_eq!(*item.secret().await?, b\"password\");\n+//! assert_eq!(item.secret().await?, oo7::Secret::text(\"password\"));\n //!\n //! #   Ok(())\n //! # }\ndiff --git a/client/src/file/api/legacy_keyring.rs b/client/src/file/api/legacy_keyring.rs\nindex a8c973a88..54a1298fa 100644\n--- a/client/src/file/api/legacy_keyring.rs\n+++ b/client/src/file/api/legacy_keyring.rs\n@@ -247,13 +247,12 @@ mod tests {\n             .join(\"legacy.keyring\");\n         let blob = std::fs::read(path)?;\n         let keyring = Keyring::try_from(blob.as_slice())?;\n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let items = keyring.decrypt_items(&secret)?;\n \n         assert_eq!(items.len(), 1);\n         assert_eq!(items[0].label(), \"foo\");\n-        assert_eq!(items[0].secret().as_ref(), b\"foo\".to_vec());\n+        assert_eq!(items[0].secret(), Secret::blob(\"foo\"));\n         let attributes = items[0].attributes();\n         assert_eq!(attributes.len(), 1);\n         assert_eq!(\ndiff --git a/client/src/file/api/mod.rs b/client/src/file/api/mod.rs\nindex 01d549b5d..01f84f764 100644\n--- a/client/src/file/api/mod.rs\n+++ b/client/src/file/api/mod.rs\n@@ -341,7 +341,7 @@ mod tests {\n \n         keyring\n             .items\n-            .push(Item::new(\"Label\", &needle, b\"MyPassword\").encrypt(&key)?);\n+            .push(Item::new(\"Label\", &needle, Secret::blob(\"MyPassword\")).encrypt(&key)?);\n \n         assert_eq!(keyring.search_items(&needle, &key)?.len(), 1);\n \n@@ -363,7 +363,7 @@ mod tests {\n             Item::new(\n                 \"My Label\",\n                 &HashMap::from([(\"my-tag\", \"my tag value\")]),\n-                \"A Password\".as_bytes(),\n+                \"A Password\",\n             )\n             .encrypt(&key)?,\n         );\n@@ -375,7 +375,7 @@ mod tests {\n         let loaded_items =\n             loaded_keyring.search_items(&HashMap::from([(\"my-tag\", \"my tag value\")]), &key)?;\n \n-        assert_eq!(*loaded_items[0].secret(), \"A Password\".as_bytes());\n+        assert_eq!(loaded_items[0].secret(), Secret::blob(\"A Password\"));\n \n         let _silent = std::fs::remove_file(\"/tmp/test.keyring\");\n \ndiff --git a/client/src/file/item.rs b/client/src/file/item.rs\nindex 76306fe48..5d2e8d6f8 100644\n--- a/client/src/file/item.rs\n+++ b/client/src/file/item.rs\n@@ -7,7 +7,7 @@ use super::{\n     api::{AttributeValue, EncryptedItem, GVARIANT_ENCODING},\n     Error,\n };\n-use crate::{crypto, AsAttributes, Key};\n+use crate::{crypto, AsAttributes, Key, Secret};\n \n /// An item stored in the file backend.\n #[derive(Deserialize, Serialize, zvariant::Type, Clone, Debug, Zeroize, ZeroizeOnDrop)]\n@@ -27,7 +27,7 @@ impl Item {\n     pub(crate) fn new(\n         label: impl ToString,\n         attributes: &impl AsAttributes,\n-        secret: impl AsRef<[u8]>,\n+        secret: impl Into<Secret>,\n     ) -> Self {\n         let now = std::time::SystemTime::UNIX_EPOCH\n             .elapsed()\n@@ -43,7 +43,7 @@ impl Item {\n             label: label.to_string(),\n             created: now,\n             modified: now,\n-            secret: secret.as_ref().to_vec(),\n+            secret: secret.into().as_bytes().to_vec(),\n         }\n     }\n \n@@ -76,17 +76,17 @@ impl Item {\n     }\n \n     /// Retrieve the currently stored secret.\n-    pub fn secret(&self) -> Zeroizing<Vec<u8>> {\n-        Zeroizing::new(self.secret.clone())\n+    pub fn secret(&self) -> Secret {\n+        Secret::blob(&self.secret)\n     }\n \n     /// Store a new secret.\n-    pub fn set_secret(&mut self, secret: impl AsRef<[u8]>) {\n+    pub fn set_secret(&mut self, secret: impl Into<Secret>) {\n         self.modified = std::time::SystemTime::UNIX_EPOCH\n             .elapsed()\n             .unwrap()\n             .as_secs();\n-        self.secret = secret.as_ref().to_vec();\n+        self.secret = secret.into().as_bytes().to_vec();\n     }\n \n     /// The UNIX time when the item was created.\ndiff --git a/client/src/file/mod.rs b/client/src/file/mod.rs\nindex c0363813c..03b4a7cb6 100644\n--- a/client/src/file/mod.rs\n+++ b/client/src/file/mod.rs\n@@ -11,7 +11,7 @@\n //!     .create_item(\n //!         \"My Label\",\n //!         &HashMap::from([(\"account\", \"alice\")]),\n-//!         b\"My Password\",\n+//!         \"My Password\",\n //!         true,\n //!     )\n //!     .await?;\n@@ -19,7 +19,7 @@\n //! let items = keyring\n //!     .search_items(&HashMap::from([(\"account\", \"alice\")]))\n //!     .await?;\n-//! assert_eq!(*items[0].secret(), b\"My Password\");\n+//! assert_eq!(items[0].secret(), oo7::Secret::blob(\"My Password\"));\n //!\n //! keyring\n //!     .delete(&HashMap::from([(\"account\", \"alice\")]))\n@@ -48,9 +48,8 @@ use tokio::{\n     io::AsyncReadExt,\n     sync::{Mutex, RwLock},\n };\n-use zeroize::Zeroizing;\n \n-use crate::{AsAttributes, Key};\n+use crate::{AsAttributes, Key, Secret};\n \n #[cfg(feature = \"unstable\")]\n #[cfg_attr(docsrs, doc(cfg(feature = \"unstable\")))]\n@@ -62,13 +61,11 @@ pub(crate) use api::AttributeValue;\n \n mod error;\n mod item;\n-mod secret;\n \n pub use error::{Error, InvalidItemError, WeakKeyError};\n pub use item::Item;\n-pub use secret::Secret;\n \n-type ItemDefinition = (String, HashMap<String, String>, Zeroizing<Vec<u8>>, bool);\n+type ItemDefinition = (String, HashMap<String, String>, Secret, bool);\n \n /// File backed keyring.\n #[derive(Debug)]\n@@ -302,7 +299,7 @@ impl Keyring {\n         &self,\n         label: &str,\n         attributes: &impl AsAttributes,\n-        secret: impl AsRef<[u8]>,\n+        secret: impl Into<Secret>,\n         replace: bool,\n     ) -> Result<Item, Error> {\n         let item = {\n@@ -367,7 +364,7 @@ impl Keyring {\n             if replace {\n                 keyring.remove_items(&attributes, &key)?;\n             }\n-            let item = Item::new(label, &attributes, &*secret);\n+            let item = Item::new(label, &attributes, secret);\n             let encrypted_item = item.encrypt(&key)?;\n             keyring.items.push(encrypted_item);\n         }\n@@ -577,7 +574,7 @@ mod tests {\n         let items = items.expect(\"unable to retrieve items\");\n         assert_eq!(items.len(), 1);\n         assert_eq!(items[0].label(), \"foo\");\n-        assert_eq!(items[0].secret().as_ref(), b\"foo\".to_vec());\n+        assert_eq!(items[0].secret(), Secret::blob(\"foo\"));\n         let attributes = items[0].attributes();\n         assert_eq!(attributes.len(), 1);\n         assert_eq!(\n@@ -606,8 +603,7 @@ mod tests {\n \n         assert!(!v1_dir.join(\"default.keyring\").exists());\n \n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let keyring = Keyring::open(\"default\", secret).await?;\n \n         check_items(&keyring).await?;\n@@ -632,8 +628,7 @@ mod tests {\n \n         std::env::set_var(\"XDG_DATA_HOME\", data_dir.path());\n \n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let keyring = Keyring::open(\"default\", secret).await?;\n \n         assert!(!v1_dir.join(\"default.keyring\").exists());\n@@ -660,15 +655,13 @@ mod tests {\n \n         std::env::set_var(\"XDG_DATA_HOME\", data_dir.path());\n \n-        let password = b\"wrong\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"wrong\");\n         let keyring = Keyring::open(\"default\", secret).await;\n \n         assert!(keyring.is_err());\n         assert!(matches!(keyring.unwrap_err(), Error::IncorrectSecret));\n \n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let keyring = Keyring::open(\"default\", secret).await;\n \n         assert!(keyring.is_ok());\n@@ -690,8 +683,7 @@ mod tests {\n \n         std::env::set_var(\"XDG_DATA_HOME\", data_dir.path());\n \n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let keyring = Keyring::open(\"default\", secret).await?;\n \n         assert!(v1_dir.join(\"default.keyring\").exists());\n@@ -713,8 +705,7 @@ mod tests {\n \n         std::env::set_var(\"XDG_DATA_HOME\", data_dir.path());\n \n-        let password = b\"test\";\n-        let secret = Secret::from(password.to_vec());\n+        let secret = Secret::blob(\"test\");\n         let keyring = Keyring::open(\"default\", secret).await?;\n \n         assert!(!v1_dir.join(\"default.keyring\").exists());\n@@ -723,7 +714,7 @@ mod tests {\n             .create_item(\n                 \"foo\",\n                 &HashMap::from([(crate::XDG_SCHEMA_ATTRIBUTE, \"org.gnome.keyring.Note\")]),\n-                b\"foo\",\n+                \"foo\",\n                 false,\n             )\n             .await?;\n@@ -744,11 +735,11 @@ mod tests {\n             .create_item(\"test\", &attributes, \"password\", false)\n             .await?;\n \n-        let new_secret = Secret::from(b\"password\".to_vec());\n-        keyring.change_secret(new_secret).await?;\n+        let secret = Secret::blob(\"new_secret\");\n+        keyring.change_secret(secret).await?;\n \n-        let new_secret = Secret::from(b\"password\".to_vec());\n-        let keyring = Keyring::load(&path, new_secret).await?;\n+        let secret = Secret::blob(\"new_secret\");\n+        let keyring = Keyring::load(&path, secret).await?;\n         let item_now = keyring.lookup_item(&attributes).await?.unwrap();\n \n         assert_eq!(item_before.label(), item_now.label());\ndiff --git a/client/src/file/secret.rs b/client/src/file/secret.rs\ndeleted file mode 100644\nindex 63c735c4f..000000000\n--- a/client/src/file/secret.rs\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-use rand::Rng;\n-use zeroize::{Zeroize, ZeroizeOnDrop};\n-\n-/// Secret used to unlock the keyring.\n-#[derive(Debug, Zeroize, ZeroizeOnDrop)]\n-pub struct Secret(Vec<u8>);\n-\n-impl From<Vec<u8>> for Secret {\n-    fn from(secret: Vec<u8>) -> Self {\n-        Self(secret)\n-    }\n-}\n-\n-impl std::ops::Deref for Secret {\n-    type Target = [u8];\n-\n-    fn deref(&self) -> &Self::Target {\n-        &self.0\n-    }\n-}\n-\n-impl Secret {\n-    pub fn random() -> Self {\n-        Self(rand::thread_rng().gen::<[u8; 8]>().to_vec())\n-    }\n-}\ndiff --git a/client/src/keyring.rs b/client/src/keyring.rs\nindex 78af0cd82..7c2827144 100644\n--- a/client/src/keyring.rs\n+++ b/client/src/keyring.rs\n@@ -4,9 +4,8 @@ use std::{collections::HashMap, sync::Arc, time::Duration};\n use async_lock::RwLock;\n #[cfg(feature = \"tokio\")]\n use tokio::sync::RwLock;\n-use zeroize::Zeroizing;\n \n-use crate::{dbus, file, AsAttributes, Result};\n+use crate::{dbus, file, AsAttributes, Result, Secret};\n \n /// A [Secret Service](crate::dbus) or [file](crate::file) backed keyring\n /// implementation.\n@@ -118,13 +117,13 @@ impl Keyring {\n         &self,\n         label: &str,\n         attributes: &impl AsAttributes,\n-        secret: impl AsRef<[u8]>,\n+        secret: impl Into<Secret>,\n         replace: bool,\n     ) -> Result<()> {\n         match self {\n             Self::DBus(backend) => {\n                 backend\n-                    .create_item(label, attributes, secret, replace, \"text/plain\", None)\n+                    .create_item(label, attributes, secret, replace, None)\n                     .await?;\n             }\n             Self::File(backend) => {\n@@ -194,7 +193,7 @@ impl Item {\n                     .create_item(\n                         item_guard.label(),\n                         &item_guard.attributes(),\n-                        &*item_guard.secret(),\n+                        item_guard.secret(),\n                         true,\n                     )\n                     .await?;\n@@ -234,7 +233,7 @@ impl Item {\n                     backend.replace_item_index(index, &item_guard).await?;\n                 } else {\n                     backend\n-                        .create_item(item_guard.label(), attributes, &*item_guard.secret(), true)\n+                        .create_item(item_guard.label(), attributes, item_guard.secret(), true)\n                         .await?;\n                 }\n             }\n@@ -244,7 +243,7 @@ impl Item {\n     }\n \n     /// Sets a new secret.\n-    pub async fn set_secret(&self, secret: impl AsRef<[u8]>) -> Result<()> {\n+    pub async fn set_secret(&self, secret: impl Into<Secret>) -> Result<()> {\n         match self {\n             Self::File(item, backend) => {\n                 item.write().await.set_secret(secret);\n@@ -254,18 +253,18 @@ impl Item {\n                     .create_item(\n                         item_guard.label(),\n                         &item_guard.attributes(),\n-                        &*item_guard.secret(),\n+                        item_guard.secret(),\n                         true,\n                     )\n                     .await?;\n             }\n-            Self::DBus(item) => item.set_secret(secret, \"text/plain\").await?,\n+            Self::DBus(item) => item.set_secret(secret).await?,\n         };\n         Ok(())\n     }\n \n     /// Retrieves the stored secret.\n-    pub async fn secret(&self) -> Result<Zeroizing<Vec<u8>>> {\n+    pub async fn secret(&self) -> Result<Secret> {\n         let secret = match self {\n             Self::File(item, _) => item.read().await.secret(),\n             Self::DBus(item) => item.secret().await?,\n@@ -352,8 +351,7 @@ mod tests {\n         fs::create_dir_all(&dir).await.unwrap();\n         let path = dir.join(\"default.keyring\");\n \n-        let password = b\"test\";\n-        let secret = file::Secret::from(password.to_vec());\n+        let secret = crate::Secret::text(\"test\");\n         let keyring = Keyring::File(file::Keyring::load(&path, secret).await?.into());\n \n         let items = keyring.items().await?;\n@@ -367,7 +365,7 @@ mod tests {\n         assert_eq!(items.len(), 1);\n         let item = items.remove(0);\n         assert_eq!(item.label().await?, \"my item\");\n-        assert_eq!(*item.secret().await?, b\"my_secret\");\n+        assert_eq!(item.secret().await?, Secret::blob(\"my_secret\"));\n         let attrs = item.attributes().await?;\n         assert_eq!(attrs.len(), 1);\n         assert_eq!(attrs.get(\"key\").unwrap(), \"value\");\n@@ -379,7 +377,7 @@ mod tests {\n         assert_eq!(items.len(), 1);\n         let item = items.remove(0);\n         assert_eq!(item.label().await?, \"my item\");\n-        assert_eq!(*item.secret().await?, b\"my_secret\");\n+        assert_eq!(item.secret().await?, Secret::blob(\"my_secret\"));\n         let attrs = item.attributes().await?;\n         assert_eq!(attrs.len(), 2);\n         assert_eq!(attrs.get(\"key\").unwrap(), \"changed_value\");\ndiff --git a/client/src/lib.rs b/client/src/lib.rs\nindex a4159246e..a0e29e837 100644\n--- a/client/src/lib.rs\n+++ b/client/src/lib.rs\n@@ -35,11 +35,13 @@ pub mod dbus;\n pub mod file;\n \n mod keyring;\n+mod secret;\n \n pub use ashpd;\n pub use error::{Error, Result};\n pub use keyring::{Item, Keyring};\n pub use migration::migrate;\n+pub use secret::Secret;\n pub use zbus;\n \n /// A schema attribute.\ndiff --git a/client/src/secret.rs b/client/src/secret.rs\nnew file mode 100644\nindex 000000000..e38dc490f\n--- /dev/null\n+++ b/client/src/secret.rs\n@@ -0,0 +1,105 @@\n+use zeroize::{Zeroize, ZeroizeOnDrop, Zeroizing};\n+\n+pub(crate) const TEXT_CONTENT_TYPE: &str = \"text/plain\";\n+pub(crate) const BLOB_CONTENT_TYPE: &str = \"application/octet-stream\";\n+\n+/// A safe wrapper around a combination of (secret, content-type).\n+#[derive(Debug, Clone, PartialEq, Eq, Zeroize, ZeroizeOnDrop)]\n+pub enum Secret {\n+    /// Corresponds to `text/plain`\n+    Text(String),\n+    /// Corresponds to application/octet-stream\n+    Blob(Vec<u8>),\n+}\n+\n+impl Secret {\n+    /// Generate a random secret, used when creating a session collection.\n+    pub fn random() -> Result<Self, getrandom::Error> {\n+        let mut secret = [0; 64];\n+        // Equivalent of `ring::rand::SecureRandom`\n+        getrandom::getrandom(&mut secret)?;\n+\n+        Ok(Self::blob(secret))\n+    }\n+\n+    /// Create a text secret, stored with `text/plain` content type.\n+    pub fn text(value: impl AsRef<str>) -> Self {\n+        Self::Text(value.as_ref().to_owned())\n+    }\n+\n+    /// Create a blob secret, stored with `application/octet-stream` content\n+    /// type.\n+    pub fn blob(value: impl AsRef<[u8]>) -> Self {\n+        Self::Blob(value.as_ref().to_owned())\n+    }\n+\n+    pub fn content_type(&self) -> &'static str {\n+        match self {\n+            Self::Text(_) => TEXT_CONTENT_TYPE,\n+            Self::Blob(_) => BLOB_CONTENT_TYPE,\n+        }\n+    }\n+\n+    pub fn as_bytes(&self) -> &[u8] {\n+        match self {\n+            Self::Text(text) => text.as_bytes(),\n+            Self::Blob(bytes) => bytes.as_ref(),\n+        }\n+    }\n+}\n+\n+impl From<&[u8]> for Secret {\n+    fn from(value: &[u8]) -> Self {\n+        Self::blob(value)\n+    }\n+}\n+\n+impl From<Zeroizing<Vec<u8>>> for Secret {\n+    fn from(value: Zeroizing<Vec<u8>>) -> Self {\n+        Self::blob(value)\n+    }\n+}\n+\n+impl From<Vec<u8>> for Secret {\n+    fn from(value: Vec<u8>) -> Self {\n+        Self::blob(value)\n+    }\n+}\n+\n+impl From<&Vec<u8>> for Secret {\n+    fn from(value: &Vec<u8>) -> Self {\n+        Self::blob(value)\n+    }\n+}\n+\n+impl<const N: usize> From<&[u8; N]> for Secret {\n+    fn from(value: &[u8; N]) -> Self {\n+        Self::blob(value)\n+    }\n+}\n+\n+impl From<String> for Secret {\n+    fn from(value: String) -> Self {\n+        Self::text(value)\n+    }\n+}\n+\n+impl From<&str> for Secret {\n+    fn from(value: &str) -> Self {\n+        Self::text(value)\n+    }\n+}\n+\n+impl std::ops::Deref for Secret {\n+    type Target = [u8];\n+\n+    fn deref(&self) -> &Self::Target {\n+        self.as_bytes()\n+    }\n+}\n+\n+impl AsRef<[u8]> for Secret {\n+    fn as_ref(&self) -> &[u8] {\n+        self.as_bytes()\n+    }\n+}\ndiff --git a/portal/Cargo.toml b/portal/Cargo.toml\nindex cfea310a1..b7b450bee 100644\n--- a/portal/Cargo.toml\n+++ b/portal/Cargo.toml\n@@ -15,7 +15,6 @@ version.workspace = true\n ashpd = {workspace = true, features = [\"backend\", \"tracing\"]}\n clap.workspace = true\n oo7.workspace = true\n-getrandom = \"0.2\"\n tokio = { workspace = true, features = [\"macros\", \"rt-multi-thread\"] }\n tracing.workspace = true\n tracing-subscriber.workspace = true\ndiff --git a/portal/src/error.rs b/portal/src/error.rs\nindex c6a91ea0c..a8d9ed4d7 100644\n--- a/portal/src/error.rs\n+++ b/portal/src/error.rs\n@@ -1,6 +1,5 @@\n #[derive(Debug)]\n pub enum Error {\n-    Rand(getrandom::Error),\n     Oo7(oo7::dbus::Error),\n     Io(std::io::Error),\n     Portal(ashpd::PortalError),\n@@ -9,7 +8,6 @@ pub enum Error {\n impl std::fmt::Display for Error {\n     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n         match self {\n-            Self::Rand(e) => f.write_fmt(format_args!(\"Rand error {e}\")),\n             Self::Oo7(e) => f.write_fmt(format_args!(\"DBus error: {e}\")),\n             Self::Io(e) => f.write_fmt(format_args!(\"IO error: {e}\")),\n             Self::Portal(e) => f.write_fmt(format_args!(\"Portal error: {e}\")),\n@@ -20,7 +18,6 @@ impl std::fmt::Display for Error {\n impl std::error::Error for Error {\n     fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {\n         match self {\n-            Self::Rand(_) => None,\n             Self::Oo7(e) => Some(e),\n             Self::Io(e) => Some(e),\n             Self::Portal(e) => Some(e),\n@@ -28,12 +25,6 @@ impl std::error::Error for Error {\n     }\n }\n \n-impl From<getrandom::Error> for Error {\n-    fn from(err: getrandom::Error) -> Self {\n-        Self::Rand(err)\n-    }\n-}\n-\n impl From<oo7::dbus::Error> for Error {\n     fn from(value: oo7::dbus::Error) -> Self {\n         Self::Oo7(value)\ndiff --git a/portal/src/main.rs b/portal/src/main.rs\nindex dcb0097a7..eb4cbc1ac 100644\n--- a/portal/src/main.rs\n+++ b/portal/src/main.rs\n@@ -11,7 +11,6 @@ pub use error::Result;\n use oo7::dbus::Service;\n use tokio::io::AsyncWriteExt;\n \n-const PORTAL_SECRET_SIZE: usize = 64;\n const PORTAL_NAME: &str = \"org.freedesktop.impl.portal.desktop.oo7\";\n \n struct Secret;\n@@ -36,13 +35,6 @@ impl ashpd::backend::secret::SecretImpl for Secret {\n     }\n }\n \n-fn generate_secret() -> Result<zeroize::Zeroizing<Vec<u8>>> {\n-    let mut secret = [0; PORTAL_SECRET_SIZE];\n-    // Equivalent of `ring::rand::SecureRandom`\n-    getrandom::getrandom(&mut secret)?;\n-    Ok(zeroize::Zeroizing::new(secret.to_vec()))\n-}\n-\n /// Generates, stores and send the secret back to the fd stream\n async fn send_secret_to_app(app_id: &AppID, fd: std::os::fd::OwnedFd) -> Result<()> {\n     let service = Service::new().await?;\n@@ -56,32 +48,30 @@ async fn send_secret_to_app(app_id: &AppID, fd: std::os::fd::OwnedFd) -> Result<\n         (oo7::XDG_SCHEMA_ATTRIBUTE, GENERIC_SCHEMA_VALUE),\n         (\"app_id\", app_id),\n     ]);\n-    let secret = if let Some(item) = collection.search_items(&attributes).await?.first() {\n-        item.secret().await?\n+\n+    // Write the secret to the FD.\n+    let std_stream = UnixStream::from(fd);\n+    std_stream.set_nonblocking(true)?;\n+    let mut stream = tokio::net::UnixStream::from_std(std_stream)?;\n+\n+    if let Some(item) = collection.search_items(&attributes).await?.first() {\n+        stream.write_all(&item.secret().await?).await?;\n     } else {\n         tracing::debug!(\"Could not find secret for {app_id}, creating one\");\n-        let secret = generate_secret()?;\n+        let secret = oo7::Secret::random().unwrap();\n \n         collection\n             .create_item(\n                 &format!(\"Secret Portal token for {app_id}\"),\n                 &attributes,\n-                &secret,\n+                secret.clone(),\n                 true,\n-                // TODO Find a better one.\n-                \"text/plain\",\n                 None,\n             )\n             .await?;\n \n-        secret\n-    };\n-\n-    // Write the secret to the FD.\n-    let std_stream = UnixStream::from(fd);\n-    std_stream.set_nonblocking(true)?;\n-    let mut stream = tokio::net::UnixStream::from_std(std_stream)?;\n-    stream.write_all(&secret).await?;\n+        stream.write_all(&secret).await?;\n+    }\n \n     Ok(())\n }\ndiff --git a/server/src/collection.rs b/server/src/collection.rs\nindex 922bc597f..49e409522 100644\n--- a/server/src/collection.rs\n+++ b/server/src/collection.rs\n@@ -8,7 +8,7 @@ use std::{\n \n use oo7::{\n     dbus::{\n-        api::{Properties, SecretInner},\n+        api::{DBusSecretInner, Properties},\n         ServiceError,\n     },\n     file::Keyring,\n@@ -76,12 +76,12 @@ impl Collection {\n     pub async fn create_item(\n         &self,\n         properties: Properties,\n-        secret: SecretInner,\n+        secret: DBusSecretInner,\n         replace: bool,\n         #[zbus(object_server)] object_server: &zbus::ObjectServer,\n         #[zbus(signal_emitter)] signal_emitter: zbus::object_server::SignalEmitter<'_>,\n     ) -> Result<(OwnedObjectPath, OwnedObjectPath), ServiceError> {\n-        let SecretInner(session, iv, secret, _content_type) = secret;\n+        let DBusSecretInner(session, iv, secret, _content_type) = secret;\n         let label = properties.label();\n         // Safe to unwrap as an item always has attributes\n         let attributes = properties.attributes().unwrap();\ndiff --git a/server/src/item.rs b/server/src/item.rs\nindex e42cf1fa0..accadcd07 100644\n--- a/server/src/item.rs\n+++ b/server/src/item.rs\n@@ -6,7 +6,7 @@ use std::{\n };\n \n use oo7::{\n-    dbus::{api::SecretInner, ServiceError},\n+    dbus::{api::DBusSecretInner, ServiceError},\n     file,\n };\n use tokio::sync::Mutex;\n@@ -58,7 +58,7 @@ impl Item {\n     pub async fn get_secret(\n         &self,\n         session: OwnedObjectPath,\n-    ) -> Result<(SecretInner,), ServiceError> {\n+    ) -> Result<(DBusSecretInner,), ServiceError> {\n         let Some(session) = self.service.session(&session).await else {\n             tracing::error!(\"The session `{}` does not exist.\", session);\n             return Err(ServiceError::NoSession(format!(\n@@ -77,6 +77,7 @@ impl Item {\n \n         let inner = self.inner.lock().await;\n         let secret = inner.secret();\n+        let content_type = secret.content_type().to_owned();\n \n         tracing::debug!(\"Secret retrieved from the item: {}.\", self.path);\n \n@@ -85,24 +86,24 @@ impl Item {\n                 let iv = oo7::crypto::generate_iv();\n                 let encrypted = oo7::crypto::encrypt(secret, &key, &iv);\n \n-                Ok((SecretInner(\n+                Ok((DBusSecretInner(\n                     session.path().clone(),\n                     iv,\n                     encrypted,\n-                    \"text/plain\".to_owned(),\n+                    content_type,\n                 ),))\n             }\n-            None => Ok((SecretInner(\n+            None => Ok((DBusSecretInner(\n                 session.path().clone(),\n                 Vec::new(),\n                 secret.to_vec(),\n-                \"text/plain\".to_owned(),\n+                content_type,\n             ),)),\n         }\n     }\n \n-    pub async fn set_secret(&self, secret: SecretInner) -> Result<(), ServiceError> {\n-        let SecretInner(session, iv, secret, _content_type) = secret;\n+    pub async fn set_secret(&self, secret: DBusSecretInner) -> Result<(), ServiceError> {\n+        let DBusSecretInner(session, iv, secret, _content_type) = secret;\n \n         let Some(session) = self.service.session(&session).await else {\n             tracing::error!(\"The session `{}` does not exist.\", session);\ndiff --git a/server/src/main.rs b/server/src/main.rs\nindex 76ce325d4..26219e9e5 100644\n--- a/server/src/main.rs\n+++ b/server/src/main.rs\n@@ -6,7 +6,6 @@ mod service;\n mod session;\n \n use clap::Parser;\n-use oo7::file::Secret;\n use service::Service;\n \n use crate::error::Error;\n@@ -31,7 +30,7 @@ struct Args {\n async fn main() -> Result<(), Error> {\n     tracing_subscriber::fmt::init();\n     let args = Args::parse();\n-    let mut secret: Option<Secret> = None;\n+    let mut secret = None;\n \n     if args.login {\n         let password = rpassword::prompt_password(\"Enter the login password: \")?;\n@@ -39,7 +38,7 @@ async fn main() -> Result<(), Error> {\n             tracing::error!(\"Login password can't be empty.\");\n             return Err(Error::EmptyPassword);\n         }\n-        secret = Some(Secret::from(password.into_bytes()));\n+        secret = Some(oo7::Secret::text(password));\n     }\n \n     let mut flags = zbus::fdo::RequestNameFlags::AllowReplacement.into();\ndiff --git a/server/src/service.rs b/server/src/service.rs\nindex 921c07e68..f5c1bb9a7 100644\n--- a/server/src/service.rs\n+++ b/server/src/service.rs\n@@ -5,11 +5,11 @@ use std::{collections::HashMap, sync::Arc};\n use enumflags2::BitFlags;\n use oo7::{\n     dbus::{\n-        api::{Properties, SecretInner},\n+        api::{DBusSecretInner, Properties},\n         Algorithm, ServiceError,\n     },\n-    file::{Keyring, Secret},\n-    Key,\n+    file::Keyring,\n+    Key, Secret,\n };\n use tokio::sync::{Mutex, RwLock};\n use zbus::{\n@@ -135,7 +135,7 @@ impl Service {\n         &self,\n         items: Vec<OwnedObjectPath>,\n         session: OwnedObjectPath,\n-    ) -> Result<HashMap<OwnedObjectPath, SecretInner>, ServiceError> {\n+    ) -> Result<HashMap<OwnedObjectPath, DBusSecretInner>, ServiceError> {\n         let mut secrets = HashMap::new();\n         let collections = self.collections.lock().await;\n \n@@ -288,7 +288,7 @@ impl Service {\n             \"session\",\n             false,\n             service.clone(),\n-            Arc::new(Keyring::temporary(Secret::random()).await?),\n+            Arc::new(Keyring::temporary(Secret::random().unwrap()).await?),\n         );\n         collections.push(collection.clone());\n         object_server\n", "test_patch": "", "problem_statement": "Usefulness of content-type in secrets\nCurrently, the secrets have a content-type field that I don't really see what it is useful for. especially that it is specific to the dbus implementation, making the generic API not cover it.\r\n\r\nIf you look for it usages inside oo7, you will find various `text/plain` and some probably mistakes of `text/utf8` (proves that we don't really make use of it).\r\n\r\ngnome-keyring also doesn't make use of it and always sets it to `text/plain`, see https://gitlab.gnome.org/GNOME/gnome-keyring/-/blob/master/daemon/dbus/gkd-secret-secret.c\r\n\r\nkwallet on the other hand, seems to only care if it is a text or a binary blob at https://github.com/KDE/kwallet/blob/28a47505b5ecfa9269fa34e8ede5ea63c041f374/src/runtime/kwalletd/kwalletfreedesktopcollection.cpp#L184\r\n\r\nSo I am considering removing that API completely and keep it an implementation detail. Replacing the usual API that takes `secret: impl AsRef<[u8]>, content_type: &str` with an enum like\r\n\r\n```rust\r\npub enum Secret {\r\n   Text(String),\r\n   Blob(Vec<u8>)\r\n}\r\n```\r\n\r\nWhich will automatically set the correct content-type.\r\n\r\ncc @ueno @A6GibKm \n", "hints_text": "I would like to know if there are more well-known content types other than `text/plain`.\nEven if an app would set one, the most popular server side implementation completely ignore them.\n\nHaving it as an enum means we can extend it in the future without making it error prone.", "created_at": "2024-11-02 12:41:21", "merge_commit_sha": "1bba4ad5220b35a1f21aee389409ed341e14ce48", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['cargo-deny', '.github/workflows/CI.yml']", "['Meson', '.github/workflows/CI.yml']"], ["['Rustfmt', '.github/workflows/CI.yml']", "['Clippy', '.github/workflows/CI.yml']"]]}
{"repo": "BurntSushi/jiff", "instance_id": "BurntSushi__jiff-281", "base_commit": "8c224104b18de25f71b8a063f0760f2c9da0c797", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 3d83d4ce..92700566 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -12,6 +12,10 @@ Add `tz::{get,include}` macros for time zone support in core-only environments.\n Switch to `rearguard` tzdb data in `jiff-tzdb` and document it.\n * [#259](https://github.com/BurntSushi/jiff/pull/259):\n De-duplicate TZif data in `jiff-tzdb`.\n+* [#273](https://github.com/BurntSushi/jiff/issues/273):\n+Add \"crate features\" documentation to `jiff-sqlx` and `jiff-diesel`.\n+* [#277](https://github.com/BurntSushi/jiff/issues/277):\n+Document semver guarantee for error conditions on `Timestamp` constructors.\n \n Bug fixes:\n \ndiff --git a/crates/jiff-diesel/src/lib.rs b/crates/jiff-diesel/src/lib.rs\nindex 822ed009..8ba5b42f 100644\n--- a/crates/jiff-diesel/src/lib.rs\n+++ b/crates/jiff-diesel/src/lib.rs\n@@ -4,6 +4,9 @@ This crate provides integration points for [Jiff](jiff) and [Diesel](diesel).\n Examples can be found in the\n [examples directory of the Jiff repository][examples].\n \n+Note that to use this crate, you'll likely need to enable one of its\n+[database backend features](#crate-features).\n+\n # Organization\n \n This crates defines several types that wrap corresponding types in\n@@ -58,6 +61,12 @@ admittedly, tough to stomach.\n In the future, it may be prudent for this crate to be upstreamed into Diesel\n itself.\n \n+# Crate features\n+\n+* **mysql** - Enables `diesel/mysql_backend`.\n+* **postgres** - Enables `diesel/postgres_backend`.\n+* **sqlite** - Enables `disel/sqlite`.\n+\n [examples]: https://github.com/BurntSushi/jiff/tree/master/examples\n [`chrono`]: https://docs.rs/chrono\n [`time`]: https://docs.rs/time\ndiff --git a/crates/jiff-sqlx/src/lib.rs b/crates/jiff-sqlx/src/lib.rs\nindex 443202a2..23978d23 100644\n--- a/crates/jiff-sqlx/src/lib.rs\n+++ b/crates/jiff-sqlx/src/lib.rs\n@@ -4,6 +4,9 @@ This crate provides integration points for [Jiff](jiff) and [SQLx][sqlx].\n Examples can be found in the\n [examples directory of the Jiff repository][examples].\n \n+Note that to use this crate, you'll likely need to enable one of its\n+[database backend features](#crate-features).\n+\n # Organization\n \n This crates defines several types that wrap corresponding types in Jiff. Each\n@@ -55,6 +58,11 @@ admittedly, tough to stomach.\n In the future, it may be prudent for this crate to be upstreamed into SQLx\n itself.\n \n+# Crate features\n+\n+* **postgres** - Enables the `sqlx-postgres` dependency.\n+* **sqlite** - Enables the `sqlx-sqlite` dependency.\n+\n [sqlx]: https://docs.rs/sqlx/0.8\n [examples]: https://github.com/BurntSushi/jiff/tree/master/examples\n [`chrono`]: https://docs.rs/chrono\ndiff --git a/src/timestamp.rs b/src/timestamp.rs\nindex 0a0fad73..ba31dc62 100644\n--- a/src/timestamp.rs\n+++ b/src/timestamp.rs\n@@ -561,6 +561,10 @@ impl Timestamp {\n     /// This returns an error if the given second corresponds to a timestamp\n     /// outside of the [`Timestamp::MIN`] and [`Timestamp::MAX`] boundaries.\n     ///\n+    /// It is a semver guarantee that the only way for this to return an error\n+    /// is if the given value is out of range. That is, when it is less than\n+    /// `Timestamp::MIN` or greater than `Timestamp::MAX`.\n+    ///\n     /// # Example\n     ///\n     /// This example shows the instants in time 1 second immediately after and\n@@ -580,6 +584,36 @@ impl Timestamp {\n     ///\n     /// # Ok::<(), Box<dyn std::error::Error>>(())\n     /// ```\n+    ///\n+    /// # Example: saturating construction\n+    ///\n+    /// If you need a way to build a `Timestamp` value that saturates to\n+    /// the minimum and maximum values supported by Jiff, then this is\n+    /// guaranteed to work:\n+    ///\n+    /// ```\n+    /// use jiff::Timestamp;\n+    ///\n+    /// fn from_second_saturating(seconds: i64) -> Timestamp {\n+    ///     Timestamp::from_second(seconds).unwrap_or_else(|_| {\n+    ///         if seconds < 0 {\n+    ///             Timestamp::MIN\n+    ///         } else {\n+    ///             Timestamp::MAX\n+    ///         }\n+    ///     })\n+    /// }\n+    ///\n+    /// assert_eq!(from_second_saturating(0), Timestamp::UNIX_EPOCH);\n+    /// assert_eq!(\n+    ///     from_second_saturating(-999999999999999999),\n+    ///     Timestamp::MIN\n+    /// );\n+    /// assert_eq!(\n+    ///     from_second_saturating(999999999999999999),\n+    ///     Timestamp::MAX\n+    /// );\n+    /// ```\n     #[inline]\n     pub fn from_second(second: i64) -> Result<Timestamp, Error> {\n         Timestamp::new(second, 0)\n@@ -598,6 +632,10 @@ impl Timestamp {\n     /// timestamp outside of the [`Timestamp::MIN`] and [`Timestamp::MAX`]\n     /// boundaries.\n     ///\n+    /// It is a semver guarantee that the only way for this to return an error\n+    /// is if the given value is out of range. That is, when it is less than\n+    /// `Timestamp::MIN` or greater than `Timestamp::MAX`.\n+    ///\n     /// # Example\n     ///\n     /// This example shows the instants in time 1 millisecond immediately after\n@@ -617,6 +655,36 @@ impl Timestamp {\n     ///\n     /// # Ok::<(), Box<dyn std::error::Error>>(())\n     /// ```\n+    ///\n+    /// # Example: saturating construction\n+    ///\n+    /// If you need a way to build a `Timestamp` value that saturates to\n+    /// the minimum and maximum values supported by Jiff, then this is\n+    /// guaranteed to work:\n+    ///\n+    /// ```\n+    /// use jiff::Timestamp;\n+    ///\n+    /// fn from_millisecond_saturating(millis: i64) -> Timestamp {\n+    ///     Timestamp::from_millisecond(millis).unwrap_or_else(|_| {\n+    ///         if millis < 0 {\n+    ///             Timestamp::MIN\n+    ///         } else {\n+    ///             Timestamp::MAX\n+    ///         }\n+    ///     })\n+    /// }\n+    ///\n+    /// assert_eq!(from_millisecond_saturating(0), Timestamp::UNIX_EPOCH);\n+    /// assert_eq!(\n+    ///     from_millisecond_saturating(-999999999999999999),\n+    ///     Timestamp::MIN\n+    /// );\n+    /// assert_eq!(\n+    ///     from_millisecond_saturating(999999999999999999),\n+    ///     Timestamp::MAX\n+    /// );\n+    /// ```\n     #[inline]\n     pub fn from_millisecond(millisecond: i64) -> Result<Timestamp, Error> {\n         let millisecond = UnixMilliseconds::try_new128(\n@@ -639,6 +707,10 @@ impl Timestamp {\n     /// timestamp outside of the [`Timestamp::MIN`] and [`Timestamp::MAX`]\n     /// boundaries.\n     ///\n+    /// It is a semver guarantee that the only way for this to return an error\n+    /// is if the given value is out of range. That is, when it is less than\n+    /// `Timestamp::MIN` or greater than `Timestamp::MAX`.\n+    ///\n     /// # Example\n     ///\n     /// This example shows the instants in time 1 microsecond immediately after\n@@ -658,6 +730,36 @@ impl Timestamp {\n     ///\n     /// # Ok::<(), Box<dyn std::error::Error>>(())\n     /// ```\n+    ///\n+    /// # Example: saturating construction\n+    ///\n+    /// If you need a way to build a `Timestamp` value that saturates to\n+    /// the minimum and maximum values supported by Jiff, then this is\n+    /// guaranteed to work:\n+    ///\n+    /// ```\n+    /// use jiff::Timestamp;\n+    ///\n+    /// fn from_microsecond_saturating(micros: i64) -> Timestamp {\n+    ///     Timestamp::from_microsecond(micros).unwrap_or_else(|_| {\n+    ///         if micros < 0 {\n+    ///             Timestamp::MIN\n+    ///         } else {\n+    ///             Timestamp::MAX\n+    ///         }\n+    ///     })\n+    /// }\n+    ///\n+    /// assert_eq!(from_microsecond_saturating(0), Timestamp::UNIX_EPOCH);\n+    /// assert_eq!(\n+    ///     from_microsecond_saturating(-999999999999999999),\n+    ///     Timestamp::MIN\n+    /// );\n+    /// assert_eq!(\n+    ///     from_microsecond_saturating(999999999999999999),\n+    ///     Timestamp::MAX\n+    /// );\n+    /// ```\n     #[inline]\n     pub fn from_microsecond(microsecond: i64) -> Result<Timestamp, Error> {\n         let microsecond = UnixMicroseconds::try_new128(\n@@ -680,6 +782,10 @@ impl Timestamp {\n     /// timestamp outside of the [`Timestamp::MIN`] and [`Timestamp::MAX`]\n     /// boundaries.\n     ///\n+    /// It is a semver guarantee that the only way for this to return an error\n+    /// is if the given value is out of range. That is, when it is less than\n+    /// `Timestamp::MIN` or greater than `Timestamp::MAX`.\n+    ///\n     /// # Example\n     ///\n     /// This example shows the instants in time 1 nanosecond immediately after\n@@ -699,6 +805,36 @@ impl Timestamp {\n     ///\n     /// # Ok::<(), Box<dyn std::error::Error>>(())\n     /// ```\n+    ///\n+    /// # Example: saturating construction\n+    ///\n+    /// If you need a way to build a `Timestamp` value that saturates to\n+    /// the minimum and maximum values supported by Jiff, then this is\n+    /// guaranteed to work:\n+    ///\n+    /// ```\n+    /// use jiff::Timestamp;\n+    ///\n+    /// fn from_nanosecond_saturating(nanos: i128) -> Timestamp {\n+    ///     Timestamp::from_nanosecond(nanos).unwrap_or_else(|_| {\n+    ///         if nanos < 0 {\n+    ///             Timestamp::MIN\n+    ///         } else {\n+    ///             Timestamp::MAX\n+    ///         }\n+    ///     })\n+    /// }\n+    ///\n+    /// assert_eq!(from_nanosecond_saturating(0), Timestamp::UNIX_EPOCH);\n+    /// assert_eq!(\n+    ///     from_nanosecond_saturating(-9999999999999999999999999999999999),\n+    ///     Timestamp::MIN\n+    /// );\n+    /// assert_eq!(\n+    ///     from_nanosecond_saturating(9999999999999999999999999999999999),\n+    ///     Timestamp::MAX\n+    /// );\n+    /// ```\n     #[inline]\n     pub fn from_nanosecond(nanosecond: i128) -> Result<Timestamp, Error> {\n         let nanosecond =\n@@ -717,6 +853,10 @@ impl Timestamp {\n     /// This returns an error if the given duration corresponds to a timestamp\n     /// outside of the [`Timestamp::MIN`] and [`Timestamp::MAX`] boundaries.\n     ///\n+    /// It is a semver guarantee that the only way for this to return an error\n+    /// is if the given value is out of range. That is, when it is less than\n+    /// `Timestamp::MIN` or greater than `Timestamp::MAX`.\n+    ///\n     /// # Example\n     ///\n     /// How one might construct a `Timestamp` from a `SystemTime`:\n@@ -761,6 +901,39 @@ impl Timestamp {\n     ///\n     /// # Ok::<(), Box<dyn std::error::Error>>(())\n     /// ```\n+    ///\n+    /// # Example: saturating construction\n+    ///\n+    /// If you need a way to build a `Timestamp` value that saturates to\n+    /// the minimum and maximum values supported by Jiff, then this is\n+    /// guaranteed to work:\n+    ///\n+    /// ```\n+    /// use jiff::{SignedDuration, Timestamp};\n+    ///\n+    /// fn from_duration_saturating(dur: SignedDuration) -> Timestamp {\n+    ///     Timestamp::from_duration(dur).unwrap_or_else(|_| {\n+    ///         if dur.is_negative() {\n+    ///             Timestamp::MIN\n+    ///         } else {\n+    ///             Timestamp::MAX\n+    ///         }\n+    ///     })\n+    /// }\n+    ///\n+    /// assert_eq!(\n+    ///     from_duration_saturating(SignedDuration::ZERO),\n+    ///     Timestamp::UNIX_EPOCH,\n+    /// );\n+    /// assert_eq!(\n+    ///     from_duration_saturating(SignedDuration::from_secs(-999999999999)),\n+    ///     Timestamp::MIN\n+    /// );\n+    /// assert_eq!(\n+    ///     from_duration_saturating(SignedDuration::from_secs(999999999999)),\n+    ///     Timestamp::MAX\n+    /// );\n+    /// ```\n     #[inline]\n     pub fn from_duration(\n         duration: SignedDuration,\n", "test_patch": "", "problem_statement": "Guarantee that `Timestamp::from_{nano,micro,milli,}second` _only_ errors on out-of-range conditions\nIt is currently not possible to infallibly convert timestamps that are obtained from either `std`, or `chrono` into `jiff::Timestamp`. `chrono` has a valid range of `i64::MIN..i64::MAX` nanoseconds, and `std` even `-u64::MAX..u64::MAX` nanoseconds, but `jiff`'s range is much more restricted. While timestamps beyond the year 10,000 are probably never needed, this introduces fallibility on the API, which could be avoided by saturating at `Timestamp::MIN` and `Timestamp::MAX`.\n\nThe current error values that `Timestamp` returns unfortunately don't let me do that in userland, as there is no information in the error (#8).\n", "hints_text": "You can definitely do it in user-land today, because the only documented way for, e.g., `Timestamp::from_second`, to fail is with an out-of-range error.\n\nPerhaps there is a question of whether a new error condition could be added in a semver compatible release, but at least for routines like that, I'd be comfortable adding language like, \"new error conditions for this routine are considered breaking, and won't be added in a semver compatible release.\"\n\nI'd also be fine adding inspection APIs on `jiff::Error` like, `is_range_error` or something.\n\nI'm unsure if I want to add explicit saturating routines for conversion. It seems like returning an error or treating it as a bug would be much more common. And adding saturating routines for `Timestamp` sets a precedent to do it for the rest of the datetime types too. That's a lot of additional APIs.\nCurrently I need to do\n```rust\nif x > Timestamp::MAX.as_nanosecond() {\n    Timestamp::MAX\n} else if x < Timestamp::MIN.as_nanosecond() {\n    Timestamp::MIN\n} else {\n    Timestamp::from_nanosecond(x).unwrap()\n}\n```\n\nThis still requires an `unwrap()`, which pulls in panic and formatting code, and to a reviewer it is not clear whether this ever panics. They would have to go read the docs to verify that this is panic-free (something the compiler probably won't be able to do), and trust that it will stay panic-free.\n\nCompare with an API like this, where reading the code suffices:\n\n```rust\nmatch Timestamp::from_nanosecond(x) {\n  Ok(t) => t\n  Err(TimestampError::Overflow) => Timestamp::MAX,\n  Err(TimestampError::Underflow) => Timestamp::MIN,\n}\n```\n\n> It seems like returning an error or treating it as a bug would be much more common\n\n`jiff` has made the decision that timestamps above 9999-12-31 are out of scope, but that doesn't make them a bug.\nFor context, I need this in https://github.com/frehberg/spa-rs/pull/12. I don't want this API to panic on timestamps beyond `jiff`s range, but I also don't want users having to unwrap every single result that this library returns.\n> And adding saturating routines for Timestamp sets a precedent to do it for the rest of the datetime types too\n\nWhich other types are there? `Date`, `DateTime`, and `DateTime`? `Date` and `Time` should saturate the year value, all other overflows are invalid, and no other datetime library produces such values.\n> jiff has made the decision that timestamps above 9999-12-31 are out of scope, but that doesn't make them a bug.\n\nCan you please not put words in my mouth? I didn't say that and didn't imply it.\n\n> Currently I need to do\n\nThat isn't how I'd write it. I'd write it like this:\n\n```rust\nuse jiff::Timestamp;\n\nfn main() -> anyhow::Result<()> {\n    assert_eq!(from_nanosecond_saturating(0), Timestamp::UNIX_EPOCH);\n    assert_eq!(\n        from_nanosecond_saturating(-9999999999999999999999999999999999),\n        Timestamp::MIN\n    );\n    assert_eq!(\n        from_nanosecond_saturating(9999999999999999999999999999999999),\n        Timestamp::MAX\n    );\n\n    Ok(())\n}\n\nfn from_nanosecond_saturating(nanos: i128) -> Timestamp {\n    Timestamp::from_nanosecond(nanos).unwrap_or_else(|_| {\n        if nanos < 0 {\n            Timestamp::MIN\n        } else {\n            Timestamp::MAX\n        }\n    })\n}\n```\n\nNo `unwrap()` or panic branches.\n\n> This still requires an unwrap(), which pulls in panic and formatting code, and to a reviewer it is not clear whether this ever panics.\n\nThere are tons of APIs in Jiff for which `unwrap()` is appropriate to use. It is not a goal of mine to offer infallible APIs for all such cases. I am okay doing it for _some_ cases where it is exceptionally well motivated, but I don't see a strong motivation here.\n\n> > And adding saturating routines for Timestamp sets a precedent to do it for the rest of the datetime types too\n> \n> Which other types are there? `Date`, `DateTime`, and `DateTime`? `Date` and `Time` should saturate the year value, all other overflows are invalid, and no other datetime library produces such values.\n\nSo what is the type signature and panicking behavior of a function that accepts a year, month and day? Does it panic for an invalid month? And thus have the same downsides as using `unwrap()` (bringing in the formatting machinery)? Or does it return a `Result` and force the caller to do an `unwrap()`?\n\nJust saying \"all other overflows are invalid\" isn't sufficient specificity for this particular conversation given that you are trying to eliminate panicking branches and `unwrap()` calls and formatting machinery.\n> Can you please not put words in my mouth? I didn't say that and didn't imply it.\n\n`jiff` hasn't made the decision that timestamps above 9999-12-31 are out of scope? \n\n> So what is the type signature and panicking behavior of a function that accepts a year, month and day? Does it panic for an invalid month?\n\nYeah there's not much value in having saturating civil constructors, because they need to be fallible anyway. This doesn't.\n\n\n> That isn't how I'd write it. I'd write it like this:\n\nThis still assumes that no other error will ever be added, but at least it avoids the panic. Thanks.\n> `jiff` hasn't made the decision that timestamps above 9999-12-31 are out of scope?\n\nI was referring to this part:\n\n> but that doesn't make them a bug\n\n...\n\n> This still assumes that no other error will ever be added, but at least it avoids the panic. Thanks.\n\nFor now, I'll change this issue to tracking a doc improvement providing this as a semver guarantee.\n\nI am not totally closed to adding saturating constructors in the future, but I'd like to see how far we can get with a lighter touch for now.\n> There are tons of APIs in Jiff for which unwrap() is appropriate to use.\n\nI think we have a fundamental mismatch of when `unwrap()` is appropriate to use. If I'm writing a library, I *never ever* want to panic. I return errors, and the downstream binary can then decide to unwrap. But returning `Result<jiff::Timestamp, jiff::Error>` as an alternative to `chrono::DateTime<Utc>` is not going to make people choose `jiff`.\n\nI use `unwrap()` in libraries all the time. Just like std does. I've [thoroughly documented my views on `unwrap()`](https://burntsushi.net/unwrap/).\n\n> I think we have a fundamental mismatch of when unwrap() is appropriate to use. If I'm writing a library, I never ever want to panic.\n\nThis is a non-sequitur. \"use unwrap\" and \"never ever want to panic\" are two distinct things. I agree with the latter. But the latter does not _necessarily_ follow from the former. An `unwrap()` introduces a panicking _branch_, not a panic. If you ever use `slice[i]` in a library, then you're just using syntactic sugar for `*slice.get(i).unwrap()`.\nOk. I don't think it's worth continuing this.", "created_at": "2025-02-27 21:58:33", "merge_commit_sha": "9aa9afe532626cd1c74c35b6270d8146fc218744", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['win-gnu', '.github/workflows/ci.yml']", "['riscv32imc-unknown-none-elf', '.github/workflows/ci.yml']"], ["['win-msvc', '.github/workflows/ci.yml']", "['wasm32-unknown-emscripten', '.github/workflows/ci.yml']"], ["['rustfmt', '.github/workflows/ci.yml']", "['cross (s390x-unknown-linux-gnu)', '.github/workflows/ci.yml']"], ["['msrv', '.github/workflows/ci.yml']", "['cross (powerpc-unknown-linux-gnu)', '.github/workflows/ci.yml']"], ["['wasm32-wasip1', '.github/workflows/ci.yml']", "['miri', '.github/workflows/ci.yml']"], ["['test (nightly, ubuntu-latest, nightly)', '.github/workflows/ci.yml']", "['test (macos, macos-latest, stable)', '.github/workflows/ci.yml']"]]}
{"repo": "sharkdp/bat", "instance_id": "sharkdp__bat-3075", "base_commit": "61c9f312c9d10103b33b7d8069401304ca938f06", "patch": "diff --git a/src/bin/bat/main.rs b/src/bin/bat/main.rs\nindex 4528a60beb..3b74ec7589 100644\n--- a/src/bin/bat/main.rs\n+++ b/src/bin/bat/main.rs\n@@ -202,7 +202,7 @@ pub fn list_themes(cfg: &Config, config_dir: &Path, cache_dir: &Path) -> Result<\n \n     let default_theme = HighlightingAssets::default_theme();\n     for theme in assets.themes() {\n-        let default_theme_info = if default_theme == theme {\n+        let default_theme_info = if !config.loop_through && default_theme == theme {\n             \" (default)\"\n         } else {\n             \"\"\n", "test_patch": "diff --git a/tests/integration_tests.rs b/tests/integration_tests.rs\nindex 8df4327cd7..c083a941ba 100644\n--- a/tests/integration_tests.rs\n+++ b/tests/integration_tests.rs\n@@ -300,6 +300,7 @@ fn list_themes_without_colors() {\n \n     bat()\n         .arg(\"--color=never\")\n+        .arg(\"--decorations=always\") // trick bat into setting `Config::loop_through` to false\n         .arg(\"--list-themes\")\n         .assert()\n         .success()\n@@ -307,6 +308,15 @@ fn list_themes_without_colors() {\n         .stdout(predicate::str::contains(default_theme_chunk).normalize());\n }\n \n+#[test]\n+fn list_themes_to_piped_output() {\n+    bat()\n+        .arg(\"--list-themes\")\n+        .assert()\n+        .success()\n+        .stdout(predicate::str::contains(\"(default)\").not());\n+}\n+\n #[test]\n #[cfg_attr(any(not(feature = \"git\"), target_os = \"windows\"), ignore)]\n fn short_help() {\n", "problem_statement": "Shell auto-completion for `--theme` list the default theme incorrectly\n*(not at all severe)*\r\n\r\nWhen using auto-complete to complete the default theme in any shell with supported completions (I've tested fish, zsh and bash) the default theme is completed as `Monokai Extended (default)` instead of `Monokai Extended`.\r\n\r\nThis is because the completion scripts use `bat --list-themes | cat` to get the list of themes.\r\n#2937 changed the basic output to include the `(default)` suffix which breaks the completions.\r\n\r\n---\r\n\r\n**What steps will reproduce the bug?**\r\n\r\n1. Start bash, fish or zsh with bat's completions installed. See #3072 \r\n3. Type `bat --theme Monokai\\ Extended` and trigger auto-complete (press tab)\r\n\r\n**What happens?**\r\n\r\nThe suggestions include `Monokai Extended (default)` (not a valid theme name)\r\n\r\n**What did you expect to happen instead?**\r\n\r\nThe suggestions should include `Monokai Extended` instead.\r\n\r\n**How did you install `bat`?**\r\n\r\nFrom source: https://github.com/sharkdp/bat/commit/b662fec214daaa77a67f08d8fbd2e81915d6e0bc\r\n\n", "hints_text": "", "created_at": "2024-08-25 09:40:42", "merge_commit_sha": "eca6b8a3768ec3931a41330c42b138107b1786b5", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['x86_64-unknown-linux-gnu (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['x86_64-apple-darwin (macos-12)', '.github/workflows/CICD.yml']"], ["['Run tests with system wide configuration', '.github/workflows/CICD.yml']", "['cargo audit', '.github/workflows/CICD.yml']"], ["['Run tests with updated syntaxes and themes', '.github/workflows/CICD.yml']", "['x86_64-pc-windows-msvc (windows-2019)', '.github/workflows/CICD.yml']"], ["['i686-unknown-linux-musl (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['aarch64-unknown-linux-musl (ubuntu-20.04)', '.github/workflows/CICD.yml']"], ["['all-jobs', '.github/workflows/CICD.yml']", "['Publish to Winget', '.github/workflows/CICD.yml']"], ["['i686-unknown-linux-gnu (ubuntu-20.04)', '.github/workflows/CICD.yml']", "['Check for changelog entry', '.github/workflows/require-changelog-for-PRs.yml']"]]}
{"repo": "AFLplusplus/LibAFL", "instance_id": "AFLplusplus__LibAFL-2968", "base_commit": "739156cb231497266222979c3a4df90725806e37", "patch": "diff --git a/libafl_qemu/src/modules/drcov.rs b/libafl_qemu/src/modules/drcov.rs\nindex cd1ca0aa31..989d601666 100644\n--- a/libafl_qemu/src/modules/drcov.rs\n+++ b/libafl_qemu/src/modules/drcov.rs\n@@ -285,7 +285,41 @@ where\n                 module_range_map.insert(range, (id, path));\n             }\n \n-            self.module_mapping = Some(module_range_map);\n+            // Split ranges larger than 4GiB into smaller ranges, as Drcov max\n+            // module size is 4GiB. This also suffices to give each range a\n+            // unique id. (Before this loop, any larger ranges split into many\n+            // by smaller ranges retain the same id, which isn't unique.)\n+            let mut split_module_range_map: RangeMap<u64, (u16, String)> = RangeMap::new();\n+            let mut i = 0;\n+            for (range, (_, path)) in module_range_map {\n+                let mut range_start = range.start;\n+                let range_end = range.end;\n+                while range_end - range_start - 1 > u64::from(u32::MAX) {\n+                    let range_end_short =\n+                        (range_start + u64::from(u32::MAX) + 1) & !u64::from(u32::MAX);\n+                    split_module_range_map.insert(\n+                        Range {\n+                            start: range_start,\n+                            end: range_end_short,\n+                        },\n+                        (i, path.clone()),\n+                    );\n+                    i += 1;\n+                    range_start = range_end_short;\n+                }\n+                split_module_range_map.insert(\n+                    Range {\n+                        start: range_start,\n+                        end: range_end,\n+                    },\n+                    (i, path.clone()),\n+                );\n+                i += 1;\n+            }\n+\n+            self.module_mapping = Some(RangeMap::<u64, (u16, String)>::from_iter(\n+                split_module_range_map,\n+            ));\n         } else {\n             log::info!(\"Using user-provided module mapping for DrCov module.\");\n         }\n", "test_patch": "", "problem_statement": "qemu_coverage: Drcov output has non-unique module IDs\n**Describe the bug**\nThe RangeMap struct which is used to track QEMU module ranges coalesces ranges with the same module path. If a smaller range is inserted in the middle of an existing coalesced range, then the outer range is split into two but retains the same ID and path.\n\nIn the DRCOV output this will appear as:\n```\n000, 0xAAAA0000, 0xAAAB0000, 0x0, 0x0, 0x0, \"/workspaces/project/libA.so\"\n001, 0xAAAB0000, 0xAAAC0000, 0x0, 0x0, 0x0, \"/workspaces/project/libB.so\"\n000, 0xAAAC0000, 0xAAAD0000, 0x0, 0x0, 0x0, \"/workspaces/project/libA.so\"\n```\nThis is illegal and DRCOV consumers will error at the duplicate IDs.\n\n**To Reproduce**\nRun qemu_coverage against a target that has one module straddling other module(s).\n\n**Expected behavior**\nThe DRCOV output should appear as:\n```\n000, 0xAAAA0000, 0xAAAB0000, 0x0, 0x0, 0x0, \"/workspaces/project/libA.so\"\n001, 0xAAAB0000, 0xAAAC0000, 0x0, 0x0, 0x0, \"/workspaces/project/libB.so\"\n002, 0xAAAC0000, 0xAAAD0000, 0x0, 0x0, 0x0, \"/workspaces/project/libA.so\"\n```\n\n", "hints_text": "", "created_at": "2025-02-11 15:56:06", "merge_commit_sha": "a30cce1d88632f18b9e72eb24bb8ac05de6eceac", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['ubuntu-check (13)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/forkserver/libafl-fuzz)', '.github/workflows/build_and_test.yml']"], ["['ubuntu-check (0)', '.github/workflows/build_and_test.yml']", "['build-docker', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/forkserver/forkserver_simple)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/structure_aware/nautilus_sync)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/binary_only/fuzzbench_fork_qemu)', '.github/workflows/build_and_test.yml']", "['ubuntu-doc-build', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/inprocess/libfuzzer_libpng_accounting)', '.github/workflows/build_and_test.yml']", "['ios', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/inprocess/libfuzzer_libmozjpeg)', '.github/workflows/build_and_test.yml']", "['ubuntu-check (3)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/baby/baby_fuzzer_minimizing)', '.github/workflows/build_and_test.yml']", "['nostd-clippy', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/full_system/nyx_launcher)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/structure_aware/baby_fuzzer_multi)', '.github/workflows/build_and_test.yml']"], ["['python-bindings', '.github/workflows/build_and_test.yml']", "['fuzzers-qemu (ubuntu-24.04, ./fuzzers/full_system/qemu_linux_process)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/structure_aware/baby_fuzzer_custom_input)', '.github/workflows/build_and_test.yml']", "['ubuntu-check (9)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/inprocess/libfuzzer_libpng)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/forkserver/fuzzbench_forkserver)', '.github/workflows/build_and_test.yml']"], ["['ubuntu-check (5)', '.github/workflows/build_and_test.yml']", "['windows-frida-libfuzzer-stb-image', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/baby/tutorial)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/fuzz_anything/libafl_atheris)', '.github/workflows/build_and_test.yml']"], ["['check-md-links', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/binary_only/frida_windows_gdiplus)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/baby/baby_fuzzer)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/fuzz_anything/push_stage_harness)', '.github/workflows/build_and_test.yml']"], ["['ubuntu-check (16)', '.github/workflows/build_and_test.yml']", "['macos', '.github/workflows/build_and_test.yml']"], ["['android', '.github/workflows/build_and_test.yml']", "['fuzzers-qemu (ubuntu-24.04, ./fuzzers/full_system/qemu_linux_kernel)', '.github/workflows/build_and_test.yml']"], ["['common (macOS-latest)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/forkserver/fuzzbench_forkserver_cmplog)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/binary_only/intel_pt_baby_fuzzer)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/structure_aware/baby_fuzzer_tokens)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/fuzz_anything/baby_no_std)', '.github/workflows/build_and_test.yml']", "['fuzzers-preflight', '.github/workflows/build_and_test.yml']"], ["['ubuntu-check (11)', '.github/workflows/build_and_test.yml']", "['changes', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/full_system/nyx_libxml2_standalone)', '.github/workflows/build_and_test.yml']", "['ubuntu-check (1)', '.github/workflows/build_and_test.yml']"], ["['ubuntu-miri', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/baby/backtrace_baby_fuzzers/rust_code_with_inprocess_executor)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/structure_aware/baby_fuzzer_nautilus)', '.github/workflows/build_and_test.yml']", "['ubuntu-check (12)', '.github/workflows/build_and_test.yml']"], ["['windows-frida-libpng', '.github/workflows/build_and_test.yml']", "['ubuntu-check (6)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/baby/backtrace_baby_fuzzers/rust_code_with_fork_executor)', '.github/workflows/build_and_test.yml']", "['fuzzers-qemu (ubuntu-24.04, ./fuzzers/binary_only/qemu_launcher)', '.github/workflows/build_and_test.yml']"], ["['fuzzers (ubuntu-24.04, ./fuzzers/binary_only/frida_libpng)', '.github/workflows/build_and_test.yml']", "['fuzzers (ubuntu-24.04, ./fuzzers/baby/backtrace_baby_fuzzers/command_executor)', '.github/workflows/build_and_test.yml']"]]}
{"repo": "nextest-rs/nextest", "instance_id": "nextest-rs__nextest-2130", "base_commit": "1391ad43a6790fa92bd329344682ea9333064936", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 2c4372414de..ffa8c6f0406 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -373,6 +373,7 @@ dependencies = [\n  \"duct\",\n  \"enable-ansi-support\",\n  \"guppy\",\n+ \"indent_write\",\n  \"itertools 0.14.0\",\n  \"miette\",\n  \"nextest-filtering\",\n@@ -872,16 +873,6 @@ version = \"1.0.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5\"\n \n-[[package]]\n-name = \"erased-serde\"\n-version = \"0.4.5\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"24e2389d65ab4fab27dc2a5de7b191e1f6617d1f1c8855c0dc569c94a4cbb18d\"\n-dependencies = [\n- \"serde\",\n- \"typeid\",\n-]\n-\n [[package]]\n name = \"errno\"\n version = \"0.3.9\"\n@@ -945,9 +936,9 @@ dependencies = [\n \n [[package]]\n name = \"fixedbitset\"\n-version = \"0.4.2\"\n+version = \"0.5.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0ce7134b9999ecaf8bcd65542e436736ef32ddca1b3e06094cb6ec5755203b80\"\n+checksum = \"1d674e81391d1e1ab681a28d99df07927c6d4aa5b027d7da16ba32d1d21ecd99\"\n \n [[package]]\n name = \"fixture-data\"\n@@ -1149,9 +1140,9 @@ dependencies = [\n \n [[package]]\n name = \"guppy\"\n-version = \"0.17.12\"\n+version = \"0.17.13\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5a70d0754232d1c2fd4819914085dc06cccbe2f2377905cdd2e668ca77df6e72\"\n+checksum = \"54c43505e290c5dc9ca6fc7117927ae46685ae6ea23f988996d622a6394ca99b\"\n dependencies = [\n  \"ahash\",\n  \"camino\",\n@@ -1161,7 +1152,7 @@ dependencies = [\n  \"fixedbitset\",\n  \"guppy-workspace-hack\",\n  \"indexmap 2.7.1\",\n- \"itertools 0.13.0\",\n+ \"itertools 0.14.0\",\n  \"nested\",\n  \"once_cell\",\n  \"pathdiff\",\n@@ -1668,12 +1659,9 @@ dependencies = [\n \n [[package]]\n name = \"log\"\n-version = \"0.4.24\"\n+version = \"0.4.25\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3d6ea2a48c204030ee31a7d7fc72c93294c92fe87ecb1789881c9543516e1a0d\"\n-dependencies = [\n- \"value-bag\",\n-]\n+checksum = \"04cbf5b083de1c7e0222a7a51dbfdba1cbe1c6ab0b15e29fff3f6c077fd9cd9f\"\n \n [[package]]\n name = \"maplit\"\n@@ -2045,9 +2033,9 @@ dependencies = [\n \n [[package]]\n name = \"once_cell\"\n-version = \"1.20.2\"\n+version = \"1.20.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775\"\n+checksum = \"945462a4b81e43c4e3ba96bd7b49d834c6f61198356aa858733bc4acf3cbe62e\"\n \n [[package]]\n name = \"openssl\"\n@@ -2155,9 +2143,9 @@ checksum = \"e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e\"\n \n [[package]]\n name = \"petgraph\"\n-version = \"0.6.5\"\n+version = \"0.7.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b4c5cc86750666a3ed20bdaf5ca2a0344f9c67674cae0515bec2da16fbaa47db\"\n+checksum = \"3672b37090dbd86368a4145bc067582552b29c27377cad4e0a306c97f9bd7772\"\n dependencies = [\n  \"fixedbitset\",\n  \"indexmap 2.7.1\",\n@@ -2651,9 +2639,9 @@ dependencies = [\n \n [[package]]\n name = \"ryu\"\n-version = \"1.0.18\"\n+version = \"1.0.19\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f3cb5ba0dc43242ce17de99c180e96db90b235b8a9fdc9543c96d2209116bd9f\"\n+checksum = \"6ea1a2d0a644769cc99faa24c3ad26b379b786fe7c36fd3c546254801650e6dd\"\n \n [[package]]\n name = \"same-file\"\n@@ -2765,15 +2753,6 @@ dependencies = [\n  \"syn\",\n ]\n \n-[[package]]\n-name = \"serde_fmt\"\n-version = \"1.0.3\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e1d4ddca14104cd60529e8c7f7ba71a2c8acd8f7f5cfcdc2faf97eeb7c3010a4\"\n-dependencies = [\n- \"serde\",\n-]\n-\n [[package]]\n name = \"serde_ignored\"\n version = \"0.1.10\"\n@@ -3037,84 +3016,6 @@ version = \"3.0.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"b7401a30af6cb5818bb64852270bb722533397edcfc7344954a38f420819ece2\"\n \n-[[package]]\n-name = \"sval\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f6dc0f9830c49db20e73273ffae9b5240f63c42e515af1da1fceefb69fceafd8\"\n-\n-[[package]]\n-name = \"sval_buffer\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"429922f7ad43c0ef8fd7309e14d750e38899e32eb7e8da656ea169dd28ee212f\"\n-dependencies = [\n- \"sval\",\n- \"sval_ref\",\n-]\n-\n-[[package]]\n-name = \"sval_dynamic\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"68f16ff5d839396c11a30019b659b0976348f3803db0626f736764c473b50ff4\"\n-dependencies = [\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_fmt\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c01c27a80b6151b0557f9ccbe89c11db571dc5f68113690c1e028d7e974bae94\"\n-dependencies = [\n- \"itoa\",\n- \"ryu\",\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_json\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0deef63c70da622b2a8069d8600cf4b05396459e665862e7bdb290fd6cf3f155\"\n-dependencies = [\n- \"itoa\",\n- \"ryu\",\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_nested\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a39ce5976ae1feb814c35d290cf7cf8cd4f045782fe1548d6bc32e21f6156e9f\"\n-dependencies = [\n- \"sval\",\n- \"sval_buffer\",\n- \"sval_ref\",\n-]\n-\n-[[package]]\n-name = \"sval_ref\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bb7c6ee3751795a728bc9316a092023529ffea1783499afbc5c66f5fabebb1fa\"\n-dependencies = [\n- \"sval\",\n-]\n-\n-[[package]]\n-name = \"sval_serde\"\n-version = \"2.13.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2a5572d0321b68109a343634e3a5d576bf131b82180c6c442dee06349dfc652a\"\n-dependencies = [\n- \"serde\",\n- \"sval\",\n- \"sval_nested\",\n-]\n-\n [[package]]\n name = \"swrite\"\n version = \"0.1.0\"\n@@ -3123,9 +3024,9 @@ checksum = \"7f3fece30b2dc06d65ecbca97b602db15bf75f932711d60cc604534f1f8b7a03\"\n \n [[package]]\n name = \"syn\"\n-version = \"2.0.96\"\n+version = \"2.0.98\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d5d0adab1ae378d7f53bdebc67a39f1f151407ef230f0ce2883572f5d8985c80\"\n+checksum = \"36147f1a48ae0ec2b5b3bc5b537d267457555a10dc06f3dbc8cb11ba3006d3b1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -3166,9 +3067,9 @@ checksum = \"61c41af27dd6d1e27b1b16b489db798443478cef1f06a660c96db617ba5de3b1\"\n \n [[package]]\n name = \"target-spec\"\n-version = \"3.3.1\"\n+version = \"3.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"da00bc76fdc8b7ed7163b7cf728179277b26985c177510b69cc320f30b5a517f\"\n+checksum = \"d9e2827c8368f9860f6d263f872175b7dd1d62628fc7518335fb1a660e477d6a\"\n dependencies = [\n  \"cfg-expr\",\n  \"guppy-workspace-hack\",\n@@ -3593,12 +3494,6 @@ version = \"0.2.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"e421abadd41a4225275504ea4d6566923418b7f05506fbc9c0fe86ba7396114b\"\n \n-[[package]]\n-name = \"typeid\"\n-version = \"1.0.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0e13db2e0ccd5e14a544e8a246ba2312cd25223f616442d7f2cb0e3db614236e\"\n-\n [[package]]\n name = \"typenum\"\n version = \"1.17.0\"\n@@ -3700,42 +3595,6 @@ version = \"0.1.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d\"\n \n-[[package]]\n-name = \"value-bag\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3ef4c4aa54d5d05a279399bfa921ec387b7aba77caf7a682ae8d86785b8fdad2\"\n-dependencies = [\n- \"value-bag-serde1\",\n- \"value-bag-sval2\",\n-]\n-\n-[[package]]\n-name = \"value-bag-serde1\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"4bb773bd36fd59c7ca6e336c94454d9c66386416734817927ac93d81cb3c5b0b\"\n-dependencies = [\n- \"erased-serde\",\n- \"serde\",\n- \"serde_fmt\",\n-]\n-\n-[[package]]\n-name = \"value-bag-sval2\"\n-version = \"1.10.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"53a916a702cac43a88694c97657d449775667bcd14b70419441d05b7fea4a83a\"\n-dependencies = [\n- \"sval\",\n- \"sval_buffer\",\n- \"sval_dynamic\",\n- \"sval_fmt\",\n- \"sval_json\",\n- \"sval_ref\",\n- \"sval_serde\",\n-]\n-\n [[package]]\n name = \"vcpkg\"\n version = \"0.2.15\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 8e07550909a..844ecfb39e5 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -61,7 +61,7 @@ fs-err = \"3.1.0\"\n future-queue = \"0.3.0\"\n futures = \"0.3.31\"\n globset = \"0.4.15\"\n-guppy = \"0.17.12\"\n+guppy = \"0.17.13\"\n hex = \"0.4.3\"\n home = \"0.5.11\"\n http = \"1.2.0\"\n@@ -76,7 +76,7 @@ is_ci = \"1.2.0\"\n itertools = \"0.14.0\"\n libc = \"0.2.169\"\n libtest-mimic = \"0.8.1\"\n-log = \"0.4.24\"\n+log = \"0.4.25\"\n maplit = \"1.0.2\"\n miette = \"7.5.0\"\n mukti-metadata = \"0.3.0\"\n@@ -116,7 +116,7 @@ supports-color = \"3.0.2\"\n supports-unicode = \"3.0.0\"\n swrite = \"0.1.0\"\n tar = \"0.4.43\"\n-target-spec = { version = \"3.3.1\", features = [\"custom\", \"summaries\"] }\n+target-spec = { version = \"3.4.0\", features = [\"custom\", \"summaries\"] }\n target-spec-miette = \"0.4.4\"\n test-case = \"3.3.1\"\n test-strategy = \"0.4.0\"\ndiff --git a/workspace-hack/Cargo.toml b/workspace-hack/Cargo.toml\nindex 5e0f0b15996..5c93155daa2 100644\n--- a/workspace-hack/Cargo.toml\n+++ b/workspace-hack/Cargo.toml\n@@ -26,7 +26,7 @@ console = { version = \"0.15.8\" }\n either = { version = \"1.13.0\" }\n getrandom = { version = \"0.2.15\", default-features = false, features = [\"std\"] }\n indexmap = { version = \"2.7.1\", features = [\"serde\"] }\n-log = { version = \"0.4.24\", default-features = false, features = [\"std\"] }\n+log = { version = \"0.4.25\", default-features = false, features = [\"std\"] }\n memchr = { version = \"2.7.4\" }\n miette = { version = \"7.5.0\", features = [\"fancy\"] }\n num-traits = { version = \"0.2.19\", default-features = false, features = [\"std\"] }\n@@ -36,7 +36,7 @@ regex-syntax = { version = \"0.8.5\" }\n serde = { version = \"1.0.217\", features = [\"alloc\", \"derive\"] }\n serde_json = { version = \"1.0.138\", features = [\"unbounded_depth\"] }\n smallvec = { version = \"1.13.2\", default-features = false, features = [\"const_generics\"] }\n-target-spec = { version = \"3.3.1\", default-features = false, features = [\"custom\", \"summaries\"] }\n+target-spec = { version = \"3.4.0\", default-features = false, features = [\"custom\", \"summaries\"] }\n target-spec-miette = { version = \"0.4.4\", default-features = false, features = [\"fixtures\"] }\n tokio = { version = \"1.43.0\", features = [\"fs\", \"io-std\", \"io-util\", \"macros\", \"process\", \"rt-multi-thread\", \"signal\", \"sync\", \"time\", \"tracing\"] }\n tracing-core = { version = \"0.1.33\" }\n@@ -50,7 +50,7 @@ cc = { version = \"1.2.5\", default-features = false, features = [\"parallel\"] }\n proc-macro2 = { version = \"1.0.93\" }\n quote = { version = \"1.0.38\" }\n serde = { version = \"1.0.217\", features = [\"alloc\", \"derive\"] }\n-syn = { version = \"2.0.96\", features = [\"extra-traits\", \"full\", \"visit\", \"visit-mut\"] }\n+syn = { version = \"2.0.98\", features = [\"extra-traits\", \"full\", \"visit\", \"visit-mut\"] }\n \n [target.x86_64-unknown-linux-gnu.dependencies]\n futures-channel = { version = \"0.3.31\", features = [\"sink\"] }\n", "test_patch": "diff --git a/cargo-nextest/Cargo.toml b/cargo-nextest/Cargo.toml\nindex 0842abf4aef..efbe3f0aa21 100644\n--- a/cargo-nextest/Cargo.toml\n+++ b/cargo-nextest/Cargo.toml\n@@ -20,6 +20,7 @@ dialoguer.workspace = true\n duct.workspace = true\n enable-ansi-support.workspace = true\n guppy.workspace = true\n+indent_write.workspace = true\n itertools.workspace = true\n miette = { workspace = true, features = [\"fancy\"] }\n # Instead of using workspace dependencies which have floating versions, we pin exact versions here\ndiff --git a/cargo-nextest/src/dispatch.rs b/cargo-nextest/src/dispatch.rs\nindex 9afba3e467b..8628261d96c 100644\n--- a/cargo-nextest/src/dispatch.rs\n+++ b/cargo-nextest/src/dispatch.rs\n@@ -1155,22 +1155,7 @@ impl BaseApp {\n         // Next, read the build platforms.\n         let build_platforms = match reuse_build.binaries_metadata() {\n             Some(kind) => kind.binary_list.rust_build_meta.build_platforms.clone(),\n-            None => {\n-                let host = HostPlatform::current(PlatformLibdir::from_rustc_stdout(\n-                    RustcCli::print_host_libdir().read(),\n-                ))?;\n-\n-                let triple_info =\n-                    discover_target_triple(&cargo_configs, cargo_opts.target.as_deref())?;\n-                let target = triple_info.map(|triple| {\n-                    let libdir = PlatformLibdir::from_rustc_stdout(\n-                        RustcCli::print_target_libdir(&triple).read(),\n-                    );\n-                    TargetPlatform::new(triple, libdir)\n-                });\n-\n-                BuildPlatforms { host, target }\n-            }\n+            None => detect_build_platforms(&cargo_configs, cargo_opts.target.as_deref())?,\n         };\n \n         // Read the Cargo metadata.\n@@ -2103,8 +2088,8 @@ enum DebugCommand {\n     /// Print the current executable path.\n     CurrentExe,\n \n-    /// Show the target platform that nextest would use.\n-    ShowTarget {\n+    /// Show the build platforms that nextest would use.\n+    BuildPlatforms {\n         /// The target triple to use.\n         #[arg(long)]\n         target: Option<String>,\n@@ -2112,6 +2097,10 @@ enum DebugCommand {\n         /// Override a Cargo configuration value.\n         #[arg(long, value_name = \"KEY=VALUE\")]\n         config: Vec<String>,\n+\n+        /// Output format.\n+        #[arg(long, value_enum, default_value_t)]\n+        output_format: BuildPlatformsOutputFormat,\n     },\n }\n \n@@ -2173,13 +2162,31 @@ impl DebugCommand {\n                     .map_err(|err| ExpectedError::GetCurrentExeFailed { err })?;\n                 println!(\"{}\", exe.display());\n             }\n-            DebugCommand::ShowTarget { target, config } => {\n+            DebugCommand::BuildPlatforms {\n+                target,\n+                config,\n+                output_format,\n+            } => {\n                 let cargo_configs = CargoConfigs::new(&config).map_err(Box::new)?;\n-                let target = discover_target_triple(&cargo_configs, target.as_deref())?;\n-                if let Some(target) = target {\n-                    println!(\"{:#?}\", target);\n-                } else {\n-                    println!(\"no target triple found\");\n+                let build_platforms = detect_build_platforms(&cargo_configs, target.as_deref())?;\n+                match output_format {\n+                    BuildPlatformsOutputFormat::Debug => {\n+                        println!(\"{:#?}\", build_platforms);\n+                    }\n+                    BuildPlatformsOutputFormat::Triple => {\n+                        println!(\n+                            \"host triple: {}\",\n+                            build_platforms.host.platform.triple().as_str()\n+                        );\n+                        if let Some(target) = &build_platforms.target {\n+                            println!(\n+                                \"target triple: {}\",\n+                                target.triple.platform.triple().as_str()\n+                            );\n+                        } else {\n+                            println!(\"target triple: (none)\");\n+                        }\n+                    }\n                 }\n             }\n         }\n@@ -2262,6 +2269,17 @@ impl fmt::Display for ExtractOutputFormat {\n     }\n }\n \n+/// Output format for `nextest debug build-platforms`.\n+#[derive(Clone, Copy, Debug, Default, ValueEnum)]\n+pub enum BuildPlatformsOutputFormat {\n+    /// Show Debug output.\n+    #[default]\n+    Debug,\n+\n+    /// Show just the triple.\n+    Triple,\n+}\n+\n fn acquire_graph_data(\n     manifest_path: Option<&Utf8Path>,\n     target_dir: Option<&Utf8Path>,\n@@ -2303,6 +2321,22 @@ fn acquire_graph_data(\n     Ok(json)\n }\n \n+fn detect_build_platforms(\n+    cargo_configs: &CargoConfigs,\n+    target_cli_option: Option<&str>,\n+) -> Result<BuildPlatforms, ExpectedError> {\n+    let host = HostPlatform::detect(PlatformLibdir::from_rustc_stdout(\n+        RustcCli::print_host_libdir().read(),\n+    ))?;\n+    let triple_info = discover_target_triple(cargo_configs, target_cli_option)?;\n+    let target = triple_info.map(|triple| {\n+        let libdir =\n+            PlatformLibdir::from_rustc_stdout(RustcCli::print_target_libdir(&triple).read());\n+        TargetPlatform::new(triple, libdir)\n+    });\n+    Ok(BuildPlatforms { host, target })\n+}\n+\n fn discover_target_triple(\n     cargo_configs: &CargoConfigs,\n     target_cli_option: Option<&str>,\ndiff --git a/cargo-nextest/src/errors.rs b/cargo-nextest/src/errors.rs\nindex fc1372f0c69..2e39c3b9d1a 100644\n--- a/cargo-nextest/src/errors.rs\n+++ b/cargo-nextest/src/errors.rs\n@@ -3,6 +3,7 @@\n \n use crate::{output::StderrStyles, ExtractOutputFormat};\n use camino::Utf8PathBuf;\n+use indent_write::indentable::Indented;\n use itertools::Itertools;\n use nextest_filtering::errors::FiltersetParseErrors;\n use nextest_metadata::NextestExitCode;\n@@ -90,9 +91,9 @@ pub enum ExpectedError {\n         err: TestFilterBuilderError,\n     },\n     #[error(\"unknown host platform\")]\n-    UnknownHostPlatform {\n+    HostPlatformDetectError {\n         #[from]\n-        err: UnknownHostPlatform,\n+        err: HostPlatformDetectError,\n     },\n     #[error(\"target triple error\")]\n     TargetTripleError {\n@@ -398,7 +399,7 @@ impl ExpectedError {\n             | Self::RootManifestNotFound { .. }\n             | Self::CargoConfigError { .. }\n             | Self::TestFilterBuilderError { .. }\n-            | Self::UnknownHostPlatform { .. }\n+            | Self::HostPlatformDetectError { .. }\n             | Self::TargetTripleError { .. }\n             | Self::MetadataMaterializeError { .. }\n             | Self::UnknownArchiveFormat { .. }\n@@ -652,8 +653,8 @@ impl ExpectedError {\n                 error!(\"{err}\");\n                 err.source()\n             }\n-            Self::UnknownHostPlatform { err } => {\n-                error!(\"the host platform was unknown to nextest\");\n+            Self::HostPlatformDetectError { err } => {\n+                error!(\"the host platform could not be detected\");\n                 Some(err as &dyn Error)\n             }\n             Self::TargetTripleError { err } => {\n@@ -882,7 +883,11 @@ impl ExpectedError {\n         };\n \n         while let Some(err) = next_error {\n-            error!(target: \"cargo_nextest::no_heading\", \"\\nCaused by:\\n  {}\", err);\n+            error!(\n+                target: \"cargo_nextest::no_heading\",\n+                \"\\nCaused by:\\n{}\",\n+                Indented { item: err, indent: \"  \" },\n+            );\n             next_error = err.source();\n         }\n     }\ndiff --git a/integration-tests/Cargo.toml b/integration-tests/Cargo.toml\nindex 90f94af4d86..a0299db871b 100644\n--- a/integration-tests/Cargo.toml\n+++ b/integration-tests/Cargo.toml\n@@ -13,6 +13,10 @@ path = \"test-helpers/cargo-nextest-dup.rs\"\n name = \"build-seed-archive\"\n path = \"test-helpers/build-seed-archive.rs\"\n \n+[[bin]]\n+name = \"rustc-shim\"\n+path = \"test-helpers/rustc-shim.rs\"\n+\n [[test]]\n name = \"custom-harness\"\n harness = false\ndiff --git a/integration-tests/src/nextest_cli.rs b/integration-tests/src/nextest_cli.rs\nindex a4d448cca4a..a9ca2a4cc50 100644\n--- a/integration-tests/src/nextest_cli.rs\n+++ b/integration-tests/src/nextest_cli.rs\n@@ -153,6 +153,24 @@ impl CargoNextestOutput {\n     pub fn decode_test_list_json(&self) -> Result<TestListSummary> {\n         Ok(serde_json::from_slice(&self.stdout)?)\n     }\n+\n+    /// Returns the output as a (hopefully) platform-independent snapshot that\n+    /// can be checked in and compared.\n+    pub fn to_snapshot(&self) -> String {\n+        // Don't include the command as its representation is\n+        // platform-dependent.\n+        let output = format!(\n+            \"exit code: {:?}\\n\\\n+            --- stdout ---\\n{}\\n\\n--- stderr ---\\n{}\\n\",\n+            self.exit_status.code(),\n+            String::from_utf8_lossy(&self.stdout),\n+            String::from_utf8_lossy(&self.stderr),\n+        );\n+\n+        // Turn \"exit status\" and \"exit code\" into \"exit status|code\"\n+        let output = output.replace(\"exit status: \", \"exit status|code: \");\n+        output.replace(\"exit code: \", \"exit status|code: \")\n+    }\n }\n \n impl fmt::Display for CargoNextestOutput {\ndiff --git a/integration-tests/test-helpers/rustc-shim.rs b/integration-tests/test-helpers/rustc-shim.rs\nnew file mode 100644\nindex 00000000000..e53550fd6c5\n--- /dev/null\n+++ b/integration-tests/test-helpers/rustc-shim.rs\n@@ -0,0 +1,82 @@\n+// Copyright (c) The nextest Contributors\n+// SPDX-License-Identifier: MIT OR Apache-2.0\n+\n+//! A shim for rustc that is used for injecting errors.\n+\n+use std::{\n+    env::args,\n+    process::{Command, ExitCode},\n+};\n+\n+fn main() -> ExitCode {\n+    // Currently, `--version --verbose` is supported. (This is a bit\n+    // overdetermined, but it's okay.)\n+    let args = args().collect::<Vec<_>>();\n+\n+    if &args[1] == \"--version\" && &args[2] == \"--verbose\" {\n+        match version_verbose_error() {\n+            Some(VersionVerboseError::NonZeroExitCode) => {\n+                println!(\"failure output to stdout\");\n+                eprintln!(\"failure output to stderr\");\n+                return ExitCode::FAILURE;\n+            }\n+            Some(VersionVerboseError::InvalidStdout) => {\n+                println!(\"invalid output to stdout\");\n+                eprintln!(\"invalid output to stderr\");\n+                return ExitCode::SUCCESS;\n+            }\n+            Some(VersionVerboseError::InvalidTriple) => {\n+                println!(\n+                    \"rustc 1.84.1 (e71f9a9a9 2025-01-27)\\n\\\n+                     binary: rustc\\n\\\n+                     commit-hash: e71f9a9a98b0faf423844bf0ba7438f29dc27d58\\n\\\n+                     commit-date: 2025-01-27\\n\\\n+                     host: invalid-triple\\n\\\n+                     release: 1.84.1\\n\\\n+                     LLVM version: 19.1.5\\n\\\n+                    \"\n+                );\n+                return ExitCode::SUCCESS;\n+            }\n+            None => {\n+                // Pass through to the real rustc.\n+            }\n+        }\n+    }\n+\n+    let code = Command::new(rustc_path())\n+        .args(&args[1..])\n+        .status()\n+        .expect(\"rustc executed successfully\")\n+        .code();\n+\n+    std::process::exit(code.unwrap_or(1))\n+}\n+\n+#[derive(Clone, Copy, Debug)]\n+enum VersionVerboseError {\n+    NonZeroExitCode,\n+    InvalidStdout,\n+    InvalidTriple,\n+}\n+\n+fn version_verbose_error() -> Option<VersionVerboseError> {\n+    const VAR: &str = \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\";\n+    match std::env::var(VAR) {\n+        Ok(s) => match s.as_str() {\n+            \"non-zero\" => Some(VersionVerboseError::NonZeroExitCode),\n+            \"invalid-stdout\" => Some(VersionVerboseError::InvalidStdout),\n+            \"invalid-triple\" => Some(VersionVerboseError::InvalidTriple),\n+            _ => panic!(\"unrecognized value for {}: {}\", VAR, s),\n+        },\n+        Err(_) => None,\n+    }\n+}\n+\n+fn rustc_path() -> String {\n+    const VAR: &str = \"__NEXTEST_RUSTC_SHIM_RUSTC\";\n+    match std::env::var(VAR) {\n+        Ok(s) => s,\n+        Err(_) => panic!(\"{VAR} not set\"),\n+    }\n+}\ndiff --git a/integration-tests/tests/datatest/custom_target.rs b/integration-tests/tests/datatest/custom_target.rs\nindex 4106850dbe6..31078af31c2 100644\n--- a/integration-tests/tests/datatest/custom_target.rs\n+++ b/integration-tests/tests/datatest/custom_target.rs\n@@ -24,7 +24,7 @@ pub(crate) fn custom_invalid(path: &Utf8Path, contents: String) -> datatest_stab\n             \"--color\",\n             \"always\",\n             \"debug\",\n-            \"show-target\",\n+            \"build-platforms\",\n             \"--target\",\n             json_path.as_str(),\n         ])\ndiff --git a/integration-tests/tests/integration/main.rs b/integration-tests/tests/integration/main.rs\nindex 25ef476ef1d..3660d39dff0 100644\n--- a/integration-tests/tests/integration/main.rs\n+++ b/integration-tests/tests/integration/main.rs\n@@ -29,7 +29,7 @@ use integration_tests::{\n };\n use nextest_metadata::{BuildPlatform, NextestExitCode, TestListSummary};\n use std::{borrow::Cow, fs::File, io::Write};\n-use target_spec::Platform;\n+use target_spec::{summaries::TargetFeaturesSummary, Platform};\n \n mod fixtures;\n mod stuck_signal;\n@@ -1382,8 +1382,9 @@ fn test_target_arg() {\n     set_env_vars();\n     let p = TempProject::new().unwrap();\n \n-    let host_platform = Platform::current().expect(\"should detect the host target successfully\");\n-    let host_triple = host_platform.triple_str();\n+    let build_target_platform =\n+        Platform::build_target().expect(\"should detect the host target successfully\");\n+    let host_triple = build_target_platform.triple_str();\n     let output = CargoNextestCli::for_test()\n         .args([\n             \"--manifest-path\",\n@@ -1400,10 +1401,110 @@ fn test_target_arg() {\n         .rust_build_meta\n         .platforms\n         .expect(\"should have the platforms field\");\n-    assert_eq!(build_platforms.host.platform, host_platform.to_summary());\n+\n+    // Target features get reset to unknown, unfortunately, so we can't compare\n+    // the full platform.\n+    let mut summary = build_target_platform.to_summary();\n+    summary.target_features = TargetFeaturesSummary::Unknown;\n+    assert_eq!(build_platforms.host.platform, summary);\n+\n     assert_eq!(build_platforms.targets[0].platform.triple, host_triple);\n     assert_eq!(\n         build_platforms.targets[0].libdir,\n         build_platforms.host.libdir\n     );\n }\n+\n+#[test]\n+fn test_rustc_version_verbose_errors() {\n+    set_env_vars();\n+\n+    // Set RUSTC to the shim.\n+    let shim_rustc = std::env::var(\"NEXTEST_BIN_EXE_rustc-shim\").unwrap();\n+\n+    let mut command = CargoNextestCli::for_test();\n+    command\n+        .args([\"debug\", \"build-platforms\", \"--output-format\", \"triple\"])\n+        .env(\"RUSTC\", &shim_rustc);\n+\n+    // --- Error cases ---\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\", \"non-zero\")\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_non_zero\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-stdout\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_stdout\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-triple\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"error\");\n+\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_triple\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    // --- Warning cases ---\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\", \"non-zero\")\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_non_zero_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-stdout\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_stdout_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+\n+    {\n+        let mut command = command.clone();\n+        command\n+            .env(\n+                \"__NEXTEST_RUSTC_SHIM_VERSION_VERBOSE_ERROR\",\n+                \"invalid-triple\",\n+            )\n+            .env(\"__NEXTEST_FORCE_BUILD_TARGET\", \"x86_64-unknown-linux-gnu\");\n+        insta::assert_snapshot!(\n+            \"rustc_vv_invalid_triple_warning\",\n+            command.unchecked(true).output().to_snapshot()\n+        );\n+    }\n+}\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap\nnew file mode 100644\nindex 00000000000..8f9e591dff3\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout.snap\n@@ -0,0 +1,28 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  parsing `rustc -vV` output failed, and detecting the build target failed as well\n+  - host platform error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        invalid output to stdout\n+        ---\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap\nnew file mode 100644\nindex 00000000000..073e5e071c9\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_stdout_warning.snap\n@@ -0,0 +1,20 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, parsing `rustc -vV` failed; falling back to build target `x86_64-unknown-linux-gnu`\n+- host platform error:\n+  error parsing `rustc -vV` output\n+    caused by:\n+    - output from `rustc -vV` did not contain a `host: ` line; output:\n+      ---\n+      invalid output to stdout\n+      ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap\nnew file mode 100644\nindex 00000000000..de75aeb40b7\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple.snap\n@@ -0,0 +1,27 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  parsing `rustc -vV` output failed, and detecting the build target failed as well\n+  - host platform error:\n+    unknown platform triple\n+      caused by:\n+      - unknown triple string: invalid-triple\n+      - triple not in builtin platforms and heuristic parsing failed\n+      - Unrecognized architecture: invalid\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap\nnew file mode 100644\nindex 00000000000..20c5245a038\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_invalid_triple_warning.snap\n@@ -0,0 +1,19 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, parsing `rustc -vV` failed; falling back to build target `x86_64-unknown-linux-gnu`\n+- host platform error:\n+  unknown platform triple\n+    caused by:\n+    - unknown triple string: invalid-triple\n+    - triple not in builtin platforms and heuristic parsing failed\n+    - Unrecognized architecture: invalid\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap\nnew file mode 100644\nindex 00000000000..86b27f9b240\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero.snap\n@@ -0,0 +1,27 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(96)\n+--- stdout ---\n+\n+\n+--- stderr ---\n+error: the host platform could not be detected\n+\n+Caused by:\n+  `rustc -vV` failed with exit status|code: 1, and detecting the build target failed as well\n+  - `rustc -vV` stdout:\n+    failure output to stdout\n+\n+  - `rustc -vV` stderr:\n+    failure output to stderr\n+\n+  - build target error:\n+    error parsing `rustc -vV` output\n+      caused by:\n+      - output from `rustc -vV` did not contain a `host: ` line; output:\n+        ---\n+        (__NEXTEST_FORCE_BUILD_TARGET set to \"error\", forcibly failing build target detection)\n+        ---\ndiff --git a/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap\nnew file mode 100644\nindex 00000000000..2d58e99b904\n--- /dev/null\n+++ b/integration-tests/tests/integration/snapshots/integration__rustc_vv_non_zero_warning.snap\n@@ -0,0 +1,18 @@\n+---\n+source: integration-tests/tests/integration/main.rs\n+expression: command.unchecked(true).output().to_snapshot()\n+snapshot_kind: text\n+---\n+exit status|code: Some(0)\n+--- stdout ---\n+host triple: x86_64-unknown-linux-gnu\n+target triple: (none)\n+\n+\n+--- stderr ---\n+warning: for host platform, `rustc -vV` failed with exit status|code: 1; falling back to build target `x86_64-unknown-linux-gnu`\n+- `rustc -vV` stdout:\n+  failure output to stdout\n+\n+- `rustc -vV` stderr:\n+  failure output to stderr\ndiff --git a/nextest-runner/src/errors.rs b/nextest-runner/src/errors.rs\nindex 93000ae1db0..6c477c139d0 100644\n--- a/nextest-runner/src/errors.rs\n+++ b/nextest-runner/src/errors.rs\n@@ -13,7 +13,7 @@ use crate::{\n };\n use camino::{FromPathBufError, Utf8Path, Utf8PathBuf};\n use config::ConfigError;\n-use indent_write::fmt::IndentWriter;\n+use indent_write::{fmt::IndentWriter, indentable::Indented};\n use itertools::{Either, Itertools};\n use nextest_filtering::errors::FiltersetParseErrors;\n use nextest_metadata::RustBinaryId;\n@@ -385,7 +385,7 @@ impl<T: std::error::Error> fmt::Display for ErrorList<T> {\n         )?;\n         for error in &self.inner {\n             let mut indent = IndentWriter::new_skip_initial(\"  \", f);\n-            writeln!(indent, \"* {}\", DisplayErrorChain(error))?;\n+            writeln!(indent, \"* {}\", DisplayErrorChain::new(error))?;\n             f = indent.into_inner();\n         }\n         Ok(())\n@@ -408,11 +408,24 @@ impl<T: std::error::Error> std::error::Error for ErrorList<T> {\n ///\n /// This is similar to the display-error-chain crate, but uses IndentWriter\n /// internally to ensure that subsequent lines are also nested.\n-pub(crate) struct DisplayErrorChain<E>(E);\n+pub(crate) struct DisplayErrorChain<E> {\n+    error: E,\n+    initial_indent: &'static str,\n+}\n \n impl<E: std::error::Error> DisplayErrorChain<E> {\n     pub(crate) fn new(error: E) -> Self {\n-        Self(error)\n+        Self {\n+            error,\n+            initial_indent: \"\",\n+        }\n+    }\n+\n+    pub(crate) fn new_with_initial_indent(initial_indent: &'static str, error: E) -> Self {\n+        Self {\n+            error,\n+            initial_indent,\n+        }\n     }\n }\n \n@@ -421,23 +434,26 @@ where\n     E: std::error::Error,\n {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        write!(f, \"{}\", self.0)?;\n+        let mut writer = IndentWriter::new(self.initial_indent, f);\n+        write!(writer, \"{}\", self.error)?;\n \n-        let Some(mut cause) = self.0.source() else {\n+        let Some(mut cause) = self.error.source() else {\n             return Ok(());\n         };\n \n-        write!(f, \"\\n  caused by:\")?;\n+        write!(writer, \"\\n  caused by:\")?;\n \n-        let mut indent = IndentWriter::new_skip_initial(\"  \", f);\n         loop {\n-            write!(indent, \"\\n- {}\", cause)?;\n+            writeln!(writer)?;\n+            let mut indent = IndentWriter::new_skip_initial(\"    \", writer);\n+            write!(indent, \"  - {}\", cause)?;\n \n             let Some(next_cause) = cause.source() else {\n                 break Ok(());\n             };\n \n             cause = next_cause;\n+            writer = indent.into_inner();\n         }\n     }\n }\n@@ -762,7 +778,7 @@ pub enum RustBuildMetaParseError {\n \n     /// The host platform could not be determined.\n     #[error(\"the host platform could not be determined\")]\n-    UnknownHostPlatform(#[source] target_spec::Error),\n+    DetectBuildTargetError(#[source] target_spec::Error),\n \n     /// The build metadata includes features unsupported.\n     #[error(\"unsupported features in the build metadata: {message}\")]\n@@ -1459,12 +1475,80 @@ pub enum InvalidCargoCliConfigReason {\n     DoesntProvideValue,\n }\n \n-/// The host platform could not be determined.\n+/// The host platform couldn't be detected.\n #[derive(Debug, Error)]\n-#[error(\"the host platform could not be determined\")]\n-pub struct UnknownHostPlatform {\n-    #[source]\n-    pub(crate) error: target_spec::Error,\n+pub enum HostPlatformDetectError {\n+    /// Spawning `rustc -vV` failed, and detecting the build target failed as\n+    /// well.\n+    #[error(\n+        \"error spawning `rustc -vV`, and detecting the build \\\n+         target failed as well\\n\\\n+         - rustc spawn error: {}\\n\\\n+         - build target error: {}\\n\",\n+        DisplayErrorChain::new_with_initial_indent(\"  \", error),\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    RustcVvSpawnError {\n+        /// The error.\n+        error: std::io::Error,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// `rustc -vV` exited with a non-zero code, and detecting the build target\n+    /// failed as well.\n+    #[error(\n+        \"`rustc -vV` failed with {}, and detecting the \\\n+         build target failed as well\\n\\\n+         - `rustc -vV` stdout:\\n{}\\n\\\n+         - `rustc -vV` stderr:\\n{}\\n\\\n+         - build target error:\\n{}\\n\",\n+        status,\n+        Indented { item: String::from_utf8_lossy(stdout), indent: \"  \" },\n+        Indented { item: String::from_utf8_lossy(stderr), indent: \"  \" },\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    RustcVvFailed {\n+        /// The status.\n+        status: ExitStatus,\n+\n+        /// The standard output from `rustc -vV`.\n+        stdout: Vec<u8>,\n+\n+        /// The standard error from `rustc -vV`.\n+        stderr: Vec<u8>,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// Parsing the host platform failed, and detecting the build target failed\n+    /// as well.\n+    #[error(\n+        \"parsing `rustc -vV` output failed, and detecting the build target \\\n+         failed as well\\n\\\n+         - host platform error:\\n{}\\n\\\n+         - build target error:\\n{}\\n\",\n+        DisplayErrorChain::new_with_initial_indent(\"  \", host_platform_error),\n+        DisplayErrorChain::new_with_initial_indent(\"  \", build_target_error)\n+    )]\n+    HostPlatformParseError {\n+        /// The error that occurred while parsing the host platform.\n+        host_platform_error: Box<target_spec::Error>,\n+\n+        /// The error that occurred while detecting the build target.\n+        build_target_error: Box<target_spec::Error>,\n+    },\n+\n+    /// Test-only code: `rustc -vV` was not queried, and detecting the build\n+    /// target failed as well.\n+    #[error(\"test-only code, so `rustc -vV` was not called; failed to detect build target\")]\n+    BuildTargetError {\n+        /// The error that occurred while detecting the build target.\n+        #[source]\n+        build_target_error: Box<target_spec::Error>,\n+    },\n }\n \n /// An error occurred while determining the cross-compiling target triple.\ndiff --git a/nextest-runner/src/list/rust_build_meta.rs b/nextest-runner/src/list/rust_build_meta.rs\nindex 9cdce9df0bf..b7ed426b524 100644\n--- a/nextest-runner/src/list/rust_build_meta.rs\n+++ b/nextest-runner/src/list/rust_build_meta.rs\n@@ -215,7 +215,7 @@ mod tests {\n         BuildPlatformsSummary, HostPlatformSummary, PlatformLibdirSummary,\n         PlatformLibdirUnavailable,\n     };\n-    use target_spec::summaries::PlatformSummary;\n+    use target_spec::{summaries::PlatformSummary, Platform};\n     use test_case::test_case;\n \n     impl Default for RustBuildMeta<BinaryListState> {\n@@ -235,15 +235,19 @@ mod tests {\n     }\n \n     fn host_current() -> HostPlatform {\n-        HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .expect(\"should detect the host platform successfully\")\n+        HostPlatform {\n+            platform: Platform::build_target()\n+                .expect(\"should detect the build target successfully\"),\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        }\n     }\n \n     fn host_current_with_libdir(libdir: &str) -> HostPlatform {\n-        HostPlatform::current(PlatformLibdir::Available(libdir.into()))\n-            .expect(\"should detect the host platform successfully\")\n+        HostPlatform {\n+            platform: Platform::build_target()\n+                .expect(\"should detect the build target successfully\"),\n+            libdir: PlatformLibdir::Available(libdir.into()),\n+        }\n     }\n \n     fn host_not_current_with_libdir(libdir: &str) -> HostPlatform {\n@@ -427,8 +431,7 @@ mod tests {\n \n         let rust_build_meta = RustBuildMeta {\n             build_platforms: BuildPlatforms {\n-                host: HostPlatform::current(PlatformLibdir::Available(host_libdir.clone()))\n-                    .expect(\"should detect the host platform successfully\"),\n+                host: host_current_with_libdir(host_libdir.as_ref()),\n                 target: Some(TargetPlatform::new(\n                     TargetTriple::x86_64_unknown_linux_gnu(),\n                     PlatformLibdir::Available(target_libdir.clone()),\n@@ -467,8 +470,7 @@ mod tests {\n             linked_paths: [(Utf8PathBuf::from(tmpdir_dirname), Default::default())].into(),\n             base_output_directories: [Utf8PathBuf::from(tmpdir_dirname)].into(),\n             build_platforms: BuildPlatforms {\n-                host: HostPlatform::current(PlatformLibdir::Available(host_libdir.clone()))\n-                    .expect(\"should detect the host platform successfully\"),\n+                host: host_current_with_libdir(host_libdir.as_ref()),\n                 target: Some(TargetPlatform::new(\n                     TargetTriple::x86_64_unknown_linux_gnu(),\n                     PlatformLibdir::Available(target_libdir.clone()),\ndiff --git a/nextest-runner/src/platform.rs b/nextest-runner/src/platform.rs\nindex a842a082c86..c32d556a316 100644\n--- a/nextest-runner/src/platform.rs\n+++ b/nextest-runner/src/platform.rs\n@@ -5,17 +5,23 @@\n \n use crate::{\n     cargo_config::{CargoTargetArg, TargetTriple},\n-    errors::{RustBuildMetaParseError, TargetTripleError, UnknownHostPlatform},\n+    errors::{\n+        DisplayErrorChain, HostPlatformDetectError, RustBuildMetaParseError, TargetTripleError,\n+    },\n     reuse_build::{LibdirMapper, PlatformLibdirMapper},\n+    RustcCli,\n };\n use camino::{Utf8Path, Utf8PathBuf};\n+use indent_write::indentable::Indented;\n use nextest_metadata::{\n     BuildPlatformsSummary, HostPlatformSummary, PlatformLibdirSummary, PlatformLibdirUnavailable,\n     TargetPlatformSummary,\n };\n-use target_spec::summaries::PlatformSummary;\n pub use target_spec::Platform;\n-use tracing::debug;\n+use target_spec::{\n+    errors::RustcVersionVerboseParseError, summaries::PlatformSummary, TargetFeatures,\n+};\n+use tracing::{debug, warn};\n \n /// A representation of host and target platform.\n #[derive(Clone, Debug, Eq, PartialEq)]\n@@ -33,11 +39,18 @@ impl BuildPlatforms {\n     /// Creates a new `BuildPlatforms` with no libdirs or targets.\n     ///\n     /// Used for testing.\n-    pub fn new_with_no_target() -> Result<Self, UnknownHostPlatform> {\n+    pub fn new_with_no_target() -> Result<Self, HostPlatformDetectError> {\n         Ok(Self {\n-            host: HostPlatform::current(PlatformLibdir::Unavailable(\n-                PlatformLibdirUnavailable::new_const(\"test\"),\n-            ))?,\n+            host: HostPlatform {\n+                // Because this is for testing, we just use the build target\n+                // rather than `rustc -vV` output.\n+                platform: Platform::build_target().map_err(|build_target_error| {\n+                    HostPlatformDetectError::BuildTargetError {\n+                        build_target_error: Box::new(build_target_error),\n+                    }\n+                })?,\n+                libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::new_const(\"test\")),\n+            },\n             target: None,\n         })\n     }\n@@ -127,10 +140,13 @@ impl BuildPlatforms {\n         // * the host might be serialized as the target platform as well (we can't detect this case\n         //   reliably, so we treat it as the target platform as well, which isn't a problem in\n         //   practice).\n-        let host = HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .map_err(|error| RustBuildMetaParseError::UnknownHostPlatform(error.error))?;\n+        let host = HostPlatform {\n+            // We don't necessarily have `rustc` available, so we use the build\n+            // target instead.\n+            platform: Platform::build_target()\n+                .map_err(RustBuildMetaParseError::DetectBuildTargetError)?,\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        };\n \n         let target = TargetTriple::deserialize(Some(summary))?.map(|triple| {\n             TargetPlatform::new(\n@@ -153,10 +169,13 @@ impl BuildPlatforms {\n         // * the host might be serialized as the target platform as well (we can't detect this case\n         //   reliably, so we treat it as the target platform as well, which isn't a problem in\n         //   practice).\n-        let host = HostPlatform::current(PlatformLibdir::Unavailable(\n-            PlatformLibdirUnavailable::OLD_SUMMARY,\n-        ))\n-        .map_err(|error| RustBuildMetaParseError::UnknownHostPlatform(error.error))?;\n+        let host = HostPlatform {\n+            // We don't necessarily have `rustc` available, so we use the build\n+            // target instead.\n+            platform: Platform::build_target()\n+                .map_err(RustBuildMetaParseError::DetectBuildTargetError)?,\n+            libdir: PlatformLibdir::Unavailable(PlatformLibdirUnavailable::OLD_SUMMARY),\n+        };\n \n         let target = TargetTriple::deserialize_str(summary)?.map(|triple| {\n             TargetPlatform::new(\n@@ -180,9 +199,12 @@ pub struct HostPlatform {\n }\n \n impl HostPlatform {\n-    /// Creates a new `HostPlatform` representing the current platform.\n-    pub fn current(libdir: PlatformLibdir) -> Result<Self, UnknownHostPlatform> {\n-        let platform = Platform::current().map_err(|error| UnknownHostPlatform { error })?;\n+    /// Creates a new `HostPlatform` representing the current platform by\n+    /// querying rustc.\n+    ///\n+    /// This may fall back to the build target if `rustc -vV` fails.\n+    pub fn detect(libdir: PlatformLibdir) -> Result<Self, HostPlatformDetectError> {\n+        let platform = detect_host_platform()?;\n         Ok(Self { platform, libdir })\n     }\n \n@@ -214,6 +236,137 @@ impl HostPlatform {\n     }\n }\n \n+/// Detect the host platform by using `rustc -vV`, and falling back to the build\n+/// target.\n+///\n+/// Returns an error if both of those methods fail, and produces a warning if\n+/// `rustc -vV` fails.\n+fn detect_host_platform() -> Result<Platform, HostPlatformDetectError> {\n+    // A test-only environment variable to always make the build target a fixed\n+    // value, or to error out.\n+    const FORCE_BUILD_TARGET_VAR: &str = \"__NEXTEST_FORCE_BUILD_TARGET\";\n+\n+    enum ForceBuildTarget {\n+        Triple(String),\n+        Error,\n+    }\n+\n+    let force_build_target = match std::env::var(FORCE_BUILD_TARGET_VAR).as_deref() {\n+        Ok(\"error\") => Some(ForceBuildTarget::Error),\n+        Ok(triple) => Some(ForceBuildTarget::Triple(triple.to_owned())),\n+        Err(_) => None,\n+    };\n+\n+    let build_target = match force_build_target {\n+        Some(ForceBuildTarget::Triple(triple)) => Platform::new(triple, TargetFeatures::Unknown),\n+        Some(ForceBuildTarget::Error) => Err(target_spec::Error::RustcVersionVerboseParse(\n+            RustcVersionVerboseParseError::MissingHostLine {\n+                output: format!(\n+                    \"({} set to \\\"error\\\", forcibly failing build target detection)\\n\",\n+                    FORCE_BUILD_TARGET_VAR\n+                ),\n+            },\n+        )),\n+        None => Platform::build_target(),\n+    };\n+\n+    let rustc_vv = RustcCli::version_verbose()\n+        .to_expression()\n+        .stdout_capture()\n+        .stderr_capture()\n+        .unchecked();\n+    match rustc_vv.run() {\n+        Ok(output) => {\n+            if output.status.success() {\n+                // Neither `rustc` nor `cargo` tell us what target features are\n+                // enabled for the host, so we must use\n+                // `TargetFeatures::Unknown`.\n+                match Platform::from_rustc_version_verbose(output.stdout, TargetFeatures::Unknown) {\n+                    Ok(platform) => Ok(platform),\n+                    Err(host_platform_error) => {\n+                        match build_target {\n+                            Ok(build_target) => {\n+                                warn!(\n+                                    \"for host platform, parsing `rustc -vV` failed; \\\n+                                     falling back to build target `{}`\\n\\\n+                                     - host platform error:\\n{}\",\n+                                    build_target.triple().as_str(),\n+                                    DisplayErrorChain::new_with_initial_indent(\n+                                        \"  \",\n+                                        host_platform_error\n+                                    ),\n+                                );\n+                                Ok(build_target)\n+                            }\n+                            Err(build_target_error) => {\n+                                // In this case, we can't do anything.\n+                                Err(HostPlatformDetectError::HostPlatformParseError {\n+                                    host_platform_error: Box::new(host_platform_error),\n+                                    build_target_error: Box::new(build_target_error),\n+                                })\n+                            }\n+                        }\n+                    }\n+                }\n+            } else {\n+                match build_target {\n+                    Ok(build_target) => {\n+                        warn!(\n+                            \"for host platform, `rustc -vV` failed with {}; \\\n+                             falling back to build target `{}`\\n\\\n+                             - `rustc -vV` stdout:\\n{}\\n\\\n+                             - `rustc -vV` stderr:\\n{}\",\n+                            output.status,\n+                            build_target.triple().as_str(),\n+                            Indented {\n+                                item: String::from_utf8_lossy(&output.stdout),\n+                                indent: \"  \"\n+                            },\n+                            Indented {\n+                                item: String::from_utf8_lossy(&output.stderr),\n+                                indent: \"  \"\n+                            },\n+                        );\n+                        Ok(build_target)\n+                    }\n+                    Err(build_target_error) => {\n+                        // If the build target isn't available either, we\n+                        // can't do anything.\n+                        Err(HostPlatformDetectError::RustcVvFailed {\n+                            status: output.status,\n+                            stdout: output.stdout,\n+                            stderr: output.stderr,\n+                            build_target_error: Box::new(build_target_error),\n+                        })\n+                    }\n+                }\n+            }\n+        }\n+        Err(error) => {\n+            match build_target {\n+                Ok(build_target) => {\n+                    warn!(\n+                        \"for host platform, failed to spawn `rustc -vV`; \\\n+                         falling back to build target `{}`\\n\\\n+                         - host platform error:\\n{}\",\n+                        build_target.triple().as_str(),\n+                        DisplayErrorChain::new_with_initial_indent(\"  \", error),\n+                    );\n+                    Ok(build_target)\n+                }\n+                Err(build_target_error) => {\n+                    // If the build target isn't available either, we\n+                    // can't do anything.\n+                    Err(HostPlatformDetectError::RustcVvSpawnError {\n+                        error,\n+                        build_target_error: Box::new(build_target_error),\n+                    })\n+                }\n+            }\n+        }\n+    }\n+}\n+\n /// The target platform.\n #[derive(Clone, Debug, Eq, PartialEq)]\n pub struct TargetPlatform {\ndiff --git a/nextest-runner/src/rustc_cli.rs b/nextest-runner/src/rustc_cli.rs\nindex bc0241367d3..4b50c095b4c 100644\n--- a/nextest-runner/src/rustc_cli.rs\n+++ b/nextest-runner/src/rustc_cli.rs\n@@ -14,6 +14,13 @@ pub struct RustcCli<'a> {\n }\n \n impl<'a> RustcCli<'a> {\n+    /// Create a rustc CLI call: `rustc --version --verbose`.\n+    pub fn version_verbose() -> Self {\n+        let mut cli = Self::default();\n+        cli.add_arg(\"--version\").add_arg(\"--verbose\");\n+        cli\n+    }\n+\n     /// Create a rustc CLI call: `rustc --print target-libdir`.\n     pub fn print_host_libdir() -> Self {\n         let mut cli = Self::default();\n@@ -36,7 +43,8 @@ impl<'a> RustcCli<'a> {\n         self\n     }\n \n-    fn to_expression(&self) -> duct::Expression {\n+    /// Convert the command to a [`duct::Expression`].\n+    pub fn to_expression(&self) -> duct::Expression {\n         duct::cmd(\n             self.rustc_path.as_str(),\n             self.args.iter().map(|arg| arg.as_ref()),\ndiff --git a/nextest-runner/src/update.rs b/nextest-runner/src/update.rs\nindex ddfc08fe424..9cb61bab551 100644\n--- a/nextest-runner/src/update.rs\n+++ b/nextest-runner/src/update.rs\n@@ -222,7 +222,10 @@ impl NextestReleases {\n     }\n \n     fn target_triple(&self) -> String {\n-        let current = Platform::current().expect(\"current platform could not be detected\");\n+        // In this case, use the build target, *not* `rustc -vV` output. This\n+        // ensures that e.g. musl binary updates continue to use the musl\n+        // target.\n+        let current = Platform::build_target().expect(\"build target could not be detected\");\n         let triple_str = current.triple_str();\n         if triple_str.ends_with(\"-apple-darwin\") {\n             // Nextest builds a universal binary for Mac.\ndiff --git a/nextest-runner/tests/integration/target_runner.rs b/nextest-runner/tests/integration/target_runner.rs\nindex 746d295cdab..f7a79530b89 100644\n--- a/nextest-runner/tests/integration/target_runner.rs\n+++ b/nextest-runner/tests/integration/target_runner.rs\n@@ -31,7 +31,7 @@ fn runner_for_target(triple: Option<&str>) -> Result<(BuildPlatforms, TargetRunn\n     .unwrap();\n \n     let build_platforms = {\n-        let host = HostPlatform::current(PlatformLibdir::from_rustc_stdout(\n+        let host = HostPlatform::detect(PlatformLibdir::from_rustc_stdout(\n             RustcCli::print_host_libdir().read(),\n         ))?;\n         let target = if let Some(triple) = TargetTriple::find(&configs, triple)? {\n@@ -170,7 +170,7 @@ fn passthrough_path() -> &'static Utf8Path {\n \n fn current_runner_env_var() -> String {\n     PlatformRunner::runner_env_var(\n-        &Platform::current().expect(\"current platform is known to target-spec\"),\n+        &Platform::build_target().expect(\"current platform is known to target-spec\"),\n     )\n }\n \n", "problem_statement": "Bug: default deduced target triple is wrong\n### Description of the issue\n\n**Description:**\n\nYou use `target_spec::Platform::current()` to deduce the host target triple in many locations ([e.g.](https://github.com/nextest-rs/nextest/blob/c872a3b8654cc1a8444d87589f969885c8ab0764/nextest-runner/src/platform.rs#L185)). This is wrong; `Platform::current()` is the triple used at *build time*, causing nextest to think, for example, that the host target triple is `x86_64-unknown-linux-musl` if you obtained a binary distribution of nextest that was built using musl. You should deduce the host target from the stdout of `rustc -vV` instead.\n\n**Steps to reproduce:**\n1. On a system with the `x86_64-unknown-linux-gnu` toolchain installed, obtain the binary distribution of nextest that is built with `x86_64-unknown-linux-musl`\n2. Run `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`\n4. Run `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`\n\n\n### Expected outcome\n\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`: nextest detects the runner.\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`: nextest does not detect the runner.\n\n### Actual result\n\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_GNU_RUNNER=foobar cargo nextest run`: nextest does not detect the runner.\nWhen running `CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_RUNNER=foobar cargo nextest run`: nextest erroneously detect the runner.\n\n\n### Nextest version\n\n```text\n0.9.88\n```\n\n### Additional context\n\n_No response_\n", "hints_text": "", "created_at": "2025-02-09 06:06:33", "merge_commit_sha": "02424f4552c4fa87724dfa962eafa8e940f7fbfc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build and test (ubuntu-latest, stable)', '.github/workflows/ci.yml']", "['Build documentation (ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Build and test (windows-latest, 1.81)', '.github/workflows/ci.yml']", "['Test archives with a runner (destination) (ubuntu-24.04, ubuntu-latest)', '.github/workflows/ci.yml']"], ["['Test archives with a runner (destination) (windows-latest, windows-latest)', '.github/workflows/ci.yml']", "['Build and deploy documentation', '.github/workflows/docs.yml']"], ["['Collect test coverage (ubuntu-latest)', '.github/workflows/coverage.yml']", "['Build and test (macos-14, 1.81)', '.github/workflows/ci.yml']"]]}
{"repo": "slint-ui/slint", "instance_id": "slint-ui__slint-7327", "base_commit": "9504a4f09005d257bfd8f5c0d3adf2bd2845c112", "patch": "diff --git a/internal/backends/qt/qt_window.rs b/internal/backends/qt/qt_window.rs\nindex dc6e52d5424..08e8ffe4c3b 100644\n--- a/internal/backends/qt/qt_window.rs\n+++ b/internal/backends/qt/qt_window.rs\n@@ -158,17 +158,23 @@ cpp! {{\n             isMouseButtonDown = false;\n \n             void *parent_of_popup_to_close = nullptr;\n+            int popup_id_to_close = 0;\n             if (auto p = dynamic_cast<const SlintWidget*>(parent())) {\n                 while (auto pp = dynamic_cast<const SlintWidget*>(p->parent())) {\n                     p = pp;\n                 }\n                 void *parent_window = p->rust_window;\n                 bool inside = rect().contains(event->pos());\n-                bool close_on_click = rust!(Slint_mouseReleaseEventPopup [parent_window: &QtWindow as \"void*\", inside: bool as \"bool\"] -> bool as \"bool\" {\n-                    let close_policy = parent_window.top_close_policy();\n-                    close_policy == PopupClosePolicy::CloseOnClick || (close_policy == PopupClosePolicy::CloseOnClickOutside && !inside)\n+                popup_id_to_close = rust!(Slint_mouseReleaseEventPopup [parent_window: &QtWindow as \"void*\", inside: bool as \"bool\"] -> u32 as \"int\" {\n+                    let active_popups = WindowInner::from_pub(&parent_window.window).active_popups();\n+                    if let Some(popup) = active_popups.last() {\n+                        if popup.close_policy == PopupClosePolicy::CloseOnClick || (popup.close_policy == PopupClosePolicy::CloseOnClickOutside && !inside) {\n+                            return popup.popup_id.get();\n+                        }\n+                    }\n+                    0\n                 });\n-                if (close_on_click) {\n+                if (popup_id_to_close) {\n                     parent_of_popup_to_close = parent_window;\n                 }\n             }\n@@ -180,9 +186,9 @@ cpp! {{\n                 let button = from_qt_button(button);\n                 rust_window.mouse_event(MouseEvent::Released{ position, button, click_count: 0 })\n             });\n-            if (parent_of_popup_to_close) {\n-                rust!(Slint_mouseReleaseEventClosePopup [parent_of_popup_to_close: &QtWindow as \"void*\"] {\n-                    parent_of_popup_to_close.close_top_popup();\n+            if (popup_id_to_close) {\n+                rust!(Slint_mouseReleaseEventClosePopup [parent_of_popup_to_close: &QtWindow as \"void*\", popup_id_to_close: std::num::NonZeroU32 as \"int\"] {\n+                    WindowInner::from_pub(&parent_of_popup_to_close.window).close_popup(popup_id_to_close);\n                 });\n             }\n         }\n@@ -1750,14 +1756,6 @@ impl QtWindow {\n         timer_event();\n     }\n \n-    fn close_top_popup(&self) {\n-        WindowInner::from_pub(&self.window).close_top_popup();\n-    }\n-\n-    fn top_close_policy(&self) -> PopupClosePolicy {\n-        WindowInner::from_pub(&self.window).top_close_policy()\n-    }\n-\n     fn window_state_event(&self) {\n         let widget_ptr = self.widget_ptr();\n \ndiff --git a/internal/core/window.rs b/internal/core/window.rs\nindex e25a80076e9..2ce23d83c84 100644\n--- a/internal/core/window.rs\n+++ b/internal/core/window.rs\n@@ -393,13 +393,13 @@ pub enum PopupWindowLocation {\n #[derive(Clone)]\n pub struct PopupWindow {\n     /// The ID of the associated popup.\n-    popup_id: NonZeroU32,\n+    pub popup_id: NonZeroU32,\n     /// The location defines where the pop up is rendered.\n     pub location: PopupWindowLocation,\n     /// The component that is responsible for providing the popup content.\n     pub component: ItemTreeRc,\n     /// Defines the close behaviour of the popup.\n-    close_policy: PopupClosePolicy,\n+    pub close_policy: PopupClosePolicy,\n     /// the item that had the focus in the parent window when the popup was opened\n     focus_item_in_parent: ItemWeak,\n     /// The item from where the Popup was invoked from\n@@ -596,8 +596,30 @@ impl WindowInner {\n             self.had_popup_on_press.set(!self.active_popups.borrow().is_empty());\n         }\n \n-        let close_policy = self.top_close_policy();\n-        let mut mouse_inside_popup = false;\n+        let popup_to_close = self.active_popups.borrow().last().and_then(|popup| {\n+            let mouse_inside_popup = || {\n+                if let PopupWindowLocation::ChildWindow(coordinates) = &popup.location {\n+                    event.position().map_or(true, |pos| {\n+                        ItemTreeRc::borrow_pin(&popup.component)\n+                            .as_ref()\n+                            .item_geometry(0)\n+                            .contains(pos - coordinates.to_vector())\n+                    })\n+                } else {\n+                    false\n+                }\n+            };\n+            match popup.close_policy {\n+                PopupClosePolicy::CloseOnClick => {\n+                    let mouse_inside_popup = mouse_inside_popup();\n+                    (mouse_inside_popup && released_event && self.had_popup_on_press.get())\n+                        || (!mouse_inside_popup && pressed_event)\n+                }\n+                PopupClosePolicy::CloseOnClickOutside => !mouse_inside_popup() && pressed_event,\n+                PopupClosePolicy::NoAutoClose => false,\n+            }\n+            .then_some(popup.popup_id)\n+        });\n \n         mouse_input_state = if let Some(mut event) =\n             crate::input::handle_mouse_grab(event, &window_adapter, &mut mouse_input_state)\n@@ -610,7 +632,7 @@ impl WindowInner {\n             {\n                 let geom = ItemTreeRc::borrow_pin(component).as_ref().item_geometry(0);\n \n-                mouse_inside_popup = event\n+                let mouse_inside_popup = event\n                     .position()\n                     .map_or(true, |pos| geom.contains(pos - coordinates.to_vector()));\n \n@@ -655,21 +677,9 @@ impl WindowInner {\n \n         self.mouse_input_state.set(mouse_input_state);\n \n-        match close_policy {\n-            PopupClosePolicy::CloseOnClick => {\n-                if (mouse_inside_popup && released_event && self.had_popup_on_press.get())\n-                    || (!mouse_inside_popup && pressed_event)\n-                {\n-                    self.close_top_popup();\n-                }\n-            }\n-            PopupClosePolicy::CloseOnClickOutside => {\n-                if !mouse_inside_popup && pressed_event {\n-                    self.close_top_popup();\n-                }\n-            }\n-            PopupClosePolicy::NoAutoClose => {}\n-        };\n+        if let Some(popup_id) = popup_to_close {\n+            self.close_popup(popup_id);\n+        }\n \n         crate::properties::ChangeTracker::run_change_handlers();\n     }\n@@ -1203,14 +1213,6 @@ impl WindowInner {\n         }\n     }\n \n-    /// Returns the close policy of the top-most popup. PopupClosePolicy::NoAutoClose if there is no active popup.\n-    pub fn top_close_policy(&self) -> PopupClosePolicy {\n-        self.active_popups\n-            .borrow()\n-            .last()\n-            .map_or(PopupClosePolicy::NoAutoClose, |popup| popup.close_policy)\n-    }\n-\n     /// Returns the scale factor set on the window, as provided by the windowing system.\n     pub fn scale_factor(&self) -> f32 {\n         self.pinned_fields.as_ref().project_ref().scale_factor.get()\n", "test_patch": "diff --git a/tests/cases/elements/popupwindow_nested_close-on-click.slint b/tests/cases/elements/popupwindow_nested_close-on-click.slint\nnew file mode 100644\nindex 00000000000..69a3a60ff3f\n--- /dev/null\n+++ b/tests/cases/elements/popupwindow_nested_close-on-click.slint\n@@ -0,0 +1,115 @@\n+// Copyright \u00a9 SixtyFPS GmbH <info@slint.dev>\n+// SPDX-License-Identifier: GPL-3.0-only OR LicenseRef-Slint-Royalty-free-2.0 OR LicenseRef-Slint-Software-3.0\n+\n+// https://github.com/slint-ui/slint/issues/7322\n+// A PopupWindow with the default close-on-click policy should close when clicked, even if the click open a nested popup\n+\n+export component TestCase {\n+    width: 300px;\n+    height: 300px;\n+\n+    in-out property <int> popup1-clicked;\n+    in-out property <int> popup2-clicked;\n+    in-out property <int> root-clicked;\n+\n+    popup1 := PopupWindow {\n+        Rectangle {\n+            background: yellow;\n+        }\n+\n+        x: 10px;\n+        y: 10px;\n+        height: 50px;\n+        width: 50px;\n+\n+        TouchArea {\n+            clicked => {\n+                popup1-clicked += 1;\n+                popup2.show();\n+            }\n+        }\n+    }\n+\n+    popup2 := PopupWindow {\n+        Rectangle {\n+            background: red;\n+        }\n+\n+        x: 40px;\n+        y: 40px;\n+        height: 50px;\n+        width: 50px;\n+\n+        TouchArea {\n+            clicked => {\n+                popup2-clicked += 1;\n+            }\n+        }\n+    }\n+\n+    TouchArea {\n+        clicked => {\n+            root-clicked += 1;\n+            popup1.show();\n+        }\n+    }\n+}\n+\n+/*\n+\n+```rust\n+\n+let instance = TestCase::new().unwrap();\n+\n+\n+slint_testing::send_mouse_click(&instance, 15., 15.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 0);\n+assert_eq!(instance.get_popup2_clicked(), 0);\n+\n+// popup1 is open\n+\n+slint_testing::send_mouse_click(&instance, 15., 15.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 0);\n+\n+// popup2 is open, popup1 is closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 1);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// all popup closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 1);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// popup1 is open\n+// click where popup2 will be\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 1);\n+\n+// popup2 is open, popup1 is closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 2);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 2);\n+\n+// all popup closed\n+\n+slint_testing::send_mouse_click(&instance, 45., 45.);\n+assert_eq!(instance.get_root_clicked(), 3);\n+assert_eq!(instance.get_popup1_clicked(), 2);\n+assert_eq!(instance.get_popup2_clicked(), 2);\n+\n+```\n+\n+*/\n\\ No newline at end of file\n", "problem_statement": "Qt backend: Nested PopupWindow is not shown\n### Bug Description\n\nOpening a `PopupWindow` from within another `PopupWindow` does work on WASM and winit, but not with the Qt backend. With the Qt backend, only the first popup is opened. When calling `show()` on the second one, nothing happens.\r\n\r\nSide note: With the given example, clicking into the yellow popup does not work in the bottom right corner (where the second popup is placed). This is independent of the renderer (reproducible in SlintPad) so I'm not sure if this is expected behavior or not. I'd assume the second popup should not intercept click events in its area while it is not shown...\n\n### Reproducible Code (if applicable)\n\n```slint\nexport component AppWindow inherits Window {\r\n    width: 100px;\r\n    height: 100px;\r\n\r\n    popup1 := PopupWindow {\r\n        Rectangle {\r\n            background: yellow;\r\n        }\r\n\r\n        x: 10px;\r\n        y: 10px;\r\n        height: 50px;\r\n        width: 50px;\r\n\r\n        TouchArea {\r\n            clicked => {\r\n                popup2.show();\r\n            }\r\n        }\r\n    }\r\n\r\n    popup2 := PopupWindow {\r\n        Rectangle {\r\n            background: red;\r\n        }\r\n\r\n        x: 40px;\r\n        y: 40px;\r\n        height: 50px;\r\n        width: 50px;\r\n    }\r\n\r\n    TouchArea {\r\n        clicked => {\r\n            popup1.show();\r\n        }\r\n    }\r\n}\n```\n\n\n### Environment Details\n\n- Slint Version: 1.9.1\r\n- Platform/OS: Linux\r\n- Backend/Renderer: Qt\r\n\n\n### Product Impact\n\n_No response_\n", "hints_text": "Another issue with `PopupWindow` with Qt backend:\r\n- On Wayland, popups stay at their absolute screen position even when moving the application window around. With winit the popup keeps its relative window position, i.e. moves with the application window as desired.\r\n- On X11, a popup is somehow globally modal, not just application modal. While a popup is open, no other application receives click events anymore. Instead, any click event outside the application window will just close the popup.\r\n\r\nSlint 1.9.1 on Ubuntu 22.04.\nThe problem is that the yellow PopupWindow's close policy is set to close on click, so the release event is supposed to close the yellow popup.\r\nBut the thing is that the release event also open the red popup.\r\n\r\nSo while delivering the release event this happens\r\n 1. check that there is currently a popup (yes, the yellow popup)\r\n 2. clicked callback open the yellow popup\r\n 3. attempt to close the yellow popup by closing the top popup. Ooops, this closes the red popup instead\r\n\r\nThe thing is that with winit, there is an additional check that the mouse cursor is inside the popup in step 3, and this fails because it uses the red instead of the yellow. Which is why your testcase \"works\" when not clicking in the bottom-right corner.\r\n\r\nSo two possible fix: either\r\n - Fix the step 3 to close the right popup always (the yellow popup, which will also result of the closing of the red one) \r\n - Or if a popup is getting open while processing the event, do not close any popup (which i think is what you expect, but is probably technically wrong.\r\n\r\n\n> - Fix the step 3 to close the right popup always (the yellow popup, which will also result of the closing of the red one)\r\n> - Or if a popup is getting open while processing the event, do not close any popup (which i think is what you expect, but is probably technically wrong.\r\n\r\nActually in my particular case I expect the yellow popup to close when clicked, leaving only the red one open. Not sure if everyone would expect this behavior but at least the yellow popups close policy is set to close if there's a click. That the click also opens a new popup is a quite specific case but IMHO (by default) it should not have an impact on the close behavior of the yellow popup.\r\n\r\nEDIT: Btw my yellow popup is in the real use-case a menu item and the red popup a normal popup (like a message box). This is a very common use-case and it is expected that the menu closes when an item is triggered, no matter what action it executes.", "created_at": "2025-01-10 18:00:41", "merge_commit_sha": "e34c19325c82ab2323da6264a573a0a794d0fd67", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build_and_test (ubuntu-22.04, nightly)', '.github/workflows/ci.yaml']", "['miri', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (macos-13)', '.github/workflows/ci.yaml']", "['wasm', '.github/workflows/ci.yaml']"], ["['tree-sitter-tests', '.github/workflows/ci.yaml']", "['cpp_cmake (macos-14, 1.77)', '.github/workflows/ci.yaml']"], ["['build_and_test (ubuntu-22.04, 1.77)', '.github/workflows/ci.yaml']", "['build_and_test (windows-2022, beta, --exclude ffmpeg --exclude gstreamer-player)', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (windows-2022)', '.github/workflows/ci.yaml']", "['ffi_32bit_build', '.github/workflows/ci.yaml']"], ["['build_and_test (windows-2022, 1.77)', '.github/workflows/ci.yaml']", "['mcu_esp', '.github/workflows/ci.yaml']"], ["['build_and_test (windows-2022, stable)', '.github/workflows/ci.yaml']", "['build-slintpad', '.github/workflows/ci.yaml']"], ["['esp-idf-quick', '.github/workflows/ci.yaml']", "['python_test (macos-14)', '.github/workflows/ci.yaml']"], ["['cpp_test_driver (ubuntu-22.04)', '.github/workflows/ci.yaml']", "['cpp_cmake (windows-2022, nightly)', '.github/workflows/ci.yaml']"]]}
{"repo": "GraphiteEditor/Graphite", "instance_id": "GraphiteEditor__Graphite-2067", "base_commit": "63d44f22e319abba16f58e2076421fdb5c9b1552", "patch": "diff --git a/libraries/bezier-rs/src/bezier/core.rs b/libraries/bezier-rs/src/bezier/core.rs\nindex ea1930f667..9124012a75 100644\n--- a/libraries/bezier-rs/src/bezier/core.rs\n+++ b/libraries/bezier-rs/src/bezier/core.rs\n@@ -201,8 +201,11 @@ impl Bezier {\n \t/// Returns true if the corresponding points of the two `Bezier`s are within the provided absolute value difference from each other.\n \t/// The points considered includes the start, end, and any relevant handles.\n \tpub fn abs_diff_eq(&self, other: &Bezier, max_abs_diff: f64) -> bool {\n-\t\tlet self_points = self.get_points().collect::<Vec<DVec2>>();\n-\t\tlet other_points = other.get_points().collect::<Vec<DVec2>>();\n+\t\tlet a = if self.is_linear() { Self::from_linear_dvec2(self.start, self.end) } else { *self };\n+\t\tlet b = if other.is_linear() { Self::from_linear_dvec2(other.start, other.end) } else { *other };\n+\n+\t\tlet self_points = a.get_points().collect::<Vec<DVec2>>();\n+\t\tlet other_points = b.get_points().collect::<Vec<DVec2>>();\n \n \t\tself_points.len() == other_points.len() && self_points.into_iter().zip(other_points).all(|(a, b)| a.abs_diff_eq(b, max_abs_diff))\n \t}\ndiff --git a/libraries/bezier-rs/src/bezier/mod.rs b/libraries/bezier-rs/src/bezier/mod.rs\nindex c6eeb6ccde..fb2c11b6a7 100644\n--- a/libraries/bezier-rs/src/bezier/mod.rs\n+++ b/libraries/bezier-rs/src/bezier/mod.rs\n@@ -102,7 +102,7 @@ impl BezierHandles {\n \t}\n \n \t#[must_use]\n-\tpub fn flipped(self) -> Self {\n+\tpub fn reversed(self) -> Self {\n \t\tmatch self {\n \t\t\tBezierHandles::Cubic { handle_start, handle_end } => Self::Cubic {\n \t\t\t\thandle_start: handle_end,\ndiff --git a/libraries/bezier-rs/src/bezier/transform.rs b/libraries/bezier-rs/src/bezier/transform.rs\nindex 9db2106cfd..d65cfb74ec 100644\n--- a/libraries/bezier-rs/src/bezier/transform.rs\n+++ b/libraries/bezier-rs/src/bezier/transform.rs\n@@ -605,6 +605,16 @@ impl Bezier {\n \n \t\t(arcs, low)\n \t}\n+\n+\t/// Reverses the direction of the b\u00e9zier.\n+\t#[must_use]\n+\tpub fn reversed(self) -> Self {\n+\t\tSelf {\n+\t\t\tstart: self.end,\n+\t\t\tend: self.start,\n+\t\t\thandles: self.handles.reversed(),\n+\t\t}\n+\t}\n }\n \n #[cfg(test)]\ndiff --git a/node-graph/gcore/src/vector/vector_data/attributes.rs b/node-graph/gcore/src/vector/vector_data/attributes.rs\nindex 03492b6afc..42ce9ff653 100644\n--- a/node-graph/gcore/src/vector/vector_data/attributes.rs\n+++ b/node-graph/gcore/src/vector/vector_data/attributes.rs\n@@ -144,6 +144,10 @@ impl PointDomain {\n \t\tself.id.iter().copied().zip(self.positions.iter_mut())\n \t}\n \n+\tpub fn set_position(&mut self, index: usize, position: DVec2) {\n+\t\tself.positions[index] = position;\n+\t}\n+\n \tpub fn ids(&self) -> &[PointId] {\n \t\t&self.id\n \t}\n@@ -270,6 +274,14 @@ impl SegmentDomain {\n \t\t&self.end_point\n \t}\n \n+\tpub fn set_start_point(&mut self, segment_index: usize, new: usize) {\n+\t\tself.start_point[segment_index] = new;\n+\t}\n+\n+\tpub fn set_end_point(&mut self, segment_index: usize, new: usize) {\n+\t\tself.end_point[segment_index] = new;\n+\t}\n+\n \tpub fn handles(&self) -> &[bezier_rs::BezierHandles] {\n \t\t&self.handles\n \t}\n@@ -310,6 +322,11 @@ impl SegmentDomain {\n \t\tnested.map(|(((&a, b), &c), &d)| (a, b, c, d))\n \t}\n \n+\tpub(crate) fn handles_and_points_mut(&mut self) -> impl Iterator<Item = (&mut bezier_rs::BezierHandles, &mut usize, &mut usize)> {\n+\t\tlet nested = self.handles.iter_mut().zip(&mut self.start_point).zip(&mut self.end_point);\n+\t\tnested.map(|((a, b), c)| (a, b, c))\n+\t}\n+\n \tpub fn stroke_mut(&mut self) -> impl Iterator<Item = (SegmentId, &mut StrokeId)> {\n \t\tself.ids.iter().copied().zip(self.stroke.iter_mut())\n \t}\n@@ -501,12 +518,16 @@ impl super::VectorData {\n \n \t/// Tries to convert a segment with the specified id to the start and end points and a [`bezier_rs::Bezier`], returning None if the id is invalid.\n \tpub fn segment_points_from_id(&self, id: SegmentId) -> Option<(PointId, PointId, bezier_rs::Bezier)> {\n-\t\tlet index: usize = self.segment_domain.id_to_index(id)?;\n+\t\tSome(self.segment_points_from_index(self.segment_domain.id_to_index(id)?))\n+\t}\n+\n+\t/// Tries to convert a segment with the specified index to the start and end points and a [`bezier_rs::Bezier`].\n+\tpub fn segment_points_from_index(&self, index: usize) -> (PointId, PointId, bezier_rs::Bezier) {\n \t\tlet start = self.segment_domain.start_point[index];\n \t\tlet end = self.segment_domain.end_point[index];\n \t\tlet start_id = self.point_domain.ids()[start];\n \t\tlet end_id = self.point_domain.ids()[end];\n-\t\tSome((start_id, end_id, self.segment_to_bezier_with_index(start, end, self.segment_domain.handles[index])))\n+\t\t(start_id, end_id, self.segment_to_bezier_with_index(start, end, self.segment_domain.handles[index]))\n \t}\n \n \t/// Iterator over all of the [`bezier_rs::Bezier`] following the order that they are stored in the segment domain, skipping invalid segments.\n@@ -722,7 +743,7 @@ impl<'a> Iterator for StrokePathIter<'a> {\n \n \t\t\tlet mut handles = self.vector_data.segment_domain.handles()[val.segment_index];\n \t\t\tif val.start_from_end {\n-\t\t\t\thandles = handles.flipped();\n+\t\t\t\thandles = handles.reversed();\n \t\t\t}\n \t\t\tlet next_point_index = if val.start_from_end {\n \t\t\t\tself.vector_data.segment_domain.start_point()[val.segment_index]\ndiff --git a/node-graph/gcore/src/vector/vector_nodes.rs b/node-graph/gcore/src/vector/vector_nodes.rs\nindex 1613534ce3..1e889ac179 100644\n--- a/node-graph/gcore/src/vector/vector_nodes.rs\n+++ b/node-graph/gcore/src/vector/vector_nodes.rs\n@@ -5,6 +5,7 @@ use crate::registry::types::{Angle, Fraction, IntegerCount, Length, SeedValue};\n use crate::renderer::GraphicElementRendered;\n use crate::transform::{Footprint, Transform, TransformMut};\n use crate::vector::style::LineJoin;\n+use crate::vector::PointDomain;\n use crate::{Color, GraphicElement, GraphicGroup};\n \n use bezier_rs::{Cap, Join, Subpath, SubpathTValue, TValue};\n@@ -292,7 +293,7 @@ async fn circular_repeat<F: 'n + Send + Copy, I: 'n + GraphicElementRendered + T\n \t)]\n \tinstance: impl Node<F, Output = I>,\n \tangle_offset: Angle,\n-\t#[default(5)] radius: Length,\n+\t#[default(5)] radius: f64,\n \t#[default(5)] instances: IntegerCount,\n ) -> GraphicGroup {\n \tlet instance = instance.eval(footprint).await;\n@@ -861,6 +862,123 @@ async fn morph<F: 'n + Send + Copy>(\n \tresult\n }\n \n+fn bevel_algorithm(mut vector_data: VectorData, distance: f64) -> VectorData {\n+\t// Splits a b\u00e9zier curve based on a distance measurement\n+\tfn split_distance(bezier: bezier_rs::Bezier, distance: f64, length: f64) -> bezier_rs::Bezier {\n+\t\tconst EUCLIDEAN_ERROR: f64 = 0.001;\n+\t\tlet parametric = bezier.euclidean_to_parametric_with_total_length(distance / length, EUCLIDEAN_ERROR, length);\n+\t\tbezier.split(bezier_rs::TValue::Parametric(parametric))[1]\n+\t}\n+\n+\t/// Produces a list that correspons with the point id. The value is how many segments are connected.\n+\tfn segments_connected_count(vector_data: &VectorData) -> Vec<u8> {\n+\t\t// Count the number of segments connectign to each point.\n+\t\tlet mut segments_connected_count = vec![0; vector_data.point_domain.ids().len()];\n+\t\tfor &point_index in vector_data.segment_domain.start_point().iter().chain(vector_data.segment_domain.end_point()) {\n+\t\t\tsegments_connected_count[point_index] += 1;\n+\t\t}\n+\n+\t\t// Zero out points without exactly two connectors. These are ignored\n+\t\tfor count in &mut segments_connected_count {\n+\t\t\tif *count != 2 {\n+\t\t\t\t*count = 0;\n+\t\t\t}\n+\t\t}\n+\t\tsegments_connected_count\n+\t}\n+\n+\t/// Updates the index so that it points at a point with the position. If nobody else will look at the index, the original point is updated. Otherwise a new point is created.\n+\tfn create_or_modify_point(point_domain: &mut PointDomain, segments_connected_count: &mut [u8], pos: DVec2, index: &mut usize, next_id: &mut PointId, new_segments: &mut Vec<[usize; 2]>) {\n+\t\tsegments_connected_count[*index] -= 1;\n+\t\tif segments_connected_count[*index] == 0 {\n+\t\t\t// If nobody else is going to look at this point, we're alright to modify it\n+\t\t\tpoint_domain.set_position(*index, pos);\n+\t\t} else {\n+\t\t\tlet new_index = point_domain.ids().len();\n+\t\t\tlet original_index = *index;\n+\n+\t\t\t// Create a new point (since someone will wish to look at the point in the original position in future)\n+\t\t\t*index = new_index;\n+\t\t\tpoint_domain.push(next_id.next_id(), pos);\n+\n+\t\t\t// Add a new segment to be created later\n+\t\t\tnew_segments.push([new_index, original_index])\n+\t\t}\n+\t}\n+\n+\tfn update_existing_segments(vector_data: &mut VectorData, distance: f64, segments_connected: &mut [u8]) -> Vec<[usize; 2]> {\n+\t\tlet mut next_id = vector_data.point_domain.next_id();\n+\t\tlet mut new_segments = Vec::new();\n+\n+\t\tfor (handles, start_point_index, end_point_index) in vector_data.segment_domain.handles_and_points_mut() {\n+\t\t\t// Convert the original segment to a bezier\n+\t\t\tlet mut bezier = bezier_rs::Bezier {\n+\t\t\t\tstart: vector_data.point_domain.positions()[*start_point_index],\n+\t\t\t\tend: vector_data.point_domain.positions()[*end_point_index],\n+\t\t\t\thandles: *handles,\n+\t\t\t};\n+\n+\t\t\tif bezier.is_linear() {\n+\t\t\t\tbezier.handles = bezier_rs::BezierHandles::Linear;\n+\t\t\t}\n+\t\t\tbezier = bezier.apply_transformation(|p| vector_data.transform.transform_point2(p));\n+\t\t\tlet inverse_transform = (vector_data.transform.matrix2.determinant() != 0.).then(|| vector_data.transform.inverse()).unwrap_or_default();\n+\n+\t\t\tlet original_length = bezier.length(None);\n+\t\t\tlet mut length = original_length;\n+\n+\t\t\tif segments_connected[*start_point_index] > 0 {\n+\t\t\t\t// Apply the bevel to the start\n+\t\t\t\tbezier = split_distance(bezier, distance.min(original_length / 2.), length);\n+\t\t\t\tlength = (length - distance).max(0.);\n+\t\t\t\t// Update the start position\n+\t\t\t\tlet pos = inverse_transform.transform_point2(bezier.start);\n+\t\t\t\tcreate_or_modify_point(&mut vector_data.point_domain, segments_connected, pos, start_point_index, &mut next_id, &mut new_segments);\n+\t\t\t}\n+\t\t\tif segments_connected[*end_point_index] > 0 {\n+\t\t\t\t// Apply the bevel to the end\n+\t\t\t\tbezier = split_distance(bezier.reversed(), distance.min(original_length / 2.), length).reversed();\n+\t\t\t\t// Update the end position\n+\t\t\t\tlet pos = inverse_transform.transform_point2(bezier.end);\n+\t\t\t\tcreate_or_modify_point(&mut vector_data.point_domain, segments_connected, pos, end_point_index, &mut next_id, &mut new_segments);\n+\t\t\t}\n+\t\t\t// Update the handles\n+\t\t\t*handles = bezier.handles.apply_transformation(|p| inverse_transform.transform_point2(p));\n+\t\t}\n+\t\tnew_segments\n+\t}\n+\n+\tfn insert_new_segments(vector_data: &mut VectorData, new_segments: &[[usize; 2]]) {\n+\t\tlet mut next_id = vector_data.segment_domain.next_id();\n+\t\tfor &[start, end] in new_segments {\n+\t\t\tvector_data.segment_domain.push(next_id.next_id(), start, end, bezier_rs::BezierHandles::Linear, StrokeId::ZERO);\n+\t\t}\n+\t}\n+\n+\tlet mut segments_connected = segments_connected_count(&vector_data);\n+\tlet new_segments = update_existing_segments(&mut vector_data, distance, &mut segments_connected);\n+\tinsert_new_segments(&mut vector_data, &new_segments);\n+\n+\tvector_data\n+}\n+\n+#[node_macro::node(category(\"Vector\"), path(graphene_core::vector))]\n+async fn bevel<F: 'n + Send + Copy>(\n+\t#[implementations(\n+\t\t(),\n+\t\tFootprint,\n+\t)]\n+\tfootprint: F,\n+\t#[implementations(\n+\t\t() -> VectorData,\n+\t\tFootprint -> VectorData,\n+\t)]\n+\tsource: impl Node<F, Output = VectorData>,\n+\t#[default(10.)] distance: Length,\n+) -> VectorData {\n+\tbevel_algorithm(source.eval(footprint).await, distance)\n+}\n+\n #[node_macro::node(category(\"Vector\"), path(graphene_core::vector))]\n async fn area(_: (), vector_data: impl Node<Footprint, Output = VectorData>) -> f64 {\n \tlet vector_data = vector_data.eval(Footprint::default()).await;\n@@ -1076,4 +1194,88 @@ mod test {\n \t\t\tvec![DVec2::new(-25., -50.), DVec2::new(50., -25.), DVec2::new(25., 50.), DVec2::new(-50., 25.)]\n \t\t);\n \t}\n+\n+\t#[track_caller]\n+\tfn contains_segment(vector: &VectorData, target: bezier_rs::Bezier) {\n+\t\tlet segments = vector.segment_bezier_iter().map(|x| x.1);\n+\t\tlet count = segments.filter(|bezier| bezier.abs_diff_eq(&target, 0.01) || bezier.reversed().abs_diff_eq(&target, 0.01)).count();\n+\t\tassert_eq!(count, 1, \"Incorrect number of {target:#?} in {:#?}\", vector.segment_bezier_iter().collect::<Vec<_>>());\n+\t}\n+\n+\t#[tokio::test]\n+\tasync fn bevel_rect() {\n+\t\tlet source = Subpath::new_rect(DVec2::ZERO, DVec2::ONE * 100.);\n+\t\tlet beveled = super::bevel(Footprint::default(), &vector_node(source), 5.).await;\n+\t\tassert_eq!(beveled.point_domain.positions().len(), 8);\n+\t\tassert_eq!(beveled.segment_domain.ids().len(), 8);\n+\n+\t\t// Segments\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(5., 0.), DVec2::new(95., 0.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(5., 100.), DVec2::new(95., 100.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(0., 5.), DVec2::new(0., 95.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(100., 5.), DVec2::new(100., 95.)));\n+\n+\t\t// Joins\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(5., 0.), DVec2::new(0., 5.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(95., 0.), DVec2::new(100., 5.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(100., 95.), DVec2::new(95., 100.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(5., 100.), DVec2::new(0., 95.)));\n+\t}\n+\n+\t#[tokio::test]\n+\tasync fn bevel_open_curve() {\n+\t\tlet curve = Bezier::from_cubic_dvec2(DVec2::ZERO, DVec2::new(10., 0.), DVec2::new(10., 100.), DVec2::X * 100.);\n+\t\tlet source = Subpath::from_beziers(&[Bezier::from_linear_dvec2(DVec2::X * -100., DVec2::ZERO), curve], false);\n+\t\tlet beveled = super::bevel(Footprint::default(), &vector_node(source), 5.).await;\n+\n+\t\tassert_eq!(beveled.point_domain.positions().len(), 4);\n+\t\tassert_eq!(beveled.segment_domain.ids().len(), 3);\n+\n+\t\t// Segments\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(-5., 0.), DVec2::new(-100., 0.)));\n+\t\tlet trimmed = curve.trim(bezier_rs::TValue::Euclidean(5. / curve.length(Some(0.00001))), bezier_rs::TValue::Parametric(1.));\n+\t\tcontains_segment(&beveled, trimmed);\n+\n+\t\t// Join\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(-5., 0.), trimmed.start));\n+\t}\n+\n+\t#[tokio::test]\n+\tasync fn bevel_with_transform() {\n+\t\tlet curve = Bezier::from_cubic_dvec2(DVec2::ZERO, DVec2::new(1., 0.), DVec2::new(1., 10.), DVec2::X * 10.);\n+\t\tlet source = Subpath::<PointId>::from_beziers(&[Bezier::from_linear_dvec2(DVec2::X * -10., DVec2::ZERO), curve], false);\n+\t\tlet mut vector_data = VectorData::from_subpath(source);\n+\t\tlet transform = DAffine2::from_scale_angle_translation(DVec2::splat(10.), 1., DVec2::new(99., 77.));\n+\t\tvector_data.transform = transform;\n+\t\tlet beveled = super::bevel(Footprint::default(), &FutureWrapperNode(vector_data), 5.).await;\n+\n+\t\tassert_eq!(beveled.point_domain.positions().len(), 4);\n+\t\tassert_eq!(beveled.segment_domain.ids().len(), 3);\n+\t\tassert_eq!(beveled.transform, transform);\n+\n+\t\t// Segments\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(-0.5, 0.), DVec2::new(-10., 0.)));\n+\t\tlet trimmed = curve.trim(bezier_rs::TValue::Euclidean(0.5 / curve.length(Some(0.00001))), bezier_rs::TValue::Parametric(1.));\n+\t\tcontains_segment(&beveled, trimmed);\n+\n+\t\t// Join\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(-0.5, 0.), trimmed.start));\n+\t}\n+\n+\t#[tokio::test]\n+\tasync fn bevel_too_high() {\n+\t\tlet source = Subpath::from_anchors([DVec2::ZERO, DVec2::new(100., 0.), DVec2::new(100., 100.), DVec2::new(0., 100.)], false);\n+\t\tlet beveled = super::bevel(Footprint::default(), &vector_node(source), 999.).await;\n+\t\tassert_eq!(beveled.point_domain.positions().len(), 6);\n+\t\tassert_eq!(beveled.segment_domain.ids().len(), 5);\n+\n+\t\t// Segments\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(0., 0.), DVec2::new(50., 0.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(100., 50.), DVec2::new(100., 50.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(100., 50.), DVec2::new(50., 100.)));\n+\n+\t\t// Joins\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(50., 0.), DVec2::new(100., 50.)));\n+\t\tcontains_segment(&beveled, bezier_rs::Bezier::from_linear_dvec2(DVec2::new(100., 50.), DVec2::new(50., 100.)));\n+\t}\n }\n", "test_patch": "", "problem_statement": "Bevel path node\nAdd a node to bevel some vector data.\r\n\r\nFor each point in a path that is connected to exactly 2 segments:\r\n- Offset by the user specified amount into both segments and insert new points.\r\n- Delete the original point.\r\n- Connect the two new points with a straight line segment.\r\n\r\nNot in scope:\r\n- Cleaning up paths where bevels overlap.\r\n- Beveling only particular points (possibly selected in the viewport by the user).\r\n- Dealing with mesh paths (beyond ignoring points with >2 segments).\r\n- Angle limit parameter\r\n- Fillets or polyline fillets\n", "hints_text": "", "created_at": "2024-10-22 15:54:37", "merge_commit_sha": "dae6b2f23942faa116703278d20e1fcc20ec0adc", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['profile', '.github/workflows/comment-profiling-changes.yaml']", "['cargo-deny', '.github/workflows/build-dev-and-ci.yml']"]]}
{"repo": "alacritty/alacritty", "instance_id": "alacritty__alacritty-7991", "base_commit": "38fed9a7c233e11e5f62433298235281fc3de885", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex b4947c03a83..0c6aafe3ed6 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -24,6 +24,7 @@ Notable changes to the `alacritty_terminal` crate are documented in its\n - Dynamic title disabled for new windows when initial one has title as CLI option\n - While terminal in mouse mode, mouse bindings that used the shift modifier and\n   had multiple actions only performed the first action\n+- Leaking FDs when closing windows on Unix systems\n \n ## 0.13.2\n \ndiff --git a/alacritty_terminal/src/tty/unix.rs b/alacritty_terminal/src/tty/unix.rs\nindex 1336fd04364..1a2104c6e0b 100644\n--- a/alacritty_terminal/src/tty/unix.rs\n+++ b/alacritty_terminal/src/tty/unix.rs\n@@ -19,8 +19,8 @@ use rustix_openpty::openpty;\n use rustix_openpty::rustix::termios::Winsize;\n #[cfg(any(target_os = \"linux\", target_os = \"macos\"))]\n use rustix_openpty::rustix::termios::{self, InputModes, OptionalActions};\n-use signal_hook::consts as sigconsts;\n-use signal_hook::low_level::pipe as signal_pipe;\n+use signal_hook::low_level::{pipe as signal_pipe, unregister as unregister_signal};\n+use signal_hook::{consts as sigconsts, SigId};\n \n use crate::event::{OnResize, WindowSize};\n use crate::tty::{ChildEvent, EventedPty, EventedReadWrite, Options};\n@@ -102,6 +102,7 @@ pub struct Pty {\n     child: Child,\n     file: File,\n     signals: UnixStream,\n+    sig_id: SigId,\n }\n \n impl Pty {\n@@ -260,13 +261,13 @@ pub fn from_fd(config: &Options, window_id: u64, master: OwnedFd, slave: OwnedFd\n     }\n \n     // Prepare signal handling before spawning child.\n-    let signals = {\n+    let (signals, sig_id) = {\n         let (sender, recv) = UnixStream::pair()?;\n \n         // Register the recv end of the pipe for SIGCHLD.\n-        signal_pipe::register(sigconsts::SIGCHLD, sender)?;\n+        let sig_id = signal_pipe::register(sigconsts::SIGCHLD, sender)?;\n         recv.set_nonblocking(true)?;\n-        recv\n+        (recv, sig_id)\n     };\n \n     match builder.spawn() {\n@@ -277,7 +278,7 @@ pub fn from_fd(config: &Options, window_id: u64, master: OwnedFd, slave: OwnedFd\n                 set_nonblocking(master_fd);\n             }\n \n-            Ok(Pty { child, file: File::from(master), signals })\n+            Ok(Pty { child, file: File::from(master), signals, sig_id })\n         },\n         Err(err) => Err(Error::new(\n             err.kind(),\n@@ -296,6 +297,10 @@ impl Drop for Pty {\n         unsafe {\n             libc::kill(self.child.id() as i32, libc::SIGHUP);\n         }\n+\n+        // Clear signal-hook handler.\n+        unregister_signal(self.sig_id);\n+\n         let _ = self.child.wait();\n     }\n }\n", "test_patch": "", "problem_statement": "alacritty msg create-window leaves a unix fifo behind\nI like my terminals popping up fast, so I use this script to start them:\r\n```\r\n/home/greg/.cargo/bin/alacritty msg create-window ||\r\nexec /home/greg/.cargo/bin/alacritty -o 'font.normal.family: JetBrainsMono' -o 'draw_bold_text_with_bright_colors: true'\r\n```\r\n\r\nCuriously, starting a new window and closing it reliably leaves an extra fd behind.\r\n\r\nThe way I monitor it is I lookup the `alacritty` pid and then run this loop:\r\n```\r\n% while sleep 1; do lsof -p 17550 | grep -v mem | wc -l; done\r\n31\r\n31\r\n31\r\n... launched then closed a window captured logs of FDs attached\r\n32\r\n38 <- new window\r\n33 <- closed\r\n39 <- new window\r\n34 <- closed\r\n40 <- new window\r\n35 <- closed\r\n35\r\n35\r\n```\r\n\r\nThe delta between the initial set of FDs and after the second window is closed is this single FIFO.\r\n% diff /tmp/{1,3}.txt\r\n28a29\r\n> alacritty 17550 greg   30u     unix 0x000000003920e72d       0t0  167503 type=STREAM (CONNECTED)\r\n\r\nStarting more windows adds more such FIFOs left behind.\r\n\r\n### System\r\n\r\nOS: Linux 6.1.0-21-amd64 (also OpenBSD)\r\nVersion: alacritty 0.13.2\r\nLinux/BSD: X11\r\n\r\n### Logs\r\n\r\nFDs right after the first process starts [1.txt](https://github.com/alacritty/alacritty/files/15329794/1.txt)\r\nFDs when the second window is opened [2.txt](https://github.com/alacritty/alacritty/files/15329795/2.txt)\r\nFDs after the second window is closed [3.txt](https://github.com/alacritty/alacritty/files/15329796/3.txt)\r\n\n", "hints_text": "Very odd, I can confirm this behavior, but even explicitly shutting down the socket streams does not seem to fix it.\r\n\r\nThis does not fix it:\r\n\r\n```diff\r\ndiff --git a/alacritty/src/ipc.rs b/alacritty/src/ipc.rs\r\nindex 1cb7a1c8..82cbf698 100644\r\n--- a/alacritty/src/ipc.rs\r\n+++ b/alacritty/src/ipc.rs\r\n@@ -41,9 +41,9 @@ pub fn spawn_ipc_socket(options: &Options, event_proxy: EventLoopProxy<Event>) -\r\n         let mut data = String::new();\r\n         for stream in listener.incoming().filter_map(Result::ok) {\r\n             data.clear();\r\n-            let mut stream = BufReader::new(stream);\r\n+            let mut xxx = BufReader::new(&stream);\r\n\r\n-            match stream.read_line(&mut data) {\r\n+            match xxx.read_line(&mut data) {\r\n                 Ok(0) | Err(_) => continue,\r\n                 Ok(_) => (),\r\n             };\r\n@@ -72,6 +72,8 @@ pub fn spawn_ipc_socket(options: &Options, event_proxy: EventLoopProxy<Event>) -\r\n                     let _ = event_proxy.send_event(event);\r\n                 },\r\n             }\r\n+\r\n+            stream.shutdown(std::net::Shutdown::Both).unwrap();\r\n         }\r\n     });\r\n```\r\n\r\nThis does:\r\n\r\n```diff\r\ndiff --git a/alacritty/src/ipc.rs b/alacritty/src/ipc.rs\r\nindex 1cb7a1c8..0d2f7870 100644\r\n--- a/alacritty/src/ipc.rs\r\n+++ b/alacritty/src/ipc.rs\r\n@@ -40,38 +40,6 @@ pub fn spawn_ipc_socket(options: &Options, event_proxy: EventLoopProxy<Event>) -\r\n     thread::spawn_named(\"socket listener\", move || {\r\n         let mut data = String::new();\r\n         for stream in listener.incoming().filter_map(Result::ok) {\r\n-            data.clear();\r\n-            let mut stream = BufReader::new(stream);\r\n-\r\n-            match stream.read_line(&mut data) {\r\n-                Ok(0) | Err(_) => continue,\r\n-                Ok(_) => (),\r\n-            };\r\n-\r\n-            // Read pending events on socket.\r\n-            let message: SocketMessage = match serde_json::from_str(&data) {\r\n-                Ok(message) => message,\r\n-                Err(err) => {\r\n-                    warn!(\"Failed to convert data from socket: {}\", err);\r\n-                    continue;\r\n-                },\r\n-            };\r\n-\r\n-            // Handle IPC events.\r\n-            match message {\r\n-                SocketMessage::CreateWindow(options) => {\r\n-                    let event = Event::new(EventType::CreateWindow(options), None);\r\n-                    let _ = event_proxy.send_event(event);\r\n-                },\r\n-                SocketMessage::Config(ipc_config) => {\r\n-                    let window_id = ipc_config\r\n-                        .window_id\r\n-                        .and_then(|id| u64::try_from(id).ok())\r\n-                        .map(WindowId::from);\r\n-                    let event = Event::new(EventType::IpcConfig(ipc_config), window_id);\r\n-                    let _ = event_proxy.send_event(event);\r\n-                },\r\n-            }\r\n         }\r\n     });\r\n```\r\n\r\nSo I'd assume the issue is somewhere in-between. Don't have time to look more into it myself but I figure it shouldn't be that hart to narrow down further.", "created_at": "2024-05-22 03:53:30", "merge_commit_sha": "f04b16161bc542075fdb8e5946a8eed976f26b0b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['check-macos-arm', '.github/workflows/ci.yml']", "['build (windows-latest)', '.github/workflows/ci.yml']"]]}
{"repo": "dathere/qsv", "instance_id": "dathere__qsv-2362", "base_commit": "9c0c1a7a63ef3773e599f6fa91e6fa3b734936df", "patch": "diff --git a/src/cmd/luau.rs b/src/cmd/luau.rs\nindex e0dcba7b2..e0da289b6 100644\n--- a/src/cmd/luau.rs\n+++ b/src/cmd/luau.rs\n@@ -238,6 +238,8 @@ Common options:\n \"#;\n \n use std::{\n+    cell::RefCell,\n+    collections::HashMap,\n     env, fs, io,\n     io::Write,\n     path::Path,\n@@ -1762,6 +1764,71 @@ fn setup_helpers(\n     )?;\n     luau.globals().set(\"qsv_loadcsv\", qsv_loadcsv)?;\n \n+    // this is a helper function to load a JSON file into a Luau table.\n+    //\n+    //   qsv_loadjson(table_name: string, filepath: string)\n+    //      table_name: the name of the Luau table to load the JSON data into.\n+    //        filepath: the path of the JSON file to load\n+    //         returns: true if successful.\n+    //                  A Luau runtime error if the filepath is invalid or JSON parsing fails.\n+    //\n+    let qsv_loadjson =\n+        luau.create_function(move |luau, (table_name, filepath): (String, String)| {\n+            if filepath.is_empty() {\n+                return helper_err!(\"qsv_loadjson\", \"filepath cannot be empty.\");\n+            }\n+\n+            let path = Path::new(&filepath);\n+            if !path.exists() {\n+                return helper_err!(\"qsv_loadjson\", \"\\\"{}\\\" does not exist.\", path.display());\n+            }\n+\n+            // Read the JSON file\n+            let json_str = match fs::read_to_string(path) {\n+                Ok(content) => content,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to read JSON file \\\"{}\\\": {e}\",\n+                        path.display()\n+                    );\n+                },\n+            };\n+\n+            // Parse the JSON string\n+            let json_value: serde_json::Value = match serde_json::from_str(&json_str) {\n+                Ok(v) => v,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to parse JSON from \\\"{}\\\": {e}\",\n+                        path.display()\n+                    );\n+                },\n+            };\n+\n+            // Convert JSON value to Luau value and store it in the global table\n+            let luau_value = match luau.to_value(&json_value) {\n+                Ok(v) => v,\n+                Err(e) => {\n+                    return helper_err!(\n+                        \"qsv_loadjson\",\n+                        \"Failed to convert JSON to Luau value: {e}\"\n+                    );\n+                },\n+            };\n+\n+            luau.globals().raw_set(table_name.clone(), luau_value)?;\n+\n+            info!(\n+                \"{} successfully loaded JSON into table '{}'.\",\n+                filepath, table_name\n+            );\n+\n+            Ok(true)\n+        })?;\n+    luau.globals().set(\"qsv_loadjson\", qsv_loadjson)?;\n+\n     // this is a helper function that can be called from the BEGIN, MAIN & END scripts to write to\n     // a file. The file will be created if it does not exist. The file will be appended to if it\n     // already exists. The filename will be sanitized and will be written to the current working\n@@ -2030,6 +2097,271 @@ fn setup_helpers(\n     })?;\n     luau.globals().set(\"qsv_shellcmd\", qsv_shellcmd)?;\n \n+    // this is a helper function that calculates the cumulative sum of a numeric column.\n+    // if the input cannot be converted to a number, it returns 0 for that row.\n+    //\n+    //   qsv_cumsum(name, value)\n+    //          name: identifier for this cumulative sum (allows multiple sums to run in parallel)\n+    //         value: the numeric value to add to the cumulative sum\n+    //       returns: the cumulative sum up to the current row for the named sum\n+    //\n+    let qsv_cumsum = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        // Get static cumulative sums using thread_local storage\n+        thread_local! {\n+            static CUMSUMS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        // Convert input value to number, defaulting to 0.0 if conversion fails\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or_default(),\n+            _ => 0.0,\n+        };\n+\n+        // Update cumulative sum for this name\n+        CUMSUMS.with(|cs| {\n+            let mut sums = cs.borrow_mut();\n+            let sum = sums.entry(name).or_insert(0.0);\n+            *sum += num;\n+            Ok(*sum)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumsum\", qsv_cumsum)?;\n+\n+    // this is a helper function that calculates the cumulative product of a numeric column.\n+    // if the input cannot be converted to a number, it returns 1 for that row.\n+    //\n+    //   qsv_cumprod(name, value)\n+    //          name: identifier for this cumulative product (allows multiple products to run in\n+    // parallel)         value: the numeric value to multiply with the cumulative product\n+    //       returns: the cumulative product up to the current row for the named product\n+    //\n+    let qsv_cumprod = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMPRODS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(1.0),\n+            _ => 1.0,\n+        };\n+\n+        CUMPRODS.with(|cp| {\n+            let mut prods = cp.borrow_mut();\n+            let prod = prods.entry(name).or_insert(1.0);\n+            *prod *= num;\n+            Ok(*prod)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumprod\", qsv_cumprod)?;\n+\n+    // this is a helper function that calculates the cumulative maximum of a numeric column.\n+    // if the input cannot be converted to a number, it returns negative infinity for that row.\n+    //\n+    //   qsv_cummax(name, value)\n+    //          name: identifier for this cumulative maximum (allows multiple maximums to run in\n+    // parallel)         value: the numeric value to compare with the cumulative maximum\n+    //       returns: the cumulative maximum up to the current row for the named maximum\n+    //\n+    let qsv_cummax = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMMAXS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s\n+                .to_string_lossy()\n+                .parse::<f64>()\n+                .unwrap_or(f64::NEG_INFINITY),\n+            _ => f64::NEG_INFINITY,\n+        };\n+\n+        CUMMAXS.with(|cm| {\n+            let mut maxs = cm.borrow_mut();\n+            let max = maxs.entry(name).or_insert(f64::NEG_INFINITY);\n+            *max = max.max(num);\n+            Ok(*max)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cummax\", qsv_cummax)?;\n+\n+    // this is a helper function that calculates the cumulative minimum of a numeric column.\n+    // if the input cannot be converted to a number, it returns positive infinity for that row.\n+    //\n+    //   qsv_cummin(name, value)\n+    //          name: identifier for this cumulative minimum (allows multiple minimums to run in\n+    // parallel)         value: the numeric value to compare with the cumulative minimum\n+    //       returns: the cumulative minimum up to the current row for the named minimum\n+    //\n+    let qsv_cummin = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMMINS: RefCell<HashMap<String, f64>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let num = match value {\n+            Value::Number(n) => n,\n+            Value::Integer(i) => i as f64,\n+            Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(f64::INFINITY),\n+            _ => f64::INFINITY,\n+        };\n+\n+        CUMMINS.with(|cm| {\n+            let mut mins = cm.borrow_mut();\n+            let min = mins.entry(name).or_insert(f64::INFINITY);\n+            *min = min.min(num);\n+            Ok(*min)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cummin\", qsv_cummin)?;\n+\n+    // qsv_lag - returns lagged value with optional default\n+    //\n+    //   qsv_lag(name, value, lag, default)\n+    //          name: identifier for this lag (allows multiple lags to run in parallel)\n+    //         value: the value to lag\n+    //           lag: (optional) number of rows to lag by (default: 1)\n+    //       default: (optional) value to return for rows before lag is available (default: \"0\")\n+    //       returns: the value from 'lag' rows ago, or default if not enough rows seen yet\n+    let qsv_lag = luau.create_function(|luau, (name, value, lag, default): (String, mlua::Value, Option<i64>, Option<mlua::Value>)| {\n+        thread_local! {\n+            static LAGS: RefCell<HashMap<String, Vec<String>>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let lag = lag.unwrap_or(1);\n+        let key = format!(\"{name}_{lag}\");\n+\n+        // Convert the value to a string to store it\n+        let value_str = match &value {\n+            Value::String(s) => s.to_string_lossy(),\n+            Value::Number(n) => n.to_string(),\n+            Value::Integer(i) => i.to_string(),\n+            Value::Boolean(b) => b.to_string(),\n+            Value::Nil => String::new(),\n+            _ => value.to_string().unwrap_or_default(),\n+        };\n+\n+        LAGS.with(|l| {\n+            let mut lags = l.borrow_mut();\n+            let values = lags.entry(key).or_default();\n+            values.push(value_str);\n+\n+            if values.len() as i64 <= lag {\n+                // Return the default value when not enough history\n+                Ok(default.unwrap_or_else(|| {\n+                    mlua::Value::String(luau.create_string(\"0\").unwrap())\n+                }))\n+            } else {\n+                let lagged_value = &values[values.len() - 1 - lag as usize];\n+                Ok(mlua::Value::String(luau.create_string(lagged_value)?))\n+            }\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_lag\", qsv_lag)?;\n+\n+    // qsv_cumany - returns true if any value so far has been truthy\n+    //\n+    //   qsv_cumany(name, value)\n+    //          name: identifier for this cumulative any (allows multiple cumany's to run in\n+    // parallel)         value: the value to check for truthiness\n+    //       returns: true if any value seen so far has been truthy, false otherwise\n+    let qsv_cumany = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMANYS: RefCell<HashMap<String, bool>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let is_truthy = match value {\n+            Value::Boolean(b) => b,\n+            Value::Number(n) => n != 0.0,\n+            Value::Integer(i) => i != 0,\n+            Value::String(s) => !s.to_string_lossy().is_empty(),\n+            Value::Nil => false,\n+            _ => true, // Tables, functions, etc. are considered truthy\n+        };\n+\n+        CUMANYS.with(|ca| {\n+            let mut anys = ca.borrow_mut();\n+            let any = anys.entry(name).or_insert(false);\n+            *any = *any || is_truthy;\n+            Ok(*any)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumany\", qsv_cumany)?;\n+\n+    // qsv_cumall - returns true if all values so far have been truthy\n+    //\n+    //   qsv_cumall(name, value)\n+    //          name: identifier for this cumulative all (allows multiple cumall's to run in\n+    // parallel)         value: the value to check for truthiness\n+    //       returns: true if all values seen so far have been truthy, false otherwise\n+    let qsv_cumall = luau.create_function(|_, (name, value): (String, mlua::Value)| {\n+        thread_local! {\n+            static CUMALLS: RefCell<HashMap<String, bool>> = RefCell::new(HashMap::new());\n+        }\n+\n+        let is_truthy = match value {\n+            Value::Boolean(b) => b,\n+            Value::Number(n) => n != 0.0,\n+            Value::Integer(i) => i != 0,\n+            Value::String(s) => !s.to_string_lossy().is_empty(),\n+            Value::Nil => false,\n+            _ => true, // Tables, functions, etc. are considered truthy\n+        };\n+\n+        CUMALLS.with(|ca| {\n+            let mut all_vals = ca.borrow_mut();\n+            let all = all_vals.entry(name).or_insert(true);\n+            *all = *all && is_truthy;\n+            Ok(*all)\n+        })\n+    })?;\n+    luau.globals().set(\"qsv_cumall\", qsv_cumall)?;\n+\n+    // qsv_diff - returns difference between current and previous value\n+    //\n+    //   qsv_diff(name, value[, periods])\n+    //          name: identifier for this diff (allows multiple diffs to run in parallel)\n+    //         value: the value to calculate difference for\n+    //       periods: optional number of periods to look back (default: 1)\n+    //       returns: difference between current value and value 'periods' rows back\n+    //               returns 0 if not enough history available yet\n+    let qsv_diff = luau.create_function(\n+        |_, (name, value, periods): (String, mlua::Value, Option<i64>)| {\n+            thread_local! {\n+                static DIFFS: RefCell<HashMap<String, Vec<f64>>> = RefCell::new(HashMap::new());\n+            }\n+\n+            let periods = periods.unwrap_or(1);\n+            // Create a unique key that includes both the name and periods\n+            let key = format!(\"{name}_{periods}\");\n+\n+            let num = match value {\n+                Value::Number(n) => n,\n+                Value::Integer(i) => i as f64,\n+                Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(0.0),\n+                _ => 0.0,\n+            };\n+\n+            DIFFS.with(|d| {\n+                let mut diffs = d.borrow_mut();\n+                let values = diffs.entry(key).or_default();\n+                values.push(num);\n+\n+                if values.len() as i64 <= periods {\n+                    Ok(0.0) // Return 0 when not enough history\n+                } else {\n+                    let prev_value = values[values.len() - 1 - periods as usize];\n+                    Ok(num - prev_value)\n+                }\n+            })\n+        },\n+    )?;\n+    luau.globals().set(\"qsv_diff\", qsv_diff)?;\n+\n     // this is a helper function that can be called from the BEGIN script to register\n     // and load a lookup table. It expects two arguments - the lookup_name & the\n     // lookup_table_uri - the URI of the CSV to use as a lookup table.\n", "test_patch": "diff --git a/tests/test_luau.rs b/tests/test_luau.rs\nindex a926845ac..599d065c2 100644\n--- a/tests/test_luau.rs\n+++ b/tests/test_luau.rs\n@@ -2313,3 +2313,621 @@ return tonumber(number) > tonumber(limit)\n     ];\n     assert_eq!(got, expected);\n }\n+\n+#[test]\n+fn luau_loadjson() {\n+    let wrk = Workdir::new(\"luau_loadjson\");\n+\n+    // Create a test JSON file\n+    wrk.create_from_string(\n+        \"test.json\",\n+        r#\"{\n+            \"string\": \"hello\",\n+            \"number\": 42,\n+            \"boolean\": true,\n+            \"null\": null,\n+            \"array\": [1, 2, 3],\n+            \"object\": {\n+                \"nested\": \"value\",\n+                \"numbers\": [4, 5, 6]\n+            }\n+        }\"#,\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(r#\"\n+BEGIN {\n+    qsv_loadjson(\"config_tbl\", \"test.json\");\n+    qsv_log(\"debug\", \"Loaded JSON table:\", config_tbl)\n+    for k,v in pairs(config_tbl) do\n+        qsv_log(\"debug\", \"Key:\", k, \"Value:\", v)\n+    end\n+}!\n+\n+return config_tbl[\"string\"] .. \" \" .. config_tbl[\"object\"][\"nested\"] .. \" \" .. tostring(config_tbl[\"number\"])\n+\"#)\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"], svec![\"b\"], svec![\"c\"]]);\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"a\", \"hello value 42\"],\n+        svec![\"b\", \"hello value 42\"],\n+        svec![\"c\", \"hello value 42\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_loadjson_array() {\n+    let wrk = Workdir::new(\"luau_loadjson_array\");\n+\n+    // Create a test JSON file with an array\n+    wrk.create_from_string(\n+        \"test.json\",\n+        r#\"[\n+            {\"id\": 1, \"name\": \"Alice\"},\n+            {\"id\": 2, \"name\": \"Bob\"},\n+            {\"id\": 3, \"name\": \"Charlie\"}\n+        ]\"#,\n+    );\n+\n+    // Create input CSV with index references\n+    wrk.create(\n+        \"data.csv\",\n+        vec![svec![\"x\"], svec![\"1\"], svec![\"2\"], svec![\"3\"]],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"name\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"users\", \"test.json\");\n+}!\n+\n+local idx = tonumber(x)\n+return users[idx][\"name\"]\n+\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"x\", \"name\"],\n+        svec![\"1\", \"Alice\"],\n+        svec![\"2\", \"Bob\"],\n+        svec![\"3\", \"Charlie\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_loadjson_error_missing_file() {\n+    let wrk = Workdir::new(\"luau_loadjson_error\");\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"config\", \"nonexistent.json\");\n+}!\n+\n+return \"should not get here\"\n+\"#,\n+        )\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"]]);\n+\n+    wrk.assert_err(&mut cmd);\n+}\n+\n+#[test]\n+fn luau_loadjson_error_invalid_json() {\n+    let wrk = Workdir::new(\"luau_loadjson_invalid\");\n+\n+    // Create an invalid JSON file\n+    wrk.create_from_string(\n+        \"invalid.json\",\n+        r#\"{\n+            \"unclosed\": \"object\"\n+        \"#, // Missing closing brace\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"result\")\n+        .arg(\n+            r#\"\n+BEGIN {\n+    qsv_loadjson(\"config\", \"invalid.json\");\n+}!\n+\n+return \"should not get here\"\n+\"#,\n+        )\n+        .arg(\"--no-headers\")\n+        .arg(\"data.csv\");\n+\n+    // Create a simple input CSV\n+    wrk.create(\"data.csv\", vec![svec![\"a\"]]);\n+\n+    wrk.assert_err(&mut cmd);\n+}\n+\n+#[test]\n+fn luau_cumsum_single() {\n+    let wrk = Workdir::new(\"luau_cumsum_single\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"amount\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+            svec![\"50\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_total\")\n+        .arg(r#\"qsv_cumsum(\"total\", amount)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"amount\", \"running_total\"],\n+        svec![\"10\", \"10\"],\n+        svec![\"20\", \"30\"],\n+        svec![\"30\", \"60\"],\n+        svec![\"40\", \"100\"],\n+        svec![\"50\", \"150\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumsum_multiple() {\n+    let wrk = Workdir::new(\"luau_cumsum_multiple\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"revenue\", \"expenses\"],\n+            svec![\"100\", \"50\"],\n+            svec![\"200\", \"75\"],\n+            svec![\"150\", \"80\"],\n+            svec![\"300\", \"100\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"revenue_total,expenses_total,profit_total\")\n+        .arg(\n+            r#\"{\n+            qsv_cumsum(\"revenue\", revenue),\n+            qsv_cumsum(\"expenses\", expenses),\n+            qsv_cumsum(\"profit\", tonumber(revenue) - tonumber(expenses))\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\n+            \"revenue\",\n+            \"expenses\",\n+            \"revenue_total\",\n+            \"expenses_total\",\n+            \"profit_total\"\n+        ],\n+        svec![\"100\", \"50\", \"100\", \"50\", \"50\"],\n+        svec![\"200\", \"75\", \"300\", \"125\", \"175\"],\n+        svec![\"150\", \"80\", \"450\", \"205\", \"245\"],\n+        svec![\"300\", \"100\", \"750\", \"305\", \"445\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumsum_invalid_input() {\n+    let wrk = Workdir::new(\"luau_cumsum_invalid\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"invalid\"],\n+            svec![\"20\"],\n+            svec![\"bad\"],\n+            svec![\"30\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"total\")\n+        .arg(r#\"qsv_cumsum(\"sum\", value)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"total\"],\n+        svec![\"10\", \"10\"],\n+        svec![\"invalid\", \"10\"], // Invalid input treated as 0\n+        svec![\"20\", \"30\"],\n+        svec![\"bad\", \"30\"], // Invalid input treated as 0\n+        svec![\"30\", \"60\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumprod() {\n+    let wrk = Workdir::new(\"luau_cumprod\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"2\"],\n+            svec![\"3\"],\n+            svec![\"4\"],\n+            svec![\"5\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_product\")\n+        .arg(r#\"qsv_cumprod(\"prod\", value)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_product\"],\n+        svec![\"2\", \"2\"],\n+        svec![\"3\", \"6\"],\n+        svec![\"4\", \"24\"],\n+        svec![\"5\", \"120\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cummax_cummin() {\n+    let wrk = Workdir::new(\"luau_cummax_cummin\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"5\"],\n+            svec![\"15\"],\n+            svec![\"3\"],\n+            svec![\"12\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_max,running_min\")\n+        .arg(r#\"{qsv_cummax(\"max\", value), qsv_cummin(\"min\", value)}\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_max\", \"running_min\"],\n+        svec![\"10\", \"10\", \"10\"],\n+        svec![\"5\", \"10\", \"5\"],\n+        svec![\"15\", \"15\", \"5\"],\n+        svec![\"3\", \"15\", \"3\"],\n+        svec![\"12\", \"15\", \"3\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_lag() {\n+    let wrk = Workdir::new(\"luau_lag\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+            svec![\"50\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"lag1,lag2\")\n+        .arg(\n+            r#\"{\n+            qsv_lag(\"val\", value, 1, \"0\"),\n+            qsv_lag(\"val\", value, 2, \"0\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"lag1\", \"lag2\"],\n+        svec![\"10\", \"0\", \"0\"],\n+        svec![\"20\", \"10\", \"0\"],\n+        svec![\"30\", \"20\", \"10\"],\n+        svec![\"40\", \"30\", \"20\"],\n+        svec![\"50\", \"40\", \"30\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumany_cumall() {\n+    let wrk = Workdir::new(\"luau_cumany_cumall\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"0\"],\n+            svec![\"15\"],\n+            svec![\"0\"],\n+            svec![\"20\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"has_zero,all_positive\")\n+        .arg(\n+            r#\"{\n+            qsv_cumany(\"zero\", value == \"0\"),\n+            qsv_cumall(\"pos\", tonumber(value) > 0)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"has_zero\", \"all_positive\"],\n+        svec![\"10\", \"false\", \"true\"],\n+        svec![\"0\", \"true\", \"false\"],\n+        svec![\"15\", \"true\", \"false\"],\n+        svec![\"0\", \"true\", \"false\"],\n+        svec![\"20\", \"true\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_diff() {\n+    let wrk = Workdir::new(\"luau_diff\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"15\"],\n+            svec![\"13\"],\n+            svec![\"20\"],\n+            svec![\"18\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"diff1,diff2\")\n+        .arg(\n+            r#\"{\n+            qsv_diff(\"val\", value),\n+            qsv_diff(\"val\", value, 2)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"diff1\", \"diff2\"],\n+        svec![\"10\", \"0\", \"0\"],\n+        svec![\"15\", \"5\", \"0\"],\n+        svec![\"13\", \"-2\", \"3\"],\n+        svec![\"20\", \"7\", \"5\"],\n+        svec![\"18\", \"-2\", \"5\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumany_cumall_with_strings() {\n+    let wrk = Workdir::new(\"luau_cumany_cumall_strings\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"hello\"],\n+            svec![\"\"],\n+            svec![\"world\"],\n+            svec![\"\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"has_empty,all_nonempty\")\n+        .arg(\n+            r#\"{\n+            qsv_cumany(\"empty\", value == \"\"),\n+            qsv_cumall(\"nonempty\", value ~= \"\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"has_empty\", \"all_nonempty\"],\n+        svec![\"hello\", \"false\", \"true\"],\n+        svec![\"\", \"true\", \"false\"],\n+        svec![\"world\", \"true\", \"false\"],\n+        svec![\"\", \"true\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumall_with_numbers() {\n+    let wrk = Workdir::new(\"luau_cumall_numbers\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"0\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"all_positive\")\n+        .arg(r#\"qsv_cumall(\"pos\", tonumber(value) > 0)\"#)\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"all_positive\"],\n+        svec![\"10\", \"true\"],\n+        svec![\"0\", \"false\"],\n+        svec![\"20\", \"false\"],\n+        svec![\"30\", \"false\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_lag_with_default() {\n+    let wrk = Workdir::new(\"luau_lag_default\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"10\"],\n+            svec![\"20\"],\n+            svec![\"30\"],\n+            svec![\"40\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"prev,prev2\")\n+        .arg(\n+            r#\"{\n+            qsv_lag(\"lag1\", value, 1, \"N/A\"),\n+            qsv_lag(\"lag2\", value, 2, \"N/A\")\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"prev\", \"prev2\"],\n+        svec![\"10\", \"N/A\", \"N/A\"],\n+        svec![\"20\", \"10\", \"N/A\"],\n+        svec![\"30\", \"20\", \"10\"],\n+        svec![\"40\", \"30\", \"20\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_diff_with_lag() {\n+    let wrk = Workdir::new(\"luau_diff_lag\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"100\"],\n+            svec![\"120\"],\n+            svec![\"115\"],\n+            svec![\"140\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"diff1,diff2,pct_change\")\n+        .arg(\n+            r#\"{\n+            qsv_diff(\"d1\", value),\n+            qsv_diff(\"d2\", value, 2),\n+            string.format(\"%.1f%%\", ((tonumber(value) / tonumber(qsv_lag(\"pct\", value))) - 1.0) * 100)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"diff1\", \"diff2\", \"pct_change\"],\n+        svec![\"100\", \"0\", \"0\", \"inf%\"], // division by zero\n+        svec![\"120\", \"20\", \"0\", \"20.0%\"],\n+        svec![\"115\", \"-5\", \"15\", \"-4.2%\"],\n+        svec![\"140\", \"25\", \"20\", \"21.7%\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n+\n+#[test]\n+fn luau_cumulative_stats() {\n+    let wrk = Workdir::new(\"luau_cumulative_stats\");\n+    wrk.create(\n+        \"data.csv\",\n+        vec![\n+            svec![\"value\"],\n+            svec![\"15\"],\n+            svec![\"10\"],\n+            svec![\"25\"],\n+            svec![\"5\"],\n+            svec![\"20\"],\n+        ],\n+    );\n+\n+    let mut cmd = wrk.command(\"luau\");\n+    cmd.arg(\"map\")\n+        .arg(\"running_max,running_min,running_sum\")\n+        .arg(\n+            r#\"{\n+            qsv_cummax(\"max\", value),\n+            qsv_cummin(\"min\", value),\n+            qsv_cumsum(\"sum\", value)\n+        }\"#,\n+        )\n+        .arg(\"data.csv\");\n+\n+    let got: Vec<Vec<String>> = wrk.read_stdout(&mut cmd);\n+    let expected = vec![\n+        svec![\"value\", \"running_max\", \"running_min\", \"running_sum\"],\n+        svec![\"15\", \"15\", \"15\", \"15\"],\n+        svec![\"10\", \"15\", \"10\", \"25\"],\n+        svec![\"25\", \"25\", \"10\", \"50\"],\n+        svec![\"5\", \"25\", \"5\", \"55\"],\n+        svec![\"20\", \"25\", \"5\", \"75\"],\n+    ];\n+    assert_eq!(got, expected);\n+}\n", "problem_statement": "`luau`: additional helper functions\n@ggrothendieck came up with an extensive list of helper functions to add to `luau` as qsv's DSL.\r\n\r\n---\r\n\r\nIf you are implementing cumsum there are a number of other related functions that have proven to be useful in other languages which\u00a0follow a similar pattern and so could be readily implemented at the same time.\r\n\r\n**cumprod, cumany, cumall, cummax, cummin**\r\nThese are like cumsum but in place of + they use *, or, and, max amd min. cummean is also useful but does not fit exactly into the same pattern.\r\n\r\n**accumulate**\r\nHas three arguments. A column, a function of two arguments and an optional initial value. If\u00a0\u00a0 y = accum(x, f, init)then y[1] = init and for i >1 we have y[i] = f(y[i-1], x[i]). The default for init is x[1]. Note that if f is +, *, or, and, max or min we get the above cum... functions.\r\n\r\n**Other**\r\n\r\nThe following are also useful and are related in so far as they also involve storing the previous value.\r\n\r\n**lag**\r\nIt has three arguments. The column, how many positions to lag (default is 1) and in the case that the lag is off the front of the column use default. If y = lag(x, k, default) then\u00a0y[i] = x[i-k] if i > k and default otherwise. Negative k could be considered too if not too hard to implement.\r\nRecall we discussed enum by group with the shortest solution being the following where Name is the column to group by:\r\n```\r\nqsv luau map seq \"x = (Name == prev and 1 or 0) * (x or 0) + 1; prev = Name; return x\" file1.csv\r\n```\r\nWith lag we could omit setting prev\r\n```\r\nqsv luau map seq \"x = (Name == lag(Name, 1) and 1 or 0) * (x or 0) + 1; return x\" file1.csv\r\n```\r\n\r\n**diff**\r\nSame args as lag. Defined as x - lag(x, k, default)\r\n\r\n_Originally posted by @ggrothendieck in https://github.com/jqnatividad/qsv/discussions/1760#discussioncomment-9262555_\n", "hints_text": "Do this once mlua 0.10.0 is released\r\nhttps://github.com/mlua-rs/mlua/blob/main/docs/release_notes/v0.10.md", "created_at": "2024-12-21 14:45:24", "merge_commit_sha": "e87f3e9a957c59cf084aab7ed94656e8d37bfdcd", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['build', '.github/workflows/rust-windows.yml']", "['build', '.github/workflows/rust-macos-polars.yml']"], ["['DevSkim', '.github/workflows/devskim.yml']", "['build', '.github/workflows/rust.yml']"], ["['build', '.github/workflows/rust-qsvdp.yml']", "['build', '.github/workflows/rust-qsvlite.yml']"]]}
{"repo": "Nukesor/pueue", "instance_id": "Nukesor__pueue-583", "base_commit": "d43beefff66d976bf53cde3d8546ab6da6967460", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 03e141234..bd2722c30 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -81,6 +81,7 @@ TLDR: The new task state representation is more verbose but significantly cleane\n - Add `command` filter to `pueue status`. [#524](https://github.com/Nukesor/pueue/issues/524) [#560](https://github.com/Nukesor/pueue/pull/560)\n - Allow `pueue status` to order tasks by `enqueue_at`. [#554](https://github.com/Nukesor/pueue/issues/554)\n - Added Windows service on Windows to allow a true daemon experience. [#344](https://github.com/Nukesor/pueue/issues/344) [#567](https://github.com/Nukesor/pueue/pull/567)\n+- Add `queued_count` and `stashed_count` to callback template variables. This allows users to fire callbacks when whole groups are finished. [#578](https://github.com/Nukesor/pueue/issues/578)\n \n ### Fixed\n \ndiff --git a/pueue/src/daemon/callbacks.rs b/pueue/src/daemon/callbacks.rs\nindex 0e05513f7..0884488b0 100644\n--- a/pueue/src/daemon/callbacks.rs\n+++ b/pueue/src/daemon/callbacks.rs\n@@ -21,7 +21,7 @@ pub fn spawn_callback(settings: &Settings, state: &mut LockedState, task: &Task)\n     };\n \n     // Build the command to be called from the template string in the configuration file.\n-    let callback_command = match build_callback_command(settings, task, template_string) {\n+    let callback_command = match build_callback_command(settings, state, task, template_string) {\n         Ok(callback_command) => callback_command,\n         Err(err) => {\n             error!(\"Failed to create callback command from template with error: {err}\");\n@@ -49,6 +49,7 @@ pub fn spawn_callback(settings: &Settings, state: &mut LockedState, task: &Task)\n /// finished task.\n pub fn build_callback_command(\n     settings: &Settings,\n+    state: &mut LockedState,\n     task: &Task,\n     template_string: &str,\n ) -> Result<String, RenderError> {\n@@ -62,7 +63,20 @@ pub fn build_callback_command(\n     parameters.insert(\"id\", task.id.to_string());\n     parameters.insert(\"command\", task.command.clone());\n     parameters.insert(\"path\", (*task.path.to_string_lossy()).to_owned());\n+\n+    // Add group information to template\n+    // This includes how many stashed and queued tasks are left in the group.\n     parameters.insert(\"group\", task.group.clone());\n+    let queued_tasks = state\n+        .filter_tasks_of_group(|task| task.is_queued(), &task.group)\n+        .matching_ids\n+        .len();\n+    parameters.insert(\"queued_count\", queued_tasks.to_string());\n+    let stashed_tasks = state\n+        .filter_tasks_of_group(|task| task.is_stashed(), &task.group)\n+        .matching_ids\n+        .len();\n+    parameters.insert(\"stashed_count\", stashed_tasks.to_string());\n \n     // Result takes the TaskResult Enum strings, unless it didn't finish yet.\n     if let TaskStatus::Done { result, .. } = &task.status {\ndiff --git a/pueue/src/daemon/process_handler/finish.rs b/pueue/src/daemon/process_handler/finish.rs\nindex a124f3551..292a18a7c 100644\n--- a/pueue/src/daemon/process_handler/finish.rs\n+++ b/pueue/src/daemon/process_handler/finish.rs\n@@ -97,6 +97,8 @@ pub fn handle_finished_tasks(settings: &Settings, state: &mut LockedState) {\n             None => TaskResult::Killed,\n         };\n \n+        info!(\"Task {task_id} finished with result: {result:?}\");\n+\n         // Update the tasks's state and return a clone for callback handling.\n         let task = {\n             let task = state\n@@ -147,7 +149,6 @@ fn get_finished(state: &mut LockedState) -> Vec<((usize, String, usize), Option<\n                 // Child process did not exit yet\n                 Ok(None) => continue,\n                 Ok(_exit_status) => {\n-                    info!(\"Task {task_id} just finished\");\n                     finished.push(((*task_id, group.clone(), *worker_id), None));\n                 }\n             }\n", "test_patch": "diff --git a/pueue/tests/daemon/integration/callback.rs b/pueue/tests/daemon/integration/callback.rs\nnew file mode 100644\nindex 000000000..c5021d9ed\n--- /dev/null\n+++ b/pueue/tests/daemon/integration/callback.rs\n@@ -0,0 +1,43 @@\n+use std::fs::read_to_string;\n+\n+use anyhow::{Context, Result};\n+\n+use crate::helper::*;\n+\n+/// Make sure that callback commands are executed while variables are\n+/// templated into the command as expected.\n+#[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n+async fn test_callback_variables() -> Result<()> {\n+    let (mut settings, tempdir) = daemon_base_setup()?;\n+\n+    // Configure the daemon to use a callback command that echos some variables into a file\n+    // that's located in the temporary runtime directory of the daemon.\n+    let tempdir_path = tempdir.path().to_path_buf();\n+    let echo_command =\n+        \"echo '{{queued_count}}\\n{{stashed_count}}\\n{{command}}\\n{{id}}\\n{{result}}'\";\n+    settings.daemon.callback = Some(format!(\n+        \"{echo_command} > {}/testfile\",\n+        tempdir_path.to_string_lossy()\n+    ));\n+    settings\n+        .save(&Some(tempdir_path.join(\"pueue.yml\")))\n+        .context(\"Couldn't write pueue config to temporary directory\")?;\n+\n+    // Create the daemon with the changed settings.\n+    let daemon = daemon_with_settings(settings, tempdir).await?;\n+    let shared = &daemon.settings.shared;\n+\n+    // Create one stashed task.\n+    assert_success(create_stashed_task(shared, \"stashed\", None).await?);\n+    // Create a task that'll then trigger the callback\n+    assert_success(add_task(shared, \"ls\").await?);\n+\n+    // Give the callback command some time to be executed.\n+    sleep_ms(3000).await;\n+\n+    let callback_output = read_to_string(tempdir_path.join(\"testfile\"))?;\n+\n+    assert_eq!(callback_output, \"0\\n1\\nls\\n1\\nSuccess\\n\");\n+\n+    Ok(())\n+}\ndiff --git a/pueue/tests/daemon/integration/mod.rs b/pueue/tests/daemon/integration/mod.rs\nindex 65099ac7d..509d6b5da 100644\n--- a/pueue/tests/daemon/integration/mod.rs\n+++ b/pueue/tests/daemon/integration/mod.rs\n@@ -1,5 +1,6 @@\n mod add;\n mod aliases;\n+mod callback;\n mod clean;\n mod edit;\n mod environment_variables;\ndiff --git a/pueue/tests/daemon/integration/stashed.rs b/pueue/tests/daemon/integration/stashed.rs\nindex beee041a8..915e81d3a 100644\n--- a/pueue/tests/daemon/integration/stashed.rs\n+++ b/pueue/tests/daemon/integration/stashed.rs\n@@ -4,43 +4,22 @@ use pueue_lib::state::GroupStatus;\n use rstest::rstest;\n \n use pueue_lib::network::message::*;\n-use pueue_lib::settings::Shared;\n use pueue_lib::task::*;\n \n use crate::helper::*;\n \n-/// Helper to pause the whole daemon\n-pub async fn add_stashed_task(\n-    shared: &Shared,\n-    command: &str,\n-    stashed: bool,\n-    enqueue_at: Option<DateTime<Local>>,\n-) -> Result<Message> {\n-    let mut message = create_add_message(shared, command);\n-    message.stashed = stashed;\n-    message.enqueue_at = enqueue_at;\n-\n-    send_message(shared, message)\n-        .await\n-        .context(\"Failed to to add task message\")\n-}\n-\n /// Tasks can be stashed and scheduled for being enqueued at a specific point in time.\n ///\n /// Furthermore these stashed tasks can then be manually enqueued again.\n #[rstest]\n-#[case(true, None)]\n-#[case(true, Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n-#[case(false, Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n+#[case(None)]\n+#[case(Some(Local::now() + TimeDelta::try_minutes(2).unwrap()))]\n #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n-async fn test_enqueued_tasks(\n-    #[case] stashed: bool,\n-    #[case] enqueue_at: Option<DateTime<Local>>,\n-) -> Result<()> {\n+async fn test_enqueued_tasks(#[case] enqueue_at: Option<DateTime<Local>>) -> Result<()> {\n     let daemon = daemon().await?;\n     let shared = &daemon.settings.shared;\n \n-    assert_success(add_stashed_task(shared, \"sleep 10\", stashed, enqueue_at).await?);\n+    assert_success(create_stashed_task(shared, \"sleep 10\", enqueue_at).await?);\n \n     // The task should be added in stashed state.\n     let task = wait_for_task_condition(shared, 0, |task| task.is_stashed()).await?;\n@@ -77,10 +56,9 @@ async fn test_delayed_tasks() -> Result<()> {\n     let shared = &daemon.settings.shared;\n \n     // The task will be stashed and automatically enqueued after about 1 second.\n-    let response = add_stashed_task(\n+    let response = create_stashed_task(\n         shared,\n         \"sleep 10\",\n-        true,\n         Some(Local::now() + TimeDelta::try_seconds(1).unwrap()),\n     )\n     .await?;\ndiff --git a/pueue/tests/helper/mod.rs b/pueue/tests/helper/mod.rs\nindex bc0a96ec4..85cfa0532 100644\n--- a/pueue/tests/helper/mod.rs\n+++ b/pueue/tests/helper/mod.rs\n@@ -1,5 +1,7 @@\n //! This module contains helper functions, which are used by both, the client and daemon tests.\n+use ::log::{warn, LevelFilter};\n use anyhow::Result;\n+use simplelog::{Config, ConfigBuilder, TermLogger, TerminalMode};\n use tokio::io::{self, AsyncWriteExt};\n \n pub use pueue_lib::state::PUEUE_DEFAULT_GROUP;\n@@ -27,6 +29,31 @@ pub use wait::*;\n // Global acceptable test timeout\n const TIMEOUT: u64 = 5000;\n \n+/// Use this function to enable log output for in-runtime daemon output.\n+#[allow(dead_code)]\n+pub fn enable_logger() {\n+    let level = LevelFilter::Debug;\n+\n+    // Try to initialize the logger with the timezone set to the Local time of the machine.\n+    let mut builder = ConfigBuilder::new();\n+    let logger_config = match builder.set_time_offset_to_local() {\n+        Err(_) => {\n+            warn!(\"Failed to determine the local time of this machine. Fallback to UTC.\");\n+            Config::default()\n+        }\n+        Ok(builder) => builder.build(),\n+    };\n+\n+    // Init a terminal logger\n+    TermLogger::init(\n+        level,\n+        logger_config.clone(),\n+        TerminalMode::Stderr,\n+        simplelog::ColorChoice::Auto,\n+    )\n+    .unwrap()\n+}\n+\n /// A helper function to sleep for ms time.\n /// Only used to avoid the biolerplate of importing the same stuff all over the place.\n pub async fn sleep_ms(ms: u64) {\ndiff --git a/pueue/tests/helper/task.rs b/pueue/tests/helper/task.rs\nindex b7e877357..4cca395a9 100644\n--- a/pueue/tests/helper/task.rs\n+++ b/pueue/tests/helper/task.rs\n@@ -3,6 +3,7 @@ use std::env::vars;\n \n use anyhow::{anyhow, Context, Result};\n \n+use chrono::{DateTime, Local};\n use pueue_lib::network::message::*;\n use pueue_lib::settings::*;\n use pueue_lib::task::{Task, TaskStatus};\n@@ -27,6 +28,21 @@ pub fn create_add_message(shared: &Shared, command: &str) -> AddMessage {\n     }\n }\n \n+/// Helper to create a stashed task\n+pub async fn create_stashed_task(\n+    shared: &Shared,\n+    command: &str,\n+    enqueue_at: Option<DateTime<Local>>,\n+) -> Result<Message> {\n+    let mut message = create_add_message(shared, command);\n+    message.stashed = true;\n+    message.enqueue_at = enqueue_at;\n+\n+    send_message(shared, message)\n+        .await\n+        .context(\"Failed to to add task message\")\n+}\n+\n /// Helper to either continue the daemon or start specific tasks\n pub async fn start_tasks(shared: &Shared, tasks: TaskSelection) -> Result<Message> {\n     let message = StartMessage { tasks };\n", "problem_statement": "Run an extra command whenever the queue is empty\n### A detailed description of the feature you would like to see added.\r\n\r\nHi thanks for the tool! My (summarized) use case is like this: Submit a few deep learning jobs to my workstation, and when all are done (i.e. GPU is finally idle), send me a Slack message such that I can come and use the idle GPU. For example, I will need the idle GPU to tentatively run some new commands, debug them, and when they look good, submit it to the queue.\r\n\r\nThus, it would be great to have a feature that, when the job is empty, run an extra command to notify users.\r\n\r\nEDIT: Looks like I can use the \"Callback\" feature to handle it. Seems need to query `pueue` to know whether the queue is empty now, thus it would be great if this information could be provided as a variable.\r\n\r\nEDIT: Looks like it can be done by e.g. https://github.com/Nukesor/pueue/blob/9a402d5f895fdf489e7b8006375946080ea5950c/pueue/src/daemon/callbacks.rs#L62 add a line here, giving `state.tasks.len()`.\r\n\r\n### Explain your usecase of the requested feature\r\n\r\n(see above)\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\n", "hints_text": "(The content in the root post is edited for more details)\nI see how that can be useful.\r\n\r\nHow about, we add a simple boolean to the template variables. Something like `group_finished`?\r\nI'm against just using `state.tasks.len()` as users will probably be interested in which group just finished. This will give them more control on what should happen based on the finished group.\r\n\r\nThe `group` is already part of the variables that're available.\nYou could then do stuff like\r\n\r\n```sh\r\nif [[ \"true\" = \"{{ group_finished }}\" ]]; then\r\n  if [[ \"default\" = \"{{ group }}\" ]]; then\r\n    # Do your stuff\r\n  fi\r\nfi\r\n```\n> we add a simple boolean to the template variables. Something like group_finished\r\n\r\nThanks, that looks pretty reasonable!\nOne more question.\r\n\r\nWhen should a queue be considered finished?\r\nI feel at least when there're no more `queued`, `running` or `paused` entries. But what about stashed entries with a delay? I guess a group shouldn't be considered \"finished\" if there's still a task that'll be queued sometime in the future.\r\n\r\n`stashed` entries without a delay oh the other hand should probably be ignored?\nHmm, then maybe provide fine-grained details, such as `{{ num_queued }}`, `{{ num_stashed }}`, etc?", "created_at": "2024-12-01 14:55:51", "merge_commit_sha": "8cc78935513920d86b593e848e0807c79570af73", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test on ubuntu-latest for arm-unknown-linux-musleabihf', '.github/workflows/test.yml']", "['Test on macos-latest for aarch64-apple-darwin', '.github/workflows/test.yml']"], ["['Lint on macos-latest for aarch64-apple-darwin', '.github/workflows/lint.yml']", "['Test on ubuntu-latest for aarch64-unknown-linux-musl', '.github/workflows/test.yml']"], ["['Lint on ubuntu-latest for x86_64-unknown-linux-gnu', '.github/workflows/lint.yml']", "['Lint on macos-latest for x86_64-apple-darwin', '.github/workflows/lint.yml']"], ["['Event File', '.github/workflows/test.yml']", "['Test on ubuntu-latest for x86_64-unknown-linux-gnu', '.github/workflows/test.yml']"]]}
{"repo": "SeaQL/sea-query", "instance_id": "SeaQL__sea-query-835", "base_commit": "b91ba14f680e11a2e813880b775299a44d279c66", "patch": "diff --git a/src/backend/query_builder.rs b/src/backend/query_builder.rs\nindex 7bf29b55..ae7358b1 100644\n--- a/src/backend/query_builder.rs\n+++ b/src/backend/query_builder.rs\n@@ -777,15 +777,6 @@ pub trait QueryBuilder:\n             \"Cannot build a with query that has no common table expression!\"\n         );\n \n-        if with_clause.recursive {\n-            assert_eq!(\n-                with_clause.cte_expressions.len(),\n-                1,\n-                \"Cannot build a recursive query with more than one common table! \\\n-                A recursive with query must have a single cte inside it that has a union query of \\\n-                two queries!\"\n-            );\n-        }\n         for cte in &with_clause.cte_expressions {\n             if !cte_first {\n                 write!(sql, \", \").unwrap();\n", "test_patch": "diff --git a/tests/sqlite/query.rs b/tests/sqlite/query.rs\nindex 9a9c8fcf..48a6f90b 100644\n--- a/tests/sqlite/query.rs\n+++ b/tests/sqlite/query.rs\n@@ -1792,3 +1792,47 @@ fn sub_query_with_fn() {\n         r#\"SELECT ARRAY((SELECT * FROM \"character\"))\"#\n     );\n }\n+\n+#[test]\n+fn recursive_with_multiple_ctes() {\n+    let sub_select1 = Query::select()\n+        .column(Asterisk)\n+        .from(Char::Table)\n+        .to_owned();\n+    let sub_select1_name = SeaRc::new(Alias::new(\"sub1\"));\n+    let mut sub_select1_cte = CommonTableExpression::new();\n+    sub_select1_cte.table_name(sub_select1_name.clone());\n+    sub_select1_cte.column(SeaRc::new(Alias::new(\"a\")));\n+    sub_select1_cte.query(sub_select1);\n+    let sub_select2 = Query::select()\n+        .column(Asterisk)\n+        .from(Char::Table)\n+        .to_owned();\n+    let sub_select2_name = SeaRc::new(Alias::new(\"sub2\"));\n+    let mut sub_select2_cte = CommonTableExpression::new();\n+    sub_select2_cte.table_name(sub_select2_name.clone());\n+    sub_select2_cte.column(SeaRc::new(Alias::new(\"b\")));\n+    sub_select2_cte.query(sub_select2);\n+\n+    let mut with = WithClause::new();\n+    with.recursive(true)\n+        .cte(sub_select1_cte)\n+        .cte(sub_select2_cte);\n+\n+    let mut main_sel2 = Query::select();\n+    main_sel2\n+        .expr(Expr::col(Asterisk))\n+        .from(TableRef::Table(sub_select2_name));\n+    let mut main_sel1 = Query::select();\n+    main_sel1\n+        .expr(Expr::col(Asterisk))\n+        .from(TableRef::Table(sub_select1_name))\n+        .union(UnionType::All, main_sel2);\n+\n+    let query = with.query(main_sel1);\n+\n+    assert_eq!(\n+        query.to_string(SqliteQueryBuilder),\n+        r#\"WITH RECURSIVE \"sub1\" (\"a\") AS (SELECT * FROM \"character\") , \"sub2\" (\"b\") AS (SELECT * FROM \"character\") SELECT * FROM \"sub1\" UNION ALL SELECT * FROM \"sub2\"\"#\n+    );\n+}\n", "problem_statement": "Multiple CTEs should be allowed in recursive WithQuery\n## Description\r\n\r\nPer https://github.com/SeaQL/sea-query/blob/master/src/backend/query_builder.rs#L780 when you do a recursive with you are only allowed to have one CTE.\r\n\r\nIn sqlite at least (and I'd expect other dbs as well) you can have any number of CTEs in the with query, per https://sqlite.org/lang_with.html .  Especially see example 3.5 which has 4 recursive CTEs and one non-recursive CTE (`a`).\r\n\r\nI'm not sure what the impetus for that line was, but would dropping it be reasonable? Are there other parts of the code that rely on it having only one CTE?\nMultiple CTEs should be allowed in recursive WithQuery\n## Description\r\n\r\nPer https://github.com/SeaQL/sea-query/blob/master/src/backend/query_builder.rs#L780 when you do a recursive with you are only allowed to have one CTE.\r\n\r\nIn sqlite at least (and I'd expect other dbs as well) you can have any number of CTEs in the with query, per https://sqlite.org/lang_with.html .  Especially see example 3.5 which has 4 recursive CTEs and one non-recursive CTE (`a`).\r\n\r\nI'm not sure what the impetus for that line was, but would dropping it be reasonable? Are there other parts of the code that rely on it having only one CTE?\n", "hints_text": "\n", "created_at": "2024-11-08 17:52:09", "merge_commit_sha": "67713263e4ddc4efc3629bce2425af6a50c481db", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build `sea-query-postgres`', '.github/workflows/rust.yml']", "['PostgreSQL (14.4, sqlx_postgres)', '.github/workflows/rust.yml']"], ["['SQLite (sqlx_sqlite)', '.github/workflows/rust.yml']", "['PostgreSQL (13.7, postgres)', '.github/workflows/rust.yml']"], ["['PostgreSQL (12.11, sqlx_postgres)', '.github/workflows/rust.yml']", "['Derive Tests', '.github/workflows/rust.yml']"], ["['Clippy', '.github/workflows/rust.yml']", "['PostgreSQL (13.7, sqlx_postgres)', '.github/workflows/rust.yml']"], ["['MySQL (8, sqlx_mysql)', '.github/workflows/rust.yml']", "['PostgreSQL (14.4, postgres)', '.github/workflows/rust.yml']"]]}
{"repo": "IgnisDa/ryot", "instance_id": "IgnisDa__ryot-1092", "base_commit": "eb644f736404e88cbec99766536c5281949aa3e5", "patch": "diff --git a/crates/models/media/src/lib.rs b/crates/models/media/src/lib.rs\nindex 4555aa7067..20c7fdac3d 100644\n--- a/crates/models/media/src/lib.rs\n+++ b/crates/models/media/src/lib.rs\n@@ -1220,33 +1220,34 @@ pub struct GraphqlMediaAssets {\n pub struct GraphqlMetadataDetails {\n     pub id: String,\n     pub title: String,\n+    pub lot: MediaLot,\n     pub identifier: String,\n+    pub source: MediaSource,\n     pub is_nsfw: Option<bool>,\n     pub is_partial: Option<bool>,\n+    pub suggestions: Vec<String>,\n+    pub publish_year: Option<i32>,\n+    pub source_url: Option<String>,\n+    pub genres: Vec<GenreListItem>,\n+    pub assets: GraphqlMediaAssets,\n     pub description: Option<String>,\n-    pub original_language: Option<String>,\n+    pub publish_date: Option<NaiveDate>,\n     pub provider_rating: Option<Decimal>,\n+    pub original_language: Option<String>,\n     pub production_status: Option<String>,\n-    pub lot: MediaLot,\n-    pub source: MediaSource,\n-    pub creators: Vec<MetadataCreatorGroupedByRole>,\n+    pub group: Option<GraphqlMetadataGroup>,\n     pub watch_providers: Vec<WatchProvider>,\n-    pub genres: Vec<GenreListItem>,\n-    pub assets: GraphqlMediaAssets,\n-    pub publish_year: Option<i32>,\n-    pub publish_date: Option<NaiveDate>,\n+    pub show_specifics: Option<ShowSpecifics>,\n     pub book_specifics: Option<BookSpecifics>,\n     pub movie_specifics: Option<MovieSpecifics>,\n-    pub show_specifics: Option<ShowSpecifics>,\n-    pub video_game_specifics: Option<VideoGameSpecifics>,\n-    pub visual_novel_specifics: Option<VisualNovelSpecifics>,\n-    pub audio_book_specifics: Option<AudioBookSpecifics>,\n     pub podcast_specifics: Option<PodcastSpecifics>,\n     pub manga_specifics: Option<MangaSpecifics>,\n     pub anime_specifics: Option<AnimeSpecifics>,\n-    pub source_url: Option<String>,\n-    pub suggestions: Vec<String>,\n-    pub group: Option<GraphqlMetadataGroup>,\n+    pub creators: Vec<MetadataCreatorGroupedByRole>,\n+    pub audio_book_specifics: Option<AudioBookSpecifics>,\n+    pub video_game_specifics: Option<VideoGameSpecifics>,\n+    pub external_identifiers: Option<ExternalIdentifiers>,\n+    pub visual_novel_specifics: Option<VisualNovelSpecifics>,\n }\n \n #[derive(Debug, Serialize, Deserialize, Enum, Clone, PartialEq, Eq, Copy, Default)]\ndiff --git a/crates/services/miscellaneous/src/lib.rs b/crates/services/miscellaneous/src/lib.rs\nindex 1c736e2cef..2dab99d625 100644\n--- a/crates/services/miscellaneous/src/lib.rs\n+++ b/crates/services/miscellaneous/src/lib.rs\n@@ -507,8 +507,15 @@ ORDER BY RANDOM() LIMIT 10;\n         let watch_providers = model.watch_providers.unwrap_or_default();\n \n         let resp = GraphqlMetadataDetails {\n+            group,\n+            assets,\n+            genres,\n+            creators,\n+            source_url,\n+            suggestions,\n             id: model.id,\n             lot: model.lot,\n+            watch_providers,\n             title: model.title,\n             source: model.source,\n             is_nsfw: model.is_nsfw,\n@@ -517,25 +524,19 @@ ORDER BY RANDOM() LIMIT 10;\n             description: model.description,\n             publish_date: model.publish_date,\n             publish_year: model.publish_year,\n-            provider_rating: model.provider_rating,\n-            production_status: model.production_status,\n-            original_language: model.original_language,\n             book_specifics: model.book_specifics,\n             show_specifics: model.show_specifics,\n             movie_specifics: model.movie_specifics,\n             manga_specifics: model.manga_specifics,\n             anime_specifics: model.anime_specifics,\n+            provider_rating: model.provider_rating,\n+            production_status: model.production_status,\n+            original_language: model.original_language,\n             podcast_specifics: model.podcast_specifics,\n+            external_identifiers: model.external_identifiers,\n             video_game_specifics: model.video_game_specifics,\n             audio_book_specifics: model.audio_book_specifics,\n             visual_novel_specifics: model.visual_novel_specifics,\n-            group,\n-            assets,\n-            genres,\n-            creators,\n-            source_url,\n-            suggestions,\n-            watch_providers,\n         };\n         Ok(resp)\n     }\ndiff --git a/libs/generated/src/graphql/backend/gql.ts b/libs/generated/src/graphql/backend/gql.ts\nindex 67bf4c17d5..c789ce8269 100644\n--- a/libs/generated/src/graphql/backend/gql.ts\n+++ b/libs/generated/src/graphql/backend/gql.ts\n@@ -11,6 +11,7 @@ import { TypedDocumentNode as DocumentNode } from '@graphql-typed-document-node/\n  * 3. It does not support dead code elimination, so it will add unused operations.\n  *\n  * Therefore it is highly recommended to use the babel or swc plugin for production.\n+ * Learn more about it here: https://the-guild.dev/graphql/codegen/plugins/presets/preset-client#reducing-bundle-size\n  */\n const documents = {\n     \"mutation RegisterUser($input: RegisterUserInput!) {\\n  registerUser(input: $input) {\\n    __typename\\n    ... on RegisterError {\\n      error\\n    }\\n    ... on StringIdObject {\\n      id\\n    }\\n  }\\n}\\n\\nmutation LoginUser($input: AuthUserInput!) {\\n  loginUser(input: $input) {\\n    __typename\\n    ... on LoginError {\\n      error\\n    }\\n    ... on LoginResponse {\\n      apiKey\\n    }\\n  }\\n}\\n\\nmutation AddEntityToCollection($input: ChangeCollectionToEntityInput!) {\\n  addEntityToCollection(input: $input)\\n}\\n\\nmutation CommitMetadata($input: CommitMediaInput!) {\\n  commitMetadata(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CommitMetadataGroup($input: CommitMediaInput!) {\\n  commitMetadataGroup(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CommitPerson($input: CommitPersonInput!) {\\n  commitPerson(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CreateCustomExercise($input: ExerciseInput!) {\\n  createCustomExercise(input: $input)\\n}\\n\\nmutation UpdateCustomExercise($input: UpdateCustomExerciseInput!) {\\n  updateCustomExercise(input: $input)\\n}\\n\\nmutation UpdateUserIntegration($input: UpdateUserIntegrationInput!) {\\n  updateUserIntegration(input: $input)\\n}\\n\\nmutation CreateCustomMetadata($input: CreateCustomMetadataInput!) {\\n  createCustomMetadata(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CreateOrUpdateCollection($input: CreateOrUpdateCollectionInput!) {\\n  createOrUpdateCollection(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CreateReviewComment($input: CreateReviewCommentInput!) {\\n  createReviewComment(input: $input)\\n}\\n\\nmutation CreateUserMeasurement($input: UserMeasurementInput!) {\\n  createUserMeasurement(input: $input)\\n}\\n\\nmutation CreateUserNotificationPlatform($input: CreateUserNotificationPlatformInput!) {\\n  createUserNotificationPlatform(input: $input)\\n}\\n\\nmutation CreateUserIntegration($input: CreateUserIntegrationInput!) {\\n  createUserIntegration(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation CreateOrUpdateUserWorkout($input: UserWorkoutInput!) {\\n  createOrUpdateUserWorkout(input: $input)\\n}\\n\\nmutation CreateOrUpdateUserWorkoutTemplate($input: UserWorkoutInput!) {\\n  createOrUpdateUserWorkoutTemplate(input: $input)\\n}\\n\\nmutation DeleteCollection($collectionName: String!) {\\n  deleteCollection(collectionName: $collectionName)\\n}\\n\\nmutation DeleteReview($reviewId: String!) {\\n  deleteReview(reviewId: $reviewId)\\n}\\n\\nmutation DeleteS3Object($key: String!) {\\n  deleteS3Object(key: $key)\\n}\\n\\nmutation DeleteSeenItem($seenId: String!) {\\n  deleteSeenItem(seenId: $seenId) {\\n    id\\n  }\\n}\\n\\nmutation DeleteUser($toDeleteUserId: String!) {\\n  deleteUser(toDeleteUserId: $toDeleteUserId)\\n}\\n\\nmutation DeleteUserIntegration($integrationId: String!) {\\n  deleteUserIntegration(integrationId: $integrationId)\\n}\\n\\nmutation DeleteUserMeasurement($timestamp: DateTime!) {\\n  deleteUserMeasurement(timestamp: $timestamp)\\n}\\n\\nmutation DeleteUserNotificationPlatform($notificationId: String!) {\\n  deleteUserNotificationPlatform(notificationId: $notificationId)\\n}\\n\\nmutation DeleteUserWorkout($workoutId: String!) {\\n  deleteUserWorkout(workoutId: $workoutId)\\n}\\n\\nmutation DeleteUserWorkoutTemplate($workoutTemplateId: String!) {\\n  deleteUserWorkoutTemplate(workoutTemplateId: $workoutTemplateId)\\n}\\n\\nmutation DeployBackgroundJob($jobName: BackgroundJob!) {\\n  deployBackgroundJob(jobName: $jobName)\\n}\\n\\nmutation DeployBulkProgressUpdate($input: [ProgressUpdateInput!]!) {\\n  deployBulkProgressUpdate(input: $input)\\n}\\n\\nmutation DeployExportJob {\\n  deployExportJob\\n}\\n\\nmutation DeployImportJob($input: DeployImportJobInput!) {\\n  deployImportJob(input: $input)\\n}\\n\\nmutation DeployUpdateMetadataJob($metadataId: String!) {\\n  deployUpdateMetadataJob(metadataId: $metadataId)\\n}\\n\\nmutation DeployUpdatePersonJob($personId: String!) {\\n  deployUpdatePersonJob(personId: $personId)\\n}\\n\\nmutation DeployUpdateMetadataGroupJob($metadataGroupId: String!) {\\n  deployUpdateMetadataGroupJob(metadataGroupId: $metadataGroupId)\\n}\\n\\nmutation UpdateSeenItem($input: UpdateSeenItemInput!) {\\n  updateSeenItem(input: $input)\\n}\\n\\nmutation UpdateUserNotificationPlatform($input: UpdateUserNotificationPlatformInput!) {\\n  updateUserNotificationPlatform(input: $input)\\n}\\n\\nmutation UpdateUserWorkoutAttributes($input: UpdateUserWorkoutAttributesInput!) {\\n  updateUserWorkoutAttributes(input: $input)\\n}\\n\\nmutation GenerateAuthToken {\\n  generateAuthToken\\n}\\n\\nmutation MergeMetadata($mergeFrom: String!, $mergeInto: String!) {\\n  mergeMetadata(mergeFrom: $mergeFrom, mergeInto: $mergeInto)\\n}\\n\\nmutation DisassociateMetadata($metadataId: String!) {\\n  disassociateMetadata(metadataId: $metadataId)\\n}\\n\\nmutation CreateOrUpdateReview($input: CreateOrUpdateReviewInput!) {\\n  createOrUpdateReview(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation PresignedPutS3Url($input: PresignedPutUrlInput!) {\\n  presignedPutS3Url(input: $input) {\\n    key\\n    uploadUrl\\n  }\\n}\\n\\nmutation RemoveEntityFromCollection($input: ChangeCollectionToEntityInput!) {\\n  removeEntityFromCollection(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation TestUserNotificationPlatforms {\\n  testUserNotificationPlatforms\\n}\\n\\nmutation UpdateUser($input: UpdateUserInput!) {\\n  updateUser(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation UpdateUserPreference($input: UpdateComplexJsonInput!) {\\n  updateUserPreference(input: $input)\\n}\\n\\nmutation CreateAccessLink($input: CreateAccessLinkInput!) {\\n  createAccessLink(input: $input) {\\n    id\\n  }\\n}\\n\\nmutation ProcessAccessLink($input: ProcessAccessLinkInput!) {\\n  processAccessLink(input: $input) {\\n    __typename\\n    ... on ProcessAccessLinkError {\\n      error\\n    }\\n    ... on ProcessAccessLinkResponse {\\n      apiKey\\n      redirectTo\\n      tokenValidForDays\\n    }\\n  }\\n}\\n\\nmutation RevokeAccessLink($accessLinkId: String!) {\\n  revokeAccessLink(accessLinkId: $accessLinkId)\\n}\\n\\nmutation UpdateUserExerciseSettings($input: UpdateUserExerciseSettings!) {\\n  updateUserExerciseSettings(input: $input)\\n}\": types.RegisterUserDocument,\ndiff --git a/libs/generated/src/graphql/backend/graphql.ts b/libs/generated/src/graphql/backend/graphql.ts\nindex 7853c79a57..4247093d64 100644\n--- a/libs/generated/src/graphql/backend/graphql.ts\n+++ b/libs/generated/src/graphql/backend/graphql.ts\n@@ -641,6 +641,10 @@ export type ExportJob = {\n   url: Scalars['String']['output'];\n };\n \n+export type ExternalIdentifiers = {\n+  tvdbId?: Maybe<Scalars['Int']['output']>;\n+};\n+\n export type FrontendConfig = {\n   /** A message to be displayed on the dashboard. */\n   dashboardMessage: Scalars['String']['output'];\n@@ -711,6 +715,7 @@ export type GraphqlMetadataDetails = {\n   bookSpecifics?: Maybe<BookSpecifics>;\n   creators: Array<MetadataCreatorGroupedByRole>;\n   description?: Maybe<Scalars['String']['output']>;\n+  externalIdentifiers?: Maybe<ExternalIdentifiers>;\n   genres: Array<GenreListItem>;\n   group?: Maybe<GraphqlMetadataGroup>;\n   id: Scalars['String']['output'];\n", "test_patch": "", "problem_statement": "[Feature Request] - Add external_identifiers to GraphQL metadataDetails query.\nHi!\r\nFirst of all, thanks for the hard work, I have been trying Ryot for a couple weeks and I am absolutely loving it.\r\n\r\nI have been integrating it with a bunch of tooling and automated systems I have in my own self-hosted homelab.\r\nBut one of the issues I have been having, is the lack of `external_identifiers` in the GraphQL queries. I have to match data using the title of an Anime/Show/Movie, and thats just not very accurate.\r\n\r\nI looked pretty extensively through the schema and could not find a way to pull the field.\r\n\r\nI was wondering if you were willing to add this field to the GraphQL query `metadataDetails` or provide another query which I could use to pull that data from.\r\n\r\nI could also look into opening a PR, but I am not very well versed with Rust. \ud83d\ude05 \n", "hints_text": "", "created_at": "2024-10-31 19:23:27", "merge_commit_sha": "c399b0dac1ef3802ba376b968f492bfc879be98b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['deploy-docs', '.github/workflows/main.yml']", "['pre-workflow-checks', '.github/workflows/main.yml']"], ["['build-docker', '.github/workflows/main.yml']", "['build-transactional', '.github/workflows/main.yml']"]]}
{"repo": "IgnisDa/ryot", "instance_id": "IgnisDa__ryot-1010", "base_commit": "5eb6a1567b6e4006add271d9e73d0f0db1c5225a", "patch": "diff --git a/apps/frontend/app/lib/generals.ts b/apps/frontend/app/lib/generals.ts\nindex 9b8664d432..e1638657af 100644\n--- a/apps/frontend/app/lib/generals.ts\n+++ b/apps/frontend/app/lib/generals.ts\n@@ -60,6 +60,7 @@ export const CurrentWorkoutKey = \"CurrentWorkout\";\n export const LOGO_IMAGE_URL =\n \t\"https://raw.githubusercontent.com/IgnisDa/ryot/main/libs/assets/icon-512x512.png\";\n export const redirectToQueryParam = \"redirectTo\";\n+export const pageQueryParam = \"page\";\n export const AUTH_COOKIE_NAME = \"Auth\";\n export const toastKey = \"Toast\";\n \ndiff --git a/apps/frontend/app/lib/utilities.server.ts b/apps/frontend/app/lib/utilities.server.ts\nindex b3ef4f2f41..07cc71b048 100644\n--- a/apps/frontend/app/lib/utilities.server.ts\n+++ b/apps/frontend/app/lib/utilities.server.ts\n@@ -34,6 +34,7 @@ import {\n \tAUTH_COOKIE_NAME,\n \tCurrentWorkoutKey,\n \tdayjsLib,\n+\tpageQueryParam,\n \tqueryClient,\n \tqueryFactory,\n \tredirectToQueryParam,\n@@ -397,19 +398,34 @@ export const isWorkoutActive = (request: Request) => {\n \treturn inProgress;\n };\n \n+export const getEnhancedCookieName = async (path: string, request: Request) => {\n+\tconst userDetails = await redirectIfNotAuthenticatedOrUpdated(request);\n+\treturn `SearchParams__${userDetails.id}__${path}`;\n+};\n+\n export const redirectUsingEnhancedCookieSearchParams = async (\n \trequest: Request,\n \tcookieName: string,\n ) => {\n \tconst preferences = await getCachedUserPreferences(request);\n-\tconst searchParams = new URL(request.url).searchParams;\n+\tconst { searchParams } = new URL(request.url);\n \tif (searchParams.size > 0 || !preferences.general.persistQueries) return;\n \tconst cookies = parse(request.headers.get(\"cookie\") || \"\");\n \tconst savedSearchParams = cookies[cookieName];\n \tif (!isEmpty(savedSearchParams)) throw redirect(`?${savedSearchParams}`);\n };\n \n-export const getEnhancedCookieName = async (path: string, request: Request) => {\n-\tconst userDetails = await redirectIfNotAuthenticatedOrUpdated(request);\n-\treturn `SearchParams__${userDetails.id}__${path}`;\n+export const redirectToFirstPageIfOnInvalidPage = async (\n+\trequest: Request,\n+\ttotalResults: number,\n+\tcurrentPage: number,\n+) => {\n+\tconst { coreDetails } = await getCachedCoreDetails();\n+\tconst { searchParams } = new URL(request.url);\n+\tconst totalPages = Math.ceil(totalResults / coreDetails.pageLimit);\n+\tif (currentPage > totalPages && currentPage !== 1) {\n+\t\tsearchParams.set(pageQueryParam, \"1\");\n+\t\tthrow redirect(`?${searchParams.toString()}`);\n+\t}\n+\treturn totalPages;\n };\ndiff --git a/apps/frontend/app/routes/_dashboard.collections.$id._index.tsx b/apps/frontend/app/routes/_dashboard.collections.$id._index.tsx\nindex 7fdabc4984..b8080492f6 100644\n--- a/apps/frontend/app/routes/_dashboard.collections.$id._index.tsx\n+++ b/apps/frontend/app/routes/_dashboard.collections.$id._index.tsx\n@@ -57,17 +57,15 @@ import {\n import {\n \tclientGqlService,\n \tdayjsLib,\n+\tpageQueryParam,\n \tqueryClient,\n \tqueryFactory,\n } from \"~/lib/generals\";\n-import {\n-\tuseAppSearchParam,\n-\tuseCoreDetails,\n-\tuseUserPreferences,\n-} from \"~/lib/hooks\";\n+import { useAppSearchParam, useUserPreferences } from \"~/lib/hooks\";\n import { useReviewEntity } from \"~/lib/state/media\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tremoveCachedUserCollectionsList,\n \tserverGqlService,\n@@ -82,7 +80,7 @@ const defaultFiltersValue = {\n \n const searchParamsSchema = z.object({\n \tdefaultTab: z.string().optional(),\n-\tpage: zx.IntAsString.optional(),\n+\t[pageQueryParam]: zx.IntAsString.optional(),\n \tquery: z.string().optional(),\n \tsortBy: z\n \t\t.nativeEnum(CollectionContentsSortBy)\n@@ -111,11 +109,16 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t\tmetadataLot: query.metadataLot,\n \t\t\t\t},\n \t\t\t\tsort: { by: query.sortBy, order: query.orderBy },\n-\t\t\t\tsearch: { page: query.page, query: query.query },\n+\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t},\n \t\t}),\n \t]);\n-\treturn { collectionId, query, collectionContents, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\tcollectionContents.results.details.total,\n+\t\tquery[pageQueryParam] || 1,\n+\t);\n+\treturn { collectionId, query, collectionContents, cookieName, totalPages };\n });\n \n export const meta = ({ data }: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -160,7 +163,6 @@ const bulkRemoveSchema = z.object({\n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n \tconst userPreferences = useUserPreferences();\n-\tconst coreDetails = useCoreDetails();\n \tconst [tab, setTab] = useState<string | null>(\n \t\tloaderData.query.defaultTab || DEFAULT_TAB,\n \t);\n@@ -364,12 +366,9 @@ export default function Page() {\n \t\t\t\t\t\t\t\t<Center>\n \t\t\t\t\t\t\t\t\t<Pagination\n \t\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\t\tvalue={loaderData.query.page}\n-\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\t\tloaderData.collectionContents.results.details.total /\n-\t\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t\t)}\n+\t\t\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\t\t\tvalue={loaderData.query[pageQueryParam]}\n+\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t\t\t\t\t/>\n \t\t\t\t\t\t\t\t</Center>\n \t\t\t\t\t\t\t) : null}\ndiff --git a/apps/frontend/app/routes/_dashboard.fitness.$entity.list.tsx b/apps/frontend/app/routes/_dashboard.fitness.$entity.list.tsx\nindex 284f199783..5181dd268d 100644\n--- a/apps/frontend/app/routes/_dashboard.fitness.$entity.list.tsx\n+++ b/apps/frontend/app/routes/_dashboard.fitness.$entity.list.tsx\n@@ -41,22 +41,22 @@ import {\n \tdisplayWeightWithUnit,\n \tgetSetStatisticsTextToDisplay,\n } from \"~/components/fitness\";\n-import { dayjsLib } from \"~/lib/generals\";\n+import { dayjsLib, pageQueryParam } from \"~/lib/generals\";\n import {\n \tuseAppSearchParam,\n-\tuseCoreDetails,\n \tuseGetWorkoutStarter,\n \tuseUserUnitSystem,\n } from \"~/lib/hooks\";\n import { getDefaultWorkout } from \"~/lib/state/fitness\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n \n const searchParamsSchema = z.object({\n-\tpage: zx.IntAsString.default(\"1\"),\n+\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n \tquery: z.string().optional(),\n });\n \n@@ -76,7 +76,7 @@ export const loader = unstable_defineLoader(async ({ params, request }) => {\n \t\t\tconst { userWorkoutsList } = await serverGqlService.authenticatedRequest(\n \t\t\t\trequest,\n \t\t\t\tUserWorkoutsListDocument,\n-\t\t\t\t{ input: { page: query.page, query: query.query } },\n+\t\t\t\t{ input: { page: query[pageQueryParam], query: query.query } },\n \t\t\t);\n \t\t\treturn {\n \t\t\t\tdetails: userWorkoutsList.details,\n@@ -96,7 +96,12 @@ export const loader = unstable_defineLoader(async ({ params, request }) => {\n \t\t\t};\n \t\t})\n \t\t.exhaustive();\n-\treturn { query, entity, itemList, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\titemList.details.total,\n+\t\tquery[pageQueryParam],\n+\t);\n+\treturn { query, entity, itemList, cookieName, totalPages };\n });\n \n export const meta = ({ data }: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -105,7 +110,6 @@ export const meta = ({ data }: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \tconst startWorkout = useGetWorkoutStarter();\n \tconst unitSystem = useUserUnitSystem();\n@@ -221,11 +225,9 @@ export default function Page() {\n \t\t\t\t<Center>\n \t\t\t\t\t<Pagination\n \t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\tvalue={loaderData.query.page}\n-\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\tloaderData.itemList.details.total / coreDetails.pageLimit,\n-\t\t\t\t\t\t)}\n+\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\tvalue={loaderData.query[pageQueryParam]}\n+\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t/>\n \t\t\t\t</Center>\n \t\t\t</Stack>\ndiff --git a/apps/frontend/app/routes/_dashboard.fitness.exercises.list.tsx b/apps/frontend/app/routes/_dashboard.fitness.exercises.list.tsx\nindex 3b566e050f..fe31e47de3 100644\n--- a/apps/frontend/app/routes/_dashboard.fitness.exercises.list.tsx\n+++ b/apps/frontend/app/routes/_dashboard.fitness.exercises.list.tsx\n@@ -49,17 +49,14 @@ import { $path } from \"remix-routes\";\n import { z } from \"zod\";\n import { zx } from \"zodix\";\n import { DebouncedSearchInput, FiltersModal } from \"~/components/common\";\n-import { dayjsLib } from \"~/lib/generals\";\n-import {\n-\tuseAppSearchParam,\n-\tuseCoreDetails,\n-\tuseUserCollections,\n-} from \"~/lib/hooks\";\n+import { dayjsLib, pageQueryParam } from \"~/lib/generals\";\n+import { useAppSearchParam, useUserCollections } from \"~/lib/hooks\";\n import { addExerciseToWorkout, useCurrentWorkout } from \"~/lib/state/fitness\";\n import {\n \tgetCachedExerciseParameters,\n \tgetEnhancedCookieName,\n \tgetWorkoutCookieValue,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n@@ -76,7 +73,7 @@ const defaultFiltersValue = {\n };\n \n const searchParamsSchema = z.object({\n-\tpage: zx.IntAsString.optional(),\n+\t[pageQueryParam]: zx.IntAsString.optional(),\n \tquery: z.string().optional(),\n \tsortBy: z.nativeEnum(ExerciseSortBy).optional(),\n \ttype: z.nativeEnum(ExerciseLot).optional(),\n@@ -96,31 +93,41 @@ export const loader = unstable_defineLoader(async ({ request }) => {\n \tconst query = zx.parseQuery(request, searchParamsSchema);\n \tconst workoutInProgress = !!getWorkoutCookieValue(request);\n \tquery.sortBy = query.sortBy ?? defaultFiltersValue.sortBy;\n-\tquery.page = query.page ?? 1;\n+\tquery[pageQueryParam] = query[pageQueryParam] ?? 1;\n \tconst [exerciseParameters, { exercisesList }] = await Promise.all([\n \t\tgetCachedExerciseParameters(),\n-\t\tserverGqlService.authenticatedRequest(request, ExercisesListDocument, {\n-\t\t\tinput: {\n-\t\t\t\tsearch: { page: query.page, query: query.query },\n-\t\t\t\tfilter: {\n-\t\t\t\t\tequipment: query.equipment,\n-\t\t\t\t\tforce: query.force,\n-\t\t\t\t\tlevel: query.level,\n-\t\t\t\t\tmechanic: query.mechanic,\n-\t\t\t\t\tmuscle: query.muscle,\n-\t\t\t\t\ttype: query.type,\n-\t\t\t\t\tcollection: query.collection,\n+\t\tserverGqlService.authenticatedRequest(\n+\t\t\trequest.clone(),\n+\t\t\tExercisesListDocument,\n+\t\t\t{\n+\t\t\t\tinput: {\n+\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n+\t\t\t\t\tfilter: {\n+\t\t\t\t\t\tequipment: query.equipment,\n+\t\t\t\t\t\tforce: query.force,\n+\t\t\t\t\t\tlevel: query.level,\n+\t\t\t\t\t\tmechanic: query.mechanic,\n+\t\t\t\t\t\tmuscle: query.muscle,\n+\t\t\t\t\t\ttype: query.type,\n+\t\t\t\t\t\tcollection: query.collection,\n+\t\t\t\t\t},\n+\t\t\t\t\tsortBy: query.sortBy,\n \t\t\t\t},\n-\t\t\t\tsortBy: query.sortBy,\n \t\t\t},\n-\t\t}),\n+\t\t),\n \t]);\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\texercisesList.details.total,\n+\t\tquery[pageQueryParam],\n+\t);\n \treturn {\n \t\tquery,\n+\t\ttotalPages,\n+\t\tcookieName,\n+\t\texercisesList,\n \t\tworkoutInProgress,\n \t\texerciseParameters,\n-\t\texercisesList,\n-\t\tcookieName,\n \t};\n });\n \n@@ -130,7 +137,6 @@ export const meta = (_args: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst navigate = useNavigate();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \tconst [selectedExercises, setSelectedExercises] = useListState<{\n@@ -145,7 +151,7 @@ export default function Page() {\n \tconst [currentWorkout, setCurrentWorkout] = useCurrentWorkout();\n \n \tconst isFilterChanged = Object.keys(defaultFiltersValue)\n-\t\t.filter((k) => k !== \"page\" && k !== \"query\")\n+\t\t.filter((k) => k !== pageQueryParam && k !== \"query\")\n \t\t.some(\n \t\t\t// biome-ignore lint/suspicious/noExplicitAny: required here\n \t\t\t(k) => (loaderData.query as any)[k] !== (defaultFiltersValue as any)[k],\n@@ -287,19 +293,14 @@ export default function Page() {\n \t\t\t\t\t\t) : (\n \t\t\t\t\t\t\t<Text>No information to display</Text>\n \t\t\t\t\t\t)}\n-\t\t\t\t\t\t{loaderData.exercisesList.details.total > 0 ? (\n-\t\t\t\t\t\t\t<Center>\n-\t\t\t\t\t\t\t\t<Pagination\n-\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\tvalue={loaderData.query.page}\n-\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\tloaderData.exercisesList.details.total /\n-\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t)}\n-\t\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t\t</Center>\n-\t\t\t\t\t\t) : null}\n+\t\t\t\t\t\t<Center>\n+\t\t\t\t\t\t\t<Pagination\n+\t\t\t\t\t\t\t\tsize=\"sm\"\n+\t\t\t\t\t\t\t\tvalue={loaderData.query[pageQueryParam]}\n+\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n+\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t</Center>\n \t\t\t\t\t</>\n \t\t\t\t)}\n \t\t\t</Stack>\ndiff --git a/apps/frontend/app/routes/_dashboard.media.$action.$lot.tsx b/apps/frontend/app/routes/_dashboard.media.$action.$lot.tsx\nindex 9a16a0eb1a..b1782560b5 100644\n--- a/apps/frontend/app/routes/_dashboard.media.$action.$lot.tsx\n+++ b/apps/frontend/app/routes/_dashboard.media.$action.$lot.tsx\n@@ -60,11 +60,16 @@ import {\n \tFiltersModal,\n } from \"~/components/common\";\n import { BaseMediaDisplayItem, MetadataDisplayItem } from \"~/components/media\";\n-import { Verb, commaDelimitedString, getLot, getVerb } from \"~/lib/generals\";\n+import {\n+\tVerb,\n+\tcommaDelimitedString,\n+\tgetLot,\n+\tgetVerb,\n+\tpageQueryParam,\n+} from \"~/lib/generals\";\n import {\n \tuseAppSearchParam,\n \tuseApplicationEvents,\n-\tuseCoreDetails,\n \tuseUserDetails,\n \tuseUserPreferences,\n } from \"~/lib/hooks\";\n@@ -74,6 +79,7 @@ import {\n } from \"~/lib/state/media\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n@@ -120,12 +126,11 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\trequest,\n \t);\n \tawait redirectUsingEnhancedCookieSearchParams(request, cookieName);\n-\tconst { query, page } = zx.parseQuery(request, {\n+\tconst query = zx.parseQuery(request, {\n \t\tquery: z.string().optional(),\n-\t\tpage: zx.IntAsString.default(\"1\"),\n+\t\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n \t});\n-\tconst numPage = Number(page);\n-\tconst [mediaList, mediaSearch] = await match(action)\n+\tconst [totalResults, mediaList, mediaSearch] = await match(action)\n \t\t.with(Action.List, async () => {\n \t\t\tconst urlParse = zx.parseQuery(request, {\n \t\t\t\tsortOrder: z\n@@ -144,7 +149,7 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t{\n \t\t\t\t\tinput: {\n \t\t\t\t\t\tlot,\n-\t\t\t\t\t\tsearch: { page: numPage, query },\n+\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t\t\t\tsort: { order: urlParse.sortOrder, by: urlParse.sortBy },\n \t\t\t\t\t\tfilter: {\n \t\t\t\t\t\t\tgeneral: urlParse.generalFilter,\n@@ -154,7 +159,11 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t\t},\n \t\t\t\t},\n \t\t\t);\n-\t\t\treturn [{ list: metadataList, url: urlParse }, undefined] as const;\n+\t\t\treturn [\n+\t\t\t\tmetadataList.details.total,\n+\t\t\t\t{ list: metadataList, url: urlParse },\n+\t\t\t\tundefined,\n+\t\t\t] as const;\n \t\t})\n \t\t.with(Action.Search, async () => {\n \t\t\tconst metadataSourcesForLot = metadataMapping[lot];\n@@ -167,12 +176,13 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t{\n \t\t\t\t\tinput: {\n \t\t\t\t\t\tlot,\n-\t\t\t\t\t\tsearch: { page, query },\n+\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t\t\t\tsource: urlParse.source,\n \t\t\t\t\t},\n \t\t\t\t},\n \t\t\t);\n \t\t\treturn [\n+\t\t\t\tmetadataSearch.details.total,\n \t\t\t\tundefined,\n \t\t\t\t{\n \t\t\t\t\tsearch: metadataSearch,\n@@ -183,15 +193,21 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t})\n \t\t.exhaustive();\n \tconst url = new URL(request.url);\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\ttotalResults,\n+\t\tquery[pageQueryParam],\n+\t);\n \treturn {\n \t\tlot,\n \t\tquery,\n \t\taction,\n-\t\tnumPage,\n \t\tmediaList,\n+\t\ttotalPages,\n \t\tcookieName,\n \t\tmediaSearch,\n \t\turl: withoutHost(url.href),\n+\t\t[pageQueryParam]: Number(query[pageQueryParam]),\n \t};\n });\n \n@@ -208,7 +224,6 @@ export const meta = ({ params }: MetaArgs_SingleFetch<typeof loader>) => {\n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n \tconst userPreferences = useUserPreferences();\n-\tconst coreDetails = useCoreDetails();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \tconst [\n \t\tfiltersModalOpened,\n@@ -235,7 +250,11 @@ export default function Page() {\n \t\t\t\t\t\t\t$path(\n \t\t\t\t\t\t\t\t\"/media/:action/:lot\",\n \t\t\t\t\t\t\t\t{ action: v, lot: loaderData.lot.toLowerCase() },\n-\t\t\t\t\t\t\t\t{ ...(loaderData.query && { query: loaderData.query }) },\n+\t\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\t\t...(loaderData.query.query && {\n+\t\t\t\t\t\t\t\t\t\tquery: loaderData.query.query,\n+\t\t\t\t\t\t\t\t\t}),\n+\t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t),\n \t\t\t\t\t\t);\n \t\t\t\t}}\n@@ -265,7 +284,7 @@ export default function Page() {\n \t\t\t\t\t<>\n \t\t\t\t\t\t<Group wrap=\"nowrap\">\n \t\t\t\t\t\t\t<DebouncedSearchInput\n-\t\t\t\t\t\t\t\tinitialValue={loaderData.query}\n+\t\t\t\t\t\t\t\tinitialValue={loaderData.query.query}\n \t\t\t\t\t\t\t\tenhancedQueryParams={loaderData.cookieName}\n \t\t\t\t\t\t\t\tplaceholder={`Sift through your ${changeCase(\n \t\t\t\t\t\t\t\t\tloaderData.lot.toLowerCase(),\n@@ -310,12 +329,9 @@ export default function Page() {\n \t\t\t\t\t\t\t<Center>\n \t\t\t\t\t\t\t\t<Pagination\n \t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\tvalue={loaderData.numPage}\n-\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\tloaderData.mediaList.list.details.total /\n-\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t)}\n+\t\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t\t\t\t/>\n \t\t\t\t\t\t\t</Center>\n \t\t\t\t\t\t) : null}\n@@ -328,7 +344,7 @@ export default function Page() {\n \t\t\t\t\t\t\t\tplaceholder={`Sift through your ${changeCase(\n \t\t\t\t\t\t\t\t\tloaderData.lot.toLowerCase(),\n \t\t\t\t\t\t\t\t).toLowerCase()}s`}\n-\t\t\t\t\t\t\t\tinitialValue={loaderData.query}\n+\t\t\t\t\t\t\t\tinitialValue={loaderData.query.query}\n \t\t\t\t\t\t\t\tenhancedQueryParams={loaderData.cookieName}\n \t\t\t\t\t\t\t/>\n \t\t\t\t\t\t\t{loaderData.mediaSearch.mediaSources.length > 1 ? (\n@@ -378,12 +394,9 @@ export default function Page() {\n \t\t\t\t\t\t\t<Center>\n \t\t\t\t\t\t\t\t<Pagination\n \t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\tvalue={loaderData.numPage}\n-\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\tloaderData.mediaSearch.search.details.total /\n-\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t)}\n+\t\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t\t\t\t/>\n \t\t\t\t\t\t\t</Center>\n \t\t\t\t\t\t) : null}\ndiff --git a/apps/frontend/app/routes/_dashboard.media.genre.$id._index.tsx b/apps/frontend/app/routes/_dashboard.media.genre.$id._index.tsx\nindex 16df903475..7a66f3ccc7 100644\n--- a/apps/frontend/app/routes/_dashboard.media.genre.$id._index.tsx\n+++ b/apps/frontend/app/routes/_dashboard.media.genre.$id._index.tsx\n@@ -15,15 +15,17 @@ import { z } from \"zod\";\n import { zx } from \"zodix\";\n import { ApplicationGrid } from \"~/components/common\";\n import { MetadataDisplayItem } from \"~/components/media\";\n-import { useAppSearchParam, useCoreDetails } from \"~/lib/hooks\";\n+import { pageQueryParam } from \"~/lib/generals\";\n+import { useAppSearchParam } from \"~/lib/hooks\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n \n const searchParamsSchema = z.object({\n-\tpage: zx.IntAsString.default(\"1\"),\n+\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n });\n \n export type SearchParams = z.infer<typeof searchParamsSchema>;\n@@ -35,10 +37,15 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \tconst query = zx.parseQuery(request, searchParamsSchema);\n \tconst [{ genreDetails }] = await Promise.all([\n \t\tserverGqlService.request(GenreDetailsDocument, {\n-\t\t\tinput: { genreId, page: query.page },\n+\t\t\tinput: { genreId, page: query[pageQueryParam] },\n \t\t}),\n \t]);\n-\treturn { query, genreDetails, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\tgenreDetails.contents.details.total,\n+\t\tquery[pageQueryParam],\n+\t);\n+\treturn { query, genreDetails, cookieName, totalPages };\n });\n \n export const meta = ({ data }: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -47,7 +54,6 @@ export const meta = ({ data }: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \n \treturn (\n@@ -65,12 +71,9 @@ export default function Page() {\n \t\t\t\t<Center>\n \t\t\t\t\t<Pagination\n \t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\tvalue={loaderData.query.page}\n-\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\tloaderData.genreDetails.contents.details.total /\n-\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t)}\n+\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\tvalue={loaderData.query[pageQueryParam]}\n+\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t/>\n \t\t\t\t</Center>\n \t\t\t</Stack>\ndiff --git a/apps/frontend/app/routes/_dashboard.media.genre.list.tsx b/apps/frontend/app/routes/_dashboard.media.genre.list.tsx\nindex c0b7482b53..7293c969aa 100644\n--- a/apps/frontend/app/routes/_dashboard.media.genre.list.tsx\n+++ b/apps/frontend/app/routes/_dashboard.media.genre.list.tsx\n@@ -30,20 +30,21 @@ import {\n \tDebouncedSearchInput,\n \tProRequiredAlert,\n } from \"~/components/common\";\n+import { pageQueryParam } from \"~/lib/generals\";\n import {\n \tuseAppSearchParam,\n-\tuseCoreDetails,\n \tuseFallbackImageUrl,\n \tuseGetMantineColor,\n } from \"~/lib/hooks\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n \n const searchParamsSchema = z.object({\n-\tpage: zx.IntAsString.default(\"1\"),\n+\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n \tquery: z.string().optional(),\n });\n \n@@ -55,10 +56,15 @@ export const loader = unstable_defineLoader(async ({ request }) => {\n \tconst query = zx.parseQuery(request, searchParamsSchema);\n \tconst [{ genresList }] = await Promise.all([\n \t\tserverGqlService.request(GenresListDocument, {\n-\t\t\tinput: { page: query.page, query: query.query },\n+\t\t\tinput: { page: query[pageQueryParam], query: query.query },\n \t\t}),\n \t]);\n-\treturn { query, genresList, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\tgenresList.details.total,\n+\t\tquery[pageQueryParam],\n+\t);\n+\treturn { query, genresList, cookieName, totalPages };\n });\n \n export const meta = (_args: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -67,7 +73,6 @@ export const meta = (_args: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \n \treturn (\n@@ -102,11 +107,9 @@ export default function Page() {\n \t\t\t\t\t<Center mt=\"xl\">\n \t\t\t\t\t\t<Pagination\n \t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\tvalue={loaderData.query.page}\n-\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\tloaderData.genresList.details.total / coreDetails.pageLimit,\n-\t\t\t\t\t\t\t)}\n+\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\tvalue={loaderData.query[pageQueryParam]}\n+\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n \t\t\t\t\t\t/>\n \t\t\t\t\t</Center>\n \t\t\t\t) : null}\ndiff --git a/apps/frontend/app/routes/_dashboard.media.groups.$action.tsx b/apps/frontend/app/routes/_dashboard.media.groups.$action.tsx\nindex 3a3da4d16b..b9110afbf1 100644\n--- a/apps/frontend/app/routes/_dashboard.media.groups.$action.tsx\n+++ b/apps/frontend/app/routes/_dashboard.media.groups.$action.tsx\n@@ -54,10 +54,11 @@ import {\n \tBaseMediaDisplayItem,\n \tMetadataGroupDisplayItem,\n } from \"~/components/media\";\n-import { commaDelimitedString } from \"~/lib/generals\";\n-import { useAppSearchParam, useCoreDetails } from \"~/lib/hooks\";\n+import { commaDelimitedString, pageQueryParam } from \"~/lib/generals\";\n+import { useAppSearchParam } from \"~/lib/hooks\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n \n@@ -83,11 +84,11 @@ const SEARCH_SOURCES_ALLOWED: Partial<Record<MediaSource, MediaLot>> = {\n export const loader = unstable_defineLoader(async ({ request, params }) => {\n \tconst { action } = zx.parseParams(params, { action: z.nativeEnum(Action) });\n \tconst cookieName = await getEnhancedCookieName(`groups.${action}`, request);\n-\tconst { query, page } = zx.parseQuery(request, {\n+\tconst query = zx.parseQuery(request, {\n \t\tquery: z.string().optional(),\n-\t\tpage: zx.IntAsString.default(\"1\"),\n+\t\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n \t});\n-\tconst [list, search] = await match(action)\n+\tconst [totalResults, list, search] = await match(action)\n \t\t.with(Action.List, async () => {\n \t\t\tconst urlParse = zx.parseQuery(request, {\n \t\t\t\tsortBy: z.nativeEnum(PersonSortBy).default(defaultFilters.sortBy),\n@@ -101,14 +102,18 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t\tMetadataGroupsListDocument,\n \t\t\t\t\t{\n \t\t\t\t\t\tinput: {\n-\t\t\t\t\t\t\tsearch: { page, query },\n+\t\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t\t\t\t\tsort: { by: urlParse.sortBy, order: urlParse.orderBy },\n \t\t\t\t\t\t\tfilter: { collections: urlParse.collections },\n \t\t\t\t\t\t\tinvertCollection: urlParse.invertCollection,\n \t\t\t\t\t\t},\n \t\t\t\t\t},\n \t\t\t\t);\n-\t\t\treturn [{ list: metadataGroupsList, url: urlParse }, undefined] as const;\n+\t\t\treturn [\n+\t\t\t\tmetadataGroupsList.details.total,\n+\t\t\t\t{ list: metadataGroupsList, url: urlParse },\n+\t\t\t\tundefined,\n+\t\t\t] as const;\n \t\t})\n \t\t.with(Action.Search, async () => {\n \t\t\tconst urlParse = zx.parseQuery(request, {\n@@ -120,15 +125,35 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\tawait serverGqlService.authenticatedRequest(\n \t\t\t\t\trequest,\n \t\t\t\t\tMetadataGroupSearchDocument,\n-\t\t\t\t\t{ input: { lot, source: urlParse.source, search: { page, query } } },\n+\t\t\t\t\t{\n+\t\t\t\t\t\tinput: {\n+\t\t\t\t\t\t\tlot,\n+\t\t\t\t\t\t\tsource: urlParse.source,\n+\t\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n \t\t\t\t);\n \t\t\treturn [\n+\t\t\t\tmetadataGroupSearch.details.total,\n \t\t\t\tundefined,\n \t\t\t\t{ search: metadataGroupSearch, url: urlParse, lot },\n \t\t\t] as const;\n \t\t})\n \t\t.exhaustive();\n-\treturn { action, query, page, list, search, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\ttotalResults,\n+\t\tquery[pageQueryParam],\n+\t);\n+\treturn {\n+\t\tlist,\n+\t\tquery,\n+\t\taction,\n+\t\tsearch,\n+\t\ttotalPages,\n+\t\tcookieName,\n+\t\t[pageQueryParam]: query[pageQueryParam],\n+\t};\n });\n \n export const meta = ({ params }: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -137,7 +162,6 @@ export const meta = ({ params }: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst [_, { setP }] = useAppSearchParam(loaderData.cookieName);\n \tconst navigate = useNavigate();\n \tconst [\n@@ -158,7 +182,11 @@ export default function Page() {\n \t\t\t\t\t\t\t\t$path(\n \t\t\t\t\t\t\t\t\t\"/media/groups/:action\",\n \t\t\t\t\t\t\t\t\t{ action: v },\n-\t\t\t\t\t\t\t\t\t{ query: loaderData.query },\n+\t\t\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\t\t\t...(loaderData.query.query && {\n+\t\t\t\t\t\t\t\t\t\t\tquery: loaderData.query.query,\n+\t\t\t\t\t\t\t\t\t\t}),\n+\t\t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t\t),\n \t\t\t\t\t\t\t);\n \t\t\t\t\t}}\n@@ -176,7 +204,7 @@ export default function Page() {\n \t\t\t\t<Group wrap=\"nowrap\">\n \t\t\t\t\t<DebouncedSearchInput\n \t\t\t\t\t\tplaceholder=\"Search for groups\"\n-\t\t\t\t\t\tinitialValue={loaderData.query}\n+\t\t\t\t\t\tinitialValue={loaderData.query.query}\n \t\t\t\t\t\tenhancedQueryParams={loaderData.cookieName}\n \t\t\t\t\t/>\n \t\t\t\t\t{loaderData.action === Action.List ? (\n@@ -225,27 +253,22 @@ export default function Page() {\n \t\t\t\t\t\t\titems found\n \t\t\t\t\t\t</Box>\n \t\t\t\t\t\t{loaderData.list.list.details.total > 0 ? (\n-\t\t\t\t\t\t\t<>\n-\t\t\t\t\t\t\t\t<ApplicationGrid>\n-\t\t\t\t\t\t\t\t\t{loaderData.list.list.items.map((gr) => (\n-\t\t\t\t\t\t\t\t\t\t<MetadataGroupDisplayItem key={gr} metadataGroupId={gr} />\n-\t\t\t\t\t\t\t\t\t))}\n-\t\t\t\t\t\t\t\t</ApplicationGrid>\n-\t\t\t\t\t\t\t\t<Center>\n-\t\t\t\t\t\t\t\t\t<Pagination\n-\t\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\t\tvalue={loaderData.page}\n-\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\t\tloaderData.list.list.details.total /\n-\t\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t\t)}\n-\t\t\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t\t\t</Center>\n-\t\t\t\t\t\t\t</>\n+\t\t\t\t\t\t\t<ApplicationGrid>\n+\t\t\t\t\t\t\t\t{loaderData.list.list.items.map((gr) => (\n+\t\t\t\t\t\t\t\t\t<MetadataGroupDisplayItem key={gr} metadataGroupId={gr} />\n+\t\t\t\t\t\t\t\t))}\n+\t\t\t\t\t\t\t</ApplicationGrid>\n \t\t\t\t\t\t) : (\n \t\t\t\t\t\t\t<Text>No information to display</Text>\n \t\t\t\t\t\t)}\n+\t\t\t\t\t\t<Center>\n+\t\t\t\t\t\t\t<Pagination\n+\t\t\t\t\t\t\t\tsize=\"sm\"\n+\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n+\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t</Center>\n \t\t\t\t\t</>\n \t\t\t\t) : null}\n \n@@ -258,27 +281,22 @@ export default function Page() {\n \t\t\t\t\t\t\titems found\n \t\t\t\t\t\t</Box>\n \t\t\t\t\t\t{loaderData.search.search.details.total > 0 ? (\n-\t\t\t\t\t\t\t<>\n-\t\t\t\t\t\t\t\t<ApplicationGrid>\n-\t\t\t\t\t\t\t\t\t{loaderData.search.search.items.map((group) => (\n-\t\t\t\t\t\t\t\t\t\t<GroupSearchItem item={group} key={group.identifier} />\n-\t\t\t\t\t\t\t\t\t))}\n-\t\t\t\t\t\t\t\t</ApplicationGrid>\n-\t\t\t\t\t\t\t\t<Center>\n-\t\t\t\t\t\t\t\t\t<Pagination\n-\t\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\t\tvalue={loaderData.page}\n-\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\t\tloaderData.search.search.details.total /\n-\t\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t\t)}\n-\t\t\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t\t\t</Center>\n-\t\t\t\t\t\t\t</>\n+\t\t\t\t\t\t\t<ApplicationGrid>\n+\t\t\t\t\t\t\t\t{loaderData.search.search.items.map((group) => (\n+\t\t\t\t\t\t\t\t\t<GroupSearchItem item={group} key={group.identifier} />\n+\t\t\t\t\t\t\t\t))}\n+\t\t\t\t\t\t\t</ApplicationGrid>\n \t\t\t\t\t\t) : (\n \t\t\t\t\t\t\t<Text>No groups found matching your query</Text>\n \t\t\t\t\t\t)}\n+\t\t\t\t\t\t<Center>\n+\t\t\t\t\t\t\t<Pagination\n+\t\t\t\t\t\t\t\tsize=\"sm\"\n+\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n+\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t</Center>\n \t\t\t\t\t</>\n \t\t\t\t) : null}\n \t\t\t</Stack>\ndiff --git a/apps/frontend/app/routes/_dashboard.media.people.$action.tsx b/apps/frontend/app/routes/_dashboard.media.people.$action.tsx\nindex 3677470d51..774cc376bc 100644\n--- a/apps/frontend/app/routes/_dashboard.media.people.$action.tsx\n+++ b/apps/frontend/app/routes/_dashboard.media.people.$action.tsx\n@@ -49,10 +49,11 @@ import {\n \tFiltersModal,\n } from \"~/components/common\";\n import { BaseMediaDisplayItem, PersonDisplayItem } from \"~/components/media\";\n-import { commaDelimitedString } from \"~/lib/generals\";\n-import { useAppSearchParam, useCoreDetails } from \"~/lib/hooks\";\n+import { commaDelimitedString, pageQueryParam } from \"~/lib/generals\";\n+import { useAppSearchParam } from \"~/lib/hooks\";\n import {\n \tgetEnhancedCookieName,\n+\tredirectToFirstPageIfOnInvalidPage,\n \tredirectUsingEnhancedCookieSearchParams,\n \tserverGqlService,\n } from \"~/lib/utilities.server\";\n@@ -85,11 +86,11 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \tconst { action } = zx.parseParams(params, { action: z.nativeEnum(Action) });\n \tconst cookieName = await getEnhancedCookieName(`people.${action}`, request);\n \tawait redirectUsingEnhancedCookieSearchParams(request, cookieName);\n-\tconst { query, page } = zx.parseQuery(request, {\n+\tconst query = zx.parseQuery(request, {\n \t\tquery: z.string().optional(),\n-\t\tpage: zx.IntAsString.default(\"1\"),\n+\t\t[pageQueryParam]: zx.IntAsString.default(\"1\"),\n \t});\n-\tconst [peopleList, peopleSearch] = await match(action)\n+\tconst [totalResults, peopleList, peopleSearch] = await match(action)\n \t\t.with(Action.List, async () => {\n \t\t\tconst urlParse = zx.parseQuery(request, {\n \t\t\t\tsortBy: z.nativeEnum(PersonSortBy).default(defaultFilters.sortBy),\n@@ -102,14 +103,18 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\tPeopleListDocument,\n \t\t\t\t{\n \t\t\t\t\tinput: {\n-\t\t\t\t\t\tsearch: { page, query },\n-\t\t\t\t\t\tsort: { by: urlParse.sortBy, order: urlParse.orderBy },\n-\t\t\t\t\t\tfilter: { collections: urlParse.collections },\n \t\t\t\t\t\tinvertCollection: urlParse.invertCollection,\n+\t\t\t\t\t\tfilter: { collections: urlParse.collections },\n+\t\t\t\t\t\tsort: { by: urlParse.sortBy, order: urlParse.orderBy },\n+\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t\t\t},\n \t\t\t\t},\n \t\t\t);\n-\t\t\treturn [{ list: peopleList, url: urlParse }, undefined] as const;\n+\t\t\treturn [\n+\t\t\t\tpeopleList.details.total,\n+\t\t\t\t{ list: peopleList, url: urlParse },\n+\t\t\t\tundefined,\n+\t\t\t] as const;\n \t\t})\n \t\t.with(Action.Search, async () => {\n \t\t\tconst urlParse = zx.parseQuery(request, {\n@@ -123,18 +128,35 @@ export const loader = unstable_defineLoader(async ({ request, params }) => {\n \t\t\t\t{\n \t\t\t\t\tinput: {\n \t\t\t\t\t\tsource: urlParse.source,\n-\t\t\t\t\t\tsearch: { page, query },\n \t\t\t\t\t\tsourceSpecifics: {\n \t\t\t\t\t\t\tisAnilistStudio: urlParse.isAnilistStudio,\n \t\t\t\t\t\t\tisTmdbCompany: urlParse.isTmdbCompany,\n \t\t\t\t\t\t},\n+\t\t\t\t\t\tsearch: { page: query[pageQueryParam], query: query.query },\n \t\t\t\t\t},\n \t\t\t\t},\n \t\t\t);\n-\t\t\treturn [undefined, { search: peopleSearch, url: urlParse }] as const;\n+\t\t\treturn [\n+\t\t\t\tpeopleSearch.details.total,\n+\t\t\t\tundefined,\n+\t\t\t\t{ search: peopleSearch, url: urlParse },\n+\t\t\t] as const;\n \t\t})\n \t\t.exhaustive();\n-\treturn { action, query, page, peopleList, peopleSearch, cookieName };\n+\tconst totalPages = await redirectToFirstPageIfOnInvalidPage(\n+\t\trequest,\n+\t\ttotalResults,\n+\t\tquery[pageQueryParam],\n+\t);\n+\treturn {\n+\t\tquery,\n+\t\taction,\n+\t\tpeopleList,\n+\t\ttotalPages,\n+\t\tcookieName,\n+\t\tpeopleSearch,\n+\t\t[pageQueryParam]: query[pageQueryParam],\n+\t};\n });\n \n export const meta = ({ params }: MetaArgs_SingleFetch<typeof loader>) => {\n@@ -143,7 +165,6 @@ export const meta = ({ params }: MetaArgs_SingleFetch<typeof loader>) => {\n \n export default function Page() {\n \tconst loaderData = useLoaderData<typeof loader>();\n-\tconst coreDetails = useCoreDetails();\n \tconst navigate = useNavigate();\n \tconst [_e, { setP }] = useAppSearchParam(loaderData.cookieName);\n \tconst [\n@@ -164,7 +185,11 @@ export default function Page() {\n \t\t\t\t\t\t\t\t$path(\n \t\t\t\t\t\t\t\t\t\"/media/people/:action\",\n \t\t\t\t\t\t\t\t\t{ action: v },\n-\t\t\t\t\t\t\t\t\t{ query: loaderData.query },\n+\t\t\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\t\t\t...(loaderData.query.query && {\n+\t\t\t\t\t\t\t\t\t\t\tquery: loaderData.query.query,\n+\t\t\t\t\t\t\t\t\t\t}),\n+\t\t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t\t),\n \t\t\t\t\t\t\t);\n \t\t\t\t\t}}\n@@ -182,7 +207,7 @@ export default function Page() {\n \t\t\t\t<Group wrap=\"nowrap\">\n \t\t\t\t\t<DebouncedSearchInput\n \t\t\t\t\t\tplaceholder=\"Search for people\"\n-\t\t\t\t\t\tinitialValue={loaderData.query}\n+\t\t\t\t\t\tinitialValue={loaderData.query.query}\n \t\t\t\t\t\tenhancedQueryParams={loaderData.cookieName}\n \t\t\t\t\t/>\n \t\t\t\t\t{loaderData.action === Action.List ? (\n@@ -248,27 +273,22 @@ export default function Page() {\n \t\t\t\t\t\t\titems found\n \t\t\t\t\t\t</Box>\n \t\t\t\t\t\t{loaderData.peopleList.list.details.total > 0 ? (\n-\t\t\t\t\t\t\t<>\n-\t\t\t\t\t\t\t\t<ApplicationGrid>\n-\t\t\t\t\t\t\t\t\t{loaderData.peopleList?.list.items.map((person) => (\n-\t\t\t\t\t\t\t\t\t\t<PersonDisplayItem key={person} personId={person} />\n-\t\t\t\t\t\t\t\t\t))}\n-\t\t\t\t\t\t\t\t</ApplicationGrid>\n-\t\t\t\t\t\t\t\t<Center>\n-\t\t\t\t\t\t\t\t\t<Pagination\n-\t\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\t\tvalue={loaderData.page}\n-\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\t\tloaderData.peopleList.list.details.total /\n-\t\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t\t)}\n-\t\t\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t\t\t</Center>\n-\t\t\t\t\t\t\t</>\n+\t\t\t\t\t\t\t<ApplicationGrid>\n+\t\t\t\t\t\t\t\t{loaderData.peopleList.list.items.map((person) => (\n+\t\t\t\t\t\t\t\t\t<PersonDisplayItem key={person} personId={person} />\n+\t\t\t\t\t\t\t\t))}\n+\t\t\t\t\t\t\t</ApplicationGrid>\n \t\t\t\t\t\t) : (\n \t\t\t\t\t\t\t<Text>No information to display</Text>\n \t\t\t\t\t\t)}\n+\t\t\t\t\t\t<Center>\n+\t\t\t\t\t\t\t<Pagination\n+\t\t\t\t\t\t\t\tsize=\"sm\"\n+\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n+\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t</Center>\n \t\t\t\t\t</>\n \t\t\t\t) : null}\n \t\t\t\t{loaderData.peopleSearch ? (\n@@ -280,27 +300,22 @@ export default function Page() {\n \t\t\t\t\t\t\titems found\n \t\t\t\t\t\t</Box>\n \t\t\t\t\t\t{loaderData.peopleSearch.search.details.total > 0 ? (\n-\t\t\t\t\t\t\t<>\n-\t\t\t\t\t\t\t\t<ApplicationGrid>\n-\t\t\t\t\t\t\t\t\t{loaderData.peopleSearch.search.items.map((person) => (\n-\t\t\t\t\t\t\t\t\t\t<PersonSearchItem item={person} key={person.identifier} />\n-\t\t\t\t\t\t\t\t\t))}\n-\t\t\t\t\t\t\t\t</ApplicationGrid>\n-\t\t\t\t\t\t\t\t<Center>\n-\t\t\t\t\t\t\t\t\t<Pagination\n-\t\t\t\t\t\t\t\t\t\tsize=\"sm\"\n-\t\t\t\t\t\t\t\t\t\tvalue={loaderData.page}\n-\t\t\t\t\t\t\t\t\t\tonChange={(v) => setP(\"page\", v.toString())}\n-\t\t\t\t\t\t\t\t\t\ttotal={Math.ceil(\n-\t\t\t\t\t\t\t\t\t\t\tloaderData.peopleSearch.search.details.total /\n-\t\t\t\t\t\t\t\t\t\t\t\tcoreDetails.pageLimit,\n-\t\t\t\t\t\t\t\t\t\t)}\n-\t\t\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t\t\t</Center>\n-\t\t\t\t\t\t\t</>\n+\t\t\t\t\t\t\t<ApplicationGrid>\n+\t\t\t\t\t\t\t\t{loaderData.peopleSearch.search.items.map((person) => (\n+\t\t\t\t\t\t\t\t\t<PersonSearchItem item={person} key={person.identifier} />\n+\t\t\t\t\t\t\t\t))}\n+\t\t\t\t\t\t\t</ApplicationGrid>\n \t\t\t\t\t\t) : (\n \t\t\t\t\t\t\t<Text>No people found matching your query</Text>\n \t\t\t\t\t\t)}\n+\t\t\t\t\t\t<Center>\n+\t\t\t\t\t\t\t<Pagination\n+\t\t\t\t\t\t\t\tsize=\"sm\"\n+\t\t\t\t\t\t\t\ttotal={loaderData.totalPages}\n+\t\t\t\t\t\t\t\tvalue={loaderData[pageQueryParam]}\n+\t\t\t\t\t\t\t\tonChange={(v) => setP(pageQueryParam, v.toString())}\n+\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t</Center>\n \t\t\t\t\t</>\n \t\t\t\t) : null}\n \t\t\t</Stack>\ndiff --git a/apps/frontend/app/routes/actions.tsx b/apps/frontend/app/routes/actions.tsx\nindex 4938fd14a5..a19df9178f 100644\n--- a/apps/frontend/app/routes/actions.tsx\n+++ b/apps/frontend/app/routes/actions.tsx\n@@ -53,8 +53,8 @@ export const loader = async () => redirect($path(\"/\"));\n \n export const action = unstable_defineAction(async ({ request }) => {\n \tconst formData = await request.clone().formData();\n-\tconst url = new URL(request.url);\n-\tconst intent = url.searchParams.get(\"intent\") as string;\n+\tconst { searchParams } = new URL(request.url);\n+\tconst intent = searchParams.get(\"intent\") as string;\n \tinvariant(intent);\n \tconst redirectToForm = formData.get(redirectToQueryParam);\n \tlet redirectTo = redirectToForm ? redirectToForm.toString() : undefined;\n", "test_patch": "", "problem_statement": "Exercise Page: Exercises don't show if already on page not populatable by search results\nSay if you're on page 20 and search for dumbell, there will be zero results displayed as the total results for dumbell are 4 pages and search seems to remain at page 20.\r\nPressing on available page numbers (1,2,3,4) will give you the results you desire.\r\n\r\n**Expectation**\r\nAutomatically reset to page 1 upon searching and display available items.\n", "hints_text": "", "created_at": "2024-09-05 01:17:26", "merge_commit_sha": "cd44a9856849a69e8cc4adc6aa71baa9fa24903b", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['deploy-docs', '.github/workflows/main.yml']", "['pre-workflow-checks', '.github/workflows/main.yml']"], ["['build-docker', '.github/workflows/main.yml']", "['build-transactional', '.github/workflows/main.yml']"]]}
{"repo": "Shopify/bluejay", "instance_id": "Shopify__bluejay-34", "base_commit": "536a324711e11949e7da67109f97f0b6bf7f9bd6", "patch": "diff --git a/bluejay-parser/src/ast/definition/enum_value_definition.rs b/bluejay-parser/src/ast/definition/enum_value_definition.rs\nindex a9ef7b7..761f933 100644\n--- a/bluejay-parser/src/ast/definition/enum_value_definition.rs\n+++ b/bluejay-parser/src/ast/definition/enum_value_definition.rs\n@@ -1,8 +1,11 @@\n-use crate::ast::{\n-    definition::{Context, Directives},\n-    ConstDirectives, FromTokens, ParseError, Tokens, TryFromTokens,\n-};\n use crate::lexical_token::{Name, StringValue};\n+use crate::{\n+    ast::{\n+        definition::{Context, Directives},\n+        ConstDirectives, FromTokens, ParseError, Tokens, TryFromTokens,\n+    },\n+    HasSpan,\n+};\n use bluejay_core::definition::{EnumValueDefinition as CoreEnumValueDefinition, HasDirectives};\n \n #[derive(Debug)]\n@@ -32,6 +35,13 @@ impl<'a, C: Context> FromTokens<'a> for EnumValueDefinition<'a, C> {\n     fn from_tokens(tokens: &mut impl Tokens<'a>) -> Result<Self, ParseError> {\n         let description = tokens.next_if_string_value();\n         let name = tokens.expect_name()?;\n+        if matches!(name.as_str(), \"null\" | \"true\" | \"false\") {\n+            return Err(ParseError::InvalidEnumValue {\n+                span: name.span().clone(),\n+                value: name.as_str().to_string(),\n+            });\n+        }\n+\n         let directives = ConstDirectives::try_from_tokens(tokens).transpose()?;\n         Ok(Self {\n             description,\ndiff --git a/bluejay-parser/src/ast/parse_error.rs b/bluejay-parser/src/ast/parse_error.rs\nindex dd826cc..880142f 100644\n--- a/bluejay-parser/src/ast/parse_error.rs\n+++ b/bluejay-parser/src/ast/parse_error.rs\n@@ -3,6 +3,10 @@ use crate::Span;\n \n #[derive(Debug)]\n pub enum ParseError {\n+    InvalidEnumValue {\n+        span: Span,\n+        value: String,\n+    },\n     ExpectedOneOf {\n         span: Span,\n         values: &'static [&'static str],\n@@ -26,6 +30,14 @@ pub enum ParseError {\n impl From<ParseError> for Error {\n     fn from(val: ParseError) -> Self {\n         match val {\n+            ParseError::InvalidEnumValue { span, value } => Self::new(\n+                \"Parse error\",\n+                Some(Annotation::new(\n+                    format!(\"{value} is not an allowed enum value\"),\n+                    span,\n+                )),\n+                Vec::new(),\n+            ),\n             ParseError::ExpectedOneOf { span, values } => Self::new(\n                 \"Parse error\",\n                 Some(Annotation::new(\n", "test_patch": "diff --git a/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap b/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap\nnew file mode 100644\nindex 0000000..294fe82\n--- /dev/null\n+++ b/bluejay-parser/tests/snapshots/schema_definition_integration_test__error@reserved_keyword_enum_value.graphql.snap\n@@ -0,0 +1,12 @@\n+---\n+source: bluejay-parser/tests/schema_definition_integration_test.rs\n+expression: formatted_errors\n+input_file: bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\n+---\n+Error: Parse error\n+   \u256d\u2500[<unknown>:2:3]\n+   \u2502\n+ 2 \u2502   true\n+   \u2502   \u2500\u2500\u252c\u2500  \n+   \u2502     \u2570\u2500\u2500\u2500 true is not an allowed enum value\n+\u2500\u2500\u2500\u256f\ndiff --git a/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql b/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\nnew file mode 100644\nindex 0000000..ede8c0c\n--- /dev/null\n+++ b/bluejay-parser/tests/test_data/schema_definition/error/reserved_keyword_enum_value.graphql\n@@ -0,0 +1,7 @@\n+enum Test {\n+  true\n+}\n+\n+schema {\n+  query: Test\n+}\n", "problem_statement": "Parse error for enum value definitions `true`, `false`, and `null`\nPer the [spec](https://spec.graphql.org/draft/#sel-HAFdTDCAEAAAAEBCAE8oE) enum values must be:\r\n> _Name_ but not `true` or `false` or `null`\r\n\r\nWe should return a parse error when parsing enum value definitions where the name is invalid\n", "hints_text": "", "created_at": "2024-06-28 14:58:46", "merge_commit_sha": "c823823cb1819a4c40af2f05dce83bb68bfe84d7", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['lint', '.github/workflows/rust.yml']", "['test', '.github/workflows/rust.yml']"]]}
{"repo": "bytecodealliance/rustix", "instance_id": "bytecodealliance__rustix-1335", "base_commit": "cc673a10b6a44acb9800599e6cbfa072011f85c9", "patch": "diff --git a/CHANGES.md b/CHANGES.md\nindex 8df8801cf..c8a1de780 100644\n--- a/CHANGES.md\n+++ b/CHANGES.md\n@@ -223,5 +223,12 @@ specific socket types using `From`/`Into`/`TryFrom`/`TryInto` conversions.\n \n [`SocketAddrAny`]: https://docs.rs/rustix/1.0.0/rustix/net/struct.SocketAddrAny.html\n \n+The `len` parameter to [`rustix::fs::fadvise`] has changed from `u64` to\n+`Option<NonZeroU64>`, to reflect that zero is a special case meaning the\n+advice applies to the end of the file. To convert an arbitrary `u64` value to\n+`Option<NonZeroU64>`, use `NonZeroU64::new`.\n+\n+[`rustix::fs::fadvise`]: https://docs.rs/rustix/1.0.0/rustix/fs/fn.fadvise.html\n+\n All explicitly deprecated functions and types have been removed. Their\n deprecation messages will have identified alternatives.\ndiff --git a/src/backend/libc/fs/syscalls.rs b/src/backend/libc/fs/syscalls.rs\nindex 468b9dfed..74108a7bb 100644\n--- a/src/backend/libc/fs/syscalls.rs\n+++ b/src/backend/libc/fs/syscalls.rs\n@@ -10,17 +10,6 @@ use crate::ffi::CString;\n use crate::ffi::{self, CStr};\n #[cfg(not(any(target_os = \"espidf\", target_os = \"vita\")))]\n use crate::fs::Access;\n-#[cfg(not(any(\n-    apple,\n-    netbsdlike,\n-    target_os = \"solaris\",\n-    target_os = \"dragonfly\",\n-    target_os = \"espidf\",\n-    target_os = \"haiku\",\n-    target_os = \"redox\",\n-    target_os = \"vita\",\n-)))]\n-use crate::fs::Advice;\n #[cfg(not(any(target_os = \"espidf\", target_os = \"redox\")))]\n use crate::fs::AtFlags;\n #[cfg(not(any(\n@@ -75,6 +64,17 @@ use {\n     crate::backend::conv::nonnegative_ret,\n     crate::fs::{copyfile_state_t, CloneFlags, CopyfileFlags},\n };\n+#[cfg(not(any(\n+    apple,\n+    netbsdlike,\n+    target_os = \"solaris\",\n+    target_os = \"dragonfly\",\n+    target_os = \"espidf\",\n+    target_os = \"haiku\",\n+    target_os = \"redox\",\n+    target_os = \"vita\",\n+)))]\n+use {crate::fs::Advice, core::num::NonZeroU64};\n #[cfg(any(apple, linux_kernel, target_os = \"hurd\"))]\n use {crate::fs::XattrFlags, core::mem::size_of, core::ptr::null_mut};\n #[cfg(linux_kernel)]\n@@ -1213,9 +1213,17 @@ pub(crate) fn copy_file_range(\n     target_os = \"redox\",\n     target_os = \"vita\",\n )))]\n-pub(crate) fn fadvise(fd: BorrowedFd<'_>, offset: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub(crate) fn fadvise(\n+    fd: BorrowedFd<'_>,\n+    offset: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n     let offset = offset as i64;\n-    let len = len as i64;\n+    let len = match len {\n+        None => 0,\n+        Some(len) => len.get() as i64,\n+    };\n \n     // Our public API uses `u64` following the [Rust convention], but the\n     // underlying host APIs use a signed `off_t`. Converting these values may\ndiff --git a/src/backend/linux_raw/fs/syscalls.rs b/src/backend/linux_raw/fs/syscalls.rs\nindex 882a47724..7dd19ab09 100644\n--- a/src/backend/linux_raw/fs/syscalls.rs\n+++ b/src/backend/linux_raw/fs/syscalls.rs\n@@ -41,6 +41,7 @@ use crate::fs::{\n };\n use crate::io;\n use core::mem::MaybeUninit;\n+use core::num::NonZeroU64;\n #[cfg(any(target_arch = \"mips64\", target_arch = \"mips64r6\"))]\n use linux_raw_sys::general::stat as linux_stat64;\n use linux_raw_sys::general::{\n@@ -364,7 +365,17 @@ pub(crate) fn fallocate(\n }\n \n #[inline]\n-pub(crate) fn fadvise(fd: BorrowedFd<'_>, pos: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub(crate) fn fadvise(\n+    fd: BorrowedFd<'_>,\n+    pos: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n+    let len = match len {\n+        None => 0,\n+        Some(len) => len.get(),\n+    };\n+\n     // On ARM, the arguments are reordered so that the `len` and `pos` argument\n     // pairs are aligned. And ARM has a custom syscall code for this.\n     #[cfg(target_arch = \"arm\")]\ndiff --git a/src/fs/fadvise.rs b/src/fs/fadvise.rs\nindex f4732d55f..93d8e0810 100644\n--- a/src/fs/fadvise.rs\n+++ b/src/fs/fadvise.rs\n@@ -1,10 +1,13 @@\n use crate::{backend, io};\n use backend::fd::AsFd;\n use backend::fs::types::Advice;\n+use core::num::NonZeroU64;\n \n /// `posix_fadvise(fd, offset, len, advice)`\u2014Declares an expected access\n /// pattern for a file.\n ///\n+/// If `len` is `None`, the advice extends to the end of the file.\n+///\n /// # References\n ///  - [POSIX]\n ///  - [Linux]\n@@ -15,6 +18,11 @@ use backend::fs::types::Advice;\n /// [FreeBSD]: https://man.freebsd.org/cgi/man.cgi?query=posix_fadvise&sektion=2\n #[inline]\n #[doc(alias = \"posix_fadvise\")]\n-pub fn fadvise<Fd: AsFd>(fd: Fd, offset: u64, len: u64, advice: Advice) -> io::Result<()> {\n+pub fn fadvise<Fd: AsFd>(\n+    fd: Fd,\n+    offset: u64,\n+    len: Option<NonZeroU64>,\n+    advice: Advice,\n+) -> io::Result<()> {\n     backend::fs::syscalls::fadvise(fd.as_fd(), offset, len, advice)\n }\ndiff --git a/src/net/addr.rs b/src/net/addr.rs\nindex 8b2aa177c..015997e45 100644\n--- a/src/net/addr.rs\n+++ b/src/net/addr.rs\n@@ -155,7 +155,7 @@ mod tests {\n     fn test_layouts() {\n         assert_eq_size!(SocketAddrLen, c::socklen_t);\n \n-        #[cfg(not(windows))]\n+        #[cfg(not(any(windows, target_os = \"redox\")))]\n         assert_eq!(\n             memoffset::span_of!(c::msghdr, msg_namelen).len(),\n             size_of::<SocketAddrLen>()\n", "test_patch": "diff --git a/tests/fs/file.rs b/tests/fs/file.rs\nindex 9aa6216df..1c0b11cb9 100644\n--- a/tests/fs/file.rs\n+++ b/tests/fs/file.rs\n@@ -1,3 +1,14 @@\n+#[cfg(not(any(\n+    apple,\n+    netbsdlike,\n+    target_os = \"solaris\",\n+    target_os = \"dragonfly\",\n+    target_os = \"espidf\",\n+    target_os = \"haiku\",\n+    target_os = \"redox\",\n+)))]\n+use core::num::NonZeroU64;\n+\n #[cfg(not(target_os = \"redox\"))]\n #[test]\n fn test_file() {\n@@ -88,9 +99,8 @@ fn test_file() {\n         target_os = \"dragonfly\",\n         target_os = \"espidf\",\n         target_os = \"haiku\",\n-        target_os = \"redox\",\n     )))]\n-    rustix::fs::fadvise(&file, 0, 10, rustix::fs::Advice::Normal).unwrap();\n+    rustix::fs::fadvise(&file, 0, NonZeroU64::new(10), rustix::fs::Advice::Normal).unwrap();\n \n     rustix::fs::fsync(&file).unwrap();\n \n@@ -99,7 +109,6 @@ fn test_file() {\n         target_os = \"dragonfly\",\n         target_os = \"espidf\",\n         target_os = \"haiku\",\n-        target_os = \"redox\",\n     )))]\n     rustix::fs::fdatasync(&file).unwrap();\n \n@@ -134,7 +143,6 @@ fn test_file() {\n         solarish,\n         target_os = \"haiku\",\n         target_os = \"netbsd\",\n-        target_os = \"redox\",\n         target_os = \"wasi\",\n     )))]\n     {\ndiff --git a/tests/fs/invalid_offset.rs b/tests/fs/invalid_offset.rs\nindex 85b964c79..75dfb2ec1 100644\n--- a/tests/fs/invalid_offset.rs\n+++ b/tests/fs/invalid_offset.rs\n@@ -67,6 +67,7 @@ fn invalid_offset_fallocate() {\n )))]\n #[test]\n fn invalid_offset_fadvise() {\n+    use core::num::NonZeroU64;\n     use rustix::fs::{fadvise, openat, Advice, Mode, OFlags, CWD};\n     let tmp = tempfile::tempdir().unwrap();\n     let dir = openat(CWD, tmp.path(), OFlags::RDONLY, Mode::empty()).unwrap();\n@@ -79,21 +80,63 @@ fn invalid_offset_fadvise() {\n     .unwrap();\n \n     // `fadvise` never fails on invalid offsets.\n-    fadvise(&file, i64::MAX as u64, i64::MAX as u64, Advice::Normal).unwrap();\n-    fadvise(&file, u64::MAX, 0, Advice::Normal).unwrap();\n-    fadvise(&file, i64::MAX as u64, 1, Advice::Normal).unwrap();\n-    fadvise(&file, 1, i64::MAX as u64, Advice::Normal).unwrap();\n-    fadvise(&file, i64::MAX as u64 + 1, 0, Advice::Normal).unwrap();\n-    fadvise(&file, u64::MAX, i64::MAX as u64, Advice::Normal).unwrap();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(i64::MAX as u64),\n+        Advice::Normal,\n+    )\n+    .unwrap();\n+    fadvise(&file, u64::MAX, None, Advice::Normal).unwrap();\n+    fadvise(&file, i64::MAX as u64, NonZeroU64::new(1), Advice::Normal).unwrap();\n+    fadvise(&file, 1, NonZeroU64::new(i64::MAX as u64), Advice::Normal).unwrap();\n+    fadvise(&file, i64::MAX as u64 + 1, None, Advice::Normal).unwrap();\n+    fadvise(\n+        &file,\n+        u64::MAX,\n+        NonZeroU64::new(i64::MAX as u64),\n+        Advice::Normal,\n+    )\n+    .unwrap();\n \n     // `fadvise` fails on invalid lengths.\n-    fadvise(&file, u64::MAX, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, 0, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, u64::MAX, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64 + 1, u64::MAX, Advice::Normal).unwrap_err();\n-    fadvise(&file, i64::MAX as u64, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n-    fadvise(&file, 0, i64::MAX as u64 + 1, Advice::Normal).unwrap_err();\n+    fadvise(&file, u64::MAX, NonZeroU64::new(u64::MAX), Advice::Normal).unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(u64::MAX),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(&file, 0, NonZeroU64::new(u64::MAX), Advice::Normal).unwrap_err();\n+    fadvise(\n+        &file,\n+        u64::MAX,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64 + 1,\n+        NonZeroU64::new(u64::MAX),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        i64::MAX as u64,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n+    fadvise(\n+        &file,\n+        0,\n+        NonZeroU64::new(i64::MAX as u64 + 1),\n+        Advice::Normal,\n+    )\n+    .unwrap_err();\n }\n \n #[test]\ndiff --git a/tests/fs/negative_timestamp.rs b/tests/fs/negative_timestamp.rs\nindex 84d74c1db..63e2e81b1 100644\n--- a/tests/fs/negative_timestamp.rs\n+++ b/tests/fs/negative_timestamp.rs\n@@ -1,3 +1,4 @@\n+#[cfg(not(target_os = \"redox\"))]\n #[test]\n fn negative_file_timetamp() {\n     use rustix::fs::{\n", "problem_statement": "Consider replacing `len: u64` with `len: Option<NonZeroU64>` in `fs::fadvise`\nAt the moment the [`rustix::fs::fadvise`](https://docs.rs/rustix/latest/rustix/fs/fn.fadvise.html) function uses a `u64` as the type of it's `len` parameter. POSIX, however, specifies for zero to have the special value, meaning the whole file instead of a specific length.\n\n> If len is zero, all data from offset to the largest possible value of the file offset for that file shall be specified.\n> -- <cite>[POSIX](https://pubs.opengroup.org/onlinepubs/9799919799/functions/posix_fadvise.html)</cite>\n\n> [...] extending for len bytes (or until the end of the file if len is 0) within the file referred to by fd.\n> -- <cite>[Linux](https://man7.org/linux/man-pages/man2/posix_fadvise.2.html)\n\nI'm not sure whether rustix usually abstracts over things like that or whether other similar apis simply use a `u64` as a parameter. I would think a `Option<NonZeroU64>`  would make this difference between passing `0` and `5` for example more clear to the reader, without providing a performance degradation.\n\nOtherwise, at least hinting at the possibility of passing `0` for `len` in the documentation would probably be enough for most cases as well.\n\n", "hints_text": "", "created_at": "2025-02-15 22:58:04", "merge_commit_sha": "76c657f5534e83fbb8009bc1591a5671d6485b86", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Test use-libc (riscv64-linux)', '.github/workflows/main.yml']", "['Test (i686-linux-stable)', '.github/workflows/main.yml']"], ["['Test use-libc (powerpc64le-linux)', '.github/workflows/main.yml']", "['Test (s390x-linux-1.63)', '.github/workflows/main.yml']"], ["['Test (ubuntu-1.63)', '.github/workflows/main.yml']", "['Test (s390x-linux)', '.github/workflows/main.yml']"], ["['Test (powerpc64le-linux-1.63)', '.github/workflows/main.yml']", "['Test use-libc (ubuntu)', '.github/workflows/main.yml']"], ["['Test (riscv64-linux)', '.github/workflows/main.yml']", "['Test (arm-linux-stable)', '.github/workflows/main.yml']"], ["['Test use-libc (aarch64-linux)', '.github/workflows/main.yml']", "['Check selected Tier 3 platforms (nightly)', '.github/workflows/main.yml']"], ["['Test (ubuntu-stable)', '.github/workflows/main.yml']", "['Test rustix_use_experimental_asm (s390x-linux)', '.github/workflows/main.yml']"], ["['Test (i686-linux)', '.github/workflows/main.yml']", "['Rustfmt', '.github/workflows/main.yml']"], ["['Check (nightly)', '.github/workflows/main.yml']", "['Test rustix_use_experimental_asm (powerpc64le-linux)', '.github/workflows/main.yml']"], ["['Check --no-default-features (nightly)', '.github/workflows/main.yml']", "['Test (arm-linux-1.63)', '.github/workflows/main.yml']"], ["['Test (powerpc64le-linux)', '.github/workflows/main.yml']", "['Test (aarch64-linux-stable)', '.github/workflows/main.yml']"]]}
{"repo": "yamafaktory/hypergraph", "instance_id": "yamafaktory__hypergraph-47", "base_commit": "e8be7ca46216b0afe71c3ea919a9a8cf47dc8414", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 3f5c76d..3ac2620 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -4,14 +4,24 @@ version = 3\n \n [[package]]\n name = \"ahash\"\n-version = \"0.8.3\"\n+version = \"0.8.11\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2c99f64d1e06488f620f932677e24bc6e2897582980441ae90a671415bd7ec2f\"\n+checksum = \"e89da841a80418a9b391ebaea17f5c112ffaaa96f621d2c285b5174da76b9011\"\n dependencies = [\n  \"cfg-if\",\n  \"getrandom\",\n  \"once_cell\",\n  \"version_check\",\n+ \"zerocopy\",\n+]\n+\n+[[package]]\n+name = \"aho-corasick\"\n+version = \"1.1.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916\"\n+dependencies = [\n+ \"memchr\",\n ]\n \n [[package]]\n@@ -22,27 +32,21 @@ checksum = \"4b46cbb362ab8752921c97e041f5e366ee6297bd428a31275b9fcf1e380f7299\"\n \n [[package]]\n name = \"anstyle\"\n-version = \"1.0.2\"\n+version = \"1.0.10\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"15c4c2c83f81532e5845a733998b6971faca23490340a418e9b72a3ec9de12ea\"\n+checksum = \"55cc3b69f167a1ef2e161439aa98aed94e6028e5f9a59be9a6ffb47aef1651f9\"\n \n [[package]]\n name = \"autocfg\"\n-version = \"1.0.1\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"cdb031dd78e28731d87d56cc8ffef4a8f36ca26c38fe2de700543e627f8a464a\"\n-\n-[[package]]\n-name = \"bitflags\"\n-version = \"2.4.0\"\n+version = \"1.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b4682ae6287fcf752ecaabbfcc7b6f9b72aa33933dc23a554d853aea8eea8635\"\n+checksum = \"ace50bade8e6234aa140d9a2f552bbee1db4d353f69b8217bc503490fc1a9f26\"\n \n [[package]]\n name = \"bumpalo\"\n-version = \"3.9.1\"\n+version = \"3.16.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a4a45a46ab1f2412e53d3a0ade76ffad2025804294569aae387231a0cd6e0899\"\n+checksum = \"79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c\"\n \n [[package]]\n name = \"cast\"\n@@ -50,15 +54,6 @@ version = \"0.3.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"37b2a672a2cb129a2e41c10b1224bb368f9f37a2b16b612598138befd7b37eb5\"\n \n-[[package]]\n-name = \"cc\"\n-version = \"1.0.83\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f1174fb0b6ec23863f8b971027804a42614e347eafb0a95bf0b12cdae21fc4d0\"\n-dependencies = [\n- \"libc\",\n-]\n-\n [[package]]\n name = \"cfg-if\"\n version = \"1.0.0\"\n@@ -67,9 +62,9 @@ checksum = \"baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd\"\n \n [[package]]\n name = \"ciborium\"\n-version = \"0.2.0\"\n+version = \"0.2.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b0c137568cc60b904a7724001b35ce2630fd00d5d84805fbb608ab89509d788f\"\n+checksum = \"42e69ffd6f0917f5c029256a24d0161db17cea3997d185db0d35926308770f0e\"\n dependencies = [\n  \"ciborium-io\",\n  \"ciborium-ll\",\n@@ -78,15 +73,15 @@ dependencies = [\n \n [[package]]\n name = \"ciborium-io\"\n-version = \"0.2.0\"\n+version = \"0.2.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"346de753af073cc87b52b2083a506b38ac176a44cfb05497b622e27be899b369\"\n+checksum = \"05afea1e0a06c9be33d539b876f1ce3692f4afea2cb41f740e7743225ed1c757\"\n \n [[package]]\n name = \"ciborium-ll\"\n-version = \"0.2.0\"\n+version = \"0.2.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"213030a2b5a4e0c0892b6652260cf6ccac84827b83a85a534e178e3906c4cf1b\"\n+checksum = \"57663b653d948a338bfb3eeba9bb2fd5fcfaecb9e199e87e1eda4d9e8b240fd9\"\n dependencies = [\n  \"ciborium-io\",\n  \"half\",\n@@ -94,18 +89,18 @@ dependencies = [\n \n [[package]]\n name = \"clap\"\n-version = \"4.4.0\"\n+version = \"4.5.21\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1d5f1946157a96594eb2d2c10eb7ad9a2b27518cb3000209dec700c35df9197d\"\n+checksum = \"fb3b4b9e5a7c7514dfa52869339ee98b3156b0bfb4e8a77c4ff4babb64b1604f\"\n dependencies = [\n  \"clap_builder\",\n ]\n \n [[package]]\n name = \"clap_builder\"\n-version = \"4.4.0\"\n+version = \"4.5.21\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"78116e32a042dd73c2901f0dc30790d20ff3447f3e3472fad359e8c3d282bcd6\"\n+checksum = \"b17a95aa67cc7b5ebd32aa5370189aa0d79069ef1c64ce893bd30fb24bff20ec\"\n dependencies = [\n  \"anstyle\",\n  \"clap_lex\",\n@@ -113,9 +108,9 @@ dependencies = [\n \n [[package]]\n name = \"clap_lex\"\n-version = \"0.5.1\"\n+version = \"0.7.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"cd7cc57abe963c6d3b9d8be5b06ba7c8957a930305ca90304f24ef040aa6f961\"\n+checksum = \"afb84c814227b90d6895e01398aee0d8033c00e7466aca416fb6a8e0eb19d8a7\"\n \n [[package]]\n name = \"criterion\"\n@@ -153,55 +148,42 @@ dependencies = [\n  \"itertools 0.10.5\",\n ]\n \n-[[package]]\n-name = \"crossbeam-channel\"\n-version = \"0.5.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e54ea8bc3fb1ee042f5aace6e3c6e025d3874866da222930f70ce62aceba0bfa\"\n-dependencies = [\n- \"cfg-if\",\n- \"crossbeam-utils\",\n-]\n-\n [[package]]\n name = \"crossbeam-deque\"\n-version = \"0.8.1\"\n+version = \"0.8.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6455c0ca19f0d2fbf751b908d5c55c1f5cbc65e03c4225427254b46890bdde1e\"\n+checksum = \"613f8cc01fe9cf1a3eb3d7f488fd2fa8388403e97039e2f73692932e291a770d\"\n dependencies = [\n- \"cfg-if\",\n  \"crossbeam-epoch\",\n  \"crossbeam-utils\",\n ]\n \n [[package]]\n name = \"crossbeam-epoch\"\n-version = \"0.9.7\"\n+version = \"0.9.18\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c00d6d2ea26e8b151d99093005cb442fb9a37aeaca582a03ec70946f49ab5ed9\"\n+checksum = \"5b82ac4a3c2ca9c3460964f020e1402edd5753411d7737aa39c3714ad1b5420e\"\n dependencies = [\n- \"cfg-if\",\n  \"crossbeam-utils\",\n- \"lazy_static\",\n- \"memoffset\",\n- \"scopeguard\",\n ]\n \n [[package]]\n name = \"crossbeam-utils\"\n-version = \"0.8.7\"\n+version = \"0.8.20\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b5e5bed1f1c269533fa816a0a5492b3545209a205ca1a54842be180eb63a16a6\"\n-dependencies = [\n- \"cfg-if\",\n- \"lazy_static\",\n-]\n+checksum = \"22ec99545bb0ed0ea7bb9b8e1e9122ea386ff8a48c0922e43f36d45ab09e0e80\"\n+\n+[[package]]\n+name = \"crunchy\"\n+version = \"0.2.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"7a81dae078cea95a014a339291cec439d2f232ebe854a9d672b796c6afafa9b7\"\n \n [[package]]\n name = \"either\"\n-version = \"1.6.1\"\n+version = \"1.13.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457\"\n+checksum = \"60b1af1c220855b6ceac025d3f6ecdd2b7c4894bfe9cd9bda4fbb4bc7c0d4cf0\"\n \n [[package]]\n name = \"equivalent\"\n@@ -209,32 +191,11 @@ version = \"1.0.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5\"\n \n-[[package]]\n-name = \"errno\"\n-version = \"0.3.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6b30f669a7961ef1631673d2766cc92f52d64f7ef354d4fe0ddfd30ed52f0f4f\"\n-dependencies = [\n- \"errno-dragonfly\",\n- \"libc\",\n- \"windows-sys\",\n-]\n-\n-[[package]]\n-name = \"errno-dragonfly\"\n-version = \"0.1.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"aa68f1b12764fab894d2755d2518754e71b4fd80ecfb822714a1206c2aab39bf\"\n-dependencies = [\n- \"cc\",\n- \"libc\",\n-]\n-\n [[package]]\n name = \"getrandom\"\n-version = \"0.2.8\"\n+version = \"0.2.15\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c05aeb6a22b8f62540c194aac980f2115af067bfe15a0734d7277a768d396b31\"\n+checksum = \"c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7\"\n dependencies = [\n  \"cfg-if\",\n  \"libc\",\n@@ -243,30 +204,25 @@ dependencies = [\n \n [[package]]\n name = \"half\"\n-version = \"1.8.2\"\n+version = \"2.4.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"eabb4a44450da02c90444cf74558da904edde8fb4e9035a9a6a4e15445af0bd7\"\n+checksum = \"6dd08c532ae367adf81c312a4580bc67f1d0fe8bc9c460520283f4c0ff277888\"\n+dependencies = [\n+ \"cfg-if\",\n+ \"crunchy\",\n+]\n \n [[package]]\n name = \"hashbrown\"\n-version = \"0.14.0\"\n+version = \"0.15.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2c6201b9ff9fd90a5a3bac2e56a830d0caa509576f0e503818ee82c181b3437a\"\n+checksum = \"3a9bfc1af68b1726ea47d3d5109de126281def866b33970e10fbab11b5dafab3\"\n \n [[package]]\n name = \"hermit-abi\"\n-version = \"0.1.19\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"62b467343b94ba476dcb2500d242dadbb39557df889310ac77c5d99100aaac33\"\n-dependencies = [\n- \"libc\",\n-]\n-\n-[[package]]\n-name = \"hermit-abi\"\n-version = \"0.3.2\"\n+version = \"0.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"443144c8cdadd93ebf52ddb4056d257f5b52c04d3c804e657d19eb73fc33668b\"\n+checksum = \"fbf6a919d6cf397374f7dfeeea91d974c7c0a7221d0d0f4f20d859d329e53fcc\"\n \n [[package]]\n name = \"hypergraph\"\n@@ -275,16 +231,16 @@ dependencies = [\n  \"ahash\",\n  \"criterion\",\n  \"indexmap\",\n- \"itertools 0.11.0\",\n+ \"itertools 0.13.0\",\n  \"rayon\",\n  \"thiserror\",\n ]\n \n [[package]]\n name = \"indexmap\"\n-version = \"2.0.0\"\n+version = \"2.6.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d5477fe2230a79769d8dc68e0eabf5437907c0457a5614a9e8dddb67f65eb65d\"\n+checksum = \"707907fe3c25f5424cce2cb7e1cbcafee6bdbe735ca90ef77c29e84591e5b9da\"\n dependencies = [\n  \"equivalent\",\n  \"hashbrown\",\n@@ -293,13 +249,13 @@ dependencies = [\n \n [[package]]\n name = \"is-terminal\"\n-version = \"0.4.9\"\n+version = \"0.4.13\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"cb0889898416213fab133e1d33a0e5858a48177452750691bde3666d0fdbaf8b\"\n+checksum = \"261f68e344040fbd0edea105bef17c66edf46f984ddb1115b775ce31be948f4b\"\n dependencies = [\n- \"hermit-abi 0.3.2\",\n- \"rustix\",\n- \"windows-sys\",\n+ \"hermit-abi\",\n+ \"libc\",\n+ \"windows-sys 0.52.0\",\n ]\n \n [[package]]\n@@ -313,100 +269,72 @@ dependencies = [\n \n [[package]]\n name = \"itertools\"\n-version = \"0.11.0\"\n+version = \"0.13.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b1c173a5686ce8bfa551b3563d0c2170bf24ca44da99c7ca4bfdab5418c3fe57\"\n+checksum = \"413ee7dfc52ee1a4949ceeb7dbc8a33f2d6c088194d9f922fb8318faf1f01186\"\n dependencies = [\n  \"either\",\n ]\n \n [[package]]\n name = \"itoa\"\n-version = \"1.0.1\"\n+version = \"1.0.11\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1aab8fc367588b89dcee83ab0fd66b72b50b72fa1904d7095045ace2b0c81c35\"\n+checksum = \"49f1f14873335454500d59611f1cf4a4b0f786f9ac11f4312a78e4cf2566695b\"\n \n [[package]]\n name = \"js-sys\"\n-version = \"0.3.56\"\n+version = \"0.3.72\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a38fc24e30fd564ce974c02bf1d337caddff65be6cc4735a1f7eab22a7440f04\"\n+checksum = \"6a88f1bda2bd75b0452a14784937d796722fdebfe50df998aeb3f0b7603019a9\"\n dependencies = [\n  \"wasm-bindgen\",\n ]\n \n-[[package]]\n-name = \"lazy_static\"\n-version = \"1.4.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646\"\n-\n [[package]]\n name = \"libc\"\n-version = \"0.2.147\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b4668fb0ea861c1df094127ac5f1da3409a82116a4ba74fca2e58ef927159bb3\"\n-\n-[[package]]\n-name = \"linux-raw-sys\"\n-version = \"0.4.5\"\n+version = \"0.2.164\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"57bcfdad1b858c2db7c38303a6d2ad4dfaf5eb53dfeb0910128b2c26d6158503\"\n+checksum = \"433bfe06b8c75da9b2e3fbea6e5329ff87748f0b144ef75306e674c3f6f7c13f\"\n \n [[package]]\n name = \"log\"\n-version = \"0.4.14\"\n+version = \"0.4.22\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"51b9bbe6c47d51fc3e1a9b945965946b4c44142ab8792c50835a980d362c2710\"\n-dependencies = [\n- \"cfg-if\",\n-]\n+checksum = \"a7a70ba024b9dc04c27ea2f0c0548feb474ec5c54bba33a7f72f873a39d07b24\"\n \n [[package]]\n-name = \"memoffset\"\n-version = \"0.6.5\"\n+name = \"memchr\"\n+version = \"2.7.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5aa361d4faea93603064a027415f07bd8e1d5c88c9fbf68bf56a285428fd79ce\"\n-dependencies = [\n- \"autocfg\",\n-]\n+checksum = \"78ca9ab1a0babb1e7d5695e3530886289c18cf2f87ec19a575a0abdce112e3a3\"\n \n [[package]]\n name = \"num-traits\"\n-version = \"0.2.14\"\n+version = \"0.2.19\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"9a64b1ec5cda2586e284722486d802acf1f7dbdc623e2bfc57e65ca1cd099290\"\n+checksum = \"071dfc062690e90b734c0b2273ce72ad0ffa95f0c74596bc250dcfd960262841\"\n dependencies = [\n  \"autocfg\",\n ]\n \n-[[package]]\n-name = \"num_cpus\"\n-version = \"1.13.1\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"19e64526ebdee182341572e50e9ad03965aa510cd94427a4549448f285e957a1\"\n-dependencies = [\n- \"hermit-abi 0.1.19\",\n- \"libc\",\n-]\n-\n [[package]]\n name = \"once_cell\"\n-version = \"1.17.1\"\n+version = \"1.20.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b7e5500299e16ebb147ae15a00a942af264cf3688f47923b8fc2cd5858f23ad3\"\n+checksum = \"1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775\"\n \n [[package]]\n name = \"oorandom\"\n-version = \"11.1.3\"\n+version = \"11.1.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0ab1bc2a289d34bd04a330323ac98a1b4bc82c9d9fcb1e66b63caa84da26b575\"\n+checksum = \"b410bbe7e14ab526a0e86877eb47c6996a2bd7746f027ba551028c925390e4e9\"\n \n [[package]]\n name = \"plotters\"\n-version = \"0.3.1\"\n+version = \"0.3.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"32a3fd9ec30b9749ce28cd91f255d569591cdf937fe280c312143e3c4bad6f2a\"\n+checksum = \"5aeb6f403d7a4911efb1e33402027fc44f29b5bf6def3effcc22d7bb75f2b747\"\n dependencies = [\n  \"num-traits\",\n  \"plotters-backend\",\n@@ -417,42 +345,42 @@ dependencies = [\n \n [[package]]\n name = \"plotters-backend\"\n-version = \"0.3.2\"\n+version = \"0.3.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d88417318da0eaf0fdcdb51a0ee6c3bed624333bff8f946733049380be67ac1c\"\n+checksum = \"df42e13c12958a16b3f7f4386b9ab1f3e7933914ecea48da7139435263a4172a\"\n \n [[package]]\n name = \"plotters-svg\"\n-version = \"0.3.1\"\n+version = \"0.3.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"521fa9638fa597e1dc53e9412a4f9cefb01187ee1f7413076f9e6749e2885ba9\"\n+checksum = \"51bae2ac328883f7acdfea3d66a7c35751187f870bc81f94563733a154d7a670\"\n dependencies = [\n  \"plotters-backend\",\n ]\n \n [[package]]\n name = \"proc-macro2\"\n-version = \"1.0.66\"\n+version = \"1.0.89\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"18fb31db3f9bddb2ea821cde30a9f70117e3f119938b5ee630b7403aa6e2ead9\"\n+checksum = \"f139b0662de085916d1fb67d2b4169d1addddda1919e696f3252b740b629986e\"\n dependencies = [\n  \"unicode-ident\",\n ]\n \n [[package]]\n name = \"quote\"\n-version = \"1.0.33\"\n+version = \"1.0.37\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5267fca4496028628a95160fc423a33e8b2e6af8a5302579e322e4b520293cae\"\n+checksum = \"b5b9d34b8991d19d98081b46eacdd8eb58c6f2b201139f7c5f643cc155a633af\"\n dependencies = [\n  \"proc-macro2\",\n ]\n \n [[package]]\n name = \"rayon\"\n-version = \"1.7.0\"\n+version = \"1.10.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1d2df5196e37bcc87abebc0053e20787d73847bb33134a69841207dd0a47f03b\"\n+checksum = \"b418a60154510ca1a002a752ca9714984e21e4241e804d32555251faf8b78ffa\"\n dependencies = [\n  \"either\",\n  \"rayon-core\",\n@@ -460,49 +388,48 @@ dependencies = [\n \n [[package]]\n name = \"rayon-core\"\n-version = \"1.11.0\"\n+version = \"1.12.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"4b8f95bd6966f5c87776639160a66bd8ab9895d9d4ab01ddba9fc60661aebe8d\"\n+checksum = \"1465873a3dfdaa8ae7cb14b4383657caab0b3e8a0aa9ae8e04b044854c8dfce2\"\n dependencies = [\n- \"crossbeam-channel\",\n  \"crossbeam-deque\",\n  \"crossbeam-utils\",\n- \"num_cpus\",\n ]\n \n [[package]]\n name = \"regex\"\n-version = \"1.5.4\"\n+version = \"1.11.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d07a8629359eb56f1e2fb1652bb04212c072a87ba68546a04065d525673ac461\"\n+checksum = \"b544ef1b4eac5dc2db33ea63606ae9ffcfac26c1416a2806ae0bf5f56b201191\"\n dependencies = [\n+ \"aho-corasick\",\n+ \"memchr\",\n+ \"regex-automata\",\n  \"regex-syntax\",\n ]\n \n [[package]]\n-name = \"regex-syntax\"\n-version = \"0.6.25\"\n+name = \"regex-automata\"\n+version = \"0.4.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f497285884f3fcff424ffc933e56d7cbca511def0c9831a7f9b5f6153e3cc89b\"\n+checksum = \"809e8dc61f6de73b46c85f4c96486310fe304c434cfa43669d7b40f711150908\"\n+dependencies = [\n+ \"aho-corasick\",\n+ \"memchr\",\n+ \"regex-syntax\",\n+]\n \n [[package]]\n-name = \"rustix\"\n-version = \"0.38.8\"\n+name = \"regex-syntax\"\n+version = \"0.8.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"19ed4fa021d81c8392ce04db050a3da9a60299050b7ae1cf482d862b54a7218f\"\n-dependencies = [\n- \"bitflags\",\n- \"errno\",\n- \"libc\",\n- \"linux-raw-sys\",\n- \"windows-sys\",\n-]\n+checksum = \"2b15c43186be67a4fd63bee50d0303afffcef381492ebe2c5d87f324e1b8815c\"\n \n [[package]]\n name = \"ryu\"\n-version = \"1.0.9\"\n+version = \"1.0.18\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"73b4b750c782965c211b42f022f59af1fbceabdd026623714f104152f1ec149f\"\n+checksum = \"f3cb5ba0dc43242ce17de99c180e96db90b235b8a9fdc9543c96d2209116bd9f\"\n \n [[package]]\n name = \"same-file\"\n@@ -513,59 +440,43 @@ dependencies = [\n  \"winapi-util\",\n ]\n \n-[[package]]\n-name = \"scopeguard\"\n-version = \"1.1.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d29ab0c6d3fc0ee92fe66e2d99f700eab17a8d57d1c1d3b748380fb20baa78cd\"\n-\n [[package]]\n name = \"serde\"\n-version = \"1.0.136\"\n+version = \"1.0.215\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"ce31e24b01e1e524df96f1c2fdd054405f8d7376249a5110886fb4b658484789\"\n+checksum = \"6513c1ad0b11a9376da888e3e0baa0077f1aed55c17f50e7b2397136129fb88f\"\n dependencies = [\n  \"serde_derive\",\n ]\n \n [[package]]\n name = \"serde_derive\"\n-version = \"1.0.136\"\n+version = \"1.0.215\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"08597e7152fcd306f41838ed3e37be9eaeed2b61c42e2117266a554fab4662f9\"\n+checksum = \"ad1e866f866923f252f05c889987993144fb74e722403468a4ebd70c3cd756c0\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 1.0.82\",\n+ \"syn\",\n ]\n \n [[package]]\n name = \"serde_json\"\n-version = \"1.0.79\"\n+version = \"1.0.133\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8e8d9fa5c3b304765ce1fd9c4c8a3de2c8db365a5b91be52f186efc675681d95\"\n+checksum = \"c7fceb2473b9166b2294ef05efcb65a3db80803f0b03ef86a5fc88a2b85ee377\"\n dependencies = [\n  \"itoa\",\n+ \"memchr\",\n  \"ryu\",\n  \"serde\",\n ]\n \n [[package]]\n name = \"syn\"\n-version = \"1.0.82\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8daf5dd0bb60cbd4137b1b587d2fc0ae729bc07cf01cd70b36a1ed5ade3b9d59\"\n-dependencies = [\n- \"proc-macro2\",\n- \"quote\",\n- \"unicode-xid\",\n-]\n-\n-[[package]]\n-name = \"syn\"\n-version = \"2.0.29\"\n+version = \"2.0.87\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c324c494eba9d92503e6f1ef2e6df781e78f6a7705a0202d9801b198807d518a\"\n+checksum = \"25aa4ce346d03a6dcd68dd8b4010bcb74e54e62c90c573f394c46eae99aba32d\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -574,22 +485,22 @@ dependencies = [\n \n [[package]]\n name = \"thiserror\"\n-version = \"1.0.47\"\n+version = \"2.0.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"97a802ec30afc17eee47b2855fc72e0c4cd62be9b4efe6591edde0ec5bd68d8f\"\n+checksum = \"c006c85c7651b3cf2ada4584faa36773bd07bac24acfb39f3c431b36d7e667aa\"\n dependencies = [\n  \"thiserror-impl\",\n ]\n \n [[package]]\n name = \"thiserror-impl\"\n-version = \"1.0.47\"\n+version = \"2.0.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6bb623b56e39ab7dcd4b1b98bb6c8f8d907ed255b18de254088016b27a8ee19b\"\n+checksum = \"f077553d607adc1caf65430528a576c757a71ed73944b66ebb58ef2bbd243568\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.29\",\n+ \"syn\",\n ]\n \n [[package]]\n@@ -604,30 +515,23 @@ dependencies = [\n \n [[package]]\n name = \"unicode-ident\"\n-version = \"1.0.8\"\n+version = \"1.0.13\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e5464a87b239f13a63a501f2701565754bae92d243d4bb7eb12f6d57d2269bf4\"\n-\n-[[package]]\n-name = \"unicode-xid\"\n-version = \"0.2.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8ccb82d61f80a663efe1f787a51b16b5a51e3314d6ac365b08639f52387b33f3\"\n+checksum = \"e91b56cd4cadaeb79bbf1a5645f6b4f8dc5bde8834ad5894a8db35fda9efa1fe\"\n \n [[package]]\n name = \"version_check\"\n-version = \"0.9.4\"\n+version = \"0.9.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"49874b5167b65d7193b8aba1567f5c7d93d001cafc34600cee003eda787e483f\"\n+checksum = \"0b928f33d975fc6ad9f86c8f283853ad26bdd5b10b7f1542aa2fa15e2289105a\"\n \n [[package]]\n name = \"walkdir\"\n-version = \"2.3.2\"\n+version = \"2.5.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"808cf2735cd4b6866113f648b791c6adc5714537bc222d9347bb203386ffda56\"\n+checksum = \"29790946404f91d9c5d06f9874efddea1dc06c5efe94541a7d6863108e3a5e4b\"\n dependencies = [\n  \"same-file\",\n- \"winapi\",\n  \"winapi-util\",\n ]\n \n@@ -639,34 +543,35 @@ checksum = \"9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423\"\n \n [[package]]\n name = \"wasm-bindgen\"\n-version = \"0.2.79\"\n+version = \"0.2.95\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"25f1af7423d8588a3d840681122e72e6a24ddbcb3f0ec385cac0d12d24256c06\"\n+checksum = \"128d1e363af62632b8eb57219c8fd7877144af57558fb2ef0368d0087bddeb2e\"\n dependencies = [\n  \"cfg-if\",\n+ \"once_cell\",\n  \"wasm-bindgen-macro\",\n ]\n \n [[package]]\n name = \"wasm-bindgen-backend\"\n-version = \"0.2.79\"\n+version = \"0.2.95\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8b21c0df030f5a177f3cba22e9bc4322695ec43e7257d865302900290bcdedca\"\n+checksum = \"cb6dd4d3ca0ddffd1dd1c9c04f94b868c37ff5fac97c30b97cff2d74fce3a358\"\n dependencies = [\n  \"bumpalo\",\n- \"lazy_static\",\n  \"log\",\n+ \"once_cell\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 1.0.82\",\n+ \"syn\",\n  \"wasm-bindgen-shared\",\n ]\n \n [[package]]\n name = \"wasm-bindgen-macro\"\n-version = \"0.2.79\"\n+version = \"0.2.95\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2f4203d69e40a52ee523b2529a773d5ffc1dc0071801c87b3d270b471b80ed01\"\n+checksum = \"e79384be7f8f5a9dd5d7167216f022090cf1f9ec128e6e6a482a2cb5c5422c56\"\n dependencies = [\n  \"quote\",\n  \"wasm-bindgen-macro-support\",\n@@ -674,82 +579,70 @@ dependencies = [\n \n [[package]]\n name = \"wasm-bindgen-macro-support\"\n-version = \"0.2.79\"\n+version = \"0.2.95\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"bfa8a30d46208db204854cadbb5d4baf5fcf8071ba5bf48190c3e59937962ebc\"\n+checksum = \"26c6ab57572f7a24a4985830b120de1594465e5d500f24afe89e16b4e833ef68\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 1.0.82\",\n+ \"syn\",\n  \"wasm-bindgen-backend\",\n  \"wasm-bindgen-shared\",\n ]\n \n [[package]]\n name = \"wasm-bindgen-shared\"\n-version = \"0.2.79\"\n+version = \"0.2.95\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3d958d035c4438e28c70e4321a2911302f10135ce78a9c7834c0cab4123d06a2\"\n+checksum = \"65fc09f10666a9f147042251e0dda9c18f166ff7de300607007e96bdebc1068d\"\n \n [[package]]\n name = \"web-sys\"\n-version = \"0.3.56\"\n+version = \"0.3.72\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c060b319f29dd25724f09a2ba1418f142f539b2be99fbf4d2d5a8f7330afb8eb\"\n+checksum = \"f6488b90108c040df0fe62fa815cbdee25124641df01814dd7282749234c6112\"\n dependencies = [\n  \"js-sys\",\n  \"wasm-bindgen\",\n ]\n \n [[package]]\n-name = \"winapi\"\n-version = \"0.3.9\"\n+name = \"winapi-util\"\n+version = \"0.1.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419\"\n+checksum = \"cf221c93e13a30d793f7645a0e7762c55d169dbb0a49671918a2319d289b10bb\"\n dependencies = [\n- \"winapi-i686-pc-windows-gnu\",\n- \"winapi-x86_64-pc-windows-gnu\",\n+ \"windows-sys 0.59.0\",\n ]\n \n [[package]]\n-name = \"winapi-i686-pc-windows-gnu\"\n-version = \"0.4.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6\"\n-\n-[[package]]\n-name = \"winapi-util\"\n-version = \"0.1.5\"\n+name = \"windows-sys\"\n+version = \"0.52.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"70ec6ce85bb158151cae5e5c87f95a8e97d2c0c4b001223f33a334e3ce5de178\"\n+checksum = \"282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d\"\n dependencies = [\n- \"winapi\",\n+ \"windows-targets\",\n ]\n \n-[[package]]\n-name = \"winapi-x86_64-pc-windows-gnu\"\n-version = \"0.4.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f\"\n-\n [[package]]\n name = \"windows-sys\"\n-version = \"0.48.0\"\n+version = \"0.59.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9\"\n+checksum = \"1e38bc4d79ed67fd075bcc251a1c39b32a1776bbe92e5bef1f0bf1f8c531853b\"\n dependencies = [\n  \"windows-targets\",\n ]\n \n [[package]]\n name = \"windows-targets\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c\"\n+checksum = \"9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973\"\n dependencies = [\n  \"windows_aarch64_gnullvm\",\n  \"windows_aarch64_msvc\",\n  \"windows_i686_gnu\",\n+ \"windows_i686_gnullvm\",\n  \"windows_i686_msvc\",\n  \"windows_x86_64_gnu\",\n  \"windows_x86_64_gnullvm\",\n@@ -758,42 +651,68 @@ dependencies = [\n \n [[package]]\n name = \"windows_aarch64_gnullvm\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8\"\n+checksum = \"32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3\"\n \n [[package]]\n name = \"windows_aarch64_msvc\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc\"\n+checksum = \"09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469\"\n \n [[package]]\n name = \"windows_i686_gnu\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b\"\n+\n+[[package]]\n+name = \"windows_i686_gnullvm\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e\"\n+checksum = \"0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66\"\n \n [[package]]\n name = \"windows_i686_msvc\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406\"\n+checksum = \"240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66\"\n \n [[package]]\n name = \"windows_x86_64_gnu\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e\"\n+checksum = \"147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78\"\n \n [[package]]\n name = \"windows_x86_64_gnullvm\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc\"\n+checksum = \"24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d\"\n \n [[package]]\n name = \"windows_x86_64_msvc\"\n-version = \"0.48.5\"\n+version = \"0.52.6\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec\"\n+\n+[[package]]\n+name = \"zerocopy\"\n+version = \"0.7.35\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1b9b4fd18abc82b8136838da5d50bae7bdea537c574d8dc1a34ed098d6c166f0\"\n+dependencies = [\n+ \"zerocopy-derive\",\n+]\n+\n+[[package]]\n+name = \"zerocopy-derive\"\n+version = \"0.7.35\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538\"\n+checksum = \"fa4f8080344d4671fb4e831a13ad1e68092748387dfc4f55e356242fae12ce3e\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 23c2634..a0ca65c 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -13,11 +13,11 @@ rust-version = \"1.56\"\n version = \"2.1.2\"\n \n [dependencies]\n-ahash = \"0.8.3\"\n-indexmap = { version = \"2.0.0\", features = [\"rayon\"] }\n-itertools = \"0.11.0\"\n+ahash = \"0.8.11\"\n+indexmap = { version = \"2.6.0\", features = [\"rayon\"] }\n+itertools = \"0.13.0\"\n rayon = \"1.7.0\"\n-thiserror = \"1.0.47\"\n+thiserror = \"2.0.3\"\n \n [dev-dependencies]\n criterion = \"0.5.1\"\n@@ -25,3 +25,14 @@ criterion = \"0.5.1\"\n [[bench]]\n name = \"performance\"\n harness = false\n+\n+[lints.rust]\n+missing_debug_implementations = \"warn\"\n+missing_docs = \"warn\"\n+nonstandard_style = { level = \"deny\", priority= -1 }\n+rust_2021_compatibility = { level = \"forbid\", priority= -1 }\n+unreachable_pub = \"warn\"\n+unsafe_code = \"deny\"\n+\n+[lints.clippy]\n+all = \"deny\"\ndiff --git a/benches/performance.rs b/benches/performance.rs\nindex fd58821..3df63df 100644\n--- a/benches/performance.rs\n+++ b/benches/performance.rs\n@@ -1,9 +1,21 @@\n #![deny(unsafe_code, nonstandard_style)]\n \n-use std::fmt::{Display, Formatter, Result};\n-\n-use criterion::{criterion_group, criterion_main, Criterion};\n-use hypergraph::{HyperedgeIndex, Hypergraph, VertexIndex};\n+use std::fmt::{\n+    Display,\n+    Formatter,\n+    Result,\n+};\n+\n+use criterion::{\n+    Criterion,\n+    criterion_group,\n+    criterion_main,\n+};\n+use hypergraph::{\n+    HyperedgeIndex,\n+    Hypergraph,\n+    VertexIndex,\n+};\n use itertools::Itertools;\n \n static HYPEREDGES: usize = 10_000;\ndiff --git a/rustfmt.toml b/rustfmt.toml\nindex 3d97daf..0d8880c 100644\n--- a/rustfmt.toml\n+++ b/rustfmt.toml\n@@ -1,5 +1,6 @@\n edition = \"2021\"\n group_imports = \"StdExternalCrate\"\n imports_granularity = \"Crate\"\n+imports_layout=\"Vertical\"\n use_field_init_shorthand = true\n-version = \"Two\"\n+style_edition = \"2024\"\ndiff --git a/src/core/bi_hash_map.rs b/src/core/bi_hash_map.rs\nindex 41170ca..e383b03 100644\n--- a/src/core/bi_hash_map.rs\n+++ b/src/core/bi_hash_map.rs\n@@ -1,4 +1,7 @@\n-use std::{collections::HashMap, fmt::Debug};\n+use std::{\n+    collections::HashMap,\n+    fmt::Debug,\n+};\n \n /// Bi-directional hashmap used to store the mapping between the internal\n /// unstable indexes - generated by `IndexMap` and `IndexSet` - and the exposed\ndiff --git a/src/core/errors.rs b/src/core/errors.rs\nindex d379d13..f5b980e 100644\n--- a/src/core/errors.rs\n+++ b/src/core/errors.rs\n@@ -1,6 +1,9 @@\n use thiserror::Error;\n \n-use crate::{HyperedgeIndex, VertexIndex};\n+use crate::{\n+    HyperedgeIndex,\n+    VertexIndex,\n+};\n \n /// Enumeration of all the possible errors.\n #[derive(Clone, Debug, Eq, Error, PartialEq)]\n@@ -9,7 +12,7 @@ where\n     V: Copy + Eq,\n     HE: Copy + Eq,\n {\n-    /// Error when a HyperedgeIndex was not found.\n+    /// Error when a `HyperedgeIndex` was not found.\n     #[error(\"HyperedgeIndex {0} was not found\")]\n     HyperedgeIndexNotFound(HyperedgeIndex),\n \n@@ -66,7 +69,7 @@ where\n     #[error(\"At least two hyperedges must be provided to be joined\")]\n     HyperedgesInvalidJoin,\n \n-    /// Error when a VertexIndex was not found.\n+    /// Error when a `VertexIndex` was not found.\n     #[error(\"VertexIndex {0} was not found\")]\n     VertexIndexNotFound(VertexIndex),\n \ndiff --git a/src/core/hyperedges/add_hyperedge.rs b/src/core/hyperedges/add_hyperedge.rs\nindex ebf3f9c..ce63128 100644\n--- a/src/core/hyperedges/add_hyperedge.rs\n+++ b/src/core/hyperedges/add_hyperedge.rs\n@@ -1,6 +1,11 @@\n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/add_hyperedge_index.rs b/src/core/hyperedges/add_hyperedge_index.rs\nindex 4ecce4f..f0c4d7e 100644\n--- a/src/core/hyperedges/add_hyperedge_index.rs\n+++ b/src/core/hyperedges/add_hyperedge_index.rs\n@@ -1,4 +1,9 @@\n-use crate::{HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/clear_hyperedges.rs b/src/core/hyperedges/clear_hyperedges.rs\nindex 5d61c92..d276674 100644\n--- a/src/core/hyperedges/clear_hyperedges.rs\n+++ b/src/core/hyperedges/clear_hyperedges.rs\n@@ -1,7 +1,11 @@\n use rayon::prelude::*;\n \n use crate::{\n-    bi_hash_map::BiHashMap, errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexTrait,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    bi_hash_map::BiHashMap,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/contract_hyperedge_vertices.rs b/src/core/hyperedges/contract_hyperedge_vertices.rs\nindex c0b128b..dc54214 100644\n--- a/src/core/hyperedges/contract_hyperedge_vertices.rs\n+++ b/src/core/hyperedges/contract_hyperedge_vertices.rs\n@@ -2,8 +2,13 @@ use itertools::Itertools;\n use rayon::prelude::*;\n \n use crate::{\n-    core::utils::are_slices_equal, errors::HypergraphError, HyperedgeIndex, HyperedgeTrait,\n-    Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::utils::are_slices_equal,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/count_hyperedges.rs b/src/core/hyperedges/count_hyperedges.rs\nindex 8b929e6..6472aef 100644\n--- a/src/core/hyperedges/count_hyperedges.rs\n+++ b/src/core/hyperedges/count_hyperedges.rs\n@@ -1,4 +1,8 @@\n-use crate::{HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/get_hyperedge.rs b/src/core/hyperedges/get_hyperedge.rs\nindex 07e05ad..869115c 100644\n--- a/src/core/hyperedges/get_hyperedge.rs\n+++ b/src/core/hyperedges/get_hyperedge.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/get_hyperedge_vertices.rs b/src/core/hyperedges/get_hyperedge_vertices.rs\nindex 4e4364f..80b4f2a 100644\n--- a/src/core/hyperedges/get_hyperedge_vertices.rs\n+++ b/src/core/hyperedges/get_hyperedge_vertices.rs\n@@ -1,6 +1,11 @@\n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/get_hyperedge_weight.rs b/src/core/hyperedges/get_hyperedge_weight.rs\nindex 1c2f01c..df3b7f9 100644\n--- a/src/core/hyperedges/get_hyperedge_weight.rs\n+++ b/src/core/hyperedges/get_hyperedge_weight.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/get_hyperedges.rs b/src/core/hyperedges/get_hyperedges.rs\nindex 1048d69..295e8c6 100644\n--- a/src/core/hyperedges/get_hyperedges.rs\n+++ b/src/core/hyperedges/get_hyperedges.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/get_hyperedges_connecting.rs b/src/core/hyperedges/get_hyperedges_connecting.rs\nindex 387eb08..cab1855 100644\n--- a/src/core/hyperedges/get_hyperedges_connecting.rs\n+++ b/src/core/hyperedges/get_hyperedges_connecting.rs\n@@ -1,8 +1,13 @@\n use rayon::prelude::*;\n \n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph,\n-    VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/get_hyperedges_intersections.rs b/src/core/hyperedges/get_hyperedges_intersections.rs\nindex 5bbb1e9..674262b 100644\n--- a/src/core/hyperedges/get_hyperedges_intersections.rs\n+++ b/src/core/hyperedges/get_hyperedges_intersections.rs\n@@ -1,9 +1,13 @@\n use itertools::Itertools;\n-use rayon::prelude::*;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\n@@ -26,7 +30,7 @@ where\n \n         // Get the internal vertices of the hyperedges and keep the eventual error.\n         let vertices = hyperedges\n-            .into_par_iter()\n+            .into_iter()\n             .map(|hyperedge_index| {\n                 self.get_internal_hyperedge(hyperedge_index)\n                     .and_then(|internal_index| {\ndiff --git a/src/core/hyperedges/get_internal_hyperedge.rs b/src/core/hyperedges/get_internal_hyperedge.rs\nindex 36d8822..aff5722 100644\n--- a/src/core/hyperedges/get_internal_hyperedge.rs\n+++ b/src/core/hyperedges/get_internal_hyperedge.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/get_internal_hyperedges.rs b/src/core/hyperedges/get_internal_hyperedges.rs\nindex 76ddf8c..2cb7849 100644\n--- a/src/core/hyperedges/get_internal_hyperedges.rs\n+++ b/src/core/hyperedges/get_internal_hyperedges.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/join_hyperedges.rs b/src/core/hyperedges/join_hyperedges.rs\nindex 3c16cb7..bbcdabd 100644\n--- a/src/core/hyperedges/join_hyperedges.rs\n+++ b/src/core/hyperedges/join_hyperedges.rs\n@@ -1,7 +1,12 @@\n use rayon::prelude::*;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/remove_hyperedge.rs b/src/core/hyperedges/remove_hyperedge.rs\nindex ba96539..86b7675 100644\n--- a/src/core/hyperedges/remove_hyperedge.rs\n+++ b/src/core/hyperedges/remove_hyperedge.rs\n@@ -1,5 +1,10 @@\n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\n@@ -33,7 +38,7 @@ where\n         for vertex in vertices {\n             match self.vertices.get_index_mut(vertex) {\n                 Some((_, index_set)) => {\n-                    index_set.remove(&internal_index);\n+                    index_set.swap_remove(&internal_index);\n                 }\n                 None => return Err(HypergraphError::InternalVertexIndexNotFound(vertex)),\n             }\n@@ -113,7 +118,7 @@ where\n                         // Perform an insertion of the current hyperedge and a\n                         // removal of the swapped one.\n                         index_set.insert(internal_index);\n-                        index_set.remove(&last_index);\n+                        index_set.swap_remove(&last_index);\n                     }\n                     None => return Err(HypergraphError::InternalVertexIndexNotFound(vertex)),\n                 }\ndiff --git a/src/core/hyperedges/reverse_hyperedge.rs b/src/core/hyperedges/reverse_hyperedge.rs\nindex d44da9a..a6596c9 100644\n--- a/src/core/hyperedges/reverse_hyperedge.rs\n+++ b/src/core/hyperedges/reverse_hyperedge.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/hyperedges/update_hyperedge_vertices.rs b/src/core/hyperedges/update_hyperedge_vertices.rs\nindex 59c433a..959f5ea 100644\n--- a/src/core/hyperedges/update_hyperedge_vertices.rs\n+++ b/src/core/hyperedges/update_hyperedge_vertices.rs\n@@ -1,8 +1,14 @@\n use rayon::prelude::*;\n \n use crate::{\n-    core::utils::are_slices_equal, errors::HypergraphError, HyperedgeIndex, HyperedgeKey,\n-    HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::utils::are_slices_equal,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/hyperedges/update_hyperedge_weight.rs b/src/core/hyperedges/update_hyperedge_weight.rs\nindex 3ac9f31..a948087 100644\n--- a/src/core/hyperedges/update_hyperedge_weight.rs\n+++ b/src/core/hyperedges/update_hyperedge_weight.rs\n@@ -1,5 +1,10 @@\n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/indexes.rs b/src/core/indexes.rs\nindex 4990e4b..0cfbf3c 100644\n--- a/src/core/indexes.rs\n+++ b/src/core/indexes.rs\n@@ -1,4 +1,8 @@\n-use std::fmt::{Display, Formatter, Result};\n+use std::fmt::{\n+    Display,\n+    Formatter,\n+    Result,\n+};\n \n /// Vertex stable index representation as usize.\n /// Uses the newtype index pattern.\ndiff --git a/src/core/iterator.rs b/src/core/iterator.rs\nindex 9157d09..fa32570 100644\n--- a/src/core/iterator.rs\n+++ b/src/core/iterator.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> IntoIterator for Hypergraph<V, HE>\n where\ndiff --git a/src/core/mod.rs b/src/core/mod.rs\nindex 53b88ce..06e4781 100644\n--- a/src/core/mod.rs\n+++ b/src/core/mod.rs\n@@ -14,16 +14,28 @@ mod utils;\n pub mod vertices;\n \n use std::{\n-    fmt::{Debug, Display, Formatter, Result},\n+    fmt::{\n+        Debug,\n+        Display,\n+        Formatter,\n+        Result,\n+    },\n     hash::Hash,\n     ops::Deref,\n };\n \n use bi_hash_map::BiHashMap;\n-use types::{AIndexMap, AIndexSet, ARandomState};\n+use types::{\n+    AIndexMap,\n+    AIndexSet,\n+    ARandomState,\n+};\n \n // Reexport indexes at this level.\n-pub use crate::core::indexes::{HyperedgeIndex, VertexIndex};\n+pub use crate::core::indexes::{\n+    HyperedgeIndex,\n+    VertexIndex,\n+};\n \n /// Shared Trait for the vertices.\n /// Must be implemented to use the library.\ndiff --git a/src/core/shared.rs b/src/core/shared.rs\nindex b37f9e8..06f55d8 100644\n--- a/src/core/shared.rs\n+++ b/src/core/shared.rs\n@@ -2,7 +2,12 @@ use itertools::Itertools;\n use rayon::prelude::*;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n /// Enumeration of the different types of connection.\ndiff --git a/src/core/types.rs b/src/core/types.rs\nindex 0a01564..783b726 100644\n--- a/src/core/types.rs\n+++ b/src/core/types.rs\n@@ -1,5 +1,8 @@\n use ahash::RandomState;\n-use indexmap::{IndexMap, IndexSet};\n+use indexmap::{\n+    IndexMap,\n+    IndexSet,\n+};\n \n /// Type alias to use `AHash` as a faster hasher for `IndexMap`.\n pub(crate) type AIndexMap<K, V> = IndexMap<K, V, RandomState>;\ndiff --git a/src/core/vertices/add_vertex.rs b/src/core/vertices/add_vertex.rs\nindex 46a35ff..f7a9bf8 100644\n--- a/src/core/vertices/add_vertex.rs\n+++ b/src/core/vertices/add_vertex.rs\n@@ -1,7 +1,13 @@\n use crate::{\n-    core::types::{AIndexSet, ARandomState},\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::types::{\n+        AIndexSet,\n+        ARandomState,\n+    },\n     errors::HypergraphError,\n-    HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/add_vertex_index.rs b/src/core/vertices/add_vertex_index.rs\nindex 42ab5f8..57f0146 100644\n--- a/src/core/vertices/add_vertex_index.rs\n+++ b/src/core/vertices/add_vertex_index.rs\n@@ -1,4 +1,9 @@\n-use crate::{HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/count_vertices.rs b/src/core/vertices/count_vertices.rs\nindex 80fbb1c..4b713e9 100644\n--- a/src/core/vertices/count_vertices.rs\n+++ b/src/core/vertices/count_vertices.rs\n@@ -1,4 +1,8 @@\n-use crate::{HyperedgeTrait, Hypergraph, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexTrait,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_adjacent_vertices_from.rs b/src/core/vertices/get_adjacent_vertices_from.rs\nindex f2cc93f..6659d31 100644\n--- a/src/core/vertices/get_adjacent_vertices_from.rs\n+++ b/src/core/vertices/get_adjacent_vertices_from.rs\n@@ -1,8 +1,12 @@\n use rayon::prelude::*;\n \n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/get_adjacent_vertices_to.rs b/src/core/vertices/get_adjacent_vertices_to.rs\nindex 9f69820..57d57e4 100644\n--- a/src/core/vertices/get_adjacent_vertices_to.rs\n+++ b/src/core/vertices/get_adjacent_vertices_to.rs\n@@ -1,8 +1,12 @@\n use rayon::prelude::*;\n \n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/get_dijkstra_connections.rs b/src/core/vertices/get_dijkstra_connections.rs\nindex 6aad381..bbae907 100644\n--- a/src/core/vertices/get_dijkstra_connections.rs\n+++ b/src/core/vertices/get_dijkstra_connections.rs\n@@ -1,13 +1,21 @@\n use std::{\n     cmp::Ordering,\n-    collections::{BinaryHeap, HashMap},\n+    collections::{\n+        BinaryHeap,\n+        HashMap,\n+    },\n     fmt::Debug,\n };\n \n use rayon::prelude::*;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n #[derive(Clone, Copy, Debug, PartialEq, Eq)]\ndiff --git a/src/core/vertices/get_full_adjacent_vertices_from.rs b/src/core/vertices/get_full_adjacent_vertices_from.rs\nindex 9cc391c..84dacce 100644\n--- a/src/core/vertices/get_full_adjacent_vertices_from.rs\n+++ b/src/core/vertices/get_full_adjacent_vertices_from.rs\n@@ -1,9 +1,17 @@\n use indexmap::IndexMap;\n-use itertools::{fold, Itertools};\n+use itertools::{\n+    Itertools,\n+    fold,\n+};\n \n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph,\n-    VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n #[allow(clippy::type_complexity)]\ndiff --git a/src/core/vertices/get_full_adjacent_vertices_to.rs b/src/core/vertices/get_full_adjacent_vertices_to.rs\nindex 7a5f93e..c04fd59 100644\n--- a/src/core/vertices/get_full_adjacent_vertices_to.rs\n+++ b/src/core/vertices/get_full_adjacent_vertices_to.rs\n@@ -1,9 +1,17 @@\n use indexmap::IndexMap;\n-use itertools::{fold, Itertools};\n+use itertools::{\n+    Itertools,\n+    fold,\n+};\n \n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph,\n-    VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n #[allow(clippy::type_complexity)]\ndiff --git a/src/core/vertices/get_full_vertex_hyperedges.rs b/src/core/vertices/get_full_vertex_hyperedges.rs\nindex c746f30..7336ba5 100644\n--- a/src/core/vertices/get_full_vertex_hyperedges.rs\n+++ b/src/core/vertices/get_full_vertex_hyperedges.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_internal_vertex.rs b/src/core/vertices/get_internal_vertex.rs\nindex e4182ce..4acd7fe 100644\n--- a/src/core/vertices/get_internal_vertex.rs\n+++ b/src/core/vertices/get_internal_vertex.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_internal_vertices.rs b/src/core/vertices/get_internal_vertices.rs\nindex bf57b48..1e3bad7 100644\n--- a/src/core/vertices/get_internal_vertices.rs\n+++ b/src/core/vertices/get_internal_vertices.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_vertex.rs b/src/core/vertices/get_vertex.rs\nindex 62ae72b..ce97b06 100644\n--- a/src/core/vertices/get_vertex.rs\n+++ b/src/core/vertices/get_vertex.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_vertex_degree_in.rs b/src/core/vertices/get_vertex_degree_in.rs\nindex 23a9be1..cbdfe89 100644\n--- a/src/core/vertices/get_vertex_degree_in.rs\n+++ b/src/core/vertices/get_vertex_degree_in.rs\n@@ -1,6 +1,10 @@\n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/get_vertex_degree_out.rs b/src/core/vertices/get_vertex_degree_out.rs\nindex 0309a15..b067a75 100644\n--- a/src/core/vertices/get_vertex_degree_out.rs\n+++ b/src/core/vertices/get_vertex_degree_out.rs\n@@ -1,6 +1,10 @@\n use crate::{\n-    core::shared::Connection, errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n     VertexTrait,\n+    core::shared::Connection,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/get_vertex_hyperedges.rs b/src/core/vertices/get_vertex_hyperedges.rs\nindex 39e8018..f722217 100644\n--- a/src/core/vertices/get_vertex_hyperedges.rs\n+++ b/src/core/vertices/get_vertex_hyperedges.rs\n@@ -1,7 +1,12 @@\n use itertools::Itertools;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeIndex, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeIndex,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/get_vertex_weight.rs b/src/core/vertices/get_vertex_weight.rs\nindex 0b0e820..afa2398 100644\n--- a/src/core/vertices/get_vertex_weight.rs\n+++ b/src/core/vertices/get_vertex_weight.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/get_vertices.rs b/src/core/vertices/get_vertices.rs\nindex 813e2db..4a72cba 100644\n--- a/src/core/vertices/get_vertices.rs\n+++ b/src/core/vertices/get_vertices.rs\n@@ -1,6 +1,12 @@\n use rayon::prelude::*;\n \n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/core/vertices/remove_vertex.rs b/src/core/vertices/remove_vertex.rs\nindex 8bbbe38..e11779a 100644\n--- a/src/core/vertices/remove_vertex.rs\n+++ b/src/core/vertices/remove_vertex.rs\n@@ -1,7 +1,12 @@\n use rayon::prelude::*;\n \n use crate::{\n-    errors::HypergraphError, HyperedgeKey, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait,\n+    HyperedgeKey,\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n };\n \n impl<V, HE> Hypergraph<V, HE>\ndiff --git a/src/core/vertices/update_vertex_weight.rs b/src/core/vertices/update_vertex_weight.rs\nindex d2cf6f6..079f6ea 100644\n--- a/src/core/vertices/update_vertex_weight.rs\n+++ b/src/core/vertices/update_vertex_weight.rs\n@@ -1,4 +1,10 @@\n-use crate::{errors::HypergraphError, HyperedgeTrait, Hypergraph, VertexIndex, VertexTrait};\n+use crate::{\n+    HyperedgeTrait,\n+    Hypergraph,\n+    VertexIndex,\n+    VertexTrait,\n+    errors::HypergraphError,\n+};\n \n impl<V, HE> Hypergraph<V, HE>\n where\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 4101a5a..8ae1534 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -1,9 +1,3 @@\n-#![deny(clippy::pedantic)]\n-#![allow(clippy::module_name_repetitions, clippy::must_use_candidate)]\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n-#![warn(missing_debug_implementations, missing_docs, unreachable_pub)]\n-\n //! Hypergraph is data structure library to generate directed [hypergraphs](https://en.wikipedia.org/wiki/Hypergraph).\n //!\n //! > A hypergraph is a generalization of a graph in which a hyperedge can join any number of vertices.\n", "test_patch": "diff --git a/tests/common/mod.rs b/tests/common/mod.rs\nindex 0f69dae..d520a7e 100644\n--- a/tests/common/mod.rs\n+++ b/tests/common/mod.rs\n@@ -1,21 +1,25 @@\n #![deny(unsafe_code, nonstandard_style)]\n \n-use std::fmt::{Display, Formatter, Result};\n+use std::fmt::{\n+    Display,\n+    Formatter,\n+    Result,\n+};\n \n #[derive(Debug, Copy, Clone, Hash, Eq, PartialEq)]\n-pub struct Vertex<'a> {\n+pub(crate) struct Vertex<'a> {\n     name: &'a str,\n }\n \n impl<'a> Vertex<'a> {\n-    pub fn new(name: &'a str) -> Self {\n+    pub(crate) fn new(name: &'a str) -> Self {\n         Vertex { name }\n     }\n }\n \n-impl<'a> Display for Vertex<'a> {\n+impl Display for Vertex<'_> {\n     fn fmt(&self, formatter: &mut Formatter<'_>) -> Result {\n-        write!(formatter, \"{}\", self)\n+        write!(formatter, \"{}\", self.name)\n     }\n }\n \n@@ -31,9 +35,9 @@ impl<'a> Hyperedge<'a> {\n     }\n }\n \n-impl<'a> Display for Hyperedge<'a> {\n+impl Display for Hyperedge<'_> {\n     fn fmt(&self, formatter: &mut Formatter<'_>) -> Result {\n-        write!(formatter, \"{}\", self)\n+        write!(formatter, \"{}\", self.name)\n     }\n }\n \ndiff --git a/tests/integration_contraction.rs b/tests/integration_contraction.rs\nindex 1e8ebee..16e81c8 100644\n--- a/tests/integration_contraction.rs\n+++ b/tests/integration_contraction.rs\n@@ -1,10 +1,17 @@\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n+//! Integration tests.\n \n mod common;\n \n-use common::{Hyperedge, Vertex};\n-use hypergraph::{errors::HypergraphError, HyperedgeIndex, Hypergraph, VertexIndex};\n+use common::{\n+    Hyperedge,\n+    Vertex,\n+};\n+use hypergraph::{\n+    HyperedgeIndex,\n+    Hypergraph,\n+    VertexIndex,\n+    errors::HypergraphError,\n+};\n \n #[test]\n fn integration_contration() {\ndiff --git a/tests/integration_dijkstra.rs b/tests/integration_dijkstra.rs\nindex 5d5a130..77b7e93 100644\n--- a/tests/integration_dijkstra.rs\n+++ b/tests/integration_dijkstra.rs\n@@ -1,9 +1,11 @@\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n+//! Integration tests.\n \n mod common;\n \n-use common::{Hyperedge, Vertex};\n+use common::{\n+    Hyperedge,\n+    Vertex,\n+};\n use hypergraph::Hypergraph;\n \n #[test]\ndiff --git a/tests/integration_iterator.rs b/tests/integration_iterator.rs\nindex a37e5d6..a45e3cf 100644\n--- a/tests/integration_iterator.rs\n+++ b/tests/integration_iterator.rs\n@@ -1,9 +1,11 @@\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n+//! Integration tests.\n \n mod common;\n \n-use common::{Hyperedge, Vertex};\n+use common::{\n+    Hyperedge,\n+    Vertex,\n+};\n use hypergraph::Hypergraph;\n \n #[test]\n@@ -44,14 +46,17 @@ fn integration_iterator() {\n         vec![\n             (hyperedge_one, vec![vertex_one, vertex_two, vertex_three]),\n             (hyperedge_two, vec![vertex_four, vertex_five]),\n-            (\n-                hyperedge_three,\n-                vec![vertex_three, vertex_three, vertex_three]\n-            ),\n-            (\n-                hyperedge_four,\n-                vec![vertex_five, vertex_four, vertex_three, vertex_one]\n-            )\n+            (hyperedge_three, vec![\n+                vertex_three,\n+                vertex_three,\n+                vertex_three\n+            ]),\n+            (hyperedge_four, vec![\n+                vertex_five,\n+                vertex_four,\n+                vertex_three,\n+                vertex_one\n+            ])\n         ],\n         \"should provide `into_iter()` yelding a vector of tuples of the form (hyperedge, vector of vertices)\"\n     );\ndiff --git a/tests/integration_join.rs b/tests/integration_join.rs\nindex da912f3..804241c 100644\n--- a/tests/integration_join.rs\n+++ b/tests/integration_join.rs\n@@ -1,10 +1,15 @@\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n+//! Integration tests.\n \n mod common;\n \n-use common::{Hyperedge, Vertex};\n-use hypergraph::{errors::HypergraphError, Hypergraph};\n+use common::{\n+    Hyperedge,\n+    Vertex,\n+};\n+use hypergraph::{\n+    Hypergraph,\n+    errors::HypergraphError,\n+};\n \n #[test]\n fn integration_contration() {\ndiff --git a/tests/integration_main.rs b/tests/integration_main.rs\nindex 97b6a93..a636106 100644\n--- a/tests/integration_main.rs\n+++ b/tests/integration_main.rs\n@@ -1,10 +1,17 @@\n-#![deny(unsafe_code, nonstandard_style)]\n-#![forbid(rust_2021_compatibility)]\n+//! Integration tests.\n \n mod common;\n \n-use common::{Hyperedge, Vertex};\n-use hypergraph::{errors::HypergraphError, HyperedgeIndex, Hypergraph, VertexIndex};\n+use common::{\n+    Hyperedge,\n+    Vertex,\n+};\n+use hypergraph::{\n+    HyperedgeIndex,\n+    Hypergraph,\n+    VertexIndex,\n+    errors::HypergraphError,\n+};\n \n #[test]\n fn integration_main() {\n", "problem_statement": "bump ahash crate to fix build\ncurrently, cargo build generates the following error due to the removal of stdsim: https://github.com/rust-lang/rust/pull/117372\r\n\r\n```bash\r\nerror[E0635]: unknown feature stdsimd\r\n--> /home/cmichaud/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ahash-0.8.3/src/lib.rs:99:42\r\n|\r\n99 | #![cfg_attr(feature = \"stdsimd\", feature(stdsimd))]\r\n```\r\n\r\nAlso fixed two errors reported by clippy and ran `cargo fmt` that updated around 40 files.\n", "hints_text": "the workflow is green in my fork after those changes.", "created_at": "2024-11-19 10:35:07", "merge_commit_sha": "be27586e077b5caf3584b70c7018a94ea7b18981", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['\u2697\ufe0f test (ubuntu-latest, stable)', '.github/workflows/ci.yml']", "['\u2697\ufe0f test (windows-latest, stable)', '.github/workflows/ci.yml']"], ["['\ud83d\udee0\ufe0f format', '.github/workflows/ci.yml']", "['\u2697\ufe0f test (windows-latest, beta)', '.github/workflows/ci.yml']"], ["['\u2697\ufe0f test (windows-latest, nightly)', '.github/workflows/ci.yml']", "['\ud83d\udd0d clippy', '.github/workflows/ci.yml']"]]}
{"repo": "uutils/coreutils", "instance_id": "uutils__coreutils-7115", "base_commit": "2dbc941b69917df058c57746dae0a18dcc01b57a", "patch": "diff --git a/src/uu/stat/src/stat.rs b/src/uu/stat/src/stat.rs\nindex 5e617e7a31a..a6220267314 100644\n--- a/src/uu/stat/src/stat.rs\n+++ b/src/uu/stat/src/stat.rs\n@@ -94,6 +94,7 @@ pub enum OutputType {\n     Unsigned(u64),\n     UnsignedHex(u64),\n     UnsignedOct(u32),\n+    Float(f64),\n     Unknown,\n }\n \n@@ -120,6 +121,13 @@ impl std::str::FromStr for QuotingStyle {\n     }\n }\n \n+#[derive(Debug, PartialEq, Eq, Clone, Copy)]\n+enum Precision {\n+    NotSpecified,\n+    NoNumber,\n+    Number(usize),\n+}\n+\n #[derive(Debug, PartialEq, Eq)]\n enum Token {\n     Char(char),\n@@ -127,7 +135,7 @@ enum Token {\n     Directive {\n         flag: Flags,\n         width: usize,\n-        precision: Option<usize>,\n+        precision: Precision,\n         format: char,\n     },\n }\n@@ -238,10 +246,10 @@ struct Stater {\n /// * `output` - A reference to the OutputType enum containing the value to be printed.\n /// * `flags` - A Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed output.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n ///\n /// This function delegates the printing process to more specialized functions depending on the output type.\n-fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<usize>) {\n+fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Precision) {\n     // If the precision is given as just '.', the precision is taken to be zero.\n     // A negative precision is taken as if the precision were omitted.\n     // This gives the minimum number of digits to appear for d, i, o, u, x, and X conversions,\n@@ -271,7 +279,7 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n     // A sign (+ or -) should always be placed before a number produced by a signed conversion.\n     // By default, a sign  is  used only for negative numbers.\n     // A + overrides a space if both are used.\n-    let padding_char = determine_padding_char(&flags, &precision);\n+    let padding_char = determine_padding_char(&flags);\n \n     match output {\n         OutputType::Str(s) => print_str(s, &flags, width, precision),\n@@ -283,6 +291,9 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n         OutputType::UnsignedHex(num) => {\n             print_unsigned_hex(*num, &flags, width, precision, padding_char);\n         }\n+        OutputType::Float(num) => {\n+            print_float(*num, &flags, width, precision, padding_char);\n+        }\n         OutputType::Unknown => print!(\"?\"),\n     }\n }\n@@ -292,13 +303,12 @@ fn print_it(output: &OutputType, flags: Flags, width: usize, precision: Option<u\n /// # Arguments\n ///\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n-/// * `precision` - An Option containing the precision value.\n ///\n /// # Returns\n ///\n /// * Padding - An instance of the Padding enum representing the padding character.\n-fn determine_padding_char(flags: &Flags, precision: &Option<usize>) -> Padding {\n-    if flags.zero && !flags.left && precision.is_none() {\n+fn determine_padding_char(flags: &Flags) -> Padding {\n+    if flags.zero && !flags.left {\n         Padding::Zero\n     } else {\n         Padding::Space\n@@ -312,10 +322,10 @@ fn determine_padding_char(flags: &Flags, precision: &Option<usize>) -> Padding {\n /// * `s` - The string to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed string.\n-/// * `precision` - An Option containing the precision value.\n-fn print_str(s: &str, flags: &Flags, width: usize, precision: Option<usize>) {\n+/// * `precision` - How many digits of precision, if any.\n+fn print_str(s: &str, flags: &Flags, width: usize, precision: Precision) {\n     let s = match precision {\n-        Some(p) if p < s.len() => &s[..p],\n+        Precision::Number(p) if p < s.len() => &s[..p],\n         _ => s,\n     };\n     pad_and_print(s, flags.left, width, Padding::Space);\n@@ -415,13 +425,13 @@ fn process_token_filesystem(t: &Token, meta: StatFs, display_name: &str) {\n /// * `num` - The integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_integer(\n     num: i64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let num = num.to_string();\n@@ -437,13 +447,66 @@ fn print_integer(\n     } else {\n         \"\"\n     };\n-    let extended = format!(\n-        \"{prefix}{arg:0>precision$}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let extended = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{arg}\"),\n+        Precision::NoNumber => format!(\"{prefix}{arg}\"),\n+        Precision::Number(p) => format!(\"{prefix}{arg:0>precision$}\", precision = p),\n+    };\n     pad_and_print(&extended, flags.left, width, padding_char);\n }\n \n+/// Truncate a float to the given number of digits after the decimal point.\n+fn precision_trunc(num: f64, precision: Precision) -> String {\n+    // GNU `stat` doesn't round, it just seems to truncate to the\n+    // given precision:\n+    //\n+    //     $ stat -c \"%.5Y\" /dev/pts/ptmx\n+    //     1736344012.76399\n+    //     $ stat -c \"%.4Y\" /dev/pts/ptmx\n+    //     1736344012.7639\n+    //     $ stat -c \"%.3Y\" /dev/pts/ptmx\n+    //     1736344012.763\n+    //\n+    // Contrast this with `printf`, which seems to round the\n+    // numbers:\n+    //\n+    //     $ printf \"%.5f\\n\" 1736344012.76399\n+    //     1736344012.76399\n+    //     $ printf \"%.4f\\n\" 1736344012.76399\n+    //     1736344012.7640\n+    //     $ printf \"%.3f\\n\" 1736344012.76399\n+    //     1736344012.764\n+    //\n+    let num_str = num.to_string();\n+    let n = num_str.len();\n+    match (num_str.find('.'), precision) {\n+        (None, Precision::NotSpecified) => num_str,\n+        (None, Precision::NoNumber) => num_str,\n+        (None, Precision::Number(0)) => num_str,\n+        (None, Precision::Number(p)) => format!(\"{num_str}.{zeros}\", zeros = \"0\".repeat(p)),\n+        (Some(i), Precision::NotSpecified) => num_str[..i].to_string(),\n+        (Some(_), Precision::NoNumber) => num_str,\n+        (Some(i), Precision::Number(0)) => num_str[..i].to_string(),\n+        (Some(i), Precision::Number(p)) if p < n - i => num_str[..i + 1 + p].to_string(),\n+        (Some(i), Precision::Number(p)) => {\n+            format!(\"{num_str}{zeros}\", zeros = \"0\".repeat(p - (n - i - 1)))\n+        }\n+    }\n+}\n+\n+fn print_float(num: f64, flags: &Flags, width: usize, precision: Precision, padding_char: Padding) {\n+    let prefix = if flags.sign {\n+        \"+\"\n+    } else if flags.space {\n+        \" \"\n+    } else {\n+        \"\"\n+    };\n+    let num_str = precision_trunc(num, precision);\n+    let extended = format!(\"{prefix}{num_str}\");\n+    pad_and_print(&extended, flags.left, width, padding_char)\n+}\n+\n /// Prints an unsigned integer value based on the provided flags, width, and precision.\n ///\n /// # Arguments\n@@ -451,13 +514,13 @@ fn print_integer(\n /// * `num` - The unsigned integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned(\n     num: u64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let num = num.to_string();\n@@ -466,7 +529,11 @@ fn print_unsigned(\n     } else {\n         Cow::Borrowed(num.as_str())\n     };\n-    let s = format!(\"{s:0>precision$}\", precision = precision.unwrap_or(0));\n+    let s = match precision {\n+        Precision::NotSpecified => s,\n+        Precision::NoNumber => s,\n+        Precision::Number(p) => format!(\"{s:0>precision$}\", precision = p).into(),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -477,20 +544,21 @@ fn print_unsigned(\n /// * `num` - The unsigned octal integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned octal integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned_oct(\n     num: u32,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let prefix = if flags.alter { \"0\" } else { \"\" };\n-    let s = format!(\n-        \"{prefix}{num:0>precision$o}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let s = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{num:o}\"),\n+        Precision::NoNumber => format!(\"{prefix}{num:o}\"),\n+        Precision::Number(p) => format!(\"{prefix}{num:0>precision$o}\", precision = p),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -501,20 +569,21 @@ fn print_unsigned_oct(\n /// * `num` - The unsigned hexadecimal integer value to be printed.\n /// * `flags` - A reference to the Flags struct containing formatting flags.\n /// * `width` - The width of the field for the printed unsigned hexadecimal integer.\n-/// * `precision` - An Option containing the precision value.\n+/// * `precision` - How many digits of precision, if any.\n /// * `padding_char` - The padding character as determined by `determine_padding_char`.\n fn print_unsigned_hex(\n     num: u64,\n     flags: &Flags,\n     width: usize,\n-    precision: Option<usize>,\n+    precision: Precision,\n     padding_char: Padding,\n ) {\n     let prefix = if flags.alter { \"0x\" } else { \"\" };\n-    let s = format!(\n-        \"{prefix}{num:0>precision$x}\",\n-        precision = precision.unwrap_or(0)\n-    );\n+    let s = match precision {\n+        Precision::NotSpecified => format!(\"{prefix}{num:x}\"),\n+        Precision::NoNumber => format!(\"{prefix}{num:x}\"),\n+        Precision::Number(p) => format!(\"{prefix}{num:0>precision$x}\", precision = p),\n+    };\n     pad_and_print(&s, flags.left, width, padding_char);\n }\n \n@@ -530,6 +599,10 @@ impl Stater {\n                 '0' => flag.zero = true,\n                 '-' => flag.left = true,\n                 ' ' => flag.space = true,\n+                // This is not documented but the behavior seems to be\n+                // the same as a space. For example `stat -c \"%I5s\" f`\n+                // prints \"    0\".\n+                'I' => flag.space = true,\n                 '+' => flag.sign = true,\n                 '\\'' => flag.group = true,\n                 _ => break,\n@@ -560,7 +633,7 @@ impl Stater {\n         Self::process_flags(chars, i, bound, &mut flag);\n \n         let mut width = 0;\n-        let mut precision = None;\n+        let mut precision = Precision::NotSpecified;\n         let mut j = *i;\n \n         if let Some((field_width, offset)) = format_str[j..].scan_num::<usize>() {\n@@ -585,11 +658,11 @@ impl Stater {\n             match format_str[j..].scan_num::<i32>() {\n                 Some((value, offset)) => {\n                     if value >= 0 {\n-                        precision = Some(value as usize);\n+                        precision = Precision::Number(value as usize);\n                     }\n                     j += offset;\n                 }\n-                None => precision = Some(0),\n+                None => precision = Precision::NoNumber,\n             }\n             check_bound(format_str, bound, old, j)?;\n         }\n@@ -898,7 +971,24 @@ impl Stater {\n                     // time of last data modification, human-readable\n                     'y' => OutputType::Str(pretty_time(meta.mtime(), meta.mtime_nsec())),\n                     // time of last data modification, seconds since Epoch\n-                    'Y' => OutputType::Integer(meta.mtime()),\n+                    'Y' => {\n+                        let sec = meta.mtime();\n+                        let nsec = meta.mtime_nsec();\n+                        let tm =\n+                            chrono::DateTime::from_timestamp(sec, nsec as u32).unwrap_or_default();\n+                        let tm: DateTime<Local> = tm.into();\n+                        match tm.timestamp_nanos_opt() {\n+                            None => {\n+                                let micros = tm.timestamp_micros();\n+                                let secs = micros as f64 / 1_000_000.0;\n+                                OutputType::Float(secs)\n+                            }\n+                            Some(ns) => {\n+                                let secs = ns as f64 / 1_000_000_000.0;\n+                                OutputType::Float(secs)\n+                            }\n+                        }\n+                    }\n                     // time of last status change, human-readable\n                     'z' => OutputType::Str(pretty_time(meta.ctime(), meta.ctime_nsec())),\n                     // time of last status change, seconds since Epoch\n@@ -1107,7 +1197,7 @@ fn pretty_time(sec: i64, nsec: i64) -> String {\n \n #[cfg(test)]\n mod tests {\n-    use super::{group_num, Flags, ScanUtil, Stater, Token};\n+    use super::{group_num, precision_trunc, Flags, Precision, ScanUtil, Stater, Token};\n \n     #[test]\n     fn test_scanners() {\n@@ -1155,7 +1245,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 10,\n-                precision: Some(2),\n+                precision: Precision::Number(2),\n                 format: 'a',\n             },\n             Token::Char('c'),\n@@ -1166,7 +1256,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 5,\n-                precision: Some(0),\n+                precision: Precision::NoNumber,\n                 format: 'w',\n             },\n             Token::Char('\\n'),\n@@ -1186,7 +1276,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 15,\n-                precision: None,\n+                precision: Precision::NotSpecified,\n                 format: 'a',\n             },\n             Token::Byte(b'\\t'),\n@@ -1205,7 +1295,7 @@ mod tests {\n                     ..Default::default()\n                 },\n                 width: 20,\n-                precision: None,\n+                precision: Precision::NotSpecified,\n                 format: 'w',\n             },\n             Token::Byte(b'\\x12'),\n@@ -1216,4 +1306,16 @@ mod tests {\n         ];\n         assert_eq!(&expected, &Stater::generate_tokens(s, true).unwrap());\n     }\n+\n+    #[test]\n+    fn test_precision_trunc() {\n+        assert_eq!(precision_trunc(123.456, Precision::NotSpecified), \"123\");\n+        assert_eq!(precision_trunc(123.456, Precision::NoNumber), \"123.456\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(0)), \"123\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(1)), \"123.4\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(2)), \"123.45\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(3)), \"123.456\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(4)), \"123.4560\");\n+        assert_eq!(precision_trunc(123.456, Precision::Number(5)), \"123.45600\");\n+    }\n }\n", "test_patch": "diff --git a/tests/by-util/test_stat.rs b/tests/by-util/test_stat.rs\nindex cbd36832f48..cd74767283a 100644\n--- a/tests/by-util/test_stat.rs\n+++ b/tests/by-util/test_stat.rs\n@@ -184,9 +184,74 @@ fn test_char() {\n     ];\n     let ts = TestScenario::new(util_name!());\n     let expected_stdout = unwrap_or_return!(expected_result(&ts, &args)).stdout_move_str();\n+    eprintln!(\"{expected_stdout}\");\n     ts.ucmd().args(&args).succeeds().stdout_is(expected_stdout);\n }\n \n+#[cfg(target_os = \"linux\")]\n+#[test]\n+fn test_printf_mtime_precision() {\n+    // TODO Higher precision numbers (`%.3Y`, `%.4Y`, etc.) are\n+    // formatted correctly, but we are not precise enough when we do\n+    // some `mtime` computations, so we get `.7640` instead of\n+    // `.7639`. This can be fixed by being more careful when\n+    // transforming the number from `Metadata::mtime_nsec()` to the form\n+    // used in rendering.\n+    let args = [\"-c\", \"%.0Y %.1Y %.2Y\", \"/dev/pts/ptmx\"];\n+    let ts = TestScenario::new(util_name!());\n+    let expected_stdout = unwrap_or_return!(expected_result(&ts, &args)).stdout_move_str();\n+    eprintln!(\"{expected_stdout}\");\n+    ts.ucmd().args(&args).succeeds().stdout_is(expected_stdout);\n+}\n+\n+#[cfg(feature = \"touch\")]\n+#[test]\n+fn test_timestamp_format() {\n+    let ts = TestScenario::new(util_name!());\n+\n+    // Create a file with a specific timestamp for testing\n+    ts.ccmd(\"touch\")\n+        .args(&[\"-d\", \"1970-01-01 18:43:33.023456789\", \"k\"])\n+        .succeeds()\n+        .no_stderr();\n+\n+    let test_cases = vec![\n+        // Basic timestamp formats\n+        (\"%Y\", \"67413\"),\n+        (\"%.Y\", \"67413.023456789\"),\n+        (\"%.1Y\", \"67413.0\"),\n+        (\"%.3Y\", \"67413.023\"),\n+        (\"%.6Y\", \"67413.023456\"),\n+        (\"%.9Y\", \"67413.023456789\"),\n+        // Width and padding tests\n+        (\"%13.6Y\", \" 67413.023456\"),\n+        (\"%013.6Y\", \"067413.023456\"),\n+        (\"%-13.6Y\", \"67413.023456 \"),\n+        // Longer width/precision combinations\n+        (\"%18.10Y\", \"  67413.0234567890\"),\n+        (\"%I18.10Y\", \"  67413.0234567890\"),\n+        (\"%018.10Y\", \"0067413.0234567890\"),\n+        (\"%-18.10Y\", \"67413.0234567890  \"),\n+    ];\n+\n+    for (format_str, expected) in test_cases {\n+        let result = ts\n+            .ucmd()\n+            .args(&[\"-c\", format_str, \"k\"])\n+            .succeeds()\n+            .stdout_move_str();\n+\n+        assert_eq!(\n+            result,\n+            format!(\"{expected}\\n\"),\n+            \"Format '{}' failed.\\nExpected: '{}'\\nGot: '{}'\",\n+            format_str,\n+            expected,\n+            result,\n+        );\n+    }\n+}\n+\n #[cfg(any(target_os = \"linux\", target_os = \"android\", target_vendor = \"apple\"))]\n #[test]\n fn test_date() {\n", "problem_statement": "stat: --printf option does not support precision in certain format strings\nThe `--printf` argument to `stat` does not seem to correctly support precision specifiers in format strings.\n\nFor example, after creating a file with `touch f`...\n\nGNU stat:\n```\n$ stat --printf='%.1Y\\n' f\n1646795086.4\n```\n\nuutils stat:\n```\n$ ./target/release/stat --printf='%.1Y\\n' f\n1\n```\n\nFrom the documentation\n> The \u2018%W\u2019, \u2018%X\u2019, \u2018%Y\u2019, and \u2018%Z\u2019 formats accept a precision preceded by a period to specify the number of digits to print after the decimal point. For example, \u2018%.3X\u2019 outputs the access timestamp to millisecond precision. If a period is given but no precision, stat uses 9 digits, so \u2018%.X\u2019 is equivalent to \u2018%.9X\u2019. When discarding excess precision, timestamps are truncated toward minus infinity. \n\n-- https://www.gnu.org/software/coreutils/manual/html_node/stat-invocation.html\n\n*Edit:* this is causing a test failure in the GNU test file `tests/stat/stat-nanoseconds.sh`.\n", "hints_text": "@jfinkels, err\u2026 Where did you find `stat` in this package?\r\nThat's what I see on Windows 7 x64 upon launch of _coreutils 0.0.16_\r\n```console\r\n$ coreutils.exe\r\ncoreutils 0.0.16 (multi-call binary)\r\n\r\nUsage: coreutils [function [arguments...]]                                                      \r\n                                                                                                \r\nCurrently defined functions:                                                                    \r\n                                                                                                \r\n    [, arch, b2sum, b3sum, base32, base64, basename, basenc, cat, cksum, comm, cp, csplit, cut, \r\n    date, dd, df, dir, dircolors, dirname, du, echo, env, expand, expr, factor, false, fmt,     \r\n    fold, hashsum, head, hostname, join, link, ln, ls, md5sum, mkdir, mktemp, more, mv, nl,     \r\n    nproc, numfmt, od, paste, pr, printenv, printf, ptx, pwd, readlink, realpath, relpath, rm,  \r\n    rmdir, seq, sha1sum, sha224sum, sha256sum, sha3-224sum, sha3-256sum, sha3-384sum, sha3-     \r\n    512sum, sha384sum, sha3sum, sha512sum, shake128sum, shake256sum, shred, shuf, sleep, sort,  \r\n    split, sum, sync, tac, tail, tee, test, touch, tr, true, truncate, tsort, unexpand, uniq,   \r\n    unlink, vdir, wc, whoami, yes\r\n\r\n$ coreutils.exe stat\r\nstat: function/utility not found                                                             \r\n```\n@sergeevabc It's not available on Windows. If you're on unix, you can compile with `--features unix` or `--features stat` to access it.\nEven though it was supposed to be only a refactor, https://github.com/uutils/coreutils/pull/4150 solves part of this:\r\n```shell\r\n$ cargo run --quiet -- stat --printf='%.1Y\\n' f\r\n1668645806\r\n```\r\nAt least it shows some useful info and not just `1` \ud83d\ude04 I think the problem is that the precision is misinterpreted as the length of the string instead of the digits after the decimal point or something like that.\nI've dug into this issue and found that the implementation of `stat` does not care about type `float`. Furthermore, the function used to get meta info of files `fs::symlink_metadata` or `fs::metadata` returns values no more than type `integer` or `unsigned` in the first place. So I suppose fixing this would require another dependency such as filetime for higher precision. @tertsdiepraam", "created_at": "2025-01-11 02:34:56", "merge_commit_sha": "988cc4eae36a4098b07ed5447f4f26de436459e5", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build/stable (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']", "['Build (windows-latest, aarch64-pc-windows-msvc, feat_os_windows, use-cross, true)', '.github/workflows/CICD.yml']"], ["['Separate Builds (macos-latest)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_env, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_seq, false)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_wc, false)', '.github/workflows/fuzzing.yml']"], ["['Style/format (ubuntu-latest, feat_os_unix)', '.github/workflows/code-quality.yml']", "['Style and Lint (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']"], ["['Tests (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']", "['Run the fuzzers (fuzz_split, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_echo, true)', '.github/workflows/fuzzing.yml']", "['Binary sizes (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Build/stable (windows-latest, feat_os_windows)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (macos-latest, aarch64-apple-darwin, feat_os_macos)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, i686-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Tests/Toybox test suite (ubuntu-latest)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, i686-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']", "['Update/format (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']"], ["['Run the fuzzers (fuzz_test, true)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_expr, true)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_parse_size, true)', '.github/workflows/fuzzing.yml']", "['Dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_seq_parse_number, true)', '.github/workflows/fuzzing.yml']", "['Test all features separately (macos-latest)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_cksum, false)', '.github/workflows/fuzzing.yml']", "['Run GNU tests', '.github/workflows/GnuTests.yml']"], ["['Build the fuzzers', '.github/workflows/fuzzing.yml']", "['Update/dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']"], ["['Run the fuzzers (fuzz_sort, false)', '.github/workflows/fuzzing.yml']", "['Build (windows-latest, x86_64-pc-windows-msvc, feat_os_windows)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, arm-unknown-linux-gnueabihf, feat_os_unix_gnueabihf, use-cross, true)', '.github/workflows/CICD.yml']", "['Build (windows-latest, x86_64-pc-windows-gnu, feat_os_windows)', '.github/workflows/CICD.yml']"]]}
{"repo": "uutils/coreutils", "instance_id": "uutils__coreutils-7225", "base_commit": "7747351cb1f54a3b5f4f3316e909f5a9e1f309e7", "patch": "diff --git a/src/uu/kill/src/kill.rs b/src/uu/kill/src/kill.rs\nindex 7638dcc4488..18b45e2eff6 100644\n--- a/src/uu/kill/src/kill.rs\n+++ b/src/uu/kill/src/kill.rs\n@@ -17,6 +17,11 @@ use uucore::{format_usage, help_about, help_usage, show};\n static ABOUT: &str = help_about!(\"kill.md\");\n const USAGE: &str = help_usage!(\"kill.md\");\n \n+// When the -l option is selected, the program displays the type of signal related to a certain\n+// value or string. In case of a value, the program should control the lower 8 bits, but there is\n+// a particular case in which if the value is in range [128, 159], it is translated to a signal\n+const OFFSET: usize = 128;\n+\n pub mod options {\n     pub static PIDS_OR_SIGNALS: &str = \"pids_or_signals\";\n     pub static LIST: &str = \"list\";\n@@ -164,13 +169,24 @@ fn table() {\n }\n \n fn print_signal(signal_name_or_value: &str) -> UResult<()> {\n+    // Closure used to track the last 8 bits of the signal value\n+    // when the -l option is passed only the lower 8 bits are important\n+    // or the value is in range [128, 159]\n+    // Example: kill -l 143 => TERM because 143 = 15 + 128\n+    // Example: kill -l 2304 => EXIT\n+    let lower_8_bits = |x: usize| x & 0xff;\n+    let option_num_parse = signal_name_or_value.parse::<usize>().ok();\n+\n     for (value, &signal) in ALL_SIGNALS.iter().enumerate() {\n         if signal.eq_ignore_ascii_case(signal_name_or_value)\n             || format!(\"SIG{signal}\").eq_ignore_ascii_case(signal_name_or_value)\n         {\n             println!(\"{value}\");\n             return Ok(());\n-        } else if signal_name_or_value == value.to_string() {\n+        } else if signal_name_or_value == value.to_string()\n+            || option_num_parse.is_some_and(|signal_value| lower_8_bits(signal_value) == value)\n+            || option_num_parse.is_some_and(|signal_value| signal_value == value + OFFSET)\n+        {\n             println!(\"{signal}\");\n             return Ok(());\n         }\ndiff --git a/util/why-error.md b/util/why-error.md\nindex 6c7865e430a..b387bf3310c 100644\n--- a/util/why-error.md\n+++ b/util/why-error.md\n@@ -27,7 +27,6 @@ This file documents why some tests are failing:\n * gnu/tests/ls/stat-free-symlinks.sh\n * gnu/tests/misc/close-stdout.sh\n * gnu/tests/misc/comm.pl\n-* gnu/tests/misc/kill.sh - https://github.com/uutils/coreutils/issues/7218\n * gnu/tests/misc/nohup.sh\n * gnu/tests/misc/numfmt.pl - https://github.com/uutils/coreutils/issues/7219 / https://github.com/uutils/coreutils/issues/7221\n * gnu/tests/misc/stdbuf.sh - https://github.com/uutils/coreutils/issues/7072\n", "test_patch": "diff --git a/tests/by-util/test_kill.rs b/tests/by-util/test_kill.rs\nindex b42e34828f6..0cdfd9aae4f 100644\n--- a/tests/by-util/test_kill.rs\n+++ b/tests/by-util/test_kill.rs\n@@ -334,6 +334,39 @@ fn test_kill_with_signal_and_list() {\n         .fails();\n }\n \n+#[test]\n+fn test_kill_with_list_lower_bits() {\n+    new_ucmd!()\n+        .arg(\"-l\")\n+        .arg(\"128\")\n+        .succeeds()\n+        .stdout_contains(\"EXIT\");\n+\n+    new_ucmd!()\n+        .arg(\"-l\")\n+        .arg(\"143\")\n+        .succeeds()\n+        .stdout_contains(\"TERM\");\n+\n+    new_ucmd!()\n+        .arg(\"-l\")\n+        .arg(\"256\")\n+        .succeeds()\n+        .stdout_contains(\"EXIT\");\n+\n+    new_ucmd!()\n+        .arg(\"-l\")\n+        .arg(\"2304\")\n+        .succeeds()\n+        .stdout_contains(\"EXIT\");\n+}\n+\n+#[test]\n+fn test_kill_with_list_lower_bits_unrecognized() {\n+    new_ucmd!().arg(\"-l\").arg(\"111\").fails();\n+    new_ucmd!().arg(\"-l\").arg(\"384\").fails();\n+}\n+\n #[test]\n fn test_kill_with_signal_and_table() {\n     let target = Target::new();\n", "problem_statement": "kill: fails to use only least significant bits to identify signal with -l\nEnvironment: Ubuntu 24.04, uutils `main` branch (git commit 2430e2a396d5f5ae29413081b065a4503245839b), GNU coreutils v9.6.8-fbfd88-dirty\n\nSteps to reproduce:\n```\n# 143 is 15 + 128, and signal number 15 is SIGTERM.\nkill -l 143\n```\n\nWhat happens now: uutils `kill` displays an error message that the signal 143 is unknown:\n```\nkill: unknown signal name '143'\n```\n\nWhat I expected to happen: GNU `kill` lists the name of the signal `TERM`:\n```\nTERM\n```\n\nNotes: This is causing a failure in the GNU test file `tests/misc/kill.sh`. It seems that only some number of the least significant bits are used to identify the signal.\n", "hints_text": "", "created_at": "2025-01-26 23:01:32", "merge_commit_sha": "1595b6afaaadb78890de977d67cd367095404255", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build/stable (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']", "['Build (windows-latest, aarch64-pc-windows-msvc, feat_os_windows, use-cross, true)', '.github/workflows/CICD.yml']"], ["['Separate Builds (macos-latest)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_env, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_seq, false)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_wc, false)', '.github/workflows/fuzzing.yml']"], ["['Style/format (ubuntu-latest, feat_os_unix)', '.github/workflows/code-quality.yml']", "['Style and Lint (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']"], ["['Tests (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']", "['Run the fuzzers (fuzz_split, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_echo, true)', '.github/workflows/fuzzing.yml']", "['Binary sizes (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Build/stable (windows-latest, feat_os_windows)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (macos-latest, aarch64-apple-darwin, feat_os_macos)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, i686-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Tests/Toybox test suite (ubuntu-latest)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, i686-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_parse_size, true)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_test, true)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_expr, true)', '.github/workflows/fuzzing.yml']"], ["['Update/format (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']", "['Dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_seq_parse_number, true)', '.github/workflows/fuzzing.yml']", "['Test all features separately (macos-latest)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_cksum, false)', '.github/workflows/fuzzing.yml']", "['Run GNU tests', '.github/workflows/GnuTests.yml']"], ["['Build the fuzzers', '.github/workflows/fuzzing.yml']", "['Update/dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']"], ["['Run the fuzzers (fuzz_sort, false)', '.github/workflows/fuzzing.yml']", "['Build (windows-latest, x86_64-pc-windows-msvc, feat_os_windows)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, arm-unknown-linux-gnueabihf, feat_os_unix_gnueabihf, use-cross, true)', '.github/workflows/CICD.yml']", "['Build (windows-latest, x86_64-pc-windows-gnu, feat_os_windows)', '.github/workflows/CICD.yml']"]]}
{"repo": "uutils/coreutils", "instance_id": "uutils__coreutils-7154", "base_commit": "64dad0c3ab704be4c6c4f4fb802afa026b983a5e", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 1b2a67c13aa..29b14b9a820 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -2871,9 +2871,11 @@ version = \"0.0.29\"\n dependencies = [\n  \"ansi-width\",\n  \"chrono\",\n+ \"chrono-tz\",\n  \"clap\",\n  \"glob\",\n  \"hostname\",\n+ \"iana-time-zone\",\n  \"lscolors\",\n  \"number_prefix\",\n  \"once_cell\",\ndiff --git a/src/uu/ls/Cargo.toml b/src/uu/ls/Cargo.toml\nindex 17cef9b8aa4..0b60009e65b 100644\n--- a/src/uu/ls/Cargo.toml\n+++ b/src/uu/ls/Cargo.toml\n@@ -18,13 +18,17 @@ path = \"src/ls.rs\"\n \n [dependencies]\n ansi-width = { workspace = true }\n-clap = { workspace = true, features = [\"env\"] }\n chrono = { workspace = true }\n-number_prefix = { workspace = true }\n-uutils_term_grid = { workspace = true }\n-terminal_size = { workspace = true }\n+chrono-tz = { workspace = true }\n+clap = { workspace = true, features = [\"env\"] }\n glob = { workspace = true }\n+hostname = { workspace = true }\n+iana-time-zone = { workspace = true }\n lscolors = { workspace = true }\n+number_prefix = { workspace = true }\n+once_cell = { workspace = true }\n+selinux = { workspace = true, optional = true }\n+terminal_size = { workspace = true }\n uucore = { workspace = true, features = [\n   \"colors\",\n   \"entries\",\n@@ -34,9 +38,7 @@ uucore = { workspace = true, features = [\n   \"quoting-style\",\n   \"version-cmp\",\n ] }\n-once_cell = { workspace = true }\n-selinux = { workspace = true, optional = true }\n-hostname = { workspace = true }\n+uutils_term_grid = { workspace = true }\n \n [[bin]]\n name = \"ls\"\ndiff --git a/src/uu/ls/src/ls.rs b/src/uu/ls/src/ls.rs\nindex 994eabc21b6..9aaa0d0a4e3 100644\n--- a/src/uu/ls/src/ls.rs\n+++ b/src/uu/ls/src/ls.rs\n@@ -5,19 +5,9 @@\n \n // spell-checker:ignore (ToDO) somegroup nlink tabsize dired subdired dtype colorterm stringly\n \n-use clap::{\n-    builder::{NonEmptyStringValueParser, PossibleValue, ValueParser},\n-    crate_version, Arg, ArgAction, Command,\n-};\n-use glob::{MatchOptions, Pattern};\n-use lscolors::LsColors;\n-\n-use ansi_width::ansi_width;\n-use std::{cell::OnceCell, num::IntErrorKind};\n-use std::{collections::HashSet, io::IsTerminal};\n-\n #[cfg(windows)]\n use std::os::windows::fs::MetadataExt;\n+use std::{cell::OnceCell, num::IntErrorKind};\n use std::{\n     cmp::Reverse,\n     error::Error,\n@@ -34,7 +24,20 @@ use std::{\n     os::unix::fs::{FileTypeExt, MetadataExt},\n     time::Duration,\n };\n+use std::{collections::HashSet, io::IsTerminal};\n+\n+use ansi_width::ansi_width;\n+use chrono::{DateTime, Local, TimeDelta, TimeZone, Utc};\n+use chrono_tz::{OffsetName, Tz};\n+use clap::{\n+    builder::{NonEmptyStringValueParser, PossibleValue, ValueParser},\n+    crate_version, Arg, ArgAction, Command,\n+};\n+use glob::{MatchOptions, Pattern};\n+use iana_time_zone::get_timezone;\n+use lscolors::LsColors;\n use term_grid::{Direction, Filling, Grid, GridOptions};\n+\n use uucore::error::USimpleError;\n use uucore::format::human::{human_readable, SizeFormat};\n #[cfg(all(unix, not(any(target_os = \"android\", target_os = \"macos\"))))]\n@@ -67,10 +70,12 @@ use uucore::{\n     version_cmp::version_cmp,\n };\n use uucore::{help_about, help_section, help_usage, parse_glob, show, show_error, show_warning};\n+\n mod dired;\n use dired::{is_dired_arg_present, DiredOutput};\n mod colors;\n use colors::{color_name, StyleManager};\n+\n #[cfg(not(feature = \"selinux\"))]\n static CONTEXT_HELP_TEXT: &str = \"print any security context of each file (not enabled)\";\n #[cfg(feature = \"selinux\")]\n@@ -334,6 +339,58 @@ enum TimeStyle {\n     Format(String),\n }\n \n+/// Whether the given date is considered recent (i.e., in the last 6 months).\n+fn is_recent(time: DateTime<Local>) -> bool {\n+    // According to GNU a Gregorian year has 365.2425 * 24 * 60 * 60 == 31556952 seconds on the average.\n+    time + TimeDelta::try_seconds(31_556_952 / 2).unwrap() > Local::now()\n+}\n+\n+/// Get the alphabetic abbreviation of the current timezone.\n+///\n+/// For example, \"UTC\" or \"CET\" or \"PDT\".\n+fn timezone_abbrev() -> String {\n+    let tz = match std::env::var(\"TZ\") {\n+        // TODO Support other time zones...\n+        Ok(s) if s == \"UTC0\" || s.is_empty() => Tz::Etc__UTC,\n+        _ => match get_timezone() {\n+            Ok(tz_str) => tz_str.parse().unwrap(),\n+            Err(_) => Tz::Etc__UTC,\n+        },\n+    };\n+    let offset = tz.offset_from_utc_date(&Utc::now().date_naive());\n+    offset.abbreviation().unwrap_or(\"UTC\").to_string()\n+}\n+\n+/// Format the given time according to a custom format string.\n+fn custom_time_format(fmt: &str, time: DateTime<Local>) -> String {\n+    // TODO Refactor the common code from `ls` and `date` for rendering dates.\n+    // TODO - Revisit when chrono 0.5 is released. https://github.com/chronotope/chrono/issues/970\n+    // GNU `date` uses `%N` for nano seconds, however the `chrono` crate uses `%f`.\n+    let fmt = fmt.replace(\"%N\", \"%f\").replace(\"%Z\", &timezone_abbrev());\n+    time.format(&fmt).to_string()\n+}\n+\n+impl TimeStyle {\n+    /// Format the given time according to this time format style.\n+    fn format(&self, time: DateTime<Local>) -> String {\n+        let recent = is_recent(time);\n+        match (self, recent) {\n+            (Self::FullIso, _) => time.format(\"%Y-%m-%d %H:%M:%S.%f %z\").to_string(),\n+            (Self::LongIso, _) => time.format(\"%Y-%m-%d %H:%M\").to_string(),\n+            (Self::Iso, true) => time.format(\"%m-%d %H:%M\").to_string(),\n+            (Self::Iso, false) => time.format(\"%Y-%m-%d \").to_string(),\n+            // spell-checker:ignore (word) datetime\n+            //In this version of chrono translating can be done\n+            //The function is chrono::datetime::DateTime::format_localized\n+            //However it's currently still hard to get the current pure-rust-locale\n+            //So it's not yet implemented\n+            (Self::Locale, true) => time.format(\"%b %e %H:%M\").to_string(),\n+            (Self::Locale, false) => time.format(\"%b %e  %Y\").to_string(),\n+            (Self::Format(e), _) => custom_time_format(e, time),\n+        }\n+    }\n+}\n+\n fn parse_time_style(options: &clap::ArgMatches) -> Result<TimeStyle, LsError> {\n     let possible_time_styles = vec![\n         \"full-iso\".to_string(),\n@@ -3115,31 +3172,7 @@ fn get_time(md: &Metadata, config: &Config) -> Option<chrono::DateTime<chrono::L\n \n fn display_date(metadata: &Metadata, config: &Config) -> String {\n     match get_time(metadata, config) {\n-        Some(time) => {\n-            //Date is recent if from past 6 months\n-            //According to GNU a Gregorian year has 365.2425 * 24 * 60 * 60 == 31556952 seconds on the average.\n-            let recent = time + chrono::TimeDelta::try_seconds(31_556_952 / 2).unwrap()\n-                > chrono::Local::now();\n-\n-            match &config.time_style {\n-                TimeStyle::FullIso => time.format(\"%Y-%m-%d %H:%M:%S.%f %z\"),\n-                TimeStyle::LongIso => time.format(\"%Y-%m-%d %H:%M\"),\n-                TimeStyle::Iso => time.format(if recent { \"%m-%d %H:%M\" } else { \"%Y-%m-%d \" }),\n-                TimeStyle::Locale => {\n-                    let fmt = if recent { \"%b %e %H:%M\" } else { \"%b %e  %Y\" };\n-\n-                    // spell-checker:ignore (word) datetime\n-                    //In this version of chrono translating can be done\n-                    //The function is chrono::datetime::DateTime::format_localized\n-                    //However it's currently still hard to get the current pure-rust-locale\n-                    //So it's not yet implemented\n-\n-                    time.format(fmt)\n-                }\n-                TimeStyle::Format(e) => time.format(e),\n-            }\n-            .to_string()\n-        }\n+        Some(time) => config.time_style.format(time),\n         None => \"???\".into(),\n     }\n }\n", "test_patch": "diff --git a/tests/by-util/test_ls.rs b/tests/by-util/test_ls.rs\nindex 6ef7ac93a2e..715f18a1eaf 100644\n--- a/tests/by-util/test_ls.rs\n+++ b/tests/by-util/test_ls.rs\n@@ -5628,3 +5628,14 @@ fn test_non_unicode_names() {\n         .succeeds()\n         .stdout_is_bytes(b\"\\xC0.dir\\n\\xC0.file\\n\");\n }\n+\n+#[test]\n+fn test_time_style_timezone_name() {\n+    let re_custom_format = Regex::new(r\"[a-z-]* \\d* [\\w.]* [\\w.]* \\d* UTC f\\n\").unwrap();\n+    let (at, mut ucmd) = at_and_ucmd!();\n+    at.touch(\"f\");\n+    ucmd.env(\"TZ\", \"UTC0\")\n+        .args(&[\"-l\", \"--time-style=+%Z\"])\n+        .succeeds()\n+        .stdout_matches(&re_custom_format);\n+}\n", "problem_statement": "ls: --time-style with format string %Z gives timezone offset instead of name\nEnvironment: Ubuntu 20.04, uutils `main` branch (git commit 00d186606035876ad47afb82ad7d109a1868201b), GNU coreutils v8.30\n\nSteps to reproduce:\n```\nTZ=UTC0 touch -d '1970-07-08 09:10:11' f\nTZ=UTC0 ls -l --time-style=\"+%Z\" f\n```\n\nWhat happens now: uutils `ls` shows a numeric offset for the timezone\n```\n-rw-rw-r-- 1 jeffrey jeffrey 0 +00:00 f\n```\n\nWhat I expected to happen: GNU `ls` shows the timezone name\n```\n-rw-rw-r-- 1 jeffrey jeffrey 0 UTC f\n```\n\nNotes: This causes a failure in the GNU test file `tests/ls/time-style.sh`.\n\nWe use the `chrono` package for time formatting. It seems that they have an open issue about this here https://github.com/chronotope/chrono/issues/288\n", "hints_text": "Related to #5164", "created_at": "2025-01-18 14:38:19", "merge_commit_sha": "79f4b8976c2ceb9c401bf30ecbf029810d42e25d", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Build/stable (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']", "['Build (windows-latest, aarch64-pc-windows-msvc, feat_os_windows, use-cross, true)', '.github/workflows/CICD.yml']"], ["['Separate Builds (macos-latest)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_env, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_seq, false)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_wc, false)', '.github/workflows/fuzzing.yml']"], ["['Style/format (ubuntu-latest, feat_os_unix)', '.github/workflows/code-quality.yml']", "['Style and Lint (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']"], ["['Tests (ubuntu-24.04, unix)', '.github/workflows/freebsd.yml']", "['Run the fuzzers (fuzz_split, false)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_echo, true)', '.github/workflows/fuzzing.yml']", "['Binary sizes (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Build/stable (windows-latest, feat_os_windows)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (macos-latest, aarch64-apple-darwin, feat_os_macos)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, i686-unknown-linux-musl, feat_os_unix_musl, use-cross)', '.github/workflows/CICD.yml']"], ["['Tests/Toybox test suite (ubuntu-latest)', '.github/workflows/CICD.yml']", "['Build (ubuntu-latest, x86_64-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, i686-unknown-linux-gnu, feat_os_unix,test_risky_names, use-cross)', '.github/workflows/CICD.yml']", "['Run the fuzzers (fuzz_parse_size, true)', '.github/workflows/fuzzing.yml']"], ["['Run the fuzzers (fuzz_test, true)', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_expr, true)', '.github/workflows/fuzzing.yml']"], ["['Update/format (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']", "['Dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_seq_parse_number, true)', '.github/workflows/fuzzing.yml']", "['Test all features separately (macos-latest)', '.github/workflows/CICD.yml']"], ["['Run the fuzzers (fuzz_cksum, false)', '.github/workflows/fuzzing.yml']", "['Run GNU tests', '.github/workflows/GnuTests.yml']"], ["['Build the fuzzers', '.github/workflows/fuzzing.yml']", "['Run the fuzzers (fuzz_sort, false)', '.github/workflows/fuzzing.yml']"], ["['Update/dependencies (ubuntu-latest, feat_os_unix)', '.github/workflows/FixPR.yml']", "['Build (windows-latest, x86_64-pc-windows-msvc, feat_os_windows)', '.github/workflows/CICD.yml']"], ["['Build (ubuntu-latest, arm-unknown-linux-gnueabihf, feat_os_unix_gnueabihf, use-cross, true)', '.github/workflows/CICD.yml']", "['Build (windows-latest, x86_64-pc-windows-gnu, feat_os_windows)', '.github/workflows/CICD.yml']"]]}
{"repo": "kevinmehall/rust-peg", "instance_id": "kevinmehall__rust-peg-385", "base_commit": "fc7d27f0a5c52666f645bec6ec9be001f171b411", "patch": "diff --git a/peg-macros/grammar.rs b/peg-macros/grammar.rs\nindex f7debea..0ea43e8 100644\n--- a/peg-macros/grammar.rs\n+++ b/peg-macros/grammar.rs\n@@ -1388,7 +1388,7 @@ pub mod peg {\n                                                     ::peg::RuleResult::Failed => break,\n                                                 }\n                                             };\n-                                            let __step_res = match __parse_rust_type(\n+                                            let __step_res = match __parse_rust_ty_param_bound(\n                                                 __input,\n                                                 __state,\n                                                 __err_state,\n@@ -1456,19 +1456,24 @@ pub mod peg {\n                                                                 ::peg::RuleResult::Failed => break,\n                                                             }\n                                                         };\n-                                                        let __step_res = match __parse_rust_type(\n-                                                            __input,\n-                                                            __state,\n-                                                            __err_state,\n-                                                            __pos,\n-                                                        ) {\n-                                                            ::peg::RuleResult::Matched(pos, _) => {\n-                                                                ::peg::RuleResult::Matched(pos, ())\n-                                                            }\n-                                                            ::peg::RuleResult::Failed => {\n-                                                                ::peg::RuleResult::Failed\n-                                                            }\n-                                                        };\n+                                                        let __step_res =\n+                                                            match __parse_rust_ty_param_bound(\n+                                                                __input,\n+                                                                __state,\n+                                                                __err_state,\n+                                                                __pos,\n+                                                            ) {\n+                                                                ::peg::RuleResult::Matched(\n+                                                                    pos,\n+                                                                    _,\n+                                                                ) => ::peg::RuleResult::Matched(\n+                                                                    pos,\n+                                                                    (),\n+                                                                ),\n+                                                                ::peg::RuleResult::Failed => {\n+                                                                    ::peg::RuleResult::Failed\n+                                                                }\n+                                                            };\n                                                         match __step_res {\n                                                             ::peg::RuleResult::Matched(\n                                                                 __newpos,\n@@ -2036,45 +2041,24 @@ pub mod peg {\n                                     ::peg::RuleResult::Matched(__pos, __value)\n                                 }\n                                 ::peg::RuleResult::Failed => {\n-                                    let __seq_res =\n-                                        match match ::peg::ParseLiteral::parse_string_literal(\n-                                            __input, __pos, \"for\",\n-                                        ) {\n-                                            ::peg::RuleResult::Matched(__pos, __val) => {\n-                                                let __seq_res = match __parse_rust_ty_params(\n-                                                    __input,\n-                                                    __state,\n-                                                    __err_state,\n-                                                    __pos,\n-                                                ) {\n-                                                    ::peg::RuleResult::Matched(pos, _) => {\n-                                                        ::peg::RuleResult::Matched(pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                };\n-                                                match __seq_res {\n-                                                    ::peg::RuleResult::Matched(__pos, _) => {\n-                                                        ::peg::RuleResult::Matched(__pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                }\n-                                            }\n-                                            ::peg::RuleResult::Failed => {\n-                                                __err_state.mark_failure(__pos, \"\\\"for\\\"\");\n-                                                ::peg::RuleResult::Failed\n-                                            }\n-                                        } {\n-                                            ::peg::RuleResult::Matched(__newpos, _) => {\n-                                                ::peg::RuleResult::Matched(__newpos, ())\n-                                            }\n-                                            ::peg::RuleResult::Failed => {\n-                                                ::peg::RuleResult::Matched(__pos, ())\n-                                            }\n-                                        };\n+                                    let __seq_res = match match __parse_rust_for_lifetimes(\n+                                        __input,\n+                                        __state,\n+                                        __err_state,\n+                                        __pos,\n+                                    ) {\n+                                        ::peg::RuleResult::Matched(pos, _) => {\n+                                            ::peg::RuleResult::Matched(pos, ())\n+                                        }\n+                                        ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                    } {\n+                                        ::peg::RuleResult::Matched(__newpos, _) => {\n+                                            ::peg::RuleResult::Matched(__newpos, ())\n+                                        }\n+                                        ::peg::RuleResult::Failed => {\n+                                            ::peg::RuleResult::Matched(__pos, ())\n+                                        }\n+                                    };\n                                     match __seq_res {\n                                         ::peg::RuleResult::Matched(__pos, _) => {\n                                             let __seq_res = match __parse_rust_type(\n@@ -2112,10 +2096,7 @@ pub mod peg {\n                                                                         let __sep_res = match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"+\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"+\\\"\") ; :: peg :: RuleResult :: Failed } } ;\n                                                                         match __sep_res { :: peg :: RuleResult :: Matched (__newpos , _) => { __newpos } , :: peg :: RuleResult :: Failed => break , }\n                                                                     };\n-                                                                    let __step_res = {\n-                                                                        let __choice_res = match __parse_LIFETIME (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n-                                                                        match __choice_res { :: peg :: RuleResult :: Matched (__pos , __value) => :: peg :: RuleResult :: Matched (__pos , __value) , :: peg :: RuleResult :: Failed => { let __seq_res = match match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"?\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"?\\\"\") ; :: peg :: RuleResult :: Failed } } { :: peg :: RuleResult :: Matched (__newpos , _) => { :: peg :: RuleResult :: Matched (__newpos , ()) } , :: peg :: RuleResult :: Failed => { :: peg :: RuleResult :: Matched (__pos , ()) } , } ; match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { { let __seq_res = match __parse_rust_ty_path (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ; match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { :: peg :: RuleResult :: Matched (__pos , ()) } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } } } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } } }\n-                                                                    };\n+                                                                    let __step_res = match __parse_rust_ty_param_bound (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n                                                                     match __step_res { :: peg :: RuleResult :: Matched (__newpos , __value) => { __repeat_pos = __newpos ; __repeat_value . push (__value) ; } , :: peg :: RuleResult :: Failed => { break ; } }\n                                                                 }\n                                                                 if __repeat_value.len() >= 1 {\n@@ -2347,52 +2328,17 @@ pub mod peg {\n                                                     ::peg::RuleResult::Failed => break,\n                                                 }\n                                             };\n-                                            let __step_res = {\n-                                                let __choice_res = match __parse_LIFETIME(\n-                                                    __input,\n-                                                    __state,\n-                                                    __err_state,\n-                                                    __pos,\n-                                                ) {\n-                                                    ::peg::RuleResult::Matched(pos, _) => {\n-                                                        ::peg::RuleResult::Matched(pos, ())\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        ::peg::RuleResult::Failed\n-                                                    }\n-                                                };\n-                                                match __choice_res {\n-                                                    ::peg::RuleResult::Matched(__pos, __value) => {\n-                                                        ::peg::RuleResult::Matched(__pos, __value)\n-                                                    }\n-                                                    ::peg::RuleResult::Failed => {\n-                                                        let __seq_res = match match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \"?\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , __val) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\"?\\\"\") ; :: peg :: RuleResult :: Failed } } { :: peg :: RuleResult :: Matched (__newpos , _) => { :: peg :: RuleResult :: Matched (__newpos , ()) } , :: peg :: RuleResult :: Failed => { :: peg :: RuleResult :: Matched (__pos , ()) } , } ;\n-                                                        match __seq_res {\n-                                                            ::peg::RuleResult::Matched(\n-                                                                __pos,\n-                                                                _,\n-                                                            ) => {\n-                                                                let __seq_res = match __parse_rust_ty_path (__input , __state , __err_state , __pos) { :: peg :: RuleResult :: Matched (pos , _) => :: peg :: RuleResult :: Matched (pos , ()) , :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , } ;\n-                                                                match __seq_res {\n-                                                                    ::peg::RuleResult::Matched(\n-                                                                        __pos,\n-                                                                        _,\n-                                                                    ) => {\n-                                                                        ::peg::RuleResult::Matched(\n-                                                                            __pos,\n-                                                                            (),\n-                                                                        )\n-                                                                    }\n-                                                                    ::peg::RuleResult::Failed => {\n-                                                                        ::peg::RuleResult::Failed\n-                                                                    }\n-                                                                }\n-                                                            }\n-                                                            ::peg::RuleResult::Failed => {\n-                                                                ::peg::RuleResult::Failed\n-                                                            }\n-                                                        }\n-                                                    }\n+                                            let __step_res = match __parse_rust_ty_param_bound(\n+                                                __input,\n+                                                __state,\n+                                                __err_state,\n+                                                __pos,\n+                                            ) {\n+                                                ::peg::RuleResult::Matched(pos, _) => {\n+                                                    ::peg::RuleResult::Matched(pos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n                                                 }\n                                             };\n                                             match __step_res {\n@@ -2441,6 +2387,194 @@ pub mod peg {\n             }\n         }\n     }\n+    fn __parse_rust_for_lifetimes<'input>(\n+        __input: &'input Input,\n+        __state: &mut ParseState<'input>,\n+        __err_state: &mut ::peg::error::ErrorState,\n+        __pos: usize,\n+    ) -> ::peg::RuleResult<()> {\n+        #![allow(non_snake_case, unused, clippy::redundant_closure_call)]\n+        match ::peg::ParseLiteral::parse_string_literal(__input, __pos, \"for\") {\n+            ::peg::RuleResult::Matched(__pos, __val) => {\n+                let __seq_res = match __parse_rust_ty_params(__input, __state, __err_state, __pos) {\n+                    ::peg::RuleResult::Matched(pos, _) => ::peg::RuleResult::Matched(pos, ()),\n+                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                };\n+                match __seq_res {\n+                    ::peg::RuleResult::Matched(__pos, _) => ::peg::RuleResult::Matched(__pos, ()),\n+                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                }\n+            }\n+            ::peg::RuleResult::Failed => {\n+                __err_state.mark_failure(__pos, \"\\\"for\\\"\");\n+                ::peg::RuleResult::Failed\n+            }\n+        }\n+    }\n+    fn __parse_rust_ty_param_bound<'input>(\n+        __input: &'input Input,\n+        __state: &mut ParseState<'input>,\n+        __err_state: &mut ::peg::error::ErrorState,\n+        __pos: usize,\n+    ) -> ::peg::RuleResult<()> {\n+        #![allow(non_snake_case, unused, clippy::redundant_closure_call)]\n+        {\n+            let __choice_res = match __parse_LIFETIME(__input, __state, __err_state, __pos) {\n+                ::peg::RuleResult::Matched(pos, _) => ::peg::RuleResult::Matched(pos, ()),\n+                ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+            };\n+            match __choice_res {\n+                ::peg::RuleResult::Matched(__pos, __value) => {\n+                    ::peg::RuleResult::Matched(__pos, __value)\n+                }\n+                ::peg::RuleResult::Failed => {\n+                    let __choice_res = {\n+                        let __seq_res = match match ::peg::ParseLiteral::parse_string_literal(\n+                            __input, __pos, \"?\",\n+                        ) {\n+                            ::peg::RuleResult::Matched(__pos, __val) => {\n+                                ::peg::RuleResult::Matched(__pos, __val)\n+                            }\n+                            ::peg::RuleResult::Failed => {\n+                                __err_state.mark_failure(__pos, \"\\\"?\\\"\");\n+                                ::peg::RuleResult::Failed\n+                            }\n+                        } {\n+                            ::peg::RuleResult::Matched(__newpos, _) => {\n+                                ::peg::RuleResult::Matched(__newpos, ())\n+                            }\n+                            ::peg::RuleResult::Failed => ::peg::RuleResult::Matched(__pos, ()),\n+                        };\n+                        match __seq_res {\n+                            ::peg::RuleResult::Matched(__pos, _) => {\n+                                let __seq_res = match match __parse_rust_for_lifetimes(\n+                                    __input,\n+                                    __state,\n+                                    __err_state,\n+                                    __pos,\n+                                ) {\n+                                    ::peg::RuleResult::Matched(pos, _) => {\n+                                        ::peg::RuleResult::Matched(pos, ())\n+                                    }\n+                                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                } {\n+                                    ::peg::RuleResult::Matched(__newpos, _) => {\n+                                        ::peg::RuleResult::Matched(__newpos, ())\n+                                    }\n+                                    ::peg::RuleResult::Failed => {\n+                                        ::peg::RuleResult::Matched(__pos, ())\n+                                    }\n+                                };\n+                                match __seq_res {\n+                                    ::peg::RuleResult::Matched(__pos, _) => {\n+                                        let __seq_res = match __parse_rust_ty_path(\n+                                            __input,\n+                                            __state,\n+                                            __err_state,\n+                                            __pos,\n+                                        ) {\n+                                            ::peg::RuleResult::Matched(pos, _) => {\n+                                                ::peg::RuleResult::Matched(pos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                        };\n+                                        match __seq_res {\n+                                            ::peg::RuleResult::Matched(__pos, _) => {\n+                                                ::peg::RuleResult::Matched(__pos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                        }\n+                                    }\n+                                    ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                }\n+                            }\n+                            ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                        }\n+                    };\n+                    match __choice_res {\n+                        ::peg::RuleResult::Matched(__pos, __value) => {\n+                            ::peg::RuleResult::Matched(__pos, __value)\n+                        }\n+                        ::peg::RuleResult::Failed => {\n+                            match ::peg::ParseLiteral::parse_string_literal(__input, __pos, \"(\") {\n+                                ::peg::RuleResult::Matched(__pos, __val) => {\n+                                    let __seq_res =\n+                                        match match ::peg::ParseLiteral::parse_string_literal(\n+                                            __input, __pos, \"?\",\n+                                        ) {\n+                                            ::peg::RuleResult::Matched(__pos, __val) => {\n+                                                ::peg::RuleResult::Matched(__pos, __val)\n+                                            }\n+                                            ::peg::RuleResult::Failed => {\n+                                                __err_state.mark_failure(__pos, \"\\\"?\\\"\");\n+                                                ::peg::RuleResult::Failed\n+                                            }\n+                                        } {\n+                                            ::peg::RuleResult::Matched(__newpos, _) => {\n+                                                ::peg::RuleResult::Matched(__newpos, ())\n+                                            }\n+                                            ::peg::RuleResult::Failed => {\n+                                                ::peg::RuleResult::Matched(__pos, ())\n+                                            }\n+                                        };\n+                                    match __seq_res {\n+                                        ::peg::RuleResult::Matched(__pos, _) => {\n+                                            let __seq_res = match match __parse_rust_for_lifetimes(\n+                                                __input,\n+                                                __state,\n+                                                __err_state,\n+                                                __pos,\n+                                            ) {\n+                                                ::peg::RuleResult::Matched(pos, _) => {\n+                                                    ::peg::RuleResult::Matched(pos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n+                                                }\n+                                            } {\n+                                                ::peg::RuleResult::Matched(__newpos, _) => {\n+                                                    ::peg::RuleResult::Matched(__newpos, ())\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Matched(__pos, ())\n+                                                }\n+                                            };\n+                                            match __seq_res {\n+                                                ::peg::RuleResult::Matched(__pos, _) => {\n+                                                    let __seq_res = match __parse_rust_ty_path(\n+                                                        __input,\n+                                                        __state,\n+                                                        __err_state,\n+                                                        __pos,\n+                                                    ) {\n+                                                        ::peg::RuleResult::Matched(pos, _) => {\n+                                                            ::peg::RuleResult::Matched(pos, ())\n+                                                        }\n+                                                        ::peg::RuleResult::Failed => {\n+                                                            ::peg::RuleResult::Failed\n+                                                        }\n+                                                    };\n+                                                    match __seq_res { :: peg :: RuleResult :: Matched (__pos , _) => { match :: peg :: ParseLiteral :: parse_string_literal (__input , __pos , \")\") { :: peg :: RuleResult :: Matched (__pos , __val) => { :: peg :: RuleResult :: Matched (__pos , ()) } :: peg :: RuleResult :: Failed => { __err_state . mark_failure (__pos , \"\\\")\\\"\") ; :: peg :: RuleResult :: Failed } } } :: peg :: RuleResult :: Failed => :: peg :: RuleResult :: Failed , }\n+                                                }\n+                                                ::peg::RuleResult::Failed => {\n+                                                    ::peg::RuleResult::Failed\n+                                                }\n+                                            }\n+                                        }\n+                                        ::peg::RuleResult::Failed => ::peg::RuleResult::Failed,\n+                                    }\n+                                }\n+                                ::peg::RuleResult::Failed => {\n+                                    __err_state.mark_failure(__pos, \"\\\"(\\\"\");\n+                                    ::peg::RuleResult::Failed\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n     fn __parse_expression<'input>(\n         __input: &'input Input,\n         __state: &mut ParseState<'input>,\ndiff --git a/peg-macros/grammar.rustpeg b/peg-macros/grammar.rustpeg\nindex 7fdafdb..a534d04 100644\n--- a/peg-macros/grammar.rustpeg\n+++ b/peg-macros/grammar.rustpeg\n@@ -59,8 +59,8 @@ rule rust_path()\n rule rust_type()\n     = BRACKET_GROUP()\n     / \"&\" LIFETIME()? \"mut\"? rust_type()\n-    / \"dyn\" rust_type() ++ \"+\"\n-    / \"impl\" rust_type() ++ \"+\"\n+    / \"dyn\" rust_ty_param_bound() ++ \"+\"\n+    / \"impl\" rust_ty_param_bound() ++ \"+\"\n     / \"(\" (rust_type() ++ \",\" \",\"?)? \")\"\n     / (\"<\" rust_type() (\"as\" rust_ty_path())? \">\")? rust_ty_path()\n \n@@ -73,12 +73,20 @@ rule rust_ty_params() -> Vec<TokenStream>\n rule rust_where_clause()\n     = \"where\" (\n         LIFETIME() (\":\" LIFETIME() ++ \"+\")?\n-        / (\"for\" rust_ty_params())? rust_type() \":\" (LIFETIME() / \"?\"? rust_ty_path()) ++ \"+\"\n+        / rust_for_lifetimes()? rust_type() \":\" rust_ty_param_bound() ++ \"+\"\n     ) ** \",\" \",\"?\n \n rule rust_generic_param()\n     = LIFETIME() (\":\" LIFETIME() ++ \"+\")?\n-    / IDENT() (\":\"(LIFETIME() / \"?\"? rust_ty_path()) ++ \"+\")?\n+    / IDENT() (\":\" rust_ty_param_bound() ++ \"+\")?\n+\n+rule rust_for_lifetimes()\n+    = \"for\" rust_ty_params()\n+\n+rule rust_ty_param_bound()\n+    = LIFETIME() \n+    / \"?\"? rust_for_lifetimes()? rust_ty_path()\n+    / \"(\" \"?\"? rust_for_lifetimes()? rust_ty_path() \")\"\n \n rule expression() -> SpannedExpr = choice()\n \n", "test_patch": "diff --git a/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs b/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs\nnew file mode 100644\nindex 0000000..8116666\n--- /dev/null\n+++ b/tests/run-pass/assembly_ast_dyn_type_param_bounds.rs\n@@ -0,0 +1,122 @@\n+extern crate peg;\n+use peg::parser;\n+\n+// C++ in Rust\n+trait Operation<'a>: std::fmt::Debug {}\n+trait Operand<'a>: std::fmt::Debug + AsDynOperand<'a> {}\n+trait Location<'a>: Operand<'a> {}\n+impl<'a, T: ?Sized + Location<'a>> Operand<'a> for T {}\n+\n+// Thanks to quinedot for their comprehensive write-up on dyn Traits.\n+// https://quinedot.github.io/rust-learning/dyn-trait-combining.html#manual-supertrait-upcasting\n+trait AsDynOperand<'a> {\n+    fn as_dyn_operand(self: Box<Self>) -> Box<dyn Operand<'a> + 'a>;\n+}\n+\n+impl<'a, T: /* Sized + */ Operand<'a> + 'a> AsDynOperand<'a> for T {\n+    fn as_dyn_operand(self: Box<Self>) -> Box<dyn Operand<'a> + 'a> {\n+        self\n+    }\n+}\n+\n+\n+\n+#[derive(Debug)]\n+pub struct Program<'a>(Vec<Box<dyn Operation<'a> + 'a>>);\n+\n+#[derive(Debug)]\n+struct Add<'a> {\n+    result: Box<dyn Location<'a> + 'a>,\n+    lhs: Box<dyn Operand<'a> + 'a>,\n+    rhs: Box<dyn Operand<'a> + 'a>,\n+}\n+impl<'a> Operation<'a> for Add<'a> {}\n+\n+#[derive(Debug)]\n+struct Sub<'a> {\n+    result: Box<dyn Location<'a> + 'a>,\n+    lhs: Box<dyn Operand<'a> + 'a>,\n+    rhs: Box<dyn Operand<'a> + 'a>,\n+}\n+impl<'a> Operation<'a> for Sub<'a> {}\n+\n+#[derive(Debug)]\n+struct Register<'a>(&'a str);\n+impl<'a> Location<'a> for Register<'a> {}\n+\n+#[derive(Debug)]\n+struct Global<'a>(&'a str);\n+impl<'a> Location<'a> for Global<'a> {}\n+\n+#[derive(Debug)]\n+struct Literal(i32);\n+impl<'a> Operand<'a> for Literal {}\n+\n+parser!{\n+grammar assembly() for str {\n+    pub rule program() -> Program<'input>\n+        = op:operation() ** \"\\n\" { Program(op) }\n+\n+    rule _ = [' ']*\n+\n+    rule operation() -> Box<dyn Operation<'input> + 'input>\n+        = a:add() {a} / s:sub() {s}\n+\n+    rule add() -> Box<Add<'input>>\n+        = result:location() _ \"=\" _ \"add\" _ lhs:operand() _ rhs:operand() { Box::new(Add{ result, lhs, rhs }) }\n+\n+    rule sub() -> Box<Sub<'input>>\n+        = result:location() _ \"=\" _ \"sub\" _ lhs:operand() _ rhs:operand() { Box::new(Sub{ result, lhs, rhs }) }\n+\n+    rule location() -> Box<dyn Location<'input> + 'input>\n+        = r:register() {r} / g:global() {g}\n+\n+    rule register() -> Box<Register<'input>>\n+        = \"%\" _ id:identifier() { Box::new(Register(id)) }\n+\n+    rule global() -> Box<Global<'input>>\n+        = \"@\" _ id:identifier() { Box::new(Global(id)) }\n+\n+    rule identifier() -> &'input str\n+        = $(['a'..='z' | 'A'..='Z']+)\n+\n+    rule operand() -> Box<dyn Operand<'input> + 'input>\n+        = l:location() {l.as_dyn_operand()} / l:literal() {Box::new(l)}\n+    \n+    rule literal() -> Literal\n+        = n:$(['0'..='9']+) {?\n+            let n = n.parse::<i32>().map_err(|_| \"invalid int literal\")?;\n+            Ok(Literal(n))\n+        }\n+\n+}}\n+\n+fn main() {\n+    let parsed = assembly::program(\"%apple = add 1 @g\n+@b = add 2 %a\n+%c = sub 82 @b\n+@dog = sub @b 12\").unwrap();\n+    let expected = Program(vec![\n+        Box::new(Add{\n+            result: Box::new(Register(\"apple\")),\n+            lhs: Box::new(Literal(1)),\n+            rhs: Box::new(Global(\"g\"))\n+        }),\n+        Box::new(Add{\n+            result: Box::new(Global(\"b\")),\n+            lhs: Box::new(Literal(2)),\n+            rhs: Box::new(Register(\"a\"))\n+        }),\n+        Box::new(Sub{\n+            result: Box::new(Register(\"c\")),\n+            lhs: Box::new(Literal(82)),\n+            rhs: Box::new(Global(\"b\"))\n+        }),\n+        Box::new(Sub{\n+            result: Box::new(Global(\"dog\")),\n+            lhs: Box::new(Global(\"b\")),\n+            rhs: Box::new(Literal(12))\n+        }),\n+    ]);\n+    assert_eq!(format!(\"{parsed:?}\"), format!(\"{expected:?}\"));\n+}\n", "problem_statement": "Bounds on trait objects and impl traits not parsing\nThe following will not parse:\r\n```rust\r\nrule operand() -> Box<dyn Operand + 'input>\r\n```\r\n\r\nThe error:\r\n```\r\nerror: expected one of \"&\", \"(\", \"::\", \"<\", \"dyn\", \"impl\"\r\n  --> src\\parse.rs:41:45\r\n   |\r\n41 |         rule operand() -> Box<dyn Operand + 'input>\r\n   |                                             ^^^^^^\r\n```\r\n\r\nI believe the issue is in the `rust_type()` rule of grammar.rustpeg:\r\n\r\n```\r\nrule rust_type()\r\n    = BRACKET_GROUP()\r\n    / \"&\" LIFETIME()? \"mut\"? rust_type()\r\n    / \"dyn\" rust_type() ++ \"+\" // here\r\n    / \"impl\" rust_type() ++ \"+\" // and here\r\n    / \"(\" (rust_type() ++ \",\" \",\"?)? \")\"\r\n    / (\"<\" rust_type() (\"as\" rust_ty_path())? \">\")? rust_ty_path()\r\n```\r\n\r\n`rust_type()`s shouldn't follow \"dyn\" and \"impl\"; instead it should be something that parses type parameter bounds (which include rust types AND lifetimes) ([ref](https://doc.rust-lang.org/reference/types/trait-object.html)) ([ref](https://doc.rust-lang.org/reference/types/impl-trait.html)). I think the rules to parse that might already be written as part of `rust_where_clause()` and could perhaps be factored out, but I don't have time at the moment to verify that.\n", "hints_text": "", "created_at": "2024-10-09 21:02:40", "merge_commit_sha": "4928a1e6748b981ce1d5bf72c2d35d1516d9b4ec", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Rust 1.68.0', '.github/workflows/rust.yml']", "['Rust stable', '.github/workflows/rust.yml']"]]}
{"repo": "thomas-mauran/chess-tui", "instance_id": "thomas-mauran__chess-tui-84", "base_commit": "d07506d99d5db756c3137565ded7338d1412a909", "patch": "diff --git a/src/board.rs b/src/board.rs\nindex 050fd84..edf39e1 100644\n--- a/src/board.rs\n+++ b/src/board.rs\n@@ -132,6 +132,8 @@ pub struct Board {\n     pub is_game_against_bot: bool,\n     // the display mode\n     pub display_mode: DisplayMode,\n+    /// Used to indicate if a bot move is following\n+    pub bot_will_move: bool,\n     // coordinates of the interactable part of the screen (either normal chess board or promotion screen)\n     pub top_x: u16,\n     pub top_y: u16,\n@@ -215,6 +217,7 @@ impl Default for Board {\n             engine: None,\n             is_game_against_bot: false,\n             display_mode: DisplayMode::DEFAULT,\n+            bot_will_move: false,\n             top_x: 0,\n             top_y: 0,\n             width: 0,\n@@ -246,6 +249,7 @@ impl Board {\n             engine: None,\n             is_game_against_bot: false,\n             display_mode: DisplayMode::DEFAULT,\n+            bot_will_move: false,\n             top_x: 0,\n             top_y: 0,\n             width: 0,\n@@ -452,13 +456,13 @@ impl Board {\n                     }\n                     // If we play against a bot we will play his move and switch the player turn again\n                     if self.is_game_against_bot {\n+                        // do this in background\n                         self.is_promotion = self.is_latest_move_promotion();\n                         if !self.is_promotion {\n                             self.is_checkmate = self.is_checkmate();\n                             self.is_promotion = self.is_latest_move_promotion();\n                             if !self.is_checkmate {\n-                                self.bot_move();\n-                                self.switch_player_turn();\n+                                self.bot_will_move = true;\n                             }\n                         }\n                     }\n@@ -1169,7 +1173,6 @@ impl Board {\n \n             pieces.push_str(&format!(\"{utf_icon_white} \"));\n         }\n-\n         let white_material_paragraph = Paragraph::new(pieces)\n             .alignment(Alignment::Center)\n             .add_modifier(Modifier::BOLD);\n@@ -1180,7 +1183,6 @@ impl Board {\n             .direction(Direction::Vertical)\n             .constraints([Constraint::Length(height - 1), Constraint::Length(1)].as_ref())\n             .split(area);\n-\n         frame.render_widget(white_block.clone(), right_panel_layout[0]);\n         frame.render_widget(\n             white_material_paragraph,\ndiff --git a/src/main.rs b/src/main.rs\nindex 0feef37..84d2770 100644\n--- a/src/main.rs\n+++ b/src/main.rs\n@@ -77,6 +77,15 @@ fn main() -> AppResult<()> {\n             Event::Mouse(mouse_event) => handle_mouse_events(mouse_event, &mut app)?,\n             Event::Resize(_, _) => {}\n         }\n+        if app.board.bot_will_move {\n+            app.board.bot_move();\n+            app.board.switch_player_turn();\n+            app.board.bot_will_move = false;\n+            // need to be centralised\n+            app.board.is_checkmate = app.board.is_checkmate();\n+            app.board.is_draw = app.board.is_draw();\n+            tui.draw(&mut app)?;\n+        }\n     }\n \n     // Exit the user interface.\n", "test_patch": "", "problem_statement": "Bot move freezes player move.\n## Description\r\nThis bug occurs in a game agains a bot. After the player confirms its move the piece is not drawn on the board and the game waits for the bot's move. Only after the bot's move is received both the player and the bots moves are displayed simultaneously as opposed to being drawn in turn.\r\n\r\nThis can be observed for bots that take some time to compute the next move.\r\n\r\n## To Reproduce\r\nSet up a game against a bot. Make sure that the bot is not extremely optimised (as is [Stockfish](https://stockfishchess.org), for example). Make a move (as a player).\r\n\r\n## Expected behaviour\r\n\r\nThe game will freeze until the bot move is computed. Until that the player move won't be displayed.\r\n\r\n## Environment\r\n\r\n- OS: MacOS\r\n- Terminal Emulator: Terminal\r\n- Font: ASCII\r\n- Crate version: 1.2.1\r\n- Backend: termion\r\n\n", "hints_text": "", "created_at": "2024-11-10 13:45:44", "merge_commit_sha": "433709e766955939d65a24cc068cfab4d0983247", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Rust project - latest (beta)', '.github/workflows/build_and_test.yml']", "['Rust project - latest (stable)', '.github/workflows/build_and_test.yml']"]]}
{"repo": "ashvardanian/StringZilla", "instance_id": "ashvardanian__StringZilla-202", "base_commit": "645539b468f3c2902061425684d9b002c43a14f7", "patch": "diff --git a/include/stringzilla/stringzilla.hpp b/include/stringzilla/stringzilla.hpp\nindex a80da804..43869f08 100644\n--- a/include/stringzilla/stringzilla.hpp\n+++ b/include/stringzilla/stringzilla.hpp\n@@ -283,7 +283,7 @@ class basic_charset {\n     template <std::size_t count_characters>\n     explicit basic_charset(char_type const (&chars)[count_characters]) noexcept : basic_charset() {\n         static_assert(count_characters > 0, \"Character array cannot be empty\");\n-        for (std::size_t i = 0; i < count_characters - 1; ++i) { // count_characters - 1 to exclude the null terminator\n+        for (std::size_t i = 0; i != count_characters; ++i) {\n             char_type c = chars[i];\n             bitset_._u64s[sz_bitcast(sz_u8_t, c) >> 6] |= (1ull << (sz_bitcast(sz_u8_t, c) & 63u));\n         }\n@@ -292,7 +292,7 @@ class basic_charset {\n     template <std::size_t count_characters>\n     explicit basic_charset(std::array<char_type, count_characters> const &chars) noexcept : basic_charset() {\n         static_assert(count_characters > 0, \"Character array cannot be empty\");\n-        for (std::size_t i = 0; i < count_characters - 1; ++i) { // count_characters - 1 to exclude the null terminator\n+        for (std::size_t i = 0; i != count_characters; ++i) {\n             char_type c = chars[i];\n             bitset_._u64s[sz_bitcast(sz_u8_t, c) >> 6] |= (1ull << (sz_bitcast(sz_u8_t, c) & 63u));\n         }\n", "test_patch": "diff --git a/scripts/test.cpp b/scripts/test.cpp\nindex eecc97f0..72379f78 100644\n--- a/scripts/test.cpp\n+++ b/scripts/test.cpp\n@@ -137,6 +137,49 @@ static void test_arithmetical_utilities() {\n                    (static_cast<sz_u8_t>(number) / static_cast<sz_u8_t>(divisor)));\n }\n \n+/**\n+ *  @brief  Tests various ASCII-based methods (e.g., `is_alpha`, `is_digit`)\n+ *          provided by `sz::string` and `sz::string_view`.\n+ */\n+template <typename string_type>\n+static void test_ascii_utilities() {\n+\n+    using str = string_type;\n+\n+    assert(!str(\"\").is_alpha());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\").is_alpha());\n+    assert(!str(\"abc9\").is_alpha());\n+\n+    assert(!str(\"\").is_alnum());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\").is_alnum());\n+    assert(!str(\"abc!\").is_alnum());\n+\n+    assert(str(\"\").is_ascii());\n+    assert(str(\"\\x00x7F\").is_ascii());\n+    assert(!str(\"abc123\ud83d\udd25\").is_ascii());\n+\n+    assert(!str(\"\").is_digit());\n+    assert(str(\"0123456789\").is_digit());\n+    assert(!str(\"012a\").is_digit());\n+\n+    assert(!str(\"\").is_lower());\n+    assert(str(\"abcdefghijklmnopqrstuvwxyz\").is_lower());\n+    assert(!str(\"abcA\").is_lower());\n+    assert(!str(\"abc\\n\").is_lower());\n+\n+    assert(!str(\"\").is_space());\n+    assert(str(\" \\t\\n\\r\\f\\v\").is_space());\n+    assert(!str(\" \\t\\r\\na\").is_space());\n+\n+    assert(!str(\"\").is_upper());\n+    assert(str(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\").is_upper());\n+    assert(!str(\"ABCa\").is_upper());\n+\n+    assert(str(\"\").is_printable());\n+    assert(str(\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!@#$%^&*()_+\").is_printable());\n+    assert(!str(\"012\ud83d\udd25\").is_printable());\n+}\n+\n inline void expect_equality(char const *a, char const *b, std::size_t size) {\n     if (std::memcmp(a, b, size) == 0) return;\n     std::size_t mismatch_position = 0;\n@@ -1583,6 +1626,8 @@ int main(int argc, char const **argv) {\n \n     // Basic utilities\n     test_arithmetical_utilities();\n+    test_ascii_utilities<sz::string>();\n+    test_ascii_utilities<sz::string_view>();\n     test_memory_utilities();\n     test_replacements();\n \n", "problem_statement": "Bug: Last elements in `basic_charset` initialization are discarded\n### Describe the bug\r\n\r\nWhen initializing a `basic_charset` with character arrays such as those returned by `digits()` and others, the last element of the array is incorrectly discarded. \r\n\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L192-L195\r\n\r\nThis is due to the use of `count_characters - 1` in the loop that populates the `basic_charset` bitset. \r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L283-L289\r\n\r\nWhile this prevents including a null terminator for string literals like `sz::char_set(\"x\")`, it causes incorrect behavior when handling character arrays that do not have a null terminator, resulting in the exclusion of the final character.\r\n\r\nI believe this local block is the full extent of the affected code.\r\n\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L330-L341\r\n\r\n### Steps to reproduce\r\n\r\n```cpp\r\n#include \"stringzilla/stringzilla.hpp\"\r\n\r\nnamespace sz = ashvardanian::stringzilla;\r\n\r\nint main() {\r\n        sz::string haystack = \"239\";\r\n\r\n        // Test with null-terminated string\r\n        assert(haystack.contains_only(sz::char_set(\"0123456789\")));\r\n        // Passes: null terminator is correctly discarded\r\n\r\n        // Test with initializer list\r\n        static std::initializer_list all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n        assert (haystack.contains_only(sz::char_set {all}));\r\n        // Passes: constructor for initializer list is called\r\n\r\n        // Test with carray\r\n        sz::carray<10> all_digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n        assert(haystack.contains_only(sz::char_set {all_digits}));\r\n        assert(haystack.is_digit());\r\n        // Fails: '9' is incorrectly discarded\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo asserts\r\n\r\n### StringZilla version\r\n\r\nv3.11.0\r\n\r\n### Operating System\r\n\r\nUbuntu 22.04.5\r\n\r\n### Hardware architecture\r\n\r\nx86\r\n\r\n### Which interface are you using?\r\n\r\nC++ bindings\r\n\r\n### Contact Details\r\n\r\n_No response_\r\n\r\n### Are you open to being tagged as a contributor?\r\n\r\n- [X] I am open to being mentioned in the project `.git` history as a contributor\r\n\r\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's Code of Conduct\n", "hints_text": "I will be able to write fix and add tests tomorrow\r\n\r\n**Proposed Fix**\r\n\r\n1. **Using `carray` with Null Terminator**\r\n\r\n```cpp\r\ninline carray<11> const &digits() noexcept { \r\n    static carray<11> const all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\0'}; \r\n    return all;\r\n}\r\n```\r\n\r\n2. **Using `std::initializer_list<char>`**\r\n\r\n```cpp\r\ninline auto const &digits() noexcept {\r\n    static std::initializer_list all = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'};\r\n    return all;\r\n}\r\n```\r\n\r\nThen the constructor for the initializer list will be called here:\r\nhttps://github.com/ashvardanian/StringZilla/blob/152ed046701aceae2c3d4c995b0b661a9aa06d95/include/stringzilla/stringzilla.hpp#L335-L335\r\n\r\n\nThank you @alexbarev! Let's start with tests, and later merge a patch.", "created_at": "2024-12-07 18:17:07", "merge_commit_sha": "002e433c0265f4bb7628988f1b6b29d45ba53e03", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['Update Version', '.github/workflows/prerelease.yml']", "['Swift on Linux', '.github/workflows/prerelease.yml']"], ["['Ubuntu (Clang 16)', '.github/workflows/prerelease.yml']", "['Cross Compilation (amd64, x86_64-linux-gnu)', '.github/workflows/prerelease.yml']"], ["['Windows', '.github/workflows/prerelease.yml']", "['Alpine Linux', '.github/workflows/prerelease.yml']"]]}
{"repo": "quickwit-oss/tantivy", "instance_id": "quickwit-oss__tantivy-2557", "base_commit": "6e02c5cb258b784b00d5769c08cc5ddacc258bed", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex 68075af718..41058dcfb5 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -53,7 +53,7 @@ rayon = \"1.5.2\"\n lru = \"0.12.0\"\n fastdivide = \"0.4.0\"\n itertools = \"0.13.0\"\n-measure_time = \"0.8.2\"\n+measure_time = \"0.9.0\"\n arc-swap = \"1.5.0\"\n \n columnar = { version = \"0.3\", path = \"./columnar\", package = \"tantivy-columnar\" }\n", "test_patch": "", "problem_statement": "RUSTSEC-2024-0384: Unmaintained transitive dependency\n**Describe the bug**\r\n\r\nHey! I use cargo-deny and noticed I was getting an error for one of tantivys transitive dependencies.\r\n\r\n[measure_time](https://github.com/PSeitz/rust_measure_time) depends on [instant](https://github.com/sebcrozet/instant) which has been marked as unmaintained. There is no security vuln associated or anything requiring immediate action as best as I could tell.\r\n\r\nThere is only one place in tantivy I could see that uses `measure_time`, [src/indexer/merger.rs](https://github.com/quickwit-oss/tantivy/blob/876a579e5da1223b40df044be396faac8151ff91/src/indexer/merger.rs#L8).\r\n\r\nIt is importing the `debug_time` macro and using it to instrument a few methods.\r\n```rust\r\n        debug_time!(\"write-fast-fields\");\r\n```\r\n\r\nGiven the small usage here, could this replaced with a `log::debug!` and remove the dependency to `measure_time`? If so, I'm happy to open a PR doing that. :grinning: \r\n\r\nAdvisory:\r\nhttps://rustsec.org/advisories/RUSTSEC-2024-0384\r\n\r\n**Which version of tantivy are you using?**\r\n0.22\r\n\r\n**To Reproduce**\r\n```shell\r\ncargo deny check\r\n```\n", "hints_text": "@PSeitz can you have a look?", "created_at": "2024-12-09 20:32:51", "merge_commit_sha": "0f99d4f420179e62362119f785ad44b6599d66bf", "environment_setup_commit": "", "version": "0.0", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "ci_name_list": [["['test-quickwit', '.github/workflows/test.yml']", "['test-all', '.github/workflows/test.yml']"]]}
