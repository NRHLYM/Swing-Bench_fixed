{"problem_statement": "\u2753 [Question] operator being decomposed rather than being converted when a corresponding converter exists?\n## \u2753 Question\r\n\r\nFrom the debug log below, it seems that the `aten.grid_sampler_2d` operator gets decomposed into several lower-level operators. But isn't there a corresponding [converter](https://github.com/pytorch/TensorRT/blob/9a100b6414bee175040bcaa275ecb71df54836e4/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py#L333-L358) which should be used?\r\n\r\n## What you have already tried\r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch_tensorrt\r\n\r\n\r\nclass MyModule(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n    def forward(self, input, grid):\r\n        return F.grid_sample(input, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\r\n    \r\nmodel = MyModule().eval().cuda()\r\n\r\ninputs = [\r\n    torch.randn((1, 3, 8, 8), dtype=torch.float, device=\"cuda\"),\r\n    torch.randn((1, 16, 16, 2), dtype=torch.float, device=\"cuda\")\r\n]\r\n\r\noptimized_model = torch_tensorrt.compile(\r\n    model,\r\n    ir=\"dynamo\",\r\n    inputs=inputs,\r\n    enabled_precisions={torch.float},\r\n    debug=True,\r\n    min_block_size=1,\r\n    truncate_long_and_double=True,\r\n    output_format=\"fx\",\r\n)\r\n```\r\n\r\n```\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_1 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_1 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_2 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_2 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_3 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_3 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_4 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_4 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_5 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_5 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_6 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_6 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_7 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_7 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\r\nSupported Nodes:\r\n- torch.ops.aten.reshape.default + Operator Count: 13\r\n- torch.ops.aten.expand.default + Operator Count: 1\r\n- torch.ops.aten.select.int + Operator Count: 2\r\n- torch.ops.aten.mul.Tensor + Operator Count: 10\r\n- torch.ops.aten.add.Tensor + Operator Count: 7\r\n- torch.ops.aten.clamp.default + Operator Count: 2\r\n- torch.ops.aten.floor.default + Operator Count: 2\r\n- torch.ops.aten.sub.Tensor + Operator Count: 8\r\n- torch.ops.aten.ge.Scalar + Operator Count: 8\r\n- torch.ops.aten.lt.Scalar + Operator Count: 8\r\n- torch.ops.aten.logical_and.default + Operator Count: 12\r\n- torch.ops.aten.where.self + Operator Count: 12\r\n- torch.ops.aten.index.Tensor + Operator Count: 4\r\n\r\nDEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\r\nUnsupported or Excluded Nodes:\r\n- torch.ops.aten._to_copy.default + Operator Count: 8\r\n\r\nDEBUG:torch_tensorrt.dynamo._compiler:Detected support for 89 operators out of 97 in subgraph.\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_1 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_1 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_2 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_2 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_3 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_3 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_4 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_4 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_5 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_5 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_6 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_6 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_7 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.conversion.aten_ops_converters:_to_copy converter rejected node _to_copy_7 with dtype torch.int64\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nNumber of TensorRT-Accelerated Engines Generated: 2\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nSupported Nodes:\r\n- torch.ops.aten.reshape.default + Operator Count: 13\r\n- torch.ops.aten.expand.default + Operator Count: 1\r\n- torch.ops.aten.select.int + Operator Count: 2\r\n- torch.ops.aten.mul.Tensor + Operator Count: 10\r\n- torch.ops.aten.add.Tensor + Operator Count: 7\r\n- torch.ops.aten.clamp.default + Operator Count: 2\r\n- torch.ops.aten.floor.default + Operator Count: 2\r\n- torch.ops.aten.sub.Tensor + Operator Count: 8\r\n- torch.ops.aten.ge.Scalar + Operator Count: 8\r\n- torch.ops.aten.lt.Scalar + Operator Count: 8\r\n- torch.ops.aten.logical_and.default + Operator Count: 12\r\n- torch.ops.aten.where.self + Operator Count: 12\r\n- torch.ops.aten.index.Tensor + Operator Count: 4\r\n\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nUnsupported or Excluded Nodes:\r\n- torch.ops.aten._to_copy.default + Operator Count: 8\r\n\r\n++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++\r\n\r\nThe graph consists of 97 Total Operators, of which 89 operators are supported, 91.75% coverage\r\n\r\nThe following ops are currently unsupported or excluded from conversion, and are listed with their op-count in the graph:\r\n torch.ops.aten._to_copy.default: 8\r\n\r\nThe following nodes are currently set to run in Torch:\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_1\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_2\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_3\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_4\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_5\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_6\r\nNode: torch.ops.aten._to_copy.default, with layer location: __/_to_copy_7\r\nNote: Some of the above nodes may be supported, but were not included in a TRT graph by the partitioner\r\n\r\nCompiled with: CompilationSettings(precision=torch.float32, debug=True, workspace_size=0, min_block_size=1, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_long_and_double=True, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.DEFAULT: 0>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False, output_format='fx')\r\n\r\n  Graph Structure:\r\n\r\n   Inputs: List[Tensor: (1, 3, 8, 8)@float32, Tensor: (1, 16, 16, 2)@float32]\r\n    ...\r\n    TRT Engine #1 - Submodule name: _run_on_acc_0\r\n     Engine Inputs: List[Tensor: (1, 16, 16, 2)@float32]\r\n     Number of Operators in Engine: 58\r\n     Engine Outputs: Tuple(Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@float32)\r\n    ...\r\n    TRT Engine #2 - Submodule name: _run_on_acc_2\r\n     Engine Inputs: List[Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 8, 8)@float32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@float32, Tensor: (1, 3, 16, 16)@bool, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@int32, Tensor: (1, 3, 16, 16)@float32]\r\n     Number of Operators in Engine: 31\r\n     Engine Outputs: Tensor: (1, 3, 16, 16)@float32\r\n    ...\r\n   Outputs: List[Tensor: (1, 3, 16, 16)@float32]\r\n\r\n  ------------------------- Aggregate Stats -------------------------\r\n\r\n   Average Number of Operators per TRT Engine: 44.5\r\n   Most Operators in a TRT Engine: 58\r\n\r\n  ********** Recommendations **********\r\n\r\n   - For minimal graph segmentation, select min_block_size=58 which would generate 1 TRT engine(s)\r\n   - For moderate graph segmentation, select min_block_size=45 which would generate 1 TRT engine(s)\r\n   - The current level of graph segmentation is equivalent to selecting min_block_size=31 which generates 2 TRT engine(s)\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 2.3.0.dev20240221+cu121\r\n - CPU Architecture: x64\r\n - OS (e.g., Linux): Ubuntu 22.04 LTS\r\n - How you installed PyTorch (`conda`, `pip`, `libtorch`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Are you using local sources or building from archives:\r\n - Python version: 3.10.12\r\n - CUDA version: 12.1\r\n - GPU models and configuration: RTX 3050\r\n - Any other relevant information:\n", "patch": "diff --git a/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py b/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\nindex 9acb750aed..0ea2f0304d 100644\n--- a/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\n+++ b/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\n@@ -332,6 +332,8 @@ def aten_ops_fmod(\n \n @dynamo_tensorrt_converter(torch.ops.aten.grid_sampler)\n @dynamo_tensorrt_converter(torch.ops.aten.grid_sampler_2d)\n+@dynamo_tensorrt_converter(torch.ops.aten.grid_sampler.default)\n+@dynamo_tensorrt_converter(torch.ops.aten.grid_sampler_2d.default)\n @enforce_tensor_types(\n     {\n         0: (TRTTensor,),\ndiff --git a/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py b/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\nindex de791851db..d365d6d9c5 100644\n--- a/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\n+++ b/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\n@@ -46,7 +46,6 @@\n     aten.gelu,\n     aten.gelu_backward,\n     aten.glu_backward,\n-    aten.grid_sampler_2d,\n     aten.hardshrink,\n     aten.hardshrink_backward,\n     aten.hardsigmoid,\n", "instance_id": "pytorch__TensorRT-2686", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `aten.grid_sampler_2d` operator is being decomposed into lower-level operators instead of being converted using an existing converter in the PyTorch TensorRT framework. The goal is to ensure the operator uses the converter rather than being decomposed. The statement includes a reproducible code snippet, debug logs, and environment details, which help in understanding the context. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior or output after the fix, and no discussion of potential edge cases or constraints (e.g., specific configurations or versions where this issue occurs). Additionally, the problem does not clarify whether the decomposition is incorrect or just suboptimal, which could affect the approach to solving it. Overall, it is clear enough to understand the issue but misses some finer details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively small, involving modifications in two files: adding support for additional operator variants in `aten_ops_converters.py` and removing `aten.grid_sampler_2d` from the decomposition group in `_decomposition_groups.py`. These changes are localized and do not appear to impact the broader system architecture significantly. However, solving this requires a moderate understanding of the PyTorch TensorRT framework, specifically the operator conversion and decomposition mechanisms in the Dynamo backend, which involves concepts like operator registration, graph partitioning, and TensorRT engine generation. The debug logs indicate that other operators (e.g., `_to_copy`) are also unsupported, which might hint at related issues, though they are not directly part of this problem. Edge case handling does not seem to be a major concern based on the problem statement, as no specific edge cases are mentioned, though ensuring compatibility with different input shapes or precision settings might be implicitly required. Overall, this problem requires understanding a few technical concepts and making targeted changes, placing it in the medium difficulty range (0.4-0.6), with a score of 0.45 reflecting the balance between the simplicity of the code changes and the need for domain-specific knowledge of PyTorch TensorRT internals.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Changing Parcels -> parcels and importing all of parcels throughout\nMost python packages are all-lowercase ([as suggested by pep8](https://peps.python.org/pep-0008/#package-and-module-names)), so we should also change the writing of `parcels` to lowercase throughout in the examples, documentation, docstrings and website. \r\n\r\nWhile we're at it, we could also change the notebooks in the examples to refer to the classes as `parcels.ClassName`, to better highlight that these are specific to `parcels`. So e.g. change\r\n```python\r\nfrom parcels import FieldSet\r\n\r\nfset = FieldSet(...)\r\n```\r\nto\r\n```python\r\nimport parcels\r\n\r\nfset = parcels.FieldSet(...)\r\n```\nRemoving `delta` alias for `timedelta`\nThe codebase uses `delta` as an alias for `datetime.timedelta` through the codebase and through the notebooks. This is a bit unique to `parcels`, and can lead to confusion to those famliar with Python wanting to use parcels for simulation.\r\n\r\nIt would be good to remove this in favour of using `timedelta`.\r\n\nSupport use of parcels.rng in Kernels\nThis PR fixes #1608. it supports using `parcels.rng` directly in Kernels (note, `parcels.ParcelsRandom` is also supported, but since the module itself it called `rng` better to use `parcels.rng` for traceability)\nKernel can't be converted to C depending on method of import\nIn pursuit of #1581, I encountered the following error message:\r\n\r\n```\r\nNotImplementedError: Cannot convert 'parcels' used in kernel to C-code\r\n```\r\n\r\nSee the following minimal example. Doing the suggested change causes the error message:\r\n\r\n```diff\r\nimport parcels\r\n- from parcels import ParcelsRandom\r\n\r\nexample_dataset_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\r\nfilenames = {\r\n    \"U\": f\"{example_dataset_folder}/20*.nc\",\r\n    \"V\": f\"{example_dataset_folder}/20*.nc\",\r\n}\r\nvariables = {\r\n    \"U\": \"eastward_eulerian_current_velocity\",\r\n    \"V\": \"northward_eulerian_current_velocity\",\r\n}\r\ndimensions = {\"lat\": \"lat\", \"lon\": \"lon\", \"time\": \"time\"}\r\n\r\nfieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\r\n\r\ndef nudge_kernel(particle, fieldset, time):\r\n-   dlat = ParcelsRandom.normalvariate(0.0, 1.0)\r\n+   dlat = parcels.ParcelsRandom.normalvariate(0.0, 1.0)\r\n    particle_dlat += dlat\r\n\r\ntime = np.arange(0, 43200, 3600)\r\nlon = [29] * len(time)\r\nlat = [-33] * len(time)\r\npset = parcels.ParticleSet(fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time)\r\n\r\n\r\npset.execute(\r\n    nudge_kernel,\r\n    runtime=timedelta(days=7),\r\n    dt=timedelta(minutes=5),\r\n    output_file=output_file,\r\n)\r\n```\r\n\r\n\r\nLikely culplit is code here:\r\nhttps://github.com/OceanParcels/parcels/blob/035159a1b4002bc1550cebd275f93c0adaa8513d/parcels/compilation/codegenerator.py#L251-L265\r\n\nSupport use of parcels.rng in Kernels\nThis PR fixes #1608. it supports using `parcels.rng` directly in Kernels (note, `parcels.ParcelsRandom` is also supported, but since the module itself it called `rng` better to use `parcels.rng` for traceability)\n", "patch": "diff --git a/.flake8 b/.flake8\nindex c770e35a7..e2e4bfc1b 100644\n--- a/.flake8\n+++ b/.flake8\n@@ -10,3 +10,5 @@ ignore =\n     E501,\n     # \u2018from module import *\u2019 used; unable to detect undefined names\n     F403,\n+    # incompatability with Black\n+    E203\ndiff --git a/.git-blame-ignore-revs b/.git-blame-ignore-revs\nindex 2496866d6..56ae0cbf0 100644\n--- a/.git-blame-ignore-revs\n+++ b/.git-blame-ignore-revs\n@@ -3,3 +3,6 @@\n \n # whitespace fixes\n e99e0d170a7dc128031b82ea00cdf9e9a0164b03\n+\n+# Run black on examples\n+cbf96c4e1f58fa6348695d7553900ba3f7c3383b\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 1283515f6..5411ee6e1 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -13,10 +13,11 @@ repos:\n     rev: \"7.1.0\"\n     hooks:\n       - id: flake8\n-  - repo: https://github.com/pycqa/pydocstyle\n-    rev: 6.3.0 # pick a git hash / tag to point to\n+  - repo: https://github.com/psf/black-pre-commit-mirror\n+    rev: 24.4.2\n     hooks:\n-      - id: pydocstyle\n+      - id: black\n+        files: docs/examples\n   - repo: https://github.com/nbQA-dev/nbQA\n     rev: 1.8.5\n     hooks:\n@@ -28,7 +29,7 @@ repos:\n     hooks:\n       - id: isort\n         name: isort (python)\n-  - repo: https://github.com/pre-commit/mirrors-prettier\n-    rev: \"v4.0.0-alpha.8\"\n+  - repo: https://github.com/biomejs/pre-commit\n+    rev: v0.4.0\n     hooks:\n-      - id: prettier\n+      - id: biome-format\ndiff --git a/.readthedocs.yaml b/.readthedocs.yaml\nindex 96f1e7f8a..e7e60beb1 100644\n--- a/.readthedocs.yaml\n+++ b/.readthedocs.yaml\n@@ -9,6 +9,7 @@ build:\n \n sphinx:\n   configuration: docs/conf.py\n+  builder: dirhtml\n \n conda:\n   environment: environment.yml\ndiff --git a/docs/conf.py b/docs/conf.py\nindex 3dbe88a3a..a35144d52 100755\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -317,7 +317,11 @@ def linkcode_resolve(domain, info):\n # html_use_smartypants = True\n \n # Custom sidebar templates, maps document names to template names.\n-# html_sidebars = {}\n+\n+html_sidebars = {\n+    \"**\": [\"sidebar-nav-bs\"],\n+    \"documentation/additional_examples\": []\n+}\n \n # Additional templates that should be rendered to pages, maps page names to\n # template names.\ndiff --git a/docs/contributing.rst b/docs/contributing.rst\nindex 904de0da6..1962a732b 100644\n--- a/docs/contributing.rst\n+++ b/docs/contributing.rst\n@@ -41,7 +41,7 @@ Your first contribution\n There are many ways that you can contribute to Parcels. You can:\n \n - Participate in discussion about Parcels, either through the `issues <https://github.com/OceanParcels/parcels/issues>`_ or `discussions <https://github.com/OceanParcels/parcels/discussions>`_ tab\n-- Suggest improvements to `tutorials <documentation.rst>`_\n+- Suggest improvements to `tutorials <documentation/index.rst>`_\n - Suggest improvements to `documentation <index.rst>`_\n - Write code (fix bugs, implement features, codebase improvements, etc)\n \ndiff --git a/docs/documentation.rst b/docs/documentation.rst\ndeleted file mode 100644\nindex 210c84b15..000000000\n--- a/docs/documentation.rst\n+++ /dev/null\n@@ -1,69 +0,0 @@\n-Documentation and Tutorials\n-===========================\n-\n-Parcels has several documentation and tutorial Jupyter notebooks which go through various aspects of Parcels. Static versions of these notebooks are available below in the site, with the interactive notebooks being available either completely online at the following `Binder link <https://mybinder.org/v2/gh/OceanParcels/parcels/master?labpath=docs%2Fexamples%2Fparcels_tutorial.ipynb>`_, or locally by downloading :download:`parcels_tutorials.zip </_downloads/parcels_tutorials.zip>` and running with your own Parcels installation.\n-\n-\n-\n-.. nbgallery::\n-   :caption: Overview\n-   :name: tutorial-overview\n-\n-   examples/tutorial_parcels_structure.ipynb\n-   examples/parcels_tutorial.ipynb\n-   examples/tutorial_output.ipynb\n-\n-\n-.. nbgallery::\n-   :caption: Setting up FieldSets\n-   :name: tutorial-fieldsets\n-\n-   examples/documentation_indexing.ipynb\n-   examples/tutorial_nemo_curvilinear.ipynb\n-   examples/tutorial_nemo_3D.ipynb\n-   examples/tutorial_NestedFields.ipynb\n-   examples/tutorial_timevaryingdepthdimensions.ipynb\n-   examples/tutorial_periodic_boundaries.ipynb\n-   examples/tutorial_interpolation.ipynb\n-   examples/tutorial_unitconverters.ipynb\n-   examples/tutorial_timestamps.ipynb\n-\n-\n-.. nbgallery::\n-   :caption: Creating ParticleSets\n-   :name: tutorial-particlesets\n-\n-   examples/tutorial_jit_vs_scipy.ipynb\n-   examples/tutorial_delaystart.ipynb\n-\n-\n-.. nbgallery::\n-   :caption: Writing kernels to be executed on each particle\n-   :name: tutorial-kernels\n-\n-   examples/tutorial_diffusion.ipynb\n-   examples/tutorial_sampling.ipynb\n-   examples/tutorial_particle_field_interaction.ipynb\n-   examples/tutorial_interaction.ipynb\n-   examples/tutorial_analyticaladvection.ipynb\n-   examples/tutorial_kernelloop.ipynb\n-\n-\n-.. nbgallery::\n-   :caption: Other tutorials\n-   :name: tutorial-other\n-\n-   examples/documentation_MPI.ipynb\n-   examples/documentation_stuck_particles.ipynb\n-   examples/documentation_unstuck_Agrid.ipynb\n-   examples/documentation_LargeRunsOutput.ipynb\n-   examples/documentation_geospatial.ipynb\n-   examples/documentation_advanced_zarr.ipynb\n-\n-\n-.. nbgallery::\n-   :caption: Worked examples\n-   :name: tutorial-examples\n-\n-   examples/tutorial_Argofloats.ipynb\n-   examples/documentation_homepage_animation.ipynb\ndiff --git a/docs/documentation/additional_examples.rst b/docs/documentation/additional_examples.rst\nnew file mode 100644\nindex 000000000..54db65047\n--- /dev/null\n+++ b/docs/documentation/additional_examples.rst\n@@ -0,0 +1,76 @@\n+example_brownian.py\n+-------------------\n+\n+.. literalinclude:: ../examples/example_brownian.py\n+   :language: python\n+   :linenos:\n+\n+example_dask_chunk_OCMs.py\n+--------------------------\n+\n+.. literalinclude:: ../examples/example_dask_chunk_OCMs.py\n+   :language: python\n+   :linenos:\n+\n+example_decaying_moving_eddy.py\n+-------------------------------\n+\n+.. literalinclude:: ../examples/example_decaying_moving_eddy.py\n+   :language: python\n+   :linenos:\n+\n+example_globcurrent.py\n+----------------------\n+\n+.. literalinclude:: ../examples/example_globcurrent.py\n+   :language: python\n+   :linenos:\n+\n+example_mitgcm.py\n+-----------------\n+\n+.. literalinclude:: ../examples/example_mitgcm.py\n+   :language: python\n+   :linenos:\n+\n+example_moving_eddies.py\n+------------------------\n+\n+.. literalinclude:: ../examples/example_moving_eddies.py\n+   :language: python\n+   :linenos:\n+\n+example_nemo_curvilinear.py\n+---------------------------\n+\n+.. literalinclude:: ../examples/example_nemo_curvilinear.py\n+   :language: python\n+   :linenos:\n+\n+example_ofam.py\n+---------------\n+\n+.. literalinclude:: ../examples/example_ofam.py\n+   :language: python\n+   :linenos:\n+\n+example_peninsula.py\n+--------------------\n+\n+.. literalinclude:: ../examples/example_peninsula.py\n+   :language: python\n+   :linenos:\n+\n+example_radial_rotation.py\n+--------------------------\n+\n+.. literalinclude:: ../examples/example_radial_rotation.py\n+   :language: python\n+   :linenos:\n+\n+example_stommel.py\n+------------------\n+\n+.. literalinclude:: ../examples/example_stommel.py\n+   :language: python\n+   :linenos:\ndiff --git a/docs/documentation/index.rst b/docs/documentation/index.rst\nnew file mode 100644\nindex 000000000..7356a9b68\n--- /dev/null\n+++ b/docs/documentation/index.rst\n@@ -0,0 +1,76 @@\n+Documentation and Tutorials\n+===========================\n+\n+Parcels has several documentation and tutorial Jupyter notebooks and scripts which go through various aspects of Parcels. Static versions of the notebooks are available below via the gallery in the site, with the interactive notebooks being available either completely online at the following `Binder link <https://mybinder.org/v2/gh/OceanParcels/parcels/master?labpath=docs%2Fexamples%2Fparcels_tutorial.ipynb>`_. Following the gallery of notebooks is a list of scripts which provide additional examples to users. You can work with the example notebooks and scripts locally by downloading :download:`parcels_tutorials.zip </_downloads/parcels_tutorials.zip>` and running with your own Parcels installation.\n+\n+.. nbgallery::\n+   :caption: Overview\n+   :name: tutorial-overview\n+\n+   ../examples/tutorial_parcels_structure.ipynb\n+   ../examples/parcels_tutorial.ipynb\n+   ../examples/tutorial_output.ipynb\n+\n+\n+.. nbgallery::\n+   :caption: Setting up FieldSets\n+   :name: tutorial-fieldsets\n+\n+   ../examples/documentation_indexing.ipynb\n+   ../examples/tutorial_nemo_curvilinear.ipynb\n+   ../examples/tutorial_nemo_3D.ipynb\n+   ../examples/tutorial_NestedFields.ipynb\n+   ../examples/tutorial_timevaryingdepthdimensions.ipynb\n+   ../examples/tutorial_periodic_boundaries.ipynb\n+   ../examples/tutorial_interpolation.ipynb\n+   ../examples/tutorial_unitconverters.ipynb\n+   ../examples/tutorial_timestamps.ipynb\n+\n+\n+.. nbgallery::\n+   :caption: Creating ParticleSets\n+   :name: tutorial-particlesets\n+\n+   ../examples/tutorial_jit_vs_scipy.ipynb\n+   ../examples/tutorial_delaystart.ipynb\n+\n+\n+.. nbgallery::\n+   :caption: Writing kernels to be executed on each particle\n+   :name: tutorial-kernels\n+\n+   ../examples/tutorial_diffusion.ipynb\n+   ../examples/tutorial_sampling.ipynb\n+   ../examples/tutorial_particle_field_interaction.ipynb\n+   ../examples/tutorial_interaction.ipynb\n+   ../examples/tutorial_analyticaladvection.ipynb\n+   ../examples/tutorial_kernelloop.ipynb\n+\n+\n+.. nbgallery::\n+   :caption: Other tutorials\n+   :name: tutorial-other\n+\n+   ../examples/documentation_MPI.ipynb\n+   ../examples/documentation_stuck_particles.ipynb\n+   ../examples/documentation_unstuck_Agrid.ipynb\n+   ../examples/documentation_LargeRunsOutput.ipynb\n+   ../examples/documentation_geospatial.ipynb\n+   ../examples/documentation_advanced_zarr.ipynb\n+\n+\n+.. nbgallery::\n+   :caption: Worked examples\n+   :name: tutorial-examples\n+\n+   ../examples/tutorial_Argofloats.ipynb\n+   ../examples/documentation_homepage_animation.ipynb\n+\n+\n+Python Example Scripts\n+----------------------\n+\n+\n+.. toctree::\n+\n+   additional_examples\ndiff --git a/docs/examples/documentation_MPI.ipynb b/docs/examples/documentation_MPI.ipynb\nindex 01a5171b2..eab6293d8 100644\n--- a/docs/examples/documentation_MPI.ipynb\n+++ b/docs/examples/documentation_MPI.ipynb\n@@ -216,14 +216,14 @@\n     \"%pylab inline\\n\",\n     \"import os\\n\",\n     \"import time\\n\",\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"from glob import glob\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import numpy as np\\n\",\n     \"import psutil\\n\",\n     \"\\n\",\n-    \"from parcels import AdvectionRK4, FieldSet, JITParticle, ParticleSet\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"def set_cms_fieldset(cs):\\n\",\n@@ -235,7 +235,7 @@\n     \"\\n\",\n     \"    if cs not in [\\\"auto\\\", False]:\\n\",\n     \"        cs = {\\\"time\\\": (\\\"time\\\", 1), \\\"lat\\\": (\\\"latitude\\\", cs), \\\"lon\\\": (\\\"longitude\\\", cs)}\\n\",\n-    \"    return FieldSet.from_netcdf(files, variables, dimensions, chunksize=cs)\\n\",\n+    \"    return parcels.FieldSet.from_netcdf(files, variables, dimensions, chunksize=cs)\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"func_time = []\\n\",\n@@ -243,12 +243,16 @@\n     \"chunksize = [128, 256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2610, \\\"auto\\\", False]\\n\",\n     \"for cs in chunksize:\\n\",\n     \"    fieldset = set_cms_fieldset(cs)\\n\",\n-    \"    pset = ParticleSet(\\n\",\n-    \"        fieldset=fieldset, pclass=JITParticle, lon=[0], lat=[0], repeatdt=delta(hours=1)\\n\",\n+    \"    pset = parcels.ParticleSet(\\n\",\n+    \"        fieldset=fieldset,\\n\",\n+    \"        pclass=parcels.JITParticle,\\n\",\n+    \"        lon=[0],\\n\",\n+    \"        lat=[0],\\n\",\n+    \"        repeatdt=timedelta(hours=1),\\n\",\n     \"    )\\n\",\n     \"\\n\",\n     \"    tic = time.time()\\n\",\n-    \"    pset.execute(AdvectionRK4, dt=delta(hours=1))\\n\",\n+    \"    pset.execute(parcels.AdvectionRK4, dt=timedelta(hours=1))\\n\",\n     \"    func_time.append(time.time() - tic)\\n\",\n     \"    process = psutil.Process(os.getpid())\\n\",\n     \"    mem_B_used = process.memory_info().rss\\n\",\n@@ -387,19 +391,23 @@\n     \"            \\\"lat\\\": (\\\"latitude\\\", cs),\\n\",\n     \"            \\\"lon\\\": (\\\"longitude\\\", cs),\\n\",\n     \"        }\\n\",\n-    \"    return FieldSet.from_netcdf(files, variables, dimensions, chunksize=cs)\\n\",\n+    \"    return parcels.FieldSet.from_netcdf(files, variables, dimensions, chunksize=cs)\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"chunksize_3D = [128, 256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2610, \\\"auto\\\", False]\\n\",\n     \"func_time3D = []\\n\",\n     \"for cs in chunksize_3D:\\n\",\n     \"    fieldset = set_cms_fieldset_3D(cs)\\n\",\n-    \"    pset = ParticleSet(\\n\",\n-    \"        fieldset=fieldset, pclass=JITParticle, lon=[0], lat=[0], repeatdt=delta(hours=1)\\n\",\n+    \"    pset = parcels.ParticleSet(\\n\",\n+    \"        fieldset=fieldset,\\n\",\n+    \"        pclass=parcels.JITParticle,\\n\",\n+    \"        lon=[0],\\n\",\n+    \"        lat=[0],\\n\",\n+    \"        repeatdt=timedelta(hours=1),\\n\",\n     \"    )\\n\",\n     \"\\n\",\n     \"    tic = time.time()\\n\",\n-    \"    pset.execute(AdvectionRK4_3D, dt=delta(hours=1))\\n\",\n+    \"    pset.execute(AdvectionRK4_3D, dt=timedelta(hours=1))\\n\",\n     \"    func_time3D.append(time.time() - tic)\"\n    ]\n   },\ndiff --git a/docs/examples/documentation_advanced_zarr.ipynb b/docs/examples/documentation_advanced_zarr.ipynb\nindex adcc084b0..c79a6157a 100644\n--- a/docs/examples/documentation_advanced_zarr.ipynb\n+++ b/docs/examples/documentation_advanced_zarr.ipynb\n@@ -598,7 +598,7 @@\n     \"import numpy as np\\n\",\n     \"import zarr\\n\",\n     \"\\n\",\n-    \"from parcels import AdvectionRK4, FieldSet, ParticleSet, ScipyParticle\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -617,7 +617,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"fieldset = FieldSet.from_xarray_dataset(\\n\",\n+    \"fieldset = parcels.FieldSet.from_xarray_dataset(\\n\",\n     \"    ds_flow_field,\\n\",\n     \"    variables={\\\"U\\\": \\\"U\\\", \\\"V\\\": \\\"V\\\"},\\n\",\n     \"    dimensions={\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\"},\\n\",\n@@ -682,9 +682,9 @@\n     \"def create_random_pset(\\n\",\n     \"    fieldset=None, lon_range=(-15, 15), lat_range=(-15, 15), number_particles=200\\n\",\n     \"):\\n\",\n-    \"    return ParticleSet.from_list(\\n\",\n+    \"    return parcels.ParticleSet.from_list(\\n\",\n     \"        fieldset=fieldset,\\n\",\n-    \"        pclass=ScipyParticle,\\n\",\n+    \"        pclass=parcels.ScipyParticle,\\n\",\n     \"        lon=np.random.uniform(*lon_range, size=(number_particles,)),\\n\",\n     \"        lat=np.random.uniform(*lat_range, size=(number_particles,)),\\n\",\n     \"        time=np.zeros(shape=(number_particles,)),\\n\",\n@@ -1477,7 +1477,7 @@\n     \"\\n\",\n     \"# run experiment\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4,\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n     \"    runtime=timedelta(days=17),\\n\",\n     \"    dt=timedelta(hours=3),\\n\",\n     \"    output_file=outputfile,\\n\",\ndiff --git a/docs/examples/documentation_geospatial.ipynb b/docs/examples/documentation_geospatial.ipynb\nindex e644c2617..fcf3491ff 100644\n--- a/docs/examples/documentation_geospatial.ipynb\n+++ b/docs/examples/documentation_geospatial.ipynb\n@@ -36,7 +36,7 @@\n     \"- [geopandas](https://geopandas.org/) to create the geospatial datasets (`conda install -c conda-forge geopandas`)\\n\",\n     \"- [lxml](https://lxml.de/) to create KML files for Google Earth (`conda install lxml`)\\n\",\n     \"\\n\",\n-    \"This tutorial saves all generated data output in a folder `tutorial_geospatial_output` to avoid clogging the working directory.\"\n+    \"This tutorial saves all generated data output in a folder `tutorial_geospatial_output` to avoid clogging the working directory. If you're re-running this notebook, be sure to delete this folder so that the notebook doesn't encounter writing issues.\"\n    ]\n   },\n   {\n@@ -127,13 +127,7 @@\n     \"from lxml import etree\\n\",\n     \"from shapely.geometry import LineString, Point\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n     \"DATA_OUTPUT_FOLDER = Path(\\\"tutorial_geospatial_output\\\")\\n\",\n     \"DATA_OUTPUT_FOLDER.mkdir(exist_ok=True)\\n\",\n@@ -156,7 +150,7 @@\n    ],\n    \"source\": [\n     \"# An example parcels simulation\\n\",\n-    \"data_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"data_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"filenames = filename = str(data_folder / \\\"20*.nc\\\")\\n\",\n     \"variables = {\\n\",\n     \"    \\\"U\\\": \\\"eastward_eulerian_current_velocity\\\",\\n\",\n@@ -168,11 +162,13 @@\n     \"    \\\"time\\\": \\\"time\\\",\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n     \"# Mesh of particles\\n\",\n     \"lons, lats = np.meshgrid(range(15, 35, 2), range(-40, -30, 2))\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats\\n\",\n+    \")\\n\",\n     \"dt = timedelta(hours=24)\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\n     \"    name=DATA_OUTPUT_FOLDER / f\\\"{DATA_OUTPUT_NAME}.zarr\\\", outputdt=dt\\n\",\n@@ -185,7 +181,7 @@\n     \"\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, DeleteParticle],\\n\",\n+    \"    [parcels.AdvectionRK4, DeleteParticle],\\n\",\n     \"    runtime=timedelta(days=120),\\n\",\n     \"    dt=dt,\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -467,7 +463,10 @@\n     \"]\\n\",\n     \"for trajectory_idx, trajectory_gdf in gdf_parcels.groupby(\\\"trajectory\\\"):\\n\",\n     \"    trajectory_gdf = trajectory_gdf.sort_values(\\\"obs\\\")\\n\",\n-    \"    linestring = LineString(trajectory_gdf[\\\"geometry\\\"])\\n\",\n+    \"    points = trajectory_gdf[\\\"geometry\\\"]\\n\",\n+    \"    if points.shape[0] == 1:\\n\",\n+    \"        continue  # Can't create LineString with one point\\n\",\n+    \"    linestring = LineString()\\n\",\n     \"    linestrings.append((trajectory_idx, linestring))\\n\",\n     \"\\n\",\n     \"gdf_parcels_linestring = gpd.GeoDataFrame(\\n\",\ndiff --git a/docs/examples/documentation_indexing.ipynb b/docs/examples/documentation_indexing.ipynb\nindex c8cf0fcfc..28b54b3f7 100644\n--- a/docs/examples/documentation_indexing.ipynb\n+++ b/docs/examples/documentation_indexing.ipynb\n@@ -54,13 +54,12 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n     \"from glob import glob\\n\",\n     \"from os import path\\n\",\n     \"\\n\",\n     \"import numpy as np\\n\",\n     \"\\n\",\n-    \"from parcels import FieldSet, download_example_dataset\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -95,7 +94,9 @@\n     }\n    ],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoNorthSeaORCA025-N006_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\n\",\n+    \"    \\\"NemoNorthSeaORCA025-N006_data\\\"\\n\",\n+    \")\\n\",\n     \"ufiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*U.nc\\\"))\\n\",\n     \"vfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*V.nc\\\"))\\n\",\n     \"wfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*W.nc\\\"))\\n\",\n@@ -123,7 +124,7 @@\n     \"    },\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\"\n+    \"fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -140,7 +141,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"fieldsetA = FieldSet.from_netcdf(filenames, variables, dimensions)\"\n+    \"fieldsetA = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -178,7 +179,7 @@\n     \"    \\\"W\\\": c_grid_dimensions,\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldsetC = FieldSet.from_nemo(\\n\",\n+    \"fieldsetC = parcels.FieldSet.from_nemo(\\n\",\n     \"    filenames, variables, dimensions, netcdf_decodewarning=False\\n\",\n     \")\"\n    ]\ndiff --git a/docs/examples/documentation_stuck_particles.ipynb b/docs/examples/documentation_stuck_particles.ipynb\nindex 82f8354ed..ef6b298a6 100644\n--- a/docs/examples/documentation_stuck_particles.ipynb\n+++ b/docs/examples/documentation_stuck_particles.ipynb\n@@ -63,7 +63,7 @@\n    \"source\": [\n     \"import math\\n\",\n     \"from copy import copy\\n\",\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"from glob import glob\\n\",\n     \"\\n\",\n     \"import cmocean\\n\",\n@@ -76,14 +76,7 @@\n     \"from matplotlib.lines import Line2D\\n\",\n     \"from scipy import interpolate\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    Field,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -480,7 +473,7 @@\n     \"dims = {\\\"lon\\\": \\\"longitude\\\", \\\"lat\\\": \\\"latitude\\\", \\\"depth\\\": \\\"depth\\\", \\\"time\\\": \\\"time\\\"}\\n\",\n     \"dimensions = {\\\"U\\\": dims, \\\"V\\\": dims}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n     \"npart = 3  # number of particles to be released\\n\",\n     \"lon = np.linspace(7, 7.2, npart, dtype=np.float32)\\n\",\n@@ -488,14 +481,19 @@\n     \"lons, lats = np.meshgrid(lon, lat)\\n\",\n     \"time = np.zeros(lons.size)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"kernels = pset.Kernel(AdvectionRK4)\\n\",\n+    \"kernels = pset.Kernel(parcels.AdvectionRK4)\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SMOC.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"SMOC.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    kernels, runtime=delta(hours=119), dt=delta(minutes=12), output_file=output_file\\n\",\n+    \"    kernels,\\n\",\n+    \"    runtime=timedelta(hours=119),\\n\",\n+    \"    dt=timedelta(minutes=12),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -706,7 +704,9 @@\n    },\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoNorthSeaORCA025-N006_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\n\",\n+    \"    \\\"NemoNorthSeaORCA025-N006_data\\\"\\n\",\n+    \")\\n\",\n     \"cufields = xr.open_dataset(f\\\"{example_dataset_folder}/ORCA025-N06_20000104d05U.nc\\\")\\n\",\n     \"cvfields = xr.open_dataset(f\\\"{example_dataset_folder}/ORCA025-N06_20000104d05V.nc\\\")\\n\",\n     \"\\n\",\n@@ -890,7 +890,9 @@\n    },\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoNorthSeaORCA025-N006_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\n\",\n+    \"    \\\"NemoNorthSeaORCA025-N006_data\\\"\\n\",\n+    \")\\n\",\n     \"ufiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*U.nc\\\"))\\n\",\n     \"vfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*V.nc\\\"))\\n\",\n     \"wfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*W.nc\\\"))\\n\",\n@@ -930,19 +932,24 @@\n     \"    \\\"W\\\": {\\\"lon\\\": \\\"glamf\\\", \\\"lat\\\": \\\"gphif\\\", \\\"depth\\\": \\\"depthw\\\", \\\"time\\\": \\\"time_counter\\\"},\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n     \"npart = 10  # number of particles to be released\\n\",\n     \"lon = np.linspace(3, 4, npart, dtype=np.float32)\\n\",\n     \"lat = 51.5 * np.ones(npart)\\n\",\n     \"time = np.zeros(npart)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"Cgrid-stuck.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"Cgrid-stuck.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(days=10), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(days=10),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -1202,17 +1209,6 @@\n     \"This random walk can be added using a diffusion kernel, as documented in [this notebook](tutorial_diffusion.ipynb). Since the particles will move randomly through the domain, without awareness of the solid-fluid boundaries in the velocity field, we cannot define stuck particles as having moved less than a tolerance value and we will instead check whether particles find themselves on land or not. To do this, we sample a `landmask` field which denotes the f-nodes that are part of the solid.\\n\"\n    ]\n   },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 20,\n-   \"metadata\": {\n-    \"hide_input\": true\n-   },\n-   \"outputs\": [],\n-   \"source\": [\n-    \"from parcels import DiffusionUniformKh, Kernel\"\n-   ]\n-  },\n   {\n    \"cell_type\": \"code\",\n    \"execution_count\": 21,\n@@ -1221,7 +1217,7 @@\n    },\n    \"outputs\": [],\n    \"source\": [\n-    \"LandParticle = JITParticle.add_variable(\\\"on_land\\\")\\n\",\n+    \"LandParticle = parcels.JITParticle.add_variable(\\\"on_land\\\")\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"def Sample_land(particle, fieldset, time):\\n\",\n@@ -1274,13 +1270,13 @@\n     \"    \\\"W\\\": {\\\"lon\\\": \\\"glamf\\\", \\\"lat\\\": \\\"gphif\\\", \\\"depth\\\": \\\"depthw\\\", \\\"time\\\": \\\"time_counter\\\"},\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_zonal\\\", 5, mesh=\\\"spherical\\\")\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_meridional\\\", 5, mesh=\\\"spherical\\\")\\n\",\n     \"#  fieldset.add_constant('dres', 0.00005)\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"landmask\\\",\\n\",\n     \"        data=landmask,\\n\",\n     \"        lon=coords[\\\"glamf\\\"],\\n\",\n@@ -1294,15 +1290,19 @@\n     \"lat = 51.5 * np.ones(npart)\\n\",\n     \"time = np.zeros(npart)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=LandParticle, lon=lon, lat=lat, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=LandParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"Cgrid-diffusion.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(\\n\",\n+    \"    name=\\\"Cgrid-diffusion.zarr\\\", outputdt=timedelta(hours=1)\\n\",\n+    \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    pset.Kernel(AdvectionRK4)\\n\",\n-    \"    + pset.Kernel(DiffusionUniformKh)\\n\",\n+    \"    pset.Kernel(parcels.AdvectionRK4)\\n\",\n+    \"    + pset.Kernel(parcels.DiffusionUniformKh)\\n\",\n     \"    + pset.Kernel(Sample_land),\\n\",\n-    \"    runtime=delta(days=10),\\n\",\n-    \"    dt=delta(minutes=5),\\n\",\n+    \"    runtime=timedelta(days=10),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n     \")\"\n    ]\ndiff --git a/docs/examples/documentation_unstuck_Agrid.ipynb b/docs/examples/documentation_unstuck_Agrid.ipynb\nindex 5cb51cf39..4599fcdae 100644\n--- a/docs/examples/documentation_unstuck_Agrid.ipynb\n+++ b/docs/examples/documentation_unstuck_Agrid.ipynb\n@@ -43,7 +43,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from copy import copy\\n\",\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import cmocean\\n\",\n     \"import matplotlib.gridspec as gridspec\\n\",\n@@ -56,17 +56,7 @@\n     \"from netCDF4 import Dataset\\n\",\n     \"from scipy import interpolate\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    Field,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    Geographic,\\n\",\n-    \"    GeographicPolar,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    ScipyParticle,\\n\",\n-    \"    Variable,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -809,11 +799,11 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"DisplacementParticle = JITParticle.add_variables(\\n\",\n+    \"DisplacementParticle = parcels.JITParticle.add_variables(\\n\",\n     \"    [\\n\",\n-    \"        Variable(\\\"dU\\\"),\\n\",\n-    \"        Variable(\\\"dV\\\"),\\n\",\n-    \"        Variable(\\\"d2s\\\", initial=1e3),\\n\",\n+    \"        parcels.Variable(\\\"dU\\\"),\\n\",\n+    \"        parcels.Variable(\\\"dV\\\"),\\n\",\n+    \"        parcels.Variable(\\\"d2s\\\", initial=1e3),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -867,7 +857,9 @@\n     \"    \\\"lat\\\": range(latmin, latmax),\\n\",\n     \"}  # to load only a small part of the domain\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, indices=indices)\"\n+    \"fieldset = parcels.FieldSet.from_netcdf(\\n\",\n+    \"    filenames, variables, dimensions, indices=indices\\n\",\n+    \")\"\n    ]\n   },\n   {\n@@ -890,8 +882,8 @@\n     \"lons, lats = np.meshgrid(lon, lat)\\n\",\n     \"time = np.zeros(lons.size)\\n\",\n     \"\\n\",\n-    \"runtime = delta(hours=100)\\n\",\n-    \"dt = delta(minutes=10)\"\n+    \"runtime = timedelta(hours=100)\\n\",\n+    \"dt = timedelta(minutes=10)\"\n    ]\n   },\n   {\n@@ -909,11 +901,13 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"kernels = AdvectionRK4\\n\",\n+    \"kernels = parcels.AdvectionRK4\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SMOC.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"SMOC.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(kernels, runtime=runtime, dt=dt, output_file=output_file)\"\n    ]\n@@ -932,11 +926,13 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, indices=indices)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(\\n\",\n+    \"    filenames, variables, dimensions, indices=indices\\n\",\n+    \")\\n\",\n     \"u_displacement = v_x\\n\",\n     \"v_displacement = v_y\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"dispU\\\",\\n\",\n     \"        data=u_displacement[latmin:latmax, lonmin:lonmax],\\n\",\n     \"        lon=fieldset.U.grid.lon,\\n\",\n@@ -945,7 +941,7 @@\n     \"    )\\n\",\n     \")\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"dispV\\\",\\n\",\n     \"        data=v_displacement[latmin:latmax, lonmin:lonmax],\\n\",\n     \"        lon=fieldset.U.grid.lon,\\n\",\n@@ -953,8 +949,8 @@\n     \"        mesh=\\\"spherical\\\",\\n\",\n     \"    )\\n\",\n     \")\\n\",\n-    \"fieldset.dispU.units = GeographicPolar()\\n\",\n-    \"fieldset.dispV.units = Geographic()\"\n+    \"fieldset.dispU.units = parcels.GeographicPolar()\\n\",\n+    \"fieldset.dispV.units = parcels.Geographic()\"\n    ]\n   },\n   {\n@@ -964,7 +960,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"landmask\\\",\\n\",\n     \"        landmask[latmin:latmax, lonmin:lonmax],\\n\",\n     \"        lon=fieldset.U.grid.lon,\\n\",\n@@ -973,7 +969,7 @@\n     \"    )\\n\",\n     \")\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"distance2shore\\\",\\n\",\n     \"        d_2_s[latmin:latmax, lonmin:lonmax],\\n\",\n     \"        lon=fieldset.U.grid.lon,\\n\",\n@@ -998,13 +994,13 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset, pclass=DisplacementParticle, lon=lons, lat=lats, time=time\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"kernels = [displace, AdvectionRK4, set_displacement]\\n\",\n+    \"kernels = [displace, parcels.AdvectionRK4, set_displacement]\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SMOC-disp.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"SMOC-disp.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(kernels, runtime=runtime, dt=dt, output_file=output_file)\"\n    ]\n@@ -1500,7 +1496,7 @@\n     }\n    ],\n    \"source\": [\n-    \"fieldset = FieldSet.from_netcdf(\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(\\n\",\n     \"    filenames,\\n\",\n     \"    variables,\\n\",\n     \"    dimensions,\\n\",\n@@ -1511,11 +1507,15 @@\n     \"    },  # Setting the interpolation for U and V\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"kernels = pset.Kernel(AdvectionRK4)\\n\",\n+    \"kernels = pset.Kernel(parcels.AdvectionRK4)\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SMOC_partialslip.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(\\n\",\n+    \"    name=\\\"SMOC_partialslip.zarr\\\", outputdt=timedelta(hours=1)\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"pset.execute(kernels, runtime=runtime, dt=dt, output_file=output_file)\"\n    ]\n@@ -1543,7 +1543,7 @@\n     }\n    ],\n    \"source\": [\n-    \"fieldset = FieldSet.from_netcdf(\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(\\n\",\n     \"    filenames,\\n\",\n     \"    variables,\\n\",\n     \"    dimensions,\\n\",\n@@ -1553,11 +1553,13 @@\n     \"        \\\"V\\\": \\\"freeslip\\\",\\n\",\n     \"    },  # Setting the interpolation for U and V\\n\",\n     \")\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lons, lat=lats, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"kernels = pset.Kernel(AdvectionRK4)\\n\",\n+    \"kernels = pset.Kernel(parcels.AdvectionRK4)\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SMOC_freeslip.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"SMOC_freeslip.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(kernels, runtime=runtime, dt=dt, output_file=output_file)\"\n    ]\ndiff --git a/docs/examples/example_brownian.py b/docs/examples/example_brownian.py\nindex 7f996c059..370bd843f 100644\n--- a/docs/examples/example_brownian.py\n+++ b/docs/examples/example_brownian.py\n@@ -1,54 +1,54 @@\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import (\n-    DiffusionUniformKh,\n-    Field,\n-    FieldSet,\n-    JITParticle,\n-    ParcelsRandom,\n-    ParticleSet,\n-    ScipyParticle,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n def mesh_conversion(mesh):\n-    return (1852. * 60) if mesh == 'spherical' else 1.\n+    return (1852.0 * 60) if mesh == \"spherical\" else 1.0\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_brownian_example(mode, mesh, npart=3000):\n-    fieldset = FieldSet.from_data({'U': 0, 'V': 0}, {'lon': 0, 'lat': 0}, mesh=mesh)\n+    fieldset = parcels.FieldSet.from_data(\n+        {\"U\": 0, \"V\": 0}, {\"lon\": 0, \"lat\": 0}, mesh=mesh\n+    )\n \n     # Set diffusion constants.\n     kh_zonal = 100  # in m^2/s\n     kh_meridional = 100  # in m^2/s\n \n     # Create field of constant Kh_zonal and Kh_meridional\n-    fieldset.add_field(Field('Kh_zonal', kh_zonal, lon=0, lat=0, mesh=mesh))\n-    fieldset.add_field(Field('Kh_meridional', kh_meridional, lon=0, lat=0, mesh=mesh))\n+    fieldset.add_field(parcels.Field(\"Kh_zonal\", kh_zonal, lon=0, lat=0, mesh=mesh))\n+    fieldset.add_field(\n+        parcels.Field(\"Kh_meridional\", kh_meridional, lon=0, lat=0, mesh=mesh)\n+    )\n \n     # Set random seed\n-    ParcelsRandom.seed(123456)\n+    parcels.ParcelsRandom.seed(123456)\n \n-    runtime = delta(days=1)\n+    runtime = timedelta(days=1)\n \n-    ParcelsRandom.seed(1234)\n-    pset = ParticleSet(fieldset=fieldset, pclass=ptype[mode],\n-                       lon=np.zeros(npart), lat=np.zeros(npart))\n-    pset.execute(pset.Kernel(DiffusionUniformKh),\n-                 runtime=runtime, dt=delta(hours=1))\n+    parcels.ParcelsRandom.seed(1234)\n+    pset = parcels.ParticleSet(\n+        fieldset=fieldset, pclass=ptype[mode], lon=np.zeros(npart), lat=np.zeros(npart)\n+    )\n+    pset.execute(\n+        pset.Kernel(parcels.DiffusionUniformKh), runtime=runtime, dt=timedelta(hours=1)\n+    )\n \n-    expected_std_x = np.sqrt(2*kh_zonal*runtime.total_seconds())\n-    expected_std_y = np.sqrt(2*kh_meridional*runtime.total_seconds())\n+    expected_std_x = np.sqrt(2 * kh_zonal * runtime.total_seconds())\n+    expected_std_y = np.sqrt(2 * kh_meridional * runtime.total_seconds())\n \n     ys = pset.lat * mesh_conversion(mesh)\n-    xs = pset.lon * mesh_conversion(mesh)  # since near equator, we do not need to care about curvature effect\n+    xs = pset.lon * mesh_conversion(\n+        mesh\n+    )  # since near equator, we do not need to care about curvature effect\n \n     tol = 250  # 250m tolerance\n     assert np.allclose(np.std(xs), expected_std_x, atol=tol)\n@@ -58,4 +58,4 @@ def test_brownian_example(mode, mesh, npart=3000):\n \n \n if __name__ == \"__main__\":\n-    test_brownian_example('jit', 'spherical', npart=2000)\n+    test_brownian_example(\"jit\", \"spherical\", npart=2000)\ndiff --git a/docs/examples/example_dask_chunk_OCMs.py b/docs/examples/example_dask_chunk_OCMs.py\nindex def7bdd53..99c65d957 100644\n--- a/docs/examples/example_dask_chunk_OCMs.py\n+++ b/docs/examples/example_dask_chunk_OCMs.py\n@@ -1,30 +1,21 @@\n import math\n-from datetime import timedelta as delta\n+from datetime import timedelta\n from glob import glob\n \n import dask\n import numpy as np\n import pytest\n \n-from parcels import (\n-    AdvectionRK4,\n-    Field,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    Variable,\n-    download_example_dataset,\n-)\n+import parcels\n from parcels.tools.statuscodes import DaskChunkingError\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n def compute_nemo_particle_advection(fieldset, mode):\n     npart = 20\n     lonp = 2.5 * np.ones(npart)\n-    latp = [i for i in 52.0+(-1e-3+np.random.rand(npart)*2.0*1e-3)]\n+    latp = [i for i in 52.0 + (-1e-3 + np.random.rand(npart) * 2.0 * 1e-3)]\n \n     def periodicBC(particle, fieldSet, time):\n         if particle.lon > 15.0:\n@@ -36,206 +27,331 @@ def periodicBC(particle, fieldSet, time):\n         if particle.lat < 49.0:\n             particle_dlat += 11.0  # noqa\n \n-    pset = ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n-    kernels = pset.Kernel(AdvectionRK4) + periodicBC\n-    pset.execute(kernels, runtime=delta(days=4), dt=delta(hours=6))\n+    pset = parcels.ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n+    kernels = pset.Kernel(parcels.AdvectionRK4) + periodicBC\n+    pset.execute(kernels, runtime=timedelta(days=4), dt=timedelta(hours=6))\n     return pset\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific', 'failsafe'])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\"chunk_mode\", [False, \"auto\", \"specific\", \"failsafe\"])\n def test_nemo_3D(mode, chunk_mode):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '256KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"256KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-    data_folder = download_example_dataset('NemoNorthSeaORCA025-N006_data')\n-    ufiles = sorted(glob(f'{data_folder}/ORCA*U.nc'))\n-    vfiles = sorted(glob(f'{data_folder}/ORCA*V.nc'))\n-    wfiles = sorted(glob(f'{data_folder}/ORCA*W.nc'))\n-    mesh_mask = f'{data_folder}/coordinates.nc'\n-\n-    filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n-                 'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles},\n-                 'W': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': wfiles}}\n-    variables = {'U': 'uo',\n-                 'V': 'vo',\n-                 'W': 'wo'}\n-    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'W': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+    data_folder = parcels.download_example_dataset(\"NemoNorthSeaORCA025-N006_data\")\n+    ufiles = sorted(glob(f\"{data_folder}/ORCA*U.nc\"))\n+    vfiles = sorted(glob(f\"{data_folder}/ORCA*V.nc\"))\n+    wfiles = sorted(glob(f\"{data_folder}/ORCA*W.nc\"))\n+    mesh_mask = f\"{data_folder}/coordinates.nc\"\n+\n+    filenames = {\n+        \"U\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": ufiles},\n+        \"V\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": vfiles},\n+        \"W\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": wfiles},\n+    }\n+    variables = {\"U\": \"uo\", \"V\": \"vo\", \"W\": \"wo\"}\n+    dimensions = {\n+        \"U\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"V\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"W\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+    }\n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific':\n-        chs = {'U': {'depth': ('depthu', 75), 'lat': ('y', 16), 'lon': ('x', 16)},\n-               'V': {'depth': ('depthv', 75), 'lat': ('y', 16), 'lon': ('x', 16)},\n-               'W': {'depth': ('depthw', 75), 'lat': ('y', 16), 'lon': ('x', 16)}}\n-    elif chunk_mode == 'failsafe':  # chunking time and but not depth\n-        filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n-                     'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles}}\n-        variables = {'U': 'uo',\n-                     'V': 'vo'}\n-        dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                      'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n-        chs = {'U': {'time': ('time_counter', 1), 'depth': ('depthu', 25), 'lat': ('y', -1), 'lon': ('x', -1)},\n-               'V': {'time': ('time_counter', 1), 'depth': ('depthv', 75), 'lat': ('y', -1), 'lon': ('x', -1)}}\n-\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions, chunksize=chs)\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific\":\n+        chs = {\n+            \"U\": {\"depth\": (\"depthu\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+            \"V\": {\"depth\": (\"depthv\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+            \"W\": {\"depth\": (\"depthw\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+        }\n+    elif chunk_mode == \"failsafe\":  # chunking time and but not depth\n+        filenames = {\n+            \"U\": {\n+                \"lon\": mesh_mask,\n+                \"lat\": mesh_mask,\n+                \"depth\": wfiles[0],\n+                \"data\": ufiles,\n+            },\n+            \"V\": {\n+                \"lon\": mesh_mask,\n+                \"lat\": mesh_mask,\n+                \"depth\": wfiles[0],\n+                \"data\": vfiles,\n+            },\n+        }\n+        variables = {\"U\": \"uo\", \"V\": \"vo\"}\n+        dimensions = {\n+            \"U\": {\n+                \"lon\": \"glamf\",\n+                \"lat\": \"gphif\",\n+                \"depth\": \"depthw\",\n+                \"time\": \"time_counter\",\n+            },\n+            \"V\": {\n+                \"lon\": \"glamf\",\n+                \"lat\": \"gphif\",\n+                \"depth\": \"depthw\",\n+                \"time\": \"time_counter\",\n+            },\n+        }\n+        chs = {\n+            \"U\": {\n+                \"time\": (\"time_counter\", 1),\n+                \"depth\": (\"depthu\", 25),\n+                \"lat\": (\"y\", -1),\n+                \"lon\": (\"x\", -1),\n+            },\n+            \"V\": {\n+                \"time\": (\"time_counter\", 1),\n+                \"depth\": (\"depthv\", 75),\n+                \"lat\": (\"y\", -1),\n+                \"lon\": (\"x\", -1),\n+            },\n+        }\n+\n+    fieldset = parcels.FieldSet.from_nemo(\n+        filenames, variables, dimensions, chunksize=chs\n+    )\n \n     compute_nemo_particle_advection(fieldset, mode)\n     # Nemo sample file dimensions: depthu=75, y=201, x=151\n-    if chunk_mode != 'failsafe':\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.W.grid.load_chunk))\n+    if chunk_mode != \"failsafe\":\n+        assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n+        assert len(fieldset.U.grid.load_chunk) == len(fieldset.W.grid.load_chunk)\n     if chunk_mode is False:\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n-    elif chunk_mode == 'auto':\n-        assert fieldset.gridset.size == 3  # because three different grids in 'auto' mode\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-    elif chunk_mode == 'specific':\n-        assert (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(75.0/75.0)) * int(math.ceil(201.0/16.0)) * int(math.ceil(151.0/16.0))))\n-    elif chunk_mode == 'failsafe':  # chunking time and depth but not lat and lon\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-        assert (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(75.0/25.0)) * int(math.ceil(201.0/171.0)) * int(math.ceil(151.0/151.0))))\n-        assert (len(fieldset.V.grid.load_chunk) != 1)\n-        assert (len(fieldset.V.grid.load_chunk) == (1 * int(math.ceil(75.0/75.0)) * int(math.ceil(201.0/171.0)) * int(math.ceil(151.0/151.0))))\n-\n-\n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific', 'failsafe'])\n+        assert len(fieldset.U.grid.load_chunk) == 1\n+    elif chunk_mode == \"auto\":\n+        assert (\n+            fieldset.gridset.size == 3\n+        )  # because three different grids in 'auto' mode\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+    elif chunk_mode == \"specific\":\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(75.0 / 75.0))\n+            * int(math.ceil(201.0 / 16.0))\n+            * int(math.ceil(151.0 / 16.0))\n+        )\n+    elif chunk_mode == \"failsafe\":  # chunking time and depth but not lat and lon\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(75.0 / 25.0))\n+            * int(math.ceil(201.0 / 171.0))\n+            * int(math.ceil(151.0 / 151.0))\n+        )\n+        assert len(fieldset.V.grid.load_chunk) != 1\n+        assert len(fieldset.V.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(75.0 / 75.0))\n+            * int(math.ceil(201.0 / 171.0))\n+            * int(math.ceil(151.0 / 151.0))\n+        )\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\"chunk_mode\", [False, \"auto\", \"specific\", \"failsafe\"])\n def test_globcurrent_2D(mode, chunk_mode):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '16KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"16KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-    data_folder = download_example_dataset(\"GlobCurrent_example_data\")\n-    filenames = str(data_folder / '200201*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc')\n-    variables = {'U': 'eastward_eulerian_current_velocity', 'V': 'northward_eulerian_current_velocity'}\n-    dimensions = {'lat': 'lat', 'lon': 'lon', 'time': 'time'}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+    data_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\n+    filenames = str(\n+        data_folder / \"200201*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc\"\n+    )\n+    variables = {\n+        \"U\": \"eastward_eulerian_current_velocity\",\n+        \"V\": \"northward_eulerian_current_velocity\",\n+    }\n+    dimensions = {\"lat\": \"lat\", \"lon\": \"lon\", \"time\": \"time\"}\n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific':\n-        chs = {'U': {'lat': ('lat', 8), 'lon': ('lon', 8)},\n-               'V': {'lat': ('lat', 8), 'lon': ('lon', 8)}}\n-    elif chunk_mode == 'failsafe':  # chunking time but not lat\n-        chs = {'U': {'time': ('time', 1), 'lat': ('lat', 10), 'lon': ('lon', -1)},\n-               'V': {'time': ('time', 1), 'lat': ('lat', -1), 'lon': ('lon', -1)}}\n-\n-    fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, chunksize=chs)\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific\":\n+        chs = {\n+            \"U\": {\"lat\": (\"lat\", 8), \"lon\": (\"lon\", 8)},\n+            \"V\": {\"lat\": (\"lat\", 8), \"lon\": (\"lon\", 8)},\n+        }\n+    elif chunk_mode == \"failsafe\":  # chunking time but not lat\n+        chs = {\n+            \"U\": {\"time\": (\"time\", 1), \"lat\": (\"lat\", 10), \"lon\": (\"lon\", -1)},\n+            \"V\": {\"time\": (\"time\", 1), \"lat\": (\"lat\", -1), \"lon\": (\"lon\", -1)},\n+        }\n+\n+    fieldset = parcels.FieldSet.from_netcdf(\n+        filenames, variables, dimensions, chunksize=chs\n+    )\n     try:\n-        pset = ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n-        pset.execute(AdvectionRK4, runtime=delta(days=1), dt=delta(minutes=5))\n+        pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n+        pset.execute(\n+            parcels.AdvectionRK4, runtime=timedelta(days=1), dt=timedelta(minutes=5)\n+        )\n     except DaskChunkingError:\n         # we removed the failsafe, so now if all chunksize dimensions are incorrect, there is nothing left to chunk,\n         # which raises an error saying so. This is the expected behaviour\n-        if chunk_mode == 'failsafe':\n+        if chunk_mode == \"failsafe\":\n             return\n     # GlobCurrent sample file dimensions: time=UNLIMITED, lat=41, lon=81\n-    if chunk_mode != 'failsafe':  # chunking time but not lat\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n+    if chunk_mode != \"failsafe\":  # chunking time but not lat\n+        assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n     if chunk_mode is False:\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n-    elif chunk_mode == 'auto':\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-    elif chunk_mode == 'specific':\n-        assert (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(41.0/8.0)) * int(math.ceil(81.0/8.0))))\n-    elif chunk_mode == 'failsafe':  # chunking time but not lat\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-        assert (len(fieldset.V.grid.load_chunk) != 1)\n+        assert len(fieldset.U.grid.load_chunk) == 1\n+    elif chunk_mode == \"auto\":\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+    elif chunk_mode == \"specific\":\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            1 * int(math.ceil(41.0 / 8.0)) * int(math.ceil(81.0 / 8.0))\n+        )\n+    elif chunk_mode == \"failsafe\":  # chunking time but not lat\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+        assert len(fieldset.V.grid.load_chunk) != 1\n     assert abs(pset[0].lon - 23.8) < 1\n     assert abs(pset[0].lat - -35.3) < 1\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific', 'failsafe'])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\"chunk_mode\", [False, \"auto\", \"specific\", \"failsafe\"])\n def test_pop(mode, chunk_mode):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '256KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"256KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-    data_folder = download_example_dataset('POPSouthernOcean_data')\n-    filenames = str(data_folder / 't.x1_SAMOC_flux.1690*.nc')\n-    variables = {'U': 'UVEL', 'V': 'VVEL', 'W': 'WVEL'}\n-    timestamps = np.expand_dims(np.array([np.datetime64('2000-%.2d-01' % m) for m in range(1, 7)]), axis=1)\n-    dimensions = {'lon': 'ULON', 'lat': 'ULAT', 'depth': 'w_dep'}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+    data_folder = parcels.download_example_dataset(\"POPSouthernOcean_data\")\n+    filenames = str(data_folder / \"t.x1_SAMOC_flux.1690*.nc\")\n+    variables = {\"U\": \"UVEL\", \"V\": \"VVEL\", \"W\": \"WVEL\"}\n+    timestamps = np.expand_dims(\n+        np.array([np.datetime64(\"2000-%.2d-01\" % m) for m in range(1, 7)]), axis=1\n+    )\n+    dimensions = {\"lon\": \"ULON\", \"lat\": \"ULAT\", \"depth\": \"w_dep\"}\n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific':\n-        chs = {'lon': ('i', 8), 'lat': ('j', 8), 'depth': ('k', 3)}\n-    elif chunk_mode == 'failsafe':  # here: bad depth entry\n-        chs = {'depth': ('wz', 3), 'lat': ('j', 8), 'lon': ('i', 8)}\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific\":\n+        chs = {\"lon\": (\"i\", 8), \"lat\": (\"j\", 8), \"depth\": (\"k\", 3)}\n+    elif chunk_mode == \"failsafe\":  # here: bad depth entry\n+        chs = {\"depth\": (\"wz\", 3), \"lat\": (\"j\", 8), \"lon\": (\"i\", 8)}\n \n-    fieldset = FieldSet.from_pop(filenames, variables, dimensions, chunksize=chs, timestamps=timestamps)\n+    fieldset = parcels.FieldSet.from_pop(\n+        filenames, variables, dimensions, chunksize=chs, timestamps=timestamps\n+    )\n \n     npart = 20\n     lonp = 70.0 * np.ones(npart)\n-    latp = [i for i in -45.0+(-0.25+np.random.rand(npart)*2.0*0.25)]\n-    pset = ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n-    pset.execute(AdvectionRK4, runtime=delta(days=90), dt=delta(days=2))\n+    latp = [i for i in -45.0 + (-0.25 + np.random.rand(npart) * 2.0 * 0.25)]\n+    pset = parcels.ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n+    pset.execute(parcels.AdvectionRK4, runtime=timedelta(days=90), dt=timedelta(days=2))\n     # POP sample file dimensions: k=21, j=60, i=60\n-    assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n-    assert (len(fieldset.U.grid.load_chunk) == len(fieldset.W.grid.load_chunk))\n+    assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n+    assert len(fieldset.U.grid.load_chunk) == len(fieldset.W.grid.load_chunk)\n     if chunk_mode is False:\n         assert fieldset.gridset.size == 1\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n-        assert (len(fieldset.V.grid.load_chunk) == 1)\n-        assert (len(fieldset.W.grid.load_chunk) == 1)\n-    elif chunk_mode == 'auto':\n-        assert fieldset.gridset.size == 3  # because three different grids in 'auto' mode\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-        assert (len(fieldset.V.grid.load_chunk) != 1)\n-        assert (len(fieldset.W.grid.load_chunk) != 1)\n-    elif chunk_mode == 'specific':\n+        assert len(fieldset.U.grid.load_chunk) == 1\n+        assert len(fieldset.V.grid.load_chunk) == 1\n+        assert len(fieldset.W.grid.load_chunk) == 1\n+    elif chunk_mode == \"auto\":\n+        assert (\n+            fieldset.gridset.size == 3\n+        )  # because three different grids in 'auto' mode\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+        assert len(fieldset.V.grid.load_chunk) != 1\n+        assert len(fieldset.W.grid.load_chunk) != 1\n+    elif chunk_mode == \"specific\":\n         assert fieldset.gridset.size == 1\n-        assert (len(fieldset.U.grid.load_chunk) == (int(math.ceil(21.0/3.0)) * int(math.ceil(60.0/8.0)) * int(math.ceil(60.0/8.0))))\n-    elif chunk_mode == 'failsafe':   # here: done a typo in the netcdf dimname field\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            int(math.ceil(21.0 / 3.0))\n+            * int(math.ceil(60.0 / 8.0))\n+            * int(math.ceil(60.0 / 8.0))\n+        )\n+    elif chunk_mode == \"failsafe\":  # here: done a typo in the netcdf dimname field\n         assert fieldset.gridset.size == 1\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-        assert (len(fieldset.V.grid.load_chunk) != 1)\n-        assert (len(fieldset.W.grid.load_chunk) != 1)\n-        assert (len(fieldset.U.grid.load_chunk) == (int(math.ceil(21.0/3.0)) * int(math.ceil(60.0/8.0)) * int(math.ceil(60.0/8.0))))\n-\n-\n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific', 'failsafe'])\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+        assert len(fieldset.V.grid.load_chunk) != 1\n+        assert len(fieldset.W.grid.load_chunk) != 1\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            int(math.ceil(21.0 / 3.0))\n+            * int(math.ceil(60.0 / 8.0))\n+            * int(math.ceil(60.0 / 8.0))\n+        )\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\"chunk_mode\", [False, \"auto\", \"specific\", \"failsafe\"])\n def test_swash(mode, chunk_mode):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '32KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"32KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-    data_folder = download_example_dataset('SWASH_data')\n-    filenames = str(data_folder / 'field_*.nc')\n-    variables = {'U': 'cross-shore velocity',\n-                 'V': 'along-shore velocity',\n-                 'W': 'vertical velocity',\n-                 'depth_w': 'time varying depth',\n-                 'depth_u': 'time varying depth_u'}\n-    dimensions = {'U': {'lon': 'x', 'lat': 'y', 'depth': 'not_yet_set', 'time': 't'},\n-                  'V': {'lon': 'x', 'lat': 'y', 'depth': 'not_yet_set', 'time': 't'},\n-                  'W': {'lon': 'x', 'lat': 'y', 'depth': 'not_yet_set', 'time': 't'},\n-                  'depth_w': {'lon': 'x', 'lat': 'y', 'depth': 'not_yet_set', 'time': 't'},\n-                  'depth_u': {'lon': 'x', 'lat': 'y', 'depth': 'not_yet_set', 'time': 't'}}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+    data_folder = parcels.download_example_dataset(\"SWASH_data\")\n+    filenames = str(data_folder / \"field_*.nc\")\n+    variables = {\n+        \"U\": \"cross-shore velocity\",\n+        \"V\": \"along-shore velocity\",\n+        \"W\": \"vertical velocity\",\n+        \"depth_w\": \"time varying depth\",\n+        \"depth_u\": \"time varying depth_u\",\n+    }\n+    dimensions = {\n+        \"U\": {\"lon\": \"x\", \"lat\": \"y\", \"depth\": \"not_yet_set\", \"time\": \"t\"},\n+        \"V\": {\"lon\": \"x\", \"lat\": \"y\", \"depth\": \"not_yet_set\", \"time\": \"t\"},\n+        \"W\": {\"lon\": \"x\", \"lat\": \"y\", \"depth\": \"not_yet_set\", \"time\": \"t\"},\n+        \"depth_w\": {\"lon\": \"x\", \"lat\": \"y\", \"depth\": \"not_yet_set\", \"time\": \"t\"},\n+        \"depth_u\": {\"lon\": \"x\", \"lat\": \"y\", \"depth\": \"not_yet_set\", \"time\": \"t\"},\n+    }\n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific':\n-        chs = {'U': {'depth': ('z_u', 6), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'V': {'depth': ('z_u', 6), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'W': {'depth': ('z', 7), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'depth_w': {'depth': ('z', 7), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'depth_u': {'depth': ('z_u', 6), 'lat': ('y', 4), 'lon': ('x', 4)}\n-               }\n-    elif chunk_mode == 'failsafe':  # here: incorrect matching between dimension names and their attachment to the NC-variables\n-        chs = {'U': {'depth': ('depth', 7), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'V': {'depth': ('z_u', 6), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'W': {'depth': ('z', 7), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'depth_w': {'depth': ('z', 7), 'lat': ('y', 4), 'lon': ('x', 4)},\n-               'depth_u': {'depth': ('z_u', 6), 'lat': ('y', 4), 'lon': ('x', 4)}\n-               }\n-    fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, mesh='flat', allow_time_extrapolation=True, chunksize=chs)\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific\":\n+        chs = {\n+            \"U\": {\"depth\": (\"z_u\", 6), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"V\": {\"depth\": (\"z_u\", 6), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"W\": {\"depth\": (\"z\", 7), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"depth_w\": {\"depth\": (\"z\", 7), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"depth_u\": {\"depth\": (\"z_u\", 6), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+        }\n+    elif (\n+        chunk_mode == \"failsafe\"\n+    ):  # here: incorrect matching between dimension names and their attachment to the NC-variables\n+        chs = {\n+            \"U\": {\"depth\": (\"depth\", 7), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"V\": {\"depth\": (\"z_u\", 6), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"W\": {\"depth\": (\"z\", 7), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"depth_w\": {\"depth\": (\"z\", 7), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+            \"depth_u\": {\"depth\": (\"z_u\", 6), \"lat\": (\"y\", 4), \"lon\": (\"x\", 4)},\n+        }\n+    fieldset = parcels.FieldSet.from_netcdf(\n+        filenames,\n+        variables,\n+        dimensions,\n+        mesh=\"flat\",\n+        allow_time_extrapolation=True,\n+        chunksize=chs,\n+    )\n     fieldset.U.set_depth_from_field(fieldset.depth_u)\n     fieldset.V.set_depth_from_field(fieldset.depth_u)\n     fieldset.W.set_depth_from_field(fieldset.depth_w)\n@@ -243,207 +359,344 @@ def test_swash(mode, chunk_mode):\n     npart = 20\n     lonp = [i for i in 9.5 + (-0.2 + np.random.rand(npart) * 2.0 * 0.2)]\n     latp = [i for i in np.arange(start=12.3, stop=13.1, step=0.04)[0:20]]\n-    depthp = [-0.1, ] * npart\n-    pset = ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp, depth=depthp)\n-    pset.execute(AdvectionRK4, runtime=delta(seconds=0.2), dt=delta(seconds=0.005))\n+    depthp = [\n+        -0.1,\n+    ] * npart\n+    pset = parcels.ParticleSet.from_list(\n+        fieldset, ptype[mode], lon=lonp, lat=latp, depth=depthp\n+    )\n+    pset.execute(\n+        parcels.AdvectionRK4,\n+        runtime=timedelta(seconds=0.2),\n+        dt=timedelta(seconds=0.005),\n+    )\n     # SWASH sample file dimensions: t=1, z=7, z_u=6, y=21, x=51\n-    if chunk_mode not in ['failsafe', ]:\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)), f\"U {fieldset.U.grid.chunk_info} vs V {fieldset.V.grid.chunk_info}\"\n-    if chunk_mode not in ['failsafe', 'auto']:\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.W.grid.load_chunk)), f\"U {fieldset.U.grid.chunk_info} vs W {fieldset.W.grid.chunk_info}\"\n+    if chunk_mode not in [\n+        \"failsafe\",\n+    ]:\n+        assert len(fieldset.U.grid.load_chunk) == len(\n+            fieldset.V.grid.load_chunk\n+        ), f\"U {fieldset.U.grid.chunk_info} vs V {fieldset.V.grid.chunk_info}\"\n+    if chunk_mode not in [\"failsafe\", \"auto\"]:\n+        assert len(fieldset.U.grid.load_chunk) == len(\n+            fieldset.W.grid.load_chunk\n+        ), f\"U {fieldset.U.grid.chunk_info} vs W {fieldset.W.grid.chunk_info}\"\n     if chunk_mode is False:\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n+        assert len(fieldset.U.grid.load_chunk) == 1\n     else:\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-        assert (len(fieldset.V.grid.load_chunk) != 1)\n-        assert (len(fieldset.W.grid.load_chunk) != 1)\n-    if chunk_mode == 'specific':\n-        assert (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(6.0 / 6.0)) * int(math.ceil(21.0 / 4.0)) * int(math.ceil(51.0 / 4.0))))\n-        assert (len(fieldset.V.grid.load_chunk) == (1 * int(math.ceil(6.0 / 6.0)) * int(math.ceil(21.0 / 4.0)) * int(math.ceil(51.0 / 4.0))))\n-        assert (len(fieldset.W.grid.load_chunk) == (1 * int(math.ceil(7.0 / 7.0)) * int(math.ceil(21.0 / 4.0)) * int(math.ceil(51.0 / 4.0))))\n-\n-\n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific'])\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+        assert len(fieldset.V.grid.load_chunk) != 1\n+        assert len(fieldset.W.grid.load_chunk) != 1\n+    if chunk_mode == \"specific\":\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(6.0 / 6.0))\n+            * int(math.ceil(21.0 / 4.0))\n+            * int(math.ceil(51.0 / 4.0))\n+        )\n+        assert len(fieldset.V.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(6.0 / 6.0))\n+            * int(math.ceil(21.0 / 4.0))\n+            * int(math.ceil(51.0 / 4.0))\n+        )\n+        assert len(fieldset.W.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(7.0 / 7.0))\n+            * int(math.ceil(21.0 / 4.0))\n+            * int(math.ceil(51.0 / 4.0))\n+        )\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\"chunk_mode\", [False, \"auto\", \"specific\"])\n def test_ofam_3D(mode, chunk_mode):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '1024KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"1024KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-\n-    data_folder = download_example_dataset(\"OFAM_example_data\")\n-    filenames = {'U': f\"{data_folder}/OFAM_simple_U.nc\",\n-                 'V': f\"{data_folder}/OFAM_simple_V.nc\"}\n-    variables = {'U': 'u', 'V': 'v'}\n-    dimensions = {'lat': 'yu_ocean', 'lon': 'xu_ocean', 'depth': 'st_ocean',\n-                  'time': 'Time'}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+\n+    data_folder = parcels.download_example_dataset(\"OFAM_example_data\")\n+    filenames = {\n+        \"U\": f\"{data_folder}/OFAM_simple_U.nc\",\n+        \"V\": f\"{data_folder}/OFAM_simple_V.nc\",\n+    }\n+    variables = {\"U\": \"u\", \"V\": \"v\"}\n+    dimensions = {\n+        \"lat\": \"yu_ocean\",\n+        \"lon\": \"xu_ocean\",\n+        \"depth\": \"st_ocean\",\n+        \"time\": \"Time\",\n+    }\n \n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific':\n-        chs = {'lon': ('xu_ocean', 100), 'lat': ('yu_ocean', 50), 'depth': ('st_edges_ocean', 60), 'time': ('Time', 1)}\n-    fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, allow_time_extrapolation=True, chunksize=chs)\n-\n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=180, lat=10, depth=2.5)\n-    pset.execute(AdvectionRK4, runtime=delta(days=10), dt=delta(minutes=5))\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific\":\n+        chs = {\n+            \"lon\": (\"xu_ocean\", 100),\n+            \"lat\": (\"yu_ocean\", 50),\n+            \"depth\": (\"st_edges_ocean\", 60),\n+            \"time\": (\"Time\", 1),\n+        }\n+    fieldset = parcels.FieldSet.from_netcdf(\n+        filenames, variables, dimensions, allow_time_extrapolation=True, chunksize=chs\n+    )\n+\n+    pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=180, lat=10, depth=2.5)\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=10), dt=timedelta(minutes=5)\n+    )\n     # OFAM sample file dimensions: time=UNLIMITED, st_ocean=1, st_edges_ocean=52, lat=601, lon=2001\n-    assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n+    assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n     if chunk_mode is False:\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n-    elif chunk_mode == 'auto':\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-    elif chunk_mode == 'specific':\n+        assert len(fieldset.U.grid.load_chunk) == 1\n+    elif chunk_mode == \"auto\":\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+    elif chunk_mode == \"specific\":\n         numblocks = [i for i in fieldset.U.grid.chunk_info[1:3]]\n         dblocks = 1\n         vblocks = 0\n-        for bsize in fieldset.U.grid.chunk_info[3:3+numblocks[0]]:\n+        for bsize in fieldset.U.grid.chunk_info[3 : 3 + numblocks[0]]:\n             vblocks += bsize\n         ublocks = 0\n-        for bsize in fieldset.U.grid.chunk_info[3+numblocks[0]:3+numblocks[0]+numblocks[1]]:\n+        for bsize in fieldset.U.grid.chunk_info[\n+            3 + numblocks[0] : 3 + numblocks[0] + numblocks[1]\n+        ]:\n             ublocks += bsize\n-        matching_numblocks = (ublocks == 2001 and vblocks == 601 and dblocks == 1)\n-        matching_fields = (fieldset.U.grid.chunk_info == fieldset.V.grid.chunk_info)\n-        matching_uniformblocks = (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(1.0/60.0)) * int(math.ceil(601.0/50.0)) * int(math.ceil(2001.0/100.0))))\n-        assert (matching_uniformblocks or (matching_fields and matching_numblocks))\n+        matching_numblocks = ublocks == 2001 and vblocks == 601 and dblocks == 1\n+        matching_fields = fieldset.U.grid.chunk_info == fieldset.V.grid.chunk_info\n+        matching_uniformblocks = len(fieldset.U.grid.load_chunk) == (\n+            1\n+            * int(math.ceil(1.0 / 60.0))\n+            * int(math.ceil(601.0 / 50.0))\n+            * int(math.ceil(2001.0 / 100.0))\n+        )\n+        assert matching_uniformblocks or (matching_fields and matching_numblocks)\n     assert abs(pset[0].lon - 173) < 1\n     assert abs(pset[0].lat - 11) < 1\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n-@pytest.mark.parametrize('chunk_mode', [False, 'auto', 'specific_same', 'specific_different'])\n-@pytest.mark.parametrize('using_add_field', [False, True])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n+@pytest.mark.parametrize(\n+    \"chunk_mode\", [False, \"auto\", \"specific_same\", \"specific_different\"]\n+)\n+@pytest.mark.parametrize(\"using_add_field\", [False, True])\n def test_mitgcm(mode, chunk_mode, using_add_field):\n-    if chunk_mode in ['auto', ]:\n-        dask.config.set({'array.chunk-size': '512KiB'})\n+    if chunk_mode in [\n+        \"auto\",\n+    ]:\n+        dask.config.set({\"array.chunk-size\": \"512KiB\"})\n     else:\n-        dask.config.set({'array.chunk-size': '128MiB'})\n-    data_folder = download_example_dataset(\"MITgcm_example_data\")\n-    filenames = {\"U\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\",\n-                 \"V\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\"}\n+        dask.config.set({\"array.chunk-size\": \"128MiB\"})\n+    data_folder = parcels.download_example_dataset(\"MITgcm_example_data\")\n+    filenames = {\n+        \"U\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\",\n+        \"V\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\",\n+    }\n     variables = {\"U\": \"UVEL\", \"V\": \"VVEL\"}\n-    dimensions = {\"U\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"},\n-                  \"V\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"}}\n+    dimensions = {\n+        \"U\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"},\n+        \"V\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"},\n+    }\n \n     chs = False\n-    if chunk_mode == 'auto':\n-        chs = 'auto'\n-    elif chunk_mode == 'specific_same':\n-        chs = {'U': {'lat': ('YC', 50), 'lon': ('XG', 100)},\n-               'V': {'lat': ('YG', 50), 'lon': ('XC', 100)}}\n-    elif chunk_mode == 'specific_different':\n-        chs = {'U': {'lat': ('YC', 50), 'lon': ('XG', 100)},\n-               'V': {'lat': ('YG', 40), 'lon': ('XC', 100)}}\n+    if chunk_mode == \"auto\":\n+        chs = \"auto\"\n+    elif chunk_mode == \"specific_same\":\n+        chs = {\n+            \"U\": {\"lat\": (\"YC\", 50), \"lon\": (\"XG\", 100)},\n+            \"V\": {\"lat\": (\"YG\", 50), \"lon\": (\"XC\", 100)},\n+        }\n+    elif chunk_mode == \"specific_different\":\n+        chs = {\n+            \"U\": {\"lat\": (\"YC\", 50), \"lon\": (\"XG\", 100)},\n+            \"V\": {\"lat\": (\"YG\", 40), \"lon\": (\"XC\", 100)},\n+        }\n     if using_add_field:\n-        if chs in [False, 'auto']:\n-            chs = {'U': chs, 'V': chs}\n-        fieldset = FieldSet.from_mitgcm(filenames['U'], {'U': variables['U']}, dimensions['U'],\n-                                        mesh='flat', chunksize=chs['U'])\n-        fieldset2 = FieldSet.from_mitgcm(filenames['V'], {'V': variables['V']}, dimensions['V'],\n-                                         mesh='flat', chunksize=chs['V'])\n+        if chs in [False, \"auto\"]:\n+            chs = {\"U\": chs, \"V\": chs}\n+        fieldset = parcels.FieldSet.from_mitgcm(\n+            filenames[\"U\"],\n+            {\"U\": variables[\"U\"]},\n+            dimensions[\"U\"],\n+            mesh=\"flat\",\n+            chunksize=chs[\"U\"],\n+        )\n+        fieldset2 = parcels.FieldSet.from_mitgcm(\n+            filenames[\"V\"],\n+            {\"V\": variables[\"V\"]},\n+            dimensions[\"V\"],\n+            mesh=\"flat\",\n+            chunksize=chs[\"V\"],\n+        )\n         fieldset.add_field(fieldset2.V)\n     else:\n-        fieldset = FieldSet.from_mitgcm(filenames, variables, dimensions, mesh='flat', chunksize=chs)\n-\n-    pset = ParticleSet.from_list(fieldset=fieldset, pclass=ptype[mode], lon=5e5, lat=5e5)\n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=delta(minutes=5))\n+        fieldset = parcels.FieldSet.from_mitgcm(\n+            filenames, variables, dimensions, mesh=\"flat\", chunksize=chs\n+        )\n+\n+    pset = parcels.ParticleSet.from_list(\n+        fieldset=fieldset, pclass=ptype[mode], lon=5e5, lat=5e5\n+    )\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=1), dt=timedelta(minutes=5)\n+    )\n     # MITgcm sample file dimensions: time=10, XG=400, YG=200\n-    if chunk_mode != 'specific_different':\n-        assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n-    if chunk_mode in [False, ]:\n-        assert (len(fieldset.U.grid.load_chunk) == 1)\n-    elif chunk_mode in ['auto', ]:\n-        assert (len(fieldset.U.grid.load_chunk) != 1)\n-    elif 'specific' in chunk_mode:\n-        assert (len(fieldset.U.grid.load_chunk) == (1 * int(math.ceil(400.0/50.0)) * int(math.ceil(200.0/100.0))))\n-    if chunk_mode == 'specific_same':\n+    if chunk_mode != \"specific_different\":\n+        assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n+    if chunk_mode in [\n+        False,\n+    ]:\n+        assert len(fieldset.U.grid.load_chunk) == 1\n+    elif chunk_mode in [\n+        \"auto\",\n+    ]:\n+        assert len(fieldset.U.grid.load_chunk) != 1\n+    elif \"specific\" in chunk_mode:\n+        assert len(fieldset.U.grid.load_chunk) == (\n+            1 * int(math.ceil(400.0 / 50.0)) * int(math.ceil(200.0 / 100.0))\n+        )\n+    if chunk_mode == \"specific_same\":\n         assert fieldset.gridset.size == 1\n-    elif chunk_mode == 'specific_different':\n+    elif chunk_mode == \"specific_different\":\n         assert fieldset.gridset.size == 2\n     assert np.allclose(pset[0].lon, 5.27e5, atol=1e3)\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n def test_diff_entry_dimensions_chunks(mode):\n-    data_folder = download_example_dataset('NemoNorthSeaORCA025-N006_data')\n-    ufiles = sorted(glob(f'{data_folder}/ORCA*U.nc'))\n-    vfiles = sorted(glob(f'{data_folder}/ORCA*V.nc'))\n-    mesh_mask = f'{data_folder}/coordinates.nc'\n-\n-    filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'data': ufiles},\n-                 'V': {'lon': mesh_mask, 'lat': mesh_mask, 'data': vfiles}}\n-    variables = {'U': 'uo',\n-                 'V': 'vo'}\n-    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'},\n-                  'V': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'}}\n-    chs = {'U': {'depth': ('depthu', 75), 'lat': ('y', 16), 'lon': ('x', 16)},\n-           'V': {'depth': ('depthv', 75), 'lat': ('y', 16), 'lon': ('x', 16)}}\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions, chunksize=chs)\n+    data_folder = parcels.download_example_dataset(\"NemoNorthSeaORCA025-N006_data\")\n+    ufiles = sorted(glob(f\"{data_folder}/ORCA*U.nc\"))\n+    vfiles = sorted(glob(f\"{data_folder}/ORCA*V.nc\"))\n+    mesh_mask = f\"{data_folder}/coordinates.nc\"\n+\n+    filenames = {\n+        \"U\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"data\": ufiles},\n+        \"V\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"data\": vfiles},\n+    }\n+    variables = {\"U\": \"uo\", \"V\": \"vo\"}\n+    dimensions = {\n+        \"U\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"time\": \"time_counter\"},\n+        \"V\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"time\": \"time_counter\"},\n+    }\n+    chs = {\n+        \"U\": {\"depth\": (\"depthu\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+        \"V\": {\"depth\": (\"depthv\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+    }\n+    fieldset = parcels.FieldSet.from_nemo(\n+        filenames, variables, dimensions, chunksize=chs\n+    )\n     compute_nemo_particle_advection(fieldset, mode)\n     # Nemo sample file dimensions: depthu=75, y=201, x=151\n-    assert (len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk))\n+    assert len(fieldset.U.grid.load_chunk) == len(fieldset.V.grid.load_chunk)\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_3d_2dfield_sampling(mode):\n-    data_folder = download_example_dataset('NemoNorthSeaORCA025-N006_data')\n-    ufiles = sorted(glob(f'{data_folder}/ORCA*U.nc'))\n-    vfiles = sorted(glob(f'{data_folder}/ORCA*V.nc'))\n-    mesh_mask = f'{data_folder}/coordinates.nc'\n-\n-    filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'data': ufiles},\n-                 'V': {'lon': mesh_mask, 'lat': mesh_mask, 'data': vfiles},\n-                 'nav_lon': {'lon': mesh_mask, 'lat': mesh_mask, 'data': [ufiles[0], ]}}\n-    variables = {'U': 'uo',\n-                 'V': 'vo',\n-                 'nav_lon': 'nav_lon'}\n-    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'},\n-                  'V': {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'},\n-                  'nav_lon': {'lon': 'glamf', 'lat': 'gphif'}}\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions, chunksize=False)\n+    data_folder = parcels.download_example_dataset(\"NemoNorthSeaORCA025-N006_data\")\n+    ufiles = sorted(glob(f\"{data_folder}/ORCA*U.nc\"))\n+    vfiles = sorted(glob(f\"{data_folder}/ORCA*V.nc\"))\n+    mesh_mask = f\"{data_folder}/coordinates.nc\"\n+\n+    filenames = {\n+        \"U\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"data\": ufiles},\n+        \"V\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"data\": vfiles},\n+        \"nav_lon\": {\n+            \"lon\": mesh_mask,\n+            \"lat\": mesh_mask,\n+            \"data\": [\n+                ufiles[0],\n+            ],\n+        },\n+    }\n+    variables = {\"U\": \"uo\", \"V\": \"vo\", \"nav_lon\": \"nav_lon\"}\n+    dimensions = {\n+        \"U\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"time\": \"time_counter\"},\n+        \"V\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"time\": \"time_counter\"},\n+        \"nav_lon\": {\"lon\": \"glamf\", \"lat\": \"gphif\"},\n+    }\n+    fieldset = parcels.FieldSet.from_nemo(\n+        filenames, variables, dimensions, chunksize=False\n+    )\n     fieldset.nav_lon.data = np.ones(fieldset.nav_lon.data.shape, dtype=np.float32)\n-    fieldset.add_field(Field('rectilinear_2D', np.ones((2, 2)),\n-                             lon=np.array([-10, 20]), lat=np.array([40, 80]), chunksize=False))\n+    fieldset.add_field(\n+        parcels.Field(\n+            \"rectilinear_2D\",\n+            np.ones((2, 2)),\n+            lon=np.array([-10, 20]),\n+            lat=np.array([40, 80]),\n+            chunksize=False,\n+        )\n+    )\n \n     class MyParticle(ptype[mode]):\n-        sample_var_curvilinear = Variable('sample_var_curvilinear')\n-        sample_var_rectilinear = Variable('sample_var_rectilinear')\n-    pset = ParticleSet(fieldset, pclass=MyParticle, lon=2.5, lat=52)\n+        sample_var_curvilinear = parcels.Variable(\"sample_var_curvilinear\")\n+        sample_var_rectilinear = parcels.Variable(\"sample_var_rectilinear\")\n+\n+    pset = parcels.ParticleSet(fieldset, pclass=MyParticle, lon=2.5, lat=52)\n \n     def Sample2D(particle, fieldset, time):\n-        particle.sample_var_curvilinear += fieldset.nav_lon[time, particle.depth, particle.lat, particle.lon]\n-        particle.sample_var_rectilinear += fieldset.rectilinear_2D[time, particle.depth, particle.lat, particle.lon]\n+        particle.sample_var_curvilinear += fieldset.nav_lon[\n+            time, particle.depth, particle.lat, particle.lon\n+        ]\n+        particle.sample_var_rectilinear += fieldset.rectilinear_2D[\n+            time, particle.depth, particle.lat, particle.lon\n+        ]\n \n-    runtime, dt = 86400*4, 6*3600\n-    pset.execute(pset.Kernel(AdvectionRK4) + Sample2D, runtime=runtime, dt=dt)\n+    runtime, dt = 86400 * 4, 6 * 3600\n+    pset.execute(pset.Kernel(parcels.AdvectionRK4) + Sample2D, runtime=runtime, dt=dt)\n \n-    assert pset.sample_var_rectilinear == runtime/dt\n-    assert pset.sample_var_curvilinear == runtime/dt\n+    assert pset.sample_var_rectilinear == runtime / dt\n+    assert pset.sample_var_curvilinear == runtime / dt\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n def test_diff_entry_chunksize_error_nemo_complex_conform_depth(mode):\n     # ==== this test is expected to fall-back to a pre-defined minimal chunk as ==== #\n     # ==== the requested chunks don't match, or throw a value error.            ==== #\n-    data_folder = download_example_dataset('NemoNorthSeaORCA025-N006_data')\n-    ufiles = sorted(glob(f'{data_folder}/ORCA*U.nc'))\n-    vfiles = sorted(glob(f'{data_folder}/ORCA*V.nc'))\n-    wfiles = sorted(glob(f'{data_folder}/ORCA*W.nc'))\n-    mesh_mask = f'{data_folder}/coordinates.nc'\n-\n-    filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n-                 'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles},\n-                 'W': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': wfiles}}\n-    variables = {'U': 'uo',\n-                 'V': 'vo',\n-                 'W': 'wo'}\n-    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'W': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n-    chs = {'U': {'depth': ('depthu', 75), 'lat': ('y', 16), 'lon': ('x', 16)},\n-           'V': {'depth': ('depthv', 75), 'lat': ('y', 4), 'lon': ('x', 16)},\n-           'W': {'depth': ('depthw', 75), 'lat': ('y', 16), 'lon': ('x', 4)}}\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions, chunksize=chs)\n+    data_folder = parcels.download_example_dataset(\"NemoNorthSeaORCA025-N006_data\")\n+    ufiles = sorted(glob(f\"{data_folder}/ORCA*U.nc\"))\n+    vfiles = sorted(glob(f\"{data_folder}/ORCA*V.nc\"))\n+    wfiles = sorted(glob(f\"{data_folder}/ORCA*W.nc\"))\n+    mesh_mask = f\"{data_folder}/coordinates.nc\"\n+\n+    filenames = {\n+        \"U\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": ufiles},\n+        \"V\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": vfiles},\n+        \"W\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": wfiles},\n+    }\n+    variables = {\"U\": \"uo\", \"V\": \"vo\", \"W\": \"wo\"}\n+    dimensions = {\n+        \"U\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"V\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"W\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+    }\n+    chs = {\n+        \"U\": {\"depth\": (\"depthu\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 16)},\n+        \"V\": {\"depth\": (\"depthv\", 75), \"lat\": (\"y\", 4), \"lon\": (\"x\", 16)},\n+        \"W\": {\"depth\": (\"depthw\", 75), \"lat\": (\"y\", 16), \"lon\": (\"x\", 4)},\n+    }\n+    fieldset = parcels.FieldSet.from_nemo(\n+        filenames, variables, dimensions, chunksize=chs\n+    )\n     compute_nemo_particle_advection(fieldset, mode)\n     # Nemo sample file dimensions: depthu=75, y=201, x=151\n     npart_U = 1\n@@ -452,47 +705,72 @@ def test_diff_entry_chunksize_error_nemo_complex_conform_depth(mode):\n     npart_V = [npart_V * k for k in fieldset.V.nchunks[1:]]\n     npart_W = 1\n     npart_W = [npart_W * k for k in fieldset.V.nchunks[1:]]\n-    chn = {'U': {'lat': int(math.ceil(201.0/chs['U']['lat'][1])),\n-                 'lon': int(math.ceil(151.0/chs['U']['lon'][1])),\n-                 'depth': int(math.ceil(75.0/chs['U']['depth'][1]))},\n-           'V': {'lat': int(math.ceil(201.0/chs['V']['lat'][1])),\n-                 'lon': int(math.ceil(151.0/chs['V']['lon'][1])),\n-                 'depth': int(math.ceil(75.0/chs['V']['depth'][1]))},\n-           'W': {'lat': int(math.ceil(201.0/chs['W']['lat'][1])),\n-                 'lon': int(math.ceil(151.0/chs['W']['lon'][1])),\n-                 'depth': int(math.ceil(75.0/chs['W']['depth'][1]))}}\n+    chn = {\n+        \"U\": {\n+            \"lat\": int(math.ceil(201.0 / chs[\"U\"][\"lat\"][1])),\n+            \"lon\": int(math.ceil(151.0 / chs[\"U\"][\"lon\"][1])),\n+            \"depth\": int(math.ceil(75.0 / chs[\"U\"][\"depth\"][1])),\n+        },\n+        \"V\": {\n+            \"lat\": int(math.ceil(201.0 / chs[\"V\"][\"lat\"][1])),\n+            \"lon\": int(math.ceil(151.0 / chs[\"V\"][\"lon\"][1])),\n+            \"depth\": int(math.ceil(75.0 / chs[\"V\"][\"depth\"][1])),\n+        },\n+        \"W\": {\n+            \"lat\": int(math.ceil(201.0 / chs[\"W\"][\"lat\"][1])),\n+            \"lon\": int(math.ceil(151.0 / chs[\"W\"][\"lon\"][1])),\n+            \"depth\": int(math.ceil(75.0 / chs[\"W\"][\"depth\"][1])),\n+        },\n+    }\n     npart_U_request = 1\n-    npart_U_request = [npart_U_request * chn['U'][k] for k in chn['U']]\n+    npart_U_request = [npart_U_request * chn[\"U\"][k] for k in chn[\"U\"]]\n     npart_V_request = 1\n-    npart_V_request = [npart_V_request * chn['V'][k] for k in chn['V']]\n+    npart_V_request = [npart_V_request * chn[\"V\"][k] for k in chn[\"V\"]]\n     npart_W_request = 1\n-    npart_W_request = [npart_W_request * chn['W'][k] for k in chn['W']]\n-    assert (npart_U != npart_U_request)\n-    assert (npart_V != npart_V_request)\n-    assert (npart_W != npart_W_request)\n+    npart_W_request = [npart_W_request * chn[\"W\"][k] for k in chn[\"W\"]]\n+    assert npart_U != npart_U_request\n+    assert npart_V != npart_V_request\n+    assert npart_W != npart_W_request\n \n \n-@pytest.mark.parametrize('mode', ['jit'])\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])\n def test_diff_entry_chunksize_correction_globcurrent(mode):\n-    data_folder = download_example_dataset(\"GlobCurrent_example_data\")\n-    filenames = str(data_folder / '200201*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc')\n-    variables = {'U': 'eastward_eulerian_current_velocity', 'V': 'northward_eulerian_current_velocity'}\n-    dimensions = {'lat': 'lat', 'lon': 'lon', 'time': 'time'}\n-    chs = {'U': {'lat': ('lat', 16), 'lon': ('lon', 16)},\n-           'V': {'lat': ('lat', 16), 'lon': ('lon', 4)}}\n-    fieldset = FieldSet.from_netcdf(filenames, variables, dimensions, chunksize=chs)\n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=delta(minutes=5))\n+    data_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\n+    filenames = str(\n+        data_folder / \"200201*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc\"\n+    )\n+    variables = {\n+        \"U\": \"eastward_eulerian_current_velocity\",\n+        \"V\": \"northward_eulerian_current_velocity\",\n+    }\n+    dimensions = {\"lat\": \"lat\", \"lon\": \"lon\", \"time\": \"time\"}\n+    chs = {\n+        \"U\": {\"lat\": (\"lat\", 16), \"lon\": (\"lon\", 16)},\n+        \"V\": {\"lat\": (\"lat\", 16), \"lon\": (\"lon\", 4)},\n+    }\n+    fieldset = parcels.FieldSet.from_netcdf(\n+        filenames, variables, dimensions, chunksize=chs\n+    )\n+    pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=1), dt=timedelta(minutes=5)\n+    )\n     # GlobCurrent sample file dimensions: time=UNLIMITED, lat=41, lon=81\n     npart_U = 1\n     npart_U = [npart_U * k for k in fieldset.U.nchunks[1:]]\n     npart_V = 1\n     npart_V = [npart_V * k for k in fieldset.V.nchunks[1:]]\n     npart_V_request = 1\n-    chn = {'U': {'lat': int(math.ceil(41.0/chs['U']['lat'][1])),\n-                 'lon': int(math.ceil(81.0/chs['U']['lon'][1]))},\n-           'V': {'lat': int(math.ceil(41.0/chs['V']['lat'][1])),\n-                 'lon': int(math.ceil(81.0/chs['V']['lon'][1]))}}\n-    npart_V_request = [npart_V_request * chn['V'][k] for k in chn['V']]\n-    assert (npart_V_request != npart_U)\n-    assert (npart_V_request == npart_V)\n+    chn = {\n+        \"U\": {\n+            \"lat\": int(math.ceil(41.0 / chs[\"U\"][\"lat\"][1])),\n+            \"lon\": int(math.ceil(81.0 / chs[\"U\"][\"lon\"][1])),\n+        },\n+        \"V\": {\n+            \"lat\": int(math.ceil(41.0 / chs[\"V\"][\"lat\"][1])),\n+            \"lon\": int(math.ceil(81.0 / chs[\"V\"][\"lon\"][1])),\n+        },\n+    }\n+    npart_V_request = [npart_V_request * chn[\"V\"][k] for k in chn[\"V\"]]\n+    assert npart_V_request != npart_U\n+    assert npart_V_request == npart_V\ndiff --git a/docs/examples/example_decaying_moving_eddy.py b/docs/examples/example_decaying_moving_eddy.py\nindex 8d75c066d..46bc10d5c 100644\n--- a/docs/examples/example_decaying_moving_eddy.py\n+++ b/docs/examples/example_decaying_moving_eddy.py\n@@ -1,23 +1,27 @@\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import AdvectionRK4, FieldSet, JITParticle, ParticleSet, ScipyParticle\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n # Define some constants.\n-u_g = .04  # Geostrophic current\n-u_0 = .3  # Initial speed in x dirrection. v_0 = 0\n-gamma = 1./delta(days=2.89).total_seconds()  # Dissipitave effects due to viscousity.\n-gamma_g = 1./delta(days=28.9).total_seconds()\n-f = 1.e-4  # Coriolis parameter.\n-start_lon = [10000.]  # Define the start longitude and latitude for the particle.\n-start_lat = [10000.]\n-\n-\n-def decaying_moving_eddy_fieldset(xdim=2, ydim=2):  # Define 2D flat, square fieldset for testing purposes.\n+u_g = 0.04  # Geostrophic current\n+u_0 = 0.3  # Initial speed in x dirrection. v_0 = 0\n+gamma = (\n+    1.0 / timedelta(days=2.89).total_seconds()\n+)  # Dissipitave effects due to viscousity.\n+gamma_g = 1.0 / timedelta(days=28.9).total_seconds()\n+f = 1.0e-4  # Coriolis parameter.\n+start_lon = [10000.0]  # Define the start longitude and latitude for the particle.\n+start_lat = [10000.0]\n+\n+\n+def decaying_moving_eddy_fieldset(\n+    xdim=2, ydim=2\n+):  # Define 2D flat, square fieldset for testing purposes.\n     \"\"\"Simulate an ocean that accelerates subject to Coriolis force\n     and dissipative effects, upon which a geostrophic current is\n     superimposed.\n@@ -28,7 +32,7 @@ def decaying_moving_eddy_fieldset(xdim=2, ydim=2):  # Define 2D flat, square fie\n     http://amsdottorato.unibo.it/1733/1/Fabbroni_Nicoletta_Tesi.pdf\n     \"\"\"\n     depth = np.zeros(1, dtype=np.float32)\n-    time = np.arange(0., 2. * 86400.+1e-5, 60.*5., dtype=np.float64)\n+    time = np.arange(0.0, 2.0 * 86400.0 + 1e-5, 60.0 * 5.0, dtype=np.float64)\n     lon = np.linspace(0, 20000, xdim, dtype=np.float32)\n     lat = np.linspace(5000, 12000, ydim, dtype=np.float32)\n \n@@ -36,46 +40,73 @@ def decaying_moving_eddy_fieldset(xdim=2, ydim=2):  # Define 2D flat, square fie\n     V = np.zeros((time.size, lat.size, lon.size), dtype=np.float32)\n \n     for t in range(time.size):\n-        U[t, :, :] = u_g*np.exp(-gamma_g*time[t]) + (u_0-u_g)*np.exp(-gamma*time[t])*np.cos(f*time[t])\n-        V[t, :, :] = -(u_0-u_g)*np.exp(-gamma*time[t])*np.sin(f*time[t])\n-\n-    data = {'U': U, 'V': V}\n-    dimensions = {'lon': lon, 'lat': lat, 'depth': depth, 'time': time}\n-    return FieldSet.from_data(data, dimensions, mesh='flat')\n-\n-\n-def true_values(t, x_0, y_0):  # Calculate the expected values for particles at the endtime, given their start location.\n-    x = x_0 + (u_g/gamma_g)*(1-np.exp(-gamma_g*t)) + f*((u_0-u_g)/(f**2 + gamma**2))*((gamma/f) + np.exp(-gamma*t)*(np.sin(f*t) - (gamma/f)*np.cos(f*t)))\n-    y = y_0 - ((u_0-u_g)/(f**2+gamma**2))*f*(1 - np.exp(-gamma*t)*(np.cos(f*t) + (gamma/f)*np.sin(f*t)))\n+        U[t, :, :] = u_g * np.exp(-gamma_g * time[t]) + (u_0 - u_g) * np.exp(\n+            -gamma * time[t]\n+        ) * np.cos(f * time[t])\n+        V[t, :, :] = -(u_0 - u_g) * np.exp(-gamma * time[t]) * np.sin(f * time[t])\n+\n+    data = {\"U\": U, \"V\": V}\n+    dimensions = {\"lon\": lon, \"lat\": lat, \"depth\": depth, \"time\": time}\n+    return parcels.FieldSet.from_data(data, dimensions, mesh=\"flat\")\n+\n+\n+def true_values(\n+    t, x_0, y_0\n+):  # Calculate the expected values for particles at the endtime, given their start location.\n+    x = (\n+        x_0\n+        + (u_g / gamma_g) * (1 - np.exp(-gamma_g * t))\n+        + f\n+        * ((u_0 - u_g) / (f**2 + gamma**2))\n+        * (\n+            (gamma / f)\n+            + np.exp(-gamma * t) * (np.sin(f * t) - (gamma / f) * np.cos(f * t))\n+        )\n+    )\n+    y = y_0 - ((u_0 - u_g) / (f**2 + gamma**2)) * f * (\n+        1 - np.exp(-gamma * t) * (np.cos(f * t) + (gamma / f) * np.sin(f * t))\n+    )\n \n     return np.array([x, y])\n \n \n-def decaying_moving_example(fieldset, outfile, mode='scipy', method=AdvectionRK4):\n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=start_lon, lat=start_lat)\n+def decaying_moving_example(\n+    fieldset, outfile, mode=\"scipy\", method=parcels.AdvectionRK4\n+):\n+    pset = parcels.ParticleSet(\n+        fieldset, pclass=ptype[mode], lon=start_lon, lat=start_lat\n+    )\n \n-    dt = delta(minutes=5)\n-    runtime = delta(days=2)\n-    outputdt = delta(hours=1)\n+    dt = timedelta(minutes=5)\n+    runtime = timedelta(days=2)\n+    outputdt = timedelta(hours=1)\n \n-    pset.execute(method, runtime=runtime, dt=dt,\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=outputdt))\n+    pset.execute(\n+        method,\n+        runtime=runtime,\n+        dt=dt,\n+        output_file=pset.ParticleFile(name=outfile, outputdt=outputdt),\n+    )\n \n     return pset\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_rotation_example(mode, tmpdir):\n-    outfile = tmpdir.join('DecayingMovingParticle.zarr')\n+    outfile = tmpdir.join(\"DecayingMovingParticle.zarr\")\n     fieldset = decaying_moving_eddy_fieldset()\n     pset = decaying_moving_example(fieldset, outfile, mode=mode)\n-    vals = true_values(pset[0].time, start_lon, start_lat)  # Calculate values for the particle.\n-    assert (np.allclose(np.array([[pset[0].lon], [pset[0].lat]]), vals, 1e-2))   # Check advected values against calculated values.\n+    vals = true_values(\n+        pset[0].time, start_lon, start_lat\n+    )  # Calculate values for the particle.\n+    assert np.allclose(\n+        np.array([[pset[0].lon], [pset[0].lat]]), vals, 1e-2\n+    )  # Check advected values against calculated values.\n \n \n def main():\n-    fset_filename = 'decaying_moving_eddy'\n-    outfile = 'DecayingMovingParticle.zarr'\n+    fset_filename = \"decaying_moving_eddy\"\n+    outfile = \"DecayingMovingParticle.zarr\"\n     fieldset = decaying_moving_eddy_fieldset()\n     fieldset.write(fset_filename)\n \ndiff --git a/docs/examples/example_globcurrent.py b/docs/examples/example_globcurrent.py\nindex dfca927ef..663d0dd31 100755\n--- a/docs/examples/example_globcurrent.py\n+++ b/docs/examples/example_globcurrent.py\n@@ -1,41 +1,54 @@\n-from datetime import timedelta as delta\n+from datetime import timedelta\n from glob import glob\n \n import numpy as np\n import pytest\n import xarray as xr\n \n-from parcels import (\n-    AdvectionRK4,\n-    Field,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    TimeExtrapolationError,\n-    download_example_dataset,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n-def set_globcurrent_fieldset(filename=None, indices=None, deferred_load=True, use_xarray=False, time_periodic=False, timestamps=None):\n+def set_globcurrent_fieldset(\n+    filename=None,\n+    indices=None,\n+    deferred_load=True,\n+    use_xarray=False,\n+    time_periodic=False,\n+    timestamps=None,\n+):\n     if filename is None:\n-        data_folder = download_example_dataset(\"GlobCurrent_example_data\")\n-        filename = str(data_folder / '2002*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc')\n-    variables = {'U': 'eastward_eulerian_current_velocity', 'V': 'northward_eulerian_current_velocity'}\n+        data_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\n+        filename = str(\n+            data_folder / \"2002*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc\"\n+        )\n+    variables = {\n+        \"U\": \"eastward_eulerian_current_velocity\",\n+        \"V\": \"northward_eulerian_current_velocity\",\n+    }\n     if timestamps is None:\n-        dimensions = {'lat': 'lat', 'lon': 'lon', 'time': 'time'}\n+        dimensions = {\"lat\": \"lat\", \"lon\": \"lon\", \"time\": \"time\"}\n     else:\n-        dimensions = {'lat': 'lat', 'lon': 'lon'}\n+        dimensions = {\"lat\": \"lat\", \"lon\": \"lon\"}\n     if use_xarray:\n-        ds = xr.open_mfdataset(filename, combine='by_coords')\n-        return FieldSet.from_xarray_dataset(ds, variables, dimensions, time_periodic=time_periodic)\n+        ds = xr.open_mfdataset(filename, combine=\"by_coords\")\n+        return parcels.FieldSet.from_xarray_dataset(\n+            ds, variables, dimensions, time_periodic=time_periodic\n+        )\n     else:\n-        return FieldSet.from_netcdf(filename, variables, dimensions, indices, deferred_load=deferred_load, time_periodic=time_periodic, timestamps=timestamps)\n-\n-\n-@pytest.mark.parametrize('use_xarray', [True, False])\n+        return parcels.FieldSet.from_netcdf(\n+            filename,\n+            variables,\n+            dimensions,\n+            indices,\n+            deferred_load=deferred_load,\n+            time_periodic=time_periodic,\n+            timestamps=timestamps,\n+        )\n+\n+\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n def test_globcurrent_fieldset(use_xarray):\n     fieldset = set_globcurrent_fieldset(use_xarray=use_xarray)\n     assert fieldset.U.lon.size == 81\n@@ -44,102 +57,124 @@ def test_globcurrent_fieldset(use_xarray):\n     assert fieldset.V.lat.size == 41\n \n     if not use_xarray:\n-        indices = {'lon': [5], 'lat': range(20, 30)}\n+        indices = {\"lon\": [5], \"lat\": range(20, 30)}\n         fieldsetsub = set_globcurrent_fieldset(indices=indices, use_xarray=use_xarray)\n-        assert np.allclose(fieldsetsub.U.lon, fieldset.U.lon[indices['lon']])\n-        assert np.allclose(fieldsetsub.U.lat, fieldset.U.lat[indices['lat']])\n-        assert np.allclose(fieldsetsub.V.lon, fieldset.V.lon[indices['lon']])\n-        assert np.allclose(fieldsetsub.V.lat, fieldset.V.lat[indices['lat']])\n+        assert np.allclose(fieldsetsub.U.lon, fieldset.U.lon[indices[\"lon\"]])\n+        assert np.allclose(fieldsetsub.U.lat, fieldset.U.lat[indices[\"lat\"]])\n+        assert np.allclose(fieldsetsub.V.lon, fieldset.V.lon[indices[\"lon\"]])\n+        assert np.allclose(fieldsetsub.V.lat, fieldset.V.lat[indices[\"lat\"]])\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('dt, lonstart, latstart', [(3600., 25, -35), (-3600., 20, -39)])\n-@pytest.mark.parametrize('use_xarray', [True, False])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\n+    \"dt, lonstart, latstart\", [(3600.0, 25, -35), (-3600.0, 20, -39)]\n+)\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n def test_globcurrent_fieldset_advancetime(mode, dt, lonstart, latstart, use_xarray):\n-    data_folder = download_example_dataset(\"GlobCurrent_example_data\")\n-    basepath = str(data_folder / '20*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc')\n+    data_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\n+    basepath = str(data_folder / \"20*-GLOBCURRENT-L4-CUReul_hs-ALT_SUM-v02.0-fv01.0.nc\")\n     files = sorted(glob(str(basepath)))\n \n     fieldsetsub = set_globcurrent_fieldset(files[0:10], use_xarray=use_xarray)\n-    psetsub = ParticleSet.from_list(fieldset=fieldsetsub, pclass=ptype[mode], lon=[lonstart], lat=[latstart])\n-\n-    fieldsetall = set_globcurrent_fieldset(files[0:10], deferred_load=False, use_xarray=use_xarray)\n-    psetall = ParticleSet.from_list(fieldset=fieldsetall, pclass=ptype[mode], lon=[lonstart], lat=[latstart])\n+    psetsub = parcels.ParticleSet.from_list(\n+        fieldset=fieldsetsub, pclass=ptype[mode], lon=[lonstart], lat=[latstart]\n+    )\n+\n+    fieldsetall = set_globcurrent_fieldset(\n+        files[0:10], deferred_load=False, use_xarray=use_xarray\n+    )\n+    psetall = parcels.ParticleSet.from_list(\n+        fieldset=fieldsetall, pclass=ptype[mode], lon=[lonstart], lat=[latstart]\n+    )\n     if dt < 0:\n         psetsub[0].time_nextloop = fieldsetsub.U.grid.time[-1]\n         psetall[0].time_nextloop = fieldsetall.U.grid.time[-1]\n \n-    psetsub.execute(AdvectionRK4, runtime=delta(days=7), dt=dt)\n-    psetall.execute(AdvectionRK4, runtime=delta(days=7), dt=dt)\n+    psetsub.execute(parcels.AdvectionRK4, runtime=timedelta(days=7), dt=dt)\n+    psetall.execute(parcels.AdvectionRK4, runtime=timedelta(days=7), dt=dt)\n \n     assert abs(psetsub[0].lon - psetall[0].lon) < 1e-4\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('use_xarray', [True, False])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n def test_globcurrent_particles(mode, use_xarray):\n     fieldset = set_globcurrent_fieldset(use_xarray=use_xarray)\n \n     lonstart = [25]\n     latstart = [-35]\n \n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=lonstart, lat=latstart)\n+    pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=lonstart, lat=latstart)\n \n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=delta(minutes=5))\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=1), dt=timedelta(minutes=5)\n+    )\n \n     assert abs(pset[0].lon - 23.8) < 1\n     assert abs(pset[0].lat - -35.3) < 1\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('rundays', [300, 900])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"rundays\", [300, 900])\n def test_globcurrent_time_periodic(mode, rundays):\n     sample_var = []\n     for deferred_load in [True, False]:\n-        fieldset = set_globcurrent_fieldset(time_periodic=delta(days=365), deferred_load=deferred_load)\n+        fieldset = set_globcurrent_fieldset(\n+            time_periodic=timedelta(days=365), deferred_load=deferred_load\n+        )\n \n-        MyParticle = ptype[mode].add_variable('sample_var', initial=0.)\n+        MyParticle = ptype[mode].add_variable(\"sample_var\", initial=0.0)\n \n-        pset = ParticleSet(fieldset, pclass=MyParticle, lon=25, lat=-35, time=fieldset.U.grid.time[0])\n+        pset = parcels.ParticleSet(\n+            fieldset, pclass=MyParticle, lon=25, lat=-35, time=fieldset.U.grid.time[0]\n+        )\n \n         def SampleU(particle, fieldset, time):\n             u, v = fieldset.UV[time, particle.depth, particle.lat, particle.lon]\n             particle.sample_var += u\n \n-        pset.execute(SampleU, runtime=delta(days=rundays), dt=delta(days=1))\n+        pset.execute(SampleU, runtime=timedelta(days=rundays), dt=timedelta(days=1))\n         sample_var.append(pset[0].sample_var)\n \n     assert np.allclose(sample_var[0], sample_var[1])\n \n \n-@pytest.mark.parametrize('dt', [-300, 300])\n+@pytest.mark.parametrize(\"dt\", [-300, 300])\n def test_globcurrent_xarray_vs_netcdf(dt):\n     fieldsetNetcdf = set_globcurrent_fieldset(use_xarray=False)\n     fieldsetxarray = set_globcurrent_fieldset(use_xarray=True)\n-    lonstart, latstart, runtime = (25, -35, delta(days=7))\n+    lonstart, latstart, runtime = (25, -35, timedelta(days=7))\n \n-    psetN = ParticleSet(fieldsetNetcdf, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetN.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetN = parcels.ParticleSet(\n+        fieldsetNetcdf, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetN.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n-    psetX = ParticleSet(fieldsetxarray, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetX.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetX = parcels.ParticleSet(\n+        fieldsetxarray, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetX.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n     assert np.allclose(psetN[0].lon, psetX[0].lon)\n     assert np.allclose(psetN[0].lat, psetX[0].lat)\n \n \n-@pytest.mark.parametrize('dt', [-300, 300])\n+@pytest.mark.parametrize(\"dt\", [-300, 300])\n def test_globcurrent_netcdf_timestamps(dt):\n     fieldsetNetcdf = set_globcurrent_fieldset()\n     timestamps = fieldsetNetcdf.U.grid.timeslices\n     fieldsetTimestamps = set_globcurrent_fieldset(timestamps=timestamps)\n-    lonstart, latstart, runtime = (25, -35, delta(days=7))\n+    lonstart, latstart, runtime = (25, -35, timedelta(days=7))\n \n-    psetN = ParticleSet(fieldsetNetcdf, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetN.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetN = parcels.ParticleSet(\n+        fieldsetNetcdf, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetN.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n-    psetT = ParticleSet(fieldsetTimestamps, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetT.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetT = parcels.ParticleSet(\n+        fieldsetTimestamps, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetT.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n     assert np.allclose(psetN.lon[0], psetT.lon[0])\n     assert np.allclose(psetN.lat[0], psetT.lat[0])\n@@ -152,57 +187,105 @@ def test__particles_init_time():\n     latstart = [-35]\n \n     # tests the different ways of initialising the time of a particle\n-    pset = ParticleSet(fieldset, pclass=JITParticle, lon=lonstart, lat=latstart, time=np.datetime64('2002-01-15'))\n-    pset2 = ParticleSet(fieldset, pclass=JITParticle, lon=lonstart, lat=latstart, time=14*86400)\n-    pset3 = ParticleSet(fieldset, pclass=JITParticle, lon=lonstart, lat=latstart, time=np.array([np.datetime64('2002-01-15')]))\n-    pset4 = ParticleSet(fieldset, pclass=JITParticle, lon=lonstart, lat=latstart, time=[np.datetime64('2002-01-15')])\n+    pset = parcels.ParticleSet(\n+        fieldset,\n+        pclass=parcels.JITParticle,\n+        lon=lonstart,\n+        lat=latstart,\n+        time=np.datetime64(\"2002-01-15\"),\n+    )\n+    pset2 = parcels.ParticleSet(\n+        fieldset,\n+        pclass=parcels.JITParticle,\n+        lon=lonstart,\n+        lat=latstart,\n+        time=14 * 86400,\n+    )\n+    pset3 = parcels.ParticleSet(\n+        fieldset,\n+        pclass=parcels.JITParticle,\n+        lon=lonstart,\n+        lat=latstart,\n+        time=np.array([np.datetime64(\"2002-01-15\")]),\n+    )\n+    pset4 = parcels.ParticleSet(\n+        fieldset,\n+        pclass=parcels.JITParticle,\n+        lon=lonstart,\n+        lat=latstart,\n+        time=[np.datetime64(\"2002-01-15\")],\n+    )\n     assert pset[0].time - pset2[0].time == 0\n     assert pset[0].time - pset3[0].time == 0\n     assert pset[0].time - pset4[0].time == 0\n \n \n @pytest.mark.xfail(reason=\"Time extrapolation error expected to be thrown\", strict=True)\n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('use_xarray', [True, False])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n def test_globcurrent_time_extrapolation_error(mode, use_xarray):\n     fieldset = set_globcurrent_fieldset(use_xarray=use_xarray)\n \n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=[25], lat=[-35],\n-                       time=fieldset.U.time[0]-delta(days=1).total_seconds())\n+    pset = parcels.ParticleSet(\n+        fieldset,\n+        pclass=ptype[mode],\n+        lon=[25],\n+        lat=[-35],\n+        time=fieldset.U.time[0] - timedelta(days=1).total_seconds(),\n+    )\n \n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=delta(minutes=5))\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=1), dt=timedelta(minutes=5)\n+    )\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('dt', [-300, 300])\n-@pytest.mark.parametrize('with_starttime', [True, False])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"dt\", [-300, 300])\n+@pytest.mark.parametrize(\"with_starttime\", [True, False])\n def test_globcurrent_startparticles_between_time_arrays(mode, dt, with_starttime):\n     fieldset = set_globcurrent_fieldset()\n \n-    data_folder = download_example_dataset(\"GlobCurrent_example_data\")\n+    data_folder = parcels.download_example_dataset(\"GlobCurrent_example_data\")\n     fnamesFeb = sorted(glob(f\"{data_folder}/200202*.nc\"))\n-    fieldset.add_field(Field.from_netcdf(fnamesFeb, ('P', 'eastward_eulerian_current_velocity'),\n-                                         {'lat': 'lat', 'lon': 'lon', 'time': 'time'}))\n+    fieldset.add_field(\n+        parcels.Field.from_netcdf(\n+            fnamesFeb,\n+            (\"P\", \"eastward_eulerian_current_velocity\"),\n+            {\"lat\": \"lat\", \"lon\": \"lon\", \"time\": \"time\"},\n+        )\n+    )\n \n-    MyParticle = ptype[mode].add_variable('sample_var', initial=0.)\n+    MyParticle = ptype[mode].add_variable(\"sample_var\", initial=0.0)\n \n     def SampleP(particle, fieldset, time):\n-        particle.sample_var += fieldset.P[time, particle.depth, particle.lat, particle.lon]\n+        particle.sample_var += fieldset.P[\n+            time, particle.depth, particle.lat, particle.lon\n+        ]\n \n     if with_starttime:\n         time = fieldset.U.grid.time[0] if dt > 0 else fieldset.U.grid.time[-1]\n-        pset = ParticleSet(fieldset, pclass=MyParticle, lon=[25], lat=[-35], time=time)\n+        pset = parcels.ParticleSet(\n+            fieldset, pclass=MyParticle, lon=[25], lat=[-35], time=time\n+        )\n     else:\n-        pset = ParticleSet(fieldset, pclass=MyParticle, lon=[25], lat=[-35])\n+        pset = parcels.ParticleSet(fieldset, pclass=MyParticle, lon=[25], lat=[-35])\n \n     if with_starttime:\n-        with pytest.raises(TimeExtrapolationError):\n-            pset.execute(pset.Kernel(AdvectionRK4)+SampleP, runtime=delta(days=1), dt=dt)\n+        with pytest.raises(parcels.TimeExtrapolationError):\n+            pset.execute(\n+                pset.Kernel(parcels.AdvectionRK4) + SampleP,\n+                runtime=timedelta(days=1),\n+                dt=dt,\n+            )\n     else:\n-        pset.execute(pset.Kernel(AdvectionRK4) + SampleP, runtime=delta(days=1), dt=dt)\n+        pset.execute(\n+            pset.Kernel(parcels.AdvectionRK4) + SampleP,\n+            runtime=timedelta(days=1),\n+            dt=dt,\n+        )\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_globcurrent_particle_independence(mode, rundays=5):\n     fieldset = set_globcurrent_fieldset()\n     time0 = fieldset.U.grid.time[0]\n@@ -211,50 +294,56 @@ def DeleteP0(particle, fieldset, time):\n         if particle.id == 0:\n             particle.delete()\n \n-    pset0 = ParticleSet(fieldset, pclass=ptype[mode],\n-                        lon=[25, 25],\n-                        lat=[-35, -35],\n-                        time=time0)\n+    pset0 = parcels.ParticleSet(\n+        fieldset, pclass=ptype[mode], lon=[25, 25], lat=[-35, -35], time=time0\n+    )\n \n-    pset0.execute(pset0.Kernel(DeleteP0)+AdvectionRK4,\n-                  runtime=delta(days=rundays),\n-                  dt=delta(minutes=5))\n+    pset0.execute(\n+        pset0.Kernel(DeleteP0) + parcels.AdvectionRK4,\n+        runtime=timedelta(days=rundays),\n+        dt=timedelta(minutes=5),\n+    )\n \n-    pset1 = ParticleSet(fieldset, pclass=ptype[mode],\n-                        lon=[25, 25],\n-                        lat=[-35, -35],\n-                        time=time0)\n+    pset1 = parcels.ParticleSet(\n+        fieldset, pclass=ptype[mode], lon=[25, 25], lat=[-35, -35], time=time0\n+    )\n \n-    pset1.execute(AdvectionRK4,\n-                  runtime=delta(days=rundays),\n-                  dt=delta(minutes=5))\n+    pset1.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=rundays), dt=timedelta(minutes=5)\n+    )\n \n     assert np.allclose([pset0[-1].lon, pset0[-1].lat], [pset1[-1].lon, pset1[-1].lat])\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('dt', [-300, 300])\n-@pytest.mark.parametrize('pid_offset', [0, 20])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"dt\", [-300, 300])\n+@pytest.mark.parametrize(\"pid_offset\", [0, 20])\n def test_globcurrent_pset_fromfile(mode, dt, pid_offset, tmpdir):\n     filename = tmpdir.join(\"pset_fromparticlefile.zarr\")\n     fieldset = set_globcurrent_fieldset()\n \n     ptype[mode].setLastID(pid_offset)\n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n-    pfile = pset.ParticleFile(filename, outputdt=delta(hours=6))\n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=dt, output_file=pfile)\n+    pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=25, lat=-35)\n+    pfile = pset.ParticleFile(filename, outputdt=timedelta(hours=6))\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=1), dt=dt, output_file=pfile\n+    )\n     pfile.write_latest_locations(pset, max(pset.time_nextloop))\n \n     restarttime = np.nanmax if dt > 0 else np.nanmin\n-    pset_new = ParticleSet.from_particlefile(fieldset, pclass=ptype[mode], filename=filename, restarttime=restarttime)\n-    pset.execute(AdvectionRK4, runtime=delta(days=1), dt=dt)\n-    pset_new.execute(AdvectionRK4, runtime=delta(days=1), dt=dt)\n+    pset_new = parcels.ParticleSet.from_particlefile(\n+        fieldset, pclass=ptype[mode], filename=filename, restarttime=restarttime\n+    )\n+    pset.execute(parcels.AdvectionRK4, runtime=timedelta(days=1), dt=dt)\n+    pset_new.execute(parcels.AdvectionRK4, runtime=timedelta(days=1), dt=dt)\n \n-    for var in ['lon', 'lat', 'depth', 'time', 'id']:\n-        assert np.allclose([getattr(p, var) for p in pset], [getattr(p, var) for p in pset_new])\n+    for var in [\"lon\", \"lat\", \"depth\", \"time\", \"id\"]:\n+        assert np.allclose(\n+            [getattr(p, var) for p in pset], [getattr(p, var) for p in pset_new]\n+        )\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_error_outputdt_not_multiple_dt(mode, tmpdir):\n     # Test that outputdt is a multiple of dt\n     fieldset = set_globcurrent_fieldset()\n@@ -263,11 +352,11 @@ def test_error_outputdt_not_multiple_dt(mode, tmpdir):\n \n     dt = 81.2584344538292  # number for which output writing fails\n \n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=[0], lat=[0])\n-    ofile = pset.ParticleFile(name=filepath, outputdt=delta(days=1))\n+    pset = parcels.ParticleSet(fieldset, pclass=ptype[mode], lon=[0], lat=[0])\n+    ofile = pset.ParticleFile(name=filepath, outputdt=timedelta(days=1))\n \n     def DoNothing(particle, fieldset, time):\n         pass\n \n     with pytest.raises(ValueError):\n-        pset.execute(DoNothing, runtime=delta(days=10), dt=dt, output_file=ofile)\n+        pset.execute(DoNothing, runtime=timedelta(days=10), dt=dt, output_file=ofile)\ndiff --git a/docs/examples/example_mitgcm.py b/docs/examples/example_mitgcm.py\nindex f4f86d471..dcc52781a 100644\n--- a/docs/examples/example_mitgcm.py\n+++ b/docs/examples/example_mitgcm.py\n@@ -1,24 +1,16 @@\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import xarray as xr\n \n-from parcels import (\n-    AdvectionRK4,\n-    FieldSet,\n-    JITParticle,\n-    ParticleFile,\n-    ParticleSet,\n-    ScipyParticle,\n-    download_example_dataset,\n-)\n+import parcels\n \n-ptype = {\"scipy\": ScipyParticle, \"jit\": JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n def run_mitgcm_zonally_reentrant(mode):\n     \"\"\"Function that shows how to load MITgcm data in a zonally periodic domain.\"\"\"\n-    data_folder = download_example_dataset(\"MITgcm_example_data\")\n+    data_folder = parcels.download_example_dataset(\"MITgcm_example_data\")\n     filenames = {\n         \"U\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\",\n         \"V\": f\"{data_folder}/mitgcm_UV_surface_zonally_reentrant.nc\",\n@@ -28,10 +20,12 @@ def run_mitgcm_zonally_reentrant(mode):\n         \"U\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"},\n         \"V\": {\"lon\": \"XG\", \"lat\": \"YG\", \"time\": \"time\"},\n     }\n-    fieldset = FieldSet.from_mitgcm(filenames, variables, dimensions, mesh=\"flat\")\n+    fieldset = parcels.FieldSet.from_mitgcm(\n+        filenames, variables, dimensions, mesh=\"flat\"\n+    )\n \n     fieldset.add_periodic_halo(zonal=True)\n-    fieldset.add_constant('domain_width', 1000000)\n+    fieldset.add_constant(\"domain_width\", 1000000)\n \n     def periodicBC(particle, fieldset, time):\n         if particle.lon < 0:\n@@ -40,19 +34,22 @@ def periodicBC(particle, fieldset, time):\n             particle_dlon -= fieldset.domain_width  # noqa\n \n     # Release particles 5 cells away from the Eastern boundary\n-    pset = ParticleSet.from_line(\n+    pset = parcels.ParticleSet.from_line(\n         fieldset,\n         pclass=ptype[mode],\n         start=(fieldset.U.grid.lon[-5], fieldset.U.grid.lat[5]),\n         finish=(fieldset.U.grid.lon[-5], fieldset.U.grid.lat[-5]),\n         size=10,\n     )\n-    pfile = ParticleFile(\n-        \"MIT_particles_\" + str(mode) + \".zarr\", pset, outputdt=delta(days=1), chunks=(len(pset), 1)\n+    pfile = parcels.ParticleFile(\n+        \"MIT_particles_\" + str(mode) + \".zarr\",\n+        pset,\n+        outputdt=timedelta(days=1),\n+        chunks=(len(pset), 1),\n     )\n-    kernels = AdvectionRK4 + pset.Kernel(periodicBC)\n+    kernels = parcels.AdvectionRK4 + pset.Kernel(periodicBC)\n     pset.execute(\n-        kernels, runtime=delta(days=5), dt=delta(minutes=30), output_file=pfile\n+        kernels, runtime=timedelta(days=5), dt=timedelta(minutes=30), output_file=pfile\n     )\n \n \ndiff --git a/docs/examples/example_moving_eddies.py b/docs/examples/example_moving_eddies.py\nindex 25fafa950..21b5fd211 100644\n--- a/docs/examples/example_moving_eddies.py\n+++ b/docs/examples/example_moving_eddies.py\n@@ -1,27 +1,22 @@\n import gc\n import math\n from argparse import ArgumentParser\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import (\n-    AdvectionEE,\n-    AdvectionRK4,\n-    AdvectionRK45,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    download_example_dataset,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n-method = {'RK4': AdvectionRK4, 'EE': AdvectionEE, 'RK45': AdvectionRK45}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n+method = {\n+    \"RK4\": parcels.AdvectionRK4,\n+    \"EE\": parcels.AdvectionEE,\n+    \"RK45\": parcels.AdvectionRK45,\n+}\n \n \n-def moving_eddies_fieldset(xdim=200, ydim=350, mesh='flat'):\n+def moving_eddies_fieldset(xdim=200, ydim=350, mesh=\"flat\"):\n     \"\"\"Generate a fieldset encapsulating the flow field consisting of two\n     moving eddies, one moving westward and the other moving northwestward.\n \n@@ -49,21 +44,26 @@ def moving_eddies_fieldset(xdim=200, ydim=350, mesh='flat'):\n \n     \"\"\"\n     # Set Parcels FieldSet variables\n-    time = np.arange(0., 8. * 86400., 86400., dtype=np.float64)\n+    time = np.arange(0.0, 8.0 * 86400.0, 86400.0, dtype=np.float64)\n \n     # Coordinates of the test fieldset (on A-grid in m)\n-    if mesh == 'spherical':\n+    if mesh == \"spherical\":\n         lon = np.linspace(0, 4, xdim, dtype=np.float32)\n         lat = np.linspace(45, 52, ydim, dtype=np.float32)\n     else:\n-        lon = np.linspace(0, 4.e5, xdim, dtype=np.float32)\n-        lat = np.linspace(0, 7.e5, ydim, dtype=np.float32)\n+        lon = np.linspace(0, 4.0e5, xdim, dtype=np.float32)\n+        lat = np.linspace(0, 7.0e5, ydim, dtype=np.float32)\n \n     # Grid spacing in m\n     def cosd(x):\n         return math.cos(math.radians(float(x)))\n-    dx = (lon[1] - lon[0]) * 1852 * 60 * cosd(lat.mean()) if mesh == 'spherical' else lon[1] - lon[0]\n-    dy = (lat[1] - lat[0]) * 1852 * 60 if mesh == 'spherical' else lat[1] - lat[0]\n+\n+    dx = (\n+        (lon[1] - lon[0]) * 1852 * 60 * cosd(lat.mean())\n+        if mesh == \"spherical\"\n+        else lon[1] - lon[0]\n+    )\n+    dy = (lat[1] - lat[0]) * 1852 * 60 if mesh == \"spherical\" else lat[1] - lat[0]\n \n     # Define arrays U (zonal), V (meridional), and P (sea surface height) on A-grid\n     U = np.zeros((lon.size, lat.size, time.size), dtype=np.float32)\n@@ -71,7 +71,7 @@ def cosd(x):\n     P = np.zeros((lon.size, lat.size, time.size), dtype=np.float32)\n \n     # Some constants\n-    corio_0 = 1.e-4  # Coriolis parameter\n+    corio_0 = 1.0e-4  # Coriolis parameter\n     h0 = 1  # Max eddy height\n     sig = 0.5  # Eddy e-folding decay scale (in degrees)\n     g = 10  # Gravitational constant\n@@ -79,15 +79,21 @@ def cosd(x):\n     dX = eddyspeed * 86400 / dx  # Grid cell movement of eddy max each day\n     dY = eddyspeed * 86400 / dy  # Grid cell movement of eddy max each day\n \n-    [x, y] = np.mgrid[:lon.size, :lat.size]\n+    [x, y] = np.mgrid[: lon.size, : lat.size]\n     for t in range(time.size):\n-        hymax_1 = lat.size / 7.\n-        hxmax_1 = .75 * lon.size - dX * t\n-        hymax_2 = 3. * lat.size / 7. + dY * t\n-        hxmax_2 = .75 * lon.size - dX * t\n-\n-        P[:, :, t] = h0 * np.exp(-(x-hxmax_1)**2/(sig*lon.size/4.)**2-(y-hymax_1)**2/(sig*lat.size/7.)**2)\n-        P[:, :, t] += h0 * np.exp(-(x-hxmax_2)**2/(sig*lon.size/4.)**2-(y-hymax_2)**2/(sig*lat.size/7.)**2)\n+        hymax_1 = lat.size / 7.0\n+        hxmax_1 = 0.75 * lon.size - dX * t\n+        hymax_2 = 3.0 * lat.size / 7.0 + dY * t\n+        hxmax_2 = 0.75 * lon.size - dX * t\n+\n+        P[:, :, t] = h0 * np.exp(\n+            -((x - hxmax_1) ** 2) / (sig * lon.size / 4.0) ** 2\n+            - (y - hymax_1) ** 2 / (sig * lat.size / 7.0) ** 2\n+        )\n+        P[:, :, t] += h0 * np.exp(\n+            -((x - hxmax_2) ** 2) / (sig * lon.size / 4.0) ** 2\n+            - (y - hymax_2) ** 2 / (sig * lat.size / 7.0) ** 2\n+        )\n \n         V[:-1, :, t] = -np.diff(P[:, :, t], axis=0) / dx / corio_0 * g\n         V[-1, :, t] = V[-2, :, t]  # Fill in the last column\n@@ -95,10 +101,10 @@ def cosd(x):\n         U[:, :-1, t] = np.diff(P[:, :, t], axis=1) / dy / corio_0 * g\n         U[:, -1, t] = U[:, -2, t]  # Fill in the last row\n \n-    data = {'U': U, 'V': V, 'P': P}\n-    dimensions = {'lon': lon, 'lat': lat, 'time': time}\n+    data = {\"U\": U, \"V\": V, \"P\": P}\n+    dimensions = {\"lon\": lon, \"lat\": lat, \"time\": time}\n \n-    fieldset = FieldSet.from_data(data, dimensions, transpose=True, mesh=mesh)\n+    fieldset = parcels.FieldSet.from_data(data, dimensions, transpose=True, mesh=mesh)\n \n     # setting some constants for AdvectionRK45 kernel\n     fieldset.RK45_min_dt = 1e-3\n@@ -107,8 +113,9 @@ def cosd(x):\n     return fieldset\n \n \n-def moving_eddies_example(fieldset, outfile, npart=2, mode='jit', verbose=False,\n-                          method=AdvectionRK4):\n+def moving_eddies_example(\n+    fieldset, outfile, npart=2, mode=\"jit\", verbose=False, method=parcels.AdvectionRK4\n+):\n     \"\"\"Configuration of a particle set that follows two moving eddies.\n \n \n@@ -128,19 +135,24 @@ def moving_eddies_example(fieldset, outfile, npart=2, mode='jit', verbose=False,\n          (Default value = AdvectionRK4)\n     \"\"\"\n     # Determine particle class according to mode\n-    start = (3.3, 46.) if fieldset.U.grid.mesh == 'spherical' else (3.3e5, 1e5)\n-    finish = (3.3, 47.8) if fieldset.U.grid.mesh == 'spherical' else (3.3e5, 2.8e5)\n-    pset = ParticleSet.from_line(fieldset=fieldset, size=npart, pclass=ptype[mode],\n-                                 start=start, finish=finish)\n+    start = (3.3, 46.0) if fieldset.U.grid.mesh == \"spherical\" else (3.3e5, 1e5)\n+    finish = (3.3, 47.8) if fieldset.U.grid.mesh == \"spherical\" else (3.3e5, 2.8e5)\n+    pset = parcels.ParticleSet.from_line(\n+        fieldset=fieldset, size=npart, pclass=ptype[mode], start=start, finish=finish\n+    )\n \n     if verbose:\n         print(f\"Initial particle positions:\\n{pset}\")\n \n     # Execute for 1 week, with 1 hour timesteps and hourly output\n-    runtime = delta(days=7)\n+    runtime = timedelta(days=7)\n     print(\"MovingEddies: Advecting %d particles for %s\" % (npart, str(runtime)))\n-    pset.execute(method, runtime=runtime, dt=delta(hours=1),\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=delta(hours=1)))\n+    pset.execute(\n+        method,\n+        runtime=runtime,\n+        dt=timedelta(hours=1),\n+        output_file=pset.ParticleFile(name=outfile, outputdt=timedelta(hours=1)),\n+    )\n \n     if verbose:\n         print(f\"Final particle positions:\\n{pset}\")\n@@ -148,97 +160,114 @@ def moving_eddies_example(fieldset, outfile, npart=2, mode='jit', verbose=False,\n     return pset\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_moving_eddies_fwdbwd(mode, mesh, tmpdir, npart=2):\n-    method = AdvectionRK4\n+    method = parcels.AdvectionRK4\n     fieldset = moving_eddies_fieldset(mesh=mesh)\n \n     # Determine particle class according to mode\n-    lons = [3.3, 3.3] if fieldset.U.grid.mesh == 'spherical' else [3.3e5, 3.3e5]\n-    lats = [46., 47.8] if fieldset.U.grid.mesh == 'spherical' else [1e5, 2.8e5]\n-    pset = ParticleSet(fieldset=fieldset, pclass=ptype[mode], lon=lons, lat=lats)\n+    lons = [3.3, 3.3] if fieldset.U.grid.mesh == \"spherical\" else [3.3e5, 3.3e5]\n+    lats = [46.0, 47.8] if fieldset.U.grid.mesh == \"spherical\" else [1e5, 2.8e5]\n+    pset = parcels.ParticleSet(\n+        fieldset=fieldset, pclass=ptype[mode], lon=lons, lat=lats\n+    )\n \n     # Execte for 14 days, with 30sec timesteps and hourly output\n-    runtime = delta(days=1)\n-    dt = delta(minutes=5)\n-    outputdt = delta(hours=1)\n+    runtime = timedelta(days=1)\n+    dt = timedelta(minutes=5)\n+    outputdt = timedelta(hours=1)\n     print(\"MovingEddies: Advecting %d particles for %s\" % (npart, str(runtime)))\n     outfile = tmpdir.join(\"EddyParticlefwd\")\n-    pset.execute(method, runtime=runtime, dt=dt,\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=outputdt))\n+    pset.execute(\n+        method,\n+        runtime=runtime,\n+        dt=dt,\n+        output_file=pset.ParticleFile(name=outfile, outputdt=outputdt),\n+    )\n \n     print(\"Now running in backward time mode\")\n     outfile = tmpdir.join(\"EddyParticlebwd\")\n-    pset.execute(method, endtime=0, dt=-dt,\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=outputdt))\n+    pset.execute(\n+        method,\n+        endtime=0,\n+        dt=-dt,\n+        output_file=pset.ParticleFile(name=outfile, outputdt=outputdt),\n+    )\n \n     # Also include last timestep\n-    for var in ['lon', 'lat', 'depth', 'time']:\n-        pset.particledata.setallvardata(f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\"))\n+    for var in [\"lon\", \"lat\", \"depth\", \"time\"]:\n+        pset.particledata.setallvardata(\n+            f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\")\n+        )\n \n     assert np.allclose(pset.lon, lons)\n     assert np.allclose(pset.lat, lats)\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_moving_eddies_fieldset(mode, mesh, tmpdir):\n     fieldset = moving_eddies_fieldset(mesh=mesh)\n     outfile = tmpdir.join(\"EddyParticle\")\n     pset = moving_eddies_example(fieldset, outfile, 2, mode=mode)\n     # Also include last timestep\n-    for var in ['lon', 'lat', 'depth', 'time']:\n-        pset.particledata.setallvardata(f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\"))\n-    if mesh == 'flat':\n-        assert (pset[0].lon < 2.2e5 and 1.1e5 < pset[0].lat < 1.2e5)\n-        assert (pset[1].lon < 2.2e5 and 3.7e5 < pset[1].lat < 3.8e5)\n+    for var in [\"lon\", \"lat\", \"depth\", \"time\"]:\n+        pset.particledata.setallvardata(\n+            f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\")\n+        )\n+    if mesh == \"flat\":\n+        assert pset[0].lon < 2.2e5 and 1.1e5 < pset[0].lat < 1.2e5\n+        assert pset[1].lon < 2.2e5 and 3.7e5 < pset[1].lat < 3.8e5\n     else:\n-        assert (pset[0].lon < 2.0 and 46.2 < pset[0].lat < 46.25)\n-        assert (pset[1].lon < 2.0 and 48.8 < pset[1].lat < 48.85)\n+        assert pset[0].lon < 2.0 and 46.2 < pset[0].lat < 46.25\n+        assert pset[1].lon < 2.0 and 48.8 < pset[1].lat < 48.85\n \n \n def fieldsetfile(mesh, tmpdir):\n     \"\"\"Generate fieldset files for moving_eddies test.\"\"\"\n-    filename = tmpdir.join('moving_eddies')\n+    filename = tmpdir.join(\"moving_eddies\")\n     fieldset = moving_eddies_fieldset(200, 350, mesh=mesh)\n     fieldset.write(filename)\n     return filename\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_moving_eddies_file(mode, mesh, tmpdir):\n     gc.collect()\n-    fieldset = FieldSet.from_parcels(fieldsetfile(mesh, tmpdir), extra_fields={'P': 'P'})\n+    fieldset = parcels.FieldSet.from_parcels(\n+        fieldsetfile(mesh, tmpdir), extra_fields={\"P\": \"P\"}\n+    )\n     outfile = tmpdir.join(\"EddyParticle\")\n     pset = moving_eddies_example(fieldset, outfile, 2, mode=mode)\n     # Also include last timestep\n-    for var in ['lon', 'lat', 'depth', 'time']:\n-        pset.particledata.setallvardata(f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\"))\n-    if mesh == 'flat':\n-        assert (pset[0].lon < 2.2e5 and 1.1e5 < pset[0].lat < 1.2e5)\n-        assert (pset[1].lon < 2.2e5 and 3.7e5 < pset[1].lat < 3.8e5)\n+    for var in [\"lon\", \"lat\", \"depth\", \"time\"]:\n+        pset.particledata.setallvardata(\n+            f\"{var}\", pset.particledata.getvardata(f\"{var}_nextloop\")\n+        )\n+    if mesh == \"flat\":\n+        assert pset[0].lon < 2.2e5 and 1.1e5 < pset[0].lat < 1.2e5\n+        assert pset[1].lon < 2.2e5 and 3.7e5 < pset[1].lat < 3.8e5\n     else:\n-        assert (pset[0].lon < 2.0 and 46.2 < pset[0].lat < 46.25)\n-        assert (pset[1].lon < 2.0 and 48.8 < pset[1].lat < 48.85)\n+        assert pset[0].lon < 2.0 and 46.2 < pset[0].lat < 46.25\n+        assert pset[1].lon < 2.0 and 48.8 < pset[1].lat < 48.85\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_periodic_and_computeTimeChunk_eddies(mode):\n-    data_folder = download_example_dataset(\"MovingEddies_data\")\n+    data_folder = parcels.download_example_dataset(\"MovingEddies_data\")\n     filename = str(data_folder / \"moving_eddies\")\n \n-    fieldset = FieldSet.from_parcels(filename)\n-    fieldset.add_constant('halo_west', fieldset.U.grid.lon[0])\n-    fieldset.add_constant('halo_east', fieldset.U.grid.lon[-1])\n-    fieldset.add_constant('halo_south', fieldset.U.grid.lat[0])\n-    fieldset.add_constant('halo_north', fieldset.U.grid.lat[-1])\n+    fieldset = parcels.FieldSet.from_parcels(filename)\n+    fieldset.add_constant(\"halo_west\", fieldset.U.grid.lon[0])\n+    fieldset.add_constant(\"halo_east\", fieldset.U.grid.lon[-1])\n+    fieldset.add_constant(\"halo_south\", fieldset.U.grid.lat[0])\n+    fieldset.add_constant(\"halo_north\", fieldset.U.grid.lat[-1])\n     fieldset.add_periodic_halo(zonal=True, meridional=True)\n-    pset = ParticleSet.from_list(fieldset=fieldset,\n-                                 pclass=ptype[mode],\n-                                 lon=[3.3, 3.3],\n-                                 lat=[46.0, 47.8])\n+    pset = parcels.ParticleSet.from_list(\n+        fieldset=fieldset, pclass=ptype[mode], lon=[3.3, 3.3], lat=[46.0, 47.8]\n+    )\n \n     def periodicBC(particle, fieldset, time):\n         if particle.lon < fieldset.halo_west:\n@@ -254,46 +283,87 @@ def slowlySouthWestward(particle, fieldset, time):\n         particle_dlon -= 5 * particle.dt / 1e5  # noqa\n         particle_dlat -= 3 * particle.dt / 1e5  # noqa\n \n-    kernels = pset.Kernel(AdvectionRK4)+slowlySouthWestward+periodicBC\n-    pset.execute(kernels, runtime=delta(days=6), dt=delta(hours=1))\n+    kernels = pset.Kernel(parcels.AdvectionRK4) + slowlySouthWestward + periodicBC\n+    pset.execute(kernels, runtime=timedelta(days=6), dt=timedelta(hours=1))\n \n \n def main(args=None):\n-    p = ArgumentParser(description=\"\"\"\n-Example of particle advection around an idealised peninsula\"\"\")\n-    p.add_argument('mode', choices=('scipy', 'jit'), nargs='?', default='jit',\n-                   help='Execution mode for performing RK4 computation')\n-    p.add_argument('-p', '--particles', type=int, default=2,\n-                   help='Number of particles to advect')\n-    p.add_argument('-v', '--verbose', action='store_true', default=False,\n-                   help='Print particle information before and after execution')\n-    p.add_argument('--profiling', action='store_true', default=False,\n-                   help='Print profiling information after run')\n-    p.add_argument('-f', '--fieldset', type=int, nargs=2, default=None,\n-                   help='Generate fieldset file with given dimensions')\n-    p.add_argument('-m', '--method', choices=('RK4', 'EE', 'RK45'), default='RK4',\n-                   help='Numerical method used for advection')\n+    p = ArgumentParser(\n+        description=\"\"\"\n+Example of particle advection around an idealised peninsula\"\"\"\n+    )\n+    p.add_argument(\n+        \"mode\",\n+        choices=(\"scipy\", \"jit\"),\n+        nargs=\"?\",\n+        default=\"jit\",\n+        help=\"Execution mode for performing RK4 computation\",\n+    )\n+    p.add_argument(\n+        \"-p\", \"--particles\", type=int, default=2, help=\"Number of particles to advect\"\n+    )\n+    p.add_argument(\n+        \"-v\",\n+        \"--verbose\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Print particle information before and after execution\",\n+    )\n+    p.add_argument(\n+        \"--profiling\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Print profiling information after run\",\n+    )\n+    p.add_argument(\n+        \"-f\",\n+        \"--fieldset\",\n+        type=int,\n+        nargs=2,\n+        default=None,\n+        help=\"Generate fieldset file with given dimensions\",\n+    )\n+    p.add_argument(\n+        \"-m\",\n+        \"--method\",\n+        choices=(\"RK4\", \"EE\", \"RK45\"),\n+        default=\"RK4\",\n+        help=\"Numerical method used for advection\",\n+    )\n     args = p.parse_args(args)\n-    data_folder = download_example_dataset(\"MovingEddies_data\")\n+    data_folder = parcels.download_example_dataset(\"MovingEddies_data\")\n     filename = str(data_folder / \"moving_eddies\")\n \n     # Generate fieldset files according to given dimensions\n     if args.fieldset is not None:\n-        fieldset = moving_eddies_fieldset(args.fieldset[0], args.fieldset[1], mesh='flat')\n+        fieldset = moving_eddies_fieldset(\n+            args.fieldset[0], args.fieldset[1], mesh=\"flat\"\n+        )\n     else:\n-        fieldset = moving_eddies_fieldset(mesh='flat')\n+        fieldset = moving_eddies_fieldset(mesh=\"flat\")\n     outfile = \"EddyParticle\"\n \n     if args.profiling:\n         from cProfile import runctx\n         from pstats import Stats\n-        runctx(\"moving_eddies_example(fieldset, outfile, args.particles, mode=args.mode, \\\n+\n+        runctx(\n+            \"moving_eddies_example(fieldset, outfile, args.particles, mode=args.mode, \\\n                               verbose=args.verbose, method=method[args.method])\",\n-               globals(), locals(), \"Profile.prof\")\n+            globals(),\n+            locals(),\n+            \"Profile.prof\",\n+        )\n         Stats(\"Profile.prof\").strip_dirs().sort_stats(\"time\").print_stats(10)\n     else:\n-        moving_eddies_example(fieldset, outfile, args.particles, mode=args.mode,\n-                              verbose=args.verbose, method=method[args.method])\n+        moving_eddies_example(\n+            fieldset,\n+            outfile,\n+            args.particles,\n+            mode=args.mode,\n+            verbose=args.verbose,\n+            method=method[args.method],\n+        )\n \n \n if __name__ == \"__main__\":\ndiff --git a/docs/examples/example_nemo_curvilinear.py b/docs/examples/example_nemo_curvilinear.py\nindex efe095fec..181733a37 100644\n--- a/docs/examples/example_nemo_curvilinear.py\n+++ b/docs/examples/example_nemo_curvilinear.py\n@@ -1,97 +1,113 @@\n \"\"\"Example script that runs a set of particles in a NEMO curvilinear grid.\"\"\"\n+\n from argparse import ArgumentParser\n-from datetime import timedelta as delta\n+from datetime import timedelta\n from glob import glob\n \n import numpy as np\n import pytest\n \n-from parcels import (\n-    AdvectionAnalytical,\n-    AdvectionRK4,\n-    FieldSet,\n-    JITParticle,\n-    ParticleFile,\n-    ParticleSet,\n-    ScipyParticle,\n-    download_example_dataset,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n-advection = {'RK4': AdvectionRK4, 'AA': AdvectionAnalytical}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n+advection = {\"RK4\": parcels.AdvectionRK4, \"AA\": parcels.AdvectionAnalytical}\n \n \n-def run_nemo_curvilinear(mode, outfile, advtype='RK4'):\n+def run_nemo_curvilinear(mode, outfile, advtype=\"RK4\"):\n     \"\"\"Run parcels on the NEMO curvilinear grid.\"\"\"\n-    data_folder = download_example_dataset('NemoCurvilinear_data')\n-\n-    filenames = {'U': {'lon': f'{data_folder}/mesh_mask.nc4',\n-                       'lat': f'{data_folder}/mesh_mask.nc4',\n-                       'data': f'{data_folder}/U_purely_zonal-ORCA025_grid_U.nc4'},\n-                 'V': {'lon': f'{data_folder}/mesh_mask.nc4',\n-                       'lat': f'{data_folder}/mesh_mask.nc4',\n-                       'data': f'{data_folder}/V_purely_zonal-ORCA025_grid_V.nc4'}}\n-    variables = {'U': 'U', 'V': 'V'}\n-    dimensions = {'lon': 'glamf', 'lat': 'gphif'}\n-    chunksize = {'lat': ('y', 256), 'lon': ('x', 512)}\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions, chunksize=chunksize)\n+    data_folder = parcels.download_example_dataset(\"NemoCurvilinear_data\")\n+\n+    filenames = {\n+        \"U\": {\n+            \"lon\": f\"{data_folder}/mesh_mask.nc4\",\n+            \"lat\": f\"{data_folder}/mesh_mask.nc4\",\n+            \"data\": f\"{data_folder}/U_purely_zonal-ORCA025_grid_U.nc4\",\n+        },\n+        \"V\": {\n+            \"lon\": f\"{data_folder}/mesh_mask.nc4\",\n+            \"lat\": f\"{data_folder}/mesh_mask.nc4\",\n+            \"data\": f\"{data_folder}/V_purely_zonal-ORCA025_grid_V.nc4\",\n+        },\n+    }\n+    variables = {\"U\": \"U\", \"V\": \"V\"}\n+    dimensions = {\"lon\": \"glamf\", \"lat\": \"gphif\"}\n+    chunksize = {\"lat\": (\"y\", 256), \"lon\": (\"x\", 512)}\n+    fieldset = parcels.FieldSet.from_nemo(\n+        filenames, variables, dimensions, chunksize=chunksize\n+    )\n     assert fieldset.U.chunksize == chunksize\n \n     # Now run particles as normal\n     npart = 20\n     lonp = 30 * np.ones(npart)\n-    if advtype == 'RK4':\n+    if advtype == \"RK4\":\n         latp = np.linspace(-70, 88, npart)\n-        runtime = delta(days=160)\n+        runtime = timedelta(days=160)\n     else:\n         latp = np.linspace(-70, 70, npart)\n-        runtime = delta(days=15)\n+        runtime = timedelta(days=15)\n \n     def periodicBC(particle, fieldSet, time):\n         if particle.lon > 180:\n             particle_dlon -= 360  # noqa\n \n-    pset = ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n-    pfile = ParticleFile(outfile, pset, outputdt=delta(days=1))\n+    pset = parcels.ParticleSet.from_list(fieldset, ptype[mode], lon=lonp, lat=latp)\n+    pfile = parcels.ParticleFile(outfile, pset, outputdt=timedelta(days=1))\n     kernels = pset.Kernel(advection[advtype]) + periodicBC\n-    pset.execute(kernels, runtime=runtime, dt=delta(hours=6),\n-                 output_file=pfile)\n+    pset.execute(kernels, runtime=runtime, dt=timedelta(hours=6), output_file=pfile)\n     assert np.allclose(pset.lat - latp, 0, atol=2e-2)\n \n \n-@pytest.mark.parametrize('mode', ['jit'])  # Only testing jit as scipy is very slow\n+@pytest.mark.parametrize(\"mode\", [\"jit\"])  # Only testing jit as scipy is very slow\n def test_nemo_curvilinear(mode, tmpdir):\n     \"\"\"Test the NEMO curvilinear example.\"\"\"\n-    outfile = tmpdir.join('nemo_particles')\n+    outfile = tmpdir.join(\"nemo_particles\")\n     run_nemo_curvilinear(mode, outfile)\n \n \n def test_nemo_curvilinear_AA(tmpdir):\n     \"\"\"Test the NEMO curvilinear example with analytical advection.\"\"\"\n-    outfile = tmpdir.join('nemo_particlesAA')\n-    run_nemo_curvilinear('scipy', outfile, 'AA')\n+    outfile = tmpdir.join(\"nemo_particlesAA\")\n+    run_nemo_curvilinear(\"scipy\", outfile, \"AA\")\n \n \n def test_nemo_3D_samegrid():\n     \"\"\"Test that the same grid is used for U and V in 3D NEMO fields.\"\"\"\n-    data_folder = download_example_dataset('NemoNorthSeaORCA025-N006_data')\n-    ufiles = sorted(glob(f'{data_folder}/ORCA*U.nc'))\n-    vfiles = sorted(glob(f'{data_folder}/ORCA*V.nc'))\n-    wfiles = sorted(glob(f'{data_folder}/ORCA*W.nc'))\n-    mesh_mask = f'{data_folder}/coordinates.nc'\n-\n-    filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n-                 'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles},\n-                 'W': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': wfiles}}\n-\n-    variables = {'U': 'uo',\n-                 'V': 'vo',\n-                 'W': 'wo'}\n-    dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n-                  'W': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n-\n-    fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\n+    data_folder = parcels.download_example_dataset(\"NemoNorthSeaORCA025-N006_data\")\n+    ufiles = sorted(glob(f\"{data_folder}/ORCA*U.nc\"))\n+    vfiles = sorted(glob(f\"{data_folder}/ORCA*V.nc\"))\n+    wfiles = sorted(glob(f\"{data_folder}/ORCA*W.nc\"))\n+    mesh_mask = f\"{data_folder}/coordinates.nc\"\n+\n+    filenames = {\n+        \"U\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": ufiles},\n+        \"V\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": vfiles},\n+        \"W\": {\"lon\": mesh_mask, \"lat\": mesh_mask, \"depth\": wfiles[0], \"data\": wfiles},\n+    }\n+\n+    variables = {\"U\": \"uo\", \"V\": \"vo\", \"W\": \"wo\"}\n+    dimensions = {\n+        \"U\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"V\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+        \"W\": {\n+            \"lon\": \"glamf\",\n+            \"lat\": \"gphif\",\n+            \"depth\": \"depthw\",\n+            \"time\": \"time_counter\",\n+        },\n+    }\n+\n+    fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\n \n     assert fieldset.U.dataFiles is not fieldset.W.dataFiles\n \n@@ -99,8 +115,13 @@ def test_nemo_3D_samegrid():\n def main(args=None):\n     \"\"\"Run the example with given arguments.\"\"\"\n     p = ArgumentParser(description=\"\"\"Chose the mode using mode option\"\"\")\n-    p.add_argument('mode', choices=('scipy', 'jit'), nargs='?', default='jit',\n-                   help='Execution mode for performing computation')\n+    p.add_argument(\n+        \"mode\",\n+        choices=(\"scipy\", \"jit\"),\n+        nargs=\"?\",\n+        default=\"jit\",\n+        help=\"Execution mode for performing computation\",\n+    )\n     args = p.parse_args(args)\n \n     outfile = \"nemo_particles\"\ndiff --git a/docs/examples/example_ofam.py b/docs/examples/example_ofam.py\nindex 251bc3cf7..791d890ff 100644\n--- a/docs/examples/example_ofam.py\n+++ b/docs/examples/example_ofam.py\n@@ -1,61 +1,73 @@\n import gc\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n import xarray as xr\n \n-from parcels import (\n-    AdvectionRK4,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    download_example_dataset,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n def set_ofam_fieldset(deferred_load=True, use_xarray=False):\n-    data_folder = download_example_dataset(\"OFAM_example_data\")\n-    filenames = {'U': f\"{data_folder}/OFAM_simple_U.nc\",\n-                 'V': f\"{data_folder}/OFAM_simple_V.nc\"}\n-    variables = {'U': 'u', 'V': 'v'}\n-    dimensions = {'lat': 'yu_ocean', 'lon': 'xu_ocean', 'depth': 'st_ocean',\n-                  'time': 'Time'}\n+    data_folder = parcels.download_example_dataset(\"OFAM_example_data\")\n+    filenames = {\n+        \"U\": f\"{data_folder}/OFAM_simple_U.nc\",\n+        \"V\": f\"{data_folder}/OFAM_simple_V.nc\",\n+    }\n+    variables = {\"U\": \"u\", \"V\": \"v\"}\n+    dimensions = {\n+        \"lat\": \"yu_ocean\",\n+        \"lon\": \"xu_ocean\",\n+        \"depth\": \"st_ocean\",\n+        \"time\": \"Time\",\n+    }\n     if use_xarray:\n-        ds = xr.open_mfdataset([filenames['U'], filenames['V']], combine='by_coords')\n-        return FieldSet.from_xarray_dataset(ds, variables, dimensions, allow_time_extrapolation=True)\n+        ds = xr.open_mfdataset([filenames[\"U\"], filenames[\"V\"]], combine=\"by_coords\")\n+        return parcels.FieldSet.from_xarray_dataset(\n+            ds, variables, dimensions, allow_time_extrapolation=True\n+        )\n     else:\n-        return FieldSet.from_netcdf(filenames, variables, dimensions, allow_time_extrapolation=True, deferred_load=deferred_load, chunksize=False)\n+        return parcels.FieldSet.from_netcdf(\n+            filenames,\n+            variables,\n+            dimensions,\n+            allow_time_extrapolation=True,\n+            deferred_load=deferred_load,\n+            chunksize=False,\n+        )\n \n \n-@pytest.mark.parametrize('use_xarray', [True, False])\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n def test_ofam_fieldset_fillvalues(use_xarray):\n     fieldset = set_ofam_fieldset(deferred_load=False, use_xarray=use_xarray)\n     # V.data[0, 0, 150] is a landpoint, that makes NetCDF4 generate a masked array, instead of an ndarray\n     assert fieldset.V.data[0, 0, 150] == 0\n \n \n-@pytest.mark.parametrize('dt', [delta(minutes=-5), delta(minutes=5)])\n+@pytest.mark.parametrize(\"dt\", [timedelta(minutes=-5), timedelta(minutes=5)])\n def test_ofam_xarray_vs_netcdf(dt):\n     fieldsetNetcdf = set_ofam_fieldset(use_xarray=False)\n     fieldsetxarray = set_ofam_fieldset(use_xarray=True)\n-    lonstart, latstart, runtime = (180, 10, delta(days=7))\n+    lonstart, latstart, runtime = (180, 10, timedelta(days=7))\n \n-    psetN = ParticleSet(fieldsetNetcdf, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetN.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetN = parcels.ParticleSet(\n+        fieldsetNetcdf, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetN.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n-    psetX = ParticleSet(fieldsetxarray, pclass=JITParticle, lon=lonstart, lat=latstart)\n-    psetX.execute(AdvectionRK4, runtime=runtime, dt=dt)\n+    psetX = parcels.ParticleSet(\n+        fieldsetxarray, pclass=parcels.JITParticle, lon=lonstart, lat=latstart\n+    )\n+    psetX.execute(parcels.AdvectionRK4, runtime=runtime, dt=dt)\n \n     assert np.allclose(psetN[0].lon, psetX[0].lon)\n     assert np.allclose(psetN[0].lat, psetX[0].lat)\n \n \n-@pytest.mark.parametrize('use_xarray', [True, False])\n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"use_xarray\", [True, False])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_ofam_particles(mode, use_xarray):\n     gc.collect()\n     fieldset = set_ofam_fieldset(use_xarray=use_xarray)\n@@ -64,9 +76,13 @@ def test_ofam_particles(mode, use_xarray):\n     latstart = [10]\n     depstart = [2.5]  # the depth of the first layer in OFAM\n \n-    pset = ParticleSet(fieldset, pclass=ptype[mode], lon=lonstart, lat=latstart, depth=depstart)\n+    pset = parcels.ParticleSet(\n+        fieldset, pclass=ptype[mode], lon=lonstart, lat=latstart, depth=depstart\n+    )\n \n-    pset.execute(AdvectionRK4, runtime=delta(days=10), dt=delta(minutes=5))\n+    pset.execute(\n+        parcels.AdvectionRK4, runtime=timedelta(days=10), dt=timedelta(minutes=5)\n+    )\n \n     assert abs(pset[0].lon - 173) < 1\n     assert abs(pset[0].lat - 11) < 1\ndiff --git a/docs/examples/example_peninsula.py b/docs/examples/example_peninsula.py\nindex 4597f90a4..3299d638c 100644\n--- a/docs/examples/example_peninsula.py\n+++ b/docs/examples/example_peninsula.py\n@@ -1,28 +1,22 @@\n import gc\n import math  # NOQA\n from argparse import ArgumentParser\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import (\n-    AdvectionAnalytical,\n-    AdvectionEE,\n-    AdvectionRK4,\n-    AdvectionRK45,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    Variable,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n-method = {'RK4': AdvectionRK4, 'EE': AdvectionEE, 'RK45': AdvectionRK45}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n+method = {\n+    \"RK4\": parcels.AdvectionRK4,\n+    \"EE\": parcels.AdvectionEE,\n+    \"RK45\": parcels.AdvectionRK45,\n+}\n \n \n-def peninsula_fieldset(xdim, ydim, mesh='flat', grid_type='A'):\n+def peninsula_fieldset(xdim, ydim, mesh=\"flat\", grid_type=\"A\"):\n     \"\"\"Construct a fieldset encapsulating the flow field around an idealised peninsula.\n \n     Parameters\n@@ -54,7 +48,7 @@ def peninsula_fieldset(xdim, ydim, mesh='flat', grid_type='A'):\n     # Set Parcels FieldSet variables\n \n     # Generate the original test setup on A-grid in m\n-    domainsizeX, domainsizeY = (1.e5, 5.e4)\n+    domainsizeX, domainsizeY = (1.0e5, 5.0e4)\n     La = np.linspace(1e3, domainsizeX, xdim, dtype=np.float32)\n     Wa = np.linspace(1e3, domainsizeY, ydim, dtype=np.float32)\n \n@@ -63,37 +57,37 @@ def peninsula_fieldset(xdim, ydim, mesh='flat', grid_type='A'):\n     R = 0.32 * domainsizeX / 2\n \n     # Create the fields\n-    x, y = np.meshgrid(La, Wa, sparse=True, indexing='xy')\n-    P = (u0*R**2*y/((x-x0)**2+y**2)-u0*y) / 1e3\n+    x, y = np.meshgrid(La, Wa, sparse=True, indexing=\"xy\")\n+    P = (u0 * R**2 * y / ((x - x0) ** 2 + y**2) - u0 * y) / 1e3\n \n-    if grid_type == 'A':\n-        U = u0-u0*R**2*((x-x0)**2-y**2)/(((x-x0)**2+y**2)**2)\n-        V = -2*u0*R**2*((x-x0)*y)/(((x-x0)**2+y**2)**2)\n-    elif grid_type == 'C':\n+    if grid_type == \"A\":\n+        U = u0 - u0 * R**2 * ((x - x0) ** 2 - y**2) / (((x - x0) ** 2 + y**2) ** 2)\n+        V = -2 * u0 * R**2 * ((x - x0) * y) / (((x - x0) ** 2 + y**2) ** 2)\n+    elif grid_type == \"C\":\n         U = np.zeros(P.shape)\n         V = np.zeros(P.shape)\n-        V[:, 1:] = (P[:, 1:] - P[:, :-1])\n+        V[:, 1:] = P[:, 1:] - P[:, :-1]\n         U[1:, :] = -(P[1:, :] - P[:-1, :])\n     else:\n-        raise RuntimeError(f'Grid_type {grid_type} is not a valid option')\n+        raise RuntimeError(f\"Grid_type {grid_type} is not a valid option\")\n \n     # Set land points to NaN\n-    landpoints = P >= 0.\n+    landpoints = P >= 0.0\n     P[landpoints] = np.nan\n     U[landpoints] = np.nan\n     V[landpoints] = np.nan\n \n     # Convert from m to lat/lon for spherical meshes\n-    lon = La / 1852. / 60. if mesh == 'spherical' else La\n-    lat = Wa / 1852. / 60. if mesh == 'spherical' else Wa\n+    lon = La / 1852.0 / 60.0 if mesh == \"spherical\" else La\n+    lat = Wa / 1852.0 / 60.0 if mesh == \"spherical\" else Wa\n \n-    data = {'U': U, 'V': V, 'P': P}\n-    dimensions = {'lon': lon, 'lat': lat}\n+    data = {\"U\": U, \"V\": V, \"P\": P}\n+    dimensions = {\"lon\": lon, \"lat\": lat}\n \n-    fieldset = FieldSet.from_data(data, dimensions, mesh=mesh)\n-    if grid_type == 'C':\n-        fieldset.U.interp_method = 'cgrid_velocity'\n-        fieldset.V.interp_method = 'cgrid_velocity'\n+    fieldset = parcels.FieldSet.from_data(data, dimensions, mesh=mesh)\n+    if grid_type == \"C\":\n+        fieldset.U.interp_method = \"cgrid_velocity\"\n+        fieldset.V.interp_method = \"cgrid_velocity\"\n     return fieldset\n \n \n@@ -103,8 +97,16 @@ def UpdateP(particle, fieldset, time):\n     particle.p = fieldset.P[time, particle.depth, particle.lat, particle.lon]\n \n \n-def peninsula_example(fieldset, outfile, npart, mode='jit', degree=1,\n-                      verbose=False, output=True, method=AdvectionRK4):\n+def peninsula_example(\n+    fieldset,\n+    outfile,\n+    npart,\n+    mode=\"jit\",\n+    degree=1,\n+    verbose=False,\n+    output=True,\n+    method=parcels.AdvectionRK4,\n+):\n     \"\"\"Example configuration of particle flow around an idealised Peninsula\n \n     Parameters\n@@ -130,27 +132,42 @@ def peninsula_example(fieldset, outfile, npart, mode='jit', degree=1,\n     # First, we define a custom Particle class to which we add a\n     # custom variable, the initial stream function value p.\n     # We determine the particle base class according to mode.\n-    MyParticle = ptype[mode].add_variable([Variable('p', dtype=np.float32, initial=0.),\n-                                           Variable('p_start', dtype=np.float32, initial=0)])\n+    MyParticle = ptype[mode].add_variable(\n+        [\n+            parcels.Variable(\"p\", dtype=np.float32, initial=0.0),\n+            parcels.Variable(\"p_start\", dtype=np.float32, initial=0),\n+        ]\n+    )\n \n     # Initialise particles\n-    if fieldset.U.grid.mesh == 'flat':\n+    if fieldset.U.grid.mesh == \"flat\":\n         x = 3000  # 3 km offset from boundary\n     else:\n-        x = 3. * (1. / 1.852 / 60)  # 3 km offset from boundary\n-    y = (fieldset.U.lat[0] + x, fieldset.U.lat[-1] - x)  # latitude range, including offsets\n-    pset = ParticleSet.from_line(fieldset, size=npart, pclass=MyParticle,\n-                                 start=(x, y[0]), finish=(x, y[1]), time=0)\n+        x = 3.0 * (1.0 / 1.852 / 60)  # 3 km offset from boundary\n+    y = (\n+        fieldset.U.lat[0] + x,\n+        fieldset.U.lat[-1] - x,\n+    )  # latitude range, including offsets\n+    pset = parcels.ParticleSet.from_line(\n+        fieldset,\n+        size=npart,\n+        pclass=MyParticle,\n+        start=(x, y[0]),\n+        finish=(x, y[1]),\n+        time=0,\n+    )\n \n     if verbose:\n         print(f\"Initial particle positions:\\n{pset}\")\n \n     # Advect the particles for 24h\n-    time = delta(hours=24)\n-    dt = delta(minutes=5)\n+    time = timedelta(hours=24)\n+    dt = timedelta(minutes=5)\n     k_adv = pset.Kernel(method)\n     k_p = pset.Kernel(UpdateP)\n-    out = pset.ParticleFile(name=outfile, outputdt=delta(hours=1)) if output else None\n+    out = (\n+        pset.ParticleFile(name=outfile, outputdt=timedelta(hours=1)) if output else None\n+    )\n     print(\"Peninsula: Advecting %d particles for %s\" % (npart, str(time)))\n     pset.execute(k_adv + k_p, runtime=time, dt=dt, output_file=out)\n \n@@ -160,8 +177,8 @@ def peninsula_example(fieldset, outfile, npart, mode='jit', degree=1,\n     return pset\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_peninsula_fieldset(mode, mesh, tmpdir):\n     \"\"\"Execute peninsula test from fieldset generated in memory.\"\"\"\n     fieldset = peninsula_fieldset(100, 50, mesh)\n@@ -169,95 +186,166 @@ def test_peninsula_fieldset(mode, mesh, tmpdir):\n     pset = peninsula_example(fieldset, outfile, 5, mode=mode, degree=1)\n     # Test advection accuracy by comparing streamline values\n     err_adv = np.abs(pset.p_start - pset.p)\n-    assert (err_adv <= 1.e-3).all()\n+    assert (err_adv <= 1.0e-3).all()\n     # Test Field sampling accuracy by comparing kernel against Field sampling\n-    err_smpl = np.array([abs(pset.p[i] - pset.fieldset.P[0., pset.depth[i], pset.lat[i], pset.lon[i]]) for i in range(pset.size)])\n-    assert (err_smpl <= 1.e-3).all()\n-\n-\n-@pytest.mark.parametrize('mode', ['scipy'])  # Analytical Advection only implemented in Scipy mode\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+    err_smpl = np.array(\n+        [\n+            abs(\n+                pset.p[i]\n+                - pset.fieldset.P[0.0, pset.depth[i], pset.lat[i], pset.lon[i]]\n+            )\n+            for i in range(pset.size)\n+        ]\n+    )\n+    assert (err_smpl <= 1.0e-3).all()\n+\n+\n+@pytest.mark.parametrize(\n+    \"mode\", [\"scipy\"]\n+)  # Analytical Advection only implemented in Scipy mode\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_peninsula_fieldset_AnalyticalAdvection(mode, mesh, tmpdir):\n     \"\"\"Execute peninsula test using Analytical Advection on C grid.\"\"\"\n-    fieldset = peninsula_fieldset(101, 51, 'flat', grid_type='C')\n+    fieldset = peninsula_fieldset(101, 51, \"flat\", grid_type=\"C\")\n     outfile = tmpdir.join(\"PeninsulaAA\")\n-    pset = peninsula_example(fieldset, outfile, npart=10, mode=mode,\n-                             method=AdvectionAnalytical)\n+    pset = peninsula_example(\n+        fieldset, outfile, npart=10, mode=mode, method=parcels.AdvectionAnalytical\n+    )\n     # Test advection accuracy by comparing streamline values\n     err_adv = np.array([abs(p.p_start - p.p) for p in pset])\n \n-    tol = {\"scipy\": 3.e-1, \"jit\": 1.e-1}.get(mode)\n+    tol = {\"scipy\": 3.0e-1, \"jit\": 1.0e-1}.get(mode)\n     assert (err_adv <= tol).all()\n \n \n def fieldsetfile(mesh, tmpdir):\n     \"\"\"Generate fieldset files for peninsula test.\"\"\"\n-    filename = tmpdir.join('peninsula')\n+    filename = tmpdir.join(\"peninsula\")\n     fieldset = peninsula_fieldset(100, 50, mesh=mesh)\n     fieldset.write(filename)\n     return filename\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n-@pytest.mark.parametrize('mesh', ['flat', 'spherical'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n+@pytest.mark.parametrize(\"mesh\", [\"flat\", \"spherical\"])\n def test_peninsula_file(mode, mesh, tmpdir):\n     \"\"\"Open fieldset files and execute.\"\"\"\n     gc.collect()\n-    fieldset = FieldSet.from_parcels(fieldsetfile(mesh, tmpdir), extra_fields={'P': 'P'}, allow_time_extrapolation=True)\n+    fieldset = parcels.FieldSet.from_parcels(\n+        fieldsetfile(mesh, tmpdir),\n+        extra_fields={\"P\": \"P\"},\n+        allow_time_extrapolation=True,\n+    )\n     outfile = tmpdir.join(\"Peninsula\")\n     pset = peninsula_example(fieldset, outfile, 5, mode=mode, degree=1)\n     # Test advection accuracy by comparing streamline values\n     err_adv = np.abs(pset.p_start - pset.p)\n-    assert (err_adv <= 1.e-3).all()\n+    assert (err_adv <= 1.0e-3).all()\n     # Test Field sampling accuracy by comparing kernel against Field sampling\n-    err_smpl = np.array([abs(pset.p[i] - pset.fieldset.P[0., pset.depth[i], pset.lat[i], pset.lon[i]]) for i in range(pset.size)])\n-    assert (err_smpl <= 1.e-3).all()\n+    err_smpl = np.array(\n+        [\n+            abs(\n+                pset.p[i]\n+                - pset.fieldset.P[0.0, pset.depth[i], pset.lat[i], pset.lon[i]]\n+            )\n+            for i in range(pset.size)\n+        ]\n+    )\n+    assert (err_smpl <= 1.0e-3).all()\n \n \n def main(args=None):\n-    p = ArgumentParser(description=\"\"\"\n-Example of particle advection around an idealised peninsula\"\"\")\n-    p.add_argument('mode', choices=('scipy', 'jit'), nargs='?', default='jit',\n-                   help='Execution mode for performing RK4 computation')\n-    p.add_argument('-p', '--particles', type=int, default=20,\n-                   help='Number of particles to advect')\n-    p.add_argument('-d', '--degree', type=int, default=1,\n-                   help='Degree of spatial interpolation')\n-    p.add_argument('-v', '--verbose', action='store_true', default=False,\n-                   help='Print particle information before and after execution')\n-    p.add_argument('-o', '--nooutput', action='store_true', default=False,\n-                   help='Suppress trajectory output')\n-    p.add_argument('--profiling', action='store_true', default=False,\n-                   help='Print profiling information after run')\n-    p.add_argument('-f', '--fieldset', type=int, nargs=2, default=None,\n-                   help='Generate fieldset file with given dimensions')\n-    p.add_argument('-m', '--method', choices=('RK4', 'EE', 'RK45'), default='RK4',\n-                   help='Numerical method used for advection')\n+    p = ArgumentParser(\n+        description=\"\"\"\n+Example of particle advection around an idealised peninsula\"\"\"\n+    )\n+    p.add_argument(\n+        \"mode\",\n+        choices=(\"scipy\", \"jit\"),\n+        nargs=\"?\",\n+        default=\"jit\",\n+        help=\"Execution mode for performing RK4 computation\",\n+    )\n+    p.add_argument(\n+        \"-p\", \"--particles\", type=int, default=20, help=\"Number of particles to advect\"\n+    )\n+    p.add_argument(\n+        \"-d\", \"--degree\", type=int, default=1, help=\"Degree of spatial interpolation\"\n+    )\n+    p.add_argument(\n+        \"-v\",\n+        \"--verbose\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Print particle information before and after execution\",\n+    )\n+    p.add_argument(\n+        \"-o\",\n+        \"--nooutput\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Suppress trajectory output\",\n+    )\n+    p.add_argument(\n+        \"--profiling\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Print profiling information after run\",\n+    )\n+    p.add_argument(\n+        \"-f\",\n+        \"--fieldset\",\n+        type=int,\n+        nargs=2,\n+        default=None,\n+        help=\"Generate fieldset file with given dimensions\",\n+    )\n+    p.add_argument(\n+        \"-m\",\n+        \"--method\",\n+        choices=(\"RK4\", \"EE\", \"RK45\"),\n+        default=\"RK4\",\n+        help=\"Numerical method used for advection\",\n+    )\n     args = p.parse_args(args)\n \n-    filename = 'peninsula'\n+    filename = \"peninsula\"\n     if args.fieldset is not None:\n-        fieldset = peninsula_fieldset(args.fieldset[0], args.fieldset[1], mesh='flat')\n+        fieldset = peninsula_fieldset(args.fieldset[0], args.fieldset[1], mesh=\"flat\")\n     else:\n-        fieldset = peninsula_fieldset(100, 50, mesh='flat')\n+        fieldset = peninsula_fieldset(100, 50, mesh=\"flat\")\n     fieldset.write(filename)\n \n     # Open fieldset file set\n-    fieldset = FieldSet.from_parcels('peninsula', extra_fields={'P': 'P'}, allow_time_extrapolation=True)\n+    fieldset = parcels.FieldSet.from_parcels(\n+        \"peninsula\", extra_fields={\"P\": \"P\"}, allow_time_extrapolation=True\n+    )\n     outfile = \"Peninsula\"\n \n     if args.profiling:\n         from cProfile import runctx\n         from pstats import Stats\n-        runctx(\"peninsula_example(fieldset, outfile, args.particles, mode=args.mode,\\\n+\n+        runctx(\n+            \"peninsula_example(fieldset, outfile, args.particles, mode=args.mode,\\\n                                    degree=args.degree, verbose=args.verbose,\\\n                                    output=not args.nooutput, method=method[args.method])\",\n-               globals(), locals(), \"Profile.prof\")\n+            globals(),\n+            locals(),\n+            \"Profile.prof\",\n+        )\n         Stats(\"Profile.prof\").strip_dirs().sort_stats(\"time\").print_stats(10)\n     else:\n-        peninsula_example(fieldset, outfile, args.particles, mode=args.mode,\n-                          degree=args.degree, verbose=args.verbose,\n-                          output=not args.nooutput, method=method[args.method])\n+        peninsula_example(\n+            fieldset,\n+            outfile,\n+            args.particles,\n+            mode=args.mode,\n+            degree=args.degree,\n+            verbose=args.verbose,\n+            output=not args.nooutput,\n+            method=method[args.method],\n+        )\n \n \n if __name__ == \"__main__\":\ndiff --git a/docs/examples/example_radial_rotation.py b/docs/examples/example_radial_rotation.py\nindex cd00aba18..12ddb6379 100644\n--- a/docs/examples/example_radial_rotation.py\n+++ b/docs/examples/example_radial_rotation.py\n@@ -1,84 +1,100 @@\n import math\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import AdvectionRK4, FieldSet, JITParticle, ParticleSet, ScipyParticle\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n \n \n-def radial_rotation_fieldset(xdim=200, ydim=200):  # Define 2D flat, square fieldset for testing purposes.\n+def radial_rotation_fieldset(\n+    xdim=200, ydim=200\n+):  # Define 2D flat, square fieldset for testing purposes.\n \n     lon = np.linspace(0, 60, xdim, dtype=np.float32)\n     lat = np.linspace(0, 60, ydim, dtype=np.float32)\n \n-    x0 = 30.                                   # Define the origin to be the centre of the Field.\n-    y0 = 30.\n+    x0 = 30.0  # Define the origin to be the centre of the Field.\n+    y0 = 30.0\n \n     U = np.zeros((ydim, xdim), dtype=np.float32)\n     V = np.zeros((ydim, xdim), dtype=np.float32)\n \n-    T = delta(days=1)\n-    omega = 2*np.pi/T.total_seconds()          # Define the rotational period as 1 day.\n+    T = timedelta(days=1)\n+    omega = 2 * np.pi / T.total_seconds()  # Define the rotational period as 1 day.\n \n     for i in range(lon.size):\n         for j in range(lat.size):\n \n-            r = np.sqrt((lon[i]-x0)**2 + (lat[j]-y0)**2)  # Define radial displacement.\n-            assert r >= 0.\n+            r = np.sqrt(\n+                (lon[i] - x0) ** 2 + (lat[j] - y0) ** 2\n+            )  # Define radial displacement.\n+            assert r >= 0.0\n             assert r <= np.sqrt(x0**2 + y0**2)\n \n-            theta = math.atan2((lat[j]-y0), (lon[i]-x0))  # Define the polar angle.\n+            theta = math.atan2((lat[j] - y0), (lon[i] - x0))  # Define the polar angle.\n             assert abs(theta) <= np.pi\n \n             U[j, i] = r * math.sin(theta) * omega\n             V[j, i] = -r * math.cos(theta) * omega\n \n-    data = {'U': U, 'V': V}\n-    dimensions = {'lon': lon, 'lat': lat}\n-    return FieldSet.from_data(data, dimensions, mesh='flat')\n+    data = {\"U\": U, \"V\": V}\n+    dimensions = {\"lon\": lon, \"lat\": lat}\n+    return parcels.FieldSet.from_data(data, dimensions, mesh=\"flat\")\n \n \n def true_values(age):  # Calculate the expected values for particle 2 at the endtime.\n \n-    x = 20*math.sin(2*np.pi*age/(24.*60.**2)) + 30.\n-    y = 20*math.cos(2*np.pi*age/(24.*60.**2)) + 30.\n+    x = 20 * math.sin(2 * np.pi * age / (24.0 * 60.0**2)) + 30.0\n+    y = 20 * math.cos(2 * np.pi * age / (24.0 * 60.0**2)) + 30.0\n \n     return [x, y]\n \n \n-def rotation_example(fieldset, outfile, mode='jit', method=AdvectionRK4):\n+def rotation_example(fieldset, outfile, mode=\"jit\", method=parcels.AdvectionRK4):\n \n-    npart = 2          # Test two particles on the rotating fieldset.\n-    pset = ParticleSet.from_line(fieldset, size=npart, pclass=ptype[mode],\n-                                 start=(30., 30.),\n-                                 finish=(30., 50.))  # One particle in centre, one on periphery of Field.\n+    npart = 2  # Test two particles on the rotating fieldset.\n+    pset = parcels.ParticleSet.from_line(\n+        fieldset,\n+        size=npart,\n+        pclass=ptype[mode],\n+        start=(30.0, 30.0),\n+        finish=(30.0, 50.0),\n+    )  # One particle in centre, one on periphery of Field.\n \n-    runtime = delta(hours=17)\n-    dt = delta(minutes=5)\n-    outputdt = delta(hours=1)\n+    runtime = timedelta(hours=17)\n+    dt = timedelta(minutes=5)\n+    outputdt = timedelta(hours=1)\n \n-    pset.execute(method, runtime=runtime, dt=dt,\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=outputdt))\n+    pset.execute(\n+        method,\n+        runtime=runtime,\n+        dt=dt,\n+        output_file=pset.ParticleFile(name=outfile, outputdt=outputdt),\n+    )\n \n     return pset\n \n \n-@pytest.mark.parametrize('mode', ['scipy', 'jit'])\n+@pytest.mark.parametrize(\"mode\", [\"scipy\", \"jit\"])\n def test_rotation_example(mode, tmpdir):\n     fieldset = radial_rotation_fieldset()\n     outfile = tmpdir.join(\"RadialParticle\")\n     pset = rotation_example(fieldset, outfile, mode=mode)\n-    assert (pset[0].lon == 30. and pset[0].lat == 30.)  # Particle at centre of Field remains stationary.\n+    assert (\n+        pset[0].lon == 30.0 and pset[0].lat == 30.0\n+    )  # Particle at centre of Field remains stationary.\n     vals = true_values(pset.time[1])\n-    assert np.allclose(pset[1].lon, vals[0], 1e-5)    # Check advected values against calculated values.\n+    assert np.allclose(\n+        pset[1].lon, vals[0], 1e-5\n+    )  # Check advected values against calculated values.\n     assert np.allclose(pset[1].lat, vals[1], 1e-5)\n \n \n def main():\n-    filename = 'radial_rotation'\n+    filename = \"radial_rotation\"\n     fieldset = radial_rotation_fieldset()\n     fieldset.write(filename)\n \ndiff --git a/docs/examples/example_stommel.py b/docs/examples/example_stommel.py\nindex 052987638..58150eec6 100755\n--- a/docs/examples/example_stommel.py\n+++ b/docs/examples/example_stommel.py\n@@ -1,27 +1,21 @@\n import math\n from argparse import ArgumentParser\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import pytest\n \n-from parcels import (\n-    AdvectionEE,\n-    AdvectionRK4,\n-    AdvectionRK45,\n-    FieldSet,\n-    JITParticle,\n-    ParticleSet,\n-    ScipyParticle,\n-    Variable,\n-    timer,\n-)\n+import parcels\n \n-ptype = {'scipy': ScipyParticle, 'jit': JITParticle}\n-method = {'RK4': AdvectionRK4, 'EE': AdvectionEE, 'RK45': AdvectionRK45}\n+ptype = {\"scipy\": parcels.ScipyParticle, \"jit\": parcels.JITParticle}\n+method = {\n+    \"RK4\": parcels.AdvectionRK4,\n+    \"EE\": parcels.AdvectionEE,\n+    \"RK45\": parcels.AdvectionRK45,\n+}\n \n \n-def stommel_fieldset(xdim=200, ydim=200, grid_type='A'):\n+def stommel_fieldset(xdim=200, ydim=200, grid_type=\"A\"):\n     \"\"\"Simulate a periodic current along a western boundary, with significantly\n     larger velocities along the western edge than the rest of the region\n \n@@ -44,27 +38,42 @@ def stommel_fieldset(xdim=200, ydim=200, grid_type='A'):\n     P = np.zeros((lat.size, lon.size), dtype=np.float32)\n \n     beta = 2e-11\n-    r = 1/(11.6*86400)\n-    es = r/(beta*a)\n+    r = 1 / (11.6 * 86400)\n+    es = r / (beta * a)\n \n     for j in range(lat.size):\n         for i in range(lon.size):\n             xi = lon[i] / a\n             yi = lat[j] / b\n-            P[j, i] = (1 - math.exp(-xi / es) - xi) * math.pi * np.sin(math.pi * yi) * scalefac\n-            if grid_type == 'A':\n-                U[j, i] = -(1 - math.exp(-xi / es) - xi) * math.pi ** 2 * np.cos(math.pi * yi) * scalefac\n-                V[j, i] = (math.exp(-xi / es) / es - 1) * math.pi * np.sin(math.pi * yi) * scalefac\n-    if grid_type == 'C':\n+            P[j, i] = (\n+                (1 - math.exp(-xi / es) - xi)\n+                * math.pi\n+                * np.sin(math.pi * yi)\n+                * scalefac\n+            )\n+            if grid_type == \"A\":\n+                U[j, i] = (\n+                    -(1 - math.exp(-xi / es) - xi)\n+                    * math.pi**2\n+                    * np.cos(math.pi * yi)\n+                    * scalefac\n+                )\n+                V[j, i] = (\n+                    (math.exp(-xi / es) / es - 1)\n+                    * math.pi\n+                    * np.sin(math.pi * yi)\n+                    * scalefac\n+                )\n+    if grid_type == \"C\":\n         V[:, 1:] = (P[:, 1:] - P[:, 0:-1]) / dx * a\n         U[1:, :] = -(P[1:, :] - P[0:-1, :]) / dy * b\n \n-    data = {'U': U, 'V': V, 'P': P}\n-    dimensions = {'lon': lon, 'lat': lat}\n-    fieldset = FieldSet.from_data(data, dimensions, mesh='flat')\n-    if grid_type == 'C':\n-        fieldset.U.interp_method = 'cgrid_velocity'\n-        fieldset.V.interp_method = 'cgrid_velocity'\n+    data = {\"U\": U, \"V\": V, \"P\": P}\n+    dimensions = {\"lon\": lon, \"lat\": lat}\n+    fieldset = parcels.FieldSet.from_data(data, dimensions, mesh=\"flat\")\n+    if grid_type == \"C\":\n+        fieldset.U.interp_method = \"cgrid_velocity\"\n+        fieldset.V.interp_method = \"cgrid_velocity\"\n     return fieldset\n \n \n@@ -85,111 +94,207 @@ def simple_partition_function(coords, mpi_size=1):\n     return np.linspace(0, mpi_size, coords.shape[0], endpoint=False, dtype=np.int32)\n \n \n-def stommel_example(npart=1, mode='jit', verbose=False, method=AdvectionRK4, grid_type='A',\n-                    outfile=\"StommelParticle.zarr\", repeatdt=None, maxage=None, write_fields=True,\n-                    custom_partition_function=False):\n-    timer.fieldset = timer.Timer('FieldSet', parent=timer.stommel)\n+def stommel_example(\n+    npart=1,\n+    mode=\"jit\",\n+    verbose=False,\n+    method=parcels.AdvectionRK4,\n+    grid_type=\"A\",\n+    outfile=\"StommelParticle.zarr\",\n+    repeatdt=None,\n+    maxage=None,\n+    write_fields=True,\n+    custom_partition_function=False,\n+):\n+    parcels.timer.fieldset = parcels.timer.Timer(\n+        \"FieldSet\", parent=parcels.timer.stommel\n+    )\n     fieldset = stommel_fieldset(grid_type=grid_type)\n     if write_fields:\n-        filename = 'stommel'\n+        filename = \"stommel\"\n         fieldset.write(filename)\n-    timer.fieldset.stop()\n+    parcels.timer.fieldset.stop()\n \n     # Determine particle class according to mode\n-    timer.pset = timer.Timer('Pset', parent=timer.stommel)\n-    timer.psetinit = timer.Timer('Pset_init', parent=timer.pset)\n-    ParticleClass = JITParticle if mode == 'jit' else ScipyParticle\n+    parcels.timer.pset = parcels.timer.Timer(\"Pset\", parent=parcels.timer.stommel)\n+    parcels.timer.psetinit = parcels.timer.Timer(\"Pset_init\", parent=parcels.timer.pset)\n+    ParticleClass = parcels.JITParticle if mode == \"jit\" else parcels.ScipyParticle\n \n     # Execute for 600 days, with 1-hour timesteps and 5-day output\n-    runtime = delta(days=600)\n-    dt = delta(hours=1)\n-    outputdt = delta(days=5)\n-\n-    extra_vars = [Variable('p', dtype=np.float32, initial=0.),\n-                  Variable('p_start', dtype=np.float32, initial=0.),\n-                  Variable('next_dt', dtype=np.float64, initial=dt.total_seconds()),\n-                  Variable('age', dtype=np.float32, initial=0.)]\n+    runtime = timedelta(days=600)\n+    dt = timedelta(hours=1)\n+    outputdt = timedelta(days=5)\n+\n+    extra_vars = [\n+        parcels.Variable(\"p\", dtype=np.float32, initial=0.0),\n+        parcels.Variable(\"p_start\", dtype=np.float32, initial=0.0),\n+        parcels.Variable(\"next_dt\", dtype=np.float64, initial=dt.total_seconds()),\n+        parcels.Variable(\"age\", dtype=np.float32, initial=0.0),\n+    ]\n     MyParticle = ParticleClass.add_variables(extra_vars)\n \n     if custom_partition_function:\n-        pset = ParticleSet.from_line(fieldset, size=npart, pclass=MyParticle, repeatdt=repeatdt,\n-                                     start=(10e3, 5000e3), finish=(100e3, 5000e3), time=0,\n-                                     partition_function=simple_partition_function)\n+        pset = parcels.ParticleSet.from_line(\n+            fieldset,\n+            size=npart,\n+            pclass=MyParticle,\n+            repeatdt=repeatdt,\n+            start=(10e3, 5000e3),\n+            finish=(100e3, 5000e3),\n+            time=0,\n+            partition_function=simple_partition_function,\n+        )\n     else:\n-        pset = ParticleSet.from_line(fieldset, size=npart, pclass=MyParticle, repeatdt=repeatdt,\n-                                     start=(10e3, 5000e3), finish=(100e3, 5000e3), time=0)\n+        pset = parcels.ParticleSet.from_line(\n+            fieldset,\n+            size=npart,\n+            pclass=MyParticle,\n+            repeatdt=repeatdt,\n+            start=(10e3, 5000e3),\n+            finish=(100e3, 5000e3),\n+            time=0,\n+        )\n \n     if verbose:\n         print(f\"Initial particle positions:\\n{pset}\")\n \n     maxage = runtime.total_seconds() if maxage is None else maxage\n-    fieldset.add_constant('maxage', maxage)\n+    fieldset.add_constant(\"maxage\", maxage)\n     print(\"Stommel: Advecting %d particles for %s\" % (npart, runtime))\n-    timer.psetinit.stop()\n-    timer.psetrun = timer.Timer('Pset_run', parent=timer.pset)\n-    pset.execute(method + pset.Kernel(UpdateP) + pset.Kernel(AgeP), runtime=runtime, dt=dt,\n-                 output_file=pset.ParticleFile(name=outfile, outputdt=outputdt))\n+    parcels.timer.psetinit.stop()\n+    parcels.timer.psetrun = parcels.timer.Timer(\"Pset_run\", parent=parcels.timer.pset)\n+    pset.execute(\n+        method + pset.Kernel(UpdateP) + pset.Kernel(AgeP),\n+        runtime=runtime,\n+        dt=dt,\n+        output_file=pset.ParticleFile(name=outfile, outputdt=outputdt),\n+    )\n \n     if verbose:\n         print(f\"Final particle positions:\\n{pset}\")\n-    timer.psetrun.stop()\n-    timer.pset.stop()\n+    parcels.timer.psetrun.stop()\n+    parcels.timer.pset.stop()\n \n     return pset\n \n \n-@pytest.mark.parametrize('grid_type', ['A', 'C'])\n-@pytest.mark.parametrize('mode', ['jit', 'scipy'])\n+@pytest.mark.parametrize(\"grid_type\", [\"A\", \"C\"])\n+@pytest.mark.parametrize(\"mode\", [\"jit\", \"scipy\"])\n def test_stommel_fieldset(mode, grid_type, tmpdir):\n-    timer.root = timer.Timer('Main')\n-    timer.stommel = timer.Timer('Stommel', parent=timer.root)\n+    parcels.timer.root = parcels.timer.Timer(\"Main\")\n+    parcels.timer.stommel = parcels.timer.Timer(\"Stommel\", parent=parcels.timer.root)\n     outfile = tmpdir.join(\"StommelParticle\")\n-    psetRK4 = stommel_example(1, mode=mode, method=method['RK4'], grid_type=grid_type, outfile=outfile, write_fields=False)\n-    psetRK45 = stommel_example(1, mode=mode, method=method['RK45'], grid_type=grid_type, outfile=outfile, write_fields=False)\n+    psetRK4 = stommel_example(\n+        1,\n+        mode=mode,\n+        method=method[\"RK4\"],\n+        grid_type=grid_type,\n+        outfile=outfile,\n+        write_fields=False,\n+    )\n+    psetRK45 = stommel_example(\n+        1,\n+        mode=mode,\n+        method=method[\"RK45\"],\n+        grid_type=grid_type,\n+        outfile=outfile,\n+        write_fields=False,\n+    )\n     assert np.allclose(psetRK4.lon, psetRK45.lon, rtol=1e-3)\n     assert np.allclose(psetRK4.lat, psetRK45.lat, rtol=1.1e-3)\n     err_adv = np.abs(psetRK4.p_start - psetRK4.p)\n-    assert (err_adv <= 1.e-1).all()\n-    err_smpl = np.array([abs(psetRK4.p[i] - psetRK4.fieldset.P[0., psetRK4.lon[i], psetRK4.lat[i], psetRK4.depth[i]]) for i in range(psetRK4.size)])\n-    assert (err_smpl <= 1.e-1).all()\n-    timer.stommel.stop()\n-    timer.root.stop()\n-    timer.root.print_tree()\n+    assert (err_adv <= 1.0e-1).all()\n+    err_smpl = np.array(\n+        [\n+            abs(\n+                psetRK4.p[i]\n+                - psetRK4.fieldset.P[\n+                    0.0, psetRK4.lon[i], psetRK4.lat[i], psetRK4.depth[i]\n+                ]\n+            )\n+            for i in range(psetRK4.size)\n+        ]\n+    )\n+    assert (err_smpl <= 1.0e-1).all()\n+    parcels.timer.stommel.stop()\n+    parcels.timer.root.stop()\n+    parcels.timer.root.print_tree()\n \n \n def main(args=None):\n-    timer.root = timer.Timer('Main')\n-    timer.args = timer.Timer('Args', parent=timer.root)\n-    p = ArgumentParser(description=\"\"\"\n-Example of particle advection in the steady-state solution of the Stommel equation\"\"\")\n-    p.add_argument('mode', choices=('scipy', 'jit'), nargs='?', default='jit',\n-                   help='Execution mode for performing computation')\n-    p.add_argument('-p', '--particles', type=int, default=1,\n-                   help='Number of particles to advect')\n-    p.add_argument('-v', '--verbose', action='store_true', default=False,\n-                   help='Print particle information before and after execution')\n-    p.add_argument('-m', '--method', choices=('RK4', 'EE', 'RK45'), default='RK4',\n-                   help='Numerical method used for advection')\n-    p.add_argument('-o', '--outfile', default='StommelParticle.zarr',\n-                   help='Name of output file')\n-    p.add_argument('-r', '--repeatdt', default=None, type=int,\n-                   help='repeatdt of the ParticleSet')\n-    p.add_argument('-a', '--maxage', default=None, type=int,\n-                   help='max age of the particles (after which particles are deleted)')\n-    p.add_argument('-wf', '--write_fields', default=True,\n-                   help='Write the hydrodynamic fields to NetCDF')\n-    p.add_argument('-cpf', '--custom_partition_function', default=False,\n-                   help='Use a custom partition_function (for MPI testing purposes)')\n+    parcels.timer.root = parcels.timer.Timer(\"Main\")\n+    parcels.timer.args = parcels.timer.Timer(\"Args\", parent=parcels.timer.root)\n+    p = ArgumentParser(\n+        description=\"\"\"\n+Example of particle advection in the steady-state solution of the Stommel equation\"\"\"\n+    )\n+    p.add_argument(\n+        \"mode\",\n+        choices=(\"scipy\", \"jit\"),\n+        nargs=\"?\",\n+        default=\"jit\",\n+        help=\"Execution mode for performing computation\",\n+    )\n+    p.add_argument(\n+        \"-p\", \"--particles\", type=int, default=1, help=\"Number of particles to advect\"\n+    )\n+    p.add_argument(\n+        \"-v\",\n+        \"--verbose\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Print particle information before and after execution\",\n+    )\n+    p.add_argument(\n+        \"-m\",\n+        \"--method\",\n+        choices=(\"RK4\", \"EE\", \"RK45\"),\n+        default=\"RK4\",\n+        help=\"Numerical method used for advection\",\n+    )\n+    p.add_argument(\n+        \"-o\", \"--outfile\", default=\"StommelParticle.zarr\", help=\"Name of output file\"\n+    )\n+    p.add_argument(\n+        \"-r\", \"--repeatdt\", default=None, type=int, help=\"repeatdt of the ParticleSet\"\n+    )\n+    p.add_argument(\n+        \"-a\",\n+        \"--maxage\",\n+        default=None,\n+        type=int,\n+        help=\"max age of the particles (after which particles are deleted)\",\n+    )\n+    p.add_argument(\n+        \"-wf\",\n+        \"--write_fields\",\n+        default=True,\n+        help=\"Write the hydrodynamic fields to NetCDF\",\n+    )\n+    p.add_argument(\n+        \"-cpf\",\n+        \"--custom_partition_function\",\n+        default=False,\n+        help=\"Use a custom partition_function (for MPI testing purposes)\",\n+    )\n     args = p.parse_args(args)\n \n-    timer.args.stop()\n-    timer.stommel = timer.Timer('Stommel', parent=timer.root)\n-    stommel_example(args.particles, mode=args.mode, verbose=args.verbose, method=method[args.method],\n-                    outfile=args.outfile, repeatdt=args.repeatdt, maxage=args.maxage, write_fields=args.write_fields,\n-                    custom_partition_function=args.custom_partition_function)\n-    timer.stommel.stop()\n-    timer.root.stop()\n-    timer.root.print_tree()\n+    parcels.timer.args.stop()\n+    parcels.timer.stommel = parcels.timer.Timer(\"Stommel\", parent=parcels.timer.root)\n+    stommel_example(\n+        args.particles,\n+        mode=args.mode,\n+        verbose=args.verbose,\n+        method=method[args.method],\n+        outfile=args.outfile,\n+        repeatdt=args.repeatdt,\n+        maxage=args.maxage,\n+        write_fields=args.write_fields,\n+        custom_partition_function=args.custom_partition_function,\n+    )\n+    parcels.timer.stommel.stop()\n+    parcels.timer.root.stop()\n+    parcels.timer.root.print_tree()\n \n \n if __name__ == \"__main__\":\ndiff --git a/docs/examples/parcels_tutorial.ipynb b/docs/examples/parcels_tutorial.ipynb\nindex 62acc6f18..c98513f6c 100644\n--- a/docs/examples/parcels_tutorial.ipynb\n+++ b/docs/examples/parcels_tutorial.ipynb\n@@ -49,14 +49,7 @@\n     \"from IPython.display import HTML\\n\",\n     \"from matplotlib.animation import FuncAnimation\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    Variable,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -81,9 +74,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"MovingEddies_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"MovingEddies_data\\\")\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_parcels(f\\\"{example_dataset_folder}/moving_eddies\\\")\"\n+    \"fieldset = parcels.FieldSet.from_parcels(f\\\"{example_dataset_folder}/moving_eddies\\\")\"\n    ]\n   },\n   {\n@@ -134,9 +127,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"pset = ParticleSet.from_list(\\n\",\n+    \"pset = parcels.ParticleSet.from_list(\\n\",\n     \"    fieldset=fieldset,  # the fields on which the particles are advected\\n\",\n-    \"    pclass=JITParticle,  # the type of particles (JITParticle or ScipyParticle)\\n\",\n+    \"    pclass=parcels.JITParticle,  # the type of particles (JITParticle or ScipyParticle)\\n\",\n     \"    lon=[3.3e5, 3.3e5],  # a vector of release longitudes\\n\",\n     \"    lat=[1e5, 2.8e5],  # a vector of release latitudes\\n\",\n     \")\"\n@@ -238,7 +231,7 @@\n     \"    outputdt=timedelta(hours=1),  # the time step of the outputs\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4,  # the kernel (which defines how particles move)\\n\",\n+    \"    parcels.AdvectionRK4,  # the kernel (which defines how particles move)\\n\",\n     \"    runtime=timedelta(days=6),  # the total length of the run\\n\",\n     \"    dt=timedelta(minutes=5),  # the timestep of the kernel\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -13211,7 +13204,7 @@\n     \"    outputdt=timedelta(hours=1),  # the time step of the outputs\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4,\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n     \"    dt=-timedelta(minutes=5),  # negative timestep for backward run\\n\",\n     \"    runtime=timedelta(days=6),  # the run time\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -13328,15 +13321,15 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet.from_list(\\n\",\n-    \"    fieldset=fieldset, pclass=JITParticle, lon=[3.3e5, 3.3e5], lat=[1e5, 2.8e5]\\n\",\n+    \"pset = parcels.ParticleSet.from_list(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=[3.3e5, 3.3e5], lat=[1e5, 2.8e5]\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\n     \"    name=\\\"EddyParticles_WestVel.zarr\\\", outputdt=timedelta(hours=1)\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, WestVel],  # simply combine the Kernels in a list\\n\",\n+    \"    [parcels.AdvectionRK4, WestVel],  # simply combine the Kernels in a list\\n\",\n     \"    runtime=timedelta(days=2),\\n\",\n     \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -13416,7 +13409,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n@@ -13459,7 +13452,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\"\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -13476,9 +13469,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"pset = ParticleSet.from_line(\\n\",\n+    \"pset = parcels.ParticleSet.from_line(\\n\",\n     \"    fieldset=fieldset,\\n\",\n-    \"    pclass=JITParticle,\\n\",\n+    \"    pclass=parcels.JITParticle,\\n\",\n     \"    size=5,  # releasing 5 particles\\n\",\n     \"    start=(28, -33),  # releasing on a line: the start longitude and latitude\\n\",\n     \"    finish=(30, -33),  # releasing on a line: the end longitude and latitude\\n\",\n@@ -13512,7 +13505,7 @@\n     \"    name=\\\"GlobCurrentParticles.zarr\\\", outputdt=timedelta(hours=6)\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4,\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n     \"    runtime=timedelta(days=10),\\n\",\n     \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -13579,8 +13572,8 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\",\\n\",\n     \"    extra_fields={\\\"P\\\": \\\"P\\\"},\\n\",\n     \"    allow_time_extrapolation=True,\\n\",\n@@ -13601,7 +13594,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"SampleParticle = JITParticle.add_variable(\\\"p\\\")\"\n+    \"SampleParticle = parcels.JITParticle.add_variable(\\\"p\\\")\"\n    ]\n   },\n   {\n@@ -13642,7 +13635,7 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet.from_line(\\n\",\n+    \"pset = parcels.ParticleSet.from_line(\\n\",\n     \"    fieldset=fieldset,\\n\",\n     \"    pclass=SampleParticle,\\n\",\n     \"    start=(3000, 3000),\\n\",\n@@ -13706,7 +13699,7 @@\n     \"    name=\\\"PeninsulaPressure.zarr\\\", outputdt=timedelta(hours=1)\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, SampleP],  # list of kernels to be executed\\n\",\n+    \"    [parcels.AdvectionRK4, SampleP],  # list of kernels to be executed\\n\",\n     \"    runtime=timedelta(hours=20),\\n\",\n     \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -13787,12 +13780,16 @@\n    \"outputs\": [],\n    \"source\": [\n     \"extra_vars = [\\n\",\n-    \"    Variable(\\\"distance\\\", initial=0.0, dtype=np.float32),\\n\",\n-    \"    Variable(\\\"prev_lon\\\", dtype=np.float32, to_write=False, initial=attrgetter(\\\"lon\\\")),\\n\",\n-    \"    Variable(\\\"prev_lat\\\", dtype=np.float32, to_write=False, initial=attrgetter(\\\"lat\\\")),\\n\",\n+    \"    parcels.Variable(\\\"distance\\\", initial=0.0, dtype=np.float32),\\n\",\n+    \"    parcels.Variable(\\n\",\n+    \"        \\\"prev_lon\\\", dtype=np.float32, to_write=False, initial=attrgetter(\\\"lon\\\")\\n\",\n+    \"    ),\\n\",\n+    \"    parcels.Variable(\\n\",\n+    \"        \\\"prev_lat\\\", dtype=np.float32, to_write=False, initial=attrgetter(\\\"lat\\\")\\n\",\n+    \"    ),\\n\",\n     \"]\\n\",\n     \"\\n\",\n-    \"DistParticle = JITParticle.add_variables(extra_vars)\"\n+    \"DistParticle = parcels.JITParticle.add_variables(extra_vars)\"\n    ]\n   },\n   {\n@@ -13848,7 +13845,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n     \"    \\\"V\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n@@ -13858,8 +13855,8 @@\n     \"    \\\"V\\\": \\\"northward_eulerian_current_velocity\\\",\\n\",\n     \"}\\n\",\n     \"dimensions = {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"}\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n-    \"pset = ParticleSet.from_line(\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"pset = parcels.ParticleSet.from_line(\\n\",\n     \"    fieldset=fieldset, pclass=DistParticle, size=5, start=(28, -33), finish=(30, -33)\\n\",\n     \")\"\n    ]\n@@ -13888,7 +13885,7 @@\n    ],\n    \"source\": [\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, TotalDistance],  # list of kernels to be executed\\n\",\n+    \"    [parcels.AdvectionRK4, TotalDistance],  # list of kernels to be executed\\n\",\n     \"    runtime=timedelta(days=6),\\n\",\n     \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=pset.ParticleFile(\\n\",\ndiff --git a/docs/examples/tutorial_Argofloats.ipynb b/docs/examples/tutorial_Argofloats.ipynb\nindex 96c5ab81b..368a12068 100644\n--- a/docs/examples/tutorial_Argofloats.ipynb\n+++ b/docs/examples/tutorial_Argofloats.ipynb\n@@ -103,18 +103,10 @@\n     \"\\n\",\n     \"import numpy as np\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    StatusCode,\\n\",\n-    \"    Variable,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n     \"# Load the GlobCurrent data in the Agulhas region from the example_data\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n     \"    \\\"V\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n@@ -124,13 +116,13 @@\n     \"    \\\"V\\\": \\\"northward_eulerian_current_velocity\\\",\\n\",\n     \"}\\n\",\n     \"dimensions = {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"}\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n     \"# uppermost layer in the hydrodynamic data\\n\",\n     \"fieldset.mindepth = fieldset.U.depth[0]\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"# Define a new Particle type including extra Variables\\n\",\n-    \"ArgoParticle = JITParticle.add_variables(\\n\",\n+    \"ArgoParticle = parcels.JITParticle.add_variables(\\n\",\n     \"    [\\n\",\n     \"        # Phase of cycle:\\n\",\n     \"        # init_descend=0,\\n\",\n@@ -138,21 +130,21 @@\n     \"        # profile_descend=2,\\n\",\n     \"        # profile_ascend=3,\\n\",\n     \"        # transmit=4\\n\",\n-    \"        Variable(\\\"cycle_phase\\\", dtype=np.int32, initial=0.0),\\n\",\n-    \"        Variable(\\\"cycle_age\\\", dtype=np.float32, initial=0.0),\\n\",\n-    \"        Variable(\\\"drift_age\\\", dtype=np.float32, initial=0.0),\\n\",\n+    \"        parcels.Variable(\\\"cycle_phase\\\", dtype=np.int32, initial=0.0),\\n\",\n+    \"        parcels.Variable(\\\"cycle_age\\\", dtype=np.float32, initial=0.0),\\n\",\n+    \"        parcels.Variable(\\\"drift_age\\\", dtype=np.float32, initial=0.0),\\n\",\n     \"        # if fieldset has temperature\\n\",\n     \"        # Variable('temp', dtype=np.float32, initial=np.nan),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"# Initiate one Argo float in the Agulhas Current\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset, pclass=ArgoParticle, lon=[32], lat=[-31], depth=[0]\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"# combine Argo vertical movement kernel with built-in Advection kernel\\n\",\n-    \"kernels = [ArgoVerticalMovement, AdvectionRK4]\\n\",\n+    \"kernels = [ArgoVerticalMovement, parcels.AdvectionRK4]\\n\",\n     \"\\n\",\n     \"# Create a ParticleFile object to store the output\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\ndiff --git a/docs/examples/tutorial_NestedFields.ipynb b/docs/examples/tutorial_NestedFields.ipynb\nindex 664e01365..9ea16da61 100644\n--- a/docs/examples/tutorial_NestedFields.ipynb\n+++ b/docs/examples/tutorial_NestedFields.ipynb\n@@ -46,7 +46,7 @@\n     \"import numpy as np\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import AdvectionRK4, Field, FieldSet, JITParticle, NestedField, ParticleSet\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -68,8 +68,8 @@\n     \"lat = np.linspace(0.0, 2e3, dim, dtype=np.float32)\\n\",\n     \"lon_g, lat_g = np.meshgrid(lon, lat)\\n\",\n     \"V1_data = np.cos(lon_g / 200 * np.pi / 2)\\n\",\n-    \"U1 = Field(\\\"U1\\\", np.ones((dim, dim), dtype=np.float32), lon=lon, lat=lat)\\n\",\n-    \"V1 = Field(\\\"V1\\\", V1_data, grid=U1.grid)\"\n+    \"U1 = parcels.Field(\\\"U1\\\", np.ones((dim, dim), dtype=np.float32), lon=lon, lat=lat)\\n\",\n+    \"V1 = parcels.Field(\\\"V1\\\", V1_data, grid=U1.grid)\"\n    ]\n   },\n   {\n@@ -92,8 +92,8 @@\n     \"lat = np.linspace(-1e3, 3e3, ydim, dtype=np.float32)\\n\",\n     \"lon_g, lat_g = np.meshgrid(lon, lat)\\n\",\n     \"V2_data = np.cos(lon_g / 200 * np.pi / 2)\\n\",\n-    \"U2 = Field(\\\"U2\\\", np.ones((ydim, xdim), dtype=np.float32), lon=lon, lat=lat)\\n\",\n-    \"V2 = Field(\\\"V2\\\", V2_data, grid=U2.grid)\"\n+    \"U2 = parcels.Field(\\\"U2\\\", np.ones((ydim, xdim), dtype=np.float32), lon=lon, lat=lat)\\n\",\n+    \"V2 = parcels.Field(\\\"V2\\\", V2_data, grid=U2.grid)\"\n    ]\n   },\n   {\n@@ -110,9 +110,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"U = NestedField(\\\"U\\\", [U1, U2])\\n\",\n-    \"V = NestedField(\\\"V\\\", [V1, V2])\\n\",\n-    \"fieldset = FieldSet(U, V)\"\n+    \"U = parcels.NestedField(\\\"U\\\", [U1, U2])\\n\",\n+    \"V = parcels.NestedField(\\\"V\\\", [V1, V2])\\n\",\n+    \"fieldset = parcels.FieldSet(U, V)\"\n    ]\n   },\n   {\n@@ -140,13 +140,13 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(fieldset, pclass=JITParticle, lon=[0], lat=[1000])\\n\",\n+    \"pset = parcels.ParticleSet(fieldset, pclass=parcels.JITParticle, lon=[0], lat=[1000])\\n\",\n     \"\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\n     \"    name=\\\"NestedFieldParticle.zarr\\\", outputdt=50, chunks=(1, 100)\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"pset.execute(AdvectionRK4, runtime=14000, dt=10, output_file=output_file)\\n\",\n+    \"pset.execute(parcels.AdvectionRK4, runtime=14000, dt=10, output_file=output_file)\\n\",\n     \"\\n\",\n     \"ds = xr.open_zarr(\\\"NestedFieldParticle.zarr\\\")\\n\",\n     \"plt.plot(ds.lon.T, ds.lat.T, \\\".-\\\")\\n\",\n@@ -197,15 +197,15 @@\n    \"outputs\": [],\n    \"source\": [\n     \"# Need to redefine fieldset\\n\",\n-    \"fieldset = FieldSet(U, V)\\n\",\n+    \"fieldset = parcels.FieldSet(U, V)\\n\",\n     \"\\n\",\n     \"ones_array1 = np.ones((U1.grid.ydim, U1.grid.xdim), dtype=np.float32)\\n\",\n-    \"F1 = Field(\\\"F1\\\", ones_array1, grid=U1.grid)\\n\",\n+    \"F1 = parcels.Field(\\\"F1\\\", ones_array1, grid=U1.grid)\\n\",\n     \"\\n\",\n     \"ones_array2 = np.ones((U2.grid.ydim, U2.grid.xdim), dtype=np.float32)\\n\",\n-    \"F2 = Field(\\\"F2\\\", 2 * ones_array2, grid=U2.grid)\\n\",\n+    \"F2 = parcels.Field(\\\"F2\\\", 2 * ones_array2, grid=U2.grid)\\n\",\n     \"\\n\",\n-    \"F = NestedField(\\\"F\\\", [F1, F2])\\n\",\n+    \"F = parcels.NestedField(\\\"F\\\", [F1, F2])\\n\",\n     \"fieldset.add_field(F)\"\n    ]\n   },\n@@ -230,9 +230,9 @@\n     \"    particle.f = fieldset.F[time, particle.depth, particle.lat, particle.lon]\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"SampleParticle = JITParticle.add_variable(\\\"f\\\", dtype=np.int32)\\n\",\n+    \"SampleParticle = parcels.JITParticle.add_variable(\\\"f\\\", dtype=np.int32)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset, pclass=SampleParticle, lon=[1000], lat=[500])\\n\",\n+    \"pset = parcels.ParticleSet(fieldset, pclass=SampleParticle, lon=[1000], lat=[500])\\n\",\n     \"pset.execute(SampleNestedFieldIndex, runtime=1)\\n\",\n     \"print(\\n\",\n     \"    f\\\"Particle ({pset[0].lon:g}, {pset[0].lat:g}) \\\"\\n\",\ndiff --git a/docs/examples/tutorial_analyticaladvection.ipynb b/docs/examples/tutorial_analyticaladvection.ipynb\nindex 84d7f96dc..59f036ea9 100644\n--- a/docs/examples/tutorial_analyticaladvection.ipynb\n+++ b/docs/examples/tutorial_analyticaladvection.ipynb\n@@ -52,7 +52,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import numpy as np\\n\",\n@@ -60,15 +60,7 @@\n     \"from IPython.display import HTML\\n\",\n     \"from matplotlib.animation import FuncAnimation\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionAnalytical,\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    ScipyParticle,\\n\",\n-    \"    Variable,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -102,7 +94,7 @@\n     \"    def calc_r_phi(ln, lt):\\n\",\n     \"        return np.sqrt(ln**2 + lt**2), np.arctan2(ln, lt)\\n\",\n     \"\\n\",\n-    \"    omega = 2 * np.pi / delta(days=1).total_seconds()\\n\",\n+    \"    omega = 2 * np.pi / timedelta(days=1).total_seconds()\\n\",\n     \"    for i in range(lon.size):\\n\",\n     \"        for j in range(lat.size):\\n\",\n     \"            r, phi = calc_r_phi(lon[i], lat[j])\\n\",\n@@ -114,7 +106,7 @@\n     \"\\n\",\n     \"    data = {\\\"U\\\": U, \\\"V\\\": V, \\\"R\\\": R}\\n\",\n     \"    dimensions = {\\\"lon\\\": lon, \\\"lat\\\": lat}\\n\",\n-    \"    fieldset = FieldSet.from_data(data, dimensions, mesh=\\\"flat\\\")\\n\",\n+    \"    fieldset = parcels.FieldSet.from_data(data, dimensions, mesh=\\\"flat\\\")\\n\",\n     \"    fieldset.U.interp_method = \\\"cgrid_velocity\\\"\\n\",\n     \"    fieldset.V.interp_method = \\\"cgrid_velocity\\\"\\n\",\n     \"    return fieldset\\n\",\n@@ -154,22 +146,22 @@\n     \"    particle.radius = fieldset.R[time, particle.depth, particle.lat, particle.lon]\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"MyParticle = ScipyParticle.add_variables(\\n\",\n+    \"MyParticle = parcels.ScipyParticle.add_variables(\\n\",\n     \"    [\\n\",\n-    \"        Variable(\\\"radius\\\", dtype=np.float32, initial=0.0),\\n\",\n-    \"        Variable(\\\"radius_start\\\", dtype=np.float32, initial=0.0),\\n\",\n+    \"        parcels.Variable(\\\"radius\\\", dtype=np.float32, initial=0.0),\\n\",\n+    \"        parcels.Variable(\\\"radius_start\\\", dtype=np.float32, initial=0.0),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldsetRR, pclass=MyParticle, lon=0, lat=4e3, time=0)\\n\",\n+    \"pset = parcels.ParticleSet(fieldsetRR, pclass=MyParticle, lon=0, lat=4e3, time=0)\\n\",\n     \"\\n\",\n-    \"output = pset.ParticleFile(name=\\\"radialAnalytical.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output = pset.ParticleFile(name=\\\"radialAnalytical.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    pset.Kernel(UpdateR) + AdvectionAnalytical,\\n\",\n-    \"    runtime=delta(hours=25),\\n\",\n-    \"    dt=delta(hours=1),  # needs to be the same as outputdt for Analytical Advection\\n\",\n+    \"    pset.Kernel(UpdateR) + parcels.AdvectionAnalytical,\\n\",\n+    \"    runtime=timedelta(hours=25),\\n\",\n+    \"    dt=timedelta(hours=1),  # needs to be the same as outputdt for Analytical Advection\\n\",\n     \"    output_file=output,\\n\",\n     \")\"\n    ]\n@@ -273,7 +265,7 @@\n     \"    data = {\\\"U\\\": U, \\\"V\\\": V}\\n\",\n     \"    dimensions = {\\\"lon\\\": lon, \\\"lat\\\": lat, \\\"time\\\": times}\\n\",\n     \"    allow_time_extrapolation = True if len(times) == 1 else False\\n\",\n-    \"    fieldset = FieldSet.from_data(\\n\",\n+    \"    fieldset = parcels.FieldSet.from_data(\\n\",\n     \"        data, dimensions, mesh=\\\"flat\\\", allow_time_extrapolation=allow_time_extrapolation\\n\",\n     \"    )\\n\",\n     \"    fieldset.U.interp_method = \\\"cgrid_velocity\\\"\\n\",\n@@ -309,12 +301,12 @@\n    \"source\": [\n     \"X, Y = np.meshgrid(np.arange(0.15, 1.85, 0.1), np.arange(0.15, 0.85, 0.1))\\n\",\n     \"\\n\",\n-    \"psetAA = ParticleSet(fieldsetDG, pclass=ScipyParticle, lon=X, lat=Y)\\n\",\n+    \"psetAA = parcels.ParticleSet(fieldsetDG, pclass=parcels.ScipyParticle, lon=X, lat=Y)\\n\",\n     \"\\n\",\n     \"output = psetAA.ParticleFile(name=\\\"doublegyreAA.zarr\\\", outputdt=0.1)\\n\",\n     \"\\n\",\n     \"psetAA.execute(\\n\",\n-    \"    AdvectionAnalytical,\\n\",\n+    \"    parcels.AdvectionAnalytical,\\n\",\n     \"    dt=0.1,  # needs to be the same as outputdt for Analytical Advection\\n\",\n     \"    runtime=3,\\n\",\n     \"    output_file=output,\\n\",\n@@ -14699,8 +14691,8 @@\n     }\n    ],\n    \"source\": [\n-    \"psetRK4 = ParticleSet(fieldsetDG, pclass=JITParticle, lon=X, lat=Y)\\n\",\n-    \"psetRK4.execute(AdvectionRK4, dt=0.01, runtime=3)\"\n+    \"psetRK4 = parcels.ParticleSet(fieldsetDG, pclass=parcels.JITParticle, lon=X, lat=Y)\\n\",\n+    \"psetRK4.execute(parcels.AdvectionRK4, dt=0.01, runtime=3)\"\n    ]\n   },\n   {\n@@ -14807,7 +14799,7 @@\n     \"    data = {\\\"U\\\": U, \\\"V\\\": V, \\\"P\\\": P}\\n\",\n     \"    dimensions = {\\\"lon\\\": lon, \\\"lat\\\": lat, \\\"time\\\": times}\\n\",\n     \"    allow_time_extrapolation = True if len(times) == 1 else False\\n\",\n-    \"    fieldset = FieldSet.from_data(\\n\",\n+    \"    fieldset = parcels.FieldSet.from_data(\\n\",\n     \"        data, dimensions, mesh=\\\"flat\\\", allow_time_extrapolation=allow_time_extrapolation\\n\",\n     \"    )\\n\",\n     \"    fieldset.U.interp_method = \\\"cgrid_velocity\\\"\\n\",\n@@ -14869,14 +14861,16 @@\n    \"source\": [\n     \"X, Y = np.meshgrid(np.arange(0, 19900, 100), np.arange(-100, 100, 100))\\n\",\n     \"\\n\",\n-    \"psetAA = ParticleSet(fieldsetBJ, pclass=ScipyParticle, lon=X, lat=Y, time=0)\\n\",\n+    \"psetAA = parcels.ParticleSet(\\n\",\n+    \"    fieldsetBJ, pclass=parcels.ScipyParticle, lon=X, lat=Y, time=0\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"output = psetAA.ParticleFile(name=\\\"bickleyjetAA.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output = psetAA.ParticleFile(name=\\\"bickleyjetAA.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"psetAA.execute(\\n\",\n-    \"    AdvectionAnalytical + psetAA.Kernel(ZonalBC),\\n\",\n-    \"    dt=delta(hours=1),  # needs to be the same as outputdt for Analytical Advection\\n\",\n-    \"    runtime=delta(days=1),\\n\",\n+    \"    parcels.AdvectionAnalytical + psetAA.Kernel(ZonalBC),\\n\",\n+    \"    dt=timedelta(hours=1),  # needs to be the same as outputdt for Analytical Advection\\n\",\n+    \"    runtime=timedelta(days=1),\\n\",\n     \"    output_file=output,\\n\",\n     \")\"\n    ]\n@@ -28012,9 +28006,11 @@\n     }\n    ],\n    \"source\": [\n-    \"psetRK4 = ParticleSet(fieldsetBJ, pclass=JITParticle, lon=X, lat=Y)\\n\",\n+    \"psetRK4 = parcels.ParticleSet(fieldsetBJ, pclass=parcels.JITParticle, lon=X, lat=Y)\\n\",\n     \"\\n\",\n-    \"psetRK4.execute([AdvectionRK4, ZonalBC], dt=delta(minutes=5), runtime=delta(days=1))\"\n+    \"psetRK4.execute(\\n\",\n+    \"    [parcels.AdvectionRK4, ZonalBC], dt=timedelta(minutes=5), runtime=timedelta(days=1)\\n\",\n+    \")\"\n    ]\n   },\n   {\ndiff --git a/docs/examples/tutorial_delaystart.ipynb b/docs/examples/tutorial_delaystart.ipynb\nindex 68d0bce87..1ed477b14 100644\n--- a/docs/examples/tutorial_delaystart.ipynb\n+++ b/docs/examples/tutorial_delaystart.ipynb\n@@ -24,7 +24,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import numpy as np\\n\",\n@@ -32,15 +32,7 @@\n     \"from IPython.display import HTML\\n\",\n     \"from matplotlib.animation import FuncAnimation\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleFile,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    Variable,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -57,8 +49,8 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\", allow_time_extrapolation=True\\n\",\n     \")\"\n    ]\n@@ -98,9 +90,11 @@\n     \"lat = np.linspace(3e3, 45e3, npart, dtype=np.float32)\\n\",\n     \"\\n\",\n     \"# release every particle one hour later\\n\",\n-    \"time = np.arange(0, npart) * delta(hours=1).total_seconds()\\n\",\n+    \"time = np.arange(0, npart) * timedelta(hours=1).total_seconds()\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\"\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\"\n    ]\n   },\n   {\n@@ -126,10 +120,15 @@\n     }\n    ],\n    \"source\": [\n-    \"output_file = pset.ParticleFile(name=\\\"DelayParticle_time.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(\\n\",\n+    \"    name=\\\"DelayParticle_time.zarr\\\", outputdt=timedelta(hours=1)\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(hours=24), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(hours=24),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -10046,10 +10045,10 @@\n     \"npart = 10  # number of particles to be released\\n\",\n     \"lon = 3e3 * np.ones(npart)\\n\",\n     \"lat = np.linspace(3e3, 45e3, npart, dtype=np.float32)\\n\",\n-    \"repeatdt = delta(hours=3)  # release from the same set of locations every 3h\\n\",\n+    \"repeatdt = timedelta(hours=3)  # release from the same set of locations every 3h\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n-    \"    fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, repeatdt=repeatdt\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, repeatdt=repeatdt\\n\",\n     \")\"\n    ]\n   },\n@@ -10077,10 +10076,15 @@\n     }\n    ],\n    \"source\": [\n-    \"output_file = pset.ParticleFile(name=\\\"DelayParticle_releasedt\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(\\n\",\n+    \"    name=\\\"DelayParticle_releasedt\\\", outputdt=timedelta(hours=1)\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(hours=24), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(hours=24),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -21258,16 +21262,19 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(\\n\",\n-    \"    fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, repeatdt=repeatdt\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, repeatdt=repeatdt\\n\",\n     \")\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\n-    \"    name=\\\"DelayParticle_releasedt_9hrs\\\", outputdt=delta(hours=1)\\n\",\n+    \"    name=\\\"DelayParticle_releasedt_9hrs\\\", outputdt=timedelta(hours=1)\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"# first run for 3 * 3 hrs\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(hours=9), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(hours=9),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"# now stop the repeated release\\n\",\n@@ -21275,7 +21282,10 @@\n     \"\\n\",\n     \"# now continue running for the remaining 15 hours\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(hours=15), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(hours=15),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -32246,13 +32256,17 @@\n    \"source\": [\n     \"outfilepath = \\\"DelayParticle_nonmatchingtime.zarr\\\"\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n-    \"    fieldset=fieldset, pclass=JITParticle, lat=[3e3] * 3, lon=[3e3] * 3, time=[0, 1, 2]\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset,\\n\",\n+    \"    pclass=parcels.JITParticle,\\n\",\n+    \"    lat=[3e3] * 3,\\n\",\n+    \"    lon=[3e3] * 3,\\n\",\n+    \"    time=[0, 1, 2],\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"output_file = pset.ParticleFile(name=outfilepath, outputdt=2)\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4,\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n     \"    endtime=4,\\n\",\n     \"    dt=1,\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -32324,16 +32338,16 @@\n    ],\n    \"source\": [\n     \"for times in [[0, 2], [1]]:\\n\",\n-    \"    pset = ParticleSet(\\n\",\n+    \"    pset = parcels.ParticleSet(\\n\",\n     \"        fieldset=fieldset,\\n\",\n-    \"        pclass=JITParticle,\\n\",\n+    \"        pclass=parcels.JITParticle,\\n\",\n     \"        lat=[3e3] * len(times),\\n\",\n     \"        lon=[3e3] * len(times),\\n\",\n     \"        time=times,\\n\",\n     \"    )\\n\",\n     \"    output_file = pset.ParticleFile(name=outfilepath, outputdt=2)\\n\",\n     \"    pset.execute(\\n\",\n-    \"        AdvectionRK4,\\n\",\n+    \"        parcels.AdvectionRK4,\\n\",\n     \"        endtime=4,\\n\",\n     \"        dt=1,\\n\",\n     \"        output_file=output_file,\\n\",\n@@ -32416,11 +32430,11 @@\n     }\n    ],\n    \"source\": [\n-    \"GrowingParticle = JITParticle.add_variables(\\n\",\n+    \"GrowingParticle = parcels.JITParticle.add_variables(\\n\",\n     \"    [\\n\",\n-    \"        Variable(\\\"mass\\\", initial=0),\\n\",\n-    \"        Variable(\\\"splittime\\\", initial=-1),\\n\",\n-    \"        Variable(\\\"splitmass\\\", initial=0),\\n\",\n+    \"        parcels.Variable(\\\"mass\\\", initial=0),\\n\",\n+    \"        parcels.Variable(\\\"splittime\\\", initial=-1),\\n\",\n+    \"        parcels.Variable(\\\"splitmass\\\", initial=0),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -32435,8 +32449,8 @@\n     \"        particle.mass = particle.mass / 2.0\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=GrowingParticle, lon=0, lat=0)\\n\",\n-    \"outfile = ParticleFile(\\\"growingparticles.zarr\\\", pset, outputdt=1)\\n\",\n+    \"pset = parcels.ParticleSet(fieldset=fieldset, pclass=GrowingParticle, lon=0, lat=0)\\n\",\n+    \"outfile = parcels.ParticleFile(\\\"growingparticles.zarr\\\", pset, outputdt=1)\\n\",\n     \"\\n\",\n     \"for t in range(40):\\n\",\n     \"    pset.execute(\\n\",\n@@ -32445,7 +32459,7 @@\n     \"    for p in pset:\\n\",\n     \"        if p.splittime > 0:\\n\",\n     \"            pset.add(\\n\",\n-    \"                ParticleSet(\\n\",\n+    \"                parcels.ParticleSet(\\n\",\n     \"                    fieldset=fieldset,\\n\",\n     \"                    pclass=GrowingParticle,\\n\",\n     \"                    lon=0,\\n\",\ndiff --git a/docs/examples/tutorial_diffusion.ipynb b/docs/examples/tutorial_diffusion.ipynb\nindex 33f469f4e..a14dd466d 100644\n--- a/docs/examples/tutorial_diffusion.ipynb\n+++ b/docs/examples/tutorial_diffusion.ipynb\n@@ -117,19 +117,7 @@\n     \"import trajan as ta\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionDiffusionEM,\\n\",\n-    \"    AdvectionDiffusionM1,\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    DiffusionUniformKh,\\n\",\n-    \"    Field,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParcelsRandom,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    StatusCode,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -219,7 +207,9 @@\n     \"    \\\"Kh_meridional\\\": Kh_meridional,\\n\",\n     \"}\\n\",\n     \"dims = {\\\"lon\\\": 1, \\\"lat\\\": np.linspace(-0.01, 1.01, ydim, dtype=np.float32)}\\n\",\n-    \"fieldset = FieldSet.from_data(data, dims, mesh=\\\"flat\\\", allow_time_extrapolation=True)\\n\",\n+    \"fieldset = parcels.FieldSet.from_data(\\n\",\n+    \"    data, dims, mesh=\\\"flat\\\", allow_time_extrapolation=True\\n\",\n+    \")\\n\",\n     \"fieldset.add_constant(\\\"dres\\\", 0.00005)\"\n    ]\n   },\n@@ -238,9 +228,9 @@\n    \"outputs\": [],\n    \"source\": [\n     \"def get_test_particles():\\n\",\n-    \"    return ParticleSet.from_list(\\n\",\n+    \"    return parcels.ParticleSet.from_list(\\n\",\n     \"        fieldset,\\n\",\n-    \"        pclass=JITParticle,\\n\",\n+    \"        pclass=parcels.JITParticle,\\n\",\n     \"        lon=np.zeros(100),\\n\",\n     \"        lat=np.ones(100) * 0.75,\\n\",\n     \"        time=np.zeros(100),\\n\",\n@@ -278,10 +268,10 @@\n     \"    name=\\\"M1_out.zarr\\\", chunks=(len(testParticles), 50), outputdt=timedelta(seconds=dt)\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n+    \"parcels.ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n     \"\\n\",\n     \"testParticles.execute(\\n\",\n-    \"    AdvectionDiffusionM1,\\n\",\n+    \"    parcels.AdvectionDiffusionM1,\\n\",\n     \"    runtime=timedelta(seconds=0.3),\\n\",\n     \"    dt=timedelta(seconds=dt),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -373,9 +363,9 @@\n     \"output_file = testParticles.ParticleFile(\\n\",\n     \"    name=\\\"EM_out.zarr\\\", chunks=(len(testParticles), 50), outputdt=timedelta(seconds=dt)\\n\",\n     \")\\n\",\n-    \"ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n+    \"parcels.ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n     \"testParticles.execute(\\n\",\n-    \"    AdvectionDiffusionEM,\\n\",\n+    \"    parcels.AdvectionDiffusionEM,\\n\",\n     \"    runtime=timedelta(seconds=0.3),\\n\",\n     \"    dt=timedelta(seconds=dt),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -482,10 +472,10 @@\n     \"    A = A / sq_deg_to_sq_m\\n\",\n     \"    Kh = fieldset.Cs * A * math.sqrt(dudx**2 + 0.5 * (dudy + dvdx) ** 2 + dvdy**2)\\n\",\n     \"\\n\",\n-    \"    dlat = ParcelsRandom.normalvariate(0.0, 1.0) * math.sqrt(\\n\",\n+    \"    dlat = parcels.ParcelsRandom.normalvariate(0.0, 1.0) * math.sqrt(\\n\",\n     \"        2 * math.fabs(particle.dt) * Kh\\n\",\n     \"    )\\n\",\n-    \"    dlon = ParcelsRandom.normalvariate(0.0, 1.0) * math.sqrt(\\n\",\n+    \"    dlon = parcels.ParcelsRandom.normalvariate(0.0, 1.0) * math.sqrt(\\n\",\n     \"        2 * math.fabs(particle.dt) * Kh\\n\",\n     \"    )\\n\",\n     \"\\n\",\n@@ -507,7 +497,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n     \"    \\\"V\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n@@ -518,7 +508,7 @@\n     \"}\\n\",\n     \"dimensions = {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\"\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -538,7 +528,9 @@\n     \"x = fieldset.U.grid.lon\\n\",\n     \"y = fieldset.U.grid.lat\\n\",\n     \"\\n\",\n-    \"cell_areas = Field(name=\\\"cell_areas\\\", data=fieldset.U.cell_areas(), lon=x, lat=y)\\n\",\n+    \"cell_areas = parcels.Field(\\n\",\n+    \"    name=\\\"cell_areas\\\", data=fieldset.U.cell_areas(), lon=x, lat=y\\n\",\n+    \")\\n\",\n     \"fieldset.add_field(cell_areas)\\n\",\n     \"\\n\",\n     \"fieldset.add_constant(\\\"Cs\\\", 0.1)\"\n@@ -561,7 +553,9 @@\n     \"time = np.arange(0, 43200, 3600)\\n\",\n     \"lon = [29] * len(time)\\n\",\n     \"lat = [-33] * len(time)\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\"\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\"\n    ]\n   },\n   {\n@@ -588,7 +582,7 @@\n    ],\n    \"source\": [\n     \"def DeleteParticle(particle, fieldset, time):\\n\",\n-    \"    if particle.state == StatusCode.ErrorOutOfBounds:\\n\",\n+    \"    if particle.state == parcels.StatusCode.ErrorOutOfBounds:\\n\",\n     \"        particle.delete()\\n\",\n     \"\\n\",\n     \"\\n\",\n@@ -596,10 +590,10 @@\n     \"    name=\\\"Global_smagdiff.zarr\\\", outputdt=timedelta(hours=3), chunks=(1, 57)\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n+    \"parcels.ParcelsRandom.seed(1636)  # Random seed for reproducibility\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, smagdiff, DeleteParticle],\\n\",\n+    \"    [parcels.AdvectionRK4, smagdiff, DeleteParticle],\\n\",\n     \"    runtime=timedelta(days=7),\\n\",\n     \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\ndiff --git a/docs/examples/tutorial_interaction.ipynb b/docs/examples/tutorial_interaction.ipynb\nindex 2d6d09f81..8ceee2d6a 100644\n--- a/docs/examples/tutorial_interaction.ipynb\n+++ b/docs/examples/tutorial_interaction.ipynb\n@@ -61,16 +61,7 @@\n     \"from IPython.display import HTML\\n\",\n     \"from matplotlib.animation import FuncAnimation\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    DiffusionUniformKh,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    MergeWithNearestNeighbor,\\n\",\n-    \"    NearestNeighborWithinRange,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    ScipyInteractionParticle,\\n\",\n-    \"    ScipyParticle,\\n\",\n-    \"    Variable,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -145,14 +136,16 @@\n     \"X, Y = np.meshgrid(np.linspace(-1, 1, npart), np.linspace(-1, 1, npart))\\n\",\n     \"\\n\",\n     \"# Define a fieldset without flow\\n\",\n-    \"fieldset = FieldSet.from_data({\\\"U\\\": 0, \\\"V\\\": 0}, {\\\"lon\\\": 0, \\\"lat\\\": 0}, mesh=\\\"flat\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_data(\\n\",\n+    \"    {\\\"U\\\": 0, \\\"V\\\": 0}, {\\\"lon\\\": 0, \\\"lat\\\": 0}, mesh=\\\"flat\\\"\\n\",\n+    \")\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_zonal\\\", 0.0005, mesh=\\\"flat\\\")\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_meridional\\\", 0.0005, mesh=\\\"flat\\\")\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"# Create custom particle class with extra variable that indicates\\n\",\n     \"# whether the interaction kernel should be executed on this particle.\\n\",\n-    \"InteractingParticle = ScipyParticle.add_variable(\\n\",\n+    \"InteractingParticle = parcels.ScipyParticle.add_variable(\\n\",\n     \"    \\\"attractor\\\", dtype=np.bool_, to_write=\\\"once\\\"\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -161,7 +154,7 @@\n     \"    True if i in [int(npart * npart / 2 - 3), int(npart * npart / 2 + 3)] else False\\n\",\n     \"    for i in range(npart * npart)\\n\",\n     \"]\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset,\\n\",\n     \"    pclass=InteractingParticle,\\n\",\n     \"    lon=X,\\n\",\n@@ -173,7 +166,7 @@\n     \"output_file = pset.ParticleFile(name=\\\"InteractingParticles.zarr\\\", outputdt=1)\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    pyfunc=DiffusionUniformKh,\\n\",\n+    \"    pyfunc=parcels.DiffusionUniformKh,\\n\",\n     \"    pyfunc_inter=Pull,  # note the pyfunc_inter here\\n\",\n     \"    runtime=60,\\n\",\n     \"    dt=1,\\n\",\n@@ -30832,21 +30825,23 @@\n     \"Y = np.random.uniform(-1, 1, size=npart)\\n\",\n     \"\\n\",\n     \"# Define a fieldset without flow\\n\",\n-    \"fieldset = FieldSet.from_data({\\\"U\\\": 0, \\\"V\\\": 0}, {\\\"lon\\\": 0, \\\"lat\\\": 0}, mesh=\\\"flat\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_data(\\n\",\n+    \"    {\\\"U\\\": 0, \\\"V\\\": 0}, {\\\"lon\\\": 0, \\\"lat\\\": 0}, mesh=\\\"flat\\\"\\n\",\n+    \")\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_zonal\\\", 0.0005, mesh=\\\"flat\\\")\\n\",\n     \"fieldset.add_constant_field(\\\"Kh_meridional\\\", 0.0005, mesh=\\\"flat\\\")\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"# Create custom InteractionParticle class\\n\",\n     \"# with extra variables nearest_neighbor and mass\\n\",\n-    \"MergeParticle = ScipyInteractionParticle.add_variables(\\n\",\n+    \"MergeParticle = parcels.ScipyInteractionParticle.add_variables(\\n\",\n     \"    [\\n\",\n-    \"        Variable(\\\"nearest_neighbor\\\", dtype=np.int64, to_write=False),\\n\",\n-    \"        Variable(\\\"mass\\\", initial=1, dtype=np.float32),\\n\",\n+    \"        parcels.Variable(\\\"nearest_neighbor\\\", dtype=np.int64, to_write=False),\\n\",\n+    \"        parcels.Variable(\\\"mass\\\", initial=1, dtype=np.float32),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset,\\n\",\n     \"    pclass=MergeParticle,\\n\",\n     \"    lon=X,\\n\",\n@@ -30857,9 +30852,9 @@\n     \"output_file = pset.ParticleFile(name=\\\"MergingParticles.zarr\\\", outputdt=1)\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    pyfunc=DiffusionUniformKh,\\n\",\n-    \"    pyfunc_inter=pset.InteractionKernel(NearestNeighborWithinRange)\\n\",\n-    \"    + MergeWithNearestNeighbor,  # note the pyfunc_inter here\\n\",\n+    \"    pyfunc=parcels.DiffusionUniformKh,\\n\",\n+    \"    pyfunc_inter=pset.InteractionKernel(parcels.NearestNeighborWithinRange)\\n\",\n+    \"    + parcels.MergeWithNearestNeighbor,  # note the pyfunc_inter here\\n\",\n     \"    runtime=60,\\n\",\n     \"    dt=1,\\n\",\n     \"    output_file=output_file,\\n\",\ndiff --git a/docs/examples/tutorial_interpolation.ipynb b/docs/examples/tutorial_interpolation.ipynb\nindex 421bfc830..e41e5b064 100644\n--- a/docs/examples/tutorial_interpolation.ipynb\n+++ b/docs/examples/tutorial_interpolation.ipynb\n@@ -28,7 +28,7 @@\n     \"import numpy as np\\n\",\n     \"from matplotlib import cm\\n\",\n     \"\\n\",\n-    \"from parcels import FieldSet, JITParticle, ParticleSet\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -57,7 +57,7 @@\n     \"    \\\"P\\\": np.random.rand(dims[0], dims[1]) + 0.1,\\n\",\n     \"}\\n\",\n     \"data[\\\"P\\\"][1, 1] = 0.0\\n\",\n-    \"fieldset = FieldSet.from_data(data, dimensions, mesh=\\\"flat\\\")\"\n+    \"fieldset = parcels.FieldSet.from_data(data, dimensions, mesh=\\\"flat\\\")\"\n    ]\n   },\n   {\n@@ -74,7 +74,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"SampleParticle = JITParticle.add_variable(\\\"p\\\", dtype=np.float32)\\n\",\n+    \"SampleParticle = parcels.JITParticle.add_variable(\\\"p\\\", dtype=np.float32)\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"def SampleP(particle, fieldset, time):\\n\",\n@@ -114,7 +114,7 @@\n     \"    )\\n\",\n     \"\\n\",\n     \"    xv, yv = np.meshgrid(np.linspace(0, 1, 8), np.linspace(0, 1, 8))\\n\",\n-    \"    pset[p_interp] = ParticleSet(\\n\",\n+    \"    pset[p_interp] = parcels.ParticleSet(\\n\",\n     \"        fieldset, pclass=SampleParticle, lon=xv.flatten(), lat=yv.flatten()\\n\",\n     \"    )\\n\",\n     \"    pset[p_interp].execute(SampleP, endtime=1, dt=1)\"\ndiff --git a/docs/examples/tutorial_jit_vs_scipy.ipynb b/docs/examples/tutorial_jit_vs_scipy.ipynb\nindex 9ed7774db..5db839c87 100644\n--- a/docs/examples/tutorial_jit_vs_scipy.ipynb\n+++ b/docs/examples/tutorial_jit_vs_scipy.ipynb\n@@ -49,36 +49,30 @@\n     }\n    ],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    ScipyParticle,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \"    timer,\\n\",\n-    \")\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n-    \"timer.root = timer.Timer(\\\"root\\\")\\n\",\n+    \"parcels.timer.root = parcels.timer.Timer(\\\"root\\\")\\n\",\n     \"\\n\",\n-    \"timer.fieldset = timer.Timer(\\\"fieldset creation\\\", parent=timer.root)\\n\",\n+    \"parcels.timer.fieldset = parcels.timer.Timer(\\n\",\n+    \"    \\\"fieldset creation\\\", parent=parcels.timer.root\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\", allow_time_extrapolation=True\\n\",\n     \")\\n\",\n-    \"timer.fieldset.stop()\\n\",\n+    \"parcels.timer.fieldset.stop()\\n\",\n     \"\\n\",\n-    \"ptype = {\\\"scipy\\\": ScipyParticle, \\\"jit\\\": JITParticle}\\n\",\n+    \"ptype = {\\\"scipy\\\": parcels.ScipyParticle, \\\"jit\\\": parcels.JITParticle}\\n\",\n     \"ptimer = {\\n\",\n-    \"    \\\"scipy\\\": timer.Timer(\\\"scipy\\\", parent=timer.root, start=False),\\n\",\n-    \"    \\\"jit\\\": timer.Timer(\\\"jit\\\", parent=timer.root, start=False),\\n\",\n+    \"    \\\"scipy\\\": parcels.timer.Timer(\\\"scipy\\\", parent=parcels.timer.root, start=False),\\n\",\n+    \"    \\\"jit\\\": parcels.timer.Timer(\\\"jit\\\", parent=parcels.timer.root, start=False),\\n\",\n     \"}\\n\",\n     \"\\n\",\n     \"for p in [\\\"scipy\\\", \\\"jit\\\"]:\\n\",\n-    \"    pset = ParticleSet.from_line(\\n\",\n+    \"    pset = parcels.ParticleSet.from_line(\\n\",\n     \"        fieldset=fieldset,\\n\",\n     \"        pclass=ptype[p],\\n\",\n     \"        size=100,\\n\",\n@@ -87,11 +81,13 @@\n     \"    )\\n\",\n     \"\\n\",\n     \"    ptimer[p].start()\\n\",\n-    \"    pset.execute(AdvectionRK4, runtime=delta(hours=1), dt=delta(seconds=30))\\n\",\n+    \"    pset.execute(\\n\",\n+    \"        parcels.AdvectionRK4, runtime=timedelta(hours=1), dt=timedelta(seconds=30)\\n\",\n+    \"    )\\n\",\n     \"    ptimer[p].stop()\\n\",\n     \"\\n\",\n-    \"timer.root.stop()\\n\",\n-    \"timer.root.print_tree()\"\n+    \"parcels.timer.root.stop()\\n\",\n+    \"parcels.timer.root.print_tree()\"\n    ]\n   },\n   {\n@@ -183,27 +179,27 @@\n     }\n    ],\n    \"source\": [\n-    \"timer.root = timer.Timer(\\\"root\\\")\\n\",\n+    \"parcels.timer.root = parcels.timer.Timer(\\\"root\\\")\\n\",\n     \"ptimer = {\\n\",\n-    \"    \\\"dllt\\\": timer.Timer(\\\"dllt\\\", parent=timer.root, start=False),\\n\",\n-    \"    \\\"dllt_p\\\": timer.Timer(\\\"dllt_p\\\", parent=timer.root, start=False),\\n\",\n+    \"    \\\"dllt\\\": parcels.timer.Timer(\\\"dllt\\\", parent=parcels.timer.root, start=False),\\n\",\n+    \"    \\\"dllt_p\\\": parcels.timer.Timer(\\\"dllt_p\\\", parent=parcels.timer.root, start=False),\\n\",\n     \"}\\n\",\n     \"\\n\",\n     \"for k in [\\\"dllt\\\", \\\"dllt_p\\\"]:\\n\",\n-    \"    pset = ParticleSet.from_line(\\n\",\n+    \"    pset = parcels.ParticleSet.from_line(\\n\",\n     \"        fieldset=fieldset,\\n\",\n-    \"        pclass=ScipyParticle,\\n\",\n+    \"        pclass=parcels.ScipyParticle,\\n\",\n     \"        size=100,\\n\",\n     \"        start=(3e3, 3e3),\\n\",\n     \"        finish=(3e3, 45e3),\\n\",\n     \"    )\\n\",\n     \"\\n\",\n     \"    ptimer[k].start()\\n\",\n-    \"    pset.execute(kernels[k], runtime=delta(hours=1), dt=delta(seconds=30))\\n\",\n+    \"    pset.execute(kernels[k], runtime=timedelta(hours=1), dt=timedelta(seconds=30))\\n\",\n     \"    ptimer[k].stop()\\n\",\n     \"\\n\",\n-    \"timer.root.stop()\\n\",\n-    \"timer.root.print_tree()\"\n+    \"parcels.timer.root.stop()\\n\",\n+    \"parcels.timer.root.print_tree()\"\n    ]\n   },\n   {\n@@ -222,7 +218,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoCurvilinear_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"NemoCurvilinear_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": {\\n\",\n     \"        \\\"lon\\\": f\\\"{example_dataset_folder}/mesh_mask.nc4\\\",\\n\",\n@@ -237,7 +233,7 @@\n     \"}\\n\",\n     \"variables = {\\\"U\\\": \\\"U\\\", \\\"V\\\": \\\"V\\\"}\\n\",\n     \"dimensions = {\\\"lon\\\": \\\"glamf\\\", \\\"lat\\\": \\\"gphif\\\", \\\"time\\\": \\\"time_counter\\\"}\\n\",\n-    \"fieldset = FieldSet.from_nemo(\\n\",\n+    \"fieldset = parcels.FieldSet.from_nemo(\\n\",\n     \"    filenames, variables, dimensions, allow_time_extrapolation=True\\n\",\n     \")\"\n    ]\n@@ -260,27 +256,27 @@\n     }\n    ],\n    \"source\": [\n-    \"timer.root = timer.Timer(\\\"root\\\")\\n\",\n+    \"parcels.timer.root = parcels.timer.Timer(\\\"root\\\")\\n\",\n     \"ptimer = {\\n\",\n-    \"    \\\"dllt\\\": timer.Timer(\\\"dllt\\\", parent=timer.root, start=False),\\n\",\n-    \"    \\\"dllt_p\\\": timer.Timer(\\\"dllt_p\\\", parent=timer.root, start=False),\\n\",\n+    \"    \\\"dllt\\\": parcels.timer.Timer(\\\"dllt\\\", parent=parcels.timer.root, start=False),\\n\",\n+    \"    \\\"dllt_p\\\": parcels.timer.Timer(\\\"dllt_p\\\", parent=parcels.timer.root, start=False),\\n\",\n     \"}\\n\",\n     \"\\n\",\n     \"for k in [\\\"dllt\\\", \\\"dllt_p\\\"]:\\n\",\n-    \"    pset = ParticleSet.from_line(\\n\",\n+    \"    pset = parcels.ParticleSet.from_line(\\n\",\n     \"        fieldset=fieldset,\\n\",\n-    \"        pclass=ScipyParticle,\\n\",\n+    \"        pclass=parcels.ScipyParticle,\\n\",\n     \"        size=10,\\n\",\n     \"        start=(45, 40),\\n\",\n     \"        finish=(60, 40),\\n\",\n     \"    )\\n\",\n     \"\\n\",\n     \"    ptimer[k].start()\\n\",\n-    \"    pset.execute(kernels[k], runtime=delta(days=10), dt=delta(hours=6))\\n\",\n+    \"    pset.execute(kernels[k], runtime=timedelta(days=10), dt=timedelta(hours=6))\\n\",\n     \"    ptimer[k].stop()\\n\",\n     \"\\n\",\n-    \"timer.root.stop()\\n\",\n-    \"timer.root.print_tree()\"\n+    \"parcels.timer.root.stop()\\n\",\n+    \"parcels.timer.root.print_tree()\"\n    ]\n   },\n   {\ndiff --git a/docs/examples/tutorial_kernelloop.ipynb b/docs/examples/tutorial_kernelloop.ipynb\nindex 0824d28dc..b2d3ea8fe 100644\n--- a/docs/examples/tutorial_kernelloop.ipynb\n+++ b/docs/examples/tutorial_kernelloop.ipynb\n@@ -105,17 +105,10 @@\n     \"import numpy as np\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    Field,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n     \"# Load the GlobCurrent data in the Agulhas region from the example_data\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n     \"    \\\"V\\\": f\\\"{example_dataset_folder}/20*.nc\\\",\\n\",\n@@ -125,7 +118,7 @@\n     \"    \\\"V\\\": \\\"northward_eulerian_current_velocity\\\",\\n\",\n     \"}\\n\",\n     \"dimensions = {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"}\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n     \"# uppermost layer in the hydrodynamic data\\n\",\n     \"fieldset.mindepth = fieldset.U.depth[0]\"\n    ]\n@@ -138,7 +131,7 @@\n    \"source\": [\n     \"# Create an idealised wind field and add it to the fieldset\\n\",\n     \"xdim, ydim = (len(fieldset.U.lon), len(fieldset.U.lat))\\n\",\n-    \"UWind = Field(\\n\",\n+    \"UWind = parcels.Field(\\n\",\n     \"    \\\"UWind\\\",\\n\",\n     \"    np.ones((ydim, xdim), dtype=np.float32) * np.sin(fieldset.U.lat)[:, None],\\n\",\n     \"    lon=fieldset.U.lon,\\n\",\n@@ -146,10 +139,10 @@\n     \"    mesh=\\\"spherical\\\",\\n\",\n     \"    fieldtype=\\\"U\\\",\\n\",\n     \")\\n\",\n-    \"VWind = Field(\\n\",\n+    \"VWind = parcels.Field(\\n\",\n     \"    \\\"VWind\\\", np.zeros((ydim, xdim), dtype=np.float32), grid=UWind.grid, fieldtype=\\\"V\\\"\\n\",\n     \")\\n\",\n-    \"fieldset_wind = FieldSet(UWind, VWind)\\n\",\n+    \"fieldset_wind = parcels.FieldSet(UWind, VWind)\\n\",\n     \"\\n\",\n     \"fieldset.add_field(fieldset_wind.U, name=\\\"UWind\\\")\\n\",\n     \"fieldset.add_field(fieldset_wind.V, name=\\\"VWind\\\")\"\n@@ -202,12 +195,12 @@\n     \"lons = 26.0 * np.ones(10)\\n\",\n     \"lats = np.linspace(-37.5, -34.5, 10)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset, pclass=JITParticle, lon=lons, lat=lats)\\n\",\n+    \"pset = parcels.ParticleSet(fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats)\\n\",\n     \"output_file = pset.ParticleFile(\\n\",\n     \"    name=\\\"advection_then_wind.zarr\\\", outputdt=timedelta(hours=6)\\n\",\n     \")\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, wind_kernel],\\n\",\n+    \"    [parcels.AdvectionRK4, wind_kernel],\\n\",\n     \"    runtime=timedelta(days=5),\\n\",\n     \"    dt=timedelta(hours=1),\\n\",\n     \"    output_file=output_file,\\n\",\n@@ -236,12 +229,14 @@\n     }\n    ],\n    \"source\": [\n-    \"pset_reverse = ParticleSet(fieldset, pclass=JITParticle, lon=lons, lat=lats)\\n\",\n+    \"pset_reverse = parcels.ParticleSet(\\n\",\n+    \"    fieldset, pclass=parcels.JITParticle, lon=lons, lat=lats\\n\",\n+    \")\\n\",\n     \"output_file_reverse = pset_reverse.ParticleFile(\\n\",\n     \"    name=\\\"wind_then_advection.zarr\\\", outputdt=timedelta(hours=6)\\n\",\n     \")\\n\",\n     \"pset_reverse.execute(\\n\",\n-    \"    [wind_kernel, AdvectionRK4],\\n\",\n+    \"    [wind_kernel, parcels.AdvectionRK4],\\n\",\n     \"    runtime=timedelta(days=5),\\n\",\n     \"    dt=timedelta(hours=1),\\n\",\n     \"    output_file=output_file_reverse,\\n\",\ndiff --git a/docs/examples/tutorial_nemo_3D.ipynb b/docs/examples/tutorial_nemo_3D.ipynb\nindex ac31b93f4..240435b6d 100644\n--- a/docs/examples/tutorial_nemo_3D.ipynb\n+++ b/docs/examples/tutorial_nemo_3D.ipynb\n@@ -52,26 +52,21 @@\n     }\n    ],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"from glob import glob\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4_3D,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    XarrayDecodedFilter,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \"    logger,\\n\",\n-    \")\\n\",\n+    \"import parcels\\n\",\n+    \"from parcels import logger\\n\",\n     \"\\n\",\n     \"# Add a filter for the xarray decoding warning\\n\",\n-    \"logger.addFilter(XarrayDecodedFilter())\\n\",\n+    \"logger.addFilter(parcels.XarrayDecodedFilter())\\n\",\n     \"\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoNorthSeaORCA025-N006_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\n\",\n+    \"    \\\"NemoNorthSeaORCA025-N006_data\\\"\\n\",\n+    \")\\n\",\n     \"ufiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*U.nc\\\"))\\n\",\n     \"vfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*V.nc\\\"))\\n\",\n     \"wfiles = sorted(glob(f\\\"{example_dataset_folder}/ORCA*W.nc\\\"))\\n\",\n@@ -98,19 +93,19 @@\n     \"    \\\"W\\\": c_grid_dimensions,\\n\",\n     \"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet.from_line(\\n\",\n+    \"pset = parcels.ParticleSet.from_line(\\n\",\n     \"    fieldset=fieldset,\\n\",\n-    \"    pclass=JITParticle,\\n\",\n+    \"    pclass=parcels.JITParticle,\\n\",\n     \"    size=10,\\n\",\n     \"    start=(1.9, 52.5),\\n\",\n     \"    finish=(3.4, 51.6),\\n\",\n     \"    depth=1,\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"kernels = pset.Kernel(AdvectionRK4_3D)\\n\",\n-    \"pset.execute(kernels, runtime=delta(days=4), dt=delta(hours=6))\"\n+    \"kernels = pset.Kernel(parcels.AdvectionRK4_3D)\\n\",\n+    \"pset.execute(kernels, runtime=timedelta(days=4), dt=timedelta(hours=6))\"\n    ]\n   },\n   {\n@@ -183,7 +178,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"fieldset = FieldSet.from_nemo(filenames, variables, dimensions)\"\n+    \"fieldset = parcels.FieldSet.from_nemo(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -239,7 +234,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.6\"\n+   \"version\": \"3.12.4\"\n   }\n  },\n  \"nbformat\": 4,\ndiff --git a/docs/examples/tutorial_nemo_curvilinear.ipynb b/docs/examples/tutorial_nemo_curvilinear.ipynb\nindex f509ef114..065c96dd4 100644\n--- a/docs/examples/tutorial_nemo_curvilinear.ipynb\n+++ b/docs/examples/tutorial_nemo_curvilinear.ipynb\n@@ -24,20 +24,13 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import numpy as np\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleFile,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -64,7 +57,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"NemoCurvilinear_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"NemoCurvilinear_data\\\")\\n\",\n     \"filenames = {\\n\",\n     \"    \\\"U\\\": {\\n\",\n     \"        \\\"lon\\\": f\\\"{example_dataset_folder}/mesh_mask.nc4\\\",\\n\",\n@@ -81,7 +74,7 @@\n     \"\\n\",\n     \"dimensions = {\\\"lon\\\": \\\"glamf\\\", \\\"lat\\\": \\\"gphif\\\", \\\"time\\\": \\\"time_counter\\\"}\\n\",\n     \"\\n\",\n-    \"fieldset = FieldSet.from_nemo(\\n\",\n+    \"fieldset = parcels.FieldSet.from_nemo(\\n\",\n     \"    filenames, variables, dimensions, allow_time_extrapolation=True\\n\",\n     \")\"\n    ]\n@@ -184,10 +177,15 @@\n     \"lonp = -180 * np.ones(npart)\\n\",\n     \"latp = [i for i in np.linspace(-70, 85, npart)]\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet.from_list(fieldset, JITParticle, lon=lonp, lat=latp)\\n\",\n-    \"pfile = ParticleFile(\\\"nemo_particles\\\", pset, outputdt=delta(days=1))\\n\",\n+    \"pset = parcels.ParticleSet.from_list(fieldset, parcels.JITParticle, lon=lonp, lat=latp)\\n\",\n+    \"pfile = parcels.ParticleFile(\\\"nemo_particles\\\", pset, outputdt=timedelta(days=1))\\n\",\n     \"\\n\",\n-    \"pset.execute(AdvectionRK4, runtime=delta(days=30), dt=delta(hours=6), output_file=pfile)\"\n+    \"pset.execute(\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(days=30),\\n\",\n+    \"    dt=timedelta(hours=6),\\n\",\n+    \"    output_file=pfile,\\n\",\n+    \")\"\n    ]\n   },\n   {\n@@ -237,7 +235,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"pset = ParticleSet.from_list(fieldset, JITParticle, lon=lonp, lat=latp)\\n\",\n+    \"pset = parcels.ParticleSet.from_list(fieldset, parcels.JITParticle, lon=lonp, lat=latp)\\n\",\n     \"pset.populate_indices()\"\n    ]\n   }\n@@ -258,7 +256,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.5\"\n+   \"version\": \"3.12.4\"\n   }\n  },\n  \"nbformat\": 4,\ndiff --git a/docs/examples/tutorial_output.ipynb b/docs/examples/tutorial_output.ipynb\nindex 4f1642201..f98e6972c 100644\n--- a/docs/examples/tutorial_output.ipynb\n+++ b/docs/examples/tutorial_output.ipynb\n@@ -30,17 +30,11 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import numpy as np\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -58,8 +52,8 @@\n     }\n    ],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\", allow_time_extrapolation=True\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -68,14 +62,19 @@\n     \"lat = np.linspace(3e3, 45e3, npart, dtype=np.float32)\\n\",\n     \"\\n\",\n     \"# release every particle two hours later\\n\",\n-    \"time = np.arange(npart) * delta(hours=2).total_seconds()\\n\",\n+    \"time = np.arange(npart) * timedelta(hours=2).total_seconds()\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"Output.zarr\\\", outputdt=delta(hours=2))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"Output.zarr\\\", outputdt=timedelta(hours=2))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    AdvectionRK4, runtime=delta(hours=24), dt=delta(minutes=5), output_file=output_file\\n\",\n+    \"    parcels.AdvectionRK4,\\n\",\n+    \"    runtime=timedelta(hours=24),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n+    \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n   },\n@@ -330,8 +329,8 @@\n     \"\\n\",\n     \"timerange = np.arange(\\n\",\n     \"    np.nanmin(data_xarray[\\\"time\\\"].values),\\n\",\n-    \"    np.nanmax(data_xarray[\\\"time\\\"].values) + np.timedelta64(delta(hours=2)),\\n\",\n-    \"    delta(hours=2),\\n\",\n+    \"    np.nanmax(data_xarray[\\\"time\\\"].values) + np.timedelta64(timedelta(hours=2)),\\n\",\n+    \"    timedelta(hours=2),\\n\",\n     \")  # timerange in nanoseconds\\n\",\n     \"\\n\",\n     \"for time in timerange:\\n\",\n@@ -460,7 +459,7 @@\n     \"from IPython.display import HTML\\n\",\n     \"from matplotlib.animation import FuncAnimation\\n\",\n     \"\\n\",\n-    \"outputdt = delta(hours=2)\\n\",\n+    \"outputdt = timedelta(hours=2)\\n\",\n     \"\\n\",\n     \"# timerange in nanoseconds\\n\",\n     \"timerange = np.arange(\\n\",\ndiff --git a/docs/examples/tutorial_parcels_structure.ipynb b/docs/examples/tutorial_parcels_structure.ipynb\nindex 898cdf85f..29d9f2c8e 100644\n--- a/docs/examples/tutorial_parcels_structure.ipynb\n+++ b/docs/examples/tutorial_parcels_structure.ipynb\n@@ -72,9 +72,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from parcels import download_example_dataset\\n\",\n+    \"import parcels\\n\",\n     \"\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"fname = f\\\"{example_dataset_folder}/*.nc\\\"\\n\",\n     \"filenames = {\\\"U\\\": fname, \\\"V\\\": fname}\"\n    ]\n@@ -134,9 +134,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from parcels import FieldSet\\n\",\n-    \"\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\"\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\"\n    ]\n   },\n   {\n@@ -200,12 +198,10 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from parcels import JITParticle, ParticleSet, Variable\\n\",\n-    \"\\n\",\n     \"# Define a new particleclass with Variable 'age' with initial value 0.\\n\",\n-    \"AgeParticle = JITParticle.add_variable(Variable(\\\"age\\\", initial=0))\\n\",\n+    \"AgeParticle = parcels.JITParticle.add_variable(parcels.Variable(\\\"age\\\", initial=0))\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset,  # the fields that the particleset uses\\n\",\n     \"    pclass=AgeParticle,  # define the type of particle\\n\",\n     \"    lon=29,  # release longitude\\n\",\n@@ -251,9 +247,6 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"# load basic advection kernels\\n\",\n-    \"from parcels import AdvectionRK4\\n\",\n-    \"\\n\",\n     \"# Create a custom kernel which displaces each particle southward\\n\",\n     \"\\n\",\n     \"\\n\",\n@@ -271,7 +264,7 @@\n     \"\\n\",\n     \"\\n\",\n     \"# define all kernels to be executed on particles using an (ordered) list\\n\",\n-    \"kernels = [Age, NorthVel, AdvectionRK4]\"\n+    \"kernels = [Age, NorthVel, parcels.AdvectionRK4]\"\n    ]\n   },\n   {\ndiff --git a/docs/examples/tutorial_particle_field_interaction.ipynb b/docs/examples/tutorial_particle_field_interaction.ipynb\nindex 143837ba9..c8c2f7196 100644\n--- a/docs/examples/tutorial_particle_field_interaction.ipynb\n+++ b/docs/examples/tutorial_particle_field_interaction.ipynb\n@@ -43,21 +43,14 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import netCDF4\\n\",\n     \"import numpy as np\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    Field,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    ScipyParticle,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -79,7 +72,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"# Velocity fields\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"GlobCurrent_example_data\\\")\\n\",\n     \"fname = f\\\"{example_dataset_folder}/*.nc\\\"\\n\",\n     \"filenames = {\\\"U\\\": fname, \\\"V\\\": fname}\\n\",\n     \"variables = {\\n\",\n@@ -90,7 +83,7 @@\n     \"    \\\"U\\\": {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"},\\n\",\n     \"    \\\"V\\\": {\\\"lat\\\": \\\"lat\\\", \\\"lon\\\": \\\"lon\\\", \\\"time\\\": \\\"time\\\"},\\n\",\n     \"}\\n\",\n-    \"fieldset = FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(filenames, variables, dimensions)\\n\",\n     \"\\n\",\n     \"# In order to assign the same grid to the tracer field,\\n\",\n     \"# it is convenient to load a single velocity file\\n\",\n@@ -99,7 +92,7 @@\n     \"\\n\",\n     \"# a field with the same variables and dimensions\\n\",\n     \"# as the other velocity fields\\n\",\n-    \"field_for_size = FieldSet.from_netcdf(filenames1, variables, dimensions)\\n\",\n+    \"field_for_size = parcels.FieldSet.from_netcdf(filenames1, variables, dimensions)\\n\",\n     \"\\n\",\n     \"# Adding the tracer field to the FieldSet\\n\",\n     \"# with same dimensions as the velocity fields\\n\",\n@@ -108,7 +101,7 @@\n     \"\\n\",\n     \"# the new Field will be called C, for tracer Concentration.\\n\",\n     \"# For mass conservation, interp_method='nearest'\\n\",\n-    \"fieldC = Field(\\\"C\\\", dataC, grid=field_for_size.U.grid, interp_method=\\\"nearest\\\")\\n\",\n+    \"fieldC = parcels.Field(\\\"C\\\", dataC, grid=field_for_size.U.grid, interp_method=\\\"nearest\\\")\\n\",\n     \"\\n\",\n     \"# aad C field to the velocity FieldSet\\n\",\n     \"fieldset.add_field(fieldC)\\n\",\n@@ -159,7 +152,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"VectorParticle = ScipyParticle.add_variable(\\\"c\\\", dtype=np.float32, initial=100.0)\\n\",\n+    \"VectorParticle = parcels.ScipyParticle.add_variable(\\n\",\n+    \"    \\\"c\\\", dtype=np.float32, initial=100.0\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"def Interaction(particle, fieldset, time):\\n\",\n@@ -196,7 +191,9 @@\n     \"\\n\",\n     \"\\n\",\n     \"# for simplicity, we'll track a single particle here\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=VectorParticle, lon=[24.5], lat=[-34.8])\"\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=VectorParticle, lon=[24.5], lat=[-34.8]\\n\",\n+    \")\"\n    ]\n   },\n   {\n@@ -247,16 +244,16 @@\n     }\n    ],\n    \"source\": [\n-    \"output_file = pset.ParticleFile(name=r\\\"interaction.zarr\\\", outputdt=delta(days=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=r\\\"interaction.zarr\\\", outputdt=timedelta(days=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n     \"    # the particle will FIRST be transported by currents\\n\",\n     \"    # and THEN interact with the field\\n\",\n-    \"    AdvectionRK4 + pset.Kernel(Interaction),\\n\",\n-    \"    dt=delta(days=1),\\n\",\n+    \"    parcels.AdvectionRK4 + pset.Kernel(Interaction),\\n\",\n+    \"    dt=timedelta(days=1),\\n\",\n     \"    # we are going to track the particle and save\\n\",\n     \"    # its trajectory and tracer concentration for 24 days\\n\",\n-    \"    runtime=delta(days=25),\\n\",\n+    \"    runtime=timedelta(days=25),\\n\",\n     \"    output_file=output_file,\\n\",\n     \")\\n\",\n     \"\\n\",\ndiff --git a/docs/examples/tutorial_periodic_boundaries.ipynb b/docs/examples/tutorial_periodic_boundaries.ipynb\nindex d8db300b0..8f25e27f4 100644\n--- a/docs/examples/tutorial_periodic_boundaries.ipynb\n+++ b/docs/examples/tutorial_periodic_boundaries.ipynb\n@@ -63,18 +63,12 @@\n    \"outputs\": [],\n    \"source\": [\n     \"import math\\n\",\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -91,8 +85,8 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\", allow_time_extrapolation=True\\n\",\n     \")\"\n    ]\n@@ -165,16 +159,20 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet.from_line(\\n\",\n-    \"    fieldset, pclass=JITParticle, size=10, start=(20e3, 3e3), finish=(20e3, 45e3)\\n\",\n+    \"pset = parcels.ParticleSet.from_line(\\n\",\n+    \"    fieldset,\\n\",\n+    \"    pclass=parcels.JITParticle,\\n\",\n+    \"    size=10,\\n\",\n+    \"    start=(20e3, 3e3),\\n\",\n+    \"    finish=(20e3, 45e3),\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"PeriodicParticle\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"PeriodicParticle\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, periodicBC],\\n\",\n-    \"    runtime=delta(hours=24),\\n\",\n-    \"    dt=delta(minutes=5),\\n\",\n+    \"    [parcels.AdvectionRK4, periodicBC],\\n\",\n+    \"    runtime=timedelta(hours=24),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n     \")\"\n    ]\ndiff --git a/docs/examples/tutorial_sampling.ipynb b/docs/examples/tutorial_sampling.ipynb\nindex 7a4d88595..e568d969a 100644\n--- a/docs/examples/tutorial_sampling.ipynb\n+++ b/docs/examples/tutorial_sampling.ipynb\n@@ -29,7 +29,7 @@\n    \"source\": [\n     \"## Basic sampling\\n\",\n     \"\\n\",\n-    \"We import the `Variable` class as well as the standard modules needed to set up a simulation.\\n\"\n+    \"We import both the packages that we need to set up the simulation, as well as the parcels package.\\n\"\n    ]\n   },\n   {\n@@ -39,7 +39,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"# Modules needed for the Parcels simulation\\n\",\n-    \"from datetime import timedelta as delta\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib as mpl\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n@@ -48,14 +48,7 @@\n     \"# To open and look at the temperature data\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    Variable,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -84,8 +77,8 @@\n    ],\n    \"source\": [\n     \"# Velocity and temperature fields\\n\",\n-    \"example_dataset_folder = download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n-    \"fieldset = FieldSet.from_parcels(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"Peninsula_data\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_parcels(\\n\",\n     \"    f\\\"{example_dataset_folder}/peninsula\\\",\\n\",\n     \"    extra_fields={\\\"T\\\": \\\"T\\\"},\\n\",\n     \"    allow_time_extrapolation=True,\\n\",\n@@ -96,7 +89,7 @@\n     \"lon = 3e3 * np.ones(npart)\\n\",\n     \"lat = np.linspace(3e3, 45e3, npart, dtype=np.float32)\\n\",\n     \"time = (\\n\",\n-    \"    np.arange(0, npart) * delta(hours=2).total_seconds()\\n\",\n+    \"    np.arange(0, npart) * timedelta(hours=2).total_seconds()\\n\",\n     \")  # release each particle two hours later\\n\",\n     \"\\n\",\n     \"# Plot temperature field and initial particle locations\\n\",\n@@ -107,7 +100,7 @@\n     \"    T_data.x.values, T_data.y.values, T_data.T.values[0, 0], cmap=plt.cm.inferno\\n\",\n     \")\\n\",\n     \"ax.scatter(lon, lat, c=\\\"w\\\")\\n\",\n-    \"plt.colorbar(T_contour, label=\\\"T [$^{\\\\circ} C$]\\\")\\n\",\n+    \"plt.colorbar(T_contour, label=r\\\"T [$^{\\\\circ} C$]\\\")\\n\",\n     \"plt.show()\"\n    ]\n   },\n@@ -125,9 +118,9 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"SampleParticle = JITParticle.add_variable(\\\"temperature\\\")\\n\",\n+    \"SampleParticle = parcels.JITParticle.add_variable(\\\"temperature\\\")\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset, pclass=SampleParticle, lon=lon, lat=lat, time=time\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -159,16 +152,16 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset, pclass=SampleParticle, lon=lon, lat=lat, time=time\\n\",\n     \")\\n\",\n     \"\\n\",\n-    \"output_file = pset.ParticleFile(name=\\\"SampleTemp.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"SampleTemp.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, SampleT],\\n\",\n-    \"    runtime=delta(hours=30),\\n\",\n-    \"    dt=delta(minutes=5),\\n\",\n+    \"    [parcels.AdvectionRK4, SampleT],\\n\",\n+    \"    runtime=timedelta(hours=30),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n@@ -216,7 +209,7 @@\n     \"    edgecolor=\\\"k\\\",\\n\",\n     \"    zorder=2,\\n\",\n     \")\\n\",\n-    \"plt.colorbar(T_scatter, label=\\\"T [$^{\\\\circ} C$]\\\")\\n\",\n+    \"plt.colorbar(T_scatter, label=r\\\"T [$^{\\\\circ} C$]\\\")\\n\",\n     \"plt.show()\"\n    ]\n   },\n@@ -255,7 +248,9 @@\n     \"    u = fieldset.U[particle]\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"pset.execute(SampleVel_wrong)\"\n    ]\n@@ -286,7 +281,9 @@\n     \"    u, v = fieldset.UV[particle]\\n\",\n     \"\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=lon, lat=lat, time=time)\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n+    \"    fieldset=fieldset, pclass=parcels.JITParticle, lon=lon, lat=lat, time=time\\n\",\n+    \")\\n\",\n     \"\\n\",\n     \"pset.execute(SampleVel_correct)\"\n    ]\n@@ -305,10 +302,10 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"SampleParticle = JITParticle.add_variables(\\n\",\n+    \"SampleParticle = parcels.JITParticle.add_variables(\\n\",\n     \"    [\\n\",\n-    \"        Variable(\\\"U\\\", dtype=np.float32, initial=np.nan),\\n\",\n-    \"        Variable(\\\"V\\\", dtype=np.float32, initial=np.nan),\\n\",\n+    \"        parcels.Variable(\\\"U\\\", dtype=np.float32, initial=np.nan),\\n\",\n+    \"        parcels.Variable(\\\"V\\\", dtype=np.float32, initial=np.nan),\\n\",\n     \"    ]\\n\",\n     \")\\n\",\n     \"\\n\",\n@@ -351,9 +348,11 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"SampleParticleOnce = JITParticle.add_variable(\\\"temperature\\\", initial=0, to_write=\\\"once\\\")\\n\",\n+    \"SampleParticleOnce = parcels.JITParticle.add_variable(\\n\",\n+    \"    \\\"temperature\\\", initial=0, to_write=\\\"once\\\"\\n\",\n+    \")\\n\",\n     \"\\n\",\n-    \"pset = ParticleSet(\\n\",\n+    \"pset = parcels.ParticleSet(\\n\",\n     \"    fieldset=fieldset, pclass=SampleParticleOnce, lon=lon, lat=lat, time=time\\n\",\n     \")\"\n    ]\n@@ -373,12 +372,12 @@\n     }\n    ],\n    \"source\": [\n-    \"output_file = pset.ParticleFile(name=\\\"WriteOnce.zarr\\\", outputdt=delta(hours=1))\\n\",\n+    \"output_file = pset.ParticleFile(name=\\\"WriteOnce.zarr\\\", outputdt=timedelta(hours=1))\\n\",\n     \"\\n\",\n     \"pset.execute(\\n\",\n-    \"    [AdvectionRK4, SampleT],\\n\",\n-    \"    runtime=delta(hours=24),\\n\",\n-    \"    dt=delta(minutes=5),\\n\",\n+    \"    [parcels.AdvectionRK4, SampleT],\\n\",\n+    \"    runtime=timedelta(hours=24),\\n\",\n+    \"    dt=timedelta(minutes=5),\\n\",\n     \"    output_file=output_file,\\n\",\n     \")\"\n    ]\n@@ -426,7 +425,7 @@\n     \"    edgecolor=\\\"k\\\",\\n\",\n     \"    zorder=2,\\n\",\n     \")\\n\",\n-    \"plt.colorbar(T_scatter, label=\\\"Initial T [$^{\\\\circ} C$]\\\")\\n\",\n+    \"plt.colorbar(T_scatter, label=r\\\"Initial T [$^{\\\\circ} C$]\\\")\\n\",\n     \"plt.show()\"\n    ]\n   }\ndiff --git a/docs/examples/tutorial_timestamps.ipynb b/docs/examples/tutorial_timestamps.ipynb\nindex 0375f30bc..3217d77e1 100644\n--- a/docs/examples/tutorial_timestamps.ipynb\n+++ b/docs/examples/tutorial_timestamps.ipynb\n@@ -18,7 +18,7 @@\n     \"\\n\",\n     \"import numpy as np\\n\",\n     \"\\n\",\n-    \"from parcels import Field, download_example_dataset\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -100,8 +100,8 @@\n     }\n    ],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"WOA_data\\\")\\n\",\n-    \"tempfield = Field.from_netcdf(\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"WOA_data\\\")\\n\",\n+    \"tempfield = parcels.Field.from_netcdf(\\n\",\n     \"    glob(f\\\"{example_dataset_folder}/woa18_decav_*_04.nc\\\"),\\n\",\n     \"    \\\"t_an\\\",\\n\",\n     \"    {\\\"lon\\\": \\\"lon\\\", \\\"lat\\\": \\\"lat\\\", \\\"time\\\": \\\"time\\\"},\\n\",\n@@ -141,7 +141,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"tempfield = Field.from_netcdf(\\n\",\n+    \"tempfield = parcels.Field.from_netcdf(\\n\",\n     \"    glob(f\\\"{example_dataset_folder}/woa18_decav_*_04.nc\\\"),\\n\",\n     \"    \\\"t_an\\\",\\n\",\n     \"    {\\\"lon\\\": \\\"lon\\\", \\\"lat\\\": \\\"lat\\\", \\\"time\\\": \\\"time\\\"},\\n\",\ndiff --git a/docs/examples/tutorial_timevaryingdepthdimensions.ipynb b/docs/examples/tutorial_timevaryingdepthdimensions.ipynb\nindex 1ef9a8b4a..8f89b89be 100644\n--- a/docs/examples/tutorial_timevaryingdepthdimensions.ipynb\n+++ b/docs/examples/tutorial_timevaryingdepthdimensions.ipynb\n@@ -22,21 +22,12 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from datetime import timedelta as delta\\n\",\n-    \"from os import path\\n\",\n+    \"from datetime import timedelta\\n\",\n     \"\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n-    \"import numpy as np\\n\",\n     \"import xarray as xr\\n\",\n     \"\\n\",\n-    \"from parcels import (\\n\",\n-    \"    AdvectionRK4,\\n\",\n-    \"    FieldSet,\\n\",\n-    \"    JITParticle,\\n\",\n-    \"    ParticleFile,\\n\",\n-    \"    ParticleSet,\\n\",\n-    \"    download_example_dataset,\\n\",\n-    \")\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -53,7 +44,7 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"example_dataset_folder = download_example_dataset(\\\"SWASH_data\\\")\\n\",\n+    \"example_dataset_folder = parcels.download_example_dataset(\\\"SWASH_data\\\")\\n\",\n     \"filenames = f\\\"{example_dataset_folder}/field_*.nc\\\"\\n\",\n     \"variables = {\\n\",\n     \"    \\\"U\\\": \\\"cross-shore velocity\\\",\\n\",\n@@ -105,7 +96,7 @@\n     }\n    ],\n    \"source\": [\n-    \"fieldset = FieldSet.from_netcdf(\\n\",\n+    \"fieldset = parcels.FieldSet.from_netcdf(\\n\",\n     \"    filenames, variables, dimensions, mesh=\\\"flat\\\", allow_time_extrapolation=True\\n\",\n     \")\\n\",\n     \"fieldset.U.set_depth_from_field(fieldset.depth_u)\\n\",\n@@ -135,11 +126,11 @@\n     }\n    ],\n    \"source\": [\n-    \"pset = ParticleSet(fieldset, JITParticle, lon=9.5, lat=12.5, depth=-0.1)\\n\",\n+    \"pset = parcels.ParticleSet(fieldset, parcels.JITParticle, lon=9.5, lat=12.5, depth=-0.1)\\n\",\n     \"\\n\",\n-    \"pfile = pset.ParticleFile(\\\"SwashParticles\\\", outputdt=delta(seconds=0.05))\\n\",\n+    \"pfile = pset.ParticleFile(\\\"SwashParticles\\\", outputdt=timedelta(seconds=0.05))\\n\",\n     \"\\n\",\n-    \"pset.execute(AdvectionRK4, dt=delta(seconds=0.005), output_file=pfile)\"\n+    \"pset.execute(parcels.AdvectionRK4, dt=timedelta(seconds=0.005), output_file=pfile)\"\n    ]\n   },\n   {\ndiff --git a/docs/examples/tutorial_unitconverters.ipynb b/docs/examples/tutorial_unitconverters.ipynb\nindex d6ed3b253..d39586067 100644\n--- a/docs/examples/tutorial_unitconverters.ipynb\n+++ b/docs/examples/tutorial_unitconverters.ipynb\n@@ -35,7 +35,7 @@\n     \"import matplotlib.pyplot as plt\\n\",\n     \"import numpy as np\\n\",\n     \"\\n\",\n-    \"from parcels import Field, FieldSet\"\n+    \"import parcels\"\n    ]\n   },\n   {\n@@ -83,7 +83,7 @@\n     }\n    ],\n    \"source\": [\n-    \"fieldset = FieldSet.from_data(data, dims, mesh=\\\"spherical\\\")\\n\",\n+    \"fieldset = parcels.FieldSet.from_data(data, dims, mesh=\\\"spherical\\\")\\n\",\n     \"plt.pcolormesh(fieldset.U.lon, fieldset.U.lat, fieldset.U.data[0, :, :], vmin=0, vmax=1)\\n\",\n     \"plt.colorbar()\\n\",\n     \"plt.show()\"\n@@ -242,7 +242,7 @@\n     }\n    ],\n    \"source\": [\n-    \"fieldset_flat = FieldSet.from_data(data, dims, mesh=\\\"flat\\\")\\n\",\n+    \"fieldset_flat = parcels.FieldSet.from_data(data, dims, mesh=\\\"flat\\\")\\n\",\n     \"\\n\",\n     \"plt.pcolormesh(\\n\",\n     \"    fieldset_flat.U.lon,\\n\",\n@@ -302,14 +302,14 @@\n     \"kh_meridional = 100  # in m^2/s\\n\",\n     \"\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"Kh_zonal\\\",\\n\",\n     \"        kh_zonal * np.ones((ydim, xdim), dtype=np.float32),\\n\",\n     \"        grid=fieldset.U.grid,\\n\",\n     \"    )\\n\",\n     \")\\n\",\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"Kh_meridional\\\",\\n\",\n     \"        kh_meridional * np.ones((ydim, xdim), dtype=np.float32),\\n\",\n     \"        grid=fieldset.U.grid,\\n\",\n@@ -391,7 +391,9 @@\n    ],\n    \"source\": [\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\\"Ustokes\\\", np.ones((ydim, xdim), dtype=np.float32), grid=fieldset.U.grid)\\n\",\n+    \"    parcels.Field(\\n\",\n+    \"        \\\"Ustokes\\\", np.ones((ydim, xdim), dtype=np.float32), grid=fieldset.U.grid\\n\",\n+    \"    )\\n\",\n     \")\\n\",\n     \"print(fieldset.Ustokes[0, 0, 40, -5])\"\n    ]\n@@ -449,7 +451,7 @@\n    ],\n    \"source\": [\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        \\\"Ustokes2\\\",\\n\",\n     \"        np.ones((ydim, xdim), dtype=np.float32),\\n\",\n     \"        grid=fieldset.U.grid,\\n\",\n@@ -492,7 +494,7 @@\n    ],\n    \"source\": [\n     \"fieldset.add_field(\\n\",\n-    \"    Field(\\n\",\n+    \"    parcels.Field(\\n\",\n     \"        name=\\\"Ucm\\\",\\n\",\n     \"        data=0.01 * np.ones((ydim, xdim), dtype=np.float32),\\n\",\n     \"        grid=fieldset.U.grid,\\n\",\ndiff --git a/docs/index.rst b/docs/index.rst\nindex 730917df5..2e0fcc50a 100755\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -7,7 +7,7 @@ Welcome to the documentation of Parcels. **Parcels** (Probably A Really Computat\n \n *Animation of virtual particles carried by ocean surface flow in the global oceans. The particles are advected with Parcels in data from the* `NEMO Ocean Model <https://www.nemo-ocean.eu/>`_.\n \n-Here you'll find `installation instructions <installation.rst>`_, `tutorials and documentation <documentation.rst>`_, and the `API reference <reference.rst>`_ for Parcels. You can browse the documentation for older versions by using the version switcher in the left sidebar.\n+Here you'll find `installation instructions <installation.rst>`_, `tutorials and documentation <documentation/index.rst>`_, and the `API reference <reference.rst>`_ for Parcels. You can browse the documentation for older versions by using the version switcher in the left sidebar.\n \n If you need more help with Parcels, try the `Discussions page on GitHub <https://github.com/OceanParcels/parcels/discussions>`_. If you think you found a bug, file an `Issue on GitHub <https://github.com/OceanParcels/parcels/issues>`_. If you want to help improve Parcels, see the `Contributing <contributing.rst>`_ page.\n \n@@ -17,7 +17,7 @@ If you need more help with Parcels, try the `Discussions page on GitHub <https:/\n \n    Home <self>\n    Installation <installation>\n-   Tutorials & Documentation <documentation>\n+   Tutorials & Documentation <documentation/index>\n    API reference <reference>\n    Contributing <contributing>\n    OceanParcels website <https://oceanparcels.org/>\ndiff --git a/parcels/application_kernels/advectiondiffusion.py b/parcels/application_kernels/advectiondiffusion.py\nindex 1cab7436b..259ae3059 100644\n--- a/parcels/application_kernels/advectiondiffusion.py\n+++ b/parcels/application_kernels/advectiondiffusion.py\n@@ -4,7 +4,7 @@\n \"\"\"\n import math\n \n-import parcels.rng as ParcelsRandom\n+import parcels\n \n __all__ = ['DiffusionUniformKh', 'AdvectionDiffusionM1', 'AdvectionDiffusionEM', ]\n \n@@ -25,8 +25,8 @@ def AdvectionDiffusionM1(particle, fieldset, time):\n     mean and a standard deviation of sqrt(dt).\n     \"\"\"\n     # Wiener increment with zero mean and std of sqrt(dt)\n-    dWx = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n-    dWy = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWx = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWy = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n \n     Kxp1 = fieldset.Kh_zonal[time, particle.depth, particle.lat, particle.lon + fieldset.dres]\n     Kxm1 = fieldset.Kh_zonal[time, particle.depth, particle.lat, particle.lon - fieldset.dres]\n@@ -60,8 +60,8 @@ def AdvectionDiffusionEM(particle, fieldset, time):\n     mean and a standard deviation of sqrt(dt).\n     \"\"\"\n     # Wiener increment with zero mean and std of sqrt(dt)\n-    dWx = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n-    dWy = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWx = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWy = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n \n     u, v = fieldset.UV[time, particle.depth, particle.lat, particle.lon]\n \n@@ -101,8 +101,8 @@ def DiffusionUniformKh(particle, fieldset, time):\n     mean and a standard deviation of sqrt(dt).\n     \"\"\"\n     # Wiener increment with zero mean and std of sqrt(dt)\n-    dWx = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n-    dWy = ParcelsRandom.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWx = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n+    dWy = parcels.rng.normalvariate(0, math.sqrt(math.fabs(particle.dt)))\n \n     bx = math.sqrt(2 * fieldset.Kh_zonal[particle])\n     by = math.sqrt(2 * fieldset.Kh_meridional[particle])\ndiff --git a/parcels/compilation/codegenerator.py b/parcels/compilation/codegenerator.py\nindex 197ec1279..effa9ed6a 100644\n--- a/parcels/compilation/codegenerator.py\n+++ b/parcels/compilation/codegenerator.py\n@@ -238,7 +238,7 @@ def visit_Name(self, node):\n             node = StatusCodeNode(math, ccode='')\n         elif node.id == 'math':\n             node = MathNode(math, ccode='')\n-        elif node.id == 'ParcelsRandom':\n+        elif node.id in ['ParcelsRandom', 'rng']:\n             node = RandomNode(math, ccode='')\n         elif node.id == 'print':\n             node = PrintNode()\ndiff --git a/parcels/kernel.py b/parcels/kernel.py\nindex 1825b0efe..0e297eceb 100644\n--- a/parcels/kernel.py\n+++ b/parcels/kernel.py\n@@ -5,8 +5,8 @@\n import math  # noqa\n import os\n import random  # noqa\n-import re\n import sys\n+import textwrap\n import types\n from copy import deepcopy\n from ctypes import byref, c_double, c_int\n@@ -23,6 +23,7 @@\n     MPI = None\n \n import parcels.rng as ParcelsRandom  # noqa\n+from parcels import rng  # noqa\n from parcels.application_kernels.advection import (\n     AdvectionAnalytical,\n     AdvectionRK4_3D,\n@@ -119,15 +120,6 @@ def _cache_key(self):\n         key = self.name + self.ptype._cache_key + field_keys + ('TIME:%f' % ostime())\n         return hashlib.md5(key.encode('utf-8')).hexdigest()\n \n-    @staticmethod\n-    def fix_indentation(string):\n-        \"\"\"Fix indentation to allow in-lined kernel definitions.\"\"\"\n-        lines = string.split('\\n')\n-        indent = re.compile(r\"^(\\s+)\").match(lines[0])\n-        if indent:\n-            lines = [line.replace(indent.groups()[0], '', 1) for line in lines]\n-        return \"\\n\".join(lines)\n-\n     def remove_deleted(self, pset):\n         \"\"\"Utility to remove all particles that signalled deletion.\"\"\"\n         bool_indices = pset.particledata.state == StatusCode.Delete\n@@ -176,8 +168,15 @@ def __init__(self, fieldset, ptype, pyfunc=None, funcname=None, funccode=None, p\n         else:\n             self.funcvars = None\n         self.funccode = funccode or inspect.getsource(pyfunc.__code__)\n+        self.funccode = (  # Remove parcels. prefix (see #1608)\n+            self.funccode\n+            .replace('parcels.rng', 'rng')\n+            .replace('parcels.ParcelsRandom', 'ParcelsRandom')\n+            .replace('parcels.StatusCode', 'StatusCode')\n+        )\n+\n         # Parse AST if it is not provided explicitly\n-        self.py_ast = py_ast or ast.parse(BaseKernel.fix_indentation(self.funccode)).body[0]\n+        self.py_ast = py_ast or ast.parse(textwrap.dedent(self.funccode)).body[0]  # Dedent allows for in-lined kernel definitions\n         if pyfunc is None:\n             # Extract user context by inspecting the call stack\n             stack = inspect.stack()\n@@ -185,6 +184,7 @@ def __init__(self, fieldset, ptype, pyfunc=None, funcname=None, funccode=None, p\n                 user_ctx = stack[-1][0].f_globals\n                 user_ctx['math'] = globals()['math']\n                 user_ctx['ParcelsRandom'] = globals()['ParcelsRandom']\n+                user_ctx['rng'] = globals()['rng']\n                 user_ctx['random'] = globals()['random']\n                 user_ctx['StatusCode'] = globals()['StatusCode']\n             except:\ndiff --git a/parcels/particlefile.py b/parcels/particlefile.py\nindex e301f137b..3044b8142 100644\n--- a/parcels/particlefile.py\n+++ b/parcels/particlefile.py\n@@ -1,7 +1,7 @@\n \"\"\"Module controlling the writing of ParticleSets to Zarr file.\"\"\"\n import os\n from abc import ABC\n-from datetime import timedelta as delta\n+from datetime import timedelta\n \n import numpy as np\n import xarray as xr\n@@ -61,7 +61,7 @@ class ParticleFile(ABC):\n \n     def __init__(self, name, particleset, outputdt=np.inf, chunks=None, create_new_zarrfile=True):\n \n-        self.outputdt = outputdt.total_seconds() if isinstance(outputdt, delta) else outputdt\n+        self.outputdt = outputdt.total_seconds() if isinstance(outputdt, timedelta) else outputdt\n         self.chunks = chunks\n         self.particleset = particleset\n         self.parcels_mesh = 'spherical'\n@@ -202,7 +202,7 @@ def write(self, pset, time, indices=None):\n         time :\n             Time at which to write ParticleSet\n         \"\"\"\n-        time = time.total_seconds() if isinstance(time, delta) else time\n+        time = time.total_seconds() if isinstance(time, timedelta) else time\n \n         if pset.particledata._ncount == 0:\n             logger.warning(\"ParticleSet is empty on writing as array at time %g\" % time)\ndiff --git a/parcels/particleset.py b/parcels/particleset.py\nindex a62004b6f..35c0d7bb2 100644\n--- a/parcels/particleset.py\n+++ b/parcels/particleset.py\n@@ -2,8 +2,7 @@\n import sys\n from abc import ABC\n from copy import copy\n-from datetime import date, datetime\n-from datetime import timedelta as delta\n+from datetime import date, datetime, timedelta\n \n import cftime\n import numpy as np\n@@ -187,7 +186,7 @@ def ArrayClass_init(self, *args, **kwargs):\n                 assert lon.size == kwargs[kwvar].size, (\n                     f\"{kwvar} and positions (lon, lat, depth) don't have the same lengths.\")\n \n-        self.repeatdt = repeatdt.total_seconds() if isinstance(repeatdt, delta) else repeatdt\n+        self.repeatdt = repeatdt.total_seconds() if isinstance(repeatdt, timedelta) else repeatdt\n         if self.repeatdt:\n             if self.repeatdt <= 0:\n                 raise 'Repeatdt should be > 0'\n@@ -868,7 +867,7 @@ def execute(self, pyfunc=AdvectionRK4, pyfunc_inter=None, endtime=None, runtime=\n                 self.interaction_kernel = self.InteractionKernel(pyfunc_inter, delete_cfiles=delete_cfiles)\n \n         # Convert all time variables to seconds\n-        if isinstance(endtime, delta):\n+        if isinstance(endtime, timedelta):\n             raise RuntimeError('endtime must be either a datetime or a double')\n         if isinstance(endtime, datetime):\n             endtime = np.datetime64(endtime)\n@@ -878,18 +877,18 @@ def execute(self, pyfunc=AdvectionRK4, pyfunc_inter=None, endtime=None, runtime=\n             if self.time_origin.calendar is None:\n                 raise NotImplementedError('If fieldset.time_origin is not a date, execution endtime must be a double')\n             endtime = self.time_origin.reltime(endtime)\n-        if isinstance(runtime, delta):\n+        if isinstance(runtime, timedelta):\n             runtime = runtime.total_seconds()\n-        if isinstance(dt, delta):\n+        if isinstance(dt, timedelta):\n             dt = dt.total_seconds()\n         if abs(dt) <= 1e-6:\n             raise ValueError('Time step dt is too small')\n         if (dt * 1e6) % 1 != 0:\n             raise ValueError('Output interval should not have finer precision than 1e-6 s')\n         outputdt = output_file.outputdt if output_file else np.inf\n-        if isinstance(outputdt, delta):\n+        if isinstance(outputdt, timedelta):\n             outputdt = outputdt.total_seconds()\n-        if isinstance(callbackdt, delta):\n+        if isinstance(callbackdt, timedelta):\n             callbackdt = callbackdt.total_seconds()\n \n         assert runtime is None or runtime >= 0, 'runtime must be positive'\ndiff --git a/parcels/tools/converters.py b/parcels/tools/converters.py\nindex 71ffdf9b5..5f9e8205e 100644\n--- a/parcels/tools/converters.py\n+++ b/parcels/tools/converters.py\n@@ -1,6 +1,6 @@\n # flake8: noqa: E999\n import inspect\n-from datetime import timedelta as delta\n+from datetime import timedelta\n from math import cos, pi\n \n import cftime\n@@ -116,7 +116,7 @@ def fulltime(self, time):\n             else:\n                 return self.time_origin + np.timedelta64(int(time), 's')\n         elif self.calendar in _get_cftime_calendars():\n-            return self.time_origin + delta(seconds=time)\n+            return self.time_origin + timedelta(seconds=time)\n         elif self.calendar is None:\n             return self.time_origin + time\n         else:\n", "instance_id": "OceanParcels__Parcels-1610", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in outlining the goals of the changes, such as renaming `Parcels` to lowercase `parcels`, removing the `delta` alias for `timedelta`, and supporting `parcels.rng` in kernels. It provides specific examples of code changes (e.g., import style modifications) and references related issues (#1608, #1581). However, there are minor ambiguities and missing details. For instance, the exact scope of renaming (whether it applies to all internal code or just examples and documentation) is not fully specified, and the implications of kernel conversion errors to C are mentioned but not detailed in terms of expected behavior or resolution. Additionally, edge cases or potential conflicts arising from these changes are not addressed, which could lead to implementation uncertainties.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the nature of the changes required. The tasks primarily involve straightforward code modifications, such as renaming conventions across multiple files (e.g., changing `Parcels` to `parcels` and replacing `delta` with `timedelta`), which requires basic search-and-replace operations and understanding of Python naming conventions. Supporting `parcels.rng` in kernels involves modifying import statements and ensuring compatibility in the kernel compilation process, which requires a moderate understanding of the codebase's structure and random number generation module. The changes span multiple files and examples, as seen in the extensive diff, but do not significantly impact the system's architecture or require complex logic or algorithm design. The issue with kernel conversion to C (e.g., `NotImplementedError` for `parcels` in kernels) adds a slight layer of complexity, but it is still within the realm of adjusting import handling or code generation logic, which is relatively contained. Edge case handling and performance considerations appear minimal, as the changes are mostly syntactic or stylistic. Overall, this task requires understanding some code logic and making simple modifications, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Article cannot be omitted with languages when source language is Dutch\nEngels with article is not accepted, but should be when source language is Dutch:\r\n\r\n```console\r\nTranslate into Dutch:\r\nenglanti\r\n> Engels\r\n\u26a0\ufe0f  Incorrect. Please try again.\r\n> het Engels\r\n\u2705 Correct.\r\nNote: In Finnish, the names of languages are not capitalized.\r\n```\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex c117a41a..4e0439f6 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -10,6 +10,7 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)\n \n - Accept an initial capital letter when both the question and the answer of a quiz are in lower case. Fixes [#671](https://github.com/fniessink/toisto/issues/671).\n - Use single quotes when referring to questions and answers in user feedback. When feedback ends with a quoted question or answer, only add a period if the quoted question or answer does not already end with punctuation. Fixes [#675](https://github.com/fniessink/toisto/issues/675).\n+- Allow for omitting the article when the source language is Dutch and the answer has a capital. For example, when the correct answer was \"het Engels\", answering \"Engels\" would be marked as incorrect. Fixes [#680](https://github.com/fniessink/toisto/issues/680).\n \n ### Added\n \ndiff --git a/src/toisto/model/quiz/quiz.py b/src/toisto/model/quiz/quiz.py\nindex 7fab04ed..79e4082a 100644\n--- a/src/toisto/model/quiz/quiz.py\n+++ b/src/toisto/model/quiz/quiz.py\n@@ -120,7 +120,7 @@ def key(self) -> str:\n \n     def is_correct(self, guess: Label) -> bool:\n         \"\"\"Return whether the guess is correct.\"\"\"\n-        ignore_first_upper_case = not self.question.starts_with_upper_case and not self.answer.starts_with_upper_case\n+        ignore_first_upper_case = all(not label.starts_with_upper_case for label in (*self.answers, self.question))\n         return match(guess.with_lower_case_first_letter if ignore_first_upper_case else guess, *self.answers)\n \n     def is_question(self, guess: Label) -> bool:\n", "instance_id": "fniessink__toisto-682", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when the source language is Dutch, answers for language names without an article (e.g., \"Engels\" instead of \"het Engels\") should be accepted, but currently, they are marked as incorrect. The example provided in the console output helps illustrate the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention whether this rule applies only to Dutch as the source language or if there are other languages with similar rules. Additionally, it does not specify if there are other conditions or edge cases (e.g., capitalization variations beyond the initial capital) that need to be considered. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. The code change is localized to a single file (quiz.py) and involves a small modification to the logic for handling capitalization in the `is_correct` method. The change adjusts the condition for ignoring the first uppercase letter by checking all answers and the question, rather than just the primary answer and question. This requires a basic understanding of the codebase's logic around quiz answer validation and string manipulation, likely in Python. The scope of the change is minimal, with no impact on the broader system architecture or interactions between modules. The technical concepts involved are straightforward\u2014basic conditional logic and string property checks. Edge cases seem limited to capitalization variations, and the problem does not appear to introduce complex error handling requirements. Overall, this is a simple bug fix that a junior to mid-level developer could handle with minimal guidance.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "pifpaf has a dependency on distutils (and current releases on pkg_resources as well) that are not installed by default on python 3.12 and above\npifpaf imports [distutils](https://github.com/jd/pifpaf/blob/d37bae254a627c0aef2e760b697336080193681f/pifpaf/drivers/__init__.py#L24)  and current release versions also import pkg_resources which was [removed here](https://github.com/jd/pifpaf/commit/16a22f67b993f0bb392877f76934bdc3ba463e2c) (but it's still on current pypi release 3.1.5)\r\n\r\n\r\nboth of these tools are no longer included in python virtual environments:\r\n\r\nhttps://docs.python.org/3/whatsnew/3.12.html\r\n\r\n> [gh-95299](https://github.com/python/cpython/issues/95299): Do not pre-install setuptools in virtual environments created with [venv](https://docs.python.org/3/library/venv.html#module-venv). This means that distutils, setuptools, pkg_resources, and easy_install will no longer available by default; to access these run pip install setuptools in the [activated](https://docs.python.org/3/library/venv.html#venv-explanation) virtual environment.\r\n\r\n\r\nthe simple solution is to add setuptools as a dependency.   alternatively, remove the use of distutils\r\n\r\n\r\nI can't easily demonstrate the distutils failure installing from github master due to #177 , unless you modify setup.cfg to also install drivers, it then looks like this:\r\n\r\n```\r\n(.venv) [classic@framework pifpaf:add_valkey]$ pip install .\r\nProcessing /home/classic/dev/pifpaf\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nCollecting daiquiri (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached daiquiri-3.2.5.1-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting click (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting jinja2 (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting fixtures (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached fixtures-4.1.0-py3-none-any.whl.metadata (21 kB)\r\nCollecting packaging (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting psutil (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\nCollecting xattr (from pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\nCollecting python-json-logger (from daiquiri->pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\r\nCollecting pbr>=5.7.0 (from fixtures->pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\nCollecting MarkupSafe>=2.0 (from jinja2->pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\nCollecting cffi>=1.16.0 (from xattr->pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting pycparser (from cffi>=1.16.0->xattr->pifpaf==3.1.6.dev47+ge7251a1.d20240808)\r\n  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nUsing cached daiquiri-3.2.5.1-py3-none-any.whl (19 kB)\r\nUsing cached fixtures-4.1.0-py3-none-any.whl (64 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nUsing cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\r\nUsing cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39 kB)\r\nUsing cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\r\nUsing cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\r\nUsing cached pbr-6.0.0-py2.py3-none-any.whl (107 kB)\r\nUsing cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\r\nUsing cached pycparser-2.22-py3-none-any.whl (117 kB)\r\nBuilding wheels for collected packages: pifpaf\r\n  Building wheel for pifpaf (pyproject.toml) ... done\r\n  Created wheel for pifpaf: filename=pifpaf-3.1.6.dev47+ge7251a1.d20240808-py3-none-any.whl size=61472 sha256=4646f6c6d5e10e7b6fc5c7d0a96164a316be1f0e9772578c51168701b5fb055d\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2nk31oob/wheels/6a/5c/ed/c42abf20db70490f731c7550eee38a8793ca115412ab3a65fb\r\nSuccessfully built pifpaf\r\nInstalling collected packages: python-json-logger, pycparser, psutil, pbr, packaging, MarkupSafe, click, jinja2, fixtures, daiquiri, cffi, xattr, pifpaf\r\nSuccessfully installed MarkupSafe-2.1.5 cffi-1.17.0 click-8.1.7 daiquiri-3.2.5.1 fixtures-4.1.0 jinja2-3.1.4 packaging-24.1 pbr-6.0.0 pifpaf-3.1.6.dev47+ge7251a1.d20240808 psutil-6.0.0 pycparser-2.22 python-json-logger-2.0.7 xattr-1.1.0\r\n\r\n[notice] A new release of pip is available: 24.1.2 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n(.venv) [classic@framework pifpaf:add_valkey]$ cd ~/tmp/\r\n(.venv) [classic@framework tmp]$ pifpaf run redis\r\nCRITICAL [root] Traceback (most recent call last):\r\n  File \"/home/classic/tmp/.venv/bin/pifpaf\", line 8, in <module>\r\n    sys.exit(run_main())\r\n             ^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/pifpaf/__main__.py\", line 299, in run_main\r\n    return main.main(standalone_mode=False)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/click/core.py\", line 1682, in invoke\r\n    cmd_name, cmd, args = self.resolve_command(ctx, args)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/click/core.py\", line 1729, in resolve_command\r\n    cmd = self.get_command(ctx, cmd_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/pifpaf/__main__.py\", line 133, in get_command\r\n    plugin = [e for e in DAEMONS if e.name == name][0].load()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/metadata/__init__.py\", line 205, in load\r\n    module = import_module(match.group('module'))\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/__init__.py\", line 90, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\n  File \"/home/classic/tmp/.venv/lib/python3.12/site-packages/pifpaf/drivers/__init__.py\", line 24, in <module>\r\n    from distutils import spawn\r\nModuleNotFoundError: No module named 'distutils'\r\n\r\n\r\n```\r\n\r\nan  install from pypi fails on pkg_resources instead, we have to have setuptools in our tox dependencies to work around this:\r\n\r\n```\r\n[classic@framework tmp]$ python -m venv .venv\r\n[classic@framework tmp]$ .venv/bin/pip install pifpaf\r\nCollecting pifpaf\r\n  Using cached pifpaf-3.1.5-py3-none-any.whl.metadata (7.0 kB)\r\nCollecting daiquiri (from pifpaf)\r\n  Using cached daiquiri-3.2.5.1-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting click (from pifpaf)\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting pbr (from pifpaf)\r\n  Using cached pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\nCollecting jinja2 (from pifpaf)\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting fixtures (from pifpaf)\r\n  Using cached fixtures-4.1.0-py3-none-any.whl.metadata (21 kB)\r\nCollecting psutil (from pifpaf)\r\n  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\nCollecting xattr (from pifpaf)\r\n  Using cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\nCollecting python-json-logger (from daiquiri->pifpaf)\r\n  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\r\nCollecting MarkupSafe>=2.0 (from jinja2->pifpaf)\r\n  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\nCollecting cffi>=1.16.0 (from xattr->pifpaf)\r\n  Using cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting pycparser (from cffi>=1.16.0->xattr->pifpaf)\r\n  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\r\nUsing cached pifpaf-3.1.5-py3-none-any.whl (62 kB)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nUsing cached daiquiri-3.2.5.1-py3-none-any.whl (19 kB)\r\nUsing cached fixtures-4.1.0-py3-none-any.whl (64 kB)\r\nUsing cached pbr-6.0.0-py2.py3-none-any.whl (107 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\r\nUsing cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39 kB)\r\nUsing cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\r\nUsing cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\r\nUsing cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\r\nUsing cached pycparser-2.22-py3-none-any.whl (117 kB)\r\nInstalling collected packages: python-json-logger, pycparser, psutil, pbr, MarkupSafe, click, jinja2, fixtures, daiquiri, cffi, xattr, pifpaf\r\nSuccessfully installed MarkupSafe-2.1.5 cffi-1.17.0 click-8.1.7 daiquiri-3.2.5.1 fixtures-4.1.0 jinja2-3.1.4 pbr-6.0.0 pifpaf-3.1.5 psutil-6.0.0 pycparser-2.22 python-json-logger-2.0.7 xattr-1.1.0\r\n\r\n[notice] A new release of pip is available: 23.3.2 -> 24.2\r\n[notice] To update, run: python3.12 -m pip install --upgrade pip\r\n[classic@framework tmp]$ .venv/bin/pifpaf run redis\r\nTraceback (most recent call last):\r\n  File \"/home/classic/tmp/.venv/bin/pifpaf\", line 5, in <module>\r\n    from pifpaf.__main__ import run_main\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/pifpaf/__main__.py\", line 31, in <module>\r\n    import pkg_resources\r\nModuleNotFoundError: No module named 'pkg_resources'\r\n```\nsetup.cfg is not functional for current pypa tooling\nsetup.cfg contains only the \"pifpaf\" module as an install:\r\n\r\nhttps://github.com/jd/pifpaf/blob/d37bae254a627c0aef2e760b697336080193681f/setup.cfg#L21-L24\r\n\r\nI'm not sure of this history of this but this fails to install \"drivers\", both from a pip install from github, as well as if I do a \"python setup.py sdist\" the drivers are missing when you do the actual install\r\n\r\ndemo one, install from master:\r\n\r\n```\r\n[classic@framework tmp]$ python -V\r\nPython 3.12.4\r\n[classic@framework tmp]$ python -m venv .venv\r\n[classic@framework tmp]$ .venv/bin/pip install git+https://github.com/jd/pifpaf/\r\nCollecting git+https://github.com/jd/pifpaf/\r\n  Cloning https://github.com/jd/pifpaf/ to /tmp/pip-req-build-8tgv063q\r\n  Running command git clone --filter=blob:none --quiet https://github.com/jd/pifpaf/ /tmp/pip-req-build-8tgv063q\r\n  Resolved https://github.com/jd/pifpaf/ to commit d37bae254a627c0aef2e760b697336080193681f\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nCollecting daiquiri (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached daiquiri-3.2.5.1-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting click (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting jinja2 (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting fixtures (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached fixtures-4.1.0-py3-none-any.whl.metadata (21 kB)\r\nCollecting packaging (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting psutil (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\nCollecting xattr (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\nCollecting python-json-logger (from daiquiri->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\r\nCollecting pbr>=5.7.0 (from fixtures->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\nCollecting MarkupSafe>=2.0 (from jinja2->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\nCollecting cffi>=1.16.0 (from xattr->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting pycparser (from cffi>=1.16.0->xattr->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nUsing cached daiquiri-3.2.5.1-py3-none-any.whl (19 kB)\r\nUsing cached fixtures-4.1.0-py3-none-any.whl (64 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nUsing cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\r\nUsing cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39 kB)\r\nUsing cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\r\nUsing cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\r\nUsing cached pbr-6.0.0-py2.py3-none-any.whl (107 kB)\r\nUsing cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\r\nUsing cached pycparser-2.22-py3-none-any.whl (117 kB)\r\nBuilding wheels for collected packages: pifpaf\r\n  Building wheel for pifpaf (pyproject.toml) ... done\r\n  Created wheel for pifpaf: filename=pifpaf-3.1.6.dev46+gd37bae2-py3-none-any.whl size=10519 sha256=6048c96e4578c2b1bd8fe300b630cb6d982e11e39e087520db796dbd0b0ce9a3\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-3o3uyqfv/wheels/21/51/37/1f567de3e004a4469dc2c2da83baf62eef0247babfac0f6cdd\r\nSuccessfully built pifpaf\r\nInstalling collected packages: python-json-logger, pycparser, psutil, pbr, packaging, MarkupSafe, click, jinja2, fixtures, daiquiri, cffi, xattr, pifpaf\r\nSuccessfully installed MarkupSafe-2.1.5 cffi-1.17.0 click-8.1.7 daiquiri-3.2.5.1 fixtures-4.1.0 jinja2-3.1.4 packaging-24.1 pbr-6.0.0 pifpaf-3.1.6.dev46+gd37bae2 psutil-6.0.0 pycparser-2.22 python-json-logger-2.0.7 xattr-1.1.0\r\n\r\n[notice] A new release of pip is available: 23.3.2 -> 24.2\r\n[notice] To update, run: python3.12 -m pip install --upgrade pip\r\n[classic@framework tmp]$ .venv/bin/pifpaf run redis\r\nCRITICAL [root] Traceback (most recent call last):\r\n  File \"/home/classic/tmp/.venv/bin/pifpaf\", line 8, in <module>\r\n    sys.exit(run_main())\r\n             ^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/pifpaf/__main__.py\", line 299, in run_main\r\n    return main.main(standalone_mode=False)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1682, in invoke\r\n    cmd_name, cmd, args = self.resolve_command(ctx, args)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1729, in resolve_command\r\n    cmd = self.get_command(ctx, cmd_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/pifpaf/__main__.py\", line 133, in get_command\r\n    plugin = [e for e in DAEMONS if e.name == name][0].load()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/metadata/__init__.py\", line 205, in load\r\n    module = import_module(match.group('module'))\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/__init__.py\", line 90, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'pifpaf.drivers'\r\n\r\n```\r\n\r\n\r\ndemo two, python setup.py sdist .  it seems to put the drivers into the tar.gz but they still dont install\r\n\r\n```\r\n[classic@framework tmp]$ git clone https://github.com/jd/pifpaf/\r\nCloning into 'pifpaf'...\r\nremote: Enumerating objects: 2349, done.\r\nremote: Counting objects: 100% (304/304), done.\r\nremote: Compressing objects: 100% (129/129), done.\r\nremote: Total 2349 (delta 182), reused 270 (delta 170), pack-reused 2045\r\nReceiving objects: 100% (2349/2349), 802.70 KiB | 8.82 MiB/s, done.\r\nResolving deltas: 100% (1554/1554), done.\r\n[classic@framework tmp]$ cd pifpaf/\r\n[classic@framework pifpaf:master]$ python setup.py  sdist\r\nrunning sdist\r\nrunning egg_info\r\ncreating pifpaf.egg-info\r\nwriting pifpaf.egg-info/PKG-INFO\r\nwriting dependency_links to pifpaf.egg-info/dependency_links.txt\r\nwriting entry points to pifpaf.egg-info/entry_points.txt\r\nwriting requirements to pifpaf.egg-info/requires.txt\r\nwriting top-level names to pifpaf.egg-info/top_level.txt\r\nwriting manifest file 'pifpaf.egg-info/SOURCES.txt'\r\nadding license file 'LICENSE'\r\nwriting manifest file 'pifpaf.egg-info/SOURCES.txt'\r\nrunning check\r\ncreating pifpaf-3.1.6.dev46+gd37bae2\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/.github\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/.github/workflows\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/artemis\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/qdrouterd\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncreating pifpaf-3.1.6.dev46+gd37bae2/pifpaf/tests\r\ncopying files to pifpaf-3.1.6.dev46+gd37bae2...\r\ncopying .dockerignore -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying .gitignore -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying .mergify.yml -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying .stestr.conf -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying Dockerfile -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying LICENSE -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying README.rst -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying pifpaf.jpg -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying pyproject.toml -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying setup.cfg -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying setup.py -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying tox.ini -> pifpaf-3.1.6.dev46+gd37bae2\r\ncopying .github/workflows/pifpaf-ci-image.yaml -> pifpaf-3.1.6.dev46+gd37bae2/.github/workflows\r\ncopying .github/workflows/pifpaf.yaml -> pifpaf-3.1.6.dev46+gd37bae2/.github/workflows\r\ncopying pifpaf/__init__.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf\r\ncopying pifpaf/__main__.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf\r\ncopying pifpaf/util.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf\r\ncopying pifpaf.egg-info/PKG-INFO -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf.egg-info/SOURCES.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf.egg-info/dependency_links.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf.egg-info/entry_points.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf.egg-info/requires.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf.egg-info/top_level.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\ncopying pifpaf/drivers/__init__.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/aodh.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/artemis.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/ceph.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/consul.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/couchdb.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/elasticsearch.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/etcd.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/fakes3.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/gnocchi.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/httpbin.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/influxdb.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/kafka.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/keystone.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/memcached.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/mongodb.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/mysql.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/postgresql.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/qdrouterd.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/rabbitmq.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/redis.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/s3rver.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/swift.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/vault.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/zookeeper.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers\r\ncopying pifpaf/drivers/templates/artemis/broker.xml -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/artemis\r\ncopying pifpaf/drivers/templates/qdrouterd/broker_om.conf.inc -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/qdrouterd\r\ncopying pifpaf/drivers/templates/qdrouterd/direct_om.conf.inc -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/qdrouterd\r\ncopying pifpaf/drivers/templates/qdrouterd/qdrouterd.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/qdrouterd\r\ncopying pifpaf/drivers/templates/qdrouterd/sasl_qdrouterd.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/qdrouterd\r\ncopying pifpaf/drivers/templates/swift/account.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/common.conf.inc -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/container-sync-realms.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/container.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/object.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/proxy.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/sitecustomize.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/drivers/templates/swift/swift.conf -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/drivers/templates/swift\r\ncopying pifpaf/tests/__init__.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/tests\r\ncopying pifpaf/tests/test_cli.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/tests\r\ncopying pifpaf/tests/test_drivers.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/tests\r\ncopying pifpaf/tests/unkillable.py -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf/tests\r\ncopying pifpaf.egg-info/SOURCES.txt -> pifpaf-3.1.6.dev46+gd37bae2/pifpaf.egg-info\r\nWriting pifpaf-3.1.6.dev46+gd37bae2/setup.cfg\r\ncreating dist\r\nCreating tar archive\r\nremoving 'pifpaf-3.1.6.dev46+gd37bae2' (and everything under it)\r\n[classic@framework pifpaf:master]$ cd ..\r\n[classic@framework tmp]$ python -m venv .venv\r\n[classic@framework tmp]$ .venv/bin/pip install pifpaf/dist/pifpaf-3.1.6.dev46+gd37bae2.tar.gz \r\nProcessing ./pifpaf/dist/pifpaf-3.1.6.dev46+gd37bae2.tar.gz\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nCollecting daiquiri (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached daiquiri-3.2.5.1-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting click (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting jinja2 (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting fixtures (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached fixtures-4.1.0-py3-none-any.whl.metadata (21 kB)\r\nCollecting packaging (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting psutil (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\nCollecting xattr (from pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\nCollecting python-json-logger (from daiquiri->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\r\nCollecting pbr>=5.7.0 (from fixtures->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\nCollecting MarkupSafe>=2.0 (from jinja2->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\nCollecting cffi>=1.16.0 (from xattr->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting pycparser (from cffi>=1.16.0->xattr->pifpaf==3.1.6.dev46+gd37bae2)\r\n  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nUsing cached daiquiri-3.2.5.1-py3-none-any.whl (19 kB)\r\nUsing cached fixtures-4.1.0-py3-none-any.whl (64 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nUsing cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\r\nUsing cached xattr-1.1.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39 kB)\r\nUsing cached cffi-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\r\nUsing cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\r\nUsing cached pbr-6.0.0-py2.py3-none-any.whl (107 kB)\r\nUsing cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\r\nUsing cached pycparser-2.22-py3-none-any.whl (117 kB)\r\nBuilding wheels for collected packages: pifpaf\r\n  Building wheel for pifpaf (pyproject.toml) ... done\r\n  Created wheel for pifpaf: filename=pifpaf-3.1.6.dev46+gd37bae2-py3-none-any.whl size=10519 sha256=a8030ad6260456dedce24a7a5679d4500b16430f1a41f572d72f7927b259f45a\r\n  Stored in directory: /home/classic/.cache/pip/wheels/f7/24/4c/72741e04e537b6069a46b88f898ac9110b9a031c6b0f7a91fb\r\nSuccessfully built pifpaf\r\nInstalling collected packages: python-json-logger, pycparser, psutil, pbr, packaging, MarkupSafe, click, jinja2, fixtures, daiquiri, cffi, xattr, pifpaf\r\nSuccessfully installed MarkupSafe-2.1.5 cffi-1.17.0 click-8.1.7 daiquiri-3.2.5.1 fixtures-4.1.0 jinja2-3.1.4 packaging-24.1 pbr-6.0.0 pifpaf-3.1.6.dev46+gd37bae2 psutil-6.0.0 pycparser-2.22 python-json-logger-2.0.7 xattr-1.1.0\r\n\r\n[notice] A new release of pip is available: 23.3.2 -> 24.2\r\n[notice] To update, run: python3.12 -m pip install --upgrade pip\r\n[classic@framework tmp]$ .venv/bin/pifpaf run redis\r\nCRITICAL [root] Traceback (most recent call last):\r\n  File \"/home/classic/tmp/.venv/bin/pifpaf\", line 8, in <module>\r\n    sys.exit(run_main())\r\n             ^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/pifpaf/__main__.py\", line 299, in run_main\r\n    return main.main(standalone_mode=False)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1682, in invoke\r\n    cmd_name, cmd, args = self.resolve_command(ctx, args)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/click/core.py\", line 1729, in resolve_command\r\n    cmd = self.get_command(ctx, cmd_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/classic/tmp/.venv/lib64/python3.12/site-packages/pifpaf/__main__.py\", line 133, in get_command\r\n    plugin = [e for e in DAEMONS if e.name == name][0].load()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/metadata/__init__.py\", line 205, in load\r\n    module = import_module(match.group('module'))\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/importlib/__init__.py\", line 90, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'pifpaf.drivers'\r\n\r\n```\n", "patch": "diff --git a/README.rst b/README.rst\nindex 39b2d16..e9cee1f 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -24,6 +24,7 @@ Pifpaf currently supports:\n * `InfluxDB`_\n * `Etcd`_ (with clustering)\n * `Redis`_ (with sentinel mode)\n+* `Valkey`_\n * `Elasticsearch`_\n * `ZooKeeper`_\n * `Gnocchi`_\n@@ -47,6 +48,7 @@ Pifpaf currently supports:\n .. _InfluxDB: http://influxdb.org\n .. _Etcd: https://coreos.com/etcd/\n .. _Redis: http://redis.io/\n+.. _Valkey: https://valkey.io/\n .. _Elasticsearch: https://www.elastic.co/\n .. _ZooKeeper: https://zookeeper.apache.org/\n .. _Gnocchi: http://gnocchi.xyz\ndiff --git a/pifpaf/drivers/valkey.py b/pifpaf/drivers/valkey.py\nnew file mode 100644\nindex 0000000..07f7511\n--- /dev/null\n+++ b/pifpaf/drivers/valkey.py\n@@ -0,0 +1,90 @@\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+# implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+\n+from pifpaf import drivers\n+\n+\n+class ValkeyDriver(drivers.Driver):\n+\n+    DEFAULT_PORT = 6379\n+    DEFAULT_PORT_SENTINEL = 6380\n+    DEFAULT_PASSWORD = ''\n+\n+    def __init__(self, port=DEFAULT_PORT,\n+                 sentinel=False, sentinel_port=DEFAULT_PORT_SENTINEL,\n+                 password=DEFAULT_PASSWORD, **kwargs):\n+        \"\"\"Create a new Valkey server.\"\"\"\n+        super(ValkeyDriver, self).__init__(**kwargs)\n+        self.port = port\n+        self.sentinel = sentinel\n+        self.sentinel_port = sentinel_port\n+        self.password = password\n+\n+    @classmethod\n+    def get_options(cls):\n+        return [\n+            {\"param_decls\": [\"--port\"],\n+             \"type\": int,\n+             \"default\": cls.DEFAULT_PORT,\n+             \"help\": \"port to use for Valkey\"},\n+            {\"param_decls\": [\"--sentinel\"],\n+             \"is_flag\": True,\n+             \"help\": \"activate Valkey sentinel\"},\n+            {\"param_decls\": [\"--sentinel-port\"],\n+             \"type\": int,\n+             \"default\": cls.DEFAULT_PORT_SENTINEL,\n+             \"help\": \"port to use for Valkey sentinel\"},\n+            {\"param_decls\": [\"--password\"],\n+             \"default\": cls.DEFAULT_PASSWORD,\n+             \"help\": \"Valkey and Valkey sentinel password\"},\n+        ]\n+\n+    def _setUp(self):\n+        super(ValkeyDriver, self)._setUp()\n+        valkey_conf = \"\"\"dir %s\n+port %d\n+\"\"\" % (self.tempdir, self.port)\n+        if self.password:\n+            valkey_conf += \"requirepass %s\\n\" % self.password\n+        c, _ = self._exec(\n+            [\"valkey-server\", \"-\"],\n+            stdin=(valkey_conf).encode('ascii'),\n+            wait_for_line=\"eady to accept connections\")\n+\n+        if self.sentinel:\n+            cfg = os.path.join(self.tempdir, \"valkey-sentinel.conf\")\n+            sentinel_conf = \"\"\"dir %s\n+port %d\n+sentinel monitor pifpaf localhost %d 1\n+\"\"\" % (self.tempdir, self.sentinel_port, self.port)\n+            if self.password:\n+                sentinel_conf += (\n+                    \"sentinel auth-pass pifpaf %s\\n\" % self.password)\n+                sentinel_conf += \"requirepass %s\\n\" % self.password\n+            with open(cfg, \"w\") as f:\n+                f.write(sentinel_conf)\n+\n+            c, _ = self._exec(\n+                [\"valkey-sentinel\", cfg],\n+                wait_for_line=r\"# Sentinel (runid|ID) is\")\n+\n+            self.addCleanup(self._kill, c)\n+\n+            self.putenv(\"VALKEY_SENTINEL_PORT\",\n+                        str(self.sentinel_port))\n+\n+        self.putenv(\"VALKEY_PORT\", str(self.port))\n+        self.url = \"valkey://localhost:%d\" % self.port\n+        self.putenv(\"URL\", self.url)\ndiff --git a/setup.cfg b/setup.cfg\nindex 0818a22..c19fc1a 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -66,6 +66,7 @@ pifpaf.daemons =\n     qdrouterd = pifpaf.drivers.qdrouterd:QdrouterdDriver\n     rabbitmq = pifpaf.drivers.rabbitmq:RabbitMQDriver\n     redis = pifpaf.drivers.redis:RedisDriver\n+    valkey = pifpaf.drivers.valkey:ValkeyDriver\n     s3rver = pifpaf.drivers.s3rver:S3rverDriver\n     zookeeper = pifpaf.drivers.zookeeper:ZooKeeperDriver\n     vault = pifpaf.drivers.vault:VaultDriver\n", "instance_id": "jd__pifpaf-175", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear in identifying the core issue: the `pifpaf` library has dependencies on `distutils` and `pkg_resources`, which are no longer included by default in Python 3.12 and above, causing installation and runtime failures. The statement provides detailed error logs and reproduction steps, which are helpful for understanding the problem's scope. It also suggests potential solutions, such as adding `setuptools` as a dependency or removing the use of `distutils`. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior or constraints for the solution (e.g., whether compatibility with older Python versions must be maintained). Additionally, while the issue with `setup.cfg` and missing `drivers` during installation is mentioned, it is not clear whether this is a separate problem to solve or just context for the main issue. The problem statement also lacks explicit mention of edge cases or specific testing requirements for the solution. Overall, it is mostly clear but could benefit from additional precision regarding the solution's requirements and scope.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range. Here's the reasoning based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes focus on adding support for `Valkey` as a new driver in `pifpaf`, which involves creating a new file (`valkey.py`) with a moderate amount of code (around 90 lines) and updating `setup.cfg` and `README.rst`. However, the core issue described in the problem statement (dependency on `distutils` and `pkg_resources`) is not directly addressed in the code changes provided. Solving the dependency issue would likely require minimal changes, such as adding `setuptools` as a dependency in the project's configuration or refactoring small parts of the code to remove reliance on `distutils`. The changes are localized and do not appear to impact the broader system architecture significantly. However, addressing the `setup.cfg` issue (ensuring `drivers` are installed correctly) might require understanding Python packaging mechanisms, which adds a slight layer of complexity.\n\n2. **Number of Technical Concepts**: Solving the dependency issue requires knowledge of Python's packaging ecosystem, including virtual environments, `setuptools`, and changes in Python 3.12. It also involves understanding how `distutils` and `pkg_resources` are used in the codebase and finding alternatives or ensuring compatibility. For the `Valkey` driver addition (as shown in the code changes), concepts like subprocess management, configuration file handling, and environment variable setup are needed, which are moderately complex but standard in Python development. No advanced algorithms or design patterns are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but potential issues include ensuring compatibility across different Python versions (e.g., maintaining support for Python < 3.12 while fixing issues for 3.12+), handling environments where `setuptools` cannot be installed, and ensuring the `drivers` installation issue is resolved for various installation methods (e.g., PyPI, source). Error handling for the `Valkey` driver might involve managing failures in starting the server or sentinel processes, which is reflected in the code changes but not overly complex.\n\n4. **Overall Complexity**: The dependency issue is relatively straightforward to fix (adding a dependency or minor refactoring), but the additional context about packaging issues (`setup.cfg` not installing `drivers`) introduces a moderate challenge that requires understanding Python's build and installation tools. The `Valkey` driver implementation is a standard feature addition with moderate complexity due to process management and configuration. The problem does not require deep architectural changes or advanced technical knowledge beyond typical Python development skills.\n\nGiven these factors, the problem falls into the medium difficulty range. It requires understanding multiple concepts (Python packaging, dependency management, and driver implementation) and making changes across a few files, but it does not involve significant architectural impact or highly complex logic. A score of 0.45 reflects this balance between straightforward fixes and moderate technical challenges.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "test optimization: reuse authentication state in frontend-tests\n### Rationale / Begr\u00fcndung\r\n\r\nThe playwright test cases for the frontend log in for every test scenario. This should be optimized to reuse the authentication state.\r\n\r\n### Affected\r\n\r\nDevelopers\r\n\r\n### Minimal functionality\r\n\r\nA test case, that starts with an already logged in user.\r\n\r\n### References / Verweise\r\n\r\n* #769 \r\n* https://playwright.dev/python/docs/auth\r\n\nci: update build-wheel job\n## Description\r\n\r\nThis PR updates the `build-wheel` job.\r\n\r\nI think it is better to not install rdmo[dev] to build the wheel and then force-reinstall. Installing just build and then installing \"fresh\" from the wheel, is much better for testing the wheel. Also it is much faster to not install rdmo in this step.\r\n\r\nAlso the output in the actions summary is now more verbose and includes a list of files in the wheel.\r\n\r\nThe Changes are:\r\n\r\n- install build and twine, not rdmo[dev] to build the wheel\r\n- build wheel and sdist\r\n- check metadata of wheel and sdist\r\n- write the output once, not step by step\r\n- output in the actions summary is now more verbose and includes a list of files in the wheel\r\n\r\n## Types of Changes\r\n- [x] Other (please describe):\r\n\r\n## Checklist\r\n- [x] I have read the [contributor guide](https://github.com/rdmorganiser/rdmo/blob/main/CONTRIBUTING.md).\n", "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex 921c235d4c..905ee415d2 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -38,14 +38,75 @@ jobs:\n     # Ref: https://github.com/rdmorganiser/.github/blob/main/.github/workflows/_lint.yml\n     uses: rdmorganiser/.github/.github/workflows/_lint.yml@main\n \n+  build-wheel:\n+    name: Build python wheel\n+    runs-on: ubuntu-24.04\n+    steps:\n+    - uses: actions/checkout@v4\n+    # update the version\n+    - name: Get short commit SHA\n+      run: |\n+        if [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n+          SHA=\"${{ github.event.pull_request.head.sha }}\"\n+        else\n+          SHA=\"${{ github.sha }}\"\n+        fi\n+        echo \"SHA=$(git rev-parse --short $SHA)\" >> $GITHUB_ENV\n+    - name: Get current version (MAJOR.MINOR.PATCH)\n+      id: current-version\n+      run: echo \"current_version=$(grep -Po '(?<=__version__ = \")[\\d\\w.]+(?=\")' rdmo/__init__.py)\" >> $GITHUB_OUTPUT\n+    - name: Generate new version (current version + SHA)\n+      id: new-version\n+      run: echo \"new_version=${{ steps.current-version.outputs.current_version }}+$SHA\" >> $GITHUB_OUTPUT\n+    - name: Update version in rdmo/__init__.py\n+      run: |\n+        sed -i \"s/__version__ = .*/__version__ = \\\"${{ steps.new-version.outputs.new_version }}\\\"/\" rdmo/__init__.py\n+    # build the webpack bundle\n+    - uses: actions/setup-node@v4\n+      with:\n+        node-version: 18\n+        cache: npm\n+    - run: npm install && npm run build:prod\n+    # build the wheel\n+    - uses: actions/setup-python@v5\n+      with:\n+        python-version: '3.12'\n+        cache: pip\n+    - run: |\n+        python -m pip install --upgrade pip build[uv] twine\n+        python -m pip --version\n+    - name: Build the wheel\n+      run: python -m build --installer=uv\n+    - name: Check the metadata of wheel and sdist\n+      run: python -m twine check --strict dist/*\n+    - name: Install package from built wheel\n+      run: python -m pip install --no-compile dist/rdmo*.whl # do not create __pycache__/*.pyc files\n+    - name: Write info to step summary\n+      run: |\n+          {\n+            echo -e \"# \u2713 Wheel successfully built (v${{ steps.new-version.outputs.new_version }})\\n\\n\"\n+            echo '<details><summary>Information about installed wheel</summary>'\n+            echo -e \"\\n\\`\\`\\`console\"\n+            echo \"$ python -m pip show --files --verbose rdmo\"\n+            python -m pip show --files --verbose rdmo\n+            echo -e \"\\`\\`\\`\\n</details>\"\n+          } >> $GITHUB_STEP_SUMMARY\n+    - name: Upload wheel as artifact\n+      uses: actions/upload-artifact@v4\n+      with:\n+        name: wheel\n+        path: dist/rdmo*.whl\n+        if-no-files-found: error\n+        retention-days: 30\n+\n   test:\n+    name: \"Test (Python: ${{ matrix.python-version }}, DB: ${{ matrix.db-backend }})\"\n+    needs: build-wheel\n     runs-on: ubuntu-24.04\n     strategy:\n       matrix:\n         python-version: ['3.8', '3.12']\n         db-backend: [mysql, postgres]\n-    name: \"Test (Python: ${{ matrix.python-version }}, DB: ${{ matrix.db-backend }})\"\n-    needs: lint\n     steps:\n     - uses: actions/checkout@v4\n     - name: Set up Python ${{ matrix.python-version }}\n@@ -53,32 +114,35 @@ jobs:\n       with:\n         python-version: ${{ matrix.python-version }}\n         cache: pip\n+    - name: Download wheel\n+      uses: actions/download-artifact@v4\n+      with:\n+        name: wheel\n+        path: dist\n     - name: Install Dependencies\n       run: |\n-        sudo apt update\n-        sudo apt install --yes pandoc texlive-xetex librsvg2-bin\n-        python -m pip install --upgrade pip\n+        sudo apt-get update && sudo apt-get install --yes pandoc texlive-xetex librsvg2-bin\n         pandoc --version\n-    - name: Install rdmo[mysql] and start mysql\n+        python -m pip install --upgrade pip\n+        python -m pip --version\n+    - name: Install rdmo[mysql] from wheel and start mysql\n       run: |\n-        python -m pip install --editable .[ci,mysql]\n+        python -m pip install \"$(ls dist/*.whl)[ci,mysql]\"\n         sudo systemctl start mysql.service\n       if: matrix.db-backend == 'mysql'\n-    - name: Install rdmo[postgres] and start postgresql\n+    - name: Install rdmo[postgres] from wheel and start postgresql\n       run: |\n-        python -m pip install --editable .[ci,postgres]\n+        python -m pip install \"$(ls dist/*.whl)[ci,postgres]\"\n         sudo systemctl start postgresql.service\n         pg_isready\n         sudo -u postgres psql --command=\"CREATE USER postgres_user PASSWORD 'postgres_password' CREATEDB\"\n       if: matrix.db-backend == 'postgres'\n     - name: Prepare Env\n       run: |\n-        cp -r testing/media testing/media_root\n-        mkdir testing/log\n+        cp -r testing/media testing/media_root && mkdir testing/log\n     - name: Run package status tests first\n       run: |\n-        pytest rdmo/core/tests/test_package_status.py::test_package_json_and_pre_commit_versions_match \\\n-        --nomigrations --verbose\n+        pytest rdmo/core/tests/test_package_status.py --nomigrations --verbose\n       if: matrix.python-version == '3.12' && matrix.db-backend == 'postgres'\n     - name: Run Tests\n       run: |\n@@ -90,22 +154,48 @@ jobs:\n       with:\n         flag-name: '${{ matrix.db-backend }}: ${{ matrix.python-version }}'\n         parallel: true\n-    # end-to-end tests\n-    - uses: actions/setup-node@v4\n+\n+  test-e2e:\n+    name: \"End-to-end Test (Python: ${{ matrix.python-version }}, DB: ${{ matrix.db-backend }})\"\n+    needs: build-wheel\n+    runs-on: ubuntu-24.04\n+    strategy:\n+      matrix:\n+        python-version: ['3.12']\n+        db-backend: [postgres]\n+    steps:\n+    - uses: actions/checkout@v4\n+    - name: Set up Python ${{ matrix.python-version }}\n+      uses: actions/setup-python@v5\n       with:\n-        node-version: 18\n-        cache: npm\n-      if: matrix.python-version == '3.12' && matrix.db-backend == 'postgres'\n+        python-version: ${{ matrix.python-version }}\n+        cache: pip\n+    - name: Download wheel\n+      uses: actions/download-artifact@v4\n+      with:\n+        name: wheel\n+        path: dist\n+    - name: Install Dependencies\n+      run: |\n+        sudo apt-get update && sudo apt install --yes pandoc texlive-xetex librsvg2-bin\n+        python -m pip install --upgrade pip\n+    - name: Install rdmo[postgres] from wheel and start postgresql\n+      run: |\n+        python -m pip install \"$(ls dist/*.whl)[ci,postgres]\"\n+        sudo systemctl start postgresql.service\n+        pg_isready\n+        sudo -u postgres psql --command=\"CREATE USER postgres_user PASSWORD 'postgres_password' CREATEDB\"\n+    - name: Prepare Env\n+      run: |\n+        cp -r testing/media testing/media_root && mkdir testing/log\n     - name: Install e2e tests dependencies\n       run: |\n-        npm install\n-        npm run build:prod\n-        playwright install chromium\n-      if: matrix.python-version == '3.12' && matrix.db-backend == 'postgres'\n+        playwright install --with-deps chromium\n     - run: mkdir screenshots\n+    - name: Collect static files into static root (only required if rdmo is installed from wheel)\n+      run: python testing/manage.py collectstatic --noinput\n     - name: Run end-to-end tests\n       run: pytest -p randomly -p no:cacheprovider --reuse-db --numprocesses=auto --dist=loadscope -m e2e --nomigrations\n-      if: matrix.python-version == '3.12' && matrix.db-backend == 'postgres'\n       env:\n         DJANGO_DEBUG: True\n         GITHUB_DB_BACKEND: ${{ matrix.db-backend }}\n@@ -113,7 +203,6 @@ jobs:\n       with:\n         name: screenshots\n         path: screenshots/*.png\n-      if: matrix.python-version == '3.12' && matrix.db-backend == 'postgres'\n \n   coveralls:\n     name: Indicate completion to coveralls\n@@ -126,68 +215,6 @@ jobs:\n       with:\n         parallel-finished: true\n \n-  build-wheel:\n-    name: Build python wheel\n-    needs: test\n-    runs-on: ubuntu-24.04\n-    steps:\n-    - uses: actions/checkout@v4\n-    # update the version\n-    - name: Get short commit SHA\n-      run: |\n-        if [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n-          SHA=\"${{ github.event.pull_request.head.sha }}\"\n-        else\n-          SHA=\"${{ github.sha }}\"\n-        fi\n-        echo \"SHA=$(git rev-parse --short $SHA)\" >> $GITHUB_ENV\n-    - name: Get current version (MAJOR.MINOR.PATCH)\n-      id: current-version\n-      run: echo \"current_version=$(grep -Po '(?<=__version__ = \")[\\d\\w.]+(?=\")' rdmo/__init__.py)\" >> $GITHUB_OUTPUT\n-    - name: Generate new version (current version + SHA)\n-      id: new-version\n-      run: echo \"new_version=${{ steps.current-version.outputs.current_version }}+$SHA\" >> $GITHUB_OUTPUT\n-    - name: Update version in rdmo/__init__.py\n-      run: |\n-        sed -i \"s/__version__ = .*/__version__ = \\\"${{ steps.new-version.outputs.new_version }}\\\"/\" rdmo/__init__.py\n-    # build the webpack bundle\n-    - uses: actions/setup-node@v4\n-      with:\n-        node-version: 18\n-        cache: npm\n-    - run: npm install && npm run build:prod\n-    # build the wheel\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n-        cache: pip\n-    - run: |\n-        python -m pip install --upgrade pip build[uv] twine\n-        pip --version\n-    - name: Build the wheel\n-      run: python -m build --installer=uv\n-    - name: Check the metadata of wheel and sdist\n-      run: python -m twine check --strict dist/*\n-    - name: Install package from built wheel\n-      run: python -m pip install --no-compile dist/rdmo*.whl # do not create __pycache__/*.pyc files\n-    - name: Write info to step summary\n-      run: |\n-          {\n-            echo -e \"# \u2713 Wheel successfully built (v${{ steps.new-version.outputs.new_version }})\\n\\n\"\n-            echo '<details><summary>Information about installed wheel</summary>'\n-            echo -e \"\\n\\`\\`\\`console\"\n-            echo \"$ python -m pip show --files --verbose rdmo\"\n-            python -m pip show --files --verbose rdmo\n-            echo -e \"\\`\\`\\`\\n</details>\"\n-          } >> $GITHUB_STEP_SUMMARY\n-    - name: Upload wheel as artifact\n-      uses: actions/upload-artifact@v4\n-      with:\n-        name: wheel\n-        path: dist/rdmo*.whl\n-        if-no-files-found: error\n-        retention-days: 30\n-\n   dev-setup:\n     # Ref: structlog (MIT licensed) <https://github.com/hynek/structlog/blob/main/.github/workflows/ci.yml>\n     name: \"Test dev setup on ${{ matrix.os }}\"\n@@ -206,6 +233,7 @@ jobs:\n \n   dependencies:\n     name: Test installation of all dependencies\n+    needs: build-wheel\n     runs-on: ubuntu-24.04\n     steps:\n       - uses: actions/checkout@v4\n@@ -213,12 +241,18 @@ jobs:\n         with:\n           python-version: \"3.12\"\n           cache: pip\n+      - name: Download wheel\n+        uses: actions/download-artifact@v4\n+        with:\n+          name: wheel\n+          path: dist\n       - name: Install os requirements for python-ldap\n-        run: |\n-          sudo apt update\n-          sudo apt install --yes libldap2-dev libsasl2-dev\n+        run: sudo apt-get update && sudo apt-get install --yes libldap2-dev libsasl2-dev\n       - run: python -m pip install --upgrade pip\n-      - run: python -m pip install .[allauth,ci,dev,gunicorn,ldap,mysql,postgres,pytest]\n+      - name: Install rdmo wheel with all optional dependency groups\n+        run: python -m pip install --no-compile \"$(ls dist/*.whl)[allauth,ci,dev,gunicorn,ldap,mysql,postgres,pytest]\"\n+      - name: Verify installed packages have compatible dependencies\n+        run: python -m pip check\n       - uses: actions/setup-node@v4\n         with:\n           node-version: 18\n@@ -254,9 +288,10 @@ jobs:\n     if: always()\n     needs:\n       - lint\n+      - build-wheel\n       - test\n       - coveralls\n-      - build-wheel\n+      - test-e2e\n       - dev-setup\n       - dependencies\n     runs-on: ubuntu-24.04\n", "instance_id": "rdmorganiser__rdmo-1061", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear but lacks critical details in certain areas. For the test optimization part (\"reuse authentication state in frontend-tests\"), the goal is understandable: optimize Playwright test cases by reusing authentication state to avoid logging in for every test scenario. However, it misses specifics such as how the authentication state should be reused (e.g., via cookies, tokens, or session storage), the exact scope of tests affected, and any constraints or potential risks (e.g., test isolation issues). The reference to Playwright documentation is helpful but does not fully compensate for the lack of detailed requirements or examples. For the CI update part (\"update build-wheel job\"), the description is clearer, with explicit changes listed (e.g., not installing rdmo[dev], building wheel and sdist, verbose output). However, the rationale for why these changes are \"better\" is somewhat vague, and there are no mentions of potential edge cases or failure scenarios. Overall, while the intent and high-level changes are comprehensible, minor ambiguities and missing details prevent a perfect clarity score.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range, as it involves a mix of straightforward and moderately complex tasks. Let's break it down by the factors considered:\n\n1. **Clarity and Complexity of Problem Description**: As noted, the problem description has some ambiguities, particularly for the test optimization part. Understanding the goal requires some inference, which adds a slight cognitive load, though the CI update part is more straightforward.\n\n2. **Scope and Depth of Code Changes**: The code changes primarily affect the CI workflow file (`.github/workflows/ci.yml`), which is a single file, but the modifications are extensive. The changes include restructuring the `build-wheel` job, updating dependencies, modifying test jobs to use the built wheel, and adding a dedicated end-to-end test job. While the changes are mostly confined to one file, they impact the overall build and test pipeline, requiring an understanding of how different jobs (e.g., `build-wheel`, `test`, `test-e2e`) interact within GitHub Actions. The amount of code change is significant (over 100 lines modified), but it does not fundamentally alter the system's architecture.\n\n3. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including GitHub Actions workflows (e.g., job dependencies, artifacts, matrix testing), Python packaging (e.g., building wheels and sdist with `build` and `twine`), Playwright for end-to-end testing (specifically authentication state management, though not directly implemented in the provided diff), and shell scripting for version updates. These concepts are moderately complex for someone unfamiliar with CI/CD pipelines or Python packaging, but they are well-documented and commonly used in modern development workflows.\n\n4. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases for the test optimization part, such as handling expired authentication states or ensuring test isolation. For the CI update, potential issues like build failures, artifact download errors, or version conflicts are not addressed in the description, though the code includes some safeguards (e.g., `if-no-files-found: error`). Implementing robust error handling in the CI pipeline could add complexity, but the provided changes do not delve deeply into this.\n\nOverall, the difficulty is medium because while the changes are extensive and require understanding multiple technical concepts (GitHub Actions, Python packaging, testing frameworks), they do not involve deep architectural changes or highly complex logic. The test optimization part, though not fully detailed in the code diff, hints at additional complexity (e.g., managing authentication state in Playwright), but based on the provided changes, the focus is more on CI pipeline updates, which are manageable with moderate experience. A score of 0.45 reflects the balance between the breadth of changes and the moderate technical depth required.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "top_k is slow and uses a lot of memory\n### Checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\r\n\r\n### Reproducible example\r\n\r\nThe issue is best shown with quite large data, so I've made a dummy 500 million record dataset \r\n```python\r\nimport numpy as np\r\nimport polars as pl\r\n\r\nnp.random.seed(43)\r\n\r\nTOTAL_ROWS = 500_000_000\r\nINITIAL_ROWS = int(TOTAL_ROWS ** 0.5)\r\n\r\n# Create the cross product of two smaller datasets\r\nlf_a = pl.LazyFrame({'a': np.random.randint(0, 1_000_000, size=INITIAL_ROWS)})\r\nlf_b = pl.LazyFrame({'b': np.random.randint(0, 1_000_000, size=INITIAL_ROWS)})\r\ncross_product = lf_a.join(lf_b, how=\"cross\")\r\n\r\n# Sink into a parquet file\r\ncross_product.sink_parquet(\"data.parquet\", compression=\"snappy\")\r\n```\r\n\r\nSelect the top 5 `a` values\r\n\r\n```python\r\nimport polars as pl\r\nres = pl.scan_parquet(\"data.parquet\").select(\"a\").top_k(5, by='a').collect()\r\nprint(res)\r\n```\r\n\r\nFind mean of `a`\r\n\r\n```python\r\nimport polars as pl\r\nres = pl.scan_parquet(\"data.parquet\").select(\"a\").mean().collect()\r\nprint(res)\r\n```\r\n\r\nSort `a`\r\n\r\n```python\r\nimport polars as pl\r\nres = pl.scan_parquet(\"data.parquet\").select(\"a\").sort(by=\"a\").collect()\r\nprint(res)\r\n```\r\n\r\nRead `a`\r\n\r\n```python\r\nimport polars as pl\r\ndf = pl.scan_parquet(\"data.parquet\").select(\"a\").collect()\r\nprint(df)\r\n```\r\n\r\n### Log output\r\n\r\n\r\n> [!NOTE]\r\n> I have not set `POLARS_VERBOSE=1` in these outputs as it does not write anything to stderr in this case and potentially interferes with performance.\r\n\r\nThe logs are edited for clarity (removed extra lines and converted memory to GB).\r\n\r\n```shell\r\n/usr/bin/time -lh python top_5.py\r\nshape: (5, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a      \u2502\r\n\u2502 ---    \u2502\r\n\u2502 i64    \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\t20.87s real\t\t5.22s user\t\t11.11s sys\r\n        28.7 GB  peak memory footprint\r\n```\r\n\r\n```shell\r\n/usr/bin/time -lh python mean.py\r\nshape: (1, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a             \u2502\r\n\u2502 ---           \u2502\r\n\u2502 f64           \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 502017.772764 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\t0.56s real\t\t0.40s user\t\t1.15s sys\r\n        4.2 GB peak memory footprint\r\n```\r\n\r\n```shell\r\n/usr/bin/time -lh python sort.py\r\nshape: (499_969_600, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a      \u2502\r\n\u2502 ---    \u2502\r\n\u2502 i64    \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 19     \u2502\r\n\u2502 19     \u2502\r\n\u2502 19     \u2502\r\n\u2502 19     \u2502\r\n\u2502 19     \u2502\r\n\u2502 \u2026      \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\t3.38s real\t\t8.32s user\t\t1.94s sys\r\n        12.2 GB peak memory footprint\r\n```\r\n\r\n```shell\r\n/usr/bin/time -lh python read_a.py\r\nshape: (499_969_600, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a      \u2502\r\n\u2502 ---    \u2502\r\n\u2502 i64    \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 276292 \u2502\r\n\u2502 276292 \u2502\r\n\u2502 276292 \u2502\r\n\u2502 276292 \u2502\r\n\u2502 276292 \u2502\r\n\u2502 \u2026      \u2502\r\n\u2502 624089 \u2502\r\n\u2502 624089 \u2502\r\n\u2502 624089 \u2502\r\n\u2502 624089 \u2502\r\n\u2502 624089 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\t0.39s real\t\t0.29s user\t\t1.06s sys\r\n        4.2 GB  peak memory footprint\r\n```\r\n\r\nEquivalent to top_5 in duckdb\r\n\r\n```shell\r\n/usr/bin/time -lh duckdb -c \"select a from 'data.parquet' order by a desc limit 5;\"\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   a    \u2502\r\n\u2502 int64  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2502 999938 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\t0.58s real\t\t2.04s user\t\t1.54s sys\r\n        0.026 GB peak memory footprint\r\n```\r\n\r\n### Issue description\r\n\r\nTop k uses a lot more memory and takes much longer than expected. The large memory use means I get OOM errors and my actual queries fail to execute with polars.\r\n\r\n### Expected behavior\r\n\r\nAccording to the the [top_k docs](https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.top_k.html) the time complexity is $O(n)$, so I would expect it to perform like `.mean`. To be honest, I don't see how top_k is able to perform in $O(n)$. In algorithms where you keep the top k contenders in a heap, I believe you have $O(n \\cdot log(k))$ time. But as shown above, the runtime is much worse than sorting the whole `a` column, so I'm not sure what is going on.\r\n\r\nFor memory use, I was really hoping that top_k would scale with the size of k. I have a large dataset where it is not feasible to load the whole thing into memory. It would be a great addition if polars could make optimizations like that. I have tried to enable the `collect(streaming=True)` flag, but that does not seem to affect the memory use of this query.\r\n\r\nI included the max and count stats here, as that is probably related to improving the top_k performance. I would not expect a simple max and count to load the whole dataset into memory. For those two statistics, all the information is actually available in the rowgroup metadata so there are opportunities for optimizing them much further.\r\n\r\n### Installed versions\r\n\r\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.9.0\r\nIndex type:          UInt32\r\nPlatform:            macOS-14.7-arm64-arm-64bit\r\nPython:              3.12.3 (main, Aug  1 2024, 12:12:15) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  <not installed>\r\naltair               <not installed>\r\ncloudpickle          <not installed>\r\nconnectorx           <not installed>\r\ndeltalake            <not installed>\r\nfastexcel            <not installed>\r\nfsspec               <not installed>\r\ngevent               <not installed>\r\ngreat_tables         <not installed>\r\nmatplotlib           <not installed>\r\nnest_asyncio         <not installed>\r\nnumpy                2.1.2\r\nopenpyxl             <not installed>\r\npandas               <not installed>\r\npyarrow              17.0.0\r\npydantic             <not installed>\r\npyiceberg            <not installed>\r\nsqlalchemy           <not installed>\r\ntorch                <not installed>\r\nxlsx2csv             <not installed>\r\nxlsxwriter           <not installed>\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/crates/polars-core/src/chunked_array/ops/sort/arg_bottom_k.rs b/crates/polars-core/src/chunked_array/ops/sort/arg_bottom_k.rs\nindex 7f257f23f59e..7787ef28076f 100644\n--- a/crates/polars-core/src/chunked_array/ops/sort/arg_bottom_k.rs\n+++ b/crates/polars-core/src/chunked_array/ops/sort/arg_bottom_k.rs\n@@ -39,6 +39,11 @@ pub fn _arg_bottom_k(\n     _broadcast_bools(by_column.len(), &mut sort_options.descending);\n     _broadcast_bools(by_column.len(), &mut sort_options.nulls_last);\n \n+    // Don't go into row encoding.\n+    if by_column.len() == 1 && sort_options.limit.is_some() && !sort_options.maintain_order {\n+        return Ok(NoNull::new(by_column[0].arg_sort((&*sort_options).into())));\n+    }\n+\n     let encoded = _get_rows_encoded(\n         by_column,\n         &sort_options.descending,\ndiff --git a/crates/polars-core/src/chunked_array/ops/sort/arg_sort.rs b/crates/polars-core/src/chunked_array/ops/sort/arg_sort.rs\nindex ca34d37318a7..4f9a1ff9e9b3 100644\n--- a/crates/polars-core/src/chunked_array/ops/sort/arg_sort.rs\n+++ b/crates/polars-core/src/chunked_array/ops/sort/arg_sort.rs\n@@ -18,7 +18,7 @@ pub(super) fn arg_sort<I, J, T>(\n     iters: I,\n     options: SortOptions,\n     null_count: usize,\n-    len: usize,\n+    mut len: usize,\n ) -> IdxCa\n where\n     I: IntoIterator<Item = J>,\n@@ -49,14 +49,46 @@ where\n         vals.extend(iter);\n     }\n \n-    sort_impl(vals.as_mut_slice(), options);\n+    let vals = if let Some((limit, desc)) = options.limit {\n+        let limit = limit as usize;\n+        // Overwrite output len.\n+        len = limit;\n+        let out = if limit >= vals.len() {\n+            vals.as_mut_slice()\n+        } else if desc {\n+            let (lower, _el, _upper) = vals\n+                .as_mut_slice()\n+                .select_nth_unstable_by(limit, |a, b| b.1.tot_cmp(&a.1));\n+            lower\n+        } else {\n+            let (lower, _el, _upper) = vals\n+                .as_mut_slice()\n+                .select_nth_unstable_by(limit, |a, b| a.1.tot_cmp(&b.1));\n+            lower\n+        };\n+\n+        sort_impl(out, options);\n+        out\n+    } else {\n+        sort_impl(vals.as_mut_slice(), options);\n+        vals.as_slice()\n+    };\n \n-    let iter = vals.into_iter().map(|(idx, _v)| idx);\n+    let iter = vals.iter().map(|(idx, _v)| idx).copied();\n     let idx = if nulls_last {\n         let mut idx = Vec::with_capacity(len);\n         idx.extend(iter);\n-        idx.extend(nulls_idx);\n+\n+        let nulls_idx = if options.limit.is_some() {\n+            &nulls_idx[..len - idx.len()]\n+        } else {\n+            &nulls_idx\n+        };\n+        idx.extend_from_slice(nulls_idx);\n         idx\n+    } else if options.limit.is_some() {\n+        nulls_idx.extend(iter.take(len - nulls_idx.len()));\n+        nulls_idx\n     } else {\n         let ptr = nulls_idx.as_ptr() as usize;\n         nulls_idx.extend(iter);\n@@ -90,9 +122,29 @@ where\n         }));\n     }\n \n-    sort_impl(vals.as_mut_slice(), options);\n+    let vals = if let Some((limit, desc)) = options.limit {\n+        let limit = limit as usize;\n+        let out = if limit >= vals.len() {\n+            vals.as_mut_slice()\n+        } else if desc {\n+            let (lower, _el, _upper) = vals\n+                .as_mut_slice()\n+                .select_nth_unstable_by(limit, |a, b| b.1.tot_cmp(&a.1));\n+            lower\n+        } else {\n+            let (lower, _el, _upper) = vals\n+                .as_mut_slice()\n+                .select_nth_unstable_by(limit, |a, b| a.1.tot_cmp(&b.1));\n+            lower\n+        };\n+        sort_impl(out, options);\n+        out\n+    } else {\n+        sort_impl(vals.as_mut_slice(), options);\n+        vals.as_slice()\n+    };\n \n-    let iter = vals.into_iter().map(|(idx, _v)| idx);\n+    let iter = vals.iter().map(|(idx, _v)| idx).copied();\n     let idx: Vec<_> = iter.collect_trusted();\n \n     ChunkedArray::with_chunk(name, IdxArr::from_data_default(Buffer::from(idx), None))\ndiff --git a/crates/polars-core/src/chunked_array/ops/sort/categorical.rs b/crates/polars-core/src/chunked_array/ops/sort/categorical.rs\nindex 5dd71a7b1eb8..c89e0790f251 100644\n--- a/crates/polars-core/src/chunked_array/ops/sort/categorical.rs\n+++ b/crates/polars-core/src/chunked_array/ops/sort/categorical.rs\n@@ -53,6 +53,7 @@ impl CategoricalChunked {\n             descending,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         })\n     }\n \ndiff --git a/crates/polars-core/src/chunked_array/ops/sort/mod.rs b/crates/polars-core/src/chunked_array/ops/sort/mod.rs\nindex 727f2ace15a8..add7e8b696a4 100644\n--- a/crates/polars-core/src/chunked_array/ops/sort/mod.rs\n+++ b/crates/polars-core/src/chunked_array/ops/sort/mod.rs\n@@ -335,6 +335,7 @@ impl ChunkSort<StringType> for StringChunked {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         })\n     }\n \n@@ -406,6 +407,7 @@ impl ChunkSort<BinaryType> for BinaryChunked {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         })\n     }\n \n@@ -536,6 +538,7 @@ impl ChunkSort<BinaryOffsetType> for BinaryOffsetChunked {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         })\n     }\n \n@@ -672,6 +675,7 @@ impl ChunkSort<BooleanType> for BooleanChunked {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         })\n     }\n \n@@ -797,6 +801,7 @@ mod test {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n         assert_eq!(\n             Vec::from(&out),\n@@ -816,6 +821,7 @@ mod test {\n             nulls_last: true,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n         assert_eq!(\n             Vec::from(&out),\n@@ -925,6 +931,7 @@ mod test {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n         let expected = &[None, None, Some(\"a\"), Some(\"b\"), Some(\"c\")];\n         assert_eq!(Vec::from(&out), expected);\n@@ -934,6 +941,7 @@ mod test {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n \n         let expected = &[None, None, Some(\"c\"), Some(\"b\"), Some(\"a\")];\n@@ -944,6 +952,7 @@ mod test {\n             nulls_last: true,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n         let expected = &[Some(\"a\"), Some(\"b\"), Some(\"c\"), None, None];\n         assert_eq!(Vec::from(&out), expected);\n@@ -953,6 +962,7 @@ mod test {\n             nulls_last: true,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         });\n         let expected = &[Some(\"c\"), Some(\"b\"), Some(\"a\"), None, None];\n         assert_eq!(Vec::from(&out), expected);\ndiff --git a/crates/polars-core/src/chunked_array/ops/sort/options.rs b/crates/polars-core/src/chunked_array/ops/sort/options.rs\nindex 046d0b251b04..95bff0b1b47a 100644\n--- a/crates/polars-core/src/chunked_array/ops/sort/options.rs\n+++ b/crates/polars-core/src/chunked_array/ops/sort/options.rs\n@@ -41,6 +41,10 @@ pub struct SortOptions {\n     /// If true maintain the order of equal elements.\n     /// Default `false`.\n     pub maintain_order: bool,\n+    /// Limit a sort output, this is for optimization purposes and might be ignored.\n+    /// - Len\n+    /// - Descending\n+    pub limit: Option<(IdxSize, bool)>,\n }\n \n /// Sort options for multi-series sorting.\n@@ -96,6 +100,10 @@ pub struct SortMultipleOptions {\n     pub multithreaded: bool,\n     /// Whether maintain the order of equal elements. Default `false`.\n     pub maintain_order: bool,\n+    /// Limit a sort output, this is for optimization purposes and might be ignored.\n+    /// - Len\n+    /// - Descending\n+    pub limit: Option<(IdxSize, bool)>,\n }\n \n impl Default for SortOptions {\n@@ -105,6 +113,7 @@ impl Default for SortOptions {\n             nulls_last: false,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         }\n     }\n }\n@@ -116,6 +125,7 @@ impl Default for SortMultipleOptions {\n             nulls_last: vec![false],\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         }\n     }\n }\n@@ -224,6 +234,7 @@ impl From<&SortOptions> for SortMultipleOptions {\n             nulls_last: vec![value.nulls_last],\n             multithreaded: value.multithreaded,\n             maintain_order: value.maintain_order,\n+            limit: value.limit,\n         }\n     }\n }\n@@ -235,6 +246,7 @@ impl From<&SortMultipleOptions> for SortOptions {\n             nulls_last: value.nulls_last.first().copied().unwrap_or(false),\n             multithreaded: value.multithreaded,\n             maintain_order: value.maintain_order,\n+            limit: value.limit,\n         }\n     }\n }\ndiff --git a/crates/polars-core/src/frame/mod.rs b/crates/polars-core/src/frame/mod.rs\nindex e3b969a81756..7e2d7b050dcf 100644\n--- a/crates/polars-core/src/frame/mod.rs\n+++ b/crates/polars-core/src/frame/mod.rs\n@@ -1989,6 +1989,12 @@ impl DataFrame {\n             return Ok(out);\n         }\n         if let Some((0, k)) = slice {\n+            let desc = if sort_options.descending.len() == 1 {\n+                sort_options.descending[0]\n+            } else {\n+                false\n+            };\n+            sort_options.limit = Some((k as IdxSize, desc));\n             return self.bottom_k_impl(k, by_column, sort_options);\n         }\n \n@@ -2012,6 +2018,7 @@ impl DataFrame {\n                     nulls_last: sort_options.nulls_last[0],\n                     multithreaded: sort_options.multithreaded,\n                     maintain_order: sort_options.maintain_order,\n+                    limit: sort_options.limit,\n                 };\n                 // fast path for a frame with a single series\n                 // no need to compute the sort indices and then take by these indices\ndiff --git a/crates/polars-expr/src/expressions/sortby.rs b/crates/polars-expr/src/expressions/sortby.rs\nindex 1624d7c9bcd6..fad081cb49ed 100644\n--- a/crates/polars-expr/src/expressions/sortby.rs\n+++ b/crates/polars-expr/src/expressions/sortby.rs\n@@ -160,6 +160,7 @@ fn sort_by_groups_multiple_by(\n                 nulls_last: nulls_last.to_owned(),\n                 multithreaded,\n                 maintain_order,\n+                limit: None,\n             };\n \n             let sorted_idx = groups[0]\n@@ -180,6 +181,7 @@ fn sort_by_groups_multiple_by(\n                 nulls_last: nulls_last.to_owned(),\n                 multithreaded,\n                 maintain_order,\n+                limit: None,\n             };\n             let sorted_idx = groups[0]\n                 .as_materialized_series()\ndiff --git a/crates/polars-ops/src/chunked_array/top_k.rs b/crates/polars-ops/src/chunked_array/top_k.rs\nindex 9caf861b6cd9..ef37e267c10f 100644\n--- a/crates/polars-ops/src/chunked_array/top_k.rs\n+++ b/crates/polars-ops/src/chunked_array/top_k.rs\n@@ -285,6 +285,7 @@ fn top_k_by_impl(\n         nulls_last: vec![true; by.len()],\n         multithreaded,\n         maintain_order: false,\n+        limit: None,\n     };\n \n     let idx = _arg_bottom_k(k, by, &mut sort_options)?;\ndiff --git a/crates/polars-ops/src/frame/join/hash_join/sort_merge.rs b/crates/polars-ops/src/frame/join/hash_join/sort_merge.rs\nindex fce2f2bf6cf0..95cde8387733 100644\n--- a/crates/polars-ops/src/frame/join/hash_join/sort_merge.rs\n+++ b/crates/polars-ops/src/frame/join/hash_join/sort_merge.rs\n@@ -225,6 +225,7 @@ pub(crate) fn _sort_or_hash_inner(\n                 nulls_last: false,\n                 multithreaded: true,\n                 maintain_order: false,\n+                limit: None,\n             });\n             let s_right = unsafe { s_right.take_unchecked(&sort_idx) };\n             let ids = par_sorted_merge_inner_no_nulls(s_left, &s_right);\n@@ -252,6 +253,7 @@ pub(crate) fn _sort_or_hash_inner(\n                 nulls_last: false,\n                 multithreaded: true,\n                 maintain_order: false,\n+                limit: None,\n             });\n             let s_left = unsafe { s_left.take_unchecked(&sort_idx) };\n             let ids = par_sorted_merge_inner_no_nulls(&s_left, s_right);\n@@ -323,6 +325,7 @@ pub(crate) fn sort_or_hash_left(\n                 nulls_last: false,\n                 multithreaded: true,\n                 maintain_order: false,\n+                limit: None,\n             });\n             let s_right = unsafe { s_right.take_unchecked(&sort_idx) };\n \ndiff --git a/crates/polars-pipe/src/executors/sinks/sort/source.rs b/crates/polars-pipe/src/executors/sinks/sort/source.rs\nindex 1c1fa2984a0e..6f544e8e6ef1 100644\n--- a/crates/polars-pipe/src/executors/sinks/sort/source.rs\n+++ b/crates/polars-pipe/src/executors/sinks/sort/source.rs\n@@ -101,6 +101,7 @@ impl SortSource {\n                     nulls_last: self.nulls_last,\n                     multithreaded: true,\n                     maintain_order: false,\n+                    limit: None,\n                 },\n             ),\n             Some((offset, len)) => {\n@@ -119,6 +120,7 @@ impl SortSource {\n                             nulls_last: self.nulls_last,\n                             multithreaded: true,\n                             maintain_order: false,\n+                            limit: None,\n                         },\n                     );\n                     *len = len.saturating_sub(df_len);\ndiff --git a/crates/polars-python/src/expr/general.rs b/crates/polars-python/src/expr/general.rs\nindex 7125388e88cd..fe5fdafdbbb8 100644\n--- a/crates/polars-python/src/expr/general.rs\n+++ b/crates/polars-python/src/expr/general.rs\n@@ -260,6 +260,7 @@ impl PyExpr {\n                 nulls_last,\n                 multithreaded: true,\n                 maintain_order: false,\n+                limit: None,\n             })\n             .into()\n     }\n@@ -272,6 +273,7 @@ impl PyExpr {\n                 nulls_last,\n                 multithreaded: true,\n                 maintain_order: false,\n+                limit: None,\n             })\n             .into()\n     }\n@@ -349,6 +351,7 @@ impl PyExpr {\n                     nulls_last,\n                     multithreaded,\n                     maintain_order,\n+                    limit: None,\n                 },\n             )\n             .into()\ndiff --git a/crates/polars-python/src/functions/lazy.rs b/crates/polars-python/src/functions/lazy.rs\nindex d3ebb376d10f..1c4e738ea69c 100644\n--- a/crates/polars-python/src/functions/lazy.rs\n+++ b/crates/polars-python/src/functions/lazy.rs\n@@ -75,6 +75,7 @@ pub fn arg_sort_by(\n             nulls_last,\n             multithreaded,\n             maintain_order,\n+            limit: None,\n         },\n     )\n     .into()\ndiff --git a/crates/polars-python/src/lazyframe/general.rs b/crates/polars-python/src/lazyframe/general.rs\nindex fd89884ece82..f9fb740d4cae 100644\n--- a/crates/polars-python/src/lazyframe/general.rs\n+++ b/crates/polars-python/src/lazyframe/general.rs\n@@ -539,6 +539,7 @@ impl PyLazyFrame {\n                 nulls_last: vec![nulls_last],\n                 multithreaded,\n                 maintain_order,\n+                limit: None,\n             },\n         )\n         .into()\n@@ -561,6 +562,7 @@ impl PyLazyFrame {\n                 nulls_last,\n                 maintain_order,\n                 multithreaded,\n+                limit: None,\n             },\n         )\n         .into()\ndiff --git a/crates/polars-python/src/series/general.rs b/crates/polars-python/src/series/general.rs\nindex b14285e77aa0..7312995d7606 100644\n--- a/crates/polars-python/src/series/general.rs\n+++ b/crates/polars-python/src/series/general.rs\n@@ -457,6 +457,7 @@ impl PySeries {\n             nulls_last,\n             multithreaded: true,\n             maintain_order: false,\n+            limit: None,\n         };\n         Ok(self.series.is_sorted(options).map_err(PyPolarsErr::from)?)\n     }\n", "instance_id": "pola-rs__polars-19731", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `top_k` operation in the Polars library, specifically its high memory usage and slow performance compared to other operations like `mean` and `sort`. It provides a reproducible example with a large dataset, performance logs, and comparisons with another library (DuckDB), which helps in understanding the problem's scope. The expected behavior is articulated with references to the documentation and algorithmic complexity expectations (e.g., O(n) vs. O(n log k)). However, there are minor ambiguities: the problem statement does not explicitly define the desired memory complexity or specific constraints for optimization beyond general expectations. Additionally, while edge cases like out-of-memory (OOM) errors are mentioned, there is no detailed discussion on specific scenarios or data distributions that exacerbate the issue. Overall, the statement is valid and clear but lacks some finer details on constraints and edge cases.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules within the Polars library, a high-performance data processing framework written in Rust. The changes primarily focus on optimizing the `top_k` operation by introducing a `limit` parameter to the sorting logic, which suggests a partial sort or selection algorithm (e.g., using `select_nth_unstable_by` for efficiency). This requires a deep understanding of Rust's low-level memory management, sorting algorithms, and Polars' internal architecture, including how data is chunked and processed in a streaming or lazy evaluation context. \n\nSecond, the number of technical concepts involved is substantial: knowledge of sorting algorithms (e.g., partial sorting vs. full sorting), Rust's ownership and borrowing model, performance optimization techniques, and Polars-specific abstractions like `ChunkedArray` and `DataFrame`. Additionally, the problem touches on domain-specific knowledge of data processing libraries and their performance characteristics with large datasets.\n\nThird, the changes impact core functionality (sorting and top-k operations), which are critical to the library's performance and correctness. Implementing an optimization like partial sorting requires careful handling of edge cases, such as null values, descending/ascending order, and ensuring that the `limit` optimization does not break existing behavior in other parts of the codebase. While the problem statement does not explicitly list all edge cases, the code changes suggest considerations for null handling and maintaining order, adding to the complexity.\n\nFinally, the performance implications are significant, as the goal is to reduce memory usage and runtime for large datasets, which involves trade-offs and potential risks of introducing new bugs or regressions. This is not a trivial bug fix or feature addition but a performance optimization in a complex system, requiring both theoretical understanding of algorithms and practical experience with Rust and data processing frameworks. A score of 0.75 reflects the challenging nature of the task, balancing between deep technical requirements and the absence of extreme system-level or distributed system complexities that would push it into the 0.8-1.0 range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add Provider for vi_VN (Vietnamese)\n* Faker version: Any\r\n* OS: Any\r\n\r\nCurrently, the Faker library does not provide support for Vietnamese locale (vi_VN). To improve the localization and make it more useful for Vietnamese users, we should add a provider for Vietnamese names and other locale-specific data.\r\n\r\n### Steps to reproduce\r\n\r\n```\r\nfrom faker import Faker\r\nfake = Faker(\"vi_VN\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe provider should generate realistic Vietnamese names and other locale-specific data, such as:\r\n\r\nVietnamese male and female first names\r\nVietnamese last names\r\nAddresses formatted according to Vietnamese standards\r\n\r\n### Actual behavior\r\n\r\nCurrently, the vi_VN locale does not exist, and attempting to use it results in errors or fallback data.\r\n\n", "patch": "diff --git a/faker/providers/address/vi_VN/__init__.py b/faker/providers/address/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..1bd55ed67e\n--- /dev/null\n+++ b/faker/providers/address/vi_VN/__init__.py\n@@ -0,0 +1,308 @@\n+\"\"\"This module provides address-related functionalities for Vietnamese addresses.\"\"\"\n+\n+from collections import OrderedDict\n+from typing import Optional, Tuple\n+\n+from .. import Provider as AddressProvider\n+\n+\n+class Provider(AddressProvider):\n+    \"\"\"Provider for generating Vietnamese addresses.\n+    Sources:\n+\n+    # https://vi.wikipedia.org/wiki/B%E1%BA%A3n_m%E1%BA%ABu:K%C3%BD_hi%E1%BB%87u_quy_%C6%B0%E1%BB%9Bc_c%C3%A1c_t%E1%BB%89nh_th%C3%A0nh_Vi%E1%BB%87t_Nam\n+    \"\"\"\n+\n+    city_prefixes = (\"Th\u00e0nh ph\u1ed1\", \"Qu\u1eadn\", \"Huy\u1ec7n\", \"Th\u1ecb x\u00e3\")\n+\n+    city_suffixes = (\n+        \"Th\u00e0nh ph\u1ed1\",\n+        \"Qu\u1eadn\",\n+        \"Huy\u1ec7n\",\n+        \"Th\u1ecb x\u00e3\",\n+        \"X\u00e3\",\n+        \"Ph\u01b0\u1eddng\",\n+    )\n+\n+    building_number_formats = (\"###\", \"##\", \"#\")\n+\n+    street_suffixes = (\n+        \"\u0110\u01b0\u1eddng\",\n+        \"Ng\u00f5\",\n+        \"H\u1ebbm\",\n+        \"L\u00e0ng\",\n+        \"Khu\",\n+        \"T\u1ed5\",\n+        \"S\u1ed1\",\n+        \"D\u00e3y\",\n+    )\n+\n+    postcode_formats = (\"######\",)\n+\n+    provinces = (\n+        \"An Giang\",\n+        \"B\u00e0 R\u1ecba \u2013 V\u0169ng T\u00e0u\",\n+        \"B\u1ea1c Li\u00eau\",\n+        \"B\u1eafc K\u1ea1n\",\n+        \"B\u1eafc Giang\",\n+        \"B\u1eafc Ninh\",\n+        \"B\u1ebfn Tre\",\n+        \"B\u00ecnh D\u01b0\u01a1ng\",\n+        \"B\u00ecnh \u0110\u1ecbnh\",\n+        \"B\u00ecnh Ph\u01b0\u1edbc\",\n+        \"B\u00ecnh Thu\u1eadn\",\n+        \"C\u00e0 Mau\",\n+        \"Cao B\u1eb1ng\",\n+        \"C\u1ea7n Th\u01a1\",\n+        \"\u0110\u00e0 N\u1eb5ng\",\n+        \"\u0110\u1eafk L\u1eafk\",\n+        \"\u0110\u1eafk N\u00f4ng\",\n+        \"\u0110i\u1ec7n Bi\u00ean\",\n+        \"\u0110\u1ed3ng Nai\",\n+        \"\u0110\u1ed3ng Th\u00e1p\",\n+        \"Gia Lai\",\n+        \"H\u00e0 Giang\",\n+        \"H\u00e0 Nam\",\n+        \"H\u00e0 N\u1ed9i\",\n+        \"H\u00e0 T\u0129nh\",\n+        \"H\u1ea3i D\u01b0\u01a1ng\",\n+        \"H\u1ea3i Ph\u00f2ng\",\n+        \"H\u1eadu Giang\",\n+        \"H\u00f2a B\u00ecnh\",\n+        \"Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh\",\n+        \"H\u01b0ng Y\u00ean\",\n+        \"Kh\u00e1nh H\u00f2a\",\n+        \"Ki\u00ean Giang\",\n+        \"Kon Tum\",\n+        \"Lai Ch\u00e2u\",\n+        \"L\u1ea1ng S\u01a1n\",\n+        \"L\u00e0o Cai\",\n+        \"L\u00e2m \u0110\u1ed3ng\",\n+        \"Long An\",\n+        \"Nam \u0110\u1ecbnh\",\n+        \"Ngh\u1ec7 An\",\n+        \"Ninh B\u00ecnh\",\n+        \"Ninh Thu\u1eadn\",\n+        \"Ph\u00fa Th\u1ecd\",\n+        \"Ph\u00fa Y\u00ean\",\n+        \"Qu\u1ea3ng B\u00ecnh\",\n+        \"Qu\u1ea3ng Nam\",\n+        \"Qu\u1ea3ng Ng\u00e3i\",\n+        \"Qu\u1ea3ng Ninh\",\n+        \"Qu\u1ea3ng Tr\u1ecb\",\n+        \"S\u00f3c Tr\u0103ng\",\n+        \"S\u01a1n La\",\n+        \"T\u00e2y Ninh\",\n+        \"Th\u00e1i B\u00ecnh\",\n+        \"Th\u00e1i Nguy\u00ean\",\n+        \"Thanh H\u00f3a\",\n+        \"Th\u1eeba Thi\u00ean Hu\u1ebf\",\n+        \"Ti\u1ec1n Giang\",\n+        \"Tr\u00e0 Vinh\",\n+        \"Tuy\u00ean Quang\",\n+        \"V\u0129nh Long\",\n+        \"V\u0129nh Ph\u00fac\",\n+        \"Y\u00ean B\u00e1i\",\n+    )\n+\n+    provinces_abbr = (\n+        \"AG\",\n+        \"BV\",\n+        \"BL\",\n+        \"BK\",\n+        \"BG\",\n+        \"BN\",\n+        \"BT\",\n+        \"BD\",\n+        \"B\u0110\",\n+        \"BP\",\n+        \"BTh\",\n+        \"CM\",\n+        \"CB\",\n+        \"CT\",\n+        \"\u0110Na\",\n+        \"\u0110L\",\n+        \"\u0110No\",\n+        \"\u0110B\",\n+        \"\u0110N\",\n+        \"\u0110T\",\n+        \"GL\",\n+        \"HG\",\n+        \"HNa\",\n+        \"HN\",\n+        \"HT\",\n+        \"HD\",\n+        \"HP\",\n+        \"HGi\",\n+        \"HB\",\n+        \"SG\",\n+        \"HY\",\n+        \"KH\",\n+        \"KG\",\n+        \"KT\",\n+        \"LC\",\n+        \"LS\",\n+        \"LCa\",\n+        \"L\u0110\",\n+        \"LA\",\n+        \"N\u0110\",\n+        \"NA\",\n+        \"NB\",\n+        \"NT\",\n+        \"PT\",\n+        \"PY\",\n+        \"QB\",\n+        \"QNa\",\n+        \"QNg\",\n+        \"QN\",\n+        \"QT\",\n+        \"ST\",\n+        \"SL\",\n+        \"TN\",\n+        \"TB\",\n+        \"TNg\",\n+        \"TH\",\n+        \"TTH\",\n+        \"TG\",\n+        \"TV\",\n+        \"TQ\",\n+        \"VL\",\n+        \"VP\",\n+        \"YB\",\n+    )\n+\n+    provinces_postcode = {\n+        \"AG\": (88000, 88999),\n+        \"BV\": (79000, 79999),\n+        \"BL\": (96000, 96999),\n+        \"BK\": (26000, 26999),\n+        \"BG\": (23000, 23999),\n+        \"BN\": (22000, 22999),\n+        \"BT\": (93000, 93999),\n+        \"BD\": (82000, 82999),\n+        \"B\u0110\": (59000, 59999),\n+        \"BP\": (83000, 83999),\n+        \"BTh\": (80000, 80999),\n+        \"CM\": (97000, 97999),\n+        \"CB\": (27000, 27999),\n+        \"CT\": (92000, 92999),\n+        \"\u0110Na\": (55000, 55999),\n+        \"\u0110L\": (63000, 63999),\n+        \"\u0110No\": (64000, 64999),\n+        \"\u0110B\": (38000, 38999),\n+        \"\u0110N\": (81000, 81999),\n+        \"\u0110T\": (87000, 87999),\n+        \"GL\": (60000, 60999),\n+        \"HG\": (31000, 31999),\n+        \"HNa\": (40000, 40999),\n+        \"HN\": (10000, 15999),\n+        \"HT\": (48000, 48999),\n+        \"HD\": (17000, 17999),\n+        \"HP\": (18000, 18999),\n+        \"HGi\": (91000, 91999),\n+        \"HB\": (35000, 35999),\n+        \"SG\": (70000, 76999),\n+        \"HY\": (16000, 16999),\n+        \"KH\": (65000, 65999),\n+        \"KG\": (92000, 92999),\n+        \"KT\": (58000, 58999),\n+        \"LC\": (39000, 39999),\n+        \"LS\": (24000, 24999),\n+        \"LCa\": (33000, 33999),\n+        \"L\u0110\": (67000, 67999),\n+        \"LA\": (85000, 85999),\n+        \"N\u0110\": (42000, 42999),\n+        \"NA\": (46000, 47999),\n+        \"NB\": (43000, 43999),\n+        \"NT\": (66000, 66999),\n+        \"PT\": (29000, 29999),\n+        \"PY\": (62000, 62999),\n+        \"QB\": (51000, 51999),\n+        \"QNa\": (56000, 56999),\n+        \"QNg\": (57000, 57999),\n+        \"QN\": (20000, 20999),\n+        \"QT\": (52000, 52999),\n+        \"ST\": (95000, 95999),\n+        \"SL\": (36000, 36999),\n+        \"TN\": (84000, 84999),\n+        \"TB\": (41000, 41999),\n+        \"TNg\": (25000, 25999),\n+        \"TH\": (44000, 45999),\n+        \"TTH\": (53000, 53999),\n+        \"TG\": (86000, 86999),\n+        \"TV\": (94000, 94999),\n+        \"TQ\": (30000, 30999),\n+        \"VL\": (89000, 89999),\n+        \"VP\": (28000, 28999),\n+        \"YB\": (32000, 32999),\n+    }\n+\n+    address_formats = OrderedDict(\n+        (\n+            (\"{{street_address}}\\n{{city}}, {{postcode}}\", 25.0),\n+            (\"{{city}}\\n{{street_address}}, {{postcode}}\", 1.0),\n+        )\n+    )\n+\n+    city_formats = (\n+        \"{{city_prefix}} {{first_name}}{{city_suffix}}\",\n+        \"{{first_name}}{{city_suffix}}\",\n+    )\n+\n+    street_name_formats = (\n+        \"{{first_name}} {{street_suffix}}\",\n+        \"{{last_name}} {{street_suffix}}\",\n+    )\n+\n+    street_address_formats = (\n+        \"{{building_number}} {{street_name}}\",\n+    )\n+\n+    def city_prefix(self) -> str:\n+        \"\"\"Returns a random city prefix.\"\"\"\n+        return self.random_element(self.city_prefixes)\n+\n+    def administrative_unit(self) -> str:\n+        \"\"\"Returns a random administrative unit (province).\"\"\"\n+        return self.random_element(self.provinces)\n+\n+    state = administrative_unit\n+\n+    def state_abbr(self) -> str:\n+        \"\"\"\n+        Returns a random two-letter abbreviation for Vietnamese provinces.\n+\n+        \"\"\"\n+        abbreviations: Tuple[str, ...] = self.provinces_abbr\n+        return self.random_element(abbreviations)\n+\n+    def postcode(self) -> str:\n+        \"\"\"Returns a random postcode.\"\"\"\n+        return f\"{self.generator.random.randint(100000, 999999):06d}\"\n+\n+    def postcode_in_state(self, state_abbr: Optional[str] = None) -> str:\n+        \"\"\"\n+        Returns a random postcode within the provided province abbreviation.\n+\n+        :param state_abbr: A province abbreviation.\n+        :returns: A random postcode within the provided province abbreviation.\n+        \"\"\"\n+        if state_abbr is None:\n+            state_abbr = self.random_element(self.provinces_abbr)\n+\n+        if state_abbr in self.provinces_abbr:\n+            postcode = f\"{self.generator.random.randint(\n+                self.provinces_postcode[state_abbr][0],\n+                self.provinces_postcode[state_abbr][1]\n+            )}\"\n+\n+            # zero left pad up until desired length (length is 6)\n+            target_postcode_len = 6\n+            current_postcode_len = len(postcode)\n+            if current_postcode_len < target_postcode_len:\n+                pad = target_postcode_len - current_postcode_len\n+                postcode = f\"{'0' * pad}{postcode}\"\n+\n+            return postcode\n+\n+        raise ValueError(\"Province Abbreviation not found in list\")\ndiff --git a/faker/providers/color/vi_VN/__init__.py b/faker/providers/color/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..1d06fd0b2e\n--- /dev/null\n+++ b/faker/providers/color/vi_VN/__init__.py\n@@ -0,0 +1,90 @@\n+from collections import OrderedDict\n+\n+from .. import Provider as ColorProvider\n+\n+localized = True\n+\n+\n+class Provider(ColorProvider):\n+    \"\"\"\n+    Implement color provider for ``vi_VN`` locale.\n+\n+    #Sources: https://vi.wikipedia.org/wiki/Danh_s%C3%A1ch_m%C3%A0u\n+    \"\"\"\n+\n+    all_colors = OrderedDict(\n+        (\n+            (\"Tr\u1eafng Antique\", \"#FAEBD7\"),\n+            (\"Aquamarine\", \"#7FFFD4\"),\n+            (\"Azure\", \"#F0FFFF\"),\n+            (\"Beige\", \"#F5F5DC\"),\n+            (\"\u0110en\", \"#000000\"),\n+            (\"Xanh d\u01b0\u01a1ng\", \"#0000FF\"),\n+            (\"Xanh t\u00edm\", \"#8A2BE2\"),\n+            (\"N\u00e2u\", \"#A52A2A\"),\n+            (\"S\u00f4 c\u00f4 la\", \"#D2691E\"),\n+            (\"San h\u00f4\", \"#FF7F50\"),\n+            (\"Xanh h\u1ea3i qu\u00e2n\", \"#6495ED\"),\n+            (\"H\u1ed3ng \u0111\u00e0o\", \"#DC143C\"),\n+            (\"Xanh \u0111\u1eadm\", \"#00008B\"),\n+            (\"Xanh bi\u1ec3n \u0111\u1eadm\", \"#008B8B\"),\n+            (\"X\u00e1m \u0111\u1eadm\", \"#A9A9A9\"),\n+            (\"Xanh l\u00e1 \u0111\u1eadm\", \"#006400\"),\n+            (\"R\u00eau \u0111\u1eadm\", \"#BDB76B\"),\n+            (\"Cam \u0111\u1eadm\", \"#FF8C00\"),\n+            (\"\u0110\u1ecf \u0111\u1eadm\", \"#8B0000\"),\n+            (\"Xanh ng\u1ecdc \u0111\u1eadm\", \"#00CED1\"),\n+            (\"T\u00edm \u0111\u1eadm\", \"#9400D3\"),\n+            (\"H\u1ed3ng \u0111\u1eadm\", \"#FF1493\"),\n+            (\"X\u00e1m x\u1ec9n\", \"#696969\"),\n+            (\"H\u1ed3ng fuchsia\", \"#FF00FF\"),\n+            (\"V\u00e0ng\", \"#FFD700\"),\n+            (\"X\u00e1m\", \"#808080\"),\n+            (\"Xanh l\u00e1 c\u00e2y\", \"#008000\"),\n+            (\"Xanh l\u00e1 c\u00e2y nh\u1ea1t\", \"#ADFF2F\"),\n+            (\"H\u1ed3ng s\u00e1ng\", \"#FF69B4\"),\n+            (\"Indigo\", \"#4B0082\"),\n+            (\"Ng\u00e0 voi\", \"#FFFFF0\"),\n+            (\"R\u00eau\", \"#F0E68C\"),\n+            (\"H\u1ed3ng lavender\", \"#FFF0F5\"),\n+            (\"Xanh d\u01b0\u01a1ng nh\u1ea1t\", \"#ADD8E6\"),\n+            (\"Xanh bi\u1ec3n nh\u1ea1t\", \"#E0FFFF\"),\n+            (\"X\u00e1m s\u00e1ng\", \"#D3D3D3\"),\n+            (\"Xanh l\u00e1 c\u00e2y s\u00e1ng\", \"#90EE90\"),\n+            (\"H\u1ed3ng s\u00e1ng\", \"#FFB6C1\"),\n+            (\"Xanh bi\u1ec3n s\u00e1ng\", \"#87CEFA\"),\n+            (\"V\u00e0ng s\u00e1ng\", \"#FFFFE0\"),\n+            (\"H\u1ea1t D\u1ebb\", \"#800000\"),\n+            (\"Cam\", \"#FFA500\"),\n+            (\"Cam \u0111\u1ecf\", \"#FF4500\"),\n+            (\"Xanh l\u00e1 c\u00e2y nh\u1ea1t\", \"#98FB98\"),\n+            (\"Xanh bi\u1ec3n nh\u1ea1t\", \"#AFEEEE\"),\n+            (\"H\u1ed3ng\", \"#FFC0CB\"),\n+            (\"T\u00edm\", \"#DDA0DD\"),\n+            (\"T\u00edm \u0111\u1eadm\", \"#800080\"),\n+            (\"\u0110\u1ecf\", \"#FF0000\"),\n+            (\"Xanh bi\u1ec3n xanh\", \"#2E8B57\"),\n+            (\"B\u1ea1c\", \"#C0C0C0\"),\n+            (\"Xanh l\u1ee5c b\u1ea3o\", \"#40E0D0\"),\n+            (\"T\u00edm violet\", \"#EE82EE\"),\n+            (\"Tr\u1eafng\", \"#FFFFFF\"),\n+            (\"V\u00e0ng\", \"#FFFF00\"),\n+            (\"Xanh l\u00e1 c\u00e2y v\u00e0ng\", \"#9ACD32\"),\n+        )\n+    )\n+\n+    safe_colors = (\n+        \"\u0111en\",\n+        \"\u0111\u1ecf r\u01b0\u1ee3u\",\n+        \"xanh l\u00e1 c\u00e2y\",\n+        \"r\u00eau\",\n+        \"t\u00edm\",\n+        \"xanh bi\u1ec3n\",\n+        \"xanh chanh\",\n+        \"xanh d\u01b0\u01a1ng\",\n+        \"b\u1ea1c\",\n+        \"x\u00e1m\",\n+        \"v\u00e0ng\",\n+        \"h\u1ed3ng fuchsia\",\n+        \"tr\u1eafng\",\n+    )\ndiff --git a/faker/providers/company/vi_VN/__init__.py b/faker/providers/company/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..480b74d2be\n--- /dev/null\n+++ b/faker/providers/company/vi_VN/__init__.py\n@@ -0,0 +1,24 @@\n+from .. import Provider as CompanyProvider\n+\n+\n+class Provider(CompanyProvider):\n+    # Source: https://vi.wikipedia.org/wiki/Danh_s%C3%A1ch_c%C3%B4ng_ty_Vi%E1%BB%87t_Nam\n+    formats = (\n+        \"{{last_name}} {{company_suffix}}\",\n+        \"{{last_name}} {{last_name}} {{company_suffix}}\",\n+        \"{{last_name}} v\u00e0 {{last_name}} {{company_suffix}}\",\n+        \"{{last_name}} v\u00e0 \u0111\u1ed1i t\u00e1c {{company_suffix}}\",\n+    )\n+\n+    company_suffixes = (\n+        \"C\u00f4ng ty TNHH\",\n+        \"C\u00f4ng ty C\u1ed5 ph\u1ea7n\",\n+        \"Doanh nghi\u1ec7p t\u01b0 nh\u00e2n\",\n+        \"C\u00f4ng ty TNHH MTV\",\n+        \"C\u00f4ng ty H\u1ee3p danh\",\n+        \"C\u00f4ng ty Tr\u00e1ch nhi\u1ec7m h\u1eefu h\u1ea1n\",\n+        \"T\u1eadp \u0110o\u00e0n\",\n+    )\n+\n+    def company_suffix(self) -> str:\n+        return self.random_element(self.company_suffixes)\ndiff --git a/faker/providers/currency/vi_VN/__init__.py b/faker/providers/currency/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..a98a283654\n--- /dev/null\n+++ b/faker/providers/currency/vi_VN/__init__.py\n@@ -0,0 +1,9 @@\n+from .. import Provider as CurrencyProvider\n+\n+\n+class Provider(CurrencyProvider):\n+    # Source: https://vi.wikipedia.org/wiki/%C4%90%E1%BB%93ng_(%C4%91%C6%A1n_v%E1%BB%8B_ti%E1%BB%81n_t%E1%BB%87)#Ti%E1%BB%81n_gi%E1%BA%A5y_-_Ti%E1%BB%81n_polymer\n+    price_formats = [\"#.##\", \"%#.##\", \"%##.##\", \"%,###.##\", \"%#,###.##\"]\n+\n+    def pricetag(self) -> str:\n+        return \"\u20ab\" + self.numerify(self.random_element(self.price_formats))\ndiff --git a/faker/providers/date_time/vi_VN/__init__.py b/faker/providers/date_time/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..9de93fe0e2\n--- /dev/null\n+++ b/faker/providers/date_time/vi_VN/__init__.py\n@@ -0,0 +1,37 @@\n+from .. import Provider as DateTimeProvider\n+\n+\n+class Provider(DateTimeProvider):\n+    # Source: https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_d%E1%BA%A1ng_ng%C3%A0y_v%C3%A0_gi%E1%BB%9D_%E1%BB%9F_Vi%E1%BB%87t_Nam\n+    DAY_NAMES = {\n+        \"0\": \"Ch\u1ee7 Nh\u1eadt\",\n+        \"1\": \"Th\u1ee9 Hai\",\n+        \"2\": \"Th\u1ee9 Ba\",\n+        \"3\": \"Th\u1ee9 T\u01b0\",\n+        \"4\": \"Th\u1ee9 N\u0103m\",\n+        \"5\": \"Th\u1ee9 S\u00e1u\",\n+        \"6\": \"Th\u1ee9 B\u1ea3y\",\n+    }\n+\n+    MONTH_NAMES = {\n+        \"01\": \"Th\u00e1ng M\u1ed9t\",\n+        \"02\": \"Th\u00e1ng Hai\",\n+        \"03\": \"Th\u00e1ng Ba\",\n+        \"04\": \"Th\u00e1ng T\u01b0\",\n+        \"05\": \"Th\u00e1ng N\u0103m\",\n+        \"06\": \"Th\u00e1ng S\u00e1u\",\n+        \"07\": \"Th\u00e1ng B\u1ea3y\",\n+        \"08\": \"Th\u00e1ng T\u00e1m\",\n+        \"09\": \"Th\u00e1ng Ch\u00edn\",\n+        \"10\": \"Th\u00e1ng M\u01b0\u1eddi\",\n+        \"11\": \"Th\u00e1ng M\u01b0\u1eddi M\u1ed9t\",\n+        \"12\": \"Th\u00e1ng M\u01b0\u1eddi Hai\",\n+    }\n+\n+    def day_of_week(self):\n+        day = self.date(\"%w\")\n+        return self.DAY_NAMES[day]\n+\n+    def month_name(self):\n+        month = self.month()\n+        return self.MONTH_NAMES[month]\ndiff --git a/faker/providers/job/vi_VN/__init__.py b/faker/providers/job/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..ab22d56e62\n--- /dev/null\n+++ b/faker/providers/job/vi_VN/__init__.py\n@@ -0,0 +1,47 @@\n+from .. import Provider as JobProvider\n+\n+\n+class Provider(JobProvider):\n+    \"\"\"Translated from Super class\"\"\"\n+\n+    jobs = (\n+        # Information technology field\n+        \"L\u1eadp tr\u00ecnh vi\u00ean\", \"K\u1ef9 s\u01b0 ph\u1ea7n m\u1ec1m\", \"Ki\u1ebfn tr\u00fac s\u01b0 ph\u1ea7n m\u1ec1m\", \"Nh\u00e0 ph\u00e2n t\u00edch d\u1eef li\u1ec7u\", \"Chuy\u00ean vi\u00ean b\u1ea3o m\u1eadt\",\n+        \"Tester\", \"DevOps Engineer\", \"Project Manager\", \"UX/UI Designer\", \"Digital Marketer\", \"Th\u1ef1c T\u1eadp\",\n+\n+        # Finance - banking sector\n+        \"Nh\u00e2n vi\u00ean ng\u00e2n h\u00e0ng\", \"Chuy\u00ean vi\u00ean t\u00edn d\u1ee5ng\", \"K\u1ebf to\u00e1n\", \"Ki\u1ec3m to\u00e1n\", \"Nh\u00e0 t\u01b0 v\u1ea5n t\u00e0i ch\u00ednh\",\n+        \"Chuy\u00ean vi\u00ean ph\u00e2n t\u00edch th\u1ecb tr\u01b0\u1eddng\",\n+\n+        # Business areas\n+        \"Gi\u00e1m \u0111\u1ed1c kinh doanh\", \"Tr\u01b0\u1edfng ph\u00f2ng kinh doanh\", \"Nh\u00e2n vi\u00ean kinh doanh\", \"Marketing Manager\",\n+        \"Sales Representative\", \"Chuy\u00ean vi\u00ean b\u00e1n h\u00e0ng tr\u1ef1c tuy\u1ebfn\",\n+\n+        # Education Department\n+        \"Gi\u00e1o vi\u00ean\", \"Gi\u1ea3ng vi\u00ean\", \"Chuy\u00ean vi\u00ean t\u01b0 v\u1ea5n tuy\u1ec3n sinh\", \"Th\u1ef1c t\u1eadp sinh gi\u00e1o d\u1ee5c\",\n+\n+        # Medical\n+        \"B\u00e1c s\u0129\", \"Y t\u00e1\", \"D\u01b0\u1ee3c s\u0129\", \"\u0110i\u1ec1u D\u01b0\u1ee1ng\",\n+\n+        # Building sector\n+        \"K\u1ef9 s\u01b0 x\u00e2y d\u1ef1ng\", \"Ki\u1ebfn tr\u00fac s\u01b0\", \"Th\u1ee3 x\u00e2y\", \"K\u1ef9 s\u01b0 gi\u00e1m s\u00e1t\",\n+\n+        # Service sector\n+        \"Nh\u00e2n vi\u00ean kh\u00e1ch s\u1ea1n\", \"Nh\u00e2n vi\u00ean nh\u00e0 h\u00e0ng\", \"T\u01b0 v\u1ea5n kh\u00e1ch h\u00e0ng\", \"Nh\u00e2n vi\u00ean l\u1ec5 t\u00e2n\",\n+\n+        # Manufacturing sector\n+        \"C\u00f4ng nh\u00e2n s\u1ea3n xu\u1ea5t\", \"K\u1ef9 s\u01b0 s\u1ea3n xu\u1ea5t\", \"Qu\u1ea3n l\u00fd s\u1ea3n xu\u1ea5t\",\n+\n+        # Agriculture sector\n+        \"N\u00f4ng d\u00e2n\", \"K\u1ef9 s\u01b0 n\u00f4ng nghi\u1ec7p\",\n+\n+        # Law field\n+        \"Lu\u1eadt s\u01b0\", \"Th\u01b0 k\u00fd ph\u00e1p l\u00fd\",\n+\n+        # Other areas\n+        \"Nh\u00e0 b\u00e1o\", \"Bi\u00ean d\u1ecbch vi\u00ean\", \"Ngh\u1ec7 s\u0129\", \"Nh\u00e0 thi\u1ebft k\u1ebf \u0111\u1ed3 h\u1ecda\", \"Nh\u00e2n vi\u00ean h\u00e0nh ch\u00ednh\", \"Chuy\u00ean vi\u00ean nh\u00e2n s\u1ef1\",\n+        \"Nh\u00e2n Vi\u00ean B\u00e1n H\u00e0ng\",\n+    )\n+\n+    def job(self) -> str:\n+        return self.random_element(self.jobs)\ndiff --git a/faker/providers/lorem/vi_VN/__init__.py b/faker/providers/lorem/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..a4aff13c49\n--- /dev/null\n+++ b/faker/providers/lorem/vi_VN/__init__.py\n@@ -0,0 +1,193 @@\n+from typing import Dict\n+\n+from .. import Provider as LoremProvider\n+\n+\n+class Provider(LoremProvider):\n+    \"\"\"Implement lorem provider for ``vi_VN`` locale.\n+\n+    Word list is based on common Vietnamese words and phrases.\n+    # Source : https://vi.wikipedia.org/wiki/Ng%E1%BB%AF_ph%C3%A1p_ti%E1%BA%BFng_Vi%E1%BB%87t\n+    \"\"\"\n+\n+    word_list = (\n+        \"c\u00e1i\",\n+        \"\u0111\u00f3\",\n+        \"l\u00e0\",\n+        \"v\u00e0\",\n+        \"c\u00f3\",\n+        \"nh\u01b0\",\n+        \"m\u1ed9t\",\n+        \"\u0111\u1ec3\",\n+        \"c\u0169ng\",\n+        \"v\u1edbi\",\n+        \"cho\",\n+        \"trong\",\n+        \"t\u00f4i\",\n+        \"c\u1ee7a\",\n+        \"ng\u01b0\u1eddi\",\n+        \"kh\u00f4ng\",\n+        \"s\u1ebd\",\n+        \"\u0111\u00e3\",\n+        \"n\u00e0y\",\n+        \"theo\",\n+        \"l\u00e0m\",\n+        \"n\u01a1i\",\n+        \"\u0111ang\",\n+        \"n\u1ebfu\",\n+        \"b\u1ea1n\",\n+        \"\u0111\u01b0\u1ee3c\",\n+        \"khi\",\n+        \"th\u00ec\",\n+        \"v\u1ec1\",\n+        \"m\u00e0\",\n+        \"c\u0169ng\",\n+        \"n\u00e0o\",\n+        \"c\u1ee7a\",\n+        \"nh\u01b0ng\",\n+        \"v\u00ec\",\n+        \"r\u1ea5t\",\n+        \"t\u1ea1i\",\n+        \"t\u1ea1i\",\n+        \"th\u1ebf\",\n+        \"\u0111\u1ec3\",\n+        \"gi\u1eefa\",\n+        \"v\u1edbi\",\n+        \"c\u00e1ch\",\n+        \"t\u1eeb\",\n+        \"l\u1edbn\",\n+        \"c\u00f3\",\n+        \"v\u00e0i\",\n+        \"h\u01a1n\",\n+        \"v\u1eabn\",\n+        \"d\u01b0\u1edbi\",\n+        \"\u0111i\",\n+        \"\u0111\u1ebfn\",\n+        \"v\u1eady\",\n+        \"\u0111i\u1ec1u\",\n+        \"ho\u1eb7c\",\n+        \"ch\u1ec9\",\n+        \"h\u01a1n\",\n+        \"khi\u1ebfn\",\n+        \"gi\u1ed1ng\",\n+        \"sau\",\n+        \"trong\",\n+        \"\u0111\u00fang\",\n+        \"c\u1ee7a\",\n+        \"m\u1ed7i\",\n+        \"nh\u01b0\",\n+        \"b\u00ean\",\n+        \"\u0111\u1ec3\",\n+        \"ch\u01b0a\",\n+        \"nh\u01b0\",\n+        \"thay\",\n+        \"nh\u01b0\",\n+        \"c\u00e1c\",\n+        \"t\u1ef1\",\n+        \"s\u1ed1\",\n+        \"t\u1eebng\",\n+        \"nhi\u1ec1u\",\n+        \"g\u1ea7n\",\n+        \"t\u1eeb\",\n+    )\n+\n+    parts_of_speech: Dict[str, tuple] = {\n+        \"verb\": (\n+            \"l\u00e0\",\n+            \"c\u00f3\",\n+            \"l\u00e0m\",\n+            \"\u0111i\",\n+            \"n\u00f3i\",\n+            \"th\u1ea5y\",\n+            \"nghe\",\n+            \"\u0111\u1ecdc\",\n+            \"vi\u1ebft\",\n+            \"mu\u1ed1n\",\n+            \"\u0111i\",\n+            \"ng\u1ed3i\",\n+            \"u\u1ed1ng\",\n+            \"\u0103n\",\n+            \"h\u1ecdc\",\n+            \"ch\u01a1i\",\n+            \"nh\u00ecn\",\n+            \"\u0111\u01b0\u1ee3c\",\n+            \"t\u00ecm\",\n+            \"\u0111\u1eb7t\",\n+            \"gi\u00fap\",\n+            \"h\u1ecfi\",\n+            \"gi\u1ea3i\",\n+            \"mua\",\n+            \"b\u00e1n\",\n+            \"n\u00f3i\",\n+        ),\n+        \"noun\": (\n+            \"ng\u01b0\u1eddi\",\n+            \"s\u00e1ch\",\n+            \"m\u00e1y\",\n+            \"b\u00e0n\",\n+            \"gh\u1ebf\",\n+            \"c\u1eeda\",\n+            \"nh\u00e0\",\n+            \"b\u00fat\",\n+            \"xe\",\n+            \"\u0111i\u1ec7n tho\u1ea1i\",\n+            \"b\u00e1nh\",\n+            \"c\u00e0 ph\u00ea\",\n+            \"n\u01b0\u1edbc\",\n+            \"tr\u01b0\u1eddng\",\n+            \"ch\u00fang t\u00f4i\",\n+            \"h\u1ecdc sinh\",\n+            \"gi\u00e1o vi\u00ean\",\n+            \"b\u1ed1\",\n+            \"m\u1eb9\",\n+            \"em\",\n+            \"anh\",\n+            \"ch\u1ecb\",\n+        ),\n+        \"adverb\": (\n+            \"th\u1ef1c s\u1ef1\",\n+            \"r\u1ea5t\",\n+            \"nhanh\",\n+            \"ch\u1eadm\",\n+            \"t\u1ed1t\",\n+            \"x\u1ea5u\",\n+            \"\u0111\u00fang\",\n+            \"sai\",\n+            \"vui\",\n+            \"bu\u1ed3n\",\n+            \"m\u1edbi\",\n+            \"c\u0169\",\n+            \"d\u1ec5\",\n+            \"kh\u00f3\",\n+            \"g\u1ea7n\",\n+            \"xa\",\n+            \"h\u01a1n\",\n+            \"v\u1eabn\",\n+            \"\u0111\u00e3\",\n+            \"m\u1edbi\",\n+        ),\n+        \"adjective\": (\n+            \"\u0111\u1eb9p\",\n+            \"x\u1ea5u\",\n+            \"t\u1ed1t\",\n+            \"x\u1ea5u\",\n+            \"to\",\n+            \"nh\u1ecf\",\n+            \"ng\u1ecdt\",\n+            \"chua\",\n+            \"m\u1eb7n\",\n+            \"nhanh\",\n+            \"ch\u1eadm\",\n+            \"\u0111\u1ea7u\",\n+            \"cu\u1ed1i\",\n+            \"m\u1edbi\",\n+            \"c\u0169\",\n+            \"d\u1ec5\",\n+            \"kh\u00f3\",\n+            \"h\u01a1i\",\n+            \"vui\",\n+            \"bu\u1ed3n\",\n+            \"m\u1ea1nh\",\n+            \"y\u1ebfu\",\n+        ),\n+    }\ndiff --git a/faker/providers/person/vi_VN/__init__.py b/faker/providers/person/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..94774884de\n--- /dev/null\n+++ b/faker/providers/person/vi_VN/__init__.py\n@@ -0,0 +1,131 @@\n+from .. import Provider as PersonProvider\n+\n+\n+class Provider(PersonProvider):\n+    formats_female = (\n+        \"{{first_name_female}} {{last_name}}\",\n+        \"{{first_name_unisex}} {{last_name}}\",\n+        \"{{prefix_female}} {{first_name_unisex}} {{last_name}}\",\n+        \"{{prefix_female}} {{first_name_female}} {{last_name}}\",\n+    )\n+    formats_male = (\n+        \"{{first_name_male}} {{last_name}}\",\n+        \"{{first_name_male}} {{middle_name}} {{last_name}}\",\n+        \"{{first_name_unisex}} {{middle_name}} {{last_name}}\",\n+        \"{{prefix_male}} {{first_name_male}} {{last_name}}\",\n+    )\n+    formats = formats_female + formats_male\n+\n+    # Name from : https://en.wikipedia.org/wiki/Vietnamese_name\n+    # and https://vinpearl.com/en/vietnamese-names-top-200-popular-names-for-boys-and-girls\n+\n+    first_names_female = (\n+        \"Ng\u1ecdc\",\n+        \"H\u01b0\u01a1ng\",\n+        \"Lan\",\n+        \"Mai\",\n+        \"Th\u1ea3o\",\n+        \"Linh\",\n+        \"H\u1ed3ng\",\n+        \"Chi\",\n+        \"V\u00e2n\",\n+        \"Duy\u00ean\"\n+        \"D\u01b0\u01a1ng\",\n+        \"Y\u1ebfn\",\n+        \"Vi\",\n+        \"\u00c1nh\",\n+        \"Xu\u00e2n\"\n+    )\n+\n+    first_names_unisex = (\n+        \"An\",\n+        \"H\u00e0\",\n+        \"B\u1ea3o\",\n+        \"L\u00e2m\",\n+        \"H\u1ea1nh\",\n+        \"Th\u00e0nh\",\n+        \"Kim\",\n+        \"Nh\u1eadt\",\n+        \"Ph\u01b0\u01a1ng\",\n+        \"Khoa\"\n+        \"H\u1ea3i\",\n+        \"Nh\u1eadt\",\n+    )\n+\n+    first_names_male = (\n+        \"Nam\",\n+        \"H\u01b0ng\",\n+        \"V\u0169\",\n+        \"T\u00fa\",\n+        \"Ho\u00e0ng\",\n+        \"Ph\u00fac\",\n+        \"Trung\",\n+        \"Quang\",\n+        \"Anh\",\n+        \"Khoa\",\n+        \"D\u0169ng\",\n+        \"Quang\",\n+        \"Th\u00e0nh\",\n+        \"Huy\",\n+        \"B\u1ea3o\",\n+        \"Ch\u00e2u\",\n+        \"Minh\",\n+        \"T\u00f9ng\",\n+        \"Nhi\u00ean\",\n+        \"Tr\u1ecdng\"\n+    )\n+\n+    middle_names = (\n+        \"V\u0103n\",\n+        \"Th\u1ecb\",\n+        \"Quang\",\n+        \"\u0110\u1ee9c\",\n+        \"Tr\u00ed\",\n+        \"Xu\u00e2n\",\n+        \"Ho\u00e0ng\",\n+        \"H\u1ea3i\",\n+        \"\u0110\u1ee9c\",\n+        \"Th\u1ebf\",\n+        \"T\u1ea5n\",\n+        \"Ph\u00fa\",\n+        \"H\u1eefu\",\n+        \"B\u1ea3o\",\n+        \"Mai\",\n+        \"Mai B\u1ea3o\",\n+    )\n+\n+    last_names = (\n+        \"Nguy\u1ec5n\",\n+        \"Tr\u1ea7n\",\n+        \"L\u00ea\",\n+        \"Ph\u1ea1m\",\n+        \"V\u0169\",\n+        \"\u0110\u1eb7ng\",\n+        \"B\u00f9i\",\n+        \"D\u01b0\u01a1ng\",\n+        \"Mai\",\n+        \"Ho\u00e0ng\"\n+    )\n+\n+    # Typically, Vietnamese will be addressed with their given name and a prefix\n+    # https://en.wikipedia.org/wiki/Vietnamese_name#Given_name\n+\n+    prefixes_female = (\n+        \"C\u00f4\",\n+        \"Ch\u1ecb\",\n+        \"B\u00e0\",\n+        \"Qu\u00fd c\u00f4\"\n+    )\n+\n+    prefixes_male = (\n+        \"\u00d4ng\",\n+        \"Anh\",\n+        \"B\u00e1c\",\n+        \"Qu\u00fd \u00f4ng\"\n+    )\n+\n+    def first_name_unisex(self) -> str:\n+        return self.random_element(self.first_names_unisex)\n+\n+    def middle_name(self) -> str:\n+        return self.random_element(self.middle_names)\ndiff --git a/faker/providers/phone_number/vi_VN/__init__.py b/faker/providers/phone_number/vi_VN/__init__.py\nnew file mode 100644\nindex 0000000000..c76cc3af16\n--- /dev/null\n+++ b/faker/providers/phone_number/vi_VN/__init__.py\n@@ -0,0 +1,16 @@\n+from .. import Provider as PhoneNumberProvider\n+\n+\n+class Provider(PhoneNumberProvider):\n+    \"\"\" Implement phone_number provider for ``vi_VN`` locale.\n+        # Source : https://vi.wikipedia.org/wiki/M%C3%A3_%C4%91i%E1%BB%87n_tho%E1%BA%A1i_Vi%E1%BB%87t_Nam\n+    \"\"\"\n+    formats = (\n+        \"+84 ## #######\",\n+        \"(0#) #### ####\",\n+        \"0# #### ####\",\n+        \"0# #######\",\n+        \"+84-##-######\",\n+        \"+84-##-### ####\",\n+        \"(0#)###-####\",\n+    )\n", "instance_id": "joke2k__faker-2076", "clarity": 3, "difficulty": 0.45, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly outlines the goal of adding support for the Vietnamese locale (vi_VN) to the Faker library, specifying the need for generating realistic Vietnamese names, addresses, and other locale-specific data. The expected behavior is detailed with examples of data to be generated (e.g., male/female first names, last names, and address formats). The actual behavior (lack of support leading to errors or fallback data) is also described, along with steps to reproduce the issue. There are no significant ambiguities, and the requirements are explicit, making it easy to understand the task at hand. The inclusion of specific data categories to be supported further enhances the clarity of the problem description.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, as it involves creating multiple new files (e.g., for address, person, phone number, color, company, currency, date_time, job, and lorem providers) to support the vi_VN locale in the Faker library. However, the changes are largely isolated to adding new data and formatting logic without requiring deep modifications to the existing codebase or architecture. Second, the technical concepts involved are relatively straightforward\u2014primarily data structuring (lists, dictionaries) and inheritance from base provider classes in Python, along with basic string formatting and randomization logic. No advanced algorithms, design patterns, or domain-specific knowledge beyond understanding Vietnamese naming and address conventions are required. Third, while the problem does not explicitly mention edge cases, the code changes (e.g., postcode generation within specific ranges for provinces) show some consideration for localized accuracy, though error handling is minimal and not complex. Overall, the task requires understanding multiple components of the Faker library and implementing consistent locale-specific data across several modules, but it does not involve intricate logic or significant architectural impact, placing it at a moderate difficulty level of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG]: Strange behavior with quadtree point in polygon\n### Version\n\n23.04\n\n### On which installation method(s) does this occur?\n\nRapids-Compose\n\n### Describe the issue\n\nquadtree pip seems to experience a silent OOM or other error in certain larger data cases. This particular example requires the availability of our demo datasets `taxi2015.csv` and `taxi_zones.zip`. I'm not sure of another way to reproduce it, so I'm including the full example here. It doesn't always appear so I'm just remembering the issue here.\r\n\r\nAs you can see from the Relevant log output, quadtree returns 3.7m samples when only the first 120 polygons in `zones` are used. If the entire `zones` polygons list (263 polygons) is used, _something_ happens and only 16314 rows are returned. This is unexpected behavior and needs investigation.\n\n### Minimum reproducible example\n\n```shell\n# https://github.com/rapidsai/cuspatial/issues/890\r\nimport cuspatial\r\nimport cudf\r\nimport geopandas\r\nhost_zones = geopandas.read_file('taxi_zones.zip')\r\nhost_lonlat = host_zones.to_crs(epsg=4326)\r\nzones = cuspatial.from_geopandas(host_lonlat)\r\ntaxi2015 = cudf.read_csv('taxi2015.csv')\r\ndef quadtree(polygons, points):\r\n    poly_points_x = polygons.polygons.x\r\n    poly_points_y = polygons.polygons.y\r\n    poly_offsets = polygons.polygons.part_offset\r\n    poly_ring_offsets = polygons.polygons.ring_offset\r\n    test_points_x = points.points.x\r\n    test_points_y = points.points.y\r\n    scale = 50\r\n    max_depth = 7\r\n    min_size = 125\r\n    x_max = poly_points_x.max()\r\n    x_min = poly_points_x.min()\r\n    y_max = poly_points_y.max()\r\n    y_min = poly_points_y.min()\r\n    point_indices, quadtree = cuspatial.quadtree_on_points(\r\n        test_points_x,\r\n        test_points_y,\r\n        x_min,\r\n        x_max,\r\n        y_min,\r\n        y_max,\r\n        scale,\r\n        max_depth,\r\n        min_size,\r\n    )\r\n    poly_bboxes = cuspatial.polygon_bounding_boxes(\r\n        poly_offsets, poly_ring_offsets, poly_points_x, poly_points_y\r\n    )\r\n    intersections = cuspatial.join_quadtree_and_bounding_boxes(\r\n        quadtree, poly_bboxes, x_min, x_max, y_min, y_max, scale, max_depth\r\n    )\r\n    polygons_and_points = cuspatial.quadtree_point_in_polygon(\r\n        intersections,\r\n        quadtree,\r\n        point_indices,\r\n        test_points_x,\r\n        test_points_y,\r\n        poly_offsets,\r\n        poly_ring_offsets,\r\n        poly_points_x,\r\n        poly_points_y,\r\n    )\r\n    return polygons_and_points\r\ndef make_geoseries_from_lon_lat(lon, lat):\r\n    # Scatter the two columns into one column\r\n    assert len(lon) == len(lat)\r\n    xy = cudf.Series(cp.zeros(len(lon) * 2))\r\n    xy[::2] = lon\r\n    xy[1::2] = lat\r\n\r\n    return cuspatial.GeoSeries(cuspatial.core._column.geocolumn.GeoColumn._from_points_xy(xy._column))\r\ndropoffs = make_geoseries_from_lon_lat(\r\n    taxi2015['dropoff_longitude'],\r\n    taxi2015['dropoff_latitude']\r\n)\r\nprint(quadtree(zones['geometry'].iloc[0:120], dropoffs))\r\nprint(quadtree(zones['geometry'], dropoffs))\n```\n\n\n### Relevant log output\n\n```shell\npolygon_index  point_index\r\n0                    0          116\r\n1                    0          387\r\n2                    0          685\r\n3                    0         2607\r\n4                    0         3141\r\n...                ...          ...\r\n3686177            167     12304018\r\n3686178            167     12323531\r\n3686179            167     12351800\r\n3686180            167     12444884\r\n3686181            167     12484251\r\n\r\n[3686182 rows x 2 columns]\r\n       polygon_index  point_index\r\n0                  0          116\r\n1                  0          387\r\n2                  0          685\r\n3                  0         2608\r\n4                  0         3142\r\n...              ...          ...\r\n16309              0     12504645\r\n16310              1      7456107\r\n16311              1      7530752\r\n16312              1      7704910\r\n16313              1     11938181\r\n\r\n[16314 rows x 2 columns]\n```\n\n\n### Environment details\n\n```shell\nrapids-compose 23.04\n```\n\n\n### Other/Misc.\n\n_No response_\n", "patch": "diff --git a/ci/build_docs.sh b/ci/build_docs.sh\nindex 65ddc2350..27c2270ea 100755\n--- a/ci/build_docs.sh\n+++ b/ci/build_docs.sh\n@@ -6,27 +6,21 @@ set -euo pipefail\n rapids-logger \"Create test conda environment\"\n . /opt/conda/etc/profile.d/conda.sh\n \n+rapids-logger \"Downloading artifacts from previous jobs\"\n+CPP_CHANNEL=\"$(rapids-download-conda-from-s3 cpp)\"\n+PYTHON_CHANNEL=\"$(rapids-download-conda-from-s3 python)\"\n+\n rapids-dependency-file-generator \\\n   --output conda \\\n   --file_key docs \\\n-  --matrix \"cuda=${RAPIDS_CUDA_VERSION%.*};arch=$(arch);py=${RAPIDS_PY_VERSION}\" | tee env.yaml\n+  --matrix \"cuda=${RAPIDS_CUDA_VERSION%.*};arch=$(arch);py=${RAPIDS_PY_VERSION}\" \\\n+  --prepend-channel \"${CPP_CHANNEL}\" --prepend-channel \"${PYTHON_CHANNEL}\" | tee env.yaml\n \n rapids-mamba-retry env create --yes -f env.yaml -n docs\n conda activate docs\n \n rapids-print-env\n \n-rapids-logger \"Downloading artifacts from previous jobs\"\n-CPP_CHANNEL=$(rapids-download-conda-from-s3 cpp)\n-PYTHON_CHANNEL=$(rapids-download-conda-from-s3 python)\n-\n-rapids-mamba-retry install \\\n-  --channel \"${CPP_CHANNEL}\" \\\n-  --channel \"${PYTHON_CHANNEL}\" \\\n-  libcuspatial \\\n-  cuspatial \\\n-  cuproj\n-\n export RAPIDS_VERSION=\"$(rapids-version)\"\n export RAPIDS_VERSION_MAJOR_MINOR=\"$(rapids-version-major-minor)\"\n export RAPIDS_VERSION_NUMBER=\"24.08\"\ndiff --git a/ci/release/update-version.sh b/ci/release/update-version.sh\nindex 5392183dc..ae13d7629 100755\n--- a/ci/release/update-version.sh\n+++ b/ci/release/update-version.sh\n@@ -45,7 +45,10 @@ NEXT_SHORT_TAG_PEP440=$(python -c \"from setuptools.extern import packaging; prin\n DEPENDENCIES=(\n   cudf\n   cuml\n+  cuspatial\n   libcudf\n+  libcuspatial\n+  libcuspatial-tests\n   librmm\n   rmm\n   cuspatial\ndiff --git a/conda/environments/all_cuda-118_arch-x86_64.yaml b/conda/environments/all_cuda-118_arch-x86_64.yaml\nindex cce5b78f3..9868e1917 100644\n--- a/conda/environments/all_cuda-118_arch-x86_64.yaml\n+++ b/conda/environments/all_cuda-118_arch-x86_64.yaml\n@@ -23,6 +23,8 @@ dependencies:\n - ipython\n - ipywidgets\n - libcudf==24.8.*\n+- libcuspatial-tests==24.8.*\n+- libcuspatial==24.8.*\n - librmm==24.8.*\n - myst-parser\n - nbsphinx\ndiff --git a/conda/environments/all_cuda-122_arch-x86_64.yaml b/conda/environments/all_cuda-122_arch-x86_64.yaml\nindex 713334e67..4951df0ba 100644\n--- a/conda/environments/all_cuda-122_arch-x86_64.yaml\n+++ b/conda/environments/all_cuda-122_arch-x86_64.yaml\n@@ -26,6 +26,8 @@ dependencies:\n - ipython\n - ipywidgets\n - libcudf==24.8.*\n+- libcuspatial-tests==24.8.*\n+- libcuspatial==24.8.*\n - librmm==24.8.*\n - myst-parser\n - nbsphinx\ndiff --git a/cpp/benchmarks/CMakeLists.txt b/cpp/benchmarks/CMakeLists.txt\nindex 99780a677..10e626f3f 100644\n--- a/cpp/benchmarks/CMakeLists.txt\n+++ b/cpp/benchmarks/CMakeLists.txt\n@@ -1,5 +1,5 @@\n #=============================================================================\n-# Copyright (c) 2019-2021, NVIDIA CORPORATION.\n+# Copyright (c) 2019-2024, NVIDIA CORPORATION.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -23,6 +23,17 @@ add_library(cuspatial_benchmark_common OBJECT\n \n target_compile_features(cuspatial_benchmark_common PUBLIC cxx_std_17 cuda_std_17)\n \n+set_target_properties(cuspatial_benchmark_common\n+    PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n+               INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n+               CXX_STANDARD                        17\n+               CXX_STANDARD_REQUIRED               ON\n+               CUDA_STANDARD                       17\n+               CUDA_STANDARD_REQUIRED              ON\n+               POSITION_INDEPENDENT_CODE           ON\n+               INTERFACE_POSITION_INDEPENDENT_CODE ON\n+)\n+\n target_link_libraries(cuspatial_benchmark_common\n     PUBLIC benchmark::benchmark\n            cudf::cudftestutil\n@@ -43,6 +54,10 @@ function(ConfigureBench CMAKE_BENCH_NAME)\n     set_target_properties(${CMAKE_BENCH_NAME}\n         PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n                    INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n+                   CXX_STANDARD 17\n+                   CXX_STANDARD_REQUIRED ON\n+                   CUDA_STANDARD 17\n+                   CUDA_STANDARD_REQUIRED ON\n         )\n     target_link_libraries(${CMAKE_BENCH_NAME} PRIVATE benchmark::benchmark_main cuspatial_benchmark_common)\n     install(\n@@ -61,7 +76,11 @@ function(ConfigureNVBench CMAKE_BENCH_NAME)\n     ${CMAKE_BENCH_NAME}\n     PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"$<BUILD_INTERFACE:${CUSPATIAL_BINARY_DIR}/benchmarks>\"\n                INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n-  )\n+               CXX_STANDARD 17\n+               CXX_STANDARD_REQUIRED ON\n+               CUDA_STANDARD 17\n+               CUDA_STANDARD_REQUIRED ON\n+)\n   target_link_libraries(\n     ${CMAKE_BENCH_NAME} PRIVATE cuspatial_benchmark_common nvbench::main\n   )\ndiff --git a/cpp/include/cuspatial/detail/index/construction/phase_2.cuh b/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\nindex 88c4947e0..df28ecca1 100644\n--- a/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\n+++ b/cpp/include/cuspatial/detail/index/construction/phase_2.cuh\n@@ -85,9 +85,9 @@ inline rmm::device_uvector<uint32_t> flatten_point_keys(\n                     keys_and_levels + num_valid_nodes,\n                     flattened_keys.begin(),\n                     [last_level = max_depth - 1] __device__(auto const& val) {\n-                      bool is_parent{false};\n-                      uint32_t key{}, level{};\n-                      thrust::tie(key, level, is_parent) = val;\n+                      auto& key       = thrust::get<0>(val);\n+                      auto& level     = thrust::get<1>(val);\n+                      auto& is_parent = thrust::get<2>(val);\n                       // if this is a parent node, return max_key. otherwise\n                       // compute the key for one level up the tree. Leaf nodes\n                       // whose keys are zero will be removed in a subsequent\ndiff --git a/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh b/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\nindex 0b8f23e19..68a2e9596 100644\n--- a/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\n+++ b/cpp/include/cuspatial/detail/join/quadtree_point_in_polygon.cuh\n@@ -24,14 +24,16 @@\n #include <cuspatial/range/multipolygon_range.cuh>\n #include <cuspatial/traits.hpp>\n \n+#include <rmm/cuda_device.hpp>\n #include <rmm/device_uvector.hpp>\n #include <rmm/exec_policy.hpp>\n #include <rmm/resource_ref.hpp>\n \n #include <thrust/iterator/permutation_iterator.h>\n+#include <thrust/iterator/transform_iterator.h>\n #include <thrust/scan.h>\n \n-#include <cstdint>\n+#include <limits>\n \n namespace cuspatial {\n namespace detail {\n@@ -57,7 +59,7 @@ struct compute_poly_and_point_indices {\n   using IndexType = iterator_value_type<QuadOffsetsIterator>;\n \n   inline thrust::tuple<IndexType, IndexType> __device__\n-  operator()(IndexType const global_index) const\n+  operator()(std::uint64_t const global_index) const\n   {\n     auto const [quad_poly_index, local_point_index] =\n       get_quad_and_local_point_indices(global_index, point_offsets_begin, point_offsets_end);\n@@ -118,16 +120,26 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n \n   auto num_poly_quad_pairs = std::distance(poly_indices_first, poly_indices_last);\n \n-  auto quad_lengths_iter =\n-    thrust::make_permutation_iterator(quadtree.length_begin(), quad_indices_first);\n+  // The quadtree length is an iterator of uint32_t, but we have to transform into uint64_t values\n+  // so the thrust::inclusive_scan accumulates into uint64_t outputs. Changing the output iterator\n+  // to uint64_t isn't sufficient to achieve this behavior.\n+  auto quad_lengths_iter = thrust::make_transform_iterator(\n+    thrust::make_permutation_iterator(quadtree.length_begin(), quad_indices_first),\n+    cuda::proclaim_return_type<std::uint64_t>([] __device__(IndexType const& i) -> std::uint64_t {\n+      return static_cast<std::uint64_t>(i);\n+    }));\n \n   auto quad_offsets_iter =\n     thrust::make_permutation_iterator(quadtree.offset_begin(), quad_indices_first);\n \n-  // Compute a \"local\" set of zero-based point offsets from number of points in each quadrant\n+  // Compute a \"local\" set of zero-based point offsets from the number of points in each quadrant.\n+  //\n   // Use `num_poly_quad_pairs + 1` as the length so that the last element produced by\n   // `inclusive_scan` is the total number of points to be tested against any polygon.\n-  rmm::device_uvector<IndexType> local_point_offsets(num_poly_quad_pairs + 1, stream);\n+  //\n+  // Accumulate into uint64_t, because the prefix sums can overflow the size of uint32_t\n+  // when testing a large number of polygons against a large quadtree.\n+  rmm::device_uvector<std::uint64_t> local_point_offsets(num_poly_quad_pairs + 1, stream);\n \n   // inclusive scan of quad_lengths_iter\n   thrust::inclusive_scan(rmm::exec_policy(stream),\n@@ -136,21 +148,27 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n                          local_point_offsets.begin() + 1);\n \n   // Ensure local point offsets starts at 0\n-  IndexType init{0};\n+  std::uint64_t init{0};\n   local_point_offsets.set_element_async(0, init, stream);\n \n   // The last element is the total number of points to test against any polygon.\n   auto num_total_points = local_point_offsets.back_element(stream);\n \n-  // Allocate the output polygon and point index pair vectors\n-  rmm::device_uvector<IndexType> poly_indices(num_total_points, stream);\n-  rmm::device_uvector<IndexType> point_indices(num_total_points, stream);\n-\n-  auto poly_and_point_indices =\n-    thrust::make_zip_iterator(poly_indices.begin(), point_indices.begin());\n-\n-  // Enumerate the point X/Ys using the sorted `point_indices` (from quadtree construction)\n-  auto point_xys_iter = thrust::make_permutation_iterator(points_first, point_indices_first);\n+  // The largest supported input size for thrust::count_if/copy_if is INT32_MAX.\n+  // This functor iterates over the input space and processes up to INT32_MAX elements at a time.\n+  std::uint64_t max_points_to_test = std::numeric_limits<std::int32_t>::max();\n+  auto count_in_chunks             = [&](auto const& func) {\n+    std::uint64_t memo{};\n+    for (std::uint64_t offset{0}; offset < num_total_points; offset += max_points_to_test) {\n+      memo += func(memo, offset, std::min(max_points_to_test, num_total_points - offset));\n+    }\n+    return memo;\n+  };\n+\n+  detail::test_poly_point_intersection test_poly_point_pair{\n+    // Enumerate the point X/Ys using the sorted `point_indices` (from quadtree construction)\n+    thrust::make_permutation_iterator(points_first, point_indices_first),\n+    polygons};\n \n   // Compute the combination of polygon and point index pairs. For each polygon/quadrant pair,\n   // enumerate pairs of (poly_index, point_index) for each point in each quadrant.\n@@ -163,28 +181,57 @@ std::pair<rmm::device_uvector<IndexType>, rmm::device_uvector<IndexType>> quadtr\n   //     pp_pairs.append((polygon, point))\n   // ```\n   //\n-  auto global_to_poly_and_point_indices = detail::make_counting_transform_iterator(\n-    0,\n-    detail::compute_poly_and_point_indices{quad_offsets_iter,\n-                                           local_point_offsets.begin(),\n-                                           local_point_offsets.end(),\n-                                           poly_indices_first});\n-\n-  // Compute the number of intersections by removing (poly, point) pairs that don't intersect\n-  auto num_intersections = thrust::distance(\n-    poly_and_point_indices,\n-    thrust::copy_if(rmm::exec_policy(stream),\n-                    global_to_poly_and_point_indices,\n-                    global_to_poly_and_point_indices + num_total_points,\n-                    poly_and_point_indices,\n-                    detail::test_poly_point_intersection{point_xys_iter, polygons}));\n-\n-  poly_indices.resize(num_intersections, stream);\n-  poly_indices.shrink_to_fit(stream);\n-  point_indices.resize(num_intersections, stream);\n-  point_indices.shrink_to_fit(stream);\n-\n-  return std::pair{std::move(poly_indices), std::move(point_indices)};\n+  auto global_to_poly_and_point_indices = [&](auto offset = 0) {\n+    return detail::make_counting_transform_iterator(\n+      offset,\n+      detail::compute_poly_and_point_indices{quad_offsets_iter,\n+                                             local_point_offsets.begin(),\n+                                             local_point_offsets.end(),\n+                                             poly_indices_first});\n+  };\n+\n+  auto run_quadtree_point_in_polygon = [&](auto output_size) {\n+    // Allocate the output polygon and point index pair vectors\n+    rmm::device_uvector<IndexType> poly_indices(output_size, stream);\n+    rmm::device_uvector<IndexType> point_indices(output_size, stream);\n+\n+    auto num_intersections = count_in_chunks([&](auto memo, auto offset, auto size) {\n+      auto poly_and_point_indices =\n+        thrust::make_zip_iterator(poly_indices.begin(), point_indices.begin()) + memo;\n+      // Remove (poly, point) pairs that don't intersect\n+      return thrust::distance(poly_and_point_indices,\n+                              thrust::copy_if(rmm::exec_policy(stream),\n+                                              global_to_poly_and_point_indices(offset),\n+                                              global_to_poly_and_point_indices(offset) + size,\n+                                              poly_and_point_indices,\n+                                              test_poly_point_pair));\n+    });\n+\n+    if (num_intersections < output_size) {\n+      poly_indices.resize(num_intersections, stream);\n+      point_indices.resize(num_intersections, stream);\n+      poly_indices.shrink_to_fit(stream);\n+      point_indices.shrink_to_fit(stream);\n+    }\n+\n+    return std::pair{std::move(poly_indices), std::move(point_indices)};\n+  };\n+\n+  try {\n+    // First attempt to run the hit test assuming allocating space for all possible intersections\n+    // fits into the available memory.\n+    return run_quadtree_point_in_polygon(num_total_points);\n+  } catch (rmm::out_of_memory const&) {\n+    // If we OOM the first time, pre-compute the number of hits and allocate only that amount of\n+    // space for the output buffers. This halves performance, but it should at least return valid\n+    // results.\n+    return run_quadtree_point_in_polygon(count_in_chunks([&](auto memo, auto offset, auto size) {\n+      return thrust::count_if(rmm::exec_policy(stream),\n+                              global_to_poly_and_point_indices(offset),\n+                              global_to_poly_and_point_indices(offset) + size,\n+                              test_poly_point_pair);\n+    }));\n+  }\n }\n \n }  // namespace cuspatial\ndiff --git a/cpp/include/cuspatial/detail/range/multilinestring_range.cuh b/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\nindex b9b53bfc0..03ad0fe27 100644\n--- a/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\n+++ b/cpp/include/cuspatial/detail/range/multilinestring_range.cuh\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022-2023, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -74,6 +74,7 @@ template <typename GeometryIterator, typename PartIterator, typename VecIterator\n class multilinestring_range;\n \n template <typename GeometryIterator, typename PartIterator, typename VecIterator>\n+CUSPATIAL_HOST_DEVICE\n multilinestring_range<GeometryIterator, PartIterator, VecIterator>::multilinestring_range(\n   GeometryIterator geometry_begin,\n   GeometryIterator geometry_end,\ndiff --git a/cpp/include/cuspatial/geometry/box.hpp b/cpp/include/cuspatial/geometry/box.hpp\nindex 1041c4de2..4a9f97639 100644\n--- a/cpp/include/cuspatial/geometry/box.hpp\n+++ b/cpp/include/cuspatial/geometry/box.hpp\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2022, NVIDIA CORPORATION.\n+ * Copyright (c) 2022-2024, NVIDIA CORPORATION.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -40,9 +40,9 @@ class alignas(sizeof(Vertex)) box {\n \n  private:\n   /**\n-   * @brief Output stream operator for `vec_2d<T>` for human-readable formatting\n+   * @brief Output stream operator for `box<T>` for human-readable formatting\n    */\n-  friend std::ostream& operator<<(std::ostream& os, cuspatial::box<T> const& b)\n+  friend std::ostream& operator<<(std::ostream& os, cuspatial::box<T, Vertex> const& b)\n   {\n     return os << \"{\" << b.v1 << \", \" << b.v2 << \"}\";\n   }\ndiff --git a/dependencies.yaml b/dependencies.yaml\nindex a74eb245b..fa7b82381 100644\n--- a/dependencies.yaml\n+++ b/dependencies.yaml\n@@ -18,6 +18,7 @@ files:\n       - depends_on_cuml\n       - depends_on_cupy\n       - run_python_cuspatial\n+      - test_libcuspatial\n       - test_python_cuspatial\n       - test_python_cuproj\n       - notebooks\n@@ -25,6 +26,7 @@ files:\n     output: none\n     includes:\n       - cuda_version\n+      - test_libcuspatial\n   test_python:\n     output: none\n     includes:\n@@ -32,6 +34,7 @@ files:\n       - py_version\n       - test_python_cuspatial\n       - test_python_cuproj\n+      - test_cuspatial\n   test_notebooks:\n     output: none\n     includes:\n@@ -39,6 +42,7 @@ files:\n       - depends_on_cuml\n       - notebooks\n       - py_version\n+      - test_cuspatial\n   checks:\n     output: none\n     includes:\n@@ -50,6 +54,7 @@ files:\n       - cuda_version\n       - docs\n       - py_version\n+      - test_cuspatial\n   py_build_cuspatial:\n     output: [pyproject]\n     pyproject_dir: python/cuspatial\n@@ -105,6 +110,7 @@ files:\n     includes:\n       - test_python_cuproj\n       - depends_on_cuspatial\n+      - test_cuspatial\n \n channels:\n   - rapidsai\n@@ -451,3 +457,16 @@ dependencies:\n             packages:\n               - cupy-cuda11x>=12.0.0\n           - {matrix: null, packages: [cupy-cuda11x>=12.0.0]}\n+  test_libcuspatial:\n+    common:\n+      - output_types: conda\n+        packages:\n+          - libcuspatial==24.8.*\n+          - libcuspatial-tests==24.8.*\n+  test_cuspatial:\n+    common:\n+      - output_types: conda\n+        packages:\n+          - libcuspatial==24.8.*\n+          - cuspatial==24.8.*\n+          - cuproj==24.8.*\n", "instance_id": "rapidsai__cuspatial-1390", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear, as it describes a specific issue with the quadtree point-in-polygon functionality in the cuSpatial library, where unexpected behavior (a significant drop in the number of returned rows) occurs with larger datasets. It provides a minimum reproducible example, relevant log output, and environment details, which are helpful for understanding the issue. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the expected output or behavior for the full dataset (263 polygons), nor does it specify potential causes beyond a \"silent OOM or other error.\" Additionally, edge cases or constraints (e.g., dataset size limits, memory constraints) are not clearly outlined, and the reliance on specific demo datasets (not publicly accessible) limits reproducibility for external contributors. Overall, while the goal and issue are clear, these minor gaps in detail slightly reduce the clarity.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope and depth of code changes are significant, as they involve modifications to core functionality in the cuSpatial library, specifically in the quadtree point-in-polygon algorithm. The changes span multiple files, including critical CUDA kernel implementations (e.g., `quadtree_point_in_polygon.cuh`), and address a memory-related issue (potential OOM) by introducing a fallback mechanism for large datasets. This requires a deep understanding of the codebase architecture and interactions between components like quadtree construction, bounding box joins, and point-in-polygon tests.\n\nSecond, the number of technical concepts involved is substantial. Solving this requires expertise in CUDA programming, GPU memory management (via RMM), parallel algorithms (using Thrust for scans and transformations), and spatial data structures (quadtrees). Additionally, the changes involve handling large data volumes, necessitating knowledge of integer overflow prevention (e.g., using `uint64_t` for accumulations) and performance optimization on GPUs. The domain-specific knowledge of geospatial processing (e.g., polygon-point intersection tests) adds further complexity.\n\nThird, the problem involves critical edge cases and error handling, as evidenced by the introduction of a two-pass approach to handle out-of-memory scenarios. The code changes explicitly address large input sizes by chunking operations to avoid exceeding `INT32_MAX` limits in Thrust operations and by pre-computing intersection counts when initial allocation fails. These modifications indicate a need to handle non-trivial edge cases related to memory constraints and large-scale data processing.\n\nFinally, while the changes do not appear to impact the overall system architecture fundamentally, they do affect a core component of the library, requiring careful consideration of performance implications and correctness across various input sizes. The combination of deep technical requirements, complex modifications, and significant edge case handling justifies a difficulty score of 0.75. It does not reach the \"Very Hard\" range (0.8-1.0) as it does not involve system-level redesign or highly intricate domain-specific challenges beyond geospatial processing on GPUs.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add pandas.api.typing module\nThe `pandas.api.typing` module was added in pandas 2.1, but is not present in pandas-stubs\r\n\r\n**To Reproduce**\r\n```python\r\nfrom pandas.api.typing import NAType\r\n```\r\nmypy gives `error: Library stubs not installed for \"pandas.api.typing\"  [import-untyped]`\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Windows\r\n - OS Version 10\r\n - Python version 3.12.6\r\n - mypy version 1.13.0\r\n - pandas-stubs version 2.2.3.241009\r\n\n", "patch": "diff --git a/pandas-stubs/api/typing/__init__.pyi b/pandas-stubs/api/typing/__init__.pyi\nnew file mode 100644\nindex 00000000..c81f444c\n--- /dev/null\n+++ b/pandas-stubs/api/typing/__init__.pyi\n@@ -0,0 +1,29 @@\n+from pandas.core.groupby import (\n+    DataFrameGroupBy as DataFrameGroupBy,\n+    SeriesGroupBy as SeriesGroupBy,\n+)\n+from pandas.core.resample import (\n+    DatetimeIndexResamplerGroupby as DatetimeIndexResamplerGroupby,\n+    PeriodIndexResamplerGroupby as PeriodIndexResamplerGroupby,\n+    Resampler as Resampler,\n+    TimedeltaIndexResamplerGroupby as TimedeltaIndexResamplerGroupby,\n+    TimeGrouper as TimeGrouper,\n+)\n+from pandas.core.window import (\n+    Expanding as Expanding,\n+    ExpandingGroupby as ExpandingGroupby,\n+    ExponentialMovingWindow as ExponentialMovingWindow,\n+    ExponentialMovingWindowGroupby as ExponentialMovingWindowGroupby,\n+    Rolling as Rolling,\n+    RollingGroupby as RollingGroupby,\n+    Window as Window,\n+)\n+\n+from pandas._libs import NaTType as NaTType\n+from pandas._libs.missing import NAType as NAType\n+\n+from pandas.io.json._json import JsonReader as JsonReader\n+\n+# SASReader is not defined so commenting it out for now\n+# from pandas.io.sas.sasreader import SASReader as SASReader\n+from pandas.io.stata import StataReader as StataReader\n", "instance_id": "pandas-dev__pandas-stubs-1058", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes the need to add the `pandas.api.typing` module to the `pandas-stubs` repository to resolve a mypy error related to missing type stubs. The goal is evident, and a reproducible example is provided, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the expected contents or structure of the `pandas.api.typing` module (e.g., which types or classes should be included beyond `NAType`). Additionally, there is no mention of potential constraints, compatibility concerns with different pandas versions, or edge cases to consider. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4). The task involves creating a new type stub file (`pandas-stubs/api/typing/__init__.pyi`) to mirror the structure and types provided by the `pandas.api.typing` module in pandas 2.1. The code changes are confined to a single file and consist of importing and exposing existing types from other parts of the pandas library, which requires minimal logic or architectural changes. The scope of the change is limited, with no impact on the broader codebase or system architecture. \n\nFrom a technical perspective, the problem requires a basic understanding of Python type hints and stub files (used by mypy for static type checking), as well as familiarity with the pandas library's structure to identify the correct types to include. No complex algorithms, design patterns, or domain-specific knowledge beyond pandas internals are needed. The provided code changes are straightforward, involving around 29 lines of type imports and aliases, with no significant error handling or edge case logic required. While the problem statement does not explicitly mention edge cases, the nature of type stubs implies that compatibility with different pandas versions or handling of deprecated types might be a minor concern, but this is not complex to address.\n\nOverall, this task is relatively simple for someone with moderate experience in Python and type hinting, requiring only a basic understanding of the pandas library's typing system and minimal code modification.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add mypy static type checking\nLet's add a static type checking tool to this project and get all of the classes and methods to be statically typed. We can use [mypy](https://mypy-lang.org/) for this since this is what we are mainly using at Mozilla.\r\n\r\n- [ ] Add pyright to poetry and configure it in `pyproject.toml`\r\n- [ ] Fix all objects and methods to be type checked\r\n- [ ] Update test files to be type checked\r\n- [ ] Add a command to the `Makefile` to run the static type checking\r\n- [ ] Add a test to the github actions workflow to run the type checker on pull requests. \r\n\r\nHere are some basic options we should use my mypy, we can adjust these later as well. These should exist within the `pyproject.toml` file.\r\n\r\n```\r\n[tool.mypy]\r\npython_version = \"3.12\"\r\ndisable_error_code = \"attr-defined\"\r\ndisallow_untyped_calls = false\r\nfollow_imports = \"normal\"\r\nignore_missing_imports = true\r\npretty = true\r\nshow_error_codes = true\r\nstrict_optional = true\r\nwarn_no_return = true\r\nwarn_redundant_casts = true\r\nwarn_return_any = true\r\nwarn_unused_ignores = true\r\nwarn_unreachable = true\r\n```\n", "patch": "diff --git a/Makefile b/Makefile\nindex 4c942f5..380d05a 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -18,3 +18,6 @@ test: install_dependencies\n \n lint: install_dependencies\n \tpoetry run $(BLACK_CHECK)\n+\n+typecheck: install_dependencies\n+\tpoetry run mypy .\ndiff --git a/foxpuppet/expected.py b/foxpuppet/expected.py\nindex dc4a52f..c189b00 100644\n--- a/foxpuppet/expected.py\n+++ b/foxpuppet/expected.py\n@@ -4,6 +4,8 @@\n \"\"\"Module contained custom expected waits and conditions.\"\"\"\n \n from __future__ import absolute_import\n+from selenium.webdriver.remote.webdriver import WebDriver\n+from typing import Any, TYPE_CHECKING, Optional, Type\n \n \n class new_browser_window_is_opened(object):\n@@ -14,7 +16,7 @@ class new_browser_window_is_opened(object):\n \n     \"\"\"\n \n-    def __init__(self, selenium, handles):\n+    def __init__(self, selenium: WebDriver, handles: list[str]):\n         \"\"\"Create new_browser_window_is_opened object.\n \n         Args:\n@@ -27,7 +29,10 @@ def __init__(self, selenium, handles):\n         self.selenium = selenium\n         self.handles = handles\n \n-    def __call__(self, *args, **kwargs):\n+    if TYPE_CHECKING:\n+        from foxpuppet.windows import BrowserWindow  # Import for static typing\n+\n+    def __call__(self, *args: Any, **kwargs: Any) -> Optional[\"BrowserWindow\"]:\n         \"\"\"Check to see if a new window has opened.\n \n         Returns:\n@@ -39,3 +44,5 @@ def __call__(self, *args, **kwargs):\n             from foxpuppet.windows import BrowserWindow\n \n             return BrowserWindow(self.selenium, handles[0])\n+        else:\n+            return None\ndiff --git a/foxpuppet/foxpuppet.py b/foxpuppet/foxpuppet.py\nindex 5a9f4b9..135d585 100644\n--- a/foxpuppet/foxpuppet.py\n+++ b/foxpuppet/foxpuppet.py\n@@ -6,6 +6,7 @@\n from __future__ import absolute_import\n \n from foxpuppet.windows import WindowManager\n+from selenium.webdriver.remote.webdriver import WebDriver\n \n \n class FoxPuppet(object):\n@@ -17,7 +18,7 @@ class FoxPuppet(object):\n             Firefox WebDriver object.\n     \"\"\"\n \n-    def __init__(self, selenium):\n+    def __init__(self, selenium: WebDriver):\n         \"\"\"Create FoxPuppet object.\n \n         Args:\ndiff --git a/foxpuppet/region.py b/foxpuppet/region.py\nindex d157a7f..21ca1b3 100644\n--- a/foxpuppet/region.py\n+++ b/foxpuppet/region.py\n@@ -3,6 +3,11 @@\n # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n \"\"\"A Region object model for interacting with different parts of Firefox.\"\"\"\n \n+from selenium.webdriver.remote.webelement import WebElement\n+from selenium.webdriver.support.wait import WebDriverWait\n+from selenium.webdriver.remote.webdriver import WebDriver\n+from foxpuppet.windows import BaseWindow\n+\n \n class Region(object):\n     \"\"\"A region object.\n@@ -15,7 +20,9 @@ class Region(object):\n     :type root: :py:class:`~selenium.webdriver.remote.webelement.WebElement`\n     \"\"\"\n \n-    def __init__(self, window, root):\n+    from foxpuppet.windows import BaseWindow\n+\n+    def __init__(self, window: BaseWindow, root: WebElement):\n         \"\"\"Create a Region object.\n \n         Args:\n@@ -27,7 +34,7 @@ def __init__(self, window, root):\n                 region.\n \n         \"\"\"\n-        self.root = root\n-        self.selenium = window.selenium\n-        self.wait = window.wait\n-        self.window = window\n+        self.root: WebElement = root\n+        self.selenium: WebDriver = window.selenium\n+        self.wait: WebDriverWait = window.wait\n+        self.window: BaseWindow = window\ndiff --git a/foxpuppet/windows/base.py b/foxpuppet/windows/base.py\nindex c55d00f..c6d5ab7 100644\n--- a/foxpuppet/windows/base.py\n+++ b/foxpuppet/windows/base.py\n@@ -5,6 +5,8 @@\n \n from selenium.webdriver.common.by import By\n from selenium.webdriver.support.wait import WebDriverWait\n+from selenium.webdriver.remote.webdriver import WebDriver\n+from selenium.webdriver.remote.webelement import WebElement\n \n \n class BaseWindow(object):\n@@ -12,7 +14,7 @@ class BaseWindow(object):\n \n     _document_element = (By.CSS_SELECTOR, \":root\")\n \n-    def __init__(self, selenium, handle):\n+    def __init__(self, selenium: WebDriver, handle: str) -> None:\n         \"\"\"Create a BaseWindow object.\n \n         Args:\n@@ -21,12 +23,12 @@ def __init__(self, selenium, handle):\n                 Firefox WebDriver object.\n             handle: (str): WebDriver Firefox window handle.\n         \"\"\"\n-        self.selenium = selenium\n-        self.handle = handle\n-        self.wait = WebDriverWait(self.selenium, timeout=10)\n+        self.selenium: WebDriver = selenium\n+        self.handle: str = handle\n+        self.wait: WebDriverWait = WebDriverWait(self.selenium, timeout=10)\n \n     @property\n-    def document_element(self):\n+    def document_element(self) -> WebElement:\n         \"\"\"Return the inner DOM window element.\n \n         Returns:\n@@ -37,21 +39,21 @@ def document_element(self):\n         return self.selenium.find_element(*self._document_element)\n \n     @property\n-    def firefox_version(self):\n+    def firefox_version(self) -> int:\n         \"\"\"Major version of Firefox in use.\n \n         Returns:\n             int: Major component of the Firefox version.\n \n         \"\"\"\n-        version = self.selenium.capabilities[\"browserVersion\"]\n+        version: str = self.selenium.capabilities[\"browserVersion\"]\n         return int(version.partition(\".\")[0])\n \n-    def close(self):\n+    def close(self) -> None:\n         \"\"\"Close the window.\"\"\"\n         self.switch_to()\n         self.selenium.close()\n \n-    def switch_to(self):\n+    def switch_to(self) -> None:\n         \"\"\"Switch focus for Selenium commands to this window.\"\"\"\n         self.selenium.switch_to.window(self.handle)\ndiff --git a/foxpuppet/windows/browser/navbar.py b/foxpuppet/windows/browser/navbar.py\nindex 0fec61b..b141239 100644\n--- a/foxpuppet/windows/browser/navbar.py\n+++ b/foxpuppet/windows/browser/navbar.py\n@@ -23,7 +23,7 @@ class NavBar(Region):\n     _tracking_protection_shield_locator = (By.ID, \"tracking-protection-icon-box\")\n \n     @property\n-    def is_tracking_shield_displayed(self):\n+    def is_tracking_shield_displayed(self) -> bool:\n         \"\"\"Tracking Protection shield.\n \n         Returns:\ndiff --git a/foxpuppet/windows/browser/notifications/addons.py b/foxpuppet/windows/browser/notifications/addons.py\nindex 014df36..47b3846 100644\n--- a/foxpuppet/windows/browser/notifications/addons.py\n+++ b/foxpuppet/windows/browser/notifications/addons.py\n@@ -6,12 +6,13 @@\n from selenium.webdriver.common.by import By\n \n from foxpuppet.windows.browser.notifications import BaseNotification\n+from selenium.webdriver.remote.webelement import WebElement\n \n \n class AddOnInstallBlocked(BaseNotification):\n     \"\"\"Add-on install blocked notification.\"\"\"\n \n-    def allow(self):\n+    def allow(self) -> None:\n         \"\"\"Allow the add-on to be installed.\"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             self.find_primary_button().click()\n@@ -21,7 +22,7 @@ class AddOnInstallConfirmation(BaseNotification):\n     \"\"\"Add-on install confirmation notification.\"\"\"\n \n     @property\n-    def addon_name(self):\n+    def addon_name(self) -> str:\n         \"\"\"Provide access to the add-on name.\n \n         Returns:\n@@ -29,15 +30,15 @@ def addon_name(self):\n \n         \"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n-            el = self.find_description()\n+            el: WebElement = self.find_description()\n             return el.find_element(By.CSS_SELECTOR, \"b\").text\n \n-    def cancel(self):\n+    def cancel(self) -> None:\n         \"\"\"Cancel add-on install.\"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             self.find_secondary_button().click()\n \n-    def install(self):\n+    def install(self) -> None:\n         \"\"\"Confirm add-on install.\"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             self.find_primary_button().click()\n@@ -46,7 +47,7 @@ def install(self):\n class AddOnInstallComplete(BaseNotification):\n     \"\"\"Add-on install complete notification.\"\"\"\n \n-    def close(self):\n+    def close(self) -> None:\n         \"\"\"Close the notification.\"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             if self.window.firefox_version > 63:\ndiff --git a/foxpuppet/windows/browser/notifications/base.py b/foxpuppet/windows/browser/notifications/base.py\nindex d987033..6f73882 100644\n--- a/foxpuppet/windows/browser/notifications/base.py\n+++ b/foxpuppet/windows/browser/notifications/base.py\n@@ -8,15 +8,21 @@\n from selenium.webdriver.common.by import By\n \n from foxpuppet.region import Region\n+from selenium.webdriver.remote.webelement import WebElement\n+from typing import Type, Any, TYPE_CHECKING, Optional\n \n \n class BaseNotification(Region):\n     \"\"\"Abstract base class for any kind of notification.\"\"\"\n \n     __metaclass__ = ABCMeta\n+    if TYPE_CHECKING:\n+        from foxpuppet.windows import BrowserWindow\n \n     @staticmethod\n-    def create(window, root):\n+    def create(\n+        window: Optional[\"BrowserWindow\"], root: WebElement\n+    ) -> Type[\"BaseNotification\"] | Any:\n         \"\"\"Create a notification object.\n \n         Args:\n@@ -31,15 +37,15 @@ def create(window, root):\n             :py:class:`BaseNotification`: Firefox notification.\n \n         \"\"\"\n-        notifications = {}\n-        _id = root.get_property(\"id\")\n+        notifications: dict = {}\n+        _id: str | bool | WebElement | dict = root.get_property(\"id\")\n         from foxpuppet.windows.browser.notifications import addons\n \n         notifications.update(addons.NOTIFICATIONS)\n         return notifications.get(_id, BaseNotification)(window, root)\n \n     @property\n-    def label(self):\n+    def label(self) -> str | None:\n         \"\"\"Provide access to the notification label.\n \n         Returns:\n@@ -50,7 +56,7 @@ def label(self):\n             return self.root.get_attribute(\"label\")\n \n     @property\n-    def origin(self):\n+    def origin(self) -> str | None:\n         \"\"\"Provide access to the notification origin.\n \n         Returns:\n@@ -60,7 +66,7 @@ def origin(self):\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             return self.root.get_attribute(\"name\")\n \n-    def find_primary_button(self):\n+    def find_primary_button(self) -> WebElement | Any:\n         \"\"\"Retrieve the primary button.\"\"\"\n         if self.window.firefox_version >= 67:\n             return self.root.find_element(\n@@ -68,7 +74,7 @@ def find_primary_button(self):\n             )\n         return self.root.find_anonymous_element_by_attribute(\"anonid\", \"button\")\n \n-    def find_secondary_button(self):\n+    def find_secondary_button(self) -> WebElement | Any:\n         \"\"\"Retrieve the secondary button.\"\"\"\n         if self.window.firefox_version >= 67:\n             return self.root.find_element(\n@@ -76,7 +82,7 @@ def find_secondary_button(self):\n             )\n         return self.root.find_anonymous_element_by_attribute(\"anonid\", \"secondarybutton\")\n \n-    def find_description(self):\n+    def find_description(self) -> WebElement | Any:\n         \"\"\"Retrieve the notification description.\"\"\"\n         if self.window.firefox_version >= 67:\n             return self.root.find_element(By.CLASS_NAME, \"popup-notification-description\")\n@@ -84,13 +90,13 @@ def find_description(self):\n             \"class\", \"popup-notification-description\"\n         )\n \n-    def find_close_button(self):\n+    def find_close_button(self) -> WebElement | Any:\n         \"\"\"Retrieve the close button.\"\"\"\n         if self.window.firefox_version >= 67:\n             return self.root.find_element(By.CLASS_NAME, \"popup-notification-closebutton\")\n         return self.root.find_anonymous_element_by_attribute(\"anonid\", \"closebutton\")\n \n-    def close(self):\n+    def close(self) -> None:\n         \"\"\"Close the notification.\"\"\"\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n             self.find_close_button().click()\ndiff --git a/foxpuppet/windows/browser/window.py b/foxpuppet/windows/browser/window.py\nindex 091cff4..373eb54 100644\n--- a/foxpuppet/windows/browser/window.py\n+++ b/foxpuppet/windows/browser/window.py\n@@ -10,6 +10,8 @@\n from foxpuppet.windows import BaseWindow\n from foxpuppet.windows.browser.navbar import NavBar\n from foxpuppet.windows.browser.notifications import BaseNotification\n+from selenium.webdriver.remote.webelement import WebElement\n+from typing import Any, Optional, Union\n \n \n class BrowserWindow(BaseWindow):\n@@ -27,7 +29,7 @@ class BrowserWindow(BaseWindow):\n     _tab_browser_locator = (By.ID, \"tabbrowser-tabs\")\n \n     @property\n-    def navbar(self):\n+    def navbar(self) -> NavBar:\n         \"\"\"Provide access to the Navigation Bar.\n \n         Returns:\n@@ -36,11 +38,11 @@ def navbar(self):\n         \"\"\"\n         window = BaseWindow(self.selenium, self.selenium.current_window_handle)\n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\n-            el = self.selenium.find_element(*self._nav_bar_locator)\n+            el: WebElement = self.selenium.find_element(*self._nav_bar_locator)\n             return NavBar(window, el)\n \n     @property\n-    def notification(self):\n+    def notification(self) -> BaseNotification | Any:\n         \"\"\"Provide access to the currently displayed notification.\n \n         Returns:\n@@ -63,7 +65,9 @@ def notification(self):\n                 pass\n         return None  # no notification is displayed\n \n-    def wait_for_notification(self, notification_class=BaseNotification):\n+    def wait_for_notification(\n+        self, notification_class: Optional[type[\"BaseNotification\"]] = BaseNotification\n+    ) -> BaseNotification | Any:\n         \"\"\"Wait for the specified notification to be displayed.\n \n         Args:\n@@ -91,9 +95,10 @@ def wait_for_notification(self, notification_class=BaseNotification):\n                 lambda _: self.notification is None,\n                 message=\"Unexpected notification shown.\",\n             )\n+        return None\n \n     @property\n-    def is_private(self):\n+    def is_private(self) -> bool | Any:\n         \"\"\"Property that checks if the specified window is private or not.\n \n         Returns:\n@@ -112,7 +117,7 @@ def is_private(self):\n                 self.document_element,\n             )\n \n-    def open_window(self, private=False):\n+    def open_window(self, private: bool = False) -> Union[\"BrowserWindow\", Any]:\n         \"\"\"Open a new browser window.\n \n         Args:\n@@ -123,7 +128,7 @@ def open_window(self, private=False):\n             :py:class:`BrowserWindow`: Opened window.\n \n         \"\"\"\n-        handles_before = self.selenium.window_handles\n+        handles_before: list[str] = self.selenium.window_handles\n         self.switch_to()\n \n         with self.selenium.context(self.selenium.CONTEXT_CHROME):\ndiff --git a/foxpuppet/windows/manager.py b/foxpuppet/windows/manager.py\nindex b4cb2c6..68bf96b 100644\n--- a/foxpuppet/windows/manager.py\n+++ b/foxpuppet/windows/manager.py\n@@ -3,6 +3,9 @@\n # You can obtain one at http://mozilla.org/MPL/2.0/.\n \"\"\"Window Management for FoxPuppet.\"\"\"\n \n+from selenium.webdriver.remote.webdriver import WebDriver\n+from foxpuppet.windows import BrowserWindow\n+\n \n class WindowManager(object):\n     \"\"\"A window manager that controls the creation of window objects.\n@@ -12,7 +15,7 @@ class WindowManager(object):\n             Firefox WebDriver object.\n     \"\"\"\n \n-    def __init__(self, selenium):\n+    def __init__(self, selenium: WebDriver) -> None:\n         \"\"\"Create WindowManager Object.\n \n         Args:\n@@ -23,7 +26,7 @@ def __init__(self, selenium):\n         self.selenium = selenium\n \n     @property\n-    def windows(self):\n+    def windows(self) -> list[BrowserWindow]:\n         \"\"\"Return a list of all open windows.\n \n         Returns:\ndiff --git a/poetry.lock b/poetry.lock\nindex 7585bb7..bc26ce9 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1,4 +1,4 @@\n-# This file is automatically @generated by Poetry 1.8.3 and should not be changed by hand.\n+# This file is automatically @generated by Poetry 1.8.4 and should not be changed by hand.\n \n [[package]]\n name = \"attrs\"\n@@ -294,73 +294,73 @@ files = [\n \n [[package]]\n name = \"coverage\"\n-version = \"7.6.3\"\n+version = \"7.6.4\"\n description = \"Code coverage measurement for Python\"\n optional = false\n python-versions = \">=3.9\"\n files = [\n-    {file = \"coverage-7.6.3-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:6da42bbcec130b188169107ecb6ee7bd7b4c849d24c9370a0c884cf728d8e976\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:c222958f59b0ae091f4535851cbb24eb57fc0baea07ba675af718fb5302dddb2\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ab84a8b698ad5a6c365b08061920138e7a7dd9a04b6feb09ba1bfae68346ce6d\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:70a6756ce66cd6fe8486c775b30889f0dc4cb20c157aa8c35b45fd7868255c5c\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3c2e6fa98032fec8282f6b27e3f3986c6e05702828380618776ad794e938f53a\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:921fbe13492caf6a69528f09d5d7c7d518c8d0e7b9f6701b7719715f29a71e6e\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:6d99198203f0b9cb0b5d1c0393859555bc26b548223a769baf7e321a627ed4fc\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:87cd2e29067ea397a47e352efb13f976eb1b03e18c999270bb50589323294c6e\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-win32.whl\", hash = \"sha256:a3328c3e64ea4ab12b85999eb0779e6139295bbf5485f69d42cf794309e3d007\"},\n-    {file = \"coverage-7.6.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:bca4c8abc50d38f9773c1ec80d43f3768df2e8576807d1656016b9d3eeaa96fd\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:c51ef82302386d686feea1c44dbeef744585da16fcf97deea2a8d6c1556f519b\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:0ca37993206402c6c35dc717f90d4c8f53568a8b80f0bf1a1b2b334f4d488fba\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c77326300b839c44c3e5a8fe26c15b7e87b2f32dfd2fc9fee1d13604347c9b38\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6e484e479860e00da1f005cd19d1c5d4a813324e5951319ac3f3eefb497cc549\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0c6c0f4d53ef603397fc894a895b960ecd7d44c727df42a8d500031716d4e8d2\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:37be7b5ea3ff5b7c4a9db16074dc94523b5f10dd1f3b362a827af66a55198175\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:43b32a06c47539fe275106b376658638b418c7cfdfff0e0259fbf877e845f14b\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:ee77c7bef0724165e795b6b7bf9c4c22a9b8468a6bdb9c6b4281293c6b22a90f\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-win32.whl\", hash = \"sha256:43517e1f6b19f610a93d8227e47790722c8bf7422e46b365e0469fc3d3563d97\"},\n-    {file = \"coverage-7.6.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:04f2189716e85ec9192df307f7c255f90e78b6e9863a03223c3b998d24a3c6c6\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:27bd5f18d8f2879e45724b0ce74f61811639a846ff0e5c0395b7818fae87aec6\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:d546cfa78844b8b9c1c0533de1851569a13f87449897bbc95d698d1d3cb2a30f\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9975442f2e7a5cfcf87299c26b5a45266ab0696348420049b9b94b2ad3d40234\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:583049c63106c0555e3ae3931edab5669668bbef84c15861421b94e121878d3f\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2341a78ae3a5ed454d524206a3fcb3cec408c2a0c7c2752cd78b606a2ff15af4\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:a4fb91d5f72b7e06a14ff4ae5be625a81cd7e5f869d7a54578fc271d08d58ae3\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:e279f3db904e3b55f520f11f983cc8dc8a4ce9b65f11692d4718ed021ec58b83\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:aa23ce39661a3e90eea5f99ec59b763b7d655c2cada10729ed920a38bfc2b167\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-win32.whl\", hash = \"sha256:52ac29cc72ee7e25ace7807249638f94c9b6a862c56b1df015d2b2e388e51dbd\"},\n-    {file = \"coverage-7.6.3-cp312-cp312-win_amd64.whl\", hash = \"sha256:40e8b1983080439d4802d80b951f4a93d991ef3261f69e81095a66f86cf3c3c6\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:9134032f5aa445ae591c2ba6991d10136a1f533b1d2fa8f8c21126468c5025c6\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:99670790f21a96665a35849990b1df447993880bb6463a0a1d757897f30da929\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2dc7d6b380ca76f5e817ac9eef0c3686e7834c8346bef30b041a4ad286449990\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f7b26757b22faf88fcf232f5f0e62f6e0fd9e22a8a5d0d5016888cdfe1f6c1c4\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4c59d6a4a4633fad297f943c03d0d2569867bd5372eb5684befdff8df8522e39\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:f263b18692f8ed52c8de7f40a0751e79015983dbd77b16906e5b310a39d3ca21\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:79644f68a6ff23b251cae1c82b01a0b51bc40c8468ca9585c6c4b1aeee570e0b\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:71967c35828c9ff94e8c7d405469a1fb68257f686bca7c1ed85ed34e7c2529c4\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-win32.whl\", hash = \"sha256:e266af4da2c1a4cbc6135a570c64577fd3e6eb204607eaff99d8e9b710003c6f\"},\n-    {file = \"coverage-7.6.3-cp313-cp313-win_amd64.whl\", hash = \"sha256:ea52bd218d4ba260399a8ae4bb6b577d82adfc4518b93566ce1fddd4a49d1dce\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-macosx_10_13_x86_64.whl\", hash = \"sha256:8d4c6ea0f498c7c79111033a290d060c517853a7bcb2f46516f591dab628ddd3\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-macosx_11_0_arm64.whl\", hash = \"sha256:331b200ad03dbaa44151d74daeb7da2cf382db424ab923574f6ecca7d3b30de3\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:54356a76b67cf8a3085818026bb556545ebb8353951923b88292556dfa9f812d\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ebec65f5068e7df2d49466aab9128510c4867e532e07cb6960075b27658dca38\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d33a785ea8354c480515e781554d3be582a86297e41ccbea627a5c632647f2cd\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-musllinux_1_2_aarch64.whl\", hash = \"sha256:f7ddb920106bbbbcaf2a274d56f46956bf56ecbde210d88061824a95bdd94e92\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-musllinux_1_2_i686.whl\", hash = \"sha256:70d24936ca6c15a3bbc91ee9c7fc661132c6f4c9d42a23b31b6686c05073bde5\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-musllinux_1_2_x86_64.whl\", hash = \"sha256:c30e42ea11badb147f0d2e387115b15e2bd8205a5ad70d6ad79cf37f6ac08c91\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-win32.whl\", hash = \"sha256:365defc257c687ce3e7d275f39738dcd230777424117a6c76043459db131dd43\"},\n-    {file = \"coverage-7.6.3-cp313-cp313t-win_amd64.whl\", hash = \"sha256:23bb63ae3f4c645d2d82fa22697364b0046fbafb6261b258a58587441c5f7bd0\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:da29ceabe3025a1e5a5aeeb331c5b1af686daab4ff0fb4f83df18b1180ea83e2\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:df8c05a0f574d480947cba11b947dc41b1265d721c3777881da2fb8d3a1ddfba\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ec1e3b40b82236d100d259854840555469fad4db64f669ab817279eb95cd535c\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b4adeb878a374126f1e5cf03b87f66279f479e01af0e9a654cf6d1509af46c40\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:43d6a66e33b1455b98fc7312b124296dad97a2e191c80320587234a77b1b736e\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:1990b1f4e2c402beb317840030bb9f1b6a363f86e14e21b4212e618acdfce7f6\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:12f9515d875859faedb4144fd38694a761cd2a61ef9603bf887b13956d0bbfbb\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:99ded130555c021d99729fabd4ddb91a6f4cc0707df4b1daf912c7850c373b13\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-win32.whl\", hash = \"sha256:c3a79f56dee9136084cf84a6c7c4341427ef36e05ae6415bf7d787c96ff5eaa3\"},\n-    {file = \"coverage-7.6.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:aac7501ae73d4a02f4b7ac8fcb9dc55342ca98ffb9ed9f2dfb8a25d53eda0e4d\"},\n-    {file = \"coverage-7.6.3-pp39.pp310-none-any.whl\", hash = \"sha256:b9853509b4bf57ba7b1f99b9d866c422c9c5248799ab20e652bbb8a184a38181\"},\n-    {file = \"coverage-7.6.3.tar.gz\", hash = \"sha256:bb7d5fe92bd0dc235f63ebe9f8c6e0884f7360f88f3411bfed1350c872ef2054\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:5f8ae553cba74085db385d489c7a792ad66f7f9ba2ee85bfa508aeb84cf0ba07\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:8165b796df0bd42e10527a3f493c592ba494f16ef3c8b531288e3d0d72c1f6f0\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c7c8b95bf47db6d19096a5e052ffca0a05f335bc63cef281a6e8fe864d450a72\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8ed9281d1b52628e81393f5eaee24a45cbd64965f41857559c2b7ff19385df51\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0809082ee480bb8f7416507538243c8863ac74fd8a5d2485c46f0f7499f2b491\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:d541423cdd416b78626b55f123412fcf979d22a2c39fce251b350de38c15c15b\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:58809e238a8a12a625c70450b48e8767cff9eb67c62e6154a642b21ddf79baea\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:c9b8e184898ed014884ca84c70562b4a82cbc63b044d366fedc68bc2b2f3394a\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-win32.whl\", hash = \"sha256:6bd818b7ea14bc6e1f06e241e8234508b21edf1b242d49831831a9450e2f35fa\"},\n+    {file = \"coverage-7.6.4-cp310-cp310-win_amd64.whl\", hash = \"sha256:06babbb8f4e74b063dbaeb74ad68dfce9186c595a15f11f5d5683f748fa1d172\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:73d2b73584446e66ee633eaad1a56aad577c077f46c35ca3283cd687b7715b0b\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:51b44306032045b383a7a8a2c13878de375117946d68dcb54308111f39775a25\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0b3fb02fe73bed561fa12d279a417b432e5b50fe03e8d663d61b3d5990f29546\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ed8fe9189d2beb6edc14d3ad19800626e1d9f2d975e436f84e19efb7fa19469b\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b369ead6527d025a0fe7bd3864e46dbee3aa8f652d48df6174f8d0bac9e26e0e\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:ade3ca1e5f0ff46b678b66201f7ff477e8fa11fb537f3b55c3f0568fbfe6e718\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:27fb4a050aaf18772db513091c9c13f6cb94ed40eacdef8dad8411d92d9992db\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:4f704f0998911abf728a7783799444fcbbe8261c4a6c166f667937ae6a8aa522\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-win32.whl\", hash = \"sha256:29155cd511ee058e260db648b6182c419422a0d2e9a4fa44501898cf918866cf\"},\n+    {file = \"coverage-7.6.4-cp311-cp311-win_amd64.whl\", hash = \"sha256:8902dd6a30173d4ef09954bfcb24b5d7b5190cf14a43170e386979651e09ba19\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:12394842a3a8affa3ba62b0d4ab7e9e210c5e366fbac3e8b2a68636fb19892c2\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:2b6b4c83d8e8ea79f27ab80778c19bc037759aea298da4b56621f4474ffeb117\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1d5b8007f81b88696d06f7df0cb9af0d3b835fe0c8dbf489bad70b45f0e45613\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b57b768feb866f44eeed9f46975f3d6406380275c5ddfe22f531a2bf187eda27\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5915fcdec0e54ee229926868e9b08586376cae1f5faa9bbaf8faf3561b393d52\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:0b58c672d14f16ed92a48db984612f5ce3836ae7d72cdd161001cc54512571f2\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:2fdef0d83a2d08d69b1f2210a93c416d54e14d9eb398f6ab2f0a209433db19e1\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:8cf717ee42012be8c0cb205dbbf18ffa9003c4cbf4ad078db47b95e10748eec5\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-win32.whl\", hash = \"sha256:7bb92c539a624cf86296dd0c68cd5cc286c9eef2d0c3b8b192b604ce9de20a17\"},\n+    {file = \"coverage-7.6.4-cp312-cp312-win_amd64.whl\", hash = \"sha256:1032e178b76a4e2b5b32e19d0fd0abbce4b58e77a1ca695820d10e491fa32b08\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:023bf8ee3ec6d35af9c1c6ccc1d18fa69afa1cb29eaac57cb064dbb262a517f9\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:b0ac3d42cb51c4b12df9c5f0dd2f13a4f24f01943627120ec4d293c9181219ba\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f8fe4984b431f8621ca53d9380901f62bfb54ff759a1348cd140490ada7b693c\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:5fbd612f8a091954a0c8dd4c0b571b973487277d26476f8480bfa4b2a65b5d06\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:dacbc52de979f2823a819571f2e3a350a7e36b8cb7484cdb1e289bceaf35305f\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:dab4d16dfef34b185032580e2f2f89253d302facba093d5fa9dbe04f569c4f4b\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:862264b12ebb65ad8d863d51f17758b1684560b66ab02770d4f0baf2ff75da21\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:5beb1ee382ad32afe424097de57134175fea3faf847b9af002cc7895be4e2a5a\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-win32.whl\", hash = \"sha256:bf20494da9653f6410213424f5f8ad0ed885e01f7e8e59811f572bdb20b8972e\"},\n+    {file = \"coverage-7.6.4-cp313-cp313-win_amd64.whl\", hash = \"sha256:182e6cd5c040cec0a1c8d415a87b67ed01193ed9ad458ee427741c7d8513d963\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-macosx_10_13_x86_64.whl\", hash = \"sha256:a181e99301a0ae128493a24cfe5cfb5b488c4e0bf2f8702091473d033494d04f\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-macosx_11_0_arm64.whl\", hash = \"sha256:df57bdbeffe694e7842092c5e2e0bc80fff7f43379d465f932ef36f027179806\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0bcd1069e710600e8e4cf27f65c90c7843fa8edfb4520fb0ccb88894cad08b11\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:99b41d18e6b2a48ba949418db48159d7a2e81c5cc290fc934b7d2380515bd0e3\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a6b1e54712ba3474f34b7ef7a41e65bd9037ad47916ccb1cc78769bae324c01a\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-musllinux_1_2_aarch64.whl\", hash = \"sha256:53d202fd109416ce011578f321460795abfe10bb901b883cafd9b3ef851bacfc\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-musllinux_1_2_i686.whl\", hash = \"sha256:c48167910a8f644671de9f2083a23630fbf7a1cb70ce939440cd3328e0919f70\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-musllinux_1_2_x86_64.whl\", hash = \"sha256:cc8ff50b50ce532de2fa7a7daae9dd12f0a699bfcd47f20945364e5c31799fef\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-win32.whl\", hash = \"sha256:b8d3a03d9bfcaf5b0141d07a88456bb6a4c3ce55c080712fec8418ef3610230e\"},\n+    {file = \"coverage-7.6.4-cp313-cp313t-win_amd64.whl\", hash = \"sha256:f3ddf056d3ebcf6ce47bdaf56142af51bb7fad09e4af310241e9db7a3a8022e1\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:9cb7fa111d21a6b55cbf633039f7bc2749e74932e3aa7cb7333f675a58a58bf3\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:11a223a14e91a4693d2d0755c7a043db43d96a7450b4f356d506c2562c48642c\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a413a096c4cbac202433c850ee43fa326d2e871b24554da8327b01632673a076\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:00a1d69c112ff5149cabe60d2e2ee948752c975d95f1e1096742e6077affd376\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1f76846299ba5c54d12c91d776d9605ae33f8ae2b9d1d3c3703cf2db1a67f2c0\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:fe439416eb6380de434886b00c859304338f8b19f6f54811984f3420a2e03858\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:0294ca37f1ba500667b1aef631e48d875ced93ad5e06fa665a3295bdd1d95111\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:6f01ba56b1c0e9d149f9ac85a2f999724895229eb36bd997b61e62999e9b0901\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-win32.whl\", hash = \"sha256:bc66f0bf1d7730a17430a50163bb264ba9ded56739112368ba985ddaa9c3bd09\"},\n+    {file = \"coverage-7.6.4-cp39-cp39-win_amd64.whl\", hash = \"sha256:c481b47f6b5845064c65a7bc78bc0860e635a9b055af0df46fdf1c58cebf8e8f\"},\n+    {file = \"coverage-7.6.4-pp39.pp310-none-any.whl\", hash = \"sha256:3c65d37f3a9ebb703e710befdc489a38683a5b152242664b973a7b7b22348a4e\"},\n+    {file = \"coverage-7.6.4.tar.gz\", hash = \"sha256:29fc0f17b1d3fea332f8001d4558f8214af7f1d87a345f3a133c901d60347c73\"},\n ]\n \n [package.extras]\n@@ -484,72 +484,72 @@ i18n = [\"Babel (>=2.7)\"]\n \n [[package]]\n name = \"markupsafe\"\n-version = \"3.0.1\"\n+version = \"3.0.2\"\n description = \"Safely add untrusted strings to HTML/XML markup.\"\n optional = false\n python-versions = \">=3.9\"\n files = [\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:db842712984e91707437461930e6011e60b39136c7331e971952bb30465bc1a1\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:3ffb4a8e7d46ed96ae48805746755fadd0909fea2306f93d5d8233ba23dda12a\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:67c519635a4f64e495c50e3107d9b4075aec33634272b5db1cde839e07367589\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:48488d999ed50ba8d38c581d67e496f955821dc183883550a6fbc7f1aefdc170\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f31ae06f1328595d762c9a2bf29dafd8621c7d3adc130cbb46278079758779ca\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:80fcbf3add8790caddfab6764bde258b5d09aefbe9169c183f88a7410f0f6dea\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:3341c043c37d78cc5ae6e3e305e988532b072329639007fd408a476642a89fd6\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:cb53e2a99df28eee3b5f4fea166020d3ef9116fdc5764bc5117486e6d1211b25\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-win32.whl\", hash = \"sha256:db15ce28e1e127a0013dfb8ac243a8e392db8c61eae113337536edb28bdc1f97\"},\n-    {file = \"MarkupSafe-3.0.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:4ffaaac913c3f7345579db4f33b0020db693f302ca5137f106060316761beea9\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:26627785a54a947f6d7336ce5963569b5d75614619e75193bdb4e06e21d447ad\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:b954093679d5750495725ea6f88409946d69cfb25ea7b4c846eef5044194f583\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:973a371a55ce9ed333a3a0f8e0bcfae9e0d637711534bcb11e130af2ab9334e7\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:244dbe463d5fb6d7ce161301a03a6fe744dac9072328ba9fc82289238582697b\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d98e66a24497637dd31ccab090b34392dddb1f2f811c4b4cd80c230205c074a3\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:ad91738f14eb8da0ff82f2acd0098b6257621410dcbd4df20aaa5b4233d75a50\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:7044312a928a66a4c2a22644147bc61a199c1709712069a344a3fb5cfcf16915\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:a4792d3b3a6dfafefdf8e937f14906a51bd27025a36f4b188728a73382231d91\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-win32.whl\", hash = \"sha256:fa7d686ed9883f3d664d39d5a8e74d3c5f63e603c2e3ff0abcba23eac6542635\"},\n-    {file = \"MarkupSafe-3.0.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:9ba25a71ebf05b9bb0e2ae99f8bc08a07ee8e98c612175087112656ca0f5c8bf\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-macosx_10_13_universal2.whl\", hash = \"sha256:8ae369e84466aa70f3154ee23c1451fda10a8ee1b63923ce76667e3077f2b0c4\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:40f1e10d51c92859765522cbd79c5c8989f40f0419614bcdc5015e7b6bf97fc5\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5a4cb365cb49b750bdb60b846b0c0bc49ed62e59a76635095a179d440540c346\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ee3941769bd2522fe39222206f6dd97ae83c442a94c90f2b7a25d847d40f4729\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:62fada2c942702ef8952754abfc1a9f7658a4d5460fabe95ac7ec2cbe0d02abc\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:4c2d64fdba74ad16138300815cfdc6ab2f4647e23ced81f59e940d7d4a1469d9\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:fb532dd9900381d2e8f48172ddc5a59db4c445a11b9fab40b3b786da40d3b56b\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:0f84af7e813784feb4d5e4ff7db633aba6c8ca64a833f61d8e4eade234ef0c38\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-win32.whl\", hash = \"sha256:cbf445eb5628981a80f54087f9acdbf84f9b7d862756110d172993b9a5ae81aa\"},\n-    {file = \"MarkupSafe-3.0.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:a10860e00ded1dd0a65b83e717af28845bb7bd16d8ace40fe5531491de76b79f\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:e81c52638315ff4ac1b533d427f50bc0afc746deb949210bc85f05d4f15fd772\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:312387403cd40699ab91d50735ea7a507b788091c416dd007eac54434aee51da\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2ae99f31f47d849758a687102afdd05bd3d3ff7dbab0a8f1587981b58a76152a\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c97ff7fedf56d86bae92fa0a646ce1a0ec7509a7578e1ed238731ba13aabcd1c\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a7420ceda262dbb4b8d839a4ec63d61c261e4e77677ed7c66c99f4e7cb5030dd\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:45d42d132cff577c92bfba536aefcfea7e26efb975bd455db4e6602f5c9f45e7\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:4c8817557d0de9349109acb38b9dd570b03cc5014e8aabf1cbddc6e81005becd\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:6a54c43d3ec4cf2a39f4387ad044221c66a376e58c0d0e971d47c475ba79c6b5\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-win32.whl\", hash = \"sha256:c91b394f7601438ff79a4b93d16be92f216adb57d813a78be4446fe0f6bc2d8c\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313-win_amd64.whl\", hash = \"sha256:fe32482b37b4b00c7a52a07211b479653b7fe4f22b2e481b9a9b099d8a430f2f\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-macosx_10_13_universal2.whl\", hash = \"sha256:17b2aea42a7280db02ac644db1d634ad47dcc96faf38ab304fe26ba2680d359a\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-macosx_11_0_arm64.whl\", hash = \"sha256:852dc840f6d7c985603e60b5deaae1d89c56cb038b577f6b5b8c808c97580f1d\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0778de17cff1acaeccc3ff30cd99a3fd5c50fc58ad3d6c0e0c4c58092b859396\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:800100d45176652ded796134277ecb13640c1a537cad3b8b53da45aa96330453\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d06b24c686a34c86c8c1fba923181eae6b10565e4d80bdd7bc1c8e2f11247aa4\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-musllinux_1_2_aarch64.whl\", hash = \"sha256:33d1c36b90e570ba7785dacd1faaf091203d9942bc036118fab8110a401eb1a8\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-musllinux_1_2_i686.whl\", hash = \"sha256:beeebf760a9c1f4c07ef6a53465e8cfa776ea6a2021eda0d0417ec41043fe984\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-musllinux_1_2_x86_64.whl\", hash = \"sha256:bbde71a705f8e9e4c3e9e33db69341d040c827c7afa6789b14c6e16776074f5a\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-win32.whl\", hash = \"sha256:82b5dba6eb1bcc29cc305a18a3c5365d2af06ee71b123216416f7e20d2a84e5b\"},\n-    {file = \"MarkupSafe-3.0.1-cp313-cp313t-win_amd64.whl\", hash = \"sha256:730d86af59e0e43ce277bb83970530dd223bf7f2a838e086b50affa6ec5f9295\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:4935dd7883f1d50e2ffecca0aa33dc1946a94c8f3fdafb8df5c330e48f71b132\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:e9393357f19954248b00bed7c56f29a25c930593a77630c719653d51e7669c2a\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:40621d60d0e58aa573b68ac5e2d6b20d44392878e0bfc159012a5787c4e35bc8\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f94190df587738280d544971500b9cafc9b950d32efcb1fba9ac10d84e6aa4e6\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b6a387d61fe41cdf7ea95b38e9af11cfb1a63499af2759444b99185c4ab33f5b\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:8ad4ad1429cd4f315f32ef263c1342166695fad76c100c5d979c45d5570ed58b\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:e24bfe89c6ac4c31792793ad9f861b8f6dc4546ac6dc8f1c9083c7c4f2b335cd\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:2a4b34a8d14649315c4bc26bbfa352663eb51d146e35eef231dd739d54a5430a\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-win32.whl\", hash = \"sha256:242d6860f1fd9191aef5fae22b51c5c19767f93fb9ead4d21924e0bcb17619d8\"},\n-    {file = \"MarkupSafe-3.0.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:93e8248d650e7e9d49e8251f883eed60ecbc0e8ffd6349e18550925e31bd029b\"},\n-    {file = \"markupsafe-3.0.1.tar.gz\", hash = \"sha256:3e683ee4f5d0fa2dde4db77ed8dd8a876686e3fc417655c2ece9a90576905344\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:7e94c425039cde14257288fd61dcfb01963e658efbc0ff54f5306b06054700f8\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9e2d922824181480953426608b81967de705c3cef4d1af983af849d7bd619158\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:38a9ef736c01fccdd6600705b09dc574584b89bea478200c5fbf112a6b0d5579\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bbcb445fa71794da8f178f0f6d66789a28d7319071af7a496d4d507ed566270d\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:57cb5a3cf367aeb1d316576250f65edec5bb3be939e9247ae594b4bcbc317dfb\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:3809ede931876f5b2ec92eef964286840ed3540dadf803dd570c3b7e13141a3b\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:e07c3764494e3776c602c1e78e298937c3315ccc9043ead7e685b7f2b8d47b3c\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:b424c77b206d63d500bcb69fa55ed8d0e6a3774056bdc4839fc9298a7edca171\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-win32.whl\", hash = \"sha256:fcabf5ff6eea076f859677f5f0b6b5c1a51e70a376b0579e0eadef8db48c6b50\"},\n+    {file = \"MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:6af100e168aa82a50e186c82875a5893c5597a0c1ccdb0d8b40240b1f28b969a\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:9025b4018f3a1314059769c7bf15441064b2207cb3f065e6ea1e7359cb46db9d\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:93335ca3812df2f366e80509ae119189886b0f3c2b81325d39efdb84a1e2ae93\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2cb8438c3cbb25e220c2ab33bb226559e7afb3baec11c4f218ffa7308603c832\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a123e330ef0853c6e822384873bef7507557d8e4a082961e1defa947aa59ba84\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1e084f686b92e5b83186b07e8a17fc09e38fff551f3602b249881fec658d3eca\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:d8213e09c917a951de9d09ecee036d5c7d36cb6cb7dbaece4c71a60d79fb9798\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:5b02fb34468b6aaa40dfc198d813a641e3a63b98c2b05a16b9f80b7ec314185e\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:0bff5e0ae4ef2e1ae4fdf2dfd5b76c75e5c2fa4132d05fc1b0dabcd20c7e28c4\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-win32.whl\", hash = \"sha256:6c89876f41da747c8d3677a2b540fb32ef5715f97b66eeb0c6b66f5e3ef6f59d\"},\n+    {file = \"MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:70a87b411535ccad5ef2f1df5136506a10775d267e197e4cf531ced10537bd6b\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-macosx_10_13_universal2.whl\", hash = \"sha256:9778bd8ab0a994ebf6f84c2b949e65736d5575320a17ae8984a77fab08db94cf\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:846ade7b71e3536c4e56b386c2a47adf5741d2d8b94ec9dc3e92e5e1ee1e2225\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1c99d261bd2d5f6b59325c92c73df481e05e57f19837bdca8413b9eac4bd8028\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e17c96c14e19278594aa4841ec148115f9c7615a47382ecb6b82bd8fea3ab0c8\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:88416bd1e65dcea10bc7569faacb2c20ce071dd1f87539ca2ab364bf6231393c\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:2181e67807fc2fa785d0592dc2d6206c019b9502410671cc905d132a92866557\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:52305740fe773d09cffb16f8ed0427942901f00adedac82ec8b67752f58a1b22\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:ad10d3ded218f1039f11a75f8091880239651b52e9bb592ca27de44eed242a48\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-win32.whl\", hash = \"sha256:0f4ca02bea9a23221c0182836703cbf8930c5e9454bacce27e767509fa286a30\"},\n+    {file = \"MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:8e06879fc22a25ca47312fbe7c8264eb0b662f6db27cb2d3bbbc74b1df4b9b87\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:ba9527cdd4c926ed0760bc301f6728ef34d841f405abf9d4f959c478421e4efd\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:f8b3d067f2e40fe93e1ccdd6b2e1d16c43140e76f02fb1319a05cf2b79d99430\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:569511d3b58c8791ab4c2e1285575265991e6d8f8700c7be0e88f86cb0672094\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:15ab75ef81add55874e7ab7055e9c397312385bd9ced94920f2802310c930396\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f3818cb119498c0678015754eba762e0d61e5b52d34c8b13d770f0719f7b1d79\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:cdb82a876c47801bb54a690c5ae105a46b392ac6099881cdfb9f6e95e4014c6a\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:cabc348d87e913db6ab4aa100f01b08f481097838bdddf7c7a84b7575b7309ca\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:444dcda765c8a838eaae23112db52f1efaf750daddb2d9ca300bcae1039adc5c\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-win32.whl\", hash = \"sha256:bcf3e58998965654fdaff38e58584d8937aa3096ab5354d493c77d1fdd66d7a1\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl\", hash = \"sha256:e6a2a455bd412959b57a172ce6328d2dd1f01cb2135efda2e4576e8a23fa3b0f\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-macosx_10_13_universal2.whl\", hash = \"sha256:b5a6b3ada725cea8a5e634536b1b01c30bcdcd7f9c6fff4151548d5bf6b3a36c\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-macosx_11_0_arm64.whl\", hash = \"sha256:a904af0a6162c73e3edcb969eeeb53a63ceeb5d8cf642fade7d39e7963a22ddb\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:4aa4e5faecf353ed117801a068ebab7b7e09ffb6e1d5e412dc852e0da018126c\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c0ef13eaeee5b615fb07c9a7dadb38eac06a0608b41570d8ade51c56539e509d\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d16a81a06776313e817c951135cf7340a3e91e8c1ff2fac444cfd75fffa04afe\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_aarch64.whl\", hash = \"sha256:6381026f158fdb7c72a168278597a5e3a5222e83ea18f543112b2662a9b699c5\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_i686.whl\", hash = \"sha256:3d79d162e7be8f996986c064d1c7c817f6df3a77fe3d6859f6f9e7be4b8c213a\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_x86_64.whl\", hash = \"sha256:131a3c7689c85f5ad20f9f6fb1b866f402c445b220c19fe4308c0b147ccd2ad9\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-win32.whl\", hash = \"sha256:ba8062ed2cf21c07a9e295d5b8a2a5ce678b913b45fdf68c32d95d6c1291e0b6\"},\n+    {file = \"MarkupSafe-3.0.2-cp313-cp313t-win_amd64.whl\", hash = \"sha256:e444a31f8db13eb18ada366ab3cf45fd4b31e4db1236a4448f68778c1d1a5a2f\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:eaa0a10b7f72326f1372a713e73c3f739b524b3af41feb43e4921cb529f5929a\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:48032821bbdf20f5799ff537c7ac3d1fba0ba032cfc06194faffa8cda8b560ff\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1a9d3f5f0901fdec14d8d2f66ef7d035f2157240a433441719ac9a3fba440b13\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:88b49a3b9ff31e19998750c38e030fc7bb937398b1f78cfa599aaef92d693144\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:cfad01eed2c2e0c01fd0ecd2ef42c492f7f93902e39a42fc9ee1692961443a29\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:1225beacc926f536dc82e45f8a4d68502949dc67eea90eab715dea3a21c1b5f0\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:3169b1eefae027567d1ce6ee7cae382c57fe26e82775f460f0b2778beaad66c0\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:eb7972a85c54febfb25b5c4b4f3af4dcc731994c7da0d8a0b4a6eb0640e1d178\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-win32.whl\", hash = \"sha256:8c4e8c3ce11e1f92f6536ff07154f9d49677ebaaafc32db9db4620bc11ed480f\"},\n+    {file = \"MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:6e296a513ca3d94054c2c881cc913116e90fd030ad1c656b3869762b754f5f8a\"},\n+    {file = \"markupsafe-3.0.2.tar.gz\", hash = \"sha256:ee55d3edf80167e48ea11a923c7386f4669df67d7994554387f84e7d8b0a2bf0\"},\n ]\n \n [[package]]\n@@ -563,6 +563,58 @@ files = [\n     {file = \"mccabe-0.7.0.tar.gz\", hash = \"sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325\"},\n ]\n \n+[[package]]\n+name = \"mypy\"\n+version = \"1.13.0\"\n+description = \"Optional static typing for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"mypy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:6607e0f1dd1fb7f0aca14d936d13fd19eba5e17e1cd2a14f808fa5f8f6d8f60a\"},\n+    {file = \"mypy-1.13.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:8a21be69bd26fa81b1f80a61ee7ab05b076c674d9b18fb56239d72e21d9f4c80\"},\n+    {file = \"mypy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:7b2353a44d2179846a096e25691d54d59904559f4232519d420d64da6828a3a7\"},\n+    {file = \"mypy-1.13.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:0730d1c6a2739d4511dc4253f8274cdd140c55c32dfb0a4cf8b7a43f40abfa6f\"},\n+    {file = \"mypy-1.13.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:c5fc54dbb712ff5e5a0fca797e6e0aa25726c7e72c6a5850cfd2adbc1eb0a372\"},\n+    {file = \"mypy-1.13.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:581665e6f3a8a9078f28d5502f4c334c0c8d802ef55ea0e7276a6e409bc0d82d\"},\n+    {file = \"mypy-1.13.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:3ddb5b9bf82e05cc9a627e84707b528e5c7caaa1c55c69e175abb15a761cec2d\"},\n+    {file = \"mypy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:20c7ee0bc0d5a9595c46f38beb04201f2620065a93755704e141fcac9f59db2b\"},\n+    {file = \"mypy-1.13.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:3790ded76f0b34bc9c8ba4def8f919dd6a46db0f5a6610fb994fe8efdd447f73\"},\n+    {file = \"mypy-1.13.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:51f869f4b6b538229c1d1bcc1dd7d119817206e2bc54e8e374b3dfa202defcca\"},\n+    {file = \"mypy-1.13.0-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:5c7051a3461ae84dfb5dd15eff5094640c61c5f22257c8b766794e6dd85e72d5\"},\n+    {file = \"mypy-1.13.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:39bb21c69a5d6342f4ce526e4584bc5c197fd20a60d14a8624d8743fffb9472e\"},\n+    {file = \"mypy-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:164f28cb9d6367439031f4c81e84d3ccaa1e19232d9d05d37cb0bd880d3f93c2\"},\n+    {file = \"mypy-1.13.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:a4c1bfcdbce96ff5d96fc9b08e3831acb30dc44ab02671eca5953eadad07d6d0\"},\n+    {file = \"mypy-1.13.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:a0affb3a79a256b4183ba09811e3577c5163ed06685e4d4b46429a271ba174d2\"},\n+    {file = \"mypy-1.13.0-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:a7b44178c9760ce1a43f544e595d35ed61ac2c3de306599fa59b38a6048e1aa7\"},\n+    {file = \"mypy-1.13.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:5d5092efb8516d08440e36626f0153b5006d4088c1d663d88bf79625af3d1d62\"},\n+    {file = \"mypy-1.13.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:de2904956dac40ced10931ac967ae63c5089bd498542194b436eb097a9f77bc8\"},\n+    {file = \"mypy-1.13.0-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:7bfd8836970d33c2105562650656b6846149374dc8ed77d98424b40b09340ba7\"},\n+    {file = \"mypy-1.13.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:9f73dba9ec77acb86457a8fc04b5239822df0c14a082564737833d2963677dbc\"},\n+    {file = \"mypy-1.13.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:100fac22ce82925f676a734af0db922ecfea991e1d7ec0ceb1e115ebe501301a\"},\n+    {file = \"mypy-1.13.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:7bcb0bb7f42a978bb323a7c88f1081d1b5dee77ca86f4100735a6f541299d8fb\"},\n+    {file = \"mypy-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:bde31fc887c213e223bbfc34328070996061b0833b0a4cfec53745ed61f3519b\"},\n+    {file = \"mypy-1.13.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:07de989f89786f62b937851295ed62e51774722e5444a27cecca993fc3f9cd74\"},\n+    {file = \"mypy-1.13.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:4bde84334fbe19bad704b3f5b78c4abd35ff1026f8ba72b29de70dda0916beb6\"},\n+    {file = \"mypy-1.13.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:0246bcb1b5de7f08f2826451abd947bf656945209b140d16ed317f65a17dc7dc\"},\n+    {file = \"mypy-1.13.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:7f5b7deae912cf8b77e990b9280f170381fdfbddf61b4ef80927edd813163732\"},\n+    {file = \"mypy-1.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:7029881ec6ffb8bc233a4fa364736789582c738217b133f1b55967115288a2bc\"},\n+    {file = \"mypy-1.13.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:3e38b980e5681f28f033f3be86b099a247b13c491f14bb8b1e1e134d23bb599d\"},\n+    {file = \"mypy-1.13.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:a6789be98a2017c912ae6ccb77ea553bbaf13d27605d2ca20a76dfbced631b24\"},\n+    {file = \"mypy-1.13.0-py3-none-any.whl\", hash = \"sha256:9c250883f9fd81d212e0952c92dbfcc96fc237f4b7c92f56ac81fd48460b3e5a\"},\n+    {file = \"mypy-1.13.0.tar.gz\", hash = \"sha256:0291a61b6fbf3e6673e3405cfcc0e7650bebc7939659fdca2702958038bd835e\"},\n+]\n+\n+[package.dependencies]\n+mypy-extensions = \">=1.0.0\"\n+typing-extensions = \">=4.6.0\"\n+\n+[package.extras]\n+dmypy = [\"psutil (>=4.0)\"]\n+faster-cache = [\"orjson\"]\n+install-types = [\"pip\"]\n+mypyc = [\"setuptools (>=50)\"]\n+reports = [\"lxml\"]\n+\n [[package]]\n name = \"mypy-extensions\"\n version = \"1.0.0\"\n@@ -930,13 +982,13 @@ test = [\"pytest\", \"tornado (>=4.5)\", \"typeguard\"]\n \n [[package]]\n name = \"trio\"\n-version = \"0.26.2\"\n+version = \"0.27.0\"\n description = \"A friendly Python library for async concurrency and I/O\"\n optional = false\n python-versions = \">=3.8\"\n files = [\n-    {file = \"trio-0.26.2-py3-none-any.whl\", hash = \"sha256:c5237e8133eb0a1d72f09a971a55c28ebe69e351c783fc64bc37db8db8bbe1d0\"},\n-    {file = \"trio-0.26.2.tar.gz\", hash = \"sha256:0346c3852c15e5c7d40ea15972c4805689ef2cb8b5206f794c9c19450119f3a4\"},\n+    {file = \"trio-0.27.0-py3-none-any.whl\", hash = \"sha256:68eabbcf8f457d925df62da780eff15ff5dc68fd6b367e2dde59f7aaf2a0b884\"},\n+    {file = \"trio-0.27.0.tar.gz\", hash = \"sha256:1dcc95ab1726b2da054afea8fd761af74bad79bd52381b84eae408e983c76831\"},\n ]\n \n [package.dependencies]\n@@ -1026,4 +1078,4 @@ h11 = \">=0.9.0,<1\"\n [metadata]\n lock-version = \"2.0\"\n python-versions = \"^3.12\"\n-content-hash = \"0c3aebede8d745ea7ec624ae1f2fe472ce964c8d2cde282f345cfc6741ca724a\"\n+content-hash = \"20b6ead3b56a318d71eee3843aecf83a62a5d7e0576dfbad0a82467ba9294d28\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 29f1fa3..adc2e25 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -18,6 +18,7 @@ pytest = \"^8.3.3\"\n pytest-cov = \"^5.0.0\"\n pytest-selenium = \"^4.1.0\"\n black = \"^24.10.0\"\n+mypy = \"^1.12.1\"\n \n [build-system]\n requires = [\"poetry-core\"]\n@@ -29,3 +30,19 @@ line-length = 99\n [tool.isort]\n profile = \"black\"\n skip_gitignore = true\n+\n+[tool.mypy]\n+python_version = \"3.12\"\n+disable_error_code = \"attr-defined\"\n+disallow_untyped_calls = false\n+follow_imports = \"normal\"\n+ignore_missing_imports = true\n+pretty = true\n+show_error_codes = true\n+strict_optional = true\n+warn_no_return = true\n+warn_redundant_casts = true\n+warn_return_any = true\n+warn_unused_ignores = true\n+warn_unreachable = true\n+exclude = \"docs/.*\"\n", "instance_id": "mozilla__FoxPuppet-321", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to add static type checking using mypy to the project. It outlines specific tasks such as adding mypy to the project configuration, fixing type annotations in classes and methods, updating test files, and integrating type checking into the Makefile and GitHub Actions workflow. Additionally, it provides a sample configuration for mypy in the `pyproject.toml` file. However, there are minor ambiguities and missing details. For instance, it does not specify the expected level of strictness for type annotations beyond the provided configuration, nor does it address potential challenges or edge cases in retrofitting type annotations to an existing codebase (e.g., handling third-party libraries without type hints). There are also no examples of how to handle complex typing scenarios that might arise in the codebase. Furthermore, the problem statement mentions \"pyright\" in one of the tasks, which appears to be a typo or confusion with mypy, adding a slight ambiguity. Overall, while the goal and steps are clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, as it involves adding type annotations across multiple files and modules in the codebase, as seen in the provided diffs (e.g., `foxpuppet/expected.py`, `foxpuppet/region.py`, etc.). This requires a thorough understanding of the existing code structure and logic to ensure correct type annotations. Second, it involves multiple technical concepts, including Python's typing system (e.g., `typing` module, `TYPE_CHECKING` for conditional imports), familiarity with mypy configuration and its strictness options, and integration with build tools like Poetry, Makefile, and GitHub Actions. While these concepts are not overly complex for an experienced Python developer, they do require a moderate level of expertise and attention to detail, especially when retrofitting types to an existing project. Third, the changes impact multiple parts of the development workflow (code, tests, CI/CD), but they do not significantly alter the system's architecture or core functionality. Finally, while the problem statement does not explicitly mention edge cases, adding type annotations often involves handling tricky scenarios such as dynamic behavior, third-party library interactions, or circular imports, which may require workarounds like `TYPE_CHECKING` or `Any` types. Overall, this task requires a solid understanding of Python typing and project configuration, placing it in the medium difficulty range at 0.55, as it is more involved than a simple feature addition but not as complex as a deep architectural refactor or advanced algorithmic implementation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature] MQE support `sort/sort_labels` functions.\n### Search before asking\n\n- [X] I had searched in the [issues](https://github.com/apache/skywalking/issues?q=is%3Aissue) and found no similar feature requirement.\n\n\n### Description\n\n- `sort` function: Sort the expression result values, support in ascending order and descending order.\r\n- `sort_labels`: Sort the expression result labels, support in ascending order and descending order. The order should be [natural sort order](https://en.wikipedia.org/wiki/Natural_sort_order)\r\n\n\n### Use case\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a pull request to implement this on your own?\n\n- [X] Yes I am willing to submit a pull request on my own!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://www.apache.org/foundation/policies/conduct)\n\n", "patch": "diff --git a/docs/en/api/metrics-query-expression.md b/docs/en/api/metrics-query-expression.md\nindex 95a7d88060ef..b43c615d0f4b 100644\n--- a/docs/en/api/metrics-query-expression.md\n+++ b/docs/en/api/metrics-query-expression.md\n@@ -232,9 +232,8 @@ Expression:\n top_n(<metric_name>, <top_number>, <order>)\n ```\n \n-`top_number` is the number of the top results, should be a positive integer.\n-\n-`order` is the order of the top results. The value of `order` can be `asc` or `des`.\n+- `top_number` is the number of the top results, should be a positive integer.\n+- `order` is the order of the top results. The value of `order` can be `asc` or `des`.\n \n For example:\n If we want to query the current service's top 10 instances with the highest `service_instance_cpm` metric value, we can use the following expression\n@@ -273,11 +272,11 @@ AggregateLabels Operation takes an expression and performs an aggregate calculat\n \n Expression:\n ```text\n-aggregate_labels(Expression, AggregateType<Optional>(<label1_name>,<label2_name>...))\n+aggregate_labels(Expression, <AggregateType>(<label1_name>,<label2_name>...))\n ```\n \n - `AggregateType` is the type of the aggregation operation.\n-- `<label1_name>,<label2_name>...` is the label names that need to be aggregated. If not specified, all labels will be aggregated.\n+- `<label1_name>,<label2_name>...` is the label names that need to be aggregated. If not specified, all labels will be aggregated. Optional.\n \n | AggregateType | Definition                                         | ExpressionResultType |\n |---------------|----------------------------------------------------|----------------------|\n@@ -379,6 +378,62 @@ V(T1)-V(T1-2), V(T2)-V(T1-1), V(T3)-V(T1)\n ### Result Type\n TIME_SERIES_VALUES.\n \n+## Sort Operation\n+### SortValues Operation\n+SortValues Operation takes an expression and sorts the values of the input expression result.\n+\n+Expression:\n+```text\n+sort_values(Expression, <limit>, <order>)\n+```\n+- `limit` is the number of the sort results, should be a positive integer, if not specified, will return all results. Optional.\n+- `order` is the order of the sort results. The value of `order` can be `asc` or `des`.\n+\n+For example:\n+If we want to sort the `service_resp_time` metric values in descending order and get the top 10 values, we can use the following expression:\n+```text\n+sort_values(service_resp_time, 10, des)\n+```\n+\n+#### Result Type\n+The result type follows the input expression.\n+\n+### SortLabelValues Operation\n+SortLabelValues Operation takes an expression and sorts the label values of the input expression result. This function uses `natural sort order`.\n+\n+Expression:\n+```text\n+sort_label_values(Expression, <order>, <label1_name>, <label2_name> ...)\n+```\n+- `order` is the order of the sort results. The value of `order` can be `asc` or `des`.\n+- `<label1_name>, <label2_name> ...` is the label names that need to be sorted by their values. At least one label name should be specified.\n+The labels in the head of the list will be sorted first, and if the label not be included in the expression result will be ignored.\n+\n+For example:\n+If we want to sort the `service_percentile` metric label values in descending order by the `p` label, we can use the following expression:\n+```text\n+sort_label_values(service_percentile{p='50,75,90,95,99'}, des, p)\n+```\n+\n+For multiple labels, assume the metric has 2 labels\uff1a\n+```text\n+metric{label1='a', label2='2a'} \n+metric{label1='a', label2='2c'}\n+metric{label1='b', label2='2a'}\n+metric{label1='b', label2='2c'}\n+```\n+If we want to sort the `metric` metric label values in descending order by the `label1` and `label2` labels, we can use the following expression:\n+```text\n+sort_label_values(metric, des, label1, label2)\n+```\n+And the result will be:\n+```text\n+metric{label1='b', label2='2c'}\n+metric{label1='b', label2='2a'}\n+metric{label1='a', label2='2c'}\n+metric{label1='a', label2='2a'}\n+```\n+\n ## Expression Query Example\n ### Labeled Value Metrics\n ```text\ndiff --git a/docs/en/changes/changes.md b/docs/en/changes/changes.md\nindex a14d03b635cc..8b9f28a1d277 100644\n--- a/docs/en/changes/changes.md\n+++ b/docs/en/changes/changes.md\n@@ -120,6 +120,7 @@\n * Add component definition(ID=152) for `c3p0`(JDBC3 Connection and Statement Pooling).\n * Fix MQE `top_n` global query. \n * Fix inaccurate Pulsar and Bookkeeper metrics.\n+* MQE support `sort_values` and `sort_label_values` functions.\n \n #### UI\n \ndiff --git a/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQELexer.g4 b/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQELexer.g4\nindex 7302c7ed7ba9..b91139ee8552 100644\n--- a/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQELexer.g4\n+++ b/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQELexer.g4\n@@ -81,6 +81,10 @@ DES options { caseInsensitive=true; }: 'des';\n // AGGREGATE_LABELS\n AGGREGATE_LABELS:   'aggregate_labels';\n \n+// Sort\n+SORT_VALUES: 'sort_values';\n+SORT_LABEL_VALUES: 'sort_label_values';\n+\n // Literals\n INTEGER: Digit+;\n DECIMAL: Digit+ DOT Digit+;\ndiff --git a/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQEParser.g4 b/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQEParser.g4\nindex b8768392b850..d83c72365c82 100644\n--- a/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQEParser.g4\n+++ b/oap-server/mqe-grammar/src/main/antlr4/org/apache/skywalking/mqe/rt/grammar/MQEParser.g4\n@@ -36,6 +36,8 @@ expression\n     | topN L_PAREN metric COMMA INTEGER COMMA order R_PAREN  #topNOP\n     | relabels L_PAREN expression COMMA label COMMA replaceLabel R_PAREN #relablesOP\n     | aggregateLabels L_PAREN expression COMMA aggregateLabelsFunc R_PAREN #aggregateLabelsOp\n+    | sort_values L_PAREN expression (COMMA INTEGER)? COMMA order R_PAREN #sortValuesOP\n+    | sort_label_values L_PAREN expression COMMA order COMMA labelNameList R_PAREN #sortLabelValuesOP\n     ;\n \n expressionList\n@@ -91,3 +93,9 @@ aggregateLabelsFunc: aggregateLabelsFuncName (L_PAREN labelNameList R_PAREN)?;\n \n aggregateLabelsFuncName:\n     AVG | SUM | MAX | MIN;\n+\n+sort_values:\n+    SORT_VALUES;\n+\n+sort_label_values:\n+    SORT_LABEL_VALUES;\ndiff --git a/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/MQEVisitorBase.java b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/MQEVisitorBase.java\nindex 23b2d677d630..4416176b83ea 100644\n--- a/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/MQEVisitorBase.java\n+++ b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/MQEVisitorBase.java\n@@ -22,6 +22,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.skywalking.mqe.rt.grammar.MQEParser;\n import org.apache.skywalking.mqe.rt.grammar.MQEParserBaseVisitor;\n@@ -31,6 +32,8 @@\n import org.apache.skywalking.mqe.rt.operation.CompareOp;\n import org.apache.skywalking.mqe.rt.operation.LogicalFunctionOp;\n import org.apache.skywalking.mqe.rt.operation.MathematicalFunctionOp;\n+import org.apache.skywalking.mqe.rt.operation.SortLabelValuesOp;\n+import org.apache.skywalking.mqe.rt.operation.SortValuesOp;\n import org.apache.skywalking.mqe.rt.operation.TrendOp;\n import org.apache.skywalking.mqe.rt.type.ExpressionResult;\n import org.apache.skywalking.mqe.rt.exception.IllegalExpressionException;\n@@ -311,6 +314,42 @@ public ExpressionResult visitTrendOP(MQEParser.TrendOPContext ctx) {\n         }\n     }\n \n+    @Override\n+    public ExpressionResult visitSortValuesOP(MQEParser.SortValuesOPContext ctx) {\n+        ExpressionResult result = visit(ctx.expression());\n+        int order = ctx.order().getStart().getType();\n+        Optional<Integer> limit = Optional.empty();\n+        if (ctx.INTEGER() != null) {\n+            limit = Optional.of(Integer.valueOf(ctx.INTEGER().getText()));\n+        }\n+        try {\n+            return SortValuesOp.doSortValuesOp(result, limit, order);\n+        } catch (IllegalExpressionException e) {\n+            ExpressionResult errorResult = new ExpressionResult();\n+            errorResult.setType(ExpressionResultType.UNKNOWN);\n+            errorResult.setError(e.getMessage());\n+            return errorResult;\n+        }\n+    }\n+\n+    @Override\n+    public ExpressionResult visitSortLabelValuesOP(MQEParser.SortLabelValuesOPContext ctx) {\n+        ExpressionResult result = visit(ctx.expression());\n+        int order = ctx.order().getStart().getType();\n+        List<String> labelNames = new ArrayList<>();\n+        for (MQEParser.LabelNameContext labelNameContext : ctx.labelNameList().labelName()) {\n+            labelNames.add(labelNameContext.getText());\n+        }\n+        try {\n+            return SortLabelValuesOp.doSortLabelValuesOp(result, order, labelNames);\n+        } catch (IllegalExpressionException e) {\n+            ExpressionResult errorResult = new ExpressionResult();\n+            errorResult.setType(ExpressionResultType.UNKNOWN);\n+            errorResult.setError(e.getMessage());\n+            return errorResult;\n+        }\n+    }\n+\n     @Override\n     public abstract ExpressionResult visitMetric(MQEParser.MetricContext ctx);\n \ndiff --git a/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortLabelValuesOp.java b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortLabelValuesOp.java\nnew file mode 100644\nindex 000000000000..828e5e84a55e\n--- /dev/null\n+++ b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortLabelValuesOp.java\n@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.skywalking.mqe.rt.operation;\n+\n+import java.util.Comparator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.skywalking.mqe.rt.exception.IllegalExpressionException;\n+import org.apache.skywalking.mqe.rt.grammar.MQEParser;\n+import org.apache.skywalking.mqe.rt.type.ExpressionResult;\n+import org.apache.skywalking.mqe.rt.type.MQEValues;\n+import org.apache.skywalking.oap.server.core.query.type.KeyValue;\n+import org.apache.skywalking.oap.server.library.util.CollectionUtils;\n+\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+\n+@Slf4j\n+public class SortLabelValuesOp {\n+    public static ExpressionResult doSortLabelValuesOp(ExpressionResult expResult,\n+                                                      int order,\n+                                                      List<String> labelNames) throws IllegalExpressionException {\n+        if (CollectionUtils.isNotEmpty(labelNames)) {\n+            labelNames = labelNames.stream().distinct().collect(toList());\n+            if (MQEParser.ASC == order) {\n+                expResult.setResults(\n+                    sort(expResult.getResults(), labelNames, labelNames.get(0), Comparator.naturalOrder()));\n+            } else if (MQEParser.DES == order) {\n+                expResult.setResults(\n+                    sort(expResult.getResults(), labelNames, labelNames.get(0), Comparator.reverseOrder()));\n+            } else {\n+                throw new IllegalExpressionException(\"Unsupported sort order.\");\n+            }\n+        }\n+        return expResult;\n+    }\n+\n+    private static List<MQEValues> sort(List<MQEValues> results,\n+                                        List<String> sortLabels,\n+                                        String currentSortLabel,\n+                                        Comparator<String> comparator) {\n+        if (!sortLabels.contains(currentSortLabel)) {\n+            log.error(\"Current sort label {} not found in the sort labels {} \", currentSortLabel, sortLabels);\n+            return results;\n+        }\n+        if (sortLabels.indexOf(\n+            currentSortLabel) == sortLabels.size() - 1) { //only one label or the latest label no need to group\n+            results.sort(Comparator.comparing(mqeValues -> mqeValues.getMetric()\n+                                                                    .getLabels()\n+                                                                    .stream()\n+                                                                    .filter(kv -> kv.getKey().equals(currentSortLabel))\n+                                                                    .findFirst()\n+                                                                    .orElse(new KeyValue(currentSortLabel, \"\"))\n+                                                                    .getValue(), comparator));\n+        } else {\n+            LinkedHashMap<KeyValue, List<MQEValues>> groupResult = group(results, currentSortLabel);\n+            LinkedHashMap<KeyValue, List<MQEValues>> sortedGroup = new LinkedHashMap<>(groupResult.size());\n+            for (Map.Entry<KeyValue, List<MQEValues>> entry : groupResult.entrySet()) {\n+                //sort by next label\n+                List<MQEValues> sortedResult = sort(\n+                    entry.getValue(), sortLabels, sortLabels.get(sortLabels.indexOf(currentSortLabel) + 1), comparator);\n+                sortedGroup.put(entry.getKey(), sortedResult);\n+            }\n+            //combine the sorted group\n+            results = sortedGroup.keySet()\n+                                 .stream()\n+                                 .sorted(Comparator.comparing(KeyValue::getValue, comparator))\n+                                 .flatMap(keyValue -> sortedGroup.get(keyValue).stream())\n+                                 .collect(toList());\n+        }\n+        return results;\n+    }\n+\n+    //group by current label for sub sorting\n+    private static LinkedHashMap<KeyValue, List<MQEValues>> group(List<MQEValues> results, String labelName) {\n+        return results\n+            .stream()\n+            .collect(groupingBy(\n+                mqeValues -> mqeValues.getMetric().getLabels()\n+                                      .stream()\n+                                      .filter(kv -> kv.getKey().equals(labelName))\n+                                      .findFirst().orElse(new KeyValue(labelName, \"\")),\n+                LinkedHashMap::new,\n+                toList()\n+            ));\n+    }\n+}\n+\ndiff --git a/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortValuesOp.java b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortValuesOp.java\nnew file mode 100644\nindex 000000000000..44053555f994\n--- /dev/null\n+++ b/oap-server/mqe-rt/src/main/java/org/apache/skywalking/mqe/rt/operation/SortValuesOp.java\n@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.skywalking.mqe.rt.operation;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.skywalking.mqe.rt.exception.IllegalExpressionException;\n+import org.apache.skywalking.mqe.rt.grammar.MQEParser;\n+import org.apache.skywalking.mqe.rt.type.ExpressionResult;\n+import org.apache.skywalking.mqe.rt.type.MQEValue;\n+\n+public class SortValuesOp {\n+    public static ExpressionResult doSortValuesOp(ExpressionResult expResult,\n+                                                 Optional<Integer> limit,\n+                                                 int order) throws IllegalExpressionException {\n+        if (MQEParser.ASC == order || MQEParser.DES == order) {\n+            expResult.getResults().forEach(mqeValues -> {\n+                List<MQEValue> values = mqeValues.getValues()\n+                                                 .stream()\n+                                                 // Filter out empty values\n+                                                 .filter(mqeValue -> !mqeValue.isEmptyValue())\n+                                                 .sorted(MQEParser.ASC == order ? Comparator.comparingDouble(\n+                                                     MQEValue::getDoubleValue) :\n+                                                             Comparator.comparingDouble(MQEValue::getDoubleValue)\n+                                                                       .reversed())\n+                                                 .collect(\n+                                                     Collectors.toList());\n+                if (limit.isPresent() && limit.get() < values.size()) {\n+                    mqeValues.setValues(values.subList(0, limit.get()));\n+                } else {\n+                    mqeValues.setValues(values);\n+                }\n+            });\n+        } else {\n+            throw new IllegalExpressionException(\"Unsupported sort order.\");\n+        }\n+        return expResult;\n+    }\n+}\ndiff --git a/skywalking-ui b/skywalking-ui\nindex 13b2693f2919..e4b2203cf6f3 160000\n--- a/skywalking-ui\n+++ b/skywalking-ui\n@@ -1,1 +1,1 @@\n-Subproject commit 13b2693f291903349ebaffa661092e2747141f1c\n+Subproject commit e4b2203cf6f33b1fd84b79ced30ee7c50e15e6d8\n", "instance_id": "apache__skywalking-12193", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in defining the goal of adding `sort` and `sort_labels` functions to support sorting of expression result values and labels in ascending or descending order, with a reference to natural sort order for labels. It provides a basic description of the features and includes examples in the documentation changes. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention edge cases (e.g., handling empty results, duplicate values, or invalid input for sort order). Additionally, the use case section is empty, which could have provided more context on real-world applications of these features. While the code changes and documentation updates help clarify the intent, the lack of comprehensive constraints or edge case specifications prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files, including grammar definitions (ANTLR files), core logic implementation in Java, and documentation updates, requiring a moderate understanding of the codebase structure. The changes involve adding new functionality (`sort_values` and `sort_label_values`) which necessitates implementing sorting logic with natural sort order for labels and handling optional parameters like limit. This requires familiarity with Java's Comparator, stream API, and data structure manipulation (e.g., LinkedHashMap for grouping), as well as understanding the existing expression evaluation framework in the project. The technical concepts involved include parsing (ANTLR grammar), sorting algorithms, and handling hierarchical label sorting, which adds moderate complexity. Edge case handling (e.g., empty results, missing labels, invalid sort order) is implemented to some extent, but not exhaustively tested or specified, adding a layer of complexity in ensuring robustness. However, the changes do not significantly impact the system's architecture or require deep domain-specific knowledge beyond the project's context, preventing it from being classified as hard. Overall, this task requires a solid understanding of multiple concepts and careful implementation across several files, justifying a score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "JSON Convertor :  Pipeline Component\n**Is your feature request related to a problem? Please describe.**\r\nCurrently we have a `.txt` to `Document` convertor besides others and unstructured. But I see most of the data we deal with are in the form of JSON.\r\n\r\n**Describe the solution you'd like**\r\nSo a `.json` to `Document` convertor will be a bread winner while consuming API data in pipelines.\r\n\r\n**Describe alternatives you've considered**\r\n[Unstructured file convertor](https://docs.haystack.deepset.ai/docs/unstructuredfileconverter) is present but JSON schema as a individual convertor adds more sense and value.\r\n\n", "patch": "diff --git a/docs/pydoc/config/converters_api.yml b/docs/pydoc/config/converters_api.yml\nindex 945ae37430..6c89138c98 100644\n--- a/docs/pydoc/config/converters_api.yml\n+++ b/docs/pydoc/config/converters_api.yml\n@@ -4,17 +4,18 @@ loaders:\n     modules:\n       [\n         \"azure\",\n+        \"csv\",\n+        \"docx\",\n         \"html\",\n+        \"json\",\n         \"markdown\",\n+        \"openapi_functions\",\n+        \"output_adapter\",\n         \"pdfminer\",\n-        \"pypdf\",\n         \"pptx\",\n+        \"pypdf\",\n         \"tika\",\n         \"txt\",\n-        \"output_adapter\",\n-        \"openapi_functions\",\n-        \"docx\",\n-        \"csv\"\n       ]\n     ignore_when_discovered: [\"__init__\"]\n processors:\ndiff --git a/haystack/components/converters/__init__.py b/haystack/components/converters/__init__.py\nindex 681ab85c35..4561dd1e0a 100644\n--- a/haystack/components/converters/__init__.py\n+++ b/haystack/components/converters/__init__.py\n@@ -6,6 +6,7 @@\n from haystack.components.converters.csv import CSVToDocument\n from haystack.components.converters.docx import DOCXMetadata, DOCXToDocument\n from haystack.components.converters.html import HTMLToDocument\n+from haystack.components.converters.json import JSONConverter\n from haystack.components.converters.markdown import MarkdownToDocument\n from haystack.components.converters.openapi_functions import OpenAPIServiceToFunctions\n from haystack.components.converters.output_adapter import OutputAdapter\n@@ -29,4 +30,5 @@\n     \"DOCXMetadata\",\n     \"PPTXToDocument\",\n     \"CSVToDocument\",\n+    \"JSONConverter\",\n ]\ndiff --git a/haystack/components/converters/json.py b/haystack/components/converters/json.py\nnew file mode 100644\nindex 0000000000..966da881b8\n--- /dev/null\n+++ b/haystack/components/converters/json.py\n@@ -0,0 +1,277 @@\n+# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>\n+#\n+# SPDX-License-Identifier: Apache-2.0\n+\n+import json\n+from pathlib import Path\n+from typing import Any, Dict, List, Literal, Optional, Set, Tuple, Union\n+\n+from haystack import component, default_from_dict, default_to_dict, logging\n+from haystack.components.converters.utils import get_bytestream_from_source, normalize_metadata\n+from haystack.dataclasses import ByteStream, Document\n+from haystack.lazy_imports import LazyImport\n+\n+logger = logging.getLogger(__name__)\n+\n+with LazyImport(\"Run 'pip install jq'\") as jq_import:\n+    import jq\n+\n+\n+@component\n+class JSONConverter:\n+    \"\"\"\n+    Converts one or more JSON files into a text document.\n+\n+    ### Usage examples\n+\n+    ```python\n+    import json\n+\n+    from haystack.components.converters import JSONConverter\n+    from haystack.dataclasses import ByteStream\n+\n+    source = ByteStream.from_string(json.dumps({\"text\": \"This is the content of my document\"}))\n+\n+    converter = JSONConverter(content_key=\"text\")\n+    results = converter.run(sources=[source])\n+    documents = results[\"documents\"]\n+    print(documents[0].content)\n+    # 'This is the content of my document'\n+    ```\n+\n+    Optionally, you can also provide a `jq_schema` string to filter the JSON source files and `extra_meta_fields`\n+    to extract from the filtered data:\n+\n+    ```python\n+    import json\n+\n+    from haystack.components.converters import JSONConverter\n+    from haystack.dataclasses import ByteStream\n+\n+    data = {\n+        \"laureates\": [\n+            {\n+                \"firstname\": \"Enrico\",\n+                \"surname\": \"Fermi\",\n+                \"motivation\": \"for his demonstrations of the existence of new radioactive elements produced \"\n+                \"by neutron irradiation, and for his related discovery of nuclear reactions brought about by\"\n+                \" slow neutrons\",\n+            },\n+            {\n+                \"firstname\": \"Rita\",\n+                \"surname\": \"Levi-Montalcini\",\n+                \"motivation\": \"for their discoveries of growth factors\",\n+            },\n+        ],\n+    }\n+    source = ByteStream.from_string(json.dumps(data))\n+    converter = JSONConverter(\n+        jq_schema=\".laureates[]\", content_key=\"motivation\", extra_meta_fields={\"firstname\", \"surname\"}\n+    )\n+\n+    results = converter.run(sources=[source])\n+    documents = results[\"documents\"]\n+    print(documents[0].content)\n+    # 'for his demonstrations of the existence of new radioactive elements produced by\n+    # neutron irradiation, and for his related discovery of nuclear reactions brought\n+    # about by slow neutrons'\n+\n+    print(documents[0].meta)\n+    # {'firstname': 'Enrico', 'surname': 'Fermi'}\n+\n+    print(documents[1].content)\n+    # 'for their discoveries of growth factors'\n+\n+    print(documents[1].meta)\n+    # {'firstname': 'Rita', 'surname': 'Levi-Montalcini'}\n+    ```\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        jq_schema: Optional[str] = None,\n+        content_key: Optional[str] = None,\n+        extra_meta_fields: Optional[Union[Set[str], Literal[\"*\"]]] = None,\n+    ):\n+        \"\"\"\n+        Creates a JSONConverter component.\n+\n+        An optional `jq_schema` can be provided to extract nested data in the JSON source files.\n+        See the [official jq documentation](https://jqlang.github.io/jq/) for more info on the filters syntax.\n+        If `jq_schema` is not set, whole JSON source files will be used to extract content.\n+\n+        Optionally, you can provide a `content_key` to specify which key in the extracted object must\n+        be set as the document's content.\n+\n+        If both `jq_schema` and `content_key` are set, the component will search for the `content_key` in\n+        the JSON object extracted by `jq_schema`. If the extracted data is not a JSON object, it will be skipped.\n+\n+        If only `jq_schema` is set, the extracted data must be a scalar value. If it's a JSON object or array,\n+        it will be skipped.\n+\n+        If only `content_key` is set, the source JSON file must be a JSON object, else it will be skipped.\n+\n+        `extra_meta_fields` can either be set to a set of strings or a literal `\"*\"` string.\n+        If it's a set of strings, it must specify fields in the extracted objects that must be set in\n+        the extracted documents. If a field is not found, the meta value will be `None`.\n+        If set to `\"*\"`, all fields that are not `content_key` found in the filtered JSON object will\n+        be saved as metadata.\n+\n+        Initialization will fail if neither `jq_schema` nor `content_key` are set.\n+\n+        :param jq_schema:\n+            Optional jq filter string to extract content.\n+            If not specified, whole JSON object will be used to extract information.\n+        :param content_key:\n+            Optional key to extract document content.\n+            If `jq_schema` is specified, the `content_key` will be extracted from that object.\n+        :param extra_meta_fields:\n+            An optional set of meta keys to extract from the content.\n+            If `jq_schema` is specified, all keys will be extracted from that object.\n+        \"\"\"\n+        self._compiled_filter = None\n+        if jq_schema:\n+            jq_import.check()\n+            self._compiled_filter = jq.compile(jq_schema)\n+\n+        self._jq_schema = jq_schema\n+        self._content_key = content_key\n+        self._meta_fields = extra_meta_fields\n+\n+        if self._compiled_filter is None and self._content_key is None:\n+            msg = \"No `jq_schema` nor `content_key` specified. Set either or both to extract data.\"\n+            raise ValueError(msg)\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Serializes the component to a dictionary.\n+\n+        :returns:\n+            Dictionary with serialized data.\n+        \"\"\"\n+        return default_to_dict(\n+            self, jq_schema=self._jq_schema, content_key=self._content_key, extra_meta_fields=self._meta_fields\n+        )\n+\n+    @classmethod\n+    def from_dict(cls, data: Dict[str, Any]) -> \"JSONConverter\":\n+        \"\"\"\n+        Deserializes the component from a dictionary.\n+\n+        :param data:\n+            Dictionary to deserialize from.\n+        :returns:\n+            Deserialized component.\n+        \"\"\"\n+        return default_from_dict(cls, data)\n+\n+    def _get_content_and_meta(self, source: ByteStream) -> List[Tuple[str, Dict[str, Any]]]:\n+        \"\"\"\n+        Utility function to extract text and metadata from a JSON file.\n+\n+        :param source:\n+            UTF-8 byte stream.\n+        :returns:\n+            Collection of text and metadata dict tuples, each corresponding\n+            to a different document.\n+        \"\"\"\n+        try:\n+            file_content = source.data.decode(\"utf-8\")\n+        except UnicodeError as exc:\n+            logger.warning(\n+                \"Failed to extract text from {source}. Skipping it. Error: {error}\",\n+                source=source.meta[\"file_path\"],\n+                error=exc,\n+            )\n+\n+        meta_fields = self._meta_fields or set()\n+\n+        if self._compiled_filter is not None:\n+            try:\n+                objects = list(self._compiled_filter.input_text(file_content))\n+            except Exception as exc:\n+                logger.warning(\n+                    \"Failed to extract text from {source}. Skipping it. Error: {error}\",\n+                    source=source.meta[\"file_path\"],\n+                    error=exc,\n+                )\n+                return []\n+        else:\n+            # We just load the whole file as JSON if the user didn't provide a jq filter.\n+            # We put it in a list even if it's not to ease handling it later on.\n+            objects = [json.loads(file_content)]\n+\n+        result = []\n+        if self._content_key is not None:\n+            for obj in objects:\n+                if not isinstance(obj, dict):\n+                    logger.warning(\"Expected a dictionary but got {obj}. Skipping it.\", obj=obj)\n+                    continue\n+                if self._content_key not in obj:\n+                    logger.warning(\n+                        \"'{content_key}' not found in {obj}. Skipping it.\", content_key=self._content_key, obj=obj\n+                    )\n+                    continue\n+\n+                text = obj[self._content_key]\n+                if isinstance(text, (dict, list)):\n+                    logger.warning(\"Expected a scalar value but got {obj}. Skipping it.\", obj=obj)\n+                    continue\n+\n+                meta = {}\n+                if meta_fields == \"*\":\n+                    meta = {k: v for k, v in obj.items() if k != self._content_key}\n+                else:\n+                    for field in meta_fields:\n+                        meta[field] = obj.get(field, None)\n+                result.append((text, meta))\n+        else:\n+            for obj in objects:\n+                if isinstance(obj, (dict, list)):\n+                    logger.warning(\"Expected a scalar value but got {obj}. Skipping it.\", obj=obj)\n+                    continue\n+                result.append((str(obj), {}))\n+\n+        return result\n+\n+    @component.output_types(documents=List[Document])\n+    def run(\n+        self,\n+        sources: List[Union[str, Path, ByteStream]],\n+        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n+    ):\n+        \"\"\"\n+        Converts a list of JSON files to documents.\n+\n+        :param sources:\n+            A list of file paths or ByteStream objects.\n+        :param meta:\n+            Optional metadata to attach to the documents.\n+            This value can be either a list of dictionaries or a single dictionary.\n+            If it's a single dictionary, its content is added to the metadata of all produced documents.\n+            If it's a list, the length of the list must match the number of sources.\n+            If `sources` contain ByteStream objects, their `meta` will be added to the output documents.\n+\n+        :returns:\n+            A dictionary with the following keys:\n+            - `documents`: A list of created documents.\n+        \"\"\"\n+        documents = []\n+        meta_list = normalize_metadata(meta=meta, sources_count=len(sources))\n+\n+        for source, metadata in zip(sources, meta_list):\n+            try:\n+                bytestream = get_bytestream_from_source(source)\n+            except Exception as exc:\n+                logger.warning(\"Could not read {source}. Skipping it. Error: {error}\", source=source, error=exc)\n+                continue\n+\n+            data = self._get_content_and_meta(bytestream)\n+\n+            for text, extra_meta in data:\n+                merged_metadata = {**bytestream.meta, **metadata, **extra_meta}\n+                document = Document(content=text, meta=merged_metadata)\n+                documents.append(document)\n+\n+        return {\"documents\": documents}\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 5c75292f7d..be286052be 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -102,8 +102,9 @@ extra-dependencies = [\n   \"trafilatura\",                      # HTMLToDocument\n   \"python-pptx\",                      # PPTXToDocument\n   \"python-docx\",                      # DocxToDocument\n+  \"jq\",                               #JSONConverter\n \n-  \"nltk\",                             # NLTKDocumentSplitter\n+  \"nltk\", # NLTKDocumentSplitter\n \n   # OpenAPI\n   \"jsonref\",  # OpenAPIServiceConnector, OpenAPIServiceToFunctions\ndiff --git a/releasenotes/notes/json-converter-a747e9c44543dfb5.yaml b/releasenotes/notes/json-converter-a747e9c44543dfb5.yaml\nnew file mode 100644\nindex 0000000000..f8a269d2c0\n--- /dev/null\n+++ b/releasenotes/notes/json-converter-a747e9c44543dfb5.yaml\n@@ -0,0 +1,48 @@\n+---\n+features:\n+  - |\n+    Add new `JSONConverter` Component to convert JSON files to `Document`.\n+    Optionally it can use jq to filter the source JSON files and extract only specific parts.\n+\n+    ```python\n+    import json\n+\n+    from haystack.components.converters import JSONConverter\n+    from haystack.dataclasses import ByteStream\n+\n+    data = {\n+        \"laureates\": [\n+            {\n+                \"firstname\": \"Enrico\",\n+                \"surname\": \"Fermi\",\n+                \"motivation\": \"for his demonstrations of the existence of new radioactive elements produced \"\n+                \"by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons\",\n+            },\n+            {\n+                \"firstname\": \"Rita\",\n+                \"surname\": \"Levi-Montalcini\",\n+                \"motivation\": \"for their discoveries of growth factors\",\n+            },\n+        ],\n+    }\n+    source = ByteStream.from_string(json.dumps(data))\n+    converter = JSONConverter(\n+        jq_schema=\".laureates[]\", content_key=\"motivation\", extra_meta_fields=[\"firstname\", \"surname\"]\n+    )\n+\n+    results = converter.run(sources=[source])\n+    documents = results[\"documents\"]\n+    print(documents[0].content)\n+    # 'for his demonstrations of the existence of new radioactive elements produced by\n+    # neutron irradiation, and for his related discovery of nuclear reactions brought\n+    # about by slow neutrons'\n+\n+    print(documents[0].meta)\n+    # {'firstname': 'Enrico', 'surname': 'Fermi'}\n+\n+    print(documents[1].content)\n+    # 'for their discoveries of growth factors'\n+\n+    print(documents[1].meta)\n+    # {'firstname': 'Rita', 'surname': 'Levi-Montalcini'}\n+    ```\n", "instance_id": "deepset-ai__haystack-8397", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in expressing the need for a JSON to Document converter as a pipeline component in the Haystack framework. It identifies the gap (lack of a dedicated JSON converter despite frequent JSON data usage) and proposes a solution (a dedicated JSON converter). However, it lacks critical details such as specific input/output formats, constraints, or requirements for handling different JSON structures. Edge cases, such as malformed JSON or deeply nested structures, are not mentioned. Additionally, while the motivation for the feature is provided, there are no explicit examples or detailed use cases in the problem statement itself (though the code changes include examples). This results in minor ambiguities, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves creating a new module (`JSONConverter`) with a significant amount of new code (over 200 lines in `json.py`), as well as minor updates to configuration files and documentation. This requires understanding the existing codebase structure, particularly how converters are integrated into the Haystack framework. Second, the implementation involves multiple technical concepts, including JSON parsing, the `jq` library for filtering JSON data (a non-trivial dependency requiring specific syntax knowledge), and integration with Haystack's `ByteStream` and `Document` data structures. Third, the code handles various edge cases, such as invalid JSON, missing keys, and non-scalar content values, with appropriate error logging, which adds to the complexity. However, the problem does not impact the broader system architecture or require advanced domain-specific knowledge beyond the Haystack framework and JSON processing. It also does not involve performance-critical optimizations or complex algorithms. Therefore, a score of 0.55 reflects a medium difficulty task that requires understanding multiple concepts and implementing a moderately complex feature with some edge case handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ImportError: cannot import 'load_specific_dataset' from 'pypots.data'\n### 1. System Info\n\nWhen I import the function with \r\n\r\n`from pypots.data import load_specific_dataset`\r\n\r\nIt shows\r\n\r\n`ImportError: cannot import name 'load_specific_dataset' from 'pypots.data'`\r\n\r\nI checked the source code and I didn't find a function named `load_specific_dataset` in the directory of `pypots.data`.\r\n\r\nThere may be something wrong.\n\n### 2. Information\n\n- [X] The official example scripts\n- [ ] My own created scripts\n\n### 3. Reproduction\n\n`from pypots.data import load_specific_dataset`\n\n### 4. Expected behavior\n\nIt should successfully load the dataset:\r\n\r\n`data = load_specific_dataset('physionet_2012')`\n", "patch": "diff --git a/.github/workflows/publish_to_PyPI.yml b/.github/workflows/publish_to_PyPI.yml\nindex 6b2bffe2..3f18a373 100644\n--- a/.github/workflows/publish_to_PyPI.yml\n+++ b/.github/workflows/publish_to_PyPI.yml\n@@ -48,7 +48,7 @@ jobs:\n \n     - name: Build package\n       run: |\n-        python -m build --no-isolation\n+        python -m build\n \n     - name: Publish the new package to PyPI\n       uses: pypa/gh-action-pypi-publish@v1.8.6\ndiff --git a/pypots/data/__init__.py b/pypots/data/__init__.py\nindex 976a1bf8..26c4ba1d 100644\n--- a/pypots/data/__init__.py\n+++ b/pypots/data/__init__.py\n@@ -23,6 +23,7 @@\n     sliding_window,\n     inverse_sliding_window,\n )\n+from .load_specific_datasets import load_specific_dataset\n \n __all__ = [\n     # base dataset classes\n@@ -33,6 +34,7 @@\n     \"gene_complete_random_walk_for_anomaly_detection\",\n     \"gene_complete_random_walk_for_classification\",\n     \"gene_random_walk\",\n+    \"load_specific_dataset\",\n     # utils\n     \"parse_delta\",\n     \"sliding_window\",\ndiff --git a/pypots/data/load_specific_datasets.py b/pypots/data/load_specific_datasets.py\nnew file mode 100644\nindex 00000000..69e79615\n--- /dev/null\n+++ b/pypots/data/load_specific_datasets.py\n@@ -0,0 +1,73 @@\n+\"\"\"\n+Functions to load supported open-source time-series datasets.\n+\"\"\"\n+\n+# Created by Wenjie Du <wenjay.du@gmail.com>\n+# License: BSD-3-Clause\n+\n+\n+from benchpots.datasets import preprocess_physionet2012\n+\n+from ..utils.logging import logger\n+\n+# currently supported datasets\n+SUPPORTED_DATASETS = [\n+    \"physionet_2012\",\n+]\n+\n+# preprocessing functions of the supported datasets\n+PREPROCESSING_FUNC = {\n+    \"physionet_2012\": preprocess_physionet2012,\n+}\n+\n+\n+def list_supported_datasets() -> list:\n+    \"\"\"Return the datasets natively supported by PyPOTS so far.\n+\n+    Returns\n+    -------\n+    SUPPORTED_DATASETS :\n+        A list including all supported datasets.\n+\n+    \"\"\"\n+    return SUPPORTED_DATASETS\n+\n+\n+def load_specific_dataset(dataset_name: str, use_cache: bool = True) -> dict:\n+    \"\"\"Load specific datasets supported by PyPOTS.\n+    Different from tsdb.load_dataset(), which only produces merely raw data,\n+    load_specific_dataset here does some preprocessing operations,\n+    like truncating time series to generate samples with the same length.\n+\n+    Parameters\n+    ----------\n+    dataset_name :\n+        The name of the dataset to be loaded, which should be supported, i.e. in SUPPORTED_DATASETS.\n+\n+    use_cache :\n+        Whether to use cache. This is an argument of tsdb.load_dataset().\n+\n+    Returns\n+    -------\n+    data :\n+        A dict contains the preprocessed dataset.\n+        Users only need to continue the preprocessing steps to generate the data they want,\n+        e.g. standardizing and splitting.\n+\n+    \"\"\"\n+    logger.info(\n+        f\"Loading the dataset {dataset_name} with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\"\n+    )\n+    assert dataset_name in SUPPORTED_DATASETS, (\n+        f\"Dataset {dataset_name} is not supported. \"\n+        f\"If you believe this dataset is valuable to be supported by PyPOTS,\"\n+        f\"please create an issue on GitHub \"\n+        f\"https://github.com/WenjieDu/PyPOTS/issues\"\n+    )\n+    logger.info(f\"Starting preprocessing {dataset_name}...\")\n+    data = PREPROCESSING_FUNC[dataset_name](\"all\", 0.1)\n+    logger.warning(\n+        \"\u26a0\ufe0f load_specific_dataset() will be deprecated in the near future. Data preprocessing functions \"\n+        \"are moved to BenchPOTS, which now supports processing 170+ public time-series datasets.\"\n+    )\n+    return data\ndiff --git a/pypots/version.py b/pypots/version.py\nindex 582517f6..9e2606ec 100644\n--- a/pypots/version.py\n+++ b/pypots/version.py\n@@ -22,4 +22,4 @@\n #\n # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n # 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n-__version__ = \"0.7\"\n+__version__ = \"0.7.1\"\ndiff --git a/requirements/conda_env.yml b/requirements/conda_env.yml\nindex 86a0926a..180e0136 100644\n--- a/requirements/conda_env.yml\n+++ b/requirements/conda_env.yml\n@@ -8,12 +8,12 @@ channels:\n \n dependencies:\n     # basic\n+    - conda-forge::python >=3.8\n     - conda-forge::pip\n     - conda-forge::h5py\n     - conda-forge::numpy\n     - conda-forge::scipy\n     - conda-forge::sympy\n-    - conda-forge::python\n     - conda-forge::einops\n     - conda-forge::pandas\n     - conda-forge::seaborn\n@@ -46,6 +46,7 @@ dependencies:\n     # dev\n     - conda-forge::black\n     - conda-forge::flake8\n+    - conda-forge::flake8-pyproject\n     - conda-forge::pre-commit\n     - conda-forge::jupyterlab\n \n", "instance_id": "WenjieDu__PyPOTS-481", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: an `ImportError` when trying to use `load_specific_dataset` from `pypots.data`, with the user noting that the function does not exist in the source code. The goal is implicitly to resolve this import error, likely by adding the missing functionality or correcting the import path. The reproduction steps and expected behavior are provided, which helps in understanding the issue. However, there are minor ambiguities: the problem statement does not explicitly state whether the function should be implemented, if it was removed intentionally, or if this is a documentation error. Additionally, there are no detailed constraints or requirements for the solution (e.g., what datasets should be supported, expected output format). Hence, while the issue is valid and mostly clear, it lacks some critical details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The code changes involve multiple files but are relatively straightforward. A new file `load_specific_datasets.py` is added with a clear implementation of the `load_specific_dataset` function. Modifications to `pypots/data/__init__.py` are minimal, just adding an import and updating the `__all__` list. Other changes (e.g., version bump in `version.py`, updates to CI workflow, and dependency file tweaks) are minor and unrelated to the core logic of solving the import error. The changes do not impact the broader system architecture significantly and are localized to the data loading module.\n\n2. **Number of Technical Concepts:** The solution requires basic understanding of Python module imports, file organization, and dataset preprocessing. The new code in `load_specific_datasets.py` uses simple dictionary mappings and function calls to preprocess datasets, with no complex algorithms or advanced language features involved. It also references an external library (`benchpots.datasets`) for preprocessing, which assumes basic familiarity with third-party dependencies. Overall, the technical concepts are not particularly deep or challenging.\n\n3. **Potential Edge Cases and Error Handling:** The code includes basic error handling (e.g., asserting the dataset name is supported) and logs warnings about deprecation, which shows some consideration for user experience. However, the problem statement does not explicitly mention edge cases like invalid dataset names or preprocessing failures beyond what is already handled in the code. The complexity of edge cases appears minimal, as the function's purpose is straightforward (loading and preprocessing datasets).\n\n4. **Overall Assessment:** This task requires understanding some code logic (e.g., how dataset loading works in the library) and making simple modifications (adding a new function and updating imports). It does not involve deep architectural changes or complex logic. The primary challenge might be ensuring compatibility with the existing codebase and external dependencies, but this is still within the realm of an easy-to-moderate task for a developer familiar with Python libraries.\n\nThus, a score of 0.35 reflects an \"Easy\" problem that is slightly more involved than a trivial fix but does not reach the complexity of a medium-difficulty task.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "validator would fail on \"legit\" \"n/a\" value\non \"spacetop\" dataset (which is yet to be made public) there is .tsv files with `n/a` in some .tsv's, which is the way BIDS mandates to code \"missing values\".  See https://bids-specification.readthedocs.io/en/stable/common-principles.html#tabular-files which states\r\n\r\n> String values containing tabs MUST be escaped using double quotes. Missing and non-applicable values MUST be coded as `n/a`. Numerical values MUST employ the dot (`.`) as decimal separator and MAY be specified in scientific notation, using `e` or `E` to separate the significand from the exponent. TSV files MUST be in UTF-8 encoding.\r\n\r\nI filed \r\n- https://github.com/bids-standard/bids-specification/issues/1938\r\nthough.\r\n\r\nATM for my sample dataset (not yet public on openneuro, attn @jungheejung - remind me if we do have it somewhere public meanwhile), a version of `hed-validator` (#1025) blows with\r\n\r\n```\r\n(deno) yoh@typhon:/mnt/DATA/data/yoh/1076_spacetop$ ~/bin/hed-validator .\r\nUsing HEDTOOLS version: {'date': '2024-06-14T17:02:33-0500', 'dirty': False, 'error': None, 'full-revisionid': '940e75ddcedd5a14910098b60277413edc3c024e', 'version': '0.5.0'}\r\nTraceback (most recent call last):\r\n  File \"/home/yoh/bin/hed-validator\", line 71, in <module>\r\n    main()\r\n  File \"/home/yoh/bin/hed-validator\", line 37, in main\r\n    issue_list = bids.validate(check_for_warnings=args.check_for_warnings)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/hed/tools/bids/bids_dataset.py\", line 84, in validate\r\n    issues += files.validate_datafiles(self.schema, check_for_warnings=check_for_warnings)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/hed/tools/bids/bids_file_group.py\", line 157, in validate_datafiles\r\n    issues += data_obj.contents.validate(hed_schema, extra_def_dicts=extra_def_dicts, name=name,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/hed/models/base_input.py\", line 359, in validate\r\n    validation_issues = tab_validator.validate(self, self._mapper.get_def_dict(hed_schema, extra_def_dicts), name,\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/hed/validator/spreadsheet_validator.py\", line 63, in validate\r\n    data_new._dataframe = df_util.sort_dataframe_by_onsets(data.dataframe)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/hed/models/df_util.py\", line 118, in sort_dataframe_by_onsets\r\n    df_copy['_temp_onset_sort'] = df_copy['onset'].astype(float)\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/generic.py\", line 6643, in astype\r\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/internals/managers.py\", line 430, in astype\r\n    return self.apply(\r\n           ^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/internals/managers.py\", line 363, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/internals/blocks.py\", line 758, in astype\r\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\r\n    new_values = astype_array(values, dtype, copy=copy)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\r\n    values = _astype_nansafe(values, dtype, copy=copy)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/yoh/miniconda3/envs/deno/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 133, in _astype_nansafe\r\n    return arr.astype(dtype, copy=True)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: could not convert string to float: 'n/a'\r\n```\r\n\r\nnote that it is also unclear on which file it blows -- so might be worth providing some feedback (logging ERROR level or just catch/enhance exception while working on a specific file) on which file it happens.\r\n\r\nI think it is likely on `n/a` in onset field in some `_events.tsv`, e.g.\r\n\r\n```\r\n(deno) yoh@typhon:/mnt/DATA/data/yoh/1076_spacetop$ git grep '^n/a' | grep _events.tsv | head\r\nsub-0001/ses-02/func/sub-0001_ses-02_task-faces_acq-mb8_run-01_events.tsv:n/a   n/a     rating_mouse_trajectory n/a     intensity       disgust male    African old     n/a\r\nsub-0001/ses-02/func/sub-0001_ses-02_task-faces_acq-mb8_run-01_events.tsv:n/a   n/a     rating_mouse_trajectory n/a     intensity       happy   male    EA      young   n/a\r\nsub-0001/ses-02/func/sub-0001_ses-02_task-faces_acq-mb8_run-02_events.tsv:n/a   n/a     rating_mouse_trajectory n/a     sex     happy   male    WC      old     n/a\r\n```\r\n\r\nFWIW -- we did not see similar crash while running deno bids-validator (I guess uses JS version?).\r\n\nModified validator to handle n/a in onset column\n\n", "patch": "diff --git a/hed/errors/error_messages.py b/hed/errors/error_messages.py\nindex 06c9c651..bef3a420 100644\n--- a/hed/errors/error_messages.py\n+++ b/hed/errors/error_messages.py\n@@ -25,9 +25,9 @@ def val_error_empty_group(tag):\n     return f\"HED tags cannot be empty.  Extra delimiters found: '{tag}'\"\n \n \n-@hed_tag_error(TemporalErrors.HED_ONSET_WITH_NO_COLUMN, actual_code=ValidationErrors.TEMPORAL_TAG_ERROR)\n-def val_error_hed_onset_with_no_column(tag):\n-    return f\"Cannot have Temporal tags without an 'Onset' column.  Found tag: '{tag}'\"\n+@hed_tag_error(TemporalErrors.TEMPORAL_TAG_NO_TIME, actual_code=ValidationErrors.TEMPORAL_TAG_ERROR)\n+def val_error_temporal_tag_no_time(tag):\n+    return f\"Cannot have Temporal tags without an 'Onset' column and a time.  Found tag: '{tag}'\"\n \n \n @hed_tag_error(ValidationErrors.TAG_EXTENDED, has_sub_tag=True, default_severity=ErrorSeverity.WARNING)\n@@ -77,9 +77,9 @@ def val_error_CURLY_BRACE_UNSUPPORTED_HERE(tag, problem_tag):\n             f\"Invalid character '{problem_tag}' in tag '{tag}'\")\n \n \n-@hed_error(ValidationErrors.ONSETS_OUT_OF_ORDER, default_severity=ErrorSeverity.WARNING)\n-def val_error_ONSETS_OUT_OF_ORDER():\n-    return \"Onsets need to be temporally increasing for most downstream tools to work.\"\n+@hed_error(ValidationErrors.ONSETS_UNORDERED, default_severity=ErrorSeverity.WARNING)\n+def val_error_ONSETS_UNORDERED():\n+    return \"Onsets need to be temporally increasing and defined for many downstream tools to work.\"\n \n \n @hed_error(ValidationErrors.COMMA_MISSING)\ndiff --git a/hed/errors/error_reporter.py b/hed/errors/error_reporter.py\nindex 42b12842..b108c61b 100644\n--- a/hed/errors/error_reporter.py\n+++ b/hed/errors/error_reporter.py\n@@ -1,686 +1,686 @@\n-\"\"\"\n-Support functions for reporting validation errors.\n-\n-You can scope the formatted errors with calls to push_error_context and pop_error_context.\n-\"\"\"\n-\n-from functools import wraps\n-import xml.etree.ElementTree as ET\n-\n-from hed.errors.error_types import ErrorContext, ErrorSeverity\n-from hed.errors.known_error_codes import known_error_codes\n-\n-error_functions = {}\n-\n-# Controls if the default issue printing skips adding indentation for this context.\n-no_tab_context = {ErrorContext.HED_STRING, ErrorContext.SCHEMA_ATTRIBUTE}\n-\n-# Default sort ordering for issues list.\n-default_sort_list = [\n-    ErrorContext.CUSTOM_TITLE,\n-    ErrorContext.FILE_NAME,\n-    ErrorContext.SIDECAR_COLUMN_NAME,\n-    ErrorContext.SIDECAR_KEY_NAME,\n-    ErrorContext.ROW,\n-    ErrorContext.COLUMN,\n-    ErrorContext.LINE,\n-    ErrorContext.SCHEMA_SECTION,\n-    ErrorContext.SCHEMA_TAG,\n-    ErrorContext.SCHEMA_ATTRIBUTE,\n-]\n-\n-# ErrorContext which is expected to be int based.\n-int_sort_list = [\n-    ErrorContext.ROW\n-]\n-\n-\n-def _register_error_function(error_type, wrapper_func):\n-    if error_type in error_functions:\n-        raise KeyError(f\"{error_type} defined more than once.\")\n-\n-    error_functions[error_type] = wrapper_func\n-\n-\n-def hed_error(error_type, default_severity=ErrorSeverity.ERROR, actual_code=None):\n-    \"\"\" Decorator for errors in error handler or inherited classes.\n-\n-    Parameters:\n-        error_type (str): A value from error_types or optionally another value.\n-        default_severity (ErrorSeverity): The default severity for the decorated error.\n-        actual_code (str): The actual error to report to the outside world.\n-\n-    \"\"\"\n-    if actual_code is None:\n-        actual_code = error_type\n-\n-    def inner_decorator(func):\n-        @wraps(func)\n-        def wrapper(*args, severity=default_severity, **kwargs):\n-            \"\"\" Wrapper function for error handling non-tag errors.\n-\n-            Parameters:\n-                args (args): non keyword args.\n-                severity (ErrorSeverity): Will override the default error value if passed.\n-                kwargs (**kwargs): Any keyword args to be passed down to error message function.\n-\n-            Returns:\n-                list: A list of dict with the errors.\n-            \"\"\"\n-            base_message = func(*args, **kwargs)\n-            error_object = ErrorHandler._create_error_object(actual_code, base_message, severity)\n-            return error_object\n-\n-        _register_error_function(error_type, wrapper_func=wrapper)\n-        return wrapper\n-\n-    return inner_decorator\n-\n-\n-def hed_tag_error(error_type, default_severity=ErrorSeverity.ERROR, has_sub_tag=False, actual_code=None):\n-    \"\"\"  Decorator for errors in error handler or inherited classes.\n-\n-    Parameters:\n-        error_type (str): A value from error_types or optionally another value.\n-        default_severity (ErrorSeverity): The default severity for the decorated error.\n-        has_sub_tag (bool): If True, this error message also wants a sub_tag passed down.  eg \"This\" in \"This/Is/A/Tag\"\n-        actual_code (str): The actual error to report to the outside world.\n-\n-    \"\"\"\n-    if actual_code is None:\n-        actual_code = error_type\n-\n-    def inner_decorator(func):\n-        if has_sub_tag:\n-            @wraps(func)\n-            def wrapper(tag, index_in_tag, index_in_tag_end, *args, severity=default_severity, **kwargs):\n-                \"\"\" Wrapper function for error handling tag errors with sub tags.\n-\n-                Parameters:\n-                    tag (HedTag): The HED tag object with the problem.\n-                    index_in_tag (int): The index into the tag with a problem(usually 0).\n-                    index_in_tag_end (int): The last index into the tag with a problem - usually len(tag).\n-                    args (args): Any other non keyword args.\n-                    severity (ErrorSeverity): Used to include warnings as well as errors.\n-                    kwargs (**kwargs): Any keyword args to be passed down to error message function.\n-\n-                Returns:\n-                    list: A list of dict with the errors.\n-\n-                \"\"\"\n-                try:\n-                    tag_as_string = tag.tag\n-                except AttributeError:\n-                    tag_as_string = str(tag)\n-\n-                if index_in_tag_end is None:\n-                    index_in_tag_end = len(tag_as_string)\n-                problem_sub_tag = tag_as_string[index_in_tag: index_in_tag_end]\n-                try:\n-                    org_tag_text = tag.org_tag\n-                except AttributeError:\n-                    org_tag_text = str(tag)\n-\n-                base_message = func(org_tag_text, problem_sub_tag, *args, **kwargs)\n-                error_object = ErrorHandler._create_error_object(actual_code, base_message, severity,\n-                                                                 index_in_tag=index_in_tag,\n-                                                                 index_in_tag_end=index_in_tag_end, source_tag=tag)\n-\n-                return error_object\n-\n-            _register_error_function(error_type, wrapper_func=wrapper)\n-            return wrapper\n-        else:\n-            @wraps(func)\n-            def wrapper(tag, *args, severity=default_severity, **kwargs):\n-                \"\"\" Wrapper function for error handling tag errors.\n-\n-                Parameters:\n-                    tag (HedTag or HedGroup): The HED tag object with the problem.\n-                    args (non keyword args): Any other non keyword args.\n-                    severity (ErrorSeverity): For including warnings.\n-                    kwargs (keyword args): Any keyword args to be passed down to error message function.\n-\n-                Returns:\n-                    list: A list of dict with the errors.\n-\n-                \"\"\"\n-                from hed.models.hed_tag import HedTag\n-                from hed.models.hed_group import HedGroup\n-                if isinstance(tag, HedTag):\n-                    org_tag_text = tag.org_tag\n-                elif isinstance(tag, HedGroup):\n-                    org_tag_text = tag.get_original_hed_string()\n-                else:\n-                    org_tag_text = str(tag)\n-                base_message = func(org_tag_text, *args, **kwargs)\n-                error_object = ErrorHandler._create_error_object(actual_code, base_message, severity,\n-                                                                 source_tag=tag)\n-\n-                return error_object\n-\n-            _register_error_function(error_type, wrapper_func=wrapper)\n-            return wrapper\n-\n-    return inner_decorator\n-\n-\n-# Import after hed_error decorators are defined.\n-from hed.errors import error_messages  # noqa:E402\n-from hed.errors import schema_error_messages  # noqa:E402\n-\n-# Intentional to make sure tools don't think the import is unused\n-error_messages.mark_as_used = True\n-schema_error_messages.mark_as_used = True\n-\n-\n-class ErrorHandler:\n-    \"\"\"Class to hold error context and having general error functions.\"\"\"\n-    def __init__(self, check_for_warnings=True):\n-        # The current (ordered) dictionary of contexts.\n-        self.error_context = []\n-        self._check_for_warnings = check_for_warnings\n-\n-    def push_error_context(self, context_type, context):\n-        \"\"\" Push a new error context to narrow down error scope.\n-\n-        Parameters:\n-            context_type (ErrorContext): A value from ErrorContext representing the type of scope.\n-            context (str, int, or HedString): The main value for the context_type.\n-\n-        Notes:\n-            The context depends on the context_type. For ErrorContext.FILE_NAME this would be the actual filename.\n-\n-        \"\"\"\n-        if context is None:\n-            if context_type in int_sort_list:\n-                context = 0\n-            else:\n-                context = \"\"\n-        self.error_context.append((context_type, context))\n-\n-    def pop_error_context(self):\n-        \"\"\" Remove the last scope from the error context.\n-\n-        Notes:\n-            Modifies the error context of this reporter.\n-\n-        \"\"\"\n-\n-        self.error_context.pop(-1)\n-\n-    def reset_error_context(self):\n-        \"\"\" Reset all error context information to defaults.\n-\n-        Notes:\n-            This function is mainly for testing and should not be needed with proper usage.\n-\n-        \"\"\"\n-        self.error_context = []\n-\n-    def format_error_with_context(self, *args, **kwargs):\n-        error_object = ErrorHandler.format_error(*args, **kwargs)\n-        if self is not None:\n-            actual_error = error_object[0]\n-            # # Filter out warning errors\n-            if not self._check_for_warnings and actual_error['severity'] >= ErrorSeverity.WARNING:\n-                return []\n-            self._add_context_to_errors(actual_error, self.error_context)\n-            self._update_error_with_char_pos(actual_error)\n-\n-        return error_object\n-\n-    @staticmethod\n-    def format_error(error_type, *args, actual_error=None, **kwargs):\n-        \"\"\" Format an error based on the parameters, which vary based on what type of error this is.\n-\n-        Parameters:\n-            error_type (str): The type of error for this.  Registered with @hed_error or @hed_tag_error.\n-            args (args): Any remaining non keyword args after those required by the error type.\n-            actual_error (str or None): Code to actually add to report out.\n-            kwargs (kwargs): The other keyword args to pass down to the error handling func.\n-\n-        Returns:\n-            list:   A list containing a single dictionary representing a single error.\n-\n-        Notes:\n-            The actual error is useful for errors that are shared like invalid character.\n-\n-        \"\"\"\n-        error_func = error_functions.get(error_type)\n-        if not error_func:\n-            error_object = ErrorHandler.val_error_unknown(*args, **kwargs)\n-            error_object['code'] = error_type\n-        else:\n-            error_object = error_func(*args, **kwargs)\n-\n-        if actual_error:\n-            error_object['code'] = actual_error\n-\n-        return [error_object]\n-\n-    def add_context_and_filter(self, issues):\n-        \"\"\" Filter out warnings if requested, while adding context to issues.\n-\n-            issues(list):\n-                list:   A list containing a single dictionary representing a single error.\n-        \"\"\"\n-        if not self._check_for_warnings:\n-            issues[:] = self.filter_issues_by_severity(issues, ErrorSeverity.ERROR)\n-\n-        for error_object in issues:\n-            self._add_context_to_errors(error_object, self.error_context)\n-            self._update_error_with_char_pos(error_object)\n-\n-    @staticmethod\n-    def format_error_from_context(error_type, error_context, *args, actual_error=None, **kwargs):\n-        \"\"\" Format an error based on the error type.\n-\n-        Parameters:\n-            error_type (str): The type of error.  Registered with @hed_error or @hed_tag_error.\n-            error_context (list): Contains the error context to use for this error.\n-            args (args): Any remaining non keyword args.\n-            actual_error (str or None): Error code to actually add to report out.\n-            kwargs (kwargs): Keyword parameters to pass down to the error handling func.\n-\n-        Returns:\n-            list:  A list containing a single dictionary.\n-\n-        Notes:\n-            - Generally the error_context is returned from _add_context_to_errors.\n-            - The actual_error is useful for errors that are shared like invalid character.\n-            - This can't filter out warnings like the other ones.\n-\n-        \"\"\"\n-        error_list = ErrorHandler.format_error(error_type, *args, actual_error=actual_error, **kwargs)\n-\n-        ErrorHandler._add_context_to_errors(error_list[0], error_context)\n-        ErrorHandler._update_error_with_char_pos(error_list[0])\n-        return error_list\n-\n-    @staticmethod\n-    def _add_context_to_errors(error_object, error_context_to_add):\n-        \"\"\" Add relevant context such as row number or column name around an error object.\n-\n-        Parameters:\n-            error_object (dict): Generated error containing at least a code and message entry.\n-            error_context_to_add (list): Source context to use.  If none, the error handler context is used.\n-\n-        Returns:\n-            list: A list of dict with needed context strings added at the beginning of the list.\n-\n-        \"\"\"\n-        for (context_type, context) in error_context_to_add:\n-            error_object[context_type] = context\n-\n-        return error_object\n-\n-    @staticmethod\n-    def _create_error_object(error_type, base_message, severity, **kwargs):\n-        error_object = {'code': error_type,\n-                        'message': base_message,\n-                        'severity': severity\n-                        }\n-\n-        for key, value in kwargs.items():\n-            error_object.setdefault(key, value)\n-\n-        return error_object\n-\n-    @staticmethod\n-    def _get_tag_span_to_error_object(error_object):\n-        if ErrorContext.HED_STRING not in error_object:\n-            return None, None\n-\n-        if 'source_tag' in error_object:\n-            source_tag = error_object['source_tag']\n-            if isinstance(source_tag, int):\n-                return None, None\n-        else:\n-            return None, None\n-\n-        hed_string = error_object[ErrorContext.HED_STRING]\n-        span = hed_string._get_org_span(source_tag)\n-        return span\n-\n-    @staticmethod\n-    def _update_error_with_char_pos(error_object):\n-        # This part is optional as you can always generate these as needed.\n-        start, end = ErrorHandler._get_tag_span_to_error_object(error_object)\n-        if start is not None:\n-            # silence warning in pycharm\n-            start = int(start)\n-            source_tag = error_object.get('source_tag', None)\n-            # Todo: Move this functionality somewhere more centralized.\n-            # If the tag has been modified from the original, don't try to use sub indexing.\n-            if source_tag and source_tag._tag:\n-                new_start, new_end = start, end\n-            else:\n-                new_start = start + error_object.get('index_in_tag', 0)\n-                index_in_tag_end = end\n-                if 'index_in_tag_end' in error_object:\n-                    index_in_tag_end = start + error_object['index_in_tag_end']\n-                new_end = index_in_tag_end\n-            error_object['char_index'], error_object['char_index_end'] = new_start, new_end\n-            error_object['message'] += f\"  Problem spans string indexes: {new_start}, {new_end}\"\n-\n-    @hed_error(\"Unknown\")\n-    def val_error_unknown(*args, **kwargs):\n-        \"\"\" Default error handler if no error of this type was registered.\n-\n-        Parameters:\n-            args (args):  List of non-keyword parameters (varies).\n-            kwargs (kwargs): Keyword parameters (varies)\n-\n-        Returns:\n-            str: The error message.\n-\n-        \"\"\"\n-        return f\"Unknown error.  Args: {str(args), str(kwargs)}\"\n-\n-    @staticmethod\n-    def filter_issues_by_severity(issues_list, severity):\n-        \"\"\" Gather all issues matching or below a given severity.\n-\n-        Parameters:\n-            issues_list (list): A list of dictionaries containing the full issue list.\n-            severity (int): The level of issues to keep.\n-\n-        Returns:\n-            list: A list of dictionaries containing the issue list after filtering by severity.\n-\n-        \"\"\"\n-        return [issue for issue in issues_list if issue['severity'] <= severity]\n-\n-\n-def sort_issues(issues, reverse=False):\n-    \"\"\"Sort a list of issues by the error context values.\n-\n-    Parameters:\n-        issues (list): A list of dictionaries representing the issues to be sorted.\n-        reverse (bool, optional): If True, sorts the list in descending order. Default is False.\n-\n-    Returns:\n-        list: The sorted list of issues.\"\"\"\n-    def _get_keys(d):\n-        result = []\n-        for key in default_sort_list:\n-            if key in int_sort_list:\n-                result.append(d.get(key, -1))\n-            else:\n-                result.append(d.get(key, \"\"))\n-        return tuple(result)\n-\n-    issues = sorted(issues, key=_get_keys, reverse=reverse)\n-\n-    return issues\n-\n-\n-def check_for_any_errors(issues_list):\n-    \"\"\" Return True if there are any errors with a severity of warning. \"\"\"\n-    for issue in issues_list:\n-        if issue['severity'] < ErrorSeverity.WARNING:\n-            return True\n-\n-    return False\n-\n-\n-def get_printable_issue_string(issues, title=None, severity=None, skip_filename=True, add_link=False):\n-    \"\"\" Return a string with issues list flatted into single string, one per line.\n-\n-    Parameters:\n-        issues (list):  Issues to print.\n-        title (str):  Optional title that will always show up first if present(even if there are no validation issues).\n-        severity (int):        Return only warnings >= severity.\n-        skip_filename (bool):  If True, don't add the filename context to the printable string.\n-        add_link (bool): Add a link at the end of message to the appropriate error if True\n-    Returns:\n-        str:   A string containing printable version of the issues or ''.\n-\n-    \"\"\"\n-    if severity is not None:\n-        issues = ErrorHandler.filter_issues_by_severity(issues, severity)\n-\n-    output_dict = _build_error_context_dict(issues, skip_filename)\n-    issue_string = _error_dict_to_string(output_dict, add_link=add_link)\n-\n-    if title:\n-        issue_string = title + '\\n' + issue_string\n-    return issue_string\n-\n-\n-def get_printable_issue_string_html(issues, title=None, severity=None, skip_filename=True):\n-    \"\"\" Return a string with issues list as an HTML tree.\n-\n-    Parameters:\n-        issues (list):  Issues to print.\n-        title (str):  Optional title that will always show up first if present.\n-        severity (int): Return only warnings >= severity.\n-        skip_filename (bool): If True, don't add the filename context to the printable string.\n-\n-    Returns:\n-        str: An HTML string containing the issues or ''.\n-    \"\"\"\n-    if severity is not None:\n-        issues = ErrorHandler.filter_issues_by_severity(issues, severity)\n-\n-    output_dict = _build_error_context_dict(issues, skip_filename)\n-\n-    root_element = _create_error_tree(output_dict)\n-    if title:\n-        title_element = ET.Element(\"h1\")\n-        title_element.text = title\n-        root_element.insert(0, title_element)\n-    return ET.tostring(root_element, encoding='unicode')\n-\n-\n-def create_doc_link(error_code):\n-    \"\"\"If error code is a known code, return a documentation url for it.\n-\n-    Parameters:\n-        error_code(str): A HED error code.\n-\n-    Returns:\n-        url(str or None): The URL if it's a valid code.\n-    \"\"\"\n-    if error_code in known_error_codes[\"hed_validation_errors\"] \\\n-            or error_code in known_error_codes[\"schema_validation_errors\"]:\n-        modified_error_code = error_code.replace(\"_\", \"-\").lower()\n-        return f\"https://hed-specification.readthedocs.io/en/latest/Appendix_B.html#{modified_error_code}\"\n-    return None\n-\n-\n-def _build_error_context_dict(issues, skip_filename):\n-    \"\"\"Build the context -> error dictionary for an entire list of issues.\n-\n-    Returns:\n-        dict: A nested dictionary structure with a \"children\" key at each level for unrelated children.\n-    \"\"\"\n-    output_dict = None\n-    for single_issue in issues:\n-        single_issue_context = _get_context_from_issue(single_issue, skip_filename)\n-        output_dict = _add_single_error_to_dict(single_issue_context, output_dict, single_issue)\n-\n-    return output_dict\n-\n-\n-def _add_single_error_to_dict(items, root=None, issue_to_add=None):\n-    \"\"\" Build a nested dictionary out of the context lists.\n-\n-    Parameters:\n-        items (list): A list of error contexts\n-        root (dict, optional): An existing nested dictionary structure to update.\n-        issue_to_add (dict, optional): The issue to add at this level of context.\n-\n-    Returns:\n-        dict: A nested dictionary structure with a \"children\" key at each level for unrelated children.\n-    \"\"\"\n-    if root is None:\n-        root = {\"children\": []}\n-\n-    current_dict = root\n-    for item in items:\n-        # Navigate to the next level if the item already exists, or create a new level\n-        next_dict = current_dict.get(item, {\"children\": []})\n-        current_dict[item] = next_dict\n-        current_dict = next_dict\n-\n-    if issue_to_add:\n-        current_dict[\"children\"].append(issue_to_add)\n-\n-    return root\n-\n-\n-def _error_dict_to_string(print_dict, add_link=True, level=0):\n-    output = \"\"\n-    if print_dict is None:\n-        return output\n-    for context, value in print_dict.items():\n-        if context == \"children\":\n-            for child in value:\n-                single_issue_message = child[\"message\"]\n-                issue_string = level * \"\\t\" + _get_error_prefix(child)\n-                issue_string += f\"{single_issue_message}\\n\"\n-                if add_link:\n-                    link_url = create_doc_link(child['code'])\n-                    if link_url:\n-                        single_issue_message += f\"   See... {link_url}\"\n-                output += issue_string\n-            continue\n-        output += _format_single_context_string(context[0], context[1], level)\n-        output += _error_dict_to_string(value, add_link, level + 1)\n-\n-    return output\n-\n-\n-def _get_context_from_issue(val_issue, skip_filename=True):\n-    \"\"\" Extract all the context values from the given issue.\n-\n-    Parameters:\n-        val_issue (dict): A dictionary a representing a single error.\n-        skip_filename (bool): If True, don't gather the filename context.\n-\n-    Returns:\n-        list: A list of tuples containing the context_type and context for the given issue.\n-\n-    \"\"\"\n-    single_issue_context = []\n-    for key, value in val_issue.items():\n-        if skip_filename and key == ErrorContext.FILE_NAME:\n-            continue\n-        if key == ErrorContext.HED_STRING:\n-            value = value.get_original_hed_string()\n-        if key.startswith(\"ec_\"):\n-            single_issue_context.append((key, str(value)))\n-\n-    return single_issue_context\n-\n-\n-def _get_error_prefix(single_issue):\n-    \"\"\"Return the prefix for the error message based on severity and error code.\n-\n-    Parameters:\n-        single_issue(dict): A single issue object.\n-\n-    Returns:\n-        error_prefix(str):  the prefix to use.\n-    \"\"\"\n-    severity = single_issue.get('severity', ErrorSeverity.ERROR)\n-    error_code = single_issue['code']\n-\n-    if severity == ErrorSeverity.ERROR:\n-        error_prefix = f\"{error_code}: \"\n-    else:\n-        error_prefix = f\"{error_code}: (Warning) \"\n-    return error_prefix\n-\n-\n-def _format_single_context_string(context_type, context, tab_count=0):\n-    \"\"\" Return the human-readable form of a single context tuple.\n-\n-    Parameters:\n-        context_type (str): The context type of this entry.\n-        context (str or HedString): The value of this context.\n-        tab_count (int): Number of tabs to name_prefix each line with.\n-\n-    Returns:\n-        str: A string containing the context, including tabs.\n-\n-    \"\"\"\n-    tab_string = tab_count * '\\t'\n-    error_types = {\n-        ErrorContext.FILE_NAME: f\"\\nErrors in file '{context}'\",\n-        ErrorContext.SIDECAR_COLUMN_NAME: f\"Column '{context}':\",\n-        ErrorContext.SIDECAR_KEY_NAME: f\"Key: {context}\",\n-        ErrorContext.ROW: f'Issues in row {context}:',\n-        ErrorContext.COLUMN: f'Issues in column {context}:',\n-        ErrorContext.CUSTOM_TITLE: context,\n-        ErrorContext.LINE: f\"Line: {context}\",\n-        ErrorContext.HED_STRING: f\"hed string: {context}\",\n-        ErrorContext.SCHEMA_SECTION: f\"Schema Section: {context}\",\n-        ErrorContext.SCHEMA_TAG: f\"Source tag: {context}\",\n-        ErrorContext.SCHEMA_ATTRIBUTE: f\"Source Attribute: {context}\",\n-    }\n-    context_portion = error_types[context_type]\n-    context_string = f\"{tab_string}{context_portion}\\n\"\n-    return context_string\n-\n-\n-def _create_error_tree(error_dict, parent_element=None, add_link=True):\n-    if parent_element is None:\n-        parent_element = ET.Element(\"ul\")\n-\n-    for context, value in error_dict.items():\n-        if context == \"children\":\n-            for child in value:\n-                child_li = ET.SubElement(parent_element, \"li\")\n-                error_prefix = _get_error_prefix(child)\n-                single_issue_message = child[\"message\"]\n-\n-                # Create a link for the error prefix if add_link is True.\n-                if add_link:\n-                    link_url = create_doc_link(child['code'])\n-                    if link_url:\n-                        a_element = ET.SubElement(child_li, \"a\", href=link_url)\n-                        a_element.text = error_prefix\n-                        a_element.tail = \" \" + single_issue_message\n-                    else:\n-                        child_li.text = error_prefix + \" \" + single_issue_message\n-                else:\n-                    child_li.text = error_prefix + \" \" + single_issue_message\n-            continue\n-\n-        context_li = ET.SubElement(parent_element, \"li\")\n-        context_li.text = _format_single_context_string(context[0], context[1])\n-        context_ul = ET.SubElement(context_li, \"ul\")\n-        _create_error_tree(value, context_ul, add_link)\n-\n-    return parent_element\n-\n-\n-def replace_tag_references(list_or_dict):\n-    \"\"\" Utility function to remove any references to tags, strings, etc. from any type of nested list or dict.\n-\n-       Use this if you want to save out issues to a file.\n-\n-       If you'd prefer a copy returned, use replace_tag_references(list_or_dict.copy()).\n-\n-    Parameters:\n-       list_or_dict(list or dict): An arbitrarily nested list/dict structure\n-    \"\"\"\n-    if isinstance(list_or_dict, dict):\n-        for key, value in list_or_dict.items():\n-            if isinstance(value, (dict, list)):\n-                replace_tag_references(value)\n-            elif isinstance(value, (bool, float, int)):\n-                list_or_dict[key] = value\n-            else:\n-                list_or_dict[key] = str(value)\n-    elif isinstance(list_or_dict, list):\n-        for key, value in enumerate(list_or_dict):\n-            if isinstance(value, (dict, list)):\n-                replace_tag_references(value)\n-            elif isinstance(value, (bool, float, int)):\n-                list_or_dict[key] = value\n-            else:\n-                list_or_dict[key] = str(value)\n+\"\"\"\"\r\n+Support functions for reporting validation errors.\r\n+\r\n+You can scope the formatted errors with calls to push_error_context and pop_error_context.\r\n+\"\"\"\r\n+\r\n+from functools import wraps\r\n+import xml.etree.ElementTree as ET\r\n+\r\n+from hed.errors.error_types import ErrorContext, ErrorSeverity\r\n+from hed.errors.known_error_codes import known_error_codes\r\n+\r\n+error_functions = {}\r\n+\r\n+# Controls if the default issue printing skips adding indentation for this context.\r\n+no_tab_context = {ErrorContext.HED_STRING, ErrorContext.SCHEMA_ATTRIBUTE}\r\n+\r\n+# Default sort ordering for issues list.\r\n+default_sort_list = [\r\n+    ErrorContext.CUSTOM_TITLE,\r\n+    ErrorContext.FILE_NAME,\r\n+    ErrorContext.SIDECAR_COLUMN_NAME,\r\n+    ErrorContext.SIDECAR_KEY_NAME,\r\n+    ErrorContext.ROW,\r\n+    ErrorContext.COLUMN,\r\n+    ErrorContext.LINE,\r\n+    ErrorContext.SCHEMA_SECTION,\r\n+    ErrorContext.SCHEMA_TAG,\r\n+    ErrorContext.SCHEMA_ATTRIBUTE,\r\n+]\r\n+\r\n+# ErrorContext which is expected to be int based.\r\n+int_sort_list = [\r\n+    ErrorContext.ROW\r\n+]\r\n+\r\n+\r\n+def _register_error_function(error_type, wrapper_func):\r\n+    if error_type in error_functions:\r\n+        raise KeyError(f\"{error_type} defined more than once.\")\r\n+\r\n+    error_functions[error_type] = wrapper_func\r\n+\r\n+\r\n+def hed_error(error_type, default_severity=ErrorSeverity.ERROR, actual_code=None):\r\n+    \"\"\" Decorator for errors in error handler or inherited classes.\r\n+\r\n+    Parameters:\r\n+        error_type (str): A value from error_types or optionally another value.\r\n+        default_severity (ErrorSeverity): The default severity for the decorated error.\r\n+        actual_code (str): The actual error to report to the outside world.\r\n+\r\n+    \"\"\"\r\n+    if actual_code is None:\r\n+        actual_code = error_type\r\n+\r\n+    def inner_decorator(func):\r\n+        @wraps(func)\r\n+        def wrapper(*args, severity=default_severity, **kwargs):\r\n+            \"\"\" Wrapper function for error handling non-tag errors.\r\n+\r\n+            Parameters:\r\n+                args (args): non keyword args.\r\n+                severity (ErrorSeverity): Will override the default error value if passed.\r\n+                kwargs (**kwargs): Any keyword args to be passed down to error message function.\r\n+\r\n+            Returns:\r\n+                list: A list of dict with the errors.\r\n+            \"\"\"\r\n+            base_message = func(*args, **kwargs)\r\n+            error_object = ErrorHandler._create_error_object(actual_code, base_message, severity)\r\n+            return error_object\r\n+\r\n+        _register_error_function(error_type, wrapper_func=wrapper)\r\n+        return wrapper\r\n+\r\n+    return inner_decorator\r\n+\r\n+\r\n+def hed_tag_error(error_type, default_severity=ErrorSeverity.ERROR, has_sub_tag=False, actual_code=None):\r\n+    \"\"\"  Decorator for errors in error handler or inherited classes.\r\n+\r\n+    Parameters:\r\n+        error_type (str): A value from error_types or optionally another value.\r\n+        default_severity (ErrorSeverity): The default severity for the decorated error.\r\n+        has_sub_tag (bool): If True, this error message also wants a sub_tag passed down.  eg \"This\" in \"This/Is/A/Tag\"\r\n+        actual_code (str): The actual error to report to the outside world.\r\n+\r\n+    \"\"\"\r\n+    if actual_code is None:\r\n+        actual_code = error_type\r\n+\r\n+    def inner_decorator(func):\r\n+        if has_sub_tag:\r\n+            @wraps(func)\r\n+            def wrapper(tag, index_in_tag, index_in_tag_end, *args, severity=default_severity, **kwargs):\r\n+                \"\"\" Wrapper function for error handling tag errors with sub tags.\r\n+\r\n+                Parameters:\r\n+                    tag (HedTag): The HED tag object with the problem.\r\n+                    index_in_tag (int): The index into the tag with a problem(usually 0).\r\n+                    index_in_tag_end (int): The last index into the tag with a problem - usually len(tag).\r\n+                    args (args): Any other non keyword args.\r\n+                    severity (ErrorSeverity): Used to include warnings as well as errors.\r\n+                    kwargs (**kwargs): Any keyword args to be passed down to error message function.\r\n+\r\n+                Returns:\r\n+                    list: A list of dict with the errors.\r\n+\r\n+                \"\"\"\r\n+                try:\r\n+                    tag_as_string = tag.tag\r\n+                except AttributeError:\r\n+                    tag_as_string = str(tag)\r\n+\r\n+                if index_in_tag_end is None:\r\n+                    index_in_tag_end = len(tag_as_string)\r\n+                problem_sub_tag = tag_as_string[index_in_tag: index_in_tag_end]\r\n+                try:\r\n+                    org_tag_text = tag.org_tag\r\n+                except AttributeError:\r\n+                    org_tag_text = str(tag)\r\n+\r\n+                base_message = func(org_tag_text, problem_sub_tag, *args, **kwargs)\r\n+                error_object = ErrorHandler._create_error_object(actual_code, base_message, severity,\r\n+                                                                 index_in_tag=index_in_tag,\r\n+                                                                 index_in_tag_end=index_in_tag_end, source_tag=tag)\r\n+\r\n+                return error_object\r\n+\r\n+            _register_error_function(error_type, wrapper_func=wrapper)\r\n+            return wrapper\r\n+        else:\r\n+            @wraps(func)\r\n+            def wrapper(tag, *args, severity=default_severity, **kwargs):\r\n+                \"\"\" Wrapper function for error handling tag errors.\r\n+\r\n+                Parameters:\r\n+                    tag (HedTag or HedGroup): The HED tag object with the problem.\r\n+                    args (non keyword args): Any other non keyword args.\r\n+                    severity (ErrorSeverity): For including warnings.\r\n+                    kwargs (keyword args): Any keyword args to be passed down to error message function.\r\n+\r\n+                Returns:\r\n+                    list: A list of dict with the errors.\r\n+\r\n+                \"\"\"\r\n+                from hed.models.hed_tag import HedTag\r\n+                from hed.models.hed_group import HedGroup\r\n+                if isinstance(tag, HedTag):\r\n+                    org_tag_text = tag.org_tag\r\n+                elif isinstance(tag, HedGroup):\r\n+                    org_tag_text = tag.get_original_hed_string()\r\n+                else:\r\n+                    org_tag_text = str(tag)\r\n+                base_message = func(org_tag_text, *args, **kwargs)\r\n+                error_object = ErrorHandler._create_error_object(actual_code, base_message, severity,\r\n+                                                                 source_tag=tag)\r\n+\r\n+                return error_object\r\n+\r\n+            _register_error_function(error_type, wrapper_func=wrapper)\r\n+            return wrapper\r\n+\r\n+    return inner_decorator\r\n+\r\n+\r\n+# Import after hed_error decorators are defined.\r\n+from hed.errors import error_messages  # noqa:E402\r\n+from hed.errors import schema_error_messages  # noqa:E402\r\n+\r\n+# Intentional to make sure tools don't think the import is unused\r\n+error_messages.mark_as_used = True\r\n+schema_error_messages.mark_as_used = True\r\n+\r\n+\r\n+class ErrorHandler:\r\n+    \"\"\"Class to hold error context and having general error functions.\"\"\"\r\n+    def __init__(self, check_for_warnings=True):\r\n+        # The current (ordered) dictionary of contexts.\r\n+        self.error_context = []\r\n+        self._check_for_warnings = check_for_warnings\r\n+\r\n+    def push_error_context(self, context_type, context):\r\n+        \"\"\" Push a new error context to narrow down error scope.\r\n+\r\n+        Parameters:\r\n+            context_type (ErrorContext): A value from ErrorContext representing the type of scope.\r\n+            context (str, int, or HedString): The main value for the context_type.\r\n+\r\n+        Notes:\r\n+            The context depends on the context_type. For ErrorContext.FILE_NAME this would be the actual filename.\r\n+\r\n+        \"\"\"\r\n+        if context is None:\r\n+            if context_type in int_sort_list:\r\n+                context = 0\r\n+            else:\r\n+                context = \"\"\r\n+        self.error_context.append((context_type, context))\r\n+\r\n+    def pop_error_context(self):\r\n+        \"\"\" Remove the last scope from the error context.\r\n+\r\n+        Notes:\r\n+            Modifies the error context of this reporter.\r\n+\r\n+        \"\"\"\r\n+\r\n+        self.error_context.pop(-1)\r\n+\r\n+    def reset_error_context(self):\r\n+        \"\"\" Reset all error context information to defaults.\r\n+\r\n+        Notes:\r\n+            This function is mainly for testing and should not be needed with proper usage.\r\n+\r\n+        \"\"\"\r\n+        self.error_context = []\r\n+\r\n+    def format_error_with_context(self, *args, **kwargs):\r\n+        error_object = ErrorHandler.format_error(*args, **kwargs)\r\n+        if self is not None:\r\n+            actual_error = error_object[0]\r\n+            # # Filter out warning errors\r\n+            if not self._check_for_warnings and actual_error['severity'] >= ErrorSeverity.WARNING:\r\n+                return []\r\n+            self._add_context_to_errors(actual_error, self.error_context)\r\n+            self._update_error_with_char_pos(actual_error)\r\n+\r\n+        return error_object\r\n+\r\n+    @staticmethod\r\n+    def format_error(error_type, *args, actual_error=None, **kwargs):\r\n+        \"\"\" Format an error based on the parameters, which vary based on what type of error this is.\r\n+\r\n+        Parameters:\r\n+            error_type (str): The type of error for this.  Registered with @hed_error or @hed_tag_error.\r\n+            args (args): Any remaining non keyword args after those required by the error type.\r\n+            actual_error (str or None): Code to actually add to report out.\r\n+            kwargs (kwargs): The other keyword args to pass down to the error handling func.\r\n+\r\n+        Returns:\r\n+            list:   A list containing a single dictionary representing a single error.\r\n+\r\n+        Notes:\r\n+            The actual error is useful for errors that are shared like invalid character.\r\n+\r\n+        \"\"\"\r\n+        error_func = error_functions.get(error_type)\r\n+        if not error_func:\r\n+            error_object = ErrorHandler.val_error_unknown(*args, **kwargs)\r\n+            error_object['code'] = error_type\r\n+        else:\r\n+            error_object = error_func(*args, **kwargs)\r\n+\r\n+        if actual_error:\r\n+            error_object['code'] = actual_error\r\n+\r\n+        return [error_object]\r\n+\r\n+    def add_context_and_filter(self, issues):\r\n+        \"\"\" Filter out warnings if requested, while adding context to issues.\r\n+\r\n+            issues(list):\r\n+                list:   A list containing a single dictionary representing a single error.\r\n+        \"\"\"\r\n+        if not self._check_for_warnings:\r\n+            issues[:] = self.filter_issues_by_severity(issues, ErrorSeverity.ERROR)\r\n+\r\n+        for error_object in issues:\r\n+            self._add_context_to_errors(error_object, self.error_context)\r\n+            self._update_error_with_char_pos(error_object)\r\n+\r\n+    @staticmethod\r\n+    def format_error_from_context(error_type, error_context, *args, actual_error=None, **kwargs):\r\n+        \"\"\" Format an error based on the error type.\r\n+\r\n+        Parameters:\r\n+            error_type (str): The type of error.  Registered with @hed_error or @hed_tag_error.\r\n+            error_context (list): Contains the error context to use for this error.\r\n+            args (args): Any remaining non keyword args.\r\n+            actual_error (str or None): Error code to actually add to report out.\r\n+            kwargs (kwargs): Keyword parameters to pass down to the error handling func.\r\n+\r\n+        Returns:\r\n+            list:  A list containing a single dictionary.\r\n+\r\n+        Notes:\r\n+            - Generally the error_context is returned from _add_context_to_errors.\r\n+            - The actual_error is useful for errors that are shared like invalid character.\r\n+            - This can't filter out warnings like the other ones.\r\n+\r\n+        \"\"\"\r\n+        error_list = ErrorHandler.format_error(error_type, *args, actual_error=actual_error, **kwargs)\r\n+\r\n+        ErrorHandler._add_context_to_errors(error_list[0], error_context)\r\n+        ErrorHandler._update_error_with_char_pos(error_list[0])\r\n+        return error_list\r\n+\r\n+    @staticmethod\r\n+    def _add_context_to_errors(error_object, error_context_to_add):\r\n+        \"\"\" Add relevant context such as row number or column name around an error object.\r\n+\r\n+        Parameters:\r\n+            error_object (dict): Generated error containing at least a code and message entry.\r\n+            error_context_to_add (list): Source context to use.  If none, the error handler context is used.\r\n+\r\n+        Returns:\r\n+            list: A list of dict with needed context strings added at the beginning of the list.\r\n+\r\n+        \"\"\"\r\n+        for (context_type, context) in error_context_to_add:\r\n+            error_object[context_type] = context\r\n+\r\n+        return error_object\r\n+\r\n+    @staticmethod\r\n+    def _create_error_object(error_type, base_message, severity, **kwargs):\r\n+        error_object = {'code': error_type,\r\n+                        'message': base_message,\r\n+                        'severity': severity\r\n+                        }\r\n+\r\n+        for key, value in kwargs.items():\r\n+            error_object.setdefault(key, value)\r\n+\r\n+        return error_object\r\n+\r\n+    @staticmethod\r\n+    def _get_tag_span_to_error_object(error_object):\r\n+        if ErrorContext.HED_STRING not in error_object:\r\n+            return None, None\r\n+\r\n+        if 'source_tag' in error_object:\r\n+            source_tag = error_object['source_tag']\r\n+            if isinstance(source_tag, int):\r\n+                return None, None\r\n+        else:\r\n+            return None, None\r\n+\r\n+        hed_string = error_object[ErrorContext.HED_STRING]\r\n+        span = hed_string._get_org_span(source_tag)\r\n+        return span\r\n+\r\n+    @staticmethod\r\n+    def _update_error_with_char_pos(error_object):\r\n+        # This part is optional as you can always generate these as needed.\r\n+        start, end = ErrorHandler._get_tag_span_to_error_object(error_object)\r\n+        if start is not None:\r\n+            # silence warning in pycharm\r\n+            start = int(start)\r\n+            source_tag = error_object.get('source_tag', None)\r\n+            # Todo: Move this functionality somewhere more centralized.\r\n+            # If the tag has been modified from the original, don't try to use sub indexing.\r\n+            if source_tag and source_tag._tag:\r\n+                new_start, new_end = start, end\r\n+            else:\r\n+                new_start = start + error_object.get('index_in_tag', 0)\r\n+                index_in_tag_end = end\r\n+                if 'index_in_tag_end' in error_object:\r\n+                    index_in_tag_end = start + error_object['index_in_tag_end']\r\n+                new_end = index_in_tag_end\r\n+            error_object['char_index'], error_object['char_index_end'] = new_start, new_end\r\n+            error_object['message'] += f\"  Problem spans string indexes: {new_start}, {new_end}\"\r\n+\r\n+    @hed_error(\"Unknown\")\r\n+    def val_error_unknown(*args, **kwargs):\r\n+        \"\"\" Default error handler if no error of this type was registered.\r\n+\r\n+        Parameters:\r\n+            args (args):  List of non-keyword parameters (varies).\r\n+            kwargs (kwargs): Keyword parameters (varies)\r\n+\r\n+        Returns:\r\n+            str: The error message.\r\n+\r\n+        \"\"\"\r\n+        return f\"Unknown error.  Args: {str(args), str(kwargs)}\"\r\n+\r\n+    @staticmethod\r\n+    def filter_issues_by_severity(issues_list, severity):\r\n+        \"\"\" Gather all issues matching or below a given severity.\r\n+\r\n+        Parameters:\r\n+            issues_list (list): A list of dictionaries containing the full issue list.\r\n+            severity (int): The level of issues to keep.\r\n+\r\n+        Returns:\r\n+            list: A list of dictionaries containing the issue list after filtering by severity.\r\n+\r\n+        \"\"\"\r\n+        return [issue for issue in issues_list if issue['severity'] <= severity]\r\n+\r\n+\r\n+def sort_issues(issues, reverse=False):\r\n+    \"\"\"Sort a list of issues by the error context values.\r\n+\r\n+    Parameters:\r\n+        issues (list): A list of dictionaries representing the issues to be sorted.\r\n+        reverse (bool, optional): If True, sorts the list in descending order. Default is False.\r\n+\r\n+    Returns:\r\n+        list: The sorted list of issues.\"\"\"\r\n+    def _get_keys(d):\r\n+        result = []\r\n+        for key in default_sort_list:\r\n+            if key in int_sort_list:\r\n+                result.append(d.get(key, -1))\r\n+            else:\r\n+                result.append(d.get(key, \"\"))\r\n+        return tuple(result)\r\n+\r\n+    issues = sorted(issues, key=_get_keys, reverse=reverse)\r\n+\r\n+    return issues\r\n+\r\n+\r\n+def check_for_any_errors(issues_list):\r\n+    \"\"\" Return True if there are any errors with a severity of warning. \"\"\"\r\n+    for issue in issues_list:\r\n+        if issue['severity'] < ErrorSeverity.WARNING:\r\n+            return True\r\n+\r\n+    return False\r\n+\r\n+\r\n+def get_printable_issue_string(issues, title=None, severity=None, skip_filename=True, add_link=False):\r\n+    \"\"\" Return a string with issues list flatted into single string, one per line.\r\n+\r\n+    Parameters:\r\n+        issues (list):  Issues to print.\r\n+        title (str):  Optional title that will always show up first if present(even if there are no validation issues).\r\n+        severity (int):        Return only warnings >= severity.\r\n+        skip_filename (bool):  If True, don't add the filename context to the printable string.\r\n+        add_link (bool): Add a link at the end of message to the appropriate error if True\r\n+    Returns:\r\n+        str:   A string containing printable version of the issues or ''.\r\n+\r\n+    \"\"\"\r\n+    if severity is not None:\r\n+        issues = ErrorHandler.filter_issues_by_severity(issues, severity)\r\n+\r\n+    output_dict = _build_error_context_dict(issues, skip_filename)\r\n+    issue_string = _error_dict_to_string(output_dict, add_link=add_link)\r\n+\r\n+    if title:\r\n+        issue_string = title + '\\n' + issue_string\r\n+    return issue_string\r\n+\r\n+\r\n+def get_printable_issue_string_html(issues, title=None, severity=None, skip_filename=True):\r\n+    \"\"\" Return a string with issues list as an HTML tree.\r\n+\r\n+    Parameters:\r\n+        issues (list):  Issues to print.\r\n+        title (str):  Optional title that will always show up first if present.\r\n+        severity (int): Return only warnings >= severity.\r\n+        skip_filename (bool): If True, don't add the filename context to the printable string.\r\n+\r\n+    Returns:\r\n+        str: An HTML string containing the issues or ''.\r\n+    \"\"\"\r\n+    if severity is not None:\r\n+        issues = ErrorHandler.filter_issues_by_severity(issues, severity)\r\n+\r\n+    output_dict = _build_error_context_dict(issues, skip_filename)\r\n+\r\n+    root_element = _create_error_tree(output_dict)\r\n+    if title:\r\n+        title_element = ET.Element(\"h1\")\r\n+        title_element.text = title\r\n+        root_element.insert(0, title_element)\r\n+    return ET.tostring(root_element, encoding='unicode')\r\n+\r\n+\r\n+def create_doc_link(error_code):\r\n+    \"\"\"If error code is a known code, return a documentation url for it.\r\n+\r\n+    Parameters:\r\n+        error_code(str): A HED error code.\r\n+\r\n+    Returns:\r\n+        url(str or None): The URL if it's a valid code.\r\n+    \"\"\"\r\n+    if error_code in known_error_codes[\"hed_validation_errors\"] \\\r\n+            or error_code in known_error_codes[\"schema_validation_errors\"]:\r\n+        modified_error_code = error_code.replace(\"_\", \"-\").lower()\r\n+        return f\"https://hed-specification.readthedocs.io/en/latest/Appendix_B.html#{modified_error_code}\"\r\n+    return None\r\n+\r\n+\r\n+def _build_error_context_dict(issues, skip_filename):\r\n+    \"\"\"Build the context -> error dictionary for an entire list of issues.\r\n+\r\n+    Returns:\r\n+        dict: A nested dictionary structure with a \"children\" key at each level for unrelated children.\r\n+    \"\"\"\r\n+    output_dict = None\r\n+    for single_issue in issues:\r\n+        single_issue_context = _get_context_from_issue(single_issue, skip_filename)\r\n+        output_dict = _add_single_error_to_dict(single_issue_context, output_dict, single_issue)\r\n+\r\n+    return output_dict\r\n+\r\n+\r\n+def _add_single_error_to_dict(items, root=None, issue_to_add=None):\r\n+    \"\"\" Build a nested dictionary out of the context lists.\r\n+\r\n+    Parameters:\r\n+        items (list): A list of error contexts\r\n+        root (dict, optional): An existing nested dictionary structure to update.\r\n+        issue_to_add (dict, optional): The issue to add at this level of context.\r\n+\r\n+    Returns:\r\n+        dict: A nested dictionary structure with a \"children\" key at each level for unrelated children.\r\n+    \"\"\"\r\n+    if root is None:\r\n+        root = {\"children\": []}\r\n+\r\n+    current_dict = root\r\n+    for item in items:\r\n+        # Navigate to the next level if the item already exists, or create a new level\r\n+        next_dict = current_dict.get(item, {\"children\": []})\r\n+        current_dict[item] = next_dict\r\n+        current_dict = next_dict\r\n+\r\n+    if issue_to_add:\r\n+        current_dict[\"children\"].append(issue_to_add)\r\n+\r\n+    return root\r\n+\r\n+\r\n+def _error_dict_to_string(print_dict, add_link=True, level=0):\r\n+    output = \"\"\r\n+    if print_dict is None:\r\n+        return output\r\n+    for context, value in print_dict.items():\r\n+        if context == \"children\":\r\n+            for child in value:\r\n+                single_issue_message = child[\"message\"]\r\n+                issue_string = level * \"\\t\" + _get_error_prefix(child)\r\n+                issue_string += f\"{single_issue_message}\\n\"\r\n+                if add_link:\r\n+                    link_url = create_doc_link(child['code'])\r\n+                    if link_url:\r\n+                        single_issue_message += f\"   See... {link_url}\"\r\n+                output += issue_string\r\n+            continue\r\n+        output += _format_single_context_string(context[0], context[1], level)\r\n+        output += _error_dict_to_string(value, add_link, level + 1)\r\n+\r\n+    return output\r\n+\r\n+\r\n+def _get_context_from_issue(val_issue, skip_filename=True):\r\n+    \"\"\" Extract all the context values from the given issue.\r\n+\r\n+    Parameters:\r\n+        val_issue (dict): A dictionary a representing a single error.\r\n+        skip_filename (bool): If True, don't gather the filename context.\r\n+\r\n+    Returns:\r\n+        list: A list of tuples containing the context_type and context for the given issue.\r\n+\r\n+    \"\"\"\r\n+    single_issue_context = []\r\n+    for key, value in val_issue.items():\r\n+        if skip_filename and key == ErrorContext.FILE_NAME:\r\n+            continue\r\n+        if key == ErrorContext.HED_STRING:\r\n+            value = value.get_original_hed_string()\r\n+        if key.startswith(\"ec_\"):\r\n+            single_issue_context.append((key, str(value)))\r\n+\r\n+    return single_issue_context\r\n+\r\n+\r\n+def _get_error_prefix(single_issue):\r\n+    \"\"\"Return the prefix for the error message based on severity and error code.\r\n+\r\n+    Parameters:\r\n+        single_issue(dict): A single issue object.\r\n+\r\n+    Returns:\r\n+        error_prefix(str):  the prefix to use.\r\n+    \"\"\"\r\n+    severity = single_issue.get('severity', ErrorSeverity.ERROR)\r\n+    error_code = single_issue['code']\r\n+\r\n+    if severity == ErrorSeverity.ERROR:\r\n+        error_prefix = f\"{error_code}: \"\r\n+    else:\r\n+        error_prefix = f\"{error_code}: (Warning) \"\r\n+    return error_prefix\r\n+\r\n+\r\n+def _format_single_context_string(context_type, context, tab_count=0):\r\n+    \"\"\" Return the human-readable form of a single context tuple.\r\n+\r\n+    Parameters:\r\n+        context_type (str): The context type of this entry.\r\n+        context (str or HedString): The value of this context.\r\n+        tab_count (int): Number of tabs to name_prefix each line with.\r\n+\r\n+    Returns:\r\n+        str: A string containing the context, including tabs.\r\n+\r\n+    \"\"\"\r\n+    tab_string = tab_count * '\\t'\r\n+    error_types = {\r\n+        ErrorContext.FILE_NAME: f\"\\nErrors in file '{context}'\",\r\n+        ErrorContext.SIDECAR_COLUMN_NAME: f\"Column '{context}':\",\r\n+        ErrorContext.SIDECAR_KEY_NAME: f\"Key: {context}\",\r\n+        ErrorContext.ROW: f'Issues in row {context}:',\r\n+        ErrorContext.COLUMN: f'Issues in column {context}:',\r\n+        ErrorContext.CUSTOM_TITLE: context,\r\n+        ErrorContext.LINE: f\"Line: {context}\",\r\n+        ErrorContext.HED_STRING: f\"hed string: {context}\",\r\n+        ErrorContext.SCHEMA_SECTION: f\"Schema Section: {context}\",\r\n+        ErrorContext.SCHEMA_TAG: f\"Source tag: {context}\",\r\n+        ErrorContext.SCHEMA_ATTRIBUTE: f\"Source Attribute: {context}\",\r\n+    }\r\n+    context_portion = error_types[context_type]\r\n+    context_string = f\"{tab_string}{context_portion}\\n\"\r\n+    return context_string\r\n+\r\n+\r\n+def _create_error_tree(error_dict, parent_element=None, add_link=True):\r\n+    if parent_element is None:\r\n+        parent_element = ET.Element(\"ul\")\r\n+\r\n+    for context, value in error_dict.items():\r\n+        if context == \"children\":\r\n+            for child in value:\r\n+                child_li = ET.SubElement(parent_element, \"li\")\r\n+                error_prefix = _get_error_prefix(child)\r\n+                single_issue_message = child[\"message\"]\r\n+\r\n+                # Create a link for the error prefix if add_link is True.\r\n+                if add_link:\r\n+                    link_url = create_doc_link(child['code'])\r\n+                    if link_url:\r\n+                        a_element = ET.SubElement(child_li, \"a\", href=link_url)\r\n+                        a_element.text = error_prefix\r\n+                        a_element.tail = \" \" + single_issue_message\r\n+                    else:\r\n+                        child_li.text = error_prefix + \" \" + single_issue_message\r\n+                else:\r\n+                    child_li.text = error_prefix + \" \" + single_issue_message\r\n+            continue\r\n+\r\n+        context_li = ET.SubElement(parent_element, \"li\")\r\n+        context_li.text = _format_single_context_string(context[0], context[1])\r\n+        context_ul = ET.SubElement(context_li, \"ul\")\r\n+        _create_error_tree(value, context_ul, add_link)\r\n+\r\n+    return parent_element\r\n+\r\n+\r\n+def replace_tag_references(list_or_dict):\r\n+    \"\"\" Utility function to remove any references to tags, strings, etc. from any type of nested list or dict.\r\n+\r\n+       Use this if you want to save out issues to a file.\r\n+\r\n+       If you'd prefer a copy returned, use replace_tag_references(list_or_dict.copy()).\r\n+\r\n+    Parameters:\r\n+       list_or_dict(list or dict): An arbitrarily nested list/dict structure\r\n+    \"\"\"\r\n+    if isinstance(list_or_dict, dict):\r\n+        for key, value in list_or_dict.items():\r\n+            if isinstance(value, (dict, list)):\r\n+                replace_tag_references(value)\r\n+            elif isinstance(value, (bool, float, int)):\r\n+                list_or_dict[key] = value\r\n+            else:\r\n+                list_or_dict[key] = str(value)\r\n+    elif isinstance(list_or_dict, list):\r\n+        for key, value in enumerate(list_or_dict):\r\n+            if isinstance(value, (dict, list)):\r\n+                replace_tag_references(value)\r\n+            elif isinstance(value, (bool, float, int)):\r\n+                list_or_dict[key] = value\r\n+            else:\r\n+                list_or_dict[key] = str(value)\r\ndiff --git a/hed/errors/error_types.py b/hed/errors/error_types.py\nindex b07e3544..1ca1e047 100644\n--- a/hed/errors/error_types.py\n+++ b/hed/errors/error_types.py\n@@ -93,7 +93,7 @@ class ValidationErrors:\n     INVALID_TAG_CHARACTER = 'invalidTagCharacter'\n \n     CURLY_BRACE_UNSUPPORTED_HERE = \"CURLY_BRACE_UNSUPPORTED_HERE\"\n-    ONSETS_OUT_OF_ORDER = \"ONSETS_OUT_OF_ORDER\"\n+    ONSETS_UNORDERED = \"ONSETS_UNORDERED\"\n \n \n class SidecarErrors:\n@@ -176,7 +176,7 @@ class TemporalErrors:\n     ONSET_TAG_OUTSIDE_OF_GROUP = \"ONSET_TAG_OUTSIDE_OF_GROUP\"\n     INSET_BEFORE_ONSET = \"INSET_BEFORE_ONSET\"\n     ONSET_SAME_DEFS_ONE_ROW = \"ONSET_SAME_DEFS_ONE_ROW\"\n-    HED_ONSET_WITH_NO_COLUMN = 'HED_ONSET_WITH_NO_COLUMN'\n+    TEMPORAL_TAG_NO_TIME = 'TEMPORAL_TAG_NO_TIME'\n \n     DURATION_HAS_OTHER_TAGS = \"DURATION_HAS_OTHER_TAGS\"\n     DURATION_WRONG_NUMBER_GROUPS = \"DURATION_WRONG_NUMBER_GROUPS\"\ndiff --git a/hed/models/base_input.py b/hed/models/base_input.py\nindex 24dc033c..f02ffe62 100644\n--- a/hed/models/base_input.py\n+++ b/hed/models/base_input.py\n@@ -1,477 +1,480 @@\n-\"\"\"\n-Superclass representing a basic columnar file.\n-\"\"\"\n-import os\n-\n-import openpyxl\n-import pandas as pd\n-\n-from hed.models.column_mapper import ColumnMapper\n-from hed.errors.exceptions import HedFileError, HedExceptions\n-\n-from hed.models.df_util import _handle_curly_braces_refs, filter_series_by_onset\n-\n-\n-class BaseInput:\n-    \"\"\" Superclass representing a basic columnar file. \"\"\"\n-\n-    TEXT_EXTENSION = ['.tsv', '.txt']\n-    EXCEL_EXTENSION = ['.xlsx']\n-\n-    def __init__(self, file, file_type=None, worksheet_name=None, has_column_names=True, mapper=None, name=None,\n-                 allow_blank_names=True):\n-        \"\"\" Constructor for the BaseInput class.\n-\n-        Parameters:\n-            file (str or file-like or pd.Dataframe): An xlsx/tsv file to open.\n-            file_type (str or None): \".xlsx\" (Excel), \".tsv\" or \".txt\" (tab-separated text).\n-                Derived from file if file is a filename.  Ignored if pandas dataframe.\n-            worksheet_name (str or None): Name of Excel workbook worksheet name to use.\n-                (Not applicable to tsv files.)\n-            has_column_names (bool): True if file has column names.\n-                This value is ignored if you pass in a pandas dataframe.\n-            mapper (ColumnMapper or None):  Indicates which columns have HED tags.\n-                See SpreadsheetInput or TabularInput for examples of how to use built-in a ColumnMapper.\n-            name (str or None): Optional field for how this file will report errors.\n-            allow_blank_names(bool): If True, column names can be blank\n-\n-        :raises HedFileError:\n-            - file is blank.\n-            - An invalid dataframe was passed with size 0.\n-            - An invalid extension was provided.\n-            - A duplicate or empty column name appears.\n-            - Cannot open the indicated file.\n-            - The specified worksheet name does not exist.\n-            - If the sidecar file or tabular file had invalid format and could not be read.\n-\n-         \"\"\"\n-        if mapper is None:\n-            mapper = ColumnMapper()\n-        self._mapper = mapper\n-        self._has_column_names = has_column_names\n-        self._name = name\n-        # This is the loaded workbook if we loaded originally from an Excel file.\n-        self._loaded_workbook = None\n-        self._worksheet_name = worksheet_name\n-        self._dataframe = None\n-\n-        input_type = file_type\n-        if isinstance(file, str):\n-            if file_type is None:\n-                _, input_type = os.path.splitext(file)\n-            if self.name is None:\n-                self._name = file\n-\n-        self._open_dataframe_file(file, has_column_names, input_type)\n-\n-        column_issues = ColumnMapper.check_for_blank_names(self.columns, allow_blank_names=allow_blank_names)\n-        if column_issues:\n-            raise HedFileError(HedExceptions.BAD_COLUMN_NAMES, \"Duplicate or blank columns found. See issues.\",\n-                               self.name, issues=column_issues)\n-\n-        self.reset_mapper(mapper)\n-\n-    def reset_mapper(self, new_mapper):\n-        \"\"\" Set mapper to a different view of the file.\n-\n-        Parameters:\n-            new_mapper (ColumnMapper): A column mapper to be associated with this base input.\n-        \"\"\"\n-        self._mapper = new_mapper\n-        if not self._mapper:\n-            self._mapper = ColumnMapper()\n-\n-        if self._dataframe is not None and self._has_column_names:\n-            columns = self._dataframe.columns\n-            self._mapper.set_column_map(columns)\n-\n-    @property\n-    def dataframe(self):\n-        \"\"\" The underlying dataframe. \"\"\"\n-        return self._dataframe\n-\n-    @property\n-    def dataframe_a(self):\n-        \"\"\"Return the assembled dataframe Probably a placeholder name.\n-\n-        Returns:\n-            Dataframe: the assembled dataframe\"\"\"\n-        return self.assemble()\n-\n-    @property\n-    def series_a(self):\n-        \"\"\"Return the assembled dataframe as a series.\n-\n-        Returns:\n-            Series: the assembled dataframe with columns merged.\n-        \"\"\"\n-        return self.combine_dataframe(self.assemble())\n-\n-    @property\n-    def series_filtered(self):\n-        \"\"\"Return the assembled dataframe as a series, with rows that have the same onset combined.\n-\n-        Returns:\n-            Series or None: the assembled dataframe with columns merged, and the rows filtered together.\n-        \"\"\"\n-        if self.onsets is not None:\n-            return filter_series_by_onset(self.series_a, self.onsets)\n-\n-    @property\n-    def onsets(self):\n-        \"\"\"Return the onset column if it exists. \"\"\"\n-        if \"onset\" in self.columns:\n-            return self._dataframe[\"onset\"]\n-\n-    @property\n-    def needs_sorting(self):\n-        \"\"\"Return True if this both has an onset column, and it needs sorting.\"\"\"\n-        onsets = self.onsets\n-        if onsets is not None:\n-            onsets = pd.to_numeric(self.dataframe['onset'], errors='coerce')\n-            return not onsets.is_monotonic_increasing\n-\n-    @property\n-    def name(self):\n-        \"\"\" Name of the data. \"\"\"\n-        return self._name\n-\n-    @property\n-    def has_column_names(self):\n-        \"\"\" True if dataframe has column names. \"\"\"\n-        return self._has_column_names\n-\n-    @property\n-    def loaded_workbook(self):\n-        \"\"\" The underlying loaded workbooks. \"\"\"\n-        return self._loaded_workbook\n-\n-    @property\n-    def worksheet_name(self):\n-        \"\"\" The worksheet name. \"\"\"\n-        return self._worksheet_name\n-\n-    def convert_to_form(self, hed_schema, tag_form):\n-        \"\"\" Convert all tags in underlying dataframe to the specified form.\n-\n-        Parameters:\n-            hed_schema (HedSchema): The schema to use to convert tags.\n-            tag_form(str): HedTag property to convert tags to.\n-                Most cases should use convert_to_short or convert_to_long below.\n-        \"\"\"\n-        from hed.models.df_util import convert_to_form\n-        convert_to_form(self._dataframe, hed_schema, tag_form, self._mapper.get_tag_columns())\n-\n-    def convert_to_short(self, hed_schema):\n-        \"\"\" Convert all tags in underlying dataframe to short form.\n-\n-        Parameters:\n-            hed_schema (HedSchema): The schema to use to convert tags.\n-        \"\"\"\n-        return self.convert_to_form(hed_schema, \"short_tag\")\n-\n-    def convert_to_long(self, hed_schema):\n-        \"\"\" Convert all tags in underlying dataframe to long form.\n-\n-        Parameters:\n-            hed_schema (HedSchema or None): The schema to use to convert tags.\n-        \"\"\"\n-        return self.convert_to_form(hed_schema, \"long_tag\")\n-\n-    def shrink_defs(self, hed_schema):\n-        \"\"\" Shrinks any def-expand found in the underlying dataframe.\n-\n-        Parameters:\n-            hed_schema (HedSchema or None): The schema to use to identify defs.\n-        \"\"\"\n-        from df_util import shrink_defs\n-        shrink_defs(self._dataframe, hed_schema=hed_schema, columns=self._mapper.get_tag_columns())\n-\n-    def expand_defs(self, hed_schema, def_dict):\n-        \"\"\" Shrinks any def-expand found in the underlying dataframe.\n-\n-        Parameters:\n-            hed_schema (HedSchema or None): The schema to use to identify defs.\n-            def_dict (DefinitionDict): The definitions to expand.\n-        \"\"\"\n-        from df_util import expand_defs\n-        expand_defs(self._dataframe, hed_schema=hed_schema, def_dict=def_dict, columns=self._mapper.get_tag_columns())\n-\n-    def to_excel(self, file):\n-        \"\"\" Output to an Excel file.\n-\n-        Parameters:\n-            file (str or file-like): Location to save this base input.\n-\n-        :raises ValueError:\n-            - If empty file object was passed.\n-\n-        :raises OSError:\n-            - Cannot open the indicated file.\n-        \"\"\"\n-        if not file:\n-            raise ValueError(\"Empty file name or object passed in to BaseInput.save.\")\n-\n-        dataframe = self._dataframe\n-        if self._loaded_workbook:\n-            old_worksheet = self.get_worksheet(self._worksheet_name)\n-            # Excel spreadsheets are 1 based, then add another 1 for column names if present\n-            adj_row_for_col_names = 1\n-            if self._has_column_names:\n-                adj_row_for_col_names += 1\n-            adj_for_one_based_cols = 1\n-            for row_number, text_file_row in dataframe.iterrows():\n-                for column_number, column_text in enumerate(text_file_row):\n-                    cell_value = dataframe.iloc[row_number, column_number]\n-                    old_worksheet.cell(row_number + adj_row_for_col_names,\n-                                       column_number + adj_for_one_based_cols).value = cell_value\n-\n-            self._loaded_workbook.save(file)\n-        else:\n-            dataframe.to_excel(file, header=self._has_column_names)\n-\n-    def to_csv(self, file=None):\n-        \"\"\" Write to file or return as a string.\n-\n-        Parameters:\n-            file (str, file-like, or None): Location to save this file. If None, return as string.\n-        Returns:\n-            None or str:  None if file is given or the contents as a str if file is None.\n-\n-        :raises OSError:\n-            - Cannot open the indicated file.\n-        \"\"\"\n-        dataframe = self._dataframe\n-        csv_string_if_filename_none = dataframe.to_csv(file, sep='\\t', index=False, header=self._has_column_names)\n-        return csv_string_if_filename_none\n-\n-    @property\n-    def columns(self):\n-        \"\"\" Returns a list of the column names.\n-\n-            Empty if no column names.\n-\n-        Returns:\n-            columns(list): The column names.\n-        \"\"\"\n-        columns = []\n-        if self._dataframe is not None and self._has_column_names:\n-            columns = list(self._dataframe.columns)\n-        return columns\n-\n-    def column_metadata(self):\n-        \"\"\" Return the metadata for each column.\n-\n-        Returns:\n-            dict: Number/ColumnMeta pairs.\n-        \"\"\"\n-        if self._mapper:\n-            return self._mapper._final_column_map\n-        return {}\n-\n-    def set_cell(self, row_number, column_number, new_string_obj, tag_form=\"short_tag\"):\n-        \"\"\" Replace the specified cell with transformed text.\n-\n-        Parameters:\n-            row_number (int):    The row number of the spreadsheet to set.\n-            column_number (int): The column number of the spreadsheet to set.\n-            new_string_obj (HedString): Object with text to put in the given cell.\n-            tag_form (str): Version of the tags (short_tag, long_tag, base_tag, etc)\n-\n-        Notes:\n-             Any attribute of a HedTag that returns a string is a valid value of tag_form.\n-\n-        :raises ValueError:\n-            - There is not a loaded dataframe.\n-\n-        :raises KeyError:\n-            - The indicated row/column does not exist.\n-\n-        :raises AttributeError:\n-            - The indicated tag_form is not an attribute of HedTag.\n-        \"\"\"\n-        if self._dataframe is None:\n-            raise ValueError(\"No data frame loaded\")\n-\n-        new_text = new_string_obj.get_as_form(tag_form)\n-        self._dataframe.iloc[row_number, column_number] = new_text\n-\n-    def get_worksheet(self, worksheet_name=None):\n-        \"\"\" Get the requested worksheet.\n-\n-        Parameters:\n-            worksheet_name (str or None): The name of the requested worksheet by name or the first one if None.\n-\n-        Returns:\n-            openpyxl.workbook.Workbook: The workbook request.\n-\n-        Notes:\n-            If None, returns the first worksheet.\n-\n-        :raises KeyError:\n-            - The specified worksheet name does not exist.\n-        \"\"\"\n-        if worksheet_name and self._loaded_workbook:\n-            # return self._loaded_workbook.get_sheet_by_name(worksheet_name)\n-            return self._loaded_workbook[worksheet_name]\n-        elif self._loaded_workbook:\n-            return self._loaded_workbook.worksheets[0]\n-        else:\n-            return None\n-\n-    @staticmethod\n-    def _get_dataframe_from_worksheet(worksheet, has_headers):\n-        \"\"\" Create a dataframe from the worksheet.\n-\n-        Parameters:\n-            worksheet (Worksheet): The loaded worksheet to convert.\n-            has_headers (bool): True if this worksheet has column headers.\n-\n-        Returns:\n-            DataFrame: The converted data frame.\n-\n-        \"\"\"\n-        if has_headers:\n-            data = worksheet.values\n-            # first row is columns\n-            cols = next(data)\n-            data = list(data)\n-            return pd.DataFrame(data, columns=cols, dtype=str)\n-        else:\n-            return pd.DataFrame(worksheet.values, dtype=str)\n-\n-    def validate(self, hed_schema, extra_def_dicts=None, name=None, error_handler=None):\n-        \"\"\"Creates a SpreadsheetValidator and returns all issues with this file.\n-\n-        Parameters:\n-            hed_schema(HedSchema): The schema to use for validation.\n-            extra_def_dicts(list of DefDict or DefDict): All definitions to use for validation.\n-            name(str): The name to report errors from this file as.\n-            error_handler (ErrorHandler): Error context to use.  Creates a new one if None.\n-\n-        Returns:\n-            issues (list of dict): A list of issues for a HED string.\n-        \"\"\"\n-        from hed.validator.spreadsheet_validator import SpreadsheetValidator\n-        if not name:\n-            name = self.name\n-        tab_validator = SpreadsheetValidator(hed_schema)\n-        validation_issues = tab_validator.validate(self, self._mapper.get_def_dict(hed_schema, extra_def_dicts), name,\n-                                                   error_handler=error_handler)\n-        return validation_issues\n-\n-    @staticmethod\n-    def _dataframe_has_names(dataframe):\n-        for column in dataframe.columns:\n-            if isinstance(column, str):\n-                return True\n-        return False\n-\n-    def assemble(self, mapper=None, skip_curly_braces=False):\n-        \"\"\" Assembles the HED strings.\n-\n-        Parameters:\n-            mapper(ColumnMapper or None): Generally pass none here unless you want special behavior.\n-            skip_curly_braces (bool): If True, don't plug in curly brace values into columns.\n-        Returns:\n-            Dataframe: The assembled dataframe.\n-        \"\"\"\n-        if mapper is None:\n-            mapper = self._mapper\n-\n-        all_columns = self._handle_transforms(mapper)\n-        if skip_curly_braces:\n-            return all_columns\n-        transformers, _ = mapper.get_transformers()\n-        refs = self.get_column_refs()\n-        column_names = list(transformers)\n-        return _handle_curly_braces_refs(all_columns, refs, column_names)\n-\n-    def _handle_transforms(self, mapper):\n-        transformers, need_categorical = mapper.get_transformers()\n-        if transformers:\n-            all_columns = self._dataframe\n-            if need_categorical:\n-                all_columns[need_categorical] = all_columns[need_categorical].astype('category')\n-\n-            all_columns = all_columns.transform(transformers)\n-\n-            if need_categorical:\n-                all_columns[need_categorical] = all_columns[need_categorical].astype('str')\n-        else:\n-            all_columns = self._dataframe\n-\n-        return all_columns\n-\n-    @staticmethod\n-    def combine_dataframe(dataframe):\n-        \"\"\" Combine all columns in the given dataframe into a single HED string series,\n-            skipping empty columns and columns with empty strings.\n-\n-        Parameters:\n-            dataframe(Dataframe): The dataframe to combine\n-\n-        Returns:\n-            Series: The assembled series.\n-        \"\"\"\n-        dataframe = dataframe.apply(\n-            lambda x: ', '.join(filter(lambda e: bool(e) and e != \"n/a\", map(str, x))),\n-            axis=1\n-        )\n-        return dataframe\n-\n-    def get_def_dict(self, hed_schema, extra_def_dicts=None):\n-        \"\"\" Return the definition dict for this file.\n-\n-        Note: Baseclass implementation returns just extra_def_dicts.\n-\n-        Parameters:\n-            hed_schema(HedSchema): Identifies tags to find definitions(if needed).\n-            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\n-\n-        Returns:\n-            DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\n-        \"\"\"\n-        from hed.models.definition_dict import DefinitionDict\n-        return DefinitionDict(extra_def_dicts, hed_schema)\n-\n-    def get_column_refs(self):\n-        \"\"\" Return a list of column refs for this file.\n-\n-            Default implementation returns none.\n-\n-        Returns:\n-            column_refs(list): A list of unique column refs found.\n-        \"\"\"\n-        return []\n-\n-    def _open_dataframe_file(self, file, has_column_names, input_type):\n-        pandas_header = 0\n-        if not has_column_names:\n-            pandas_header = None\n-\n-        if isinstance(file, pd.DataFrame):\n-            self._dataframe = file.astype(str)\n-            self._has_column_names = self._dataframe_has_names(self._dataframe)\n-        elif not file:\n-            raise HedFileError(HedExceptions.FILE_NOT_FOUND, \"Empty file passed to BaseInput.\", file)\n-        elif input_type in self.TEXT_EXTENSION:\n-            try:\n-                self._dataframe = pd.read_csv(file, delimiter='\\t', header=pandas_header,\n-                                              dtype=str, keep_default_na=True, na_values=(\"\", \"null\"))\n-            except Exception as e:\n-                raise HedFileError(HedExceptions.INVALID_FILE_FORMAT, str(e), self.name) from e\n-            # Convert nan values to a known value\n-            self._dataframe = self._dataframe.fillna(\"n/a\")\n-        elif input_type in self.EXCEL_EXTENSION:\n-            try:\n-                self._loaded_workbook = openpyxl.load_workbook(file)\n-                loaded_worksheet = self.get_worksheet(self._worksheet_name)\n-                self._dataframe = self._get_dataframe_from_worksheet(loaded_worksheet, has_column_names)\n-            except Exception as e:\n-                raise HedFileError(HedExceptions.GENERIC_ERROR, str(e), self.name) from e\n-        else:\n-            raise HedFileError(HedExceptions.INVALID_EXTENSION, \"\", file)\n-\n-        if self._dataframe.size == 0:\n-            raise HedFileError(HedExceptions.INVALID_DATAFRAME, \"Invalid dataframe(malformed datafile, etc)\", file)\n+\"\"\"\r\n+Superclass representing a basic columnar file.\r\n+\"\"\"\r\n+import os\r\n+\r\n+import openpyxl\r\n+import pandas as pd\r\n+\r\n+from hed.models.column_mapper import ColumnMapper\r\n+from hed.errors.exceptions import HedFileError, HedExceptions\r\n+\r\n+from hed.models.df_util import _handle_curly_braces_refs, filter_series_by_onset\r\n+\r\n+\r\n+class BaseInput:\r\n+    \"\"\" Superclass representing a basic columnar file. \"\"\"\r\n+\r\n+    TEXT_EXTENSION = ['.tsv', '.txt']\r\n+    EXCEL_EXTENSION = ['.xlsx']\r\n+\r\n+    def __init__(self, file, file_type=None, worksheet_name=None, has_column_names=True, mapper=None, name=None,\r\n+                 allow_blank_names=True):\r\n+        \"\"\" Constructor for the BaseInput class.\r\n+\r\n+        Parameters:\r\n+            file (str or file-like or pd.Dataframe): An xlsx/tsv file to open.\r\n+            file_type (str or None): \".xlsx\" (Excel), \".tsv\" or \".txt\" (tab-separated text).\r\n+                Derived from file if file is a filename.  Ignored if pandas dataframe.\r\n+            worksheet_name (str or None): Name of Excel workbook worksheet name to use.\r\n+                (Not applicable to tsv files.)\r\n+            has_column_names (bool): True if file has column names.\r\n+                This value is ignored if you pass in a pandas dataframe.\r\n+            mapper (ColumnMapper or None):  Indicates which columns have HED tags.\r\n+                See SpreadsheetInput or TabularInput for examples of how to use built-in a ColumnMapper.\r\n+            name (str or None): Optional field for how this file will report errors.\r\n+            allow_blank_names(bool): If True, column names can be blank\r\n+\r\n+        :raises HedFileError:\r\n+            - file is blank.\r\n+            - An invalid dataframe was passed with size 0.\r\n+            - An invalid extension was provided.\r\n+            - A duplicate or empty column name appears.\r\n+            - Cannot open the indicated file.\r\n+            - The specified worksheet name does not exist.\r\n+            - If the sidecar file or tabular file had invalid format and could not be read.\r\n+\r\n+         \"\"\"\r\n+        if mapper is None:\r\n+            mapper = ColumnMapper()\r\n+        self._mapper = mapper\r\n+        self._has_column_names = has_column_names\r\n+        self._name = name\r\n+        # This is the loaded workbook if we loaded originally from an Excel file.\r\n+        self._loaded_workbook = None\r\n+        self._worksheet_name = worksheet_name\r\n+        self._dataframe = None\r\n+\r\n+        input_type = file_type\r\n+        if isinstance(file, str):\r\n+            if file_type is None:\r\n+                _, input_type = os.path.splitext(file)\r\n+            if self.name is None:\r\n+                self._name = file\r\n+\r\n+        self._open_dataframe_file(file, has_column_names, input_type)\r\n+\r\n+        column_issues = ColumnMapper.check_for_blank_names(self.columns, allow_blank_names=allow_blank_names)\r\n+        if column_issues:\r\n+            raise HedFileError(HedExceptions.BAD_COLUMN_NAMES, \"Duplicate or blank columns found. See issues.\",\r\n+                               self.name, issues=column_issues)\r\n+\r\n+        self.reset_mapper(mapper)\r\n+\r\n+    def reset_mapper(self, new_mapper):\r\n+        \"\"\" Set mapper to a different view of the file.\r\n+\r\n+        Parameters:\r\n+            new_mapper (ColumnMapper): A column mapper to be associated with this base input.\r\n+        \"\"\"\r\n+        self._mapper = new_mapper\r\n+        if not self._mapper:\r\n+            self._mapper = ColumnMapper()\r\n+\r\n+        if self._dataframe is not None and self._has_column_names:\r\n+            columns = self._dataframe.columns\r\n+            self._mapper.set_column_map(columns)\r\n+\r\n+    @property\r\n+    def dataframe(self):\r\n+        \"\"\" The underlying dataframe. \"\"\"\r\n+        return self._dataframe\r\n+\r\n+    @property\r\n+    def dataframe_a(self):\r\n+        \"\"\"Return the assembled dataframe Probably a placeholder name.\r\n+\r\n+        Returns:\r\n+            Dataframe: the assembled dataframe\"\"\"\r\n+        return self.assemble()\r\n+\r\n+    @property\r\n+    def series_a(self):\r\n+        \"\"\"Return the assembled dataframe as a series.\r\n+\r\n+        Returns:\r\n+            Series: the assembled dataframe with columns merged.\r\n+        \"\"\"\r\n+\r\n+        return self.combine_dataframe(self.assemble())\r\n+\r\n+    @property\r\n+    def series_filtered(self):\r\n+        \"\"\"Return the assembled dataframe as a series, with rows that have the same onset combined.\r\n+\r\n+        Returns:\r\n+            Series or None: the assembled dataframe with columns merged, and the rows filtered together.\r\n+        \"\"\"\r\n+        if self.onsets is not None:\r\n+            return filter_series_by_onset(self.series_a, self.onsets)\r\n+\r\n+    @property\r\n+    def onsets(self):\r\n+        \"\"\"Return the onset column if it exists. \"\"\"\r\n+        if \"onset\" in self.columns:\r\n+            return self._dataframe[\"onset\"]\r\n+\r\n+    @property\r\n+    def needs_sorting(self):\r\n+        \"\"\"Return True if this both has an onset column, and it needs sorting.\"\"\"\r\n+        onsets = self.onsets\r\n+        if onsets is not None:\r\n+            onsets = pd.to_numeric(self.dataframe['onset'], errors='coerce')\r\n+            return not onsets.is_monotonic_increasing\r\n+        else:\r\n+            return False\r\n+\r\n+    @property\r\n+    def name(self):\r\n+        \"\"\" Name of the data. \"\"\"\r\n+        return self._name\r\n+\r\n+    @property\r\n+    def has_column_names(self):\r\n+        \"\"\" True if dataframe has column names. \"\"\"\r\n+        return self._has_column_names\r\n+\r\n+    @property\r\n+    def loaded_workbook(self):\r\n+        \"\"\" The underlying loaded workbooks. \"\"\"\r\n+        return self._loaded_workbook\r\n+\r\n+    @property\r\n+    def worksheet_name(self):\r\n+        \"\"\" The worksheet name. \"\"\"\r\n+        return self._worksheet_name\r\n+\r\n+    def convert_to_form(self, hed_schema, tag_form):\r\n+        \"\"\" Convert all tags in underlying dataframe to the specified form.\r\n+\r\n+        Parameters:\r\n+            hed_schema (HedSchema): The schema to use to convert tags.\r\n+            tag_form(str): HedTag property to convert tags to.\r\n+                Most cases should use convert_to_short or convert_to_long below.\r\n+        \"\"\"\r\n+        from hed.models.df_util import convert_to_form\r\n+        convert_to_form(self._dataframe, hed_schema, tag_form, self._mapper.get_tag_columns())\r\n+\r\n+    def convert_to_short(self, hed_schema):\r\n+        \"\"\" Convert all tags in underlying dataframe to short form.\r\n+\r\n+        Parameters:\r\n+            hed_schema (HedSchema): The schema to use to convert tags.\r\n+        \"\"\"\r\n+        return self.convert_to_form(hed_schema, \"short_tag\")\r\n+\r\n+    def convert_to_long(self, hed_schema):\r\n+        \"\"\" Convert all tags in underlying dataframe to long form.\r\n+\r\n+        Parameters:\r\n+            hed_schema (HedSchema or None): The schema to use to convert tags.\r\n+        \"\"\"\r\n+        return self.convert_to_form(hed_schema, \"long_tag\")\r\n+\r\n+    def shrink_defs(self, hed_schema):\r\n+        \"\"\" Shrinks any def-expand found in the underlying dataframe.\r\n+\r\n+        Parameters:\r\n+            hed_schema (HedSchema or None): The schema to use to identify defs.\r\n+        \"\"\"\r\n+        from df_util import shrink_defs\r\n+        shrink_defs(self._dataframe, hed_schema=hed_schema, columns=self._mapper.get_tag_columns())\r\n+\r\n+    def expand_defs(self, hed_schema, def_dict):\r\n+        \"\"\" Shrinks any def-expand found in the underlying dataframe.\r\n+\r\n+        Parameters:\r\n+            hed_schema (HedSchema or None): The schema to use to identify defs.\r\n+            def_dict (DefinitionDict): The definitions to expand.\r\n+        \"\"\"\r\n+        from df_util import expand_defs\r\n+        expand_defs(self._dataframe, hed_schema=hed_schema, def_dict=def_dict, columns=self._mapper.get_tag_columns())\r\n+\r\n+    def to_excel(self, file):\r\n+        \"\"\" Output to an Excel file.\r\n+\r\n+        Parameters:\r\n+            file (str or file-like): Location to save this base input.\r\n+\r\n+        :raises ValueError:\r\n+            - If empty file object was passed.\r\n+\r\n+        :raises OSError:\r\n+            - Cannot open the indicated file.\r\n+        \"\"\"\r\n+        if not file:\r\n+            raise ValueError(\"Empty file name or object passed in to BaseInput.save.\")\r\n+\r\n+        dataframe = self._dataframe\r\n+        if self._loaded_workbook:\r\n+            old_worksheet = self.get_worksheet(self._worksheet_name)\r\n+            # Excel spreadsheets are 1 based, then add another 1 for column names if present\r\n+            adj_row_for_col_names = 1\r\n+            if self._has_column_names:\r\n+                adj_row_for_col_names += 1\r\n+            adj_for_one_based_cols = 1\r\n+            for row_number, text_file_row in dataframe.iterrows():\r\n+                for column_number, column_text in enumerate(text_file_row):\r\n+                    cell_value = dataframe.iloc[row_number, column_number]\r\n+                    old_worksheet.cell(row_number + adj_row_for_col_names,\r\n+                                       column_number + adj_for_one_based_cols).value = cell_value\r\n+\r\n+            self._loaded_workbook.save(file)\r\n+        else:\r\n+            dataframe.to_excel(file, header=self._has_column_names)\r\n+\r\n+    def to_csv(self, file=None):\r\n+        \"\"\" Write to file or return as a string.\r\n+\r\n+        Parameters:\r\n+            file (str, file-like, or None): Location to save this file. If None, return as string.\r\n+        Returns:\r\n+            None or str:  None if file is given or the contents as a str if file is None.\r\n+\r\n+        :raises OSError:\r\n+            - Cannot open the indicated file.\r\n+        \"\"\"\r\n+        dataframe = self._dataframe\r\n+        csv_string_if_filename_none = dataframe.to_csv(file, sep='\\t', index=False, header=self._has_column_names)\r\n+        return csv_string_if_filename_none\r\n+\r\n+    @property\r\n+    def columns(self):\r\n+        \"\"\" Returns a list of the column names.\r\n+\r\n+            Empty if no column names.\r\n+\r\n+        Returns:\r\n+            columns(list): The column names.\r\n+        \"\"\"\r\n+        columns = []\r\n+        if self._dataframe is not None and self._has_column_names:\r\n+            columns = list(self._dataframe.columns)\r\n+        return columns\r\n+\r\n+    def column_metadata(self):\r\n+        \"\"\" Return the metadata for each column.\r\n+\r\n+        Returns:\r\n+            dict: Number/ColumnMeta pairs.\r\n+        \"\"\"\r\n+        if self._mapper:\r\n+            return self._mapper._final_column_map\r\n+        return {}\r\n+\r\n+    def set_cell(self, row_number, column_number, new_string_obj, tag_form=\"short_tag\"):\r\n+        \"\"\" Replace the specified cell with transformed text.\r\n+\r\n+        Parameters:\r\n+            row_number (int):    The row number of the spreadsheet to set.\r\n+            column_number (int): The column number of the spreadsheet to set.\r\n+            new_string_obj (HedString): Object with text to put in the given cell.\r\n+            tag_form (str): Version of the tags (short_tag, long_tag, base_tag, etc.)\r\n+\r\n+        Notes:\r\n+             Any attribute of a HedTag that returns a string is a valid value of tag_form.\r\n+\r\n+        :raises ValueError:\r\n+            - There is not a loaded dataframe.\r\n+\r\n+        :raises KeyError:\r\n+            - The indicated row/column does not exist.\r\n+\r\n+        :raises AttributeError:\r\n+            - The indicated tag_form is not an attribute of HedTag.\r\n+        \"\"\"\r\n+        if self._dataframe is None:\r\n+            raise ValueError(\"No data frame loaded\")\r\n+\r\n+        new_text = new_string_obj.get_as_form(tag_form)\r\n+        self._dataframe.iloc[row_number, column_number] = new_text\r\n+\r\n+    def get_worksheet(self, worksheet_name=None):\r\n+        \"\"\" Get the requested worksheet.\r\n+\r\n+        Parameters:\r\n+            worksheet_name (str or None): The name of the requested worksheet by name or the first one if None.\r\n+\r\n+        Returns:\r\n+            openpyxl.workbook.Workbook: The workbook request.\r\n+\r\n+        Notes:\r\n+            If None, returns the first worksheet.\r\n+\r\n+        :raises KeyError:\r\n+            - The specified worksheet name does not exist.\r\n+        \"\"\"\r\n+        if worksheet_name and self._loaded_workbook:\r\n+            # return self._loaded_workbook.get_sheet_by_name(worksheet_name)\r\n+            return self._loaded_workbook[worksheet_name]\r\n+        elif self._loaded_workbook:\r\n+            return self._loaded_workbook.worksheets[0]\r\n+        else:\r\n+            return None\r\n+\r\n+    @staticmethod\r\n+    def _get_dataframe_from_worksheet(worksheet, has_headers):\r\n+        \"\"\" Create a dataframe from the worksheet.\r\n+\r\n+        Parameters:\r\n+            worksheet (Worksheet): The loaded worksheet to convert.\r\n+            has_headers (bool): True if this worksheet has column headers.\r\n+\r\n+        Returns:\r\n+            DataFrame: The converted data frame.\r\n+\r\n+        \"\"\"\r\n+        if has_headers:\r\n+            data = worksheet.values\r\n+            # first row is columns\r\n+            cols = next(data)\r\n+            data = list(data)\r\n+            return pd.DataFrame(data, columns=cols, dtype=str)\r\n+        else:\r\n+            return pd.DataFrame(worksheet.values, dtype=str)\r\n+\r\n+    def validate(self, hed_schema, extra_def_dicts=None, name=None, error_handler=None):\r\n+        \"\"\"Creates a SpreadsheetValidator and returns all issues with this file.\r\n+\r\n+        Parameters:\r\n+            hed_schema(HedSchema): The schema to use for validation.\r\n+            extra_def_dicts(list of DefDict or DefDict): All definitions to use for validation.\r\n+            name(str): The name to report errors from this file as.\r\n+            error_handler (ErrorHandler): Error context to use.  Creates a new one if None.\r\n+\r\n+        Returns:\r\n+            issues (list of dict): A list of issues for a HED string.\r\n+        \"\"\"\r\n+        from hed.validator.spreadsheet_validator import SpreadsheetValidator\r\n+        if not name:\r\n+            name = self.name\r\n+        tab_validator = SpreadsheetValidator(hed_schema)\r\n+        validation_issues = tab_validator.validate(self, self._mapper.get_def_dict(hed_schema, extra_def_dicts), name,\r\n+                                                   error_handler=error_handler)\r\n+        return validation_issues\r\n+\r\n+    @staticmethod\r\n+    def _dataframe_has_names(dataframe):\r\n+        for column in dataframe.columns:\r\n+            if isinstance(column, str):\r\n+                return True\r\n+        return False\r\n+\r\n+    def assemble(self, mapper=None, skip_curly_braces=False):\r\n+        \"\"\" Assembles the HED strings.\r\n+\r\n+        Parameters:\r\n+            mapper(ColumnMapper or None): Generally pass none here unless you want special behavior.\r\n+            skip_curly_braces (bool): If True, don't plug in curly brace values into columns.\r\n+        Returns:\r\n+            Dataframe: The assembled dataframe.\r\n+        \"\"\"\r\n+        if mapper is None:\r\n+            mapper = self._mapper\r\n+\r\n+        all_columns = self._handle_transforms(mapper)\r\n+        if skip_curly_braces:\r\n+            return all_columns\r\n+        transformers, _ = mapper.get_transformers()\r\n+        refs = self.get_column_refs()\r\n+        column_names = list(transformers)\r\n+        return _handle_curly_braces_refs(all_columns, refs, column_names)\r\n+\r\n+    def _handle_transforms(self, mapper):\r\n+        transformers, need_categorical = mapper.get_transformers()\r\n+        if transformers:\r\n+            all_columns = self._dataframe\r\n+            if need_categorical:\r\n+                all_columns[need_categorical] = all_columns[need_categorical].astype('category')\r\n+\r\n+            all_columns = all_columns.transform(transformers)\r\n+\r\n+            if need_categorical:\r\n+                all_columns[need_categorical] = all_columns[need_categorical].astype('str')\r\n+        else:\r\n+            all_columns = self._dataframe\r\n+\r\n+        return all_columns\r\n+\r\n+    @staticmethod\r\n+    def combine_dataframe(dataframe):\r\n+        \"\"\" Combine all columns in the given dataframe into a single HED string series,\r\n+            skipping empty columns and columns with empty strings.\r\n+\r\n+        Parameters:\r\n+            dataframe(Dataframe): The dataframe to combine\r\n+\r\n+        Returns:\r\n+            Series: The assembled series.\r\n+        \"\"\"\r\n+        dataframe = dataframe.apply(\r\n+            lambda x: ', '.join(filter(lambda e: bool(e) and e != \"n/a\", map(str, x))),\r\n+            axis=1\r\n+        )\r\n+        return dataframe\r\n+\r\n+    def get_def_dict(self, hed_schema, extra_def_dicts=None):\r\n+        \"\"\" Return the definition dict for this file.\r\n+\r\n+        Note: Baseclass implementation returns just extra_def_dicts.\r\n+\r\n+        Parameters:\r\n+            hed_schema(HedSchema): Identifies tags to find definitions(if needed).\r\n+            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\r\n+\r\n+        Returns:\r\n+            DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\r\n+        \"\"\"\r\n+        from hed.models.definition_dict import DefinitionDict\r\n+        return DefinitionDict(extra_def_dicts, hed_schema)\r\n+\r\n+    def get_column_refs(self):\r\n+        \"\"\" Return a list of column refs for this file.\r\n+\r\n+            Default implementation returns none.\r\n+\r\n+        Returns:\r\n+            column_refs(list): A list of unique column refs found.\r\n+        \"\"\"\r\n+        return []\r\n+\r\n+    def _open_dataframe_file(self, file, has_column_names, input_type):\r\n+        pandas_header = 0\r\n+        if not has_column_names:\r\n+            pandas_header = None\r\n+\r\n+        if isinstance(file, pd.DataFrame):\r\n+            self._dataframe = file.astype(str)\r\n+            self._has_column_names = self._dataframe_has_names(self._dataframe)\r\n+        elif not file:\r\n+            raise HedFileError(HedExceptions.FILE_NOT_FOUND, \"Empty file passed to BaseInput.\", file)\r\n+        elif input_type in self.TEXT_EXTENSION:\r\n+            try:\r\n+                self._dataframe = pd.read_csv(file, delimiter='\\t', header=pandas_header,\r\n+                                              dtype=str, keep_default_na=True, na_values=(\"\", \"null\"))\r\n+            except Exception as e:\r\n+                raise HedFileError(HedExceptions.INVALID_FILE_FORMAT, str(e), self.name) from e\r\n+            # Convert nan values to a known value\r\n+            self._dataframe = self._dataframe.fillna(\"n/a\")\r\n+        elif input_type in self.EXCEL_EXTENSION:\r\n+            try:\r\n+                self._loaded_workbook = openpyxl.load_workbook(file)\r\n+                loaded_worksheet = self.get_worksheet(self._worksheet_name)\r\n+                self._dataframe = self._get_dataframe_from_worksheet(loaded_worksheet, has_column_names)\r\n+            except Exception as e:\r\n+                raise HedFileError(HedExceptions.GENERIC_ERROR, str(e), self.name) from e\r\n+        else:\r\n+            raise HedFileError(HedExceptions.INVALID_EXTENSION, \"\", file)\r\n+\r\n+        if self._dataframe.size == 0:\r\n+            raise HedFileError(HedExceptions.INVALID_DATAFRAME, \"Invalid dataframe(malformed datafile, etc)\", file)\r\ndiff --git a/hed/models/column_mapper.py b/hed/models/column_mapper.py\nindex d9fc51f8..203ee05f 100644\n--- a/hed/models/column_mapper.py\n+++ b/hed/models/column_mapper.py\n@@ -1,421 +1,422 @@\n-\"\"\"\n-Mapping of a base input file columns into HED tags.\n-\"\"\"\n-from hed.models.column_metadata import ColumnMetadata, ColumnType\n-from hed.errors.error_reporter import ErrorHandler\n-from hed.errors.error_types import ValidationErrors\n-from hed.models.definition_dict import DefinitionDict\n-\n-import copy\n-from collections import Counter\n-\n-PANDAS_COLUMN_PREFIX_TO_IGNORE = \"Unnamed: \"\n-\n-\n-class ColumnMapper:\n-    \"\"\" Mapping of a base input file columns into HED tags.\n-\n-    Notes:\n-        - All column numbers are 0 based.\n-    \"\"\"\n-\n-    def __init__(self, sidecar=None, tag_columns=None, column_prefix_dictionary=None,\n-                 optional_tag_columns=None, warn_on_missing_column=False):\n-        \"\"\" Constructor for ColumnMapper.\n-\n-        Parameters:\n-            sidecar (Sidecar): A sidecar to gather column data from.\n-            tag_columns: (list):  A list of ints or strings containing the columns that contain the HED tags.\n-                Sidecar column definitions will take precedent if there is a conflict with tag_columns.\n-            column_prefix_dictionary (dict): Dictionary with keys that are column numbers/names and values are HED tag\n-                prefixes to prepend to the tags in that column before processing.\n-            optional_tag_columns (list): A list of ints or strings containing the columns that contain\n-                the HED tags. If the column is otherwise unspecified, convert this column type to HEDTags.\n-            warn_on_missing_column (bool): If True, issue mapping warnings on column names that are missing from\n-                                            the sidecar.\n-\n-        Notes:\n-            - All column numbers are 0 based.\n-            - The column_prefix_dictionary may be deprecated/renamed in the future.\n-                - These are no longer prefixes, but rather converted to value columns:\n-                  {\"key\": \"Description\", 1: \"Label/\"} will turn into value columns as\n-                  {\"key\": \"Description/#\", 1: \"Label/#\"}\n-                  It will be a validation issue if column 1 is called \"key\" in the above example.\n-                  This means it no longer accepts anything but the value portion only in the columns.\n-\n-        \"\"\"\n-\n-        # Maps column number to column_entry.  This is what's actually used by most code.\n-        self._final_column_map = {}\n-        self._no_mapping_info = True\n-\n-        self._column_map = {}\n-        self._reverse_column_map = {}\n-        self._warn_on_missing_column = warn_on_missing_column\n-        if tag_columns is None:\n-            tag_columns = []\n-        self._tag_columns = tag_columns\n-        if optional_tag_columns is None:\n-            optional_tag_columns = []\n-        self._optional_tag_columns = optional_tag_columns\n-        if column_prefix_dictionary is None:\n-            column_prefix_dictionary = {}\n-        self._column_prefix_dictionary = column_prefix_dictionary\n-\n-        self._na_patterns = [\"n/a\", \"nan\"]\n-        self._sidecar = None\n-        self._set_sidecar(sidecar)\n-\n-        # finalize the column map based on initial settings with no header\n-        self._finalize_mapping()\n-\n-    @property\n-    def tag_columns(self):\n-        \"\"\" Return the known tag and optional tag columns with numbers as names when possible.\n-\n-            Returns:\n-                tag_columns(list of str or int): A list of all tag and optional tag columns as labels.\n-        \"\"\"\n-        joined_list = self._tag_columns + self._optional_tag_columns\n-        return list(set(self._convert_to_names(self._column_map, joined_list)))\n-\n-    @property\n-    def column_prefix_dictionary(self):\n-        \"\"\" Return the column_prefix_dictionary with numbers turned into names where possible.\n-\n-            Returns:\n-                column_prefix_dictionary(list of str or int): A column_prefix_dictionary with column labels as keys.\n-        \"\"\"\n-        return self._convert_to_names_dict(self._column_map, self._column_prefix_dictionary)\n-\n-    def get_transformers(self):\n-        \"\"\" Return the transformers to use on a dataframe.\n-\n-            Returns:\n-                tuple(dict, list):\n-                    dict({str or int: func}): The functions to use to transform each column.\n-                    need_categorical(list of int): A list of columns to treat as categorical.\n-        \"\"\"\n-        final_transformers = {}\n-        need_categorical = []\n-        for column in self._final_column_map.values():\n-            assign_to_column = column.column_name\n-            if isinstance(assign_to_column, int):\n-                if self._column_map:\n-                    assign_to_column = self._column_map[assign_to_column]\n-                else:\n-                    assign_to_column = assign_to_column\n-            if column.column_type == ColumnType.Ignore:\n-                continue\n-            elif column.column_type == ColumnType.Value:\n-                value_str = column.hed_dict\n-                from functools import partial\n-                final_transformers[assign_to_column] = partial(self._value_handler, value_str)\n-            elif column.column_type == ColumnType.Categorical:\n-                need_categorical.append(column.column_name)\n-                category_values = column.hed_dict\n-                from functools import partial\n-                final_transformers[assign_to_column] = partial(self._category_handler, category_values)\n-            else:\n-                final_transformers[assign_to_column] = lambda x: x\n-\n-        return final_transformers, need_categorical\n-\n-    @staticmethod\n-    def check_for_blank_names(column_map, allow_blank_names):\n-        \"\"\" Validate there are no blank column names.\n-\n-        Parameters:\n-            column_map(iterable): A list of column names.\n-            allow_blank_names(bool): Only find issues if True.\n-\n-        Returns:\n-            issues(list): A list of dicts, one per issue.\n-        \"\"\"\n-        # We don't have any checks right now if blank/duplicate is allowed\n-        if allow_blank_names:\n-            return []\n-\n-        issues = []\n-\n-        for column_number, name in enumerate(column_map):\n-            if name is None or not name or name.startswith(PANDAS_COLUMN_PREFIX_TO_IGNORE):\n-                issues += ErrorHandler.format_error(ValidationErrors.HED_BLANK_COLUMN, column_number)\n-                continue\n-\n-        return issues\n-\n-    def _set_sidecar(self, sidecar):\n-        \"\"\" Set the sidecar this column mapper uses.\n-\n-        Parameters:\n-            sidecar (Sidecar or None): The sidecar to use.\n-\n-        :raises ValueError:\n-            - A sidecar was previously set.\n-        \"\"\"\n-        if self._sidecar:\n-            raise ValueError(\"Trying to set a second sidecar on a column mapper.\")\n-        if not sidecar:\n-            return None\n-\n-        self._sidecar = sidecar\n-\n-    @property\n-    def sidecar_column_data(self):\n-        \"\"\" Pass through to get the sidecar ColumnMetadata.\n-\n-        Returns:\n-            dict({str:ColumnMetadata}): The column metadata defined by this sidecar.\n-        \"\"\"\n-        if self._sidecar:\n-            return self._sidecar.column_data\n-\n-        return {}\n-\n-    def get_tag_columns(self):\n-        \"\"\" Return the column numbers or names that are mapped to be HedTags.\n-\n-            Note: This is NOT the tag_columns or optional_tag_columns parameter, though they set it.\n-\n-        Returns:\n-            column_identifiers(list): A list of column numbers or names that are ColumnType.HedTags.\n-                0-based if integer-based, otherwise column name.\n-        \"\"\"\n-        return [column_entry.column_name for number, column_entry in self._final_column_map.items()\n-                if column_entry.column_type == ColumnType.HEDTags]\n-\n-    def set_tag_columns(self, tag_columns=None, optional_tag_columns=None, finalize_mapping=True):\n-        \"\"\" Set tag columns and optional tag columns.\n-\n-        Parameters:\n-            tag_columns (list): A list of ints or strings containing the columns that contain the HED tags.\n-                                If None, clears existing tag_columns\n-            optional_tag_columns (list): A list of ints or strings containing the columns that contain the HED tags,\n-                                         but not an error if missing.\n-                                         If None, clears existing tag_columns\n-            finalize_mapping (bool): Re-generate the internal mapping if True, otherwise no effect until finalize.\n-        \"\"\"\n-        if tag_columns is None:\n-            tag_columns = []\n-        if optional_tag_columns is None:\n-            optional_tag_columns = []\n-        self._tag_columns = tag_columns\n-        self._optional_tag_columns = optional_tag_columns\n-        if finalize_mapping:\n-            self._finalize_mapping()\n-\n-    def set_column_map(self, new_column_map=None):\n-        \"\"\" Set the column number to name mapping.\n-\n-        Parameters:\n-            new_column_map (list or dict):  Either an ordered list of the column names or column_number:column name.\n-                dictionary. In both cases, column numbers start at 0.\n-\n-        Returns:\n-            list: List of issues. Each issue is a dictionary.\n-\n-        \"\"\"\n-        if new_column_map is None:\n-            new_column_map = {}\n-        if isinstance(new_column_map, dict):\n-            column_map = new_column_map\n-        # List like\n-        else:\n-            column_map = {column_number: column_name for column_number, column_name in enumerate(new_column_map)}\n-        self._column_map = column_map\n-        self._reverse_column_map = {column_name: column_number for column_number, column_name in column_map.items()}\n-        self._finalize_mapping()\n-\n-    def set_column_prefix_dictionary(self, column_prefix_dictionary, finalize_mapping=True):\n-        \"\"\"Set the column prefix dictionary. \"\"\"\n-        self._column_prefix_dictionary = column_prefix_dictionary\n-        if finalize_mapping:\n-            self._finalize_mapping()\n-\n-    @staticmethod\n-    def _get_sidecar_basic_map(column_map, column_data):\n-        basic_final_map = {}\n-        unhandled_cols = []\n-        if column_map:\n-            for column_number, column_name in column_map.items():\n-                if column_name is None:\n-                    continue\n-                if column_name in column_data:\n-                    column_entry = copy.deepcopy(column_data[column_name])\n-                    column_entry.column_name = column_name\n-                    basic_final_map[column_name] = column_entry\n-                    continue\n-                elif isinstance(column_name, str) and column_name.startswith(PANDAS_COLUMN_PREFIX_TO_IGNORE):\n-                    continue\n-                unhandled_cols.append(column_name)\n-\n-        return basic_final_map, unhandled_cols\n-\n-    @staticmethod\n-    def _convert_to_names(column_to_name_map, column_list):\n-        converted_names = []\n-        for index in column_list:\n-            if isinstance(index, int):\n-                if not column_to_name_map:\n-                    converted_names.append(index)\n-                elif index in column_to_name_map:\n-                    converted_names.append(column_to_name_map[index])\n-            else:\n-                if index in column_to_name_map.values():\n-                    converted_names.append(index)\n-        return converted_names\n-\n-    @staticmethod\n-    def _convert_to_names_dict(column_to_name_map, column_dict):\n-        converted_dict = {}\n-        for index, column_data in column_dict.items():\n-            if isinstance(index, int):\n-                if not column_to_name_map:\n-                    converted_dict[index] = column_data\n-                elif index in column_to_name_map:\n-                    converted_dict[column_to_name_map[index]] = column_data\n-            else:\n-                if index in column_to_name_map.values():\n-                    converted_dict[index] = column_data\n-        return converted_dict\n-\n-    @staticmethod\n-    def _add_value_columns(final_map, column_prefix_dictionary):\n-        for col, prefix in column_prefix_dictionary.items():\n-            if prefix.endswith(\"/\"):\n-                prefix = prefix + \"#\"\n-            else:\n-                prefix = prefix + \"/#\"\n-            new_def = ColumnMetadata(ColumnType.Value, col, source=prefix)\n-            final_map[col] = new_def\n-\n-    @staticmethod\n-    def _add_tag_columns(final_map, tag_columns):\n-        for col in tag_columns:\n-            new_def = ColumnMetadata(ColumnType.HEDTags, col)\n-            final_map[col] = new_def\n-\n-    def _get_column_lists(self):\n-        column_lists = self._tag_columns, self._optional_tag_columns, self._column_prefix_dictionary\n-        list_names = [\"tag_columns\", \"optional_tag_columns\", \"column_prefix_dictionary\"]\n-\n-        if not any(column for column in column_lists):\n-            return column_lists, list_names\n-        # Filter out empty lists from the above\n-        column_lists, list_names = zip(*[(col_list, list_name) for col_list, list_name in zip(column_lists, list_names)\n-                                         if col_list])\n-\n-        return column_lists, list_names\n-\n-    def _check_for_duplicates_and_required(self, list_names, column_lists):\n-        issues = []\n-        for list_name, col_list in zip(list_names, column_lists):\n-            # Convert all known strings to ints, then check for duplicates\n-            converted_list = [item if isinstance(item, int) else self._reverse_column_map.get(item, item)\n-                              for item in col_list]\n-\n-            if col_list != self._optional_tag_columns:\n-                for test_col in converted_list:\n-                    if isinstance(test_col, str) and test_col not in self._reverse_column_map:\n-                        issues += ErrorHandler.format_error(ValidationErrors.HED_MISSING_REQUIRED_COLUMN,\n-                                                            test_col, list_name)\n-\n-            issues += self._check_for_duplicates_between_lists(converted_list, list_name,\n-                                                               ValidationErrors.DUPLICATE_COLUMN_IN_LIST)\n-\n-        return issues\n-\n-    def _check_for_duplicates_between_lists(self, checking_list, list_names, error_type):\n-        issues = []\n-        duplicates = [item for item, count in Counter(checking_list).items() if count > 1]\n-        for duplicate in duplicates:\n-            issues += ErrorHandler.format_error(error_type, duplicate,\n-                                                self._column_map.get(duplicate), list_names)\n-        return issues\n-\n-    def check_for_mapping_issues(self, allow_blank_names=False):\n-        \"\"\" Find all issues given the current column_map, tag_columns, etc.\n-\n-        Parameters:\n-            allow_blank_names(bool): Only flag blank names if False.\n-\n-        Returns:\n-            issue_list(list of dict): All issues found as a list of dicts.\n-        \"\"\"\n-        # 1. Get the lists with entries\n-        column_lists, list_names = self._get_column_lists()\n-        # 2. Verify column_prefix columns and tag columns are present, and check for duplicates\n-        issues = self._check_for_duplicates_and_required(list_names, column_lists)\n-\n-        combined_list = self.tag_columns + list(self.column_prefix_dictionary)\n-        # 3. Verify prefix and tag columns do not conflict.\n-        issues += self._check_for_duplicates_between_lists(combined_list, list_names,\n-                                                           ValidationErrors.DUPLICATE_COLUMN_BETWEEN_SOURCES)\n-\n-        # 4. Verify we didn't get both a sidecar and a tag column list\n-        if self._sidecar and combined_list and combined_list != [\"HED\"]:\n-            issues += ErrorHandler.format_error(ValidationErrors.SIDECAR_AND_OTHER_COLUMNS, column_names=combined_list)\n-\n-        # 5. Verify we handled all columns\n-        if self._warn_on_missing_column:\n-            fully_combined_list = list(self.sidecar_column_data) + combined_list\n-            for column in self._column_map.values():\n-                if column not in fully_combined_list:\n-                    issues += ErrorHandler.format_error(ValidationErrors.HED_UNKNOWN_COLUMN, column)\n-\n-        issues += self.check_for_blank_names(self._column_map.values(), allow_blank_names=allow_blank_names)\n-        return issues\n-\n-    def _finalize_mapping(self):\n-        final_map, unhandled_cols = self._get_sidecar_basic_map(self._column_map, self.sidecar_column_data)\n-\n-        self._add_tag_columns(final_map, self.tag_columns)\n-        self._remove_from_list(unhandled_cols, self.tag_columns)\n-\n-        self._add_value_columns(final_map, self.column_prefix_dictionary)\n-        self._remove_from_list(unhandled_cols, self.column_prefix_dictionary)\n-\n-        self._final_column_map = dict(sorted(final_map.items()))\n-\n-    @staticmethod\n-    def _remove_from_list(list_to_alter, to_remove):\n-        return [item for item in list_to_alter if item not in to_remove]\n-\n-    def get_def_dict(self, hed_schema, extra_def_dicts=None):\n-        \"\"\" Return def dicts from every column description.\n-\n-        Parameters:\n-            hed_schema (Schema): A HED schema object to use for extracting definitions.\n-            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\n-\n-        Returns:\n-           DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\n-        \"\"\"\n-        if self._sidecar:\n-            return self._sidecar.get_def_dict(hed_schema=hed_schema, extra_def_dicts=extra_def_dicts)\n-\n-        return DefinitionDict(extra_def_dicts, hed_schema=hed_schema)\n-\n-    def get_column_mapping_issues(self):\n-        \"\"\" Get all the issues with finalizing column mapping(duplicate columns, missing required, etc.).\n-\n-        Notes:\n-            - This is deprecated and now a wrapper for \"check_for_mapping_issues()\".\n-\n-        Returns:\n-            list: A list dictionaries of all issues found from mapping column names to numbers.\n-\n-        \"\"\"\n-        return self.check_for_mapping_issues()\n-\n-    @staticmethod\n-    def _category_handler(category_values, x):\n-        return category_values.get(x, \"\")\n-\n-    @staticmethod\n-    def _value_handler(value_str, x):\n-        if x == \"n/a\":\n-            return \"n/a\"\n-\n-        return value_str.replace(\"#\", str(x))\n+\"\"\"\r\n+Mapping of a base input file columns into HED tags.\r\n+\"\"\"\r\n+from hed.models.column_metadata import ColumnMetadata, ColumnType\r\n+from hed.errors.error_reporter import ErrorHandler\r\n+from hed.errors.error_types import ValidationErrors\r\n+from hed.models.definition_dict import DefinitionDict\r\n+\r\n+import copy\r\n+from collections import Counter\r\n+\r\n+PANDAS_COLUMN_PREFIX_TO_IGNORE = \"Unnamed: \"\r\n+NO_WARN_COLUMNS = ['onset', 'duration']\r\n+\r\n+\r\n+class ColumnMapper:\r\n+    \"\"\" Mapping of a base input file columns into HED tags.\r\n+\r\n+    Notes:\r\n+        - All column numbers are 0 based.\r\n+    \"\"\"\r\n+\r\n+    def __init__(self, sidecar=None, tag_columns=None, column_prefix_dictionary=None,\r\n+                 optional_tag_columns=None, warn_on_missing_column=False):\r\n+        \"\"\" Constructor for ColumnMapper.\r\n+\r\n+        Parameters:\r\n+            sidecar (Sidecar): A sidecar to gather column data from.\r\n+            tag_columns: (list):  A list of ints or strings containing the columns that contain the HED tags.\r\n+                Sidecar column definitions will take precedent if there is a conflict with tag_columns.\r\n+            column_prefix_dictionary (dict): Dictionary with keys that are column numbers/names and values are HED tag\r\n+                prefixes to prepend to the tags in that column before processing.\r\n+            optional_tag_columns (list): A list of ints or strings containing the columns that contain\r\n+                the HED tags. If the column is otherwise unspecified, convert this column type to HEDTags.\r\n+            warn_on_missing_column (bool): If True, issue mapping warnings on column names that are missing from\r\n+                                            the sidecar.\r\n+\r\n+        Notes:\r\n+            - All column numbers are 0 based.\r\n+            - The column_prefix_dictionary may be deprecated/renamed in the future.\r\n+                - These are no longer prefixes, but rather converted to value columns:\r\n+                  {\"key\": \"Description\", 1: \"Label/\"} will turn into value columns as\r\n+                  {\"key\": \"Description/#\", 1: \"Label/#\"}\r\n+                  It will be a validation issue if column 1 is called \"key\" in the above example.\r\n+                  This means it no longer accepts anything but the value portion only in the columns.\r\n+\r\n+        \"\"\"\r\n+\r\n+        # Maps column number to column_entry.  This is what's actually used by most code.\r\n+        self._final_column_map = {}\r\n+        self._no_mapping_info = True\r\n+\r\n+        self._column_map = {}\r\n+        self._reverse_column_map = {}\r\n+        self._warn_on_missing_column = warn_on_missing_column\r\n+        if tag_columns is None:\r\n+            tag_columns = []\r\n+        self._tag_columns = tag_columns\r\n+        if optional_tag_columns is None:\r\n+            optional_tag_columns = []\r\n+        self._optional_tag_columns = optional_tag_columns\r\n+        if column_prefix_dictionary is None:\r\n+            column_prefix_dictionary = {}\r\n+        self._column_prefix_dictionary = column_prefix_dictionary\r\n+\r\n+        self._na_patterns = [\"n/a\", \"nan\"]\r\n+        self._sidecar = None\r\n+        self._set_sidecar(sidecar)\r\n+\r\n+        # finalize the column map based on initial settings with no header\r\n+        self._finalize_mapping()\r\n+\r\n+    @property\r\n+    def tag_columns(self):\r\n+        \"\"\" Return the known tag and optional tag columns with numbers as names when possible.\r\n+\r\n+            Returns:\r\n+                tag_columns(list of str or int): A list of all tag and optional tag columns as labels.\r\n+        \"\"\"\r\n+        joined_list = self._tag_columns + self._optional_tag_columns\r\n+        return list(set(self._convert_to_names(self._column_map, joined_list)))\r\n+\r\n+    @property\r\n+    def column_prefix_dictionary(self):\r\n+        \"\"\" Return the column_prefix_dictionary with numbers turned into names where possible.\r\n+\r\n+            Returns:\r\n+                column_prefix_dictionary(list of str or int): A column_prefix_dictionary with column labels as keys.\r\n+        \"\"\"\r\n+        return self._convert_to_names_dict(self._column_map, self._column_prefix_dictionary)\r\n+\r\n+    def get_transformers(self):\r\n+        \"\"\" Return the transformers to use on a dataframe.\r\n+\r\n+            Returns:\r\n+                tuple(dict, list):\r\n+                    dict({str or int: func}): The functions to use to transform each column.\r\n+                    need_categorical(list of int): A list of columns to treat as categorical.\r\n+        \"\"\"\r\n+        final_transformers = {}\r\n+        need_categorical = []\r\n+        for column in self._final_column_map.values():\r\n+            assign_to_column = column.column_name\r\n+            if isinstance(assign_to_column, int):\r\n+                if self._column_map:\r\n+                    assign_to_column = self._column_map[assign_to_column]\r\n+                else:\r\n+                    assign_to_column = assign_to_column\r\n+            if column.column_type == ColumnType.Ignore:\r\n+                continue\r\n+            elif column.column_type == ColumnType.Value:\r\n+                value_str = column.hed_dict\r\n+                from functools import partial\r\n+                final_transformers[assign_to_column] = partial(self._value_handler, value_str)\r\n+            elif column.column_type == ColumnType.Categorical:\r\n+                need_categorical.append(column.column_name)\r\n+                category_values = column.hed_dict\r\n+                from functools import partial\r\n+                final_transformers[assign_to_column] = partial(self._category_handler, category_values)\r\n+            else:\r\n+                final_transformers[assign_to_column] = lambda x: x\r\n+\r\n+        return final_transformers, need_categorical\r\n+\r\n+    @staticmethod\r\n+    def check_for_blank_names(column_map, allow_blank_names):\r\n+        \"\"\" Validate there are no blank column names.\r\n+\r\n+        Parameters:\r\n+            column_map(iterable): A list of column names.\r\n+            allow_blank_names(bool): Only find issues if True.\r\n+\r\n+        Returns:\r\n+            issues(list): A list of dicts, one per issue.\r\n+        \"\"\"\r\n+        # We don't have any checks right now if blank/duplicate is allowed\r\n+        if allow_blank_names:\r\n+            return []\r\n+\r\n+        issues = []\r\n+\r\n+        for column_number, name in enumerate(column_map):\r\n+            if name is None or not name or name.startswith(PANDAS_COLUMN_PREFIX_TO_IGNORE):\r\n+                issues += ErrorHandler.format_error(ValidationErrors.HED_BLANK_COLUMN, column_number)\r\n+                continue\r\n+\r\n+        return issues\r\n+\r\n+    def _set_sidecar(self, sidecar):\r\n+        \"\"\" Set the sidecar this column mapper uses.\r\n+\r\n+        Parameters:\r\n+            sidecar (Sidecar or None): The sidecar to use.\r\n+\r\n+        :raises ValueError:\r\n+            - A sidecar was previously set.\r\n+        \"\"\"\r\n+        if self._sidecar:\r\n+            raise ValueError(\"Trying to set a second sidecar on a column mapper.\")\r\n+        if not sidecar:\r\n+            return None\r\n+\r\n+        self._sidecar = sidecar\r\n+\r\n+    @property\r\n+    def sidecar_column_data(self):\r\n+        \"\"\" Pass through to get the sidecar ColumnMetadata.\r\n+\r\n+        Returns:\r\n+            dict({str:ColumnMetadata}): The column metadata defined by this sidecar.\r\n+        \"\"\"\r\n+        if self._sidecar:\r\n+            return self._sidecar.column_data\r\n+\r\n+        return {}\r\n+\r\n+    def get_tag_columns(self):\r\n+        \"\"\" Return the column numbers or names that are mapped to be HedTags.\r\n+\r\n+            Note: This is NOT the tag_columns or optional_tag_columns parameter, though they set it.\r\n+\r\n+        Returns:\r\n+            column_identifiers(list): A list of column numbers or names that are ColumnType.HedTags.\r\n+                0-based if integer-based, otherwise column name.\r\n+        \"\"\"\r\n+        return [column_entry.column_name for number, column_entry in self._final_column_map.items()\r\n+                if column_entry.column_type == ColumnType.HEDTags]\r\n+\r\n+    def set_tag_columns(self, tag_columns=None, optional_tag_columns=None, finalize_mapping=True):\r\n+        \"\"\" Set tag columns and optional tag columns.\r\n+\r\n+        Parameters:\r\n+            tag_columns (list): A list of ints or strings containing the columns that contain the HED tags.\r\n+                                If None, clears existing tag_columns\r\n+            optional_tag_columns (list): A list of ints or strings containing the columns that contain the HED tags,\r\n+                                         but not an error if missing.\r\n+                                         If None, clears existing tag_columns\r\n+            finalize_mapping (bool): Re-generate the internal mapping if True, otherwise no effect until finalize.\r\n+        \"\"\"\r\n+        if tag_columns is None:\r\n+            tag_columns = []\r\n+        if optional_tag_columns is None:\r\n+            optional_tag_columns = []\r\n+        self._tag_columns = tag_columns\r\n+        self._optional_tag_columns = optional_tag_columns\r\n+        if finalize_mapping:\r\n+            self._finalize_mapping()\r\n+\r\n+    def set_column_map(self, new_column_map=None):\r\n+        \"\"\" Set the column number to name mapping.\r\n+\r\n+        Parameters:\r\n+            new_column_map (list or dict):  Either an ordered list of the column names or column_number:column name.\r\n+                dictionary. In both cases, column numbers start at 0.\r\n+\r\n+        Returns:\r\n+            list: List of issues. Each issue is a dictionary.\r\n+\r\n+        \"\"\"\r\n+        if new_column_map is None:\r\n+            new_column_map = {}\r\n+        if isinstance(new_column_map, dict):\r\n+            column_map = new_column_map\r\n+        # List like\r\n+        else:\r\n+            column_map = {column_number: column_name for column_number, column_name in enumerate(new_column_map)}\r\n+        self._column_map = column_map\r\n+        self._reverse_column_map = {column_name: column_number for column_number, column_name in column_map.items()}\r\n+        self._finalize_mapping()\r\n+\r\n+    def set_column_prefix_dictionary(self, column_prefix_dictionary, finalize_mapping=True):\r\n+        \"\"\"Set the column prefix dictionary. \"\"\"\r\n+        self._column_prefix_dictionary = column_prefix_dictionary\r\n+        if finalize_mapping:\r\n+            self._finalize_mapping()\r\n+\r\n+    @staticmethod\r\n+    def _get_sidecar_basic_map(column_map, column_data):\r\n+        basic_final_map = {}\r\n+        unhandled_cols = []\r\n+        if column_map:\r\n+            for column_number, column_name in column_map.items():\r\n+                if column_name is None:\r\n+                    continue\r\n+                if column_name in column_data:\r\n+                    column_entry = copy.deepcopy(column_data[column_name])\r\n+                    column_entry.column_name = column_name\r\n+                    basic_final_map[column_name] = column_entry\r\n+                    continue\r\n+                elif isinstance(column_name, str) and column_name.startswith(PANDAS_COLUMN_PREFIX_TO_IGNORE):\r\n+                    continue\r\n+                unhandled_cols.append(column_name)\r\n+\r\n+        return basic_final_map, unhandled_cols\r\n+\r\n+    @staticmethod\r\n+    def _convert_to_names(column_to_name_map, column_list):\r\n+        converted_names = []\r\n+        for index in column_list:\r\n+            if isinstance(index, int):\r\n+                if not column_to_name_map:\r\n+                    converted_names.append(index)\r\n+                elif index in column_to_name_map:\r\n+                    converted_names.append(column_to_name_map[index])\r\n+            else:\r\n+                if index in column_to_name_map.values():\r\n+                    converted_names.append(index)\r\n+        return converted_names\r\n+\r\n+    @staticmethod\r\n+    def _convert_to_names_dict(column_to_name_map, column_dict):\r\n+        converted_dict = {}\r\n+        for index, column_data in column_dict.items():\r\n+            if isinstance(index, int):\r\n+                if not column_to_name_map:\r\n+                    converted_dict[index] = column_data\r\n+                elif index in column_to_name_map:\r\n+                    converted_dict[column_to_name_map[index]] = column_data\r\n+            else:\r\n+                if index in column_to_name_map.values():\r\n+                    converted_dict[index] = column_data\r\n+        return converted_dict\r\n+\r\n+    @staticmethod\r\n+    def _add_value_columns(final_map, column_prefix_dictionary):\r\n+        for col, prefix in column_prefix_dictionary.items():\r\n+            if prefix.endswith(\"/\"):\r\n+                prefix = prefix + \"#\"\r\n+            else:\r\n+                prefix = prefix + \"/#\"\r\n+            new_def = ColumnMetadata(ColumnType.Value, col, source=prefix)\r\n+            final_map[col] = new_def\r\n+\r\n+    @staticmethod\r\n+    def _add_tag_columns(final_map, tag_columns):\r\n+        for col in tag_columns:\r\n+            new_def = ColumnMetadata(ColumnType.HEDTags, col)\r\n+            final_map[col] = new_def\r\n+\r\n+    def _get_column_lists(self):\r\n+        column_lists = self._tag_columns, self._optional_tag_columns, self._column_prefix_dictionary\r\n+        list_names = [\"tag_columns\", \"optional_tag_columns\", \"column_prefix_dictionary\"]\r\n+\r\n+        if not any(column for column in column_lists):\r\n+            return column_lists, list_names\r\n+        # Filter out empty lists from the above\r\n+        column_lists, list_names = zip(*[(col_list, list_name) for col_list, list_name in zip(column_lists, list_names)\r\n+                                         if col_list])\r\n+\r\n+        return column_lists, list_names\r\n+\r\n+    def _check_for_duplicates_and_required(self, list_names, column_lists):\r\n+        issues = []\r\n+        for list_name, col_list in zip(list_names, column_lists):\r\n+            # Convert all known strings to ints, then check for duplicates\r\n+            converted_list = [item if isinstance(item, int) else self._reverse_column_map.get(item, item)\r\n+                              for item in col_list]\r\n+\r\n+            if col_list != self._optional_tag_columns:\r\n+                for test_col in converted_list:\r\n+                    if isinstance(test_col, str) and test_col not in self._reverse_column_map:\r\n+                        issues += ErrorHandler.format_error(ValidationErrors.HED_MISSING_REQUIRED_COLUMN,\r\n+                                                            test_col, list_name)\r\n+\r\n+            issues += self._check_for_duplicates_between_lists(converted_list, list_name,\r\n+                                                               ValidationErrors.DUPLICATE_COLUMN_IN_LIST)\r\n+\r\n+        return issues\r\n+\r\n+    def _check_for_duplicates_between_lists(self, checking_list, list_names, error_type):\r\n+        issues = []\r\n+        duplicates = [item for item, count in Counter(checking_list).items() if count > 1]\r\n+        for duplicate in duplicates:\r\n+            issues += ErrorHandler.format_error(error_type, duplicate,\r\n+                                                self._column_map.get(duplicate), list_names)\r\n+        return issues\r\n+\r\n+    def check_for_mapping_issues(self, allow_blank_names=False):\r\n+        \"\"\" Find all issues given the current column_map, tag_columns, etc.\r\n+\r\n+        Parameters:\r\n+            allow_blank_names(bool): Only flag blank names if False.\r\n+\r\n+        Returns:\r\n+            issue_list(list of dict): All issues found as a list of dicts.\r\n+        \"\"\"\r\n+        # 1. Get the lists with entries\r\n+        column_lists, list_names = self._get_column_lists()\r\n+        # 2. Verify column_prefix columns and tag columns are present, and check for duplicates\r\n+        issues = self._check_for_duplicates_and_required(list_names, column_lists)\r\n+\r\n+        combined_list = self.tag_columns + list(self.column_prefix_dictionary)\r\n+        # 3. Verify prefix and tag columns do not conflict.\r\n+        issues += self._check_for_duplicates_between_lists(combined_list, list_names,\r\n+                                                           ValidationErrors.DUPLICATE_COLUMN_BETWEEN_SOURCES)\r\n+\r\n+        # 4. Verify we didn't get both a sidecar and a tag column list\r\n+        if self._sidecar and combined_list and combined_list != [\"HED\"]:\r\n+            issues += ErrorHandler.format_error(ValidationErrors.SIDECAR_AND_OTHER_COLUMNS, column_names=combined_list)\r\n+\r\n+        # 5. Verify we handled all columns\r\n+        if self._warn_on_missing_column:\r\n+            fully_combined_list = list(self.sidecar_column_data) + combined_list + NO_WARN_COLUMNS\r\n+            for column in self._column_map.values():\r\n+                if column not in fully_combined_list:\r\n+                    issues += ErrorHandler.format_error(ValidationErrors.HED_UNKNOWN_COLUMN, column)\r\n+\r\n+        issues += self.check_for_blank_names(self._column_map.values(), allow_blank_names=allow_blank_names)\r\n+        return issues\r\n+\r\n+    def _finalize_mapping(self):\r\n+        final_map, unhandled_cols = self._get_sidecar_basic_map(self._column_map, self.sidecar_column_data)\r\n+\r\n+        self._add_tag_columns(final_map, self.tag_columns)\r\n+        self._remove_from_list(unhandled_cols, self.tag_columns)\r\n+\r\n+        self._add_value_columns(final_map, self.column_prefix_dictionary)\r\n+        self._remove_from_list(unhandled_cols, self.column_prefix_dictionary)\r\n+\r\n+        self._final_column_map = dict(sorted(final_map.items()))\r\n+\r\n+    @staticmethod\r\n+    def _remove_from_list(list_to_alter, to_remove):\r\n+        return [item for item in list_to_alter if item not in to_remove]\r\n+\r\n+    def get_def_dict(self, hed_schema, extra_def_dicts=None):\r\n+        \"\"\" Return def dicts from every column description.\r\n+\r\n+        Parameters:\r\n+            hed_schema (Schema): A HED schema object to use for extracting definitions.\r\n+            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\r\n+\r\n+        Returns:\r\n+           DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\r\n+        \"\"\"\r\n+        if self._sidecar:\r\n+            return self._sidecar.get_def_dict(hed_schema=hed_schema, extra_def_dicts=extra_def_dicts)\r\n+\r\n+        return DefinitionDict(extra_def_dicts, hed_schema=hed_schema)\r\n+\r\n+    def get_column_mapping_issues(self):\r\n+        \"\"\" Get all the issues with finalizing column mapping(duplicate columns, missing required, etc.).\r\n+\r\n+        Notes:\r\n+            - This is deprecated and now a wrapper for \"check_for_mapping_issues()\".\r\n+\r\n+        Returns:\r\n+            list: A list dictionaries of all issues found from mapping column names to numbers.\r\n+\r\n+        \"\"\"\r\n+        return self.check_for_mapping_issues()\r\n+\r\n+    @staticmethod\r\n+    def _category_handler(category_values, x):\r\n+        return category_values.get(x, \"\")\r\n+\r\n+    @staticmethod\r\n+    def _value_handler(value_str, x):\r\n+        if x == \"n/a\":\r\n+            return \"n/a\"\r\n+\r\n+        return value_str.replace(\"#\", str(x))\r\ndiff --git a/hed/models/df_util.py b/hed/models/df_util.py\nindex 39aa979b..c7caf16a 100644\n--- a/hed/models/df_util.py\n+++ b/hed/models/df_util.py\n@@ -1,5 +1,7 @@\n \"\"\" Utilities for assembly and conversion of HED strings to different forms. \"\"\"\r\n import re\r\n+import math\r\n+from collections import defaultdict\r\n from functools import partial\r\n import pandas as pd\r\n from hed.models.hed_string import HedString\r\n@@ -115,7 +117,7 @@ def sort_dataframe_by_onsets(df):\n     if \"onset\" in df.columns:\r\n         # Create a copy and sort by onsets as floats(if needed), but continue to keep the string version.\r\n         df_copy = df.copy()\r\n-        df_copy['_temp_onset_sort'] = df_copy['onset'].astype(float)\r\n+        df_copy['_temp_onset_sort'] = pd.to_numeric(df_copy['onset'], errors='coerce')\r\n         df_copy.sort_values(by='_temp_onset_sort', inplace=True)\r\n         df_copy.drop(columns=['_temp_onset_sort'], inplace=True)\r\n \r\n@@ -215,7 +217,7 @@ def split_delay_tags(series, hed_schema, onsets):\n     Note: This dataframe may be longer than the original series, but it will never be shorter.\r\n     \"\"\"\r\n     if series is None or onsets is None:\r\n-        return\r\n+        return None\r\n     split_df = pd.DataFrame({\"onset\": onsets, \"HED\": series, \"original_index\": series.index})\r\n     delay_strings = [(i, HedString(hed_string, hed_schema)) for (i, hed_string) in series.items() if\r\n                      \"delay/\" in hed_string.casefold()]\r\n@@ -251,23 +253,42 @@ def filter_series_by_onset(series, onsets):\n     Returns:\r\n         Series or Dataframe: the series with rows filtered together.\r\n     \"\"\"\r\n-    indexed_dict = _indexed_dict_from_onsets(onsets.astype(float))\r\n-    return _filter_by_index_list(series, indexed_dict=indexed_dict)\r\n+    #indexed_dict = _indexed_dict_from_onsets(pd.to_numeric(onsets, errors='coerce'))\r\n+    #return _filter_by_index_list(series, indexed_dict=indexed_dict)\r\n+    indexed_dict = _indexed_dict_from_onsets(pd.to_numeric(onsets, errors='coerce'))\r\n+    y =  _filter_by_index_list(series, indexed_dict=indexed_dict)\r\n+    return y\r\n+    # return _filter_by_index_list(series, indexed_dict=indexed_dict)\r\n \r\n \r\n def _indexed_dict_from_onsets(onsets):\r\n-    \"\"\"Finds series of consecutive lines with the same(or close enough) onset\"\"\"\r\n+    \"\"\"Finds series of consecutive lines with the same (or close enough) onset.\"\"\"\r\n     current_onset = -1000000.0\r\n     tol = 1e-9\r\n-    from collections import defaultdict\r\n     indexed_dict = defaultdict(list)\r\n+\r\n     for i, onset in enumerate(onsets):\r\n+        if math.isnan(onset): # Ignore NaNs\r\n+            continue\r\n         if abs(onset - current_onset) > tol:\r\n             current_onset = onset\r\n         indexed_dict[current_onset].append(i)\r\n \r\n     return indexed_dict\r\n \r\n+# def _indexed_dict_from_onsets(onsets):\r\n+#     \"\"\"Finds series of consecutive lines with the same(or close enough) onset\"\"\"\r\n+#     current_onset = -1000000.0\r\n+#     tol = 1e-9\r\n+#     from collections import defaultdict\r\n+#     indexed_dict = defaultdict(list)\r\n+#     for i, onset in enumerate(onsets):\r\n+#         if abs(onset - current_onset) > tol:\r\n+#             current_onset = onset\r\n+#         indexed_dict[current_onset].append(i)\r\n+#\r\n+#     return indexed_dict\r\n+\r\n \r\n def _filter_by_index_list(original_data, indexed_dict):\r\n     \"\"\"Filters a series or dataframe by the indexed_dict, joining lines as indicated\"\"\"\r\ndiff --git a/hed/models/tabular_input.py b/hed/models/tabular_input.py\nindex 61bf90ed..3fea5195 100644\n--- a/hed/models/tabular_input.py\n+++ b/hed/models/tabular_input.py\n@@ -1,88 +1,88 @@\n-\"\"\" A BIDS tabular file with sidecar. \"\"\"\n-from hed.models.column_mapper import ColumnMapper\n-from hed.models.base_input import BaseInput\n-from hed.models.sidecar import Sidecar\n-\n-\n-class TabularInput(BaseInput):\n-    \"\"\" A BIDS tabular file with sidecar. \"\"\"\n-\n-    HED_COLUMN_NAME = \"HED\"\n-\n-    def __init__(self, file=None, sidecar=None, name=None):\n-\n-        \"\"\" Constructor for the TabularInput class.\n-\n-        Parameters:\n-            file (str or FileLike): A tsv file to open.\n-            sidecar (str or Sidecar or FileLike): A Sidecar or source file/filename.\n-            name (str): The name to display for this file for error purposes.\n-\n-        :raises HedFileError:\n-            - The file is blank.\n-            - An invalid dataframe was passed with size 0.\n-            - An invalid extension was provided.\n-            - A duplicate or empty column name appears.\n-\n-        :raises OSError:\n-            - Cannot open the indicated file.\n-\n-        :raises ValueError:\n-            - This file has no column names.\n-        \"\"\"\n-        if sidecar and not isinstance(sidecar, Sidecar):\n-            sidecar = Sidecar(sidecar)\n-        new_mapper = ColumnMapper(sidecar=sidecar, optional_tag_columns=[self.HED_COLUMN_NAME],\n-                                  warn_on_missing_column=True)\n-\n-        self._sidecar = sidecar\n-\n-        super().__init__(file, file_type=\".tsv\", worksheet_name=None, has_column_names=True, mapper=new_mapper,\n-                         name=name, allow_blank_names=False, )\n-\n-        if not self._has_column_names:\n-            raise ValueError(\"You are attempting to open a bids_old style file with no column headers provided.\\n\"\n-                             \"This is probably not intended.\")\n-\n-    def reset_column_mapper(self, sidecar=None):\n-        \"\"\" Change the sidecars and settings.\n-\n-        Parameters:\n-            sidecar (str or [str] or Sidecar or [Sidecar]): A list of json filenames to pull sidecar info from.\n-\n-        \"\"\"\n-        new_mapper = ColumnMapper(sidecar=sidecar, optional_tag_columns=[self.HED_COLUMN_NAME])\n-        self._sidecar = sidecar\n-\n-        self.reset_mapper(new_mapper)\n-\n-    def get_def_dict(self, hed_schema, extra_def_dicts=None):\n-        \"\"\" Return the definition dict for this sidecar.\n-\n-        Parameters:\n-            hed_schema(HedSchema): Used to identify tags to find definitions.\n-            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\n-\n-        Returns:\n-            DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\n-        \"\"\"\n-        if self._sidecar:\n-            return self._sidecar.get_def_dict(hed_schema, extra_def_dicts)\n-        else:\n-            return super().get_def_dict(hed_schema, extra_def_dicts)\n-\n-    def get_column_refs(self):\n-        \"\"\" Return a list of column refs for this file.\n-\n-            Default implementation returns none.\n-\n-        Returns:\n-            column_refs(list): A list of unique column refs found.\n-        \"\"\"\n-        if self._sidecar:\n-            return self._sidecar.get_column_refs()\n-        return []\n-\n-    def get_sidecar(self):\n-        \"\"\"Return the sidecar associated with this TabularInput.\"\"\"\n-        return self._sidecar\n+\"\"\" A BIDS tabular file with sidecar. \"\"\"\r\n+from hed.models.column_mapper import ColumnMapper\r\n+from hed.models.base_input import BaseInput\r\n+from hed.models.sidecar import Sidecar\r\n+\r\n+\r\n+class TabularInput(BaseInput):\r\n+    \"\"\" A BIDS tabular file with sidecar. \"\"\"\r\n+\r\n+    HED_COLUMN_NAME = \"HED\"\r\n+\r\n+    def __init__(self, file=None, sidecar=None, name=None):\r\n+\r\n+        \"\"\" Constructor for the TabularInput class.\r\n+\r\n+        Parameters:\r\n+            file (str or FileLike or pd.Dataframe): A tsv file to open.\r\n+            sidecar (str or Sidecar or FileLike): A Sidecar or source file/filename.\r\n+            name (str): The name to display for this file for error purposes.\r\n+\r\n+        :raises HedFileError:\r\n+            - The file is blank.\r\n+            - An invalid dataframe was passed with size 0.\r\n+            - An invalid extension was provided.\r\n+            - A duplicate or empty column name appears.\r\n+\r\n+        :raises OSError:\r\n+            - Cannot open the indicated file.\r\n+\r\n+        :raises ValueError:\r\n+            - This file has no column names.\r\n+        \"\"\"\r\n+        if sidecar and not isinstance(sidecar, Sidecar):\r\n+            sidecar = Sidecar(sidecar)\r\n+        new_mapper = ColumnMapper(sidecar=sidecar, optional_tag_columns=[self.HED_COLUMN_NAME],\r\n+                                  warn_on_missing_column=True)\r\n+\r\n+        self._sidecar = sidecar\r\n+\r\n+        super().__init__(file, file_type=\".tsv\", worksheet_name=None, has_column_names=True, mapper=new_mapper,\r\n+                         name=name, allow_blank_names=False, )\r\n+\r\n+        if not self._has_column_names:\r\n+            raise ValueError(\"You are attempting to open a bids_old style file with no column headers provided.\\n\"\r\n+                             \"This is probably not intended.\")\r\n+\r\n+    def reset_column_mapper(self, sidecar=None):\r\n+        \"\"\" Change the sidecars and settings.\r\n+\r\n+        Parameters:\r\n+            sidecar (str or [str] or Sidecar or [Sidecar]): A list of json filenames to pull sidecar info from.\r\n+\r\n+        \"\"\"\r\n+        new_mapper = ColumnMapper(sidecar=sidecar, optional_tag_columns=[self.HED_COLUMN_NAME])\r\n+        self._sidecar = sidecar\r\n+\r\n+        self.reset_mapper(new_mapper)\r\n+\r\n+    def get_def_dict(self, hed_schema, extra_def_dicts=None):\r\n+        \"\"\" Return the definition dict for this sidecar.\r\n+\r\n+        Parameters:\r\n+            hed_schema(HedSchema): Used to identify tags to find definitions.\r\n+            extra_def_dicts (list, DefinitionDict, or None): Extra dicts to add to the list.\r\n+\r\n+        Returns:\r\n+            DefinitionDict:   A single definition dict representing all the data(and extra def dicts).\r\n+        \"\"\"\r\n+        if self._sidecar:\r\n+            return self._sidecar.get_def_dict(hed_schema, extra_def_dicts)\r\n+        else:\r\n+            return super().get_def_dict(hed_schema, extra_def_dicts)\r\n+\r\n+    def get_column_refs(self):\r\n+        \"\"\" Return a list of column refs for this file.\r\n+\r\n+            Default implementation returns none.\r\n+\r\n+        Returns:\r\n+            column_refs(list): A list of unique column refs found.\r\n+        \"\"\"\r\n+        if self._sidecar:\r\n+            return self._sidecar.get_column_refs()\r\n+        return []\r\n+\r\n+    def get_sidecar(self):\r\n+        \"\"\"Return the sidecar associated with this TabularInput.\"\"\"\r\n+        return self._sidecar\r\ndiff --git a/hed/tools/analysis/event_manager.py b/hed/tools/analysis/event_manager.py\nindex d898b1f9..2cbad4bf 100644\n--- a/hed/tools/analysis/event_manager.py\n+++ b/hed/tools/analysis/event_manager.py\n@@ -29,13 +29,15 @@ def __init__(self, input_data, hed_schema, extra_defs=None):\n         are separated from the rest of the annotations, which are contained in self.hed_strings.\r\n \r\n         \"\"\"\r\n+        if input_data.onsets is not None and input_data.needs_sorting:\r\n+            raise HedFileError(\"OnsetsNotOrdered\", \"Events must have numeric non-decreasing onset values\", \"\")\r\n         self.hed_schema = hed_schema\r\n         self.input_data = input_data\r\n         self.def_dict = input_data.get_def_dict(hed_schema, extra_def_dicts=extra_defs)\r\n-        if self.input_data.needs_sorting:\r\n-            raise HedFileError(\"OnsetsNotOrdered\", \"The onset values must be non-decreasing\", \"\")\r\n-        self.onsets = None\r\n-        self.hed_strings = None\r\n+        self.onsets = None  # list of onset times or None if not an events file\r\n+        self.base = None  # list of strings containing the starts of event processes\r\n+        self.context = None  # list of strings containing the contexts of event processes\r\n+        self.hed_strings = None  # list of HedString objects without the temporal events\r\n         self.event_list = None\r\n         self._create_event_list(input_data)\r\n \r\n@@ -53,6 +55,9 @@ def _create_event_list(self, input_data):\n         \"\"\"\r\n         hed_strings = input_data.series_a\r\n         df_util.shrink_defs(hed_strings, self.hed_schema)\r\n+        if input_data.onsets is None:\r\n+            self.hed_strings = [HedString(hed_string, self.hed_schema) for hed_string in hed_strings]\r\n+            return\r\n         delay_df = df_util.split_delay_tags(hed_strings, self.hed_schema, input_data.onsets)\r\n \r\n         hed_strings = [HedString(hed_string, self.hed_schema) for hed_string in delay_df.HED]\r\n@@ -66,6 +71,7 @@ def _create_event_list(self, input_data):\n         for item in onset_dict.values():\r\n             item.set_end(len(self.onsets), None)\r\n         self.hed_strings = hed_strings\r\n+        self._extract_context()\r\n \r\n     def _extract_duration_events(self, hed, event_index):\r\n         groups = hed.find_top_level_tags(anchor_tags={DefTagNames.DURATION_KEY})\r\n@@ -120,31 +126,42 @@ def unfold_context(self, remove_types=[]):\n \r\n         Returns:\r\n             list of str or HedString representing the information without the events of temporal extent.\r\n-            list of str or HedString representing the onsets of the events of temporal extent.\r\n-            list of str or HedString representing the ongoing context information.\r\n+            list of str or HedString or None representing the onsets of the events of temporal extent.\r\n+            list of str or HedString or None representing the ongoing context information.\r\n \r\n+        If the\r\n         \"\"\"\r\n-        placeholder = \"\"\r\n+\r\n         remove_defs = self.get_type_defs(remove_types)  # definitions corresponding to remove types to be filtered out\r\n-        new_hed = [placeholder for _ in range(len(self.hed_strings))]\r\n-        new_base = [placeholder for _ in range(len(self.hed_strings))]\r\n-        new_contexts = [placeholder for _ in range(len(self.hed_strings))]\r\n-        base, contexts = self._expand_context()\r\n+        new_hed = [\"\" for _ in range(len(self.hed_strings))]\r\n         for index, item in enumerate(self.hed_strings):\r\n             new_hed[index] = self._filter_hed(item, remove_types=remove_types,\r\n                                               remove_defs=remove_defs, remove_group=False)\r\n-            new_base[index] = self._filter_hed(base[index], remove_types=remove_types,\r\n+        if self.onsets is None:\r\n+            return new_hed, None, None\r\n+        new_base, new_contexts = self._get_base_contexts(remove_types, remove_defs)\r\n+        return new_hed, new_base, new_contexts\r\n+\r\n+    def _get_base_contexts(self, remove_types, remove_defs):\r\n+        \"\"\" Expand the context and filter to remove specified types.\r\n+\r\n+        Parameters:\r\n+            remove_types (list):  List of types to remove.\r\n+            remove_defs (list):  List of definitions to remove.\r\n+\r\n+        \"\"\"\r\n+        new_base = [\"\" for _ in range(len(self.hed_strings))]\r\n+        new_contexts = [\"\" for _ in range(len(self.hed_strings))]\r\n+        for index, item in enumerate(self.hed_strings):\r\n+            new_base[index] = self._filter_hed(self.base[index], remove_types=remove_types,\r\n                                                remove_defs=remove_defs, remove_group=True)\r\n-            new_contexts[index] = self._filter_hed(contexts[index], remove_types=remove_types,\r\n+            new_contexts[index] = self._filter_hed(self.contexts[index], remove_types=remove_types,\r\n                                                    remove_defs=remove_defs, remove_group=True)\r\n-        return new_hed, new_base, new_contexts   # these are each a list of strings\r\n+        return new_base, new_contexts   # these are each a list of strings\r\n \r\n-    def _expand_context(self):\r\n+    def _extract_context(self):\r\n         \"\"\" Expand the onset and the ongoing context for additional processing.\r\n \r\n-        Returns:\r\n-            tuple of lists: (base list of str, context list of str).\r\n-\r\n         Notes: For each event, the Onset goes in the base list and the remainder of the times go in the contexts list.\r\n \r\n         \"\"\"\r\n@@ -156,8 +173,8 @@ def _expand_context(self):\n                 base[event.start_index].append(this_str)\r\n                 for i in range(event.start_index + 1, event.end_index):\r\n                     contexts[i].append(this_str)\r\n-\r\n-        return self.compress_strings(base), self.compress_strings(contexts)\r\n+        self.base = self.compress_strings(base)\r\n+        self.contexts = self.compress_strings(contexts)\r\n \r\n     def _filter_hed(self, hed, remove_types=[], remove_defs=[], remove_group=False):\r\n         \"\"\" Remove types and definitions from a HED string.\r\ndiff --git a/hed/tools/analysis/hed_tag_counts.py b/hed/tools/analysis/hed_tag_counts.py\nindex 24133f42..542abc25 100644\n--- a/hed/tools/analysis/hed_tag_counts.py\n+++ b/hed/tools/analysis/hed_tag_counts.py\n@@ -85,7 +85,7 @@ def __init__(self, name, total_events=0):\n         self.files = {}\r\n         self.total_events = total_events\r\n \r\n-    def update_event_counts(self, hed_string_obj, file_name):\r\n+    def update_tag_counts(self, hed_string_obj, file_name):\r\n         \"\"\" Update the tag counts based on a HedString object.\r\n \r\n         Parameters:\r\ndiff --git a/hed/tools/analysis/hed_tag_manager.py b/hed/tools/analysis/hed_tag_manager.py\nindex d8cd0529..d1553f3a 100644\n--- a/hed/tools/analysis/hed_tag_manager.py\n+++ b/hed/tools/analysis/hed_tag_manager.py\n@@ -2,12 +2,13 @@\n \r\n from hed.models.hed_string import HedString\r\n from hed.models import string_util\r\n+from hed.tools.analysis.event_manager import EventManager\r\n \r\n \r\n class HedTagManager:\r\n     \"\"\" Manager for the HED tags from a columnar file. \"\"\"\r\n \r\n-    def __init__(self, event_manager, remove_types=[]):\r\n+    def __init__(self, event_manager, remove_types=[], extra_defs=None):\r\n         \"\"\" Create a tag manager for one tabular file.\r\n \r\n         Parameters:\r\ndiff --git a/hed/tools/analysis/hed_type.py b/hed/tools/analysis/hed_type.py\nindex d6c64943..96e25dc3 100644\n--- a/hed/tools/analysis/hed_type.py\n+++ b/hed/tools/analysis/hed_type.py\n@@ -14,7 +14,7 @@ def __init__(self, event_manager, name, type_tag=\"condition-variable\"):\n         \"\"\" Create a variable manager for one type-variable for one tabular file.\r\n \r\n         Parameters:\r\n-            event_manager (EventManager): An event manager for the tabular file.\r\n+            event_manager (EventManager): Event manager instance\r\n             name (str): Name of the tabular file as a unique identifier.\r\n             type_tag (str): Lowercase short form of the tag to be managed.\r\n \r\n@@ -25,7 +25,7 @@ def __init__(self, event_manager, name, type_tag=\"condition-variable\"):\n         self.name = name\r\n         self.type_tag = type_tag.casefold()\r\n         self.event_manager = event_manager\r\n-        self.type_defs = HedTypeDefs(event_manager.def_dict, type_tag=type_tag)\r\n+        self.type_defs = HedTypeDefs(self.event_manager.def_dict, type_tag=type_tag)\r\n         self._type_map = {}  # Dictionary of type tags versus dictionary with keys being definition names.\r\n         self._extract_variables()\r\n \r\ndiff --git a/hed/tools/analysis/hed_type_manager.py b/hed/tools/analysis/hed_type_manager.py\nindex 402d45d1..6d510fd7 100644\n--- a/hed/tools/analysis/hed_type_manager.py\n+++ b/hed/tools/analysis/hed_type_manager.py\n@@ -3,6 +3,7 @@\n import pandas as pd\r\n import json\r\n from hed.tools.analysis.hed_type import HedType\r\n+from hed.tools.analysis.event_manager import EventManager\r\n \r\n \r\n class HedTypeManager:\r\ndiff --git a/hed/tools/remodeling/operations/factor_hed_tags_op.py b/hed/tools/remodeling/operations/factor_hed_tags_op.py\nindex 69a1464d..7222c2a4 100644\n--- a/hed/tools/remodeling/operations/factor_hed_tags_op.py\n+++ b/hed/tools/remodeling/operations/factor_hed_tags_op.py\n@@ -119,8 +119,7 @@ def do_op(self, dispatcher, df, name, sidecar=None):\n                 raise ValueError(\"QueryNameAlreadyColumn\",\r\n                                  f\"Query [{query_name}]: is already a column name of the data frame\")\r\n         df_list = [input_data.dataframe]\r\n-        tag_man = HedTagManager(EventManager(input_data, dispatcher.hed_schema),\r\n-                                remove_types=self.remove_types)\r\n+        tag_man = HedTagManager(EventManager(input_data, dispatcher.hed_schema), remove_types=self.remove_types)\r\n         hed_objs = tag_man.get_hed_objs(include_context=self.expand_context, replace_defs=self.replace_defs)\r\n         df_factors = query_service.search_hed_objs(hed_objs, self.query_handlers, query_names=self.query_names)\r\n         if len(df_factors.columns) > 0:\r\ndiff --git a/hed/tools/remodeling/operations/factor_hed_type_op.py b/hed/tools/remodeling/operations/factor_hed_type_op.py\nindex 3d6f523f..8a3246c5 100644\n--- a/hed/tools/remodeling/operations/factor_hed_type_op.py\n+++ b/hed/tools/remodeling/operations/factor_hed_type_op.py\n@@ -73,8 +73,7 @@ def do_op(self, dispatcher, df, name, sidecar=None):\n \r\n         input_data = TabularInput(df.copy().fillna('n/a'), sidecar=sidecar, name=name)\r\n         df_list = [input_data.dataframe]\r\n-        var_manager = HedTypeManager(\r\n-            EventManager(input_data, dispatcher.hed_schema))\r\n+        var_manager = HedTypeManager(EventManager(input_data, dispatcher.hed_schema))\r\n         var_manager.add_type(self.type_tag.casefold())\r\n \r\n         df_factors = var_manager.get_factor_vectors(\r\ndiff --git a/hed/tools/remodeling/operations/merge_consecutive_op.py b/hed/tools/remodeling/operations/merge_consecutive_op.py\nindex 8041864e..8d193db2 100644\n--- a/hed/tools/remodeling/operations/merge_consecutive_op.py\n+++ b/hed/tools/remodeling/operations/merge_consecutive_op.py\n@@ -115,7 +115,7 @@ def do_op(self, dispatcher, df, name, sidecar=None):\n \r\n         df_new = df.copy()\r\n         code_mask = df_new[self.column_name] == self.event_code\r\n-        if sum(code_mask.astype(int)) == 0:\r\n+        if not code_mask.any():\r\n             return df_new\r\n         match_columns.append(self.column_name)\r\n         match_df = df_new.loc[:, match_columns]\r\ndiff --git a/hed/tools/remodeling/operations/summarize_column_names_op.py b/hed/tools/remodeling/operations/summarize_column_names_op.py\nindex 8dbdb15b..03fed614 100644\n--- a/hed/tools/remodeling/operations/summarize_column_names_op.py\n+++ b/hed/tools/remodeling/operations/summarize_column_names_op.py\n@@ -94,7 +94,7 @@ def __init__(self, sum_op):\n         \"\"\" Constructor for column name summary manager.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Operation associated with this summary.\r\n+            sum_op (SummarizeColumnNamesOp): Operation associated with this summary.\r\n \r\n         \"\"\"\r\n         super().__init__(sum_op)\r\ndiff --git a/hed/tools/remodeling/operations/summarize_column_values_op.py b/hed/tools/remodeling/operations/summarize_column_values_op.py\nindex 612e4b5e..5fa5ddff 100644\n--- a/hed/tools/remodeling/operations/summarize_column_values_op.py\n+++ b/hed/tools/remodeling/operations/summarize_column_values_op.py\n@@ -133,7 +133,7 @@ def __init__(self, sum_op):\n         \"\"\" Constructor for column value summary manager.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Operation associated with this summary.\r\n+            sum_op (SummarizeColumnValuesOp): Operation associated with this summary.\r\n \r\n         \"\"\"\r\n         super().__init__(sum_op)\r\ndiff --git a/hed/tools/remodeling/operations/summarize_definitions_op.py b/hed/tools/remodeling/operations/summarize_definitions_op.py\nindex f953d593..65503fce 100644\n--- a/hed/tools/remodeling/operations/summarize_definitions_op.py\n+++ b/hed/tools/remodeling/operations/summarize_definitions_op.py\n@@ -94,7 +94,7 @@ def __init__(self, sum_op, hed_schema, known_defs=None):\n         \"\"\" Constructor for the summary of definitions.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Summary operation class for gathering definitions.\r\n+            sum_op (SummarizeDefinitionsOp): Summary operation class for gathering definitions.\r\n             hed_schema (HedSchema or HedSchemaGroup):  Schema used for the dataset.\r\n             known_defs (str or list or DefinitionDict): Definitions already known to be used.\r\n \r\ndiff --git a/hed/tools/remodeling/operations/summarize_hed_tags_op.py b/hed/tools/remodeling/operations/summarize_hed_tags_op.py\nindex 89b678ee..5e426bae 100644\n--- a/hed/tools/remodeling/operations/summarize_hed_tags_op.py\n+++ b/hed/tools/remodeling/operations/summarize_hed_tags_op.py\n@@ -187,7 +187,7 @@ def __init__(self, parameters):\n             }\r\n             if self.word_cloud[\"use_mask\"] and not self.word_cloud[\"mask_path\"]:\r\n                 self.word_cloud[\"mask_path\"] = os.path.realpath(\r\n-                    os.path.join(os.path.dirname(__file__),  '../../../resources/word_cloud_brain_mask.png'))\r\n+                    os.path.join(os.path.dirname(__file__), '../../../resources/word_cloud_brain_mask.png'))\r\n             if self.word_cloud[\"font_path\"]:\r\n                 self.word_cloud[\"font_path\"] = os.path.realpath(self.word_cloud[\"font_path\"])\r\n \r\n@@ -224,11 +224,12 @@ def validate_input_data(parameters):\n \r\n class HedTagSummary(BaseSummary):\r\n     \"\"\" Manager of the HED tag summaries. \"\"\"\r\n+\r\n     def __init__(self, sum_op):\r\n         \"\"\" Constructor for HED tag summary manager.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Operation associated with this summary.\r\n+            sum_op (SummarizeHedTagsOp): Operation associated with this summary.\r\n \r\n         \"\"\"\r\n \r\n@@ -249,12 +250,11 @@ def update_summary(self, new_info):\n             new_info['name'], total_events=len(new_info['df']))\r\n         input_data = TabularInput(\r\n             new_info['df'], sidecar=new_info['sidecar'], name=new_info['name'])\r\n-        tag_man = HedTagManager(EventManager(input_data, new_info['schema']),\r\n-                                remove_types=self.sum_op.remove_types)\r\n+        tag_man = HedTagManager(EventManager(input_data, new_info['schema']), remove_types=self.sum_op.remove_types)\r\n         hed_objs = tag_man.get_hed_objs(include_context=self.sum_op.include_context,\r\n                                         replace_defs=self.sum_op.replace_defs)\r\n         for hed in hed_objs:\r\n-            counts.update_event_counts(hed, new_info['name'])\r\n+            counts.update_tag_counts(hed, new_info['name'])\r\n         self.summary_dict[new_info[\"name\"]] = counts\r\n \r\n     def get_details_dict(self, tag_counts):\r\n@@ -393,8 +393,7 @@ def _get_dataset_string(result, indent=BaseSummary.DISPLAY_INDENT):\n         \"\"\"\r\n         sum_list = [f\"Dataset: Total events={result.get('Total events', 0)} \"\r\n                     f\"Total files={len(result.get('Files', []))}\"]\r\n-        sum_list = sum_list + \\\r\n-            HedTagSummary._get_tag_list(result, indent=indent)\r\n+        sum_list = sum_list + HedTagSummary._get_tag_list(result, indent=indent)\r\n         return \"\\n\".join(sum_list)\r\n \r\n     @staticmethod\r\n@@ -410,8 +409,7 @@ def _get_individual_string(result, indent=BaseSummary.DISPLAY_INDENT):\n \r\n         \"\"\"\r\n         sum_list = [f\"Total events={result.get('Total events', 0)}\"]\r\n-        sum_list = sum_list + \\\r\n-            HedTagSummary._get_tag_list(result, indent=indent)\r\n+        sum_list = sum_list + HedTagSummary._get_tag_list(result, indent=indent)\r\n         return \"\\n\".join(sum_list)\r\n \r\n     @staticmethod\r\ndiff --git a/hed/tools/remodeling/operations/summarize_hed_type_op.py b/hed/tools/remodeling/operations/summarize_hed_type_op.py\nindex dbd09e57..36a9667f 100644\n--- a/hed/tools/remodeling/operations/summarize_hed_type_op.py\n+++ b/hed/tools/remodeling/operations/summarize_hed_type_op.py\n@@ -108,7 +108,7 @@ def __init__(self, sum_op):\n         \"\"\" Constructor for HED type summary manager.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Operation associated with this summary.\r\n+            sum_op (SummarizeHedTypeOp): Operation associated with this summary.\r\n \r\n         \"\"\"\r\n         super().__init__(sum_op)\r\n@@ -130,8 +130,7 @@ def update_summary(self, new_info):\n             sidecar = Sidecar(sidecar)\r\n         input_data = TabularInput(\r\n             new_info['df'], sidecar=sidecar, name=new_info['name'])\r\n-        type_values = HedType(EventManager(\r\n-            input_data, new_info['schema']), new_info['name'], type_tag=self.type_tag)\r\n+        type_values = HedType(EventManager(input_data, new_info['schema']), new_info['name'], type_tag=self.type_tag)\r\n         counts = HedTypeCounts(new_info['name'], self.type_tag)\r\n         counts.update_summary(type_values.get_summary(),\r\n                               type_values.total_events, new_info['name'])\r\ndiff --git a/hed/tools/remodeling/operations/summarize_hed_validation_op.py b/hed/tools/remodeling/operations/summarize_hed_validation_op.py\nindex 3f720740..6549c4c6 100644\n--- a/hed/tools/remodeling/operations/summarize_hed_validation_op.py\n+++ b/hed/tools/remodeling/operations/summarize_hed_validation_op.py\n@@ -106,7 +106,7 @@ def __init__(self, sum_op):\n         \"\"\" Constructor for validation issue manager.\r\n \r\n         Parameters:\r\n-            sum_op (BaseOp): Operation associated with this summary.\r\n+            sum_op (SummarizeHedValidationOp): Operation associated with this summary.\r\n \r\n         \"\"\"\r\n         super().__init__(sum_op)\r\n@@ -153,8 +153,7 @@ def update_summary(self, new_info):\n \r\n         sidecar = new_info.get('sidecar', None)\r\n         if sidecar and not isinstance(sidecar, Sidecar):\r\n-            sidecar = Sidecar(\r\n-                files=new_info['sidecar'], name=os.path.basename(sidecar))\r\n+            sidecar = Sidecar(files=new_info['sidecar'], name=os.path.basename(sidecar))\r\n         results = self._get_sidecar_results(\r\n             sidecar, new_info, self.sum_op.check_for_warnings)\r\n         if not results['sidecar_had_issues']:\r\ndiff --git a/hed/validator/onset_validator.py b/hed/validator/onset_validator.py\nindex 1983f409..1d7a04dd 100644\n--- a/hed/validator/onset_validator.py\n+++ b/hed/validator/onset_validator.py\n@@ -76,5 +76,5 @@ def check_for_banned_tags(hed_string):\n         issues = []\r\n         for tag in hed_string.get_all_tags():\r\n             if tag.short_base_tag in banned_tag_list:\r\n-                issues += ErrorHandler.format_error(TemporalErrors.HED_ONSET_WITH_NO_COLUMN, tag)\r\n+                issues += ErrorHandler.format_error(TemporalErrors.TEMPORAL_TAG_NO_TIME, tag)\r\n         return issues\r\ndiff --git a/hed/validator/spreadsheet_validator.py b/hed/validator/spreadsheet_validator.py\nindex 0e283b2a..9fd47443 100644\n--- a/hed/validator/spreadsheet_validator.py\n+++ b/hed/validator/spreadsheet_validator.py\n@@ -1,6 +1,6 @@\n \"\"\" Validates spreadsheet tabular data. \"\"\"\r\n import copy\r\n-\r\n+import pandas as pd\r\n from hed.models.base_input import BaseInput\r\n from hed.errors.error_types import ColumnErrors, ErrorContext, ValidationErrors\r\n from hed.errors.error_reporter import ErrorHandler\r\n@@ -61,7 +61,7 @@ def validate(self, data, def_dicts=None, name=None, error_handler=None):\n         if data.needs_sorting:\r\n             data_new = copy.deepcopy(data)\r\n             data_new._dataframe = df_util.sort_dataframe_by_onsets(data.dataframe)\r\n-            issues += error_handler.format_error_with_context(ValidationErrors.ONSETS_OUT_OF_ORDER)\r\n+            issues += error_handler.format_error_with_context(ValidationErrors.ONSETS_UNORDERED)\r\n             data = data_new\r\n \r\n         onsets = df_util.split_delay_tags(data.series_a, self._schema, data.onsets)\r\n@@ -70,12 +70,13 @@ def validate(self, data, def_dicts=None, name=None, error_handler=None):\n         self._hed_validator = HedValidator(self._schema, def_dicts=def_dicts)\r\n         if data.onsets is not None:\r\n             self._onset_validator = OnsetValidator()\r\n+            onset_mask = ~pd.isna(pd.to_numeric(onsets['onset'], errors='coerce'))\r\n         else:\r\n             self._onset_validator = None\r\n+            onset_mask = None\r\n \r\n         # Check the rows of the input data\r\n-        issues += self._run_checks(df, error_handler=error_handler, row_adj=row_adj,\r\n-                                   has_onsets=bool(self._onset_validator))\r\n+        issues += self._run_checks(df, error_handler=error_handler, row_adj=row_adj, onset_mask=onset_mask)\r\n         if self._onset_validator:\r\n             issues += self._run_onset_checks(onsets, error_handler=error_handler, row_adj=row_adj)\r\n         error_handler.pop_error_context()\r\n@@ -83,7 +84,7 @@ def validate(self, data, def_dicts=None, name=None, error_handler=None):\n         issues = sort_issues(issues)\r\n         return issues\r\n \r\n-    def _run_checks(self, hed_df, error_handler, row_adj, has_onsets):\r\n+    def _run_checks(self, hed_df, error_handler, row_adj, onset_mask=None):\r\n         issues = []\r\n         columns = list(hed_df.columns)\r\n         self.invalid_original_rows = set()\r\n@@ -113,7 +114,7 @@ def _run_checks(self, hed_df, error_handler, row_adj, has_onsets):\n                 error_handler.pop_error_context()  # Row\r\n                 continue\r\n \r\n-            if has_onsets or not row_strings:\r\n+            if not row_strings or (onset_mask is not None and onset_mask.iloc[row_number]):\r\n                 error_handler.pop_error_context()  # Row\r\n                 continue\r\n \r\n@@ -148,6 +149,9 @@ def _run_onset_checks(self, onset_filtered, error_handler, row_adj):\n             error_handler.pop_error_context()  # Row\r\n         return issues\r\n \r\n+    def _run_onset_nan_checks(self, onsets, error_handler, row_adj):\r\n+        return\r\n+\r\n     def _validate_column_structure(self, base_input, error_handler, row_adj):\r\n         \"\"\"\r\n         Validate that each column in the input data has valid values.\r\n@@ -160,7 +164,7 @@ def _validate_column_structure(self, base_input, error_handler, row_adj):\n             List of issues associated with each invalid value. Each issue is a dictionary.\r\n         \"\"\"\r\n         issues = []\r\n-        col_issues = base_input._mapper.check_for_mapping_issues(base_input)\r\n+        col_issues = base_input._mapper.check_for_mapping_issues()\r\n         error_handler.add_context_and_filter(col_issues)\r\n         issues += col_issues\r\n         for column in base_input.column_metadata().values():\r\ndiff --git a/hed/validator/util/class_regex.json b/hed/validator/util/class_regex.json\nindex 81f49f02..303983da 100644\n--- a/hed/validator/util/class_regex.json\n+++ b/hed/validator/util/class_regex.json\n@@ -22,9 +22,9 @@\n     \"less-than\": \"<\",\r\n     \"letters\": \"[A-Za-z]\",\r\n     \"lowercase\": \"[a-z]\",\r\n-    \"name\": \"[\\\\w\\\\-\\\\u0080-\\\\uFFFF]\",\r\n+    \"name\": \"[\\\\w\\\\-\\\\u00A0-\\\\uFFFF]\",\r\n     \"newline\": \"\\\\n\",\r\n-    \"nonascii\": \"[\\\\u0080-\\\\uFFFF]\",\r\n+    \"nonascii\": \"[\\\\u00A0-\\\\uFFFF]\",\r\n     \"number-sign\": \"#\",\r\n     \"numeric\": \"[0-9.\\\\-+^Ee]\",\r\n     \"percent-sign\": \"%\",\r\n@@ -37,7 +37,7 @@\n     \"single-quote\": \"'\",\r\n     \"forward-slash\": \"/\",\r\n     \"tab\": \"\\\\t\",\r\n-    \"text\": \"[^\\\\x00-\\\\x1F\\\\x7F,{}]\",\r\n+    \"text\": \"[^\\\\x00-\\\\x1F\\\\x7F-\\\\x9F,{}]\",\r\n     \"tilde\": \"~\",\r\n     \"underscore\": \"_\",\r\n     \"uppercase\": \"[A-Z]\",\r\n", "instance_id": "hed-standard__hed-python-1029", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `hed-validator` fails on \"n/a\" values in the onset column of TSV files, which are valid according to the BIDS specification for missing values. It provides context from the BIDS specification, a detailed error traceback, and examples of problematic data. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior for handling \"n/a\" in the onset column (e.g., should it be treated as a null value or skipped during sorting?). Additionally, while it suggests improving error logging to identify the failing file, it lacks specifics on how this feedback should be implemented. The problem also references a non-public dataset, which limits reproducibility, though this does not critically hinder understanding the core issue. Overall, the statement is valid and clear but misses some minor details on edge cases and precise requirements for the fix.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, involving multiple files (`base_input.py`, `df_util.py`, `column_mapper.py`, etc.) but primarily focusing on handling \"n/a\" values in the onset column during data processing and validation. This requires understanding the interaction between data parsing, dataframe manipulation with pandas, and validation logic in the HED library. Second, the technical concepts involved include familiarity with pandas for type conversion and sorting (e.g., using `pd.to_numeric` with error coercion), as well as knowledge of the HED schema and BIDS standards for tabular data. Third, edge cases such as non-numeric values in other columns, missing onset columns, or mixed data types need consideration, though they are not overly complex. Finally, the changes do not significantly impact the system's architecture but do require careful modification to avoid introducing new bugs in data processing pipelines. Overall, this problem requires a solid understanding of the codebase and moderate complexity in implementation, justifying a score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature] Allows snapshots to have a list as a unique key\n### Is this your first time submitting a feature request?\n\n- [X] I have read the [expectations for open source contributors](https://docs.getdbt.com/docs/contributing/oss-expectations)\n- [X] I have searched the existing issues, and I could not find an existing issue for this feature\n- [X] I am requesting a straightforward extension of existing dbt functionality, rather than a Big Idea better suited to a discussion\n\n### Describe the feature\n\nThis a clone of https://github.com/dbt-labs/dbt-adapters/issues/181\r\n\r\nRight now the 'unique_key' for a snapshot must be a single column or an expression. If a user needs a combination of columns to be unique then they can use the expression to concatenate multiple columns together how they like.\r\n\r\nIdeally users would be able to enter a list of columns, just like they can do for incremental models.\n\n### Describe alternatives you've considered\n\nI considered adding additional parameters instead of changing the existing unique_key parameter. I think using the existing parameter is best because it's the most consistent with how incremental tables work.\n\n### Who will this benefit?\n\nSnapshot users who work in Postgres or Redshift can take advantage of performance optimizations in their databases with this change.\r\n\r\nThere are performance benefits from using multiple columns rather than an expression. In Postgres users would be able to pass in partition keys or indexed columns to speed up their snapshots, without needing to make an additional expression index. In Redshift users can pass in a separate distribution key to also get performance benefits.\r\n\r\nThese benefits can't be realized unless dbt allows the unique key to natively be a combination of columns, rather than a concatenation/combination expression.\n\n### Are you interested in contributing this feature?\n\nYes, I have a PR in dbt-adapters and in dbt-core coming shortly\n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/.changes/unreleased/Features-20241001-134051.yaml b/.changes/unreleased/Features-20241001-134051.yaml\nnew file mode 100644\nindex 00000000000..60ada51ece3\n--- /dev/null\n+++ b/.changes/unreleased/Features-20241001-134051.yaml\n@@ -0,0 +1,6 @@\n+kind: Features\n+body: Enable use of multi-column unique key in snapshots\n+time: 2024-10-01T13:40:51.297529-04:00\n+custom:\n+  Author: gshank\n+  Issue: \"9992\"\ndiff --git a/core/dbt/artifacts/resources/v1/snapshot.py b/core/dbt/artifacts/resources/v1/snapshot.py\nindex 062b6a62814..1a7b9344ca0 100644\n--- a/core/dbt/artifacts/resources/v1/snapshot.py\n+++ b/core/dbt/artifacts/resources/v1/snapshot.py\n@@ -19,7 +19,7 @@ class SnapshotMetaColumnNames(dbtClassMixin):\n class SnapshotConfig(NodeConfig):\n     materialized: str = \"snapshot\"\n     strategy: Optional[str] = None\n-    unique_key: Optional[str] = None\n+    unique_key: Optional[Union[str, List[str]]] = None\n     target_schema: Optional[str] = None\n     target_database: Optional[str] = None\n     updated_at: Optional[str] = None\ndiff --git a/core/setup.py b/core/setup.py\nindex b7a8dabd14e..b787ce8a923 100644\n--- a/core/setup.py\n+++ b/core/setup.py\n@@ -71,7 +71,7 @@\n         \"dbt-extractor>=0.5.0,<=0.6\",\n         \"dbt-semantic-interfaces>=0.7.3,<0.8\",\n         # Minor versions for these are expected to be backwards-compatible\n-        \"dbt-common>=1.9.0,<2.0\",\n+        \"dbt-common>=1.11.0,<2.0\",\n         \"dbt-adapters>=1.7.0,<2.0\",\n         # ----\n         # Expect compatibility with all new versions of these packages, so lower bounds only.\ndiff --git a/schemas/dbt/manifest/v12.json b/schemas/dbt/manifest/v12.json\nindex 9cb7f732e62..cebc0970b9d 100644\n--- a/schemas/dbt/manifest/v12.json\n+++ b/schemas/dbt/manifest/v12.json\n@@ -6540,6 +6540,12 @@\n                       {\n                         \"type\": \"string\"\n                       },\n+                      {\n+                        \"type\": \"array\",\n+                        \"items\": {\n+                          \"type\": \"string\"\n+                        }\n+                      },\n                       {\n                         \"type\": \"null\"\n                       }\n@@ -16425,6 +16431,12 @@\n                             {\n                               \"type\": \"string\"\n                             },\n+                            {\n+                              \"type\": \"array\",\n+                              \"items\": {\n+                                \"type\": \"string\"\n+                              }\n+                            },\n                             {\n                               \"type\": \"null\"\n                             }\n", "instance_id": "dbt-labs__dbt-core-10795", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the feature request to allow snapshots in dbt to use a list of columns as a unique key, similar to incremental models. The goal is well-defined, and the benefits (performance optimizations in databases like Postgres and Redshift) are articulated. The statement also references an existing issue and provides context on alternatives considered, which adds to its clarity. However, there are minor ambiguities: the problem statement does not explicitly detail how the list of columns should be handled in the implementation (e.g., how they are combined or validated), nor does it mention specific edge cases or constraints (e.g., maximum number of columns, handling of null values). Additionally, while the performance benefits are mentioned, there are no concrete examples or test cases provided to illustrate the expected behavior. Thus, while the overall intent is clear, some minor details are missing that would make it fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily involving modifications to the `SnapshotConfig` class in the dbt-core repository to allow `unique_key` to accept a list of strings instead of just a single string. The changes also include updates to the JSON schema and dependency versions, which are straightforward. However, the implementation requires understanding the dbt framework's internal representation of snapshots and how unique keys are processed downstream in adapters (as hinted by the reference to a related PR in dbt-adapters). This introduces moderate complexity in terms of cross-module interactions. \n\nFrom a technical concepts perspective, the change involves basic type handling in Python (using Union types for `unique_key`), schema updates, and dependency management, which are not overly complex but require familiarity with dbt's architecture and versioning practices. The problem does not appear to significantly impact the system's architecture, as it extends existing functionality rather than overhauling it. \n\nRegarding edge cases and error handling, the problem statement does not explicitly mention specific scenarios to handle (e.g., invalid column names, empty lists, or database-specific limitations), but the code changes suggest that downstream validation or adapter-specific logic might be needed, adding a layer of complexity. Overall, this task requires understanding multiple concepts and making changes across a few files, but it does not demand deep architectural refactoring or advanced domain-specific knowledge beyond the dbt ecosystem. Hence, a difficulty score of 0.45 reflects a medium-level challenge suitable for a developer with moderate experience in the dbt codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fully type-annotate package\nRunning mypy on code that uses the latest version of `nwbinspector` fails with the error message \"module is installed, but missing library stubs or py.typed marker\".  A quick look at `nwbinspector`'s source indicates that it is making some use of type annotations, but a number of annotations are missing.\r\n\r\nPlease do the following:\r\n\r\n- [x] Fully type-annotate all public items\r\n- [x] Add a CI job to run mypy on the package, and ensure it passes\r\n- [x] Add an empty file named `py.typed` inside `src/nwbinspector/`\r\n- [x] Ensure that the `src/nwbinspector/py.typed` file is included in sdists & wheels built from the project.  This may involve configuration specific to your build backend (setuptools) that is beyond the scope of this comment.\r\n    - You can list the contents of an sdist (the `dist/nwbinspector-{version}.tar.gz` file created when building the project) by running `tar ztf dist/nwbinspector-{version}.tar.gz` on it.\r\n    - You can list the contents of a wheel (a `dist/nwbinspector-{version}-*-*-*.whl` file created when building the project) by running `zipinfo -1 dist/nwbinspector-{version}-*-*-*.whl` on it.\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 13c28f363..a90515312 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -22,3 +22,9 @@ repos:\n   - id: codespell\n     additional_dependencies:\n     - tomli\n+\n+-   repo: https://github.com/pre-commit/mirrors-mypy\n+    rev: v1.11.2\n+    hooks:\n+    -   id: mypy\n+        additional_dependencies: [\"types-PyYAML\", \"types-setuptools\"]\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 7193363b6..4cc861849 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,7 @@\n * Simplified the `nwbinspector.testing` configuration framework. [#509](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/509)\n * Cleaned old references to non-recent PyNWB and HDMF versions. Current policy is that latest NWB Inspector releases should only support compatibility with latest PyNWB and HDMF. [#510](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/510)\n * Swapped setup approach to the modern `pyproject.toml` standard. [#507](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/507)\n-\n+* Added complete annotation typing and integrated Mypy into pre-commit. [#520](https://github.com/NeurodataWithoutBorders/nwbinspector/pull/520)\n \n \n \ndiff --git a/docs/conf.py b/docs/conf.py\nindex 998c66e8d..a551bb704 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -8,7 +8,7 @@\n sys.path.append(str(Path(__file__).parent))\n from conf_extlinks import extlinks, intersphinx_mapping\n \n-sys.path.insert(0, Path(__file__).resolve().parents[1])\n+sys.path.insert(0, str(Path(__file__).resolve().parents[1]))\n \n project = \"NWBInspector\"\n copyright = \"2022-2024, CatalystNeuro\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 3839ef739..41ed89967 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -92,7 +92,17 @@ extend-exclude = '''\n exclude = [\"docs/*\"]\n \n [tool.ruff.lint]\n-select = [\"F401\", \"F541\", \"I\"] # TODO: eventually, expand to other 'D', and other 'F' linting\n+# TODO: eventually, expand to other 'D', and other 'F' linting\n+select = [\n+    \"F401\",\n+    \"F541\",\n+    \"I\",\n+    \"ANN001\",\n+    \"ANN201\",\n+    \"ANN202\",\n+    \"ANN205\",\n+    \"ANN206\",\n+]\n fixable = [\"ALL\"]\n \n [tool.ruff.lint.per-file-ignores]\n@@ -105,6 +115,7 @@ fixable = [\"ALL\"]\n \"src/nwbinspector/inspector_tools/__init__.py\" = [\"F401\", \"I\"]\n \"src/nwbinspector/version/__init__.py\" = [\"F401\", \"I\"]\n \"src/nwbinspector/register_checks/__init__.py\" = [\"F401\", \"I\"]\n+\"tests/*\" = [\"ANN\"]\n \n [tool.ruff.lint.isort]\n relative-imports-order = \"closest-to-furthest\"\ndiff --git a/src/nwbinspector/_configuration.py b/src/nwbinspector/_configuration.py\nindex c5d3995e6..efa62f82b 100644\n--- a/src/nwbinspector/_configuration.py\n+++ b/src/nwbinspector/_configuration.py\n@@ -3,22 +3,20 @@\n import json\n from pathlib import Path\n from types import FunctionType\n-from typing import Optional\n+from typing import Optional, Union\n \n import jsonschema\n import yaml\n+from typing_extensions import Callable\n \n-from nwbinspector.utils import PathType\n+from ._registration import Importance, available_checks\n \n-from . import available_checks\n-from ._registration import Importance\n-\n-INTERNAL_CONFIGS = dict(\n+INTERNAL_CONFIGS: dict[str, Path] = dict(\n     dandi=Path(__file__).parent / \"_internal_configs\" / \"dandi.inspector_config.yaml\",\n )\n \n \n-def validate_config(config: dict):\n+def validate_config(config: dict) -> None:\n     \"\"\"Validate an instance of configuration against the official schema.\"\"\"\n     config_schema_file_path = Path(__file__).parent / \"_internal_configs\" / \"config.schema.json\"\n     with open(file=config_schema_file_path, mode=\"r\") as fp:\n@@ -26,7 +24,7 @@ def validate_config(config: dict):\n     jsonschema.validate(instance=config, schema=schema)\n \n \n-def _copy_function(function):\n+def _copy_function(function: Callable) -> Callable:\n     \"\"\"Copy the core parts of a given function, excluding wrappers, then return a new function.\"\"\"\n     copied_function = FunctionType(\n         function.__code__, function.__globals__, function.__name__, function.__defaults__, function.__closure__\n@@ -34,10 +32,11 @@ def _copy_function(function):\n \n     # in case f was given attrs (note this dict is a shallow copy)\n     copied_function.__dict__.update(function.__dict__)\n+\n     return copied_function\n \n \n-def copy_check(check):\n+def copy_check(check: Callable) -> Callable:\n     \"\"\"\n     Copy a check function so that internal attributes can be adjusted without changing the original function.\n \n@@ -47,11 +46,12 @@ def copy_check(check):\n     see https://github.com/NeurodataWithoutBorders/nwbinspector/pull/218 for explanation.\n     \"\"\"\n     copied_check = _copy_function(function=check)\n-    copied_check.__wrapped__ = _copy_function(function=check.__wrapped__)\n+    copied_check.__wrapped__ = _copy_function(function=check.__wrapped__)  # type: ignore\n+\n     return copied_check\n \n \n-def load_config(filepath_or_keyword: PathType) -> dict:\n+def load_config(filepath_or_keyword: Union[str, Path]) -> dict:\n     \"\"\"\n     Load a config dictionary either via keyword search of the internal configs, or an explicit filepath.\n \n@@ -59,14 +59,15 @@ def load_config(filepath_or_keyword: PathType) -> dict:\n         - 'dandi'\n             For all DANDI archive related practices, including validation and upload.\n     \"\"\"\n-    file = INTERNAL_CONFIGS.get(filepath_or_keyword, filepath_or_keyword)\n+    file = INTERNAL_CONFIGS.get(str(filepath_or_keyword), filepath_or_keyword)\n     with open(file=file, mode=\"r\") as stream:\n         config = yaml.safe_load(stream=stream)\n+\n     return config\n \n \n def configure_checks(\n-    checks: list = available_checks,\n+    checks: Optional[list] = None,\n     config: Optional[dict] = None,\n     ignore: Optional[list[str]] = None,\n     select: Optional[list[str]] = None,\n@@ -77,12 +78,12 @@ def configure_checks(\n \n     Parameters\n     ----------\n+    checks : list of check functions, optional\n+        If None, defaults to all registered checks.\n     config : dict\n         Dictionary valid against our JSON configuration schema.\n         Can specify a mapping of importance levels and list of check functions whose importance you wish to change.\n         Typically loaded via json.load from a valid .json file\n-    checks : list of check functions\n-        Defaults to all registered checks.\n     ignore: list, optional\n         Names of functions to skip.\n     select: list, optional\n@@ -100,6 +101,8 @@ def configure_checks(\n \n         The default is the lowest level, BEST_PRACTICE_SUGGESTION.\n     \"\"\"\n+    checks = checks or available_checks\n+\n     if ignore is not None and select is not None:\n         raise ValueError(\"Options 'ignore' and 'select' cannot both be used.\")\n     if importance_threshold not in Importance:\n@@ -107,9 +110,10 @@ def configure_checks(\n             f\"Indicated importance_threshold ({importance_threshold}) is not a valid importance level! Please choose \"\n             \"from [CRITICAL_IMPORTANCE, BEST_PRACTICE_VIOLATION, BEST_PRACTICE_SUGGESTION].\"\n         )\n+\n+    checks_out: list = []\n     if config is not None:\n         validate_config(config=config)\n-        checks_out = []\n         ignore = ignore or []\n         for check in checks:\n             mapped_check = copy_check(check=check)\n@@ -118,7 +122,7 @@ def configure_checks(\n                     if importance_name == \"SKIP\":\n                         ignore.append(check.__name__)\n                         continue\n-                    mapped_check.importance = Importance[importance_name]\n+                    mapped_check.importance = Importance[importance_name]  # type: ignore\n                     # Output wrappers are apparently parsed at time of wrapping not of time of output return...\n                     # Attempting to re-wrap the copied function if the importance level is being adjusted...\n                     # From https://github.com/NeurodataWithoutBorders/nwbinspector/issues/302\n@@ -133,5 +137,6 @@ def configure_checks(\n     elif ignore:\n         checks_out = [x for x in checks_out if x.__name__ not in ignore]\n     if importance_threshold:\n-        checks_out = [x for x in checks_out if x.importance.value >= importance_threshold.value]\n+        checks_out = [x for x in checks_out if x.importance.value >= importance_threshold.value]  # type: ignore\n+\n     return checks_out\ndiff --git a/src/nwbinspector/_dandi_inspection.py b/src/nwbinspector/_dandi_inspection.py\nindex 1b3a04167..084724ec5 100644\n--- a/src/nwbinspector/_dandi_inspection.py\n+++ b/src/nwbinspector/_dandi_inspection.py\n@@ -21,7 +21,7 @@ def inspect_dandiset(\n     importance_threshold: Union[str, Importance] = Importance.BEST_PRACTICE_SUGGESTION,\n     skip_validate: bool = False,\n     show_progress_bar: bool = True,\n-    client: Union[\"dandi.dandiapi.DandiAPIClient\", None] = None,\n+    client: Union[\"dandi.dandiapi.DandiAPIClient\", None] = None,  # type: ignore\n ) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Inspect a Dandiset for common issues.\n@@ -73,12 +73,6 @@ def inspect_dandiset(\n \n     dandiset = client.get_dandiset(dandiset_id=dandiset_id, version_id=dandiset_version)\n \n-    # if not any(\n-    #     asset_type.get(\"identifier\", \"\") != \"RRID:SCR_015242\"  # Identifier for NWB standard\n-    #     for asset_type in dandiset.get_raw_metadata().get(\"assetsSummary\", {}).get(\"dataStandard\", [])\n-    # ):\n-    #     return None\n-\n     nwb_assets = [asset for asset in dandiset.get_assets() if \".nwb\" in pathlib.Path(asset.path).suffixes]\n \n     nwb_assets_iterator = nwb_assets\n@@ -101,7 +95,7 @@ def inspect_dandiset(\n             importance_threshold=importance_threshold,\n             skip_validate=skip_validate,\n         ):\n-            message.file_path = asset.path\n+            message.file_path = asset.path  # type: ignore\n             yield message\n \n \n@@ -110,13 +104,13 @@ def inspect_dandi_file_path(\n     dandi_file_path: str,\n     dandiset_id: str,\n     dandiset_version: Union[str, Literal[\"draft\"], None] = None,\n-    config: Union[str, pathlib.Path, dict, Literal[\"dandi\"]] = \"dandi\",\n+    config: Union[str, pathlib.Path, dict, Literal[\"dandi\"], None] = \"dandi\",\n     checks: Union[list, None] = None,\n     ignore: Union[list[str], None] = None,\n     select: Union[list[str], None] = None,\n     importance_threshold: Union[str, Importance] = Importance.BEST_PRACTICE_SUGGESTION,\n     skip_validate: bool = False,\n-    client: Union[\"dandi.dandiapi.DandiAPIClient\", None] = None,\n+    client: Union[\"dandi.dandiapi.DandiAPIClient\", None] = None,  # type: ignore\n ) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Inspect a Dandifile for common issues.\n@@ -131,7 +125,7 @@ def inspect_dandi_file_path(\n         The specific published version of the Dandiset to inspect.\n         If None, the latest version is used.\n         If there are no published versions, then 'draft' is used instead.\n-    config : file path, dictionary, or \"dandi\", default: \"dandi\"\n+    config : file path, dictionary, \"dandi\", or None, default: \"dandi\"\n         If a file path, loads the dictionary configuration from the file.\n         If a dictionary, it must be valid against the configuration schema.\n         If \"dandi\", uses the requirements for DANDI validation.\n@@ -177,14 +171,14 @@ def inspect_dandi_file_path(\n         importance_threshold=importance_threshold,\n         skip_validate=skip_validate,\n     ):\n-        message.file_path = dandi_file_path\n+        message.file_path = dandi_file_path  # type: ignore\n         yield message\n \n \n def inspect_url(\n     *,\n     url: str,\n-    config: Union[str, pathlib.Path, dict, Literal[\"dandi\"]] = \"dandi\",\n+    config: Union[str, pathlib.Path, dict, Literal[\"dandi\"], None] = \"dandi\",\n     checks: Union[list, None] = None,\n     ignore: Union[list[str], None] = None,\n     select: Union[list[str], None] = None,\n@@ -202,7 +196,7 @@ def inspect_url(\n         https://dandiarchive.s3.amazonaws.com/blobs/636/57e/63657e32-ad33-4625-b664-31699b5bf664\n \n         Note: this must be the `https` URL, not the 's3://' form.\n-    config : file path, dictionary, or \"dandi\", default: \"dandi\"\n+    config : file path, dictionary, \"dandi\", or None, default: \"dandi\"\n         If a file path, loads the dictionary configuration from the file.\n         If a dictionary, it must be valid against the configuration schema.\n         If \"dandi\", uses the requirements for DANDI validation.\n@@ -232,9 +226,10 @@ def inspect_url(\n     filterwarnings(action=\"ignore\", message=\"No cached namespaces found in .*\")\n     filterwarnings(action=\"ignore\", message=\"Ignoring cached namespace .*\")\n \n-    if not isinstance(config, dict):\n+    if isinstance(config, (str, pathlib.Path)):\n         config = load_config(filepath_or_keyword=config)\n-    validate_config(config=config)\n+    if isinstance(config, dict):\n+        validate_config(config=config)\n \n     byte_stream = remfile.File(url=url)\n     with (\n@@ -249,7 +244,7 @@ def inspect_url(\n                     importance=Importance.PYNWB_VALIDATION,\n                     check_function_name=validation_error.name,\n                     location=validation_error.location,\n-                    file_path=nwbfile_path,\n+                    file_path=url,\n                 )\n \n         nwbfile = io.read()\n@@ -262,5 +257,5 @@ def inspect_url(\n             select=select,\n             importance_threshold=importance_threshold,\n         ):\n-            message.file_path = url\n+            message.file_path = url  # type: ignore\n             yield message\ndiff --git a/src/nwbinspector/_formatting.py b/src/nwbinspector/_formatting.py\nindex 22c4455e9..baf147018 100644\n--- a/src/nwbinspector/_formatting.py\n+++ b/src/nwbinspector/_formatting.py\n@@ -8,19 +8,20 @@\n from enum import Enum\n from pathlib import Path\n from platform import platform\n-from typing import Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n+from packaging.version import Version\n \n from ._organization import organize_messages\n from ._types import Importance, InspectorMessage\n-from .utils import FilePathType, get_package_version\n+from .utils import get_package_version\n \n \n class InspectorOutputJSONEncoder(json.JSONEncoder):\n     \"\"\"Custom JSONEncoder for the NWBInspector.\"\"\"\n \n-    def default(self, o):  # noqa D102\n+    def default(self, o: object) -> Any:  # noqa D102\n         if isinstance(o, InspectorMessage):\n             return o.__dict__\n         if isinstance(o, Enum):\n@@ -31,7 +32,7 @@ def default(self, o):  # noqa D102\n             return super().default(o)\n \n \n-def _get_report_header():\n+def _get_report_header() -> dict[str, str]:\n     \"\"\"Grab basic information from system at time of report generation.\"\"\"\n     return dict(\n         Timestamp=str(datetime.now().astimezone()),\n@@ -44,8 +45,8 @@ class FormatterOptions:\n     \"\"\"Class structure for defining all free attributes for the design of a report format.\"\"\"\n \n     def __init__(\n-        self, indent_size: int = 2, indent: Optional[str] = None, section_headers: list[str] = [\"=\", \"-\", \"~\"]\n-    ):\n+        self, indent_size: int = 2, indent: Optional[str] = None, section_headers: tuple[str, ...] = (\"=\", \"-\", \"~\")\n+    ) -> None:\n         \"\"\"\n         Class that defines all the format parameters used by the generic MessageFormatter.\n \n@@ -58,11 +59,11 @@ def __init__(\n             Defines the specific indentation to inject between numerical sectioning and section name or message.\n             Overrides indent_size.\n             Defaults to \" \" * indent_size.\n-        section_headers : list of strings, optional\n+        section_headers : tuple of strings\n             List of characters that will be injected under the display of each new section of the report.\n             If levels is longer than this list, the last item will be repeated over the remaining levels.\n             If levels is shorter than this list, only the first len(levels) of items will be used.\n-            Defaults to the .rst style for three sub-sections: [\"=\", \"-\", \"~\"]\n+            Defaults to the .rst style for three subsections: [\"=\", \"-\", \"~\"]\n         \"\"\"\n         # TODO\n         # Future custom options could include section break sizes, section-specific indents, etc.\n@@ -75,14 +76,14 @@ class MessageFormatter:\n \n     def __init__(\n         self,\n-        messages: list[InspectorMessage],\n+        messages: list[Optional[InspectorMessage]],\n         levels: list[str],\n         reverse: Optional[list[bool]] = None,\n         detailed: bool = False,\n         formatter_options: Optional[FormatterOptions] = None,\n-    ):\n+    ) -> None:\n         self.nmessages = len(messages)\n-        self.nfiles = len(set(message.file_path for message in messages))\n+        self.nfiles = len(set(message.file_path for message in messages))  # type: ignore\n         self.message_count_by_importance = self._count_messages_by_importance(messages=messages)\n         self.initial_organized_messages = organize_messages(messages=messages, levels=levels, reverse=reverse)\n         self.detailed = detailed\n@@ -100,29 +101,29 @@ def __init__(\n                 formatter_options, FormatterOptions\n             ), \"'formatter_options' is not an instance of FormatterOptions!\"\n             self.formatter_options = formatter_options\n-        self.formatter_options.section_headers.extend(\n-            [self.formatter_options.section_headers[-1]] * (self.nlevels - len(self.formatter_options.section_headers))\n-        )\n+        self.formatter_options.section_headers = self.formatter_options.section_headers + (\n+            self.formatter_options.section_headers[-1],\n+        ) * (self.nlevels - len(self.formatter_options.section_headers))\n         self.message_counter = 0\n-        self.formatted_messages = []\n+        self.formatted_messages: list = []\n \n     @staticmethod\n-    def _count_messages_by_importance(messages: list[InspectorMessage]) -> dict[str, int]:\n+    def _count_messages_by_importance(messages: list[Optional[InspectorMessage]]) -> dict[str, int]:\n         message_count_by_importance = {importance_level.name: 0 for importance_level in Importance}\n         for message in messages:\n-            message_count_by_importance[message.importance.name] += 1\n+            message_count_by_importance[message.importance.name] += 1  # type: ignore\n         for key in [keys for keys, count in message_count_by_importance.items() if count == 0]:\n             message_count_by_importance.pop(key)\n         return message_count_by_importance\n \n     @staticmethod\n-    def _get_name(obj) -> str:\n+    def _get_name(obj: Union[Enum, str]) -> str:\n         if isinstance(obj, Enum):\n             return obj.name\n         if isinstance(obj, str):\n             return obj\n \n-    def _get_message_header(self, message: InspectorMessage):\n+    def _get_message_header(self, message: InspectorMessage) -> str:\n         message_header = \"\"\n         if \"file_path\" in self.free_levels:\n             message_header += f\"{message.file_path} - \"\n@@ -138,17 +139,17 @@ def _get_message_header(self, message: InspectorMessage):\n             message_header += f\"with name '{message.object_name}'\"\n         return message_header\n \n-    def _get_message_increment(self, level_counter: list[int]):\n+    def _get_message_increment(self, level_counter: list[int]) -> str:\n         return (\n             f\"{'.'.join(np.array(level_counter, dtype=str))}.{self.message_counter}\" f\"{self.formatter_options.indent}\"\n         )\n \n     def _add_subsection(\n         self,\n-        organized_messages: dict[str, Union[dict, list[InspectorMessage]]],\n+        organized_messages: dict,\n         levels: list[str],\n         level_counter: list[int],\n-    ):\n+    ) -> None:\n         \"\"\"Recursive helper for display_messages.\"\"\"\n         this_level_counter = list(level_counter)  # local copy passed from previous recursion level\n         if len(levels) > 1:\n@@ -217,31 +218,38 @@ def format_messages(self) -> list[str]:\n \n \n def format_messages(\n-    messages: list[InspectorMessage],\n-    levels: list[str] = None,\n+    messages: list[Optional[InspectorMessage]],\n+    levels: Optional[list[str]] = None,\n     reverse: Optional[list[bool]] = None,\n     detailed: bool = False,\n ) -> list[str]:\n     \"\"\"Print InspectorMessages in order specified by the organization structure.\"\"\"\n-    if levels is None:\n-        levels = [\"file_path\", \"importance\"]\n+    levels = levels or [\"file_path\", \"importance\"]\n+\n     message_formatter = MessageFormatter(messages=messages, levels=levels, reverse=reverse, detailed=detailed)\n     formatted_messages = message_formatter.format_messages()\n+\n     return formatted_messages\n \n \n-def print_to_console(formatted_messages: list[str]):\n+def print_to_console(formatted_messages: list[str]) -> None:\n     \"\"\"Print report file contents to console.\"\"\"\n     sys.stdout.write(os.linesep * 2)\n     for line in formatted_messages:\n         sys.stdout.write(line + \"\\n\")\n \n+    return None\n+\n \n-def save_report(report_file_path: FilePathType, formatted_messages: list[str], overwrite=False):\n+def save_report(report_file_path: Union[str, Path], formatted_messages: list[str], overwrite: bool = False) -> None:\n     \"\"\"Write the list of organized check results to a nicely formatted text file.\"\"\"\n     report_file_path = Path(report_file_path)\n+\n     if report_file_path.exists() and not overwrite:\n         raise FileExistsError(f\"The file {report_file_path} already exists! Set 'overwrite=True' or pass '-o' flag.\")\n+\n     with open(file=report_file_path, mode=\"w\", newline=\"\\n\") as file:\n         for line in formatted_messages:\n             file.write(line + \"\\n\")\n+\n+    return None\ndiff --git a/src/nwbinspector/_nwb_inspection.py b/src/nwbinspector/_nwb_inspection.py\nindex 55406998e..0c7b95669 100644\n--- a/src/nwbinspector/_nwb_inspection.py\n+++ b/src/nwbinspector/_nwb_inspection.py\n@@ -12,12 +12,10 @@\n from natsort import natsorted\n from tqdm import tqdm\n \n-from . import available_checks\n from ._configuration import configure_checks\n-from ._registration import Importance, InspectorMessage\n-from .tools._read_nwbfile import read_nwbfile\n+from ._registration import Importance, InspectorMessage, available_checks\n+from .tools._read_nwbfile import read_nwbfile, read_nwbfile_and_io\n from .utils import (\n-    FilePathType,\n     OptionalListOfStrings,\n     PathType,\n     calculate_number_of_cpu,\n@@ -38,7 +36,7 @@ def inspect_all(\n     stream: bool = False,  # TODO: remove after 3/1/2025\n     version_id: Optional[str] = None,  # TODO: remove after 3/1/2025\n     modules: OptionalListOfStrings = None,\n-):\n+) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Inspect a local NWBFile or folder of NWBFiles and return suggestions for improvements according to best practices.\n \n@@ -98,22 +96,22 @@ def inspect_all(\n \n     # TODO: remove these blocks after 3/1/2025\n     if version_id is not None:\n-        message = (\n+        deprecation_message = (\n             \"The `version_id` argument is deprecated and will be removed after 3/1/2025. \"\n             \"Please call `nwbinspector.inspect_dandiset` with the argument `dandiset_version` instead.\"\n         )\n-        warn(message=message, category=DeprecationWarning, stacklevel=2)\n+        warn(message=deprecation_message, category=DeprecationWarning, stacklevel=2)\n     if stream:\n         from ._dandi_inspection import inspect_dandiset\n \n-        message = (\n+        warning_message = (\n             \"The `stream` argument is deprecated and will be removed after 3/1/2025. \"\n             \"Please call `nwbinspector.inspect_dandiset` instead.\"\n         )\n-        warn(message=message, category=DeprecationWarning, stacklevel=2)\n+        warn(message=warning_message, category=DeprecationWarning, stacklevel=2)\n \n         for message in inspect_dandiset(\n-            dandiset_id=path,\n+            dandiset_id=str(path),\n             dandiset_version=version_id,\n             config=config,\n             ignore=ignore,\n@@ -124,7 +122,7 @@ def inspect_all(\n \n         return None\n \n-    n_jobs = calculate_number_of_cpu(requested_cpu=n_jobs)\n+    calculated_number_of_jobs = calculate_number_of_cpu(requested_cpu=n_jobs)\n     if progress_bar_options is None:\n         progress_bar_options = dict(position=0, leave=False)\n \n@@ -151,7 +149,7 @@ def inspect_all(\n                 message=traceback.format_exc(),\n                 importance=Importance.ERROR,\n                 check_function_name=f\"During io.read() - {type(exception)}: {str(exception)}\",\n-                file_path=nwbfile_path,\n+                file_path=str(nwbfile_path),\n             )\n \n     if len(identifiers) != len(nwbfiles):\n@@ -175,54 +173,57 @@ def inspect_all(\n     nwbfiles_iterable = nwbfiles\n     if progress_bar:\n         nwbfiles_iterable = progress_bar_class(nwbfiles_iterable, **progress_bar_options)\n-    if n_jobs != 1:\n+    if calculated_number_of_jobs == 1:\n+        for nwbfile_path in nwbfiles_iterable:  # type: ignore\n+            for message in inspect_nwbfile(nwbfile_path=nwbfile_path, checks=checks, skip_validate=skip_validate):\n+                yield message\n+    else:\n         progress_bar_options.update(total=len(nwbfiles))\n         futures = []\n-        n_jobs = None if n_jobs == -1 else n_jobs  # concurrents uses None instead of -1 for 'auto' mode\n-        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n+        # concurrents uses None instead of -1 for 'auto' mode\n+        max_workers = None if calculated_number_of_jobs == -1 else calculated_number_of_jobs\n+        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n             for nwbfile_path in nwbfiles:\n                 futures.append(\n                     executor.submit(\n                         _pickle_inspect_nwb,\n-                        nwbfile_path=nwbfile_path,\n+                        nwbfile_path=str(nwbfile_path),\n                         checks=checks,\n                         skip_validate=skip_validate,\n                     )\n                 )\n-            nwbfiles_iterable = as_completed(futures)\n+            async_nwbfiles_iterable = as_completed(futures)\n             if progress_bar:\n-                nwbfiles_iterable = progress_bar_class(nwbfiles_iterable, **progress_bar_options)\n-            for future in nwbfiles_iterable:\n+                async_nwbfiles_iterable = progress_bar_class(async_nwbfiles_iterable, **progress_bar_options)\n+            for future in async_nwbfiles_iterable:\n                 for message in future.result():\n                     if stream:\n                         message.file_path = nwbfiles[message.file_path]\n                     yield message\n-    else:\n-        for nwbfile_path in nwbfiles_iterable:\n-            for message in inspect_nwbfile(nwbfile_path=nwbfile_path, checks=checks, skip_validate=skip_validate):\n-                yield message\n \n \n def _pickle_inspect_nwb(\n     nwbfile_path: str,\n-    checks: list = available_checks,\n+    checks: Optional[list] = None,\n     skip_validate: bool = False,\n-):\n+) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"Auxiliary function for inspect_all to run in parallel using the ProcessPoolExecutor.\"\"\"\n+    checks = checks or available_checks\n+\n     return list(inspect_nwbfile(nwbfile_path=nwbfile_path, checks=checks, skip_validate=skip_validate))\n \n \n def inspect_nwbfile(\n-    nwbfile_path: FilePathType,\n+    nwbfile_path: Union[str, Path],\n     driver: Optional[str] = None,  # TODO: remove after 3/1/2025\n     skip_validate: bool = False,\n     max_retries: Optional[int] = None,  # TODO: remove after 3/1/2025\n-    checks: list = available_checks,\n-    config: dict = None,\n+    checks: Optional[list] = None,\n+    config: Optional[dict] = None,\n     ignore: OptionalListOfStrings = None,\n     select: OptionalListOfStrings = None,\n     importance_threshold: Union[str, Importance] = Importance.BEST_PRACTICE_SUGGESTION,\n-) -> Iterable[InspectorMessage]:\n+) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Open an NWB file, inspect the contents, and return suggestions for improvements according to best practices.\n \n@@ -256,6 +257,7 @@ def inspect_nwbfile(\n \n         The default is the lowest level, BEST_PRACTICE_SUGGESTION.\n     \"\"\"\n+    checks = checks or available_checks\n     # TODO: remove error after 3/1/2025\n     if driver is not None or max_retries is not None:\n         message = (\n@@ -269,7 +271,7 @@ def inspect_nwbfile(\n     filterwarnings(action=\"ignore\", message=\"Ignoring cached namespace .*\")\n \n     try:\n-        in_memory_nwbfile, io = read_nwbfile(nwbfile_path=nwbfile_path, return_io=True)\n+        in_memory_nwbfile, io = read_nwbfile_and_io(nwbfile_path=nwbfile_path)\n \n         if not skip_validate:\n             validation_errors = pynwb.validate(io=io)\n@@ -290,7 +292,7 @@ def inspect_nwbfile(\n             select=select,\n             importance_threshold=importance_threshold,\n         ):\n-            inspector_message.file_path = nwbfile_path\n+            inspector_message.file_path = nwbfile_path  # type: ignore\n             yield inspector_message\n     except Exception as exception:\n         yield InspectorMessage(\n@@ -302,13 +304,15 @@ def inspect_nwbfile(\n \n \n # TODO: deprecate once subject types and dandi schemas have been extended\n-def _intercept_in_vitro_protein(nwbfile_object: pynwb.NWBFile, checks: Optional[list] = None) -> list[callable]:\n+def _intercept_in_vitro_protein(nwbfile_object: pynwb.NWBFile, checks: Optional[list] = None) -> list:\n     \"\"\"\n     If the special 'protein' subject_id is specified, return a truncated list of checks to run.\n \n     This is a temporary method for allowing upload of certain in vitro data to DANDI and\n     is expected to be replaced in future versions.\n     \"\"\"\n+    checks = checks or available_checks\n+\n     subject_related_check_names = [\n         \"check_subject_exists\",\n         \"check_subject_id_exists\",\n@@ -319,7 +323,9 @@ def _intercept_in_vitro_protein(nwbfile_object: pynwb.NWBFile, checks: Optional[\n         \"check_subject_proper_age_range\",\n     ]\n     subject_related_dandi_requirements = [\n-        check.importance == Importance.CRITICAL for check in checks if check.__name__ in subject_related_check_names\n+        check.importance == Importance.CRITICAL  # type: ignore\n+        for check in checks\n+        if check.__name__ in subject_related_check_names\n     ]\n \n     subject = getattr(nwbfile_object, \"subject\", None)\n@@ -330,6 +336,7 @@ def _intercept_in_vitro_protein(nwbfile_object: pynwb.NWBFile, checks: Optional[\n     ):\n         non_subject_checks = [check for check in checks if check.__name__ not in subject_related_check_names]\n         return non_subject_checks\n+\n     return checks\n \n \n@@ -340,7 +347,7 @@ def inspect_nwbfile_object(\n     ignore: Optional[list[str]] = None,\n     select: Optional[list[str]] = None,\n     importance_threshold: Union[str, Importance] = Importance.BEST_PRACTICE_SUGGESTION,\n-) -> Iterable[InspectorMessage]:\n+) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Inspect an in-memory NWBFile object and return suggestions for improvements according to best practices.\n \n@@ -391,7 +398,7 @@ def run_checks(\n     checks: list,\n     progress_bar_class: Optional[Type[tqdm]] = None,\n     progress_bar_options: Optional[dict] = None,\n-) -> Iterable[InspectorMessage]:\n+) -> Iterable[Union[InspectorMessage, None]]:\n     \"\"\"\n     Run checks on an open NWBFile object.\n \n@@ -436,7 +443,7 @@ def run_checks(\n                     message=traceback.format_exc(),\n                     importance=Importance.ERROR,\n                     check_function_name=check_function_name,\n-                    file_path=nwbfile_path,\n+                    file_path=\"unknown\",\n                 )\n             if isinstance(output, InspectorMessage):\n                 # temporary solution to https://github.com/dandi/dandi-cli/issues/1031\ndiff --git a/src/nwbinspector/_nwbinspector_cli.py b/src/nwbinspector/_nwbinspector_cli.py\nindex 6e9fa5678..db42f5c64 100644\n--- a/src/nwbinspector/_nwbinspector_cli.py\n+++ b/src/nwbinspector/_nwbinspector_cli.py\n@@ -12,6 +12,7 @@\n from ._configuration import load_config\n from ._dandi_inspection import inspect_dandi_file_path, inspect_dandiset, inspect_url\n from ._formatting import (\n+    InspectorOutputJSONEncoder,\n     _get_report_header,\n     format_messages,\n     print_to_console,\n@@ -89,9 +90,9 @@ def _nwbinspector_cli(\n     path: str,\n     stream: bool = False,\n     version_id: Union[str, None] = None,\n-    report_file_path: str = None,\n+    report_file_path: Union[str, None] = None,\n     config: Union[str, None] = None,\n-    levels: str = None,\n+    levels: Union[str, None] = None,\n     reverse: Union[str, None] = None,\n     overwrite: bool = False,\n     ignore: Union[str, None] = None,\n@@ -137,16 +138,17 @@ def _nwbinspector_cli(\n     handled_select = select if select is None else select.split(\",\")\n     handled_importance_threshold = Importance[threshold]\n     show_progress_bar = True if progress_bar is None else strtobool(progress_bar)\n-    modules = [] if modules is None else modules.split(\",\")\n+    handled_modules = [] if modules is None else modules.split(\",\")\n \n     # Trigger the import of custom checks that have been registered and exposed to their respective modules\n-    for module in modules:\n+    for module in handled_modules:\n         importlib.import_module(name=module)\n \n     # Scan entire Dandiset\n     if stream and \":\" not in path:\n         dandiset_id = path\n         dandiset_version = version_id\n+\n         messages_iterator = inspect_dandiset(\n             dandiset_id=dandiset_id,\n             dandiset_version=dandiset_version,\n@@ -160,6 +162,8 @@ def _nwbinspector_cli(\n     # Scan a single NWB file in a Dandiset\n     elif stream and \":\" in path:\n         dandiset_id, dandi_file_path = path.split(\":\")\n+        dandiset_version = version_id\n+\n         messages_iterator = inspect_dandi_file_path(\n             dandi_file_path=dandi_file_path,\n             dandiset_id=dandiset_id,\n@@ -173,8 +177,9 @@ def _nwbinspector_cli(\n     # Scan single NWB file at URL\n     elif stream and path_is_url:\n         dandi_s3_url = path\n+\n         messages_iterator = inspect_url(\n-            dandi_s3_url=dandi_s3_url,\n+            url=dandi_s3_url,\n             config=handled_config,\n             ignore=handled_ignore,\n             select=handled_select,\ndiff --git a/src/nwbinspector/_organization.py b/src/nwbinspector/_organization.py\nindex 9caaee5e3..ed2c4d7fb 100644\n--- a/src/nwbinspector/_organization.py\n+++ b/src/nwbinspector/_organization.py\n@@ -8,7 +8,7 @@\n from ._registration import InspectorMessage\n \n \n-def _sort_unique_values(unique_values: list, reverse: bool = False):\n+def _sort_unique_values(unique_values: list, reverse: bool = False) -> list:\n     \"\"\"Technically, the 'set' method applies basic sorting to the unique contents, but natsort is more general.\"\"\"\n     if any(unique_values) and isinstance(unique_values[0], Enum):\n         return natsorted(unique_values, key=lambda x: -x.value, reverse=reverse)\n@@ -16,7 +16,9 @@ def _sort_unique_values(unique_values: list, reverse: bool = False):\n         return natsorted(unique_values, reverse=reverse)\n \n \n-def organize_messages(messages: list[InspectorMessage], levels: list[str], reverse: Optional[list[bool]] = None):\n+def organize_messages(\n+    messages: list[Optional[InspectorMessage]], levels: list[str], reverse: Optional[list[bool]] = None\n+) -> dict:\n     \"\"\"\n     General function for organizing list of InspectorMessages.\n \n@@ -25,9 +27,12 @@ def organize_messages(messages: list[InspectorMessage], levels: list[str], rever\n     Parameters\n     ----------\n     messages : list of InspectorMessages\n-    levels: list of strings\n+    levels : list of strings\n         Each string in this list must correspond onto an attribute of the InspectorMessage class, excluding the\n         'message' text and 'object_name' (this will be coupled to the 'object_type').\n+    reverse : list of bool, optional\n+        If provided, this should be a list of booleans that correspond to the 'levels' argument.\n+        If True, the values will be sorted in reverse order.\n     \"\"\"\n     assert all([x not in levels for x in [\"message\", \"object_name\", \"severity\"]]), (\n         \"You must specify levels to organize by that correspond to attributes of the InspectorMessage class, excluding \"\ndiff --git a/src/nwbinspector/_registration.py b/src/nwbinspector/_registration.py\nindex 5ce7fa515..f43e5caf6 100644\n--- a/src/nwbinspector/_registration.py\n+++ b/src/nwbinspector/_registration.py\n@@ -1,13 +1,14 @@\n \"\"\"Primary decorator used on a check function to add it to the registry and automatically parse its output.\"\"\"\n \n from functools import wraps\n-from typing import Optional\n+from typing import List, Optional, Union\n \n import h5py\n import zarr\n from pynwb import NWBFile\n from pynwb.ecephys import Device, ElectrodeGroup\n from pynwb.file import Subject\n+from typing_extensions import Callable\n \n from ._types import Importance, InspectorMessage, Severity\n \n@@ -16,7 +17,7 @@\n \n # TODO: neurodata_type could have annotation hdmf.utils.ExtenderMeta, which seems to apply to all currently checked\n # objects. We can wait and see how well that holds up before adding it in officially.\n-def register_check(importance: Importance, neurodata_type):\n+def register_check(importance: Importance, neurodata_type: object) -> Callable:\n     \"\"\"\n     Wrap a check function with this decorator to add it to the check registry and automatically parse some output.\n \n@@ -36,7 +37,7 @@ def register_check(importance: Importance, neurodata_type):\n         If this check is intended to apply to any general NWBFile object, set neurodata_type to None.\n     \"\"\"\n \n-    def register_check_and_auto_parse(check_function):\n+    def register_check_and_auto_parse(check_function: Callable) -> Callable:\n         if importance not in [\n             Importance.CRITICAL,\n             Importance.BEST_PRACTICE_VIOLATION,\n@@ -47,23 +48,30 @@ def register_check_and_auto_parse(check_function):\n                 \"importance level! Please choose one of Importance.CRITICAL, Importance.BEST_PRACTICE_VIOLATION, \"\n                 \"or Importance.BEST_PRACTICE_SUGGESTION.\"\n             )\n-        check_function.importance = importance\n-        check_function.neurodata_type = neurodata_type\n+        check_function.importance = importance  # type: ignore\n+        check_function.neurodata_type = neurodata_type  # type: ignore\n \n         @wraps(check_function)\n-        def auto_parse_some_output(*args, **kwargs) -> InspectorMessage:\n+        def auto_parse_some_output(\n+            *args, **kwargs\n+        ) -> Union[InspectorMessage, List[Union[InspectorMessage, None]], None]:\n             if args:\n-                obj = args[0]\n+                neurodata_object = args[0]\n             else:\n-                obj = kwargs[list(kwargs)[0]]\n+                neurodata_object = kwargs[list(kwargs)[0]]\n             output = check_function(*args, **kwargs)\n-            auto_parsed_result = None\n+\n+            auto_parsed_result: Union[InspectorMessage, List[Union[InspectorMessage, None]], None] = None\n             if isinstance(output, InspectorMessage):\n-                auto_parsed_result = _auto_parse(check_function=check_function, obj=obj, result=output)\n+                auto_parsed_result = _auto_parse(\n+                    check_function=check_function, neurodata_object=neurodata_object, result=output\n+                )\n             elif output is not None:\n                 auto_parsed_result = list()\n                 for result in output:\n-                    auto_parsed_result.append(_auto_parse(check_function=check_function, obj=obj, result=result))\n+                    auto_parsed_result.append(\n+                        _auto_parse(check_function=check_function, neurodata_object=neurodata_object, result=result)\n+                    )\n                 if not any(auto_parsed_result):\n                     auto_parsed_result = None\n             return auto_parsed_result\n@@ -75,7 +83,9 @@ def auto_parse_some_output(*args, **kwargs) -> InspectorMessage:\n     return register_check_and_auto_parse\n \n \n-def _auto_parse(check_function, obj, result: Optional[InspectorMessage] = None):\n+def _auto_parse(\n+    check_function: Callable, neurodata_object: object, result: Optional[InspectorMessage] = None\n+) -> Optional[InspectorMessage]:\n     \"\"\"Automatically fill values in the InspectorMessage from the check function.\"\"\"\n     if result is not None:\n         auto_parsed_result = result\n@@ -85,21 +95,24 @@ def _auto_parse(check_function, obj, result: Optional[InspectorMessage] = None):\n                 f\"({check_function.__name__}) is not a valid severity level! Please choose one of \"\n                 \"Severity.HIGH, Severity.LOW, or do not specify any severity.\"\n             )\n-        auto_parsed_result.importance = check_function.importance\n+        auto_parsed_result.importance = check_function.importance  # type: ignore\n         auto_parsed_result.check_function_name = check_function.__name__\n-        auto_parsed_result.object_type = type(obj).__name__\n-        auto_parsed_result.object_name = obj.name\n-        auto_parsed_result.location = _parse_location(neurodata_object=obj)\n+        auto_parsed_result.object_type = type(neurodata_object).__name__\n+        auto_parsed_result.object_name = neurodata_object.name  # type: ignore\n+        auto_parsed_result.location = _parse_location(neurodata_object=neurodata_object)\n+\n         return auto_parsed_result\n \n+    return None\n \n-def _parse_location(neurodata_object) -> Optional[str]:\n+\n+def _parse_location(neurodata_object: object) -> Optional[str]:\n     \"\"\"Grab the object location from a dataset or a container content that is an dataset object.\"\"\"\n     known_locations = {\n         NWBFile: \"/\",\n         Subject: \"/general/subject\",\n-        Device: f\"/general/devices/{neurodata_object.name}\",\n-        ElectrodeGroup: f\"/general/extracellular_ephys/{neurodata_object.name}\",\n+        Device: f\"/general/devices/{neurodata_object.name}\",  # type: ignore\n+        ElectrodeGroup: f\"/general/extracellular_ephys/{neurodata_object.name}\",  # type: ignore\n         # TODO: add ophys equivalents\n     }\n \n@@ -108,14 +121,16 @@ def _parse_location(neurodata_object) -> Optional[str]:\n             return val\n \n     # Infer the human-readable path of the object within an NWBFile by tracing its parents\n-    if neurodata_object.parent is None:\n+    if neurodata_object.parent is None:  # type: ignore\n         return \"/\"\n     # Best solution: object is or has a HDF5 Dataset\n     if isinstance(neurodata_object, (h5py.Dataset, zarr.Array)):\n-        return neurodata_object.name\n+        return neurodata_object.name  # type: ignore\n     else:\n-        for field_name, field in neurodata_object.fields.items():\n+        for field_name, field in neurodata_object.fields.items():  # type: ignore\n             if isinstance(field, h5py.Dataset):\n-                return field.parent.name\n+                return field.parent.name  # type: ignore\n             elif isinstance(field, zarr.Array):\n                 return field.name.removesuffix(f\"/{field_name}\")\n+\n+    return None\ndiff --git a/src/nwbinspector/_types.py b/src/nwbinspector/_types.py\nindex a329a979d..d1aebd418 100644\n--- a/src/nwbinspector/_types.py\n+++ b/src/nwbinspector/_types.py\n@@ -62,11 +62,11 @@ class InspectorMessage:\n     message: str\n     importance: Importance = Importance.BEST_PRACTICE_SUGGESTION\n     severity: Severity = Severity.LOW\n-    check_function_name: str = None\n-    object_type: str = None\n-    object_name: str = None\n+    check_function_name: Optional[str] = None\n+    object_type: Optional[str] = None\n+    object_name: Optional[str] = None\n     location: Optional[str] = None\n-    file_path: str = None\n+    file_path: Optional[str] = None\n \n     def __repr__(self):\n         \"\"\"Representation for InspectorMessage objects according to black format.\"\"\"\ndiff --git a/src/nwbinspector/checks/_behavior.py b/src/nwbinspector/checks/_behavior.py\nindex fe5b9cfb1..bca5af861 100644\n--- a/src/nwbinspector/checks/_behavior.py\n+++ b/src/nwbinspector/checks/_behavior.py\n@@ -1,5 +1,7 @@\n \"\"\"Checks for types belonging to the pynwb.behavior module.\"\"\"\n \n+from typing import Iterable, Optional\n+\n import numpy as np\n from pynwb.behavior import CompassDirection, SpatialSeries\n \n@@ -7,7 +9,7 @@\n \n \n @register_check(importance=Importance.CRITICAL, neurodata_type=SpatialSeries)\n-def check_spatial_series_dims(spatial_series: SpatialSeries):\n+def check_spatial_series_dims(spatial_series: SpatialSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if a SpatialSeries has the correct dimensions.\n \n@@ -18,9 +20,11 @@ def check_spatial_series_dims(spatial_series: SpatialSeries):\n             message=\"SpatialSeries should have 1 column (x), 2 columns (x, y), or 3 columns (x, y, z).\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=CompassDirection)\n-def check_compass_direction_unit(compass_direction: CompassDirection):\n+def check_compass_direction_unit(compass_direction: CompassDirection) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check that SpatialSeries objects within CompassDirection objects have units 'degrees' or 'radians'.\n \n@@ -33,9 +37,13 @@ def check_compass_direction_unit(compass_direction: CompassDirection):\n                 f\"unit of 'degrees' or 'radians', but '{spatial_series.name}' has units '{spatial_series.unit}'.\"\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=SpatialSeries)\n-def check_spatial_series_radians_magnitude(spatial_series: SpatialSeries, nelems: int = 200):\n+def check_spatial_series_radians_magnitude(\n+    spatial_series: SpatialSeries, nelems: int = 200\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that SpatialSeries with units radians have data values between -2*pi and 2*pi.\n \n@@ -48,9 +56,13 @@ def check_spatial_series_radians_magnitude(spatial_series: SpatialSeries, nelems\n                 message=\"SpatialSeries with units of radians must have values between -2pi and 2pi.\"\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=SpatialSeries)\n-def check_spatial_series_degrees_magnitude(spatial_series: SpatialSeries, nelems: int = 200):\n+def check_spatial_series_degrees_magnitude(\n+    spatial_series: SpatialSeries, nelems: int = 200\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that SpatialSeries with units of degrees have data values between -360 and 360.\n \n@@ -62,3 +74,5 @@ def check_spatial_series_degrees_magnitude(spatial_series: SpatialSeries, nelems\n             return InspectorMessage(\n                 message=\"SpatialSeries with units of degrees must have values between -360 and 360.\"\n             )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_ecephys.py b/src/nwbinspector/checks/_ecephys.py\nindex 5487c04cc..52cd46a72 100644\n--- a/src/nwbinspector/checks/_ecephys.py\n+++ b/src/nwbinspector/checks/_ecephys.py\n@@ -1,5 +1,7 @@\n \"\"\"Check functions specific to extracellular electrophysiology neurodata types.\"\"\"\n \n+from typing import Optional\n+\n import numpy as np\n from pynwb.ecephys import ElectricalSeries\n from pynwb.misc import Units\n@@ -9,10 +11,10 @@\n \n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Units)\n-def check_negative_spike_times(units_table: Units):\n+def check_negative_spike_times(units_table: Units) -> Optional[InspectorMessage]:\n     \"\"\"Check if the Units table contains negative spike times.\"\"\"\n     if \"spike_times\" not in units_table:\n-        return\n+        return None\n     if np.any(np.asarray(units_table[\"spike_times\"].target.data[:]) < 0):\n         return InspectorMessage(\n             message=(\n@@ -21,9 +23,11 @@ def check_negative_spike_times(units_table: Units):\n             )\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=ElectricalSeries)\n-def check_electrical_series_dims(electrical_series: ElectricalSeries):\n+def check_electrical_series_dims(electrical_series: ElectricalSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     Use the length of the linked electrode region to check the data orientation.\n \n@@ -47,9 +51,13 @@ def check_electrical_series_dims(electrical_series: ElectricalSeries):\n             )\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=ElectricalSeries)\n-def check_electrical_series_reference_electrodes_table(electrical_series: ElectricalSeries):\n+def check_electrical_series_reference_electrodes_table(\n+    electrical_series: ElectricalSeries,\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that the 'electrodes' of an ElectricalSeries references the ElectrodesTable.\n \n@@ -58,12 +66,14 @@ def check_electrical_series_reference_electrodes_table(electrical_series: Electr\n     if electrical_series.electrodes.table.name != \"electrodes\":\n         return InspectorMessage(message=\"electrodes does not  reference an electrodes table.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Units)\n-def check_spike_times_not_in_unobserved_interval(units_table: Units, nunits: int = 4):\n+def check_spike_times_not_in_unobserved_interval(units_table: Units, nunits: int = 4) -> Optional[InspectorMessage]:\n     \"\"\"Check if a Units table has spike times that occur outside of observed intervals.\"\"\"\n     if not units_table.obs_intervals:\n-        return\n+        return None\n     for unit_spike_times, unit_obs_intervals in zip(\n         units_table[\"spike_times\"][:nunits], units_table[\"obs_intervals\"][:nunits]\n     ):\n@@ -82,3 +92,5 @@ def check_spike_times_not_in_unobserved_interval(units_table: Units, nunits: int\n                     \"observed intervals.\"\n                 )\n             )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_general.py b/src/nwbinspector/checks/_general.py\nindex dd5c3afb6..f9b0b9563 100644\n--- a/src/nwbinspector/checks/_general.py\n+++ b/src/nwbinspector/checks/_general.py\n@@ -1,27 +1,33 @@\n \"\"\"Check functions that examine any general neurodata_type with the available attributes.\"\"\"\n \n+from typing import Optional\n+\n from .._registration import Importance, InspectorMessage, register_check\n \n COMMON_DESCRIPTION_PLACEHOLDERS = [\"no description\", \"no desc\", \"none\", \"placeholder\"]\n \n \n @register_check(importance=Importance.CRITICAL, neurodata_type=None)\n-def check_name_slashes(obj):\n+def check_name_slashes(neurodata_object: object) -> Optional[InspectorMessage]:\n     \"\"\"Check if there  has been added for the session.\"\"\"\n-    if hasattr(obj, \"name\") and any((x in obj.name for x in [\"/\", \"\\\\\"])):\n+    if hasattr(neurodata_object, \"name\") and any((x in neurodata_object.name for x in [\"/\", \"\\\\\"])):\n         return InspectorMessage(message=\"Object name contains slashes.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=None)\n-def check_description(obj):\n+def check_description(neurodata_object: object) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the description is a not missing or a placeholder.\n \n     Best Practice: :ref:`best_practice_placeholders`\n     \"\"\"\n-    if not hasattr(obj, \"description\"):\n-        return\n-    if obj.description is None or obj.description.strip(\" \") == \"\":\n+    if not hasattr(neurodata_object, \"description\"):\n+        return None\n+    if neurodata_object.description is None or neurodata_object.description.strip(\" \") == \"\":\n         return InspectorMessage(message=\"Description is missing.\")\n-    if obj.description.lower().strip(\".\") in COMMON_DESCRIPTION_PLACEHOLDERS:\n-        return InspectorMessage(message=f\"Description ('{obj.description}') is a placeholder.\")\n+    if neurodata_object.description.lower().strip(\".\") in COMMON_DESCRIPTION_PLACEHOLDERS:\n+        return InspectorMessage(message=f\"Description ('{neurodata_object.description}') is a placeholder.\")\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_icephys.py b/src/nwbinspector/checks/_icephys.py\nindex 02cf1d319..5fd02e8fd 100644\n--- a/src/nwbinspector/checks/_icephys.py\n+++ b/src/nwbinspector/checks/_icephys.py\n@@ -1,12 +1,16 @@\n \"\"\"Checks specific to intracellular electrophysiology neurodata types.\"\"\"\n \n+from typing import Optional\n+\n from pynwb.icephys import IntracellularElectrode\n \n from .._registration import Importance, InspectorMessage, register_check\n \n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=IntracellularElectrode)\n-def check_intracellular_electrode_cell_id_exists(intracellular_electrode: IntracellularElectrode):\n+def check_intracellular_electrode_cell_id_exists(\n+    intracellular_electrode: IntracellularElectrode,\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the IntracellularElectrode contains a cell_id.\n \n@@ -14,3 +18,5 @@ def check_intracellular_electrode_cell_id_exists(intracellular_electrode: Intrac\n     \"\"\"\n     if hasattr(intracellular_electrode, \"cell_id\") and intracellular_electrode.cell_id is None:\n         return InspectorMessage(message=\"Please include a unique cell_id associated with this IntracellularElectrode.\")\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_image_series.py b/src/nwbinspector/checks/_image_series.py\nindex ad5a76757..f59e9f5a1 100644\n--- a/src/nwbinspector/checks/_image_series.py\n+++ b/src/nwbinspector/checks/_image_series.py\n@@ -2,6 +2,7 @@\n \n import ntpath\n from pathlib import Path\n+from typing import Iterable, Optional\n \n from pynwb.image import ImageSeries\n from pynwb.ophys import TwoPhotonSeries\n@@ -11,15 +12,15 @@\n \n \n @register_check(importance=Importance.CRITICAL, neurodata_type=ImageSeries)\n-def check_image_series_external_file_valid(image_series: ImageSeries):\n+def check_image_series_external_file_valid(image_series: ImageSeries) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check if the external_file specified by an ImageSeries actually exists.\n \n     Best Practice: :ref:`best_practice_use_external_mode`\n     \"\"\"\n     if image_series.external_file is None:\n-        return\n-    nwbfile_path = Path(get_nwbfile_path_from_internal_object(obj=image_series))\n+        return None\n+    nwbfile_path = Path(get_nwbfile_path_from_internal_object(neurodata_object=image_series))\n     for file_path in image_series.external_file:\n         file_path = file_path.decode() if isinstance(file_path, bytes) else file_path\n         if not Path(file_path).is_absolute() and not (nwbfile_path.parent / file_path).exists():\n@@ -30,16 +31,18 @@ def check_image_series_external_file_valid(image_series: ImageSeries):\n                 )\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=ImageSeries)\n-def check_image_series_external_file_relative(image_series: ImageSeries):\n+def check_image_series_external_file_relative(image_series: ImageSeries) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check if the external_file specified by an ImageSeries, if it exists, is relative.\n \n     Best Practice: :ref:`best_practice_use_external_mode`\n     \"\"\"\n     if image_series.external_file is None:\n-        return\n+        return None\n     for file_path in image_series.external_file:\n         file_path = file_path.decode() if isinstance(file_path, bytes) else file_path\n         if ntpath.isabs(file_path):  # ntpath required for cross-platform detection\n@@ -50,9 +53,11 @@ def check_image_series_external_file_relative(image_series: ImageSeries):\n                 )\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=ImageSeries)\n-def check_image_series_data_size(image_series: ImageSeries, gb_lower_bound: float = 20.0):\n+def check_image_series_data_size(image_series: ImageSeries, gb_lower_bound: float = 20.0) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if an ImageSeries stored is larger than gb_lower_bound and suggests external file.\n \n@@ -61,7 +66,7 @@ def check_image_series_data_size(image_series: ImageSeries, gb_lower_bound: floa\n     # False positive case; TwoPhotonSeries are a subclass of ImageSeries, but it is very common and perfectly fine\n     # to write lots of data using one without an external file\n     if isinstance(image_series, TwoPhotonSeries):\n-        return\n+        return None\n \n     data = image_series.data\n \n@@ -72,3 +77,5 @@ def check_image_series_data_size(image_series: ImageSeries, gb_lower_bound: floa\n \n     if data_size_gb > gb_lower_bound:\n         return InspectorMessage(message=\"ImageSeries is very large. Consider using external mode for better storage.\")\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_images.py b/src/nwbinspector/checks/_images.py\nindex a68f375e7..4ae9b419e 100644\n--- a/src/nwbinspector/checks/_images.py\n+++ b/src/nwbinspector/checks/_images.py\n@@ -1,5 +1,7 @@\n \"\"\"Checks specific to the Images neurodata type.\"\"\"\n \n+from typing import Optional\n+\n from pynwb.base import Images\n from pynwb.image import IndexSeries\n \n@@ -7,30 +9,36 @@\n \n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Images)\n-def check_order_of_images_unique(images: Images):\n+def check_order_of_images_unique(images: Images) -> Optional[InspectorMessage]:\n     \"\"\"Check that all the values in the order_of_images field of an Images object are unique.\"\"\"\n     if images.order_of_images is None:\n-        return\n+        return None\n     if not len(set(images.order_of_images)) == len(images.order_of_images):\n         return InspectorMessage(message=\"order_of_images should have unique values.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Images)\n-def check_order_of_images_len(images: Images):\n+def check_order_of_images_len(images: Images) -> Optional[InspectorMessage]:\n     \"\"\"Check that all the values in the order_of_images field of an Images object are unique.\"\"\"\n     if images.order_of_images is None:\n-        return\n+        return None\n     if not len(images.order_of_images) == len(images.images):\n         return InspectorMessage(\n             message=f\"Length of order_of_images ({len(images.order_of_images)}) does not match the number of \"\n             f\"images ({len(images.images)}).\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=IndexSeries)\n-def check_index_series_points_to_image(index_series: IndexSeries):\n+def check_index_series_points_to_image(index_series: IndexSeries) -> Optional[InspectorMessage]:\n     if index_series.indexed_timeseries is not None:\n         return InspectorMessage(\n             message=\"Pointing an IndexSeries to a TimeSeries will be deprecated. Please point to an Images \"\n             \"container instead.\"\n         )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_nwb_containers.py b/src/nwbinspector/checks/_nwb_containers.py\nindex 2d45c39fa..d48d1706f 100644\n--- a/src/nwbinspector/checks/_nwb_containers.py\n+++ b/src/nwbinspector/checks/_nwb_containers.py\n@@ -1,6 +1,7 @@\n \"\"\"Check functions that can apply to any object inside an NWBContainer.\"\"\"\n \n import os\n+from typing import Iterable, Optional\n \n import h5py\n import zarr\n@@ -10,7 +11,9 @@\n \n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=NWBContainer)\n-def check_large_dataset_compression(nwb_container: NWBContainer, gb_lower_bound: float = 20.0):\n+def check_large_dataset_compression(\n+    nwb_container: NWBContainer, gb_lower_bound: float = 20.0\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     If the data in the Container object is a 'large' h5py.Dataset, check if it has compression enabled.\n \n@@ -35,6 +38,8 @@ def check_large_dataset_compression(nwb_container: NWBContainer, gb_lower_bound:\n                 message=f\"{os.path.split(field.name)[1]} is a large uncompressed dataset! Please enable compression.\",\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBContainer)\n def check_small_dataset_compression(\n@@ -42,7 +47,7 @@ def check_small_dataset_compression(\n     gb_severity_threshold: float = 10.0,\n     mb_lower_bound: float = 50.0,\n     gb_upper_bound: float = 20.0,  # 20 GB upper bound to prevent double-raise\n-):\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     If the data in the Container object is a h5py.Dataset, check if it has compression enabled.\n \n@@ -77,9 +82,11 @@ def check_small_dataset_compression(\n                 ),\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBContainer)\n-def check_empty_string_for_optional_attribute(nwb_container: NWBContainer):\n+def check_empty_string_for_optional_attribute(nwb_container: NWBContainer) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check if any NWBContainer has optional fields that are written as an empty string.\n \n@@ -101,3 +108,5 @@ def check_empty_string_for_optional_attribute(nwb_container: NWBContainer):\n             message=f'The attribute \"{field}\" is optional and you have supplied an empty string. Improve my omitting '\n             \"this attribute (in MatNWB or PyNWB) or entering as None (in PyNWB)\"\n         )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_nwbfile_metadata.py b/src/nwbinspector/checks/_nwbfile_metadata.py\nindex 301fd358a..048395032 100644\n--- a/src/nwbinspector/checks/_nwbfile_metadata.py\n+++ b/src/nwbinspector/checks/_nwbfile_metadata.py\n@@ -2,6 +2,7 @@\n \n import re\n from datetime import datetime\n+from typing import Iterable, Optional\n \n from isodate import Duration, parse_duration\n from pynwb import NWBFile, ProcessingModule\n@@ -20,7 +21,7 @@\n \n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_session_start_time_old_date(nwbfile: NWBFile):\n+def check_session_start_time_old_date(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the session_start_time was set to an appropriate value.\n \n@@ -38,9 +39,11 @@ def check_session_start_time_old_date(nwbfile: NWBFile):\n             message=(f\"The session_start_time ({session_start_time}) may not be set to the true date of the recording.\")\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=NWBFile)\n-def check_session_start_time_future_date(nwbfile: NWBFile):\n+def check_session_start_time_future_date(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the session_start_time was set to an appropriate value.\n \n@@ -57,9 +60,11 @@ def check_session_start_time_future_date(nwbfile: NWBFile):\n             message=f\"The session_start_time ({session_start_time}) is set to a future date and time.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_experimenter_exists(nwbfile: NWBFile):\n+def check_experimenter_exists(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if an experimenter has been added for the session.\n \n@@ -68,16 +73,18 @@ def check_experimenter_exists(nwbfile: NWBFile):\n     if not nwbfile.experimenter:\n         return InspectorMessage(message=\"Experimenter is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_experimenter_form(nwbfile: NWBFile):\n+def check_experimenter_form(nwbfile: NWBFile) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check the text form of each experimenter to see if it matches the DANDI regex pattern.\n \n     Best Practice: :ref:`best_practice_experimenter`\n     \"\"\"\n     if nwbfile.experimenter is None:\n-        return\n+        return None\n     if is_module_installed(module_name=\"dandi\"):\n         from dandischema.models import (\n             NAME_PATTERN,  # for most up to date version of the regex\n@@ -95,42 +102,52 @@ def check_experimenter_form(nwbfile: NWBFile):\n                 )\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_experiment_description(nwbfile: NWBFile):\n+def check_experiment_description(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"Check if a description has been added for the session.\"\"\"\n     if not nwbfile.experiment_description:\n         return InspectorMessage(message=\"Experiment description is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_institution(nwbfile: NWBFile):\n+def check_institution(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"Check if a description has been added for the session.\"\"\"\n     if not nwbfile.institution:\n         return InspectorMessage(message=\"Metadata /general/institution is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_keywords(nwbfile: NWBFile):\n+def check_keywords(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"Check if keywords have been added for the session.\"\"\"\n     if not nwbfile.keywords:\n         return InspectorMessage(message=\"Metadata /general/keywords is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_subject_exists(nwbfile: NWBFile):\n+def check_subject_exists(nwbfile: NWBFile) -> Optional[InspectorMessage]:\n     \"\"\"Check if subject exists.\"\"\"\n     if nwbfile.subject is None:\n         return InspectorMessage(message=\"Subject is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=NWBFile)\n-def check_doi_publications(nwbfile: NWBFile):\n+def check_doi_publications(nwbfile: NWBFile) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"Check if related_publications has been properly added as 'doi: ###' or an external 'doi' link.\"\"\"\n     valid_starts = [\"doi:\", \"http://dx.doi.org/\", \"https://doi.org/\"]\n \n     if not nwbfile.related_publications:\n-        return\n+        return None\n     for publication in nwbfile.related_publications:\n         publication = publication.decode() if isinstance(publication, bytes) else publication\n         if not any((publication.startswith(valid_start) for valid_start in valid_starts)):\n@@ -141,18 +158,20 @@ def check_doi_publications(nwbfile: NWBFile):\n                 )\n             )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=Subject)\n-def check_subject_age(subject: Subject):\n+def check_subject_age(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"Check if the Subject age is in ISO 8601 or our extension of it for ranges.\"\"\"\n     if subject.age is None and subject.date_of_birth is None:\n         return InspectorMessage(\n             message=\"Subject is missing age and date_of_birth. Please specify at least one of these fields.\"\n         )\n     elif subject.age is None and subject.date_of_birth is not None:\n-        return\n+        return None\n     if re.fullmatch(pattern=duration_regex, string=subject.age):\n-        return\n+        return None\n \n     if \"/\" in subject.age:\n         subject_lower_age_bound, subject_upper_age_bound = subject.age.split(\"/\")\n@@ -160,7 +179,7 @@ def check_subject_age(subject: Subject):\n         if re.fullmatch(pattern=duration_regex, string=subject_lower_age_bound) and (\n             re.fullmatch(pattern=duration_regex, string=subject_upper_age_bound) or subject_upper_age_bound == \"\"\n         ):\n-            return\n+            return None\n \n     return InspectorMessage(\n         message=(\n@@ -173,7 +192,7 @@ def check_subject_age(subject: Subject):\n \n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=Subject)\n-def check_subject_proper_age_range(subject: Subject):\n+def check_subject_proper_age_range(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the Subject age, if specified as duration range (e.g., 'P1D/P3D'), has properly increasing bounds.\n \n@@ -201,30 +220,38 @@ def check_subject_proper_age_range(subject: Subject):\n                     )\n                 )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=Subject)\n-def check_subject_id_exists(subject: Subject):\n+def check_subject_id_exists(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"Check if subject_id is defined.\"\"\"\n     if subject.subject_id is None:\n         return InspectorMessage(message=\"subject_id is missing.\")\n \n+    return None\n+\n \n-def _check_subject_sex_defaults(sex: str):\n+def _check_subject_sex_defaults(sex: str) -> Optional[InspectorMessage]:\n     \"\"\"Check if the subject sex has been specified properly for the C. elegans species.\"\"\"\n     if sex not in (\"M\", \"F\", \"O\", \"U\"):\n         return InspectorMessage(\n             message=\"Subject.sex should be one of: 'M' (male), 'F' (female), 'O' (other), or 'U' (unknown).\"\n         )\n \n+    return None\n \n-def _check_subject_sex_c_elegans(sex: str):\n+\n+def _check_subject_sex_c_elegans(sex: str) -> Optional[InspectorMessage]:\n     \"\"\"Check if the subject sex has been specified properly for the C. elegans species.\"\"\"\n     if sex not in (\"XO\", \"XX\"):\n         return InspectorMessage(message=\"For C. elegans, Subject.sex should be 'XO' (male) or 'XX' (hermaphrodite).\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=Subject)\n-def check_subject_sex(subject: Subject):\n+def check_subject_sex(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the subject sex has been specified and ensure that it has has the correct form depending on the species.\n \n@@ -237,9 +264,11 @@ def check_subject_sex(subject: Subject):\n     else:\n         return _check_subject_sex_defaults(sex=subject.sex)\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Subject)\n-def check_subject_species_exists(subject: Subject):\n+def check_subject_species_exists(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the subject species has been specified.\n \n@@ -248,9 +277,11 @@ def check_subject_species_exists(subject: Subject):\n     if not subject.species:\n         return InspectorMessage(message=\"Subject species is missing.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=Subject)\n-def check_subject_species_form(subject: Subject):\n+def check_subject_species_form(subject: Subject) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the subject species follows latin binomial form or is a link to an NCBI taxonomy in the form of a Term IRI.\n \n@@ -266,9 +297,11 @@ def check_subject_species_form(subject: Subject):\n             ),\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=ProcessingModule)\n-def check_processing_module_name(processing_module: ProcessingModule):\n+def check_processing_module_name(processing_module: ProcessingModule) -> Optional[InspectorMessage]:\n     \"\"\"Check if the name of a processing module is of a valid modality.\"\"\"\n     if processing_module.name not in PROCESSING_MODULE_CONFIG:\n         return InspectorMessage(\n@@ -277,3 +310,5 @@ def check_processing_module_name(processing_module: ProcessingModule):\n                 f\"schema module names: {', '.join(PROCESSING_MODULE_CONFIG)}\"\n             )\n         )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_ogen.py b/src/nwbinspector/checks/_ogen.py\nindex 5b0a2af5a..27e8a08cd 100644\n--- a/src/nwbinspector/checks/_ogen.py\n+++ b/src/nwbinspector/checks/_ogen.py\n@@ -1,14 +1,19 @@\n+from typing import Optional\n+\n from pynwb.ogen import OptogeneticSeries, OptogeneticStimulusSite\n \n from .._registration import Importance, InspectorMessage, register_check\n \n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=OptogeneticStimulusSite)\n-def check_optogenetic_stimulus_site_has_optogenetic_series(ogen_site: OptogeneticStimulusSite):\n+def check_optogenetic_stimulus_site_has_optogenetic_series(\n+    ogen_site: OptogeneticStimulusSite,\n+) -> Optional[InspectorMessage]:\n     \"\"\"Check if an optogenetic stimulus site has an optogenetic series linked to it.\"\"\"\n     nwbfile = ogen_site.get_ancestor(\"NWBFile\")\n     for obj in nwbfile.objects.values():\n         if isinstance(obj, OptogeneticSeries):\n             if obj.site == ogen_site:\n-                return\n+                return None\n+\n     return InspectorMessage(message=\"OptogeneticStimulusSite is not referenced by any OptogeneticStimulusSite.\")\ndiff --git a/src/nwbinspector/checks/_ophys.py b/src/nwbinspector/checks/_ophys.py\nindex c28b8119b..38e9f9711 100644\n--- a/src/nwbinspector/checks/_ophys.py\n+++ b/src/nwbinspector/checks/_ophys.py\n@@ -1,5 +1,7 @@\n \"\"\"Check functions specific to optical electrophysiology neurodata types.\"\"\"\n \n+from typing import Iterable, Optional\n+\n from pynwb.ophys import (\n     ImagingPlane,\n     OpticalChannel,\n@@ -14,7 +16,7 @@\n \n \n @register_check(importance=Importance.CRITICAL, neurodata_type=RoiResponseSeries)\n-def check_roi_response_series_dims(roi_response_series: RoiResponseSeries):\n+def check_roi_response_series_dims(roi_response_series: RoiResponseSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check the dimensions of an ROI series to ensure the time axis is the correct dimension.\n \n@@ -37,11 +39,13 @@ def check_roi_response_series_dims(roi_response_series: RoiResponseSeries):\n             message=\"The second dimension of data does not match the length of rois. Your data may be transposed.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=RoiResponseSeries)\n def check_roi_response_series_link_to_plane_segmentation(\n     roi_response_series: RoiResponseSeries,\n-):\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that each ROI response series links to a plane segmentation.\n \n@@ -50,9 +54,11 @@ def check_roi_response_series_link_to_plane_segmentation(\n     if not isinstance(roi_response_series.rois.table, PlaneSegmentation):\n         return InspectorMessage(message=\"rois field does not point to a PlaneSegmentation table.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=OpticalChannel)\n-def check_emission_lambda_in_nm(optical_channel: OpticalChannel):\n+def check_emission_lambda_in_nm(optical_channel: OpticalChannel) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that emission lambda is in feasible range for unit nanometers.\n \n@@ -61,9 +67,11 @@ def check_emission_lambda_in_nm(optical_channel: OpticalChannel):\n     if optical_channel.emission_lambda < MIN_LAMBDA:\n         return InspectorMessage(f\"emission lambda of {optical_channel.emission_lambda} should be in units of nm.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=ImagingPlane)\n-def check_excitation_lambda_in_nm(imaging_plane: ImagingPlane):\n+def check_excitation_lambda_in_nm(imaging_plane: ImagingPlane) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that emission lambda is in feasible range for unit nanometers.\n \n@@ -72,9 +80,13 @@ def check_excitation_lambda_in_nm(imaging_plane: ImagingPlane):\n     if imaging_plane.excitation_lambda < MIN_LAMBDA:\n         return InspectorMessage(f\"excitation lambda of {imaging_plane.excitation_lambda} should be in units of nm.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=PlaneSegmentation)\n-def check_plane_segmentation_image_mask_shape_against_ref_images(plane_segmentation: PlaneSegmentation):\n+def check_plane_segmentation_image_mask_shape_against_ref_images(\n+    plane_segmentation: PlaneSegmentation,\n+) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"Check that image masks and reference images have the same shape.\"\"\"\n     if plane_segmentation.reference_images and \"image_mask\" in plane_segmentation.colnames:\n         mask_shape = plane_segmentation[\"image_mask\"].shape[1:]\n@@ -84,3 +96,5 @@ def check_plane_segmentation_image_mask_shape_against_ref_images(plane_segmentat\n                     f\"image_mask of shape {mask_shape} does not match reference image {ref_image.name} with shape\"\n                     f\" {ref_image.data.shape[1:]}.\"\n                 )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_tables.py b/src/nwbinspector/checks/_tables.py\nindex daf988768..03456febf 100644\n--- a/src/nwbinspector/checks/_tables.py\n+++ b/src/nwbinspector/checks/_tables.py\n@@ -1,7 +1,7 @@\n \"\"\"Check functions that can apply to any descendant of DynamicTable.\"\"\"\n \n from numbers import Real\n-from typing import List, Optional\n+from typing import Iterable, Optional\n \n import numpy as np\n from hdmf.common import DynamicTable, DynamicTableRegion, VectorIndex\n@@ -21,7 +21,9 @@\n \n \n @register_check(importance=Importance.CRITICAL, neurodata_type=DynamicTableRegion)\n-def check_dynamic_table_region_data_validity(dynamic_table_region: DynamicTableRegion, nelems: Optional[int] = NELEMS):\n+def check_dynamic_table_region_data_validity(\n+    dynamic_table_region: DynamicTableRegion, nelems: Optional[int] = NELEMS\n+) -> Optional[InspectorMessage]:\n     \"\"\"Check if a DynamicTableRegion is valid.\"\"\"\n     if np.any(np.asarray(dynamic_table_region.data[:nelems]) > len(dynamic_table_region.table)):\n         return InspectorMessage(\n@@ -35,16 +37,22 @@ def check_dynamic_table_region_data_validity(dynamic_table_region: DynamicTableR\n             message=f\"Some elements of {dynamic_table_region.name} are out of range because they are less than 0.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=DynamicTable)\n-def check_empty_table(table: DynamicTable):\n+def check_empty_table(table: DynamicTable) -> Optional[InspectorMessage]:\n     \"\"\"Check if a DynamicTable is empty.\"\"\"\n     if len(table.id) == 0:\n         return InspectorMessage(message=\"This table has no data added to it.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeIntervals)\n-def check_time_interval_time_columns(time_intervals: TimeIntervals, nelems: Optional[int] = NELEMS):\n+def check_time_interval_time_columns(\n+    time_intervals: TimeIntervals, nelems: Optional[int] = NELEMS\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that time columns are in ascending order.\n \n@@ -69,9 +77,13 @@ def check_time_interval_time_columns(time_intervals: TimeIntervals, nelems: Opti\n             )\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeIntervals)\n-def check_time_intervals_stop_after_start(time_intervals: TimeIntervals, nelems: Optional[int] = NELEMS):\n+def check_time_intervals_stop_after_start(\n+    time_intervals: TimeIntervals, nelems: Optional[int] = NELEMS\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that all stop times on a TimeInterval object occur after their corresponding start times.\n \n@@ -97,9 +109,13 @@ def check_time_intervals_stop_after_start(time_intervals: TimeIntervals, nelems:\n             )\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=DynamicTable)\n-def check_column_binary_capability(table: DynamicTable, nelems: Optional[int] = NELEMS):\n+def check_column_binary_capability(\n+    table: DynamicTable, nelems: Optional[int] = NELEMS\n+) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check each column of a table to see if the data could be set as a boolean dtype.\n \n@@ -138,7 +154,7 @@ def check_column_binary_capability(table: DynamicTable, nelems: Optional[int] =\n                 [\"t\", \"f\"],\n                 [\"hit\", \"miss\"],\n             ]\n-            if any([set(parsed_unique_values) == set(pair) for pair in pairs_to_check]):\n+            if any([set(parsed_unique_values) == set(pair) for pair in pairs_to_check]):  # type: ignore\n                 saved_bytes = (unique_values.dtype.itemsize - 1) * np.product(\n                     get_data_shape(data=column.data, strict_no_data_load=True)\n                 )\n@@ -157,32 +173,40 @@ def check_column_binary_capability(table: DynamicTable, nelems: Optional[int] =\n                         f\"save {format_byte_size(byte_size=saved_bytes)}.\"\n                     )\n                 )\n+    return None\n \n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=DynamicTable)\n def check_single_row(\n     table: DynamicTable,\n-    exclude_types: Optional[list] = (Units,),\n-    exclude_names: Optional[List[str]] = (\"electrodes\",),\n-):\n+    exclude_types: Optional[tuple] = None,\n+    exclude_names: Optional[tuple[str]] = None,\n+) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if DynamicTable has only a single row; may be better represented by another data type.\n \n     Skips the Units table since it is OK to have only a single spiking unit.\n     Skips the Electrode table since it is OK to have only a single electrode.\n     \"\"\"\n+    exclude_types = exclude_types or (Units,)\n+    exclude_names = exclude_names or (\"electrodes\",)\n+\n     if any((isinstance(table, exclude_type) for exclude_type in exclude_types)):\n-        return\n+        return None\n     if any((table.name == exclude_name for exclude_name in exclude_names)):\n-        return\n+        return None\n     if len(table.id) == 1:\n         return InspectorMessage(\n             message=\"This table has only a single row; it may be better represented by another data type.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=DynamicTable)\n-def check_table_values_for_dict(table: DynamicTable, nelems: Optional[int] = NELEMS):\n+def check_table_values_for_dict(\n+    table: DynamicTable, nelems: Optional[int] = NELEMS\n+) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"Check if any values in a row or column of a table contain a string casting of a Python dictionary.\"\"\"\n     for column in table.columns:\n         if not hasattr(column, \"data\") or isinstance(column, VectorIndex) or not isinstance(column.data[0], str):\n@@ -197,9 +221,11 @@ def check_table_values_for_dict(table: DynamicTable, nelems: Optional[int] = NEL\n                     message += \" This string is also JSON loadable, so call `json.loads(...)` on the string to unpack.\"\n                 yield InspectorMessage(message=message)\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=DynamicTable)\n-def check_col_not_nan(table: DynamicTable, nelems: Optional[int] = NELEMS):\n+def check_col_not_nan(table: DynamicTable, nelems: Optional[int] = NELEMS) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"Check if all the values in a single column of a table are NaN.\"\"\"\n     for column in table.columns:\n         if (\n@@ -217,9 +243,11 @@ def check_col_not_nan(table: DynamicTable, nelems: Optional[int] = NELEMS):\n         if all(np.isnan(column[slice(0, None, slice_by)]).flatten()):\n             yield InspectorMessage(message=message)\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=DynamicTable)\n-def check_ids_unique(table: DynamicTable, nelems: Optional[int] = NELEMS):\n+def check_ids_unique(table: DynamicTable, nelems: Optional[int] = NELEMS) -> Optional[InspectorMessage]:\n     \"\"\"\n     Ensure that the values of the id attribute of a DynamicTable are unique.\n \n@@ -239,9 +267,11 @@ def check_ids_unique(table: DynamicTable, nelems: Optional[int] = NELEMS):\n     if len(set(data)) != len(data):\n         return InspectorMessage(message=\"This table has ids that are not unique.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=DynamicTable)\n-def check_table_time_columns_are_not_negative(table: DynamicTable):\n+def check_table_time_columns_are_not_negative(table: DynamicTable) -> Optional[Iterable[InspectorMessage]]:\n     \"\"\"\n     Check that time columns are not negative.\n \n@@ -259,3 +289,5 @@ def check_table_time_columns_are_not_negative(table: DynamicTable):\n                     message=f\"Timestamps in column {column_name} should not be negative.\"\n                     \" It is recommended to align the `session_start_time` or `timestamps_reference_time` to be the earliest time value that occurs in the data, and shift all other signals accordingly.\"\n                 )\n+\n+    return None\ndiff --git a/src/nwbinspector/checks/_time_series.py b/src/nwbinspector/checks/_time_series.py\nindex 4086a6ed5..8956171fd 100644\n--- a/src/nwbinspector/checks/_time_series.py\n+++ b/src/nwbinspector/checks/_time_series.py\n@@ -1,5 +1,7 @@\n \"\"\"Check functions that can apply to any descendant of TimeSeries.\"\"\"\n \n+from typing import Optional\n+\n import numpy as np\n from pynwb import TimeSeries\n from pynwb.image import ImageSeries, IndexSeries\n@@ -11,7 +13,7 @@\n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeSeries)\n def check_regular_timestamps(\n     time_series: TimeSeries, time_tolerance_decimals: int = 9, gb_severity_threshold: float = 1.0\n-):\n+) -> Optional[InspectorMessage]:\n     \"\"\"If the TimeSeries uses timestamps, check if they are regular (i.e., they have a constant rate).\"\"\"\n     if (\n         time_series.timestamps is not None\n@@ -33,14 +35,19 @@ def check_regular_timestamps(\n             ),\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=TimeSeries)\n-def check_data_orientation(time_series: TimeSeries):\n+def check_data_orientation(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     \"\"\"If the TimeSeries has data, check if the longest axis (almost always time) is also the zero-axis.\"\"\"\n     if time_series.data is None:\n-        return\n+        return None\n \n     data_shape = get_data_shape(time_series.data)\n+    if data_shape is None:\n+        return None\n+\n     if any(np.array(data_shape[1:]) > data_shape[0]):\n         return InspectorMessage(\n             message=(\n@@ -49,19 +56,29 @@ def check_data_orientation(time_series: TimeSeries):\n             ),\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=TimeSeries)\n-def check_timestamps_match_first_dimension(time_series: TimeSeries):\n+def check_timestamps_match_first_dimension(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     If the TimeSeries has timestamps, check if their length is the same as the zero-axis of data.\n \n     Best Practice: :ref:`best_practice_data_orientation`\n     \"\"\"\n     if time_series.data is None or time_series.timestamps is None:\n-        return\n+        return None\n+\n+    data_shape = get_data_shape(time_series.data)\n+    if data_shape is None:\n+        return None\n+\n+    timestamps_shape = get_data_shape(time_series.timestamps)\n+    if timestamps_shape is None:\n+        return None\n \n-    if getattr(time_series, \"external_file\", None) is not None and get_data_shape(time_series.data)[0] == 0:\n-        return\n+    if getattr(time_series, \"external_file\", None) is not None and data_shape[0] == 0:\n+        return None\n \n     # A very specific edge case where this has been allowed, though much more preferable\n     # to use a stack of Images rather than an ImageSeries\n@@ -72,10 +89,8 @@ def check_timestamps_match_first_dimension(time_series: TimeSeries):\n     ):\n         for neurodata_object in time_series.get_ancestor(\"NWBFile\").objects.values():\n             if isinstance(neurodata_object, IndexSeries) and neurodata_object.indexed_timeseries == time_series:\n-                return\n+                return None\n \n-    timestamps_shape = get_data_shape(time_series.timestamps)\n-    data_shape = get_data_shape(time_series.data)\n     if data_shape[0] != timestamps_shape[0]:\n         return InspectorMessage(\n             message=(\n@@ -84,23 +99,29 @@ def check_timestamps_match_first_dimension(time_series: TimeSeries):\n             )\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeSeries)\n-def check_timestamps_ascending(time_series: TimeSeries, nelems=200):\n+def check_timestamps_ascending(time_series: TimeSeries, nelems: Optional[int] = 200) -> Optional[InspectorMessage]:\n     \"\"\"Check that the values in the timestamps array are strictly increasing.\"\"\"\n     if time_series.timestamps is not None and not is_ascending_series(time_series.timestamps, nelems=nelems):\n         return InspectorMessage(f\"{time_series.name} timestamps are not ascending.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeSeries)\n-def check_timestamps_without_nans(time_series: TimeSeries, nelems=200):\n+def check_timestamps_without_nans(time_series: TimeSeries, nelems: Optional[int] = 200) -> Optional[InspectorMessage]:\n     \"\"\"Check if there are NaN values in the timestamps array.\"\"\"\n     if time_series.timestamps is not None and np.isnan(time_series.timestamps[:nelems]).any():\n         return InspectorMessage(message=f\"{time_series.name} timestamps contain NaN values.\")\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_SUGGESTION, neurodata_type=TimeSeries)\n-def check_timestamp_of_the_first_sample_is_not_negative(time_series: TimeSeries):\n+def check_timestamp_of_the_first_sample_is_not_negative(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check that the timestamp of the first sample is not negative.\n \n@@ -114,9 +135,11 @@ def check_timestamp_of_the_first_sample_is_not_negative(time_series: TimeSeries)\n             \" It is recommended to align the `session_start_time` or `timestamps_reference_time` to be the earliest time value that occurs in the data, and shift all other signals accordingly.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeSeries)\n-def check_missing_unit(time_series: TimeSeries):\n+def check_missing_unit(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     \"\"\"\n     Check if the TimeSeries.unit field is empty.\n \n@@ -127,24 +150,34 @@ def check_missing_unit(time_series: TimeSeries):\n             message=\"Missing text for attribute 'unit'. Please specify the scientific unit of the 'data'.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.BEST_PRACTICE_VIOLATION, neurodata_type=TimeSeries)\n-def check_resolution(time_series: TimeSeries):\n+def check_resolution(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     \"\"\"Check the resolution value of a TimeSeries for proper format (-1.0 or NaN for unknown).\"\"\"\n     if time_series.resolution is None or time_series.resolution == -1.0:\n-        return\n+        return None\n     if time_series.resolution <= 0:\n         return InspectorMessage(\n             message=f\"'resolution' should use -1.0 or NaN for unknown instead of {time_series.resolution}.\"\n         )\n \n+    return None\n+\n \n @register_check(importance=Importance.CRITICAL, neurodata_type=TimeSeries)\n-def check_rate_is_not_zero(time_series: TimeSeries):\n+def check_rate_is_not_zero(time_series: TimeSeries) -> Optional[InspectorMessage]:\n     if time_series.data is None:\n-        return\n+        return None\n+\n     data_shape = get_data_shape(time_series.data)\n+    if data_shape is None:\n+        return None\n+\n     if time_series.rate == 0.0 and data_shape[0] > 1:\n         return InspectorMessage(\n             f\"{time_series.name} has a sampling rate value of 0.0Hz but the series has more than one frame.\"\n         )\n+\n+    return None\ndiff --git a/src/nwbinspector/inspector_tools/__init__.py b/src/nwbinspector/inspector_tools/__init__.py\nindex 134b1f7b6..6cf0e655e 100644\n--- a/src/nwbinspector/inspector_tools/__init__.py\n+++ b/src/nwbinspector/inspector_tools/__init__.py\n@@ -9,5 +9,12 @@\n \n # Still keep imports functional with warning for soft deprecation cycle\n # TODO: remove after 9/15/2024\n-from .._organization import organize_messages, _get_report_header\n-from .._formatting import format_message, MessageFormatter, FormatterOptions, print_to_console, save_report\n+from .._organization import organize_messages\n+from .._formatting import (\n+    format_messages,\n+    MessageFormatter,\n+    FormatterOptions,\n+    print_to_console,\n+    save_report,\n+    _get_report_header,\n+)\ndiff --git a/src/nwbinspector/tools/__init__.py b/src/nwbinspector/tools/__init__.py\nindex 674f827a2..33612ae19 100644\n--- a/src/nwbinspector/tools/__init__.py\n+++ b/src/nwbinspector/tools/__init__.py\n@@ -1,6 +1,6 @@\n from ._dandi import get_s3_urls_and_dandi_paths\n from ._nwb import all_of_type, get_nwbfile_path_from_internal_object\n-from ._read_nwbfile import BACKEND_IO_CLASSES, read_nwbfile\n+from ._read_nwbfile import BACKEND_IO_CLASSES, read_nwbfile, read_nwbfile_and_io\n \n __all__ = [\n     \"BACKEND_IO_CLASSES\",\n@@ -8,4 +8,5 @@\n     \"all_of_type\",\n     \"get_nwbfile_path_from_internal_object\",\n     \"read_nwbfile\",\n+    \"read_nwbfile_and_io\",\n ]\ndiff --git a/src/nwbinspector/tools/_dandi.py b/src/nwbinspector/tools/_dandi.py\nindex e1d338c5c..87fac36e2 100644\n--- a/src/nwbinspector/tools/_dandi.py\n+++ b/src/nwbinspector/tools/_dandi.py\n@@ -46,7 +46,11 @@ def get_s3_urls_and_dandi_paths(dandiset_id: str, version_id: Optional[str] = No\n     return s3_urls_to_dandi_paths\n \n \n-def _get_content_url_and_path(asset, follow_redirects: int = 1, strip_query: bool = True) -> dict[str, str]:\n+def _get_content_url_and_path(\n+    asset: \"dandi.dandiapi.BaseRemoteAsset\",  # type: ignore\n+    follow_redirects: int = 1,\n+    strip_query: bool = True,\n+) -> dict[str, str]:\n     \"\"\"\n     Private helper function for parallelization in 'get_s3_urls_and_dandi_paths'.\n \ndiff --git a/src/nwbinspector/tools/_nwb.py b/src/nwbinspector/tools/_nwb.py\nindex 24676220d..571007d71 100644\n--- a/src/nwbinspector/tools/_nwb.py\n+++ b/src/nwbinspector/tools/_nwb.py\n@@ -1,17 +1,20 @@\n \"\"\"Helper functions related to NWB for internal use that rely on external dependencies (i.e., pynwb).\"\"\"\n \n+from typing import Iterable, Type\n+\n from pynwb import NWBFile\n \n \n-def all_of_type(nwbfile: NWBFile, neurodata_type):\n+def all_of_type(nwbfile: NWBFile, neurodata_type: Type) -> Iterable[object]:\n     \"\"\"Iterate over all objects inside an NWBFile object and return those that match the given neurodata_type.\"\"\"\n-    for obj in nwbfile.objects.values():\n-        if isinstance(obj, neurodata_type):\n-            yield obj\n+    for neurodata_object in nwbfile.objects.values():\n+        if isinstance(neurodata_object, neurodata_type):\n+            yield neurodata_object\n \n \n-def get_nwbfile_path_from_internal_object(obj):\n+def get_nwbfile_path_from_internal_object(neurodata_object: object) -> str:\n     \"\"\"Determine the file path on disk for a NWBFile given only an internal object of that file.\"\"\"\n-    if isinstance(obj, NWBFile):\n-        return obj.container_source\n-    return obj.get_ancestor(\"NWBFile\").container_source\n+    if isinstance(neurodata_object, NWBFile):\n+        return neurodata_object.container_source\n+\n+    return neurodata_object.get_ancestor(\"NWBFile\").container_source  # type: ignore\ndiff --git a/src/nwbinspector/tools/_read_nwbfile.py b/src/nwbinspector/tools/_read_nwbfile.py\nindex 20fe30b08..de6d1eb51 100644\n--- a/src/nwbinspector/tools/_read_nwbfile.py\n+++ b/src/nwbinspector/tools/_read_nwbfile.py\n@@ -15,35 +15,39 @@\n )\n \n \n-def _get_method(path: str):\n+def _get_method(path: str) -> Literal[\"local\", \"fsspec\"]:\n     if path.startswith((\"https://\", \"http://\", \"s3://\")):\n         return \"fsspec\"\n     elif Path(path).exists():\n         return \"local\"\n     else:\n-        raise ValueError(\n+        message = (\n             f\"Unable to automatically determine method. Path {path} does not appear to be a URL and is not a file on \"\n             f\"the local filesystem.\"\n         )\n+        raise ValueError(message)\n \n \n-def _init_fsspec(path):\n+def _init_fsspec(path: str) -> \"fsspec.AbstractFileSystem\":  # type: ignore\n     import fsspec\n \n     if path.startswith((\"https://\", \"http://\")):\n         return fsspec.filesystem(\"http\")\n     elif path.startswith(\"s3://\"):\n         return fsspec.filesystem(\"s3\", anon=True)\n+    else:\n+        message = f\"Unable to initialize fsspec on path '{path}'.\"\n+        raise ValueError(message)\n \n \n-def _get_backend(path: str, method: Literal[\"local\", \"fsspec\", \"ros3\"]):\n+def _get_backend(path: str, method: Literal[\"local\", \"fsspec\", \"ros3\"]) -> Union[str, Literal[\"hdf5\", \"zarr\"]]:\n     if method == \"ros3\":\n         return \"hdf5\"\n \n     possible_backends = []\n     if method == \"fsspec\":\n-        fs = _init_fsspec(path=path)\n-        with fs.open(path=path, mode=\"rb\") as file:\n+        filesystem = _init_fsspec(path=path)\n+        with filesystem.open(path=path, mode=\"rb\") as file:\n             for backend_name, backend_class in BACKEND_IO_CLASSES.items():\n                 if backend_class.can_read(path=file):\n                     possible_backends.append(backend_name)\n@@ -66,8 +70,40 @@ def read_nwbfile(\n     nwbfile_path: Union[str, Path],\n     method: Optional[Literal[\"local\", \"fsspec\", \"ros3\"]] = None,\n     backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n-    return_io: bool = False,\n-) -> Union[NWBFile, tuple[NWBFile, HDMFIO]]:\n+) -> NWBFile:\n+    \"\"\"\n+    Read an NWB file using the specified (or auto-detected) method and specified (or auto-detected) backend.\n+\n+    Parameters\n+    ----------\n+    nwbfile_path : str or pathlib.Path\n+        Path to the file on your system.\n+    method : \"local\", \"fsspec\", \"ros3\", or None (default)\n+        Where to read the file from; a local disk drive or steaming from an https:// or s3:// path.\n+        The default auto-detects based on the form of the path.\n+        When streaming, the default method is \"fsspec\".\n+        Note that \"ros3\" is specific to HDF5 backend files.\n+    backend : \"hdf5\", \"zarr\", or None (default)\n+        Type of backend used to write the file.\n+        The default auto-detects the type of the file.\n+\n+    Returns\n+    -------\n+    nwbfile : pynwb.NWBFile\n+        The in-memory NWBFile object.\n+    \"\"\"\n+    nwbfile, _ = _read_nwbfile_helper(nwbfile_path=nwbfile_path, method=method, backend=backend)\n+\n+    # Note: do not be concerned about IO object closing due to garbage collection here\n+    # (the IO object is attached as an attribute to the NWBFile object)\n+    return nwbfile\n+\n+\n+def read_nwbfile_and_io(\n+    nwbfile_path: Union[str, Path],\n+    method: Optional[Literal[\"local\", \"fsspec\", \"ros3\"]] = None,\n+    backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n+) -> tuple[NWBFile, HDMFIO]:\n     \"\"\"\n     Read an NWB file using the specified (or auto-detected) method and specified (or auto-detected) backend.\n \n@@ -83,8 +119,6 @@ def read_nwbfile(\n     backend : \"hdf5\", \"zarr\", or None (default)\n         Type of backend used to write the file.\n         The default auto-detects the type of the file.\n-    return_io : bool, default: False\n-        Whether to return the HDMFIO object used to open the file.\n \n     Returns\n     -------\n@@ -94,6 +128,16 @@ def read_nwbfile(\n         Only passed if `return_io` is True.\n         The initialized HDMFIO object used to read the file.\n     \"\"\"\n+    nwbfile, io = _read_nwbfile_helper(nwbfile_path=nwbfile_path, method=method, backend=backend)\n+\n+    return nwbfile, io\n+\n+\n+def _read_nwbfile_helper(\n+    nwbfile_path: Union[str, Path],\n+    method: Optional[Literal[\"local\", \"fsspec\", \"ros3\"]] = None,\n+    backend: Optional[Literal[\"hdf5\", \"zarr\"]] = None,\n+) -> tuple[NWBFile, HDMFIO]:\n     nwbfile_path = str(nwbfile_path)  # If pathlib.Path, cast to str; if already str, no harm done\n \n     method = method or _get_method(nwbfile_path)\n@@ -112,13 +156,14 @@ def read_nwbfile(\n             \"The ROS3 method was selected, but the URL starts with 's3://'! Please switch to an 'https://' URL.\"\n         )\n \n-    backend = backend or _get_backend(nwbfile_path, method)\n-    if method == \"local\" and not BACKEND_IO_CLASSES[  # Temporary until .can_read() is able to work on streamed bytes\n-        backend\n-    ].can_read(path=nwbfile_path):\n-        raise IOError(f\"The chosen backend ({backend}) is unable to read the file! Please select a different backend.\")\n+    chosen_backend = backend or _get_backend(path=nwbfile_path, method=method)\n+    # Temporary until .can_read() is able to work on streamed bytes\n+    if method == \"local\" and not BACKEND_IO_CLASSES[chosen_backend].can_read(path=nwbfile_path):\n+        raise IOError(\n+            f\"The chosen backend ({chosen_backend}) is unable to read the file! Please select a different backend.\"\n+        )\n \n-    # Filter out some of most common warnings that don't really matter with `load_namespaces=True`\n+    # Filter out some common warnings that don't really matter with `load_namespaces=True`\n     filterwarnings(action=\"ignore\", message=\"No cached namespaces found in .*\")\n     filterwarnings(action=\"ignore\", message=\"Ignoring cached namespace .*\")\n     io_kwargs = dict(mode=\"r\", load_namespaces=True)\n@@ -131,10 +176,7 @@ def read_nwbfile(\n         io_kwargs.update(path=nwbfile_path)\n     if method == \"ros3\":\n         io_kwargs.update(driver=\"ros3\")\n-    io = BACKEND_IO_CLASSES[backend](**io_kwargs)\n+    io = BACKEND_IO_CLASSES[chosen_backend](**io_kwargs)\n     nwbfile = io.read()\n \n-    if return_io:\n-        return (nwbfile, io)\n-    else:  # Note: do not be concerned about io object closing due to garbage collection here\n-        return nwbfile  # (it is attached as an attribute to the NWBFile object)\n+    return nwbfile, io\ndiff --git a/src/nwbinspector/utils/_utils.py b/src/nwbinspector/utils/_utils.py\nindex a1fddd411..6111a188e 100644\n--- a/src/nwbinspector/utils/_utils.py\n+++ b/src/nwbinspector/utils/_utils.py\n@@ -6,7 +6,7 @@\n from functools import lru_cache\n from importlib import import_module\n from pathlib import Path\n-from typing import Optional, TypeVar, Union\n+from typing import Any, Optional, TypeVar, Union\n \n import h5py\n import numpy as np\n@@ -52,7 +52,7 @@ def cache_data_selection(data: Union[h5py.Dataset, ArrayLike], selection: Union[\n     return _cache_data_retrieval_command(data=data, reduced_selection=reduced_selection)\n \n \n-def format_byte_size(byte_size: int, units: str = \"SI\"):\n+def format_byte_size(byte_size: int, units: str = \"SI\") -> str:\n     \"\"\"\n     Format a number representing a total number of bytes into a convenient unit.\n \n@@ -65,7 +65,7 @@ def format_byte_size(byte_size: int, units: str = \"SI\"):\n         May be either SI (orders of 1000) or binary (in memory, orders of 1024).\n         The default is SI.\n     \"\"\"\n-    num = byte_size\n+    num = float(byte_size)\n     prefixes = [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\"]\n     if units == \"SI\":\n         order = 1000.0\n@@ -79,16 +79,18 @@ def format_byte_size(byte_size: int, units: str = \"SI\"):\n         if abs(num) < order:\n             return f\"{num:3.2f}{prefix}{suffix}\"\n         num /= order\n+\n     return f\"{num:.2f}Y{suffix}\"\n \n \n-def is_regular_series(series: np.ndarray, tolerance_decimals: int = 9):\n+def is_regular_series(series: np.ndarray, tolerance_decimals: int = 9) -> bool:\n     \"\"\"General purpose function for checking if the difference between all consecutive points in a series are equal.\"\"\"\n     uniq_diff_ts = np.unique(np.diff(series).round(decimals=tolerance_decimals))\n+\n     return len(uniq_diff_ts) == 1\n \n \n-def is_ascending_series(series: Union[h5py.Dataset, ArrayLike], nelems: Optional[int] = None):\n+def is_ascending_series(series: Union[h5py.Dataset, ArrayLike], nelems: Optional[int] = None) -> bool:\n     \"\"\"General purpose function for determining if a series is monotonic increasing.\"\"\"\n     if isinstance(series, h5py.Dataset):\n         data = cache_data_selection(data=series, selection=slice(nelems))\n@@ -105,7 +107,7 @@ def is_ascending_series(series: Union[h5py.Dataset, ArrayLike], nelems: Optional\n     return np.all(differences >= 0)\n \n \n-def is_dict_in_string(string: str):\n+def is_dict_in_string(string: str) -> bool:\n     \"\"\"\n     Determine if the string value contains an encoded Python dictionary.\n \n@@ -114,7 +116,7 @@ def is_dict_in_string(string: str):\n     return any(re.findall(pattern=dict_regex, string=string))\n \n \n-def is_string_json_loadable(string: str):\n+def is_string_json_loadable(string: str) -> bool:\n     \"\"\"\n     Determine if the serialized dictionary is a JSON object.\n \n@@ -127,7 +129,7 @@ def is_string_json_loadable(string: str):\n         return False\n \n \n-def is_module_installed(module_name: str):\n+def is_module_installed(module_name: str) -> bool:\n     \"\"\"\n     Check if the given module is installed on the system.\n \n@@ -162,6 +164,7 @@ def get_package_version(name: str) -> version.Version:\n         from pkg_resources import get_distribution\n \n         package_version = get_distribution(name).version\n+\n     return version.parse(package_version)\n \n \n@@ -176,7 +179,7 @@ def calculate_number_of_cpu(requested_cpu: int = 1) -> int:\n \n         The default is 1.\n     \"\"\"\n-    total_cpu = os.cpu_count()\n+    total_cpu = os.cpu_count() or 1  # Annotations say os.cpu_count can return None for some reason\n     assert requested_cpu <= total_cpu, f\"Requested more CPUs ({requested_cpu}) than are available ({total_cpu})!\"\n     assert requested_cpu >= -(\n         total_cpu - 1\n@@ -187,11 +190,12 @@ def calculate_number_of_cpu(requested_cpu: int = 1) -> int:\n         return total_cpu + requested_cpu\n \n \n-def get_data_shape(data, strict_no_data_load=False):\n+def get_data_shape(data: Any, strict_no_data_load: bool = False) -> Optional[tuple[int, ...]]:\n     \"\"\"\n-    modified from hdmf.utils.get_data_shape to return shape instead of maxshape\n     Helper function used to determine the shape of the given array.\n \n+    Modified from hdmf.utils.get_data_shape to return shape instead of maxshape.\n+\n     In order to determine the shape of nested tuples, lists, and sets, this function\n     recursively inspects elements along the dimensions, assuming that the data has a regular,\n     rectangular shape. In the case of out-of-core iterators, this means that the first item\n@@ -199,15 +203,22 @@ def get_data_shape(data, strict_no_data_load=False):\n     to enforce that this does not happen, at the cost that we may not be able to determine\n     the shape of the array.\n \n-    :param data: Array for which we should determine the shape.\n-    :type data: List, numpy.ndarray, DataChunkIterator, any object that support __len__ or .shape.\n-    :param strict_no_data_load: If True and data is an out-of-core iterator, None may be returned. If False (default),\n-                                the first element of data may be loaded into memory.\n-    :return: Tuple of ints indicating the size of known dimensions. Dimensions for which the size is unknown\n-             will be set to None.\n+    Parameters\n+    ----------\n+    data : list, numpy.ndarray, DataChunkIterator, any object that support __len__ or .shape.\n+        Array for which we should determine the shape.\n+    strict_no_data_load : bool\n+        If True and data is an out-of-core iterator, None may be returned.\n+        If False (default), the first element of data may be loaded into memory.\n+\n+    Returns\n+    -------\n+    data_shape : tuple[int], optional\n+        Tuple of ints indicating the size of known dimensions.\n+        Dimensions for which the size is unknown will be set to None.\n     \"\"\"\n \n-    def __get_shape_helper(local_data):\n+    def __get_shape_helper(local_data: Any) -> tuple[int, ...]:\n         shape = list()\n         if hasattr(local_data, \"__len__\"):\n             shape.append(len(local_data))\n@@ -220,11 +231,13 @@ def __get_shape_helper(local_data):\n     if hasattr(data, \"shape\") and data.shape is not None:\n         return data.shape\n     if isinstance(data, dict):\n-        return\n+        return None\n     if hasattr(data, \"__len__\") and not isinstance(data, (str, bytes)):\n         if not strict_no_data_load or isinstance(data, (list, tuple, set)):\n             return __get_shape_helper(data)\n \n+    return None\n+\n \n def strtobool(val: str) -> bool:\n     \"\"\"\n", "instance_id": "NeurodataWithoutBorders__nwbinspector-520", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to fully type-annotate the `nwbinspector` package, add a CI job for mypy, and include a `py.typed` marker file for proper type hinting support. It specifies the tasks to be completed, such as annotating public items and ensuring the `py.typed` file is included in distributions. However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes \"public items\" (e.g., whether it includes all functions/classes or only those in the public API). Additionally, there are no specific guidelines on handling complex type annotations for domain-specific objects or potential backward compatibility issues with existing code. Edge cases, such as handling third-party library dependencies without type hints, are also not addressed. Despite these minor gaps, the overall goal and steps are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is significant, as it involves type-annotating an entire package across multiple files (as seen in the extensive diffs provided), requiring a thorough understanding of the codebase structure and interactions between modules. The changes impact not just individual functions but also the build and CI configurations (e.g., adding mypy to pre-commit hooks and updating `pyproject.toml`). Second, it requires familiarity with Python's typing system, including advanced features like `Union`, `Optional`, `Callable`, and third-party typing libraries like `typing_extensions`, as well as tools like mypy for static type checking. Third, while the problem does not explicitly mention complex edge cases, the code changes reveal the need to handle nuanced typing scenarios (e.g., type ignoring for external dependencies like `dandi.dandiapi.DandiAPIClient`, and ensuring compatibility with existing code). However, this task does not reach the \"hard\" level (0.6-0.8) because it does not involve deep architectural changes, complex algorithms, or domain-specific challenges beyond type annotations. It is a substantial but manageable task for someone with intermediate Python experience and familiarity with type systems, hence a score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "pytest-memray breaks anyio\nHola @pablogsal,\r\n\r\nI am facing the same issue: the async tests are skipped if we pass `--memray` argument to pytest.\r\n\r\n## Steps to reproduce the issue:\r\nUse the following test file: `test_async.py`\r\n```python\r\nimport pytest\r\n\r\n\r\n@pytest.fixture\r\ndef anyio_backend():\r\n    return 'asyncio'\r\n\r\n@pytest.mark.anyio\r\nasync def test_async():\r\n    assert True\r\n```\r\nInstall required dependencies:\r\n```shell\r\npython -m pip install pytest anyio pytest-memray\r\n```\r\n\r\nThe test runs as expected if `--memray` is not passed:\r\n```shell\r\n python -m pytest -vv -x test_async.py\r\n```\r\nOutput:\r\n```\r\nplugins: memray-1.6.0, anyio-4.0.0\r\ncollected 1 item                                                                                                                                                                                          \r\n\r\ntest_async.py::test_async PASSED\r\n```\r\n\r\nHowever, the test is skipped if we pass `--memray`:\r\n```shell\r\n python -m pytest --memray -vv -x test_async.py\r\n```\r\nOutput:\r\n```\r\nplugins: memray-1.6.0, anyio-4.0.0\r\ncollected 1 item                                                                                                                                                                                          \r\n\r\ntest_async.py::test_async SKIPPED (async def function and no async plugin installed (see warnings))                                                                                                 [100%]\r\n\r\n============================================================================================ warnings summary =============================================================================================\r\ntest_async.py::test_async\r\n  <MY_PROJECT_PATH>/.venv/lib/python3.9/site-packages/_pytest/python.py:151: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.\r\n  You need to install a suitable plugin for your async framework, for example:\r\n    - anyio\r\n    - pytest-asyncio\r\n    - pytest-tornasync\r\n    - pytest-trio\r\n    - pytest-twisted\r\n    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n```\r\n\r\n_Originally posted by @albertvillanova in https://github.com/bloomberg/pytest-memray/discussions/101#discussioncomment-9738673_\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex a92ba22..ecee2b5 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -37,6 +37,7 @@ optional-dependencies.lint = [\n   \"mypy==0.991\",\n ]\n optional-dependencies.test = [\n+  \"anyio>=4.4.0\",\n   \"covdefaults>=2.2.2\",\n   \"pytest>=7.2\",\n   \"coverage>=7.0.5\",\n", "instance_id": "bloomberg__pytest-memray-122", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: pytest-memray causes async tests using the anyio plugin to be skipped when the --memray flag is provided. The steps to reproduce the issue are well-documented with code snippets, commands, and expected versus actual outputs, which helps in understanding the problem context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the root cause of the issue (e.g., whether it's a compatibility issue between pytest-memray and anyio or a configuration problem). Additionally, it lacks clarity on the expected resolution or constraints (e.g., whether the fix should be in pytest-memray or a workaround in the test setup). Edge cases or specific scenarios beyond the provided example are also not mentioned. Thus, while the problem is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the provided code changes and the nature of the issue. The code change shown in the diff is minimal, involving only the addition of a dependency version constraint for anyio (>=4.4.0) in the pyproject.toml file, suggesting a straightforward compatibility fix. The scope of the change is limited to a single configuration file and does not impact the broader codebase architecture or require deep modifications. The technical concepts involved are basic\u2014understanding dependency management and versioning in Python projects, which is a fundamental skill for most developers. There are no complex algorithms, design patterns, or domain-specific knowledge required. Additionally, the problem statement and code changes do not indicate significant edge cases or error handling requirements beyond ensuring the correct version of anyio is installed. However, there is a slight increase in difficulty due to the need to understand the interaction between pytest plugins (pytest-memray and anyio) and potentially debug or verify compatibility issues, which requires some familiarity with pytest's plugin system. Overall, this is a relatively simple issue to resolve with minimal code changes and conceptual depth.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\ude80 Feature: Tracing support for Amazon SageMaker endpoints\n### Which component is this feature for?\n\nAll Packages\n\n### \ud83d\udd16 Feature description\n\nAdd tracing support for [Amazon SageMaker endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)\n\n### \ud83c\udfa4 Why is this feature needed ?\n\nDevelopers may deploy a variety of LLMs as endpoints on Amazon SageMaker. They'd query these endpoints using the [InvokeEndpoint](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html) API to retrieve the LLM response.\n\n### \u270c\ufe0f How do you aim to achieve this?\n\n1. Create `opentelemetry-instrumentation-sagemaker` package\r\n2. Create instrumentation for the `sagemaker-runtime` service and patch the `invoke_endpoint` method.\r\n3. Create span for the request.\n\n### \ud83d\udd04\ufe0f Additional Information\n\n_No response_\n\n### \ud83d\udc40 Have you spent some time to check if this feature request has been raised before?\n\n- [X] I checked and didn't find similar issue\n\n### Are you willing to submit PR?\n\nYes I am willing to submit a PR!\n", "patch": "diff --git a/README.md b/README.md\nindex 2848bb333..1d375d45d 100644\n--- a/README.md\n+++ b/README.md\n@@ -116,6 +116,7 @@ OpenLLMetry can instrument everything that [OpenTelemetry already instruments](h\n - \u2705 Mistral AI\n - \u2705 HuggingFace\n - \u2705 Bedrock (AWS)\n+- \u2705 SageMaker (AWS)\n - \u2705 Replicate\n - \u2705 Vertex AI (GCP)\n - \u2705 Google Generative AI (Gemini)\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/.flake8 b/packages/opentelemetry-instrumentation-sagemaker/.flake8\nnew file mode 100644\nindex 000000000..3da962c38\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/.flake8\n@@ -0,0 +1,11 @@\n+[flake8]\n+exclude =\n+\t.git,\n+\t__pycache__,\n+\tbuild,\n+\tdist,\n+\t.tox,\n+\tvenv,\n+\t.venv,\n+\t.pytest_cache\n+max-line-length = 120\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/.python-version b/packages/opentelemetry-instrumentation-sagemaker/.python-version\nnew file mode 100644\nindex 000000000..11aaa0686\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/.python-version\n@@ -0,0 +1,1 @@\n+3.9.5\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/README.md b/packages/opentelemetry-instrumentation-sagemaker/README.md\nnew file mode 100644\nindex 000000000..fa46c98d8\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/README.md\n@@ -0,0 +1,33 @@\n+# OpenTelemetry SageMaker Instrumentation\n+\n+<a href=\"https://pypi.org/project/opentelemetry-instrumentation-sagemaker/\">\n+    <img src=\"https://badge.fury.io/py/opentelemetry-instrumentation-sagemaker.svg\">\n+</a>\n+\n+This library allows tracing of any models deployed on Amazon SageMaker and invoked with [Boto3](https://github.com/boto/boto3) to SageMaker.\n+\n+## Installation\n+\n+```bash\n+pip install opentelemetry-instrumentation-sagemaker\n+```\n+\n+## Example usage\n+\n+```python\n+from opentelemetry.instrumentation.sagemaker import SageMakerInstrumentor\n+\n+SageMakerInstrumentor().instrument()\n+```\n+\n+## Privacy\n+\n+**By default, this instrumentation logs SageMaker endpoint request bodies and responses to span attributes**. This gives you a clear visibility into how your LLM application is working, and can make it easy to debug and evaluate the quality of the outputs.\n+\n+However, you may want to disable this logging for privacy reasons, as they may contain highly sensitive data from your users. You may also simply want to reduce the size of your traces.\n+\n+To disable logging, set the `TRACELOOP_TRACE_CONTENT` environment variable to `false`.\n+\n+```bash\n+TRACELOOP_TRACE_CONTENT=false\n+```\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/__init__.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/__init__.py\nnew file mode 100644\nindex 000000000..ca28515de\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/__init__.py\n@@ -0,0 +1,198 @@\n+\"\"\"OpenTelemetry SageMaker instrumentation\"\"\"\n+\n+from functools import wraps\n+import json\n+import logging\n+import os\n+from typing import Collection\n+from opentelemetry.instrumentation.sagemaker.config import Config\n+from opentelemetry.instrumentation.sagemaker.reusable_streaming_body import (\n+    ReusableStreamingBody,\n+)\n+from opentelemetry.instrumentation.sagemaker.streaming_wrapper import StreamingWrapper\n+from opentelemetry.instrumentation.sagemaker.utils import dont_throw\n+from wrapt import wrap_function_wrapper\n+\n+from opentelemetry import context as context_api\n+from opentelemetry.trace import get_tracer, SpanKind\n+\n+from opentelemetry.instrumentation.instrumentor import BaseInstrumentor\n+from opentelemetry.instrumentation.utils import (\n+    _SUPPRESS_INSTRUMENTATION_KEY,\n+    unwrap,\n+)\n+\n+from opentelemetry.semconv_ai import (\n+    SUPPRESS_LANGUAGE_MODEL_INSTRUMENTATION_KEY,\n+    SpanAttributes,\n+)\n+from opentelemetry.instrumentation.sagemaker.version import __version__\n+\n+logger = logging.getLogger(__name__)\n+\n+_instruments = (\"boto3 >= 1.28.57\",)\n+\n+WRAPPED_METHODS = [\n+    {\n+        \"package\": \"botocore.client\",\n+        \"object\": \"ClientCreator\",\n+        \"method\": \"create_client\",\n+    },\n+    {\"package\": \"botocore.session\", \"object\": \"Session\", \"method\": \"create_client\"},\n+]\n+\n+\n+def should_send_prompts():\n+    return (\n+        os.getenv(\"TRACELOOP_TRACE_CONTENT\") or \"true\"\n+    ).lower() == \"true\" or context_api.get_value(\"override_enable_content_tracing\")\n+\n+\n+def _set_span_attribute(span, name, value):\n+    if value is not None:\n+        if value != \"\":\n+            span.set_attribute(name, value)\n+    return\n+\n+\n+def _with_tracer_wrapper(func):\n+    \"\"\"Helper for providing tracer for wrapper functions.\"\"\"\n+\n+    def _with_tracer(tracer, to_wrap):\n+        def wrapper(wrapped, instance, args, kwargs):\n+            return func(tracer, to_wrap, wrapped, instance, args, kwargs)\n+\n+        return wrapper\n+\n+    return _with_tracer\n+\n+\n+@_with_tracer_wrapper\n+def _wrap(tracer, to_wrap, wrapped, instance, args, kwargs):\n+    \"\"\"Instruments and calls every function defined in TO_WRAP.\"\"\"\n+    if context_api.get_value(_SUPPRESS_INSTRUMENTATION_KEY):\n+        return wrapped(*args, **kwargs)\n+\n+    if kwargs.get(\"service_name\") == \"sagemaker-runtime\":\n+        client = wrapped(*args, **kwargs)\n+        client.invoke_endpoint = _instrumented_endpoint_invoke(\n+            client.invoke_endpoint, tracer\n+        )\n+        client.invoke_endpoint_with_response_stream = (\n+            _instrumented_endpoint_invoke_with_response_stream(\n+                client.invoke_endpoint_with_response_stream, tracer\n+            )\n+        )\n+\n+        return client\n+\n+    return wrapped(*args, **kwargs)\n+\n+\n+def _instrumented_endpoint_invoke(fn, tracer):\n+    @wraps(fn)\n+    def with_instrumentation(*args, **kwargs):\n+        if context_api.get_value(SUPPRESS_LANGUAGE_MODEL_INSTRUMENTATION_KEY):\n+            return fn(*args, **kwargs)\n+\n+        with tracer.start_as_current_span(\n+            \"sagemaker.completion\", kind=SpanKind.CLIENT\n+        ) as span:\n+            response = fn(*args, **kwargs)\n+\n+            if span.is_recording():\n+                _handle_call(span, kwargs, response)\n+\n+            return response\n+\n+    return with_instrumentation\n+\n+\n+def _instrumented_endpoint_invoke_with_response_stream(fn, tracer):\n+    @wraps(fn)\n+    def with_instrumentation(*args, **kwargs):\n+        if context_api.get_value(SUPPRESS_LANGUAGE_MODEL_INSTRUMENTATION_KEY):\n+            return fn(*args, **kwargs)\n+\n+        span = tracer.start_span(\"sagemaker.completion\", kind=SpanKind.CLIENT)\n+        response = fn(*args, **kwargs)\n+\n+        if span.is_recording():\n+            _handle_stream_call(span, kwargs, response)\n+\n+        return response\n+\n+    return with_instrumentation\n+\n+\n+def _handle_stream_call(span, kwargs, response):\n+    @dont_throw\n+    def stream_done(response_body):\n+        request_body = json.loads(kwargs.get(\"Body\"))\n+\n+        endpoint_name = kwargs.get(\"EndpointName\")\n+\n+        _set_span_attribute(span, SpanAttributes.LLM_REQUEST_MODEL, endpoint_name)\n+        _set_span_attribute(\n+            span, SpanAttributes.TRACELOOP_ENTITY_INPUT, json.dumps(request_body)\n+        )\n+        _set_span_attribute(\n+            span, SpanAttributes.TRACELOOP_ENTITY_OUTPUT, json.dumps(response_body)\n+        )\n+\n+        span.end()\n+\n+    response[\"Body\"] = StreamingWrapper(response[\"Body\"], stream_done)\n+\n+\n+@dont_throw\n+def _handle_call(span, kwargs, response):\n+    response[\"Body\"] = ReusableStreamingBody(\n+        response[\"Body\"]._raw_stream, response[\"Body\"]._content_length\n+    )\n+    request_body = json.loads(kwargs.get(\"Body\"))\n+    response_body = json.loads(response.get(\"Body\").read())\n+\n+    endpoint_name = kwargs.get(\"EndpointName\")\n+\n+    _set_span_attribute(span, SpanAttributes.LLM_REQUEST_MODEL, endpoint_name)\n+    _set_span_attribute(\n+        span, SpanAttributes.TRACELOOP_ENTITY_INPUT, json.dumps(request_body)\n+    )\n+    _set_span_attribute(\n+        span, SpanAttributes.TRACELOOP_ENTITY_OUTPUT, json.dumps(response_body)\n+    )\n+\n+\n+class SageMakerInstrumentor(BaseInstrumentor):\n+    \"\"\"An instrumentor for Bedrock's client library.\"\"\"\n+\n+    def __init__(self, enrich_token_usage: bool = False, exception_logger=None):\n+        super().__init__()\n+        Config.enrich_token_usage = enrich_token_usage\n+        Config.exception_logger = exception_logger\n+\n+    def instrumentation_dependencies(self) -> Collection[str]:\n+        return _instruments\n+\n+    def _instrument(self, **kwargs):\n+        tracer_provider = kwargs.get(\"tracer_provider\")\n+        tracer = get_tracer(__name__, __version__, tracer_provider)\n+        for wrapped_method in WRAPPED_METHODS:\n+            wrap_package = wrapped_method.get(\"package\")\n+            wrap_object = wrapped_method.get(\"object\")\n+            wrap_method = wrapped_method.get(\"method\")\n+            wrap_function_wrapper(\n+                wrap_package,\n+                f\"{wrap_object}.{wrap_method}\",\n+                _wrap(tracer, wrapped_method),\n+            )\n+\n+    def _uninstrument(self, **kwargs):\n+        for wrapped_method in WRAPPED_METHODS:\n+            wrap_package = wrapped_method.get(\"package\")\n+            wrap_object = wrapped_method.get(\"object\")\n+            unwrap(\n+                f\"{wrap_package}.{wrap_object}\",\n+                wrapped_method.get(\"method\"),\n+            )\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/config.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/config.py\nnew file mode 100644\nindex 000000000..818883ad2\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/config.py\n@@ -0,0 +1,3 @@\n+class Config:\n+    enrich_token_usage = False\n+    exception_logger = None\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/reusable_streaming_body.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/reusable_streaming_body.py\nnew file mode 100644\nindex 000000000..774e3d25d\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/reusable_streaming_body.py\n@@ -0,0 +1,43 @@\n+from botocore.response import StreamingBody\n+from botocore.exceptions import (\n+    ReadTimeoutError,\n+    ResponseStreamingError,\n+)\n+from urllib3.exceptions import ProtocolError as URLLib3ProtocolError\n+from urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\n+\n+\n+class ReusableStreamingBody(StreamingBody):\n+    \"\"\"Wrapper around StreamingBody that allows the body to be read multiple times.\"\"\"\n+\n+    def __init__(self, raw_stream, content_length):\n+        super().__init__(raw_stream, content_length)\n+        self._buffer = None\n+        self._buffer_cursor = 0\n+\n+    def read(self, amt=None):\n+        \"\"\"Read at most amt bytes from the stream.\n+\n+        If the amt argument is omitted, read all data.\n+        \"\"\"\n+        if self._buffer is None:\n+            try:\n+                self._buffer = self._raw_stream.read()\n+            except URLLib3ReadTimeoutError as e:\n+                # TODO: the url will be None as urllib3 isn't setting it yet\n+                raise ReadTimeoutError(endpoint_url=e.url, error=e)\n+            except URLLib3ProtocolError as e:\n+                raise ResponseStreamingError(error=e)\n+\n+            self._amount_read += len(self._buffer)\n+            if amt is None or (not self._buffer and amt > 0):\n+                # If the server sends empty contents or\n+                # we ask to read all of the contents, then we know\n+                # we need to verify the content length.\n+                self._verify_content_length()\n+\n+        if amt is None:\n+            return self._buffer[self._buffer_cursor:]\n+        else:\n+            self._buffer_cursor += amt\n+            return self._buffer[self._buffer_cursor-amt:self._buffer_cursor]\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/streaming_wrapper.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/streaming_wrapper.py\nnew file mode 100644\nindex 000000000..daff152a7\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/streaming_wrapper.py\n@@ -0,0 +1,29 @@\n+from opentelemetry.instrumentation.sagemaker.utils import dont_throw\n+from wrapt import ObjectProxy\n+\n+\n+class StreamingWrapper(ObjectProxy):\n+    def __init__(\n+        self,\n+        response,\n+        stream_done_callback=None,\n+    ):\n+        super().__init__(response)\n+\n+        self._stream_done_callback = stream_done_callback\n+        self._accumulating_body = \"\"\n+\n+    def __iter__(self):\n+        for event in self.__wrapped__:\n+            self._process_event(event)\n+            yield event\n+        self._stream_done_callback(self._accumulating_body)\n+\n+    @dont_throw\n+    def _process_event(self, event):\n+        payload_part = event.get(\"PayloadPart\")\n+        if not payload_part:\n+            return\n+\n+        decoded_payload_part = payload_part.get(\"Bytes\").decode()\n+        self._accumulating_body += decoded_payload_part\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/utils.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/utils.py\nnew file mode 100644\nindex 000000000..51468b65a\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/utils.py\n@@ -0,0 +1,29 @@\n+import logging\n+import traceback\n+\n+from opentelemetry.instrumentation.sagemaker.config import Config\n+\n+\n+def dont_throw(func):\n+    \"\"\"\n+    A decorator that wraps the passed in function and logs exceptions instead of throwing them.\n+\n+    @param func: The function to wrap\n+    @return: The wrapper function\n+    \"\"\"\n+    # Obtain a logger specific to the function's module\n+    logger = logging.getLogger(func.__module__)\n+\n+    def wrapper(*args, **kwargs):\n+        try:\n+            return func(*args, **kwargs)\n+        except Exception as e:\n+            logger.debug(\n+                \"OpenLLMetry failed to trace in %s, error: %s\",\n+                func.__name__,\n+                traceback.format_exc(),\n+            )\n+            if Config.exception_logger:\n+                Config.exception_logger(e)\n+\n+    return wrapper\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/version.py b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/version.py\nnew file mode 100644\nindex 000000000..84f5d8515\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/opentelemetry/instrumentation/sagemaker/version.py\n@@ -0,0 +1,1 @@\n+__version__ = \"0.25.6\"\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/poetry.lock b/packages/opentelemetry-instrumentation-sagemaker/poetry.lock\nnew file mode 100644\nindex 000000000..2022ad151\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/poetry.lock\n@@ -0,0 +1,887 @@\n+# This file is automatically @generated by Poetry 1.8.3 and should not be changed by hand.\n+\n+[[package]]\n+name = \"autopep8\"\n+version = \"2.2.0\"\n+description = \"A tool that automatically formats Python code to conform to the PEP 8 style guide\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"autopep8-2.2.0-py2.py3-none-any.whl\", hash = \"sha256:05418a981f038969d8bdcd5636bf15948db7555ae944b9f79b5a34b35f1370d4\"},\n+    {file = \"autopep8-2.2.0.tar.gz\", hash = \"sha256:d306a0581163ac29908280ad557773a95a9bede072c0fafed6f141f5311f43c1\"},\n+]\n+\n+[package.dependencies]\n+pycodestyle = \">=2.11.0\"\n+tomli = {version = \"*\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[[package]]\n+name = \"boto3\"\n+version = \"1.35.27\"\n+description = \"The AWS SDK for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"boto3-1.35.27-py3-none-any.whl\", hash = \"sha256:3da139ca038032e92086e26d23833b557f0c257520162bfd3d6f580bf8032c86\"},\n+    {file = \"boto3-1.35.27.tar.gz\", hash = \"sha256:10d0fe15670b83a3f26572ab20d9152a064cee4c54b5ea9a1eeb1f0c3b807a7b\"},\n+]\n+\n+[package.dependencies]\n+botocore = \">=1.35.27,<1.36.0\"\n+jmespath = \">=0.7.1,<2.0.0\"\n+s3transfer = \">=0.10.0,<0.11.0\"\n+\n+[package.extras]\n+crt = [\"botocore[crt] (>=1.21.0,<2.0a0)\"]\n+\n+[[package]]\n+name = \"botocore\"\n+version = \"1.35.27\"\n+description = \"Low-level, data-driven core of boto 3.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"botocore-1.35.27-py3-none-any.whl\", hash = \"sha256:c299c70b5330a8634e032883ce8a72c2c6d9fdbc985d8191199cb86b92e7cbbd\"},\n+    {file = \"botocore-1.35.27.tar.gz\", hash = \"sha256:f68875c26cd57a9d22c0f7a981ecb1636d7ce4d0e35797e04765b53e7bfed3e7\"},\n+]\n+\n+[package.dependencies]\n+jmespath = \">=0.7.1,<2.0.0\"\n+python-dateutil = \">=2.1,<3.0.0\"\n+urllib3 = [\n+    {version = \">=1.25.4,<1.27\", markers = \"python_version < \\\"3.10\\\"\"},\n+    {version = \">=1.25.4,<2.2.0 || >2.2.0,<3\", markers = \"python_version >= \\\"3.10\\\"\"},\n+]\n+\n+[package.extras]\n+crt = [\"awscrt (==0.21.5)\"]\n+\n+[[package]]\n+name = \"colorama\"\n+version = \"0.4.6\"\n+description = \"Cross-platform colored terminal text.\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"\n+files = [\n+    {file = \"colorama-0.4.6-py2.py3-none-any.whl\", hash = \"sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\"},\n+    {file = \"colorama-0.4.6.tar.gz\", hash = \"sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44\"},\n+]\n+\n+[[package]]\n+name = \"deprecated\"\n+version = \"1.2.14\"\n+description = \"Python @deprecated decorator to deprecate old python classes, functions or methods.\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"Deprecated-1.2.14-py2.py3-none-any.whl\", hash = \"sha256:6fac8b097794a90302bdbb17b9b815e732d3c4720583ff1b198499d78470466c\"},\n+    {file = \"Deprecated-1.2.14.tar.gz\", hash = \"sha256:e5323eb936458dccc2582dc6f9c322c852a775a27065ff2b0c4970b9d53d01b3\"},\n+]\n+\n+[package.dependencies]\n+wrapt = \">=1.10,<2\"\n+\n+[package.extras]\n+dev = [\"PyTest\", \"PyTest-Cov\", \"bump2version (<1)\", \"sphinx (<2)\", \"tox\"]\n+\n+[[package]]\n+name = \"exceptiongroup\"\n+version = \"1.2.2\"\n+description = \"Backport of PEP 654 (exception groups)\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"exceptiongroup-1.2.2-py3-none-any.whl\", hash = \"sha256:3111b9d131c238bec2f8f516e123e14ba243563fb135d3fe885990585aa7795b\"},\n+    {file = \"exceptiongroup-1.2.2.tar.gz\", hash = \"sha256:47c2edf7c6738fafb49fd34290706d1a1a2f4d1c6df275526b62cbb4aa5393cc\"},\n+]\n+\n+[package.extras]\n+test = [\"pytest (>=6)\"]\n+\n+[[package]]\n+name = \"flake8\"\n+version = \"7.0.0\"\n+description = \"the modular source code checker: pep8 pyflakes and co\"\n+optional = false\n+python-versions = \">=3.8.1\"\n+files = [\n+    {file = \"flake8-7.0.0-py2.py3-none-any.whl\", hash = \"sha256:a6dfbb75e03252917f2473ea9653f7cd799c3064e54d4c8140044c5c065f53c3\"},\n+    {file = \"flake8-7.0.0.tar.gz\", hash = \"sha256:33f96621059e65eec474169085dc92bf26e7b2d47366b70be2f67ab80dc25132\"},\n+]\n+\n+[package.dependencies]\n+mccabe = \">=0.7.0,<0.8.0\"\n+pycodestyle = \">=2.11.0,<2.12.0\"\n+pyflakes = \">=3.2.0,<3.3.0\"\n+\n+[[package]]\n+name = \"idna\"\n+version = \"3.10\"\n+description = \"Internationalized Domain Names in Applications (IDNA)\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"idna-3.10-py3-none-any.whl\", hash = \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\"},\n+    {file = \"idna-3.10.tar.gz\", hash = \"sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9\"},\n+]\n+\n+[package.extras]\n+all = [\"flake8 (>=7.1.1)\", \"mypy (>=1.11.2)\", \"pytest (>=8.3.2)\", \"ruff (>=0.6.2)\"]\n+\n+[[package]]\n+name = \"importlib-metadata\"\n+version = \"8.4.0\"\n+description = \"Read metadata from Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"importlib_metadata-8.4.0-py3-none-any.whl\", hash = \"sha256:66f342cc6ac9818fc6ff340576acd24d65ba0b3efabb2b4ac08b598965a4a2f1\"},\n+    {file = \"importlib_metadata-8.4.0.tar.gz\", hash = \"sha256:9a547d3bc3608b025f93d403fdd1aae741c24fbb8314df4b155675742ce303c5\"},\n+]\n+\n+[package.dependencies]\n+zipp = \">=0.5\"\n+\n+[package.extras]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n+perf = [\"ipython\"]\n+test = [\"flufl.flake8\", \"importlib-resources (>=1.3)\", \"jaraco.test (>=5.4)\", \"packaging\", \"pyfakefs\", \"pytest (>=6,!=8.1.*)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=2.2)\", \"pytest-mypy\", \"pytest-perf (>=0.9.2)\", \"pytest-ruff (>=0.2.1)\"]\n+\n+[[package]]\n+name = \"iniconfig\"\n+version = \"2.0.0\"\n+description = \"brain-dead simple config-ini parsing\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"iniconfig-2.0.0-py3-none-any.whl\", hash = \"sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\"},\n+    {file = \"iniconfig-2.0.0.tar.gz\", hash = \"sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3\"},\n+]\n+\n+[[package]]\n+name = \"jmespath\"\n+version = \"1.0.1\"\n+description = \"JSON Matching Expressions\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"jmespath-1.0.1-py3-none-any.whl\", hash = \"sha256:02e2e4cc71b5bcab88332eebf907519190dd9e6e82107fa7f83b1003a6252980\"},\n+    {file = \"jmespath-1.0.1.tar.gz\", hash = \"sha256:90261b206d6defd58fdd5e85f478bf633a2901798906be2ad389150c5c60edbe\"},\n+]\n+\n+[[package]]\n+name = \"mccabe\"\n+version = \"0.7.0\"\n+description = \"McCabe checker, plugin for flake8\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"mccabe-0.7.0-py2.py3-none-any.whl\", hash = \"sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e\"},\n+    {file = \"mccabe-0.7.0.tar.gz\", hash = \"sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325\"},\n+]\n+\n+[[package]]\n+name = \"multidict\"\n+version = \"6.1.0\"\n+description = \"multidict implementation\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"multidict-6.1.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:3380252550e372e8511d49481bd836264c009adb826b23fefcc5dd3c69692f60\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:99f826cbf970077383d7de805c0681799491cb939c25450b9b5b3ced03ca99f1\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:a114d03b938376557927ab23f1e950827c3b893ccb94b62fd95d430fd0e5cf53\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b1c416351ee6271b2f49b56ad7f308072f6f44b37118d69c2cad94f3fa8a40d5\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:6b5d83030255983181005e6cfbac1617ce9746b219bc2aad52201ad121226581\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3e97b5e938051226dc025ec80980c285b053ffb1e25a3db2a3aa3bc046bf7f56\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d618649d4e70ac6efcbba75be98b26ef5078faad23592f9b51ca492953012429\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:10524ebd769727ac77ef2278390fb0068d83f3acb7773792a5080f2b0abf7748\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:ff3827aef427c89a25cc96ded1759271a93603aba9fb977a6d264648ebf989db\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:06809f4f0f7ab7ea2cabf9caca7d79c22c0758b58a71f9d32943ae13c7ace056\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-musllinux_1_2_ppc64le.whl\", hash = \"sha256:f179dee3b863ab1c59580ff60f9d99f632f34ccb38bf67a33ec6b3ecadd0fd76\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-musllinux_1_2_s390x.whl\", hash = \"sha256:aaed8b0562be4a0876ee3b6946f6869b7bcdb571a5d1496683505944e268b160\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:3c8b88a2ccf5493b6c8da9076fb151ba106960a2df90c2633f342f120751a9e7\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-win32.whl\", hash = \"sha256:4a9cb68166a34117d6646c0023c7b759bf197bee5ad4272f420a0141d7eb03a0\"},\n+    {file = \"multidict-6.1.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:20b9b5fbe0b88d0bdef2012ef7dee867f874b72528cf1d08f1d59b0e3850129d\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:3efe2c2cb5763f2f1b275ad2bf7a287d3f7ebbef35648a9726e3b69284a4f3d6\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:c7053d3b0353a8b9de430a4f4b4268ac9a4fb3481af37dfe49825bf45ca24156\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:27e5fc84ccef8dfaabb09d82b7d179c7cf1a3fbc8a966f8274fcb4ab2eb4cadb\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0e2b90b43e696f25c62656389d32236e049568b39320e2735d51f08fd362761b\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d83a047959d38a7ff552ff94be767b7fd79b831ad1cd9920662db05fec24fe72\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d1a9dd711d0877a1ece3d2e4fea11a8e75741ca21954c919406b44e7cf971304\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ec2abea24d98246b94913b76a125e855eb5c434f7c46546046372fe60f666351\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4867cafcbc6585e4b678876c489b9273b13e9fff9f6d6d66add5e15d11d926cb\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:5b48204e8d955c47c55b72779802b219a39acc3ee3d0116d5080c388970b76e3\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:d8fff389528cad1618fb4b26b95550327495462cd745d879a8c7c2115248e399\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-musllinux_1_2_ppc64le.whl\", hash = \"sha256:a7a9541cd308eed5e30318430a9c74d2132e9a8cb46b901326272d780bf2d423\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-musllinux_1_2_s390x.whl\", hash = \"sha256:da1758c76f50c39a2efd5e9859ce7d776317eb1dd34317c8152ac9251fc574a3\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:c943a53e9186688b45b323602298ab727d8865d8c9ee0b17f8d62d14b56f0753\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-win32.whl\", hash = \"sha256:90f8717cb649eea3504091e640a1b8568faad18bd4b9fcd692853a04475a4b80\"},\n+    {file = \"multidict-6.1.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:82176036e65644a6cc5bd619f65f6f19781e8ec2e5330f51aa9ada7504cc1926\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:b04772ed465fa3cc947db808fa306d79b43e896beb677a56fb2347ca1a49c1fa\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:6180c0ae073bddeb5a97a38c03f30c233e0a4d39cd86166251617d1bbd0af436\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:071120490b47aa997cca00666923a83f02c7fbb44f71cf7f136df753f7fa8761\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:50b3a2710631848991d0bf7de077502e8994c804bb805aeb2925a981de58ec2e\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b58c621844d55e71c1b7f7c498ce5aa6985d743a1a59034c57a905b3f153c1ef\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:55b6d90641869892caa9ca42ff913f7ff1c5ece06474fbd32fb2cf6834726c95\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4b820514bfc0b98a30e3d85462084779900347e4d49267f747ff54060cc33925\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:10a9b09aba0c5b48c53761b7c720aaaf7cf236d5fe394cd399c7ba662d5f9966\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:1e16bf3e5fc9f44632affb159d30a437bfe286ce9e02754759be5536b169b305\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:76f364861c3bfc98cbbcbd402d83454ed9e01a5224bb3a28bf70002a230f73e2\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-musllinux_1_2_ppc64le.whl\", hash = \"sha256:820c661588bd01a0aa62a1283f20d2be4281b086f80dad9e955e690c75fb54a2\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-musllinux_1_2_s390x.whl\", hash = \"sha256:0e5f362e895bc5b9e67fe6e4ded2492d8124bdf817827f33c5b46c2fe3ffaca6\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:3ec660d19bbc671e3a6443325f07263be452c453ac9e512f5eb935e7d4ac28b3\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-win32.whl\", hash = \"sha256:58130ecf8f7b8112cdb841486404f1282b9c86ccb30d3519faf301b2e5659133\"},\n+    {file = \"multidict-6.1.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:188215fc0aafb8e03341995e7c4797860181562380f81ed0a87ff455b70bf1f1\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:d569388c381b24671589335a3be6e1d45546c2988c2ebe30fdcada8457a31008\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:052e10d2d37810b99cc170b785945421141bf7bb7d2f8799d431e7db229c385f\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:f90c822a402cb865e396a504f9fc8173ef34212a342d92e362ca498cad308e28\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b225d95519a5bf73860323e633a664b0d85ad3d5bede6d30d95b35d4dfe8805b\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:23bfd518810af7de1116313ebd9092cb9aa629beb12f6ed631ad53356ed6b86c\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5c09fcfdccdd0b57867577b719c69e347a436b86cd83747f179dbf0cc0d4c1f3\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bf6bea52ec97e95560af5ae576bdac3aa3aae0b6758c6efa115236d9e07dae44\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:57feec87371dbb3520da6192213c7d6fc892d5589a93db548331954de8248fd2\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:0c3f390dc53279cbc8ba976e5f8035eab997829066756d811616b652b00a23a3\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:59bfeae4b25ec05b34f1956eaa1cb38032282cd4dfabc5056d0a1ec4d696d3aa\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-musllinux_1_2_ppc64le.whl\", hash = \"sha256:b2f59caeaf7632cc633b5cf6fc449372b83bbdf0da4ae04d5be36118e46cc0aa\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-musllinux_1_2_s390x.whl\", hash = \"sha256:37bb93b2178e02b7b618893990941900fd25b6b9ac0fa49931a40aecdf083fe4\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:4e9f48f58c2c523d5a06faea47866cd35b32655c46b443f163d08c6d0ddb17d6\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-win32.whl\", hash = \"sha256:3a37ffb35399029b45c6cc33640a92bef403c9fd388acce75cdc88f58bd19a81\"},\n+    {file = \"multidict-6.1.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:e9aa71e15d9d9beaad2c6b9319edcdc0a49a43ef5c0a4c8265ca9ee7d6c67774\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:db7457bac39421addd0c8449933ac32d8042aae84a14911a757ae6ca3eef1392\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:d094ddec350a2fb899fec68d8353c78233debde9b7d8b4beeafa70825f1c281a\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:5845c1fd4866bb5dd3125d89b90e57ed3138241540897de748cdf19de8a2fca2\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9079dfc6a70abe341f521f78405b8949f96db48da98aeb43f9907f342f627cdc\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3914f5aaa0f36d5d60e8ece6a308ee1c9784cd75ec8151062614657a114c4478\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c08be4f460903e5a9d0f76818db3250f12e9c344e79314d1d570fc69d7f4eae4\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d093be959277cb7dee84b801eb1af388b6ad3ca6a6b6bf1ed7585895789d027d\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:3702ea6872c5a2a4eeefa6ffd36b042e9773f05b1f37ae3ef7264b1163c2dcf6\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:2090f6a85cafc5b2db085124d752757c9d251548cedabe9bd31afe6363e0aff2\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:f67f217af4b1ff66c68a87318012de788dd95fcfeb24cc889011f4e1c7454dfd\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-musllinux_1_2_ppc64le.whl\", hash = \"sha256:189f652a87e876098bbc67b4da1049afb5f5dfbaa310dd67c594b01c10388db6\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-musllinux_1_2_s390x.whl\", hash = \"sha256:6bb5992037f7a9eff7991ebe4273ea7f51f1c1c511e6a2ce511d0e7bdb754492\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:ac10f4c2b9e770c4e393876e35a7046879d195cd123b4f116d299d442b335bcd\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-win32.whl\", hash = \"sha256:e27bbb6d14416713a8bd7aaa1313c0fc8d44ee48d74497a0ff4c3a1b6ccb5167\"},\n+    {file = \"multidict-6.1.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:22f3105d4fb15c8f57ff3959a58fcab6ce36814486500cd7485651230ad4d4ef\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:4e18b656c5e844539d506a0a06432274d7bd52a7487e6828c63a63d69185626c\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:a185f876e69897a6f3325c3f19f26a297fa058c5e456bfcff8015e9a27e83ae1\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:ab7c4ceb38d91570a650dba194e1ca87c2b543488fe9309b4212694174fd539c\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e617fb6b0b6953fffd762669610c1c4ffd05632c138d61ac7e14ad187870669c\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:16e5f4bf4e603eb1fdd5d8180f1a25f30056f22e55ce51fb3d6ad4ab29f7d96f\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f4c035da3f544b1882bac24115f3e2e8760f10a0107614fc9839fd232200b875\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:957cf8e4b6e123a9eea554fa7ebc85674674b713551de587eb318a2df3e00255\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:483a6aea59cb89904e1ceabd2b47368b5600fb7de78a6e4a2c2987b2d256cf30\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:87701f25a2352e5bf7454caa64757642734da9f6b11384c1f9d1a8e699758057\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:682b987361e5fd7a139ed565e30d81fd81e9629acc7d925a205366877d8c8657\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-musllinux_1_2_ppc64le.whl\", hash = \"sha256:ce2186a7df133a9c895dea3331ddc5ddad42cdd0d1ea2f0a51e5d161e4762f28\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-musllinux_1_2_s390x.whl\", hash = \"sha256:9f636b730f7e8cb19feb87094949ba54ee5357440b9658b2a32a5ce4bce53972\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:73eae06aa53af2ea5270cc066dcaf02cc60d2994bbb2c4ef5764949257d10f43\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-win32.whl\", hash = \"sha256:1ca0083e80e791cffc6efce7660ad24af66c8d4079d2a750b29001b53ff59ada\"},\n+    {file = \"multidict-6.1.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:aa466da5b15ccea564bdab9c89175c762bc12825f4659c11227f515cee76fa4a\"},\n+    {file = \"multidict-6.1.0-py3-none-any.whl\", hash = \"sha256:48e171e52d1c4d33888e529b999e5900356b9ae588c2f09a52dcefb158b27506\"},\n+    {file = \"multidict-6.1.0.tar.gz\", hash = \"sha256:22ae2ebf9b0c69d206c003e2f6a914ea33f0a932d4aa16f236afc049d9958f4a\"},\n+]\n+\n+[package.dependencies]\n+typing-extensions = {version = \">=4.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[[package]]\n+name = \"opentelemetry-api\"\n+version = \"1.27.0\"\n+description = \"OpenTelemetry Python API\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"opentelemetry_api-1.27.0-py3-none-any.whl\", hash = \"sha256:953d5871815e7c30c81b56d910c707588000fff7a3ca1c73e6531911d53065e7\"},\n+    {file = \"opentelemetry_api-1.27.0.tar.gz\", hash = \"sha256:ed673583eaa5f81b5ce5e86ef7cdaf622f88ef65f0b9aab40b843dcae5bef342\"},\n+]\n+\n+[package.dependencies]\n+deprecated = \">=1.2.6\"\n+importlib-metadata = \">=6.0,<=8.4.0\"\n+\n+[[package]]\n+name = \"opentelemetry-instrumentation\"\n+version = \"0.48b0\"\n+description = \"Instrumentation Tools & Auto Instrumentation for OpenTelemetry Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"opentelemetry_instrumentation-0.48b0-py3-none-any.whl\", hash = \"sha256:a69750dc4ba6a5c3eb67986a337185a25b739966d80479befe37b546fc870b44\"},\n+    {file = \"opentelemetry_instrumentation-0.48b0.tar.gz\", hash = \"sha256:94929685d906380743a71c3970f76b5f07476eea1834abd5dd9d17abfe23cc35\"},\n+]\n+\n+[package.dependencies]\n+opentelemetry-api = \">=1.4,<2.0\"\n+setuptools = \">=16.0\"\n+wrapt = \">=1.0.0,<2.0.0\"\n+\n+[[package]]\n+name = \"opentelemetry-sdk\"\n+version = \"1.27.0\"\n+description = \"OpenTelemetry Python SDK\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"opentelemetry_sdk-1.27.0-py3-none-any.whl\", hash = \"sha256:365f5e32f920faf0fd9e14fdfd92c086e317eaa5f860edba9cdc17a380d9197d\"},\n+    {file = \"opentelemetry_sdk-1.27.0.tar.gz\", hash = \"sha256:d525017dea0ccce9ba4e0245100ec46ecdc043f2d7b8315d56b19aff0904fa6f\"},\n+]\n+\n+[package.dependencies]\n+opentelemetry-api = \"1.27.0\"\n+opentelemetry-semantic-conventions = \"0.48b0\"\n+typing-extensions = \">=3.7.4\"\n+\n+[[package]]\n+name = \"opentelemetry-semantic-conventions\"\n+version = \"0.48b0\"\n+description = \"OpenTelemetry Semantic Conventions\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl\", hash = \"sha256:a0de9f45c413a8669788a38569c7e0a11ce6ce97861a628cca785deecdc32a1f\"},\n+    {file = \"opentelemetry_semantic_conventions-0.48b0.tar.gz\", hash = \"sha256:12d74983783b6878162208be57c9effcb89dc88691c64992d70bb89dc00daa1a\"},\n+]\n+\n+[package.dependencies]\n+deprecated = \">=1.2.6\"\n+opentelemetry-api = \"1.27.0\"\n+\n+[[package]]\n+name = \"opentelemetry-semantic-conventions-ai\"\n+version = \"0.4.1\"\n+description = \"OpenTelemetry Semantic Conventions Extension for Large Language Models\"\n+optional = false\n+python-versions = \"<4,>=3.9\"\n+files = [\n+    {file = \"opentelemetry_semantic_conventions_ai-0.4.1-py3-none-any.whl\", hash = \"sha256:b6c6e3976a5ea31058faeaf0450a6a56d4576a9734c94c1a4cb82332ee635fe3\"},\n+    {file = \"opentelemetry_semantic_conventions_ai-0.4.1.tar.gz\", hash = \"sha256:aaf59b2f24d745692170b96d86d7c5560f42443dcf88ced49ae9d4542db1902f\"},\n+]\n+\n+[[package]]\n+name = \"packaging\"\n+version = \"24.1\"\n+description = \"Core utilities for Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"packaging-24.1-py3-none-any.whl\", hash = \"sha256:5b8f2217dbdbd2f7f384c41c628544e6d52f2d0f53c6d0c3ea61aa5d1d7ff124\"},\n+    {file = \"packaging-24.1.tar.gz\", hash = \"sha256:026ed72c8ed3fcce5bf8950572258698927fd1dbda10a5e981cdf0ac37f4f002\"},\n+]\n+\n+[[package]]\n+name = \"pluggy\"\n+version = \"1.5.0\"\n+description = \"plugin and hook calling mechanisms for python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pluggy-1.5.0-py3-none-any.whl\", hash = \"sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669\"},\n+    {file = \"pluggy-1.5.0.tar.gz\", hash = \"sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1\"},\n+]\n+\n+[package.extras]\n+dev = [\"pre-commit\", \"tox\"]\n+testing = [\"pytest\", \"pytest-benchmark\"]\n+\n+[[package]]\n+name = \"pycodestyle\"\n+version = \"2.11.1\"\n+description = \"Python style guide checker\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pycodestyle-2.11.1-py2.py3-none-any.whl\", hash = \"sha256:44fe31000b2d866f2e41841b18528a505fbd7fef9017b04eff4e2648a0fadc67\"},\n+    {file = \"pycodestyle-2.11.1.tar.gz\", hash = \"sha256:41ba0e7afc9752dfb53ced5489e89f8186be00e599e712660695b7a75ff2663f\"},\n+]\n+\n+[[package]]\n+name = \"pyflakes\"\n+version = \"3.2.0\"\n+description = \"passive checker of Python programs\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pyflakes-3.2.0-py2.py3-none-any.whl\", hash = \"sha256:84b5be138a2dfbb40689ca07e2152deb896a65c3a3e24c251c5c62489568074a\"},\n+    {file = \"pyflakes-3.2.0.tar.gz\", hash = \"sha256:1c61603ff154621fb2a9172037d84dca3500def8c8b630657d1701f026f8af3f\"},\n+]\n+\n+[[package]]\n+name = \"pytest\"\n+version = \"8.3.3\"\n+description = \"pytest: simple powerful testing with Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pytest-8.3.3-py3-none-any.whl\", hash = \"sha256:a6853c7375b2663155079443d2e45de913a911a11d669df02a50814944db57b2\"},\n+    {file = \"pytest-8.3.3.tar.gz\", hash = \"sha256:70b98107bd648308a7952b06e6ca9a50bc660be218d53c257cc1fc94fda10181\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+exceptiongroup = {version = \">=1.0.0rc8\", markers = \"python_version < \\\"3.11\\\"\"}\n+iniconfig = \"*\"\n+packaging = \"*\"\n+pluggy = \">=1.5,<2\"\n+tomli = {version = \">=1\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+dev = [\"argcomplete\", \"attrs (>=19.2)\", \"hypothesis (>=3.56)\", \"mock\", \"pygments (>=2.7.2)\", \"requests\", \"setuptools\", \"xmlschema\"]\n+\n+[[package]]\n+name = \"pytest-recording\"\n+version = \"0.13.2\"\n+description = \"A pytest plugin that allows you recording of network interactions via VCR.py\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"pytest_recording-0.13.2-py3-none-any.whl\", hash = \"sha256:3820fe5743d1ac46e807989e11d073cb776a60bdc544cf43ebca454051b22d13\"},\n+    {file = \"pytest_recording-0.13.2.tar.gz\", hash = \"sha256:000c3babbb466681457fd65b723427c1779a0c6c17d9e381c3142a701e124877\"},\n+]\n+\n+[package.dependencies]\n+pytest = \">=3.5.0\"\n+vcrpy = \">=2.0.1\"\n+\n+[package.extras]\n+dev = [\"pytest-httpbin\", \"pytest-mock\", \"requests\", \"werkzeug (==3.0.3)\"]\n+tests = [\"pytest-httpbin\", \"pytest-mock\", \"requests\", \"werkzeug (==3.0.3)\"]\n+\n+[[package]]\n+name = \"pytest-sugar\"\n+version = \"1.0.0\"\n+description = \"pytest-sugar is a plugin for pytest that changes the default look and feel of pytest (e.g. progressbar, show tests that fail instantly).\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"pytest-sugar-1.0.0.tar.gz\", hash = \"sha256:6422e83258f5b0c04ce7c632176c7732cab5fdb909cb39cca5c9139f81276c0a\"},\n+    {file = \"pytest_sugar-1.0.0-py3-none-any.whl\", hash = \"sha256:70ebcd8fc5795dc457ff8b69d266a4e2e8a74ae0c3edc749381c64b5246c8dfd\"},\n+]\n+\n+[package.dependencies]\n+packaging = \">=21.3\"\n+pytest = \">=6.2.0\"\n+termcolor = \">=2.1.0\"\n+\n+[package.extras]\n+dev = [\"black\", \"flake8\", \"pre-commit\"]\n+\n+[[package]]\n+name = \"python-dateutil\"\n+version = \"2.9.0.post0\"\n+description = \"Extensions to the standard Python datetime module\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,>=2.7\"\n+files = [\n+    {file = \"python-dateutil-2.9.0.post0.tar.gz\", hash = \"sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3\"},\n+    {file = \"python_dateutil-2.9.0.post0-py2.py3-none-any.whl\", hash = \"sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427\"},\n+]\n+\n+[package.dependencies]\n+six = \">=1.5\"\n+\n+[[package]]\n+name = \"pyyaml\"\n+version = \"6.0.2\"\n+description = \"YAML parser and emitter for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:0a9a2848a5b7feac301353437eb7d5957887edbf81d56e903999a75a3d743086\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:29717114e51c84ddfba879543fb232a6ed60086602313ca38cce623c1d62cfbf\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8824b5a04a04a047e72eea5cec3bc266db09e35de6bdfe34c9436ac5ee27d237\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:7c36280e6fb8385e520936c3cb3b8042851904eba0e58d277dca80a5cfed590b\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ec031d5d2feb36d1d1a24380e4db6d43695f3748343d99434e6f5f9156aaa2ed\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:936d68689298c36b53b29f23c6dbb74de12b4ac12ca6cfe0e047bedceea56180\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:23502f431948090f597378482b4812b0caae32c22213aecf3b55325e049a6c68\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-win32.whl\", hash = \"sha256:2e99c6826ffa974fe6e27cdb5ed0021786b03fc98e5ee3c5bfe1fd5015f42b99\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:a4d3091415f010369ae4ed1fc6b79def9416358877534caf6a0fdd2146c87a3e\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-win32.whl\", hash = \"sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-win32.whl\", hash = \"sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-win32.whl\", hash = \"sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-win_amd64.whl\", hash = \"sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:24471b829b3bf607e04e88d79542a9d48bb037c2267d7927a874e6c205ca7e9a\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d7fded462629cfa4b685c5416b949ebad6cec74af5e2d42905d41e257e0869f5\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d84a1718ee396f54f3a086ea0a66d8e552b2ab2017ef8b420e92edbc841c352d\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9056c1ecd25795207ad294bcf39f2db3d845767be0ea6e6a34d856f006006083\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:82d09873e40955485746739bcb8b4586983670466c23382c19cffecbf1fd8706\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-win32.whl\", hash = \"sha256:43fa96a3ca0d6b1812e01ced1044a003533c47f6ee8aca31724f78e93ccc089a\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:01179a4a8559ab5de078078f37e5c1a30d76bb88519906844fd7bdea1b7729ff\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:688ba32a1cffef67fd2e9398a2efebaea461578b0923624778664cc1c914db5d\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:a8786accb172bd8afb8be14490a16625cbc387036876ab6ba70912730faf8e1f\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d8e03406cac8513435335dbab54c0d385e4a49e4945d2909a581c83647ca0290\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f753120cb8181e736c57ef7636e83f31b9c0d1722c516f7e86cf15b7aa57ff12\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3b1fdb9dc17f5a7677423d508ab4f243a726dea51fa5e70992e59a7411c89d19\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:0b69e4ce7a131fe56b7e4d770c67429700908fc0752af059838b1cfb41960e4e\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:a9f8c2e67970f13b16084e04f134610fd1d374bf477b17ec1599185cf611d725\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-win32.whl\", hash = \"sha256:6395c297d42274772abc367baaa79683958044e5d3835486c16da75d2a694631\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:39693e1f8320ae4f43943590b49779ffb98acb81f788220ea932a6b6c51004d8\"},\n+    {file = \"pyyaml-6.0.2.tar.gz\", hash = \"sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e\"},\n+]\n+\n+[[package]]\n+name = \"s3transfer\"\n+version = \"0.10.2\"\n+description = \"An Amazon S3 Transfer Manager\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"s3transfer-0.10.2-py3-none-any.whl\", hash = \"sha256:eca1c20de70a39daee580aef4986996620f365c4e0fda6a86100231d62f1bf69\"},\n+    {file = \"s3transfer-0.10.2.tar.gz\", hash = \"sha256:0711534e9356d3cc692fdde846b4a1e4b0cb6519971860796e6bc4c7aea00ef6\"},\n+]\n+\n+[package.dependencies]\n+botocore = \">=1.33.2,<2.0a.0\"\n+\n+[package.extras]\n+crt = [\"botocore[crt] (>=1.33.2,<2.0a.0)\"]\n+\n+[[package]]\n+name = \"setuptools\"\n+version = \"75.1.0\"\n+description = \"Easily download, build, install, upgrade, and uninstall Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"setuptools-75.1.0-py3-none-any.whl\", hash = \"sha256:35ab7fd3bcd95e6b7fd704e4a1539513edad446c097797f2985e0e4b960772f2\"},\n+    {file = \"setuptools-75.1.0.tar.gz\", hash = \"sha256:d59a21b17a275fb872a9c3dae73963160ae079f1049ed956880cd7c09b120538\"},\n+]\n+\n+[package.extras]\n+check = [\"pytest-checkdocs (>=2.4)\", \"pytest-ruff (>=0.2.1)\", \"ruff (>=0.5.2)\"]\n+core = [\"importlib-metadata (>=6)\", \"importlib-resources (>=5.10.2)\", \"jaraco.collections\", \"jaraco.functools\", \"jaraco.text (>=3.7)\", \"more-itertools\", \"more-itertools (>=8.8)\", \"packaging\", \"packaging (>=24)\", \"platformdirs (>=2.6.2)\", \"tomli (>=2.0.1)\", \"wheel (>=0.43.0)\"]\n+cover = [\"pytest-cov\"]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"pyproject-hooks (!=1.1)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (>=1,<2)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\", \"towncrier (<24.7)\"]\n+enabler = [\"pytest-enabler (>=2.2)\"]\n+test = [\"build[virtualenv] (>=1.0.3)\", \"filelock (>=3.4.0)\", \"ini2toml[lite] (>=0.14)\", \"jaraco.develop (>=7.21)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"jaraco.test\", \"packaging (>=23.2)\", \"pip (>=19.1)\", \"pyproject-hooks (!=1.1)\", \"pytest (>=6,!=8.1.*)\", \"pytest-home (>=0.5)\", \"pytest-perf\", \"pytest-subprocess\", \"pytest-timeout\", \"pytest-xdist (>=3)\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel (>=0.44.0)\"]\n+type = [\"importlib-metadata (>=7.0.2)\", \"jaraco.develop (>=7.21)\", \"mypy (==1.11.*)\", \"pytest-mypy\"]\n+\n+[[package]]\n+name = \"six\"\n+version = \"1.16.0\"\n+description = \"Python 2 and 3 compatibility utilities\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*\"\n+files = [\n+    {file = \"six-1.16.0-py2.py3-none-any.whl\", hash = \"sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\"},\n+    {file = \"six-1.16.0.tar.gz\", hash = \"sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926\"},\n+]\n+\n+[[package]]\n+name = \"termcolor\"\n+version = \"2.4.0\"\n+description = \"ANSI color formatting for output in terminal\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"termcolor-2.4.0-py3-none-any.whl\", hash = \"sha256:9297c0df9c99445c2412e832e882a7884038a25617c60cea2ad69488d4040d63\"},\n+    {file = \"termcolor-2.4.0.tar.gz\", hash = \"sha256:aab9e56047c8ac41ed798fa36d892a37aca6b3e9159f3e0c24bc64a9b3ac7b7a\"},\n+]\n+\n+[package.extras]\n+tests = [\"pytest\", \"pytest-cov\"]\n+\n+[[package]]\n+name = \"tomli\"\n+version = \"2.0.1\"\n+description = \"A lil' TOML parser\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"tomli-2.0.1-py3-none-any.whl\", hash = \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\"},\n+    {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\n+]\n+\n+[[package]]\n+name = \"typing-extensions\"\n+version = \"4.12.2\"\n+description = \"Backported and Experimental Type Hints for Python 3.8+\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"typing_extensions-4.12.2-py3-none-any.whl\", hash = \"sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d\"},\n+    {file = \"typing_extensions-4.12.2.tar.gz\", hash = \"sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\"},\n+]\n+\n+[[package]]\n+name = \"urllib3\"\n+version = \"1.26.20\"\n+description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,>=2.7\"\n+files = [\n+    {file = \"urllib3-1.26.20-py2.py3-none-any.whl\", hash = \"sha256:0ed14ccfbf1c30a9072c7ca157e4319b70d65f623e91e7b32fadb2853431016e\"},\n+    {file = \"urllib3-1.26.20.tar.gz\", hash = \"sha256:40c2dc0c681e47eb8f90e7e27bf6ff7df2e677421fd46756da1161c39ca70d32\"},\n+]\n+\n+[package.extras]\n+brotli = [\"brotli (==1.0.9)\", \"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\", \"brotlipy (>=0.6.0)\"]\n+secure = [\"certifi\", \"cryptography (>=1.3.4)\", \"idna (>=2.0.0)\", \"ipaddress\", \"pyOpenSSL (>=0.14)\", \"urllib3-secure-extra\"]\n+socks = [\"PySocks (>=1.5.6,!=1.5.7,<2.0)\"]\n+\n+[[package]]\n+name = \"urllib3\"\n+version = \"2.2.3\"\n+description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"urllib3-2.2.3-py3-none-any.whl\", hash = \"sha256:ca899ca043dcb1bafa3e262d73aa25c465bfb49e0bd9dd5d59f1d0acba2f8fac\"},\n+    {file = \"urllib3-2.2.3.tar.gz\", hash = \"sha256:e7d814a81dad81e6caf2ec9fdedb284ecc9c73076b62654547cc64ccdcae26e9\"},\n+]\n+\n+[package.extras]\n+brotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\n+h2 = [\"h2 (>=4,<5)\"]\n+socks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\n+zstd = [\"zstandard (>=0.18.0)\"]\n+\n+[[package]]\n+name = \"vcrpy\"\n+version = \"6.0.1\"\n+description = \"Automatically mock your HTTP interactions to simplify and speed up testing\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"vcrpy-6.0.1-py2.py3-none-any.whl\", hash = \"sha256:621c3fb2d6bd8aa9f87532c688e4575bcbbde0c0afeb5ebdb7e14cac409edfdd\"},\n+    {file = \"vcrpy-6.0.1.tar.gz\", hash = \"sha256:9e023fee7f892baa0bbda2f7da7c8ac51165c1c6e38ff8688683a12a4bde9278\"},\n+]\n+\n+[package.dependencies]\n+PyYAML = \"*\"\n+urllib3 = {version = \"<2\", markers = \"platform_python_implementation == \\\"PyPy\\\" or python_version < \\\"3.10\\\"\"}\n+wrapt = \"*\"\n+yarl = \"*\"\n+\n+[package.extras]\n+tests = [\"Werkzeug (==2.0.3)\", \"aiohttp\", \"boto3\", \"httplib2\", \"httpx\", \"pytest\", \"pytest-aiohttp\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-httpbin\", \"requests (>=2.22.0)\", \"tornado\", \"urllib3\"]\n+\n+[[package]]\n+name = \"wrapt\"\n+version = \"1.16.0\"\n+description = \"Module for decorators, wrappers and monkey patching.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"wrapt-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:ffa565331890b90056c01db69c0fe634a776f8019c143a5ae265f9c6bc4bd6d4\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:e4fdb9275308292e880dcbeb12546df7f3e0f96c6b41197e0cf37d2826359020\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bb2dee3874a500de01c93d5c71415fcaef1d858370d405824783e7a8ef5db440\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2a88e6010048489cda82b1326889ec075a8c856c2e6a256072b28eaee3ccf487\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ac83a914ebaf589b69f7d0a1277602ff494e21f4c2f743313414378f8f50a4cf\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:73aa7d98215d39b8455f103de64391cb79dfcad601701a3aa0dddacf74911d72\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:807cc8543a477ab7422f1120a217054f958a66ef7314f76dd9e77d3f02cdccd0\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:bf5703fdeb350e36885f2875d853ce13172ae281c56e509f4e6eca049bdfb136\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-win32.whl\", hash = \"sha256:f6b2d0c6703c988d334f297aa5df18c45e97b0af3679bb75059e0e0bd8b1069d\"},\n+    {file = \"wrapt-1.16.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:decbfa2f618fa8ed81c95ee18a387ff973143c656ef800c9f24fb7e9c16054e2\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:1a5db485fe2de4403f13fafdc231b0dbae5eca4359232d2efc79025527375b09\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:75ea7d0ee2a15733684badb16de6794894ed9c55aa5e9903260922f0482e687d\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a452f9ca3e3267cd4d0fcf2edd0d035b1934ac2bd7e0e57ac91ad6b95c0c6389\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:43aa59eadec7890d9958748db829df269f0368521ba6dc68cc172d5d03ed8060\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:72554a23c78a8e7aa02abbd699d129eead8b147a23c56e08d08dfc29cfdddca1\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:d2efee35b4b0a347e0d99d28e884dfd82797852d62fcd7ebdeee26f3ceb72cf3\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:6dcfcffe73710be01d90cae08c3e548d90932d37b39ef83969ae135d36ef3956\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:eb6e651000a19c96f452c85132811d25e9264d836951022d6e81df2fff38337d\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-win32.whl\", hash = \"sha256:66027d667efe95cc4fa945af59f92c5a02c6f5bb6012bff9e60542c74c75c362\"},\n+    {file = \"wrapt-1.16.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:aefbc4cb0a54f91af643660a0a150ce2c090d3652cf4052a5397fb2de549cd89\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:5eb404d89131ec9b4f748fa5cfb5346802e5ee8836f57d516576e61f304f3b7b\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:9090c9e676d5236a6948330e83cb89969f433b1943a558968f659ead07cb3b36\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:94265b00870aa407bd0cbcfd536f17ecde43b94fb8d228560a1e9d3041462d73\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f2058f813d4f2b5e3a9eb2eb3faf8f1d99b81c3e51aeda4b168406443e8ba809\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:98b5e1f498a8ca1858a1cdbffb023bfd954da4e3fa2c0cb5853d40014557248b\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:14d7dc606219cdd7405133c713f2c218d4252f2a469003f8c46bb92d5d095d81\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:49aac49dc4782cb04f58986e81ea0b4768e4ff197b57324dcbd7699c5dfb40b9\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:418abb18146475c310d7a6dc71143d6f7adec5b004ac9ce08dc7a34e2babdc5c\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-win32.whl\", hash = \"sha256:685f568fa5e627e93f3b52fda002c7ed2fa1800b50ce51f6ed1d572d8ab3e7fc\"},\n+    {file = \"wrapt-1.16.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:dcdba5c86e368442528f7060039eda390cc4091bfd1dca41e8046af7c910dda8\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:d462f28826f4657968ae51d2181a074dfe03c200d6131690b7d65d55b0f360f8\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a33a747400b94b6d6b8a165e4480264a64a78c8a4c734b62136062e9a248dd39\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b3646eefa23daeba62643a58aac816945cadc0afaf21800a1421eeba5f6cfb9c\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3ebf019be5c09d400cf7b024aa52b1f3aeebeff51550d007e92c3c1c4afc2a40\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:0d2691979e93d06a95a26257adb7bfd0c93818e89b1406f5a28f36e0d8c1e1fc\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:1acd723ee2a8826f3d53910255643e33673e1d11db84ce5880675954183ec47e\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:bc57efac2da352a51cc4658878a68d2b1b67dbe9d33c36cb826ca449d80a8465\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-win32.whl\", hash = \"sha256:da4813f751142436b075ed7aa012a8778aa43a99f7b36afe9b742d3ed8bdc95e\"},\n+    {file = \"wrapt-1.16.0-cp36-cp36m-win_amd64.whl\", hash = \"sha256:6f6eac2360f2d543cc875a0e5efd413b6cbd483cb3ad7ebf888884a6e0d2e966\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:a0ea261ce52b5952bf669684a251a66df239ec6d441ccb59ec7afa882265d593\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:7bd2d7ff69a2cac767fbf7a2b206add2e9a210e57947dd7ce03e25d03d2de292\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9159485323798c8dc530a224bd3ffcf76659319ccc7bbd52e01e73bd0241a0c5\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a86373cf37cd7764f2201b76496aba58a52e76dedfaa698ef9e9688bfd9e41cf\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:73870c364c11f03ed072dda68ff7aea6d2a3a5c3fe250d917a429c7432e15228\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:b935ae30c6e7400022b50f8d359c03ed233d45b725cfdd299462f41ee5ffba6f\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:db98ad84a55eb09b3c32a96c576476777e87c520a34e2519d3e59c44710c002c\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-win32.whl\", hash = \"sha256:9153ed35fc5e4fa3b2fe97bddaa7cbec0ed22412b85bcdaf54aeba92ea37428c\"},\n+    {file = \"wrapt-1.16.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:66dfbaa7cfa3eb707bbfcd46dab2bc6207b005cbc9caa2199bcbc81d95071a00\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1dd50a2696ff89f57bd8847647a1c363b687d3d796dc30d4dd4a9d1689a706f0\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:44a2754372e32ab315734c6c73b24351d06e77ffff6ae27d2ecf14cf3d229202\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8e9723528b9f787dc59168369e42ae1c3b0d3fadb2f1a71de14531d321ee05b0\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:dbed418ba5c3dce92619656802cc5355cb679e58d0d89b50f116e4a9d5a9603e\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:941988b89b4fd6b41c3f0bfb20e92bd23746579736b7343283297c4c8cbae68f\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:6a42cd0cfa8ffc1915aef79cb4284f6383d8a3e9dcca70c445dcfdd639d51267\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:1ca9b6085e4f866bd584fb135a041bfc32cab916e69f714a7d1d397f8c4891ca\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:d5e49454f19ef621089e204f862388d29e6e8d8b162efce05208913dde5b9ad6\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-win32.whl\", hash = \"sha256:c31f72b1b6624c9d863fc095da460802f43a7c6868c5dda140f51da24fd47d7b\"},\n+    {file = \"wrapt-1.16.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:490b0ee15c1a55be9c1bd8609b8cecd60e325f0575fc98f50058eae366e01f41\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:9b201ae332c3637a42f02d1045e1d0cccfdc41f1f2f801dafbaa7e9b4797bfc2\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:2076fad65c6736184e77d7d4729b63a6d1ae0b70da4868adeec40989858eb3fb\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c5cd603b575ebceca7da5a3a251e69561bec509e0b46e4993e1cac402b7247b8\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b47cfad9e9bbbed2339081f4e346c93ecd7ab504299403320bf85f7f85c7d46c\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f8212564d49c50eb4565e502814f694e240c55551a5f1bc841d4fcaabb0a9b8a\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:5f15814a33e42b04e3de432e573aa557f9f0f56458745c2074952f564c50e664\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:db2e408d983b0e61e238cf579c09ef7020560441906ca990fe8412153e3b291f\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:edfad1d29c73f9b863ebe7082ae9321374ccb10879eeabc84ba3b69f2579d537\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-win32.whl\", hash = \"sha256:ed867c42c268f876097248e05b6117a65bcd1e63b779e916fe2e33cd6fd0d3c3\"},\n+    {file = \"wrapt-1.16.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:eb1b046be06b0fce7249f1d025cd359b4b80fc1c3e24ad9eca33e0dcdb2e4a35\"},\n+    {file = \"wrapt-1.16.0-py3-none-any.whl\", hash = \"sha256:6906c4100a8fcbf2fa735f6059214bb13b97f75b1a61777fcf6432121ef12ef1\"},\n+    {file = \"wrapt-1.16.0.tar.gz\", hash = \"sha256:5f370f952971e7d17c7d1ead40e49f32345a7f7a5373571ef44d800d06b1899d\"},\n+]\n+\n+[[package]]\n+name = \"yarl\"\n+version = \"1.12.1\"\n+description = \"Yet another URL library\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"yarl-1.12.1-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:64c5b0f2b937fe40d0967516eee5504b23cb247b8b7ffeba7213a467d9646fdc\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:2e430ac432f969ef21770645743611c1618362309e3ad7cab45acd1ad1a540ff\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:3e26e64f42bce5ddf9002092b2c37b13071c2e6413d5c05f9fa9de58ed2f7749\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0103c52f8dfe5d573c856322149ddcd6d28f51b4d4a3ee5c4b3c1b0a05c3d034\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b63465b53baeaf2122a337d4ab57d6bbdd09fcadceb17a974cfa8a0300ad9c67\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:17d4dc4ff47893a06737b8788ed2ba2f5ac4e8bb40281c8603920f7d011d5bdd\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a8b54949267bd5704324397efe9fbb6aa306466dee067550964e994d309db5f1\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:10b690cd78cbaca2f96a7462f303fdd2b596d3978b49892e4b05a7567c591572\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:c85ab016e96a975afbdb9d49ca90f3bca9920ef27c64300843fe91c3d59d8d20\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:c1caa5763d1770216596e0a71b5567f27aac28c95992110212c108ec74589a48\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-musllinux_1_2_ppc64le.whl\", hash = \"sha256:595bbcdbfc4a9c6989d7489dca8510cba053ff46b16c84ffd95ac8e90711d419\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-musllinux_1_2_s390x.whl\", hash = \"sha256:e64f0421892a207d3780903085c1b04efeb53b16803b23d947de5a7261b71355\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:319c206e83e46ec2421b25b300c8482b6fe8a018baca246be308c736d9dab267\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-win32.whl\", hash = \"sha256:da045bd1147d12bd43fb032296640a7cc17a7f2eaba67495988362e99db24fd2\"},\n+    {file = \"yarl-1.12.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:aebbd47df77190ada603157f0b3670d578c110c31746ecc5875c394fdcc59a99\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:28389a68981676bf74e2e199fe42f35d1aa27a9c98e3a03e6f58d2d3d054afe1\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:f736f54565f8dd7e3ab664fef2bc461d7593a389a7f28d4904af8d55a91bd55f\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:6dee0496d5f1a8f57f0f28a16f81a2033fc057a2cf9cd710742d11828f8c80e2\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f8981a94a27ac520a398302afb74ae2c0be1c3d2d215c75c582186a006c9e7b0\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ff54340fc1129e8e181827e2234af3ff659b4f17d9bbe77f43bc19e6577fadec\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:54c8cee662b5f8c30ad7eedfc26123f845f007798e4ff1001d9528fe959fd23c\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e97a29b37830ba1262d8dfd48ddb5b28ad4d3ebecc5d93a9c7591d98641ec737\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6c89894cc6f6ddd993813e79244b36b215c14f65f9e4f1660b1f2ba9e5594b95\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:712ba8722c0699daf186de089ddc4677651eb9875ed7447b2ad50697522cbdd9\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:6e9a9f50892153bad5046c2a6df153224aa6f0573a5a8ab44fc54a1e886f6e21\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-musllinux_1_2_ppc64le.whl\", hash = \"sha256:1d4017e78fb22bc797c089b746230ad78ecd3cdb215bc0bd61cb72b5867da57e\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-musllinux_1_2_s390x.whl\", hash = \"sha256:f494c01b28645c431239863cb17af8b8d15b93b0d697a0320d5dd34cd9d7c2fa\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:de4544b1fb29cf14870c4e2b8a897c0242449f5dcebd3e0366aa0aa3cf58a23a\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-win32.whl\", hash = \"sha256:7564525a4673fde53dee7d4c307a961c0951918f0b8c7f09b2c9e02067cf6504\"},\n+    {file = \"yarl-1.12.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:f23bb1a7a6e8e8b612a164fdd08e683bcc16c76f928d6dbb7bdbee2374fbfee6\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-macosx_10_13_universal2.whl\", hash = \"sha256:a3e2aff8b822ab0e0bdbed9f50494b3a35629c4b9488ae391659973a37a9f53f\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:22dda2799c8d39041d731e02bf7690f0ef34f1691d9ac9dfcb98dd1e94c8b058\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:18c2a7757561f05439c243f517dbbb174cadfae3a72dee4ae7c693f5b336570f\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:835010cc17d0020e7931d39e487d72c8e01c98e669b6896a8b8c9aa8ca69a949\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e2254fe137c4a360b0a13173a56444f756252c9283ba4d267ca8e9081cd140ea\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f6a071d2c3d39b4104f94fc08ab349e9b19b951ad4b8e3b6d7ea92d6ef7ccaf8\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:73a183042ae0918c82ce2df38c3db2409b0eeae88e3afdfc80fb67471a95b33b\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:326b8a079a9afcac0575971e56dabdf7abb2ea89a893e6949b77adfeb058b50e\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:126309c0f52a2219b3d1048aca00766429a1346596b186d51d9fa5d2070b7b13\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:ba1c779b45a399cc25f511c681016626f69e51e45b9d350d7581998722825af9\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-musllinux_1_2_ppc64le.whl\", hash = \"sha256:af1107299cef049ad00a93df4809517be432283a0847bcae48343ebe5ea340dc\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-musllinux_1_2_s390x.whl\", hash = \"sha256:20d817c0893191b2ab0ba30b45b77761e8dfec30a029b7c7063055ca71157f84\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:d4f818f6371970d6a5d1e42878389bbfb69dcde631e4bbac5ec1cb11158565ca\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-win32.whl\", hash = \"sha256:0ac33d22b2604b020569a82d5f8a03ba637ba42cc1adf31f616af70baf81710b\"},\n+    {file = \"yarl-1.12.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:fd24996e12e1ba7c397c44be75ca299da14cde34d74bc5508cce233676cc68d0\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:dea360778e0668a7ad25d7727d03364de8a45bfd5d808f81253516b9f2217765\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:1f50a37aeeb5179d293465e522fd686080928c4d89e0ff215e1f963405ec4def\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:0274b1b7a9c9c32b7bf250583e673ff99fb9fccb389215841e2652d9982de740\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a4f3ab9eb8ab2d585ece959c48d234f7b39ac0ca1954a34d8b8e58a52064bdb3\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:8d31dd0245d88cf7239e96e8f2a99f815b06e458a5854150f8e6f0e61618d41b\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:a96198d5d26f40557d986c1253bfe0e02d18c9d9b93cf389daf1a3c9f7c755fa\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ddae504cfb556fe220efae65e35be63cd11e3c314b202723fc2119ce19f0ca2e\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:bce00f3b1f7f644faae89677ca68645ed5365f1c7f874fdd5ebf730a69640d38\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:eee5ff934b0c9f4537ff9596169d56cab1890918004791a7a06b879b3ba2a7ef\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:4ea99e64b2ad2635e0f0597b63f5ea6c374791ff2fa81cdd4bad8ed9f047f56f\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-musllinux_1_2_ppc64le.whl\", hash = \"sha256:5c667b383529520b8dd6bd496fc318678320cb2a6062fdfe6d3618da6b8790f6\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-musllinux_1_2_s390x.whl\", hash = \"sha256:d920401941cb898ef089422e889759dd403309eb370d0e54f1bdf6ca07fef603\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:501a1576716032cc6d48c7c47bcdc42d682273415a8f2908e7e72cb4625801f3\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-win32.whl\", hash = \"sha256:24416bb5e221e29ddf8aac5b97e94e635ca2c5be44a1617ad6fe32556df44294\"},\n+    {file = \"yarl-1.12.1-cp313-cp313-win_amd64.whl\", hash = \"sha256:71af3766bb46738d12cc288d9b8de7ef6f79c31fd62757e2b8a505fe3680b27f\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:c924deab8105f86980983eced740433fb7554a7f66db73991affa4eda99d5402\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:5fb475a4cdde582c9528bb412b98f899680492daaba318231e96f1a0a1bb0d53\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:36ee0115b9edca904153a66bb74a9ff1ce38caff015de94eadfb9ba8e6ecd317\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2631c9d7386bd2d4ce24ecc6ebf9ae90b3efd713d588d90504eaa77fec4dba01\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:2376d8cf506dffd0e5f2391025ae8675b09711016656590cb03b55894161fcfa\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:24197ba3114cc85ddd4091e19b2ddc62650f2e4a899e51b074dfd52d56cf8c72\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bfdf419bf5d3644f94cd7052954fc233522f5a1b371fc0b00219ebd9c14d5798\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8112f640a4f7e7bf59f7cabf0d47a29b8977528c521d73a64d5cc9e99e48a174\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:607d12f0901f6419a8adceb139847c42c83864b85371f58270e42753f9780fa6\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:664380c7ed524a280b6a2d5d9126389c3e96cd6e88986cdb42ca72baa27421d6\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-musllinux_1_2_ppc64le.whl\", hash = \"sha256:0d0a5e87bc48d76dfcfc16295201e9812d5f33d55b4a0b7cad1025b92bf8b91b\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-musllinux_1_2_s390x.whl\", hash = \"sha256:eff6bac402719c14e17efe845d6b98593c56c843aca6def72080fbede755fd1f\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:22839d1d1eab9e4b427828a88a22beb86f67c14d8ff81175505f1cc8493f3500\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-win32.whl\", hash = \"sha256:717f185086bb9d817d4537dd18d5df5d657598cd00e6fc22e4d54d84de266c1d\"},\n+    {file = \"yarl-1.12.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:71978ba778948760cff528235c951ea0ef7a4f9c84ac5a49975f8540f76c3f73\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:30ffc046ebddccb3c4cac72c1a3e1bc343492336f3ca86d24672e90ccc5e788a\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:f10954b233d4df5cc3137ffa5ced97f8894152df817e5d149bf05a0ef2ab8134\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:2e912b282466444023610e4498e3795c10e7cfd641744524876239fcf01d538d\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6af871f70cfd5b528bd322c65793b5fd5659858cdfaa35fbe563fb99b667ed1f\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c3e4e1f7b08d1ec6b685ccd3e2d762219c550164fbf524498532e39f9413436e\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9a7ee79183f0b17dcede8b6723e7da2ded529cf159a878214be9a5d3098f5b1e\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:96c8ff1e1dd680e38af0887927cab407a4e51d84a5f02ae3d6eb87233036c763\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7e9905fc2dc1319e4c39837b906a024cf71b1261cc66b0cd89678f779c0c61f5\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:01549468858b87d36f967c97d02e6e54106f444aeb947ed76f8f71f85ed07cec\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:96b34830bd6825ca0220bf005ea99ac83eb9ce51301ddb882dcf613ae6cd95fb\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-musllinux_1_2_ppc64le.whl\", hash = \"sha256:2aee7594d2c2221c717a8e394bbed4740029df4c0211ceb0f04815686e99c795\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-musllinux_1_2_s390x.whl\", hash = \"sha256:15871130439ad10abb25a4631120d60391aa762b85fcab971411e556247210a0\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:838dde2cb570cfbb4cab8a876a0974e8b90973ea40b3ac27a79b8a74c8a2db15\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-win32.whl\", hash = \"sha256:eacbcf30efaca7dc5cb264228ffecdb95fdb1e715b1ec937c0ce6b734161e0c8\"},\n+    {file = \"yarl-1.12.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:76a59d1b63de859398bc7764c860a769499511463c1232155061fe0147f13e01\"},\n+    {file = \"yarl-1.12.1-py3-none-any.whl\", hash = \"sha256:dc3192a81ecd5ff954cecd690327badd5a84d00b877e1573f7c9097ce13e5bfb\"},\n+    {file = \"yarl-1.12.1.tar.gz\", hash = \"sha256:5b860055199aec8d6fe4dcee3c5196ce506ca198a50aab0059ffd26e8e815828\"},\n+]\n+\n+[package.dependencies]\n+idna = \">=2.0\"\n+multidict = \">=4.0\"\n+\n+[[package]]\n+name = \"zipp\"\n+version = \"3.20.2\"\n+description = \"Backport of pathlib-compatible object wrapper for zip files\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"zipp-3.20.2-py3-none-any.whl\", hash = \"sha256:a817ac80d6cf4b23bf7f2828b7cabf326f15a001bea8b1f9b49631780ba28350\"},\n+    {file = \"zipp-3.20.2.tar.gz\", hash = \"sha256:bc9eb26f4506fda01b81bcde0ca78103b6e62f991b381fec825435c836edbc29\"},\n+]\n+\n+[package.extras]\n+check = [\"pytest-checkdocs (>=2.4)\", \"pytest-ruff (>=0.2.1)\"]\n+cover = [\"pytest-cov\"]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n+enabler = [\"pytest-enabler (>=2.2)\"]\n+test = [\"big-O\", \"importlib-resources\", \"jaraco.functools\", \"jaraco.itertools\", \"jaraco.test\", \"more-itertools\", \"pytest (>=6,!=8.1.*)\", \"pytest-ignore-flaky\"]\n+type = [\"pytest-mypy\"]\n+\n+[metadata]\n+lock-version = \"2.0\"\n+python-versions = \">=3.9,<4\"\n+content-hash = \"324fe583ad8df2932ed81088132ab537b3a0ec83051f4c60ec64a49d1d80bc98\"\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/poetry.toml b/packages/opentelemetry-instrumentation-sagemaker/poetry.toml\nnew file mode 100644\nindex 000000000..ab1033bd3\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/poetry.toml\n@@ -0,0 +1,2 @@\n+[virtualenvs]\n+in-project = true\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/project.json b/packages/opentelemetry-instrumentation-sagemaker/project.json\nnew file mode 100644\nindex 000000000..e2c1f9e79\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/project.json\n@@ -0,0 +1,78 @@\n+{\n+  \"name\": \"opentelemetry-instrumentation-sagemaker\",\n+  \"$schema\": \"../../node_modules/nx/schemas/project-schema.json\",\n+  \"projectType\": \"library\",\n+  \"sourceRoot\": \"packages/opentelemetry-instrumentation-sagemaker/opentelemetry_instrumentation_sagemaker\",\n+  \"targets\": {\n+    \"lock\": {\n+      \"executor\": \"@nxlv/python:run-commands\",\n+      \"options\": {\n+        \"command\": \"poetry lock --no-update\",\n+        \"cwd\": \"packages/opentelemetry-instrumentation-sagemaker\"\n+      }\n+    },\n+    \"add\": {\n+      \"executor\": \"@nxlv/python:add\",\n+      \"options\": {}\n+    },\n+    \"update\": {\n+      \"executor\": \"@nxlv/python:update\",\n+      \"options\": {}\n+    },\n+    \"remove\": {\n+      \"executor\": \"@nxlv/python:remove\",\n+      \"options\": {}\n+    },\n+    \"build\": {\n+      \"executor\": \"@nxlv/python:build\",\n+      \"outputs\": [\"{projectRoot}/dist\"],\n+      \"options\": {\n+        \"outputPath\": \"packages/opentelemetry-instrumentation-sagemaker/dist\",\n+        \"publish\": false,\n+        \"lockedVersions\": true,\n+        \"bundleLocalDependencies\": true\n+      }\n+    },\n+    \"install\": {\n+      \"executor\": \"@nxlv/python:install\",\n+      \"options\": {\n+        \"silent\": false,\n+        \"args\": \"\",\n+        \"cacheDir\": \".cache/pypoetry\",\n+        \"verbose\": false,\n+        \"debug\": false\n+      }\n+    },\n+    \"lint\": {\n+      \"executor\": \"@nxlv/python:flake8\",\n+      \"outputs\": [\n+        \"{workspaceRoot}/reports/packages/opentelemetry-instrumentation-sagemaker/pylint.txt\"\n+      ],\n+      \"options\": {\n+        \"outputFile\": \"reports/packages/opentelemetry-instrumentation-sagemaker/pylint.txt\"\n+      }\n+    },\n+    \"test\": {\n+      \"executor\": \"@nxlv/python:run-commands\",\n+      \"outputs\": [\n+        \"{workspaceRoot}/reports/packages/opentelemetry-instrumentation-sagemaker/unittests\",\n+        \"{workspaceRoot}/coverage/packages/opentelemetry-instrumentation-sagemaker\"\n+      ],\n+      \"options\": {\n+        \"command\": \"poetry run pytest tests/\",\n+        \"cwd\": \"packages/opentelemetry-instrumentation-sagemaker\"\n+      }\n+    },\n+    \"build-release\": {\n+      \"executor\": \"@nxlv/python:run-commands\",\n+      \"options\": {\n+        \"commands\": [\n+          \"chmod +x ../../scripts/build-release.sh\",\n+          \"../../scripts/build-release.sh\"\n+        ],\n+        \"cwd\": \"packages/opentelemetry-instrumentation-sagemaker\"\n+      }\n+    }\n+  },\n+  \"tags\": [\"instrumentation\"]\n+}\ndiff --git a/packages/opentelemetry-instrumentation-sagemaker/pyproject.toml b/packages/opentelemetry-instrumentation-sagemaker/pyproject.toml\nnew file mode 100644\nindex 000000000..27e3d07c4\n--- /dev/null\n+++ b/packages/opentelemetry-instrumentation-sagemaker/pyproject.toml\n@@ -0,0 +1,44 @@\n+[tool.coverage.run]\n+branch = true\n+source = [ \"opentelemetry/instrumentation/sagemaker\" ]\n+\n+[tool.coverage.report]\n+exclude_lines = ['if TYPE_CHECKING:']\n+show_missing = true\n+\n+[tool.poetry]\n+name = \"opentelemetry-instrumentation-sagemaker\"\n+version = \"0.25.6\"\n+description = \"OpenTelemetry SageMaker instrumentation\"\n+authors = [\n+  \"Bobby Lindsey <bwlind@amazon.com>\"\n+]\n+repository = \"https://github.com/traceloop/openllmetry/tree/main/packages/opentelemetry-instrumentation-sagemaker\"\n+license = \"Apache-2.0\"\n+readme = \"README.md\"\n+\n+[[tool.poetry.packages]]\n+include = \"opentelemetry/instrumentation/sagemaker\"\n+\n+[tool.poetry.dependencies]\n+python = \">=3.9,<4\"\n+opentelemetry-api = \"^1.26.0\"\n+opentelemetry-instrumentation = \"^0.48b0\"\n+opentelemetry-semantic-conventions = \"^0.48b0\"\n+opentelemetry-semantic-conventions-ai = \"0.4.1\"\n+\n+[tool.poetry.group.dev.dependencies]\n+autopep8 = \"^2.2.0\"\n+flake8 = \"7.0.0\"\n+\n+[tool.poetry.group.test.dependencies]\n+boto3 = \"^1.34.120\"\n+vcrpy = \"^6.0.1\"\n+pytest = \"^8.2.2\"\n+pytest-sugar = \"1.0.0\"\n+pytest-recording = \"^0.13.1\"\n+opentelemetry-sdk = \"^1.23.0\"\n+\n+[build-system]\n+requires = [\"poetry-core\"]\n+build-backend = \"poetry.core.masonry.api\"\ndiff --git a/packages/traceloop-sdk/poetry.lock b/packages/traceloop-sdk/poetry.lock\nindex 2895782e4..5c47c2208 100644\n--- a/packages/traceloop-sdk/poetry.lock\n+++ b/packages/traceloop-sdk/poetry.lock\n@@ -989,8 +989,8 @@ langchain-core = \">=0.2.35,<0.3.0\"\n langchain-text-splitters = \">=0.2.0,<0.3.0\"\n langsmith = \">=0.1.17,<0.2.0\"\n numpy = [\n-    {version = \">=1.26.0,<2.0.0\", markers = \"python_version >= \\\"3.12\\\"\"},\n     {version = \">=1,<2\", markers = \"python_version < \\\"3.12\\\"\"},\n+    {version = \">=1.26.0,<2.0.0\", markers = \"python_version >= \\\"3.12\\\"\"},\n ]\n pydantic = \">=1,<3\"\n PyYAML = \">=5.3\"\n@@ -1014,8 +1014,8 @@ jsonpatch = \">=1.33,<2.0\"\n langsmith = \">=0.1.75,<0.2.0\"\n packaging = \">=23.2,<25\"\n pydantic = [\n-    {version = \">=2.7.4,<3.0.0\", markers = \"python_full_version >= \\\"3.12.4\\\"\"},\n     {version = \">=1,<3\", markers = \"python_full_version < \\\"3.12.4\\\"\"},\n+    {version = \">=2.7.4,<3.0.0\", markers = \"python_full_version >= \\\"3.12.4\\\"\"},\n ]\n PyYAML = \">=5.3\"\n tenacity = \">=8.1.0,<8.4.0 || >8.4.0,<9.0.0\"\n@@ -1066,8 +1066,8 @@ files = [\n httpx = \">=0.23.0,<1\"\n orjson = \">=3.9.14,<4.0.0\"\n pydantic = [\n-    {version = \">=2.7.4,<3.0.0\", markers = \"python_full_version >= \\\"3.12.4\\\"\"},\n     {version = \">=1,<3\", markers = \"python_full_version < \\\"3.12.4\\\"\"},\n+    {version = \">=2.7.4,<3.0.0\", markers = \"python_full_version >= \\\"3.12.4\\\"\"},\n ]\n requests = \">=2,<3\"\n \n@@ -1417,7 +1417,7 @@ wrapt = \">=1.0.0,<2.0.0\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-alephalpha\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Aleph Alpha instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1439,7 +1439,7 @@ url = \"../opentelemetry-instrumentation-alephalpha\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-anthropic\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Anthropic instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1461,7 +1461,7 @@ url = \"../opentelemetry-instrumentation-anthropic\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-bedrock\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Bedrock instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1481,7 +1481,7 @@ url = \"../opentelemetry-instrumentation-bedrock\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-chromadb\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Chroma DB instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1503,7 +1503,7 @@ url = \"../opentelemetry-instrumentation-chromadb\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-cohere\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Cohere instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1525,7 +1525,7 @@ url = \"../opentelemetry-instrumentation-cohere\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-google-generativeai\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Google Generative AI instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1547,7 +1547,7 @@ url = \"../opentelemetry-instrumentation-google-generativeai\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-groq\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Groq instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1569,7 +1569,7 @@ url = \"../opentelemetry-instrumentation-groq\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-haystack\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Haystack instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1591,7 +1591,7 @@ url = \"../opentelemetry-instrumentation-haystack\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-lancedb\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Lancedb instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1613,7 +1613,7 @@ url = \"../opentelemetry-instrumentation-lancedb\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-langchain\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Langchain instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1635,7 +1635,7 @@ url = \"../opentelemetry-instrumentation-langchain\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-llamaindex\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry LlamaIndex instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1658,7 +1658,7 @@ url = \"../opentelemetry-instrumentation-llamaindex\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-marqo\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Marqo instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1680,7 +1680,7 @@ url = \"../opentelemetry-instrumentation-marqo\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-milvus\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Milvus instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1702,7 +1702,7 @@ url = \"../opentelemetry-instrumentation-milvus\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-mistralai\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Mistral AI instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1724,7 +1724,7 @@ url = \"../opentelemetry-instrumentation-mistralai\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-ollama\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Ollama instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1746,7 +1746,7 @@ url = \"../opentelemetry-instrumentation-ollama\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-openai\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry OpenAI instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1769,7 +1769,7 @@ url = \"../opentelemetry-instrumentation-openai\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-pinecone\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Pinecone instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1791,7 +1791,7 @@ url = \"../opentelemetry-instrumentation-pinecone\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-qdrant\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Qdrant instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1813,7 +1813,7 @@ url = \"../opentelemetry-instrumentation-qdrant\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-replicate\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Replicate instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1853,6 +1853,25 @@ opentelemetry-util-http = \"0.48b0\"\n [package.extras]\n instruments = [\"requests (>=2.0,<3.0)\"]\n \n+[[package]]\n+name = \"opentelemetry-instrumentation-sagemaker\"\n+version = \"0.25.6\"\n+description = \"OpenTelemetry SageMaker instrumentation\"\n+optional = false\n+python-versions = \">=3.9,<4\"\n+files = []\n+develop = true\n+\n+[package.dependencies]\n+opentelemetry-api = \"^1.26.0\"\n+opentelemetry-instrumentation = \"^0.48b0\"\n+opentelemetry-semantic-conventions = \"^0.48b0\"\n+opentelemetry-semantic-conventions-ai = \"0.4.1\"\n+\n+[package.source]\n+type = \"directory\"\n+url = \"../opentelemetry-instrumentation-sagemaker\"\n+\n [[package]]\n name = \"opentelemetry-instrumentation-sqlalchemy\"\n version = \"0.48b0\"\n@@ -1892,7 +1911,7 @@ wrapt = \">=1.0.0,<2.0.0\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-together\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Together AI instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1914,7 +1933,7 @@ url = \"../opentelemetry-instrumentation-together\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-transformers\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry transformers instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1954,7 +1973,7 @@ instruments = [\"urllib3 (>=1.0.0,<3.0.0)\"]\n \n [[package]]\n name = \"opentelemetry-instrumentation-vertexai\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Vertex AI instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1976,7 +1995,7 @@ url = \"../opentelemetry-instrumentation-vertexai\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-watsonx\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry IBM Watsonx Instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -1998,7 +2017,7 @@ url = \"../opentelemetry-instrumentation-watsonx\"\n \n [[package]]\n name = \"opentelemetry-instrumentation-weaviate\"\n-version = \"0.29.2\"\n+version = \"0.31.0\"\n description = \"OpenTelemetry Weaviate instrumentation\"\n optional = false\n python-versions = \">=3.9,<4\"\n@@ -2246,8 +2265,8 @@ files = [\n annotated-types = \">=0.4.0\"\n pydantic-core = \"2.20.1\"\n typing-extensions = [\n-    {version = \">=4.12.2\", markers = \"python_version >= \\\"3.13\\\"\"},\n     {version = \">=4.6.1\", markers = \"python_version < \\\"3.13\\\"\"},\n+    {version = \">=4.12.2\", markers = \"python_version >= \\\"3.13\\\"\"},\n ]\n \n [package.extras]\n@@ -3256,4 +3275,4 @@ type = [\"pytest-mypy\"]\n [metadata]\n lock-version = \"2.0\"\n python-versions = \">=3.9,<4\"\n-content-hash = \"24e17abd0f0bb8853af8ba457ab9f0975f8fff0a9df935a97e7b552f124aeb50\"\n+content-hash = \"1fe8225961074610ef9a2bdce8946c95ba34c7b59190b8e9b9a2939c08938e54\"\ndiff --git a/packages/traceloop-sdk/pyproject.toml b/packages/traceloop-sdk/pyproject.toml\nindex 52d2ed9a0..0a1d67eba 100644\n--- a/packages/traceloop-sdk/pyproject.toml\n+++ b/packages/traceloop-sdk/pyproject.toml\n@@ -51,6 +51,7 @@ opentelemetry-instrumentation-llamaindex = {path=\"../opentelemetry-instrumentati\n opentelemetry-instrumentation-milvus = {path=\"../opentelemetry-instrumentation-milvus\", develop=true}\n opentelemetry-instrumentation-haystack = {path=\"../opentelemetry-instrumentation-haystack\", develop=true}\n opentelemetry-instrumentation-bedrock = {path=\"../opentelemetry-instrumentation-bedrock\", develop=true}\n+opentelemetry-instrumentation-sagemaker = {path=\"../opentelemetry-instrumentation-sagemaker\", develop=true}\n opentelemetry-instrumentation-replicate = {path=\"../opentelemetry-instrumentation-replicate\", develop=true}\n opentelemetry-instrumentation-vertexai = {path=\"../opentelemetry-instrumentation-vertexai\", develop=true}\n opentelemetry-instrumentation-watsonx = {path=\"../opentelemetry-instrumentation-watsonx\", develop=true}\ndiff --git a/packages/traceloop-sdk/traceloop/sdk/instruments.py b/packages/traceloop-sdk/traceloop/sdk/instruments.py\nindex 5b0fa4568..300a37a3a 100644\n--- a/packages/traceloop-sdk/traceloop/sdk/instruments.py\n+++ b/packages/traceloop-sdk/traceloop/sdk/instruments.py\n@@ -27,3 +27,4 @@ class Instruments(Enum):\n     ALEPHALPHA = \"alephalpha\"\n     MARQO = \"marqo\"\n     LANCEDB = \"lancedb\"\n+    SAGEMAKER = \"sagemaker\"\ndiff --git a/packages/traceloop-sdk/traceloop/sdk/tracing/tracing.py b/packages/traceloop-sdk/traceloop/sdk/tracing/tracing.py\nindex 9de30c61c..4453b9595 100644\n--- a/packages/traceloop-sdk/traceloop/sdk/tracing/tracing.py\n+++ b/packages/traceloop-sdk/traceloop/sdk/tracing/tracing.py\n@@ -48,6 +48,7 @@\n     posthog.com,\n     sentry.io,\n     bedrock-runtime,\n+    sagemaker-runtime,\n     googleapis.com,\n     githubusercontent.com,\n     openaipublic.blob.core.windows.net\"\"\"\n@@ -263,6 +264,12 @@ def __new__(\n                             print(Fore.RESET)\n                         else:\n                             instrument_set = True\n+                    elif instrument == Instruments.SAGEMAKER:\n+                        if not init_sagemaker_instrumentor(should_enrich_metrics):\n+                            print(Fore.RED + \"Warning: SageMaker library does not exist.\")\n+                            print(Fore.RESET)\n+                        else:\n+                            instrument_set = True\n                     elif instrument == Instruments.REPLICATE:\n                         if not init_replicate_instrumentor():\n                             print(\n@@ -560,6 +567,7 @@ def init_instrumentations(\n     init_urllib3_instrumentor()\n     init_pymysql_instrumentor()\n     init_bedrock_instrumentor(should_enrich_metrics)\n+    init_sagemaker_instrumentor(should_enrich_metrics)\n     init_replicate_instrumentor()\n     init_vertexai_instrumentor()\n     init_watsonx_instrumentor()\n@@ -918,6 +926,24 @@ def init_bedrock_instrumentor(should_enrich_metrics: bool):\n         return False\n \n \n+def init_sagemaker_instrumentor(should_enrich_metrics: bool):\n+    try:\n+        if is_package_installed(\"boto3\"):\n+            from opentelemetry.instrumentation.sagemaker import SageMakerInstrumentor\n+\n+            instrumentor = SageMakerInstrumentor(\n+                exception_logger=lambda e: Telemetry().log_exception(e),\n+                enrich_token_usage=should_enrich_metrics,\n+            )\n+            if not instrumentor.is_instrumented_by_opentelemetry:\n+                instrumentor.instrument()\n+        return True\n+    except Exception as e:\n+        logging.error(f\"Error initializing SageMaker instrumentor: {e}\")\n+        Telemetry().log_exception(e)\n+        return False\n+\n+\n def init_replicate_instrumentor():\n     try:\n         if is_package_installed(\"replicate\"):\n", "instance_id": "traceloop__openllmetry-2028", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding tracing support for Amazon SageMaker endpoints using OpenTelemetry. It specifies the component (all packages), provides a feature description, explains the need for the feature, and outlines a high-level approach to achieve it (creating a new package, instrumenting the `invoke_endpoint` method, and creating spans for requests). However, there are minor ambiguities and missing details. For instance, it lacks specific information about the expected input/output formats for the tracing data, detailed constraints or requirements for the instrumentation (e.g., performance overhead limits), and any mention of edge cases or specific error handling scenarios that should be addressed. Additionally, there are no examples or references to similar implementations that could guide the developer. Despite these gaps, the overall intent and scope are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving the creation of a new package (`opentelemetry-instrumentation-sagemaker`) with multiple files and integration into a larger codebase (as seen in updates to `traceloop-sdk`). This requires understanding and modifying interactions across different modules, such as instrumentation logic, configuration, and integration with AWS SageMaker's runtime via `boto3`. Second, the technical concepts involved are moderately complex, including OpenTelemetry instrumentation, AWS SDK (`boto3`) interactions, handling streaming responses, and managing span attributes for tracing. Developers need to be familiar with Python's advanced features (e.g., decorators, wrapping functions with `wrapt`), OpenTelemetry APIs, and AWS-specific nuances like `invoke_endpoint` and streaming responses. Third, the code changes impact the system's architecture by introducing new instrumentation logic that must seamlessly integrate with existing tracing frameworks without introducing performance bottlenecks or breaking existing functionality. Finally, while edge cases are not explicitly mentioned in the problem statement, the code handles streaming and non-streaming responses, requiring careful error handling (e.g., `dont_throw` decorator, exception logging) and consideration of data privacy (e.g., configurable content tracing via environment variables). This combination of factors\u2014significant code scope, multiple technical concepts, architectural impact, and implicit edge case handling\u2014justifies a difficulty score of 0.65, reflecting a challenging task that demands a deep understanding of both the tracing framework and AWS ecosystem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Optionally fail tests if order cannot be achieved [enhancement]\nAt present, users can create logically incompatible orderings (e.g. A after B, A before C, but C before B).  This results in silent skipping of the offending test with the warning log\r\n```\r\nWARNING: cannot execute 'test_a' relative to others: 'testb.py::test_b'\r\n```\r\n\r\nUnfortunately this can be quite a silent/subtle error.  It would be awesome if there were a flag we could add to ensure that an error is raised (e.g. tests fail) if not all tests can be ran.\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex d796cc5..3d7e5eb 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,10 @@\n \n ## Unreleased\n \n+### New features\n+- added option `--error-on-failed-ordering` to make tests that cannot be ordered fail\n+  (see [#140](https://github.com/pytest-dev/pytest-order/issues/140))\n+\n ## Infrastructure\n - added missing documentation examples, structured examples according to documentation structure\n \ndiff --git a/docs/source/configuration.rst b/docs/source/configuration.rst\nindex a9714f6..275762b 100644\n--- a/docs/source/configuration.rst\n+++ b/docs/source/configuration.rst\n@@ -603,6 +603,49 @@ tests first from the start, then from the end if there are negative numbers,\n and the rest will be in between (e.g. between positive and negative numbers),\n as it is without this option.\n \n+``--error-on-failed-ordering``\n+------------------------------\n+Relative ordering of tests my fail under some circumstances. Mostly this happens if the related marker\n+is not found, or if the tests have a cyclic dependency.\n+The default behavior in this case is not to order the test in question, issue a warning during test\n+collection and execute the test as usual. If you want to make sure that your relative markers work\n+without checking all warning messages, you can also make the tests that cannot be ordered fail, so that\n+they show up as errored in the report:\n+\n+.. code:: python\n+\n+ import pytest\n+\n+\n+ def test_one():\n+     assert True\n+\n+\n+ @pytest.mark.order(before=\"test_three\")\n+ def test_two():\n+     assert True\n+\n+\n+In this example, the test \"test_three\" on which \"test_two\" depends, does not exist.\n+If you use the option `--error-on-failed-ordering`, \"test_two\" will now error:\n+\n+    $ pytest tests -vv --error-on-failed-ordering\n+    ============================= test session starts ==============================\n+    ...\n+    WARNING: cannot execute 'test_two' relative to others: 'test_three'\n+\n+    test_failed_ordering.py::test_one PASSED\n+    test_failed_ordering.py::test_two ERROR\n+\n+    =================================== ERRORS ====================================\n+    __________________________ ERROR at setup of test_two__________________________\n+    ...\n+    =========================== short test summary info ===========================\n+    ERROR test_failed_ordering.py::test_two - Failed: The test could not be ordered\n+    ========================= 1 passed, 1 error in 0.75s ==========================\n+\n+\n+\n .. _`pytest-dependency`: https://pypi.org/project/pytest-dependency/\n .. _`dynamic compilation of marked parameters`: https://pytest-dependency.readthedocs.io/en/stable/advanced.html#dynamic-compilation-of-marked-parameters\n .. _`add dependencies at runtime`: https://pytest-dependency.readthedocs.io/en/stable/usage.html#marking-dependencies-at-runtime\n", "instance_id": "pytest-dev__pytest-order-141", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to add an optional flag that causes tests to fail if they cannot be ordered due to logical incompatibilities. It provides a specific example of the issue (cyclic dependencies in test ordering) and mentions the current behavior (silent skipping with a warning). However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes a \"logically incompatible ordering\" beyond the example, nor does it specify how the error should be reported or integrated into the test framework's output. Additionally, edge cases (e.g., partial ordering failures, interactions with other test configurations) are not addressed. While the intent is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes appears to be moderate; based on the provided diffs, the changes are limited to documentation and changelog updates, suggesting that the core implementation (not shown in the diff) likely involves modifying a single module or a few functions related to test ordering logic in a testing framework (likely pytest-order). However, understanding the test ordering mechanism and integrating a new flag requires familiarity with the pytest plugin architecture and its hook system, which adds some complexity. Second, the technical concepts involved include command-line argument parsing, test collection and execution phases in pytest, and error handling mechanisms, which are moderately complex for someone unfamiliar with pytest internals. Third, while the problem statement does not explicitly mention edge cases, implementing this feature would require considering scenarios like cyclic dependencies, missing test references, and interactions with other plugins or configurations, necessitating robust error handling. Overall, this task requires a moderate understanding of the codebase and testing frameworks, along with careful implementation to avoid unintended side effects, but it does not seem to impact the broader system architecture or demand advanced domain-specific knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Convolution filter\nI've added a convolution filter that is much faster than the regular matrix-based density filter.\r\nReasons to add:\r\n- Setup cost is next to none (for H-matrix it can go up to seconds-minutes with a large filter radius and large domain). For a 3D domain of 60^3 with filter radius 5 elements, setup of H-matrix is 12 seconds, while for convolution it is 0.0059.\r\n- Execution cost is low, since convolution makes use of efficient FFT algorithms. For the 3D domain aforementioned, running convolution costs only 0.023s, while H-matrix is almost 10x slower at 0.16s.\r\n- Flexible boundary conditions: padding is required to extend the domain for the convolution to work. This allows different regions to have different behavior (e.g. reflection, constant value, wrap around, ...).\n", "patch": "diff --git a/examples/ex_simple_module.py b/examples/ex_simple_module.py\nindex fb91db0..65fe8b1 100644\n--- a/examples/ex_simple_module.py\n+++ b/examples/ex_simple_module.py\n@@ -1,65 +1,89 @@\n-\"\"\" Example: Module template for a generic module\n+\"\"\" Example: Module templates for generic modules\n It shows possibilities for a generic Module, and also the importance of the sensitivity reset.\n \"\"\"\n from pymoto import Module, Signal, finite_difference\n \n \n-class MyNewModule(Module):\n-    \"\"\" This is my module that does awesome stuff.\n-\n-    The (Module) is required for your module to be a module, with correct behaviour.\n-    This example is a module with two inputs and two outputs.\n-    \"\"\"\n-    def _prepare(self, my_argument, optional_arg='foo'):\n-        \"\"\"\n-        This function is called by the initialization of the superclass. It can use the input parameters entered into\n-        the Module.create() method.\n-        \"\"\"\n-        print('>> Prepare my module')\n-        print('\\tmy_argument = {}'.format(my_argument))\n-        print('\\toptional_argument = {}'.format(optional_arg))\n-\n+class MostSimple(Module):\n+    \"\"\" Example of a minimal module with two inputs and one output \"\"\"\n     def _response(self, x1, x2):\n+        \"\"\" Forward-path calculation is put here \"\"\"\n+        print(f'[{type(self).__name__}] Do my response calculation')\n+        print(f'\\tInputs are {self.sig_in[0].tag} and {self.sig_in[1].tag}')\n+        return x1 * x2\n+\n+    def _sensitivity(self, df_dy):\n+        \"\"\" Backward-path sensitivity calculation here.\n+        In case df_dy is None, the function will automatically not be called\n         \"\"\"\n-        This function calculates a response based on a multiple input values, here for example 2. Multiple outputs can\n-        easily be added. Also, different response behaviours can be implemented, based on the number of inputs (function\n-        overloading). The 'self' object can be used to save state variables.\n-\n-        :param x1: First input variable\n-        :param x2: Second input variable\n-        :return: The results of the calculation\n-        \"\"\"\n-        print('>> Do my response calculation')\n-\n-        # Incorrect data\n-        if x1 is None or x2 is None:\n-            raise RuntimeError(\"You forgot to set {} and {}\".format(self.sig_in[0].tag, self.sig_in[1].tag))\n-\n+        print(f'[{type(self).__name__}] Do my sensitivity calculation')\n+        x1, x2 = [s.state for s in self.sig_in]\n+        df_dx1 = df_dy * x2  # Apply chain rule df/dx1 = df/dy * dy/dx1\n+        df_dx2 = df_dy * x1\n+        return df_dx1, df_dx2  # Return the sensitivities with respect to input signals\n+\n+\n+class WithPrepare(Module):\n+    \"\"\" This module takes a parameter during initialization\n+    Example usage:\n+    >>> import pymoto as pym\n+    >>> x = pym.Signal('x', state=3.5)\n+    >>> y = pym.Signal('y')\n+    >>> m = WithPrepare(x, y, 1.2, optional_value='bar')\n+    [WithPrepare] Prepare my module\n+    value = 1.2\n+    optional_value = bar\n+    >>> m.response() # doctest: +ELLIPSIS\n+    [WithPrepare] Do my response calculation\n+    Message = bar, x = 3.5, y=4.2\n+    ...\n+    >>> y.state\n+    4.2\n+    \"\"\"\n+    def _prepare(self, value, optional_value='foo'):\n+        \"\"\" This prepare is called during initialization of the module, and can be used for set-up \"\"\"\n+        print(f'[{type(self).__name__}] Prepare my module')\n+        print('value = {}'.format(value))\n+        print('optional_value = {}'.format(optional_value))\n+        self.value = value\n+        self.optional_value = optional_value\n+\n+    def _response(self, x):\n+        print(f'[{type(self).__name__}] Do my response calculation')\n+        y = x * self.value\n+        print(f\"Message = {self.optional_value}, x = {x}, y={y}\")\n+        return y\n+\n+    def _sensitivity(self, df_dy):\n+        print(f'[{type(self).__name__}] Do my sensitivity calculation')\n+        return df_dy * self.value\n+\n+\n+class TwoOutputs(Module):\n+    \"\"\" This module has two inputs and two outputs \"\"\"\n+    def _response(self, x1, x2):\n+        print(f'[{type(self).__name__}]Do my response calculation')\n         # Store data, which might be needed for the sensitivity calculation\n         self.x1 = x1\n         self.x2 = x2\n \n         # Calculate two response values\n-        v1 = x1 * x2\n-        v2 = x1 + x2\n+        y1 = x1 * x2\n+        y2 = x1 + x2\n \n         # Return the results\n-        return v1, v2\n-\n-    def _sensitivity(self, df_dv1, df_dv2):\n-        \"\"\"\n-        This function calculate the (backward) sensitivity. It should handle None (zero sensitivity) as incoming adjoint\n-        variable. If both are None, the sensitivity will not be called.\n+        return y1, y2\n \n-        :param df_dv1: The adjoint variable of the first output\n-        :param df_dv2: The adjoint variable of the second output\n-        :return: The adjoint variables (sensitivities) of the inputs\n+    def _sensitivity(self, df_dy1, df_dy2):\n+        \"\"\" This function calculate the (backward) sensitivity.\n+        It should handle None (zero sensitivity) as incoming adjoint variable. If both are None, the sensitivity\n+        will not be called.\n         \"\"\"\n-        print('>> Do my sensitivity calculation')\n+        print(f'[{type(self).__name__}]Do my sensitivity calculation')\n \n         # Calculate the gradients with chain-rule\n         # First initialize sensitivities with the correct size containing all zeros\n-        df_dx1 = self.x1 * 0\n+        df_dx1 = self.x1 * 0  # The sensitivity df/dx1 is the same size as x1 (in case of a vector/matrix)\n         df_dx2 = self.x2 * 0\n \n         # In case the data of x1 and x2 were not stored, it could still be obtained here by directly accessing the state\n@@ -71,108 +95,90 @@ def _sensitivity(self, df_dv1, df_dv2):\n \n         # If the sensitivity of the output signal is empty, it is None. So we only need to do calculations whenever it\n         # is not None. In case both sensitivities of the output signals are None, this function won't be called.\n-        if df_dv1 is not None:\n-            df_dx1 += df_dv1*self.x2\n-            df_dx2 += df_dv1*self.x1\n+        if df_dy1 is not None:\n+            df_dx1 += df_dy1*self.x2\n+            df_dx2 += df_dy1*self.x1\n \n-        if df_dv2 is not None:\n-            df_dx1 += df_dv2\n-            df_dx2 += df_dv2\n+        if df_dy2 is not None:\n+            df_dx1 += df_dy2\n+            df_dx2 += df_dy2\n \n         # Return the results\n         return df_dx1, df_dx2\n \n-    def _reset(self):\n-        \"\"\"\n-        This function is used for reset, called whenever a new sensitivity is to be calculated, triggered by calling\n-        reset(). This can be used to reset any internal storage required for sensitivity calculation\n-        \"\"\"\n-        print('>> Reset my module')\n-\n \n if __name__ == \"__main__\":\n     print(__doc__)\n     print(\"_\" * 80)\n-    print(\"PART 1: Setup\")\n-\n-    # The print function lists all possible Module types\n-    print(\"Show all possible modules defined, it also lists the new locally defined module\")\n-    Module.print_children()\n+    print(\"-- Module setup\")\n \n     # Create signals for the inputs. The argument is the 'tag' of the signal, which is optional.\n-    # The signal's 'tag' can be seen as its name, which can be useful for printing or when\n+    # The tag of the signal can be seen as its name, which can be useful for printing and debugging\n     x1 = Signal(\"x1\")\n \n     # Also create a second input signal (as our module has two inputs)\n     x2 = Signal(\"x2\")\n \n-    # And create two signals for the outputs\n-    y1 = Signal(\"y1\")\n-    y2 = Signal(\"y2\")\n-\n-    # The module can be instantiated using the constructor. The first argument is a list of input signals, and the\n-    # second argument is a list of output arguments. The other (keyword) arguments are directly passed to the _prepare()\n-    # function of the module.\n-    print(\"\\nInstantiate directly:\")\n-    the_mod = MyNewModule([x1, x2], [y1, y2], 'my_arg_data', optional_arg=3.14)\n-\n-    # It can also be instantiated using Module.create, by referencing its name (case insensitive) as first argument.\n-    # All the other arguments remain identical to before.\n-    print(\"\\nInstantiate by Module.create:\")\n-    also_the_mod = Module.create(\"MyNewModule\", [x1, x2], [y1, y2], 'my_arg_data1', optional_arg='bar')\n+    # And create a signal for the output\n+    y = Signal(\"y\")\n \n-    print(\"_\"*80)\n-    print(\"PART 2: Forward analysis\")\n+    # The module is instantiated using the constructor. The first argument is the input signal (or a list of input\n+    # signals), and the second argument is the output signal (or again a list). Different module types are prepared in\n+    # this example. Uncomment lines below to see what happens.\n+    print(\"Instantiate Module:\")\n+    simple_module = MostSimple([x1, x2], y)\n+    # simple_module = WithPrepare(x1, y, 3.14, optional_value='bar')  # Module with extra (constant) parameters\n+    # simple_module = TwoOutputs([x1, x2], [y, Signal('y2')])  # Module with two outputs\n \n+    print(\"\\n-- Forward analysis\")\n     try:\n-        print(\"\\nTrying to call the response without setting initial values results in an error\")\n-        the_mod.response()\n-    except RuntimeError as e:\n-        print(\"ERROR OBTAINED: \", e.__str__())\n+        print(\"Trying to call the response without setting initial values results in an error\")\n+        simple_module.response()\n+    except TypeError as e:\n+        print(\"ERROR OBTAINED: \\n TypeError: \", e.__str__())\n \n-    # Set the initial values to the state of the input signals using <Signal>.state\n-    x1.state = 2.0\n+    # To correctly do the forward calculation, a corect state need to be set in x1 and x2\n+    x1.state = 2.0  # Values for the for the forward calculation\n     x2.state = 3.0\n+    print(f\"\\nState initialized to {x1.tag} = {x1.state}, {x2.tag} = {x2.state}\")\n \n-    print(\"\\nState initialized to {0} = {1}, {2} = {3}\".format(x1.tag, x1.state, x2.tag, x2.state))\n+    # Now execute response\n     print(\"Call response\")\n-    the_mod.response()\n-    # The states of the output signals can be accessed using <Signal>.state\n-    print(\"The results: {0} = {1}, {2} = {3}\".format(y1.tag, y1.state, y2.tag, y2.state))\n+    simple_module.response()\n \n-    # Sensitivity analysis\n-    print(\"_\"*80)\n-    print(\"PART 3: Backpropagation (sensitivity analysis)\")\n+    # The state of the output signal can be accessed using <Signal>.state again\n+    print(f\"The result: {y.tag} = {y.state}\")\n \n+    print(\"\\n-- Sensitivity analysis by back-propagation\")\n     # Calculate sensitivities\n     print(\"\\nIf no seed is given, no sensitivities will be calculated\")\n-    the_mod.sensitivity()\n-    print(\"dg/d{} = {}\".format(x1.tag, x1.sensitivity))\n-    print(\"dg/d{} = {}\".format(x2.tag, x2.sensitivity))\n+    simple_module.sensitivity()\n+    print(f\"dy/d{x1.tag} = {x1.sensitivity}\")\n+    print(f\"dy/d{x2.tag} = {x2.sensitivity}\")\n \n-    print(\"\\nSeed dg/dy1 = 1.0, so we can calculate dy1/dx1 and dy1/dx2\")\n-    # An initial 'seed' sensitivity of the last response you're interested in needs to be set. We can do this by setting\n+    print(\"\\nSeed dy/dy = 1.0, so we can calculate dy/dx1 and dy/dx2\")\n+    # An initial 'seed' sensitivity of the response you're interested in needs to be set. We can do this by setting\n     # the <Signal>.sensitivity property\n-    y1.sensitivity = 1.0\n-    the_mod.sensitivity()\n-    # The sensitivities of the initial signals can also be accessed by <Signal>.sensitivity\n-    print(\"dy1/d{} = {}\".format(x1.tag, x1.sensitivity))\n-    print(\"dy1/d{} = {}\".format(x2.tag, x2.sensitivity))\n+    y.sensitivity = 1.0\n+    simple_module.sensitivity()\n+    # The sensitivities of the input signals can now be accessed by <Signal>.sensitivity\n+    print(f\"dy/d{x1.tag} = {x1.sensitivity}\")\n+    print(f\"dy/d{x2.tag} = {x2.sensitivity}\")\n \n     print(\"\\nWhen reset is not called after the sensitivity calculation, the results will not be correct.\")\n-    print(\"Seed dg/dy1 = 1.0 again (not strictly necessary, since the value already was seeded)\")\n-    y1.sensitivity = 1.0\n-    the_mod.sensitivity()\n-    print(\"Incorrect sensitivity dy1/d{} = {}\".format(x1.tag, x1.sensitivity))\n-    print(\"Incorrect sensitivity dy1/d{} = {}\".format(x2.tag, x2.sensitivity))\n+    print(\"Seed dy/dy = 1.0 again (not strictly necessary, since the value already was seeded without being cleared)\")\n+    y.sensitivity = 1.0\n+    simple_module.sensitivity()\n+    print(f\"Incorrect sensitivity dy/d{x1.tag} = {x1.sensitivity}\")\n+    print(f\"Incorrect sensitivity dy/d{x2.tag} = {x2.sensitivity}\")\n     print(\"The values are now double of what they're supposed to be, because they're added to what we already had.\")\n \n-    print(\"\\nRESET! And seed dg/dy2 = 1.0 to calculate the other sensitivities\")\n-    the_mod.reset()  # !! DON'T FORGET TO RESET, ELSE SENSITIVITIES FROM PREVIOUS RUNS WILL CONTAMINATE YOUR RESULT !!\n-    y2.sensitivity = 1.0\n-    the_mod.sensitivity()\n-    print(\"dy2/d{} = {}\".format(x1.tag, x1.sensitivity))\n-    print(\"dy2/d{} = {}\".format(x2.tag, x2.sensitivity))\n+    print(\"\\nRESET! And seed dy/dy = 1.0 to calculate the other sensitivities\")\n+    simple_module.reset()  # !! DON'T FORGET TO RESET, ELSE SENSITIVITIES FROM PREVIOUS RUNS WILL CONTAMINATE YOUR RESULT !!\n+    y.sensitivity = 1.0\n+    simple_module.sensitivity()\n+    print(f\"dy/d{x1.tag} = {x1.sensitivity}\")\n+    print(f\"dy/d{x2.tag} = {x2.sensitivity}\")\n \n     # You can always check your module with finite differencing\n-    finite_difference(the_mod)\n+    finite_difference(simple_module)\ndiff --git a/examples/example1.py b/examples/example1.py\nindex 25499ef..452b17c 100644\n--- a/examples/example1.py\n+++ b/examples/example1.py\n@@ -74,6 +74,7 @@ def _sensitivity(self, df_dy):\n     z.state = 3.4\n \n     # Create the modules\n+    # Depending on how the input and output signals are routed between the modules, different behavior can be created\n     ordering = 0\n     if ordering == 0:\n         # A __\n@@ -98,14 +99,14 @@ def _sensitivity(self, df_dy):\n     # Create a network of modules\n     func = Network(m1, m2, m3)\n \n-    print(\"\\nCurrent interconnection:\")\n+    print(\"\\nCurrent network:\")\n     print(\" -> \".join([type(m).__name__ for m in func.mods]))\n \n     # --- FORWARD ANALYSIS ---\n     # Perform forward analysis\n     func.response()\n \n-    print(\"\\nThe response is  g(x={0}, y={1}, z={2}) = {3}\".format(x.state, y.state, z.state, g.state))\n+    print(f\"\\nThe response is  g(x={x.state}, y={y.state}, z={z.state}) = {g.state}\")\n \n     # --- BACKPROPAGATION ---\n     # Clear previous sensitivities\n@@ -118,9 +119,9 @@ def _sensitivity(self, df_dy):\n     func.sensitivity()\n \n     print(\"\\nThe sensitivities are:\")\n-    print(\"d{0}/d{1} = {2}\".format(g.tag, x.tag, x.sensitivity))\n-    print(\"d{0}/d{1} = {2}\".format(g.tag, y.tag, y.sensitivity))\n-    print(\"d{0}/d{1} = {2}\".format(g.tag, z.tag, z.sensitivity))\n+    print(f\"d{g.tag}/d{x.tag} = {x.sensitivity}\")\n+    print(f\"d{g.tag}/d{y.tag} = {y.sensitivity}\")\n+    print(f\"d{g.tag}/d{z.tag} = {z.sensitivity}\")\n \n     # --- Finite difference checks ---\n     # On the individual modules\ndiff --git a/examples/topology_optimization/ex_compliance.py b/examples/topology_optimization/ex_compliance.py\nindex 9d7a3a7..016de13 100644\n--- a/examples/topology_optimization/ex_compliance.py\n+++ b/examples/topology_optimization/ex_compliance.py\n@@ -3,11 +3,11 @@\n \n import pymoto as pym\n \n-nx, ny, nz = 30, 30, 0  # Set nz to zero for the 2D problem\n+nx, ny, nz = 120, 40, 0  # Set nz to zero for the 2D problem, nz > 0 runs a 3D problem\n xmin = 1e-9\n filter_radius = 2.0\n volfrac = 0.5\n-thermal = False  # True = Static thermal analysis; False = Static mechanical analysis will be done\n+thermal = True  # True = Static thermal analysis; False = Static mechanical analysis will be done\n \n if __name__ == \"__main__\":\n     print(__doc__)\n@@ -17,12 +17,12 @@\n         domain = pym.DomainDefinition(nx, ny)\n \n         if thermal:\n+            ndof = 1  # Number of dofs per node\n             # Get dof numbers at the boundary\n             boundary_dofs = domain.get_nodenumber(0, np.arange(ny // 4, (ny + 1) - ny // 4))\n \n             # Make a force vector\n             force_dofs = domain.get_nodenumber(*np.meshgrid(np.arange(1, nx + 1), np.arange(ny + 1)))\n-            ndof = 1  # Number of dofs per node\n \n         else:  # Mechanical\n             ndof = 2\n@@ -42,7 +42,7 @@\n             force_dofs = domain.get_nodenumber(*np.meshgrid(np.arange(1, nx+1), np.arange(ny + 1), np.arange(nz + 1))).flatten()\n             ndof = 1\n \n-        else:\n+        else:  # Mechanical\n             ndof = 3\n             boundary_dofs = np.repeat(boundary_nodes * ndof, ndof, axis=-1) + np.tile(np.arange(ndof), len(boundary_nodes))\n             force_dofs = ndof * domain.get_nodenumber(nx, ny // 2, nz // 2) + 2  # Z-direction\n@@ -80,9 +80,9 @@\n \n     # Linear system solver. The linear solver can be chosen by uncommenting any of the following lines.\n     solver = None  # Default (solver is automatically chosen based on matrix properties)\n-    # solver = pym.SolverSparsePardiso()  # Requires Intel MKL installed\n-    # solver = pym.SolverSparseCholeskyCVXOPT()  # Requires cvxopt installed\n-    # solver = pym.SolverSparseCholeskyScikit()  # Requires scikit installed\n+    # solver = pym.solvers.SolverSparsePardiso()  # Requires Intel MKL installed\n+    # solver = pym.solvers.SolverSparseCholeskyCVXOPT()  # Requires cvxopt installed\n+    # solver = pym.solvers.SolverSparseCholeskyScikit()  # Requires scikit installed\n     su = func.append(pym.LinSolve([sK, sf], hermitian=True, solver=solver))\n \n     # Output the design, deformation, and force field to a Paraview file\n@@ -101,7 +101,7 @@\n     svol.tag = 'volume'\n \n     # Volume constraint\n-    sg1 = func.append(pym.MathGeneral(svol, expression='10*(inp0/{} - {})'.format(domain.nel, volfrac)))\n+    sg1 = func.append(pym.MathGeneral(svol, expression=f'10*(inp0/{domain.nel} - {volfrac})'))\n     sg1.tag = \"volume constraint\"\n \n     # Maybe you want to check the design-sensitivities?\n@@ -112,8 +112,8 @@\n \n     func.append(pym.PlotIter([sg0, sg1]))  # Plot iteration history\n \n-    # Do the optimization with MMA (requires nlopt package)\n-    # pym.minimize_mma(func, [sx], [sg0, sg1])  # TODO does not work correctly for the thermal case\n+    # Do the optimization with MMA\n+    # pym.minimize_mma(func, [sx], [sg0, sg1])\n \n     # Do the optimization with OC\n     pym.minimize_oc(func, sx, sg0)\ndiff --git a/examples/topology_optimization/ex_compliance_multigrid.py b/examples/topology_optimization/ex_compliance_multigrid.py\nnew file mode 100644\nindex 0000000..d493638\n--- /dev/null\n+++ b/examples/topology_optimization/ex_compliance_multigrid.py\n@@ -0,0 +1,138 @@\n+\"\"\" Minimal example for a compliance topology optimization with multigrid preconditioned CG as solver \"\"\"\n+import numpy as np\n+\n+import pymoto as pym\n+\n+nx, ny, nz = 128, 64, 64  # Set nz to zero for the 2D problem, nz > 0 runs a 3D problem\n+xmin = 1e-9\n+filter_radius = 2.0\n+volfrac = 0.5\n+thermal = False  # True = Static thermal analysis; False = Static mechanical analysis will be done\n+\n+if __name__ == \"__main__\":\n+    print(__doc__)\n+\n+    if nz == 0:  # 2D analysis\n+        # Generate a grid\n+        domain = pym.DomainDefinition(nx, ny)\n+\n+        if thermal:\n+            ndof = 1  # Number of dofs per node\n+            # Get dof numbers at the boundary\n+            boundary_dofs = domain.get_nodenumber(0, np.arange(ny // 4, (ny + 1) - ny // 4))\n+\n+            # Make a force vector\n+            force_dofs = domain.get_nodenumber(*np.meshgrid(np.arange(1, nx + 1), np.arange(ny + 1)))\n+\n+        else:  # Mechanical\n+            ndof = 2\n+            # Calculate boundary dof indices\n+            boundary_nodes = domain.get_nodenumber(0, np.arange(ny + 1))\n+            boundary_dofs = np.repeat(boundary_nodes * ndof, ndof, axis=-1) + np.tile(np.arange(ndof), len(boundary_nodes))\n+\n+            # Which dofs to put a force on? The 1 is added for a force in y-direction (x-direction would be zero)\n+            force_dofs = ndof * domain.get_nodenumber(nx, ny // 2) + 1\n+\n+    else:\n+        domain = pym.DomainDefinition(nx, ny, nz)\n+        boundary_nodes = domain.get_nodenumber(*np.meshgrid(0, range(ny + 1), range(nz + 1))).flatten()\n+\n+        if thermal:\n+            boundary_dofs = boundary_nodes\n+            force_dofs = domain.get_nodenumber(*np.meshgrid(np.arange(1, nx+1), np.arange(ny + 1), np.arange(nz + 1))).flatten()\n+            ndof = 1\n+\n+        else:  # Mechanical\n+            ndof = 3\n+            boundary_dofs = np.repeat(boundary_nodes * ndof, ndof, axis=-1) + np.tile(np.arange(ndof), len(boundary_nodes))\n+            force_dofs = ndof * domain.get_nodenumber(nx, ny // 2, nz // 2) + 2  # Z-direction\n+\n+    if domain.nnodes > 1e+6:\n+        print(\"Too many nodes :(\")  # Safety, to prevent overloading the memory in your machine\n+        exit()\n+\n+    # Generate a force vector\n+    f = np.zeros(domain.nnodes * ndof)\n+    f[force_dofs] = 1.0  # Uniform force of 1.0 at all selected dofs\n+\n+    # Make force and design vector, and fill with initial values\n+    sf = pym.Signal('f', state=f)\n+    sx = pym.Signal('x', state=np.ones(domain.nel) * volfrac)\n+\n+    # Start building the modular network\n+    func = pym.Network(print_timing=True)\n+\n+    # Filter\n+    sxfilt = func.append(pym.FilterConv(sx, domain=domain, radius=filter_radius))\n+    sx_analysis = sxfilt\n+\n+    # Show the design on the screen as it optimizes (plotting domain in 3D may take significant time)\n+    # func.append(pym.PlotDomain(sx_analysis, domain=domain, saveto=\"out/design\", clim=[0, 1]))\n+\n+    # SIMP material interpolation\n+    sSIMP = func.append(pym.MathGeneral(sx_analysis, expression=f\"{xmin} + {1.0 - xmin}*inp0^3\"))\n+\n+    # System matrix assembly module\n+    if thermal:\n+        sK = func.append(pym.AssemblePoisson(sSIMP, domain=domain, bc=boundary_dofs))\n+    else:\n+        sK = func.append(pym.AssembleStiffness(sSIMP, domain=domain, bc=boundary_dofs))\n+\n+    # Linear system solver. The linear solver can be chosen by uncommenting any of the following lines.\n+    solver = None  # Default (solver is automatically chosen based on matrix properties)\n+    # solver = pym.solvers.SolverSparsePardiso()  # Requires Intel MKL installed\n+    # solver = pym.solvers.SolverSparseCholeskyCVXOPT()  # Requires cvxopt installed\n+    # solver = pym.solvers.SolverSparseCholeskyScikit()  # Requires scikit installed\n+\n+    ''' Iterative solver: CG with Geometric Multi-grid preconditioning '''\n+    # Set up fine level\n+    mg1 = pym.solvers.GeometricMultigrid(domain)\n+    mgs = [mg1]\n+    n_levels = 5  # Number of levels (including fine grid)\n+    for i in range(n_levels - 1):  # Setup coarse levels\n+        mgs.append(pym.solvers.GeometricMultigrid(mgs[i].sub_domain))\n+        mgs[i].inner_level = mgs[i+1]\n+\n+    # Set up the solver (comment out to use the default factorization, try this to see the difference in time)\n+    solver = pym.solvers.CG(preconditioner=mg1, verbosity=1, tol=1e-5)\n+\n+    ''' From here the rest of the example is identical to ex_compliance.py '''\n+    su = func.append(pym.LinSolve([sK, sf], hermitian=True, solver=solver))\n+\n+    # Output the design, deformation, and force field to a Paraview file\n+    func.append(pym.WriteToVTI([sx_analysis, su, sf], domain=domain, saveto='out/dat.vti'))\n+\n+    # Compliance calculation c = f^T u\n+    scompl = func.append(pym.EinSum([su, sf], expression='i,i->'))\n+    scompl.tag = 'compliance'\n+\n+    # MMA needs correct scaling of the objective\n+    sg0 = func.append(pym.Scaling(scompl, scaling=100.0))\n+    sg0.tag = \"objective\"\n+\n+    # Calculate the volume of the domain by adding all design densities together\n+    svol = func.append(pym.EinSum(sx_analysis, expression='i->'))\n+    svol.tag = 'volume'\n+\n+    # Volume constraint\n+    sg1 = func.append(pym.MathGeneral(svol, expression=f'10*(inp0/{domain.nel} - {volfrac})'))\n+    sg1.tag = \"volume constraint\"\n+\n+    # Maybe you want to check the design-sensitivities?\n+    do_finite_difference = False\n+    if do_finite_difference:\n+        pym.finite_difference(func, sx, [sg0, sg1], dx=1e-4)\n+        exit()\n+\n+    func.append(pym.PlotIter([sg0, sg1]))  # Plot iteration history\n+\n+    # Do the optimization with MMA\n+    # pym.minimize_mma(func, [sx], [sg0, sg1])\n+\n+    # Do the optimization with OC\n+    pym.minimize_oc(func, sx, sg0)\n+\n+    # Here you can do some post processing\n+    print(\"The optimization has finished!\")\n+    print(f\"The final compliance value obtained is {scompl.state}\")\n+    print(f\"The maximum {'temperature' if thermal else 'displacement'} is {max(np.absolute(su.state))}\")\ndiff --git a/examples/topology_optimization/ex_compliance_padding_filter.py b/examples/topology_optimization/ex_compliance_padding_filter.py\nnew file mode 100644\nindex 0000000..8370912\n--- /dev/null\n+++ b/examples/topology_optimization/ex_compliance_padding_filter.py\n@@ -0,0 +1,163 @@\n+\"\"\" Compliance topology optimization with a padded-domain filter, which avoids the design 'sticking' to the boundary \"\"\"\n+import numpy as np\n+\n+import pymoto as pym\n+\n+nx, ny, nz = 120, 40, 0  # Set nz to zero for the 2D problem\n+xmin = 1e-9\n+filter_radius = 5.0\n+volfrac = 0.5\n+\n+\n+class Continuation(pym.Module):\n+    \"\"\" Module that generates a continuated value \"\"\"\n+    def _prepare(self, start=0.0, stop=1.0, nsteps=80, stepstart=10):\n+        self.startval = start\n+        self.endval = stop\n+        self.dval = (stop - start) / nsteps\n+        self.nstart = stepstart\n+        self.iter = -1\n+        self.val = self.startval\n+\n+    def _response(self):\n+        if (self.val < self.endval) and (self.iter > self.nstart):\n+            self.val += self.dval\n+\n+        self.val = np.clip(self.val, min(self.startval, self.endval), max(self.startval, self.endval))\n+        print(self.sig_out[0].tag, ' = ', self.val)\n+        self.iter += 1\n+        return self.val\n+\n+    def _sensitivity(self, *args):\n+        pass\n+\n+\n+if __name__ == \"__main__\":\n+    print(__doc__)\n+\n+    if nz == 0:  # 2D analysis\n+        ndof = 2\n+\n+        # Generate a grid\n+        domain = pym.DomainDefinition(nx, ny)\n+\n+        # Get node numbers for the boundary condition\n+        boundary_nodes = domain.get_nodenumber(0, np.arange(ny // 3, (ny+1) - ny//3))\n+\n+        # Calculate boundary dof indices\n+        boundary_dofs = np.repeat(boundary_nodes * ndof, ndof, axis=-1) + np.tile(np.arange(ndof), len(boundary_nodes))\n+\n+        # Which dofs to put a force on? The 1 is added for a force in y-direction (x-direction would be zero)\n+        force_dofs = ndof * domain.get_nodenumber(nx, ny // 2) + 1\n+\n+    else:\n+        ndof = 3\n+        domain = pym.DomainDefinition(nx, ny, nz)\n+        boundary_nodes = domain.get_nodenumber(*np.meshgrid(0, np.arange(ny // 3, (ny+1) - ny//3), np.arange(nz // 3, (nz+1) - nz//3))).flatten()\n+        boundary_dofs = np.repeat(boundary_nodes * ndof, ndof, axis=-1) + np.tile(np.arange(ndof), len(boundary_nodes))\n+        force_dofs = ndof * domain.get_nodenumber(nx, ny // 2, nz // 2) + 2  # Z-direction\n+\n+    if domain.nnodes > 1e+6:\n+        print(\"Too many nodes :(\")  # Safety, to prevent overloading the memory in your machine\n+        exit()\n+\n+    # Generate a force vector\n+    f = np.zeros(domain.nnodes * ndof)\n+    f[force_dofs] = 1.0  # Uniform force of 1.0 at all selected dofs\n+\n+    # Make force and design vector, and fill with initial values\n+    sf = pym.Signal('f', state=f)\n+    sx = pym.Signal('x', state=np.ones(domain.nel) * volfrac)\n+\n+    # Start building the modular network\n+    func = pym.Network(print_timing=False)\n+\n+    do_padding = True  # Switch padding on or off\n+    if do_padding:\n+        ''' Filter with padding to avoid the design 'sticking' to the boundaries\n+        Set densities just outside the boundaries to 0, except for the xmax. The xmax boundary is kept symmetric, which \n+        is the standard behavior of a density filter. This is done to allow full solid to be formed here in case the \n+        mechanical load is at the boundary.\n+        '''\n+        sxfilt = func.append(pym.FilterConv(sx, domain=domain, radius=filter_radius,\n+                                            xmin_bc=0, xmax_bc='symmetric',\n+                                            ymin_bc=0, ymax_bc=0,\n+                                            zmin_bc=0, zmax_bc=0))\n+        m_filt = func[-1]\n+        ''' Furthermore, a density of 1 is set just outside the boundary condition at xmin for correct filtering '''\n+        xrange = np.arange(m_filt.pad_sizes[0])\n+        yrange = m_filt.pad_sizes[1] + np.arange(domain.nely//3, domain.nely - domain.nely//3)\n+        zrange = np.array([0]) if domain.dim < 3 else m_filt.pad_sizes[2] + np.arange(domain.nelz//3, domain.nelz - domain.nelz//3)\n+        ex, ey, ez = np.meshgrid(xrange, yrange, zrange)\n+        m_filt.override_padded_values((ex, ey, ez), 1.0)\n+        ''' Everything from here is the same as in ex_compliance.py '''\n+    else:\n+        # Use regular density filter (without padding) to see the difference\n+        sxfilt = func.append(pym.DensityFilter(sx, domain=domain, radius=filter_radius))\n+\n+    # Heaviside projections\n+    etaDi, etaNo, etaEr = 0.3, 0.5, 0.7\n+    sBeta = pym.Signal(\"beta\")\n+    func.append(Continuation([], sBeta, start=1.0, stop=20.0, stepstart=10))\n+\n+    sxNom = pym.Signal(\"xnominal\")\n+    sxEr = pym.Signal(\"xeroded\")\n+    sxDi = pym.Signal(\"xdilated\")\n+\n+    heaviside = \"(tanh(inp1 * {0}) + tanh(inp1 * (inp0 - {0}))) / (tanh(inp1 * {0}) + tanh(inp1 * (1 - {0})))\"\n+    func.append(pym.MathGeneral([sxfilt, sBeta], sxNom, expression=heaviside.format(etaNo)))\n+    func.append(pym.MathGeneral([sxfilt, sBeta], sxEr, expression=heaviside.format(etaEr)))\n+    func.append(pym.MathGeneral([sxfilt, sBeta], sxDi, expression=heaviside.format(etaDi)))\n+\n+    sx_analysis = sxEr\n+\n+    # Show the design on the screen as it optimizes\n+    func.append(pym.PlotDomain(sx_analysis, domain=domain, saveto=\"out/design\", clim=[0, 1]))\n+\n+    # SIMP material interpolation\n+    sSIMP = func.append(pym.MathGeneral(sx_analysis, expression=f\"{xmin} + {1.0 - xmin}*inp0^3\"))\n+\n+    # System matrix assembly module\n+    sK = func.append(pym.AssembleStiffness(sSIMP, domain=domain, bc=boundary_dofs))\n+\n+    # Linear system solver. The linear solver can be chosen by uncommenting any of the following lines.\n+    solver = None  # Default (solver is automatically chosen based on matrix properties)\n+    # solver = pym.solvers.SolverSparsePardiso()  # Requires Intel MKL installed\n+    # solver = pym.solvers.SolverSparseCholeskyCVXOPT()  # Requires cvxopt installed\n+    # solver = pym.solvers.SolverSparseCholeskyScikit()  # Requires scikit installed\n+    su = func.append(pym.LinSolve([sK, sf], hermitian=True, solver=solver))\n+\n+    # Compliance calculation c = f^T u\n+    scompl = func.append(pym.EinSum([su, sf], expression='i,i->'))\n+    scompl.tag = 'compliance'\n+\n+    # MMA needs correct scaling of the objective\n+    sg0 = func.append(pym.Scaling(scompl, scaling=100.0))\n+    sg0.tag = \"objective\"\n+\n+    # Calculate the volume of the domain by adding all design densities together\n+    svol = func.append(pym.EinSum(sxNom, expression='i->'))\n+    svol.tag = 'volume'\n+\n+    # Volume constraint\n+    sg1 = func.append(pym.MathGeneral(svol, expression=f'10*(inp0/{domain.nel} - {volfrac})'))\n+    sg1.tag = \"volume constraint\"\n+\n+    # Maybe you want to check the design-sensitivities?\n+    do_finite_difference = False\n+    if do_finite_difference:\n+        pym.finite_difference(func, sx, [sg0, sg1], dx=1e-4)\n+        exit()\n+\n+    func.append(pym.PlotIter([sg0, sg1]))  # Plot iteration history\n+\n+    # Do the optimization with MMA\n+    pym.minimize_mma(func, [sx], [sg0, sg1])\n+\n+    # Do the optimization with OC\n+    # pym.minimize_oc(func, sx, sg0)\n+\n+    # Here you can do some post processing\n+    print(\"The optimization has finished!\")\n+    print(f\"The final compliance value obtained is {scompl.state}\")\n+    print(f\"The maximum displacement is {max(np.absolute(su.state))}\")\ndiff --git a/examples/topology_optimization/ex_volume_stress.py b/examples/topology_optimization/ex_volume_stress.py\nindex af1a604..ca60eec 100644\n--- a/examples/topology_optimization/ex_volume_stress.py\n+++ b/examples/topology_optimization/ex_volume_stress.py\n@@ -28,54 +28,6 @@\n max_displacement = 20.0\n \n \n-class Stress(pym.Module):\n-    def _prepare(self, E=1, nu=0.3, plane='strain', domain=pym.DomainDefinition, *args, **kwargs):\n-        siz = domain.element_size\n-        self.domain = domain\n-\n-        # Constitutive model\n-        self.D = siz[2] * get_D(E, nu, plane.lower())\n-\n-        # Numerical integration\n-        self.B = np.zeros((3, 8), dtype=float)\n-        w = np.prod(siz[:domain.dim] / 2)\n-        for n in domain.node_numbering:\n-            pos = n * (siz / 2) / np.sqrt(3)  # Sampling point\n-            dN_dx = domain.eval_shape_fun_der(pos)\n-            self.B += w * get_B(dN_dx)\n-\n-        self.dofconn = domain.get_dofconnectivity(2)\n-\n-    def _response(self, u):\n-        self.elemental_strain = self.B.dot(u[self.dofconn].transpose())\n-        self.elemental_strain[2, :] *= 2  # voigt notation\n-        return self.D.dot(self.elemental_strain).transpose()\n-\n-    def _sensitivity(self, dfdv):\n-        dgdsstrainmat = np.einsum('jk,kl->jl', dfdv, self.D)\n-        dgdsstrainmat[:, 2] *= 2\n-        dgdue = np.einsum('ij,jl->il', dgdsstrainmat, self.B)\n-\n-        y = np.zeros(self.domain.nnodes * 2)\n-        for i in range(0, self.domain.nel):\n-            y[self.dofconn[i, :]] += dgdue[i, :]\n-        return y\n-\n-\n-class VonMises(pym.Module):\n-    def _prepare(self, *args, **kwargs):\n-        # Vandermonde matrix\n-        self.V = np.array([[1, -0.5, 0], [-0.5, 1, 0], [0, 0, 3]])\n-\n-    def _response(self, x):\n-        self.x = x\n-        self.y = (x.dot(self.V) * x).sum(1)\n-        return np.sqrt(self.y)\n-\n-    def _sensitivity(self, dfdv):\n-        return dfdv[:, np.newaxis] * (self.y ** (-0.5))[:, np.newaxis] * self.x.dot(self.V)\n-\n-\n class ConstraintAggregation(pym.Module):\n     \"\"\"\n     Unified aggregation and relaxation.\n@@ -101,7 +53,7 @@ def _response(self, x):\n         self.x = x\n         self.y = self.x + 1\n         self.z = self.y ** self.P\n-        z = ((1 / len(self.x)) * np.sum(self.z)) ** (1 / self.P)  # P-mean aggregation function\n+        z = (np.sum(self.z) / self.n) ** (1 / self.P)  # P-mean aggregation function\n         return z - 1\n \n     def _sensitivity(self, dfdc):\n@@ -139,14 +91,18 @@ def _sensitivity(self, dfdc):\n     s_force = pym.Signal('f', state=f)\n     s_displacement = fn.append(pym.LinSolve([s_K, s_force]))\n \n-    # Calculate stress\n-    s_stress = fn.append(Stress([s_displacement], domain=domain))\n-    s_stress_vm = fn.append(VonMises([s_stress]))\n-    s_stress_constraints = fn.append(pym.Scaling([s_stress_vm], maxval=maximum_vm_stress, scaling=1.0))\n+    # Calculate stress components\n+    s_stress = fn.append(pym.Stress(s_displacement, domain=domain))\n \n-    s_stress_constraints_scaled = fn.append(\n-        pym.EinSum([s_filtered_variables, s_stress_constraints], expression='i,i->i'))\n+    # Calculate Von-Mises stress\n+    V = np.array([[1, -0.5, 0], [-0.5, 1, 0], [0, 0, 3]])  # Vandermonde matrix\n+    s_stress_vm2 = fn.append(pym.EinSum([s_stress, pym.Signal(state=V), s_stress], expression='ij,ik,kj->j'))\n+    s_stress_vm = fn.append(pym.MathGeneral(s_stress_vm2, expression='sqrt(inp0)'))\n \n+    # Stress constraint\n+    s_stress_constraints = fn.append(pym.Scaling(s_stress_vm, maxval=maximum_vm_stress, scaling=1.0))\n+    s_stress_constraints_scaled = fn.append(pym.EinSum([s_filtered_variables, s_stress_constraints],\n+                                                       expression='i,i->i'))\n     s_stress_constraint = fn.append(ConstraintAggregation([s_stress_constraints_scaled], P=10))\n     s_stress_constraint.tag = \"Stress constraint\"\n \ndiff --git a/pymoto/__init__.py b/pymoto/__init__.py\nindex 823cded..c0af7d0 100644\n--- a/pymoto/__init__.py\n+++ b/pymoto/__init__.py\n@@ -5,21 +5,23 @@\n # Imports from common\n from .common.dyadcarrier import DyadCarrier\n from .common.mma import MMA\n-from .common.solvers import matrix_is_complex, matrix_is_diagonal, matrix_is_symmetric, matrix_is_hermitian, LinearSolver, LDAWrapper\n-from .common.solvers_dense import SolverDiagonal, SolverDenseQR, SolverDenseLU, SolverDenseCholesky, SolverDenseLDL\n-from .common.solvers_sparse import SolverSparsePardiso, SolverSparseLU, SolverSparseCholeskyScikit, SolverSparseCholeskyCVXOPT\n+\n+# Import solvers\n+from . import solvers\n \n # Modular inports\n from .core_objects import Signal, Module, Network, make_signals\n \n # Import modules\n from .modules.assembly import AssembleGeneral, AssembleStiffness, AssembleMass, AssemblePoisson\n+from .modules.assembly import ElementOperation, Strain, Stress\n from .modules.autodiff import AutoMod\n from .modules.complex import MakeComplex, RealPart, ImagPart, ComplexNorm\n from .modules.filter import FilterConv, Filter, DensityFilter, OverhangFilter\n from .modules.generic import MathGeneral, EinSum, ConcatSignal\n-from .modules.io import PlotDomain, PlotGraph, PlotIter, WriteToVTI\n+from .modules.io import FigModule, PlotDomain, PlotGraph, PlotIter, WriteToVTI\n from .modules.linalg import Inverse, LinSolve, EigenSolve, SystemOfEquations, StaticCondensation\n+from .modules.aggregation import AggScaling, AggActiveSet, Aggregation, PNorm, SoftMinMax, KSFunction\n from .modules.scaling import Scaling\n \n # Further helper routines\n@@ -28,21 +30,25 @@\n __all__ = [\n     'Signal', 'Module', 'Network', 'make_signals',\n     'finite_difference', 'minimize_oc', 'minimize_mma',\n+\n     # Common\n     'MMA',\n     'DyadCarrier',\n     'DomainDefinition',\n-    'matrix_is_complex', 'matrix_is_diagonal', 'matrix_is_symmetric', 'matrix_is_hermitian',\n-    'LinearSolver', 'LDAWrapper',\n-    'SolverDiagonal', 'SolverDenseQR', 'SolverDenseLU', 'SolverDenseCholesky', 'SolverDenseLDL',\n-    'SolverSparsePardiso', 'SolverSparseLU', 'SolverSparseCholeskyScikit', 'SolverSparseCholeskyCVXOPT',\n+    'solvers',\n+\n+    # Helpers\n+    \"AggScaling\", \"AggActiveSet\",\n+\n     # Modules\n     \"MathGeneral\", \"EinSum\", \"ConcatSignal\",\n     \"Inverse\", \"LinSolve\", \"EigenSolve\", \"SystemOfEquations\", \"StaticCondensation\",\n     \"AssembleGeneral\", \"AssembleStiffness\", \"AssembleMass\", \"AssemblePoisson\",\n+    \"ElementOperation\", \"Strain\", \"Stress\",\n     \"FilterConv\", \"Filter\", \"DensityFilter\", \"OverhangFilter\",\n-    \"PlotDomain\", \"PlotGraph\", \"PlotIter\", \"WriteToVTI\",\n+    \"FigModule\", \"PlotDomain\", \"PlotGraph\", \"PlotIter\", \"WriteToVTI\",\n     \"MakeComplex\", \"RealPart\", \"ImagPart\", \"ComplexNorm\",\n     \"AutoMod\",\n-    \"Scaling\"\n+    \"Aggregation\", \"PNorm\", \"SoftMinMax\", \"KSFunction\",\n+    \"Scaling\",\n ]\ndiff --git a/pymoto/common/domain.py b/pymoto/common/domain.py\nindex facd32b..95e2883 100644\n--- a/pymoto/common/domain.py\n+++ b/pymoto/common/domain.py\n@@ -5,6 +5,31 @@\n import warnings\n from typing import Union\n import numpy as np\n+from matplotlib.patches import PathPatch\n+from matplotlib.path import Path\n+\n+\n+def plot_deformed_element(ax, x, y, **kwargs):\n+    codes, verts = zip(*[\n+        (Path.MOVETO, [x[0], y[0]]),\n+        (Path.LINETO, [x[1], y[1]]),\n+        (Path.LINETO, [x[3], y[3]]),\n+        (Path.LINETO, [x[2], y[2]]),\n+        (Path.CLOSEPOLY, [x[0], y[0]])])\n+    path = Path(verts, codes)\n+    patch = PathPatch(path, **kwargs)\n+    ax.add_artist(patch)\n+    return patch\n+\n+\n+def get_path(x, y):\n+    codes, verts = zip(*[\n+        (Path.MOVETO, [x[0], y[0]]),\n+        (Path.LINETO, [x[1], y[1]]),\n+        (Path.LINETO, [x[3], y[3]]),\n+        (Path.LINETO, [x[2], y[2]]),\n+        (Path.CLOSEPOLY, [x[0], y[0]])])\n+    return Path(verts, codes)\n \n \n class DomainDefinition:\n@@ -100,6 +125,13 @@ def __init__(self, nelx: int, nely: int, nelz: int = 0, unitx: float = 1.0, unit\n         self.conn = np.zeros((self.nel, self.elemnodes), dtype=int)\n         self.conn[el, :] = self.get_elemconnectivity(elx, ely, elz)\n \n+        # Helper for element slicing\n+        eli, elj, elk = np.meshgrid(np.arange(self.nelx), np.arange(self.nely), np.arange(self.nelz), indexing='ij')\n+        self.elements = self.get_elemnumber(eli, elj, elk)\n+\n+        ndi, ndj, ndk = np.meshgrid(np.arange(self.nelx+1), np.arange(self.nely+1), np.arange(self.nelz+1), indexing='ij')\n+        self.nodes = self.get_nodenumber(ndi, ndj, ndk)\n+\n     def get_elemnumber(self, eli: Union[int, np.ndarray], elj: Union[int, np.ndarray], elk: Union[int, np.ndarray] = 0):\n         \"\"\" Gets the element number(s) for element(s) with given Cartesian indices (i, j, k)\n \n@@ -126,7 +158,7 @@ def get_nodenumber(self, nodi: Union[int, np.ndarray], nodj: Union[int, np.ndarr\n         \"\"\"\n         return (nodk * (self.nely + 1) + nodj) * (self.nelx + 1) + nodi\n \n-    def get_node_indices(self, nod_idx: Union[int, np.ndarray]):\n+    def get_node_indices(self, nod_idx: Union[int, np.ndarray] = None):\n         \"\"\" Gets the Cartesian index (i, j, k) for given node number(s)\n \n         Args:\n@@ -135,16 +167,18 @@ def get_node_indices(self, nod_idx: Union[int, np.ndarray]):\n         Returns:\n             i, j, k for requested node(s); k is only returned in 3D\n         \"\"\"\n+        if nod_idx is None:\n+            nod_idx = np.arange(self.nnodes)\n         nodi = nod_idx % (self.nelx + 1)\n         nodj = (nod_idx // (self.nelx + 1)) % (self.nely + 1)\n         if self.dim == 2:\n-            return nodi, nodj\n+            return np.stack([nodi, nodj], axis=0)\n         nodk = nod_idx // ((self.nelx + 1)*(self.nely + 1))\n-        return nodi, nodj, nodk\n+        return np.stack([nodi, nodj, nodk], axis=0)\n \n-    def get_node_position(self, nod_idx: Union[int, np.ndarray]):\n+    def get_node_position(self, nod_idx: Union[int, np.ndarray] = None):\n         ijk = self.get_node_indices(nod_idx)\n-        return [idx * self.element_size[ii] for ii, idx in enumerate(ijk)]\n+        return (self.element_size[:self.dim] * ijk.T).T\n \n     def get_elemconnectivity(self, i: Union[int, np.ndarray], j: Union[int, np.ndarray], k: Union[int, np.ndarray] = 0):\n         \"\"\" Get the connectivity for element identified with Cartesian indices (i, j, k)\n@@ -230,6 +264,27 @@ def eval_shape_fun_der(self, pos: np.ndarray):\n             dN_dx[i, :] *= np.array([n[i] for n in self.node_numbering])  # Flip +/- signs according to node position\n         return dN_dx\n \n+    def plot(self, ax, deformation=None, scaling=None):\n+        patches = []\n+        for e in range(self.nel):\n+            n = self.conn[e]\n+            x, y = self.get_node_position(n)\n+            u, v = deformation[n * 2], deformation[n * 2 + 1]\n+            color = (1 - scaling[e], 1 - scaling[e], 1 - scaling[e]) if scaling is not None else 'grey'\n+            patch = plot_deformed_element(ax, x + u, v + y, linewidth=0.1, color=color)\n+            patches.append(patch)\n+        return patches\n+\n+    def update_plot(self, patches, deformation=None, scaling=None):\n+        for e in range(self.nel):\n+            patch = patches[e]\n+            n = self.conn[e]\n+            x, y = self.get_node_position(n)\n+            u, v = deformation[n * 2], deformation[n * 2 + 1]\n+            color = (1 - scaling[e], 1 - scaling[e], 1 - scaling[e]) if scaling is not None else 'grey'\n+            patch.set_color(color)\n+            patch.set_path(self.get_path(x + u, y + v))\n+\n     # flake8: noqa: C901\n     def write_to_vti(self, vectors: dict, filename=\"out.vti\", scale=1.0, origin=(0.0, 0.0, 0.0)):\n         \"\"\" Write all given vectors to a Paraview (VTI) file\ndiff --git a/pymoto/common/dyadcarrier.py b/pymoto/common/dyadcarrier.py\nindex 776d1ae..53ac7c1 100644\n--- a/pymoto/common/dyadcarrier.py\n+++ b/pymoto/common/dyadcarrier.py\n@@ -1,8 +1,8 @@\n-from typing import Union, Iterable\n+from typing import Union, Iterable, List\n import warnings\n import numpy as np\n from numpy.typing import NDArray\n-from scipy.sparse import spmatrix\n+from scipy.sparse import spmatrix, coo_matrix\n from ..utils import _parse_to_list\n try:  # Import fast optimized einsum\n     from opt_einsum import contract as einsum\n@@ -31,7 +31,7 @@ def isnullslice(x):\n \n \n class DyadCarrier(object):\n-    \"\"\" Efficient storage for dyadic or rank-N matrix\n+    r\"\"\" Efficient storage for dyadic or rank-N matrix\n \n     Stores only the vectors instead of creating a full rank-N matrix\n     :math:`\\mathbf{A} = \\sum_k^N \\mathbf{u}_k\\otimes\\mathbf{v}_k`\n@@ -67,7 +67,7 @@ def size(self):\n             return self.ulen * self.vlen\n \n     def add_dyad(self, u: Iterable, v: Iterable = None, fac: float = None):\n-        \"\"\" Adds a list of vectors to the dyad carrier\n+        r\"\"\" Adds a list of vectors to the dyad carrier\n \n         Checks for conforming sizes of `u` and `v`. The data inside the vectors are copied.\n \n@@ -373,6 +373,31 @@ def contract(self, mat: Union[NDArray, spmatrix] = None, rows: NDArray[int] = No\n \n         return val\n \n+    def contract_multi(self, mats: List[spmatrix], dtype=None):\n+        \"\"\" Faster version of contraction for a list of sparse matrices \"\"\"\n+        if dtype is None:\n+            dtype = np.result_type(self.dtype, mats[0].dtype)\n+        val = np.zeros(len(mats), dtype=dtype)\n+\n+        if len(self.u) == 0 or len(self.v) == 0:\n+            return val\n+        U = np.array(self.u).T\n+        V = np.array(self.v).T\n+\n+        for i, m in enumerate(mats):\n+            if m is None:\n+                vali = 0.0\n+            else:\n+                try:\n+                    if not isinstance(m, coo_matrix):\n+                        warnings.warn(\"Inefficiency: Matrix must be converted to coo_matrix for contraction\")\n+                    mat_coo = m.tocoo()\n+                    vali = np.einsum('ij,i,ij->', U[mat_coo.row, :], mat_coo.data, V[mat_coo.col, :])\n+                except AttributeError:\n+                    vali = self.contract(m)\n+            val[i] = vali\n+        return val\n+\n     def todense(self):\n         \"\"\" Returns a full (dense) matrix from the DyadCarrier matrix \"\"\"\n         warning_size = 100e+6  # Bytes\n@@ -387,6 +412,10 @@ def todense(self):\n \n         return val\n \n+    def toarray(self):\n+        \"\"\" Convert to array, same as todense(). To be consistent with scipy.sparse \"\"\"\n+        return self.todense()\n+\n     def iscomplex(self):\n         \"\"\" Check if the DyadCarrier is of complex type \"\"\"\n         return np.iscomplexobj(np.array([], dtype=self.dtype))\ndiff --git a/pymoto/common/mma.py b/pymoto/common/mma.py\nindex aa98687..38b4c6c 100644\n--- a/pymoto/common/mma.py\n+++ b/pymoto/common/mma.py\n@@ -219,11 +219,16 @@ class MMA:\n         xmin: Minimum design variable (can be a vector)\r\n         xmax: Maximum design variable (can be a vector)\r\n         fn_callback: A function that is called just before calling the response() in each iteration\r\n-        verbosity: 0 - No prints, 1 - Only convergence message, 2 - Convergence and iteration info, 3 - Extended info\r\n+        verbosity: Level of information to print\r\n+          0 - No prints\r\n+          1 - Only convergence message\r\n+          2 - Convergence and iteration info (default)\r\n+          3 - Additional info on variables\r\n+          4 - Additional info on sensitivity information\r\n \r\n     \"\"\"\r\n \r\n-    def __init__(self, function, variables, responses, tolx=1e-4, tolf=0.0, move=0.1, maxit=100, xmin=0.0, xmax=1.0, fn_callback=None, verbosity=0, **kwargs):\r\n+    def __init__(self, function, variables, responses, tolx=1e-4, tolf=0.0, move=0.1, maxit=100, xmin=0.0, xmax=1.0, fn_callback=None, verbosity=2, **kwargs):\r\n         self.funbl = function\r\n         self.verbosity = verbosity\r\n \r\n@@ -301,8 +306,7 @@ def response(self):\n                 self.xmin[self.cumlens[i]:self.cumlens[i+1]] = xminvals[i]\r\n \r\n         if len(self.xmin) != self.n:\r\n-            raise RuntimeError(\r\n-                \"Length of the xmin vector not correct ({} != {})\".format(len(self.xmin), self.n))\r\n+            raise RuntimeError(f\"Length of the xmin vector ({len(self.xmin)}) should be equal to # design variables ({self.n})\")\r\n \r\n         if not hasattr(self.xmax, '__len__'):\r\n             self.xmax = self.xmax * np.ones_like(xval)\r\n@@ -313,17 +317,19 @@ def response(self):\n                 self.xmax[self.cumlens[i]:self.cumlens[i + 1]] = xmaxvals[i]\r\n \r\n         if len(self.xmax) != self.n:\r\n-            raise RuntimeError(\"Length of the xmax vector not correct ({} != {})\".format(len(self.xmax), self.n))\r\n+            raise RuntimeError(f\"Length of the xmax vector ({len(self.xmax)}) should be equal to # design variables ({self.n})\")\r\n \r\n-        # Set movelimit in case of multiple\r\n         if hasattr(self.move, '__len__'):\r\n-            if len(self.move) == len(self.variables):\r\n-                movevals = self.move\r\n+            # Set movelimit in case of multiple are given\r\n+            move_input = np.asarray(self.move).copy()\r\n+            if move_input.size == len(self.variables):\r\n                 self.move = np.zeros_like(xval)\r\n-                for i in range(len(movevals)):\r\n-                    self.move[self.cumlens[i]:self.cumlens[i + 1]] = movevals[i]\r\n+                for i in range(move_input.size):\r\n+                    self.move[self.cumlens[i]:self.cumlens[i + 1]] = move_input[i]\r\n             elif len(self.move) != self.n:\r\n-                raise RuntimeError(\"Length of the move vector not correct ({} != {})\".format(len(self.move), self.n))\r\n+                raise RuntimeError(f\"Length of the move vector ({len(self.move)}) should be equal to number of \"\r\n+                                   f\"design variable signals ({len(self.variables)}) or \"\r\n+                                   f\"total number of design variables ({self.n}).\")\r\n \r\n         fcur = 0.0\r\n         while self.iter < self.maxIt:\r\n@@ -333,12 +339,9 @@ def response(self):\n             # Set the new states\r\n             for i, s in enumerate(self.variables):\r\n                 if self.cumlens[i+1]-self.cumlens[i] == 1:\r\n-                    try:\r\n-                        s.state[:] = xval[self.cumlens[i]]\r\n-                    except TypeError:\r\n-                        s.state = xval[self.cumlens[i]]\r\n+                    s.state = xval[self.cumlens[i]]\r\n                 else:\r\n-                    s.state[:] = xval[self.cumlens[i]:self.cumlens[i+1]]\r\n+                    s.state = xval[self.cumlens[i]:self.cumlens[i+1]]\r\n \r\n             if self.fn_callback is not None:\r\n                 self.fn_callback()\r\n@@ -346,19 +349,11 @@ def response(self):\n             # Calculate response\r\n             self.funbl.response()\r\n \r\n-            # Update the states\r\n-            for i, s in enumerate(self.variables):\r\n-                if self.cumlens[i+1]-self.cumlens[i] == 1:\r\n-                    try:\r\n-                        xval[self.cumlens[i]] = s.state[:]\r\n-                    except (TypeError, IndexError):\r\n-                        xval[self.cumlens[i]] = s.state\r\n-                else:\r\n-                    xval[self.cumlens[i]:self.cumlens[i+1]] = s.state[:]\r\n-\r\n             # Save response\r\n             f = ()\r\n             for s in self.responses:\r\n+                if not np.isscalar(s.state):\r\n+                    raise TypeError(\"State of responses must be scalar.\")\r\n                 f += (s.state, )\r\n \r\n             # Check function change convergence criterion\r\n@@ -388,27 +383,48 @@ def response(self):\n                 # Reset sensitivities for the next response\r\n                 self.funbl.reset()\r\n \r\n-            # Display info on variables\r\n             if self.verbosity >= 3:\r\n+                # Display info on variables\r\n+                show_sensitivities = self.verbosity >= 4\r\n+                msg = \"\"\r\n                 for i, s in enumerate(self.variables):\r\n-                    isscal = self.cumlens[i + 1] - self.cumlens[i] == 1\r\n-                    msg = \"{0:>10s} = \".format(s.tag)\r\n-                    if isscal:\r\n-                        try:\r\n-                            msg += \"         {0: .3e}         \".format(s.state)\r\n-                        except TypeError:\r\n-                            msg += \"         {0: .3e}         \".format(s.state[0])\r\n+                    if show_sensitivities:\r\n+                        msg += \"{0:>10s} = \".format(s.tag[:10])\r\n                     else:\r\n-                        msg += \"[{0: .3e} ... {1: .3e}] \".format(min(s.state), max(s.state))\r\n-                    for j, s_out in enumerate(self.responses):\r\n-                        msg += \"| {0:>10s}/{1:10s} = \".format(\"d\"+s_out.tag, \"d\"+s.tag)\r\n-                        if isscal:\r\n-                            msg += \"         {0: .3e}         \".format(df[j][self.cumlens[i]])\r\n+                        msg += f\"{s.tag} = \"\r\n+\r\n+                    # Display value range\r\n+                    fmt = '% .2e'\r\n+                    minval, maxval = np.min(s.state), np.max(s.state)\r\n+                    mintag, maxtag = fmt % minval, fmt % maxval\r\n+                    if mintag == maxtag:\r\n+                        if show_sensitivities:\r\n+                            msg += f\"       {mintag}      \"\r\n                         else:\r\n-                            msg += \"[{0: .3e} ... {1: .3e}] \".format(min(df[j][self.cumlens[i]:self.cumlens[i+1]]), max(df[j][self.cumlens[i]:self.cumlens[i+1]]))\r\n-                    print(msg)\r\n+                            msg += f\" {mintag}\"\r\n+                    else:\r\n+                        sep = '\u2026' if len(s.state) > 2 else ','\r\n+                        msg += f\"[{mintag}{sep}{maxtag}]\"\r\n+                        if show_sensitivities:\r\n+                            msg += \" \"\r\n+\r\n+                    if show_sensitivities:\r\n+                        # Display info on sensivity values\r\n+                        for j, s_out in enumerate(self.responses):\r\n+                            msg += \"| {0:s}/{1:11s} = \".format(\"d\" + s_out.tag, \"d\" + s.tag[:10])\r\n+                            minval = np.min(df[j][self.cumlens[i]:self.cumlens[i+1]])\r\n+                            maxval = np.max(df[j][self.cumlens[i]:self.cumlens[i+1]])\r\n+                            mintag, maxtag = fmt % minval, fmt % maxval\r\n+                            if mintag == maxtag:\r\n+                                msg += f\"       {mintag}      \"\r\n+                            else:\r\n+                                sep = '\u2026' if self.cumlens[i + 1] - self.cumlens[i] > 2 else ','\r\n+                                msg += f\"[{mintag}{sep}{maxtag}] \"\r\n+                        msg += '\\n'\r\n+                    elif i != len(self.variables)-1:\r\n+                        msg += ', '\r\n+                print(msg)\r\n \r\n-            self.iter += 1\r\n             xnew, change = self.mmasub(xval.copy(), np.hstack(f), np.vstack(df))\r\n \r\n             # Stopping criteria on step size\r\n@@ -419,6 +435,7 @@ def response(self):\n                 break\r\n \r\n             xval = xnew\r\n+            self.iter += 1\r\n \r\n     def mmasub(self, xval, g, dg):\r\n         if self.dx is None:\r\n@@ -535,20 +552,33 @@ def mmasub(self, xval, g, dg):\n         change = np.average(abs(xval - xmma))\r\n \r\n         if self.verbosity >= 2:\r\n+            # Display iteration status message\r\n             msgs = [\"g{0:d}({1:s}): {2:+.4e}\".format(i, s.tag, g[i]) for i, s in enumerate(self.responses)]\r\n-            print(\"It. {0: 4d}, {1}\".format(self.iter, \", \".join(msgs)))\r\n+            max_infeasibility = max(g[1:])\r\n+            is_feasible = max_infeasibility <= 0\r\n+\r\n+            feasibility_tag = 'f' if is_feasible else ' '\r\n+            print(\"It. {0: 4d}, [{1:1s}] {2}\".format(self.iter, feasibility_tag, \", \".join(msgs)))\r\n \r\n-        if self.verbosity >=3:\r\n-            # Print changes\r\n-            printstr = \"Changes: \"\r\n+        if self.verbosity >= 3:\r\n+            # Report design feasibility\r\n+            iconst_max = np.argmax(g[1:])\r\n+            print(f\"  | {np.sum(g[1:]>0)} / {len(g)-1} violated constraints, \"\r\n+                  f\"max. violation ({self.responses[iconst_max+1].tag}) = {'%.2g'%g[iconst_max+1]}\")\r\n+\r\n+            # Print design changes\r\n+            change_msgs = []\r\n             for i, s in enumerate(self.variables):\r\n-                isscal = self.cumlens[i + 1] - self.cumlens[i] == 1\r\n-                if isscal:\r\n-                    chg = abs(xval[self.cumlens[i]] - xmma[self.cumlens[i]])\r\n+                minchg = np.min(abs(xval[self.cumlens[i]:self.cumlens[i + 1]] - xmma[self.cumlens[i]:self.cumlens[i + 1]]))\r\n+                maxchg = np.max(abs(xval[self.cumlens[i]:self.cumlens[i + 1]] - xmma[self.cumlens[i]:self.cumlens[i + 1]]))\r\n+                fmt = '%.2g'\r\n+                mintag, maxtag = fmt % minchg, fmt % maxchg\r\n+\r\n+                if mintag == maxtag:\r\n+                    change_msgs.append(f\"\u0394({s.tag}) = {mintag}\")\r\n                 else:\r\n-                    chg = np.average(abs(xval[self.cumlens[i]:self.cumlens[i + 1]] - xmma[self.cumlens[i]:self.cumlens[i + 1]]))\r\n+                    change_msgs.append(f\"\u0394({s.tag}) = {mintag}\u2026{maxtag}\")\r\n \r\n-                printstr += \"{0:s} = {1:.3e}   \".format(\"\u0394_\"+s.tag, chg)\r\n-            print(printstr)\r\n+            print(f\"  | Changes: {', '.join(change_msgs)}\")\r\n \r\n-        return xmma, change\n\\ No newline at end of file\n+        return xmma, change\r\ndiff --git a/pymoto/common/solvers.py b/pymoto/common/solvers.py\ndeleted file mode 100644\nindex 4bb5a64..0000000\n--- a/pymoto/common/solvers.py\n+++ /dev/null\n@@ -1,236 +0,0 @@\n-import numpy as np\n-import scipy.sparse as sps\n-try:\n-    import cvxopt\n-    _has_cvxopt = True\n-except ImportError:\n-    _has_cvxopt = False\n-\n-\n-def is_cvxopt_spmatrix(A):\n-    \"\"\" Checks if the argument is a cvxopt sparse matrix \"\"\"\n-    return isinstance(A, cvxopt.spmatrix) if _has_cvxopt else False\n-\n-\n-def matrix_is_complex(A):\n-    \"\"\" Checks if the matrix is complex \"\"\"\n-    if is_cvxopt_spmatrix(A):\n-        return A.typecode == 'z'\n-    else:\n-        return np.iscomplexobj(A)\n-\n-\n-def matrix_is_diagonal(A):\n-    \"\"\" Checks if the matrix is diagonal\"\"\"\n-    if sps.issparse(A):\n-        if isinstance(A, sps.dia_matrix):\n-            return len(A.offsets) == 1 and A.offsets[0] == 0\n-        else:\n-            return np.allclose((A - sps.spdiags(A.diagonal(), 0, *A.shape)).data, 0.0)\n-    elif is_cvxopt_spmatrix(A):\n-        return max(abs(A.I - A.J)) == 0\n-    else:\n-        return np.allclose(A, np.diag(np.diag(A)))\n-\n-\n-def matrix_is_symmetric(A):\n-    \"\"\" Checks whether a matrix is numerically symmetric \"\"\"\n-    if sps.issparse(A):\n-        return np.allclose((A-A.T).data, 0)\n-    elif is_cvxopt_spmatrix(A):\n-        return np.isclose(max(abs(A-A.T)), 0.0)\n-    else:\n-        return np.allclose(A, A.T)\n-\n-\n-def matrix_is_hermitian(A):\n-    \"\"\" Checks whether a matrix is numerically Hermitian \"\"\"\n-    if matrix_is_complex(A):\n-        if sps.issparse(A):\n-            return np.allclose((A-A.T.conj()).data, 0)\n-        elif is_cvxopt_spmatrix(A):\n-            return np.isclose(max(abs(A-A.ctrans())), 0.0)\n-        else:\n-            return np.allclose(A, A.T.conj())\n-    else:\n-        return matrix_is_symmetric(A)\n-\n-\n-class LinearSolver:\n-    \"\"\" Base class of all linear solvers\n-\n-    Keyword Args:\n-        A (matrix): Optionally provide a matrix, which is used in :method:`update` right away.\n-\n-    Attributes:\n-        defined (bool): Flag if the solver is able to run, e.g. false if some dependent library is not available\n-    \"\"\"\n-\n-    defined = True\n-    _err_msg = \"\"\n-\n-    def __init__(self, A=None):\n-        if A is not None:\n-            self.update(A)\n-\n-    def update(self, A):\n-        \"\"\" Updates with a new matrix of the same structure\n-\n-        Args:\n-            A (matrix): The new matrix of size ``(N, N)``\n-\n-        Returns:\n-            self\n-        \"\"\"\n-        raise NotImplementedError(f\"Solver not implemented {self._err_msg}\")\n-\n-    def solve(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}`\n-\n-        Args:\n-            rhs: Right hand side :math:`\\mathbf{b}` of shape ``(N)`` or ``(N, K)`` for multiple right-hand-sides\n-\n-        Returns:\n-            Solution vector :math:`\\mathbf{x}` of same shape as :math:`\\mathbf{b}`\n-        \"\"\"\n-        raise NotImplementedError(f\"Solver not implemented {self._err_msg}\")\n-\n-    def adjoint(self, rhs):\n-        r\"\"\" Solves the adjoint linear system of equations\n-\n-        The system of equations is :math:`\\mathbf{A}^\\text{H} \\mathbf{x} = \\mathbf{b}` (conjugate transpose) in case of\n-        complex matrix or :math:`\\mathbf{A}^\\text{T} \\mathbf{x} = \\mathbf{b}` for a real-valued matrix.\n-\n-        Args:\n-            rhs: Right hand side :math:`\\mathbf{b}` of shape ``(N)`` or ``(N, K)`` for multiple right-hand-sides\n-\n-        Returns:\n-            Solution vector :math:`\\mathbf{x}` of same shape as :math:`\\mathbf{b}`\n-        \"\"\"\n-        raise NotImplementedError(f\"Solver not implemented {self._err_msg}\")\n-\n-    @staticmethod\n-    def residual(A, x, b):\n-        r\"\"\" Calculates the (relative) residual of the linear system of equations\n-\n-        The residual is calculated as\n-        :math:`r = \\frac{\\left| \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\right|}{\\left| \\mathbf{b} \\right|}`\n-\n-        Args:\n-            A: The matrix\n-            x: Solution vector\n-            b: Right-hand side\n-\n-        Returns:\n-            Residual value\n-        \"\"\"\n-        return np.linalg.norm(A@x - b) / np.linalg.norm(b)\n-\n-\n-class LDAWrapper(LinearSolver):\n-    r\"\"\" Linear dependency aware solver (LDAS)\n-\n-    This solver uses previous solutions of the system :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` to reduce computational\n-    effort. In case the solution :math:`\\mathbf{x}` is linearly dependent on the previous solutions, the solution\n-    will be nearly free of cost.\n-\n-    Args:\n-        solver: The internal solver to be used\n-        tol (optional): Residual tolerance above which the internal solver is used to add a new solution vector.\n-        A (optional): The matrix :math:`\\mathbf{A}`\n-\n-    References:\n-\n-    Koppen, S., van der Kolk, M., van den Boom, S., & Langelaar, M. (2022).\n-    Efficient computation of states and sensitivities for compound structural optimisation problems using a Linear Dependency Aware Solver (LDAS).\n-    Structural and Multidisciplinary Optimization, 65(9), 273.\n-    DOI: 10.1007/s00158-022-03378-8\n-    \"\"\"\n-    def __init__(self, solver: LinearSolver, tol=1e-8, A=None, hermitian=False, symmetric=False):\n-        self.solver = solver\n-        self.tol = tol\n-        self.x_stored = []\n-        self.b_stored = []\n-        self.xadj_stored = []\n-        self.badj_stored = []\n-        self.A = None\n-        self._did_solve = False  # For debugging purposes\n-        self._last_rtol = 0.\n-        self.hermitian = hermitian\n-        self.symmetric = symmetric\n-        super().__init__(A)\n-\n-    def update(self, A):\n-        \"\"\" Clear the internal stored solution vectors and update the internal ``solver`` \"\"\"\n-        self.A = A\n-        self.x_stored.clear()\n-        self.b_stored.clear()\n-        self.xadj_stored.clear()\n-        self.badj_stored.clear()\n-        self.solver.update(A)\n-\n-    def _do_solve_1rhs(self, A, rhs, x_data, b_data, solve_fn):\n-        rhs_loc = rhs.copy()\n-        sol = 0\n-\n-        # Check linear dependencies in the rhs using modified Gram-Schmidt\n-        for (x, b) in zip(x_data, b_data):\n-            alpha = rhs_loc.conj() @ b / (b.conj() @ b)\n-            rhs_loc -= alpha * b\n-            sol += alpha * x\n-\n-        # Check tolerance\n-        self._last_rtol = 1.0 if len(x_data) == 0 else self.residual(A, sol, rhs)\n-\n-        if self._last_rtol > self.tol:\n-            # Calculate a new solution\n-            xnew = solve_fn(rhs_loc)\n-            x_data.append(xnew)\n-            b_data.append(rhs_loc)\n-            sol += xnew\n-            self._did_solve = True\n-        else:\n-            self._did_solve = False\n-\n-        return sol\n-\n-    def _solve_1x(self, b):\n-        return self._do_solve_1rhs(self.A, b, self.x_stored, self.b_stored, self.solver.solve)\n-\n-    def _adjoint_1x(self, b):\n-        return self._do_solve_1rhs(self.A.conj().T, b, self.xadj_stored, self.badj_stored, self.solver.adjoint)\n-\n-    def solve(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by performing a modified\n-        Gram-Schmidt over the previously calculated solutions :math:`\\mathbf{U}` and corresponding right-hand-sides\n-        :math:`\\mathbf{F}`. This is used to construct an approximate solution\n-        :math:`\\tilde{\\mathbf{x}} = \\sum_k \\alpha_k \\mathbf{u}_k` in the subspace of :math:`\\mathbf{U}`.\n-        If the residual of :math:`\\mathbf{A} \\tilde{\\mathbf{x}} = \\mathbf{b}` is above the tolerance, a new solution\n-        :math:`\\mathbf{u}_{k+1}` will be added to the database such that\n-        :math:`\\mathbf{x} = \\tilde{\\mathbf{x}}+\\mathbf{u}_{k+1}` is the solution to the system\n-        :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}`.\n-\n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        if rhs.ndim == 1:\n-            return self._solve_1x(rhs)\n-        else:  # Multiple rhs\n-            sol = []\n-            for i in range(rhs.shape[-1]):\n-                sol.append(self._solve_1x(rhs[..., i]))\n-            return np.stack(sol, axis=-1)\n-\n-    def adjoint(self, rhs):\n-        if self.hermitian:\n-            return self.solve(rhs)\n-        elif self.symmetric:\n-            return self.solve(rhs.conj()).conj()\n-        else:\n-            if rhs.ndim == 1:\n-                return self._adjoint_1x(rhs)\n-            else:  # Multiple rhs\n-                sol = []\n-                for i in range(rhs.shape[-1]):\n-                    sol.append(self._adjoint_1x(rhs[..., i]))\n-                return np.stack(sol, axis=-1)\ndiff --git a/pymoto/core_objects.py b/pymoto/core_objects.py\nindex dc07632..2755e48 100644\n--- a/pymoto/core_objects.py\n+++ b/pymoto/core_objects.py\n@@ -1,9 +1,11 @@\n-from typing import Union, List, Any\n+import sys\n import warnings\n import inspect\n import time\n-from .utils import _parse_to_list, _concatenate_to_array, _split_from_array\n+import copy\n+from typing import Union, List, Any\n from abc import ABC, abstractmethod\n+from .utils import _parse_to_list, _concatenate_to_array, _split_from_array\n \n \n # Local helper functions\n@@ -11,7 +13,7 @@ def err_fmt(*args):\n     \"\"\" Format error strings for locating Modules and Signals\"\"\"\n     err_str = \"\"\n     for a in args:\n-        err_str += f\"\\n\\t[ {a} ]\"\n+        err_str += f\"\\n\\t| {a}\"\n     return err_str\n \n \n@@ -76,16 +78,20 @@ class Signal:\n     >> Signal(tag='x2')\n \n     \"\"\"\n-    def __init__(self, tag: str = \"\", state: Any = None, sensitivity: Any = None):\n+    def __init__(self, tag: str = \"\", state: Any = None, sensitivity: Any = None, min: Any = None, max: Any = None):\n         \"\"\"\n-\n-        :param tag: The name of the signal (string)\n-        :param state: The initialized state (optional)\n-        :param sensitivity: The initialized sensitivity (optional)\n+        Keyword Args:\n+            tag: The name of the signal\n+            state: The initialized state\n+            sensitivity: The initialized sensitivity\n+            min: Minimum allowed value\n+            max: Maximum allowed value\n         \"\"\"\n         self.tag = tag\n         self.state = state\n         self.sensitivity = sensitivity\n+        self.min = min\n+        self.max = max\n         self.keep_alloc = sensitivity is not None\n \n         # Save error string to location where it is initialized\n@@ -95,11 +101,12 @@ def _err_str(self):\n         return err_fmt(f\"Signal \\'{self.tag}\\', initialized in {self._init_loc}\")\n \n     def add_sensitivity(self, ds: Any):\n+        \"\"\" Add a new term to internal sensitivity \"\"\"\n         try:\n             if ds is None:\n                 return\n             if self.sensitivity is None:\n-                self.sensitivity = ds\n+                self.sensitivity = copy.deepcopy(ds)\n             else:\n                 self.sensitivity += ds\n             return self\n@@ -116,8 +123,12 @@ def add_sensitivity(self, ds: Any):\n     def reset(self, keep_alloc: bool = None):\n         \"\"\" Reset the sensitivities to zero or None\n         This must be called to clear internal memory of subsequent sensitivity calculations.\n-        :param keep_alloc: Keep the sensitivity allocation intact?\n-        :return: self\n+\n+        Args:\n+            keep_alloc: Keep the sensitivity allocation intact?\n+\n+        Returns:\n+            self\n         \"\"\"\n         if self.sensitivity is None:\n             return self\n@@ -138,11 +149,34 @@ def reset(self, keep_alloc: bool = None):\n \n     def __getitem__(self, item):\n         \"\"\" Obtain a sliced signal, for using its partial contents.\n-        :param item: Slice indices\n-        :return: Sliced signal (SignalSlice)\n+\n+        Args:\n+            item: Slice indices\n+\n+        Returns:\n+            Sliced signal (SignalSlice)\n         \"\"\"\n         return SignalSlice(self, item)\n \n+    def __str__(self):\n+        state_msg = f\"state {self.state}\" if self.state is not None else \"empty state\"\n+        state_msg = state_msg.split('\\n')\n+        if len(state_msg) > 1:\n+            state_msg = state_msg[0] + ' ... ' + state_msg[-1]\n+        else:\n+            state_msg = state_msg[0]\n+        return f\"Signal \\\"{self.tag}\\\" with {state_msg}\"\n+\n+    def __repr__(self):\n+        state_msg = f\"state {self.state}\" if self.state is not None else \"empty state\"\n+        state_msg = state_msg.split('\\n')\n+        if len(state_msg) > 1:\n+            state_msg = state_msg[0] + ' ... ' + state_msg[-1]\n+        else:\n+            state_msg = state_msg[0]\n+        sens_msg = 'empty sensitivity' if self.sensitivity is None else 'non-empty sensitivity'\n+        return f\"Signal \\\"{self.tag}\\\" with {state_msg} and {sens_msg} at {hex(id(self))}\"\n+\n \n class SignalSlice(Signal):\n     \"\"\" Slice operator for a Signal\n@@ -169,7 +203,8 @@ def state(self):\n             return None if self.orig_signal.state is None else self.orig_signal.state[self.slice]\n         except Exception as e:\n             # Possibilities: Unslicable object (TypeError) or Wrong dimensions or out of range (IndexError)\n-            raise type(e)(\"SignalSlice.state (getter)\" + self._err_str()) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised in SignalSlice.state (getter). Signal details:\" +\n+                      self._err_str()).with_traceback(sys.exc_info()[2])\n \n     @state.setter\n     def state(self, new_state):\n@@ -177,7 +212,8 @@ def state(self, new_state):\n             self.orig_signal.state[self.slice] = new_state\n         except Exception as e:\n             # Possibilities: Unslicable object (TypeError) or Wrong dimensions or out of range (IndexError)\n-            raise type(e)(\"SignalSlice.state (setter)\" + self._err_str()) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised in SignalSlice.state (setter). Signal details:\" +\n+                          self._err_str()).with_traceback(sys.exc_info()[2])\n \n     @property\n     def sensitivity(self):\n@@ -185,7 +221,8 @@ def sensitivity(self):\n             return None if self.orig_signal.sensitivity is None else self.orig_signal.sensitivity[self.slice]\n         except Exception as e:\n             # Possibilities: Unslicable object (TypeError) or Wrong dimensions or out of range (IndexError)\n-            raise type(e)(\"SignalSlice.sensitivity (getter)\" + self._err_str()) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised in SignalSlice.sensitivity (getter). Signal details:\" +\n+                          self._err_str()).with_traceback(sys.exc_info()[2])\n \n     @sensitivity.setter\n     def sensitivity(self, new_sens):\n@@ -207,7 +244,8 @@ def sensitivity(self, new_sens):\n             self.orig_signal.sensitivity[self.slice] = new_sens\n         except Exception as e:\n             # Possibilities: Unslicable object (TypeError) or Wrong dimensions or out of range (IndexError)\n-            raise type(e)(\"SignalSlice.sensitivity (setter)\" + self._err_str()) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised in SignalSlice.state (setter). Signal details:\" +\n+                          self._err_str()).with_traceback(sys.exc_info()[2])\n \n     def reset(self, keep_alloc: bool = None):\n         \"\"\" Reset the sensitivities to zero or None\n@@ -231,7 +269,7 @@ def make_signals(*args):\n     return ret\n \n \n-def _check_valid_signal(sig: Any):\n+def _is_valid_signal(sig: Any):\n     \"\"\" Checks if the argument is a valid Signal object\n     :param sig: The object to check\n     :return: True if it is a valid Signal\n@@ -240,10 +278,10 @@ def _check_valid_signal(sig: Any):\n         return True\n     if all([hasattr(sig, f) for f in [\"state\", \"sensitivity\", \"add_sensitivity\", \"reset\"]]):\n         return True\n-    raise TypeError(f\"Given argument with type \\'{type(sig).__name__}\\' is not a valid Signal\")\n+    return False\n \n \n-def _check_valid_module(mod: Any):\n+def _is_valid_module(mod: Any):\n     \"\"\" Checks if the argument is a valid Module object\n     :param mod: The object to check\n     :return: True if it is a valid Module\n@@ -252,7 +290,7 @@ def _check_valid_module(mod: Any):\n         return True\n     if hasattr(mod, \"response\") and hasattr(mod, \"sensitivity\") and hasattr(mod, \"reset\"):\n         return True\n-    raise TypeError(f\"Given argument with type \\'{type(mod).__name__}\\' is not a valid Module\")\n+    return False\n \n \n def _check_function_signature(fn, signals):\n@@ -370,60 +408,48 @@ class Module(ABC, RegisteredClass):\n     >> Module(sig_in=[inputs], sig_out=[outputs]\n     \"\"\"\n \n-    def _err_str(self, init: bool = True, add_signal: bool = True, fn=None):\n+    def _err_str(self, module_signature: bool = True, init: bool = True, fn=None):\n         str_list = []\n-        if init:\n-            str_list.append(f\"Module \\'{type(self).__name__}\\', initialized in {self._init_loc}\")\n-        if add_signal:\n+\n+        if module_signature:\n             inp_str = \"Inputs: \" + \", \".join([s.tag if hasattr(s, 'tag') else 'N/A' for s in self.sig_in]) if len(self.sig_in) > 0 else \"No inputs\"\n             out_str = \"Outputs: \" + \", \".join([s.tag if hasattr(s, 'tag') else 'N/A' for s in self.sig_out]) if len(self.sig_out) > 0 else \"No outputs\"\n-            str_list.append(inp_str + \" --> \" + out_str)\n+            str_list.append(f\"Module \\'{type(self).__name__}\\'( \" + inp_str + \" ) --> \" + out_str)\n+        if init:\n+            str_list.append(f\"Used in {self._init_loc}\")\n         if fn is not None:\n             name = f\"{fn.__self__.__class__.__name__}.{fn.__name__}{inspect.signature(fn)}\"\n             lineno = inspect.getsourcelines(fn)[1]\n             filename = inspect.getfile(fn)\n-            str_list.append(f\"Implemented in File \\\"{filename}\\\", line {lineno}, in {name}\")\n+            str_list.append(f\"Implementation in File \\\"{filename}\\\", line {lineno}, in {name}\")\n         return err_fmt(*str_list)\n \n     # flake8: noqa: C901\n     def __init__(self, sig_in: Union[Signal, List[Signal]] = None, sig_out: Union[Signal, List[Signal]] = None,\n                  *args, **kwargs):\n-        # TODO: Reduce complexity of this init\n         self._init_loc = get_init_str()\n \n         self.sig_in = _parse_to_list(sig_in)\n         self.sig_out = _parse_to_list(sig_out)\n         for i, s in enumerate(self.sig_in):\n-            try:\n-                _check_valid_signal(s)\n-            except Exception as e:\n-                earg0 = e.args[0] if len(e.args) > 0 else ''\n-                earg1 = e.args[1:] if len(e.args) > 1 else ()\n-                raise type(e)(f\"Invalid input signal #{i+1} - \" + str(earg0) + self._err_str(), *earg1) from None\n+            if not _is_valid_signal(s):\n+                tag = f\" (\\'{s.tag}\\')\" if hasattr(s, 'tag') else ''\n+                raise TypeError(f\"Input {i}{tag} is not a valid signal, type=\\'{type(s).__name__}\\'.\")\n \n         for i, s in enumerate(self.sig_out):\n-            try:\n-                _check_valid_signal(s)\n-            except Exception as e:\n-                earg0 = e.args[0] if len(e.args) > 0 else ''\n-                earg1 = e.args[1:] if len(e.args) > 1 else ()\n-                raise type(e)(f\"Invalid output signal #{i+1} - \" + str(earg0) + self._err_str(), *earg1) from None\n+            if not _is_valid_signal(s):\n+                tag = f\" (\\'{s.tag}\\')\" if hasattr(s, 'tag') else ''\n+                raise TypeError(f\"Output {i}{tag} is not a valid signal, type=\\'{type(s).__name__}\\'.\")\n \n-        try:\n-            # Call preparation of submodule with remaining arguments\n-            self._prepare(*args, **kwargs)\n-        except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"_prepare() - \" + str(earg0) + self._err_str(fn=self._prepare), *earg1) from e\n+        # Call preparation of submodule with remaining arguments\n+        self._prepare(*args, **kwargs)\n \n         try:\n             # Check if the signals match _response() signature\n             _check_function_signature(self._response, self.sig_in)\n         except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(str(earg0) + self._err_str(fn=self._response), *earg1) from None\n+            raise type(e)(str(e) + \"\\n\\t| Module details:\" +\n+                          self._err_str(fn=self._response)).with_traceback(sys.exc_info()[2])\n \n         try:\n             # If no output signals are given, but are required, try to initialize them here\n@@ -441,9 +467,8 @@ def __init__(self, sig_in: Union[Signal, List[Signal]] = None, sig_out: Union[Si\n             # Check if signals match _sensitivity() signature\n             _check_function_signature(self._sensitivity, self.sig_out)\n         except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(str(earg0) + self._err_str(fn=self._sensitivity), *earg1) from None\n+            raise type(e)(str(e) + \"\\n\\t| Module details:\" +\n+                          self._err_str(fn=self._sensitivity)).with_traceback(sys.exc_info()[2])\n \n     def response(self):\n         \"\"\" Calculate the response from sig_in and output this to sig_out \"\"\"\n@@ -461,9 +486,9 @@ def response(self):\n                 self.sig_out[i].state = val\n             return self\n         except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"response() - \" + str(earg0) + self._err_str(fn=self._response), *earg1) from e\n+            # https://stackoverflow.com/questions/6062576/adding-information-to-an-exception\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised when calling response(). Module details:\" +\n+                          self._err_str(fn=self._response)).with_traceback(sys.exc_info()[2])\n \n     def __call__(self):\n         return self.response()\n@@ -494,9 +519,8 @@ def sensitivity(self):\n \n             return self\n         except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"sensitivity() - \" + str(earg0) + self._err_str(fn=self._sensitivity), *earg1) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised when calling sensitivity(). Module details:\" +\n+                          self._err_str(fn=self._sensitivity)).with_traceback(sys.exc_info()[2])\n \n     def reset(self):\n         \"\"\" Reset the state of the sensitivities (they are set to zero or to None) \"\"\"\n@@ -506,9 +530,8 @@ def reset(self):\n             self._reset()\n             return self\n         except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"reset() - \" + str(earg0) + self._err_str(fn=self._reset), *earg1) from e\n+            raise type(e)(str(e) + \"\\n\\t| Above error was raised when calling reset(). Module details:\" +\n+                          self._err_str(fn=self._response)).with_traceback(sys.exc_info()[2])\n \n     # METHODS TO BE DEFINED BY USER\n     def _prepare(self, *args, **kwargs):\n@@ -542,35 +565,33 @@ class Network(Module):\n     \"\"\"\n     def __init__(self, *args, print_timing=False):\n         self._init_loc = get_init_str()\n-        try:\n-            # Obtain the internal blocks\n-            self.mods = _parse_to_list(*args)\n-\n-            # Check if the blocks are initialized, else create them\n-            for i, b in enumerate(self.mods):\n-                if isinstance(b, dict):\n-                    exclude_keys = ['type']\n-                    b_ex = {k: b[k] for k in set(list(b.keys())) - set(exclude_keys)}\n-                    self.mods[i] = Module.create(b['type'], **b_ex)\n-\n-            # Check validity of modules\n-            [_check_valid_module(m) for m in self.mods]\n-\n-            # Gather all the input and output signals of the internal blocks\n-            all_in = set()\n-            all_out = set()\n-            [all_in.update(b.sig_in) for b in self.mods]\n-            [all_out.update(b.sig_out) for b in self.mods]\n-            in_unique = all_in - all_out\n-\n-            # Initialize the parent module, with correct inputs and outputs\n-            super().__init__(list(in_unique), list(all_out))\n-\n-            self.print_timing = print_timing\n-        except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(str(earg0) + self._err_str(add_signal=False), *earg1) from None\n+\n+        # Obtain the internal blocks\n+        self.mods = _parse_to_list(*args)\n+\n+        # Check if the blocks are initialized, else create them\n+        for i, b in enumerate(self.mods):\n+            if isinstance(b, dict):\n+                exclude_keys = ['type']\n+                b_ex = {k: b[k] for k in set(list(b.keys())) - set(exclude_keys)}\n+                self.mods[i] = Module.create(b['type'], **b_ex)\n+\n+        # Check validity of modules\n+        for m in self.mods:\n+            if not _is_valid_module(m):\n+                raise TypeError(f\"Argument is not a valid Module, type=\\'{type(mod).__name__}\\'.\")\n+\n+        # Gather all the input and output signals of the internal blocks\n+        all_in = set()\n+        all_out = set()\n+        [all_in.update(b.sig_in) for b in self.mods]\n+        [all_out.update(b.sig_out) for b in self.mods]\n+        in_unique = all_in - all_out\n+\n+        # Initialize the parent module, with correct inputs and outputs\n+        super().__init__(list(in_unique), list(all_out))\n+\n+        self.print_timing = print_timing\n \n     def timefn(self, fn):\n         start_t = time.time()\n@@ -611,12 +632,9 @@ def append(self, *newmods):\n         modlist = _parse_to_list(*newmods)\n \n         # Check if the blocks are initialized, else create them\n-        for i, b in enumerate(modlist):\n-            try:  # Check validity of modules\n-                _check_valid_module(b)\n-            except Exception as e:\n-                raise type(e)(\"append() - Trying to append invalid module \" + str(e.args[0])\n-                              + self._err_str(add_signal=False), *e.args[1:]) from None\n+        for i, m in enumerate(modlist):\n+            if not _is_valid_module(m):\n+                raise TypeError(f\"Argument #{i} is not a valid module, type=\\'{type(mod).__name__}\\'.\")\n \n         # Obtain the internal blocks\n         self.mods.extend(modlist)\n@@ -624,25 +642,11 @@ def append(self, *newmods):\n         # Gather all the input and output signals of the internal blocks\n         all_in = set()\n         all_out = set()\n-        [all_in.update(b.sig_in) for b in self.mods]\n-        [all_out.update(b.sig_out) for b in self.mods]\n+        [all_in.update(m.sig_in) for m in self.mods]\n+        [all_out.update(m.sig_out) for m in self.mods]\n         in_unique = all_in - all_out\n \n         self.sig_in = _parse_to_list(in_unique)\n-        try:\n-            [_check_valid_signal(s) for s in self.sig_in]\n-        except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"append() - Invalid input signals \" + str(earg0)\n-                          + self._err_str(add_signal=False), *earg1) from None\n         self.sig_out = _parse_to_list(all_out)\n-        try:\n-            [_check_valid_signal(s) for s in self.sig_out]\n-        except Exception as e:\n-            earg0 = e.args[0] if len(e.args) > 0 else ''\n-            earg1 = e.args[1:] if len(e.args) > 1 else ()\n-            raise type(e)(\"append() - Invalid output signals \" + str(earg0)\n-                          + self._err_str(add_signal=False), *earg1) from None\n \n         return modlist[-1].sig_out[0] if len(modlist[-1].sig_out) == 1 else modlist[-1].sig_out  # Returns the output signal\ndiff --git a/pymoto/modules/aggregation.py b/pymoto/modules/aggregation.py\nnew file mode 100644\nindex 0000000..e7fb367\n--- /dev/null\n+++ b/pymoto/modules/aggregation.py\n@@ -0,0 +1,207 @@\n+import warnings\n+import abc\n+import numpy as np\n+import scipy.special as spsp\n+from pymoto import Module\n+\n+\n+class AggActiveSet:\n+    \"\"\" Determine active set by discarding lower or upper fraction of a set of values\n+\n+    Args:\n+       lower_rel: Fraction of values closest to minimum to discard (based on value)\n+       upper_rel: Fraction of values closest to maximum to discard (based on value)\n+       lower_amt: Fraction of lowest values to discard (based on sorting)\n+       upper_amt: Fraction of highest values to discard (based on sorting)\n+    \"\"\"\n+    def __init__(self, lower_rel=0.0, upper_rel=1.0, lower_amt=0.0, upper_amt=1.0):\n+        assert upper_rel > lower_rel, \"Upper must be larger than lower to keep values in the set\"\n+        assert upper_amt > lower_amt, \"Upper must be larger than lower to keep values in the set\"\n+        self.lower_rel, self.upper_rel = lower_rel, upper_rel\n+        self.lower_amt, self.upper_amt = lower_amt, upper_amt\n+\n+    def __call__(self, x):\n+        \"\"\" Generate an active set for given array \"\"\"\n+        xmin, xmax = np.min(x), np.max(x)\n+        if (xmax - xmin) == 0:  # All values are the same, so no active set can be taken\n+            return Ellipsis\n+\n+        sel = np.ones_like(x, dtype=bool)\n+\n+        # Select based on value\n+        xrel = (x - xmin) / (xmax - xmin)  # Normalize between 0 and 1\n+        if self.lower_rel > 0:\n+            sel = np.logical_and(sel, xrel >= self.lower_rel)\n+        if self.upper_rel < 1:\n+            sel = np.logical_and(sel, xrel <= self.upper_rel)\n+\n+        # Remove lowest and highest N values\n+        i_sort = np.argsort(x)\n+        if self.lower_amt > 0:\n+            n_lower_amt = int(x.size * self.lower_amt)\n+            sel[i_sort[:n_lower_amt]] = False\n+\n+        if self.upper_amt < 1:\n+            n_upper_amt = int(x.size * (1 - self.upper_amt))\n+            sel[i_sort[-n_upper_amt:]] = False\n+\n+        return sel\n+\n+\n+class AggScaling:\n+    \"\"\" Scaling strategy to absolute minimum or maximum\n+\n+    Args:\n+        which: Scale to `min` or `max`\n+    \"\"\"\n+    def __init__(self, which: str, damping=0.0):\n+        self.damping = damping\n+        if which.lower() == 'min':\n+            self.f = np.min\n+        elif which.lower() == 'max':\n+            self.f = np.max\n+        else:\n+            raise ValueError(\"Argument `which` can only be 'min' or 'max'\")\n+        self.sf = None\n+\n+    def __call__(self, x, fx_approx):\n+        \"\"\" Determine scaling factor\n+\n+        Args:\n+            x: Set of values\n+            fx_approx: Approximated minimum / maximum\n+\n+        Returns:\n+            Scaling factor\n+        \"\"\"\n+        trueval = self.f(x)\n+        scale = trueval / fx_approx\n+        if self.sf is None:\n+            self.sf = scale\n+        else:\n+            self.sf = self.damping * self.sf + (1 - self.damping) * scale\n+        return self.sf\n+\n+\n+class Aggregation(Module):\n+    \"\"\" Generic Aggregation module (cannot be used directly, but can only be used as superclass)\n+\n+    Keyword Args:\n+        scaling: Scaling strategy to improve approximation\n+        active_set: Active set strategy to improve approximation\n+    \"\"\"\n+    def _prepare(self, scaling: AggScaling = None, active_set: AggActiveSet = None):\n+        # This prepare function MUST be called in the _prepare function of sub-classes\n+        self.scaling = scaling\n+        self.active_set = active_set\n+        self.sf = 1.0\n+\n+    @abc.abstractmethod\n+    def aggregation_function(self, x):\n+        \"\"\" Calculates f(x) \"\"\"\n+        raise NotImplementedError()\n+\n+    @abc.abstractmethod\n+    def aggregation_derivative(self, x):\n+        \"\"\"\" Calculates df(x) / dx \"\"\"\n+        raise NotImplementedError()\n+\n+    def _response(self, x):\n+        # Determine active set\n+        if self.active_set is not None:\n+            self.select = self.active_set(x)\n+        else:\n+            self.select = Ellipsis\n+\n+        # Get aggregated value\n+        xagg = self.aggregation_function(x[self.select])\n+\n+        # Scale\n+        if self.scaling is not None:\n+            self.sf = self.scaling(x[self.select], xagg)\n+        return self.sf * xagg\n+\n+    def _sensitivity(self, dfdy):\n+        x = self.sig_in[0].state\n+        dydx = self.aggregation_derivative(x[self.select])\n+        dx = np.zeros_like(x)\n+        dx[self.select] += self.sf * dfdy * dydx\n+        return dx\n+\n+\n+class PNorm(Aggregation):\n+    r\"\"\" P-norm aggregration\n+\n+    :math:`S_p(x_1, x_2, \\dotsc, x_n) = \\left( \\sum_i (|x_i|^p) \\right)^{1/p}\n+\n+    Only valid for positive :math:`x_i` when approximating the minimum or maximum\n+\n+    Args:\n+        p: Power of the p-norm. Approximate maximum for `p>0` and minimum for `p<0`\n+        scaling: Scaling strategy to improve approximation\n+        active_set: Active set strategy to improve approximation\n+    \"\"\"\n+    def _prepare(self, p=2, scaling: AggScaling = None, active_set: AggActiveSet = None):\n+        self.p = p\n+        self.y = None\n+        super()._prepare(scaling, active_set)\n+\n+    def aggregation_function(self, x):\n+        if np.min(x) < 0:\n+            warnings.warn(\"PNorm is only valid for positive x\")\n+\n+        # Get p-norm\n+        return np.sum(np.abs(x) ** self.p) ** (1/self.p)\n+\n+    def aggregation_derivative(self, x):\n+        pval = np.sum(np.abs(x) ** self.p) ** (1 / self.p - 1)\n+        return pval * np.sign(x) * np.abs(x)**(self.p - 1)\n+\n+\n+class SoftMinMax(Aggregation):\n+    r\"\"\" Soft maximum/minimum function\n+\n+    :math:`S_a(x_1, x_2, \\dotsc, x_n) = \\frac{\\sum_i (x_i \\exp(a x_i))}{\\sum_i (\\exp(a x_i))}`\n+\n+    When using as maximum, it underestimates the maximum\n+    It is exact however when :math:`x_1=x_2=\\dotsc=x_n`\n+\n+    Args:\n+        alpha: Scaling factor of the soft function. Approximate maximum for `alpha>0` and minimum for `alpha<0`\n+        scaling: Scaling strategy to improve approximation\n+        active_set: Active set strategy to improve approximation\n+    \"\"\"\n+    def _prepare(self, alpha=1.0, scaling: AggScaling = None, active_set: AggActiveSet = None):\n+        self.alpha = alpha\n+        self.y = None\n+        super()._prepare(scaling, active_set)\n+\n+    def aggregation_function(self, x):\n+        self.y = np.sum(x * spsp.softmax(self.alpha * x))\n+        return self.y\n+\n+    def aggregation_derivative(self, x):\n+        return spsp.softmax(self.alpha * x) * (1 + self.alpha * (x - self.y))\n+\n+\n+class KSFunction(Aggregation):\n+    r\"\"\" Kreisselmeier and Steinhauser function from 1979\n+\n+    :math:`S_\\rho(x_1, x_2, \\dotsc, x_n) = \\frac{1}{\\rho} \\ln \\left( \\sum_i \\exp(\\rho x_i) \\right)`\n+\n+    Args:\n+        rho: Scaling factor of the KS function. Approximate maximum for `alpha>0` and minimum for `alpha<0`\n+        scaling: Scaling strategy to improve approximation\n+        active_set: Active set strategy to improve approximation\n+    \"\"\"\n+    def _prepare(self, rho=1.0, scaling: AggScaling = None, active_set: AggActiveSet = None):\n+        self.rho = rho\n+        self.y = None\n+        super()._prepare(scaling, active_set)\n+\n+    def aggregation_function(self, x):\n+        return 1/self.rho * np.log(np.sum(np.exp(self.rho * x)))\n+\n+    def aggregation_derivative(self, x):\n+        erx = np.exp(self.rho * x)\n+        return erx / np.sum(erx)\ndiff --git a/pymoto/modules/assembly.py b/pymoto/modules/assembly.py\nindex af05006..bf7d936 100644\n--- a/pymoto/modules/assembly.py\n+++ b/pymoto/modules/assembly.py\n@@ -49,8 +49,8 @@ def _prepare(self, domain: DomainDefinition, element_matrix: np.ndarray, bc=None\n         self.dofconn = domain.get_dofconnectivity(self.ndof)\n \n         # Row and column indices for the matrix\n-        self.rows = np.kron(self.dofconn, np.ones((domain.elemnodes*self.ndof, 1), dtype=int)).flatten()\n-        self.cols = np.kron(self.dofconn, np.ones((1, domain.elemnodes*self.ndof), dtype=int)).flatten()\n+        self.rows = np.kron(self.dofconn, np.ones((1, domain.elemnodes*self.ndof), dtype=int)).flatten()\n+        self.cols = np.kron(self.dofconn, np.ones((domain.elemnodes * self.ndof, 1), dtype=int)).flatten()\n         self.matrix_type = matrix_type\n \n         # Boundary conditions\n@@ -68,6 +68,8 @@ def _prepare(self, domain: DomainDefinition, element_matrix: np.ndarray, bc=None\n         self.add_constant = add_constant\n \n     def _response(self, xscale: np.ndarray):\n+        nel = self.dofconn.shape[0]\n+        assert xscale.size == nel, f\"Input vector wrong size ({xscale.size}), must be of size #nel ({nel})\"\n         scaled_el = ((self.elmat.flatten()[np.newaxis]).T * xscale).flatten(order='F')\n \n         # Set boundary conditions\n@@ -305,3 +307,116 @@ def _prepare(self, domain: DomainDefinition, *args, material_property: float = 1\n             self.poisson_element += w * self.material_property * Bn.T @ Bn  # Add contribution\n \n         super()._prepare(domain, self.poisson_element, *args, **kwargs)\n+\n+\n+class ElementOperation(Module):\n+    r\"\"\" Generic module for element-wise operations based on nodal information\n+\n+    :math:`y_e = \\mathbf{B} \\mathbf{u}_e`\n+\n+    Input Signal:\n+        - ``u``: Nodal vector of size ``(n_dof_per_element * #nodes)``\n+\n+    Output Signal:\n+        - ``y``: Elemental output data of size ``(..., #elements)``\n+\n+    Args:\n+        domain: The domain defining element and nodal connectivity\n+        element_matrix: The element operator matrix :math:`\\mathbf{B}` of size ``(..., n_dof_per_element)``\n+    \"\"\"\n+    def _prepare(self, domain: DomainDefinition, element_matrix: np.ndarray):\n+        if element_matrix.shape[-1] % domain.elemnodes != 0:\n+            raise IndexError(\"Size of last dimension of element operator matrix is not compatible with mesh. \"\n+                             \"Must be dividable by the number of nodes.\")\n+\n+        ndof = element_matrix.shape[-1] // domain.elemnodes\n+\n+        self.element_matrix = element_matrix\n+        self.dofconn = domain.get_dofconnectivity(ndof)\n+        self.usiz = ndof * domain.nnodes\n+\n+    def _response(self, u):\n+        assert u.size == self.usiz\n+        return einsum('...k, lk -> ...l', self.element_matrix, u[self.dofconn], optimize=True)\n+\n+    def _sensitivity(self, dy):\n+        du_el = einsum('...k, ...l -> lk', self.element_matrix, dy, optimize=True)\n+        du = np.zeros_like(self.sig_in[0].state)\n+        np.add.at(du, self.dofconn, du_el)\n+        return du\n+\n+\n+class Strain(ElementOperation):\n+    r\"\"\" Evaluate average mechanical strains in solid elements based on deformation\n+\n+    The strains are returned in Voigt notation.\n+    :math:`\\mathbf{\\epsilon}_e = \\mathbf{B} \\mathbf{u}_e`\n+\n+    Each integration point in the element has different strain values. Here, the average is returned.\n+\n+    The returned strain is either\n+    :math:`\\mathbf{\\epsilon} = \\begin{bmatrix}\\epsilon_{xx} & \\epsilon_{yy} & \\epsilon_{xy} \\end{bmatrix}`\n+    in case ``voigt = False`` or\n+    :math:`\\mathbf{\\epsilon} = \\begin{bmatrix}\\epsilon_{xx} & \\epsilon_{yy} & \\gamma_{xy} \\end{bmatrix}`\n+    in case ``voigt = True``, for which :math:`\\gamma_{xy}=2\\epsilon_{xy}`.\n+\n+    Input Signal:\n+       - ``u``: Nodal vector of size ``(#dof per element * #nodes)``\n+\n+    Output Signal:\n+       - ``e``: Strain matrix of size ``(#strains per element, #elements)``\n+\n+    Args:\n+        domain: The domain defining element and nodal connectivity\n+\n+    Keyword Args:\n+        voigt: Use Voigt strain notation (2x off-diagonal strain contribution)\n+    \"\"\"\n+    def _prepare(self, domain: DomainDefinition, voigt: bool = True):\n+        # Numerical integration\n+        B = None\n+        siz = domain.element_size\n+        w = 1/domain.elemnodes  # Average strain at the integration points\n+        for n in domain.node_numbering:\n+            pos = n * (siz / 2) / np.sqrt(3)  # Sampling point\n+            dN_dx = domain.eval_shape_fun_der(pos)\n+            B_add = w*get_B(dN_dx)  # Add contribution\n+            if B is None:\n+                B = B_add\n+            else:\n+                B += B_add\n+\n+        if voigt:\n+            idx_shear = np.count_nonzero(B, axis=1) == 2*domain.elemnodes  # Shear is combination of two displacements\n+            B[idx_shear, :] *= 2  # Voigt notation with engineering strain\n+\n+        super()._prepare(domain, B)\n+\n+\n+class Stress(Strain):\n+    \"\"\" Calculate the average stresses per element\n+\n+    Input Signal:\n+       - ``u``: Nodal vector of size ``(#dof per element * #nodes)``\n+\n+    Output Signal:\n+       - ``s``: Stress matrix of size ``(#stresses per element, #elements)``\n+\n+    Args:\n+        domain: The domain defining element and nodal connectivity\n+\n+    Keyword Args:\n+        e_modulus: Young's modulus\n+        poisson_ratio: Poisson ratio\n+        plane: Plane 'strain' or 'stress'\n+    \"\"\"\n+    def _prepare(self, domain: DomainDefinition,\n+                 e_modulus: float = 1.0, poisson_ratio: float = 0.3, plane: str = 'strain'):\n+\n+        super()._prepare(domain, voigt=True)\n+\n+        # Get material relation\n+        D = get_D(e_modulus, poisson_ratio, '3d' if domain.dim == 3 else plane.lower())\n+        if domain.dim == 2:\n+            D *= domain.element_size[2]\n+        self.element_matrix = D @ self.element_matrix\ndiff --git a/pymoto/modules/complex.py b/pymoto/modules/complex.py\nindex 4224daf..abfbfef 100644\n--- a/pymoto/modules/complex.py\n+++ b/pymoto/modules/complex.py\n@@ -73,7 +73,7 @@ def _sensitivity(self, dz):\n \n \n class RealPart(Module):\n-    \"\"\" Takes the real part of a complex value :math:`x = \\\\text{Re}(z)`\n+    r\"\"\" Takes the real part of a complex value :math:`x = \\text{Re}(z)`\n \n     Input Signal:\n         - ``z``: Complex value\n@@ -89,7 +89,7 @@ def _sensitivity(self, dx):\n \n \n class ImagPart(Module):\n-    \"\"\" Takes the imaginary part of a complex value :math:`y = \\\\text{Im}(z)`\n+    r\"\"\" Takes the imaginary part of a complex value :math:`y = \\text{Im}(z)`\n \n     Input Signal:\n         - ``z``: Complex value\n@@ -105,7 +105,7 @@ def _sensitivity(self, dy):\n \n \n class ComplexNorm(Module):\n-    \"\"\" Takes the complex norm :math:`A = z z^*`\n+    r\"\"\" Takes the complex norm :math:`A = \\sqrt(z z^*)`\n \n     Input Signal:\n         - ``z``: Complex value\ndiff --git a/pymoto/modules/filter.py b/pymoto/modules/filter.py\nindex b3a7a44..fc54f03 100644\n--- a/pymoto/modules/filter.py\n+++ b/pymoto/modules/filter.py\n@@ -1,46 +1,247 @@\n-from pymoto import Module\n-from .assembly import DomainDefinition\n+from pymoto import Module, DomainDefinition\n import numpy as np\n from scipy.sparse import coo_matrix\n-from scipy.ndimage import convolve\n+from scipy.signal import convolve, correlate\n+from numbers import Number\n \n \n class FilterConv(Module):\n-    def _prepare(self, domain: DomainDefinition, weights: np.ndarray, mode: str = 'reflect'):\n+    def _prepare(self, domain: DomainDefinition, radius: float = None, relative_units: bool = True, weights: np.ndarray = None,\n+                 xmin_bc='symmetric', xmax_bc='symmetric',\n+                 ymin_bc='symmetric', ymax_bc='symmetric',\n+                 zmin_bc='symmetric', zmax_bc='symmetric'):\n         self.domain = domain\n-        self.weights = weights\n-        self.mode = mode\n-\n-    def set_filter_radius(self, radius: float, relative_units: bool = False):\n+        self.weights = None\n+        if (weights is None and radius is None) or (weights is not None and radius is not None):\n+            raise ValueError(\"Only one of arguments 'filter_radius' or 'weights' must be provided.\")\n+        elif weights is not None:\n+            self.weights = weights.copy()\n+            while self.weights.ndim < 3:\n+                self.weights = np.expand_dims(self.weights, axis=-1)\n+            for i in range(self.weights.ndim):\n+                assert self.weights.shape[i] % 2 == 1, \"Size of weights must be uneven\"\n+        elif radius is not None:\n+            self.set_filter_radius(radius, relative_units)\n+\n+        # Process padding\n+        self.overrides = []\n+        self.pad_sizes = [v//2 for v in self.weights.shape]\n+\n+        domain_sizes = [self.domain.nelx, self.domain.nely, self.domain.nelz]\n+        el_x, el_y, el_z = np.meshgrid(*[np.arange(max(1, s)) for s in domain_sizes], indexing='ij')\n+        self.el3d_orig = self.domain.get_elemnumber(el_x, el_y, el_z)\n+\n+        padx = self._process_padding(self.el3d_orig, xmin_bc, xmax_bc, 0, self.pad_sizes[0])\n+        pady = self._process_padding(padx, ymin_bc, ymax_bc, 1, self.pad_sizes[1])\n+        self.el3d_pad = self._process_padding(pady, zmin_bc, zmax_bc, 2, self.pad_sizes[2])\n+        # # Process padding of the domain\n+        # if left == 'wrap' and right == 'wrap':\n+        #     pad1a = np.pad(self.el3d_orig, [(self.pad_sizes[0], self.pad_sizes[0]), (0, 0), (0, 0)], mode='wrap')\n+        # elif left == 'wrap':\n+        #     pad1a = np.pad(self.el3d_orig, [(self.pad_sizes[0], 0), (0, 0), (0, 0)], mode='wrap')\n+        # elif right == 'wrap':\n+        #     pad1a = np.pad(self.el3d_orig, [(0, self.pad_sizes[0]), (0, 0), (0, 0)], mode='wrap')\n+        # else:\n+        #     pad1a = self.el3d_orig\n+        #\n+        # if right == 'edge':\n+        #     pad1b = np.pad(pad1a, [(0, self.pad_sizes[0]), (0, 0), (0, 0)], mode='edge')\n+        # elif right == 'symmetric':\n+        #     pad1b = np.pad(pad1a, [(0, self.pad_sizes[0]), (0, 0), (0, 0)], mode='symmetric')\n+        # elif isinstance(right, Number):\n+        #     pad1b = np.pad(pad1a, [(0, self.pad_sizes[0]), (0, 0), (0, 0)], mode='constant', constant_values=0)\n+        #     xrange = self.pad_sizes[0] + domain.nelx + np.arange(max(1, self.pad_sizes[0]))\n+        #     yrange = np.arange(max(1, domain.nely + 2*self.pad_sizes[1]))\n+        #     zrange = np.arange(max(1, domain.nelz + 2*self.pad_sizes[2]))\n+        #     el_x, el_y, el_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n+        #     self.override_padded_values((el_x, el_y, el_z), right)\n+        # else:\n+        #     pad1b = pad1a\n+        #\n+        # if left == 'edge':\n+        #     pad1 = np.pad(pad1b, [(self.pad_sizes[0], 0), (0, 0), (0, 0)], mode='edge')\n+        # elif left == 'symmetric':\n+        #     pad1 = np.pad(pad1b, [(self.pad_sizes[0], 0), (0, 0), (0, 0)], mode='symmetric')\n+        # elif isinstance(left, Number):\n+        #     pad1 = np.pad(pad1b, [(self.pad_sizes[0], 0), (0, 0), (0, 0)], mode='constant', constant_values=0)\n+        #     xrange = np.arange(max(1, self.pad_sizes[0]))\n+        #     yrange = np.arange(max(1, domain.nely + 2 * self.pad_sizes[1]))\n+        #     zrange = np.arange(max(1, domain.nelz + 2 * self.pad_sizes[2]))\n+        #     el_x, el_y, el_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n+        #     self.override_padded_values((el_x, el_y, el_z), left)\n+        # else:\n+        #     pad1 = pad1b\n+        #\n+        # # Y-direction\n+        # if bottom == 'wrap' and top == 'wrap':\n+        #     pad2a = np.pad(pad1, [(0, 0), (self.pad_sizes[1], self.pad_sizes[1]), (0, 0)], mode='wrap')\n+        # elif bottom == 'wrap':\n+        #     pad2a = np.pad(pad1, [(0, 0), (self.pad_sizes[1], 0), (0, 0)], mode='wrap')\n+        # elif top == 'wrap':\n+        #     pad2a = np.pad(pad1, [(0, 0), (0, self.pad_sizes[1]), (0, 0)], mode='wrap')\n+        # else:\n+        #     pad2a = pad1\n+        #\n+        # if top == 'edge':\n+        #     pad2b = np.pad(pad2a, [(0, 0), (0, self.pad_sizes[1]), (0, 0)], mode='edge')\n+        # elif top == 'symmetric':\n+        #     pad2b = np.pad(pad2a, [(0, 0), (0, self.pad_sizes[1]), (0, 0)], mode='symmetric')\n+        # elif isinstance(top, Number):\n+        #     pad2b = np.pad(pad2a, [(0, 0), (0, self.pad_sizes[1]), (0, 0)], mode='constant', constant_values=0)\n+        #     xrange = np.arange(max(1, domain.nelx + 2*self.pad_sizes[0]))\n+        #     yrange = self.pad_sizes[1] + domain.nely + np.arange(max(1, self.pad_sizes[1]))\n+        #     zrange = np.arange(max(1, domain.nelz + 2*self.pad_sizes[2]))\n+        #     el_x, el_y, el_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n+        #     self.override_padded_values((el_x, el_y, el_z), top)\n+        # else:\n+        #     pad2b = pad2a\n+        #\n+        # if bottom == 'edge':\n+        #     pad2 = np.pad(pad2b, [(0, 0), (self.pad_sizes[1], 0), (0, 0)], mode='edge')\n+        # elif bottom == 'symmetric':\n+        #     pad2 = np.pad(pad2b, [(0, 0), (self.pad_sizes[1], 0), (0, 0)], mode='symmetric')\n+        # elif isinstance(bottom, Number):\n+        #     pad2 = np.pad(pad2b, [(0, 0), (self.pad_sizes[1], 0), (0, 0)], mode='constant', constant_values=0)\n+        #     xrange = np.arange(max(1, domain.nelx + 2 * self.pad_sizes[0]))\n+        #     yrange = np.arange(max(1, self.pad_sizes[1]))\n+        #     zrange = np.arange(max(1, domain.nelz + 2 * self.pad_sizes[2]))\n+        #     el_x, el_y, el_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n+        #     self.override_padded_values((el_x, el_y, el_z), bottom)\n+        # else:\n+        #     pad2 = pad2b\n+\n+\n+        # if isinstance(left, Number):\n+            # mode = 'constant'\n+            # mode = 'edge'\n+            # mode = 'symmetric'\n+            # mode = 'wrap'\n+\n+\n+        # Do wrap first\n+\n+        # np.pad(..., **bottom)\n+        # self.el3d_pad = pad2  #np.pad(self.el3d_orig, [(d, d) for d in pad_sizes], mode='reflect')\n+\n+    def _process_padding(self, indices, type_edge0, type_edge1, direction: int, pad_size: int):\n+        # First process wrapped padding\n+        wrap_size = (0, 0)\n+        do_wrap = False\n+        if type_edge0 == 'wrap':\n+            do_wrap = True\n+            wrap_size = (pad_size, wrap_size[1])\n+        if type_edge1 == 'wrap':\n+            do_wrap = True\n+            wrap_size = (wrap_size[0], pad_size)\n+\n+        if do_wrap:\n+            pad_width = [(0, 0) for _ in range(indices.ndim)]\n+            pad_width[direction] = wrap_size\n+            pad1a = np.pad(indices, pad_width, mode='wrap')\n+        else:\n+            pad1a = indices\n+\n+        domain_sizes = [self.domain.nelx, self.domain.nely, self.domain.nelz]\n+        padded_sizes = [self.domain.nelx + 2 * self.pad_sizes[0],\n+                        self.domain.nely + 2 * self.pad_sizes[1],\n+                        self.domain.nelz + 2 * self.pad_sizes[2]]\n+\n+        # Process edge 1\n+        pad_width = [(0, 0) for _ in range(indices.ndim)]\n+        pad_width[direction] = (0, pad_size)\n+        if type_edge1 == 'edge':\n+            pad1b = np.pad(pad1a, pad_width, mode='edge')\n+        elif type_edge1 == 'symmetric':\n+            pad1b = np.pad(pad1a, pad_width, mode='symmetric')\n+        elif isinstance(type_edge1, Number):  # Constant\n+            value = type_edge1\n+            pad1b = np.pad(pad1a, pad_width, mode='constant', constant_values=0)\n+\n+            n_range = [np.arange(max(1, s)) for s in padded_sizes]\n+            n_range[direction] = pad_size + domain_sizes[direction] + np.arange(pad_size)\n+\n+            el_nos = np.meshgrid(*n_range, indexing='ij')\n+            self.override_padded_values(tuple(el_nos), value)\n+        else:\n+            pad1b = pad1a\n+\n+        # Process edge 0\n+        pad_width = [(0, 0) for _ in range(indices.ndim)]\n+        pad_width[direction] = (pad_size, 0)\n+        if type_edge0 == 'edge':\n+            pad1 = np.pad(pad1b, pad_width, mode='edge')\n+        elif type_edge0 == 'symmetric':\n+            pad1 = np.pad(pad1b, pad_width, mode='symmetric')\n+        elif isinstance(type_edge0, Number):\n+            value = type_edge0\n+            pad1 = np.pad(pad1b, pad_width, mode='constant', constant_values=0)\n+\n+            n_range = [np.arange(max(1, s)) for s in padded_sizes]\n+            n_range[direction] = np.arange(pad_size)\n+\n+            el_nos = np.meshgrid(*n_range, indexing='ij')\n+            self.override_padded_values(tuple(el_nos), value)\n+        else:\n+            pad1 = pad1b\n+        return pad1\n+\n+    @property\n+    def padded_domain(self):\n+        domain_sizes = [self.domain.nelx, self.domain.nely, self.domain.nelz]\n+        nx, ny, nz = [n + 2*p for n, p in zip(domain_sizes, self.pad_sizes)]\n+        lx, ly, lz = self.domain.element_size\n+        return DomainDefinition(nx, ny, nz, unitx=lx, unity=ly, unitz=lz)\n+\n+    def override_padded_values(self, index, value):\n+        if all([np.asarray(i).size == 0 for i in index]):\n+            # Don't add empty sets\n+            return\n+        self.overrides.append((index, value))\n+\n+    def override_values(self, index, value):\n+        # Change index to extended domain\n+        xrange = self.pad_sizes[0] + np.arange(max(1, self.domain.nelx))\n+        yrange = self.pad_sizes[1] + np.arange(max(1, self.domain.nely))\n+        zrange = self.pad_sizes[2] + np.arange(max(1, self.domain.nelz))\n+        el_x, el_y, el_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n+        self.overrides.append(((el_x[index], el_y[index], el_z[index]), value))\n+\n+    def get_padded_vector(self, x):\n+        xpad = x[self.el3d_pad]\n+        for index, value in self.overrides:\n+            xpad[index] = value\n+        return xpad\n+\n+    def set_filter_radius(self, radius: float, relative_units: bool = True):\n         if relative_units:\n             dx, dy, dz = 1.0, 1.0, 1.0\n         else:\n             dx, dy, dz = self.domain.element_size\n-        delemx, delemy, delemz = int((radius-1e-10*dx)/dx), int((radius-1e-10*dy)/dy), int((radius-1e-10*dz)/dz)\n-        if self.domain.dim < 3:\n-            delemz = 0\n+        nx, ny, nz = self.domain.nelx, self.domain.nely, self.domain.nelz\n+        delemx = min(nx, int((radius-1e-10*dx)/dx))\n+        delemy = min(ny, int((radius-1e-10*dy)/dy))\n+        delemz = min(nz, int((radius-1e-10*dz)/dz))\n         xrange = np.arange(-delemx, delemx+1)*dx\n         yrange = np.arange(-delemy, delemy+1)*dy\n         zrange = np.arange(-delemz, delemz+1)*dz\n-        coords_x, coords_y, coords_z = np.meshgrid(xrange, yrange, zrange)\n+        coords_x, coords_y, coords_z = np.meshgrid(xrange, yrange, zrange, indexing='ij')\n         self.weights = np.maximum(0.0, radius - np.sqrt(coords_x*coords_x + coords_y*coords_y + coords_z*coords_z))\n         self.weights /= np.sum(self.weights)  # Volume preserving\n-        if self.domain.dim < 3:\n-            self.weights = self.weights[:, :, 0]\n \n     def _response(self, x):\n-        domain_sizes = [self.domain.nelx, self.domain.nely]\n-        if self.domain.dim >= 3:\n-            domain_sizes.append(self.domain.nely)\n-        xbox = x.reshape(*domain_sizes, order='F').T  # TODO 3d?\n-        return convolve(xbox, self.weights, mode=self.mode).flatten()\n+        xpad = self.get_padded_vector(x)\n+        y3d = convolve(xpad, self.weights, mode='valid')\n+        y = np.zeros_like(x)\n+        np.add.at(y, self.el3d_orig, y3d)\n+        return y\n \n     def _sensitivity(self, dfdv):\n-        domain_sizes = [self.domain.nelx, self.domain.nely]\n-        if self.domain.dim == 3:\n-            domain_sizes.append(self.domain.nely)\n-        ybox = dfdv.reshape(*domain_sizes, order='F').T  # TODO 3d\n-        return convolve(ybox, self.weights, mode=self.mode).flatten()\n+        dx3d = correlate(dfdv[self.el3d_orig], self.weights, mode='full')\n+        for index, _ in self.overrides:\n+            dx3d[index] = 0\n+        dx = np.zeros_like(self.sig_in[0].state)\n+        np.add.at(dx, self.el3d_pad, dx3d)\n+        return dx\n \n \n class Filter(Module):\n@@ -167,6 +368,8 @@ def _calculate_h(domain: DomainDefinition, radius=2.0):\n         nwindy = yupp - ylow + 1\n         nwindz = zupp - zlow + 1\n         nwind = nwindx * nwindy * nwindz\n+        if np.sum(nwind) < 0:\n+            raise OverflowError(\"Filter size too large for this mesh size\")\n \n         # Total number of window elements\n         ncum = np.cumsum(nwind)\ndiff --git a/pymoto/modules/generic.py b/pymoto/modules/generic.py\nindex 5cb0426..1fe4428 100644\n--- a/pymoto/modules/generic.py\n+++ b/pymoto/modules/generic.py\n@@ -105,10 +105,21 @@ def _sensitivity(self, df_dy):\n             if np.isrealobj(dg_dx[i]) and np.iscomplexobj(dg_dx_add):\n                 dg_dx_add = np.real(dg_dx_add)\n \n-            # Add the contribution\n+            # Add the contribution according to broadcasting rules of NumPy\n+            # https://numpy.org/doc/stable/user/basics.broadcasting.html\n             if (not hasattr(dg_dx[i], '__len__')) or (hasattr(dg_dx[i], 'ndim') and dg_dx[i].ndim == 0):\n                 # Scalar type or 0-dimensional array\n                 dg_dx[i] += np.sum(dg_dx_add)\n+            elif dg_dx[i].shape != dg_dx_add.shape:\n+                # Reverse broadcast https://stackoverflow.com/questions/76002989/numpy-is-there-a-reverse-broadcast\n+                n_leading_dims = dg_dx_add.ndim - dg_dx[i].ndim\n+                broadcasted_dims = tuple(range(n_leading_dims))\n+                for ii in range(dg_dx_add.ndim - n_leading_dims):\n+                    if dg_dx[i].shape[ii] == 1 and dg_dx_add.shape[ii+n_leading_dims] != 1:\n+                        broadcasted_dims = (*broadcasted_dims, n_leading_dims+ii)\n+\n+                dg_dx_add1 = np.add.reduce(dg_dx_add, axis=broadcasted_dims, keepdims=True)  # Sum broadcasted axis\n+                dg_dx[i] += np.squeeze(dg_dx_add1, axis=tuple(range(n_leading_dims)))  # Squeeze out singleton axis\n             else:\n                 dg_dx[i] += dg_dx_add\n \ndiff --git a/pymoto/modules/io.py b/pymoto/modules/io.py\nindex 4608d49..760275b 100644\n--- a/pymoto/modules/io.py\n+++ b/pymoto/modules/io.py\n@@ -14,7 +14,7 @@\n from .assembly import DomainDefinition\n \n \n-class _FigModule(Module):\n+class FigModule(Module):\n     \"\"\" Abstract base class for any module which produces a figure\n \n     Keyword Args:\n@@ -55,7 +55,7 @@ def __del__(self):\n         plt.close(self.fig)\n \n \n-class PlotDomain(_FigModule):\n+class PlotDomain(FigModule):\n     \"\"\" Plots the densities of a domain (2D or 3D)\n \n     Input Signal:\n@@ -96,10 +96,16 @@ def _plot_2d(self, x):\n             self.im.set_data(data)\n         else:\n             ax = self.fig.add_subplot(111)\n-            self.im = ax.imshow(data, origin='lower', cmap=self.cmap)\n+            Lx = self.domain.nelx * self.domain.unitx\n+            Ly = self.domain.nely * self.domain.unity\n+            self.im = ax.imshow(data, cmap=self.cmap, origin='lower', extent=(0.0, Lx, 0.0, Ly))\n             self.cbar = self.fig.colorbar(self.im, orientation='horizontal')\n             ax.set(xlabel='x', ylabel='y')\n-        clim = [np.min(data), np.max(data)] if self.clim is None else self.clim\n+        vmin, vmax = np.min(data), np.max(data)\n+        if vmin < 0:\n+            vabs = max(abs(vmin), abs(vmax))\n+            vmin, vmax = -vabs, vabs\n+        clim = [vmin, vmax] if self.clim is None else self.clim\n         self.im.set_clim(vmin=clim[0], vmax=clim[1])\n \n     def _plot_3d(self, x):\n@@ -138,7 +144,7 @@ def _plot_3d(self, x):\n         self.fac = ax.voxels(sel, facecolors=colors, linewidth=0.5, edgecolors='k')\n \n \n-class PlotGraph(_FigModule):\n+class PlotGraph(FigModule):\n     \"\"\" Plot an X-Y graph\n \n     Input Signals:\n@@ -175,14 +181,15 @@ def _response(self, x, *ys):\n         for i, y in enumerate(ys):\n             self.line[i].set_xdata(x)\n             self.line[i].set_ydata(y)\n-            ymin, ymax = min(ymin, min(y)), max(ymax, max(y))\n-        self.ax.set_xlim([min(x), max(x)])\n-        self.ax.set_ylim([ymin, ymax])\n+            ymin, ymax = min(ymin, np.min(y)), max(ymax, np.max(y))\n+        self.ax.set_xlim([np.min(x), np.max(x)])\n+        dy = ymax - ymin\n+        self.ax.set_ylim([ymin-0.05*dy, ymax+0.05*dy])\n \n         self._update_fig()\n \n \n-class PlotIter(_FigModule):\n+class PlotIter(FigModule):\n     \"\"\" Plot iteration history of one or more variables\n \n     Input Signals:\n@@ -193,10 +200,12 @@ class PlotIter(_FigModule):\n         overwrite (bool): Overwrite saved image every time the figure is updated, else prefix ``_0000`` is added to the\n           filename (default = ``False``)\n         show (bool): Show the figure on the screen\n+        ylim: Provide y-axis limits for the plot\n     \"\"\"\n-    def _prepare(self):\n+    def _prepare(self, ylim=None):\n         self.minlim = 1e+200\n         self.maxlim = -1e+200\n+        self.ylim = ylim\n \n     def _response(self, *args):\n         if not hasattr(self, 'ax'):\n@@ -227,7 +236,9 @@ def _response(self, *args):\n         dy = max((self.maxlim - self.minlim)*0.05, sys.float_info.min)\n \n         self.ax.set_xlim([-0.5, self.iter+0.5])\n-        if np.isfinite(self.minlim) and np.isfinite(self.maxlim):\n+        if self.ylim is not None:\n+            self.ax.set_ylim(self.ylim)\n+        elif np.isfinite(self.minlim) and np.isfinite(self.maxlim):\n             self.ax.set_ylim([self.minlim - dy, self.maxlim + dy])\n \n         self._update_fig()\ndiff --git a/pymoto/modules/linalg.py b/pymoto/modules/linalg.py\nindex 968dd2f..56a149e 100644\n--- a/pymoto/modules/linalg.py\n+++ b/pymoto/modules/linalg.py\n@@ -7,10 +7,9 @@\n import scipy.sparse as sps\n import scipy.sparse.linalg as spsla\n \n-from pymoto import Signal, Module, DyadCarrier, LDAWrapper\n-from pymoto import SolverDenseLU, SolverDenseLDL, SolverDenseCholesky, SolverDiagonal, SolverDenseQR\n-from pymoto import SolverSparseLU, SolverSparseCholeskyCVXOPT, SolverSparsePardiso, SolverSparseCholeskyScikit\n-from pymoto import matrix_is_symmetric, matrix_is_hermitian, matrix_is_diagonal\n+from pymoto import Signal, Module, DyadCarrier\n+from pymoto.solvers import auto_determine_solver\n+from pymoto.solvers import matrix_is_hermitian, LDAWrapper\n \n \n class StaticCondensation(Module):\n@@ -172,7 +171,7 @@ def _sensitivity(self, dgdx, dgdb):\n             adjoint_load += self.Afp * dgdb[self.p, ...]\n \n         lam = np.zeros_like(self.x)\n-        lamf = -1.0 * self.module_LinSolve.solver.adjoint(adjoint_load)\n+        lamf = -1.0 * self.module_LinSolve.solver.solve(adjoint_load, trans='T')\n         lam[self.f, ...] = lamf\n \n         if dgdb is not None:\n@@ -219,105 +218,6 @@ def _sensitivity(self, dB):\n         return dA if np.iscomplexobj(A) else np.real(dA)\n \n \n-# flake8: noqa: C901\n-def auto_determine_solver(A, isdiagonal=None, islowertriangular=None, isuppertriangular=None,\n-                          ishermitian=None, issymmetric=None, ispositivedefinite=None):\n-    \"\"\"\n-    Uses parts of Matlab's scheme https://nl.mathworks.com/help/matlab/ref/mldivide.html\n-    :param A: The matrix\n-    :param isdiagonal: Manual override for diagonal matrix\n-    :param islowertriangular: Override for lower triangular matrix\n-    :param isuppertriangular: Override for upper triangular matrix\n-    :param ishermitian: Override for hermitian matrix (prevents check)\n-    :param issymmetric: Override for symmetric matrix (prevents check). Is the same as hermitian for a real matrix\n-    :param ispositivedefinite: Manual override for positive definiteness\n-    :return: LinearSolver which should be 'best' for the matrix\n-    \"\"\"\n-    issparse = sps.issparse(A)  # Check if the matrix is sparse\n-    issquare = A.shape[0] == A.shape[1]  # Check if the matrix is square\n-\n-    if not issquare:\n-        if issparse:\n-            sps.SparseEfficiencyWarning(\"Only a dense version of QR solver is available\")  # TODO\n-        return SolverDenseQR()\n-\n-    # l_bw, u_bw = spla.bandwidth(A) # TODO Get bandwidth (implemented in scipy version > 1.8.0)\n-\n-    if isdiagonal is None:  # Check if matrix is diagonal\n-        # TODO: This could be improved to check other sparse matrix types as well\n-        isdiagonal = matrix_is_diagonal(A)\n-    if isdiagonal:\n-        return SolverDiagonal()\n-\n-    # Check if the matrix is triangular\n-    # TODO Currently only for dense matrices\n-    if islowertriangular is None:  # Check if matrix is lower triangular\n-        islowertriangular = False if issparse else np.allclose(A, np.tril(A))\n-    if islowertriangular:\n-        warnings.WarningMessage(\"Lower triangular solver not implemented\",\n-                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n-\n-    if isuppertriangular is None:  # Check if matrix is upper triangular\n-        isuppertriangular = False if issparse else np.allclose(A, np.triu(A))\n-    if isuppertriangular:\n-        warnings.WarningMessage(\"Upper triangular solver not implemented\",\n-                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n-\n-    ispermutedtriangular = False\n-    if ispermutedtriangular:\n-        warnings.WarningMessage(\"Permuted triangular solver not implemented\",\n-                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n-\n-    # Check if the matrix is complex-valued\n-    iscomplex = np.iscomplexobj(A)\n-    if iscomplex:\n-        # Detect if the matrix is hermitian and/or symmetric\n-        if ishermitian is None:\n-            ishermitian = matrix_is_hermitian(A)\n-        if issymmetric is None:\n-            issymmetric = matrix_is_symmetric(A)\n-    else:\n-        if ishermitian is None and issymmetric is None:\n-            # Detect if the matrix is symmetric\n-            issymmetric = matrix_is_symmetric(A)\n-            ishermitian = issymmetric\n-        elif ishermitian is not None and issymmetric is not None:\n-            assert ishermitian == issymmetric, \"For real-valued matrices, symmetry and hermitian must be equal\"\n-        elif ishermitian is None:\n-            ishermitian = issymmetric\n-        elif issymmetric is None:\n-            issymmetric = ishermitian\n-\n-    if issparse:\n-        # Prefer Intel Pardiso solver as it can solve any matrix TODO: Check for complex matrix\n-        if SolverSparsePardiso.defined and not iscomplex:\n-            # TODO check for positive definiteness?  np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0)\n-            return SolverSparsePardiso(symmetric=issymmetric, hermitian=ishermitian, positive_definite=ispositivedefinite)\n-\n-        if ishermitian:\n-            # Check if diagonal is all positive or all negative -> Cholesky\n-            if np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0):  # TODO what about the complex case?\n-                if SolverSparseCholeskyScikit.defined:\n-                    return SolverSparseCholeskyScikit()\n-                if SolverSparseCholeskyCVXOPT.defined:\n-                    return SolverSparseCholeskyCVXOPT()\n-\n-        return SolverSparseLU()  # Default to LU, which should be possible for any non-singular square matrix\n-\n-    else:  # Dense branch\n-        if ishermitian:\n-            # Check if diagonal is all positive or all negative\n-            if np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0):\n-                return SolverDenseCholesky()\n-            else:\n-                return SolverDenseLDL(hermitian=ishermitian)\n-        elif issymmetric:\n-            return SolverDenseLDL(hermitian=ishermitian)\n-        else:\n-            # TODO: Detect if the matrix is Hessenberg\n-            return SolverDenseLU()\n-\n-\n class LinSolve(Module):\n     r\"\"\" Solves linear system of equations :math:`\\mathbf{A}\\mathbf{x}=\\mathbf{b}`\n \n@@ -347,6 +247,7 @@ def _prepare(self, dep_tol=1e-5, hermitian=None, symmetric=None, solver=None):\n         self.ishermitian = hermitian\n         self.issymmetric = symmetric\n         self.solver = solver\n+        self.u = None  # Solution storage\n \n     def _response(self, mat, rhs):\n         # Do some detections on the matrix type\n@@ -365,19 +266,23 @@ def _response(self, mat, rhs):\n         if self.solver is None:\n             self.solver = auto_determine_solver(mat, ishermitian=self.ishermitian)\n         if not isinstance(self.solver, LDAWrapper) and self.use_lda_solver:\n-            self.solver = LDAWrapper(self.solver, hermitian=self.ishermitian, symmetric=self.issymmetric)\n+            lda_kwargs = dict(hermitian=self.ishermitian, symmetric=self.issymmetric)\n+            if hasattr(self.solver, 'tol'):\n+                lda_kwargs['tol'] = self.solver.tol * 2\n+            self.solver = LDAWrapper(self.solver, **lda_kwargs)\n \n         # Update solver with new matrix\n         self.solver.update(mat)\n \n         # Solution\n-        self.u = self.solver.solve(rhs)\n+        self.u = self.solver.solve(rhs, x0=self.u)\n \n         return self.u\n \n     def _sensitivity(self, dfdv):\n         mat, rhs = [s.state for s in self.sig_in]\n-        lam = self.solver.adjoint(dfdv.conj()).conj()\n+        # lam = self.solver.solve(dfdv.conj(), trans='H').conj()\n+        lam = self.solver.solve(dfdv, trans='T')\n \n         if self.issparse:\n             if self.u.ndim > 1:\n@@ -440,6 +345,7 @@ def _prepare(self, sorting_func=lambda W, Q: np.argsort(W), hermitian=None, nmod\n         self.sigma = sigma\n         self.mode = mode\n         self.Ainv = None\n+        self.do_solve = False\n \n     def _response(self, A, *args):\n         B = args[0] if len(args) > 0 else None\n@@ -500,9 +406,14 @@ def _sparse_eigs(self, A, B=None):\n         # Use shift-and-invert, so make inverse operator\n         if self.Ainv is None:\n             self.Ainv = auto_determine_solver(mat_shifted, ishermitian=self.is_hermitian)\n-        self.Ainv.update(mat_shifted)\n-\n-        AinvOp = spsla.LinearOperator(mat_shifted.shape, matvec=self.Ainv.solve, rmatvec=self.Ainv.adjoint)\n+            self.do_solve = True\n+        if self.sigma != 0:\n+            self.do_solve = True\n+        if self.do_solve:\n+            self.Ainv.update(mat_shifted)\n+\n+        AinvOp = spsla.LinearOperator(mat_shifted.shape, matvec=self.Ainv.solve,\n+                                      rmatvec=lambda b: self.Ainv.solve(b, trans='H'))\n \n         if self.is_hermitian:\n             return spsla.eigsh(A, M=B, k=self.nmodes, OPinv=AinvOp, sigma=self.sigma, mode=self.mode)\ndiff --git a/pymoto/modules/scaling.py b/pymoto/modules/scaling.py\nindex 1a48255..74d644b 100644\n--- a/pymoto/modules/scaling.py\n+++ b/pymoto/modules/scaling.py\n@@ -1,12 +1,12 @@\n from pymoto import Module\n-\n+import numpy as np\n \n class Scaling(Module):\n     r\"\"\" Scales (scalar) input for different response functions in optimization (objective / constraints).\n     This is useful, for instance, for MMA where the objective must be scaled in a certain way for good convergence.\n \n-    Objective scaling (`minval` and `maxval` are both undefined):\n-    :math:`y^{(i)} = s \\frac{x^{(i)}}{x^{(0)}}`\n+    Objective scaling using absolute value or vector norm (`minval` and `maxval` are both undefined):\n+    :math:`y^{(i)} = s \\frac{x^{(i)}}{||x^{(0)}||}`\n \n     For the constraints, the negative null-form convention is used, which means the constraint is :math:`y(x) \\leq 0`.\n \n@@ -39,7 +39,7 @@ def _prepare(self, scaling: float = 100.0, minval: float = None, maxval: float =\n \n     def _response(self, x):\n         if not hasattr(self, 'sf'):\n-            self.sf = self.scaling/x\n+            self.sf = self.scaling/np.linalg.norm(x)\n         if self.minval is not None:\n             g = 1 - x/self.minval\n         elif self.maxval is not None:\ndiff --git a/pymoto/routines.py b/pymoto/routines.py\nindex a48a684..4ed553a 100644\n--- a/pymoto/routines.py\n+++ b/pymoto/routines.py\n@@ -1,8 +1,10 @@\n+import warnings\n import numpy as np\n from .utils import _parse_to_list, _concatenate_to_array\n from .core_objects import Signal, SignalSlice, Module, Network\n from .common.mma import MMA\n from typing import List, Iterable, Union, Callable\n+from scipy.sparse import issparse\n \n \n def _has_signal_overlap(sig1: List[Signal], sig2: List[Signal]):\n@@ -171,7 +173,10 @@ def finite_difference(blk: Module, fromsig: Union[Signal, Iterable[Signal]] = No\n                 fp = Sout.state\n \n                 # Finite difference sensitivity\n-                df = (fp - f0[Iout])/(dx*sf)\n+                if issparse(fp):\n+                    df = (fp.toarray() - f0[Iout].toarray()) / (dx * sf)\n+                else:\n+                    df = (fp - f0[Iout])/(dx*sf)\n \n                 dgdx_fd = np.real(np.sum(df*df_an[Iout]))\n \n@@ -223,7 +228,10 @@ def finite_difference(blk: Module, fromsig: Union[Signal, Iterable[Signal]] = No\n                     fp = Sout.state\n \n                     # Finite difference sensitivity\n-                    df = (fp - f0[Iout])/(dx*1j*sf)\n+                    if issparse(fp):\n+                        df = (fp.toarray() - f0[Iout].toarray()) / (dx * 1j * sf)\n+                    else:\n+                        df = (fp - f0[Iout])/(dx*1j*sf)\n                     dgdx_fd = np.imag(np.sum(df*df_an[Iout]))\n \n                     if dx_an[Iout][Iin] is not None:\n@@ -335,8 +343,9 @@ def minimize_oc(function, variables, objective: Signal,\n         function.sensitivity()\n         dfdx, _ = _concatenate_to_array(obtain_sensitivities(variables))\n         maxdfdx = max(dfdx)\n-        if maxdfdx > 0:\n-            raise RuntimeError(f\"OC only works for negative sensitivities: max(dfdx) = {maxdfdx}\")\n+        if maxdfdx > 1e-15:\n+            warnings.warn(f\"OC only works for negative sensitivities: max(dfdx) = {maxdfdx}. Clipping positive values.\")\n+        dfdx = np.minimum(dfdx, 0)\n \n         # Do OC update\n         l1, l2 = l1init, l2init\ndiff --git a/pymoto/solvers/__init__.py b/pymoto/solvers/__init__.py\nnew file mode 100644\nindex 0000000..636a423\n--- /dev/null\n+++ b/pymoto/solvers/__init__.py\n@@ -0,0 +1,14 @@\n+from .solvers import LinearSolver, LDAWrapper\n+from .matrix_checks import matrix_is_complex, matrix_is_diagonal, matrix_is_symmetric, matrix_is_hermitian\n+from .dense import SolverDiagonal, SolverDenseQR, SolverDenseLU, SolverDenseCholesky, SolverDenseLDL\n+from .sparse import SolverSparsePardiso, SolverSparseLU, SolverSparseCholeskyScikit, SolverSparseCholeskyCVXOPT\n+from .iterative import Preconditioner, CG, DampedJacobi, SOR, ILU, GeometricMultigrid\n+from .auto_determine import auto_determine_solver\n+\n+__all__ = ['matrix_is_complex', 'matrix_is_diagonal', 'matrix_is_symmetric', 'matrix_is_hermitian',\n+           'LinearSolver', 'LDAWrapper',\n+           'SolverDiagonal', 'SolverDenseQR', 'SolverDenseLU', 'SolverDenseCholesky', 'SolverDenseLDL',\n+           'SolverSparsePardiso', 'SolverSparseLU', 'SolverSparseCholeskyScikit', 'SolverSparseCholeskyCVXOPT',\n+           'Preconditioner', 'CG', 'DampedJacobi', 'SOR', 'ILU', 'GeometricMultigrid',\n+           'auto_determine_solver',\n+           ]\n\\ No newline at end of file\ndiff --git a/pymoto/solvers/auto_determine.py b/pymoto/solvers/auto_determine.py\nnew file mode 100644\nindex 0000000..7ad9709\n--- /dev/null\n+++ b/pymoto/solvers/auto_determine.py\n@@ -0,0 +1,108 @@\n+import warnings\n+import scipy.sparse as sps\n+from inspect import currentframe, getframeinfo\n+\n+from .dense import *\n+from .sparse import *\n+from .matrix_checks import *\n+\n+\n+# flake8: noqa: C901\n+def auto_determine_solver(A, isdiagonal=None, islowertriangular=None, isuppertriangular=None,\n+                          ishermitian=None, issymmetric=None, ispositivedefinite=None):\n+    \"\"\"\n+    Uses parts of Matlab's scheme https://nl.mathworks.com/help/matlab/ref/mldivide.html\n+    :param A: The matrix\n+    :param isdiagonal: Manual override for diagonal matrix\n+    :param islowertriangular: Override for lower triangular matrix\n+    :param isuppertriangular: Override for upper triangular matrix\n+    :param ishermitian: Override for hermitian matrix (prevents check)\n+    :param issymmetric: Override for symmetric matrix (prevents check). Is the same as hermitian for a real matrix\n+    :param ispositivedefinite: Manual override for positive definiteness\n+    :return: LinearSolver which should be 'best' for the matrix\n+    \"\"\"\n+    issparse = sps.issparse(A)  # Check if the matrix is sparse\n+    issquare = A.shape[0] == A.shape[1]  # Check if the matrix is square\n+\n+    if not issquare:\n+        if issparse:\n+            sps.SparseEfficiencyWarning(\"Only a dense version of QR solver is available\")  # TODO\n+        return SolverDenseQR()\n+\n+    # l_bw, u_bw = spla.bandwidth(A) # TODO Get bandwidth (implemented in scipy version > 1.8.0)\n+\n+    if isdiagonal is None:  # Check if matrix is diagonal\n+        # TODO: This could be improved to check other sparse matrix types as well\n+        isdiagonal = matrix_is_diagonal(A)\n+    if isdiagonal:\n+        return SolverDiagonal()\n+\n+    # Check if the matrix is triangular\n+    # TODO Currently only for dense matrices\n+    if islowertriangular is None:  # Check if matrix is lower triangular\n+        islowertriangular = False if issparse else np.allclose(A, np.tril(A))\n+    if islowertriangular:\n+        warnings.WarningMessage(\"Lower triangular solver not implemented\",\n+                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n+\n+    if isuppertriangular is None:  # Check if matrix is upper triangular\n+        isuppertriangular = False if issparse else np.allclose(A, np.triu(A))\n+    if isuppertriangular:\n+        warnings.WarningMessage(\"Upper triangular solver not implemented\",\n+                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n+\n+    ispermutedtriangular = False\n+    if ispermutedtriangular:\n+        warnings.WarningMessage(\"Permuted triangular solver not implemented\",\n+                                UserWarning, getframeinfo(currentframe()).filename, getframeinfo(currentframe()).lineno)\n+\n+    # Check if the matrix is complex-valued\n+    iscomplex = np.iscomplexobj(A)\n+    if iscomplex:\n+        # Detect if the matrix is hermitian and/or symmetric\n+        if ishermitian is None:\n+            ishermitian = matrix_is_hermitian(A)\n+        if issymmetric is None:\n+            issymmetric = matrix_is_symmetric(A)\n+    else:\n+        if ishermitian is None and issymmetric is None:\n+            # Detect if the matrix is symmetric\n+            issymmetric = matrix_is_symmetric(A)\n+            ishermitian = issymmetric\n+        elif ishermitian is not None and issymmetric is not None:\n+            assert ishermitian == issymmetric, \"For real-valued matrices, symmetry and hermitian must be equal\"\n+        elif ishermitian is None:\n+            ishermitian = issymmetric\n+        elif issymmetric is None:\n+            issymmetric = ishermitian\n+\n+    if issparse:\n+        # Prefer Intel Pardiso solver as it can solve any matrix TODO: Check for complex matrix\n+        if SolverSparsePardiso.defined and not iscomplex:\n+            # TODO check for positive definiteness?  np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0)\n+            return SolverSparsePardiso(symmetric=issymmetric, hermitian=ishermitian, positive_definite=ispositivedefinite)\n+\n+        if ishermitian:\n+            # Check if diagonal is all positive or all negative -> Cholesky\n+            if ispositivedefinite is None:\n+                ispositivedefinite = np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0)\n+            if ispositivedefinite:  # TODO what about the complex case?\n+                if SolverSparseCholeskyScikit.defined:\n+                    return SolverSparseCholeskyScikit()\n+                if SolverSparseCholeskyCVXOPT.defined:\n+                    return SolverSparseCholeskyCVXOPT()\n+\n+        return SolverSparseLU()  # Default to LU, which should be possible for any non-singular square matrix\n+\n+    else:  # Dense branch\n+        if ishermitian:\n+            # Check if diagonal is all positive or all negative\n+            if np.all(A.diagonal() > 0) or np.all(A.diagonal() < 0):\n+                return SolverDenseCholesky()\n+            else:\n+                return SolverDenseLDL(hermitian=ishermitian)\n+        elif issymmetric:\n+            return SolverDenseLDL(hermitian=ishermitian)\n+        else:\n+            # TODO: Detect if the matrix is Hessenberg\n+            return SolverDenseLU()\ndiff --git a/pymoto/common/solvers_dense.py b/pymoto/solvers/dense.py\nsimilarity index 54%\nrename from pymoto/common/solvers_dense.py\nrename to pymoto/solvers/dense.py\nindex 3fd8c35..201c989 100644\n--- a/pymoto/common/solvers_dense.py\n+++ b/pymoto/solvers/dense.py\n@@ -1,7 +1,8 @@\n import warnings\n import numpy as np\n import scipy.linalg as spla  # Dense matrix solvers\n-from .solvers import matrix_is_hermitian, matrix_is_diagonal, LinearSolver\n+from .matrix_checks import matrix_is_hermitian, matrix_is_diagonal\n+from .solvers import LinearSolver\n \n \n class SolverDiagonal(LinearSolver):\n@@ -11,24 +12,17 @@ def update(self, A):\n         self.diag = A.diagonal()\n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solve using the diagonal only, by :math:`x_i = b_i / A_{ii}`\n \n         The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n         :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n         \"\"\"\n+        d = self.diag.conj() if trans == 'H' else self.diag\n         if rhs.ndim == 1:\n-            return rhs / self.diag\n+            return rhs / d\n         else:\n-            return rhs / self.diag[..., None]\n-\n-    def adjoint(self, rhs):\n-        r\"\"\" Solve using the diagonal only, by :math:`x_i = b_i / A_{ii}^*`\n-\n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        return self.solve(rhs.conj()).conj()\n+            return rhs / d[..., None]\n \n \n # Dense QR solver\n@@ -42,23 +36,31 @@ def update(self, A):\n         self.q, self.r = spla.qr(A)\n         return self\n \n-    def solve(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by backward substitution of\n-        :math:`\\mathbf{x} = \\mathbf{R}^{-1}\\mathbf{Q}^\\text{H}\\mathbf{b}`.\n-\n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        return spla.solve_triangular(self.r, self.q.T.conj()@rhs)\n+    def solve(self, rhs, x0=None, trans='N'):\n+        r\"\"\" Solves the linear system of equations using the QR factorization.\n \n-    def adjoint(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A}^\\text{H}\\mathbf{x} = \\mathbf{b}` by\n-        forward substitution of :math:`\\mathbf{x} = \\mathbf{Q}\\mathbf{R}^{-H}\\mathbf{b}`.\n+        ======= ================= =====================\n+        `trans`     Equation      Solution of :math:`x`\n+        ------- ----------------- ---------------------\n+          `N`   :math:`A x = b`   :math:`R^{-1} Q^H b`\n+          `T`   :math:`A^T x = b` :math:`Q^* R^{-T} b`\n+          `H`   :math:`A^H x = b` :math:`Q R^{-H} b`\n+        ======= ================= =====================\n \n         The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n         :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n         \"\"\"\n-        return self.q@spla.solve_triangular(self.r, rhs, trans='C')\n+        if trans == 'N':\n+            # A = Q R -> inv(A) = inv(R) inv(Q) = inv(R) Q^H\n+            return spla.solve_triangular(self.r, self.q.T.conj() @ rhs)\n+        elif trans == 'T':\n+            # A^T = R^T Q^T -> inv(A^T) = inv(Q^T) inv(R^T) = conj(Q) inv(R^T)\n+            return self.q.conj() @ spla.solve_triangular(self.r, rhs, trans='T')\n+        elif trans == 'H':\n+            # A^H = R^H Q^H -> inv(A^H) = inv(Q^H) inv(R^H) = Q inv(R^H)\n+            return self.q @ spla.solve_triangular(self.r, rhs, trans='C')\n+        else:\n+            raise TypeError(\"Only N, T, and H transposition is possible\")\n \n \n # Dense LU solver\n@@ -71,24 +73,34 @@ def update(self, A):\n         self.p, self.l, self.u = spla.lu(A)\n         return self\n \n-    def solve(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n-        substitution of :math:`\\mathbf{x} = \\mathbf{U}^{-1}\\mathbf{L}^{-1}\\mathbf{b}`.\n+    def solve(self, rhs, x0=None, trans='N'):\n+        r\"\"\" Solves the linear system of equations using the LU factorization.\n \n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        return spla.solve_triangular(self.u, spla.solve_triangular(self.l, self.p.T@rhs, lower=True))\n+        :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n+        substitution of :math:`\\mathbf{x} = \\mathbf{U}^{-1}\\mathbf{L}^{-1}\\mathbf{b}`.\n \n-    def adjoint(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A}^\\text{H}\\mathbf{x} = \\mathbf{b}` by forward and\n-        backward substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{U}^{-\\text{H}}\\mathbf{b}`.\n+        ======= ================= =========================\n+        `trans`     Equation        Solution of :math:`x`\n+        ------- ----------------- -------------------------\n+          `N`   :math:`A x = b`   :math:`x = U^{-1} L^{-1}`\n+          `T`   :math:`A^T x = b` :math:`x = L^{-1} U^{-1}`\n+          `H`   :math:`A^H x = b` :math:`x = L^{-*} U^{-*}`\n+        ======= ================= =========================\n \n         The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n         :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n         \"\"\"\n-        return self.p@spla.solve_triangular(self.l, spla.solve_triangular(self.u, rhs, trans='C'),\n-                                            lower=True, trans='C')  # TODO permutation\n+        if trans == 'N':\n+            # A = P L U -> x = U^-1 L^-1 P^T b\n+            return spla.solve_triangular(self.u, spla.solve_triangular(self.l, self.p.T@rhs, lower=True))\n+        elif trans == 'T':\n+            return self.p @ spla.solve_triangular(self.l, spla.solve_triangular(self.u, rhs, trans='T'),\n+                                                  lower=True, trans='T')\n+        elif trans == 'H':\n+            return self.p @ spla.solve_triangular(self.l, spla.solve_triangular(self.u, rhs, trans='C'),\n+                                                  lower=True, trans='C')\n+        else:\n+            raise TypeError(\"Only N, T, and H transposition is possible\")\n \n \n # Dense Cholesky solver\n@@ -106,7 +118,7 @@ def update(self, A):\n         upper triangular matrix.\n         \"\"\"\n         try:\n-            self.u = spla.cholesky(A)\n+            self.U = spla.cholesky(A)\n             self.success = True\n         except np.linalg.LinAlgError as err:\n             warnings.warn(f\"{type(self).__name__}: {err} -- using {type(self.backup_solver).__name__} instead\")\n@@ -114,7 +126,7 @@ def update(self, A):\n             self.success = False\n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n         substitution of :math:`\\mathbf{x} = \\mathbf{U}^{-1}\\mathbf{U}^{-\\text{H}}\\mathbf{b}`.\n \n@@ -124,21 +136,16 @@ def solve(self, rhs):\n         # TODO When Cholesky factorization A = U^T U is used, symmetric complex matrices can also be solved, but this is\n         #  not implemented in scipy\n         if self.success:\n-            return spla.solve_triangular(self.u, spla.solve_triangular(self.u, rhs, trans='C'))\n-        else:\n-            return self.backup_solver.solve(rhs)\n-\n-    def adjoint(self, rhs):\n-        r\"\"\" A Hermitian matrix is self-adjoint (:math:`\\mathbf{A}=\\mathbf{A}^\\text{H}`), so this is equal to the\n-        regular solution.\n-\n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        if self.success:\n-            return self.solve(rhs)\n+            if trans == 'N' or trans == 'H':\n+                # A = U^H U -> A^-1 = U^-1 U^-H\n+                return spla.solve_triangular(self.U, spla.solve_triangular(self.U, rhs, trans='C'))\n+            elif trans == 'T':\n+                # A^T = U^T conj(U) -> A^-T = conj(U^-1) U^-T\n+                return spla.solve_triangular(self.U, spla.solve_triangular(self.U, rhs, trans='T').conj()).conj()\n+            else:\n+                raise TypeError(\"Only N, T, and H transposition is possible\")\n         else:\n-            return self.backup_solver.adjoint(rhs)\n+            return self.backup_solver.solve(rhs, trans=trans)\n \n \n # Dense LDL solver\n@@ -171,23 +178,13 @@ def update(self, A):\n         self.lp = self.l[self.p, :]\n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n         substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{D}^{-1}\\mathbf{L}^{-1}\\mathbf{b}` in the\n         Hermitian case or as :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{T}}\\mathbf{D}^{-1}\\mathbf{L}^{-1}\\mathbf{b}` in the\n         symmetric case.\n \n-        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n-        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n-        \"\"\"\n-        u1 = spla.solve_triangular(self.lp, rhs[self.p], lower=True, unit_diagonal=True)\n-        u2 = self.dinv(u1)\n-        u = np.zeros_like(rhs, dtype=u2.dtype)\n-        u[self.p] = spla.solve_triangular(self.lp, u2, trans='C' if self.hermitian else 'T', lower=True, unit_diagonal=True)\n-        return u\n-\n-    def adjoint(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A}^\\text{H} \\mathbf{x} = \\mathbf{b}` by forward and\n+        The adjoint system of equations :math:`\\mathbf{A}^\\text{H} \\mathbf{x} = \\mathbf{b}` is solved by forward and\n         backward substitution of\n         :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{D}^{-\\text{H}}\\mathbf{L}^{-1}\\mathbf{b}` in the  Hermitian\n         case or as :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{D}^{-\\text{H}}\\mathbf{L}^{-*}\\mathbf{b}`\n@@ -196,11 +193,34 @@ def adjoint(self, rhs):\n         The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n         :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n         \"\"\"\n-        if not self.hermitian:\n-            u1 = spla.solve_triangular(self.lp, rhs[self.p].conj(), lower=True, unit_diagonal=True).conj()\n-        else:\n+        if trans == 'N':\n+            # Hermitian matrix A: A = L D L^H -> inv(A) = inv(L^H) inv(D) inv(L)\n+            # Symmetric matrix A: A = L D L^T -> inv(A) = inv(L^T) inv(D) inv(L)\n             u1 = spla.solve_triangular(self.lp, rhs[self.p], lower=True, unit_diagonal=True)\n-        u2 = self.dinvH(u1)\n-        u = np.zeros_like(rhs, dtype=u2.dtype)\n-        u[self.p] = spla.solve_triangular(self.lp, u2, trans='C', lower=True, unit_diagonal=True)\n+            u2 = self.dinv(u1)\n+            u = np.zeros_like(rhs, dtype=u2.dtype)\n+            u[self.p] = spla.solve_triangular(self.lp, u2, trans='C' if self.hermitian else 'T', lower=True, unit_diagonal=True)\n+        elif trans == 'T':\n+            # Hermitian matrix A^T: A = conj(L) D^T L^T -> inv(A) = inv(L^T) inv(D^T) inv(L^*)\n+            # Symmetric matrix A^T: A = L D^T L^T -> inv(A) = inv(L^T) inv(D^T) inv(L)\n+            if self.hermitian:\n+                u1 = spla.solve_triangular(self.lp, rhs[self.p].conj(), lower=True, unit_diagonal=True).conj()\n+            else:\n+                u1 = spla.solve_triangular(self.lp, rhs[self.p], lower=True, unit_diagonal=True)\n+\n+            u2 = self.dinvH(u1.conj()).conj()\n+            u = np.zeros_like(rhs, dtype=u2.dtype)\n+            u[self.p] = spla.solve_triangular(self.lp, u2, trans='T', lower=True, unit_diagonal=True)\n+        elif trans == 'H':\n+            # Hermitian matrix A: inv(A^H) = inv(L^H) inv(D^H) inv(L)\n+            # Symmetric matrix A: inv(A^H) = inv(L^H) inv(D^H) inv(L^*)\n+            if not self.hermitian:\n+                u1 = spla.solve_triangular(self.lp, rhs[self.p].conj(), lower=True, unit_diagonal=True).conj()\n+            else:\n+                u1 = spla.solve_triangular(self.lp, rhs[self.p], lower=True, unit_diagonal=True)\n+            u2 = self.dinvH(u1)\n+            u = np.zeros_like(rhs, dtype=u2.dtype)\n+            u[self.p] = spla.solve_triangular(self.lp, u2, trans='C', lower=True, unit_diagonal=True)\n+        else:\n+            raise TypeError(\"Only N, T, and H transposition is possible\")\n         return u\ndiff --git a/pymoto/solvers/iterative.py b/pymoto/solvers/iterative.py\nnew file mode 100644\nindex 0000000..952a3ba\n--- /dev/null\n+++ b/pymoto/solvers/iterative.py\n@@ -0,0 +1,361 @@\n+import warnings\n+import time\n+import numpy as np\n+import scipy.sparse as sps\n+from scipy.sparse.linalg import splu, spilu\n+from .solvers import LinearSolver\n+from .auto_determine import auto_determine_solver\n+from pymoto import DomainDefinition\n+\n+\n+class Preconditioner(LinearSolver):\n+    \"\"\" Abstract base class for preconditioners to inexact solvers \"\"\"\n+    def update(self, A):\n+        pass\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        return rhs.copy()\n+\n+\n+class DampedJacobi(Preconditioner):\n+    r\"\"\" Damped Jacobi preconditioner\n+    :math:`M = \\frac{1}{\\omega} D`\n+    Args:\n+        A (optional): The matrix\n+        w (optional): Weight factor :math:`0 < \\omega \\leq 1`\n+    \"\"\"\n+    def __init__(self, A=None, w=1.0):\n+        assert 0 < w <= 1, 'w must be between 0 and 1'\n+        self.w = w\n+        self.D = None\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        self.D = A.diagonal()\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        if trans == 'N' or trans == 'T':\n+            return self.w * (rhs.T/self.D).T\n+        elif trans == 'H':\n+            return self.w * (rhs.T/self.D.conj()).T\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+\n+\n+class SOR(Preconditioner):\n+    r\"\"\" Successive over-relaxation preconditioner\n+    The matrix :math:`A = L + D + U` is split into a lower triangular, diagonal, and upper triangular part.\n+    :math:`M = \\left(\\frac{D}{\\omega} + L\\right) \\frac{\\omega D^{-1}}{2-\\omega} \\left(\\frac{D}{\\omega} + U\\right)`\n+\n+    Args:\n+        A (optional): The matrix\n+        w (optional): Weight factor :math:`0 < \\omega < 2`\n+    \"\"\"\n+    def __init__(self, A=None, w=1.0):\n+        assert 0 < w < 2, 'w must be between 0 and 2'\n+        self.w = w\n+        self.L = None\n+        self.U = None\n+        self.Dw = None\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        diag = A.diagonal()\n+        diagw = sps.diags(diag)/self.w\n+        self.L = splu(sps.tril(A, k=-1) + diagw)  # Lower triangular part including diagonal\n+        self.U = splu(sps.triu(A, k=1) + diagw)\n+\n+        self.Dw = diag * (2 - self.w) / self.w\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        if trans == 'N':\n+            # M = (D/w + L) wD^-1 / (2-w) (D/w + U)\n+            # from scipy.sparse.linalg import spsolve_triangular\n+            # u1 = spsolve_triangular(self.L, rhs, lower=True, overwrite_A=False)  # Solve triangular is still very slow :(\n+            u1 = self.L.solve(rhs)\n+            u1 *= self.Dw[:, None]\n+            # u2 = spsolve_triangular(self.U, u1, lower=False, overwrite_A=False, overwrite_b=True)\n+            u2 = self.U.solve(u1)\n+            return u2\n+        elif trans == 'T':\n+            u1 = self.U.solve(rhs, trans='T')\n+            u1 *= self.Dw[:, None]\n+            u2 = self.L.solve(u1, trans='T')\n+            return u2\n+        elif trans == 'H':\n+            u1 = self.U.solve(rhs, trans='H')\n+            u1 *= self.Dw[:, None].conj()\n+            u2 = self.L.solve(u1, trans='H')\n+            return u2\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+\n+\n+class ILU(Preconditioner):\n+    \"\"\" Incomplete LU factorization\n+\n+    Args:\n+        A (optional): The matrix\n+        **kwargs (optional): Keyword arguments passed to `scipy.sparse.linalg.spilu`\n+    \"\"\"\n+    def __init__(self, A=None, **kwargs):\n+        self.kwargs = kwargs\n+        self.ilu = None\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        self.ilu = spilu(A, **self.kwargs)\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        return self.ilu.solve(rhs, trans=trans)\n+\n+\n+class GeometricMultigrid(Preconditioner):\n+    \"\"\" Geometric multigrid preconditioner\n+\n+    Args:\n+        domain: The `DomainDefinition` used for the geometry\n+        A (optional): The matrix\n+        inner_level (optional): Inner solver for the coarse grid, for instance, a direct solver or another MG level.\n+            The default is an automatically determined direct solver.\n+        smoother (optional): Smoother to use to smooth the residual and solution before and after coarse level.\n+            The default is `DampedJacobi(w=0.5)`.\n+        smooth_steps (optional): Number of smoothing steps to execute\n+    \"\"\"\n+    _available_cycles = ['v', 'w']\n+\n+    def __init__(self, domain: DomainDefinition, A=None, cycle='V', inner_level=None, smoother=None, smooth_steps=5):\n+        assert domain.nelx % 2 == 0 and domain.nely % 2 == 0 and domain.nelz % 2 == 0, \\\n+            f\"Domain sizes {domain.nelx, domain.nely, domain.nelz} must be divisible by 2\"\n+        self.domain = domain\n+        self.A = A\n+        assert cycle.lower() in self._available_cycles, f\"Cycle ({cycle}) is not available. Options are {self._available_cycles}\"\n+        self.cycle = cycle\n+        self.inner_level = None if inner_level is None else inner_level\n+        self.smoother = DampedJacobi(w=0.5) if smoother is None else None\n+        self.smooth_steps = smooth_steps\n+        self.R = None\n+        self.sub_domain = DomainDefinition(domain.nelx // 2, domain.nely // 2, domain.nelz // 2,\n+                                           domain.unitx * 2, domain.unity * 2, domain.unitz * 2)\n+\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        if self.R is None:\n+            self.setup_interpolation(A)\n+        self.A = A\n+        self.smoother.update(A)\n+        Ac = self.R.T @ A @ self.R\n+        if self.inner_level is None:\n+            self.inner_level = auto_determine_solver(Ac)\n+        self.inner_level.update(Ac)\n+\n+    def setup_interpolation(self, A):\n+        assert A.shape[0] % self.domain.nnodes == 0\n+        ndof = int(A.shape[0] / self.domain.nnodes)  # Number of dofs per node\n+\n+        w = np.ones((3, 3, 3))*0.125\n+        w[1, :, :] = 0.25\n+        w[:, 1, :] = 0.25\n+        w[:, :, 1] = 0.25\n+        w[1, 1, :] = 0.5\n+        w[1, :, 1] = 0.5\n+        w[:, 1, 1] = 0.5\n+        w[1, 1, 1] = 1.0\n+\n+        rows = []\n+        cols = []\n+        vals = []\n+        for i in [-1, 0, 1]:\n+            imin, imax = max(-i, 0), min(self.sub_domain.nelx + 1 - i, self.sub_domain.nelx + 1)\n+            ix = np.arange(imin, imax)\n+            for j in [-1, 0, 1]:\n+                jmin, jmax = max(-j, 0), min(self.sub_domain.nely + 1 - j, self.sub_domain.nely + 1)\n+                iy = np.arange(jmin, jmax)\n+                for k in ([-1, 0, 1] if self.domain.dim == 3 else [0]):\n+                    # Coarse node cartesian indices\n+                    kmin, kmax = max(-k, 0), min(self.sub_domain.nelz + 1 - k, self.sub_domain.nelz + 1)\n+                    iz = np.arange(kmin, kmax)\n+                    # Coarse node numbers\n+                    nod_c = self.sub_domain.get_nodenumber(*np.meshgrid(ix, iy, iz, indexing='ij')).flatten()\n+                    # Fine node numbers with offset\n+                    ixc, iyc, izc = ix * 2 + i, iy * 2 + j, iz * 2 + k\n+                    nod_f = self.domain.get_nodenumber(*np.meshgrid(ixc, iyc, izc, indexing='ij')).flatten()\n+                    for d in range(ndof):\n+                        rows.append(nod_f * ndof + d)\n+                        cols.append(nod_c * ndof + d)\n+                        vals.append(np.ones_like(rows[-1], dtype=w.dtype) * w[1+i, 1+j, 1+k])\n+\n+        rows = np.concatenate(rows)\n+        cols = np.concatenate(cols)\n+        vals = np.concatenate(vals)\n+        nfine = ndof * self.domain.nnodes\n+        ncoarse = ndof * self.sub_domain.nnodes\n+        self.R = sps.coo_matrix((vals, (rows, cols)), shape=(nfine, ncoarse))\n+        self.R = type(A)(self.R)  # Convert to correct matrix type\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        if trans == 'N':\n+            A = self.A\n+        elif trans == 'T':\n+            A = self.A.T\n+        elif trans == 'H':\n+            A = self.A.conj().T\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+\n+        # Pre-smoothing\n+        if x0 is None:\n+            u_f = self.smoother.solve(rhs, trans=trans)\n+        else:\n+            r = rhs - self.A @ x0\n+            u_f = x0 + self.smoother.solve(r, trans=trans)\n+        for i in range(self.smooth_steps-1):\n+            r = rhs - self.A @ u_f\n+            u_f += self.smoother.solve(r, trans=trans)\n+\n+        r = rhs - A @ u_f\n+        # Restrict residual to coarse level\n+        r_c = self.R.T @ r\n+\n+        # Solve at coarse level\n+        u_c = self.inner_level.solve(r_c)\n+\n+        # Interpolate and correct\n+        u_f += self.R @ u_c\n+\n+        # Post-smoothing\n+        for i in range(self.smooth_steps):\n+            r = rhs - self.A @ u_f\n+            u_f += self.smoother.solve(r, trans=trans)\n+        return u_f\n+\n+\n+def orth(u, normalize=True, zero_rtol=1e-15):\n+    \"\"\" Create orthogonal basis from a set of vectors\n+\n+    Args:\n+        u: Set of vectors of size (#dof, #vectors)\n+        normalize: Also normalize the basis vectors\n+        zero_rtol: Relative tolerance for detection of zero vectors (in case of a rank-deficient basis)\n+\n+    Returns:\n+        v: Orthogonal basis vectors (#dof, #non-zero-vectors)\n+    \"\"\"\n+    if u.ndim == 1:\n+        return u\n+    elif u.ndim > 2:\n+        raise TypeError(\"Only valid for 1D or 2D matrix\")\n+\n+    def dot(a, b):  # Define inner product\n+        return a @ b.conj()\n+\n+    orth_vecs = []\n+    for i in range(u.shape[-1]):\n+        vi = np.copy(u[..., i])\n+        beta_i = dot(vi, vi)\n+        for vj in orth_vecs:\n+            alpha_ij = dot(vi, vj)\n+            alpha_jj = 1.0 if normalize else dot(vj, vj)\n+            vi -= vj * alpha_ij / alpha_jj\n+        beta_i_new = dot(vi, vi)\n+        if beta_i_new / beta_i < zero_rtol:  # Detect zero vector\n+            continue\n+        if normalize:\n+            vi /= np.sqrt(beta_i_new)\n+        orth_vecs.append(vi)\n+    return np.stack(orth_vecs, axis=-1)\n+\n+\n+class CG(LinearSolver):\n+    \"\"\" Preconditioned conjugate gradient method\n+    Works for positive-definite self-adjoint matrices (:math:`A=A^H`)\n+\n+    References:\n+        Ji & Li (2017), A breakdown-free BCG method. DOI 10.1007/s10543-016-0631-z\n+          https://www.cs.odu.edu/~yaohang/portfolio/BIT2017.pdf\n+        Shewchuck (1994), Introduction to CG method without the agonzing pain.\n+          https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n+\n+    Args:\n+        A: The matrix\n+        preconditioner: Preconditioner to use\n+        tol: Convergence tolerance\n+        maxit: Maximum number of iterations\n+        restart: Restart every Nth iteration\n+        verbosity: Log level\n+    \"\"\"\n+    def __init__(self, A=None, preconditioner=Preconditioner(), tol=1e-7, maxit=10000, restart=50, verbosity=0):\n+        self.preconditioner = preconditioner\n+        self.A = A\n+        self.tol = tol\n+        self.maxit = maxit\n+        self.restart = restart\n+        self.verbosity = verbosity\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        tstart = time.perf_counter()\n+        self.A = A\n+        self.preconditioner.update(A)\n+        if self.verbosity >= 1:\n+            print(f\"Preconditioner set up in {np.round(time.perf_counter() - tstart,3)}s\")\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        if trans == 'N':\n+            A = self.A\n+        elif trans == 'T':\n+            A = self.A.T\n+        elif trans == 'H':\n+            A = self.A.conj().T\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+\n+        tstart = time.perf_counter()\n+        if rhs.ndim == 1:\n+            b = rhs.reshape((rhs.size, 1))\n+        else:\n+            b = rhs\n+        x = np.zeros_like(rhs, dtype=np.result_type(rhs, A)) if x0 is None else x0.copy()\n+        if x.ndim == 1:\n+            x = x.reshape((x.size, 1))\n+\n+        r = b - A@x\n+        z = self.preconditioner.solve(r, trans=trans)\n+        p = orth(z, normalize=True)\n+        if self.verbosity >= 2:\n+            print(f\"Initial residual = {np.linalg.norm(r, axis=0) / np.linalg.norm(b, axis=0)}\")\n+\n+        for i in range(self.maxit):\n+            q = A @ p\n+            pq = p.conj().T @ q\n+            pq_inv = np.linalg.inv(pq)\n+            alpha = pq_inv @ (p.conj().T @ r)\n+\n+            x += p @ alpha\n+            if i % 50 == 0:  # Explicit restart\n+                r = b - A@x\n+            else:\n+                r -= q @ alpha\n+\n+            if self.verbosity >= 2:\n+                print(f\"i = {i}, residuals = {np.linalg.norm(r, axis=0) / np.linalg.norm(b, axis=0)}\")\n+\n+            tval = np.linalg.norm(r)/np.linalg.norm(b)\n+            if tval <= self.tol:\n+                break\n+\n+            z = self.preconditioner.solve(r, trans=trans)\n+\n+            beta = -pq_inv @ (q.conj().T @ z)\n+            p = orth(z + p@beta, normalize=False)\n+\n+        if tval > self.tol:\n+            warnings.warn(f'Maximum iterations ({self.maxit}) reached, with final residual {tval}')\n+        elif self.verbosity >= 1:\n+            print(f\"Converged in {i} iterations and {np.round(time.perf_counter() - tstart, 3)}s, with final residuals {np.linalg.norm(r, axis=0) / np.linalg.norm(b, axis=0)}\")\n+\n+        if rhs.ndim == 1:\n+            return x.flatten()\n+        else:\n+            return x\ndiff --git a/pymoto/solvers/matrix_checks.py b/pymoto/solvers/matrix_checks.py\nnew file mode 100644\nindex 0000000..a94aead\n--- /dev/null\n+++ b/pymoto/solvers/matrix_checks.py\n@@ -0,0 +1,56 @@\n+import numpy as np\n+import scipy.sparse as sps\n+try:\n+    import cvxopt\n+    _has_cvxopt = True\n+except ImportError:\n+    _has_cvxopt = False\n+\n+\n+def is_cvxopt_spmatrix(A):\n+    \"\"\" Checks if the argument is a cvxopt sparse matrix \"\"\"\n+    return isinstance(A, cvxopt.spmatrix) if _has_cvxopt else False\n+\n+\n+def matrix_is_complex(A):\n+    \"\"\" Checks if the matrix is complex \"\"\"\n+    if is_cvxopt_spmatrix(A):\n+        return A.typecode == 'z'\n+    else:\n+        return np.iscomplexobj(A)\n+\n+\n+def matrix_is_diagonal(A):\n+    \"\"\" Checks if the matrix is diagonal\"\"\"\n+    if sps.issparse(A):\n+        if isinstance(A, sps.dia_matrix):\n+            return len(A.offsets) == 1 and A.offsets[0] == 0\n+        else:\n+            return np.allclose((A - sps.spdiags(A.diagonal(), 0, *A.shape)).data, 0.0)\n+    elif is_cvxopt_spmatrix(A):\n+        return max(abs(A.I - A.J)) == 0\n+    else:\n+        return np.allclose(A, np.diag(np.diag(A)))\n+\n+\n+def matrix_is_symmetric(A):\n+    \"\"\" Checks whether a matrix is numerically symmetric \"\"\"\n+    if sps.issparse(A):\n+        return np.allclose((A-A.T).data, 0)\n+    elif is_cvxopt_spmatrix(A):\n+        return np.isclose(max(abs(A-A.T)), 0.0)\n+    else:\n+        return np.allclose(A, A.T)\n+\n+\n+def matrix_is_hermitian(A):\n+    \"\"\" Checks whether a matrix is numerically Hermitian \"\"\"\n+    if matrix_is_complex(A):\n+        if sps.issparse(A):\n+            return np.allclose((A-A.T.conj()).data, 0)\n+        elif is_cvxopt_spmatrix(A):\n+            return np.isclose(max(abs(A-A.ctrans())), 0.0)\n+        else:\n+            return np.allclose(A, A.T.conj())\n+    else:\n+        return matrix_is_symmetric(A)\ndiff --git a/pymoto/solvers/solvers.py b/pymoto/solvers/solvers.py\nnew file mode 100644\nindex 0000000..a5b6b64\n--- /dev/null\n+++ b/pymoto/solvers/solvers.py\n@@ -0,0 +1,253 @@\n+import warnings\n+import numpy as np\n+from .matrix_checks import matrix_is_hermitian, matrix_is_symmetric, matrix_is_complex\n+\n+\n+class LinearSolver:\n+    \"\"\" Base class of all linear solvers\n+\n+    Keyword Args:\n+        A (matrix): Optionally provide a matrix, which is used in :method:`update` right away.\n+\n+    Attributes:\n+        defined (bool): Flag if the solver is able to run, e.g. false if some dependent library is not available\n+    \"\"\"\n+\n+    defined = True\n+    _err_msg = \"\"\n+\n+    def __init__(self, A=None):\n+        if A is not None:\n+            self.update(A)\n+\n+    def update(self, A):\n+        \"\"\" Updates with a new matrix of the same structure\n+\n+        Args:\n+            A (matrix): The new matrix of size ``(N, N)``\n+\n+        Returns:\n+            self\n+        \"\"\"\n+        raise NotImplementedError(f\"Solver not implemented {self._err_msg}\")\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}`\n+\n+        Args:\n+            rhs: Right hand side :math:`\\mathbf{b}` of shape ``(N)`` or ``(N, K)`` for multiple right-hand-sides\n+            x0 (optional): Initial guess for the solution\n+            trans (optional): Option to transpose matrix\n+                'N':   A   @ x == rhs   (default)   Normal matrix\n+                'T':   A^T @ x == rhs               Transposed matrix\n+                'H':   A^H @ x == rhs               Hermitian transposed matrix (conjugate transposed)\n+\n+        Returns:\n+            Solution vector :math:`\\mathbf{x}` of same shape as :math:`\\mathbf{b}`\n+        \"\"\"\n+        raise NotImplementedError(f\"Solver not implemented {self._err_msg}\")\n+\n+    @staticmethod\n+    def residual(A, x, b, trans='N'):\n+        r\"\"\" Calculates the (relative) residual of the linear system of equations\n+\n+        The residual is calculated as\n+        :math:`r = \\frac{\\left| \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\right|}{\\left| \\mathbf{b} \\right|}`\n+\n+        Args:\n+            A: The matrix\n+            x: Solution vector\n+            b: Right-hand side\n+            trans (optional): Matrix tranformation (`N` is normal, `T` is transposed, `H` is hermitian transposed)\n+\n+        Returns:\n+            Residual value\n+        \"\"\"\n+        assert x.shape == b.shape\n+        if trans == 'N':\n+            mat = A\n+        elif trans == 'T':\n+            mat = A.T\n+        elif trans == 'H':\n+            mat = A.conj().T\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+        return np.linalg.norm(mat@x - b, axis=0) / np.linalg.norm(b, axis=0)\n+\n+\n+class LDAWrapper(LinearSolver):\n+    r\"\"\" Linear dependency aware solver (LDAS)\n+\n+    This solver uses previous solutions of the system :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` to reduce computational\n+    effort. In case the solution :math:`\\mathbf{x}` is linearly dependent on the previous solutions, the solution\n+    will be nearly free of cost.\n+\n+    Args:\n+        solver: The internal solver to be used\n+        tol (optional): Residual tolerance above which the internal solver is used to add a new solution vector.\n+        A (optional): The matrix :math:`\\mathbf{A}`\n+        symmetric (optional): Flag to indicate a symmetric matrix :math:`A=A^T`\n+        hermitian (optional): Flag to indicate a Hermitian matrix :math:`A=A^H`\n+\n+    References:\n+    Koppen, S., van der Kolk, M., van den Boom, S., & Langelaar, M. (2022).\n+    Efficient computation of states and sensitivities for compound structural optimisation problems using a Linear Dependency Aware Solver (LDAS).\n+    Structural and Multidisciplinary Optimization, 65(9), 273.\n+    DOI: 10.1007/s00158-022-03378-8\n+    \"\"\"\n+    def __init__(self, solver: LinearSolver, tol=1e-7, A=None, symmetric=None, hermitian=None):\n+        self.solver = solver\n+        self.tol = tol\n+        # Storage for solution vectors (solutions of A x = b)\n+        self.x_stored = []\n+        self.b_stored = []\n+        # Storage for adjoint solution vectors (solutions of A^H x = b)\n+        self.xadj_stored = []\n+        self.badj_stored = []\n+        self.A = None\n+        self._did_solve = False  # For debugging purposes\n+        self._last_rtol = 0.\n+        self.hermitian = hermitian\n+        self.symmetric = symmetric\n+        self.complex = None\n+        super().__init__(A)\n+\n+    def update(self, A):\n+        \"\"\" Clear the internal stored solution vectors and update the internal ``solver`` \"\"\"\n+        if self.symmetric is None:\n+            self.symmetric = matrix_is_symmetric(A)\n+\n+        if self.hermitian is None:\n+            if not matrix_is_complex(A):\n+                self.hermitian = self.symmetric\n+            self.hermitian = matrix_is_hermitian(A)\n+\n+        self.A = A\n+        self.x_stored.clear()\n+        self.b_stored.clear()\n+        self.xadj_stored.clear()\n+        self.badj_stored.clear()\n+        self.solver.update(A)\n+\n+    def _do_solve_1rhs(self, A, rhs, x_data, b_data, solve_fn, x0=None):\n+        dtype = np.result_type(A, rhs)\n+        if rhs.ndim == 1:\n+            rhs_loc = np.zeros((rhs.size, 1), dtype=dtype)\n+            rhs_loc[:, 0] = rhs\n+        else:\n+            rhs_loc = np.zeros_like(rhs, dtype=dtype)\n+            rhs_loc[:] = rhs\n+        sol = np.zeros_like(rhs_loc, dtype=dtype)\n+\n+        # Check linear dependencies in the rhs using modified Gram-Schmidt\n+        for (x, b) in zip(x_data, b_data):\n+            assert x.ndim == b.ndim == 1\n+            assert x.size == b.size == rhs_loc.shape[0]\n+            alpha = rhs_loc.T @ b.conj() / (b.conj() @ b)\n+\n+            rem_rhs = alpha * b[:, None]\n+            add_sol = alpha * x[:, None]\n+\n+            if np.iscomplexobj(rem_rhs) and not np.iscomplexobj(rhs_loc):\n+                if np.linalg.norm(np.imag(rem_rhs)) < 1e-10*np.linalg.norm(np.real(rem_rhs)):\n+                    rem_rhs = np.real(rem_rhs)\n+                else:\n+                    warnings.warn('LDAS: Complex vector cannot be subtracted from real rhs')\n+                    continue\n+\n+            if np.iscomplexobj(add_sol) and not np.iscomplexobj(sol):\n+                if np.linalg.norm(np.imag(add_sol)) < 1e-10*np.linalg.norm(np.real(add_sol)):\n+                    add_sol = np.real(add_sol)\n+                else:\n+                    warnings.warn('LDAS: Complex vector cannot be added to real solution')\n+                    continue\n+\n+            rhs_loc -= rem_rhs\n+            sol += add_sol\n+\n+        # Check tolerance\n+        self._last_rtol = np.ones(rhs_loc.shape[-1]) if len(x_data) == 0 else self.residual(A, sol, rhs if rhs.ndim > 1 else rhs.reshape(-1, 1))\n+        self._did_solve = self._last_rtol > self.tol\n+        if np.any(self._did_solve):\n+            if x0 is not None:\n+                if x0.ndim == 1:\n+                    x0_loc = x0.reshape(-1, 1).copy()\n+                else:\n+                    x0_loc = x0[..., self._did_solve].copy()\n+                for x in x_data:\n+                    beta = x0_loc.T @ x.conj() / (x.conj() @ x)\n+                    x0_loc -= beta * x\n+            else:\n+                x0_loc = None\n+\n+            # Calculate a new solution\n+            xnew = solve_fn(rhs_loc[..., self._did_solve], x0_loc)\n+            self.residual(A, xnew, rhs_loc[..., self._did_solve])\n+            sol[..., self._did_solve] += xnew\n+\n+            # Add to database\n+            for i in range(xnew.shape[-1]):\n+                # Remove all previous components that are already in the database (orthogonalize)\n+                xadd = xnew[..., i]\n+                badd = A @ xadd\n+                for x, b in zip(x_data, b_data):\n+                    beta = badd @ b.conj() / (b.conj() @ b)\n+                    badd -= beta * b\n+                    xadd -= beta * x\n+                bnrm = np.linalg.norm(badd)\n+                if not np.isfinite(bnrm) or bnrm == 0:\n+                    continue\n+                badd /= bnrm\n+                xadd /= bnrm\n+                x_data.append(xadd)\n+                b_data.append(badd)\n+\n+        if rhs.ndim == 1:\n+            return sol.flatten()\n+        else:\n+            return sol\n+\n+    def solve(self, rhs, x0=None, trans='N'):\n+        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by performing a modified\n+        Gram-Schmidt over the previously calculated solutions :math:`\\mathbf{U}` and corresponding right-hand-sides\n+        :math:`\\mathbf{F}`. This is used to construct an approximate solution\n+        :math:`\\tilde{\\mathbf{x}} = \\sum_k \\alpha_k \\mathbf{u}_k` in the subspace of :math:`\\mathbf{U}`.\n+        If the residual of :math:`\\mathbf{A} \\tilde{\\mathbf{x}} = \\mathbf{b}` is above the tolerance, a new solution\n+        :math:`\\mathbf{u}_{k+1}` will be added to the database such that\n+        :math:`\\mathbf{x} = \\tilde{\\mathbf{x}}+\\mathbf{u}_{k+1}` is the solution to the system\n+        :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}`.\n+\n+        The right-hand-side :math:`\\mathbf{b}` can be of size ``(N)`` or ``(N, K)``, where ``N`` is the size of matrix\n+        :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n+        \"\"\"\n+        ''' Required    Symm. matrix    Herm. matrix    Any matrix (uses adjoint storage)\n+                        A^T = A         A^H = A\n+            A x = b\n+            A^T x = b   A x = b         A x^* = b^*     A^H x^* = b^*\n+            A^H x = b   A x^* = b^*     A x = b         A^H x = b\n+            \n+            For symmetric or Hermitian matrices, only the normal storage is required. For any other matrix, the `T` and \n+            `H` mode will require the adjoint storage space.\n+        '''\n+        if trans not in ['N', 'T', 'H']:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+\n+        # Use adjoint storage?\n+        adjoint_mode = trans != 'N' and not (self.symmetric or self.hermitian)\n+        # Use conjugation?\n+        conj_mode = self.symmetric and trans == 'H' or not self.symmetric and trans == 'T'\n+\n+        storage = 'H' if adjoint_mode else 'N'\n+        rhs = rhs.conj() if conj_mode else rhs\n+        if storage == 'N':  # Use the normal storage\n+            ret = self._do_solve_1rhs(self.A, rhs,\n+                                      self.x_stored, self.b_stored,\n+                                      lambda b, x_init: self.solver.solve(b, trans='N', x0=x_init),\n+                                      x0=x0)\n+        elif storage == 'H':  # Use adjoint storage\n+            ret = self._do_solve_1rhs(self.A.conj().T, rhs,\n+                                      self.xadj_stored, self.badj_stored,\n+                                      lambda b, x_init: self.solver.solve(b, trans='H', x0=x_init),\n+                                      x0=x0)\n+\n+        return ret.conj() if conj_mode else ret\ndiff --git a/pymoto/common/solvers_sparse.py b/pymoto/solvers/sparse.py\nsimilarity index 86%\nrename from pymoto/common/solvers_sparse.py\nrename to pymoto/solvers/sparse.py\nindex 0fc56a1..e4c48a1 100644\n--- a/pymoto/common/solvers_sparse.py\n+++ b/pymoto/solvers/sparse.py\n@@ -2,7 +2,8 @@\n import numpy as np\n import scipy.sparse as sps\n from scipy.sparse import SparseEfficiencyWarning\n-from .solvers import matrix_is_hermitian, matrix_is_complex, matrix_is_symmetric, LinearSolver\n+from .matrix_checks import matrix_is_hermitian, matrix_is_complex, matrix_is_symmetric\n+from .solvers import LinearSolver\n \n # ------------------------------------ Pardiso Solver -----------------------------------\n try:\n@@ -126,26 +127,31 @@ def update(self, A):\n \n         self._pardiso_solver.factorize(A)\n \n-    def solve(self, b):\n+    def solve(self, b, x0=None, trans='N'):\n         \"\"\" solve Ax=b for x\n \n         Args:\n             A (scipy.sparse.csr.csr_matrix): sparse square CSR matrix , CSC matrix also possible\n             b (numpy.ndarray): right-hand side(s), b.shape[0] needs to be the same as A.shape[0]\n+            x0 (unused)\n+            trans (optional): Indicate which system to solve (Normal, Transposed, or Hermitian transposed)\n \n         Returns:\n             Solution of the system of linear equations, same shape as input b\n         \"\"\"\n-        return self._pardiso_solver.solve(self.A, b)\n-\n-    def adjoint(self, b):\n-        # Cannot use _pardiso_solver.solve because it changes flag 12 internally\n-        iparm_prev = self._pardiso_solver.get_iparm(12)\n-        self._pardiso_solver.set_iparm(12, int(not iparm_prev))  # Adjoint solver (transpose)\n-        b = self._pardiso_solver._check_b(self.A, b)\n-        x = self._pardiso_solver._call_pardiso(self.A, b)\n-        self._pardiso_solver.set_iparm(12, iparm_prev)  # Revert back to normal solver\n-        return x\n+        if trans == 'N':\n+            return self._pardiso_solver.solve(self.A, b)\n+        elif trans == 'T' or trans == 'H':\n+            # T and H are the same, because only real matrix is supported\n+            # Cannot use _pardiso_solver.solve because it changes flag 12 internally\n+            iparm_prev = self._pardiso_solver.get_iparm(12)\n+            self._pardiso_solver.set_iparm(12, int(not iparm_prev))  # Adjoint solver (transpose)\n+            b = self._pardiso_solver._check_b(self.A, b)\n+            x = self._pardiso_solver._call_pardiso(self.A, b)\n+            self._pardiso_solver.set_iparm(12, iparm_prev)  # Revert back to normal solver\n+            return x\n+        else:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n \n     def _print_iparm(self):\n         \"\"\" Print all iparm settings to console \"\"\"\n@@ -219,17 +225,16 @@ def update(self, A):\n         self.inv = splu(A)\n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n         substitution of :math:`\\mathbf{x} = \\mathbf{U}^{-1}\\mathbf{L}^{-1}\\mathbf{b}`.\n-        \"\"\"\n-        return self.inv.solve(rhs)\n \n-    def adjoint(self, rhs):\n-        r\"\"\" Solves the linear system of equations :math:`\\mathbf{A}^\\text{H}\\mathbf{x} = \\mathbf{b}` by forward and\n-        backward substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{U}^{-\\text{H}}\\mathbf{b}`.\n+        Adjoint system solves the linear system of equations :math:`\\mathbf{A}^\\text{H}\\mathbf{x} = \\mathbf{b}` by\n+        forward and backward substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{U}^{-\\text{H}}\\mathbf{b}`\n         \"\"\"\n-        return self.inv.solve(rhs, trans=('H' if self.iscomplex else 'T'))\n+        if trans not in ['N', 'T', 'H']:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+        return self.inv.solve(rhs, trans=trans)\n \n \n # ------------------------------------ Cholesky Solver scikit-sparse -----------------------------------\n@@ -281,7 +286,7 @@ def update(self, A):\n \n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n         substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{L}^{-1}\\mathbf{b}` in case of an\n         Hermitian matrix.\n@@ -290,10 +295,12 @@ def solve(self, rhs):\n         :math:`\\mathbf{A}` and ``K`` is the number of right-hand sides.\n \n         \"\"\"\n-        return self.inv(rhs)\n-\n-    def adjoint(self, rhs):\n-        return self.solve(rhs)\n+        if trans not in ['N', 'T', 'H']:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n+        if trans == 'T':\n+            return self.inv(rhs.conj()).conj()\n+        else:\n+            return self.inv(rhs)\n \n \n # ------------------------------------ Cholesky Solver cvxopt -----------------------------------\n@@ -344,15 +351,20 @@ def update(self, A):\n \n         return self\n \n-    def solve(self, rhs):\n+    def solve(self, rhs, x0=None, trans='N'):\n         r\"\"\" Solves the linear system of equations :math:`\\mathbf{A} \\mathbf{x} = \\mathbf{b}` by forward and backward\n         substitution of :math:`\\mathbf{x} = \\mathbf{L}^{-\\text{H}}\\mathbf{L}^{-1}\\mathbf{b}`. \"\"\"\n+        if trans not in ['N', 'T', 'H']:\n+            raise TypeError(\"Only N, T, or H transposition is possible\")\n         if rhs.dtype != self._dtype:\n             warnings.warn(f\"{type(self).__name__}: Type warning: rhs value type ({rhs.dtype}) is converted to {self._dtype}\")\n-        B = cvxopt.matrix(rhs.astype(self._dtype))\n+        if trans == 'T':\n+            B = cvxopt.matrix(rhs.conj().astype(self._dtype))\n+        else:\n+            B = cvxopt.matrix(rhs.astype(self._dtype))\n         nrhs = 1 if rhs.ndim == 1 else rhs.shape[1]\n+\n         cvxopt.cholmod.solve(self.inv, B, nrhs=nrhs)\n-        return np.array(B).flatten() if nrhs == 1 else np.array(B)\n \n-    def adjoint(self, rhs):\n-        return self.solve(rhs)\n+        x = np.array(B).flatten() if rhs.ndim == 1 else np.array(B)\n+        return x.conj() if trans == 'T' else x\n", "instance_id": "aatmdelissen__pyMOTO-19", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement for the convolution filter is mostly clear, providing a valid goal of implementing a faster alternative to a matrix-based density filter using convolution and FFT algorithms. It includes specific performance comparisons (e.g., setup and execution costs for a 3D domain) and mentions flexibility in boundary conditions, which adds to the clarity of intent. However, there are minor ambiguities and missing critical details. For instance, the problem statement does not explicitly define the expected input and output formats for the filter, nor does it specify how the convolution should handle different data types or domain sizes beyond the example given. Additionally, while boundary conditions are mentioned, the exact requirements or constraints for implementing different padding behaviors are not detailed in the statement itself (though partially addressed in code changes). Edge cases, such as very small domains or non-uniform grids, are not discussed. Thus, while the overall intent and benefits are clear, these missing details prevent a comprehensive score of 3.", "difficulty_explanation": "The difficulty of this problem is rated at 0.65, placing it in the 'Hard' category (0.6-0.8). This assessment is based on several factors. First, the scope of code changes is significant, spanning multiple files and introducing new modules (e.g., `FilterConv` with padding support, multigrid solver in `ex_compliance_multigrid.py`, and stress calculations in `ex_volume_stress.py`). These changes require understanding interactions between different parts of the codebase, such as domain definitions, signal processing, and optimization workflows in the `pymoto` library. Second, the technical concepts involved are moderately complex, including FFT-based convolution, boundary condition handling (padding strategies), and integration with existing numerical methods like topology optimization. Implementing these requires knowledge of numerical algorithms, sparse matrix operations, and Python libraries like SciPy. Third, the problem impacts the system's architecture by replacing or augmenting core filtering mechanisms, which could affect performance and correctness across various examples and use cases. Finally, while some edge cases (e.g., boundary padding) are addressed in the code, others (e.g., handling of very small domains or numerical stability with large filter radii) may require additional consideration, adding to the complexity. This combination of deep codebase understanding, complex modifications, and potential performance considerations justifies a difficulty score of 0.65, reflecting a challenging task that demands substantial expertise but does not reach the extreme complexity of system-level redesign or highly specialized domain knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for linting ETSI website authentication certificates\nLinting to be supported with both a new CLI tool as well as REST API.\r\n\r\nAdd usage documentation to README as well as additional documentation on the Wiki.\r\n\nAdd opt-in support to use pyasn1-fasder for decoding DER\nIf [pyasn1-fasder](https://github.com/CBonnell/pyasn1-fasder) is installed, then use its DER decoder instead of the default DER decoder. This is currently opt-in, as some more testing (namely longevity) needs to be performed to ensure there are no memory or resource leaks. That being said, initial mass-linting results are promising.\r\n\r\nImprovements when installed:\r\n- Linting is 30-40% faster. No more need to re-encode a structure into DER to ensure it was originally DER encoded\r\n- \"DER-Ambiguous\" ASN.1 grammars can be parsed (e.g., empty revocation lists in CRLs no longer result in a DER decode error)\r\n- Eliminates need for ASN.1 encoding specific validators such as PrintableStringConstraintValidator, as such validation is done by the decoder function.\r\n\r\nIf further testing reveals that there are no latent issues, the package will be promoted to a dependency.\nRelax format for ISO 3166-2 state/province identifiers\nNow that SMC-03 has passed, the required format for state/provinces in organizationIdentifier values has been relaxed. Currently, the linter requires exactly 2 letters; the ballot relaxes this to 1-3 alphanumeric characters. This should be relaxed to match the updated language.\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex d2aee62..84f25b5 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -130,3 +130,7 @@ dmypy.json\n \n .vscode/\n .idea/\n+\n+# Apple-specific\n+.DS_Store\n+\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex f83de6d..5dbc38a 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,13 @@\n \n All notable changes to this project from version 0.9.3 onwards are documented in this file.\n \n+## 0.11.0 - 2024-06-14\n+\n+### New features/enhancements\n+\n+- Add support for linting ETSI website authentication certificates (#80)\n+- Add opt-in support for using [pyasn1-fasder](https://github.com/CBonnell/pyasn1-fasder) to decode DER (#81)\n+\n ## 0.10.3 - 2024-05-13\n \n ### New features/enhancements\ndiff --git a/README.md b/README.md\nindex a1bd820..e6948f1 100644\n--- a/README.md\n+++ b/README.md\n@@ -143,6 +143,7 @@ The list of command line linters bundled with pkilint:\n * [lint_pkix_cert](#lintpkixcert)\n * [lint_cabf_smime_cert](#lintcabfsmimecert)\n * [lint_cabf_serverauth_cert](#lintcabfserverauthcert)\n+* [lint_etsi_cert](#lintetsicert)\n * [lint_crl](#lintcrl)\n * [lint_ocsp_response](#lintocspresponse)\n * [lint_pkix_signer_signee_cert_chain](#lintpkixsignersigneecertchain)\n@@ -302,6 +303,77 @@ $ lint_cabf_serverauth_cert lint -d dv_final_clean.pem\n $\n ```\n \n+### lint_etsi_cert\n+\n+For further information on this linter, see [the Wiki page](https://github.com/digicert/pkilint/wiki/lint_etsi_cert).\n+\n+This tool lints certificates against the profiles specified in ETSI EN 319 412 and TS 119 495. Currently, the tool\n+has the most comprehensive support for website authentication certificates, but support for electronic signature,\n+electronic seal, and timestamping certificates is planned.\n+\n+The `lint` sub-command requires that the user provide the certificate type/profile of the certificate so that the appropriate\n+validations are performed. There are two options:\n+\n+1. Explicitly specify the type of certificate using the `-t`/`--type` option.\n+2. Have the linter detect the type of certificate using the `-d`/`--detect` option. In this case, the linter will determine the certificate type using the values of various extensions and fields included in the certificate. The detection procedure may not always be accurate, so it is recommended to use the `--type` option for the best results.\n+\n+Several parts of EN 319 412 and TS 119 495 supersede requirements specified in the TLS Baseline Requirements and RFC 5280. For example, the TLS Baseline Requirements requires that certificate validity periods be 398 days or less. However, this requirement need not be followed for PSD2 website authentication certificates that are not trusted\n+by browsers. By default, such findings are not reported. To report superseded findings, specify the `--report-all` option.\n+\n+The `-o`/`--output` option is used to specify that the certificate type used by the linter is written to standard error. This is useful when using the `--detect` option to see which certificate type was determined by the heuristics logic.\n+\n+#### Example command execution\n+\n+```shell\n+$ echo '-----BEGIN CERTIFICATE-----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\n++9Ru72tw8lf1RlWwp5ZI+7CByD0W\n+-----END CERTIFICATE-----' > qncp_w_gen.pem\n+\n+$ lint_etsi_cert lint -d qncp_w_gen.pem\n+SubjectKeyIdentifierValidator @ certificate.tbsCertificate.extensions.6.extnValue.subjectKeyIdentifier\n+    pkix.subject_key_identifier_method_1_identified (INFO)\n+\n+$\n+```\n+\n ### lint_crl\n \n This tool lints CRLs against the RFC 5280 as well as against the CA/Browser Forum profile for CRLs. It is anticipated that this\n@@ -353,10 +425,12 @@ pkilint is built on several open source packages. In particular, these packages\n | cryptography       | Apache Software License; BSD License | The Python Cryptographic Authority and individual contributors | https://github.com/pyca/cryptography              |\n | fastapi            | MIT License                          | Sebasti\u00e1n Ram\u00edrez                                              | https://github.com/tiangolo/fastapi               |\n | iso3166            | MIT License                          | Mike Spindel                                                   | http://github.com/deactivated/python-iso3166      |\n+| iso4217            | Public Domain                        | Hong Minhee                                                    | https://github.com/dahlia/iso4217                 |\n | publicsuffixlist   | Mozilla Public License 2.0 (MPL 2.0) | ko-zu                                                          | https://github.com/ko-zu/psl                      |\n | pyasn1             | BSD License                          | Christian Heimes and Simon Pichugin                            | https://github.com/pyasn1/pyasn1                  |\n | pyasn1-alt-modules | BSD License                          | Russ Housley                                                   | https://github.com/russhousley/pyasn1-alt-modules |\n | python-dateutil    | Apache Software License; BSD License | Gustavo Niemeyer                                               | https://github.com/dateutil/dateutil              |\n+| python-iso639      | Apache Software License              | Jackson L. Lee                                                 | https://github.com/jacksonllee/iso639             |\n | validators         | MIT License                          | Konsta Vesterinen                                              | https://github.com/kvesteri/validators            |\n \n The pkilint maintainers are grateful to the authors of these open source contributions.\ndiff --git a/VERSION.txt b/VERSION.txt\nindex a3f5a8e..142464b 100644\n--- a/VERSION.txt\n+++ b/VERSION.txt\n@@ -1,1 +1,1 @@\n-0.10.3\n+0.11.0\n\\ No newline at end of file\ndiff --git a/pkilint/bin/lint_etsi_cert.py b/pkilint/bin/lint_etsi_cert.py\nnew file mode 100644\nindex 0000000..3fa6ec3\n--- /dev/null\n+++ b/pkilint/bin/lint_etsi_cert.py\n@@ -0,0 +1,105 @@\n+#!/usr/bin/env python\n+\n+import argparse\n+import sys\n+\n+from pkilint import etsi\n+from pkilint import loader, report, util, finding_filter\n+from pkilint.etsi import etsi_constants\n+from pkilint.pkix import certificate\n+\n+_CERTIFICATE_TYPE_OPTIONS = [str(t).replace('_', '-') for t in etsi_constants.CertificateType]\n+\n+\n+class EtsiCertificateTypeAction(argparse.Action):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+    def __call__(self, parser, namespace, values, option_string=None):\n+        cert_type = etsi_constants.CertificateType.from_option_str(values)\n+\n+        setattr(namespace, self.dest, cert_type)\n+\n+\n+def main(cli_args=None) -> int:\n+    parser = argparse.ArgumentParser(\n+        description=f'ETSI v{etsi_constants.EN_319_412_VERSION} Certificate Linter'\n+    )\n+\n+    subparsers = parser.add_subparsers(dest='command', required=True)\n+\n+    validations_parser = subparsers.add_parser('validations',\n+                                               help='Output the set of validations which this linter performs')\n+    validations_parser.add_argument('-t', '--type', required=True,\n+                                    type=str.upper,\n+                                    action=EtsiCertificateTypeAction,\n+                                    help='The type of certificate',\n+                                    choices=_CERTIFICATE_TYPE_OPTIONS)\n+\n+    lint_parser = subparsers.add_parser('lint', help='Lint the specified certificate')\n+\n+    detect_options_group = lint_parser.add_mutually_exclusive_group(required=True)\n+    detect_options_group.add_argument('-d', '--detect', action='store_true',\n+                                      help='Detect the type of certificate from reserved CA/B Forum policy '\n+                                           'OIDs and qualified certificate statements.')\n+    detect_options_group.add_argument('-t', '--type',\n+                                      type=str.upper,\n+                                      action=EtsiCertificateTypeAction,\n+                                      help='The type of certificate',\n+                                      choices=_CERTIFICATE_TYPE_OPTIONS)\n+    lint_parser.add_argument('-o', '--output', action='store_true',\n+                             help='Output the type of certificate to standard error. This option may be '\n+                                  'useful when using the --detect option.')\n+    lint_parser.add_argument('-r', '--report-all', action='store_true', help='Report all findings without filtering '\n+                             'any findings that are superseded by other requirements')\n+\n+    util.add_standard_args(lint_parser)\n+    lint_parser.add_argument('file', type=argparse.FileType('rb'),\n+                             help='The certificate to lint'\n+                             )\n+\n+    args = parser.parse_args(cli_args)\n+\n+    if args.command == 'validations':\n+        doc_validator = certificate.create_pkix_certificate_validator_container(\n+            etsi.create_decoding_validators(args.type),\n+            etsi.create_validators(args.type)\n+        )\n+\n+        print(report.report_included_validations(doc_validator))\n+\n+        return 0\n+    else:\n+        try:\n+            cert = loader.load_certificate(args.file, args.file.name)\n+        except ValueError as e:\n+            print(f'Failed to load certificate: {e}', file=sys.stderr)\n+            return 1\n+\n+        if args.type:\n+            certificate_type = args.type\n+        else:\n+            certificate_type = etsi.determine_certificate_type(cert)\n+\n+        if args.output:\n+            print(certificate_type.to_option_str, file=sys.stderr)\n+\n+        doc_validator = certificate.create_pkix_certificate_validator_container(\n+            etsi.create_decoding_validators(certificate_type),\n+            etsi.create_validators(certificate_type)\n+        )\n+\n+        results = doc_validator.validate(cert.root)\n+\n+        if not args.report_all:\n+            results, _ = finding_filter.filter_results(\n+                etsi.create_etsi_finding_filters(certificate_type), results\n+            )\n+\n+        print(args.format(results, args.severity))\n+\n+        return util.clamp_exit_code(report.get_findings_count(results, args.severity))\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\ndiff --git a/pkilint/cabf/cabf_ca.py b/pkilint/cabf/cabf_ca.py\nindex d9d3b07..7834c2d 100644\n--- a/pkilint/cabf/cabf_ca.py\n+++ b/pkilint/cabf/cabf_ca.py\n@@ -2,6 +2,7 @@\n \n from pkilint import validation\n from pkilint.itu import bitstring\n+from pkilint.pkix.certificate.certificate_extension import KeyUsageBitName\n \n \n # BR 7.1.2.10.7\n@@ -23,7 +24,7 @@ class CaKeyUsageValidator(validation.Validator):\n \n     _PROHIBITED_KUS = {\n         str(n) for n in rfc5280.KeyUsage.namedValues\n-    } - {'digitalSignature', 'keyCertSign', 'cRLSign'}\n+    } - {KeyUsageBitName.DIGITAL_SIGNATURE, KeyUsageBitName.KEY_CERT_SIGN, KeyUsageBitName.CRL_SIGN}\n \n     def __init__(self):\n         super().__init__(\n@@ -36,12 +37,12 @@ def __init__(self):\n         )\n \n     def validate(self, node):\n-        if not bitstring.has_named_bit(node, 'keyCertSign'):\n+        if not bitstring.has_named_bit(node, KeyUsageBitName.KEY_CERT_SIGN):\n             raise validation.ValidationFindingEncountered(\n                 self.VALIDATION_CA_CERT_REQUIRED_BIT_MISSING,\n                 'keyCertSign not asserted'\n             )\n-        if not bitstring.has_named_bit(node, 'cRLSign'):\n+        if not bitstring.has_named_bit(node, KeyUsageBitName.CRL_SIGN):\n             raise validation.ValidationFindingEncountered(\n                 self.VALIDATION_CA_CERT_REQUIRED_BIT_MISSING,\n                 'cRLSign not asserted'\n@@ -55,7 +56,7 @@ def validate(self, node):\n                 f'Prohibited KUs present: {prohibited_kus_str}'\n             )\n \n-        if not bitstring.has_named_bit(node, 'digitalSignature'):\n+        if not bitstring.has_named_bit(node, KeyUsageBitName.DIGITAL_SIGNATURE):\n             raise validation.ValidationFindingEncountered(self.VALIDATION_CA_CERT_NO_DIG_SIG)\n \n \ndiff --git a/pkilint/cabf/cabf_constants.py b/pkilint/cabf/cabf_constants.py\ndeleted file mode 100644\nindex 236bc60..0000000\n--- a/pkilint/cabf/cabf_constants.py\n+++ /dev/null\n@@ -1,22 +0,0 @@\n-import enum\n-import typing\n-\n-\n-@enum.unique\n-class RegistrationSchemeCountryIdentifierType(enum.IntEnum):\n-    NONE = 0\n-    XG = 1\n-    ISO3166 = 2\n-\n-\n-class RegistrationSchemeNamingConvention(typing.NamedTuple):\n-    country_identifier_type: RegistrationSchemeCountryIdentifierType\n-    allow_state_province: bool\n-    require_registration_reference: bool\n-\n-\n-REGISTRATION_SCHEMES = {\n-    'NTR': RegistrationSchemeNamingConvention(RegistrationSchemeCountryIdentifierType.ISO3166, True, True),\n-    'VAT': RegistrationSchemeNamingConvention(RegistrationSchemeCountryIdentifierType.ISO3166, False, True),\n-    'PSD': RegistrationSchemeNamingConvention(RegistrationSchemeCountryIdentifierType.ISO3166, False, True),\n-}\ndiff --git a/pkilint/cabf/cabf_name.py b/pkilint/cabf/cabf_name.py\nindex 37d09ba..3d5fc9a 100644\n--- a/pkilint/cabf/cabf_name.py\n+++ b/pkilint/cabf/cabf_name.py\n@@ -1,18 +1,13 @@\n-import ipaddress\n-import re\n import typing\n-from urllib.parse import urlparse\n \n-import publicsuffixlist\n-import unicodedata\n from iso3166 import countries_by_alpha2\n-from pyasn1_alt_modules import rfc5280, rfc8398\n+from pyasn1_alt_modules import rfc5280\n \n-from pkilint import validation\n-from pkilint.cabf import cabf_constants\n-from pkilint.cabf.cabf_constants import REGISTRATION_SCHEMES\n+from pkilint import validation, document\n+from pkilint.common import organization_id\n+from pkilint.common.organization_id import ParsedOrganizationIdentifier\n from pkilint.itu import x520_name\n-from pkilint.pkix import general_name\n+from pkilint.pkix import Rfc2119Word\n \n \n class ValidCountryCodeValidatorBase(validation.TypeMatchingValidator):\n@@ -48,138 +43,135 @@ def __init__(self):\n                          )\n \n \n-ORG_ID_REGEX = re.compile(\n-    r'^(?P<scheme>[A-Z]{3})(?P<country>[a-zA-Z]{2})?(\\+(?P<sp>[a-zA-Z0-9]{1,3}))?'\n-    r'(-(?P<reference>.+))?$'\n-)\n-\n-\n-class OrganizationIdentifierAttributeValidator(validation.TypeMatchingValidator):\n-    VALIDATION_ORGANIZATION_ID_INVALID_ENCODING = validation.ValidationFinding(\n+class CabfOrganizationIdentifierValidatorBase(organization_id.OrganizationIdentifierValidatorBase):\n+    VALIDATION_ORGANIZATION_ID_INVALID_SCHEME = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_encoding'\n+        'cabf.invalid_organization_identifier_registration_scheme'\n     )\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_FORMAT = validation.ValidationFinding(\n+    VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_format'\n+        'cabf.invalid_organization_identifier_country'\n     )\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_SCHEME = validation.ValidationFinding(\n+    # the attribute name for this finding is prefixed with an underscore so it's not flagged by the \"validation report\"\n+    # test\n+    _VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_registration_scheme'\n+        'cabf.invalid_organization_identifier_state_province_format'\n     )\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_country'\n-    )\n+    REFERENCE_REQUIRED = (Rfc2119Word.MUST, 'cabf.organization_identifier_reference_missing_for_scheme')\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_SP = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_state_province_for_scheme'\n+    STATE_PROVINCE_PROHIBITED = (Rfc2119Word.MUST_NOT,\n+                                 'cabf.invalid_subject_organization_identifier_state_province_for_scheme')\n+\n+    _NTR_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_COUNTRY_CODES,\n+                       VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=(Rfc2119Word.MAY, None),\n+        reference=REFERENCE_REQUIRED\n     )\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.invalid_subject_organization_identifier_state_province_format'\n+    _VAT_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_COUNTRY_CODES | {organization_id.COUNTRY_CODE_GREECE_TRADITIONAL,\n+                                                                  organization_id.COUNTRY_CODE_NORTHERN_IRELAND},\n+                       VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=STATE_PROVINCE_PROHIBITED,\n+        reference=REFERENCE_REQUIRED\n     )\n \n-    _ISO3166_AND_ARTICLE_215_COUNTRY_CODES = set(countries_by_alpha2.keys()) | {'EL', 'XI'}\n+    _PSD_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_COUNTRY_CODES,\n+                       VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=STATE_PROVINCE_PROHIBITED,\n+        reference=REFERENCE_REQUIRED\n+    )\n \n-    def __init__(\n-            self,\n-            relax_stateprovince_syntax=False,\n-            additional_schemes: typing.Optional[\n-                typing.Mapping[str, cabf_constants.RegistrationSchemeNamingConvention]] = None\n-    ):\n-        super().__init__(type_oid=x520_name.id_at_organizationIdentifier,\n-                         type_path='type', value_path='value.x520OrganizationIdentifier',\n-                         pdu_class=rfc5280.AttributeTypeAndValue,\n-                         validations=[\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_ENCODING,\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_FORMAT,\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME,\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY,\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_SP,\n-                             self.VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT,\n-                         ]\n+    _ALLOWED_SCHEME_MAPPINGS = {\n+        'NTR': _NTR_SCHEME,\n+        'VAT': _VAT_SCHEME,\n+        'PSD': _PSD_SCHEME,\n+    }\n+\n+    def __init__(self,\n+                 invalid_format_validation: typing.Optional[validation.ValidationFinding],\n+                 additional_schemes: typing.Optional[\n+                     typing.Mapping[str, organization_id.OrganizationIdentifierElementAllowance]\n+                 ] = None,\n+                 enforce_strict_state_province_format=True,\n+                 additional_validations=None,\n+                 **kwargs):\n+        self._allowed_schemes = self._ALLOWED_SCHEME_MAPPINGS.copy()\n+\n+        if additional_schemes:\n+            self._allowed_schemes.update(additional_schemes)\n+\n+        self._enforce_strict_state_province_format = enforce_strict_state_province_format\n+\n+        additional_validations = [] if additional_validations is None else additional_validations.copy()\n+        additional_validations.append(self.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME)\n+\n+        if self._enforce_strict_state_province_format:\n+            additional_validations.append(self._VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT)\n+\n+        super().__init__(self._allowed_schemes,\n+                         invalid_format_validation,\n+                         additional_validations=additional_validations,\n+                         **kwargs\n                          )\n \n-        if additional_schemes is None:\n-            additional_schemes = {}\n-        self._allowed_schemes = {**REGISTRATION_SCHEMES, **additional_schemes}\n-        self._relax_stateprovince_syntax = relax_stateprovince_syntax\n+    @classmethod\n+    def handle_unknown_scheme(cls, node: document.PDUNode, parsed: ParsedOrganizationIdentifier):\n+        raise validation.ValidationFindingEncountered(\n+            cls.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME,\n+            f'Invalid registration scheme: \"{parsed.scheme}\"'\n+        )\n \n-    def validate_with_value(self, node, choice_node):\n-        name, value_node = choice_node.child\n+    def validate_with_parsed_value(self, node, parsed):\n+        if self._enforce_strict_state_province_format and parsed.state_province is not None:\n+            if len(parsed.state_province) != 2 or not parsed.state_province.isalpha():\n+                raise validation.ValidationFindingEncountered(\n+                    self._VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT,\n+                    f'State/province \"{parsed.state_province}\" is not two letters (will be fixed in erratum ballot)'\n+                )\n \n-        if name not in {'utf8String', 'printableString'}:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_ENCODING,\n-                f'Invalid ASN.1 encoding: {name}'\n-            )\n+        return super().validate_with_parsed_value(node, parsed)\n \n-        m = ORG_ID_REGEX.match(str(value_node.pdu))\n \n-        if m is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_FORMAT,\n-                f'Invalid format: \"{value_node.pdu}\"'\n-            )\n-\n-        scheme_info = self._allowed_schemes.get(m['scheme'])\n-\n-        if scheme_info is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME,\n-                f'Invalid registration scheme: \"{m[\"scheme\"]}\"'\n-            )\n+class CabfOrganizationIdentifierAttributeValidator(CabfOrganizationIdentifierValidatorBase):\n+    VALIDATION_ORGANIZATION_ID_INVALID_ENCODING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'cabf.invalid_subject_organization_identifier_encoding'\n+    )\n \n-        if scheme_info.require_registration_reference and m['reference'] is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_FORMAT,\n-                f'Missing Registration Reference: \"{value_node.pdu}\"'\n-            )\n-        elif not scheme_info.require_registration_reference and m['reference']:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_FORMAT,\n-                f'Prohibited Registration Reference is present: \"{value_node.pdu}\"'\n-            )\n+    VALIDATION_ORGANIZATION_ID_INVALID_FORMAT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'cabf.invalid_subject_organization_identifier_format'\n+    )\n \n-        country_code = '' if m['country'] is None else m['country'].upper()\n-\n-        if scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.NONE:\n-            valid_country_code = (country_code == '')\n-        elif scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.XG:\n-            valid_country_code = (country_code == 'XG')\n-        elif scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.ISO3166:\n-            if m['scheme'] == 'VAT':\n-                valid_country_code = (country_code in self._ISO3166_AND_ARTICLE_215_COUNTRY_CODES)\n-            else:\n-                valid_country_code = (country_code in countries_by_alpha2)\n-        else:\n-            raise ValueError(f'Unknown country identifier type for scheme \"{m[\"scheme\"]}\": '\n-                             f'{scheme_info.country_identifier_type}')\n-\n-        if not valid_country_code:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY,\n-                f'Invalid country code for scheme \"{m[\"scheme\"]}\": \"{country_code}\"'\n-            )\n+    def __init__(self,\n+                 additional_schemes: typing.Optional[\n+                     typing.Mapping[str, organization_id.OrganizationIdentifierElementAllowance]\n+                 ] = None,\n+                 enforce_strict_state_province_format=True):\n+        super().__init__(self.VALIDATION_ORGANIZATION_ID_INVALID_FORMAT,\n+                         additional_schemes,\n+                         enforce_strict_state_province_format,\n+                         [self.VALIDATION_ORGANIZATION_ID_INVALID_ENCODING],\n+                         pdu_class=x520_name.X520OrganizationIdentifier)\n+\n+    @classmethod\n+    def parse_organization_id_node(cls, node) -> organization_id.ParsedOrganizationIdentifier:\n+        name, value_node = node.child\n \n-        if m['sp'] is not None and not scheme_info.allow_state_province:\n+        if name not in {'utf8String', 'printableString'}:\n             raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SP,\n-                f'Scheme \"{m[\"scheme\"]}\" does not allow state/province values'\n+                cls.VALIDATION_ORGANIZATION_ID_INVALID_ENCODING,\n+                f'Invalid ASN.1 encoding: {name}'\n             )\n \n-        if m['sp'] is not None and not self._relax_stateprovince_syntax and not (\n-                len(m['sp']) == 2 and m['sp'].isalpha()):\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SP_FORMAT,\n-                f'State/province \"{m[\"sp\"]}\" is not two letters (will be fixed in erratum ballot)'\n-            )\n+        return organization_id.parse_organization_identifier(str(value_node.pdu))\n \n \n class RelativeDistinguishedNameContainsOneElementValidator(validation.Validator):\n@@ -199,173 +191,13 @@ def validate(self, node):\n             raise validation.ValidationFindingEncountered(self.VALIDATION_RDN_CONTAINS_MULTIPLE_ATVS)\n \n \n-class InternalDomainNameValidator(validation.Validator):\n-    VALIDATION_INTERNAL_DOMAIN_NAME = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.internal_domain_name'\n-    )\n-\n-    def __init__(self, *args, **kwargs):\n-        self._psl = publicsuffixlist.PublicSuffixList(accept_unknown=False)\n-\n-        super().__init__(validations=[self.VALIDATION_INTERNAL_DOMAIN_NAME], **kwargs)\n-\n-    def extract_domain_name(self, node):\n-        return str(node.pdu)\n-\n-    def validate_with_value(self, node, value):\n-        if self._psl.publicsuffix(value) is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_INTERNAL_DOMAIN_NAME,\n-                f'Internal domain name: \"{value}\"'\n-            )\n-\n-    def validate(self, node):\n-        domain_name = self.extract_domain_name(node)\n-\n-        return self.validate_with_value(node, domain_name)\n-\n-\n-class GeneralNameDnsNameInternalDomainNameValidator(InternalDomainNameValidator):\n-    def __init__(self, allow_onion_tld=False):\n-        self._allow_onion_tld = allow_onion_tld\n-\n-        super().__init__(predicate=general_name.create_generalname_type_predicate('dNSName'))\n-\n-    def validate_with_value(self, node, value):\n-        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n-            return\n-        else:\n-            if self._allow_onion_tld and value.lower().endswith('.onion'):\n-                return\n-            else:\n-                return super().validate_with_value(node, value)\n-\n-\n-class UriInternalDomainNameValidator(InternalDomainNameValidator):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-    def extract_domain_name(self, node):\n-        return urlparse(str(node.pdu)).hostname or ''\n-\n-\n-class GeneralNameUriInternalDomainNameValidator(InternalDomainNameValidator):\n-    def __init__(self):\n-        super().__init__(predicate=general_name.create_generalname_type_predicate('uniformResourceIdentifier'))\n-\n-    def extract_domain_name(self, node):\n-        if general_name.is_nameconstraints_child_node(node):\n-            return str(node.pdu).lstrip('.')\n-        else:\n-            return urlparse(str(node.pdu)).hostname or ''\n-\n-    def validate_with_value(self, node, value):\n-        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n-            return\n-        else:\n-            return super().validate_with_value(node, value)\n-\n-\n-class EmailAddressInternalDomainNameValidator(InternalDomainNameValidator):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-    def extract_domain_name(self, node):\n-        parts = str(node.pdu).split('@', maxsplit=1)\n-\n-        return parts[1] if len(parts) == 2 else ''\n-\n-\n-class GeneralNameRfc822NameInternalDomainNameValidator(EmailAddressInternalDomainNameValidator):\n-    def __init__(self):\n-        super().__init__(predicate=general_name.create_generalname_type_predicate('rfc822Name'))\n-\n-    def extract_domain_name(self, node):\n-        if general_name.is_nameconstraints_child_node(node):\n-            return str(node.pdu).lstrip('.')\n-        else:\n-            return super().extract_domain_name(node)\n-\n-    def validate_with_value(self, node, value):\n-        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n-            return\n-        else:\n-            super().validate_with_value(node, value)\n-\n-\n-class SmtpUtf8MailboxInternalDomainNameValidator(EmailAddressInternalDomainNameValidator):\n-    def __init__(self):\n-        super().__init__(pdu_class=rfc8398.SmtpUTF8Mailbox)\n-\n-    def extract_domain_name(self, node):\n-        domain_part = super().extract_domain_name(node)\n-\n-        return domain_part.encode('idna').decode()\n-\n-\n-class InternalIpAddressValidator(validation.Validator):\n-    VALIDATION_INTERNAL_IP_ADDRESS = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.internal_ip_address'\n-    )\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(validations=self.VALIDATION_INTERNAL_IP_ADDRESS, **kwargs)\n-\n-    @staticmethod\n-    def _extract_ip_address(node):\n-        octets = node.pdu.asOctets()\n-\n-        if len(octets) == 4:\n-            return ipaddress.IPv4Address(octets)\n-        else:\n-            return ipaddress.IPv6Address(octets)\n-\n-    def validate(self, node):\n-        ip_addr = InternalIpAddressValidator._extract_ip_address(node)\n-\n-        if not ip_addr.is_global:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_INTERNAL_IP_ADDRESS,\n-                f'Internal IP address: \"{ip_addr}\"'\n-            )\n-\n-\n-class GeneralNameInternalIpAddressValidator(InternalIpAddressValidator):\n-    def __init__(self):\n-        super().__init__(predicate=general_name.create_generalname_type_predicate('iPAddress'))\n-\n-    def validate(self, node):\n-        if general_name.is_nameconstraints_child_node(node):\n-            return\n-\n-        super().validate(node)\n-\n-\n-class OrganizationNameTruncatedLegalNameValidator(validation.Validator):\n-    VALIDATION_ORGANIZATION_NAME_TRUNCATED_LEGAL_NAME = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.organization_name_no_closing_parenthesis'\n-    )\n-\n-    def __init__(self):\n-        super().__init__(validations=[self.VALIDATION_ORGANIZATION_NAME_TRUNCATED_LEGAL_NAME],\n-                         pdu_class=rfc5280.X520OrganizationName)\n-\n-    def validate(self, node):\n-        _, value_node = node.child\n-\n-        value = unicodedata.normalize('NFC', str(value_node.pdu))\n-\n-        open_paren_idx = value.find('(')\n-        if open_paren_idx < 0:\n-            return\n+VALIDATION_INTERNAL_IP_ADDRESS = validation.ValidationFinding(\n+    validation.ValidationFindingSeverity.ERROR,\n+    'cabf.internal_ip_address'\n+)\n \n-        close_paren_idx = value.find(')', open_paren_idx)\n \n-        if close_paren_idx < 0:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_NAME_TRUNCATED_LEGAL_NAME,\n-                f'Organization name attribute with truncated legal name: \"{value}\"'\n-            )\n+VALIDATION_INTERNAL_DOMAIN_NAME = validation.ValidationFinding(\n+    validation.ValidationFindingSeverity.ERROR,\n+    'cabf.internal_domain_name'\n+)\ndiff --git a/pkilint/cabf/serverauth/__init__.py b/pkilint/cabf/serverauth/__init__.py\nindex 4b0a4a6..78547f5 100644\n--- a/pkilint/cabf/serverauth/__init__.py\n+++ b/pkilint/cabf/serverauth/__init__.py\n@@ -1,7 +1,7 @@\n import typing\n from typing import List\n \n-from pyasn1_alt_modules import rfc5280, rfc4985, rfc6962\n+from pyasn1_alt_modules import rfc5280, rfc6962\n \n import pkilint.cabf.cabf_name\n import pkilint.cabf.serverauth.serverauth_extension\n@@ -14,11 +14,10 @@\n     serverauth_name, serverauth_extension, serverauth_constants,\n     serverauth_key, serverauth_root, serverauth_ca, serverauth_ocsp, serverauth_cross_ca, serverauth_finding_filter\n )\n+from pkilint.common import alternative_name\n from pkilint.pkix import name, certificate\n from pkilint.pkix.certificate import certificate_validity\n \n-OTHER_NAME_MAPPINGS = rfc4985.otherNamesMap.copy()\n-\n \n def _has_name_constraints(cert: certificate.RFC5280Certificate):\n     return cert.get_extension_by_oid(rfc5280.id_ce_nameConstraints) is not None\n@@ -65,7 +64,7 @@ def _determine_subscriber_certificate_type(cert: certificate.RFC5280Certificate)\n         return (serverauth_constants.CertificateType.OV_PRE_CERTIFICATE if is_precert\n                 else serverauth_constants.CertificateType.OV_FINAL_CERTIFICATE)\n     else:\n-        # \"unknown\" certificate types are consider to be DV Subscriber certs\n+        # \"unknown\" certificate types are considered to be DV Subscriber certs\n         return (serverauth_constants.CertificateType.DV_PRE_CERTIFICATE if is_precert\n                 else serverauth_constants.CertificateType.DV_FINAL_CERTIFICATE)\n \n@@ -83,23 +82,30 @@ def determine_certificate_type(cert: certificate.RFC5280Certificate) -> serverau\n             return _determine_subscriber_certificate_type(cert)\n \n \n-def create_decoding_validators():\n+def create_decoding_validators(additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n+    additional_validators.append(\n+        certificate.create_qc_statements_decoder(pkilint.etsi.asn1.ETSI_QC_STATEMENTS_MAPPINGS)\n+    )\n+\n     return pkilint.pkix.certificate.create_decoding_validators(\n         cabf.NAME_ATTRIBUTE_MAPPINGS,\n         cabf.EXTENSION_MAPPINGS,\n-        [certificate.create_other_name_decoder(OTHER_NAME_MAPPINGS),\n-         certificate.create_qc_statements_decoder(etsi.ETSI_QC_STATEMENTS_MAPPINGS)]\n+        additional_validators\n     )\n \n \n-def create_top_level_certificate_validators(certificate_type: serverauth_constants.CertificateType):\n+def create_top_level_certificate_validators(certificate_type: serverauth_constants.CertificateType,\n+                                            additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n     validators = [\n         serverauth_key.ServerauthAllowedSignatureAlgorithmEncodingValidator(\n             path='certificate.tbsCertificate.signature'\n         ),\n-        serverauth_key.ServerauthAllowedSignatureAlgorithmEncodingValidator(\n-            path='certificate.signatureValue'\n-        ),\n         cabf_extension.CabfExtensionsPresenceValidator(),\n     ]\n \n@@ -116,18 +122,23 @@ def create_top_level_certificate_validators(certificate_type: serverauth_constan\n     else:\n         raise ValueError(f'Unsupported certificate type: {certificate_type}')\n \n-    return validators\n+    return validators + additional_validators\n \n \n-def create_spki_validator_container():\n-    return validation.ValidatorContainer(validators=[\n-        serverauth_key.ServerauthAllowedPublicKeyAlgorithmEncodingValidator(\n-            path='certificate.tbsCertificate.subjectPublicKeyInfo.algorithm'\n-        ),\n-        cabf_key.RsaKeyValidator(),\n-        cabf_key.EcdsaKeyValidator(),\n-    ],\n-        path='certificate.tbsCertificate.subjectPublicKeyInfo')\n+def create_spki_validator_container(additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n+    return validation.ValidatorContainer(\n+        validators=[\n+            serverauth_key.ServerauthAllowedPublicKeyAlgorithmEncodingValidator(\n+                path='certificate.tbsCertificate.subjectPublicKeyInfo.algorithm'\n+            ),\n+            cabf_key.RsaKeyValidator(),\n+            cabf_key.EcdsaKeyValidator(),\n+        ] + additional_validators,\n+        path='certificate.tbsCertificate.subjectPublicKeyInfo'\n+    )\n \n \n def create_subject_name_validators() -> List[validation.Validator]:\n@@ -139,12 +150,16 @@ def create_subject_name_validators() -> List[validation.Validator]:\n         serverauth_name.ValidJurisdictionCountryValidator(),\n         cabf_name.ValidCountryValidator(),\n         serverauth_name.ValidBusinessCategoryValidator(),\n-        cabf_name.OrganizationIdentifierAttributeValidator(relax_stateprovince_syntax=False),\n+        cabf_name.CabfOrganizationIdentifierAttributeValidator(),\n         serverauth_name.ServerauthRelativeDistinguishedNameContainsOneElementValidator(),\n     ]\n \n \n-def create_ca_name_validator_container(certificate_type: serverauth_constants.CertificateType):\n+def create_ca_name_validator_container(certificate_type: serverauth_constants.CertificateType,\n+                                       additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n     validators = [\n         serverauth_ca.CaRequiredSubjectAttributesValidator(certificate_type),\n         serverauth_name.ServerauthDuplicateAttributeTypeValidator(certificate_type),\n@@ -154,10 +169,14 @@ def create_ca_name_validator_container(certificate_type: serverauth_constants.Ce\n     if certificate_type == serverauth_constants.CertificateType.ROOT_CA:\n         validators.append(serverauth_root.RootSubjectIssuerIdenticalEncodingValidator())\n \n-    return certificate.create_subject_validator_container(validators)\n+    return certificate.create_subject_validator_container(validators + additional_validators)\n+\n \n+def create_subscriber_name_validator_container(certificate_type: serverauth_constants.CertificateType,\n+                                               additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n \n-def create_subscriber_name_validator_container(certificate_type: serverauth_constants.CertificateType):\n     validators = create_subject_name_validators()\n     validators.extend([\n         serverauth_subscriber.SubscriberCommonNameValidator(),\n@@ -168,7 +187,7 @@ def create_subscriber_name_validator_container(certificate_type: serverauth_cons\n         validators.extend([\n             serverauth_subscriber.EvSubscriberAttributeAllowanceValidator(),\n             serverauth_subscriber.EvSubscriberJurisdictionPresenceValidator(),\n-            serverauth_name.OrganizationIdentifierConsistentSubjectAndExtensionValidator()\n+            pkilint.cabf.serverauth.serverauth_subscriber.OrganizationIdentifierConsistentSubjectAndExtensionValidator()\n         ])\n     elif certificate_type in serverauth_constants.IV_CERTIFICATE_TYPES:\n         validators.append(serverauth_subscriber.IvSubscriberAttributeAllowanceValidator())\n@@ -182,7 +201,7 @@ def create_subscriber_name_validator_container(certificate_type: serverauth_cons\n     if certificate_type in serverauth_constants.IDENTITY_CERTIFICATE_TYPES:\n         validators.append(serverauth_subscriber.IdentityCertificateStateProvinceAndLocalityPresenceValidator())\n \n-    return certificate.create_subject_validator_container(validators)\n+    return certificate.create_subject_validator_container(validators + additional_validators)\n \n \n def create_extension_validators() -> List[validation.Validator]:\n@@ -193,11 +212,12 @@ def create_extension_validators() -> List[validation.Validator]:\n         serverauth_extension.AuthorityInformationAccessHttpUriLocationValidator(),\n         serverauth_extension.AuthorityInformationAccessUniqueLocationValidator(),\n         cabf_extension.CabfAuthorityKeyIdentifierValidator(),\n-        cabf_name.GeneralNameDnsNameInternalDomainNameValidator(allow_onion_tld=True),\n-        cabf_name.GeneralNameRfc822NameInternalDomainNameValidator(),\n-        cabf_name.GeneralNameUriInternalDomainNameValidator(),\n-        cabf_name.UriInternalDomainNameValidator(pdu_class=rfc5280.CPSuri),\n-        cabf_name.GeneralNameInternalIpAddressValidator(),\n+        alternative_name.create_internal_name_validator_container(\n+            cabf_name.VALIDATION_INTERNAL_DOMAIN_NAME,\n+            cabf_name.VALIDATION_INTERNAL_IP_ADDRESS,\n+            allow_onion_tld=True\n+        ),\n+        alternative_name.create_cpsuri_internal_domain_name_validator(cabf_name.VALIDATION_INTERNAL_DOMAIN_NAME),\n         serverauth_extension.CertificatePolicyQualifierValidator(),\n         cabf_extension.CpsUriHttpValidator(),\n         serverauth_name.DnsNameLdhLabelSyntaxValidator(),\n@@ -207,8 +227,12 @@ def create_extension_validators() -> List[validation.Validator]:\n \n def create_ca_extension_validator_container(\n         certificate_type: serverauth_constants.CertificateType,\n-        validity_period_start_retriever: document.ValidityPeriodStartRetriever\n+        validity_period_start_retriever: document.ValidityPeriodStartRetriever,\n+        additional_validators=None\n ):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n     validators = create_extension_validators()\n \n     validators.extend([\n@@ -241,13 +265,16 @@ def create_ca_extension_validator_container(\n     if certificate_type in serverauth_constants.CONSTRAINED_TLS_CA_TYPES:\n         validators.append(serverauth_ca.TlsCaTechnicallyConstrainedValidator())\n \n-    return certificate.create_extensions_validator_container(validators)\n+    return certificate.create_extensions_validator_container(validators + additional_validators)\n \n \n def create_subscriber_extension_validator_container(\n         certificate_type: serverauth_constants.CertificateType,\n-        validity_period_start_retriever: document.ValidityPeriodStartRetriever\n-):\n+        validity_period_start_retriever: document.ValidityPeriodStartRetriever,\n+        additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n     validators = create_extension_validators()\n \n     validators.extend([\n@@ -269,10 +296,13 @@ def create_subscriber_extension_validator_container(\n     else:\n         validators.append(serverauth_subscriber.SubscriberSanGeneralNameTypeValidator())\n \n-    return certificate.create_extensions_validator_container(validators)\n+    return certificate.create_extensions_validator_container(validators + additional_validators)\n \n \n-def create_ocsp_extension_validator_container():\n+def create_ocsp_extension_validator_container(additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n+\n     validators = create_extension_validators()\n \n     validators.extend([\n@@ -282,10 +312,13 @@ def create_ocsp_extension_validator_container():\n         serverauth_ocsp.OcspBasicConstraintsValidator(),\n     ])\n \n-    return certificate.create_extensions_validator_container(validators)\n+    return certificate.create_extensions_validator_container(validators + additional_validators)\n+\n \n+def create_validity_validator_container(certificate_type, additional_validators=None):\n+    if additional_validators is None:\n+        additional_validators = []\n \n-def create_validity_validator_container(certificate_type):\n     validators = []\n \n     if certificate_type == serverauth_constants.CertificateType.ROOT_CA:\n@@ -293,73 +326,131 @@ def create_validity_validator_container(certificate_type):\n     elif certificate_type in serverauth_constants.SUBSCRIBER_CERTIFICATE_TYPES:\n         validators.append(serverauth_subscriber.SubscriberValidityPeriodValidator())\n \n-    return certificate.create_validity_validator_container(validators)\n+    return certificate.create_validity_validator_container(validators + additional_validators)\n \n \n-def create_root_ca_validators(validity_period_start_retriever: document.ValidityPeriodStartRetriever):\n+def create_root_ca_validators(\n+        validity_period_start_retriever: document.ValidityPeriodStartRetriever,\n+        additional_validity_validators=None, additional_spki_validators=None, additional_name_validators=None,\n+        additional_extension_validators=None, additional_top_level_validators=None\n+):\n     return [\n-        create_validity_validator_container(serverauth_constants.CertificateType.ROOT_CA),\n-        create_spki_validator_container(),\n-        create_ca_name_validator_container(serverauth_constants.CertificateType.ROOT_CA),\n+        create_validity_validator_container(\n+            serverauth_constants.CertificateType.ROOT_CA, additional_validity_validators\n+        ),\n+        create_spki_validator_container(additional_spki_validators),\n+        create_ca_name_validator_container(serverauth_constants.CertificateType.ROOT_CA, additional_name_validators),\n         create_ca_extension_validator_container(\n-            serverauth_constants.CertificateType.ROOT_CA, validity_period_start_retriever\n+            serverauth_constants.CertificateType.ROOT_CA,\n+            validity_period_start_retriever,\n+            additional_extension_validators\n         ),\n-    ] + create_top_level_certificate_validators(serverauth_constants.CertificateType.ROOT_CA)\n+    ] + create_top_level_certificate_validators(\n+        serverauth_constants.CertificateType.ROOT_CA, additional_top_level_validators\n+    )\n \n \n-def create_intermediate_ca_validators(\n-        certificate_type: serverauth_constants.CertificateType,\n-        validity_period_start_retriever: document.ValidityPeriodStartRetriever\n-):\n+def create_intermediate_ca_validators(certificate_type: serverauth_constants.CertificateType,\n+                                      validity_period_start_retriever: document.ValidityPeriodStartRetriever,\n+                                      additional_validity_validators=None, additional_spki_validators=None,\n+                                      additional_name_validators=None,\n+                                      additional_extension_validators=None, additional_top_level_validators=None\n+                                      ):\n     return [\n-        create_validity_validator_container(certificate_type),\n-        create_spki_validator_container(),\n-        create_ca_name_validator_container(certificate_type),\n-        create_ca_extension_validator_container(certificate_type, validity_period_start_retriever),\n-    ] + create_top_level_certificate_validators(certificate_type)\n+        create_validity_validator_container(certificate_type, additional_validity_validators),\n+        create_spki_validator_container(additional_spki_validators),\n+        create_ca_name_validator_container(certificate_type, additional_name_validators),\n+        create_ca_extension_validator_container(\n+            certificate_type,\n+            validity_period_start_retriever,\n+            additional_extension_validators\n+        ),\n+    ] + create_top_level_certificate_validators(certificate_type, additional_top_level_validators)\n \n \n-def create_subscriber_validators(\n-        certificate_type: serverauth_constants.CertificateType,\n-        validity_period_start_retriever: document.ValidityPeriodStartRetriever\n-):\n+def create_subscriber_validators(certificate_type: serverauth_constants.CertificateType,\n+                                 validity_period_start_retriever: document.ValidityPeriodStartRetriever,\n+                                 additional_validity_validators=None, additional_spki_validators=None,\n+                                 additional_name_validators=None,\n+                                 additional_extension_validators=None, additional_top_level_validators=None\n+                                 ):\n     return [\n-        create_validity_validator_container(certificate_type),\n-        create_spki_validator_container(),\n-        create_subscriber_name_validator_container(certificate_type),\n-        create_subscriber_extension_validator_container(certificate_type, validity_period_start_retriever),\n-    ] + create_top_level_certificate_validators(certificate_type)\n+        create_validity_validator_container(certificate_type, additional_validity_validators),\n+        create_spki_validator_container(additional_spki_validators),\n+        create_subscriber_name_validator_container(certificate_type, additional_name_validators),\n+        create_subscriber_extension_validator_container(\n+            certificate_type,\n+            validity_period_start_retriever,\n+            additional_extension_validators\n+        ),\n+    ] + create_top_level_certificate_validators(certificate_type, additional_top_level_validators)\n \n \n-def create_ocsp_responder_validators():\n+def create_ocsp_responder_validators(\n+        additional_validity_validators=None, additional_spki_validators=None,\n+        additional_name_validators=None,\n+        additional_extension_validators=None, additional_top_level_validators=None\n+):\n     return [\n-        create_validity_validator_container(serverauth_constants.CertificateType.OCSP_RESPONDER),\n-        create_spki_validator_container(),\n-        create_ca_name_validator_container(serverauth_constants.CertificateType.OCSP_RESPONDER),\n-        create_ocsp_extension_validator_container(),\n-    ] + create_top_level_certificate_validators(serverauth_constants.CertificateType.OCSP_RESPONDER)\n+        create_validity_validator_container(\n+            serverauth_constants.CertificateType.OCSP_RESPONDER, additional_validity_validators\n+        ),\n+        create_spki_validator_container(additional_spki_validators),\n+        create_ca_name_validator_container(\n+            serverauth_constants.CertificateType.OCSP_RESPONDER, additional_name_validators\n+        ),\n+        create_ocsp_extension_validator_container(additional_extension_validators),\n+    ] + create_top_level_certificate_validators(\n+        serverauth_constants.CertificateType.OCSP_RESPONDER, additional_top_level_validators\n+    )\n \n \n-def create_validators(\n-        certificate_type: serverauth_constants.CertificateType,\n-        validity_period_start_retriever: typing.Optional[document.ValidityPeriodStartRetriever] = None\n-):\n+def create_validators(certificate_type: serverauth_constants.CertificateType,\n+                      validity_period_start_retriever: typing.Optional[document.ValidityPeriodStartRetriever] = None,\n+                      additional_validity_validators=None, additional_spki_validators=None,\n+                      additional_name_validators=None,\n+                      additional_extension_validators=None, additional_top_level_validators=None\n+                      ):\n     if validity_period_start_retriever is None:\n         validity_period_start_retriever = certificate_validity.CertificateValidityPeriodStartRetriever()\n \n     if certificate_type == serverauth_constants.CertificateType.ROOT_CA:\n-        return create_root_ca_validators(validity_period_start_retriever)\n+        return create_root_ca_validators(\n+            validity_period_start_retriever,\n+            additional_validity_validators, additional_spki_validators,\n+            additional_name_validators, additional_extension_validators,\n+            additional_top_level_validators\n+        )\n     elif certificate_type in serverauth_constants.INTERMEDIATE_CERTIFICATE_TYPES:\n-        return create_intermediate_ca_validators(certificate_type, validity_period_start_retriever)\n+        return create_intermediate_ca_validators(\n+            certificate_type,\n+            validity_period_start_retriever,\n+            additional_validity_validators,\n+            additional_spki_validators,\n+            additional_name_validators, additional_extension_validators,\n+            additional_top_level_validators\n+        )\n     elif certificate_type in serverauth_constants.SUBSCRIBER_CERTIFICATE_TYPES:\n-        return create_subscriber_validators(certificate_type, validity_period_start_retriever)\n+        return create_subscriber_validators(\n+            certificate_type,\n+            validity_period_start_retriever,\n+            additional_validity_validators,\n+            additional_spki_validators,\n+            additional_name_validators, additional_extension_validators,\n+            additional_top_level_validators\n+        )\n     elif certificate_type == serverauth_constants.CertificateType.OCSP_RESPONDER:\n-        return create_ocsp_responder_validators()\n+        return create_ocsp_responder_validators(additional_validity_validators, additional_spki_validators,\n+                                                additional_name_validators, additional_extension_validators,\n+                                                additional_top_level_validators)\n     else:\n         raise ValueError(f'Unsupported certificate type: {certificate_type}')\n \n \n-def create_serverauth_finding_filters(certificate_type: serverauth_constants.CertificateType):\n+def create_serverauth_finding_filters(certificate_type: serverauth_constants.CertificateType, additional_filters=None):\n+    if additional_filters is None:\n+        additional_filters = []\n+\n     filters = [\n         serverauth_finding_filter.NameConstraintsCriticalityFilter(),\n         serverauth_finding_filter.PolicyQualifierPresentFilter(),\n@@ -371,4 +462,4 @@ def create_serverauth_finding_filters(certificate_type: serverauth_constants.Cer\n             serverauth_finding_filter.EndEntitySubjectKeyIdentifierMissingFilter(),\n         ]\n \n-    return filters\n+    return filters + additional_filters\ndiff --git a/pkilint/cabf/serverauth/finding_metadata.csv b/pkilint/cabf/serverauth/finding_metadata.csv\nindex c44f9ce..049db01 100644\n--- a/pkilint/cabf/serverauth/finding_metadata.csv\n+++ b/pkilint/cabf/serverauth/finding_metadata.csv\n@@ -128,10 +128,7 @@ ERROR,cabf.serverauth.organization_identifier_ext_invalid_country,Validates that\n ERROR,cabf.serverauth.organization_identifier_ext_invalid_registration_scheme,Validates that the content of the CA/B Forum organizationIdentifier extension conforms with EVG 9.8.2.\n ERROR,cabf.serverauth.organization_identifier_ext_invalid_state_province_for_scheme,Validates that the content of the CA/B Forum organizationIdentifier extension conforms with EVG 9.8.2.\n ERROR,cabf.serverauth.organization_identifier_extension_absent,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n-ERROR,cabf.serverauth.organization_identifier_mismatched_country_code,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n-ERROR,cabf.serverauth.organization_identifier_mismatched_registration_reference,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n-ERROR,cabf.serverauth.organization_identifier_mismatched_scheme,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n-ERROR,cabf.serverauth.organization_identifier_mismatched_state_province,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n+ERROR,cabf.serverauth.organization_identifier_mismatched_value,\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\n ERROR,cabf.serverauth.ov.country_attribute_absent,Validates that the content of the subject field conforms with BR 7.1.7.4.: A required element is absent\n ERROR,cabf.serverauth.ov.given_name_attribute_present,Validates that the content of the subject field conforms with BR 7.1.7.4.: A prohibited element is present\n ERROR,cabf.serverauth.ov.organization_name_attribute_absent,Validates that the content of the subject field conforms with BR 7.1.7.4.: A required element is absent\ndiff --git a/pkilint/cabf/serverauth/serverauth_name.py b/pkilint/cabf/serverauth/serverauth_name.py\nindex 2d2f7cc..3ab329b 100644\n--- a/pkilint/cabf/serverauth/serverauth_name.py\n+++ b/pkilint/cabf/serverauth/serverauth_name.py\n@@ -5,9 +5,9 @@\n from cryptography.hazmat.primitives import hashes\n from pyasn1_alt_modules import rfc5280\n \n-from pkilint import validation, document\n+from pkilint import validation\n from pkilint.cabf.asn1 import ev_guidelines\n-from pkilint.cabf.cabf_name import ValidCountryCodeValidatorBase, ORG_ID_REGEX\n+from pkilint.cabf.cabf_name import ValidCountryCodeValidatorBase\n from pkilint.cabf.serverauth import serverauth_constants\n from pkilint.itu import x520_name\n from pkilint.pkix import name, general_name\n@@ -61,127 +61,6 @@ def validate(self, node):\n             )\n \n \n-class OrganizationIdentifierConsistentSubjectAndExtensionValidator(validation.Validator):\n-    \"\"\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier\n-    extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\"\"\n-\n-    VALIDATION_CABF_ORG_ID_NO_EXT = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_extension_absent'\n-    )\n-\n-    VALIDATION_CABF_ORG_ID_INVALID_SYNTAX = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.FATAL,\n-        'cabf.serverauth.organization_identifier_invalid_syntax'\n-    )\n-\n-    VALIDATION_CABF_ORG_ID_MISMATCHED_SCHEME = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_mismatched_scheme'\n-    )\n-\n-    VALIDATION_CABF_ORG_ID_MISMATCHED_COUNTRY = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_mismatched_country_code'\n-    )\n-\n-    VALIDATION_CABF_ORG_ID_MISMATCHED_SP = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_mismatched_state_province'\n-    )\n-\n-    VALIDATION_CABF_ORG_ID_MISMATCHED_REFERENCE = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_mismatched_registration_reference'\n-    )\n-\n-    def __init__(self):\n-        super().__init__(\n-            validations=[\n-                self.VALIDATION_CABF_ORG_ID_NO_EXT,\n-                self.VALIDATION_CABF_ORG_ID_INVALID_SYNTAX,\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_SCHEME,\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_COUNTRY,\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_SP,\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_REFERENCE,\n-            ],\n-            pdu_class=x520_name.X520OrganizationIdentifier,\n-            predicate=lambda n: any(n.children)\n-        )\n-\n-    def validate(self, node):\n-        ext_and_idx = node.document.get_extension_by_oid(\n-            ev_guidelines.id_CABFOrganizationIdentifier\n-        )\n-\n-        if ext_and_idx is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_CABF_ORG_ID_NO_EXT\n-            )\n-\n-        ext, _ = ext_and_idx\n-        try:\n-            ext = ext.navigate('extnValue.cABFOrganizationIdentifier')\n-        except document.PDUNavigationFailedError:\n-            return\n-\n-        attr_value = str(node.child[1].pdu)\n-\n-        m = ORG_ID_REGEX.match(attr_value)\n-        if m is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_CABF_ORG_ID_INVALID_SYNTAX,\n-                f'Invalid syntax: \"{attr_value}\"'\n-            )\n-\n-        findings = []\n-        ext_scheme = str(ext.children['registrationSchemeIdentifier'].pdu)\n-        if m['scheme'] != ext_scheme:\n-            findings.append(validation.ValidationFindingDescription(\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_SCHEME,\n-                f'Mismatched scheme: subject: \"{m[\"scheme\"]}\", extension: '\n-                f'\"{ext_scheme}\"'\n-            ))\n-\n-        ext_country = str(ext.children['registrationCountry'].pdu)\n-        if m['country'] != ext_country:\n-            findings.append(validation.ValidationFindingDescription(\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_COUNTRY,\n-                f'Mismatched country: subject: \"{m[\"country\"]}\", extension: '\n-                f'\"{ext_country}\"'\n-            ))\n-\n-        ext_sp_node = ext.children.get('registrationStateOrProvince')\n-        ext_sp = None if ext_sp_node is None else str(ext_sp_node.pdu)\n-        if m['sp'] is None and ext_sp is not None:\n-            findings.append(validation.ValidationFindingDescription(\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_SP,\n-                'Extension has state/province value but subject does not'\n-            ))\n-        elif m['sp'] is not None and ext_sp is None:\n-            findings.append(validation.ValidationFindingDescription(\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_SP,\n-                'Extension does not have state/province value but subject does'\n-            ))\n-        elif m['sp'] is not None and ext_sp is not None:\n-            if ext_sp != m['sp']:\n-                findings.append(validation.ValidationFindingDescription(\n-                    self.VALIDATION_CABF_ORG_ID_MISMATCHED_SP,\n-                    f'Mismatched state/province value: subject: \"{m[\"sp\"]}\", '\n-                    f'extension: \"{ext_sp}\"'\n-                ))\n-\n-        ext_reg_ref = str(ext.children['registrationReference'].pdu)\n-        if m['reference'] != ext_reg_ref:\n-            findings.append(validation.ValidationFindingDescription(\n-                self.VALIDATION_CABF_ORG_ID_MISMATCHED_REFERENCE,\n-                f'Mismatched Registration Reference: subject: \"{m[\"reference\"]}\"'\n-                f', extension: \"{ext_reg_ref}\"'\n-            ))\n-\n-        return validation.ValidationResult(self, node, findings)\n-\n-\n class X520NameAttributeValueLengthValidator(validation.Validator):\n     \"\"\"Validates that the length of X520Name values does not exceed the limit stated in BR 7.1.4.2.\"\"\"\n \ndiff --git a/pkilint/cabf/serverauth/serverauth_ocsp.py b/pkilint/cabf/serverauth/serverauth_ocsp.py\nindex 34372e6..ae3f39d 100644\n--- a/pkilint/cabf/serverauth/serverauth_ocsp.py\n+++ b/pkilint/cabf/serverauth/serverauth_ocsp.py\n@@ -4,7 +4,7 @@\n from pkilint import validation, common\n from pkilint.itu import bitstring\n from pkilint.pkix import Rfc2119Word\n-\n+from pkilint.pkix.certificate.certificate_extension import KeyUsageBitName\n \n _CODE_CLASSIFIER = 'cabf.serverauth.ocsp_responder'\n \n@@ -24,14 +24,14 @@ class OcspResponderKeyUsageValidator(validation.Validator):\n \n     _PROHIBITED_KUS = {\n                           str(n) for n in rfc5280.KeyUsage.namedValues\n-                      } - {'digitalSignature'}\n+                      } - {KeyUsageBitName.DIGITAL_SIGNATURE}\n \n     def __init__(self):\n         super().__init__(validations=[self.VALIDATION_DIGSIG_MISSING, self.VALIDATION_PROHIBITED_KU_PRESENT],\n                          pdu_class=rfc5280.KeyUsage)\n \n     def validate(self, node):\n-        if not bitstring.has_named_bit(node, 'digitalSignature'):\n+        if not bitstring.has_named_bit(node, KeyUsageBitName.DIGITAL_SIGNATURE):\n             raise validation.ValidationFindingEncountered(self.VALIDATION_DIGSIG_MISSING)\n \n         prohibited_kus_asserted = sorted((k for k in self._PROHIBITED_KUS if bitstring.has_named_bit(node, k)))\ndiff --git a/pkilint/cabf/serverauth/serverauth_subscriber.py b/pkilint/cabf/serverauth/serverauth_subscriber.py\nindex 4753308..606a388 100644\n--- a/pkilint/cabf/serverauth/serverauth_subscriber.py\n+++ b/pkilint/cabf/serverauth/serverauth_subscriber.py\n@@ -1,89 +1,52 @@\n-import ipaddress\n import operator\n from datetime import timedelta\n \n-from iso3166 import countries_by_alpha2\n from pyasn1_alt_modules import rfc5280, rfc6962, rfc5480\n \n import pkilint.common\n from pkilint import validation, document, oid, common\n-from pkilint.cabf import cabf_constants\n+from pkilint.cabf import cabf_name\n from pkilint.cabf.asn1 import ev_guidelines\n-from pkilint.cabf.cabf_constants import REGISTRATION_SCHEMES\n from pkilint.cabf.serverauth import serverauth_constants\n+from pkilint.common import organization_id, common_name\n+from pkilint.common.organization_id import ParsedOrganizationIdentifier\n from pkilint.itu import x520_name, bitstring\n from pkilint.pkix import Rfc2119Word, general_name, time\n+from pkilint.pkix.certificate.certificate_extension import KeyUsageBitName\n \n \n-class CABFOrganizationIdentifierExtensionValidator(validation.Validator):\n-    \"\"\"Validates that the content of the CA/B Forum organizationIdentifier extension conforms with EVG 9.8.2.\"\"\"\n+def _parse_organization_identifier_extension(\n+        ext_node: document.PDUNode) -> organization_id.ParsedOrganizationIdentifier:\n+    state_province_node = ext_node.children.get('registrationStateOrProvince')\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_SCHEME = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_ext_invalid_registration_scheme'\n-    )\n+    state_province = str(state_province_node.pdu) if state_province_node else None\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_ext_invalid_country'\n-    )\n+    reference_node = ext_node.children.get('registrationReference')\n+    reference = str(reference_node.pdu) if reference_node else None\n \n-    VALIDATION_ORGANIZATION_ID_INVALID_SP = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR,\n-        'cabf.serverauth.organization_identifier_ext_invalid_state_province_for_scheme'\n+    return organization_id.ParsedOrganizationIdentifier(\n+        raw=None,\n+        scheme=str(ext_node.children['registrationSchemeIdentifier'].pdu),\n+        is_national_scheme=False,\n+        country=str(ext_node.children['registrationCountry'].pdu),\n+        state_province=state_province,\n+        reference=reference\n     )\n \n-    _ISO3166_AND_ARTICLE_215_COUNTRY_CODES = set(countries_by_alpha2.keys()) | {'EL', 'XI'}\n+\n+class CABFOrganizationIdentifierExtensionValidator(cabf_name.CabfOrganizationIdentifierValidatorBase):\n+    \"\"\"Validates that the content of the CA/B Forum organizationIdentifier extension conforms with EVG 9.8.2.\"\"\"\n \n     def __init__(self):\n         super().__init__(\n-            pdu_class=ev_guidelines.CABFOrganizationIdentifier,\n-            validations=[\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME,\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY,\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SP,\n-            ]\n+            invalid_format_validation=None,\n+            enforce_strict_state_province_format=True,\n+            pdu_class=ev_guidelines.CABFOrganizationIdentifier\n         )\n \n-    def validate(self, node):\n-        scheme = str(node.children['registrationSchemeIdentifier'].pdu)\n-        country = str(node.children['registrationCountry'].pdu).upper()\n-\n-        scheme_node = node.children.get('registrationStateOrProvince')\n-        sp = None if scheme_node is None else str(scheme_node.pdu)\n-\n-        scheme_info = REGISTRATION_SCHEMES.get(scheme)\n-\n-        if scheme_info is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SCHEME,\n-                f'Invalid registration scheme: \"{scheme}\"'\n-            )\n-\n-        if scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.NONE:\n-            valid_country_code = (country == '')\n-        elif scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.XG:\n-            valid_country_code = (country == 'XG')\n-        elif scheme_info.country_identifier_type == cabf_constants.RegistrationSchemeCountryIdentifierType.ISO3166:\n-            if scheme == 'VAT':\n-                valid_country_code = (country in self._ISO3166_AND_ARTICLE_215_COUNTRY_CODES)\n-            else:\n-                valid_country_code = country in countries_by_alpha2\n-        else:\n-            raise ValueError(f'Unknown country identifier type for scheme \"{scheme}\": '\n-                             f'{scheme_info.country_identifier_type}')\n-\n-        if not valid_country_code:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY,\n-                f'Invalid country code for scheme \"{scheme}\": \"{country}\"'\n-            )\n-\n-        if sp is not None and not scheme_info.allow_state_province:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_ORGANIZATION_ID_INVALID_SP,\n-                f'Scheme \"{scheme}\" does not allow state/province values'\n-            )\n+    @classmethod\n+    def parse_organization_id_node(cls, node: document.PDUNode) -> ParsedOrganizationIdentifier:\n+        return _parse_organization_identifier_extension(node)\n \n \n class EvSubscriberAttributeAllowanceValidator(pkilint.common.AttributeIdentifierAllowanceValidator):\n@@ -302,13 +265,13 @@ class SubscriberKeyUsageValidator(validation.Validator):\n \n     _SPKI_OID_TO_KU_ALLOWANCES_MAPPING = {\n         rfc5480.rsaEncryption: {\n-            'digitalSignature': Rfc2119Word.SHOULD,\n-            'keyEncipherment': Rfc2119Word.MAY,\n-            'dataEncipherment': Rfc2119Word.SHOULD_NOT,\n+            KeyUsageBitName.DIGITAL_SIGNATURE: Rfc2119Word.SHOULD,\n+            KeyUsageBitName.KEY_ENCIPHERMENT: Rfc2119Word.MAY,\n+            KeyUsageBitName.DATA_ENCIPHERMENT: Rfc2119Word.SHOULD_NOT,\n         },\n         rfc5480.id_ecPublicKey: {\n-            'digitalSignature': Rfc2119Word.MUST,\n-            'keyAgreement': Rfc2119Word.SHOULD_NOT,\n+            KeyUsageBitName.DIGITAL_SIGNATURE: Rfc2119Word.MUST,\n+            KeyUsageBitName.KEY_AGREEMENT: Rfc2119Word.SHOULD_NOT,\n         },\n     }\n \n@@ -358,8 +321,8 @@ def validate(self, node):\n                     ))\n \n         if spki_alg_oid == rfc5480.rsaEncryption and (\n-                bitstring.has_named_bit(node, 'digitalSignature') and\n-                bitstring.has_named_bit(node, 'keyEncipherment')):\n+                bitstring.has_named_bit(node, KeyUsageBitName.DIGITAL_SIGNATURE) and\n+                bitstring.has_named_bit(node, KeyUsageBitName.KEY_ENCIPHERMENT)):\n             warning_findings.append(\n                 validation.ValidationFindingDescription(self.VALIDATION_RSA_DIGSIG_AND_KEYENCIPHERMENT_PRESENT, None)\n             )\n@@ -414,22 +377,26 @@ def validate(self, node):\n class SubscriberValidityPeriodValidator(time.ValidityPeriodThresholdsValidator):\n     \"\"\"Validates that the validity period conforms to BR 7.1.2.7.\"\"\"\n \n+    VALIDATION_VALIDITY_PERIOD_EXCEEDS_398_DAYS = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'cabf.certificate_validity_period_exceeds_398_days'\n+    )\n+\n+    VALIDATION_VALIDITY_PERIOD_EXCEEDS_397_DAYS = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'cabf.certificate_validity_period_exceeds_397_days'\n+    )\n+\n     _THRESHOLDS = [\n         (\n             operator.le,\n             timedelta(days=398),\n-            validation.ValidationFinding(\n-                validation.ValidationFindingSeverity.ERROR,\n-                'cabf.certificate_validity_period_exceeds_398_days'\n-            )\n+            VALIDATION_VALIDITY_PERIOD_EXCEEDS_398_DAYS\n         ),\n         (\n             operator.le,\n             timedelta(days=397),\n-            validation.ValidationFinding(\n-                validation.ValidationFindingSeverity.WARNING,\n-                'cabf.certificate_validity_period_exceeds_397_days'\n-            )\n+            VALIDATION_VALIDITY_PERIOD_EXCEEDS_397_DAYS\n         )\n     ]\n \n@@ -441,7 +408,7 @@ def __init__(self):\n                          )\n \n \n-class SubscriberCommonNameValidator(validation.Validator):\n+class SubscriberCommonNameValidator(common_name.CommonNameValidator):\n     \"\"\"Validates that the content of the commonName attribute conforms to BR 7.1.4.3.\"\"\"\n \n     VALIDATION_COMMON_NAME_UNKNOWN_SOURCE = validation.ValidationFinding(\n@@ -449,68 +416,10 @@ class SubscriberCommonNameValidator(validation.Validator):\n         'cabf.serverauth.subscriber_common_name_unknown_source'\n     )\n \n-    VALIDATION_UNPARSED_CN_ENCOUNTERED = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.NOTICE,\n-        'cabf.serverauth.unparsed_common_name_encountered'\n-    )\n-\n-    VALIDATION_UNPARSED_SAN_EXTENSION_ENCOUNTERED = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.NOTICE,\n-        'cabf.serverauth.unparsed_san_extension_encountered'\n-    )\n-\n     def __init__(self):\n         super().__init__(\n-            validations=[self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE, self.VALIDATION_UNPARSED_CN_ENCOUNTERED,\n-                         self.VALIDATION_UNPARSED_SAN_EXTENSION_ENCOUNTERED],\n-            pdu_class=rfc5280.X520CommonName)\n-\n-    def validate(self, node):\n-        # unparsed CN, return\n-        if not any(node.children):\n-            raise validation.ValidationFindingEncountered(self.VALIDATION_UNPARSED_CN_ENCOUNTERED)\n-\n-        _, value_node = node.child\n-        value_str = str(value_node.pdu)\n-\n-        san_ext_and_idx = node.document.get_extension_by_oid(rfc5280.id_ce_subjectAltName)\n-\n-        if san_ext_and_idx is None:\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE,\n-                f'Unknown source for value of common name: \"{value_str}\"'\n-            )\n-\n-        san_ext_node, _ = san_ext_and_idx\n-\n-        try:\n-            san_value_node = san_ext_node.navigate('extnValue.subjectAltName')\n-        except document.PDUNavigationFailedError:\n-            # unparsed SAN extension, return\n-            raise validation.ValidationFindingEncountered(self.VALIDATION_UNPARSED_SAN_EXTENSION_ENCOUNTERED)\n-\n-        for gn in san_value_node.children.values():\n-            gn_type, gn_value = gn.child\n-\n-            if gn_type == 'dNSName' and str(gn_value.pdu) == value_str:\n-                return\n-            elif gn_type == 'iPAddress':\n-                address_octets = gn_value.pdu.asOctets()\n-\n-                if len(address_octets) == 4:\n-                    ip_addr = ipaddress.IPv4Address(address_octets)\n-                elif len(address_octets) == 16:\n-                    ip_addr = ipaddress.IPv6Address(address_octets)\n-                else:\n-                    # whoa Nellie! let the PKIX validator complain about this one\n-                    continue\n-\n-                if str(ip_addr) == value_str:\n-                    return\n-\n-        raise validation.ValidationFindingEncountered(\n-            self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE,\n-            f'Unknown source for value of common name: \"{value_str}\"'\n+            {general_name.GeneralNameTypeName.DNS_NAME, general_name.GeneralNameTypeName.IP_ADDRESS},\n+            self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE\n         )\n \n \n@@ -654,3 +563,66 @@ class SubscriberAuthorityInformationAccessAccessMethodPresenceValidator(\n \n     def __init__(self):\n         super().__init__(self._ACCESS_METHOD_ALLOWANCES, self._CODE_CLASSIFIER, Rfc2119Word.MUST_NOT)\n+\n+\n+class OrganizationIdentifierConsistentSubjectAndExtensionValidator(validation.Validator):\n+    \"\"\"Validates that the content of the organizationIdentifier subject attributes and the organizationIdentifier\n+    extension are consistent, as per EVG 9.2.8 and 9.2.9.\"\"\"\n+\n+    VALIDATION_CABF_ORG_ID_NO_EXT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'cabf.serverauth.organization_identifier_extension_absent'\n+    )\n+\n+    VALIDATION_CABF_ORG_ID_MISMATCHED_VALUE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'cabf.serverauth.organization_identifier_mismatched_value'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_CABF_ORG_ID_NO_EXT,\n+                self.VALIDATION_CABF_ORG_ID_MISMATCHED_VALUE,\n+            ],\n+            pdu_class=x520_name.X520OrganizationIdentifier,\n+            predicate=lambda n: any(n.children)\n+        )\n+\n+    def validate(self, node):\n+        ext_and_idx = node.document.get_extension_by_oid(\n+            ev_guidelines.id_CABFOrganizationIdentifier\n+        )\n+\n+        if ext_and_idx is None:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_CABF_ORG_ID_NO_EXT\n+            )\n+\n+        ext_node, _ = ext_and_idx\n+        try:\n+            ext_node = ext_node.navigate('extnValue.cABFOrganizationIdentifier')\n+        except document.PDUNavigationFailedError:\n+            return\n+\n+        attr_value = str(node.child[1].pdu)\n+\n+        try:\n+            org_id_attr_parsed = organization_id.parse_organization_identifier(attr_value)\n+        except ValueError:\n+            # let the format validator report this error\n+            return\n+\n+        org_id_ext_parsed = _parse_organization_identifier_extension(ext_node)\n+\n+        try:\n+            organization_id.assert_parsed_organization_identifier_equal(\n+                org_id_attr_parsed, 'attribute', org_id_ext_parsed, 'extension'\n+            )\n+        except ValueError as e:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_CABF_ORG_ID_MISMATCHED_VALUE,\n+                str(e)\n+            )\n+\n+\ndiff --git a/pkilint/cabf/smime/__init__.py b/pkilint/cabf/smime/__init__.py\nindex 1cb1fbf..5c0a4fd 100644\n--- a/pkilint/cabf/smime/__init__.py\n+++ b/pkilint/cabf/smime/__init__.py\n@@ -9,14 +9,16 @@\n import pkilint.cabf.cabf_extension\n import pkilint.cabf.smime.smime_extension\n import pkilint.common\n+import pkilint.etsi.asn1\n import pkilint.pkix.certificate\n-from pkilint import validation, cabf, document, etsi\n+from pkilint import validation, cabf, document\n from pkilint.adobe import adobe_validator\n from pkilint.cabf import cabf_extension, cabf_key, cabf_name\n from pkilint.cabf.smime import (\n     smime_constants, smime_name, smime_key, smime_extension\n )\n from pkilint.cabf.smime.smime_constants import Generation\n+from pkilint.common import alternative_name\n from pkilint.iso import lei\n from pkilint.msft import asn1 as microsoft_asn1\n from pkilint.msft import msft_name\n@@ -119,7 +121,7 @@ def create_decoding_validators():\n         cabf.NAME_ATTRIBUTE_MAPPINGS,\n         _SMIME_EXTENSION_MAPPINGS,\n         [certificate.create_other_name_decoder(OTHER_NAME_MAPPINGS),\n-         certificate.create_qc_statements_decoder(etsi.ETSI_QC_STATEMENTS_MAPPINGS)]\n+         certificate.create_qc_statements_decoder(pkilint.etsi.asn1.ETSI_QC_STATEMENTS_MAPPINGS)]\n     )\n \n \n@@ -173,10 +175,14 @@ def create_extensions_validator_container(validation_level, generation):\n             cabf_extension.CabfAuthorityKeyIdentifierValidator(),\n             smime_extension.SubjectDirectoryAttributesPresenceValidator(validation_level, generation),\n             smime_extension.QCStatementsCriticalityValidator(),\n-            cabf_name.GeneralNameUriInternalDomainNameValidator(),\n-            cabf_name.GeneralNameRfc822NameInternalDomainNameValidator(),\n-            cabf_name.SmtpUtf8MailboxInternalDomainNameValidator(),\n-            cabf_name.UriInternalDomainNameValidator(pdu_class=rfc5280.CPSuri),\n+            alternative_name.create_internal_name_validator_container(\n+                cabf_name.VALIDATION_INTERNAL_DOMAIN_NAME,\n+                cabf_name.VALIDATION_INTERNAL_IP_ADDRESS,\n+                allow_onion_tld=False\n+            ),\n+            alternative_name.create_cpsuri_internal_domain_name_validator(\n+                cabf_name.VALIDATION_INTERNAL_DOMAIN_NAME\n+            ),\n             adobe_validator.AdobeTimestampValidator(),\n             smime_extension.AdobeTimestampCriticalityValidator(),\n             smime_extension.AdobeTimestampPresenceValidator(generation),\n@@ -230,8 +236,5 @@ def create_subscriber_validators(validation_level, generation):\n         smime_key.SmimeAllowedSignatureAlgorithmEncodingValidator(\n             path='certificate.tbsCertificate.signature'\n         ),\n-        smime_key.SmimeAllowedSignatureAlgorithmEncodingValidator(\n-            path='certificate.signatureValue'\n-        ),\n         cabf_extension.CabfExtensionsPresenceValidator(),\n     ]\ndiff --git a/pkilint/cabf/smime/finding_metadata.csv b/pkilint/cabf/smime/finding_metadata.csv\nindex 75c258d..c9e5a17 100644\n--- a/pkilint/cabf/smime/finding_metadata.csv\n+++ b/pkilint/cabf/smime/finding_metadata.csv\n@@ -8,7 +8,7 @@ ERROR,cabf.authority_key_identifier_has_issuer_cert,SMBR 7.1.2.3 (g),\"\"\"authorit\n ERROR,cabf.crldp_extension_missing,SMBR 7.1.2.3 (b),\"\"\"SHALL be present\"\"\"\n ERROR,cabf.internal_domain_name,,The use of an internal domain name (whose superior domain labels do not appear on the PSL)\n ERROR,cabf.invalid_country_code,,The use of a country code that does not appear on ISO 3166.\n-ERROR,cabf.invalid_subject_organization_identifier_country,SMBR 7.1.4.2.2 (d),The use of a country code that is not allowed in the organizationIdentifier attribute\n+ERROR,cabf.invalid_organization_identifier_country,SMBR 7.1.4.2.2 (d),The use of a country code that is not allowed in the organizationIdentifier attribute\n ERROR,cabf.invalid_subject_organization_identifier_encoding,SMBR 7.1.4.2.2 (d),The use of a legacy encoding for new subject names (which all S/MIME certificates will have)\n ERROR,cabf.invalid_subject_organization_identifier_format,SMBR 7.1.4.2.2 (d),The inclusion of a value which does not adhere to the specified attribute format\n ERROR,cabf.invalid_subject_organization_identifier_registration_scheme,SMBR 7.1.4.2.2 (d),The inclusion of a scheme which is not recognized\ndiff --git a/pkilint/cabf/smime/smime_extension.py b/pkilint/cabf/smime/smime_extension.py\nindex e19962e..c59830d 100644\n--- a/pkilint/cabf/smime/smime_extension.py\n+++ b/pkilint/cabf/smime/smime_extension.py\n@@ -8,7 +8,9 @@\n from pkilint.cabf.smime.smime_constants import Generation, ValidationLevel\n from pkilint.cabf.smime.smime_name import get_email_addresses_from_san\n from pkilint.iso import lei\n+from pkilint.itu import bitstring\n from pkilint.pkix import extension\n+from pkilint.pkix.certificate.certificate_extension import KeyUsageBitName\n \n \n class CertificatePoliciesPresenceValidator(extension.ExtensionPresenceValidator):\n@@ -277,25 +279,6 @@ def validate(self, node):\n         return validation.ValidationResult(self, node, findings)\n \n \n-def _has_bit(node, bit_name):\n-    bit = rfc5280.KeyUsage.namedValues[bit_name]\n-    return len(node.pdu) > bit and node.pdu[bit] != 0\n-\n-\n-def _get_prohibited_kus(node, allowed_kus):\n-    prohibited_kus = (\n-        set(map(str, rfc5280.KeyUsage.namedValues)).difference(\n-            allowed_kus\n-        )\n-    )\n-\n-    return [\n-        pk\n-        for pk in prohibited_kus\n-        if _has_bit(node, pk)\n-    ]\n-\n-\n class AllowedKeyUsageValidator(validation.Validator):\n     VALIDATION_UNKNOWN_CERT_TYPE = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n@@ -317,6 +300,8 @@ class AllowedKeyUsageValidator(validation.Validator):\n         'cabf.smime.unsupported_public_key_type'\n     )\n \n+    _ALL_KUS = {str(n) for n in rfc5280.KeyUsage.namedValues}\n+\n     def __init__(self, generation):\n         super().__init__(\n             validations=[\n@@ -330,37 +315,41 @@ def __init__(self, generation):\n \n         self._generation = generation\n \n+    @classmethod\n+    def _get_prohibited_kus(cls, node, allowed_kus):\n+        return {\n+            pk\n+            for pk in cls._ALL_KUS - allowed_kus\n+            if bitstring.has_named_bit(node, pk)\n+        }\n+\n     def validate(self, node):\n         spki_alg_oid = node.navigate(\n             ':certificate.tbsCertificate.subjectPublicKeyInfo.algorithm.algorithm'\n         ).pdu\n \n-        DS = 'digitalSignature'\n-        NR = 'nonRepudiation'\n-        DE = 'dataEncipherment'\n-        KE = 'keyEncipherment'\n-        KA = 'keyAgreement'\n-        EO = 'encipherOnly'\n-        DO = 'decipherOnly'\n-\n-        allowed_kus = {DS}\n-        is_signing_cert = _has_bit(node, DS)\n+        allowed_kus = {KeyUsageBitName.DIGITAL_SIGNATURE}\n+        is_signing_cert = bitstring.has_named_bit(node, KeyUsageBitName.DIGITAL_SIGNATURE)\n \n         if is_signing_cert:\n-            allowed_kus.add(NR)\n+            allowed_kus.add(KeyUsageBitName.NON_REPUDIATION)\n \n         if spki_alg_oid == rfc3279.rsaEncryption:\n-            allowed_kus.add(KE)\n+            allowed_kus.add(KeyUsageBitName.KEY_ENCIPHERMENT)\n \n-            is_key_mgmt_cert = _has_bit(node, KE)\n+            is_key_mgmt_cert = bitstring.has_named_bit(node, KeyUsageBitName.KEY_ENCIPHERMENT)\n \n             if is_key_mgmt_cert and self._generation != Generation.STRICT:\n-                allowed_kus.add(DE)\n+                allowed_kus.add(KeyUsageBitName.DATA_ENCIPHERMENT)\n         elif spki_alg_oid == rfc5480.id_ecPublicKey:\n-            is_key_mgmt_cert = _has_bit(node, KA)\n+            is_key_mgmt_cert = bitstring.has_named_bit(node, KeyUsageBitName.KEY_AGREEMENT)\n \n             if is_key_mgmt_cert:\n-                allowed_kus.update({EO, DO, KA})\n+                allowed_kus.update({\n+                    KeyUsageBitName.ENCIPHER_ONLY,\n+                    KeyUsageBitName.DECIPHER_ONLY,\n+                    KeyUsageBitName.KEY_AGREEMENT,\n+                })\n         elif spki_alg_oid in {rfc8410.id_Ed448, rfc8410.id_Ed25519}:\n             is_key_mgmt_cert = False\n         else:\n@@ -369,7 +358,7 @@ def validate(self, node):\n         if not is_signing_cert and not is_key_mgmt_cert:\n             raise validation.ValidationFindingEncountered(self.VALIDATION_UNKNOWN_CERT_TYPE)\n \n-        prohibited_kus = _get_prohibited_kus(node, allowed_kus)\n+        prohibited_kus = self._get_prohibited_kus(node, allowed_kus)\n \n         if len(prohibited_kus) > 0:\n             ku_str = ', '.join(sorted(prohibited_kus))\ndiff --git a/pkilint/cabf/smime/smime_name.py b/pkilint/cabf/smime/smime_name.py\nindex 789bf82..0fdb89e 100644\n--- a/pkilint/cabf/smime/smime_name.py\n+++ b/pkilint/cabf/smime/smime_name.py\n@@ -3,12 +3,13 @@\n from pyasn1_alt_modules import rfc5280, rfc8398\n \n from pkilint import validation, pkix, oid\n-from pkilint.cabf import cabf_name, cabf_constants\n-from pkilint.cabf.cabf_name import OrganizationIdentifierAttributeValidator\n+from pkilint.cabf import cabf_name\n+from pkilint.cabf.cabf_name import CabfOrganizationIdentifierAttributeValidator\n from pkilint.cabf.smime.smime_constants import Generation, ValidationLevel\n-from pkilint.iso import lei\n+from pkilint.common import organization_id\n+from pkilint.common.organization_id import OrganizationIdentifierLeiValidator\n from pkilint.itu import x520_name\n-from pkilint.pkix import certificate, name\n+from pkilint.pkix import certificate, name, Rfc2119Word\n \n SHALL = pkix.Rfc2119Word.SHALL\n SHALL_NOT = pkix.Rfc2119Word.SHALL_NOT\n@@ -175,7 +176,8 @@ def validate(self, node):\n             oids = oid.format_oids(self._required_one_of_n_attributes)\n \n             findings.append(validation.ValidationFindingDescription(self.VALIDATION_MISSING_ATTRIBUTE,\n-                                                                    f'Missing one of these required attributes: {oids}'))\n+                                                                    f'Missing one of these required attributes: {oids}')\n+                            )\n \n         findings.extend((\n             validation.ValidationFindingDescription(self.VALIDATION_PROHIBITED_ATTRIBUTE,\n@@ -198,6 +200,40 @@ def validate(self, node):\n         return validation.ValidationResult(self, node, findings)\n \n \n+class CabfSmimeOrganizationIdentifierAttributeValidator(CabfOrganizationIdentifierAttributeValidator):\n+    _REFERENCE_PROHIBITED = (Rfc2119Word.MUST_NOT,\n+                             'cabf.smime.prohibited_organization_identifier_reference_present_for_scheme')\n+\n+    _LEI_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=({organization_id.COUNTRY_CODE_GLOBAL_SCHEME},\n+                       CabfOrganizationIdentifierAttributeValidator.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=CabfOrganizationIdentifierAttributeValidator.STATE_PROVINCE_PROHIBITED,\n+        reference=CabfOrganizationIdentifierAttributeValidator.REFERENCE_REQUIRED\n+    )\n+    _GOV_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_COUNTRY_CODES,\n+                       CabfOrganizationIdentifierAttributeValidator.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=(Rfc2119Word.MAY, None),\n+        reference=_REFERENCE_PROHIBITED\n+    )\n+    _INT_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=({organization_id.COUNTRY_CODE_GLOBAL_SCHEME},\n+                       CabfOrganizationIdentifierAttributeValidator.VALIDATION_ORGANIZATION_ID_INVALID_COUNTRY),\n+        state_province=CabfOrganizationIdentifierAttributeValidator.STATE_PROVINCE_PROHIBITED,\n+        reference=_REFERENCE_PROHIBITED\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            {\n+                'LEI': self._LEI_SCHEME,\n+                'GOV': self._GOV_SCHEME,\n+                'INT': self._INT_SCHEME,\n+            },\n+            enforce_strict_state_province_format=False\n+        )\n+\n+\n class SubscriberAttributeDependencyValidator(validation.Validator):\n     VALIDATION_MISSING_REQUIRED_ATTRIBUTE = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n@@ -248,20 +284,7 @@ def create_subscriber_certificate_subject_validator_container(\n         SubjectAlternativeNameContainsSubjectEmailAddressesValidator(),\n         cabf_name.ValidCountryValidator(),\n         CommonNameValidator(validation_level, generation),\n-        OrganizationIdentifierAttributeValidator(relax_stateprovince_syntax=True, additional_schemes={\n-            'LEI': cabf_constants.RegistrationSchemeNamingConvention(\n-                cabf_constants.RegistrationSchemeCountryIdentifierType.XG,\n-                False, True\n-            ),\n-            'GOV': cabf_constants.RegistrationSchemeNamingConvention(\n-                cabf_constants.RegistrationSchemeCountryIdentifierType.ISO3166,\n-                True, False\n-            ),\n-            'INT': cabf_constants.RegistrationSchemeNamingConvention(\n-                cabf_constants.RegistrationSchemeCountryIdentifierType.XG,\n-                False, False\n-            )\n-        }),\n+        CabfSmimeOrganizationIdentifierAttributeValidator(),\n         OrganizationIdentifierLeiValidator(),\n         OrganizationIdentifierCountryNameConsistentValidator(),\n         cabf_name.RelativeDistinguishedNameContainsOneElementValidator(),\n@@ -409,36 +432,6 @@ def validate(self, node):\n             )\n \n \n-class OrganizationIdentifierLeiValidator(validation.Validator):\n-    VALIDATION_INVALID_ORGID_LEI_FORMAT = validation.ValidationFinding(\n-        validation.ValidationFindingSeverity.ERROR, 'cabf.smime.invalid_lei_scheme_format'\n-    )\n-\n-    _LEI_PREFIX = 'LEIXG-'\n-\n-    def __init__(self):\n-        super().__init__(validations=[\n-            lei.VALIDATION_INVALID_LEI_CHECKSUM, lei.VALIDATION_INVALID_LEI_FORMAT,\n-            self.VALIDATION_INVALID_ORGID_LEI_FORMAT\n-        ],\n-            pdu_class=x520_name.X520OrganizationIdentifier,\n-            predicate=lambda n: any(n.children) and str(n.child[1].pdu).startswith('LEI')\n-        )\n-\n-    def validate(self, node):\n-        value = str(node.child[1].pdu)\n-\n-        if not value.startswith(self._LEI_PREFIX):\n-            raise validation.ValidationFindingEncountered(\n-                self.VALIDATION_INVALID_ORGID_LEI_FORMAT,\n-                f'Invalid Organization Identifier format: \"{value}\"'\n-            )\n-\n-        lei_value = value[len(self._LEI_PREFIX):]\n-\n-        lei.validate_lei(lei_value)\n-\n-\n class OrganizationIdentifierCountryNameConsistentValidator(validation.Validator):\n     VALIDATION_ORGID_COUNTRYNAME_INCONSISTENT = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n@@ -464,15 +457,15 @@ def validate(self, node):\n \n             x520_value_str = str(x520_value_node.pdu)\n \n-            m = cabf_name.ORG_ID_REGEX.match(x520_value_str)\n-\n-            if m is None:\n+            try:\n+                parsed_org_id = organization_id.parse_organization_identifier(x520_value_str)\n+            except ValueError:\n                 continue\n \n-            orgid_country_name = m['country']\n+            orgid_country_name = parsed_org_id.country\n \n-            # skip this orgId attribute if it doesn't contain a countryName or contains XG\n-            if not orgid_country_name or orgid_country_name.upper() == 'XG':\n+            # skip this orgId attribute if it contains the global scheme identifier\n+            if orgid_country_name.casefold() == organization_id.COUNTRY_CODE_GLOBAL_SCHEME.casefold():\n                 continue\n \n             if orgid_country_name.casefold() != country_name_value.casefold():\ndiff --git a/pkilint/common/__init__.py b/pkilint/common/__init__.py\nindex f2bd09d..509a3bc 100644\n--- a/pkilint/common/__init__.py\n+++ b/pkilint/common/__init__.py\n@@ -1,15 +1,15 @@\n-from typing import Callable, Mapping, Optional, NamedTuple\n+from typing import Callable, Mapping, Optional\n \n from pyasn1.type import univ\n-from pyasn1_alt_modules import rfc5280, rfc6962, rfc6960\n+from pyasn1_alt_modules import rfc5280, rfc6962, rfc6960, rfc3739\n \n-from pkilint import validation, document, pkix, oid\n+from pkilint import validation, document, pkix\n from pkilint.cabf.asn1 import ev_guidelines\n from pkilint.itu import x520_name\n from pkilint.pkix import Rfc2119Word\n \n OID_TO_CODE_NAME = {\n-    # EKU\n+    # EKUs\n     rfc5280.id_kp_codeSigning: 'codesigning',\n     rfc5280.id_kp_emailProtection: 'emailprotection',\n     rfc5280.id_kp_timeStamping: 'timestamping',\n@@ -19,7 +19,7 @@\n     rfc5280.id_kp_serverAuth: 'serverauth',\n     rfc5280.id_kp_clientAuth: 'clientauth',\n \n-    # extension\n+    # extensions\n     rfc5280.id_ce_authorityKeyIdentifier: 'authority_key_identifier',\n     rfc5280.id_ce_basicConstraints: 'basic_constraints',\n     rfc5280.id_ce_certificatePolicies: 'certificate_policies',\n@@ -33,6 +33,11 @@\n     rfc5280.id_ce_subjectAltName: 'subject_altname',\n     rfc6962.id_ce_criticalPoison: 'precert_poison',\n     rfc6960.id_pkix_ocsp_nocheck: 'ocsp_nocheck',\n+    rfc3739.id_pe_qcStatements: 'qc_statements',\n+    rfc5280.id_ce_policyMappings: 'policy_mappings',\n+    rfc5280.id_ce_policyConstraints: 'policy_constraints',\n+    rfc5280.id_ce_inhibitAnyPolicy: 'inhibit_any_policy',\n+\n \n     # AIA access methods\n     rfc5280.id_ad_ocsp: 'ocsp',\n@@ -54,16 +59,20 @@\n     ev_guidelines.id_evat_jurisdiction_localityName: 'jurisdiction_locality',\n     rfc5280.id_at_surname: 'surname',\n     rfc5280.id_at_givenName: 'given_name',\n+    x520_name.id_at_organizationIdentifier: 'organization_identifier',\n }\n \n \n class ElementIdentifierAllowanceValidator(validation.Validator):\n-    @staticmethod\n-    def _create_finding(fmt: str, rfc2119word: pkix.Rfc2119Word, o: univ.ObjectIdentifier):\n+    # use global mappings by default\n+    _OID_TO_CODE_NAME = OID_TO_CODE_NAME\n+\n+    @classmethod\n+    def _create_finding(cls, fmt: str, rfc2119word: pkix.Rfc2119Word, o: univ.ObjectIdentifier):\n         if rfc2119word == pkix.Rfc2119Word.MAY:\n             return None\n         else:\n-            return validation.ValidationFinding(rfc2119word.to_severity, fmt.format(oid=OID_TO_CODE_NAME[o]))\n+            return validation.ValidationFinding(rfc2119word.to_severity, fmt.format(oid=cls._OID_TO_CODE_NAME[o]))\n \n     def __init__(self, element_name: str, element_oid_retriever: Callable[[document.PDUNode], document.PDUNode],\n                  known_element_allowances: Mapping[univ.ObjectIdentifier, pkix.Rfc2119Word],\n@@ -76,12 +85,12 @@ def __init__(self, element_name: str, element_oid_retriever: Callable[[document.\n         self._element_oid_retriever = element_oid_retriever\n \n         self._expected_element_presences = {\n-            o: ElementIdentifierAllowanceValidator._create_finding(unexpected_absence_code_format, w, o)\n+            o: self._create_finding(unexpected_absence_code_format, w, o)\n             for o, w in known_element_allowances.items()\n             if w in {Rfc2119Word.MAY, Rfc2119Word.SHOULD, Rfc2119Word.MUST}\n         }\n         self._expected_element_absences = {\n-            o: ElementIdentifierAllowanceValidator._create_finding(unexpected_presence_code_format, w, o)\n+            o: self._create_finding(unexpected_presence_code_format, w, o)\n             for o, w in known_element_allowances.items()\n             if w in {Rfc2119Word.MAY, Rfc2119Word.SHOULD_NOT, Rfc2119Word.MUST_NOT}\n         }\n@@ -123,14 +132,14 @@ def validate(self, node):\n \n \n class ExtensionsPresenceValidator(validation.Validator):\n-    def __init__(self, validation):\n-        self._validation = validation\n+    def __init__(self, validation_extensions_field_absent):\n+        self._validation_extensions_field_absent = validation_extensions_field_absent\n \n-        super().__init__(validations=validation, pdu_class=rfc5280.TBSCertificate)\n+        super().__init__(validations=validation_extensions_field_absent, pdu_class=rfc5280.TBSCertificate)\n \n     def validate(self, node):\n         if 'extensions' not in node.children:\n-            raise validation.ValidationFindingEncountered(self._validation)\n+            raise validation.ValidationFindingEncountered(self._validation_extensions_field_absent)\n \n \n class ExtensionIdentifierAllowanceValidator(ElementIdentifierAllowanceValidator):\ndiff --git a/pkilint/common/alternative_name.py b/pkilint/common/alternative_name.py\nnew file mode 100644\nindex 0000000..939cdad\n--- /dev/null\n+++ b/pkilint/common/alternative_name.py\n@@ -0,0 +1,185 @@\n+import ipaddress\n+from urllib.parse import urlparse\n+\n+import publicsuffixlist\n+from pyasn1_alt_modules import rfc8398, rfc5280\n+\n+from pkilint import validation\n+from pkilint.pkix import general_name\n+\n+\n+class InternalDomainNameValidator(validation.Validator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding, *args, **kwargs):\n+        self._psl = publicsuffixlist.PublicSuffixList(accept_unknown=False)\n+\n+        super().__init__(validations=[validation_internal_domain_name_present], **kwargs)\n+\n+        self._validation_internal_domain_name_present = validation_internal_domain_name_present\n+\n+    @classmethod\n+    def extract_domain_name(cls, node):\n+        return str(node.pdu)\n+\n+    def validate_with_value(self, node, value):\n+        if self._psl.publicsuffix(value) is None:\n+            raise validation.ValidationFindingEncountered(\n+                self._validation_internal_domain_name_present,\n+                f'Internal domain name: \"{value}\"'\n+            )\n+\n+    def validate(self, node):\n+        domain_name = self.extract_domain_name(node)\n+\n+        return self.validate_with_value(node, domain_name)\n+\n+\n+class GeneralNameDnsNameInternalDomainNameValidator(InternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding, allow_onion_tld=False):\n+        self._allow_onion_tld = allow_onion_tld\n+\n+        super().__init__(\n+            validation_internal_domain_name_present,\n+            predicate=general_name.create_generalname_type_predicate(general_name.GeneralNameTypeName.DNS_NAME)\n+        )\n+\n+    def validate_with_value(self, node, value):\n+        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n+            return\n+        else:\n+            if self._allow_onion_tld and value.lower().endswith('.onion'):\n+                return\n+            else:\n+                return super().validate_with_value(node, value)\n+\n+\n+class UriInternalDomainNameValidator(InternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding, *args, **kwargs):\n+        super().__init__(validation_internal_domain_name_present, *args, **kwargs)\n+\n+    def extract_domain_name(self, node):\n+        return urlparse(str(node.pdu)).hostname or ''\n+\n+\n+class GeneralNameUriInternalDomainNameValidator(InternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding):\n+        super().__init__(\n+            validation_internal_domain_name_present,\n+            predicate=general_name.create_generalname_type_predicate(\n+                general_name.GeneralNameTypeName.UNIFORM_RESOURCE_IDENTIFIER\n+            )\n+        )\n+\n+    def extract_domain_name(self, node):\n+        if general_name.is_nameconstraints_child_node(node):\n+            return str(node.pdu).lstrip('.')\n+        else:\n+            return urlparse(str(node.pdu)).hostname or ''\n+\n+    def validate_with_value(self, node, value):\n+        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n+            return\n+        else:\n+            return super().validate_with_value(node, value)\n+\n+\n+class EmailAddressInternalDomainNameValidator(InternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding, *args, **kwargs):\n+        super().__init__(validation_internal_domain_name_present, *args, **kwargs)\n+\n+    def extract_domain_name(self, node):\n+        parts = str(node.pdu).split('@', maxsplit=1)\n+\n+        return parts[1] if len(parts) == 2 else ''\n+\n+\n+class GeneralNameRfc822NameInternalDomainNameValidator(EmailAddressInternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding):\n+        super().__init__(\n+            validation_internal_domain_name_present,\n+            predicate=general_name.create_generalname_type_predicate(general_name.GeneralNameTypeName.RFC822_NAME)\n+        )\n+\n+    def extract_domain_name(self, node):\n+        if general_name.is_nameconstraints_child_node(node):\n+            return str(node.pdu).lstrip('.')\n+        else:\n+            return super().extract_domain_name(node)\n+\n+    def validate_with_value(self, node, value):\n+        if len(value) == 0 and general_name.is_nameconstraints_child_node(node):\n+            return\n+        else:\n+            super().validate_with_value(node, value)\n+\n+\n+class SmtpUtf8MailboxInternalDomainNameValidator(EmailAddressInternalDomainNameValidator):\n+    def __init__(self, validation_internal_domain_name_present: validation.ValidationFinding):\n+        super().__init__(validation_internal_domain_name_present, pdu_class=rfc8398.SmtpUTF8Mailbox)\n+\n+    def extract_domain_name(self, node):\n+        domain_part = super().extract_domain_name(node)\n+\n+        # remove ToASCII once RFC 9598 is published\n+        return domain_part.encode('idna').decode()\n+\n+\n+class InternalIpAddressValidator(validation.Validator):\n+    def __init__(self, validation_internal_ip_address_present: validation.ValidationFinding, *args, **kwargs):\n+        super().__init__(validations=[validation_internal_ip_address_present], **kwargs)\n+\n+        self._validation_internal_ip_address_present = validation_internal_ip_address_present\n+\n+    @classmethod\n+    def _extract_ip_address(cls, node):\n+        octets = node.pdu.asOctets()\n+\n+        if len(octets) == 4:\n+            return ipaddress.IPv4Address(octets)\n+        else:\n+            return ipaddress.IPv6Address(octets)\n+\n+    def validate(self, node):\n+        ip_addr = self._extract_ip_address(node)\n+\n+        if not ip_addr.is_global:\n+            raise validation.ValidationFindingEncountered(\n+                self._validation_internal_ip_address_present,\n+                f'Internal IP address: \"{ip_addr}\"'\n+            )\n+\n+\n+class GeneralNameInternalIpAddressValidator(InternalIpAddressValidator):\n+    def __init__(self, validation_internal_ip_address_present: validation.ValidationFinding):\n+        super().__init__(\n+            validation_internal_ip_address_present,\n+            predicate=general_name.create_generalname_type_predicate(general_name.GeneralNameTypeName.IP_ADDRESS)\n+        )\n+\n+    def validate(self, node):\n+        if general_name.is_nameconstraints_child_node(node):\n+            return\n+\n+        super().validate(node)\n+\n+\n+def create_internal_name_validator_container(\n+    validation_internal_domain_name_present: validation.ValidationFinding,\n+    validation_internal_ip_address_present: validation.ValidationFinding,\n+    allow_onion_tld: bool = False\n+):\n+    validators = [\n+        GeneralNameDnsNameInternalDomainNameValidator(validation_internal_domain_name_present, allow_onion_tld),\n+        GeneralNameUriInternalDomainNameValidator(validation_internal_domain_name_present),\n+        GeneralNameRfc822NameInternalDomainNameValidator(validation_internal_domain_name_present),\n+        SmtpUtf8MailboxInternalDomainNameValidator(validation_internal_domain_name_present),\n+        GeneralNameInternalIpAddressValidator(validation_internal_ip_address_present),\n+    ]\n+\n+    return validation.ValidatorContainer(\n+        validators=validators,\n+        pdu_class=rfc5280.GeneralName\n+    )\n+\n+\n+def create_cpsuri_internal_domain_name_validator(validation_internal_domain_name_present: validation.ValidationFinding):\n+    return UriInternalDomainNameValidator(validation_internal_domain_name_present, pdu_class=rfc5280.CPSuri)\ndiff --git a/pkilint/common/common_name.py b/pkilint/common/common_name.py\nnew file mode 100644\nindex 0000000..e8f97f9\n--- /dev/null\n+++ b/pkilint/common/common_name.py\n@@ -0,0 +1,72 @@\n+import ipaddress\n+import typing\n+\n+from pyasn1_alt_modules import rfc5280\n+\n+from pkilint import validation, document\n+from pkilint.pkix import general_name\n+\n+\n+class CommonNameValidator(validation.Validator):\n+    def __init__(\n+            self,\n+            allowed_general_name_types: typing.Set[str],\n+            validation_unknown_value_source: validation.ValidationFinding):\n+        super().__init__(\n+            validations=[validation_unknown_value_source],\n+            pdu_class=rfc5280.X520CommonName)\n+\n+        self._allowed_general_name_types = allowed_general_name_types\n+        self._validation_unknown_value_source = validation_unknown_value_source\n+\n+    def validate(self, node):\n+        # unparsed CN, return\n+        if not any(node.children):\n+            return\n+\n+        _, value_node = node.child\n+        value_str = str(value_node.pdu)\n+\n+        san_ext_and_idx = node.document.get_extension_by_oid(rfc5280.id_ce_subjectAltName)\n+\n+        if san_ext_and_idx is None:\n+            raise validation.ValidationFindingEncountered(\n+                self._validation_unknown_value_source,\n+                f'Unknown source for value of common name: \"{value_str}\"'\n+            )\n+\n+        san_ext_node, _ = san_ext_and_idx\n+\n+        try:\n+            san_value_node = san_ext_node.navigate('extnValue.subjectAltName')\n+        except document.PDUNavigationFailedError:\n+            # unparsed SAN extension, return\n+            return\n+\n+        for gn in san_value_node.children.values():\n+            gn_type, gn_value = gn.child\n+\n+            if gn_type not in self._allowed_general_name_types:\n+                continue\n+\n+            if gn_type == general_name.GeneralNameTypeName.DNS_NAME:\n+                if str(gn_value.pdu) == value_str:\n+                    return\n+            elif gn_type == general_name.GeneralNameTypeName.IP_ADDRESS:\n+                address_octets = gn_value.pdu.asOctets()\n+\n+                if len(address_octets) == 4:\n+                    ip_addr = ipaddress.IPv4Address(address_octets)\n+                elif len(address_octets) == 16:\n+                    ip_addr = ipaddress.IPv6Address(address_octets)\n+                else:\n+                    # Whoa, Nellie! Let the PKIX validator complain about this one\n+                    continue\n+\n+                if str(ip_addr) == value_str:\n+                    return\n+\n+        raise validation.ValidationFindingEncountered(\n+            self._validation_unknown_value_source,\n+            f'Unknown source for value of common name: \"{value_str}\"'\n+        )\ndiff --git a/pkilint/common/organization_id.py b/pkilint/common/organization_id.py\nnew file mode 100644\nindex 0000000..7928510\n--- /dev/null\n+++ b/pkilint/common/organization_id.py\n@@ -0,0 +1,207 @@\n+import re\n+from typing import NamedTuple, Optional, Set, Tuple, Dict, List\n+\n+from iso3166 import countries_by_alpha2\n+\n+from pkilint import validation, document\n+from pkilint.iso import lei\n+from pkilint.itu import x520_name\n+from pkilint.pkix import Rfc2119Word\n+\n+_ORG_ID_REGEX = re.compile(\n+    r'^(?P<scheme>[A-Z]{2}[A-Z:])(?P<country>[a-zA-Z]{2})(\\+(?P<sp>[a-zA-Z0-9]{1,3}))?'\n+    r'(-(?P<reference>.+))?$'\n+)\n+\n+# alternative country codes\n+COUNTRY_CODE_GREECE_TRADITIONAL = 'EL'\n+COUNTRY_CODE_NORTHERN_IRELAND = 'XI'\n+\n+# multi-national codes\n+COUNTRY_CODE_GLOBAL_SCHEME = 'XG'\n+COUNTRY_CODE_EUROPEAN_UNION = 'EU'\n+COUNTRY_CODE_EUROZONE = 'EZ'\n+COUNTRY_CODE_USSR = 'SU'\n+COUNTRY_CODE_UNITED_NATIONS = 'UN'\n+\n+TRANSNATIONAL_COUNTRY_CODES = {\n+    COUNTRY_CODE_EUROPEAN_UNION,\n+    COUNTRY_CODE_EUROZONE,\n+    COUNTRY_CODE_USSR,\n+    COUNTRY_CODE_UNITED_NATIONS,\n+}\n+\n+ISO3166_1_COUNTRY_CODES = set(countries_by_alpha2.keys())\n+ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES = ISO3166_1_COUNTRY_CODES | TRANSNATIONAL_COUNTRY_CODES\n+\n+\n+LEI_PREFIX = 'LEIXG-'\n+\n+\n+class ParsedOrganizationIdentifier(NamedTuple):\n+    raw: Optional[str]\n+    scheme: str\n+    is_national_scheme: bool\n+    country: str\n+    state_province: Optional[str]\n+    reference: Optional[str]\n+\n+\n+def parse_organization_identifier(value: str) -> ParsedOrganizationIdentifier:\n+    m = _ORG_ID_REGEX.match(value)\n+\n+    if m is None:\n+        raise ValueError(f'Invalid organization identifier syntax: \"{value}\"')\n+\n+    is_national_scheme = m['scheme'].endswith(':')\n+\n+    scheme = m['scheme'][:2] if is_national_scheme else m['scheme']\n+\n+    return ParsedOrganizationIdentifier(value, scheme, is_national_scheme, m.group('country'), m.group('sp'),\n+                                        m.group('reference'))\n+\n+\n+def assert_parsed_organization_identifier_equal(\n+        org1: ParsedOrganizationIdentifier, org1_source: str, org2: ParsedOrganizationIdentifier, org2_source: str):\n+    if org1.scheme != org2.scheme:\n+        raise ValueError(f'Mismatched scheme: {org1_source}: \"{org1.scheme}\", {org2_source}: \"{org2.scheme}\"')\n+\n+    if org1.country.casefold() != org2.country.casefold():\n+        raise ValueError(f'Mismatched country: {org1_source}: \"{org1.country}\", {org2_source}: \"{org2.country}\"')\n+\n+    if org1.state_province != org2.state_province:\n+        org1_state_province = '' if org1.state_province is None else org1.state_province\n+        org2_state_province = '' if org2.state_province is None else org2.state_province\n+\n+        raise ValueError(f'Mismatched state/province: {org1_source}: \"{org1_state_province}\", '\n+                         f'{org2_source}: \"{org2_state_province}\"')\n+\n+    if org1.reference != org2.reference:\n+        org1_reference = '' if org1.reference is None else org1.reference\n+        org2_reference = '' if org2.reference is None else org2.reference\n+\n+        raise ValueError(f'Mismatched registration reference: {org1_source}: \"{org1_reference}\", '\n+                         f'{org2_source}: \"{org2_reference}\"')\n+\n+\n+class OrganizationIdentifierLeiValidator(validation.Validator):\n+    def __init__(self):\n+        super().__init__(validations=[lei.VALIDATION_INVALID_LEI_CHECKSUM, lei.VALIDATION_INVALID_LEI_FORMAT],\n+                         pdu_class=x520_name.X520OrganizationIdentifier,\n+                         predicate=lambda n: any(n.children) and str(n.child[1].pdu).startswith('LEI')\n+                         )\n+\n+    def validate(self, node):\n+        value = str(node.child[1].pdu)\n+\n+        if not value.startswith(LEI_PREFIX):\n+            # let the syntax validator report this problem\n+            return\n+\n+        lei_value = value[len(LEI_PREFIX):]\n+\n+        lei.validate_lei(lei_value)\n+\n+\n+class OrganizationIdentifierElementAllowance(NamedTuple):\n+    country_codes: Tuple[Set[str], validation.ValidationFinding]\n+    state_province: Tuple[Rfc2119Word, Optional[str]]\n+    reference: Tuple[Rfc2119Word, Optional[str]]\n+\n+\n+class OrganizationIdentifierValidatorBase(validation.Validator):\n+    def __init__(self, element_allowances: Dict[str, OrganizationIdentifierElementAllowance],\n+                 invalid_format_validation: Optional[validation.ValidationFinding],\n+                 additional_validations: Optional[List[validation.ValidationFinding]] = None,\n+                 **kwargs):\n+        if additional_validations is None:\n+            additional_validations = []\n+\n+        self._element_allowances = element_allowances.copy()\n+\n+        self._invalid_format_validation = invalid_format_validation\n+\n+        validations = [] + additional_validations\n+\n+        if self._invalid_format_validation is not None:\n+            validations.append(self._invalid_format_validation)\n+\n+        for allowance in element_allowances.values():\n+            validations.append(allowance.country_codes[1])\n+\n+            OrganizationIdentifierValidatorBase._create_and_append_validation_finding(\n+                validations, *allowance.state_province\n+            )\n+            OrganizationIdentifierValidatorBase._create_and_append_validation_finding(\n+                validations, *allowance.reference\n+            )\n+\n+        super().__init__(validations=validations, **kwargs)\n+\n+    @staticmethod\n+    def _create_and_append_validation_finding(validations, presence_word, finding_code):\n+        if presence_word != Rfc2119Word.MAY:\n+            validations.append(validation.ValidationFinding(presence_word.to_severity, finding_code))\n+\n+    @classmethod\n+    def parse_organization_id_node(cls, node: document.PDUNode) -> ParsedOrganizationIdentifier:\n+        pass\n+\n+    @classmethod\n+    def handle_unknown_scheme(cls, node: document.PDUNode, parsed: ParsedOrganizationIdentifier):\n+        pass\n+\n+    def validate_with_parsed_value(self, node: document.PDUNode, parsed: ParsedOrganizationIdentifier):\n+        scheme_allowance = self._element_allowances.get(parsed.scheme)\n+\n+        if scheme_allowance is None:\n+            return self.handle_unknown_scheme(node, parsed)\n+\n+        allowed_country_codes, finding = scheme_allowance.country_codes\n+\n+        findings = []\n+        if parsed.country not in allowed_country_codes:\n+            findings.append(\n+                validation.ValidationFindingDescription(\n+                    finding,\n+                    f'Invalid country code for registration scheme \"{parsed.scheme}\": \"{parsed.country}\"'\n+                )\n+            )\n+\n+        allowance, finding_code = scheme_allowance.state_province\n+\n+        if parsed.state_province is None and allowance in {Rfc2119Word.SHOULD, Rfc2119Word.MUST}:\n+            findings.append(validation.ValidationFindingDescription(\n+                validation.ValidationFinding(allowance.to_severity, finding_code),\n+                f'State/province missing for registration scheme \"{parsed.scheme}\"'\n+            ))\n+        elif parsed.state_province is not None and allowance in {Rfc2119Word.SHOULD_NOT,\n+                                                                 Rfc2119Word.MUST_NOT}:\n+            findings.append(validation.ValidationFindingDescription(\n+                validation.ValidationFinding(allowance.to_severity, finding_code),\n+                f'State/province value present for scheme \"{parsed.scheme}\": \"{parsed.state_province}\"'\n+            ))\n+\n+        allowance, finding_code = scheme_allowance.reference\n+\n+        if parsed.reference is None and allowance in {Rfc2119Word.SHOULD, Rfc2119Word.MUST}:\n+            findings.append(validation.ValidationFindingDescription(\n+                validation.ValidationFinding(allowance.to_severity, finding_code),\n+                f'Registration reference missing for scheme \"{parsed.scheme}\"'\n+            ))\n+        elif parsed.reference is not None and allowance in {Rfc2119Word.SHOULD_NOT,\n+                                                            Rfc2119Word.MUST_NOT}:\n+            findings.append(validation.ValidationFindingDescription(\n+                validation.ValidationFinding(allowance.to_severity, finding_code),\n+                f'Registration reference present for scheme \"{parsed.scheme}\": \"{parsed.reference}\"'\n+            ))\n+\n+        return validation.ValidationResult(self, node, findings)\n+\n+    def validate(self, node):\n+        try:\n+            parsed = self.parse_organization_id_node(node)\n+        except ValueError as e:\n+            raise validation.ValidationFindingEncountered(self._invalid_format_validation, str(e))\n+\n+        return self.validate_with_parsed_value(node, parsed)\ndiff --git a/pkilint/document.py b/pkilint/document.py\nindex ccee795..1e72652 100644\n--- a/pkilint/document.py\n+++ b/pkilint/document.py\n@@ -2,7 +2,7 @@\n import datetime\n import logging\n import re\n-from typing import Callable, Mapping, Tuple, Type, Union, Optional, Dict, List\n+from typing import Callable, Mapping, Tuple, Type, Union, Optional, Dict, List, NamedTuple\n \n from pyasn1.codec.der.decoder import decode\n from pyasn1.codec.der.encoder import encode\n@@ -12,11 +12,22 @@\n                               Choice, BitString\n                               )\n \n+\n logger = logging.getLogger(__name__)\n \n PATH_REGEX = re.compile(r'^((?P<doc_name>[^:]*):)?(?P<node_path>([^.]+\\.)*[^.]+)?$')\n \n \n+try:\n+    # noinspection PyUnresolvedReferences\n+    from pyasn1_fasder import decode_der\n+\n+    logging.info('Using pyasn1-fasder for ASN.1 DER decoding')\n+    _USE_PYASN1_FASDER = True\n+except ImportError:\n+    _USE_PYASN1_FASDER = False\n+\n+\n class PDUNavigationFailedError(Exception):\n     \"\"\"Represents the failure to find the requested node in a document.\"\"\"\n \n@@ -26,8 +37,8 @@ def __init__(self, requested_path: str, traversed_path: str,\n         \"\"\"Creates an instance of an exception that represents a PDU node lookup failure.\n \n         Args: requested_path: The requested path relative to the node which\n-        :py:method:`pkilint.document.PDUNode.navigate was called. traversed_path: The relative path that was able to\n-        be traversed, as those nodes exist. missing_node_name: The name of the node which could not be found.\n+        :py:method:`pkilint.document.PDUNode.navigate was called. traversed_path: The relative path that was\n+        traversed, as those nodes exist. missing_node_name: The name of the node which could not be found.\n         \"\"\"\n         self.requested_path = requested_path\n         self.traversed_path = traversed_path\n@@ -50,12 +61,11 @@ def __init__(\n         \"\"\"Creates a new Document instance. It is not intended that this class be directly\n         instantiated by user code; use a sub-class of this class instead.\n \n-        Args:\n-            pdu_schema_instance: A pyasn1 ASN.1 instance that represents the top-level ASN.1 schema for this document.\n-            substrate_source: The source of the document. This can be a URI, file name, or any other identifier.\n-            substrate: The raw DER-encoded document.\n-            name: An optional name given to the document. May be useful when a :py:meth:`pkilint.validation.Validator` requires multiple documents.\n-            parent: An optional collection of documents that are related.\n+        Args: pdu_schema_instance: A pyasn1 ASN.1 instance that represents the top-level ASN.1 schema for this\n+        document. substrate_source: The source of the document. This can be a URI, file name, or any other\n+        identifier. substrate: The raw DER-encoded document. name: An optional name given to the document. May be\n+        useful when a :py:meth:`pkilint.validation.Validator` requires multiple documents. parent: An optional\n+        collection of documents that are related.\n         \"\"\"\n         self.pdu_schema_instance = pdu_schema_instance\n         self.substrate = substrate\n@@ -90,12 +100,10 @@ def __init__(self, document: Document, name: str, pdu: Asn1Type,\n                  ):\n         \"\"\"Creates a new instance representing a node within a document.\n \n-        Args:\n-            document: The document which contains this node.\n-            name: The name of the node. Generally will match the name of a component\n-            within an ASN.1 SEQUENCE.\n-            pdu: The underlying ASN.1 value.\n-            parent: The node which contains this node. In the case where the current node is the top-level node of a document, this will not be populated.\n+        Args: document: The document which contains this node. name: The name of the node. Generally will match the\n+        name of a component within an ASN.1 SEQUENCE. pdu: The underlying ASN.1 value. parent: The node which\n+        contains this node. In the case where the current node is the top-level node of a document, this will not be\n+        populated.\n         \"\"\"\n         self.document = document\n         self.name = name\n@@ -203,6 +211,7 @@ def _generate_child_nodes(self):\n             )\n             }\n         elif isinstance(self.pdu, SequenceOfAndSetOfBase):\n+            # noinspection PyTypeChecker\n             return {\n                 str(i): PDUNode(\n                     self.document, str(i), component, self\n@@ -264,13 +273,21 @@ def __init__(self, value_node: PDUNode, type_oid: ObjectIdentifier,\n         self.message = message\n \n \n+class OptionalAsn1TypeWrapper(NamedTuple):\n+    asn1_type: Asn1Type\n+\n+\n class ValueDecoder(object):\n     _BITSTRING_SCHEMA_OBJ = BitString()\n \n     VALUE_NODE_ABSENT = object()\n \n-    def __init__(self, *, type_path: str, value_path: str,\n-                 type_mappings: Dict[ObjectIdentifier, Optional[Asn1Type]], default: Optional[Asn1Type] = None):\n+    def __init__(self,\n+                 *,\n+                 type_path: str,\n+                 value_path: str,\n+                 type_mappings: Dict[ObjectIdentifier, Union[Asn1Type, OptionalAsn1TypeWrapper]],\n+                 default: Optional[Union[Asn1Type, OptionalAsn1TypeWrapper]] = None):\n         self.type_path = type_path\n         self.value_path = value_path\n         self.type_mappings = type_mappings.copy()\n@@ -293,14 +310,22 @@ def __call__(self, node):\n         pdu_type = self.type_mappings.get(type_node.pdu, self.default)\n \n         if pdu_type is not None:\n+            if type(pdu_type) is OptionalAsn1TypeWrapper:\n+                pdu_type = pdu_type.asn1_type\n+\n+                if value_node is None:\n+                    # nothing to decode, so return\n+                    return\n+\n             # value node must be absent, but it exists\n-            if pdu_type is self.VALUE_NODE_ABSENT and value_node is not None:\n+            elif pdu_type is self.VALUE_NODE_ABSENT and value_node is not None:\n                 raise ValueDecodingFailedError(\n                     value_node, type_node.pdu, pdu_type,\n                     'Value node is present, but the ASN.1 schema specifies that it must be absent'\n                 )\n+\n             # value node must be present, but it doesn't exist\n-            if pdu_type is not self.VALUE_NODE_ABSENT and value_node is None:\n+            elif pdu_type is not self.VALUE_NODE_ABSENT and value_node is None:\n                 raise ValueDecodingFailedError(\n                     node, type_node.pdu, pdu_type,\n                     'Value node is absent, but the ASN.1 schema specifies that it must be present'\n@@ -340,24 +365,29 @@ def decode_substrate(source_document: Document, substrate: bytes,\n                      )\n         return next(iter(parent_node.children.values()))\n \n-    decoded, rest = decode(substrate, asn1Spec=pdu_instance)\n+    if _USE_PYASN1_FASDER:\n+        decoded, _ = decode_der(substrate, asn1Spec=pdu_instance)\n+\n+        decoded_pdu_name = get_node_name_for_pdu(decoded)\n+    else:\n+        decoded, rest = decode(substrate, asn1Spec=pdu_instance)\n \n-    decoded_pdu_name = get_node_name_for_pdu(decoded)\n+        decoded_pdu_name = get_node_name_for_pdu(decoded)\n+\n+        if len(rest) > 0:\n+            raise ValueError(\n+                \"Unexpected {} octets following {} DER in {}: {}\".format(\n+                    len(rest), decoded_pdu_name, source_document.substrate_source,\n+                    binascii.hexlify(rest).decode('us-ascii')\n+                )\n+            )\n \n-    if len(rest) > 0:\n-        raise ValueError(\n-            \"Unexpected {} octets following {} DER in {}: {}\".format(\n-                len(rest), decoded_pdu_name, source_document.substrate_source,\n-                binascii.hexlify(rest).decode('us-ascii')\n+        encoded = encode(decoded)\n+        if encoded != substrate:\n+            type_name = decoded.__class__.__name__\n+            raise ValueError(\n+                f'Substrate of type \"{type_name}\" is not DER-encoded'\n             )\n-        )\n-\n-    encoded = encode(decoded)\n-    if encoded != substrate:\n-        type_name = decoded.__class__.__name__\n-        raise ValueError(\n-            f'Substrate of type \"{type_name}\" is not DER-encoded'\n-        )\n \n     node = PDUNode(source_document, decoded_pdu_name, decoded, parent_node)\n \ndiff --git a/pkilint/etsi/__init__.py b/pkilint/etsi/__init__.py\nindex 89ff314..142c04e 100644\n--- a/pkilint/etsi/__init__.py\n+++ b/pkilint/etsi/__init__.py\n@@ -1,14 +1,353 @@\n-from pkilint.etsi.asn1 import en_319_412_5, ts_119_495\n-from pkilint import document\n-\n-\n-ETSI_QC_STATEMENTS_MAPPINGS = {\n-    en_319_412_5.id_etsi_qcs_QcCompliance: document.ValueDecoder.VALUE_NODE_ABSENT,\n-    en_319_412_5.id_etsi_qcs_QcLimitValue: en_319_412_5.QcEuLimitValue(),\n-    en_319_412_5.id_etsi_qcs_QcRetentionPeriod: en_319_412_5.QcEuRetentionPeriod(),\n-    en_319_412_5.id_etsi_qcs_QcSSCD: document.ValueDecoder.VALUE_NODE_ABSENT,\n-    en_319_412_5.id_etsi_qcs_QcPDS: en_319_412_5.QcEuPDS(),\n-    en_319_412_5.id_etsi_qcs_QcType: en_319_412_5.QcType(),\n-    en_319_412_5.id_etsi_qcs_QcCClegislation: en_319_412_5.QcCClegislation(),\n-    ts_119_495.id_etsi_psd2_qcStatement: ts_119_495.PSD2QcType(),\n-}\n+import typing\n+from typing import List\n+\n+from pyasn1_alt_modules import rfc5280, rfc6962, rfc3739\n+\n+from pkilint import validation, finding_filter, cabf, document\n+from pkilint.cabf import serverauth\n+from pkilint.cabf.serverauth import serverauth_constants, serverauth_name, serverauth_finding_filter\n+from pkilint.common import organization_id, alternative_name\n+from pkilint.etsi import (\n+    etsi_constants, ts_119_495, en_319_412_5, en_319_412_1, en_319_412_2, en_319_412_3,\n+    ts_119_312, en_319_412_4, etsi_shared, etsi_finding_filter, en_319_411_1\n+)\n+from pkilint.etsi.asn1 import (\n+    en_319_412_1 as en_319_412_asn1, en_319_412_5 as en_319_412_5_asn1, ts_119_495 as ts_119_495_asn1\n+)\n+from pkilint.etsi.etsi_constants import CertificateType\n+from pkilint.itu import x520_name_unbounded\n+from pkilint.pkix import certificate\n+\n+\n+def determine_certificate_type(cert: certificate.RFC5280Certificate) -> CertificateType:\n+    qualified_statement_ids = cert.qualified_statement_ids\n+    policy_oids = cert.policy_oids\n+\n+    is_qualified = en_319_412_5_asn1.id_etsi_qcs_QcCompliance in qualified_statement_ids\n+    is_eidas_qualified = is_qualified and en_319_412_5_asn1.id_etsi_qcs_QcCClegislation not in qualified_statement_ids\n+    is_precert = cert.get_extension_by_oid(rfc6962.id_ce_criticalPoison) is not None\n+    is_webauth = rfc5280.id_kp_serverAuth in cert.extended_key_usages\n+\n+    if serverauth_constants.ID_POLICY_EV in policy_oids:\n+        is_psd2 = ts_119_495_asn1.id_etsi_psd2_qcStatement in qualified_statement_ids\n+\n+        if is_psd2:\n+            return (\n+                CertificateType.QEVCP_W_PSD2_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QEVCP_W_PSD2_EIDAS_FINAL_CERTIFICATE\n+            )\n+        elif is_eidas_qualified:\n+            return (\n+                CertificateType.QEVCP_W_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QEVCP_W_EIDAS_FINAL_CERTIFICATE\n+            )\n+        elif is_qualified:\n+            return (\n+                CertificateType.QEVCP_W_NON_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QEVCP_W_NON_EIDAS_FINAL_CERTIFICATE\n+            )\n+        else:\n+            return CertificateType.EVCP_PRE_CERTIFICATE if is_precert else CertificateType.EVCP_FINAL_CERTIFICATE\n+    elif serverauth_constants.ID_POLICY_OV in policy_oids:\n+        if is_eidas_qualified:\n+            return (\n+                CertificateType.QNCP_W_OV_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QNCP_W_OV_EIDAS_FINAL_CERTIFICATE\n+            )\n+        elif is_qualified:\n+            return (\n+                CertificateType.QNCP_W_OV_NON_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QNCP_W_OV_NON_EIDAS_FINAL_CERTIFICATE\n+            )\n+        else:\n+            return CertificateType.OVCP_PRE_CERTIFICATE if is_precert else CertificateType.OVCP_FINAL_CERTIFICATE\n+    elif serverauth_constants.ID_POLICY_IV in policy_oids:\n+        if is_eidas_qualified:\n+            return (\n+                CertificateType.QNCP_W_IV_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QNCP_W_IV_EIDAS_FINAL_CERTIFICATE\n+            )\n+        elif is_qualified:\n+            return (\n+                CertificateType.QNCP_W_IV_NON_EIDAS_PRE_CERTIFICATE if is_precert\n+                else CertificateType.QNCP_W_IV_NON_EIDAS_FINAL_CERTIFICATE\n+            )\n+        else:\n+            return CertificateType.IVCP_PRE_CERTIFICATE if is_precert else CertificateType.IVCP_FINAL_CERTIFICATE\n+    elif serverauth_constants.ID_POLICY_DV in policy_oids:\n+        return CertificateType.DVCP_PRE_CERTIFICATE if is_precert else CertificateType.DVCP_FINAL_CERTIFICATE\n+    else:\n+        is_natural_person = any((\n+            cert.get_subject_attributes_by_type(rfc5280.id_at_givenName),\n+            cert.get_subject_attributes_by_type(rfc5280.id_at_surname),\n+            cert.get_subject_attributes_by_type(rfc5280.id_at_pseudonym),\n+        ))\n+\n+        if is_natural_person:\n+            if is_webauth:\n+                if is_eidas_qualified:\n+                    return (\n+                        CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_PRE_CERTIFICATE if is_precert\n+                        else CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_FINAL_CERTIFICATE\n+                    )\n+                elif is_qualified:\n+                    return (\n+                        CertificateType.QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_PRE_CERTIFICATE if is_precert\n+                        else CertificateType.QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE\n+                    )\n+                else:\n+                    return (\n+                        CertificateType.NCP_W_NATURAL_PERSON_PRE_CERTIFICATE if is_webauth\n+                        else CertificateType.NCP_W_NATURAL_PERSON_FINAL_CERTIFICATE\n+                    )\n+\n+            return CertificateType.NCP_NATURAL_PERSON_CERTIFICATE\n+        else:\n+            if is_webauth:\n+                if is_eidas_qualified:\n+                    return (\n+                        CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_PRE_CERTIFICATE if is_precert\n+                        else CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_FINAL_CERTIFICATE\n+                    )\n+                elif is_qualified:\n+                    return (\n+                        CertificateType.QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_PRE_CERTIFICATE if is_precert\n+                        else CertificateType.QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE\n+                    )\n+                else:\n+                    return (\n+                        CertificateType.NCP_W_LEGAL_PERSON_PRE_CERTIFICATE if is_webauth\n+                        else CertificateType.NCP_W_LEGAL_PERSON_FINAL_CERTIFICATE\n+                    )\n+\n+            return CertificateType.NCP_LEGAL_PERSON_CERTIFICATE\n+\n+\n+def create_decoding_validators(certificate_type: CertificateType) -> List[validation.Validator]:\n+    if certificate_type in etsi_constants.CABF_CERTIFICATE_TYPES:\n+        return serverauth.create_decoding_validators()\n+    else:\n+        name_attribute_mappings = cabf.NAME_ATTRIBUTE_MAPPINGS.copy()\n+\n+        \"\"\"\n+        From EN 319 412-2:\n+        NAT-4.2.4-18 If present, the size of givenName, surname, pseudonym, commonName, organizationName and\n+        organizationalUnitName may be longer than the limit as stated in IETF RFC 5280 [1].\n+        \n+        From EN 319 412-3:\n+        LEG-4.2.1-9: If present, the size of organizationName, organizationalUnitName and commonName may\n+        be longer than the limit as stated in IETF RFC 5280 [3].\n+        \n+        From EN 319 412-4:\n+        WEB-4.1.3-4: The following certificate profile requirements specified in the BRG [9] shall apply for subject\n+        certificate fields addressed by the following sub-sections of BRG:\n+        ...\n+        (c) 7.1.4.2.2 Subject Distinguished Name - commonName.\n+        \"\"\"\n+        name_attribute_mappings.update({\n+            k: v for k, v in x520_name_unbounded.UNBOUNDED_ATTRIBUTE_TYPE_MAPPINGS.items()\n+            if k in (\n+                rfc5280.id_at_organizationName,\n+                rfc5280.id_at_organizationalUnitName,\n+                rfc5280.id_at_pseudonym,\n+            )\n+        })\n+\n+        if certificate_type not in etsi_constants.WEB_AUTHENTICATION_CERTIFICATE_TYPES:\n+            name_attribute_mappings[rfc5280.id_at_commonName]: x520_name_unbounded.X520CommonNameUnbounded()\n+\n+        additional_validators = [\n+            certificate.create_qc_statements_decoder(asn1.ETSI_QC_STATEMENTS_MAPPINGS)\n+        ]\n+\n+        return certificate.create_decoding_validators(\n+            name_attribute_mappings,\n+            cabf.EXTENSION_MAPPINGS,\n+            additional_validators\n+        )\n+\n+\n+def create_validators(certificate_type: CertificateType,\n+                      validity_period_start_retriever: typing.Optional[document.ValidityPeriodStartRetriever] = None,\n+                      additional_validity_validators=None,\n+                      additional_spki_validators=None,\n+                      additional_name_validators=None,\n+                      additional_extension_validators=None,\n+                      additional_top_level_validators=None) -> List[validation.Validator]:\n+    subject_validators = [\n+        en_319_412_1.LegalPersonOrganizationIdentifierValidator(),\n+        en_319_412_1.NaturalPersonIdentifierValidator(),\n+        en_319_412_1.EidasLegalPersonIdentifierValidator(),\n+        en_319_412_1.NaturalPersonEidasIdentifierValidator(),\n+        organization_id.OrganizationIdentifierLeiValidator(),\n+        en_319_412_3.LegalPersonOrganizationAttributesEqualityValidator(),\n+    ]\n+\n+    if additional_name_validators:\n+        subject_validators.extend(additional_name_validators)\n+\n+    qc_statement_validators = [\n+        ts_119_495.RolesOfPspValidator(),\n+        ts_119_495.NCANameLatinCharactersValidator(),\n+        ts_119_495.NCAIdValidator(),\n+        en_319_412_5.QcCCLegislationCountryCodeValidator(),\n+        en_319_412_5.QcEuRetentionPeriodValidator(),\n+        en_319_412_5.QcTypeValidator(certificate_type),\n+        en_319_412_5.QcEuPDSHttpsURLValidator(),\n+        en_319_412_5.QcEuLimitValueValidator(),\n+        en_319_412_5.QcEuPDSLanguageValidator(),\n+        en_319_412_1.LegalPersonIdentifierNameRegistrationAuthoritiesValidator(),\n+        en_319_412_1.NaturalPersonIdentifierNameRegistrationAuthoritiesValidator(),\n+        en_319_412_5.QcStatementIdentifierAllowanceValidator(certificate_type),\n+    ]\n+\n+    qc_statements_validator_container = validation.ValidatorContainer(\n+        validators=qc_statement_validators,\n+        pdu_class=rfc3739.QCStatements\n+    )\n+\n+    extension_validators = [\n+        en_319_412_2.QualifiedCertificatePoliciesValidator(certificate_type),\n+        en_319_412_5.QcStatementsExtensionCriticalityValidator(),\n+        qc_statements_validator_container,\n+    ]\n+\n+    if additional_extension_validators:\n+        extension_validators.extend(additional_extension_validators)\n+\n+    spki_validators = [\n+        ts_119_312.RsaKeyValidator(),\n+        ts_119_312.AllowedPublicKeyTypeValidator(),\n+    ]\n+\n+    if additional_spki_validators:\n+        spki_validators.extend(additional_spki_validators)\n+\n+    top_level_validators = [\n+        en_319_412_2.ExtensionsPresenceValidator(),\n+        ts_119_312.AllowedSignatureAlgorithmValidator(path='certificate.tbsCertificate.signature'),\n+    ]\n+\n+    if additional_top_level_validators:\n+        top_level_validators.extend(additional_top_level_validators)\n+\n+    if (\n+            certificate_type in etsi_constants.LEGAL_PERSON_CERTIFICATE_TYPES and\n+            certificate_type not in etsi_constants.CABF_CERTIFICATE_TYPES\n+    ):\n+        subject_validators.extend([\n+            en_319_412_3.LegalPersonSubjectAttributeAllowanceValidator(),\n+            en_319_412_3.LegalPersonDuplicateAttributeAllowanceValidator(),\n+            en_319_412_3.LegalPersonCountryCodeValidator(),\n+        ])\n+\n+    elif (\n+            certificate_type in etsi_constants.NATURAL_PERSON_CERTIFICATE_TYPES and\n+            certificate_type not in etsi_constants.CABF_CERTIFICATE_TYPES\n+    ):\n+        subject_validators.append(en_319_412_2.NaturalPersonSubjectAttributeAllowanceValidator())\n+\n+    if certificate_type not in etsi_constants.CABF_CERTIFICATE_TYPES:\n+        extension_validators.extend([\n+            en_319_412_2.CertificatePoliciesCriticalityValidator(),\n+            en_319_412_2.SubjectAlternativeNameCriticalityValidator(),\n+            en_319_412_2.IssuerAlternativeNameCriticalityValidator(),\n+            en_319_412_2.ExtendedKeyUsageCriticalityValidator(),\n+            en_319_412_2.CRLDistributionPointsCriticalityValidator(),\n+            en_319_412_2.NaturalPersonExtensionIdentifierAllowanceValidator(certificate_type),\n+            en_319_412_2.CrlDistributionPointsExtensionPresenceValidator(),\n+            en_319_412_2.CrlDistributionPointsValidator(),\n+            en_319_412_2.AuthorityInformationAccessValidator(),\n+        ])\n+\n+        if certificate_type in etsi_constants.WEB_AUTHENTICATION_CERTIFICATE_TYPES:\n+            extension_validators.extend([\n+                en_319_412_4.NcpWExtendedKeyUsagePresenceValidator(),\n+                serverauth.serverauth_subscriber.SubscriberEkuAllowanceValidator(),\n+                en_319_412_4.NcpWCriticalityExtendedKeyUsageValidator(),\n+                serverauth.serverauth_subscriber.SubscriberSanGeneralNameTypeValidator(),\n+                serverauth_name.DnsNameLdhLabelSyntaxValidator(),\n+                en_319_412_4.NcpWSubjectAltNamePresenceValidator(),\n+            ])\n+\n+        # TODO: fix commitment types when adding support for eSeal and eSignature\n+        if certificate_type in etsi_constants.LEGAL_PERSON_CERTIFICATE_TYPES:\n+            extension_validators.append(en_319_412_3.LegalPersonKeyUsageValidator(is_content_commitment_type=None))\n+        elif certificate_type in etsi_constants.NATURAL_PERSON_CERTIFICATE_TYPES:\n+            extension_validators.append(en_319_412_2.NaturalPersonKeyUsageValidator(is_content_commitment_type=None))\n+\n+    if certificate_type in etsi_constants.QEVCP_W_PSD2_EIDAS_CERTIFICATE_TYPES:\n+        qc_statement_validators.append(ts_119_495.PresenceofQCEUPDSStatementValidator())\n+\n+        subject_validators.append(ts_119_495.PsdOrganizationIdentifierFormatValidator())\n+\n+    if certificate_type in etsi_constants.QNCP_W_CERTIFICATE_TYPES:\n+        subject_validators.append(en_319_412_4.QncpWCommonNameValidator())\n+    elif certificate_type in etsi_constants.NCP_W_CERTIFICATE_TYPES:\n+        subject_validators.append(en_319_412_4.NcpWCommonNameValidator())\n+\n+    if certificate_type in etsi_constants.CABF_CERTIFICATE_TYPES:\n+        extension_validators.append(en_319_411_1.CertificatePoliciesValidator(certificate_type))\n+\n+        serverauth_cert_type = etsi_constants.ETSI_TYPE_TO_CABF_SERVERAUTH_TYPE_MAPPINGS[certificate_type]\n+\n+        return serverauth.create_validators(\n+            serverauth_cert_type,\n+            validity_period_start_retriever=validity_period_start_retriever,\n+            additional_top_level_validators=top_level_validators,\n+            additional_validity_validators=additional_validity_validators,\n+            additional_name_validators=subject_validators,\n+            additional_extension_validators=extension_validators,\n+            additional_spki_validators=spki_validators\n+        )\n+    else:\n+        spki_validator_container = validation.ValidatorContainer(\n+            validators=spki_validators,\n+            path='certificate.tbsCertificate.subjectPublicKeyInfo'\n+        )\n+\n+        top_level_container = validation.ValidatorContainer(\n+            validators=top_level_validators,\n+            pdu_class=rfc5280.Certificate\n+        )\n+\n+        extension_validators.extend([\n+            alternative_name.create_internal_name_validator_container(\n+                etsi_shared.VALIDATION_INTERNAL_DOMAIN_NAME,\n+                etsi_shared.VALIDATION_INTERNAL_IP_ADDRESS,\n+                allow_onion_tld=False\n+            ),\n+            alternative_name.create_cpsuri_internal_domain_name_validator(\n+                etsi_shared.VALIDATION_INTERNAL_DOMAIN_NAME),\n+        ])\n+\n+        return [\n+            certificate.create_issuer_validator_container(\n+                []\n+            ),\n+            certificate.create_validity_validator_container(additional_validity_validators),\n+            certificate.create_subject_validator_container(\n+                subject_validators\n+            ),\n+            certificate.create_extensions_validator_container(\n+                extension_validators\n+            ),\n+            spki_validator_container,\n+            top_level_container,\n+        ]\n+\n+\n+def create_etsi_finding_filters(certificate_type) -> List[finding_filter.FindingDescriptionFilter]:\n+    if certificate_type in etsi_constants.CABF_CERTIFICATE_TYPES:\n+        serverauth_cert_type = etsi_constants.ETSI_TYPE_TO_CABF_SERVERAUTH_TYPE_MAPPINGS[certificate_type]\n+\n+        filters = serverauth.create_serverauth_finding_filters(serverauth_cert_type)\n+    else:\n+        filters = [\n+            serverauth_finding_filter.DnsNameGeneralNamePreferredNameSyntaxFilter(),\n+        ]\n+\n+    if certificate_type in etsi_constants.QEVCP_W_PSD2_EIDAS_NON_BROWSER_CERTIFICATE_TYPES:\n+        filters.append(etsi_finding_filter.Psd2CabfServerauthValidityPeriodFilter())\n+\n+    return filters\ndiff --git a/pkilint/etsi/asn1/__init__.py b/pkilint/etsi/asn1/__init__.py\nindex e69de29..b6e1d93 100644\n--- a/pkilint/etsi/asn1/__init__.py\n+++ b/pkilint/etsi/asn1/__init__.py\n@@ -0,0 +1,17 @@\n+from pyasn1_alt_modules import rfc3739\n+\n+from pkilint import document\n+from pkilint.etsi.asn1 import en_319_412_5, ts_119_495, en_319_411_2\n+\n+ETSI_QC_STATEMENTS_MAPPINGS = {\n+    en_319_412_5.id_etsi_qcs_QcCompliance: document.ValueDecoder.VALUE_NODE_ABSENT,\n+    en_319_412_5.id_etsi_qcs_QcLimitValue: en_319_412_5.QcEuLimitValue(),\n+    en_319_412_5.id_etsi_qcs_QcRetentionPeriod: en_319_412_5.QcEuRetentionPeriod(),\n+    en_319_412_5.id_etsi_qcs_QcSSCD: document.ValueDecoder.VALUE_NODE_ABSENT,\n+    en_319_412_5.id_etsi_qcs_QcPDS: en_319_412_5.QcEuPDS(),\n+    en_319_412_5.id_etsi_qcs_QcType: en_319_412_5.QcType(),\n+    en_319_412_5.id_etsi_qcs_QcCClegislation: en_319_412_5.QcCClegislation(),\n+    ts_119_495.id_etsi_psd2_qcStatement: ts_119_495.PSD2QcType(),\n+    rfc3739.id_qcs_pkixQCSyntax_v1: document.OptionalAsn1TypeWrapper(rfc3739.SemanticsInformation()),\n+    rfc3739.id_qcs_pkixQCSyntax_v2: document.OptionalAsn1TypeWrapper(rfc3739.SemanticsInformation()),\n+}\ndiff --git a/pkilint/etsi/asn1/en_319_411_1.py b/pkilint/etsi/asn1/en_319_411_1.py\nnew file mode 100644\nindex 0000000..a340397\n--- /dev/null\n+++ b/pkilint/etsi/asn1/en_319_411_1.py\n@@ -0,0 +1,28 @@\n+from pyasn1.type import univ\n+\n+\n+_ARC = univ.ObjectIdentifier('0.4.0.2042.1.1')\n+\n+id_ncp = _ARC + (1,)\n+\n+id_ncp_plus = _ARC + (2,)\n+\n+id_lcp = _ARC + (3,)\n+\n+id_evcp = _ARC + (4,)\n+\n+id_dvcp = _ARC + (5,)\n+\n+id_ovcp = _ARC + (6,)\n+\n+id_ivcp = _ARC + (7,)\n+\n+POLICY_OIDS = {\n+    id_ncp,\n+    id_ncp_plus,\n+    id_lcp,\n+    id_evcp,\n+    id_dvcp,\n+    id_ovcp,\n+    id_ivcp,\n+}\ndiff --git a/pkilint/etsi/asn1/en_319_411_2.py b/pkilint/etsi/asn1/en_319_411_2.py\nnew file mode 100644\nindex 0000000..0a08955\n--- /dev/null\n+++ b/pkilint/etsi/asn1/en_319_411_2.py\n@@ -0,0 +1,29 @@\n+from pyasn1.type.univ import ObjectIdentifier\n+\n+\n+_ARC = ObjectIdentifier('0.4.0.194112.1')\n+\n+\n+id_qcp_natural = _ARC + (0,)\n+\n+id_qcp_legal = _ARC + (1,)\n+\n+id_qcp_natural_qscd = _ARC + (2,)\n+\n+id_qcp_legal_qscd = _ARC + (3,)\n+\n+id_qcp_web = _ARC + (4,)\n+\n+id_qncp_web = _ARC + (5,)\n+\n+id_qncp_web_gen = _ARC + (6,)\n+\n+QUALIFIED_POLICY_OIDS = {\n+    id_qcp_natural,\n+    id_qcp_legal,\n+    id_qcp_natural_qscd,\n+    id_qcp_legal_qscd,\n+    id_qcp_web,\n+    id_qncp_web,\n+    id_qncp_web_gen,\n+}\ndiff --git a/pkilint/etsi/asn1/en_319_412_1.py b/pkilint/etsi/asn1/en_319_412_1.py\nnew file mode 100644\nindex 0000000..0e91e74\n--- /dev/null\n+++ b/pkilint/etsi/asn1/en_319_412_1.py\n@@ -0,0 +1,41 @@\n+from pyasn1.type import univ\n+\n+\n+def _OID(*components):\n+    output = []\n+    for x in tuple(components):\n+        if isinstance(x, univ.ObjectIdentifier):\n+            output.extend(list(x))\n+        else:\n+            output.append(int(x))\n+\n+    return univ.ObjectIdentifier(output)\n+\n+\n+_ID_ETSI_ARC = _OID(0, 4, 0, 194121)\n+\n+\n+id_etsi_qcs_semantics_identifiers = _OID(_ID_ETSI_ARC, 1)\n+\n+\n+id_etsi_qcs_semanticsId_Natural = _OID(id_etsi_qcs_semantics_identifiers, 1)\n+\n+\n+id_etsi_qcs_SemanticsId_Legal = _OID(id_etsi_qcs_semantics_identifiers, 2)\n+\n+\n+id_etsi_qcs_semanticsId_eIDASNatural = _OID(id_etsi_qcs_semantics_identifiers, 3)\n+\n+\n+id_etsi_qcs_SemanticsId_eIDASLegal = _OID(id_etsi_qcs_semantics_identifiers, 4)\n+\n+\n+id_etsi_ext = _OID(_ID_ETSI_ARC, 2)\n+\n+\n+id_etsi_ext_valassured_ST_certs = _OID(id_etsi_ext, 1)\n+\n+\n+EXTENSION_MAPPINGS = {\n+    id_etsi_ext_valassured_ST_certs: univ.Null(),\n+}\ndiff --git a/pkilint/etsi/en_319_411_1.py b/pkilint/etsi/en_319_411_1.py\nnew file mode 100644\nindex 0000000..0acfd09\n--- /dev/null\n+++ b/pkilint/etsi/en_319_411_1.py\n@@ -0,0 +1,60 @@\n+from pyasn1_alt_modules import rfc5280\n+\n+from pkilint import validation, oid\n+from pkilint.etsi import etsi_constants\n+from pkilint.etsi.asn1 import en_319_411_1\n+\n+\n+class CertificatePoliciesValidator(validation.Validator):\n+    \"\"\"\n+    GEN-6.3.3-12: The CP identifier shall be [CHOICE]:\n+    ...\n+    \"\"\"\n+    VALIDATION_MULTIPLE_RESERVED_POLICY_OIDS_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_411_1.gen-6.3.3-12.multiple_reserved_policy_oids_present',\n+    )\n+\n+    VALIDATION_PROHIBITED_RESERVED_POLICY_OID_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_411_1.gen-6.3.3-12.prohibited_reserved_policy_oid_present',\n+    )\n+\n+    _CERTIFICATE_TYPE_SET_TO_POLICY_OID_MAPPINGS = [\n+        (etsi_constants.CABF_EV_CERTIFICATE_TYPES, en_319_411_1.id_evcp),\n+        (etsi_constants.CABF_DV_CERTIFICATE_TYPES, en_319_411_1.id_dvcp),\n+        (etsi_constants.CABF_OV_CERTIFICATE_TYPES, en_319_411_1.id_ovcp),\n+        (etsi_constants.CABF_IV_CERTIFICATE_TYPES, en_319_411_1.id_ivcp),\n+    ]\n+\n+    def __init__(self, certificate_type):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_MULTIPLE_RESERVED_POLICY_OIDS_PRESENT,\n+                self.VALIDATION_PROHIBITED_RESERVED_POLICY_OID_PRESENT\n+            ],\n+            pdu_class=rfc5280.CertificatePolicies\n+        )\n+\n+        self._expected_policy_oid = next(\n+            p for t, p in self._CERTIFICATE_TYPE_SET_TO_POLICY_OID_MAPPINGS if certificate_type in t\n+        )\n+\n+    def validate(self, node):\n+        etsi_policy_oids = en_319_411_1.POLICY_OIDS & node.document.policy_oids\n+\n+        if len(etsi_policy_oids) > 1:\n+            oids = oid.format_oids(etsi_policy_oids)\n+\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_MULTIPLE_RESERVED_POLICY_OIDS_PRESENT,\n+                f'Multiple reserved certificate policy OIDs present: {oids}'\n+            )\n+\n+        if etsi_policy_oids and self._expected_policy_oid not in etsi_policy_oids:\n+            prohibited_oid = next(iter(etsi_policy_oids))\n+\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_PROHIBITED_RESERVED_POLICY_OID_PRESENT,\n+                f'Prohibited reserved certificate policy OID present: {prohibited_oid}'\n+            )\ndiff --git a/pkilint/etsi/en_319_412_1.py b/pkilint/etsi/en_319_412_1.py\nnew file mode 100644\nindex 0000000..9ffb907\n--- /dev/null\n+++ b/pkilint/etsi/en_319_412_1.py\n@@ -0,0 +1,545 @@\n+import re\n+import typing\n+from typing import List\n+\n+from pyasn1.type import univ\n+from pyasn1_alt_modules import rfc5280, rfc3739\n+\n+from pkilint import validation, document\n+from pkilint.common import organization_id\n+from pkilint.common.organization_id import ParsedOrganizationIdentifier\n+from pkilint.etsi.asn1 import en_319_412_1\n+from pkilint.itu import x520_name\n+from pkilint.pkix import Rfc2119Word, certificate\n+\n+\n+def _get_semantics_info_nodes(cert: certificate.RFC5280Certificate) -> List[document.PDUNode]:\n+    ext_and_idx = cert.get_extension_by_oid(rfc3739.id_pe_qcStatements)\n+\n+    if ext_and_idx is None:\n+        return []\n+\n+    ext, _ = ext_and_idx\n+\n+    try:\n+        _, ext_decoded_value_node = ext.children['extnValue'].child\n+    except ValueError:\n+        return []\n+\n+    nodes = []\n+    for statement_node in ext_decoded_value_node.children.values():\n+        if statement_node.children['statementId'].pdu == rfc3739.id_qcs_pkixQCSyntax_v2:\n+            info_node = statement_node.children.get('statementInfo')\n+\n+            if info_node is not None:\n+                info_node_decoded = info_node.children.get('semanticsInformation')\n+\n+                if info_node_decoded is not None:\n+                    nodes.append(info_node_decoded)\n+\n+    return nodes\n+\n+\n+def _cert_has_semantics_id(semantics_id: univ.ObjectIdentifier, cert: certificate.RFC5280Certificate) -> bool:\n+    # noinspection PyTypeChecker\n+    semantics_info_nodes = _get_semantics_info_nodes(cert)\n+\n+    for semantics_info_node in semantics_info_nodes:\n+        id_oid_node = semantics_info_node.children.get('semanticsIdentifier')\n+\n+        if id_oid_node is not None and id_oid_node.pdu == semantics_id:\n+            return True\n+\n+    return False\n+\n+\n+class LegalPersonOrganizationIdentifierValidator(organization_id.OrganizationIdentifierValidatorBase):\n+    VALIDATION_INVALID_ORGANIZATION_ID_FORMAT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.4-02.invalid_format'\n+    )\n+\n+    VALIDATION_INVALID_ORGANIZATION_ID_SCHEME = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.4-03.invalid_scheme'\n+    )\n+\n+    VALIDATION_INVALID_ORGANIZATION_ID_COUNTRY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.4-03.invalid_country'\n+    )\n+\n+    VALIDATION_ORGANIZATION_NATIONAL_SCHEME_DETECTED = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'etsi.en_319_412_1.leg-5.1.4-03.national_identifier_scheme_detected'\n+    )\n+\n+    _STATE_PROVINCE_PROHIBITED = (Rfc2119Word.MUST_NOT, VALIDATION_INVALID_ORGANIZATION_ID_FORMAT.code)\n+    _REFERENCE_REQUIRED = (Rfc2119Word.MUST, VALIDATION_INVALID_ORGANIZATION_ID_FORMAT.code)\n+\n+    _NTR_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES,\n+                       VALIDATION_INVALID_ORGANIZATION_ID_COUNTRY),\n+        state_province=_STATE_PROVINCE_PROHIBITED,\n+        reference=_REFERENCE_REQUIRED\n+    )\n+\n+    _VAT_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES | {\n+            organization_id.COUNTRY_CODE_GREECE_TRADITIONAL,\n+            organization_id.COUNTRY_CODE_NORTHERN_IRELAND\n+        },\n+                       VALIDATION_INVALID_ORGANIZATION_ID_COUNTRY),\n+        state_province=_STATE_PROVINCE_PROHIBITED,\n+        reference=_REFERENCE_REQUIRED\n+    )\n+\n+    _PSD_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=(organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES,\n+                       VALIDATION_INVALID_ORGANIZATION_ID_COUNTRY),\n+        state_province=_STATE_PROVINCE_PROHIBITED,\n+        reference=_REFERENCE_REQUIRED\n+    )\n+\n+    _LEI_SCHEME = organization_id.OrganizationIdentifierElementAllowance(\n+        country_codes=({organization_id.COUNTRY_CODE_GLOBAL_SCHEME},\n+                       VALIDATION_INVALID_ORGANIZATION_ID_COUNTRY),\n+        state_province=_STATE_PROVINCE_PROHIBITED,\n+        reference=_REFERENCE_REQUIRED\n+    )\n+\n+    _ELEMENT_ALLOWANCES = {\n+        'NTR': _NTR_SCHEME,\n+        'VAT': _VAT_SCHEME,\n+        'PSD': _PSD_SCHEME,\n+        'LEI': _LEI_SCHEME,\n+    }\n+\n+    def __init__(self):\n+        super().__init__(element_allowances=self._ELEMENT_ALLOWANCES,\n+                         invalid_format_validation=self.VALIDATION_INVALID_ORGANIZATION_ID_FORMAT,\n+                         additional_validations=[\n+                             self.VALIDATION_INVALID_ORGANIZATION_ID_SCHEME,\n+                             self.VALIDATION_ORGANIZATION_NATIONAL_SCHEME_DETECTED\n+                         ],\n+                         pdu_class=x520_name.X520OrganizationIdentifier)\n+\n+    def match(self, node):\n+        if not super().match(node):\n+            return False\n+\n+        # skip nodes that haven't been decoded\n+        if not any(node.children):\n+            return False\n+\n+        # noinspection PyTypeChecker\n+        return _cert_has_semantics_id(en_319_412_1.id_etsi_qcs_SemanticsId_Legal, node.document)\n+\n+    @classmethod\n+    def handle_unknown_scheme(cls, node: document.PDUNode, parsed: organization_id.ParsedOrganizationIdentifier):\n+        is_valid_national_scheme = (\n+                parsed.is_national_scheme and\n+                parsed.country in organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES and\n+                parsed.state_province is None and\n+                parsed.reference\n+        )\n+\n+        value_str = str(node.child[1].pdu)\n+\n+        if is_valid_national_scheme:\n+            raise validation.ValidationFindingEncountered(\n+                cls.VALIDATION_ORGANIZATION_NATIONAL_SCHEME_DETECTED,\n+                f'National registration scheme \"{parsed.scheme}\" in organization identifier: \"{value_str}\"'\n+            )\n+        else:\n+            raise validation.ValidationFindingEncountered(\n+                cls.VALIDATION_INVALID_ORGANIZATION_ID_SCHEME,\n+                f'Invalid registration scheme \"{parsed.scheme}\" in organization identifier: \"{value_str}\"'\n+            )\n+\n+    @classmethod\n+    def parse_organization_id_node(cls, node: document.PDUNode) -> ParsedOrganizationIdentifier:\n+        value = str(node.child[1].pdu)\n+\n+        return organization_id.parse_organization_identifier(value)\n+\n+\n+_NATURAL_PERSON_ID_REGEX = re.compile(\n+    r'^(?P<scheme>[A-Z]{2}[A-Z:])(?P<country>[a-zA-Z]{2})-(?P<reference>.+)$'\n+)\n+\n+\n+class ParsedNaturalPersonIdentifier(typing.NamedTuple):\n+    raw: typing.Optional[str]\n+    scheme: str\n+    is_national_scheme: bool\n+    country: str\n+    reference: str\n+\n+\n+def parse_natural_person_identifier(value: str) -> ParsedNaturalPersonIdentifier:\n+    m = _NATURAL_PERSON_ID_REGEX.match(value)\n+\n+    if m is None:\n+        raise ValueError(f'Invalid natural person identifier syntax: \"{value}\"')\n+\n+    is_national_scheme = m['scheme'].endswith(':')\n+\n+    scheme = m['scheme'][:2] if is_national_scheme else m['scheme']\n+\n+    return ParsedNaturalPersonIdentifier(value, scheme, is_national_scheme, m['country'], m['reference'])\n+\n+\n+class NaturalPersonIdentifierValidator(validation.Validator):\n+    VALIDATION_INVALID_NATURAL_PERSON_ID_FORMAT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.nat-5.1.3.invalid_identifier_format'\n+    )\n+\n+    VALIDATION_NATURAL_PERSON_ID_NATIONAL_SCHEME_DETECTED = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'etsi.en_319_412_1.nat-5.1.3.national_identifier_scheme_detected'\n+    )\n+\n+    VALIDATION_INVALID_NATURAL_PERSON_ID_SCHEME = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-5.1.3.invalid_identifier_scheme'\n+    )\n+\n+    VALIDATION_DEPRECATED_NATURAL_PERSON_ID_SCHEME = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_3.nat-5.1.3.deprecated_identifier_scheme'\n+    )\n+\n+    VALIDATION_INVALID_NATURAL_PERSON_ID_COUNTRY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_419_412_4.nat-5.1.3.invalid_identifier_country'\n+    )\n+\n+    _KNOWN_SCHEMES = {\n+        'PAS',\n+        'IDC',\n+        'PNO',\n+        'TAX',\n+        'TIN',\n+    }\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_INVALID_NATURAL_PERSON_ID_FORMAT,\n+                self.VALIDATION_NATURAL_PERSON_ID_NATIONAL_SCHEME_DETECTED,\n+                self.VALIDATION_INVALID_NATURAL_PERSON_ID_SCHEME,\n+                self.VALIDATION_DEPRECATED_NATURAL_PERSON_ID_SCHEME,\n+                self.VALIDATION_INVALID_NATURAL_PERSON_ID_COUNTRY,\n+            ],\n+            pdu_class=rfc5280.X520SerialNumber\n+        )\n+\n+    def match(self, node):\n+        if not super().match(node):\n+            return False\n+\n+        # noinspection PyTypeChecker\n+        return _cert_has_semantics_id(en_319_412_1.id_etsi_qcs_semanticsId_Natural, node.document)\n+\n+    def validate(self, node):\n+        value = str(node.pdu)\n+\n+        try:\n+            parsed = parse_natural_person_identifier(value)\n+        except ValueError as e:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_INVALID_NATURAL_PERSON_ID_FORMAT,\n+                str(e)\n+            )\n+\n+        findings = []\n+        if parsed.scheme in self._KNOWN_SCHEMES:\n+            if parsed.scheme == 'TAX':\n+                findings.append(validation.ValidationFindingDescription(\n+                    self.VALIDATION_DEPRECATED_NATURAL_PERSON_ID_SCHEME,\n+                    f'Deprecated natural person identifier scheme: \"{parsed.scheme}\"'\n+                ))\n+        else:\n+            if parsed.is_national_scheme:\n+                findings.append(validation.ValidationFindingDescription(\n+                    self.VALIDATION_NATURAL_PERSON_ID_NATIONAL_SCHEME_DETECTED,\n+                    f'National registration scheme \"{parsed.scheme}\" in natural person identifier: \"{value}\"'\n+                ))\n+            else:\n+                findings.append(validation.ValidationFindingDescription(\n+                    self.VALIDATION_INVALID_NATURAL_PERSON_ID_SCHEME,\n+                    f'Invalid natural person identifier scheme: \"{parsed.scheme}\"'\n+                ))\n+\n+        if parsed.country not in organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES:\n+            findings.append(validation.ValidationFindingDescription(\n+                self.VALIDATION_INVALID_NATURAL_PERSON_ID_COUNTRY,\n+                f'Invalid natural person identifier country code: \"{parsed.country}\"'\n+            ))\n+\n+        return validation.ValidationResult(self, node, findings)\n+\n+\n+class EidasLegalPersonIdentifierValidator(validation.Validator):\n+    \"\"\"\n+    LEG-5.1.6-03: Any organizationIdentifier attribute present in the subject field of the certificate shall\n+    comply with the content requirement specified for the eIDAS LegalPersonIdentifier attribute.\n+\n+    From eIDAS SAML Attribute Profile v1.2 Final, section 2.5:\n+    - The Unique Identifier MUST NOT contain any whitespace.\n+    - The Unique Identifier MUST NOT exceed a total of 256 characters.\n+    \"\"\"\n+    VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_WHITESPACE_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.6-03.eidas_legal_person_identifier_whitespace_present'\n+    )\n+\n+    VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_TOO_LONG = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.6-03.eidas_legal_person_identifier_too_long'\n+    )\n+\n+    _MAX_LENGTH = 256\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_WHITESPACE_PRESENT,\n+                self.VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_TOO_LONG,\n+            ],\n+            pdu_class=x520_name.X520OrganizationIdentifier\n+        )\n+\n+    def match(self, node):\n+        # noinspection PyTypeChecker\n+        return super().match(node) and _cert_has_semantics_id(\n+            en_319_412_1.id_etsi_qcs_SemanticsId_eIDASLegal,\n+            node.document\n+        )\n+\n+    def validate(self, node):\n+        value = str(node.child[1].pdu)\n+\n+        if any(c.isspace() for c in value):\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_WHITESPACE_PRESENT,\n+                f'Whitespace present in organization identifier: \"{value}\"'\n+            )\n+\n+        value_len = len(value)\n+\n+        if value_len > self._MAX_LENGTH:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_EIDAS_LEGAL_PERSON_IDENTIFIER_TOO_LONG,\n+                f'Organization identifier \"{value}\" ({value_len} characters) exceeds maximum length of '\n+                f'{self._MAX_LENGTH} characters'\n+            )\n+\n+\n+class NameRegistrationAuthoritiesValidatorBase(validation.Validator):\n+    def __init__(\n+            self,\n+            name_registration_authorities_missing_validation: validation.ValidationFinding,\n+            name_registration_authorities_uri_missing_validation: validation.ValidationFinding,\n+            attribute_type_id: univ.ObjectIdentifier,\n+            semantics_id: univ.ObjectIdentifier\n+    ):\n+        self._name_registration_authorities_missing_validation = name_registration_authorities_missing_validation\n+        self._name_registration_authorities_uri_missing_validation = (\n+            name_registration_authorities_uri_missing_validation\n+        )\n+\n+        self._attribute_type_id = attribute_type_id\n+        self._semantics_id = semantics_id\n+\n+        validations = [\n+            name_registration_authorities_missing_validation,\n+            name_registration_authorities_uri_missing_validation,\n+        ]\n+\n+        super().__init__(validations=validations, pdu_class=rfc3739.SemanticsInformation)\n+\n+    @classmethod\n+    def is_attribute_value_national_id_scheme(cls, atv_node: document.PDUNode) -> bool:\n+        pass\n+\n+    def match(self, node):\n+        if not super().match(node):\n+            return False\n+\n+        semantics_id_node = node.children.get('semanticsIdentifier')\n+\n+        if semantics_id_node is None or semantics_id_node.pdu != self._semantics_id:\n+            return False\n+\n+        atvs_and_idxs = node.document.get_subject_attributes_by_type(self._attribute_type_id)\n+\n+        return any(self.is_attribute_value_national_id_scheme(a) for a, _ in atvs_and_idxs)\n+\n+    def validate(self, node):\n+        nra_node = node.children.get('nameRegistrationAuthorities')\n+\n+        if nra_node is None:\n+            raise validation.ValidationFindingEncountered(self._name_registration_authorities_missing_validation)\n+\n+        if not any(g.child[0] == 'uniformResourceIdentifier' for g in nra_node.children.values()):\n+            raise validation.ValidationFindingEncountered(self._name_registration_authorities_uri_missing_validation)\n+\n+\n+class NaturalPersonIdentifierNameRegistrationAuthoritiesValidator(NameRegistrationAuthoritiesValidatorBase):\n+    \"\"\"\n+    NAT-5.1.3-05: When a locally defined identity type reference is provided (two characters followed by \":\"), the\n+    nameRegistrationAuthorities element of SemanticsInformation (IETF RFC 3739 [1]) shall be present.\n+    \"\"\"\n+    VALIDATION_NAME_REGISTRATION_AUTHORITIES_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.nat-5.1.3-05.national_id_scheme_name_registration_authorities_missing'\n+    )\n+\n+    \"\"\"\n+    NAT-5.1.3-06: The nameRegistrationAuthorities element of SemanticsInformation (IETF RFC 3739 [1]) shall\n+    contain at least a uniformResourceIdentifier generalName.\n+    \"\"\"\n+    VALIDATION_NAME_REGISTRATION_AUTHORITIES_URI_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.nat-5.1.3-05.national_id_scheme_name_registration_authorities_uri_missing'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            self.VALIDATION_NAME_REGISTRATION_AUTHORITIES_MISSING,\n+            self.VALIDATION_NAME_REGISTRATION_AUTHORITIES_URI_MISSING,\n+            rfc5280.id_at_serialNumber,\n+            en_319_412_1.id_etsi_qcs_semanticsId_Natural\n+        )\n+\n+    @classmethod\n+    def is_attribute_value_national_id_scheme(cls, atv_node: document.PDUNode) -> bool:\n+        value_node = atv_node.children['value']\n+\n+        try:\n+            _, decoded_value_node = value_node.child\n+\n+            value_str = str(decoded_value_node.pdu)\n+\n+            parsed = parse_natural_person_identifier(value_str)\n+        except ValueError:\n+            return False\n+\n+        return parsed.is_national_scheme\n+\n+\n+class LegalPersonIdentifierNameRegistrationAuthoritiesValidator(NameRegistrationAuthoritiesValidatorBase):\n+    \"\"\"\n+    LEG-5.1.4-05: When a locally defined identity type reference is provided (two characters followed by \":\"), the\n+    nameRegistrationAuthorities element of SemanticsInformation (IETF RFC 3739 [1]) shall be present and\n+    shall contain at least a uniformResourceIdentifier generalName.\n+    \"\"\"\n+    VALIDATION_NAME_REGISTRATION_AUTHORITIES_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.4-05.national_id_scheme_name_registration_authorities_missing'\n+    )\n+\n+    VALIDATION_NAME_REGISTRATION_AUTHORITIES_URI_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_1.leg-5.1.4-05.national_id_scheme_name_registration_authorities_uri_missing'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            self.VALIDATION_NAME_REGISTRATION_AUTHORITIES_MISSING,\n+            self.VALIDATION_NAME_REGISTRATION_AUTHORITIES_URI_MISSING,\n+            x520_name.id_at_organizationIdentifier,\n+            en_319_412_1.id_etsi_qcs_SemanticsId_Legal\n+        )\n+\n+    @classmethod\n+    def is_attribute_value_national_id_scheme(cls, atv_node: document.PDUNode) -> bool:\n+        value_node = atv_node.children['value']\n+\n+        try:\n+            _, decoded_dirstring_node = value_node.child\n+\n+            _, decoded_value_node = decoded_dirstring_node.child\n+\n+            value_str = str(decoded_value_node.pdu)\n+\n+            parsed = organization_id.parse_organization_identifier(value_str)\n+        except ValueError:\n+            return False\n+\n+        return parsed.is_national_scheme\n+\n+\n+class NaturalPersonEidasIdentifierValidator(validation.Validator):\n+    \"\"\"\n+    NAT-5.1.5-01: If using electronic identity attributes as specified in eIDAS SAML\n+    attribute profile for a certificate issued to natural persons, the semantics of \n+    id-etsi-qcs-SemanticsId-eIDASNatural shall be as follows. \n+    NAT-5.1.5-02: If the eIDAS natural person identifier is included, the values \n+    of attributes in the subject field shall meet the content requirements of corresponding\n+    attributes defined by the eIDAS SAML attribute profile according to the following requirements.\n+\n+    NAT-5.1.5-03: Any serialNumber attribute value in the subject field of the certificate\n+    shall comply with the content requirement specified for the eIDAS PersonIdentifier attribute.\n+\n+    NAT-5.1.5-04: Attributes present in subject field of the certificate are equivalent to defined attributes \n+    in accordance to the below table. This means that the present attribute shall hold equivalent \n+    information, even if the format used to express that information differs.\n+\n+    serialNumber -> PersonIdentifier Example: ES/AT/02635542Y\n+    \"\"\"\n+    VALIDATION_INVALID_ISO_3166 = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.nat_5.1.5-03.invalid_iso_3166'\n+    )\n+\n+    VALIDATION_INVALID_SYNTAX_SERIAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.nat_5.1.5-03.invalid_syntax_serial'\n+    )\n+    VALIDATION_INVALID_CHARACTER_SERIAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.nat_5.1.5-03.invalid_character_serial'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_INVALID_ISO_3166,\n+                self.VALIDATION_INVALID_SYNTAX_SERIAL,\n+                self.VALIDATION_INVALID_CHARACTER_SERIAL\n+            ],\n+            pdu_class=rfc5280.X520SerialNumber\n+        )\n+\n+    def match(self, node):\n+        if not super().match(node):\n+            return False\n+\n+        # noinspection PyTypeChecker\n+        return _cert_has_semantics_id(en_319_412_1.id_etsi_qcs_semanticsId_eIDASNatural, node.document)\n+\n+    def validate(self, node):\n+        value = str(node.pdu)\n+        findings = []\n+        if len(value) < 7:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_INVALID_SYNTAX_SERIAL,\n+                                                          \"Invalid serial number syntax (needs more characters).\")\n+        if value[2] != '/' or value[5] != '/':\n+            findings.append(validation.ValidationFindingDescription(\n+                self.VALIDATION_INVALID_CHARACTER_SERIAL,\n+                \"No backslash found after ISO-3166 country code.\"\n+            ))\n+        if (\n+                value[0:2] not in organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES or\n+                value[3:5] not in organization_id.ISO3166_1_WITH_TRANSNATIONAL_COUNTRY_CODES\n+        ):\n+            findings.append(validation.ValidationFindingDescription(\n+                self.VALIDATION_INVALID_ISO_3166,\n+                \"Invalid 3166 ISO Code.\"\n+            ))\n+\n+        return validation.ValidationResult(self, node, findings)\ndiff --git a/pkilint/etsi/en_319_412_2.py b/pkilint/etsi/en_319_412_2.py\nnew file mode 100644\nindex 0000000..c8b28be\n--- /dev/null\n+++ b/pkilint/etsi/en_319_412_2.py\n@@ -0,0 +1,495 @@\n+import typing\n+\n+from pyasn1.type import univ\n+from pyasn1_alt_modules import rfc5280, rfc3739\n+\n+import pkilint.etsi.asn1.en_319_411_2\n+from pkilint import validation, oid, document, common\n+from pkilint.etsi import asn1 as etsi_asn1, etsi_shared\n+from pkilint.etsi import etsi_constants\n+from pkilint.etsi.asn1 import en_319_411_2\n+from pkilint.pkix import extension, name, Rfc2119Word\n+from pkilint.pkix.general_name import GeneralNameTypeName\n+\n+\n+class CertificatePoliciesCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    VALIDATION_CERTIFICATE_POLICIES_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_2.gen-4.3.3-1.critical_certificate_policies_extension'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.VALIDATION_CERTIFICATE_POLICIES_CRITICAL,\n+                         type_oid=rfc5280.id_ce_certificatePolicies,\n+                         is_critical=False)\n+\n+\n+class SubjectAlternativeNameCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    VALIDATION_SAN_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.5-1.san_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.VALIDATION_SAN_CRITICAL,\n+                         type_oid=rfc5280.id_ce_subjectAltName,\n+                         is_critical=False)\n+\n+\n+class IssuerAlternativeNameCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    VALIDATION_ISSUER_ALTERNATIVE_NAME_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.6-1.ian_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.VALIDATION_ISSUER_ALTERNATIVE_NAME_CRITICAL,\n+                         type_oid=rfc5280.id_ce_issuerAltName,\n+                         is_critical=False)\n+\n+\n+class ExtendedKeyUsageCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    VALIDATION_EXTENDED_KEY_USAGE_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.10-1.eku_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.VALIDATION_EXTENDED_KEY_USAGE_CRITICAL,\n+                         type_oid=rfc5280.id_ce_extKeyUsage,\n+                         is_critical=False)\n+\n+\n+class CRLDistributionPointsCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    CRL_DISTRIBUTION_POINTS_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.11-5.crl_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.CRL_DISTRIBUTION_POINTS_CRITICAL,\n+                         type_oid=rfc5280.id_ce_cRLDistributionPoints,\n+                         is_critical=False)\n+\n+\n+class NaturalPersonSubjectAttributeAllowanceValidator(validation.Validator):\n+    \"\"\"\n+    NAT-4.2.4-1: The subject field shall include the following attributes as specified in Recommendation ITU-T X.520:\n+    \u2022 countryName;\n+    \u2022 choice of (givenName and/or surname) or pseudonym; and\n+    \u2022 commonName.\n+    \"\"\"\n+    VALIDATION_NATURAL_PERSON_NAME_ATTRIBUTE_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-4.2.4-1.required_attribute_missing'\n+    )\n+\n+    \"\"\"\n+    NAT-4.2.4-4 The pseudonym attribute shall not be present if the givenName\n+    and surname attribute are present.\n+    \"\"\"\n+    VALIDATION_MIXED_PSEUDONYM_AND_NAME_ATTRIBUTES_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-4.2.4-4.mixed_pseudonym_and_name_attributes_present'\n+    )\n+\n+    \"\"\"\n+    NAT 4.2.4-3 The subject field shall not contain more than one instance of commonName and countryName\n+    \"\"\"\n+    VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-4.2.4-3.prohibited_duplicate_attribute_present'\n+    )\n+\n+    _REQUIRED_ATTRIBUTES = {\n+        rfc5280.id_at_countryName,\n+        rfc5280.id_at_commonName,\n+    }\n+\n+    _PSEUDONYM_AND_NAME_ATTRIBUTES = {\n+        rfc5280.id_at_givenName,\n+        rfc5280.id_at_surname,\n+        rfc5280.id_at_pseudonym,\n+    }\n+\n+    _PROHIBITED_DUPLICATE_ATTRIBUTES = {\n+        rfc5280.id_at_countryName,\n+        rfc5280.id_at_commonName,\n+    }\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_NATURAL_PERSON_NAME_ATTRIBUTE_MISSING,\n+                self.VALIDATION_MIXED_PSEUDONYM_AND_NAME_ATTRIBUTES_PRESENT,\n+                self.VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT\n+            ],\n+            pdu_class=rfc5280.Name\n+        )\n+\n+    def validate(self, node):\n+        attr_counts = name.get_name_attribute_counts(node)\n+\n+        attrs_present = set(attr_counts.keys())\n+\n+        missing_attrs = None\n+\n+        if not attrs_present.issuperset(self._REQUIRED_ATTRIBUTES):\n+            missing_attrs = self._REQUIRED_ATTRIBUTES - attrs_present\n+        elif attrs_present.isdisjoint(self._PSEUDONYM_AND_NAME_ATTRIBUTES):\n+            missing_attrs = self._PSEUDONYM_AND_NAME_ATTRIBUTES - attrs_present\n+\n+        if missing_attrs:\n+            oid_str = oid.format_oids(missing_attrs)\n+\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_NATURAL_PERSON_NAME_ATTRIBUTE_MISSING,\n+                f'Required attributes missing: {oid_str}'\n+            )\n+\n+        if all((a in attrs_present for a in self._PSEUDONYM_AND_NAME_ATTRIBUTES)):\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_MIXED_PSEUDONYM_AND_NAME_ATTRIBUTES_PRESENT)\n+\n+        for a in self._PROHIBITED_DUPLICATE_ATTRIBUTES:\n+            if attr_counts[a] > 1:\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT,\n+                    f'Prohibited duplicate attribute present: {a}'\n+                )\n+\n+\n+class NaturalPersonExtensionIdentifierAllowanceValidator(common.ExtensionIdentifierAllowanceValidator):\n+    _CODE_CLASSIFIER = 'etsi.en_319_412_2'\n+\n+    _ALLOWANCES = {\n+        # GEN-4.3.1-1\n+        rfc5280.id_ce_authorityKeyIdentifier: Rfc2119Word.MUST,\n+        # NAT-4.3.2-1\n+        rfc5280.id_ce_keyUsage: Rfc2119Word.MUST,\n+        # GEN-4.3.3-2\n+        rfc5280.id_ce_certificatePolicies: Rfc2119Word.MUST,\n+        # GEN-4.3.4-1\n+        rfc5280.id_ce_policyMappings: Rfc2119Word.MUST_NOT,\n+\n+        rfc5280.id_ce_subjectAltName: Rfc2119Word.MAY,\n+        rfc5280.id_ce_issuerAltName: Rfc2119Word.MAY,\n+        rfc5280.id_ce_subjectDirectoryAttributes: Rfc2119Word.MAY,\n+        # GEN-4.3.8-1\n+        rfc5280.id_ce_nameConstraints: Rfc2119Word.MUST_NOT,\n+        # GEN-4.3.9-1\n+        rfc5280.id_ce_policyConstraints: Rfc2119Word.MUST_NOT,\n+        rfc5280.id_ce_cRLDistributionPoints: Rfc2119Word.MAY,\n+        # GEN-4.3.12-1\n+        rfc5280.id_ce_inhibitAnyPolicy: Rfc2119Word.MUST_NOT,\n+        # GEN-4.4.1-2\n+        rfc5280.id_pe_authorityInfoAccess: Rfc2119Word.MUST,\n+    }\n+\n+    def __init__(self, certificate_type: etsi_constants.CertificateType):\n+        allowances = self._ALLOWANCES.copy()\n+\n+        if certificate_type in etsi_constants.EU_QWAC_TYPES:\n+            allowances[rfc3739.id_pe_qcStatements] = Rfc2119Word.MUST\n+        else:\n+            allowances[rfc3739.id_pe_qcStatements] = Rfc2119Word.MAY\n+\n+        super().__init__(allowances, self._CODE_CLASSIFIER, Rfc2119Word.MAY)\n+\n+\n+def _general_name_has_uri_prefixes(uri_prefixes_lower, general_name_node):\n+    gn_type, gn_value = general_name_node.child\n+\n+    if gn_type == GeneralNameTypeName.UNIFORM_RESOURCE_IDENTIFIER:\n+        uri_lower = str(gn_value.pdu).lower()\n+\n+        return any(uri_lower.startswith(p) for p in uri_prefixes_lower)\n+\n+    return False\n+\n+\n+_HTTP_OR_HTTPS_PREFIXES = {'http://', 'https://'}\n+_HTTP_OR_LDAP_PREFIXES = {'http://', 'ldap://'}\n+\n+\n+def _aia_extension_has_aia_ocsp_http_uri(aia_syntax_node):\n+    for ad in aia_syntax_node.children.values():\n+        if ad.children['accessMethod'].pdu == rfc5280.id_ad_ocsp:\n+\n+            location_node = ad.children['accessLocation']\n+\n+            if _general_name_has_uri_prefixes(_HTTP_OR_HTTPS_PREFIXES, location_node):\n+                return True\n+\n+    return False\n+\n+\n+class CrlDistributionPointsExtensionPresenceValidator(validation.Validator):\n+    \"\"\"\n+    GEN-4.3.11-2: If the certificate does not include any access location of an OCSP responder as specified in clause\n+    4.4.1, then the certificate shall include a CRL distribution point extension.\n+    \"\"\"\n+    VALIDATION_CRLDP_EXTENSION_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.11-2.crldp_extension_missing'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_CRLDP_EXTENSION_MISSING],\n+            pdu_class=rfc5280.Extensions\n+        )\n+\n+    def validate(self, node):\n+        crldp_ext_and_idx = node.document.get_extension_by_oid(rfc5280.id_ce_cRLDistributionPoints)\n+\n+        if crldp_ext_and_idx:\n+            return\n+\n+        aia_ext_and_idx = node.document.get_extension_by_oid(rfc5280.id_pe_authorityInfoAccess)\n+\n+        if aia_ext_and_idx is None:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_CRLDP_EXTENSION_MISSING)\n+\n+        ext, idx = aia_ext_and_idx\n+\n+        try:\n+            _, aia_syntax_node = ext.children['extnValue'].child\n+        except ValueError:\n+            # no decoded AIA extension. Let other validator flag that finding\n+            return\n+\n+        if not _aia_extension_has_aia_ocsp_http_uri(aia_syntax_node):\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_CRLDP_EXTENSION_MISSING)\n+\n+\n+class CrlDistributionPointsValidator(validation.Validator):\n+    \"\"\"\n+    GEN-4.3.11-4: At least one of the present references shall use either http (http://) IETF RFC 7230-7235 [3] or ldap\n+    (ldap://) IETF RFC 4516 [4] scheme.\n+    \"\"\"\n+    VALIDATION_CRLDP_DP_NO_REQUIRED_URI = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.3.11-4.http_or_ldap_crldp_distribution_point_uri_missing'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_CRLDP_DP_NO_REQUIRED_URI],\n+            pdu_class=rfc5280.CRLDistributionPoints\n+        )\n+\n+    def validate(self, node):\n+        for distribution_point in node.children.values():\n+            try:\n+                full_name_node = distribution_point.navigate('distributionPoint.fullName')\n+            except document.PDUNavigationFailedError:\n+                continue\n+\n+            if any(\n+                    _general_name_has_uri_prefixes(\n+                        _HTTP_OR_LDAP_PREFIXES, gn\n+                    ) for gn in full_name_node.children.values()\n+            ):\n+                return\n+\n+        raise validation.ValidationFindingEncountered(self.VALIDATION_CRLDP_DP_NO_REQUIRED_URI)\n+\n+\n+class AuthorityInformationAccessValidator(validation.Validator):\n+    \"\"\"\n+    GEN-4.4.1-3: The Authority Information Access extension shall include an accessMethod OID,\n+    id-ad-caIssuers, with an accessLocation value specifying at least one access location of a valid CA\n+    certificate of the issuing CA.\n+    \"\"\"\n+    VALIDATION_AIA_CA_ISSUERS_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.4.1-3.ca_issuers_aia_access_method_absent'\n+    )\n+\n+    \"\"\"\n+    GEN-4.4.1-4: At least one accessLocation shall use the http (http://) IETF RFC 7230-7235 [3] scheme or https\n+    (https://) IETF RFC 2818 [5] scheme.\n+    \"\"\"\n+    VALIDATION_AIA_CA_ISSUERS_HTTP_URI_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.4.1-4.ca_issuers_aia_access_method_http_uri_missing'\n+    )\n+\n+    \"\"\"\n+    GEN-4.4.1-6: If OCSP is supported by the issuing CA, at least one access location shall specify either the http\n+    (http://) IETF RFC 7230-7235 [3] or https (https://) IETF RFC 2818 [5] scheme.\n+    \"\"\"\n+    VALIDATION_AIA_OCSP_HTTP_URI_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.4.1-6.ocsp_aia_access_method_http_uri_missing'\n+    )\n+\n+    \"\"\"\n+    GEN-4.4.1-8: If the certificate does not include any CRL distribution point extension in accordance with clause\n+    4.3.11, a reference to at least one OCSP responder shall be present.\n+    \"\"\"\n+    VALIDATION_AIA_OCSP_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.gen-4.4.1-8.ocsp_aia_access_method_absent'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_AIA_CA_ISSUERS_MISSING,\n+                self.VALIDATION_AIA_CA_ISSUERS_HTTP_URI_MISSING,\n+                self.VALIDATION_AIA_OCSP_MISSING,\n+                self.VALIDATION_AIA_OCSP_HTTP_URI_MISSING,\n+            ],\n+            pdu_class=rfc5280.AuthorityInfoAccessSyntax\n+        )\n+\n+    @classmethod\n+    def _get_locations_for_method(cls, method_oid, aia_syntax_node):\n+        return [\n+            ad.children['accessLocation'] for ad in aia_syntax_node.children.values()\n+            if ad.children['accessMethod'].pdu == method_oid\n+        ]\n+\n+    def validate(self, node):\n+        ca_issuers_gns = self._get_locations_for_method(rfc5280.id_ad_caIssuers, node)\n+\n+        if not ca_issuers_gns:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_AIA_CA_ISSUERS_MISSING)\n+\n+        if not any(_general_name_has_uri_prefixes(_HTTP_OR_HTTPS_PREFIXES, gn) for gn in ca_issuers_gns):\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_AIA_CA_ISSUERS_HTTP_URI_MISSING)\n+\n+        # only check for OCSP if no CRL DP extension is absent\n+        if node.document.get_extension_by_oid(rfc5280.id_ce_cRLDistributionPoints) is None:\n+            ocsp_gns = self._get_locations_for_method(rfc5280.id_ad_ocsp, node)\n+\n+            if not ocsp_gns:\n+                raise validation.ValidationFindingEncountered(self.VALIDATION_AIA_OCSP_MISSING)\n+\n+            if not any(_general_name_has_uri_prefixes(_HTTP_OR_HTTPS_PREFIXES, gn) for gn in ocsp_gns):\n+                raise validation.ValidationFindingEncountered(self.VALIDATION_AIA_OCSP_HTTP_URI_MISSING)\n+\n+\n+class QualifiedCertificatePoliciesValidator(validation.Validator):\n+    \"\"\"\n+    QCS-5.2-1: When certificates are issued as EU Qualified Certificates, they should include, in the certificate\n+    policies extension, one of the certificate policy identifiers defined in clause 5.3 of ETSI EN 319 411-2\n+    \"\"\"\n+    VALIDATION_RECOMMENDED_POLICY_IDENTIFIER_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_2.qcs-5.2-1.recommended_certificate_type_policy_identifier_missing'\n+    )\n+\n+    VALIDATION_MULTIPLE_POLICY_IDENTIFIERS_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.qcs-5.2-1.multiple_certificate_type_policy_identifiers_present'\n+    )\n+\n+    \"\"\"\n+    QCS-5.2-2: Policy identifiers included in the certificate policies extension of EU Qualified Certificates shall be\n+    consistent with the QCStatements according to clause 5.1.\n+    \"\"\"\n+    VALIDATION_MISMATCHED_POLICY_IDENTIFIER_FOR_TYPE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.qcs-5.2-2.mismatched_policy_identifier_for_certificate_type'\n+    )\n+\n+    # TODO: add EU qualified non-website types\n+    _CERTIFICATE_TYPE_SET_TO_POLICY_IDENTIFIER_MAPPINGS = [\n+        (etsi_constants.QEVCP_W_EIDAS_CERTIFICATE_TYPES, en_319_411_2.id_qcp_web),\n+        (etsi_constants.QNCP_W_OV_EIDAS_CERTIFICATE_TYPES, en_319_411_2.id_qncp_web),\n+        (etsi_constants.QNCP_W_IV_EIDAS_CERTIFICATE_TYPES, en_319_411_2.id_qncp_web),\n+        (etsi_constants.QNCP_W_GEN_LP_EIDAS_CERTIFICATE_TYPES, en_319_411_2.id_qncp_web_gen),\n+        (etsi_constants.QNCP_W_GEN_NP_EIDAS_CERTIFICATE_TYPES, en_319_411_2.id_qncp_web_gen),\n+    ]\n+\n+    def __init__(self, certificate_type: etsi_constants.CertificateType):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_RECOMMENDED_POLICY_IDENTIFIER_MISSING,\n+                self.VALIDATION_MULTIPLE_POLICY_IDENTIFIERS_PRESENT,\n+                self.VALIDATION_MISMATCHED_POLICY_IDENTIFIER_FOR_TYPE,\n+            ],\n+            pdu_class=rfc5280.CertificatePolicies\n+        )\n+\n+        self._certificate_type = certificate_type\n+\n+        self._recommended_policy_oid = self._get_recommended_policy_oid_for_certificate_type()\n+\n+    def match(self, node):\n+        # TODO: add support for non-QWAC types\n+        return super().match(node) and self._recommended_policy_oid is not None\n+\n+    def _get_recommended_policy_oid_for_certificate_type(self) -> typing.Optional[univ.ObjectIdentifier]:\n+        return next(\n+            (\n+                p for t, p in self._CERTIFICATE_TYPE_SET_TO_POLICY_IDENTIFIER_MAPPINGS\n+                if self._certificate_type in t\n+            ),\n+            None\n+        )\n+\n+    def validate(self, node):\n+        policy_oids = node.document.policy_oids\n+\n+        certificate_type_policy_oids = policy_oids & pkilint.etsi.asn1.en_319_411_2.QUALIFIED_POLICY_OIDS\n+\n+        if len(certificate_type_policy_oids) > 1:\n+            oids = oid.format_oids(certificate_type_policy_oids)\n+\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_MULTIPLE_POLICY_IDENTIFIERS_PRESENT,\n+                f'Multiple certificate type policy identifiers present: {oids}'\n+            )\n+        elif not certificate_type_policy_oids:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_RECOMMENDED_POLICY_IDENTIFIER_MISSING,\n+                f'Missing recommended certificate type policy identifier \"{self._recommended_policy_oid}\"'\n+            )\n+        elif self._recommended_policy_oid not in certificate_type_policy_oids:\n+            # if we're here, then there's one element in the set, so it's safe to do this\n+            policy_oid = next(iter(certificate_type_policy_oids))\n+\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_MISMATCHED_POLICY_IDENTIFIER_FOR_TYPE,\n+                f'Certificate type is {self._certificate_type} ({self._recommended_policy_oid}) but certificate '\n+                f'contains certificate type policy identifier \"{policy_oid}\"'\n+            )\n+\n+\n+class NaturalPersonKeyUsageValidator(etsi_shared.KeyUsageValidator):\n+    \"\"\"\n+    NAT-4.3.2-2: Certificates used to validate commitment to signed content (e.g. documents, agreements and/or\n+    transactions) shall be limited to type A, B or F.\n+    \"\"\"\n+    VALIDATION_INVALID_CONTENT_COMMITMENT_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-4.3.2-1.invalid_content_commitment_setting'\n+    )\n+\n+    \"\"\"\n+    NAT-4.3.2-3: Of these alternatives, type A should be used (see the security note 2 below).\n+    \"\"\"\n+    VALIDATION_NON_PREFERRED_CONTENT_COMMITMENT_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_2.nat-4.3.2-3.non_preferred_content_commitment_setting'\n+    )\n+\n+    def __init__(self, is_content_commitment_type):\n+        super().__init__(\n+            is_content_commitment_type,\n+            self.VALIDATION_INVALID_CONTENT_COMMITMENT_SETTING,\n+            self.VALIDATION_NON_PREFERRED_CONTENT_COMMITMENT_SETTING\n+        )\n+\n+\n+class ExtensionsPresenceValidator(common.ExtensionsPresenceValidator):\n+    VALIDATION_EXTENSIONS_FIELD_ABSENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.extensions_field_absent'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(self.VALIDATION_EXTENSIONS_FIELD_ABSENT)\ndiff --git a/pkilint/etsi/en_319_412_3.py b/pkilint/etsi/en_319_412_3.py\nnew file mode 100644\nindex 0000000..9953c7b\n--- /dev/null\n+++ b/pkilint/etsi/en_319_412_3.py\n@@ -0,0 +1,155 @@\n+from pyasn1_alt_modules import rfc5280\n+\n+from pkilint import common\n+from pkilint import validation\n+from pkilint.common import organization_id\n+from pkilint.etsi import etsi_shared\n+from pkilint.itu import x520_name\n+from pkilint.pkix import Rfc2119Word, name\n+\n+_REQUIRED_ATTRIBUTES = {\n+    rfc5280.id_at_countryName,\n+    rfc5280.id_at_organizationName,\n+    x520_name.id_at_organizationIdentifier,\n+    rfc5280.id_at_commonName,\n+}\n+\n+\n+class LegalPersonSubjectAttributeAllowanceValidator(common.AttributeIdentifierAllowanceValidator):\n+    \"\"\"\n+    LEG-4.2.1-2: The subject field shall include at least the following attributes as specified in Recommendation\n+    ITU-T X.520\n+    \"\"\"\n+    _CODE_CLASSIFIER = 'etsi.en_319_412_3.leg-4.2.1-2'\n+\n+    _ATTRIBUTE_ALLOWANCES = {a: Rfc2119Word.MUST for a in _REQUIRED_ATTRIBUTES}\n+\n+    def __init__(self):\n+        super().__init__(self._ATTRIBUTE_ALLOWANCES, self._CODE_CLASSIFIER, Rfc2119Word.MAY)\n+\n+\n+class LegalPersonDuplicateAttributeAllowanceValidator(validation.Validator):\n+    \"\"\"\n+    LEG-4.2.1-3: Only one instance of each of these attributes shall be present.\n+    \"\"\"\n+    VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_3.leg-4.2.1-3.prohibited_duplicate_attribute_present'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT],\n+            pdu_class=rfc5280.Name\n+        )\n+\n+    def validate(self, node):\n+        attr_counts = name.get_name_attribute_counts(node)\n+\n+        for a in _REQUIRED_ATTRIBUTES:\n+            if attr_counts[a] > 1:\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_PROHIBITED_DUPLICATE_ATTRIBUTE_PRESENT,\n+                    f'Prohibited duplicate attribute present: {a}'\n+                )\n+\n+\n+class LegalPersonOrganizationAttributesEqualityValidator(validation.Validator):\n+    \"\"\"\n+    LEG-4.2.1-6: The organizationIdentifier attribute shall contain an identification of the subject organization\n+    different from the organization name.\n+    \"\"\"\n+    VALIDATION_ORGID_ORGNAME_ATTRIBUTE_VALUES_EQUAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_3.leg-4.2.1-6.organization_id_and_organization_name_attribute_values_equal'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_ORGID_ORGNAME_ATTRIBUTE_VALUES_EQUAL],\n+            pdu_class=rfc5280.Name\n+        )\n+\n+    @classmethod\n+    def _get_dirstring_attribute_value(cls, node):\n+        try:\n+            _, value_node = node.children['value'].child\n+            _, decoded_value_node = value_node.child\n+\n+            return str(decoded_value_node.pdu)\n+        except ValueError:\n+            return None\n+\n+    def validate(self, node):\n+        # only get the first instance of the attributes\n+        orgname_attr_and_idx = next(\n+            iter(name.get_name_attributes_by_type(node, rfc5280.id_at_organizationName)), None\n+        )\n+        orgid_attr_and_idx = next(\n+            iter(name.get_name_attributes_by_type(node, x520_name.id_at_organizationIdentifier)), None\n+        )\n+\n+        if orgname_attr_and_idx and orgid_attr_and_idx:\n+            orgname_attr, _ = orgname_attr_and_idx\n+            orgid_attr, _ = orgid_attr_and_idx\n+\n+            orgname = self._get_dirstring_attribute_value(orgname_attr)\n+            orgid = self._get_dirstring_attribute_value(orgid_attr)\n+\n+            # if any of the attributes were not decoded, then return early\n+            if orgname is None or orgid is None:\n+                return\n+\n+            if orgname.casefold() == orgid.casefold():\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_ORGID_ORGNAME_ATTRIBUTE_VALUES_EQUAL,\n+                    f'Organization name and identifier attribute values are equal: \"{orgname}\"'\n+                )\n+\n+\n+class LegalPersonKeyUsageValidator(etsi_shared.KeyUsageValidator):\n+    \"\"\"\n+    LEG-4.3.1-3: Certificates used to validate commitment to signed content (e.g. documents, agreements and/or\n+    transactions) shall be limited to type A, B or F.\n+    \"\"\"\n+    VALIDATION_INVALID_CONTENT_COMMITMENT_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_3.leg-4.3.1-3.invalid_content_commitment_setting'\n+    )\n+\n+    \"\"\"\n+    LEG-4.3.1-4: Of these alternatives, type A should be used (see the security note 2 below).\n+    \"\"\"\n+    VALIDATION_NON_PREFERRED_CONTENT_COMMITMENT_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_3.leg-4.3.1-4.non_preferred_content_commitment_setting'\n+    )\n+\n+    def __init__(self, is_content_commitment_type):\n+        super().__init__(\n+            is_content_commitment_type,\n+            self.VALIDATION_INVALID_CONTENT_COMMITMENT_SETTING,\n+            self.VALIDATION_NON_PREFERRED_CONTENT_COMMITMENT_SETTING\n+        )\n+\n+\n+class LegalPersonCountryCodeValidator(validation.Validator):\n+    \"\"\"\n+    LEG-4.2.1-4: The countryName attribute shall specify the country in which the subject (legal person) is established.\n+    \"\"\"\n+    VALIDATION_UNKNOWN_COUNTRY_CODE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'etsi.en_319_412_3.leg-4.2.1-4.unknown_country_code'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_UNKNOWN_COUNTRY_CODE], pdu_class=rfc5280.X520countryName)\n+\n+    def validate(self, node):\n+        value_str = str(node.pdu)\n+\n+        if value_str not in organization_id.ISO3166_1_COUNTRY_CODES:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_UNKNOWN_COUNTRY_CODE,\n+                f'Unknown country code: \"{value_str}\"'\n+            )\ndiff --git a/pkilint/etsi/en_319_412_4.py b/pkilint/etsi/en_319_412_4.py\nnew file mode 100644\nindex 0000000..1f18d23\n--- /dev/null\n+++ b/pkilint/etsi/en_319_412_4.py\n@@ -0,0 +1,98 @@\n+from pyasn1_alt_modules import rfc5280\n+\n+from pkilint import validation\n+from pkilint.common import common_name\n+from pkilint.pkix import general_name, extension\n+\n+_ALLOWED_GENERAL_NAME_TYPES = {general_name.GeneralNameTypeName.DNS_NAME}\n+\n+\n+class NcpWCommonNameValidator(common_name.CommonNameValidator):\n+    \"\"\"\n+    WEB-4.1.3-4 (d):\n+\n+    If necessary to distinguish the website identified by the subject name, the subject commonName may contain a\n+    domain name or a Wildcard Domain Name (as defined in BRG [9]) which is one of the dNSName values of\n+    the subjectAltName extension of a website authentication certificate.\n+    \"\"\"\n+    VALIDATION_COMMON_NAME_UNKNOWN_SOURCE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_4.web-4.1.3-4.common_name_unknown_source'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            _ALLOWED_GENERAL_NAME_TYPES,\n+            self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE\n+        )\n+\n+\n+class QncpWCommonNameValidator(common_name.CommonNameValidator):\n+    \"\"\"\n+    WEB-4.1.4-2:\n+\n+    If necessary to distinguish the website identified by the subject name, the subject commonName may contain a\n+    domain name or a Wildcard Domain Name (as defined in BRG [9]) which is one of the dNSName values of\n+    the subjectAltName extension of a website authentication certificate.\n+    \"\"\"\n+    VALIDATION_COMMON_NAME_UNKNOWN_SOURCE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_4.web-4.1.4-2.common_name_unknown_source'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            _ALLOWED_GENERAL_NAME_TYPES,\n+            self.VALIDATION_COMMON_NAME_UNKNOWN_SOURCE\n+        )\n+\n+\n+class NcpWExtendedKeyUsagePresenceValidator(extension.ExtensionPresenceValidator):\n+    \"\"\"\n+    WEB-4.1.3-4: The following certificate profile requirements specified in the BRG [9] shall apply for subject\n+    certificate fields addressed by the following sub-sections of BRG [9] (the version of BRG [9] shall be as referenced\n+    in ETSI EN 319 411-1 [6] for [WEB] requirements):\n+    a) 7.1.2.3 f) extKeyUsage\n+    \"\"\"\n+    VALIDATION_EKU_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_4.web-4.1.3-4.eku_missing')\n+\n+    def __init__(self):\n+        super().__init__(\n+            extension_oid=rfc5280.id_ce_extKeyUsage,\n+            validation=self.VALIDATION_EKU_MISSING,\n+            pdu_class=rfc5280.Extensions\n+        )\n+\n+\n+class NcpWSubjectAltNamePresenceValidator(extension.ExtensionPresenceValidator):\n+    \"\"\"\n+    WEB-4.1.3-4: The following certificate profile requirements specified in the BRG [9] shall apply for subject\n+    certificate fields addressed by the following sub-sections of BRG [9] (the version of BRG [9] shall be as referenced\n+    in ETSI EN 319 411-1 [6] for [WEB] requirements):\n+    a) 7.1.2.3 b) Subject Alternative Name\n+    \"\"\"\n+    VALIDATION_SAN_MISSING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_4.web-4.1.3-4.san_missing')\n+\n+    def __init__(self):\n+        super().__init__(\n+            extension_oid=rfc5280.id_ce_subjectAltName,\n+            validation=self.VALIDATION_SAN_MISSING,\n+            pdu_class=rfc5280.Extensions\n+        )\n+\n+\n+class NcpWCriticalityExtendedKeyUsageValidator(extension.ExtensionCriticalityValidator):\n+    \"\"\"Validates that the criticality of the EKU extension conforms to BRG.\"\"\"\n+    EXTENDED_KEY_USAGE_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_4.web-4.1.3-4.eku_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validation=self.EXTENDED_KEY_USAGE_CRITICAL,\n+                         type_oid=rfc5280.id_ce_extKeyUsage,\n+                         is_critical=False)\ndiff --git a/pkilint/etsi/en_319_412_5.py b/pkilint/etsi/en_319_412_5.py\nnew file mode 100644\nindex 0000000..e7f3f2f\n--- /dev/null\n+++ b/pkilint/etsi/en_319_412_5.py\n@@ -0,0 +1,307 @@\n+from pkilint import validation, common\n+from pkilint.etsi import etsi_constants\n+from pkilint.etsi.asn1 import en_319_412_5\n+from iso3166 import countries_by_alpha2\n+from iso4217 import Currency\n+from urllib.parse import urlparse\n+from pyasn1_alt_modules import rfc3739\n+from pkilint.pkix import extension, Rfc2119Word\n+import iso639\n+\n+\n+class QcCCLegislationCountryCodeValidator(validation.Validator):\n+    \"\"\"EN 319 412-5 4.2.4.: QCStatement stating the country or set of countries under the legislation of which the\n+    certificate is issued as a qualified certificate. Constrained by ISO 3166-1 alpha-2 codes only. This Validator\n+    will check to see if there is a country code at all or if it is a valid code.\"\"\"\n+    VALIDATION_ISO_COUNTRY_CODE_LIST_EMPTY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.2.4.iso_country_code_list_empty'\n+    )\n+    VALIDATION_ISO_COUNTRY_CODE_INVALID = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.2.4.iso_country_code_invalid'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_ISO_COUNTRY_CODE_LIST_EMPTY, self.VALIDATION_ISO_COUNTRY_CODE_INVALID],\n+            pdu_class=en_319_412_5.QcCClegislation\n+        )\n+\n+    def validate(self, node):\n+        if not node.children:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_ISO_COUNTRY_CODE_LIST_EMPTY)\n+        for children in node.children.values():\n+            country = str(children.pdu)\n+            if country not in countries_by_alpha2:\n+                raise validation.ValidationFindingEncountered(self.VALIDATION_ISO_COUNTRY_CODE_INVALID,\n+                                                              f'Invalid country code found: \"{country}\"')\n+\n+\n+class QcEuRetentionPeriodValidator(validation.Validator):\n+    \"\"\"EN 319 412-5 4.3.3 QCStatement indicating the duration of the retention period \n+    material information. This QCStatement declares a retention period for material information\n+    relevant to the use of and reliance of on a certificate, expressed as a number of years after the expiry\n+    date of the certificate. So in short anything greater will be 0 will be valid.\n+    \"\"\"\n+    VALIDATION_RETENTION_PERIOD_NOT_POSITIVE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.3.retention_period_years_not_positive'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_RETENTION_PERIOD_NOT_POSITIVE],\n+            pdu_class=en_319_412_5.QcEuRetentionPeriod\n+        )\n+\n+    def validate(self, node):\n+        # noinspection PyTypeChecker\n+        valid_yrs = int(node.pdu)\n+        if not valid_yrs > 0:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_RETENTION_PERIOD_NOT_POSITIVE)\n+\n+\n+class QcEuPDSLanguageValidator(validation.Validator):\n+    \"\"\"Content of the QcEuPDS statement, in accordance with Clause 4.3.4 of EN 319-412-5.\n+    Valid ISO 639-1 language code\"\"\"\n+    VALIDATION_ISO_LANGUAGE_CODE_INVALID = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.4.iso_language_code_invalid'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_ISO_LANGUAGE_CODE_INVALID], pdu_class=en_319_412_5.PdsLocation)\n+\n+    def validate(self, node):\n+        language_code = str(node.children['language'].pdu).lower()\n+\n+        try:\n+            iso639.Language.from_part1(language_code)\n+        except iso639.LanguageNotFoundError:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_ISO_LANGUAGE_CODE_INVALID,\n+                f'Invalid language code found: \"{language_code}\"'\n+            )\n+\n+\n+class QcEuPDSHttpsURLValidator(validation.Validator):\n+    \"\"\"Content of the QcEuPDS statement, in accordance with Clause 4.3.4 of EN 319-412-5.\n+    Validator to check if the URL has the 'https' scheme.\"\"\"\n+    VALIDATION_URL_SCHEME_NOT_HTTPS = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.4.url_scheme_not_https'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_URL_SCHEME_NOT_HTTPS], pdu_class=en_319_412_5.PdsLocation)\n+\n+    def validate(self, node):\n+        url_string = str(node.children['url'].pdu)\n+        parsed_url = urlparse(url_string)\n+\n+        if parsed_url.scheme.lower() != 'https':\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_URL_SCHEME_NOT_HTTPS,\n+                                                          f'Non-HTTPS URL scheme found: \"{parsed_url.scheme}\"')\n+\n+\n+class QcTypeValidator(validation.Validator):\n+    \"\"\"EN 319 412-5 4.2.3 Declares that a certificate is issued as one and only one of the purposes\n+    of electronic signature, electronic seal or web site authentication. According to Stephen\n+    a qwac should never have seal or sign but may have psd2.\"\"\"\n+    VALIDATION_QC_TYPE_MISMATCH = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.2.3.qc_type_mismatch'\n+    )\n+\n+    VALIDATION_MULTIPLE_QC_TYPE_VALUES_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+\n+        'etsi.en_319_412_5.gen-4.2.3.multiple_qc_type_values_present'\n+    )\n+\n+    VALIDATION_QC_TYPE_LIST_EMPTY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.2.3.qc_type_list_empty'\n+    )\n+\n+    def __init__(self, certificate_type):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_QC_TYPE_MISMATCH,\n+                self.VALIDATION_QC_TYPE_LIST_EMPTY,\n+                self.VALIDATION_MULTIPLE_QC_TYPE_VALUES_PRESENT\n+            ],\n+            pdu_class=en_319_412_5.QcType\n+        )\n+\n+        self._certificate_type = certificate_type\n+\n+        if certificate_type in etsi_constants.WEB_AUTHENTICATION_CERTIFICATE_TYPES:\n+            self._expected_qc_type = en_319_412_5.id_etsi_qct_web\n+        else:\n+            self._expected_qc_type = None\n+\n+    def validate(self, node):\n+        if not node.children.values():\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_QC_TYPE_LIST_EMPTY)\n+\n+        if len(node.children.values()) != 1:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_MULTIPLE_QC_TYPE_VALUES_PRESENT)\n+\n+        if self._expected_qc_type:\n+            _, qctype_value = node.child\n+\n+            if qctype_value.pdu != self._expected_qc_type:\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_QC_TYPE_MISMATCH,\n+                    f'Certificate type is \"{self._certificate_type.to_option_str}\" but QcType qualified '\n+                    f'statement contains \"{qctype_value.pdu}\"'\n+                )\n+\n+\n+class QcEuLimitValueValidator(validation.Validator):\n+    \"\"\"\n+    This QCStatement declares a limitation on the value of transaction for which a certificate\n+    can be used. \n+    MonetaryValue:: == SEQUENCE {\n+        currency Iso4217CurrencyCode,\n+        amount INTEGER,\n+        exponent INTEGR\n+    }\n+    -- value = amount * 10^exponent\n+    Iso4217CurrencyCode:: = CHOICE {\n+        alphabetic PrintableString (Size (3)), -- Recommended\n+        numeric INTEGER (1..999) } \n+        -- Alphabetic or numeric currency code as defined in ISO 4217\n+        -- It is recommended that the Alphabetic form is used \n+    }\n+    QCS-4.3.2-01: The currency codes shall be defined in ISO 4217.\n+    QCS-4.3.2-02: The alphabetic form of currency codes should be used.\n+\n+    Things to validate - valid iso 4217 currency code \n+                       - warning if the numeric code is used \n+                       - Positive amount and exponent value\n+    \"\"\"\n+    VALIDATION_CURRENCY_CODE_INVALID = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.2.currency_code_invalid'\n+    )\n+\n+    DISCOURAGED_VALIDATION_NUMERIC_CURRENCY_CODE_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_5.gen-4.3.2.discouraged_numeric_currency_code_present'\n+    )\n+\n+    VALIDATION_AMOUNT_NEGATIVE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.2.amount_negative'\n+    )\n+\n+    VALIDATION_EXPONENT_NEGATIVE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.gen-4.3.2.exponent_negative'\n+    )\n+\n+    def __init__(self):\n+        self._alpha_codes = set()\n+        self._numeric_codes = set()\n+\n+        for currency in Currency:\n+            self._alpha_codes.add(currency.code)\n+            self._numeric_codes.add(currency.number)\n+\n+        # Let's remove the testing and unknown currency codes from the set. \n+        alpha_bad_codes = {'XTS', 'XXX'}\n+        numeric_bad_codes = {'999', '963'}\n+\n+        self._alpha_codes -= alpha_bad_codes\n+        self._numeric_codes -= numeric_bad_codes\n+\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_CURRENCY_CODE_INVALID,\n+                self.DISCOURAGED_VALIDATION_NUMERIC_CURRENCY_CODE_PRESENT,\n+                self.VALIDATION_AMOUNT_NEGATIVE,\n+                self.VALIDATION_EXPONENT_NEGATIVE\n+            ],\n+            pdu_class=en_319_412_5.MonetaryValue)\n+\n+    def validate(self, node):\n+        findings = []\n+        currency_code_type, iso_code = node.children[\"currency\"].child\n+        if currency_code_type != \"alphabetic\":\n+            findings.append(\n+                validation.ValidationFindingDescription(\n+                    self.DISCOURAGED_VALIDATION_NUMERIC_CURRENCY_CODE_PRESENT,\n+                    None)\n+            )\n+            iso_code = int(iso_code.pdu)\n+        else:\n+            iso_code = str(iso_code.pdu)\n+\n+        if iso_code not in self._alpha_codes and iso_code not in self._numeric_codes:\n+            findings.append(validation.ValidationFindingDescription(self.VALIDATION_CURRENCY_CODE_INVALID, None))\n+        if node.children[\"amount\"].pdu < 0:\n+            findings.append(validation.ValidationFindingDescription(self.VALIDATION_AMOUNT_NEGATIVE, None))\n+        if node.children[\"exponent\"].pdu < 0:\n+            findings.append(validation.ValidationFindingEncountered(self.VALIDATION_EXPONENT_NEGATIVE, None))\n+\n+        return validation.ValidationResult(self, node, findings)\n+\n+\n+class QcStatementsExtensionCriticalityValidator(extension.ExtensionCriticalityValidator):\n+    \"\"\"EN 319 412-5 QCS-4.1-02 The qcStatements extension shall not be marked as critical\n+    \"\"\"\n+    VALIDATION_QCSTATEMENTS_EXTENSION_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_5.qcs-4.1-02.qcstatements_extension_is_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(type_oid=rfc3739.id_pe_qcStatements, is_critical=False,\n+                         validation=self.VALIDATION_QCSTATEMENTS_EXTENSION_CRITICAL)\n+\n+\n+class QcStatementIdentifierAllowanceValidator(common.ElementIdentifierAllowanceValidator):\n+    \"\"\"\n+    EN 319 412-5:\n+    QCS-5-01: EU qualified certificates shall include QCStatements in accordance with table 2\n+    \"\"\"\n+    _CODE_CLASSIFIER = 'etsi.en_319_412_5.qcs-5.01'\n+\n+    # qualified statements\n+    _OID_TO_CODE_NAME = {\n+        en_319_412_5.id_etsi_qcs_QcCompliance: 'qc_compliance',\n+        en_319_412_5.id_etsi_qcs_QcType: 'qc_type',\n+        en_319_412_5.id_etsi_qcs_QcCClegislation: 'qc_cc_legislation',\n+        en_319_412_5.id_etsi_qcs_QcSSCD: 'qc_sscd',\n+    }\n+\n+    @classmethod\n+    def retrieve_qualified_statement_id(cls, node):\n+        return node.children['statementId']\n+\n+    def __init__(self, certificate_type: etsi_constants.CertificateType):\n+        allowances = {}\n+\n+        if certificate_type in etsi_constants.EU_QWAC_TYPES:\n+            allowances[en_319_412_5.id_etsi_qcs_QcCompliance] = Rfc2119Word.MUST\n+            allowances[en_319_412_5.id_etsi_qcs_QcCClegislation] = Rfc2119Word.MUST_NOT\n+            allowances[en_319_412_5.id_etsi_qcs_QcType] = Rfc2119Word.MUST\n+        elif certificate_type in etsi_constants.NON_EU_QWAC_TYPES:\n+            allowances[en_319_412_5.id_etsi_qcs_QcCompliance] = Rfc2119Word.MUST\n+            allowances[en_319_412_5.id_etsi_qcs_QcCClegislation] = Rfc2119Word.MUST\n+\n+        if certificate_type in etsi_constants.QWAC_TYPES:\n+            allowances[en_319_412_5.id_etsi_qcs_QcSSCD] = Rfc2119Word.MUST_NOT\n+\n+        super().__init__(\n+            'qualified statement',\n+            self.retrieve_qualified_statement_id,\n+            allowances,\n+            f'{self._CODE_CLASSIFIER}.{{oid}}_qualified_statement_present',\n+            f'{self._CODE_CLASSIFIER}.{{oid}}_qualified_statement_absent',\n+            None,\n+            pdu_class=rfc3739.QCStatements\n+        )\ndiff --git a/pkilint/etsi/etsi_constants.py b/pkilint/etsi/etsi_constants.py\nnew file mode 100644\nindex 0000000..00326f6\n--- /dev/null\n+++ b/pkilint/etsi/etsi_constants.py\n@@ -0,0 +1,233 @@\n+import enum\n+from enum import auto\n+\n+from pkilint.cabf.serverauth import serverauth_constants\n+\n+EN_319_412_VERSION = '2023-09'\n+\n+\n+@enum.unique\n+class CertificateType(enum.IntEnum):\n+    # pre-certificate types\n+    NCP_W_NATURAL_PERSON_PRE_CERTIFICATE = auto()\n+    NCP_W_LEGAL_PERSON_PRE_CERTIFICATE = auto()\n+    DVCP_PRE_CERTIFICATE = auto()\n+    IVCP_PRE_CERTIFICATE = auto()\n+    OVCP_PRE_CERTIFICATE = auto()\n+    EVCP_PRE_CERTIFICATE = auto()\n+    QEVCP_W_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_IV_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_OV_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_GEN_NATURAL_PERSON_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_GEN_LEGAL_PERSON_EIDAS_PRE_CERTIFICATE = auto()\n+    QEVCP_W_NON_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_IV_NON_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_OV_NON_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_PRE_CERTIFICATE = auto()\n+    QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_PRE_CERTIFICATE = auto()\n+    QEVCP_W_PSD2_EIDAS_PRE_CERTIFICATE = auto()\n+    QEVCP_W_PSD2_EIDAS_NON_BROWSER_PRE_CERTIFICATE = auto()\n+\n+    # final certificate types\n+    NCP_W_NATURAL_PERSON_FINAL_CERTIFICATE = auto()\n+    NCP_W_LEGAL_PERSON_FINAL_CERTIFICATE = auto()\n+    DVCP_FINAL_CERTIFICATE = auto()\n+    IVCP_FINAL_CERTIFICATE = auto()\n+    OVCP_FINAL_CERTIFICATE = auto()\n+    EVCP_FINAL_CERTIFICATE = auto()\n+    QEVCP_W_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_IV_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_OV_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_GEN_NATURAL_PERSON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_GEN_LEGAL_PERSON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QEVCP_W_NON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_IV_NON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_OV_NON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE = auto()\n+    QEVCP_W_PSD2_EIDAS_FINAL_CERTIFICATE = auto()\n+    QEVCP_W_PSD2_EIDAS_NON_BROWSER_FINAL_CERTIFICATE = auto()\n+\n+    NCP_NATURAL_PERSON_CERTIFICATE = auto()\n+    NCP_LEGAL_PERSON_CERTIFICATE = auto()\n+\n+    def __str__(self):\n+        return self.name\n+\n+    @property\n+    def to_option_str(self):\n+        return self.name.replace('_', '-')\n+\n+    @staticmethod\n+    def from_option_str(value):\n+        value = value.replace('-', '_').upper()\n+\n+        return CertificateType[value]\n+\n+\n+DVCP_CERTIFICATE_TYPES = {CertificateType.DVCP_PRE_CERTIFICATE, CertificateType.DVCP_FINAL_CERTIFICATE}\n+IVCP_CERTIFICATE_TYPES = {CertificateType.IVCP_PRE_CERTIFICATE, CertificateType.IVCP_FINAL_CERTIFICATE}\n+OVCP_CERTIFICATE_TYPES = {CertificateType.OVCP_PRE_CERTIFICATE, CertificateType.OVCP_FINAL_CERTIFICATE}\n+EVCP_CERTIFICATE_TYPES = {CertificateType.EVCP_PRE_CERTIFICATE, CertificateType.EVCP_FINAL_CERTIFICATE}\n+\n+QNCP_W_IV_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_IV_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_EIDAS_FINAL_CERTIFICATE,\n+}\n+QNCP_W_IV_NON_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_IV_NON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_NON_EIDAS_FINAL_CERTIFICATE,\n+}\n+\n+QNCP_W_OV_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_OV_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_EIDAS_FINAL_CERTIFICATE,\n+}\n+QNCP_W_OV_NON_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_OV_NON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_NON_EIDAS_FINAL_CERTIFICATE,\n+}\n+\n+QNCP_W_IV_CERTIFICATE_TYPES = QNCP_W_IV_EIDAS_CERTIFICATE_TYPES | QNCP_W_IV_NON_EIDAS_CERTIFICATE_TYPES\n+QNCP_W_OV_CERTIFICATE_TYPES = QNCP_W_OV_EIDAS_CERTIFICATE_TYPES | QNCP_W_OV_NON_EIDAS_CERTIFICATE_TYPES\n+\n+QNCP_W_GEN_NP_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_FINAL_CERTIFICATE,\n+}\n+QNCP_W_GEN_NP_NON_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE,\n+}\n+\n+QNCP_W_GEN_NP_CERTIFICATE_TYPES = QNCP_W_GEN_NP_EIDAS_CERTIFICATE_TYPES | QNCP_W_GEN_NP_NON_EIDAS_CERTIFICATE_TYPES\n+\n+QNCP_W_GEN_LP_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_FINAL_CERTIFICATE,\n+}\n+QNCP_W_GEN_LP_NON_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_NON_EIDAS_FINAL_CERTIFICATE,\n+}\n+\n+QNCP_W_GEN_LP_CERTIFICATE_TYPES = QNCP_W_GEN_LP_EIDAS_CERTIFICATE_TYPES | QNCP_W_GEN_LP_NON_EIDAS_CERTIFICATE_TYPES\n+\n+QNCP_W_GEN_CERTIFICATE_TYPES = QNCP_W_GEN_NP_CERTIFICATE_TYPES | QNCP_W_GEN_LP_CERTIFICATE_TYPES\n+\n+NCP_W_NATURAL_PERSON_CERTIFICATE_TYPES = {\n+    CertificateType.NCP_W_NATURAL_PERSON_PRE_CERTIFICATE,\n+    CertificateType.NCP_W_NATURAL_PERSON_FINAL_CERTIFICATE,\n+} | QNCP_W_GEN_NP_CERTIFICATE_TYPES\n+\n+NCP_W_LEGAL_PERSON_CERTIFICATE_TYPES = {\n+    CertificateType.NCP_W_LEGAL_PERSON_PRE_CERTIFICATE,\n+    CertificateType.NCP_W_LEGAL_PERSON_FINAL_CERTIFICATE,\n+} | QNCP_W_GEN_LP_CERTIFICATE_TYPES\n+\n+NCP_W_CERTIFICATE_TYPES = NCP_W_NATURAL_PERSON_CERTIFICATE_TYPES | NCP_W_LEGAL_PERSON_CERTIFICATE_TYPES\n+\n+NCP_NP_CERTIFICATE_TYPES = {CertificateType.NCP_NATURAL_PERSON_CERTIFICATE} | NCP_W_NATURAL_PERSON_CERTIFICATE_TYPES\n+\n+NCP_LP_CERTIFICATE_TYPES = {CertificateType.NCP_LEGAL_PERSON_CERTIFICATE} | NCP_W_LEGAL_PERSON_CERTIFICATE_TYPES\n+\n+QEVCP_W_PSD2_EIDAS_NON_BROWSER_CERTIFICATE_TYPES = {\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_FINAL_CERTIFICATE,\n+}\n+\n+QEVCP_W_PSD2_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QEVCP_W_PSD2_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_FINAL_CERTIFICATE,\n+} | QEVCP_W_PSD2_EIDAS_NON_BROWSER_CERTIFICATE_TYPES\n+\n+QEVCP_W_EIDAS_CERTIFICATE_TYPES = {\n+    CertificateType.QEVCP_W_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_EIDAS_FINAL_CERTIFICATE\n+} | QEVCP_W_PSD2_EIDAS_CERTIFICATE_TYPES\n+\n+QEVCP_W_NON_EIDAS_CERTIFICATE_TYPES = {\n+      CertificateType.QEVCP_W_NON_EIDAS_PRE_CERTIFICATE,\n+      CertificateType.QEVCP_W_NON_EIDAS_FINAL_CERTIFICATE\n+}\n+\n+QEVCP_W_CERTIFICATE_TYPES = QEVCP_W_EIDAS_CERTIFICATE_TYPES | QEVCP_W_NON_EIDAS_CERTIFICATE_TYPES\n+\n+QNCP_W_CERTIFICATE_TYPES = QNCP_W_IV_CERTIFICATE_TYPES | QNCP_W_OV_CERTIFICATE_TYPES | QEVCP_W_CERTIFICATE_TYPES\n+\n+CABF_DV_CERTIFICATE_TYPES = DVCP_CERTIFICATE_TYPES\n+CABF_IV_CERTIFICATE_TYPES = IVCP_CERTIFICATE_TYPES | QNCP_W_IV_CERTIFICATE_TYPES\n+CABF_OV_CERTIFICATE_TYPES = OVCP_CERTIFICATE_TYPES | QNCP_W_OV_CERTIFICATE_TYPES\n+CABF_EV_CERTIFICATE_TYPES = EVCP_CERTIFICATE_TYPES | QEVCP_W_CERTIFICATE_TYPES | QEVCP_W_PSD2_EIDAS_CERTIFICATE_TYPES\n+\n+CABF_CERTIFICATE_TYPES = (\n+        CABF_DV_CERTIFICATE_TYPES |\n+        CABF_IV_CERTIFICATE_TYPES |\n+        CABF_OV_CERTIFICATE_TYPES |\n+        CABF_EV_CERTIFICATE_TYPES\n+)\n+\n+NATURAL_PERSON_CERTIFICATE_TYPES = (CABF_IV_CERTIFICATE_TYPES | QNCP_W_GEN_NP_CERTIFICATE_TYPES |\n+                                    NCP_NP_CERTIFICATE_TYPES)\n+\n+LEGAL_PERSON_CERTIFICATE_TYPES = (CABF_OV_CERTIFICATE_TYPES | CABF_EV_CERTIFICATE_TYPES |\n+                                  QNCP_W_GEN_LP_CERTIFICATE_TYPES | NCP_LP_CERTIFICATE_TYPES)\n+\n+QWAC_TYPES = (QEVCP_W_CERTIFICATE_TYPES | QNCP_W_IV_CERTIFICATE_TYPES |\n+              QNCP_W_OV_CERTIFICATE_TYPES | QNCP_W_GEN_NP_CERTIFICATE_TYPES | QNCP_W_GEN_LP_CERTIFICATE_TYPES |\n+              QEVCP_W_PSD2_EIDAS_CERTIFICATE_TYPES)\n+\n+WEB_AUTHENTICATION_CERTIFICATE_TYPES = CABF_CERTIFICATE_TYPES | QWAC_TYPES | NCP_W_CERTIFICATE_TYPES\n+\n+NON_CABF_WEB_AUTHENTICATION_CERTIFICATE_TYPES = WEB_AUTHENTICATION_CERTIFICATE_TYPES - CABF_CERTIFICATE_TYPES\n+\n+EU_QWAC_TYPES = {\n+    CertificateType.QEVCP_W_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_NATURAL_PERSON_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_GEN_LEGAL_PERSON_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_FINAL_CERTIFICATE,\n+}\n+\n+NON_EU_QWAC_TYPES = QWAC_TYPES - EU_QWAC_TYPES\n+\n+ETSI_TYPE_TO_CABF_SERVERAUTH_TYPE_MAPPINGS = {\n+    CertificateType.DVCP_PRE_CERTIFICATE: serverauth_constants.CertificateType.DV_PRE_CERTIFICATE,\n+    CertificateType.IVCP_PRE_CERTIFICATE: serverauth_constants.CertificateType.IV_PRE_CERTIFICATE,\n+    CertificateType.OVCP_PRE_CERTIFICATE: serverauth_constants.CertificateType.OV_PRE_CERTIFICATE,\n+    CertificateType.EVCP_PRE_CERTIFICATE: serverauth_constants.CertificateType.EV_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.EV_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.IV_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.OV_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_NON_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.EV_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_NON_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.IV_PRE_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_NON_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.OV_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_PRE_CERTIFICATE: serverauth_constants.CertificateType.EV_PRE_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_PRE_CERTIFICATE: (\n+        serverauth_constants.CertificateType.EV_PRE_CERTIFICATE\n+    ),\n+\n+    CertificateType.DVCP_FINAL_CERTIFICATE: serverauth_constants.CertificateType.DV_FINAL_CERTIFICATE,\n+    CertificateType.IVCP_FINAL_CERTIFICATE: serverauth_constants.CertificateType.IV_FINAL_CERTIFICATE,\n+    CertificateType.OVCP_FINAL_CERTIFICATE: serverauth_constants.CertificateType.OV_FINAL_CERTIFICATE,\n+    CertificateType.EVCP_FINAL_CERTIFICATE: serverauth_constants.CertificateType.EV_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.EV_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.IV_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.OV_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_NON_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.EV_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_IV_NON_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.IV_FINAL_CERTIFICATE,\n+    CertificateType.QNCP_W_OV_NON_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.OV_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_FINAL_CERTIFICATE: serverauth_constants.CertificateType.EV_FINAL_CERTIFICATE,\n+    CertificateType.QEVCP_W_PSD2_EIDAS_NON_BROWSER_FINAL_CERTIFICATE: (\n+        serverauth_constants.CertificateType.EV_FINAL_CERTIFICATE\n+    ),\n+}\ndiff --git a/pkilint/etsi/etsi_finding_filter.py b/pkilint/etsi/etsi_finding_filter.py\nnew file mode 100644\nindex 0000000..e1ce41a\n--- /dev/null\n+++ b/pkilint/etsi/etsi_finding_filter.py\n@@ -0,0 +1,21 @@\n+from pkilint import finding_filter\n+from pkilint.cabf.serverauth import serverauth_subscriber\n+from pkilint.etsi.asn1 import ts_119_495 as ts_119_495_asn1\n+\n+\n+class Psd2CabfServerauthValidityPeriodFilter(finding_filter.FindingDescriptionFilter):\n+    _TARGET_VALIDATIONS = {\n+        serverauth_subscriber.SubscriberValidityPeriodValidator.VALIDATION_VALIDITY_PERIOD_EXCEEDS_397_DAYS,\n+        serverauth_subscriber.SubscriberValidityPeriodValidator.VALIDATION_VALIDITY_PERIOD_EXCEEDS_398_DAYS,\n+    }\n+\n+    def filter(self, result, finding_description):\n+        if finding_description.finding in self._TARGET_VALIDATIONS:\n+            \"\"\"\n+            OVR-6.1-3: TSPs issuing certificates for EU PSD2 may use the following policy identifier to augment the\n+            policy requirements associated with policy identifier QEVCP-w or QNCP-w as specified in\n+            ETSI EN 319 411-2 [5] giving precedence to the requirements defined in the present document.\n+            \"\"\"\n+            return ts_119_495_asn1.qcp_web_psd2 not in result.node.document.policy_oids\n+        else:\n+            return True\ndiff --git a/pkilint/etsi/etsi_shared.py b/pkilint/etsi/etsi_shared.py\nnew file mode 100644\nindex 0000000..8917fce\n--- /dev/null\n+++ b/pkilint/etsi/etsi_shared.py\n@@ -0,0 +1,116 @@\n+import enum\n+from typing import Optional\n+\n+from pyasn1_alt_modules import rfc5280\n+\n+from pkilint import validation, document\n+from pkilint.itu import bitstring\n+from pkilint.pkix.certificate.certificate_extension import KeyUsageBitName\n+\n+\n+class KeyUsageValidator(validation.Validator):\n+    \"\"\"\n+    NAT-4.3.2-1: The key usage extension shall be present and shall contain one (and only one) of the key usage settings\n+    defined in table 1 (A, B, C, D, E or F).\n+    \"\"\"\n+    VALIDATION_UNKNOWN_KEY_USAGE_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.en_319_412_2.nat-4.3.2-1.unknown_key_usage_setting'\n+    )\n+\n+    \"\"\"\n+    NAT-4.3.2-1: ... Type A, C or E should be used to avoid mixed usage of keys.\n+    \"\"\"\n+    VALIDATION_MIXED_KEY_USAGE_SETTING = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'etsi.en_319_412_2.nat-4.3.2-1.mixed_key_usage_setting'\n+    )\n+\n+    _ALL_KUS = {str(n) for n in rfc5280.KeyUsage.namedValues}\n+\n+    def __init__(\n+            self,\n+            is_content_commitment_type: Optional[bool],\n+            validation_invalid_content_commitment_setting: validation.ValidationFinding,\n+            validation_non_preferred_content_commitment_setting: validation.ValidationFinding\n+    ):\n+        super().__init__(\n+            validations=[\n+                self.VALIDATION_UNKNOWN_KEY_USAGE_SETTING,\n+                self.VALIDATION_MIXED_KEY_USAGE_SETTING,\n+                validation_invalid_content_commitment_setting,\n+                validation_non_preferred_content_commitment_setting,\n+            ],\n+            pdu_class=rfc5280.KeyUsage\n+        )\n+\n+        self._is_content_commitment_type = is_content_commitment_type\n+\n+        self._validation_invalid_content_commitment_setting = validation_invalid_content_commitment_setting\n+        self._validation_non_preferred_content_commitment_setting = validation_non_preferred_content_commitment_setting\n+\n+    class KeyUsageSetting(enum.Enum):\n+        A = ({KeyUsageBitName.NON_REPUDIATION}, set())\n+        B = ({KeyUsageBitName.NON_REPUDIATION, KeyUsageBitName.DIGITAL_SIGNATURE}, set())\n+        C = ({KeyUsageBitName.DIGITAL_SIGNATURE}, set())\n+        D = ({KeyUsageBitName.DIGITAL_SIGNATURE}, {KeyUsageBitName.KEY_AGREEMENT, KeyUsageBitName.KEY_ENCIPHERMENT})\n+        E = (set(), {KeyUsageBitName.KEY_AGREEMENT, KeyUsageBitName.KEY_ENCIPHERMENT})\n+        F = (\n+            {KeyUsageBitName.NON_REPUDIATION, KeyUsageBitName.DIGITAL_SIGNATURE},\n+            {KeyUsageBitName.KEY_AGREEMENT, KeyUsageBitName.KEY_ENCIPHERMENT}\n+        )\n+\n+    _CONTENT_COMMITMENT_SETTINGS = {KeyUsageSetting.A, KeyUsageSetting.B, KeyUsageSetting.F}\n+    _NON_CONTENT_COMMITMENT_SETTINGS = {s for s in KeyUsageSetting} - _CONTENT_COMMITMENT_SETTINGS\n+\n+    _MIXED_USE_SETTINGS = {KeyUsageSetting.B, KeyUsageSetting.D, KeyUsageSetting.F}\n+\n+    @classmethod\n+    def _detect_setting(cls, key_usage_node: document.PDUNode) -> Optional[KeyUsageSetting]:\n+        asserted_bits = {k for k in cls._ALL_KUS if bitstring.has_named_bit(key_usage_node, k)}\n+\n+        for setting in cls.KeyUsageSetting:\n+            n_of_n_required_bits, one_of_n_required_bits = setting.value\n+\n+            allowed_bits = n_of_n_required_bits | one_of_n_required_bits\n+\n+            if (\n+                    asserted_bits >= n_of_n_required_bits and\n+                    (len(one_of_n_required_bits & asserted_bits) == 1 or not one_of_n_required_bits) and\n+                    not any(asserted_bits - allowed_bits)\n+            ):\n+                return setting\n+\n+        return None\n+\n+    def validate(self, node):\n+        setting = self._detect_setting(node)\n+\n+        if setting is None:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_UNKNOWN_KEY_USAGE_SETTING)\n+\n+        if self._is_content_commitment_type is not None:\n+            if self._is_content_commitment_type:\n+                if setting not in self._CONTENT_COMMITMENT_SETTINGS:\n+                    raise validation.ValidationFindingEncountered(self._validation_invalid_content_commitment_setting)\n+                elif setting != self.KeyUsageSetting.A:\n+                    raise validation.ValidationFindingEncountered(\n+                        self._validation_non_preferred_content_commitment_setting\n+                    )\n+            elif not self._is_content_commitment_type and setting not in self._NON_CONTENT_COMMITMENT_SETTINGS:\n+                raise validation.ValidationFindingEncountered(self._validation_invalid_content_commitment_setting)\n+\n+        if setting in self._MIXED_USE_SETTINGS:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_MIXED_KEY_USAGE_SETTING)\n+\n+\n+VALIDATION_INTERNAL_DOMAIN_NAME = validation.ValidationFinding(\n+    validation.ValidationFindingSeverity.ERROR,\n+    'etsi.internal_domain_name'\n+)\n+\n+\n+VALIDATION_INTERNAL_IP_ADDRESS = validation.ValidationFinding(\n+    validation.ValidationFindingSeverity.ERROR,\n+    'etsi.internal_ip_address'\n+)\ndiff --git a/pkilint/etsi/finding_metadata.csv b/pkilint/etsi/finding_metadata.csv\nnew file mode 100644\nindex 0000000..d0685fd\n--- /dev/null\n+++ b/pkilint/etsi/finding_metadata.csv\n@@ -0,0 +1,1 @@\n+severity,code,description\ndiff --git a/pkilint/etsi/ts_119_312.py b/pkilint/etsi/ts_119_312.py\nnew file mode 100644\nindex 0000000..a5ea62b\n--- /dev/null\n+++ b/pkilint/etsi/ts_119_312.py\n@@ -0,0 +1,233 @@\n+from typing import Optional\n+\n+from pyasn1.codec.der.encoder import encode\n+from pyasn1.type import univ, base\n+from pyasn1_alt_modules import rfc3279, rfc5280, rfc5480, rfc5639, rfc4055\n+\n+from pkilint import validation\n+from pkilint.pkix import algorithm\n+from pkilint.pkix.certificate import certificate_key\n+\n+\n+class RsaKeyValidator(validation.Validator):\n+    \"\"\"\n+    TS 119 312, clause 6.2.2.1:\n+\n+    The public exponent e shall be an odd positive integer such that 2^16 < e < 2^256.\n+    \"\"\"\n+    VALIDATION_RSA_EXPONENT_OUT_OF_RANGE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'ts_119_312.6.2.2.1.rsa_exponent_of_range'\n+    )\n+\n+    VALIDATION_RSA_SMALL_MODULUS = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'ts_119_312.8.4.rsa_small_modulus'\n+    )\n+\n+    _MIN_MODULUS_LENGTH = 1900\n+    _MIN_EXPONENT_EXCLUSIVE = 1 << 16\n+    _MAX_EXPONENT_EXCLUSIVE = 1 << 256\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_RSA_EXPONENT_OUT_OF_RANGE, self.VALIDATION_RSA_SMALL_MODULUS],\n+            pdu_class=rfc3279.RSAPublicKey\n+        )\n+\n+    def validate(self, node):\n+        modulus_len = int(node.children['modulus'].pdu).bit_length()\n+        exponent_int = int(node.children['publicExponent'].pdu)\n+\n+        findings = []\n+\n+        if modulus_len < 1900:\n+            findings.append(validation.ValidationFindingDescription(\n+                self.VALIDATION_RSA_SMALL_MODULUS,\n+                f'RSA public key has a modulus length of {modulus_len} bits'\n+            ))\n+\n+        if not self._MIN_EXPONENT_EXCLUSIVE < exponent_int < self._MAX_EXPONENT_EXCLUSIVE:\n+            findings.append(validation.ValidationFindingDescription(\n+                self.VALIDATION_RSA_EXPONENT_OUT_OF_RANGE,\n+                f'RSA public key has an exponent of {exponent_int}'\n+            ))\n+\n+        return validation.ValidationResult(self, node, findings)\n+\n+\n+def _create_alg_id_der(o: univ.ObjectIdentifier, params: Optional[base.Asn1Type]) -> bytes:\n+    alg_id = rfc5280.AlgorithmIdentifier()\n+    alg_id['algorithm'] = o\n+\n+    if params is not None:\n+        alg_id['parameters'] = params\n+\n+    return encode(alg_id)\n+\n+\n+_SPKI_ALG_ID_ENCODINGS = (\n+    [\n+        # RSA\n+        _create_alg_id_der(rfc5480.rsaEncryption, univ.Null(''))\n+    ] + [\n+        # ECDSA\n+        _create_alg_id_der(rfc5480.id_ecPublicKey, c) for c in (\n+            univ.ObjectIdentifier('1.2.250.1.223.101.256.1'),  # FRP256v1\n+            rfc5639.brainpoolP256r1,\n+            rfc5639.brainpoolP384r1,\n+            rfc5639.brainpoolP512r1,\n+            rfc5480.secp256r1,\n+            rfc5480.secp384r1,\n+            rfc5480.secp521r1,\n+        )\n+    ]\n+    # TODO: add DSA\n+)\n+\n+\n+class AllowedPublicKeyTypeValidator(certificate_key.AllowedPublicKeyAlgorithmEncodingValidator):\n+    \"\"\"\n+    From EN 319 412 2:\n+\n+    GEN-4.2.5-1\n+\n+    The subject public key should be selected according to ETSI TS 119 312 [i.7].\n+\n+    NOTE: Cryptographic suites recommendations defined in ETSI TS 119 312 [i.7] can be superseded by national\n+    recommendations.\n+    \"\"\"\n+    VALIDATION_DISCOURAGED_PUBLIC_KEY_TYPE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'etsi.en_319_412_2.gen-4.2.5-1.discouraged_public_key_type'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validation=self.VALIDATION_DISCOURAGED_PUBLIC_KEY_TYPE,\n+            allowed_encodings=_SPKI_ALG_ID_ENCODINGS,\n+            pdu_class=rfc5280.AlgorithmIdentifier\n+        )\n+\n+\n+_NIST_ALG_ARC = univ.ObjectIdentifier('2.16.840.1.101.3.4')\n+_SHA3_ARC = _NIST_ALG_ARC + (2,)\n+\n+id_sha3_224 = _SHA3_ARC + (7,)\n+id_sha3_256 = _SHA3_ARC + (8,)\n+id_sha3_384 = _SHA3_ARC + (9,)\n+id_sha3_512 = _SHA3_ARC + (10,)\n+\n+\n+_ECDSA_WITH_SHA3_ARC = _NIST_ALG_ARC + (3,)\n+\n+id_ecdsa_with_sha3_256 = _ECDSA_WITH_SHA3_ARC + (10,)\n+id_ecdsa_with_sha3_384 = _ECDSA_WITH_SHA3_ARC + (11,)\n+id_ecdsa_with_sha3_512 = _ECDSA_WITH_SHA3_ARC + (12,)\n+\n+\n+_HASH_ALG_ID_TO_HASH_LENGTH = {\n+    o: l / 8 for o, l in (\n+        (rfc4055.id_sha224, 224),\n+        (rfc4055.id_sha256, 256),\n+        (rfc4055.id_sha384, 384),\n+        (rfc4055.id_sha512, 512),\n+        (id_sha3_224, 224),\n+        (id_sha3_256, 256),\n+        (id_sha3_384, 384),\n+        (id_sha3_512, 512),\n+    )\n+}\n+\n+\n+def _create_rsapss_params(hash_alg, encode_null_params):\n+    params = rfc4055.RSASSA_PSS_params()\n+    params['hashAlgorithm']['algorithm'] = hash_alg\n+\n+    if encode_null_params:\n+        params['hashAlgorithm']['parameters'] = univ.Null('')\n+\n+    params['maskGenAlgorithm']['algorithm'] = rfc4055.id_mgf1\n+\n+    mask_gen_alg_id = rfc5280.AlgorithmIdentifier()\n+    mask_gen_alg_id['algorithm'] = hash_alg\n+\n+    if encode_null_params:\n+        mask_gen_alg_id['parameters'] = univ.Null('')\n+\n+    params['maskGenAlgorithm']['parameters'] = mask_gen_alg_id\n+\n+    params['saltLength'] = _HASH_ALG_ID_TO_HASH_LENGTH[hash_alg]\n+\n+    return params\n+\n+\n+_SIGNATURE_ALG_ID_ENCODINGS = (\n+    [\n+        # PKCS v1.5\n+        _create_alg_id_der(o, univ.Null('')) for o in (\n+            rfc4055.sha256WithRSAEncryption,\n+            rfc4055.sha384WithRSAEncryption,\n+            rfc4055.sha512WithRSAEncryption,\n+        )\n+    ] + [\n+        # RSASSA-PSS without null parameters\n+        _create_alg_id_der(rfc4055.id_RSASSA_PSS, _create_rsapss_params(h, False)) for h in (\n+            rfc4055.id_sha224,\n+            rfc4055.id_sha256,\n+            rfc4055.id_sha384,\n+            rfc4055.id_sha512,\n+            id_sha3_224,\n+            id_sha3_256,\n+            id_sha3_384,\n+            id_sha3_512,\n+        )\n+    ] + [\n+        # RSASSA-PSS with null parameters\n+        _create_alg_id_der(rfc4055.id_RSASSA_PSS, _create_rsapss_params(h, True)) for h in (\n+            rfc4055.id_sha224,\n+            rfc4055.id_sha256,\n+            rfc4055.id_sha384,\n+            rfc4055.id_sha512,\n+            id_sha3_224,\n+            id_sha3_256,\n+            id_sha3_384,\n+            id_sha3_512,\n+        )\n+    ] + [\n+        # ECDSA\n+        _create_alg_id_der(o, None) for o in (\n+            rfc5480.ecdsa_with_SHA256,\n+            rfc5480.ecdsa_with_SHA384,\n+            rfc5480.ecdsa_with_SHA512,\n+            id_ecdsa_with_sha3_256,\n+            id_ecdsa_with_sha3_384,\n+            id_ecdsa_with_sha3_512,\n+        )\n+    ]\n+    # TODO: add support for Schnorr signatures\n+)\n+\n+\n+class AllowedSignatureAlgorithmValidator(algorithm.AllowedSignatureAlgorithmEncodingValidator):\n+    \"\"\"\n+    From EN 319 412 2:\n+\n+    GEN-4.2.2-1\n+\n+    Signature algorithm should be selected according to ETSI TS 119 312 [i.7].\n+\n+    NOTE: Cryptographic suites recommendations defined in ETSI TS 119 312 [i.7] can be superseded by national\n+    recommendations.\n+    \"\"\"\n+    VALIDATION_DISCOURAGED_SIGNATURE_ALGORITHM = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.NOTICE,\n+        'etsi.en_319_412_2.gen-4.2.2-1.discouraged_signature_algorithm'\n+    )\n+\n+    def __init__(self, path: str):\n+        super().__init__(\n+            validation=self.VALIDATION_DISCOURAGED_SIGNATURE_ALGORITHM,\n+            allowed_encodings=_SIGNATURE_ALG_ID_ENCODINGS,\n+            path=path\n+        )\ndiff --git a/pkilint/etsi/ts_119_495.py b/pkilint/etsi/ts_119_495.py\nnew file mode 100644\nindex 0000000..3464d3e\n--- /dev/null\n+++ b/pkilint/etsi/ts_119_495.py\n@@ -0,0 +1,194 @@\n+import re\n+\n+from pkilint import validation\n+from pkilint.etsi.asn1 import ts_119_495 as ts_119_495_asn1\n+from pyasn1_alt_modules import rfc3739\n+from iso3166 import countries_by_alpha2\n+import pkilint.oid\n+from pkilint.itu import x520_name\n+\n+\n+class RolesOfPspValidator(validation.Validator):\n+    \"\"\"\n+    GEN-5.2.2-1: RolesOfPSP shall contain one or more roles or contain a single entry indicating that the role is\n+    unspecified.\n+    GEN-5.2.2-2 If the certificate is issued for EU PSD2 the role object identifier shall be the\n+    appropriate one of the four OIDS defined in the ASN.1 snippet below; and\n+    GEN-5.2.2-3 If the certificate is issued for EU PSD2 the role name shall be  the appropriate one of the abbreviated\n+    names defined in clause 5.1 PSP_AS, PSP_PI, or PSP_IC.\n+    GEN-5.2.2-3A If the role is unspecified the role name shall be \"Unspecified\"\n+    GEN-5.2.2-5 The TSP shall ensure that the name in roleofPSPName is the one associated with the role object\n+    identifier held in roleofPSPOid.\n+    \"\"\"\n+    VALIDATION_PSP_ROLES_EMPTY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.2-1.roles_of_psp_empty'\n+    )\n+    VALIDATION_PSP_ROLES_INVALID = validation.ValidationFinding(validation.ValidationFindingSeverity.ERROR,\n+                                                                'etsi.ts_119_495.gen-5.2-2.invalid_psp_role')\n+\n+    VALIDATION_PSP_OIDS_INVALID = validation.ValidationFinding(validation.ValidationFindingSeverity.ERROR,\n+                                                               'etsi.ts_119_495.gen-5.2-2.invalid_psp_oid')\n+\n+    VALIDATION_PSP_ROLES_MISMATCH = validation.ValidationFinding(validation.ValidationFindingSeverity.ERROR,\n+                                                                 'etsi.ts_119_495.gen-5.2.2-5.psp_role_mismatch')\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_PSP_ROLES_EMPTY, self.VALIDATION_PSP_ROLES_INVALID,\n+                                      self.VALIDATION_PSP_ROLES_MISMATCH, self.VALIDATION_PSP_OIDS_INVALID],\n+                         pdu_class=ts_119_495_asn1.RolesOfPSP)\n+        self._expected_roles = {ts_119_495_asn1.id_psd2_role_psp_ai: 'PSP_AI',\n+                                ts_119_495_asn1.id_psd2_role_psp_as: 'PSP_AS',\n+                                ts_119_495_asn1.id_psd2_role_psp_ic: 'PSP_IC',\n+                                ts_119_495_asn1.id_psd2_role_psp_pi: 'PSP_PI',\n+                                ts_119_495_asn1.id_psd2_role_psp_unspecified: 'Unspecified'}\n+\n+    def validate(self, node):\n+        if not any(node.children):\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_PSP_ROLES_EMPTY)\n+\n+        for child in node.children.values():\n+            psp_oid = child.children['roleOfPspOid'].pdu\n+            role_psp = str(child.children['roleOfPspName'].pdu)\n+            expected_role = self._expected_roles.get(psp_oid)\n+\n+            if psp_oid not in self._expected_roles.keys():\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_PSP_OIDS_INVALID,\n+                    f'expected oid values are {pkilint.oid.format_oids(self._expected_roles.keys())} got {psp_oid}'\n+                )\n+            if role_psp not in self._expected_roles.values():\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_PSP_ROLES_INVALID,\n+                    f\"expected role values are [ {', '.join(map(str, self._expected_roles.values()))}]. Got {role_psp}\"\n+                )\n+            if role_psp != expected_role:\n+                raise validation.ValidationFindingEncountered(\n+                    self.VALIDATION_PSP_ROLES_MISMATCH,\n+                    f'Expected role is: {expected_role}. Role in cert is: {role_psp}. Oid in cert is: {psp_oid}'\n+                )\n+\n+\n+class PresenceofQCEUPDSStatementValidator(validation.Validator):\n+    \"\"\"GEN-5.1.1 The Open Banking Attributes shall be included in a QCSTatement within the qcStatements extension\n+    as specified in clause 3.2.5 of IETF RFC 3739.\"\"\"\n+    VALIDATION_QC_EU_PDS_MISSING = validation.ValidationFinding(validation.ValidationFindingSeverity.ERROR,\n+                                                                'etsi.ts_119_495.gen-5.1.1.qc_eu_pds_missing')\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_QC_EU_PDS_MISSING], pdu_class=rfc3739.QCStatements)\n+\n+    def validate(self, node):\n+        if ts_119_495_asn1.id_etsi_psd2_qcStatement not in node.document.qualified_statement_ids:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_QC_EU_PDS_MISSING)\n+\n+\n+class NCANameLatinCharactersValidator(validation.Validator):\n+    \"\"\"GEN-5.2.3-1: The NCAName shall be plain text using Latin alphabet provided by the Competent Authority itself\n+    for purpose of identification in certificates.\"\"\"\n+    VALIDATION_NCA_NAME_NON_LATIN = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.3-1.nca_name_non_latin'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_NCA_NAME_NON_LATIN], pdu_class=ts_119_495_asn1.NCAName)\n+\n+    def validate(self, node):\n+        nca_name = str(node.pdu)\n+\n+        if not nca_name.isascii():\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_NCA_NAME_NON_LATIN,\n+                                                          f'invalid NCA name: {nca_name}')\n+\n+\n+class NCAIdValidator(validation.Validator):\n+    \"\"\"GEN-5.2.3-2: Validator for NCAId structure.\n+    The NCAId shall contain information using the following structure in the presented order:\n+    \u2022 2 character ISO 3166-1 country code representing the Competent Authority country;\n+    \u2022 hyphen-minus \"-\" (0x2D (ASCII), U+002D (UTF-8)); and\n+    \u2022 2-8 character Competent Authority identifier without country code (A-Z uppercase only, no separator).\"\"\"\n+\n+    VALIDATION_INVALID_STRUCTURE = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.3-2.invalid_structure'\n+    )\n+    VALIDATION_INVALID_ISO_COUNTRY = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.3-2.invalid_iso_country'\n+    )\n+    VALIDATION_INVALID_CA_IDENTIFIER = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.3-2.invalid_ca_identifier'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(validations=[self.VALIDATION_INVALID_STRUCTURE,\n+                                      self.VALIDATION_INVALID_ISO_COUNTRY,\n+                                      self.VALIDATION_INVALID_CA_IDENTIFIER],\n+                         pdu_class=ts_119_495_asn1.NCAId)\n+\n+    def validate(self, node):\n+        nca_id = str(node.pdu)\n+\n+        if nca_id.count('-') != 1:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_INVALID_STRUCTURE,\n+                                                          f'Invalid separator in NCAId: {nca_id}')\n+\n+        iso_country_code, ca_identifier = nca_id.rsplit('-', 1)\n+\n+        if iso_country_code not in countries_by_alpha2:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_INVALID_ISO_COUNTRY,\n+                                                          f'Invalid ISO country code: {iso_country_code}')\n+\n+        if not (2 <= len(ca_identifier) <= 8 and ca_identifier.isalpha() and ca_identifier.isupper()):\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_INVALID_CA_IDENTIFIER,\n+                                                          f'Invalid Competent Authority identifier: {ca_identifier}')\n+\n+\n+class PsdOrganizationIdentifierFormatValidator(validation.Validator):\n+    \"\"\"\n+    GEN-5.2.1-3: If an Authorization Number was issued by a Competent Authority the subject organizationIdentifier\n+    attribute should contain the Authorization Number encoded using the following structure in the presented order:\n+\n+    \u2022 \"PSD\" as 3 character legal person identity type reference;\n+    \u2022 2 character ISO 3166-1 [8] country code representing the Competent Authority country;\n+    \u2022 hyphen-minus \"-\" (0x2D (ASCII), U+002D (UTF-8));\n+    \u2022 2-8 character Competent Authority identifier without country code (A-Z uppercase only, no separator);\n+    \u2022 hyphen-minus \"-\" (0x2D (ASCII), U+002D (UTF-8)); and\n+    \u2022 identifier (authorization number as specified by the Competent Authority. There are no restrictions on the\n+    characters used).\n+\n+    ...\n+\n+    GEN-5.3-3: The organizationIdentifier shall be present in the Subject's Distinguished Name and encoded with legal\n+    person syntax as specified in clause 5.2.1.\n+    \"\"\"\n+    VALIDATION_INVALID_PSD_ORGANIZATION_ID_FORMAT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'etsi.ts_119_495.gen-5.2.1-3.invalid_psd_organization_id_format'\n+    )\n+\n+    _PSD_ORGID_FORMAT_REGEX = re.compile('^PSD[A-Z]{2}-[A-Z]{2,8}-.+$')\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_INVALID_PSD_ORGANIZATION_ID_FORMAT],\n+            pdu_class=x520_name.X520OrganizationIdentifier\n+        )\n+\n+    def validate(self, node):\n+        try:\n+            _, decoded_value_node = node.child\n+        except ValueError:\n+            return\n+\n+        value_str = str(decoded_value_node.pdu)\n+\n+        m = self._PSD_ORGID_FORMAT_REGEX.match(value_str)\n+\n+        if m is None:\n+            raise validation.ValidationFindingEncountered(\n+                self.VALIDATION_INVALID_PSD_ORGANIZATION_ID_FORMAT,\n+                f'Invalid PSD organization identifier format: \"{value_str}\"'\n+            )\ndiff --git a/pkilint/itu/x520_name_unbounded.py b/pkilint/itu/x520_name_unbounded.py\nnew file mode 100644\nindex 0000000..a58f761\n--- /dev/null\n+++ b/pkilint/itu/x520_name_unbounded.py\n@@ -0,0 +1,46 @@\n+from pyasn1.type import namedtype, constraint, char\n+from pyasn1_alt_modules import rfc5280\n+\n+MAX = float('inf')\n+\n+\n+def _create_unbounded_directory_string_namedtypes():\n+    return namedtype.NamedTypes(\n+        namedtype.NamedType('teletexString',\n+                            char.TeletexString().subtype(\n+                                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n+        namedtype.NamedType('printableString',\n+                            char.PrintableString().subtype(\n+                                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n+        namedtype.NamedType('universalString',\n+                            char.UniversalString().subtype(\n+                                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n+        namedtype.NamedType('utf8String',\n+                            char.UTF8String().subtype(subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n+        namedtype.NamedType('bmpString',\n+                            char.BMPString().subtype(subtypeSpec=constraint.ValueSizeConstraint(1, MAX)))\n+    )\n+\n+\n+class X520OrganizationNameUnbounded(rfc5280.X520OrganizationName):\n+    componentType = _create_unbounded_directory_string_namedtypes()\n+\n+\n+class X520OrganizationalUnitNameUnbounded(rfc5280.X520OrganizationalUnitName):\n+    componentType = _create_unbounded_directory_string_namedtypes()\n+\n+\n+class X520CommonNameUnbounded(rfc5280.X520CommonName):\n+    componentType = _create_unbounded_directory_string_namedtypes()\n+\n+\n+class X520PseudonymUnbounded(rfc5280.X520Pseudonym):\n+    componentType = _create_unbounded_directory_string_namedtypes()\n+\n+\n+UNBOUNDED_ATTRIBUTE_TYPE_MAPPINGS = {\n+    rfc5280.id_at_organizationName: X520OrganizationNameUnbounded(),\n+    rfc5280.id_at_organizationalUnitName: X520OrganizationalUnitNameUnbounded(),\n+    rfc5280.id_at_commonName: X520CommonNameUnbounded(),\n+    rfc5280.id_at_pseudonym: X520PseudonymUnbounded(),\n+}\ndiff --git a/pkilint/pkix/certificate/__init__.py b/pkilint/pkix/certificate/__init__.py\nindex 35da6b2..f93754a 100644\n--- a/pkilint/pkix/certificate/__init__.py\n+++ b/pkilint/pkix/certificate/__init__.py\n@@ -70,6 +70,12 @@ def extended_key_usages(self) -> Set[univ.ObjectIdentifier]:\n \n         return {eku_node.pdu for eku_node in decoded.children.values()} if decoded else set()\n \n+    @functools.cached_property\n+    def qualified_statement_ids(self) -> Set[univ.ObjectIdentifier]:\n+        decoded = self._decode_and_append_extension(rfc3739.id_pe_qcStatements, rfc3739.QCStatements())\n+\n+        return {qs.children['statementId'].pdu for qs in decoded.children.values()} if decoded else set()\n+\n     @functools.cached_property\n     def cryptography_object(self):\n         return x509.load_der_x509_certificate(self.substrate)\n@@ -304,6 +310,11 @@ def create_extensions_validator_container(additional_validators=None):\n             certificate_extension.CtPrecertPoisonCriticalityValidator(),\n             certificate_extension.CtPrecertPoisonSctListMutuallyExclusiveExtensionsValidator(),\n             certificate_transparency.SctListElementCountValidator(),\n+            certificate_extension.PolicyConstraintsPresenceValidator(),\n+            certificate_extension.PolicyMappingsPresenceValidator(),\n+            certificate_extension.InhibitAnyPolicyPresenceValidator(),\n+            certificate_extension.ProhibitedQualifiedStatementValidator(),\n+            certificate_extension.IssuerAlternativeNameCriticalityValidator(),\n         ] + additional_validators,\n         path='certificate.tbsCertificate.extensions'\n     )\ndiff --git a/pkilint/pkix/certificate/certificate_extension.py b/pkilint/pkix/certificate/certificate_extension.py\nindex 5c0732e..96108e3 100644\n--- a/pkilint/pkix/certificate/certificate_extension.py\n+++ b/pkilint/pkix/certificate/certificate_extension.py\n@@ -2,7 +2,7 @@\n \n import unicodedata\n from pyasn1.type.univ import ObjectIdentifier\n-from pyasn1_alt_modules import rfc5280, rfc4262, rfc6962\n+from pyasn1_alt_modules import rfc5280, rfc4262, rfc6962, rfc3739\n \n from pkilint import validation, oid\n from pkilint.itu.bitstring import has_named_bit\n@@ -448,7 +448,7 @@ def validate(self, node):\n class CertificatePoliciesUserNoticeValidator(validation.Validator):\n     VALIDATION_NOTICEREF_PRESENT = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.WARNING,\n-        'pkix.certificate_policies_usernotice_has_noticeRef'\n+        'pkix.certificate_policies_usernotice_has_noticeref'\n     )\n \n     VALIDATION_EXPLICITTEXT_INVALID_ENCODING_5280 = validation.ValidationFinding(\n@@ -568,6 +568,19 @@ def validate(self, node):\n             )\n \n \n+# TODO: consider subclassing from StrEnum when minimum supported Python version is 3.11\n+class KeyUsageBitName:\n+    DIGITAL_SIGNATURE = 'digitalSignature'\n+    NON_REPUDIATION = 'nonRepudiation'\n+    KEY_ENCIPHERMENT = 'keyEncipherment'\n+    DATA_ENCIPHERMENT = 'dataEncipherment'\n+    KEY_AGREEMENT = 'keyAgreement'\n+    KEY_CERT_SIGN = 'keyCertSign'\n+    CRL_SIGN = 'cRLSign'\n+    ENCIPHER_ONLY = 'encipherOnly'\n+    DECIPHER_ONLY = 'decipherOnly'\n+\n+\n class KeyUsageValidator(validation.Validator):\n     VALIDATION_NO_BITS_SET = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\n@@ -613,7 +626,7 @@ def validate(self, node):\n \n         is_ca = node.document.is_ca\n \n-        has_keycertsign = has_named_bit(node, 'keyCertSign')\n+        has_keycertsign = has_named_bit(node, KeyUsageBitName.KEY_CERT_SIGN)\n \n         if is_ca and not has_keycertsign:\n             raise validation.ValidationFindingEncountered(\n@@ -624,8 +637,8 @@ def validate(self, node):\n                 self.VALIDATION_EE_KEYCERTSIGN_SET\n             )\n \n-        has_eo = has_named_bit(node, 'encipherOnly')\n-        has_do = has_named_bit(node, 'decipherOnly')\n+        has_eo = has_named_bit(node, KeyUsageBitName.ENCIPHER_ONLY)\n+        has_do = has_named_bit(node, KeyUsageBitName.DECIPHER_ONLY)\n \n         if has_eo and has_do:\n             raise validation.ValidationFindingEncountered(\n@@ -780,3 +793,94 @@ def validate(self, node):\n         if poison_ext is not None:\n             raise validation.ValidationFindingEncountered(\n                 self.VALIDATION_SIMULTANEOUS_PRECERT_POISON_SCTLIST_EXTENSIONS)\n+\n+\n+class PolicyConstraintsPresenceValidator(extension.ExtensionTypeMatchingValidator):\n+    VALIDATION_EE_POLICY_CONSTRAINTS_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'pkix.end_entity_policy_constraints_extension_present'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            extension_oid=rfc5280.id_ce_policyConstraints, validations=[self.VALIDATION_EE_POLICY_CONSTRAINTS_PRESENT]\n+         )\n+\n+    def validate(self, node):\n+        if not node.document.is_ca:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_EE_POLICY_CONSTRAINTS_PRESENT)\n+\n+\n+class InhibitAnyPolicyPresenceValidator(extension.ExtensionTypeMatchingValidator):\n+    VALIDATION_EE_INHIBIT_ANYPOLICY_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'pkix.end_entity_inhibit_anypolicy_extension_present'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            extension_oid=rfc5280.id_ce_inhibitAnyPolicy, validations=[self.VALIDATION_EE_INHIBIT_ANYPOLICY_PRESENT]\n+        )\n+\n+    def validate(self, node):\n+        if not node.document.is_ca:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_EE_INHIBIT_ANYPOLICY_PRESENT)\n+\n+\n+class PolicyMappingsPresenceValidator(extension.ExtensionTypeMatchingValidator):\n+    VALIDATION_EE_POLICY_MAPPINGS_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'pkix.end_entity_policy_mappings_extension_present'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            extension_oid=rfc5280.id_ce_policyMappings, validations=[self.VALIDATION_EE_POLICY_MAPPINGS_PRESENT]\n+        )\n+\n+    def validate(self, node):\n+        if not node.document.is_ca:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_EE_POLICY_MAPPINGS_PRESENT)\n+\n+\n+class ProhibitedQualifiedStatementValidator(validation.Validator):\n+    \"\"\"RFC 3739, section 3.2.6.1 says:\n+    The certificate statement (id-qcs-pkixQCSyntax-v1), identifies\n+    conformance with requirements defined in the obsoleted RFC 3039\n+    (Version 1).  This statement is thus provided for identification of\n+    old certificates issued in conformance with RFC 3039.  This statement\n+    MUST NOT be included in certificates issued in accordance with this\n+    profile.\n+    \"\"\"\n+    VALIDATION_PROHIBITED_QUALIFIED_STATEMENT_PRESENT = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.ERROR,\n+        'pkix.prohibited_qualified_statement_present'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            validations=[self.VALIDATION_PROHIBITED_QUALIFIED_STATEMENT_PRESENT],\n+            pdu_class=rfc3739.QCStatements\n+        )\n+\n+    def validate(self, node):\n+        if rfc3739.id_qcs_pkixQCSyntax_v1 in node.document.qualified_statement_ids:\n+            raise validation.ValidationFindingEncountered(self.VALIDATION_PROHIBITED_QUALIFIED_STATEMENT_PRESENT)\n+\n+\n+class IssuerAlternativeNameCriticalityValidator(ExtensionCriticalityValidator):\n+    \"\"\"RFC 5280, section 4.2.1.7 says:\n+\n+    Where present, conforming CAs SHOULD mark this extension as non-critical.\n+    \"\"\"\n+    VALIDATION_ISSUER_ALT_NAME_CRITICAL = validation.ValidationFinding(\n+        validation.ValidationFindingSeverity.WARNING,\n+        'pkix.issuer_alt_name_extension_critical'\n+    )\n+\n+    def __init__(self):\n+        super().__init__(\n+            type_oid=rfc5280.id_ce_issuerAltName,\n+            is_critical=False,\n+            validation=self.VALIDATION_ISSUER_ALT_NAME_CRITICAL\n+        )\ndiff --git a/pkilint/pkix/extension.py b/pkilint/pkix/extension.py\nindex 73008c5..92be4d4 100644\n--- a/pkilint/pkix/extension.py\n+++ b/pkilint/pkix/extension.py\n@@ -180,6 +180,14 @@ def validate(self, node):\n             raise validation.ValidationFindingEncountered(self._validation)\n \n \n+class ExtensionTypeMatchingValidator(validation.TypeMatchingValidator):\n+    def __init__(self, *, extension_oid, validations):\n+        super().__init__(\n+            type_path='extnID', type_oid=extension_oid, value_path='extnValue', validations=validations,\n+            pdu_class=rfc5280.Extension\n+        )\n+\n+\n class AuthorityKeyIdentifierValidator(validation.Validator):\n     VALIDATION_AKI_NO_KEY_ID = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\ndiff --git a/pkilint/pkix/general_name.py b/pkilint/pkix/general_name.py\nindex caad0e4..0bc1f36 100644\n--- a/pkilint/pkix/general_name.py\n+++ b/pkilint/pkix/general_name.py\n@@ -14,6 +14,19 @@\n OTHER_NAME_MAPPINGS = rfc5280.anotherNameMap.copy()\n \n \n+# TODO: consider subclassing from StrEnum when minimum supported Python version is 3.11\n+class GeneralNameTypeName:\n+    OTHER_NAME = 'otherName'\n+    RFC822_NAME = 'rfc822Name'\n+    DNS_NAME = 'dNSName'\n+    X400_ADDRESS = 'x400Address'\n+    DIRECTORY_NAME = 'directoryName'\n+    EDI_PARTY_NAME = 'ediPartyName'\n+    UNIFORM_RESOURCE_IDENTIFIER = 'uniformResourceIdentifier'\n+    IP_ADDRESS = 'iPAddress'\n+    REGISTERED_ID = 'registeredID'\n+\n+\n def validators_predicate(func, value):\n     ret = func(value)\n \ndiff --git a/pkilint/pkix/name.py b/pkilint/pkix/name.py\nindex 1f66f55..6eb2d4d 100644\n--- a/pkilint/pkix/name.py\n+++ b/pkilint/pkix/name.py\n@@ -1,3 +1,4 @@\n+import collections\n from typing import List, Set\n \n import validators\n@@ -25,6 +26,15 @@ def get_name_attributes_by_type(name_node, type_oid):\n     return atvs\n \n \n+def get_name_attribute_counts(name_node):\n+    counts = collections.Counter()\n+\n+    for rdn_idx, rdn in name_node.children['rdnSequence'].children.items():\n+        counts.update((atv.children['type'].pdu for atv in rdn.children.values()))\n+\n+    return counts\n+\n+\n class EmptyNameValidator(validation.Validator):\n     VALIDATION_NAME_IS_EMPTY = validation.ValidationFinding(\n         validation.ValidationFindingSeverity.ERROR,\ndiff --git a/pkilint/rest/__init__.py b/pkilint/rest/__init__.py\nindex b992f43..e90bbb4 100644\n--- a/pkilint/rest/__init__.py\n+++ b/pkilint/rest/__init__.py\n@@ -3,11 +3,11 @@\n \n from fastapi import FastAPI, HTTPException\n \n-from pkilint.rest import cabf_serverauth, cabf_smime, ocsp\n+from pkilint.rest import cabf_serverauth, cabf_smime, etsi, ocsp\n from pkilint.rest import model\n \n _PKILINT_VERSION = version('pkilint')\n-_API_VERSION = 'v1.2'\n+_API_VERSION = 'v1.3'\n \n app = FastAPI(\n     title='pkilint API',\n@@ -16,9 +16,13 @@\n )\n \n _CERTIFICATE_LINTER_GROUPS = [\n-    cabf_smime.create_linter_group_instance(),\n-    cabf_serverauth.create_linter_group_instance(),\n+    m.create_linter_group_instance() for m in (\n+        cabf_smime,\n+        cabf_serverauth,\n+        etsi,\n+    )\n ]\n+\n _OCSP_PKIX_LINTER = ocsp.create_ocsp_response_linter()\n \n \ndiff --git a/pkilint/rest/etsi.py b/pkilint/rest/etsi.py\nnew file mode 100644\nindex 0000000..0a5eecc\n--- /dev/null\n+++ b/pkilint/rest/etsi.py\n@@ -0,0 +1,42 @@\n+from fastapi import HTTPException\n+from pyasn1.error import PyAsn1Error\n+from starlette import status\n+\n+from pkilint import etsi\n+from pkilint.pkix import certificate\n+from pkilint.rest import model\n+\n+\n+class EtsiLinterGroup(model.LinterGroup):\n+    def __init__(self, linters):\n+        super().__init__(name='etsi', linters=linters)\n+\n+    def determine_linter(self, doc):\n+        try:\n+            cert_type = etsi.determine_certificate_type(doc)\n+        except (ValueError, PyAsn1Error) as e:\n+            message = f'Parsing error occurred: {e}'\n+\n+            raise HTTPException(\n+                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+                detail=model.create_unprocessable_entity_error_detail(message)\n+            )\n+\n+        # this doesn't fail, so we don't need to guard against not being able to determine the certificate type\n+        return next((l for l in self.linters if l.name.casefold() == cert_type.to_option_str.casefold()))\n+\n+\n+def create_linter_group_instance():\n+    return EtsiLinterGroup(\n+        [\n+            model.Linter(\n+                validator=certificate.create_pkix_certificate_validator_container(\n+                    etsi.create_decoding_validators(cert_type),\n+                    etsi.create_validators(cert_type)\n+                ),\n+                finding_filters=etsi.create_etsi_finding_filters(cert_type),\n+                name=cert_type.to_option_str\n+            )\n+            for cert_type in etsi.CertificateType\n+        ]\n+    )\ndiff --git a/setup.cfg b/setup.cfg\nindex 2e62033..2529679 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -27,13 +27,15 @@ packages = find:\n python_requires = >=3.9\n install_requires =\n     pyasn1\n-    pyasn1-alt-modules\n+    pyasn1-alt-modules >=0.4.3\n     cryptography >=39\n     iso3166\n     # version is pinned due to https://github.com/python-validators/validators/issues/346\n     validators==0.22\n     python-dateutil\n     publicsuffixlist\n+    iso4217\n+    python-iso639 \n \n [options.extras_require]\n rest =\n@@ -51,3 +53,4 @@ console_scripts =\n     lint_pkix_signer_signee_cert_chain = pkilint.bin.lint_pkix_signer_signee_cert_chain:main\n     lint_cabf_smime_cert = pkilint.bin.lint_cabf_smime_cert:main\n     lint_ocsp_response = pkilint.bin.lint_ocsp_response:main\n+    lint_etsi_cert = pkilint.bin.lint_etsi_cert:main\n", "instance_id": "digicert__pkilint-82", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear, providing a high-level overview of the tasks to be accomplished, such as adding support for linting ETSI website authentication certificates, integrating an optional DER decoder (pyasn1-fasder), and relaxing the format for ISO 3166-2 state/province identifiers. It specifies the need for both a CLI tool and REST API support, along with documentation updates. However, there are minor ambiguities and missing details. For instance, while it mentions specific standards (ETSI EN 319 412 and TS 119 495), it lacks detailed input/output formats or specific validation rules for the linting process. Additionally, edge cases and specific error handling requirements are not explicitly defined in the problem statement, though some are implied in the code changes. The inclusion of example command execution in the README helps, but comprehensive test cases or detailed constraints are missing.", "difficulty_explanation": "The difficulty score of 0.75 reflects a hard problem that requires a deep understanding of the codebase and involves complex modifications across multiple areas. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are extensive, spanning multiple files and modules (e.g., new CLI tool in `lint_etsi_cert.py`, updates to `README.md`, `CHANGELOG.md`, and various validation logic in `cabf_name.py`, `serverauth/*`, and `etsi/*`). The changes impact the system's architecture by introducing new linting capabilities for ETSI certificates, integrating an optional DER decoder, and modifying existing validation logic for organization identifiers. This requires understanding interactions between different parts of the codebase, such as certificate parsing, validation frameworks, and integration with existing CA/B Forum standards.\n\n2. **Number of Technical Concepts**: Solving this problem demands familiarity with several advanced concepts, including:\n   - **ASN.1 and DER Encoding/Decoding**: Knowledge of certificate structures and the optional use of `pyasn1-fasder` for performance improvements.\n   - **ETSI Standards (EN 319 412, TS 119 495)**: Domain-specific knowledge of European certificate profiles and PSD2 requirements.\n   - **Validation Logic and Linting**: Understanding and extending the existing validation framework to support new certificate types and relaxed constraints (e.g., state/province format changes).\n   - **CLI and REST API Development**: Implementing user interfaces for linting via command-line tools and API endpoints.\n   - **Python Libraries and Standards**: Familiarity with `pyasn1`, `cryptography`, and ISO standards (e.g., ISO 3166-2, ISO 4217).\n   These concepts are moderately to highly complex, especially given the need to align with regulatory standards and ensure correctness in certificate validation.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and code changes imply several edge cases, such as detecting certificate types automatically versus explicit specification, handling the optional `pyasn1-fasder` decoder (with potential memory/resource leak concerns mentioned), and managing superseded validation findings with the `--report-all` option. The code introduces logic to filter findings based on certificate type and standards precedence (e.g., ETSI overriding CA/B Forum requirements), which adds complexity to error handling. While not all edge cases are explicitly detailed, the implementation suggests a need for robust handling of various certificate profiles and configurations.\n\n4. **Overall Impact and Expertise Required**: The task requires significant expertise in certificate validation, standards compliance, and integration into an existing complex codebase. The modifications are not just additive but also involve refactoring existing logic (e.g., organization identifier validation) to accommodate new requirements. Performance considerations with the optional decoder and the need to ensure no regressions in existing functionality further elevate the difficulty. This is not a trivial feature addition but a substantial enhancement that impacts core functionality and requires careful testing and validation.\n\nGiven these factors, a score of 0.75 places this task in the 'Hard' category, as it demands deep knowledge of the domain and codebase, involves complex changes across multiple components, and requires careful handling of edge cases and standards compliance, though it does not reach the extreme complexity of system-level redesign or novel algorithm development that would warrant a score closer to 1.0.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove the deprecated reciprocal lattice point class\n#### Description of the change\r\nRemoves the deprecated `diffsims.crystallography.ReciprocalLatticePoint` class, which was deprecated in June 2022. Closes #189.\r\n\r\nThree methods in the public API for generating sets of {hkl} were not deprecated. These have improved replacements in the \"new\" `ReciprocalLatticeVector` class:\r\n* `get_hkl()` -> `from_highest_hkl()`\r\n* `get_highest_hkl()` -> `from_min_dspacing()`\r\n* `get_equivalent_hkl()` -> `symmetrise()`\r\n\r\nThese functions should be deprecated and the user \"new\" methods. This can be done with the deprecation tools from #199, which should be merged before this PR.\r\n\r\n#### Progress of the PR\r\n- [x] [Docstrings for all functions](https://github.com/numpy/numpy/blob/master/doc/example.py)\r\n- [x] Unit tests with pytest for all lines\r\n- [x] Clean code style by [running black](https://diffsims.readthedocs.io/en/latest/contributing.html#get-the-style-right)\r\n- [x] Deprecate old methods to get sets of reciprocal lattice vectors\r\n\r\n#### For reviewers\r\n<!-- Don't remove the checklist below. -->\r\n- [ ] The PR title is short, concise, and will make sense 1 year later.\r\n- [ ] New functions are imported in corresponding `__init__.py`.\r\n- [ ] New features, API changes, and deprecations are mentioned in the\r\n      unreleased section in `CHANGELOG.rst`.\r\n- [ ] Contributor(s) are listed correctly in `credits` in `diffsims/release_info.py` and\r\n      in `.zenodo.json`.\n", "patch": "diff --git a/CHANGELOG.rst b/CHANGELOG.rst\nindex 11e21eca..e27a0964 100644\n--- a/CHANGELOG.rst\n+++ b/CHANGELOG.rst\n@@ -28,6 +28,8 @@ Removed\n -------\n - Removed support for Python 3.6 and Python 3.7, leaving 3.8 as the oldest supported\n   version.\n+- ``ReciprocalLatticePoint`` class; Use the ``ReciprocalLatticeVector`` class instead,\n+  which is an improved replacement.\n \n Fixed\n -----\ndiff --git a/CONTRIBUTING.rst b/CONTRIBUTING.rst\nindex 8ad2eb98..30f45633 100644\n--- a/CONTRIBUTING.rst\n+++ b/CONTRIBUTING.rst\n@@ -12,6 +12,7 @@ We have a `Code of Conduct\n <https://github.com/pyxem/diffsims/blob/master/.github/CODE_OF_CONDUCT.md>`_ that must\n be honoured by contributors.\n \n+\n Start using diffsims\n ====================\n \n@@ -42,6 +43,7 @@ install of diffsims!\n PS: If you choose to develop in Windows/Mac you may find the `Github Desktop\n <https://desktop.github.com>`_ useful.\n \n+\n Questions?\n ==========\n \n@@ -56,6 +58,7 @@ scary but it ensures that issues are identified and logged until dealt with. Thi\n also a good place to make a proposal for some new feature or tool that you want to work\n on.\n \n+\n Good coding practice\n ====================\n \n@@ -89,7 +92,6 @@ your newly added and modified files prior to each PR.\n If this doesn't work for you, you can also use the Pre-commit CI to reformat your code\n on github by commenting \"pre-commit autofix\" on your PR.\n \n-\n Run and write tests\n -------------------\n \n@@ -128,7 +130,6 @@ Useful hints on testing:\n   error-prone. See `pytest documentation for more details\n   <https://doc.pytest.org/en/latest/how-to/parametrize.html>`_.\n \n-\n Deprecations\n ------------\n We attempt to adhere to semantic versioning as best we can. This means that as little,\n@@ -138,16 +139,20 @@ so that users get a heads-up one (minor) release before something is removed or\n with a possible alternative to be used.\n \n \n-A deprecation decorator should be placed right above the object signature to be deprecated.\n+A deprecation decorator should be placed right above the object signature to be deprecated::\n \n-.. code-block:: python\n     from diffsims.utils._deprecated import deprecated\n+\n     @deprecated(since=0.8, removal=0.9, alternative=\"bar\")\n-    def foo(self, n):\n-        return n + 1\n-    @property\n-    @deprecated(since=0.9, removal=0.10, alternative=\"another\", is_function=True)\n+    def foo(self, n): ...\n \n+    @property\n+    @deprecated(\n+        since=\"0.9\",\n+        removal=\"0.10\",\n+        alternative=\"another\",\n+        alternative_is_function=True\n+    ): ...\n \n Build and write documentation\n -----------------------------\n@@ -173,6 +178,7 @@ in the `reStructuredText (reST)\n plaintext markup language. They should be accessible in the browser by typing\n ``file:///your-absolute/path/to/diffsims/doc/build/html/index.html`` in the address bar.\n \n+\n Continuous integration (CI)\n ===========================\n \n@@ -181,6 +187,7 @@ diffsims can be installed on Windows, macOS and Linux. After a successful instal\n the CI server runs the tests. After the tests return no errors, code coverage is\n reported to `Coveralls <https://coveralls.io/github/pyxem/diffsims?branch=master>`_.\n \n+\n Learn more\n ==========\n \ndiff --git a/MANIFEST.in b/MANIFEST.in\nindex c7068aa3..2153cf7e 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -1,3 +1,7 @@\n+# Info on valid syntax for controlling files in the distribution in this file:\n+# * https://setuptools.pypa.io/en/latest/userguide/miscellaneous.html\n+# * https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n+\n include CHANGELOG.rst\n include CONTRIBUTING.rst\n include LICENSE\n@@ -6,5 +10,6 @@ include README.rst\n include readthedocs.yaml\n include setup.cfg\n include setup.py\n+include diffsims/tests/**/*.npy\n \n-recursive-include doc Makefile make.bat *.rst *.py *.png\n+recursive-include doc Makefile make.bat *.rst *.py *.png\n\\ No newline at end of file\ndiff --git a/diffsims/crystallography/__init__.py b/diffsims/crystallography/__init__.py\nindex 736a8e3d..564ac510 100644\n--- a/diffsims/crystallography/__init__.py\n+++ b/diffsims/crystallography/__init__.py\n@@ -20,8 +20,7 @@\n g, hkl) for a crystal structure.\n \"\"\"\n \n-from diffsims.crystallography.reciprocal_lattice_point import (\n-    ReciprocalLatticePoint,\n+from diffsims.crystallography.get_hkl import (\n     get_equivalent_hkl,\n     get_highest_hkl,\n     get_hkl,\n@@ -32,6 +31,5 @@\n     \"get_equivalent_hkl\",\n     \"get_highest_hkl\",\n     \"get_hkl\",\n-    \"ReciprocalLatticePoint\",\n     \"ReciprocalLatticeVector\",\n ]\ndiff --git a/diffsims/crystallography/get_hkl.py b/diffsims/crystallography/get_hkl.py\nnew file mode 100644\nindex 00000000..8d54135c\n--- /dev/null\n+++ b/diffsims/crystallography/get_hkl.py\n@@ -0,0 +1,132 @@\n+# -*- coding: utf-8 -*-\n+# Copyright 2017-2023 The diffsims developers\n+#\n+# This file is part of diffsims.\n+#\n+# diffsims is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# diffsims is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with diffsims.  If not, see <http://www.gnu.org/licenses/>.\n+\n+from itertools import product\n+\n+import numpy as np\n+from orix.vector import Vector3d\n+\n+from diffsims.utils._deprecated import deprecated\n+\n+\n+@deprecated(\n+    since=\"0.6\",\n+    alternative=\"diffsims.crystallography.ReciprocalLatticeVector.from_min_dspacing\",\n+    removal=\"0.7\",\n+)\n+def get_highest_hkl(lattice, min_dspacing=0.5):\n+    \"\"\"Return the highest Miller indices hkl of the plane with a direct\n+    space interplanar spacing (d-spacing) greater than but closest to\n+    *min_dspacing*.\n+\n+    Parameters\n+    ----------\n+    lattice : diffpy.structure.Lattice\n+        Crystal lattice.\n+    min_dspacing : float, optional\n+        Smallest interplanar spacing to consider. Default is 0.5 \u00c5.\n+\n+    Returns\n+    -------\n+    highest_hkl : np.ndarray\n+        Highest Miller indices.\n+    \"\"\"\n+    highest_hkl = np.ones(3, dtype=int)\n+    for i in range(3):\n+        hkl = np.zeros(3)\n+        d = min_dspacing + 1\n+        while d > min_dspacing:\n+            hkl[i] += 1\n+            d = 1 / lattice.rnorm(hkl)\n+        highest_hkl[i] = hkl[i]\n+    return highest_hkl\n+\n+\n+@deprecated(\n+    since=\"0.6\",\n+    alternative=\"diffsims.crystallography.ReciprocalLatticeVector.from_highest_hkl\",\n+    removal=\"0.7\",\n+)\n+def get_hkl(highest_hkl):\n+    \"\"\"Return a list of planes from a set of highest Miller indices.\n+\n+    Parameters\n+    ----------\n+    highest_hkl : orix.vector.Vector3d, np.ndarray, list, or tuple of int\n+        Highest Miller indices to consider.\n+\n+    Returns\n+    -------\n+    hkl : np.ndarray\n+        An array of Miller indices.\n+    \"\"\"\n+    index_ranges = [np.arange(-i, i + 1) for i in highest_hkl]\n+    return np.asarray(list(product(*index_ranges)))\n+\n+\n+@deprecated(\n+    since=\"0.6\",\n+    alternative=\"diffsims.crystallography.ReciprocalLatticeVector.symmetrise\",\n+    removal=\"0.7\",\n+)\n+def get_equivalent_hkl(hkl, operations, unique=False, return_multiplicity=False):\n+    \"\"\"Return symmetrically equivalent Miller indices.\n+\n+    Parameters\n+    ----------\n+    hkl : orix.vector.Vector3d, np.ndarray, list or tuple of int\n+        Miller indices.\n+    operations : orix.quaternion.symmetry.Symmetry\n+        Point group describing allowed symmetry operations.\n+    unique : bool, optional\n+        Whether to return only unique Miller indices. Default is False.\n+    return_multiplicity : bool, optional\n+        Whether to return the multiplicity of the input indices. Default\n+        is False.\n+\n+    Returns\n+    -------\n+    new_hkl : orix.vector.Vector3d\n+        The symmetrically equivalent Miller indices.\n+    multiplicity : np.ndarray\n+        Number of symmetrically equivalent indices. Only returned if\n+        `return_multiplicity` is True.\n+    \"\"\"\n+    new_hkl = operations.outer(Vector3d(hkl))\n+    new_hkl = new_hkl.flatten().reshape(*new_hkl.shape[::-1])\n+\n+    multiplicity = None\n+    if unique:\n+        n_families = new_hkl.shape[0]\n+        multiplicity = np.zeros(n_families, dtype=int)\n+        temp_hkl = new_hkl[0].unique().data\n+        multiplicity[0] = temp_hkl.shape[0]\n+        if n_families > 1:\n+            for i, hkl in enumerate(new_hkl[1:]):\n+                temp_hkl2 = hkl.unique()\n+                multiplicity[i + 1] = temp_hkl2.size\n+                temp_hkl = np.append(temp_hkl, temp_hkl2.data, axis=0)\n+        new_hkl = Vector3d(temp_hkl[: multiplicity.sum()])\n+\n+    # Remove 1-dimensions\n+    new_hkl = new_hkl.squeeze()\n+\n+    if unique and return_multiplicity:\n+        return new_hkl, multiplicity\n+    else:\n+        return new_hkl\ndiff --git a/diffsims/crystallography/reciprocal_lattice_point.py b/diffsims/crystallography/reciprocal_lattice_point.py\ndeleted file mode 100644\nindex c9a9f658..00000000\n--- a/diffsims/crystallography/reciprocal_lattice_point.py\n+++ /dev/null\n@@ -1,484 +0,0 @@\n-# -*- coding: utf-8 -*-\n-# Copyright 2017-2023 The diffsims developers\n-#\n-# This file is part of diffsims.\n-#\n-# diffsims is free software: you can redistribute it and/or modify\n-# it under the terms of the GNU General Public License as published by\n-# the Free Software Foundation, either version 3 of the License, or\n-# (at your option) any later version.\n-#\n-# diffsims is distributed in the hope that it will be useful,\n-# but WITHOUT ANY WARRANTY; without even the implied warranty of\n-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-# GNU General Public License for more details.\n-#\n-# You should have received a copy of the GNU General Public License\n-# along with diffsims.  If not, see <http://www.gnu.org/licenses/>.\n-\n-from collections import defaultdict\n-from itertools import product\n-from warnings import warn\n-\n-import numpy as np\n-from orix.vector import Vector3d\n-\n-from diffsims.structure_factor.structure_factor import (\n-    get_kinematical_structure_factor,\n-    get_doyleturner_structure_factor,\n-    get_refraction_corrected_wavelength,\n-)\n-\n-_FLOAT_EPS = np.finfo(float).eps  # Used to round values below 1e-16 to zero\n-\n-\n-class ReciprocalLatticePoint:\n-    \"\"\"*[Deprecated]* Reciprocal lattice point (or crystal plane,\n-    reflector, g, etc.) with Miller indices, length of the reciprocal\n-    lattice vectors and other relevant structure_factor parameters.\n-\n-    Notes\n-    -----\n-    Deprecated since version 0.5: Class ``ReciprocalLatticePoint`` is\n-    deprecated and will be removed in version 0.6. Use\n-    :class:`~diffsims.crystallography.ReciprocalLatticeVector` instead.\n-    \"\"\"\n-\n-    def __init__(self, phase, hkl):\n-        \"\"\"A container for Miller indices, structure factors and related\n-        parameters for crystal planes (reciprocal lattice points,\n-        reflectors, g, etc.).\n-\n-        Parameters\n-        ----------\n-        phase : orix.crystal_map.phase_list.Phase\n-            A phase container with a crystal structure and a space and\n-            point group describing the allowed symmetry operations.\n-        hkl : orix.vector.Vector3d, np.ndarray, list, or tuple\n-            Miller indices.\n-\n-        \"\"\"\n-\n-        warn(\n-            message=(\n-                \"Class `ReciprocalLatticePoint` is deprecated and will be removed in \"\n-                \"version 0.6. Use `ReciprocalLatticeVector` instead.\"\n-            ),\n-            category=np.VisibleDeprecationWarning,\n-        )\n-\n-        self.phase = phase\n-        self._raise_if_no_point_group()\n-        self._hkl = Vector3d(hkl)\n-        self._structure_factor = [None] * self.size\n-        self._theta = [None] * self.size\n-\n-    def __repr__(self):\n-        return (\n-            f\"{self.__class__.__name__} {self.hkl.shape}\\n\"\n-            f\"Phase: {self.phase.name} ({self.phase.point_group.name})\\n\"\n-            f\"{np.array_str(self.hkl.data, precision=4, suppress_small=True)}\"\n-        )\n-\n-    def __getitem__(self, key):\n-        new_rlp = self.__class__(self.phase, self.hkl[key])\n-        if self.structure_factor[0] is None:\n-            new_rlp._structure_factor = [None] * new_rlp.size\n-        else:\n-            new_rlp._structure_factor = self.structure_factor[key]\n-        if self.theta[0] is None:\n-            new_rlp._theta = [None] * new_rlp.size\n-        else:\n-            new_rlp._theta = self.theta[key]\n-        return new_rlp\n-\n-    @property\n-    def hkl(self):\n-        \"\"\"Return :class:`~orix.vector.Vector3d` of Miller indices.\"\"\"\n-        return Vector3d(self._hkl.data.astype(int))\n-\n-    @property\n-    def h(self):\n-        \"\"\"Return :class:`np.ndarray` of Miller index h.\"\"\"\n-        return self.hkl.data[..., 0]\n-\n-    @property\n-    def k(self):\n-        \"\"\"Return :class:`np.ndarray` of Miller index k.\"\"\"\n-        return self.hkl.data[..., 1]\n-\n-    @property\n-    def l(self):\n-        \"\"\"Return :class:`np.ndarray` of Miller index l.\"\"\"\n-        return self.hkl.data[..., 2]\n-\n-    @property\n-    def size(self):\n-        \"\"\"Return `int`.\"\"\"\n-        return self.hkl.size\n-\n-    @property\n-    def shape(self):\n-        \"\"\"Return `tuple`.\"\"\"\n-        return self.hkl.data.shape\n-\n-    @property\n-    def multiplicity(self):\n-        \"\"\"Return either `int` or :class:`np.ndarray` of `int`.\"\"\"\n-        return self.symmetrise(antipodal=True, return_multiplicity=True)[1]\n-\n-    @property\n-    def gspacing(self):\n-        \"\"\"Return :class:`np.ndarray` of reciprocal lattice point\n-        spacings.\n-        \"\"\"\n-        return self.phase.structure.lattice.rnorm(self.hkl.data)\n-\n-    @property\n-    def dspacing(self):\n-        \"\"\"Return :class:`np.ndarray` of direct lattice interplanar\n-        spacings.\n-        \"\"\"\n-        return 1 / self.gspacing\n-\n-    @property\n-    def scattering_parameter(self):\n-        \"\"\"Return :class:`np.ndarray` of scattering parameters s.\"\"\"\n-        return 0.5 * self.gspacing\n-\n-    @property\n-    def structure_factor(self):\n-        \"\"\"Return :class:`np.ndarray` of structure factors F or None.\"\"\"\n-        return self._structure_factor\n-\n-    @property\n-    def allowed(self):\n-        \"\"\"Return whether planes diffract according to structure_factor\n-        selection rules assuming kinematical scattering theory.\n-        \"\"\"\n-        self._raise_if_no_space_group()\n-\n-        # Translational symmetry\n-        centering = self.phase.space_group.short_name[0]\n-\n-        if centering == \"P\":  # Primitive\n-            if self.phase.space_group.crystal_system == \"HEXAGONAL\":\n-                # TODO: See rules in e.g.\n-                #  https://mcl1.ncifcrf.gov/dauter_pubs/284.pdf, Table 4\n-                #  http://xrayweb.chem.ou.edu/notes/symmetry.html, Systematic Absences\n-                raise NotImplementedError\n-            else:  # Any hkl\n-                return np.ones(self.size, dtype=bool)\n-        elif centering == \"F\":  # Face-centred, hkl all odd/even\n-            selection = np.sum(np.mod(self.hkl.data, 2), axis=1)\n-            return np.array([i not in [1, 2] for i in selection], dtype=bool)\n-        elif centering == \"I\":  # Body-centred, h + k + l = 2n (even)\n-            return np.mod(np.sum(self.hkl.data, axis=1), 2) == 0\n-        elif centering == \"A\":  # Centred on A faces only\n-            return np.mod(self.k + self.l, 2) == 0\n-        elif centering == \"B\":  # Centred on B faces only\n-            return np.mod(self.h + self.l, 2) == 0\n-        elif centering == \"C\":  # Centred on C faces only\n-            return np.mod(self.h + self.k, 2) == 0\n-        elif centering in [\"R\", \"H\"]:  # Rhombohedral\n-            return np.mod(-self.h + self.k + self.l, 3) == 0\n-\n-    @property\n-    def theta(self):\n-        \"\"\"Return :class:`np.ndarray` of twice the Bragg angle.\"\"\"\n-        return self._theta\n-\n-    @classmethod\n-    def from_min_dspacing(cls, phase, min_dspacing=0.5):\n-        \"\"\"Create a CrystalPlane object populated by unique Miller indices\n-        with a direct space interplanar spacing greater than a lower\n-        threshold.\n-\n-        Parameters\n-        ----------\n-        phase : orix.crystal_map.phase_list.Phase\n-            A phase container with a crystal structure and a space and\n-            point group describing the allowed symmetry operations.\n-        min_dspacing : float, optional\n-            Smallest interplanar spacing to consider. Default is 0.5 \u00c5.\n-        \"\"\"\n-        highest_hkl = get_highest_hkl(\n-            lattice=phase.structure.lattice, min_dspacing=min_dspacing\n-        )\n-        hkl = get_hkl(highest_hkl=highest_hkl)\n-        return cls(phase=phase, hkl=hkl).unique()\n-\n-    @classmethod\n-    def from_highest_hkl(cls, phase, highest_hkl):\n-        \"\"\"Create a CrystalPlane object populated by unique Miller indices\n-        below, but including, a set of higher indices.\n-\n-        Parameters\n-        ----------\n-        phase : orix.crystal_map.phase_list.Phase\n-            A phase container with a crystal structure and a space and\n-            point group describing the allowed symmetry operations.\n-        highest_hkl : np.ndarray, list, or tuple of int\n-            Highest Miller indices to consider (including).\n-        \"\"\"\n-        hkl = get_hkl(highest_hkl=highest_hkl)\n-        return cls(phase=phase, hkl=hkl).unique()\n-\n-    def unique(self, use_symmetry=True):\n-        \"\"\"Return planes with unique Miller indices.\n-\n-        Parameters\n-        ----------\n-        use_symmetry : bool, optional\n-            Whether to use symmetry to remove the planes with indices\n-            symmetrically equivalent to another set of indices.\n-\n-        Returns\n-        -------\n-        ReciprocalLatticePoint\n-        \"\"\"\n-        if use_symmetry:\n-            all_hkl = self.hkl.data\n-            # Remove [0, 0, 0] points\n-            all_hkl = all_hkl[~np.all(np.isclose(all_hkl, 0), axis=1)]\n-\n-            families = defaultdict(list)\n-            for this_hkl in all_hkl.tolist():\n-                for that_hkl in families.keys():\n-                    if _is_equivalent(this_hkl, that_hkl):\n-                        families[tuple(that_hkl)].append(this_hkl)\n-                        break\n-                else:\n-                    families[tuple(this_hkl)].append(this_hkl)\n-\n-            n_families = len(families)\n-            unique_hkl = np.zeros((n_families, 3))\n-            for i, all_hkl_in_family in enumerate(families.values()):\n-                unique_hkl[i] = sorted(all_hkl_in_family)[-1]\n-        else:\n-            unique_hkl = self.hkl.unique()\n-        # TODO: Enable inheriting classes pass on their properties in this new object\n-        return self.__class__(phase=self.phase, hkl=unique_hkl)\n-\n-    def symmetrise(\n-        self,\n-        antipodal=True,\n-        unique=True,\n-        return_multiplicity=False,\n-    ):\n-        \"\"\"Return planes with symmetrically equivalent Miller indices.\n-\n-        Parameters\n-        ----------\n-        antipodal : bool, optional\n-            Whether to include antipodal symmetry operations. Default is\n-            True.\n-        unique : bool, optional\n-            Whether to return only distinct indices. Default is True.\n-            If True, zero-entries, which are assumed to be degenerate, are\n-            removed.\n-        return_multiplicity : bool, optional\n-            Whether to return the multiplicity of indices. This option is\n-            only available if `unique` is True. Default is False.\n-\n-        Returns\n-        -------\n-        ReciprocalLatticePoint\n-            Planes with Miller indices symmetrically equivalent to the\n-            original planes.\n-        multiplicity : np.ndarray\n-            Multiplicity of the original Miller indices. Only returned if\n-            `return_multiplicity` is True.\n-\n-        Notes\n-        -----\n-        Should be the same as EMsoft's CalcFamily in their symmetry.f90\n-        module, although not entirely sure. Use with care.\n-        \"\"\"\n-        # Get symmetry operations\n-        pg = self.phase.point_group\n-        operations = pg if antipodal else pg[~pg.improper]\n-\n-        out = get_equivalent_hkl(\n-            hkl=self.hkl,\n-            operations=operations,\n-            unique=unique,\n-            return_multiplicity=return_multiplicity,\n-        )\n-\n-        # TODO: Enable inheriting classes pass on their properties in this new object\n-        # Format output and return\n-        if unique and return_multiplicity:\n-            multiplicity = out[1]\n-            if multiplicity.size == 1:\n-                multiplicity = multiplicity[0]\n-            return self.__class__(phase=self.phase, hkl=out[0]), multiplicity\n-        else:\n-            return self.__class__(phase=self.phase, hkl=out)\n-\n-    def calculate_structure_factor(self, method=None, voltage=None):\n-        \"\"\"Populate `self.structure_factor` with the structure factor F\n-        for each plane.\n-\n-        Parameters\n-        ----------\n-        method : str, optional\n-            Either \"kinematical\" for kinematical X-ray structure factors\n-            or \"doyleturner\" for structure factors using Doyle-Turner\n-            atomic scattering factors. If None (default), kinematical\n-            structure factors are calculated.\n-        voltage : float, optional\n-            Beam energy in V used when `method=doyleturner`.\n-        \"\"\"\n-        if method is None:\n-            method = \"kinematical\"\n-        methods = [\"kinematical\", \"doyleturner\"]\n-        if method not in methods:\n-            raise ValueError(f\"method={method} must be among {methods}\")\n-        elif method == \"doyleturner\" and voltage is None:\n-            raise ValueError(\n-                \"'voltage' parameter must be set when method='doyleturner'\"\n-            )\n-\n-        # TODO: Find a better way to call different methods in the loop\n-        structure_factors = np.zeros(self.size)\n-        for i, (hkl, s) in enumerate(zip(self.hkl.data, self.scattering_parameter)):\n-            if method == \"kinematical\":\n-                structure_factors[i] = get_kinematical_structure_factor(\n-                    phase=self.phase,\n-                    hkl=hkl,\n-                    scattering_parameter=s,\n-                )\n-            else:\n-                structure_factors[i] = get_doyleturner_structure_factor(\n-                    phase=self.phase,\n-                    hkl=hkl,\n-                    scattering_parameter=s,\n-                    voltage=voltage,\n-                )\n-        self._structure_factor = np.where(\n-            structure_factors < _FLOAT_EPS, 0, structure_factors\n-        )\n-\n-    def calculate_theta(self, voltage):\n-        \"\"\"Populate `self.theta` with the Bragg angle :math:`theta_B` for\n-        each plane.\n-\n-        Parameters\n-        ----------\n-        voltage : float\n-            Beam energy in V.\n-        \"\"\"\n-        wavelength = get_refraction_corrected_wavelength(self.phase, voltage)\n-        self._theta = np.arcsin(0.5 * wavelength * self.gspacing)\n-\n-    def _raise_if_no_point_group(self):\n-        \"\"\"Raise ValueError if the phase attribute has no point group\n-        set.\n-        \"\"\"\n-        if self.phase.point_group is None:\n-            raise ValueError(f\"The phase {self.phase} must have a point group set\")\n-\n-    def _raise_if_no_space_group(self):\n-        \"\"\"Raise ValueError if the phase attribute has no space group\n-        set.\n-        \"\"\"\n-        if self.phase.space_group is None:\n-            raise ValueError(f\"The phase {self.phase} must have a space group set\")\n-\n-\n-def get_highest_hkl(lattice, min_dspacing=0.5):\n-    \"\"\"Return the highest Miller indices hkl of the plane with a direct\n-    space interplanar spacing greater than but closest to a lower\n-    threshold.\n-\n-    Parameters\n-    ----------\n-    lattice : diffpy.structure.Lattice\n-        Crystal lattice.\n-    min_dspacing : float, optional\n-        Smallest interplanar spacing to consider. Default is 0.5 \u00c5.\n-\n-    Returns\n-    -------\n-    highest_hkl : np.ndarray\n-        Highest Miller indices.\n-    \"\"\"\n-    highest_hkl = np.ones(3, dtype=int)\n-    for i in range(3):\n-        hkl = np.zeros(3)\n-        d = min_dspacing + 1\n-        while d > min_dspacing:\n-            hkl[i] += 1\n-            d = 1 / lattice.rnorm(hkl)\n-        highest_hkl[i] = hkl[i]\n-    return highest_hkl\n-\n-\n-def get_hkl(highest_hkl):\n-    \"\"\"Return a list of planes from a set of highest Miller indices.\n-\n-    Parameters\n-    ----------\n-    highest_hkl : orix.vector.Vector3d, np.ndarray, list, or tuple of int\n-        Highest Miller indices to consider.\n-\n-    Returns\n-    -------\n-    hkl : np.ndarray\n-        An array of Miller indices.\n-    \"\"\"\n-    index_ranges = [np.arange(-i, i + 1) for i in highest_hkl]\n-    return np.asarray(list(product(*index_ranges)))\n-\n-\n-def get_equivalent_hkl(hkl, operations, unique=False, return_multiplicity=False):\n-    \"\"\"Return symmetrically equivalent Miller indices.\n-\n-    Parameters\n-    ----------\n-    hkl : orix.vector.Vector3d, np.ndarray, list or tuple of int\n-        Miller indices.\n-    operations : orix.quaternion.symmetry.Symmetry\n-        Point group describing allowed symmetry operations.\n-    unique : bool, optional\n-        Whether to return only unique Miller indices. Default is False.\n-    return_multiplicity : bool, optional\n-        Whether to return the multiplicity of the input indices. Default\n-        is False.\n-\n-    Returns\n-    -------\n-    new_hkl : orix.vector.Vector3d\n-        The symmetrically equivalent Miller indices.\n-    multiplicity : np.ndarray\n-        Number of symmetrically equivalent indices. Only returned if\n-        `return_multiplicity` is True.\n-    \"\"\"\n-    new_hkl = operations.outer(Vector3d(hkl))\n-    new_hkl = new_hkl.flatten().reshape(*new_hkl.shape[::-1])\n-\n-    multiplicity = None\n-    if unique:\n-        n_families = new_hkl.shape[0]\n-        multiplicity = np.zeros(n_families, dtype=int)\n-        temp_hkl = new_hkl[0].unique().data\n-        multiplicity[0] = temp_hkl.shape[0]\n-        if n_families > 1:\n-            for i, hkl in enumerate(new_hkl[1:]):\n-                temp_hkl2 = hkl.unique()\n-                multiplicity[i + 1] = temp_hkl2.size\n-                temp_hkl = np.append(temp_hkl, temp_hkl2.data, axis=0)\n-        new_hkl = Vector3d(temp_hkl[: multiplicity.sum()])\n-\n-    # Remove 1-dimensions\n-    new_hkl = new_hkl.squeeze()\n-\n-    if unique and return_multiplicity:\n-        return new_hkl, multiplicity\n-    else:\n-        return new_hkl\n-\n-\n-def _is_equivalent(this_hkl: list, that_hkl: list) -> bool:\n-    return sorted(np.abs(this_hkl)) == sorted(np.abs(that_hkl))\ndiff --git a/doc/user/installation.rst b/doc/user/installation.rst\nindex 2f170779..32caa725 100644\n--- a/doc/user/installation.rst\n+++ b/doc/user/installation.rst\n@@ -4,7 +4,8 @@ Installation\n \n diffsims can be installed from `Anaconda <https://anaconda.org/conda-forge/diffsims>`_, the\n `Python Package Index <https://pypi.org/project/diffsims>`_ (``pip``), or from source,\n-and supports Python >= 3.6.\n+and supports Python >= 3.8.\n+\n \n With pip\n ========\n@@ -22,6 +23,7 @@ To install a specific version of diffsims (say version 0.5.1)::\n \n     pip install diffsims==0.5.1\n \n+\n With Anaconda\n =============\n \n@@ -48,6 +50,7 @@ To install a specific version of diffsims (say version 0.5.1)::\n \n     conda install diffsims==0.5.1 -c conda-forge\n \n+\n .. _install-from-source:\n \n From source\ndiff --git a/setup.cfg b/setup.cfg\nindex b804aa93..304edb0a 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -18,15 +18,15 @@ relative_files = True\n [coverage:report]\n precision = 2\n \n+# https://github.com/vidartf/manifix\n+# https://github.com/vidartf/globmatch\n [manifix]\n known_excludes =\n+     doc/build/**\n+     htmlcov/**\n      .*\n      .*/**\n      **/*.nbi\n      **/*.nbc\n      **/*.pyc\n-     .git/**\n-     doc/build/**\n-     htmlcov/**\n      *.code-workspace\n-     **/*.npy\n\\ No newline at end of file\n", "instance_id": "pyxem__diffsims-214", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to remove a deprecated class (`ReciprocalLatticePoint`) and update related methods in the `diffsims` library. It specifies the goal of deprecating old methods and replacing them with improved alternatives in the `ReciprocalLatticeVector` class, and it references a related issue (#189) and a dependency PR (#199). The description also includes a checklist for progress and reviewer guidelines, which adds to the clarity of the task's scope. However, there are minor ambiguities: the problem statement does not explicitly detail how the deprecation should be handled in terms of user migration (e.g., specific warnings or documentation updates beyond the changelog), nor does it mention potential edge cases or compatibility issues that might arise from removing the class. Additionally, the exact impact on users or downstream dependencies is not discussed, which could be critical for a library change. Overall, while the goal and high-level steps are clear, some finer details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files, including removing a deprecated class, moving related functions to a new module, updating imports, and adding deprecation decorators to existing methods. This requires understanding the interactions between different parts of the codebase, such as the crystallography module and its public API. However, the changes are mostly straightforward\u2014removing code, refactoring function locations, and applying deprecation decorators\u2014without introducing new complex logic or algorithms. Second, the technical concepts involved are relatively basic for a library like `diffsims`: familiarity with Python, deprecation mechanisms (using a provided decorator), and basic library structure (e.g., `__init__.py` updates). No advanced algorithms, design patterns, or domain-specific crystallography knowledge beyond what's already in the codebase are required to complete the task. Third, the problem does not explicitly mention edge cases or error handling beyond ensuring deprecation warnings are issued, though care must be taken to avoid breaking existing users, which adds a moderate layer of complexity. Finally, the impact on the system's architecture is minimal since this is primarily a cleanup and deprecation task rather than a fundamental redesign. Given these considerations, a score of 0.45 reflects a medium difficulty task that requires understanding multiple files and concepts but does not demand deep architectural changes or advanced technical expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Some InferenceClient tasks missing `parameters` argument, inconsistent with task specifications\nSome task methods in the `huggingface_hub.InferenceClient` do not include a `parameters` argument to allow passing additional inference params. \r\nThe tasks are : `audio-classification`, `automatic-speech-recognition`, `fill-mask, image-classification`, `image-segmentation`, `object-detection`, `question-answering`, `table-question-answering`, `text-classification`, `token-classification` and `translation`.\r\nThis inconsistency makes the implementation not fully aligned with the task specs [here](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub/inference/_generated/types) and the documentation [here](https://huggingface.co/docs/api-inference/parameters).\r\n\r\ntaking the example of text-classification:\r\n\r\nwith python requests:\r\n```python\r\nimport os\r\n\r\nimport requests\r\n\r\nfrom huggingface_hub import InferenceClient\r\n\r\n\r\nAPI_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\r\nheaders = {\"Authorization\": f\"Bearer {os.environ.get('HF_TOKEN')}\"}\r\n\r\n\r\ndef query(payload):\r\n    response = requests.post(API_URL, headers=headers, json=payload)\r\n    return response.json()\r\n\r\n\r\noutput = query(\r\n    {\r\n        \"inputs\": \"I like you. I love you\",\r\n        \"parameters\": {\"top_k\": 1},\r\n    }\r\n)\r\n\r\nprint(output)\r\n```\r\n```\r\n[{'label': 'POSITIVE', 'score': 0.9998738765716553}]\r\n```\r\nWith `InferenceClient`\r\n\r\n```python\r\nclient = InferenceClient()\r\n\r\noutput = client.text_classification(\r\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\r\n    text=\"I like you. I love you\",\r\n    parameters={\"top_k\": 1},\r\n)\r\n\r\nprint(output)\r\n```\r\n\r\n```\r\noutput = client.text_classification(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: InferenceClient.text_classification() got an unexpected keyword argument 'parameters'\r\n```\n", "patch": "diff --git a/.github/workflows/python-quality.yml b/.github/workflows/python-quality.yml\nindex acb4e9a7fe..039c146ed1 100644\n--- a/.github/workflows/python-quality.yml\n+++ b/.github/workflows/python-quality.yml\n@@ -42,6 +42,7 @@ jobs:\n       - run: .venv/bin/python utils/check_static_imports.py\n       - run: .venv/bin/python utils/generate_async_inference_client.py\n       - run: .venv/bin/python utils/generate_inference_types.py\n+      - run: .venv/bin/python utils/generate_task_parameters.py\n \n       # Run type checking at least on huggingface_hub root file to check all modules\n       # that can be lazy-loaded actually exist.\ndiff --git a/Makefile b/Makefile\nindex 341f8f9e12..c00fc30d4e 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -11,6 +11,7 @@ quality:\n \tpython utils/check_contrib_list.py\n \tpython utils/check_static_imports.py\n \tpython utils/generate_async_inference_client.py\n+\n \tmypy src\n \n style:\n@@ -20,11 +21,14 @@ style:\n \tpython utils/check_static_imports.py --update\n \tpython utils/generate_async_inference_client.py --update\n \n-inference_types_check:\n+inference_check:\n \tpython utils/generate_inference_types.py\n+\tpython utils/generate_task_parameters.py\n \n-inference_types_update:\n+inference_update:\n \tpython utils/generate_inference_types.py --update\n+\tpython utils/generate_task_parameters.py --update\n+\n \n repocard:\n \tpython utils/push_repocard_examples.py\ndiff --git a/docs/source/en/package_reference/inference_types.md b/docs/source/en/package_reference/inference_types.md\nindex f313294662..aa63c64b68 100644\n--- a/docs/source/en/package_reference/inference_types.md\n+++ b/docs/source/en/package_reference/inference_types.md\n@@ -398,5 +398,3 @@ This part of the lib is still under development and will be improved in future r\n [[autodoc]] huggingface_hub.ZeroShotObjectDetectionInputData\n \n [[autodoc]] huggingface_hub.ZeroShotObjectDetectionOutputElement\n-\n-\ndiff --git a/docs/source/ko/package_reference/inference_types.md b/docs/source/ko/package_reference/inference_types.md\nindex ef4a62a570..393481e10f 100644\n--- a/docs/source/ko/package_reference/inference_types.md\n+++ b/docs/source/ko/package_reference/inference_types.md\n@@ -397,5 +397,3 @@ rendered properly in your Markdown viewer.\n [[autodoc]] huggingface_hub.ZeroShotObjectDetectionInputData\n \n [[autodoc]] huggingface_hub.ZeroShotObjectDetectionOutputElement\n-\n-\ndiff --git a/setup.py b/setup.py\nindex e13aa28f88..373e3119cf 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -95,6 +95,7 @@ def get_version() -> str:\n extras[\"quality\"] = [\n     \"ruff>=0.5.0\",\n     \"mypy==1.5.1\",\n+    \"libcst==1.4.0\",\n ]\n \n extras[\"all\"] = extras[\"testing\"] + extras[\"quality\"] + extras[\"typing\"]\ndiff --git a/src/huggingface_hub/__init__.py b/src/huggingface_hub/__init__.py\nindex 99eacb71ba..807f265709 100644\n--- a/src/huggingface_hub/__init__.py\n+++ b/src/huggingface_hub/__init__.py\n@@ -281,9 +281,11 @@\n     \"inference._generated.types\": [\n         \"AudioClassificationInput\",\n         \"AudioClassificationOutputElement\",\n+        \"AudioClassificationOutputTransform\",\n         \"AudioClassificationParameters\",\n         \"AudioToAudioInput\",\n         \"AudioToAudioOutputElement\",\n+        \"AutomaticSpeechRecognitionEarlyStoppingEnum\",\n         \"AutomaticSpeechRecognitionGenerationParameters\",\n         \"AutomaticSpeechRecognitionInput\",\n         \"AutomaticSpeechRecognitionOutput\",\n@@ -328,6 +330,7 @@\n         \"FillMaskParameters\",\n         \"ImageClassificationInput\",\n         \"ImageClassificationOutputElement\",\n+        \"ImageClassificationOutputTransform\",\n         \"ImageClassificationParameters\",\n         \"ImageSegmentationInput\",\n         \"ImageSegmentationOutputElement\",\n@@ -336,6 +339,7 @@\n         \"ImageToImageOutput\",\n         \"ImageToImageParameters\",\n         \"ImageToImageTargetSize\",\n+        \"ImageToTextEarlyStoppingEnum\",\n         \"ImageToTextGenerationParameters\",\n         \"ImageToTextInput\",\n         \"ImageToTextOutput\",\n@@ -361,6 +365,7 @@\n         \"Text2TextGenerationParameters\",\n         \"TextClassificationInput\",\n         \"TextClassificationOutputElement\",\n+        \"TextClassificationOutputTransform\",\n         \"TextClassificationParameters\",\n         \"TextGenerationInput\",\n         \"TextGenerationInputGenerateParameters\",\n@@ -373,6 +378,7 @@\n         \"TextGenerationStreamOutput\",\n         \"TextGenerationStreamOutputStreamDetails\",\n         \"TextGenerationStreamOutputToken\",\n+        \"TextToAudioEarlyStoppingEnum\",\n         \"TextToAudioGenerationParameters\",\n         \"TextToAudioInput\",\n         \"TextToAudioOutput\",\n@@ -381,6 +387,7 @@\n         \"TextToImageOutput\",\n         \"TextToImageParameters\",\n         \"TextToImageTargetSize\",\n+        \"TextToSpeechEarlyStoppingEnum\",\n         \"TextToSpeechGenerationParameters\",\n         \"TextToSpeechInput\",\n         \"TextToSpeechOutput\",\n@@ -394,6 +401,7 @@\n         \"TranslationParameters\",\n         \"VideoClassificationInput\",\n         \"VideoClassificationOutputElement\",\n+        \"VideoClassificationOutputTransform\",\n         \"VideoClassificationParameters\",\n         \"VisualQuestionAnsweringInput\",\n         \"VisualQuestionAnsweringInputData\",\n@@ -796,9 +804,11 @@ def __dir__():\n     from .inference._generated.types import (\n         AudioClassificationInput,  # noqa: F401\n         AudioClassificationOutputElement,  # noqa: F401\n+        AudioClassificationOutputTransform,  # noqa: F401\n         AudioClassificationParameters,  # noqa: F401\n         AudioToAudioInput,  # noqa: F401\n         AudioToAudioOutputElement,  # noqa: F401\n+        AutomaticSpeechRecognitionEarlyStoppingEnum,  # noqa: F401\n         AutomaticSpeechRecognitionGenerationParameters,  # noqa: F401\n         AutomaticSpeechRecognitionInput,  # noqa: F401\n         AutomaticSpeechRecognitionOutput,  # noqa: F401\n@@ -843,6 +853,7 @@ def __dir__():\n         FillMaskParameters,  # noqa: F401\n         ImageClassificationInput,  # noqa: F401\n         ImageClassificationOutputElement,  # noqa: F401\n+        ImageClassificationOutputTransform,  # noqa: F401\n         ImageClassificationParameters,  # noqa: F401\n         ImageSegmentationInput,  # noqa: F401\n         ImageSegmentationOutputElement,  # noqa: F401\n@@ -851,6 +862,7 @@ def __dir__():\n         ImageToImageOutput,  # noqa: F401\n         ImageToImageParameters,  # noqa: F401\n         ImageToImageTargetSize,  # noqa: F401\n+        ImageToTextEarlyStoppingEnum,  # noqa: F401\n         ImageToTextGenerationParameters,  # noqa: F401\n         ImageToTextInput,  # noqa: F401\n         ImageToTextOutput,  # noqa: F401\n@@ -876,6 +888,7 @@ def __dir__():\n         Text2TextGenerationParameters,  # noqa: F401\n         TextClassificationInput,  # noqa: F401\n         TextClassificationOutputElement,  # noqa: F401\n+        TextClassificationOutputTransform,  # noqa: F401\n         TextClassificationParameters,  # noqa: F401\n         TextGenerationInput,  # noqa: F401\n         TextGenerationInputGenerateParameters,  # noqa: F401\n@@ -888,6 +901,7 @@ def __dir__():\n         TextGenerationStreamOutput,  # noqa: F401\n         TextGenerationStreamOutputStreamDetails,  # noqa: F401\n         TextGenerationStreamOutputToken,  # noqa: F401\n+        TextToAudioEarlyStoppingEnum,  # noqa: F401\n         TextToAudioGenerationParameters,  # noqa: F401\n         TextToAudioInput,  # noqa: F401\n         TextToAudioOutput,  # noqa: F401\n@@ -896,6 +910,7 @@ def __dir__():\n         TextToImageOutput,  # noqa: F401\n         TextToImageParameters,  # noqa: F401\n         TextToImageTargetSize,  # noqa: F401\n+        TextToSpeechEarlyStoppingEnum,  # noqa: F401\n         TextToSpeechGenerationParameters,  # noqa: F401\n         TextToSpeechInput,  # noqa: F401\n         TextToSpeechOutput,  # noqa: F401\n@@ -909,6 +924,7 @@ def __dir__():\n         TranslationParameters,  # noqa: F401\n         VideoClassificationInput,  # noqa: F401\n         VideoClassificationOutputElement,  # noqa: F401\n+        VideoClassificationOutputTransform,  # noqa: F401\n         VideoClassificationParameters,  # noqa: F401\n         VisualQuestionAnsweringInput,  # noqa: F401\n         VisualQuestionAnsweringInputData,  # noqa: F401\ndiff --git a/src/huggingface_hub/inference/_client.py b/src/huggingface_hub/inference/_client.py\nindex 63204da6d4..38b37b71e3 100644\n--- a/src/huggingface_hub/inference/_client.py\n+++ b/src/huggingface_hub/inference/_client.py\n@@ -64,6 +64,7 @@\n )\n from huggingface_hub.inference._generated.types import (\n     AudioClassificationOutputElement,\n+    AudioClassificationOutputTransform,\n     AudioToAudioOutputElement,\n     AutomaticSpeechRecognitionOutput,\n     ChatCompletionInputGrammarType,\n@@ -81,9 +82,12 @@\n     SummarizationOutput,\n     TableQuestionAnsweringOutputElement,\n     TextClassificationOutputElement,\n+    TextClassificationOutputTransform,\n     TextGenerationInputGrammarType,\n     TextGenerationOutput,\n     TextGenerationStreamOutput,\n+    TextToImageTargetSize,\n+    TextToSpeechEarlyStoppingEnum,\n     TokenClassificationOutputElement,\n     ToolElement,\n     TranslationOutput,\n@@ -92,6 +96,7 @@\n     ZeroShotImageClassificationOutputElement,\n )\n from huggingface_hub.utils import build_hf_headers, get_session, hf_raise_for_status\n+from huggingface_hub.utils._deprecation import _deprecate_arguments\n \n \n if TYPE_CHECKING:\n@@ -318,6 +323,8 @@ def audio_classification(\n         audio: ContentT,\n         *,\n         model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n+        function_to_apply: Optional[\"AudioClassificationOutputTransform\"] = None,\n     ) -> List[AudioClassificationOutputElement]:\n         \"\"\"\n         Perform audio classification on the provided audio content.\n@@ -330,6 +337,10 @@ def audio_classification(\n                 The model to use for audio classification. Can be a model ID hosted on the Hugging Face Hub\n                 or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for\n                 audio classification will be used.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n+            function_to_apply (`\"AudioClassificationOutputTransform\"`, *optional*):\n+                The function to apply to the output.\n \n         Returns:\n             `List[AudioClassificationOutputElement]`: List of [`AudioClassificationOutputElement`] items containing the predicted labels and their confidence.\n@@ -352,7 +363,19 @@ def audio_classification(\n         ]\n         ```\n         \"\"\"\n-        response = self.post(data=audio, model=model, task=\"audio-classification\")\n+        parameters = {\"function_to_apply\": function_to_apply, \"top_k\": top_k}\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, send audio as raw data\n+            data = audio\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # Or some parameters are provided -> send audio as base64 encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(audio)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, data=data, model=model, task=\"audio-classification\")\n         return AudioClassificationOutputElement.parse_obj_as_list(response)\n \n     def audio_to_audio(\n@@ -903,6 +926,14 @@ def document_question_answering(\n         question: str,\n         *,\n         model: Optional[str] = None,\n+        doc_stride: Optional[int] = None,\n+        handle_impossible_answer: Optional[bool] = None,\n+        lang: Optional[str] = None,\n+        max_answer_len: Optional[int] = None,\n+        max_question_len: Optional[int] = None,\n+        max_seq_len: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+        word_boxes: Optional[List[Union[List[float], str]]] = None,\n     ) -> List[DocumentQuestionAnsweringOutputElement]:\n         \"\"\"\n         Answer questions on document images.\n@@ -916,7 +947,29 @@ def document_question_answering(\n                 The model to use for the document question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended document question answering model will be used.\n                 Defaults to None.\n-\n+            doc_stride (`int`, *optional*):\n+                If the words in the document are too long to fit with the question for the model, it will\n+                be split in several chunks with some overlap. This argument controls the size of that\n+                overlap.\n+            handle_impossible_answer (`bool`, *optional*):\n+                Whether to accept impossible as an answer.\n+            lang (`str`, *optional*):\n+                Language to use while running OCR.\n+            max_answer_len (`int`, *optional*):\n+                The maximum length of predicted answers (e.g., only answers with a shorter length are\n+                considered).\n+            max_question_len (`int`, *optional*):\n+                The maximum length of the question after tokenization. It will be truncated if needed.\n+            max_seq_len (`int`, *optional*):\n+                The maximum length of the total sentence (context + question) in tokens of each chunk\n+                passed to the model. The context will be split in several chunks (using doc_stride as\n+                overlap) if needed.\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Can return less\n+                than top_k answers if there are not enough options available within the context.\n+            word_boxes (`List[Union[List[float], str]]`, *optional*):\n+                A list of words and bounding boxes (normalized 0->1000). If provided, the inference will\n+                skip the OCR step and use the provided bounding boxes instead.\n         Returns:\n             `List[DocumentQuestionAnsweringOutputElement]`: a list of [`DocumentQuestionAnsweringOutputElement`] items containing the predicted label, associated probability, word ids, and page number.\n \n@@ -926,15 +979,29 @@ def document_question_answering(\n             `HTTPError`:\n                 If the request fails with an HTTP error status code other than HTTP 503.\n \n+\n         Example:\n         ```py\n         >>> from huggingface_hub import InferenceClient\n         >>> client = InferenceClient()\n         >>> client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\n-        [DocumentQuestionAnsweringOutputElement(score=0.42515629529953003, answer='us-001', start=16, end=16)]\n+        [DocumentQuestionAnsweringOutputElement(answer='us-001', end=16, score=0.9999666213989258, start=16, words=None)]\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"question\": question, \"image\": _b64_encode(image)}\n+        parameters = {\n+            \"doc_stride\": doc_stride,\n+            \"handle_impossible_answer\": handle_impossible_answer,\n+            \"lang\": lang,\n+            \"max_answer_len\": max_answer_len,\n+            \"max_question_len\": max_question_len,\n+            \"max_seq_len\": max_seq_len,\n+            \"top_k\": top_k,\n+            \"word_boxes\": word_boxes,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(json=payload, model=model, task=\"document-question-answering\")\n         return DocumentQuestionAnsweringOutputElement.parse_obj_as_list(response)\n \n@@ -959,7 +1026,7 @@ def feature_extraction(\n                 a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.\n                 Defaults to None.\n             normalize (`bool`, *optional*):\n-                Whether to normalize the embeddings or not. Defaults to None.\n+                Whether to normalize the embeddings or not.\n                 Only available on server powered by Text-Embedding-Inference.\n             prompt_name (`str`, *optional*):\n                 The name of the prompt that should be used by for encoding. If not set, no prompt will be applied.\n@@ -968,7 +1035,7 @@ def feature_extraction(\n                 then the sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"\n                 because the prompt text will be prepended before any text to encode.\n             truncate (`bool`, *optional*):\n-                Whether to truncate the embeddings or not. Defaults to None.\n+                Whether to truncate the embeddings or not.\n                 Only available on server powered by Text-Embedding-Inference.\n             truncation_direction (`Literal[\"Left\", \"Right\"]`, *optional*):\n                 Which side of the input should be truncated when `truncate=True` is passed.\n@@ -994,19 +1061,27 @@ def feature_extraction(\n         ```\n         \"\"\"\n         payload: Dict = {\"inputs\": text}\n-        if normalize is not None:\n-            payload[\"normalize\"] = normalize\n-        if prompt_name is not None:\n-            payload[\"prompt_name\"] = prompt_name\n-        if truncate is not None:\n-            payload[\"truncate\"] = truncate\n-        if truncation_direction is not None:\n-            payload[\"truncation_direction\"] = truncation_direction\n+        parameters = {\n+            \"normalize\": normalize,\n+            \"prompt_name\": prompt_name,\n+            \"truncate\": truncate,\n+            \"truncation_direction\": truncation_direction,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(json=payload, model=model, task=\"feature-extraction\")\n         np = _import_numpy()\n         return np.array(_bytes_to_dict(response), dtype=\"float32\")\n \n-    def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[FillMaskOutputElement]:\n+    def fill_mask(\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        targets: Optional[List[str]] = None,\n+        top_k: Optional[int] = None,\n+    ) -> List[FillMaskOutputElement]:\n         \"\"\"\n         Fill in a hole with a missing word (token to be precise).\n \n@@ -1016,8 +1091,13 @@ def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[FillMaskO\n             model (`str`, *optional*):\n                 The model to use for the fill mask task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended fill mask model will be used.\n-                Defaults to None.\n-\n+            targets (`List[str]`, *optional*):\n+                When passed, the model will limit the scores to the passed targets instead of looking up\n+                in the whole vocabulary. If the provided targets are not in the model vocab, they will be\n+                tokenized and the first resulting token will be used (with a warning, and that might be\n+                slower).\n+            top_k (`int`, *optional*):\n+                When passed, overrides the number of predictions to return.\n         Returns:\n             `List[FillMaskOutputElement]`: a list of [`FillMaskOutputElement`] items containing the predicted label, associated\n             probability, token reference, and completed text.\n@@ -1039,7 +1119,12 @@ def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[FillMaskO\n         ]\n         ```\n         \"\"\"\n-        response = self.post(json={\"inputs\": text}, model=model, task=\"fill-mask\")\n+        payload: Dict = {\"inputs\": text}\n+        parameters = {\"targets\": targets, \"top_k\": top_k}\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, model=model, task=\"fill-mask\")\n         return FillMaskOutputElement.parse_obj_as_list(response)\n \n     def image_classification(\n@@ -1047,6 +1132,8 @@ def image_classification(\n         image: ContentT,\n         *,\n         model: Optional[str] = None,\n+        function_to_apply: Optional[Literal[\"sigmoid\", \"softmax\", \"none\"]] = None,\n+        top_k: Optional[int] = None,\n     ) -> List[ImageClassificationOutputElement]:\n         \"\"\"\n         Perform image classification on the given image using the specified model.\n@@ -1057,7 +1144,10 @@ def image_classification(\n             model (`str`, *optional*):\n                 The model to use for image classification. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for image classification will be used.\n-\n+            function_to_apply (`Literal[\"sigmoid\", \"softmax\", \"none\"]`, *optional*):\n+                The function to apply to the output scores.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n         Returns:\n             `List[ImageClassificationOutputElement]`: a list of [`ImageClassificationOutputElement`] items containing the predicted label and associated probability.\n \n@@ -1072,10 +1162,23 @@ def image_classification(\n         >>> from huggingface_hub import InferenceClient\n         >>> client = InferenceClient()\n         >>> client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n-        [ImageClassificationOutputElement(score=0.9779096841812134, label='Blenheim spaniel'), ...]\n+        [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]\n         ```\n         \"\"\"\n-        response = self.post(data=image, model=model, task=\"image-classification\")\n+        parameters = {\"function_to_apply\": function_to_apply, \"top_k\": top_k}\n+\n+        if all(parameter is None for parameter in parameters.values()):\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+\n+        else:\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+\n+        response = self.post(json=payload, data=data, model=model, task=\"image-classification\")\n         return ImageClassificationOutputElement.parse_obj_as_list(response)\n \n     def image_segmentation(\n@@ -1083,6 +1186,10 @@ def image_segmentation(\n         image: ContentT,\n         *,\n         model: Optional[str] = None,\n+        mask_threshold: Optional[float] = None,\n+        overlap_mask_area_threshold: Optional[float] = None,\n+        subtask: Optional[Literal[\"instance\", \"panoptic\", \"semantic\"]] = None,\n+        threshold: Optional[float] = None,\n     ) -> List[ImageSegmentationOutputElement]:\n         \"\"\"\n         Perform image segmentation on the given image using the specified model.\n@@ -1099,7 +1206,14 @@ def image_segmentation(\n             model (`str`, *optional*):\n                 The model to use for image segmentation. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for image segmentation will be used.\n-\n+            mask_threshold (`float`, *optional*):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*):\n+                Mask overlap threshold to eliminate small, disconnected segments.\n+            subtask (`Literal[\"instance\", \"panoptic\", \"semantic\"]`, *optional*):\n+                Segmentation task to be performed, depending on model capabilities.\n+            threshold (`float`, *optional*):\n+                Probability threshold to filter out predicted masks.\n         Returns:\n             `List[ImageSegmentationOutputElement]`: A list of [`ImageSegmentationOutputElement`] items containing the segmented masks and associated attributes.\n \n@@ -1113,11 +1227,28 @@ def image_segmentation(\n         ```py\n         >>> from huggingface_hub import InferenceClient\n         >>> client = InferenceClient()\n-        >>> client.image_segmentation(\"cat.jpg\"):\n+        >>> client.image_segmentation(\"cat.jpg\")\n         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]\n         ```\n         \"\"\"\n-        response = self.post(data=image, model=model, task=\"image-segmentation\")\n+        parameters = {\n+            \"mask_threshold\": mask_threshold,\n+            \"overlap_mask_area_threshold\": overlap_mask_area_threshold,\n+            \"subtask\": subtask,\n+            \"threshold\": threshold,\n+        }\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, the image can be raw bytes, an image file, or URL to an online image\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # if parameters are provided, the image needs to be a base64-encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, data=data, model=model, task=\"image-segmentation\")\n         output = ImageSegmentationOutputElement.parse_obj_as_list(response)\n         for item in output:\n             item.mask = _b64_to_image(item.mask)  # type: ignore [assignment]\n@@ -1197,7 +1328,7 @@ def image_to_image(\n             data = image\n             payload: Optional[Dict[str, Any]] = None\n         else:\n-            # Or an image + some parameters => use base64 encoding\n+            # if parameters are provided, the image needs to be a base64-encoded string\n             data = None\n             payload = {\"inputs\": _b64_encode(image)}\n             for key, value in parameters.items():\n@@ -1328,10 +1459,7 @@ def _unpack_response(framework: str, items: List[Dict]) -> None:\n         return models_by_task\n \n     def object_detection(\n-        self,\n-        image: ContentT,\n-        *,\n-        model: Optional[str] = None,\n+        self, image: ContentT, *, model: Optional[str] = None, threshold: Optional[float] = None\n     ) -> List[ObjectDetectionOutputElement]:\n         \"\"\"\n         Perform object detection on the given image using the specified model.\n@@ -1348,7 +1476,8 @@ def object_detection(\n             model (`str`, *optional*):\n                 The model to use for object detection. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for object detection (DETR) will be used.\n-\n+            threshold (`float`, *optional*):\n+                The probability necessary to make a prediction.\n         Returns:\n             `List[ObjectDetectionOutputElement]`: A list of [`ObjectDetectionOutputElement`] items containing the bounding boxes and associated attributes.\n \n@@ -1368,13 +1497,37 @@ def object_detection(\n         [ObjectDetectionOutputElement(score=0.9486683011054993, label='person', box=ObjectDetectionBoundingBox(xmin=59, ymin=39, xmax=420, ymax=510)), ...]\n         ```\n         \"\"\"\n-        # detect objects\n-        response = self.post(data=image, model=model, task=\"object-detection\")\n+        parameters = {\n+            \"threshold\": threshold,\n+        }\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, the image can be raw bytes, an image file, or URL to an online image\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # if parameters are provided, the image needs to be a base64-encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, data=data, model=model, task=\"object-detection\")\n         return ObjectDetectionOutputElement.parse_obj_as_list(response)\n \n     def question_answering(\n-        self, question: str, context: str, *, model: Optional[str] = None\n-    ) -> QuestionAnsweringOutputElement:\n+        self,\n+        question: str,\n+        context: str,\n+        *,\n+        model: Optional[str] = None,\n+        align_to_words: Optional[bool] = None,\n+        doc_stride: Optional[int] = None,\n+        handle_impossible_answer: Optional[bool] = None,\n+        max_answer_len: Optional[int] = None,\n+        max_question_len: Optional[int] = None,\n+        max_seq_len: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+    ) -> Union[QuestionAnsweringOutputElement, List[QuestionAnsweringOutputElement]]:\n         \"\"\"\n         Retrieve the answer to a question from a given text.\n \n@@ -1386,10 +1539,31 @@ def question_answering(\n             model (`str`):\n                 The model to use for the question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint.\n-\n+            align_to_words (`bool`, *optional*):\n+                Attempts to align the answer to real words. Improves quality on space separated\n+                languages. Might hurt on non-space-separated languages (like Japanese or Chinese).\n+            doc_stride (`int`, *optional*):\n+                If the context is too long to fit with the question for the model, it will be split in\n+                several chunks with some overlap. This argument controls the size of that overlap.\n+            handle_impossible_answer (`bool`, *optional*):\n+                Whether to accept impossible as an answer.\n+            max_answer_len (`int`, *optional*):\n+                The maximum length of predicted answers (e.g., only answers with a shorter length are\n+                considered).\n+            max_question_len (`int`, *optional*):\n+                The maximum length of the question after tokenization. It will be truncated if needed.\n+            max_seq_len (`int`, *optional*):\n+                The maximum length of the total sentence (context + question) in tokens of each chunk\n+                passed to the model. The context will be split in several chunks (using docStride as\n+                overlap) if needed.\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Note that we\n+                return less than topk answers if there are not enough options available within the\n+                context.\n         Returns:\n-            [`QuestionAnsweringOutputElement`]: an question answering output containing the score, start index, end index, and answer.\n-\n+            Union[`QuestionAnsweringOutputElement`, List[`QuestionAnsweringOutputElement`]]:\n+                When top_k is 1 or not provided, it returns a single `QuestionAnsweringOutputElement`.\n+                When top_k is greater than 1, it returns a list of `QuestionAnsweringOutputElement`.\n         Raises:\n             [`InferenceTimeoutError`]:\n                 If the model is unavailable or the request times out.\n@@ -1401,17 +1575,30 @@ def question_answering(\n         >>> from huggingface_hub import InferenceClient\n         >>> client = InferenceClient()\n         >>> client.question_answering(question=\"What's my name?\", context=\"My name is Clara and I live in Berkeley.\")\n-        QuestionAnsweringOutputElement(score=0.9326562285423279, start=11, end=16, answer='Clara')\n+        QuestionAnsweringOutputElement(answer='Clara', end=16, score=0.9326565265655518, start=11)\n         ```\n         \"\"\"\n-\n+        parameters = {\n+            \"align_to_words\": align_to_words,\n+            \"doc_stride\": doc_stride,\n+            \"handle_impossible_answer\": handle_impossible_answer,\n+            \"max_answer_len\": max_answer_len,\n+            \"max_question_len\": max_question_len,\n+            \"max_seq_len\": max_seq_len,\n+            \"top_k\": top_k,\n+        }\n         payload: Dict[str, Any] = {\"question\": question, \"context\": context}\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(\n             json=payload,\n             model=model,\n             task=\"question-answering\",\n         )\n-        return QuestionAnsweringOutputElement.parse_obj_as_instance(response)\n+        # Parse the response as a single `QuestionAnsweringOutputElement` when top_k is 1 or not provided, or a list of `QuestionAnsweringOutputElement` to ensure backward compatibility.\n+        output = QuestionAnsweringOutputElement.parse_obj(response)\n+        return output\n \n     def sentence_similarity(\n         self, sentence: str, other_sentences: List[str], *, model: Optional[str] = None\n@@ -1460,12 +1647,23 @@ def sentence_similarity(\n         )\n         return _bytes_to_list(response)\n \n+    @_deprecate_arguments(\n+        version=\"0.29\",\n+        deprecated_args=[\"parameters\"],\n+        custom_message=(\n+            \"The `parameters` argument is deprecated and will be removed in a future version. \"\n+            \"Provide individual parameters instead: `clean_up_tokenization_spaces`, `generate_parameters`, and `truncation`.\"\n+        ),\n+    )\n     def summarization(\n         self,\n         text: str,\n         *,\n         parameters: Optional[Dict[str, Any]] = None,\n         model: Optional[str] = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        generate_parameters: Optional[Dict[str, Any]] = None,\n+        truncation: Optional[Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]] = None,\n     ) -> SummarizationOutput:\n         \"\"\"\n         Generate a summary of a given text using a specified model.\n@@ -1478,8 +1676,13 @@ def summarization(\n                 for more details.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n-\n+                Inference Endpoint. If not provided, the default recommended model for summarization will be used.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether to clean up the potential extra spaces in the text output.\n+            generate_parameters (`Dict[str, Any]`, *optional*):\n+                Additional parametrization of the text generation algorithm.\n+            truncation (`Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]`, *optional*):\n+                The truncation strategy to use.\n         Returns:\n             [`SummarizationOutput`]: The generated summary text.\n \n@@ -1500,11 +1703,25 @@ def summarization(\n         payload: Dict[str, Any] = {\"inputs\": text}\n         if parameters is not None:\n             payload[\"parameters\"] = parameters\n+        else:\n+            parameters = {\n+                \"clean_up_tokenization_spaces\": clean_up_tokenization_spaces,\n+                \"generate_parameters\": generate_parameters,\n+                \"truncation\": truncation,\n+            }\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(json=payload, model=model, task=\"summarization\")\n         return SummarizationOutput.parse_obj_as_list(response)[0]\n \n     def table_question_answering(\n-        self, table: Dict[str, Any], query: str, *, model: Optional[str] = None\n+        self,\n+        table: Dict[str, Any],\n+        query: str,\n+        *,\n+        model: Optional[str] = None,\n+        parameters: Optional[Dict[str, Any]] = None,\n     ) -> TableQuestionAnsweringOutputElement:\n         \"\"\"\n         Retrieve the answer to a question from information given in a table.\n@@ -1518,6 +1735,8 @@ def table_question_answering(\n             model (`str`):\n                 The model to use for the table-question-answering task. Can be a model ID hosted on the Hugging Face\n                 Hub or a URL to a deployed Inference Endpoint.\n+            parameters (`Dict[str, Any]`, *optional*):\n+                Additional inference parameters. Defaults to None.\n \n         Returns:\n             [`TableQuestionAnsweringOutputElement`]: a table question answering output containing the answer, coordinates, cells and the aggregator used.\n@@ -1538,11 +1757,15 @@ def table_question_answering(\n         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')\n         ```\n         \"\"\"\n+        payload: Dict[str, Any] = {\n+            \"query\": query,\n+            \"table\": table,\n+        }\n+\n+        if parameters is not None:\n+            payload[\"parameters\"] = parameters\n         response = self.post(\n-            json={\n-                \"query\": query,\n-                \"table\": table,\n-            },\n+            json=payload,\n             model=model,\n             task=\"table-question-answering\",\n         )\n@@ -1633,7 +1856,14 @@ def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = No\n         response = self.post(json={\"table\": table}, model=model, task=\"tabular-regression\")\n         return _bytes_to_list(response)\n \n-    def text_classification(self, text: str, *, model: Optional[str] = None) -> List[TextClassificationOutputElement]:\n+    def text_classification(\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n+        function_to_apply: Optional[\"TextClassificationOutputTransform\"] = None,\n+    ) -> List[TextClassificationOutputElement]:\n         \"\"\"\n         Perform text classification (e.g. sentiment-analysis) on the given text.\n \n@@ -1644,6 +1874,10 @@ def text_classification(self, text: str, *, model: Optional[str] = None) -> List\n                 The model to use for the text classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended text classification model will be used.\n                 Defaults to None.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n+            function_to_apply (`\"TextClassificationOutputTransform\"`, *optional*):\n+                The function to apply to the output.\n \n         Returns:\n             `List[TextClassificationOutputElement]`: a list of [`TextClassificationOutputElement`] items containing the predicted label and associated probability.\n@@ -1665,7 +1899,15 @@ def text_classification(self, text: str, *, model: Optional[str] = None) -> List\n         ]\n         ```\n         \"\"\"\n-        response = self.post(json={\"inputs\": text}, model=model, task=\"text-classification\")\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"function_to_apply\": function_to_apply,\n+            \"top_k\": top_k,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, model=model, task=\"text-classification\")\n         return TextClassificationOutputElement.parse_obj_as_list(response)[0]  # type: ignore [return-value]\n \n     @overload\n@@ -2174,6 +2416,9 @@ def text_to_image(\n         num_inference_steps: Optional[float] = None,\n         guidance_scale: Optional[float] = None,\n         model: Optional[str] = None,\n+        scheduler: Optional[str] = None,\n+        target_size: Optional[TextToImageTargetSize] = None,\n+        seed: Optional[int] = None,\n         **kwargs,\n     ) -> \"Image\":\n         \"\"\"\n@@ -2202,7 +2447,14 @@ def text_to_image(\n                 usually at the expense of lower image quality.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. If not provided, the default recommended text-to-image model will be used.\n+                Defaults to None.\n+            scheduler (`str`, *optional*):\n+                Override the scheduler with a compatible one.\n+            target_size (`TextToImageTargetSize`, *optional*):\n+                The size in pixel of the output image\n+            seed (`int`, *optional*):\n+                Seed for the random number generator.\n \n         Returns:\n             `Image`: The generated image.\n@@ -2236,6 +2488,9 @@ def text_to_image(\n             \"width\": width,\n             \"num_inference_steps\": num_inference_steps,\n             \"guidance_scale\": guidance_scale,\n+            \"scheduler\": scheduler,\n+            \"target_size\": target_size,\n+            \"seed\": seed,\n             **kwargs,\n         }\n         for key, value in parameters.items():\n@@ -2244,7 +2499,28 @@ def text_to_image(\n         response = self.post(json=payload, model=model, task=\"text-to-image\")\n         return _bytes_to_image(response)\n \n-    def text_to_speech(self, text: str, *, model: Optional[str] = None) -> bytes:\n+    def text_to_speech(\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        do_sample: Optional[bool] = None,\n+        early_stopping: Optional[Union[bool, \"TextToSpeechEarlyStoppingEnum\"]] = None,\n+        epsilon_cutoff: Optional[float] = None,\n+        eta_cutoff: Optional[float] = None,\n+        max_length: Optional[int] = None,\n+        max_new_tokens: Optional[int] = None,\n+        min_length: Optional[int] = None,\n+        min_new_tokens: Optional[int] = None,\n+        num_beam_groups: Optional[int] = None,\n+        num_beams: Optional[int] = None,\n+        penalty_alpha: Optional[float] = None,\n+        temperature: Optional[float] = None,\n+        top_k: Optional[int] = None,\n+        top_p: Optional[float] = None,\n+        typical_p: Optional[float] = None,\n+        use_cache: Optional[bool] = None,\n+    ) -> bytes:\n         \"\"\"\n         Synthesize an audio of a voice pronouncing a given text.\n \n@@ -2253,7 +2529,56 @@ def text_to_speech(self, text: str, *, model: Optional[str] = None) -> bytes:\n                 The text to synthesize.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. If not provided, the default recommended text-to-speech model will be used.\n+                Defaults to None.\n+            do_sample (`bool`, *optional*):\n+                Whether to use sampling instead of greedy decoding when generating new tokens.\n+            early_stopping (`Union[bool, \"TextToSpeechEarlyStoppingEnum\"`, *optional*):\n+                Controls the stopping condition for beam-based methods.\n+            epsilon_cutoff (`float`, *optional*):\n+                If set to float strictly between 0 and 1, only tokens with a conditional probability\n+                greater than epsilon_cutoff will be sampled. In the paper, suggested values range from\n+                3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language\n+                Model Desmoothing](https://hf.co/papers/2210.15191) for more details.\n+            eta_cutoff (`float`, *optional*):\n+                Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to\n+                float strictly between 0 and 1, a token is only considered if it is greater than either\n+                eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter\n+                term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In\n+                the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.\n+                See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)\n+                for more details.\n+            max_length (`int`, *optional*):\n+                The maximum length (in tokens) of the generated text, including the input.\n+            max_new_tokens (`int`, *optional*):\n+                The maximum number of tokens to generate. Takes precedence over maxLength.\n+            min_length (`int`, *optional*):\n+                The minimum length (in tokens) of the generated text, including the input.\n+            min_new_tokens (`int`, *optional*):\n+                The minimum number of tokens to generate. Takes precedence over maxLength.\n+            num_beam_groups (`int`, *optional*):\n+                Number of groups to divide num_beams into in order to ensure diversity among different\n+                groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.\n+            num_beams (`int`, *optional*):\n+                Number of beams to use for beam search.\n+            penalty_alpha (`float`, *optional*):\n+                The value balances the model confidence and the degeneration penalty in contrastive\n+                search decoding.\n+            temperature (`float`, *optional*):\n+                The value used to modulate the next token probabilities.\n+            top_k (`int`, *optional*):\n+                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n+            top_p (`float`, *optional*):\n+                If set to float < 1, only the smallest set of most probable tokens with probabilities\n+                that add up to top_p or higher are kept for generation.\n+            typical_p (`float`, *optional*):\n+                Local typicality measures how similar the conditional probability of predicting a target token next is\n+                to the expected conditional probability of predicting a random token next, given the partial text\n+                already generated. If set to float < 1, the smallest set of the most locally typical tokens with\n+                probabilities that add up to typical_p or higher are kept for generation. See [this\n+                paper](https://hf.co/papers/2202.00666) for more details.\n+            use_cache (`bool`, *optional*):\n+                Whether the model should use the past last key/values attentions to speed up decoding\n \n         Returns:\n             `bytes`: The generated audio.\n@@ -2274,10 +2599,39 @@ def text_to_speech(self, text: str, *, model: Optional[str] = None) -> bytes:\n         >>> Path(\"hello_world.flac\").write_bytes(audio)\n         ```\n         \"\"\"\n-        return self.post(json={\"inputs\": text}, model=model, task=\"text-to-speech\")\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"do_sample\": do_sample,\n+            \"early_stopping\": early_stopping,\n+            \"epsilon_cutoff\": epsilon_cutoff,\n+            \"eta_cutoff\": eta_cutoff,\n+            \"max_length\": max_length,\n+            \"max_new_tokens\": max_new_tokens,\n+            \"min_length\": min_length,\n+            \"min_new_tokens\": min_new_tokens,\n+            \"num_beam_groups\": num_beam_groups,\n+            \"num_beams\": num_beams,\n+            \"penalty_alpha\": penalty_alpha,\n+            \"temperature\": temperature,\n+            \"top_k\": top_k,\n+            \"top_p\": top_p,\n+            \"typical_p\": typical_p,\n+            \"use_cache\": use_cache,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = self.post(json=payload, model=model, task=\"text-to-speech\")\n+        return response\n \n     def token_classification(\n-        self, text: str, *, model: Optional[str] = None\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        aggregation_strategy: Optional[Literal[\"none\", \"simple\", \"first\", \"average\", \"max\"]] = None,\n+        ignore_labels: Optional[List[str]] = None,\n+        stride: Optional[int] = None,\n     ) -> List[TokenClassificationOutputElement]:\n         \"\"\"\n         Perform token classification on the given text.\n@@ -2290,6 +2644,12 @@ def token_classification(\n                 The model to use for the token classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended token classification model will be used.\n                 Defaults to None.\n+            aggregation_strategy (`Literal[\"none\", \"simple\", \"first\", \"average\", \"max\"]`, *optional*):\n+                The strategy used to fuse tokens based on model predictions.\n+            ignore_labels (`List[str]`, *optional*):\n+                A list of labels to ignore.\n+            stride (`int`, *optional*):\n+                The number of overlapping tokens between chunks when splitting the input text.\n \n         Returns:\n             `List[TokenClassificationOutputElement]`: List of [`TokenClassificationOutputElement`] items containing the entity group, confidence score, word, start and end index.\n@@ -2324,6 +2684,14 @@ def token_classification(\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"aggregation_strategy\": aggregation_strategy,\n+            \"ignore_labels\": ignore_labels,\n+            \"stride\": stride,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(\n             json=payload,\n             model=model,\n@@ -2332,7 +2700,15 @@ def token_classification(\n         return TokenClassificationOutputElement.parse_obj_as_list(response)\n \n     def translation(\n-        self, text: str, *, model: Optional[str] = None, src_lang: Optional[str] = None, tgt_lang: Optional[str] = None\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        src_lang: Optional[str] = None,\n+        tgt_lang: Optional[str] = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        truncation: Optional[Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]] = None,\n+        generate_parameters: Optional[Dict[str, Any]] = None,\n     ) -> TranslationOutput:\n         \"\"\"\n         Convert text from one language to another.\n@@ -2341,7 +2717,6 @@ def translation(\n         your specific use case. Source and target languages usually depend on the model.\n         However, it is possible to specify source and target languages for certain models. If you are working with one of these models,\n         you can use `src_lang` and `tgt_lang` arguments to pass the relevant information.\n-        You can find this information in the model card.\n \n         Args:\n             text (`str`):\n@@ -2351,9 +2726,15 @@ def translation(\n                 a deployed Inference Endpoint. If not provided, the default recommended translation model will be used.\n                 Defaults to None.\n             src_lang (`str`, *optional*):\n-                Source language of the translation task, i.e. input language. Cannot be passed without `tgt_lang`.\n+                The source language of the text. Required for models that can translate from multiple languages.\n             tgt_lang (`str`, *optional*):\n-                Target language of the translation task, i.e. output language. Cannot be passed without `src_lang`.\n+                Target language to translate to. Required for models that can translate to multiple languages.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether to clean up the potential extra spaces in the text output.\n+            truncation (`Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]`, *optional*):\n+                The truncation strategy to use.\n+            generate_parameters (`Dict[str, Any]`, *optional*):\n+                Additional parametrization of the text generation algorithm.\n \n         Returns:\n             [`TranslationOutput`]: The generated translated text.\n@@ -2388,11 +2769,17 @@ def translation(\n \n         if src_lang is None and tgt_lang is not None:\n             raise ValueError(\"You cannot specify `tgt_lang` without specifying `src_lang`.\")\n-\n-        # If both `src_lang` and `tgt_lang` are given, pass them to the request body\n-        payload: Dict = {\"inputs\": text}\n-        if src_lang and tgt_lang:\n-            payload[\"parameters\"] = {\"src_lang\": src_lang, \"tgt_lang\": tgt_lang}\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"src_lang\": src_lang,\n+            \"tgt_lang\": tgt_lang,\n+            \"clean_up_tokenization_spaces\": clean_up_tokenization_spaces,\n+            \"truncation\": truncation,\n+            \"generate_parameters\": generate_parameters,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = self.post(json=payload, model=model, task=\"translation\")\n         return TranslationOutput.parse_obj_as_list(response)[0]\n \n@@ -2402,6 +2789,7 @@ def visual_question_answering(\n         question: str,\n         *,\n         model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n     ) -> List[VisualQuestionAnsweringOutputElement]:\n         \"\"\"\n         Answering open-ended questions based on an image.\n@@ -2415,7 +2803,10 @@ def visual_question_answering(\n                 The model to use for the visual question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended visual question answering model will be used.\n                 Defaults to None.\n-\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Note that we\n+                return less than topk answers if there are not enough options available within the\n+                context.\n         Returns:\n             `List[VisualQuestionAnsweringOutputElement]`: a list of [`VisualQuestionAnsweringOutputElement`] items containing the predicted label and associated probability.\n \n@@ -2440,6 +2831,8 @@ def visual_question_answering(\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"question\": question, \"image\": _b64_encode(image)}\n+        if top_k is not None:\n+            payload.setdefault(\"parameters\", {})[\"top_k\"] = top_k\n         response = self.post(json=payload, model=model, task=\"visual-question-answering\")\n         return VisualQuestionAnsweringOutputElement.parse_obj_as_list(response)\n \n@@ -2470,7 +2863,7 @@ def zero_shot_classification(\n                 The model then evaluates for both hypotheses if they are entailed in the provided `text` or not.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot classification model will be used.\n \n         Returns:\n             `List[ZeroShotClassificationOutputElement]`: List of [`ZeroShotClassificationOutputElement`] items containing the predicted labels and their confidence.\n@@ -2547,7 +2940,12 @@ def zero_shot_classification(\n         ]\n \n     def zero_shot_image_classification(\n-        self, image: ContentT, labels: List[str], *, model: Optional[str] = None\n+        self,\n+        image: ContentT,\n+        labels: List[str],\n+        *,\n+        model: Optional[str] = None,\n+        hypothesis_template: Optional[str] = None,\n     ) -> List[ZeroShotImageClassificationOutputElement]:\n         \"\"\"\n         Provide input image and text labels to predict text labels for the image.\n@@ -2559,8 +2957,10 @@ def zero_shot_image_classification(\n                 List of string possible labels. There must be at least 2 labels.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n-\n+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot image classification model will be used.\n+            hypothesis_template (`str`, *optional*):\n+                The sentence used in conjunction with `labels` to attempt the text classification by replacing the\n+                placeholder with the candidate labels.\n         Returns:\n             `List[ZeroShotImageClassificationOutputElement]`: List of [`ZeroShotImageClassificationOutputElement`] items containing the predicted labels and their confidence.\n \n@@ -2586,8 +2986,13 @@ def zero_shot_image_classification(\n         if len(labels) < 2:\n             raise ValueError(\"You must specify at least 2 classes to compare.\")\n \n+        payload = {\n+            \"inputs\": {\"image\": _b64_encode(image), \"candidateLabels\": \",\".join(labels)},\n+        }\n+        if hypothesis_template is not None:\n+            payload.setdefault(\"parameters\", {})[\"hypothesis_template\"] = hypothesis_template\n         response = self.post(\n-            json={\"image\": _b64_encode(image), \"parameters\": {\"candidate_labels\": \",\".join(labels)}},\n+            json=payload,\n             model=model,\n             task=\"zero-shot-image-classification\",\n         )\ndiff --git a/src/huggingface_hub/inference/_generated/_async_client.py b/src/huggingface_hub/inference/_generated/_async_client.py\nindex 83e5070b30..8a1384a671 100644\n--- a/src/huggingface_hub/inference/_generated/_async_client.py\n+++ b/src/huggingface_hub/inference/_generated/_async_client.py\n@@ -50,6 +50,7 @@\n )\n from huggingface_hub.inference._generated.types import (\n     AudioClassificationOutputElement,\n+    AudioClassificationOutputTransform,\n     AudioToAudioOutputElement,\n     AutomaticSpeechRecognitionOutput,\n     ChatCompletionInputGrammarType,\n@@ -67,9 +68,12 @@\n     SummarizationOutput,\n     TableQuestionAnsweringOutputElement,\n     TextClassificationOutputElement,\n+    TextClassificationOutputTransform,\n     TextGenerationInputGrammarType,\n     TextGenerationOutput,\n     TextGenerationStreamOutput,\n+    TextToImageTargetSize,\n+    TextToSpeechEarlyStoppingEnum,\n     TokenClassificationOutputElement,\n     ToolElement,\n     TranslationOutput,\n@@ -78,6 +82,7 @@\n     ZeroShotImageClassificationOutputElement,\n )\n from huggingface_hub.utils import build_hf_headers\n+from huggingface_hub.utils._deprecation import _deprecate_arguments\n \n from .._common import _async_yield_from, _import_aiohttp\n \n@@ -351,6 +356,8 @@ async def audio_classification(\n         audio: ContentT,\n         *,\n         model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n+        function_to_apply: Optional[\"AudioClassificationOutputTransform\"] = None,\n     ) -> List[AudioClassificationOutputElement]:\n         \"\"\"\n         Perform audio classification on the provided audio content.\n@@ -363,6 +370,10 @@ async def audio_classification(\n                 The model to use for audio classification. Can be a model ID hosted on the Hugging Face Hub\n                 or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for\n                 audio classification will be used.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n+            function_to_apply (`\"AudioClassificationOutputTransform\"`, *optional*):\n+                The function to apply to the output.\n \n         Returns:\n             `List[AudioClassificationOutputElement]`: List of [`AudioClassificationOutputElement`] items containing the predicted labels and their confidence.\n@@ -386,7 +397,19 @@ async def audio_classification(\n         ]\n         ```\n         \"\"\"\n-        response = await self.post(data=audio, model=model, task=\"audio-classification\")\n+        parameters = {\"function_to_apply\": function_to_apply, \"top_k\": top_k}\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, send audio as raw data\n+            data = audio\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # Or some parameters are provided -> send audio as base64 encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(audio)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, data=data, model=model, task=\"audio-classification\")\n         return AudioClassificationOutputElement.parse_obj_as_list(response)\n \n     async def audio_to_audio(\n@@ -945,6 +968,14 @@ async def document_question_answering(\n         question: str,\n         *,\n         model: Optional[str] = None,\n+        doc_stride: Optional[int] = None,\n+        handle_impossible_answer: Optional[bool] = None,\n+        lang: Optional[str] = None,\n+        max_answer_len: Optional[int] = None,\n+        max_question_len: Optional[int] = None,\n+        max_seq_len: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+        word_boxes: Optional[List[Union[List[float], str]]] = None,\n     ) -> List[DocumentQuestionAnsweringOutputElement]:\n         \"\"\"\n         Answer questions on document images.\n@@ -958,7 +989,29 @@ async def document_question_answering(\n                 The model to use for the document question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended document question answering model will be used.\n                 Defaults to None.\n-\n+            doc_stride (`int`, *optional*):\n+                If the words in the document are too long to fit with the question for the model, it will\n+                be split in several chunks with some overlap. This argument controls the size of that\n+                overlap.\n+            handle_impossible_answer (`bool`, *optional*):\n+                Whether to accept impossible as an answer.\n+            lang (`str`, *optional*):\n+                Language to use while running OCR.\n+            max_answer_len (`int`, *optional*):\n+                The maximum length of predicted answers (e.g., only answers with a shorter length are\n+                considered).\n+            max_question_len (`int`, *optional*):\n+                The maximum length of the question after tokenization. It will be truncated if needed.\n+            max_seq_len (`int`, *optional*):\n+                The maximum length of the total sentence (context + question) in tokens of each chunk\n+                passed to the model. The context will be split in several chunks (using doc_stride as\n+                overlap) if needed.\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Can return less\n+                than top_k answers if there are not enough options available within the context.\n+            word_boxes (`List[Union[List[float], str]]`, *optional*):\n+                A list of words and bounding boxes (normalized 0->1000). If provided, the inference will\n+                skip the OCR step and use the provided bounding boxes instead.\n         Returns:\n             `List[DocumentQuestionAnsweringOutputElement]`: a list of [`DocumentQuestionAnsweringOutputElement`] items containing the predicted label, associated probability, word ids, and page number.\n \n@@ -968,16 +1021,30 @@ async def document_question_answering(\n             `aiohttp.ClientResponseError`:\n                 If the request fails with an HTTP error status code other than HTTP 503.\n \n+\n         Example:\n         ```py\n         # Must be run in an async context\n         >>> from huggingface_hub import AsyncInferenceClient\n         >>> client = AsyncInferenceClient()\n         >>> await client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\n-        [DocumentQuestionAnsweringOutputElement(score=0.42515629529953003, answer='us-001', start=16, end=16)]\n+        [DocumentQuestionAnsweringOutputElement(answer='us-001', end=16, score=0.9999666213989258, start=16, words=None)]\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"question\": question, \"image\": _b64_encode(image)}\n+        parameters = {\n+            \"doc_stride\": doc_stride,\n+            \"handle_impossible_answer\": handle_impossible_answer,\n+            \"lang\": lang,\n+            \"max_answer_len\": max_answer_len,\n+            \"max_question_len\": max_question_len,\n+            \"max_seq_len\": max_seq_len,\n+            \"top_k\": top_k,\n+            \"word_boxes\": word_boxes,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(json=payload, model=model, task=\"document-question-answering\")\n         return DocumentQuestionAnsweringOutputElement.parse_obj_as_list(response)\n \n@@ -1002,7 +1069,7 @@ async def feature_extraction(\n                 a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.\n                 Defaults to None.\n             normalize (`bool`, *optional*):\n-                Whether to normalize the embeddings or not. Defaults to None.\n+                Whether to normalize the embeddings or not.\n                 Only available on server powered by Text-Embedding-Inference.\n             prompt_name (`str`, *optional*):\n                 The name of the prompt that should be used by for encoding. If not set, no prompt will be applied.\n@@ -1011,7 +1078,7 @@ async def feature_extraction(\n                 then the sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"\n                 because the prompt text will be prepended before any text to encode.\n             truncate (`bool`, *optional*):\n-                Whether to truncate the embeddings or not. Defaults to None.\n+                Whether to truncate the embeddings or not.\n                 Only available on server powered by Text-Embedding-Inference.\n             truncation_direction (`Literal[\"Left\", \"Right\"]`, *optional*):\n                 Which side of the input should be truncated when `truncate=True` is passed.\n@@ -1038,19 +1105,27 @@ async def feature_extraction(\n         ```\n         \"\"\"\n         payload: Dict = {\"inputs\": text}\n-        if normalize is not None:\n-            payload[\"normalize\"] = normalize\n-        if prompt_name is not None:\n-            payload[\"prompt_name\"] = prompt_name\n-        if truncate is not None:\n-            payload[\"truncate\"] = truncate\n-        if truncation_direction is not None:\n-            payload[\"truncation_direction\"] = truncation_direction\n+        parameters = {\n+            \"normalize\": normalize,\n+            \"prompt_name\": prompt_name,\n+            \"truncate\": truncate,\n+            \"truncation_direction\": truncation_direction,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(json=payload, model=model, task=\"feature-extraction\")\n         np = _import_numpy()\n         return np.array(_bytes_to_dict(response), dtype=\"float32\")\n \n-    async def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[FillMaskOutputElement]:\n+    async def fill_mask(\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        targets: Optional[List[str]] = None,\n+        top_k: Optional[int] = None,\n+    ) -> List[FillMaskOutputElement]:\n         \"\"\"\n         Fill in a hole with a missing word (token to be precise).\n \n@@ -1060,8 +1135,13 @@ async def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[Fil\n             model (`str`, *optional*):\n                 The model to use for the fill mask task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended fill mask model will be used.\n-                Defaults to None.\n-\n+            targets (`List[str]`, *optional*):\n+                When passed, the model will limit the scores to the passed targets instead of looking up\n+                in the whole vocabulary. If the provided targets are not in the model vocab, they will be\n+                tokenized and the first resulting token will be used (with a warning, and that might be\n+                slower).\n+            top_k (`int`, *optional*):\n+                When passed, overrides the number of predictions to return.\n         Returns:\n             `List[FillMaskOutputElement]`: a list of [`FillMaskOutputElement`] items containing the predicted label, associated\n             probability, token reference, and completed text.\n@@ -1084,7 +1164,12 @@ async def fill_mask(self, text: str, *, model: Optional[str] = None) -> List[Fil\n         ]\n         ```\n         \"\"\"\n-        response = await self.post(json={\"inputs\": text}, model=model, task=\"fill-mask\")\n+        payload: Dict = {\"inputs\": text}\n+        parameters = {\"targets\": targets, \"top_k\": top_k}\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, model=model, task=\"fill-mask\")\n         return FillMaskOutputElement.parse_obj_as_list(response)\n \n     async def image_classification(\n@@ -1092,6 +1177,8 @@ async def image_classification(\n         image: ContentT,\n         *,\n         model: Optional[str] = None,\n+        function_to_apply: Optional[Literal[\"sigmoid\", \"softmax\", \"none\"]] = None,\n+        top_k: Optional[int] = None,\n     ) -> List[ImageClassificationOutputElement]:\n         \"\"\"\n         Perform image classification on the given image using the specified model.\n@@ -1102,7 +1189,10 @@ async def image_classification(\n             model (`str`, *optional*):\n                 The model to use for image classification. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for image classification will be used.\n-\n+            function_to_apply (`Literal[\"sigmoid\", \"softmax\", \"none\"]`, *optional*):\n+                The function to apply to the output scores.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n         Returns:\n             `List[ImageClassificationOutputElement]`: a list of [`ImageClassificationOutputElement`] items containing the predicted label and associated probability.\n \n@@ -1118,10 +1208,23 @@ async def image_classification(\n         >>> from huggingface_hub import AsyncInferenceClient\n         >>> client = AsyncInferenceClient()\n         >>> await client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n-        [ImageClassificationOutputElement(score=0.9779096841812134, label='Blenheim spaniel'), ...]\n+        [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]\n         ```\n         \"\"\"\n-        response = await self.post(data=image, model=model, task=\"image-classification\")\n+        parameters = {\"function_to_apply\": function_to_apply, \"top_k\": top_k}\n+\n+        if all(parameter is None for parameter in parameters.values()):\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+\n+        else:\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+\n+        response = await self.post(json=payload, data=data, model=model, task=\"image-classification\")\n         return ImageClassificationOutputElement.parse_obj_as_list(response)\n \n     async def image_segmentation(\n@@ -1129,6 +1232,10 @@ async def image_segmentation(\n         image: ContentT,\n         *,\n         model: Optional[str] = None,\n+        mask_threshold: Optional[float] = None,\n+        overlap_mask_area_threshold: Optional[float] = None,\n+        subtask: Optional[Literal[\"instance\", \"panoptic\", \"semantic\"]] = None,\n+        threshold: Optional[float] = None,\n     ) -> List[ImageSegmentationOutputElement]:\n         \"\"\"\n         Perform image segmentation on the given image using the specified model.\n@@ -1145,7 +1252,14 @@ async def image_segmentation(\n             model (`str`, *optional*):\n                 The model to use for image segmentation. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for image segmentation will be used.\n-\n+            mask_threshold (`float`, *optional*):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*):\n+                Mask overlap threshold to eliminate small, disconnected segments.\n+            subtask (`Literal[\"instance\", \"panoptic\", \"semantic\"]`, *optional*):\n+                Segmentation task to be performed, depending on model capabilities.\n+            threshold (`float`, *optional*):\n+                Probability threshold to filter out predicted masks.\n         Returns:\n             `List[ImageSegmentationOutputElement]`: A list of [`ImageSegmentationOutputElement`] items containing the segmented masks and associated attributes.\n \n@@ -1160,11 +1274,28 @@ async def image_segmentation(\n         # Must be run in an async context\n         >>> from huggingface_hub import AsyncInferenceClient\n         >>> client = AsyncInferenceClient()\n-        >>> await client.image_segmentation(\"cat.jpg\"):\n+        >>> await client.image_segmentation(\"cat.jpg\")\n         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]\n         ```\n         \"\"\"\n-        response = await self.post(data=image, model=model, task=\"image-segmentation\")\n+        parameters = {\n+            \"mask_threshold\": mask_threshold,\n+            \"overlap_mask_area_threshold\": overlap_mask_area_threshold,\n+            \"subtask\": subtask,\n+            \"threshold\": threshold,\n+        }\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, the image can be raw bytes, an image file, or URL to an online image\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # if parameters are provided, the image needs to be a base64-encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, data=data, model=model, task=\"image-segmentation\")\n         output = ImageSegmentationOutputElement.parse_obj_as_list(response)\n         for item in output:\n             item.mask = _b64_to_image(item.mask)  # type: ignore [assignment]\n@@ -1245,7 +1376,7 @@ async def image_to_image(\n             data = image\n             payload: Optional[Dict[str, Any]] = None\n         else:\n-            # Or an image + some parameters => use base64 encoding\n+            # if parameters are provided, the image needs to be a base64-encoded string\n             data = None\n             payload = {\"inputs\": _b64_encode(image)}\n             for key, value in parameters.items():\n@@ -1383,10 +1514,7 @@ async def _fetch_framework(framework: str) -> None:\n         return models_by_task\n \n     async def object_detection(\n-        self,\n-        image: ContentT,\n-        *,\n-        model: Optional[str] = None,\n+        self, image: ContentT, *, model: Optional[str] = None, threshold: Optional[float] = None\n     ) -> List[ObjectDetectionOutputElement]:\n         \"\"\"\n         Perform object detection on the given image using the specified model.\n@@ -1403,7 +1531,8 @@ async def object_detection(\n             model (`str`, *optional*):\n                 The model to use for object detection. Can be a model ID hosted on the Hugging Face Hub or a URL to a\n                 deployed Inference Endpoint. If not provided, the default recommended model for object detection (DETR) will be used.\n-\n+            threshold (`float`, *optional*):\n+                The probability necessary to make a prediction.\n         Returns:\n             `List[ObjectDetectionOutputElement]`: A list of [`ObjectDetectionOutputElement`] items containing the bounding boxes and associated attributes.\n \n@@ -1424,13 +1553,37 @@ async def object_detection(\n         [ObjectDetectionOutputElement(score=0.9486683011054993, label='person', box=ObjectDetectionBoundingBox(xmin=59, ymin=39, xmax=420, ymax=510)), ...]\n         ```\n         \"\"\"\n-        # detect objects\n-        response = await self.post(data=image, model=model, task=\"object-detection\")\n+        parameters = {\n+            \"threshold\": threshold,\n+        }\n+        if all(parameter is None for parameter in parameters.values()):\n+            # if no parameters are provided, the image can be raw bytes, an image file, or URL to an online image\n+            data = image\n+            payload: Optional[Dict[str, Any]] = None\n+        else:\n+            # if parameters are provided, the image needs to be a base64-encoded string\n+            data = None\n+            payload = {\"inputs\": _b64_encode(image)}\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, data=data, model=model, task=\"object-detection\")\n         return ObjectDetectionOutputElement.parse_obj_as_list(response)\n \n     async def question_answering(\n-        self, question: str, context: str, *, model: Optional[str] = None\n-    ) -> QuestionAnsweringOutputElement:\n+        self,\n+        question: str,\n+        context: str,\n+        *,\n+        model: Optional[str] = None,\n+        align_to_words: Optional[bool] = None,\n+        doc_stride: Optional[int] = None,\n+        handle_impossible_answer: Optional[bool] = None,\n+        max_answer_len: Optional[int] = None,\n+        max_question_len: Optional[int] = None,\n+        max_seq_len: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+    ) -> Union[QuestionAnsweringOutputElement, List[QuestionAnsweringOutputElement]]:\n         \"\"\"\n         Retrieve the answer to a question from a given text.\n \n@@ -1442,10 +1595,31 @@ async def question_answering(\n             model (`str`):\n                 The model to use for the question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint.\n-\n+            align_to_words (`bool`, *optional*):\n+                Attempts to align the answer to real words. Improves quality on space separated\n+                languages. Might hurt on non-space-separated languages (like Japanese or Chinese).\n+            doc_stride (`int`, *optional*):\n+                If the context is too long to fit with the question for the model, it will be split in\n+                several chunks with some overlap. This argument controls the size of that overlap.\n+            handle_impossible_answer (`bool`, *optional*):\n+                Whether to accept impossible as an answer.\n+            max_answer_len (`int`, *optional*):\n+                The maximum length of predicted answers (e.g., only answers with a shorter length are\n+                considered).\n+            max_question_len (`int`, *optional*):\n+                The maximum length of the question after tokenization. It will be truncated if needed.\n+            max_seq_len (`int`, *optional*):\n+                The maximum length of the total sentence (context + question) in tokens of each chunk\n+                passed to the model. The context will be split in several chunks (using docStride as\n+                overlap) if needed.\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Note that we\n+                return less than topk answers if there are not enough options available within the\n+                context.\n         Returns:\n-            [`QuestionAnsweringOutputElement`]: an question answering output containing the score, start index, end index, and answer.\n-\n+            Union[`QuestionAnsweringOutputElement`, List[`QuestionAnsweringOutputElement`]]:\n+                When top_k is 1 or not provided, it returns a single `QuestionAnsweringOutputElement`.\n+                When top_k is greater than 1, it returns a list of `QuestionAnsweringOutputElement`.\n         Raises:\n             [`InferenceTimeoutError`]:\n                 If the model is unavailable or the request times out.\n@@ -1458,17 +1632,30 @@ async def question_answering(\n         >>> from huggingface_hub import AsyncInferenceClient\n         >>> client = AsyncInferenceClient()\n         >>> await client.question_answering(question=\"What's my name?\", context=\"My name is Clara and I live in Berkeley.\")\n-        QuestionAnsweringOutputElement(score=0.9326562285423279, start=11, end=16, answer='Clara')\n+        QuestionAnsweringOutputElement(answer='Clara', end=16, score=0.9326565265655518, start=11)\n         ```\n         \"\"\"\n-\n+        parameters = {\n+            \"align_to_words\": align_to_words,\n+            \"doc_stride\": doc_stride,\n+            \"handle_impossible_answer\": handle_impossible_answer,\n+            \"max_answer_len\": max_answer_len,\n+            \"max_question_len\": max_question_len,\n+            \"max_seq_len\": max_seq_len,\n+            \"top_k\": top_k,\n+        }\n         payload: Dict[str, Any] = {\"question\": question, \"context\": context}\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(\n             json=payload,\n             model=model,\n             task=\"question-answering\",\n         )\n-        return QuestionAnsweringOutputElement.parse_obj_as_instance(response)\n+        # Parse the response as a single `QuestionAnsweringOutputElement` when top_k is 1 or not provided, or a list of `QuestionAnsweringOutputElement` to ensure backward compatibility.\n+        output = QuestionAnsweringOutputElement.parse_obj(response)\n+        return output\n \n     async def sentence_similarity(\n         self, sentence: str, other_sentences: List[str], *, model: Optional[str] = None\n@@ -1518,12 +1705,23 @@ async def sentence_similarity(\n         )\n         return _bytes_to_list(response)\n \n+    @_deprecate_arguments(\n+        version=\"0.29\",\n+        deprecated_args=[\"parameters\"],\n+        custom_message=(\n+            \"The `parameters` argument is deprecated and will be removed in a future version. \"\n+            \"Provide individual parameters instead: `clean_up_tokenization_spaces`, `generate_parameters`, and `truncation`.\"\n+        ),\n+    )\n     async def summarization(\n         self,\n         text: str,\n         *,\n         parameters: Optional[Dict[str, Any]] = None,\n         model: Optional[str] = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        generate_parameters: Optional[Dict[str, Any]] = None,\n+        truncation: Optional[Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]] = None,\n     ) -> SummarizationOutput:\n         \"\"\"\n         Generate a summary of a given text using a specified model.\n@@ -1536,8 +1734,13 @@ async def summarization(\n                 for more details.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n-\n+                Inference Endpoint. If not provided, the default recommended model for summarization will be used.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether to clean up the potential extra spaces in the text output.\n+            generate_parameters (`Dict[str, Any]`, *optional*):\n+                Additional parametrization of the text generation algorithm.\n+            truncation (`Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]`, *optional*):\n+                The truncation strategy to use.\n         Returns:\n             [`SummarizationOutput`]: The generated summary text.\n \n@@ -1559,11 +1762,25 @@ async def summarization(\n         payload: Dict[str, Any] = {\"inputs\": text}\n         if parameters is not None:\n             payload[\"parameters\"] = parameters\n+        else:\n+            parameters = {\n+                \"clean_up_tokenization_spaces\": clean_up_tokenization_spaces,\n+                \"generate_parameters\": generate_parameters,\n+                \"truncation\": truncation,\n+            }\n+            for key, value in parameters.items():\n+                if value is not None:\n+                    payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(json=payload, model=model, task=\"summarization\")\n         return SummarizationOutput.parse_obj_as_list(response)[0]\n \n     async def table_question_answering(\n-        self, table: Dict[str, Any], query: str, *, model: Optional[str] = None\n+        self,\n+        table: Dict[str, Any],\n+        query: str,\n+        *,\n+        model: Optional[str] = None,\n+        parameters: Optional[Dict[str, Any]] = None,\n     ) -> TableQuestionAnsweringOutputElement:\n         \"\"\"\n         Retrieve the answer to a question from information given in a table.\n@@ -1577,6 +1794,8 @@ async def table_question_answering(\n             model (`str`):\n                 The model to use for the table-question-answering task. Can be a model ID hosted on the Hugging Face\n                 Hub or a URL to a deployed Inference Endpoint.\n+            parameters (`Dict[str, Any]`, *optional*):\n+                Additional inference parameters. Defaults to None.\n \n         Returns:\n             [`TableQuestionAnsweringOutputElement`]: a table question answering output containing the answer, coordinates, cells and the aggregator used.\n@@ -1598,11 +1817,15 @@ async def table_question_answering(\n         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')\n         ```\n         \"\"\"\n+        payload: Dict[str, Any] = {\n+            \"query\": query,\n+            \"table\": table,\n+        }\n+\n+        if parameters is not None:\n+            payload[\"parameters\"] = parameters\n         response = await self.post(\n-            json={\n-                \"query\": query,\n-                \"table\": table,\n-            },\n+            json=payload,\n             model=model,\n             task=\"table-question-answering\",\n         )\n@@ -1696,7 +1919,12 @@ async def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str\n         return _bytes_to_list(response)\n \n     async def text_classification(\n-        self, text: str, *, model: Optional[str] = None\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n+        function_to_apply: Optional[\"TextClassificationOutputTransform\"] = None,\n     ) -> List[TextClassificationOutputElement]:\n         \"\"\"\n         Perform text classification (e.g. sentiment-analysis) on the given text.\n@@ -1708,6 +1936,10 @@ async def text_classification(\n                 The model to use for the text classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended text classification model will be used.\n                 Defaults to None.\n+            top_k (`int`, *optional*):\n+                When specified, limits the output to the top K most probable classes.\n+            function_to_apply (`\"TextClassificationOutputTransform\"`, *optional*):\n+                The function to apply to the output.\n \n         Returns:\n             `List[TextClassificationOutputElement]`: a list of [`TextClassificationOutputElement`] items containing the predicted label and associated probability.\n@@ -1730,7 +1962,15 @@ async def text_classification(\n         ]\n         ```\n         \"\"\"\n-        response = await self.post(json={\"inputs\": text}, model=model, task=\"text-classification\")\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"function_to_apply\": function_to_apply,\n+            \"top_k\": top_k,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, model=model, task=\"text-classification\")\n         return TextClassificationOutputElement.parse_obj_as_list(response)[0]  # type: ignore [return-value]\n \n     @overload\n@@ -2240,6 +2480,9 @@ async def text_to_image(\n         num_inference_steps: Optional[float] = None,\n         guidance_scale: Optional[float] = None,\n         model: Optional[str] = None,\n+        scheduler: Optional[str] = None,\n+        target_size: Optional[TextToImageTargetSize] = None,\n+        seed: Optional[int] = None,\n         **kwargs,\n     ) -> \"Image\":\n         \"\"\"\n@@ -2268,7 +2511,14 @@ async def text_to_image(\n                 usually at the expense of lower image quality.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. If not provided, the default recommended text-to-image model will be used.\n+                Defaults to None.\n+            scheduler (`str`, *optional*):\n+                Override the scheduler with a compatible one.\n+            target_size (`TextToImageTargetSize`, *optional*):\n+                The size in pixel of the output image\n+            seed (`int`, *optional*):\n+                Seed for the random number generator.\n \n         Returns:\n             `Image`: The generated image.\n@@ -2303,6 +2553,9 @@ async def text_to_image(\n             \"width\": width,\n             \"num_inference_steps\": num_inference_steps,\n             \"guidance_scale\": guidance_scale,\n+            \"scheduler\": scheduler,\n+            \"target_size\": target_size,\n+            \"seed\": seed,\n             **kwargs,\n         }\n         for key, value in parameters.items():\n@@ -2311,7 +2564,28 @@ async def text_to_image(\n         response = await self.post(json=payload, model=model, task=\"text-to-image\")\n         return _bytes_to_image(response)\n \n-    async def text_to_speech(self, text: str, *, model: Optional[str] = None) -> bytes:\n+    async def text_to_speech(\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        do_sample: Optional[bool] = None,\n+        early_stopping: Optional[Union[bool, \"TextToSpeechEarlyStoppingEnum\"]] = None,\n+        epsilon_cutoff: Optional[float] = None,\n+        eta_cutoff: Optional[float] = None,\n+        max_length: Optional[int] = None,\n+        max_new_tokens: Optional[int] = None,\n+        min_length: Optional[int] = None,\n+        min_new_tokens: Optional[int] = None,\n+        num_beam_groups: Optional[int] = None,\n+        num_beams: Optional[int] = None,\n+        penalty_alpha: Optional[float] = None,\n+        temperature: Optional[float] = None,\n+        top_k: Optional[int] = None,\n+        top_p: Optional[float] = None,\n+        typical_p: Optional[float] = None,\n+        use_cache: Optional[bool] = None,\n+    ) -> bytes:\n         \"\"\"\n         Synthesize an audio of a voice pronouncing a given text.\n \n@@ -2320,7 +2594,56 @@ async def text_to_speech(self, text: str, *, model: Optional[str] = None) -> byt\n                 The text to synthesize.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. If not provided, the default recommended text-to-speech model will be used.\n+                Defaults to None.\n+            do_sample (`bool`, *optional*):\n+                Whether to use sampling instead of greedy decoding when generating new tokens.\n+            early_stopping (`Union[bool, \"TextToSpeechEarlyStoppingEnum\"`, *optional*):\n+                Controls the stopping condition for beam-based methods.\n+            epsilon_cutoff (`float`, *optional*):\n+                If set to float strictly between 0 and 1, only tokens with a conditional probability\n+                greater than epsilon_cutoff will be sampled. In the paper, suggested values range from\n+                3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language\n+                Model Desmoothing](https://hf.co/papers/2210.15191) for more details.\n+            eta_cutoff (`float`, *optional*):\n+                Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to\n+                float strictly between 0 and 1, a token is only considered if it is greater than either\n+                eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter\n+                term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In\n+                the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.\n+                See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)\n+                for more details.\n+            max_length (`int`, *optional*):\n+                The maximum length (in tokens) of the generated text, including the input.\n+            max_new_tokens (`int`, *optional*):\n+                The maximum number of tokens to generate. Takes precedence over maxLength.\n+            min_length (`int`, *optional*):\n+                The minimum length (in tokens) of the generated text, including the input.\n+            min_new_tokens (`int`, *optional*):\n+                The minimum number of tokens to generate. Takes precedence over maxLength.\n+            num_beam_groups (`int`, *optional*):\n+                Number of groups to divide num_beams into in order to ensure diversity among different\n+                groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.\n+            num_beams (`int`, *optional*):\n+                Number of beams to use for beam search.\n+            penalty_alpha (`float`, *optional*):\n+                The value balances the model confidence and the degeneration penalty in contrastive\n+                search decoding.\n+            temperature (`float`, *optional*):\n+                The value used to modulate the next token probabilities.\n+            top_k (`int`, *optional*):\n+                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n+            top_p (`float`, *optional*):\n+                If set to float < 1, only the smallest set of most probable tokens with probabilities\n+                that add up to top_p or higher are kept for generation.\n+            typical_p (`float`, *optional*):\n+                Local typicality measures how similar the conditional probability of predicting a target token next is\n+                to the expected conditional probability of predicting a random token next, given the partial text\n+                already generated. If set to float < 1, the smallest set of the most locally typical tokens with\n+                probabilities that add up to typical_p or higher are kept for generation. See [this\n+                paper](https://hf.co/papers/2202.00666) for more details.\n+            use_cache (`bool`, *optional*):\n+                Whether the model should use the past last key/values attentions to speed up decoding\n \n         Returns:\n             `bytes`: The generated audio.\n@@ -2342,10 +2665,39 @@ async def text_to_speech(self, text: str, *, model: Optional[str] = None) -> byt\n         >>> Path(\"hello_world.flac\").write_bytes(audio)\n         ```\n         \"\"\"\n-        return await self.post(json={\"inputs\": text}, model=model, task=\"text-to-speech\")\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"do_sample\": do_sample,\n+            \"early_stopping\": early_stopping,\n+            \"epsilon_cutoff\": epsilon_cutoff,\n+            \"eta_cutoff\": eta_cutoff,\n+            \"max_length\": max_length,\n+            \"max_new_tokens\": max_new_tokens,\n+            \"min_length\": min_length,\n+            \"min_new_tokens\": min_new_tokens,\n+            \"num_beam_groups\": num_beam_groups,\n+            \"num_beams\": num_beams,\n+            \"penalty_alpha\": penalty_alpha,\n+            \"temperature\": temperature,\n+            \"top_k\": top_k,\n+            \"top_p\": top_p,\n+            \"typical_p\": typical_p,\n+            \"use_cache\": use_cache,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n+        response = await self.post(json=payload, model=model, task=\"text-to-speech\")\n+        return response\n \n     async def token_classification(\n-        self, text: str, *, model: Optional[str] = None\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        aggregation_strategy: Optional[Literal[\"none\", \"simple\", \"first\", \"average\", \"max\"]] = None,\n+        ignore_labels: Optional[List[str]] = None,\n+        stride: Optional[int] = None,\n     ) -> List[TokenClassificationOutputElement]:\n         \"\"\"\n         Perform token classification on the given text.\n@@ -2358,6 +2710,12 @@ async def token_classification(\n                 The model to use for the token classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended token classification model will be used.\n                 Defaults to None.\n+            aggregation_strategy (`Literal[\"none\", \"simple\", \"first\", \"average\", \"max\"]`, *optional*):\n+                The strategy used to fuse tokens based on model predictions.\n+            ignore_labels (`List[str]`, *optional*):\n+                A list of labels to ignore.\n+            stride (`int`, *optional*):\n+                The number of overlapping tokens between chunks when splitting the input text.\n \n         Returns:\n             `List[TokenClassificationOutputElement]`: List of [`TokenClassificationOutputElement`] items containing the entity group, confidence score, word, start and end index.\n@@ -2393,6 +2751,14 @@ async def token_classification(\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"aggregation_strategy\": aggregation_strategy,\n+            \"ignore_labels\": ignore_labels,\n+            \"stride\": stride,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(\n             json=payload,\n             model=model,\n@@ -2401,7 +2767,15 @@ async def token_classification(\n         return TokenClassificationOutputElement.parse_obj_as_list(response)\n \n     async def translation(\n-        self, text: str, *, model: Optional[str] = None, src_lang: Optional[str] = None, tgt_lang: Optional[str] = None\n+        self,\n+        text: str,\n+        *,\n+        model: Optional[str] = None,\n+        src_lang: Optional[str] = None,\n+        tgt_lang: Optional[str] = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        truncation: Optional[Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]] = None,\n+        generate_parameters: Optional[Dict[str, Any]] = None,\n     ) -> TranslationOutput:\n         \"\"\"\n         Convert text from one language to another.\n@@ -2410,7 +2784,6 @@ async def translation(\n         your specific use case. Source and target languages usually depend on the model.\n         However, it is possible to specify source and target languages for certain models. If you are working with one of these models,\n         you can use `src_lang` and `tgt_lang` arguments to pass the relevant information.\n-        You can find this information in the model card.\n \n         Args:\n             text (`str`):\n@@ -2420,9 +2793,15 @@ async def translation(\n                 a deployed Inference Endpoint. If not provided, the default recommended translation model will be used.\n                 Defaults to None.\n             src_lang (`str`, *optional*):\n-                Source language of the translation task, i.e. input language. Cannot be passed without `tgt_lang`.\n+                The source language of the text. Required for models that can translate from multiple languages.\n             tgt_lang (`str`, *optional*):\n-                Target language of the translation task, i.e. output language. Cannot be passed without `src_lang`.\n+                Target language to translate to. Required for models that can translate to multiple languages.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether to clean up the potential extra spaces in the text output.\n+            truncation (`Literal[\"do_not_truncate\", \"longest_first\", \"only_first\", \"only_second\"]`, *optional*):\n+                The truncation strategy to use.\n+            generate_parameters (`Dict[str, Any]`, *optional*):\n+                Additional parametrization of the text generation algorithm.\n \n         Returns:\n             [`TranslationOutput`]: The generated translated text.\n@@ -2458,11 +2837,17 @@ async def translation(\n \n         if src_lang is None and tgt_lang is not None:\n             raise ValueError(\"You cannot specify `tgt_lang` without specifying `src_lang`.\")\n-\n-        # If both `src_lang` and `tgt_lang` are given, pass them to the request body\n-        payload: Dict = {\"inputs\": text}\n-        if src_lang and tgt_lang:\n-            payload[\"parameters\"] = {\"src_lang\": src_lang, \"tgt_lang\": tgt_lang}\n+        payload: Dict[str, Any] = {\"inputs\": text}\n+        parameters = {\n+            \"src_lang\": src_lang,\n+            \"tgt_lang\": tgt_lang,\n+            \"clean_up_tokenization_spaces\": clean_up_tokenization_spaces,\n+            \"truncation\": truncation,\n+            \"generate_parameters\": generate_parameters,\n+        }\n+        for key, value in parameters.items():\n+            if value is not None:\n+                payload.setdefault(\"parameters\", {})[key] = value\n         response = await self.post(json=payload, model=model, task=\"translation\")\n         return TranslationOutput.parse_obj_as_list(response)[0]\n \n@@ -2472,6 +2857,7 @@ async def visual_question_answering(\n         question: str,\n         *,\n         model: Optional[str] = None,\n+        top_k: Optional[int] = None,\n     ) -> List[VisualQuestionAnsweringOutputElement]:\n         \"\"\"\n         Answering open-ended questions based on an image.\n@@ -2485,7 +2871,10 @@ async def visual_question_answering(\n                 The model to use for the visual question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to\n                 a deployed Inference Endpoint. If not provided, the default recommended visual question answering model will be used.\n                 Defaults to None.\n-\n+            top_k (`int`, *optional*):\n+                The number of answers to return (will be chosen by order of likelihood). Note that we\n+                return less than topk answers if there are not enough options available within the\n+                context.\n         Returns:\n             `List[VisualQuestionAnsweringOutputElement]`: a list of [`VisualQuestionAnsweringOutputElement`] items containing the predicted label and associated probability.\n \n@@ -2511,6 +2900,8 @@ async def visual_question_answering(\n         ```\n         \"\"\"\n         payload: Dict[str, Any] = {\"question\": question, \"image\": _b64_encode(image)}\n+        if top_k is not None:\n+            payload.setdefault(\"parameters\", {})[\"top_k\"] = top_k\n         response = await self.post(json=payload, model=model, task=\"visual-question-answering\")\n         return VisualQuestionAnsweringOutputElement.parse_obj_as_list(response)\n \n@@ -2541,7 +2932,7 @@ async def zero_shot_classification(\n                 The model then evaluates for both hypotheses if they are entailed in the provided `text` or not.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot classification model will be used.\n \n         Returns:\n             `List[ZeroShotClassificationOutputElement]`: List of [`ZeroShotClassificationOutputElement`] items containing the predicted labels and their confidence.\n@@ -2620,7 +3011,12 @@ async def zero_shot_classification(\n         ]\n \n     async def zero_shot_image_classification(\n-        self, image: ContentT, labels: List[str], *, model: Optional[str] = None\n+        self,\n+        image: ContentT,\n+        labels: List[str],\n+        *,\n+        model: Optional[str] = None,\n+        hypothesis_template: Optional[str] = None,\n     ) -> List[ZeroShotImageClassificationOutputElement]:\n         \"\"\"\n         Provide input image and text labels to predict text labels for the image.\n@@ -2632,8 +3028,10 @@ async def zero_shot_image_classification(\n                 List of string possible labels. There must be at least 2 labels.\n             model (`str`, *optional*):\n                 The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed\n-                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n-\n+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot image classification model will be used.\n+            hypothesis_template (`str`, *optional*):\n+                The sentence used in conjunction with `labels` to attempt the text classification by replacing the\n+                placeholder with the candidate labels.\n         Returns:\n             `List[ZeroShotImageClassificationOutputElement]`: List of [`ZeroShotImageClassificationOutputElement`] items containing the predicted labels and their confidence.\n \n@@ -2660,8 +3058,13 @@ async def zero_shot_image_classification(\n         if len(labels) < 2:\n             raise ValueError(\"You must specify at least 2 classes to compare.\")\n \n+        payload = {\n+            \"inputs\": {\"image\": _b64_encode(image), \"candidateLabels\": \",\".join(labels)},\n+        }\n+        if hypothesis_template is not None:\n+            payload.setdefault(\"parameters\", {})[\"hypothesis_template\"] = hypothesis_template\n         response = await self.post(\n-            json={\"image\": _b64_encode(image), \"parameters\": {\"candidate_labels\": \",\".join(labels)}},\n+            json=payload,\n             model=model,\n             task=\"zero-shot-image-classification\",\n         )\ndiff --git a/src/huggingface_hub/inference/_generated/types/__init__.py b/src/huggingface_hub/inference/_generated/types/__init__.py\nindex 057a491f46..caa46d05fc 100644\n--- a/src/huggingface_hub/inference/_generated/types/__init__.py\n+++ b/src/huggingface_hub/inference/_generated/types/__init__.py\n@@ -6,10 +6,12 @@\n from .audio_classification import (\n     AudioClassificationInput,\n     AudioClassificationOutputElement,\n+    AudioClassificationOutputTransform,\n     AudioClassificationParameters,\n )\n from .audio_to_audio import AudioToAudioInput, AudioToAudioOutputElement\n from .automatic_speech_recognition import (\n+    AutomaticSpeechRecognitionEarlyStoppingEnum,\n     AutomaticSpeechRecognitionGenerationParameters,\n     AutomaticSpeechRecognitionInput,\n     AutomaticSpeechRecognitionOutput,\n@@ -59,11 +61,18 @@\n from .image_classification import (\n     ImageClassificationInput,\n     ImageClassificationOutputElement,\n+    ImageClassificationOutputTransform,\n     ImageClassificationParameters,\n )\n from .image_segmentation import ImageSegmentationInput, ImageSegmentationOutputElement, ImageSegmentationParameters\n from .image_to_image import ImageToImageInput, ImageToImageOutput, ImageToImageParameters, ImageToImageTargetSize\n-from .image_to_text import ImageToTextGenerationParameters, ImageToTextInput, ImageToTextOutput, ImageToTextParameters\n+from .image_to_text import (\n+    ImageToTextEarlyStoppingEnum,\n+    ImageToTextGenerationParameters,\n+    ImageToTextInput,\n+    ImageToTextOutput,\n+    ImageToTextParameters,\n+)\n from .object_detection import (\n     ObjectDetectionBoundingBox,\n     ObjectDetectionInput,\n@@ -84,7 +93,12 @@\n     TableQuestionAnsweringOutputElement,\n )\n from .text2text_generation import Text2TextGenerationInput, Text2TextGenerationOutput, Text2TextGenerationParameters\n-from .text_classification import TextClassificationInput, TextClassificationOutputElement, TextClassificationParameters\n+from .text_classification import (\n+    TextClassificationInput,\n+    TextClassificationOutputElement,\n+    TextClassificationOutputTransform,\n+    TextClassificationParameters,\n+)\n from .text_generation import (\n     TextGenerationInput,\n     TextGenerationInputGenerateParameters,\n@@ -98,9 +112,16 @@\n     TextGenerationStreamOutputStreamDetails,\n     TextGenerationStreamOutputToken,\n )\n-from .text_to_audio import TextToAudioGenerationParameters, TextToAudioInput, TextToAudioOutput, TextToAudioParameters\n+from .text_to_audio import (\n+    TextToAudioEarlyStoppingEnum,\n+    TextToAudioGenerationParameters,\n+    TextToAudioInput,\n+    TextToAudioOutput,\n+    TextToAudioParameters,\n+)\n from .text_to_image import TextToImageInput, TextToImageOutput, TextToImageParameters, TextToImageTargetSize\n from .text_to_speech import (\n+    TextToSpeechEarlyStoppingEnum,\n     TextToSpeechGenerationParameters,\n     TextToSpeechInput,\n     TextToSpeechOutput,\n@@ -115,6 +136,7 @@\n from .video_classification import (\n     VideoClassificationInput,\n     VideoClassificationOutputElement,\n+    VideoClassificationOutputTransform,\n     VideoClassificationParameters,\n )\n from .visual_question_answering import (\ndiff --git a/src/huggingface_hub/inference/_generated/types/audio_classification.py b/src/huggingface_hub/inference/_generated/types/audio_classification.py\nindex f828c980cb..f02447e3a2 100644\n--- a/src/huggingface_hub/inference/_generated/types/audio_classification.py\n+++ b/src/huggingface_hub/inference/_generated/types/audio_classification.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-ClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n+AudioClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n \n \n @dataclass\n@@ -18,7 +18,8 @@ class AudioClassificationParameters(BaseInferenceType):\n     Additional inference parameters for Audio Classification\n     \"\"\"\n \n-    function_to_apply: Optional[\"ClassificationOutputTransform\"] = None\n+    function_to_apply: Optional[\"AudioClassificationOutputTransform\"] = None\n+    \"\"\"The function to apply to the output.\"\"\"\n     top_k: Optional[int] = None\n     \"\"\"When specified, limits the output to the top K most probable classes.\"\"\"\n \ndiff --git a/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py b/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py\nindex 29323bf2a9..cfd35cfcb0 100644\n--- a/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py\n+++ b/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-EarlyStoppingEnum = Literal[\"never\"]\n+AutomaticSpeechRecognitionEarlyStoppingEnum = Literal[\"never\"]\n \n \n @dataclass\n@@ -20,7 +20,7 @@ class AutomaticSpeechRecognitionGenerationParameters(BaseInferenceType):\n \n     do_sample: Optional[bool] = None\n     \"\"\"Whether to use sampling instead of greedy decoding when generating new tokens.\"\"\"\n-    early_stopping: Optional[Union[bool, \"EarlyStoppingEnum\"]] = None\n+    early_stopping: Optional[Union[bool, \"AutomaticSpeechRecognitionEarlyStoppingEnum\"]] = None\n     \"\"\"Controls the stopping condition for beam-based methods.\"\"\"\n     epsilon_cutoff: Optional[float] = None\n     \"\"\"If set to float strictly between 0 and 1, only tokens with a conditional probability\n@@ -40,11 +40,11 @@ class AutomaticSpeechRecognitionGenerationParameters(BaseInferenceType):\n     max_length: Optional[int] = None\n     \"\"\"The maximum length (in tokens) of the generated text, including the input.\"\"\"\n     max_new_tokens: Optional[int] = None\n-    \"\"\"The maximum number of tokens to generate. Takes precedence over maxLength.\"\"\"\n+    \"\"\"The maximum number of tokens to generate. Takes precedence over max_length.\"\"\"\n     min_length: Optional[int] = None\n     \"\"\"The minimum length (in tokens) of the generated text, including the input.\"\"\"\n     min_new_tokens: Optional[int] = None\n-    \"\"\"The minimum number of tokens to generate. Takes precedence over maxLength.\"\"\"\n+    \"\"\"The minimum number of tokens to generate. Takes precedence over min_length.\"\"\"\n     num_beam_groups: Optional[int] = None\n     \"\"\"Number of groups to divide num_beams into in order to ensure diversity among different\n     groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.\ndiff --git a/src/huggingface_hub/inference/_generated/types/image_classification.py b/src/huggingface_hub/inference/_generated/types/image_classification.py\nindex 91b24d2c0b..3f47bb0acd 100644\n--- a/src/huggingface_hub/inference/_generated/types/image_classification.py\n+++ b/src/huggingface_hub/inference/_generated/types/image_classification.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-ClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n+ImageClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n \n \n @dataclass\n@@ -18,7 +18,8 @@ class ImageClassificationParameters(BaseInferenceType):\n     Additional inference parameters for Image Classification\n     \"\"\"\n \n-    function_to_apply: Optional[\"ClassificationOutputTransform\"] = None\n+    function_to_apply: Optional[\"ImageClassificationOutputTransform\"] = None\n+    \"\"\"The function to apply to the output.\"\"\"\n     top_k: Optional[int] = None\n     \"\"\"When specified, limits the output to the top K most probable classes.\"\"\"\n \ndiff --git a/src/huggingface_hub/inference/_generated/types/image_to_text.py b/src/huggingface_hub/inference/_generated/types/image_to_text.py\nindex 0ebb9a9bc6..0af33e89d5 100644\n--- a/src/huggingface_hub/inference/_generated/types/image_to_text.py\n+++ b/src/huggingface_hub/inference/_generated/types/image_to_text.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-EarlyStoppingEnum = Literal[\"never\"]\n+ImageToTextEarlyStoppingEnum = Literal[\"never\"]\n \n \n @dataclass\n@@ -20,7 +20,7 @@ class ImageToTextGenerationParameters(BaseInferenceType):\n \n     do_sample: Optional[bool] = None\n     \"\"\"Whether to use sampling instead of greedy decoding when generating new tokens.\"\"\"\n-    early_stopping: Optional[Union[bool, \"EarlyStoppingEnum\"]] = None\n+    early_stopping: Optional[Union[bool, \"ImageToTextEarlyStoppingEnum\"]] = None\n     \"\"\"Controls the stopping condition for beam-based methods.\"\"\"\n     epsilon_cutoff: Optional[float] = None\n     \"\"\"If set to float strictly between 0 and 1, only tokens with a conditional probability\n@@ -40,11 +40,11 @@ class ImageToTextGenerationParameters(BaseInferenceType):\n     max_length: Optional[int] = None\n     \"\"\"The maximum length (in tokens) of the generated text, including the input.\"\"\"\n     max_new_tokens: Optional[int] = None\n-    \"\"\"The maximum number of tokens to generate. Takes precedence over maxLength.\"\"\"\n+    \"\"\"The maximum number of tokens to generate. Takes precedence over max_length.\"\"\"\n     min_length: Optional[int] = None\n     \"\"\"The minimum length (in tokens) of the generated text, including the input.\"\"\"\n     min_new_tokens: Optional[int] = None\n-    \"\"\"The minimum number of tokens to generate. Takes precedence over maxLength.\"\"\"\n+    \"\"\"The minimum number of tokens to generate. Takes precedence over min_length.\"\"\"\n     num_beam_groups: Optional[int] = None\n     \"\"\"Number of groups to divide num_beams into in order to ensure diversity among different\n     groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.\ndiff --git a/src/huggingface_hub/inference/_generated/types/text_classification.py b/src/huggingface_hub/inference/_generated/types/text_classification.py\nindex bf61a4eebc..830fd6bbd1 100644\n--- a/src/huggingface_hub/inference/_generated/types/text_classification.py\n+++ b/src/huggingface_hub/inference/_generated/types/text_classification.py\n@@ -9,18 +9,23 @@\n from .base import BaseInferenceType\n \n \n-ClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n+TextClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n \n \n @dataclass\n class TextClassificationParameters(BaseInferenceType):\n-    \"\"\"Additional inference parameters\n-    Additional inference parameters for Text Classification\n+    \"\"\"\n+    Additional inference parameters for Text Classification.\n     \"\"\"\n \n-    function_to_apply: Optional[\"ClassificationOutputTransform\"] = None\n+    function_to_apply: Optional[\"TextClassificationOutputTransform\"] = None\n+    \"\"\"\n+    The function to apply to the output.\n+    \"\"\"\n     top_k: Optional[int] = None\n-    \"\"\"When specified, limits the output to the top K most probable classes.\"\"\"\n+    \"\"\"\n+    When specified, limits the output to the top K most probable classes.\n+    \"\"\"\n \n \n @dataclass\ndiff --git a/src/huggingface_hub/inference/_generated/types/text_to_audio.py b/src/huggingface_hub/inference/_generated/types/text_to_audio.py\nindex dd8369de4b..e9a26d0431 100644\n--- a/src/huggingface_hub/inference/_generated/types/text_to_audio.py\n+++ b/src/huggingface_hub/inference/_generated/types/text_to_audio.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-EarlyStoppingEnum = Literal[\"never\"]\n+TextToAudioEarlyStoppingEnum = Literal[\"never\"]\n \n \n @dataclass\n@@ -20,7 +20,7 @@ class TextToAudioGenerationParameters(BaseInferenceType):\n \n     do_sample: Optional[bool] = None\n     \"\"\"Whether to use sampling instead of greedy decoding when generating new tokens.\"\"\"\n-    early_stopping: Optional[Union[bool, \"EarlyStoppingEnum\"]] = None\n+    early_stopping: Optional[Union[bool, \"TextToAudioEarlyStoppingEnum\"]] = None\n     \"\"\"Controls the stopping condition for beam-based methods.\"\"\"\n     epsilon_cutoff: Optional[float] = None\n     \"\"\"If set to float strictly between 0 and 1, only tokens with a conditional probability\ndiff --git a/src/huggingface_hub/inference/_generated/types/text_to_speech.py b/src/huggingface_hub/inference/_generated/types/text_to_speech.py\nindex 30e0b1d7d8..fa96e885ee 100644\n--- a/src/huggingface_hub/inference/_generated/types/text_to_speech.py\n+++ b/src/huggingface_hub/inference/_generated/types/text_to_speech.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-EarlyStoppingEnum = Literal[\"never\"]\n+TextToSpeechEarlyStoppingEnum = Literal[\"never\"]\n \n \n @dataclass\n@@ -20,7 +20,7 @@ class TextToSpeechGenerationParameters(BaseInferenceType):\n \n     do_sample: Optional[bool] = None\n     \"\"\"Whether to use sampling instead of greedy decoding when generating new tokens.\"\"\"\n-    early_stopping: Optional[Union[bool, \"EarlyStoppingEnum\"]] = None\n+    early_stopping: Optional[Union[bool, \"TextToSpeechEarlyStoppingEnum\"]] = None\n     \"\"\"Controls the stopping condition for beam-based methods.\"\"\"\n     epsilon_cutoff: Optional[float] = None\n     \"\"\"If set to float strictly between 0 and 1, only tokens with a conditional probability\ndiff --git a/src/huggingface_hub/inference/_generated/types/video_classification.py b/src/huggingface_hub/inference/_generated/types/video_classification.py\nindex 0c5a9d55a8..a32249dc12 100644\n--- a/src/huggingface_hub/inference/_generated/types/video_classification.py\n+++ b/src/huggingface_hub/inference/_generated/types/video_classification.py\n@@ -9,7 +9,7 @@\n from .base import BaseInferenceType\n \n \n-ClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n+VideoClassificationOutputTransform = Literal[\"sigmoid\", \"softmax\", \"none\"]\n \n \n @dataclass\n@@ -20,7 +20,7 @@ class VideoClassificationParameters(BaseInferenceType):\n \n     frame_sampling_rate: Optional[int] = None\n     \"\"\"The sampling rate used to select frames from the video.\"\"\"\n-    function_to_apply: Optional[\"ClassificationOutputTransform\"] = None\n+    function_to_apply: Optional[\"VideoClassificationOutputTransform\"] = None\n     num_frames: Optional[int] = None\n     \"\"\"The number of sampled frames to consider for classification.\"\"\"\n     top_k: Optional[int] = None\ndiff --git a/utils/generate_async_inference_client.py b/utils/generate_async_inference_client.py\nindex 17818d1858..832049ad5d 100644\n--- a/utils/generate_async_inference_client.py\n+++ b/utils/generate_async_inference_client.py\n@@ -15,13 +15,11 @@\n \"\"\"Contains a tool to generate `src/huggingface_hub/inference/_generated/_async_client.py`.\"\"\"\n \n import argparse\n-import os\n import re\n-import tempfile\n from pathlib import Path\n from typing import NoReturn\n \n-from ruff.__main__ import find_ruff_bin\n+from helpers import format_source_code\n \n \n ASYNC_CLIENT_FILE_PATH = (\n@@ -77,17 +75,6 @@ def generate_async_client_code(code: str) -> str:\n     return code\n \n \n-def format_source_code(code: str) -> str:\n-    \"\"\"Apply formatter on a generated source code.\"\"\"\n-    with tempfile.TemporaryDirectory() as tmpdir:\n-        filepath = Path(tmpdir) / \"async_client.py\"\n-        filepath.write_text(code)\n-        ruff_bin = find_ruff_bin()\n-        os.spawnv(os.P_WAIT, ruff_bin, [\"ruff\", \"check\", str(filepath), \"--fix\", \"--quiet\"])\n-        os.spawnv(os.P_WAIT, ruff_bin, [\"ruff\", \"format\", str(filepath), \"--quiet\"])\n-        return filepath.read_text()\n-\n-\n def check_async_client(update: bool) -> NoReturn:\n     \"\"\"Check AsyncInferenceClient is correctly defined and consistent with InferenceClient.\n \ndiff --git a/utils/generate_inference_types.py b/utils/generate_inference_types.py\nindex fe5223426f..23ab7c7b9c 100644\n--- a/utils/generate_inference_types.py\n+++ b/utils/generate_inference_types.py\n@@ -15,14 +15,11 @@\n \"\"\"Contains a tool to generate `src/huggingface_hub/inference/_generated/types`.\"\"\"\n \n import argparse\n-import os\n import re\n-import tempfile\n from pathlib import Path\n from typing import Dict, List, Literal, NoReturn\n \n-from helpers import check_and_update_file_content\n-from ruff.__main__ import find_ruff_bin\n+from helpers import check_and_update_file_content, format_source_code\n \n \n huggingface_hub_folder_path = Path(__file__).parents[1] / \"src\" / \"huggingface_hub\"\n@@ -56,6 +53,16 @@\n     re.VERBOSE | re.MULTILINE,\n )\n \n+TYPE_ALIAS_REGEX = re.compile(\n+    r\"\"\"\n+    ^(?!\\s) # to make sure the line does not start with whitespace (top-level)\n+    (\\w+)\n+    \\s*=\\s*\n+    (.+)\n+    $\n+    \"\"\",\n+    re.VERBOSE | re.MULTILINE,\n+)\n OPTIONAL_FIELD_REGEX = re.compile(r\": Optional\\[(.+)\\]$\", re.MULTILINE)\n \n \n@@ -78,6 +85,7 @@\n     re.MULTILINE | re.VERBOSE | re.DOTALL,\n )\n \n+\n # List of classes that are shared across multiple modules\n # This is used to fix the naming of the classes (to make them unique by task)\n SHARED_CLASSES = [\n@@ -86,6 +94,7 @@\n     \"ClassificationOutput\",\n     \"GenerationParameters\",\n     \"TargetSize\",\n+    \"EarlyStoppingEnum\",\n ]\n \n REFERENCE_PACKAGE_EN_CONTENT = \"\"\"\n@@ -130,6 +139,27 @@\n \"\"\"\n \n \n+def _replace_class_name(content: str, cls: str, new_cls: str) -> str:\n+    \"\"\"\n+    Replace the class name `cls` with the new class name `new_cls` in the content.\n+    \"\"\"\n+    pattern = rf\"\"\"\n+        (?<![\\w'\"])\n+        (['\"]?)\n+        {cls}\n+        (['\"]?)\n+        (?![\\w'\"])\n+    \"\"\"\n+\n+    def replacement(m):\n+        quote_start = m.group(1) or \"\"\n+        quote_end = m.group(2) or \"\"\n+        return f\"{quote_start}{new_cls}{quote_end}\"\n+\n+    content = re.sub(pattern, replacement, content, flags=re.VERBOSE)\n+    return content\n+\n+\n def _inherit_from_base(content: str) -> str:\n     content = content.replace(\n         \"\\nfrom dataclasses import\", \"\\nfrom .base import BaseInferenceType\\nfrom dataclasses import\"\n@@ -144,8 +174,9 @@ def _delete_empty_lines(content: str) -> str:\n \n def _fix_naming_for_shared_classes(content: str, module_name: str) -> str:\n     for cls in SHARED_CLASSES:\n-        cls_definition = f\"\\nclass {cls}\"\n-\n+        # No need to fix the naming of a shared class if it's not used in the module\n+        if cls not in content:\n+            continue\n         # Update class definition\n         # Very hacky way to build \"AudioClassificationOutputElement\" instead of \"ClassificationOutput\"\n         new_cls = \"\".join(part.capitalize() for part in module_name.split(\"_\"))\n@@ -157,18 +188,8 @@ def _fix_naming_for_shared_classes(content: str, module_name: str) -> str:\n         if new_cls.endswith(\"ClassificationOutput\"):\n             # to get \"AudioClassificationOutputElement\"\n             new_cls += \"Element\"\n-        new_cls_definition = \"\\nclass \" + new_cls\n-        content = content.replace(cls_definition, new_cls_definition)\n-\n-        # Update regular class usage\n-        regular_cls = f\": {cls}\\n\"\n-        new_regular_cls = f\": {new_cls}\\n\"\n-        content = content.replace(regular_cls, new_regular_cls)\n-\n-        # Update optional class usage\n-        optional_cls = f\"Optional[{cls}]\"\n-        new_optional_cls = f\"Optional[{new_cls}]\"\n-        content = content.replace(optional_cls, new_optional_cls)\n+        content = _replace_class_name(content, cls, new_cls)\n+\n     return content\n \n \n@@ -201,6 +222,15 @@ def _list_dataclasses(content: str) -> List[str]:\n     return INHERITED_DATACLASS_REGEX.findall(content)\n \n \n+def _list_shared_aliases(content: str) -> List[str]:\n+    \"\"\"List all shared class aliases defined in the module.\"\"\"\n+    all_aliases = TYPE_ALIAS_REGEX.findall(content)\n+    shared_class_pattern = r\"(\\w+(?:\" + \"|\".join(re.escape(cls) for cls in SHARED_CLASSES) + r\"))$\"\n+    shared_class_regex = re.compile(shared_class_pattern)\n+    aliases = [alias_class for alias_class, _ in all_aliases if shared_class_regex.search(alias_class)]\n+    return aliases\n+\n+\n def fix_inference_classes(content: str, module_name: str) -> str:\n     content = _inherit_from_base(content)\n     content = _delete_empty_lines(content)\n@@ -227,17 +257,6 @@ def add_dataclasses_to_main_init(content: str, dataclasses: Dict[str, List[str]]\n     return MAIN_INIT_PY_REGEX.sub(f'\"inference._generated.types\": [{dataclasses_str}]', content)\n \n \n-def format_source_code(code: str) -> str:\n-    \"\"\"Apply formatter on the generated source code.\"\"\"\n-    with tempfile.TemporaryDirectory() as tmpdir:\n-        filepath = Path(tmpdir) / \"tmp.py\"\n-        filepath.write_text(code)\n-        ruff_bin = find_ruff_bin()\n-        os.spawnv(os.P_WAIT, ruff_bin, [\"ruff\", \"check\", str(filepath), \"--fix\", \"--quiet\"])\n-        os.spawnv(os.P_WAIT, ruff_bin, [\"ruff\", \"format\", str(filepath), \"--quiet\"])\n-        return filepath.read_text()\n-\n-\n def generate_reference_package(dataclasses: Dict[str, List[str]], language: Literal[\"en\", \"ko\"]) -> str:\n     \"\"\"Generate the reference package content.\"\"\"\n \n@@ -259,11 +278,12 @@ def generate_reference_package(dataclasses: Dict[str, List[str]], language: Lite\n \n \n def check_inference_types(update: bool) -> NoReturn:\n-    \"\"\"Check AsyncInferenceClient is correctly defined and consistent with InferenceClient.\n+    \"\"\"Check and update inference types.\n \n     This script is used in the `make style` and `make quality` checks.\n     \"\"\"\n     dataclasses = {}\n+    aliases = {}\n     for file in INFERENCE_TYPES_FOLDER_PATH.glob(\"*.py\"):\n         if file.name in IGNORE_FILES:\n             continue\n@@ -272,21 +292,20 @@ def check_inference_types(update: bool) -> NoReturn:\n \n         fixed_content = fix_inference_classes(content, module_name=file.stem)\n         formatted_content = format_source_code(fixed_content)\n-\n         dataclasses[file.stem] = _list_dataclasses(formatted_content)\n-\n+        aliases[file.stem] = _list_shared_aliases(formatted_content)\n         check_and_update_file_content(file, formatted_content, update)\n \n-    init_py_content = create_init_py(dataclasses)\n+    all_classes = {module: dataclasses[module] + aliases[module] for module in dataclasses.keys()}\n+    init_py_content = create_init_py(all_classes)\n     init_py_content = format_source_code(init_py_content)\n     init_py_file = INFERENCE_TYPES_FOLDER_PATH / \"__init__.py\"\n     check_and_update_file_content(init_py_file, init_py_content, update)\n \n     main_init_py_content = MAIN_INIT_PY_FILE.read_text()\n-    updated_main_init_py_content = add_dataclasses_to_main_init(main_init_py_content, dataclasses)\n+    updated_main_init_py_content = add_dataclasses_to_main_init(main_init_py_content, all_classes)\n     updated_main_init_py_content = format_source_code(updated_main_init_py_content)\n     check_and_update_file_content(MAIN_INIT_PY_FILE, updated_main_init_py_content, update)\n-\n     reference_package_content_en = generate_reference_package(dataclasses, \"en\")\n     check_and_update_file_content(REFERENCE_PACKAGE_EN_PATH, reference_package_content_en, update)\n \ndiff --git a/utils/generate_task_parameters.py b/utils/generate_task_parameters.py\nnew file mode 100644\nindex 0000000000..fa80c2f7a0\n--- /dev/null\n+++ b/utils/generate_task_parameters.py\n@@ -0,0 +1,548 @@\n+# coding=utf-8\n+# Copyright 2024-present, the HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Utility script to check and update the InferenceClient task methods arguments and docstrings\n+based on the tasks input parameters.\n+\n+What this script does:\n+- [x] detect missing parameters in method signature\n+- [x] add missing parameters to methods signature\n+- [ ] detect outdated parameters in method signature\n+- [ ] update outdated parameters in method signature\n+\n+- [x] detect missing parameters in method docstrings\n+- [x] add missing parameters to methods docstrings\n+- [ ] detect outdated parameters in method docstrings\n+- [ ] update outdated parameters in method docstrings\n+\n+- [ ] detect when parameter not used in method implementation\n+- [ ] update method implementation when parameter not used\n+Related resources:\n+- https://github.com/huggingface/huggingface_hub/issues/2063\n+- https://github.com/huggingface/huggingface_hub/issues/2557\n+- https://github.com/huggingface/huggingface_hub/pull/2561\n+\"\"\"\n+\n+import argparse\n+import builtins\n+import inspect\n+import re\n+import textwrap\n+from collections import defaultdict\n+from pathlib import Path\n+from typing import Dict, List, NoReturn, Optional, Set\n+\n+import libcst as cst\n+from helpers import format_source_code\n+from libcst.codemod import CodemodContext\n+from libcst.codemod.visitors import GatherImportsVisitor\n+\n+from huggingface_hub.inference._client import InferenceClient\n+\n+\n+# Paths to project files\n+BASE_DIR = Path(__file__).parents[1] / \"src\" / \"huggingface_hub\"\n+INFERENCE_TYPES_PATH = BASE_DIR / \"inference\" / \"_generated\" / \"types\"\n+INFERENCE_CLIENT_FILE = BASE_DIR / \"inference\" / \"_client.py\"\n+\n+DEFAULT_MODULE = \"huggingface_hub.inference._generated.types\"\n+\n+\n+# Temporary solution to skip tasks where there is no Parameters dataclass or the schema needs to be updated\n+TASKS_TO_SKIP = [\n+    \"chat_completion\",\n+    \"depth_estimation\",\n+    \"audio_to_audio\",\n+    \"feature_extraction\",\n+    \"sentence_similarity\",\n+    \"table_question_answering\",\n+    \"automatic_speech_recognition\",\n+    \"image_to_text\",\n+    \"image_to_image\",\n+]\n+\n+PARAMETERS_DATACLASS_REGEX = re.compile(\n+    r\"\"\"\n+    ^@dataclass\n+    \\nclass\\s(\\w+Parameters)\\(BaseInferenceType\\):\n+    \"\"\",\n+    re.VERBOSE | re.MULTILINE,\n+)\n+\n+#### NODE VISITORS\n+\n+\n+class DataclassFieldCollector(cst.CSTVisitor):\n+    \"\"\"A visitor that collects fields (parameters) from a dataclass.\"\"\"\n+\n+    def __init__(self, dataclass_name: str):\n+        self.dataclass_name = dataclass_name\n+        self.parameters: Dict[str, Dict[str, str]] = {}\n+\n+    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n+        \"\"\"Visit class definitions to find the target dataclass.\"\"\"\n+\n+        if node.name.value == self.dataclass_name:\n+            body_statements = node.body.body\n+            for index, field in enumerate(body_statements):\n+                # Check if the statement is a simple statement (like a variable declaration)\n+                if isinstance(field, cst.SimpleStatementLine):\n+                    for stmt in field.body:\n+                        # Check if it's an annotated assignment (typical for dataclass fields)\n+                        if isinstance(stmt, cst.AnnAssign) and isinstance(stmt.target, cst.Name):\n+                            param_name = stmt.target.value\n+                            param_type = cst.Module([]).code_for_node(stmt.annotation.annotation)\n+                            docstring = self._extract_docstring(body_statements, index)\n+                            self.parameters[param_name] = {\n+                                \"type\": param_type,\n+                                \"docstring\": docstring,\n+                            }\n+\n+    @staticmethod\n+    def _extract_docstring(body_statements: List[cst.CSTNode], field_index: int) -> str:\n+        \"\"\"Extract the docstring following a field definition.\"\"\"\n+        if field_index + 1 < len(body_statements):\n+            # Check if the next statement is a simple statement (like a string)\n+            next_stmt = body_statements[field_index + 1]\n+            if isinstance(next_stmt, cst.SimpleStatementLine):\n+                for stmt in next_stmt.body:\n+                    # Check if the statement is a string expression (potential docstring)\n+                    if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n+                        return stmt.value.evaluated_value.strip()\n+        # No docstring found or there's no statement after the field\n+        return \"\"\n+\n+\n+class ModulesCollector(cst.CSTVisitor):\n+    \"\"\"Visitor that maps type names to their defining modules.\"\"\"\n+\n+    def __init__(self):\n+        self.type_to_module = {}\n+\n+    def visit_ClassDef(self, node: cst.ClassDef):\n+        \"\"\"Map class definitions to the current module.\"\"\"\n+        self.type_to_module[node.name.value] = DEFAULT_MODULE\n+\n+    def visit_ImportFrom(self, node: cst.ImportFrom):\n+        \"\"\"Map imported types to their modules.\"\"\"\n+        if node.module:\n+            module_name = node.module.value\n+            for alias in node.names:\n+                self.type_to_module[alias.name.value] = module_name\n+\n+\n+class ArgumentsCollector(cst.CSTVisitor):\n+    \"\"\"Collects existing argument names from a method.\"\"\"\n+\n+    def __init__(self, method_name: str):\n+        self.method_name = method_name\n+        self.existing_args: Set[str] = set()\n+\n+    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n+        if node.name.value == self.method_name:\n+            self.existing_args.update(\n+                param.name.value\n+                for param in node.params.params + node.params.kwonly_params\n+                if param.name.value != \"self\"\n+            )\n+\n+\n+#### TREE TRANSFORMERS\n+\n+\n+class AddParameters(cst.CSTTransformer):\n+    \"\"\"Updates a method by adding missing parameters and updating the docstring.\"\"\"\n+\n+    def __init__(self, method_name: str, missing_params: Dict[str, Dict[str, str]]):\n+        self.method_name = method_name\n+        self.missing_params = missing_params\n+        self.found_method = False\n+\n+    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n+        if original_node.name.value == self.method_name:\n+            self.found_method = True\n+            new_params = self._update_parameters(updated_node.params)\n+            updated_body = self._update_docstring(updated_node.body)\n+            return updated_node.with_changes(params=new_params, body=updated_body)\n+        return updated_node\n+\n+    def _update_parameters(self, params: cst.Parameters) -> cst.Parameters:\n+        new_kwonly_params = list(params.kwonly_params)\n+        existing_args = {param.name.value for param in params.params + params.kwonly_params}\n+\n+        for param_name, param_info in self.missing_params.items():\n+            if param_name not in existing_args:\n+                annotation = cst.Annotation(annotation=cst.parse_expression(param_info[\"type\"]))\n+                new_param = cst.Param(\n+                    name=cst.Name(param_name),\n+                    annotation=annotation,\n+                    default=cst.Name(\"None\"),\n+                )\n+                new_kwonly_params.append(new_param)\n+\n+        return params.with_changes(kwonly_params=new_kwonly_params)\n+\n+    def _update_docstring(self, body: cst.IndentedBlock) -> cst.IndentedBlock:\n+        if not isinstance(body.body[0], cst.SimpleStatementLine) or not isinstance(body.body[0].body[0], cst.Expr):\n+            return body\n+\n+        docstring_expr = body.body[0].body[0]\n+        if not isinstance(docstring_expr.value, cst.SimpleString):\n+            return body\n+\n+        docstring = docstring_expr.value.evaluated_value\n+        updated_docstring = self._update_docstring_content(docstring)\n+        new_docstring = cst.SimpleString(f'\"\"\"{updated_docstring}\"\"\"')\n+        new_body = [body.body[0].with_changes(body=[docstring_expr.with_changes(value=new_docstring)])] + list(\n+            body.body[1:]\n+        )\n+        return body.with_changes(body=new_body)\n+\n+    def _update_docstring_content(self, docstring: str) -> str:\n+        docstring_lines = docstring.split(\"\\n\")\n+\n+        # Step 1: find the right insertion index\n+        args_index = next((i for i, line in enumerate(docstring_lines) if line.strip().lower() == \"args:\"), None)\n+        # If there is no \"Args:\" section, insert it after the first section that is not empty and not a sub-section\n+        if args_index is None:\n+            insertion_index = next(\n+                (\n+                    i\n+                    for i, line in enumerate(docstring_lines)\n+                    if line.strip().lower() in (\"returns:\", \"raises:\", \"examples:\", \"example:\")\n+                ),\n+                len(docstring_lines),\n+            )\n+            docstring_lines.insert(insertion_index, \"Args:\")\n+            args_index = insertion_index\n+            insertion_index += 1\n+        else:\n+            # Find the next section (in this order: Returns, Raises, Example(s))\n+            next_section_index = next(\n+                (\n+                    i\n+                    for i, line in enumerate(docstring_lines)\n+                    if line.strip().lower() in (\"returns:\", \"raises:\", \"example:\", \"examples:\")\n+                ),\n+                None,\n+            )\n+            if next_section_index is not None:\n+                # If there's a blank line before \"Returns:\", insert before that blank line\n+                if next_section_index > 0 and docstring_lines[next_section_index - 1].strip() == \"\":\n+                    insertion_index = next_section_index - 1\n+                else:\n+                    # If there's no blank line, insert at the \"Returns:\" line and add a blank line after insertion\n+                    insertion_index = next_section_index\n+                    docstring_lines.insert(insertion_index, \"\")\n+            else:\n+                # If there's no next section, insert at the end\n+                insertion_index = len(docstring_lines)\n+\n+        # Step 2: format the parameter docstring\n+        # Calculate the base indentation\n+        base_indentation = docstring_lines[args_index][\n+            : len(docstring_lines[args_index]) - len(docstring_lines[args_index].lstrip())\n+        ]\n+        param_indentation = base_indentation + \"    \"  # Indent parameters under \"Args:\"\n+        description_indentation = param_indentation + \"    \"  # Indent descriptions under parameter names\n+\n+        param_docs = []\n+        for param_name, param_info in self.missing_params.items():\n+            param_type_str = param_info[\"type\"].replace(\"Optional[\", \"\").rstrip(\"]\")\n+            optional_str = \"*optional*\" if \"Optional[\" in param_info[\"type\"] else \"\"\n+            param_docstring = (param_info.get(\"docstring\") or \"\").strip()\n+\n+            # Clean up the docstring to remove extra spaces\n+            param_docstring = \" \".join(param_docstring.split())\n+\n+            # Prepare the parameter line\n+            param_line = f\"{param_indentation}{param_name} (`{param_type_str}`, {optional_str}):\"\n+\n+            # Wrap the parameter docstring\n+            wrapped_description = textwrap.fill(\n+                param_docstring,\n+                width=119,\n+                initial_indent=description_indentation,\n+                subsequent_indent=description_indentation,\n+            )\n+\n+            # Combine parameter line and description\n+            if param_docstring:\n+                param_doc = f\"{param_line}\\n{wrapped_description}\"\n+            else:\n+                param_doc = param_line\n+\n+            param_docs.append(param_doc)\n+\n+        # Step 3: insert the new parameter docs into the docstring\n+        docstring_lines[insertion_index:insertion_index] = param_docs\n+        return \"\\n\".join(docstring_lines)\n+\n+\n+class AddImports(cst.CSTTransformer):\n+    \"\"\"Transformer that adds import statements to the module.\"\"\"\n+\n+    def __init__(self, imports_to_add: List[cst.BaseStatement]):\n+        self.imports_to_add = imports_to_add\n+        self.added = False\n+\n+    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n+        \"\"\"Insert the import statements into the module.\"\"\"\n+        # If imports were already added, don't add them again\n+        if self.added:\n+            return updated_node\n+        insertion_index = 0\n+        # Find the index where to insert the imports: make sure the imports are inserted before any code and after all imports (not necessary, we can remove/simplify this part)\n+        for idx, stmt in enumerate(updated_node.body):\n+            if not isinstance(stmt, cst.SimpleStatementLine):\n+                insertion_index = idx\n+                break\n+            elif not isinstance(stmt.body[0], (cst.Import, cst.ImportFrom)):\n+                insertion_index = idx\n+                break\n+        # Insert the imports\n+        new_body = (\n+            list(updated_node.body[:insertion_index])\n+            + list(self.imports_to_add)\n+            + list(updated_node.body[insertion_index:])\n+        )\n+        self.added = True\n+        return updated_node.with_changes(body=new_body)\n+\n+\n+#### UTILS\n+\n+\n+def check_missing_parameters(\n+    inference_client_module: cst.Module,\n+    parameters_module: cst.Module,\n+    method_name: str,\n+    parameter_type_name: str,\n+) -> Dict[str, Dict[str, str]]:\n+    # Get parameters from the parameters module\n+    params_collector = DataclassFieldCollector(parameter_type_name)\n+    parameters_module.visit(params_collector)\n+    parameters = params_collector.parameters\n+\n+    # Get existing arguments from the method\n+    method_argument_collector = ArgumentsCollector(method_name)\n+    inference_client_module.visit(method_argument_collector)\n+    existing_args = method_argument_collector.existing_args\n+    missing_params = {k: v for k, v in parameters.items() if k not in existing_args}\n+    return missing_params\n+\n+\n+def get_imports_to_add(\n+    parameters: Dict[str, Dict[str, str]],\n+    parameters_module: cst.Module,\n+    inference_client_module: cst.Module,\n+) -> Dict[str, List[str]]:\n+    \"\"\"\n+    Get the needed imports for missing parameters.\n+\n+    Args:\n+        parameters (Dict[str, Dict[str, str]]): Dictionary of parameters with their type and docstring.\n+        eg: {\"function_to_apply\": {\"type\": \"ClassificationOutputTransform\", \"docstring\": \"Function to apply to the input.\"}}\n+        parameters_module (cst.Module): The module where the parameters are defined.\n+        inference_client_module (cst.Module): The module of the inference client.\n+\n+    Returns:\n+        Dict[str, List[str]]: A dictionary mapping modules to list of types to import.\n+        eg: {\"huggingface_hub.inference._generated.types\": [\"ClassificationOutputTransform\"]}\n+    \"\"\"\n+    # Collect all type names from parameter annotations\n+    types_to_import = set()\n+    for param_info in parameters.values():\n+        types_to_import.update(_collect_type_hints_from_annotation(param_info[\"type\"]))\n+\n+    # Gather existing imports in the inference client module\n+    context = CodemodContext()\n+    gather_visitor = GatherImportsVisitor(context)\n+    inference_client_module.visit(gather_visitor)\n+\n+    # Map types to their defining modules in the parameters module\n+    module_collector = ModulesCollector()\n+    parameters_module.visit(module_collector)\n+\n+    # Determine which imports are needed\n+    needed_imports = {}\n+    for type_name in types_to_import:\n+        types_to_modules = module_collector.type_to_module\n+        module = types_to_modules.get(type_name, DEFAULT_MODULE)\n+        # Maybe no need to check that since the code formatter will handle duplicate imports?\n+        if module not in gather_visitor.object_mapping or type_name not in gather_visitor.object_mapping[module]:\n+            needed_imports.setdefault(module, []).append(type_name)\n+    return needed_imports\n+\n+\n+def _generate_import_statements(import_dict: Dict[str, List[str]]) -> str:\n+    \"\"\"\n+    Generate import statements from a dictionary of needed imports.\n+\n+    Args:\n+        import_dict (Dict[str, List[str]]): Dictionary mapping modules to list of types to import.\n+        eg: {\"typing\": [\"List\", \"Dict\"], \"huggingface_hub.inference._generated.types\": [\"ClassificationOutputTransform\"]}\n+\n+    Returns:\n+        str: The import statements as a string.\n+    \"\"\"\n+    import_statements = []\n+    for module, imports in import_dict.items():\n+        if imports:\n+            import_list = \", \".join(imports)\n+            import_statements.append(f\"from {module} import {import_list}\")\n+        else:\n+            import_statements.append(f\"import {module}\")\n+    return \"\\n\".join(import_statements)\n+\n+\n+# TODO: Needs to be improved, maybe using `typing.get_type_hints` instead (we gonna need to access the method though)?\n+def _collect_type_hints_from_annotation(annotation_str: str) -> Set[str]:\n+    \"\"\"\n+    Collect type hints from an annotation string.\n+\n+    Args:\n+        annotation_str (str): The annotation string.\n+\n+    Returns:\n+        Set[str]: A set of type hints.\n+    \"\"\"\n+    type_string = annotation_str.replace(\" \", \"\")\n+    builtin_types = {d for d in dir(builtins) if isinstance(getattr(builtins, d), type)}\n+    types = re.findall(r\"\\w+|'[^']+'|\\\"[^\\\"]+\\\"\", type_string)\n+    extracted_types = {t.strip(\"\\\"'\") for t in types if t.strip(\"\\\"'\") not in builtin_types}\n+    return extracted_types\n+\n+\n+def _get_parameter_type_name(method_name: str) -> Optional[str]:\n+    file_path = INFERENCE_TYPES_PATH / f\"{method_name}.py\"\n+    if not file_path.is_file():\n+        print(f\"File not found: {file_path}\")\n+        return None\n+\n+    content = file_path.read_text(encoding=\"utf-8\")\n+    match = PARAMETERS_DATACLASS_REGEX.search(content)\n+\n+    return match.group(1) if match else None\n+\n+\n+def _parse_module_from_file(filepath: Path) -> Optional[cst.Module]:\n+    try:\n+        code = filepath.read_text(encoding=\"utf-8\")\n+        return cst.parse_module(code)\n+    except FileNotFoundError:\n+        print(f\"File not found: {filepath}\")\n+    except cst.ParserSyntaxError as e:\n+        print(f\"Syntax error while parsing {filepath}: {e}\")\n+    return None\n+\n+\n+def _check_parameters(method_params: Dict[str, str], update: bool) -> NoReturn:\n+    \"\"\"\n+    Check if task methods have missing parameters and update the InferenceClient source code if needed.\n+\n+    Args:\n+        method_params (Dict[str, str]): Dictionary mapping method names to their parameters dataclass names.\n+        update (bool): Whether to update the InferenceClient source code if missing parameters are found.\n+    \"\"\"\n+    merged_imports = defaultdict(set)\n+    logs = []\n+    inference_client_filename = INFERENCE_CLIENT_FILE\n+    # Read and parse the inference client module\n+    inference_client_module = _parse_module_from_file(inference_client_filename)\n+    modified_module = inference_client_module\n+    has_changes = False\n+    for method_name, parameter_type_name in method_params.items():\n+        parameters_filename = INFERENCE_TYPES_PATH / f\"{method_name}.py\"\n+\n+        # Read and parse the parameters module\n+        parameters_module = _parse_module_from_file(parameters_filename)\n+\n+        # Check if the method has missing parameters\n+        missing_params = check_missing_parameters(modified_module, parameters_module, method_name, parameter_type_name)\n+        if not missing_params:\n+            continue\n+        if update:\n+            ## Get missing imports to add\n+            needed_imports = get_imports_to_add(missing_params, parameters_module, modified_module)\n+            for module, imports_to_add in needed_imports.items():\n+                merged_imports[module].update(imports_to_add)\n+            # Update method parameters and docstring\n+            modified_module = modified_module.visit(AddParameters(method_name, missing_params))\n+            has_changes = True\n+        else:\n+            logs.append(f\"\u274c Missing parameters found in `{method_name}`.\")\n+\n+    if has_changes:\n+        if merged_imports:\n+            import_statements = _generate_import_statements(merged_imports)\n+            imports_to_add = cst.parse_module(import_statements).body\n+            # Update inference client module with the missing imports\n+            modified_module = modified_module.visit(AddImports(imports_to_add))\n+        # Format the updated source code\n+        formatted_source_code = format_source_code(modified_module.code)\n+        INFERENCE_CLIENT_FILE.write_text(formatted_source_code)\n+\n+    if len(logs) > 0:\n+        for log in logs:\n+            print(log)\n+        print(\n+            \"\u274c Mismatch between between parameters defined in tasks methods signature in \"\n+            \"`./src/huggingface_hub/inference/_client.py` and parameters defined in \"\n+            \"`./src/huggingface_hub/inference/_generated/types.py \\n\"\n+            \"Please run `make inference_update` or `python utils/generate_task_parameters.py --update\"\n+        )\n+        exit(1)\n+    else:\n+        if update:\n+            print(\n+                \"\u2705 InferenceClient source code has been updated in\"\n+                \" `./src/huggingface_hub/inference/_client.py`.\\n   Please make sure the changes are\"\n+                \" accurate and commit them.\"\n+            )\n+        else:\n+            print(\"\u2705 All good!\")\n+        exit(0)\n+\n+\n+def update_inference_client(update: bool):\n+    print(f\"\ud83d\ude48 Skipping the following tasks: {TASKS_TO_SKIP}\")\n+    # Get all tasks from the ./src/huggingface_hub/inference/_generated/types/\n+    tasks = set()\n+    for file in INFERENCE_TYPES_PATH.glob(\"*.py\"):\n+        if file.stem not in TASKS_TO_SKIP:\n+            tasks.add(file.stem)\n+\n+    # Construct a mapping between method names and their parameters dataclass names\n+    method_params = {}\n+    for method_name, _ in inspect.getmembers(InferenceClient, predicate=inspect.isfunction):\n+        if method_name.startswith(\"_\") or method_name not in tasks:\n+            continue\n+        parameter_type_name = _get_parameter_type_name(method_name)\n+        if parameter_type_name is not None:\n+            method_params[method_name] = parameter_type_name\n+    _check_parameters(method_params, update=update)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--update\",\n+        action=\"store_true\",\n+        help=(\"Whether to update `./src/huggingface_hub/inference/_client.py` if parameters are missing.\"),\n+    )\n+    args = parser.parse_args()\n+    update_inference_client(update=args.update)\ndiff --git a/utils/helpers.py b/utils/helpers.py\nindex 34f12e11f3..a2dcecef6e 100644\n--- a/utils/helpers.py\n+++ b/utils/helpers.py\n@@ -14,10 +14,16 @@\n # limitations under the License.\n \"\"\"Contains helpers used by the scripts in `./utils`.\"\"\"\n \n+import subprocess\n+import tempfile\n from pathlib import Path\n \n+from ruff.__main__ import find_ruff_bin\n+\n \n def check_and_update_file_content(file: Path, expected_content: str, update: bool):\n+    # Ensure the expected content ends with a newline to satisfy end-of-file-fixer hook\n+    expected_content = expected_content.rstrip(\"\\n\") + \"\\n\"\n     content = file.read_text() if file.exists() else None\n     if content != expected_content:\n         if update:\n@@ -26,3 +32,19 @@ def check_and_update_file_content(file: Path, expected_content: str, update: boo\n         else:\n             print(f\"\u274c Expected content mismatch in {file}.\")\n             exit(1)\n+\n+\n+def format_source_code(code: str) -> str:\n+    \"\"\"Format the generated source code using Ruff.\"\"\"\n+    with tempfile.TemporaryDirectory() as tmpdir:\n+        filepath = Path(tmpdir) / \"tmp.py\"\n+        filepath.write_text(code)\n+        ruff_bin = find_ruff_bin()\n+        if not ruff_bin:\n+            raise FileNotFoundError(\"Ruff executable not found.\")\n+        try:\n+            subprocess.run([ruff_bin, \"check\", str(filepath), \"--fix\", \"--quiet\"], check=True)\n+            subprocess.run([ruff_bin, \"format\", str(filepath), \"--quiet\"], check=True)\n+        except subprocess.CalledProcessError as e:\n+            raise RuntimeError(f\"Error running Ruff: {e}\")\n+        return filepath.read_text()\n", "instance_id": "huggingface__huggingface_hub-2561", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, as it identifies a specific issue with the `huggingface_hub.InferenceClient` where several task methods lack a `parameters` argument, leading to inconsistency with task specifications and documentation. It provides a concrete example with the `text_classification` method, showing the discrepancy between using raw HTTP requests and the client API, which helps in understanding the problem. The goal is evident: to add the missing `parameters` argument to the listed task methods to align with the specs. However, there are minor ambiguities, such as the lack of explicit mention of all expected parameters for each task (beyond the general `parameters` argument) and how they should be integrated into the payload. Additionally, edge cases or potential backward compatibility concerns are not addressed in the statement, which could impact implementation decisions. Overall, the problem is valid and clear but misses some finer details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the 'Hard' category (0.6-0.8) due to several factors. First, the scope of code changes is significant, impacting multiple task methods across both synchronous and asynchronous client implementations in the `huggingface_hub` library, as seen in the extensive diff provided. This requires modifications in multiple files, including `_client.py` and `_async_client.py`, as well as updates to type definitions and utility scripts. Second, it demands a deep understanding of the codebase architecture, particularly how the inference client constructs payloads and interacts with the Hugging Face API, as evidenced by the need to handle raw data versus base64-encoded inputs based on parameter presence. Third, the changes involve several technical concepts, such as Python's type hints, dataclasses, CST (Concrete Syntax Tree) manipulation for code generation, and deprecation handling, which add to the complexity. Fourth, while edge cases are not explicitly mentioned in the problem statement, the implementation must consider them (e.g., ensuring backward compatibility, handling invalid parameters), as seen in the deprecation warnings and conditional payload construction in the code changes. The task also involves updating documentation and ensuring consistency with generated types, which requires careful attention to detail. However, it does not reach the 'Very Hard' category (0.8-1.0) as it does not involve system-level design or highly intricate domain-specific knowledge beyond the library's context. A score of 0.65 reflects the need for a solid grasp of the library's internals and careful implementation across multiple components, but it remains within the realm of a well-defined library enhancement rather than a groundbreaking technical challenge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Windows console-based application crashes immediately after opening\n**Describe the bug**\r\nI used the same python environment to freeze my program in Windows. The program runs well if running from source under the same python environment, but after freezing it, opening the executable will crash immediately.\r\n\r\n**Expected behavior**\r\nMy application is based on `tkinter` and `base=console` in `setup.py`, as I still need to see my helpful debugging messages. Previously, this setting worked fine - there was still GUI popping up. But now, it will open a console and quickly close itself without any debugging print messages in the console, which makes diagnosing the problem very difficult.\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - Platform information: WIndows11\r\n - OS architecture: x86\r\n - cx_Freeze version [e.g. 6.11]: 7.3.0.dev4\r\n - Python version: 3.10\r\n\r\n**Additional context**\r\nsetup.py:\r\n```python3\r\nfrom cx_Freeze import setup, Executable\r\n\r\nbuild_exe_options = {\r\n    \"build_exe\": \"dist\",\r\n    \"includes\": [\r\n        \"caiman\",\r\n    ],\r\n    # \"packages\": [\r\n    # ],\r\n    \"include_files\": [\r\n        \"config.json\",\r\n        \"resource\",\r\n        \"log\"\r\n    ],\r\n\r\n}\r\nbuild_msi_options = {}\r\nbuild_mac_options = {}\r\nbuild_appimage_options = {}\r\n\r\nexecutables = [\r\n    Executable(\r\n        \"main.py\",\r\n        base=\"console\",\r\n        # icon=\"icon.ico\",\r\n        target_name=\"Program\",\r\n        shortcut_name=\"Program\",\r\n        shortcut_dir=\"Program\",\r\n    )\r\n]\r\n\r\nsetup(\r\n    name=\"Program\",\r\n    version=\"0.5\",\r\n    description=\"Toolbox\",\r\n    options={\r\n        \"build_exe\": build_exe_options,\r\n        \"bdist_msi\": build_msi_options,\r\n        \"bdist_mac\": build_mac_options,\r\n        \"bdist_appimage\": build_appimage_options,\r\n    },\r\n    executables=executables,\r\n)\r\n```\r\n\n", "patch": "diff --git a/cx_Freeze/freezer.py b/cx_Freeze/freezer.py\nindex 2f8f67d83..f6f170979 100644\n--- a/cx_Freeze/freezer.py\n+++ b/cx_Freeze/freezer.py\n@@ -191,9 +191,15 @@ def _copy_file(\n         if self.silent < 1:\n             print(f\"copying {source} -> {target}\")\n         shutil.copyfile(source, target)\n-        shutil.copystat(source, target)\n         if include_mode:\n             shutil.copymode(source, target)\n+            shutil.copystat(source, target)\n+        else:\n+            try:\n+                shutil.copystat(source, target)\n+            except OSError:\n+                if self.silent < 3:\n+                    print(\"WARNING: unable to copy file metadata:\", target)\n         self.files_copied.add(target)\n \n         # handle post-copy tasks, including copying dependencies\ndiff --git a/cx_Freeze/hooks/__init__.py b/cx_Freeze/hooks/__init__.py\nindex 312ee8844..5728de7b6 100644\n--- a/cx_Freeze/hooks/__init__.py\n+++ b/cx_Freeze/hooks/__init__.py\n@@ -653,12 +653,6 @@ def load_twitter(finder: ModuleFinder, module: Module) -> None:\n     module.ignore_names.update([\"json\", \"simplejson\", \"django.utils\"])\n \n \n-def load_tzdata(finder: ModuleFinder, module: Module) -> None:\n-    \"\"\"The tzdata package requires its zone and timezone data.\"\"\"\n-    if module.in_file_system == 0:\n-        finder.zip_include_files(module.file.parent, \"tzdata\")\n-\n-\n def load_uvloop(finder: ModuleFinder, module: Module) -> None:\n     \"\"\"The uvloop module implicitly loads an extension module.\"\"\"\n     finder.include_module(\"uvloop._noop\")\ndiff --git a/cx_Freeze/hooks/tzdata.py b/cx_Freeze/hooks/tzdata.py\nnew file mode 100644\nindex 000000000..ad9c6bae8\n--- /dev/null\n+++ b/cx_Freeze/hooks/tzdata.py\n@@ -0,0 +1,17 @@\n+\"\"\"A collection of functions which are triggered automatically by finder when\n+tzdata package is included.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from cx_Freeze.finder import ModuleFinder\n+    from cx_Freeze.module import Module\n+\n+\n+def load_tzdata(finder: ModuleFinder, module: Module) -> None:\n+    \"\"\"The tzdata package requires its zone and timezone data.\"\"\"\n+    if module.in_file_system == 0:\n+        finder.zip_include_files(module.file.parent, \"tzdata\")\ndiff --git a/cx_Freeze/hooks/zoneinfo.py b/cx_Freeze/hooks/zoneinfo.py\nindex db55fddc9..b67a1d4c1 100644\n--- a/cx_Freeze/hooks/zoneinfo.py\n+++ b/cx_Freeze/hooks/zoneinfo.py\n@@ -5,6 +5,7 @@\n from __future__ import annotations\n \n from pathlib import Path\n+from textwrap import dedent\n from typing import TYPE_CHECKING\n \n if TYPE_CHECKING:\n@@ -16,31 +17,62 @@ def load_zoneinfo(finder: ModuleFinder, module: Module) -> None:\n     \"\"\"The zoneinfo package requires timezone data,\n     that can be the in tzdata package, if installed.\n     \"\"\"\n+    module.global_names.add(\"TZPATH\")\n     try:\n         finder.include_package(\"tzdata\")\n+        target_path = \"lib/tzdata/zoneinfo\"\n     except ImportError:\n-        pass\n-    else:\n-        return\n+        target_path = None\n \n-    # without tzdata, copy only zoneinfo directory\n-    source_path = None\n-    zoneinfo = __import__(module.name, fromlist=[\"TZPATH\"])\n-    if zoneinfo.TZPATH:\n-        for path in zoneinfo.TZPATH:\n-            if path.endswith(\"zoneinfo\"):\n-                source_path = Path(path)\n-                break\n-    if source_path is None or not source_path.is_dir():\n-        return\n-    if module.in_file_system == 0:\n-        finder.zip_include_files(source_path, \"tzdata/zoneinfo\")\n-    else:\n+    if target_path is None:\n+        # without tzdata, copy zoneinfo directory if available\n+        source_path = None\n+        zoneinfo = __import__(module.name, fromlist=[\"TZPATH\"])\n+        if zoneinfo.TZPATH:\n+            for path in zoneinfo.TZPATH:\n+                if path.endswith(\"zoneinfo\"):\n+                    source_path = Path(path).resolve()\n+                    break\n+        if source_path is None or not source_path.is_dir():\n+            # add tzdata to missing modules\n+            bad_modules = finder._bad_modules  # noqa: SLF001\n+            callers = bad_modules.setdefault(\"tzdata\", {})\n+            callers[f\"{module.name}_hook\"] = None\n+            return\n+        if module.in_file_system == 0:\n+            finder.zip_include_files(source_path, \"tzdata/zoneinfo\")\n+            return\n         target_path = \"share/zoneinfo\"\n-        finder.add_constant(\"PYTHONTZPATH\", target_path)\n         finder.include_files(\n             source_path, target_path, copy_dependent_files=False\n         )\n \n+    # patch source code\n+    if module.file.suffix == \".pyc\":  # source unavailable\n+        return\n+\n+    source = f\"\"\"\n+        # cx_Freeze patch start\n+        import os as _os\n+        import sys as _sys\n+        _prefix = _sys.prefix if _sys.prefix else _sys.frozen_dir\n+        if _sys.platform == \"darwin\":\n+            _mac_prefix = _os.path.join(_os.path.dirname(_prefix), \"Resources\")\n+            if _os.path.exists(_mac_prefix):\n+                _prefix = _mac_prefix  # using bdist_mac\n+        _os.environ[\"PYTHONTZPATH\"] = _os.path.join(\n+            _prefix, _os.path.normpath(\"{target_path}\")\n+        )\n+        # cx_Freeze patch end\n+    \"\"\"\n+    code_string = module.file.read_text(encoding=\"utf_8\")\n+    module.code = compile(\n+        dedent(source) + code_string,\n+        module.file.as_posix(),\n+        \"exec\",\n+        dont_inherit=True,\n+        optimize=finder.optimize,\n+    )\n+\n \n __all__ = [\"load_zoneinfo\"]\ndiff --git a/cx_Freeze/initscripts/__startup__.py b/cx_Freeze/initscripts/__startup__.py\nindex 069a191fa..595c49c81 100644\n--- a/cx_Freeze/initscripts/__startup__.py\n+++ b/cx_Freeze/initscripts/__startup__.py\n@@ -101,7 +101,6 @@ def init() -> None:\n     for name in (\n         \"TCL_LIBRARY\",\n         \"TK_LIBRARY\",\n-        \"PYTZ_TZDATADIR\",\n         \"PYTHONTZPATH\",\n     ):\n         try:\n", "instance_id": "marcelotduarte__cx_Freeze-2592", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a Windows console-based application built with cx_Freeze crashes immediately after opening, despite working fine when run from source. The expected behavior (GUI popping up with console debug messages) and the current behavior (console opens and closes without messages) are specified. Relevant context, such as the platform (Windows 11, x86), Python version (3.10), cx_Freeze version (7.3.0.dev4), and the setup.py configuration, is provided, which helps in understanding the environment. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether any error messages or logs are generated before the crash, nor does it mention specific conditions or recent changes (e.g., updates to dependencies or environment) that might have triggered the issue. Additionally, edge cases or specific scenarios leading to the crash are not described, which could be critical for diagnosing the root cause. Overall, while the problem is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.75) due to several factors. First, the scope and depth of code changes are significant, as they span multiple files in the cx_Freeze library (freezer.py, hooks/__init__.py, hooks/tzdata.py, hooks/zoneinfo.py, initscripts/__startup__.py), indicating that the solution requires understanding and modifying core components of a complex tool. These changes are not isolated to a single module but affect how cx_Freeze handles file copying, timezone data inclusion, and environment variable setup, which suggests a deep interaction with the library's architecture. Second, the number of technical concepts involved is substantial, including knowledge of Python's module system, file system operations (shutil), environment variable management, and the specifics of freezing Python applications with cx_Freeze. Additionally, the changes involve handling platform-specific behavior (e.g., macOS vs. Windows paths) and modifying compiled code dynamically, which requires advanced Python expertise. Third, the problem involves potential edge cases and error handling, as seen in the modifications to handle OSError during file metadata copying and the fallback logic for timezone data paths when the tzdata package is unavailable. These changes suggest a need to anticipate and manage various failure modes. Finally, solving this issue likely requires a deep understanding of how cx_Freeze interacts with the Python runtime and the frozen executable environment, which is a specialized domain. While not at the extreme end of difficulty (e.g., implementing a distributed system), this problem demands significant experience and careful consideration of the broader impact of the changes, justifying a score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Replace`credentials.valid` check for `credentials.token_state`\nNew best practice for checking validity of credentials is no longer `credentials.valid` which is now deprecated (as of google-auth v2.24.0). Instead the google-auth folks recommend using [`credentials.token_state`](https://github.com/googleapis/google-auth-library-python/blob/main/google/auth/credentials.py#L100)\r\n\r\nAll instances of the following:\r\nhttps://github.com/GoogleCloudPlatform/alloydb-python-connector/blob/473f4abbe8e7524c56bd530f13294283a6b4cbcf/google/cloud/alloydb/connector/client.py#L164-L166\r\n\r\nShould be replaced to:\r\n```python\r\nfrom google.auth.credentials import TokenState\r\n\r\n# ...\r\nif not self._credentials.token_state == TokenState.FRESH:\r\n    self._credentials.refresh(google.auth.transport.requests.Request())\r\n```\n", "patch": "diff --git a/google/cloud/alloydb/connector/client.py b/google/cloud/alloydb/connector/client.py\nindex ecb5e65..2c6afef 100644\n--- a/google/cloud/alloydb/connector/client.py\n+++ b/google/cloud/alloydb/connector/client.py\n@@ -18,7 +18,6 @@\n from typing import Dict, List, Optional, Tuple, TYPE_CHECKING\n \n import aiohttp\n-from google.auth.transport.requests import Request\n \n from google.cloud.alloydb.connector.version import __version__ as version\n \n@@ -116,10 +115,6 @@ async def _get_metadata(\n         \"\"\"\n         logger.debug(f\"['{project}/{region}/{cluster}/{name}']: Requesting metadata\")\n \n-        if not self._credentials.valid:\n-            request = Request()\n-            self._credentials.refresh(request)\n-\n         headers = {\n             \"Authorization\": f\"Bearer {self._credentials.token}\",\n         }\n@@ -167,10 +162,6 @@ async def _get_client_certificate(\n         \"\"\"\n         logger.debug(f\"['{project}/{region}/{cluster}']: Requesting client certificate\")\n \n-        if not self._credentials.valid:\n-            request = Request()\n-            self._credentials.refresh(request)\n-\n         headers = {\n             \"Authorization\": f\"Bearer {self._credentials.token}\",\n         }\ndiff --git a/google/cloud/alloydb/connector/instance.py b/google/cloud/alloydb/connector/instance.py\nindex 75169bc..c3ae25a 100644\n--- a/google/cloud/alloydb/connector/instance.py\n+++ b/google/cloud/alloydb/connector/instance.py\n@@ -20,6 +20,9 @@\n import re\n from typing import Tuple, TYPE_CHECKING\n \n+from google.auth.credentials import TokenState\n+from google.auth.transport import requests\n+\n from google.cloud.alloydb.connector.exceptions import IPTypeNotFoundError\n from google.cloud.alloydb.connector.exceptions import RefreshError\n from google.cloud.alloydb.connector.rate_limiter import AsyncRateLimiter\n@@ -130,6 +133,11 @@ async def _perform_refresh(self) -> RefreshResult:\n         try:\n             await self._refresh_rate_limiter.acquire()\n             priv_key, pub_key = await self._keys\n+\n+            # before making AlloyDB API calls, refresh creds if required\n+            if not self._client._credentials.token_state == TokenState.FRESH:\n+                self._client._credentials.refresh(requests.Request())\n+\n             # fetch metadata\n             metadata_task = asyncio.create_task(\n                 self._client._get_metadata(\n", "instance_id": "GoogleCloudPlatform__alloydb-python-connector-321", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal of replacing the deprecated `credentials.valid` check with the recommended `credentials.token_state` check using `TokenState.FRESH`. It provides a direct reference to the new best practice in the google-auth library and includes a code snippet for the replacement logic. Additionally, it specifies the exact locations in the codebase where the change is needed by linking to the relevant lines in the repository. However, there are minor ambiguities: the problem statement does not explicitly mention whether there are any specific conditions or edge cases to consider when refreshing credentials (e.g., handling refresh failures or token expiration during runtime). It also lacks details on potential impacts or side effects of this change on the rest of the system. Despite these minor omissions, the intent and scope of the task are reasonably well-defined, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The task involves a straightforward replacement of a deprecated API check (`credentials.valid`) with a new one (`credentials.token_state == TokenState.FRESH`) across a few specific locations in the codebase, as shown in the provided diff. The scope of the code changes is limited to two files (`client.py` and `instance.py`), and the modifications are localized to specific functions without requiring deep architectural changes or extensive refactoring. The technical concepts involved are minimal\u2014basic familiarity with Python, the google-auth library, and the concept of token validation/refreshing is sufficient. No complex algorithms, design patterns, or domain-specific knowledge beyond basic API usage are required. Edge cases and error handling are not explicitly mentioned in the problem statement, and the provided code changes do not introduce new error handling logic beyond the existing refresh mechanism. The overall amount of code change is small, and the impact on the system appears negligible. Therefore, a difficulty score of 0.25 is appropriate, reflecting a task that requires understanding some code logic and making simple statement modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Python 3.11 and beyond\n<!-- Thank you for contributing. These HTML commments will not render in the issue, but you can delete them once you've read them if you prefer! -->\r\n\r\n### Proposed change\r\n\r\nSupport Python 3.11, and perhaps automatically support future Python releases?\r\n\r\nPython 3.11.0 was released in [October 2022](https://peps.python.org/pep-0664/#schedule) but unfortunately I can\u2019t yet use it in mybinder.org. \r\n\r\nWhile it looks like you only need to run a simple script to add the conda environment file for new Python versions, you\u2019ll need to do this manually [once every 12 months](https://peps.python.org/pep-0602/). Is there any way this could be automated a bit or have the environment generated *on the fly* by periodically checking for newer Python versions on conda forge? \r\n\r\n### Alternative options\r\nFile a request for every new 3.x Python release, yearly. \r\n\r\n### Who would use this feature?\r\n\r\nAnyone wanting to use features in newer Python releases.\r\n\r\n### How much effort will adding it take?\r\n \r\n Adding 3.11: update conda/freeze.py to add 3.11 to the defaults, generate the environment files,  commit.\r\n\r\nAdding automated updates based on conda forge version info: hard to say, but a few days perhaps, depending on how well understood the GitHub Action environment is and how you\u2019d commit updated environment files.\r\n\r\n### Who can do this work?\r\n\r\nThe base 3.11 work: most anyone, if the #1175 PR is typical. \r\n\r\nAutomatic detection of new Python versions: requires conda forge API familiarity plus experience with GH actions to automate commits or PRs.\r\n\n", "patch": "diff --git a/repo2docker/buildpacks/conda/environment.py-3.12-linux-64.lock b/repo2docker/buildpacks/conda/environment.py-3.12-linux-64.lock\nnew file mode 100644\nindex 000000000..57d613bc6\n--- /dev/null\n+++ b/repo2docker/buildpacks/conda/environment.py-3.12-linux-64.lock\n@@ -0,0 +1,152 @@\n+# AUTO GENERATED FROM environment.py-3.12.yml, DO NOT MANUALLY MODIFY\n+# Frozen on 2024-05-21 07:52:44 UTC\n+# Generated by conda-lock.\n+# platform: linux-64\n+# input_hash: 368139795b37f28a775052c0b2270e0850ebb567402945c13fbab8dc6355cc7b\n+@EXPLICIT\n+https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2#d7c89558ba9fa0495403155b64376d81\n+https://conda.anaconda.org/conda-forge/linux-64/ca-certificates-2024.2.2-hbcca054_0.conda#2f4327a1cbe7f022401b236e915a5fef\n+https://conda.anaconda.org/conda-forge/linux-64/ld_impl_linux-64-2.40-h55db66e_0.conda#10569984e7db886e4f1abc2b47ad79a1\n+https://conda.anaconda.org/conda-forge/linux-64/libstdcxx-ng-13.2.0-hc0a3c3a_7.conda#53ebd4c833fa01cb2c6353e99f905406\n+https://conda.anaconda.org/conda-forge/linux-64/pandoc-3.2-ha770c72_0.conda#8c924f0b7f3e064b1c954a08e7c32fba\n+https://conda.anaconda.org/conda-forge/linux-64/python_abi-3.12-4_cp312.conda#dccc2d142812964fcc6abdc97b672dff\n+https://conda.anaconda.org/conda-forge/noarch/tzdata-2024a-h0c530f3_0.conda#161081fc7cec0bfda0d86d7cb595f8d8\n+https://conda.anaconda.org/conda-forge/linux-64/libgomp-13.2.0-h77fa898_7.conda#abf3fec87c2563697defa759dec3d639\n+https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_gnu.tar.bz2#73aaf86a425cc6e73fcf236a5a46396d\n+https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-13.2.0-h77fa898_7.conda#72ec1b1b04c4d15d4204ece1ecea5978\n+https://conda.anaconda.org/conda-forge/linux-64/bzip2-1.0.8-hd590300_5.conda#69b8b6202a07720f448be700e300ccf4\n+https://conda.anaconda.org/conda-forge/linux-64/icu-73.2-h59595ed_0.conda#cc47e1facc155f91abd89b11e48e72ff\n+https://conda.anaconda.org/conda-forge/linux-64/keyutils-1.6.1-h166bdaf_0.tar.bz2#30186d27e2c9fa62b45fb1476b7200e3\n+https://conda.anaconda.org/conda-forge/linux-64/libexpat-2.6.2-h59595ed_0.conda#e7ba12deb7020dd080c6c70e7b6f6a3d\n+https://conda.anaconda.org/conda-forge/linux-64/libffi-3.4.2-h7f98852_5.tar.bz2#d645c6d2ac96843a2bfaccd2d62b3ac3\n+https://conda.anaconda.org/conda-forge/linux-64/libnsl-2.0.1-hd590300_0.conda#30fd6e37fe21f86f4bd26d6ee73eeec7\n+https://conda.anaconda.org/conda-forge/linux-64/libsodium-1.0.18-h36c2ea0_1.tar.bz2#c3788462a6fbddafdb413a9f9053e58d\n+https://conda.anaconda.org/conda-forge/linux-64/libuuid-2.38.1-h0b41bf4_0.conda#40b61aab5c7ba9ff276c41cfffe6b80b\n+https://conda.anaconda.org/conda-forge/linux-64/libuv-1.48.0-hd590300_0.conda#7e8b914b1062dd4386e3de4d82a3ead6\n+https://conda.anaconda.org/conda-forge/linux-64/libxcrypt-4.4.36-hd590300_1.conda#5aa797f8787fe7a17d1b0821485b5adc\n+https://conda.anaconda.org/conda-forge/linux-64/libzlib-1.2.13-hd590300_5.conda#f36c115f1ee199da648e0597ec2047ad\n+https://conda.anaconda.org/conda-forge/linux-64/ncurses-6.5-h59595ed_0.conda#fcea371545eda051b6deafb24889fc69\n+https://conda.anaconda.org/conda-forge/linux-64/openssl-3.3.0-h4ab18f5_1.conda#ba01a2fef8a67913acc4ab8da101dd27\n+https://conda.anaconda.org/conda-forge/linux-64/xz-5.2.6-h166bdaf_0.tar.bz2#2161070d867d1b1204ea749c8eec4ef0\n+https://conda.anaconda.org/conda-forge/linux-64/libedit-3.1.20191231-he28a2e2_2.tar.bz2#4d331e44109e3f0e19b4cb8f9b82f3e1\n+https://conda.anaconda.org/conda-forge/linux-64/libsqlite-3.45.3-h2797004_0.conda#b3316cbe90249da4f8e84cd66e1cc55b\n+https://conda.anaconda.org/conda-forge/linux-64/readline-8.2-h8228510_1.conda#47d31b792659ce70f470b5c82fdfb7a4\n+https://conda.anaconda.org/conda-forge/linux-64/tk-8.6.13-noxft_h4845f30_101.conda#d453b98d9c83e71da0741bb0ff4d76bc\n+https://conda.anaconda.org/conda-forge/linux-64/zlib-1.2.13-hd590300_5.conda#68c34ec6149623be41a1933ab996a209\n+https://conda.anaconda.org/conda-forge/linux-64/krb5-1.21.2-h659d440_0.conda#cd95826dbd331ed1be26bdf401432844\n+https://conda.anaconda.org/conda-forge/linux-64/nodejs-18.20.2-h84e79e0_0.conda#de31995a7a3eae6d2230ffa5d68e9aa2\n+https://conda.anaconda.org/conda-forge/linux-64/python-3.12.3-hab00c5b_0_cpython.conda#2540b74d304f71d3e89c81209db4db84\n+https://conda.anaconda.org/conda-forge/noarch/async_generator-1.10-py_0.tar.bz2#d56c596e61b1c4952acf0a9920856c12\n+https://conda.anaconda.org/conda-forge/noarch/attrs-23.2.0-pyh71513ae_0.conda#5e4c0743c70186509d1412e03c2d8dfa\n+https://conda.anaconda.org/conda-forge/noarch/blinker-1.8.2-pyhd8ed1ab_0.conda#cf85c002319c15e9721934104aaa1137\n+https://conda.anaconda.org/conda-forge/linux-64/brotli-python-1.1.0-py312h30efb56_1.conda#45801a89533d3336a365284d93298e36\n+https://conda.anaconda.org/conda-forge/noarch/certifi-2024.2.2-pyhd8ed1ab_0.conda#0876280e409658fc6f9e75d035960333\n+https://conda.anaconda.org/conda-forge/noarch/charset-normalizer-3.3.2-pyhd8ed1ab_0.conda#7f4a9e3fcff3f6356ae99244a014da6a\n+https://conda.anaconda.org/conda-forge/linux-64/debugpy-1.8.1-py312h30efb56_0.conda#bdd639417094ace2fb1ce10b20d68d5d\n+https://conda.anaconda.org/conda-forge/noarch/decorator-5.1.1-pyhd8ed1ab_0.tar.bz2#43afe5ab04e35e17ba28649471dd7364\n+https://conda.anaconda.org/conda-forge/noarch/defusedxml-0.7.1-pyhd8ed1ab_0.tar.bz2#961b3a227b437d82ad7054484cfa71b2\n+https://conda.anaconda.org/conda-forge/noarch/entrypoints-0.4-pyhd8ed1ab_0.tar.bz2#3cf04868fee0a029769bd41f4b2fbf2d\n+https://conda.anaconda.org/conda-forge/noarch/exceptiongroup-1.2.0-pyhd8ed1ab_2.conda#8d652ea2ee8eaee02ed8dc820bc794aa\n+https://conda.anaconda.org/conda-forge/noarch/executing-2.0.1-pyhd8ed1ab_0.conda#e16be50e378d8a4533b989035b196ab8\n+https://conda.anaconda.org/conda-forge/linux-64/greenlet-3.0.3-py312h30efb56_0.conda#a8a85268e01ddee952f4105f3a8122a4\n+https://conda.anaconda.org/conda-forge/noarch/idna-3.7-pyhd8ed1ab_0.conda#c0cc1420498b17414d8617d0b9f506ca\n+https://conda.anaconda.org/conda-forge/noarch/ipython_genutils-0.2.0-py_1.tar.bz2#5071c982548b3a20caf70462f04f5287\n+https://conda.anaconda.org/conda-forge/noarch/json5-0.9.25-pyhd8ed1ab_0.conda#5d8c241a9261e720a34a07a3e1ac4109\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_widgets-3.0.10-pyhd8ed1ab_0.conda#16b73b2c4ff7dda8bbecf88aadfe2027\n+https://conda.anaconda.org/conda-forge/linux-64/markupsafe-2.1.5-py312h98912ed_0.conda#6ff0b9582da2d4a74a1f9ae1f9ce2af6\n+https://conda.anaconda.org/conda-forge/noarch/mistune-3.0.2-pyhd8ed1ab_0.conda#5cbee699846772cc939bef23a0d524ed\n+https://conda.anaconda.org/conda-forge/noarch/nest-asyncio-1.6.0-pyhd8ed1ab_0.conda#6598c056f64dc8800d40add25e4e2c34\n+https://conda.anaconda.org/conda-forge/noarch/packaging-24.0-pyhd8ed1ab_0.conda#248f521b64ce055e7feae3105e7abeb8\n+https://conda.anaconda.org/conda-forge/noarch/pamela-1.1.0-pyh1a96a4e_0.conda#dce93ccf07159017c69c6d9cea2d5d61\n+https://conda.anaconda.org/conda-forge/noarch/pandocfilters-1.5.0-pyhd8ed1ab_0.tar.bz2#457c2c8c08e54905d6954e79cb5b5db9\n+https://conda.anaconda.org/conda-forge/noarch/parso-0.8.4-pyhd8ed1ab_0.conda#81534b420deb77da8833f2289b8d47ac\n+https://conda.anaconda.org/conda-forge/noarch/pickleshare-0.7.5-py_1003.tar.bz2#415f0ebb6198cc2801c73438a9fb5761\n+https://conda.anaconda.org/conda-forge/noarch/pkgutil-resolve-name-1.3.10-pyhd8ed1ab_1.conda#405678b942f2481cecdb3e010f4925d9\n+https://conda.anaconda.org/conda-forge/noarch/platformdirs-4.2.2-pyhd8ed1ab_0.conda#6f6cf28bf8e021933869bae3f84b8fc9\n+https://conda.anaconda.org/conda-forge/noarch/prometheus_client-0.20.0-pyhd8ed1ab_0.conda#9a19b94034dd3abb2b348c8b93388035\n+https://conda.anaconda.org/conda-forge/linux-64/psutil-5.9.8-py312h98912ed_0.conda#3facaca6cc0f7988df3250efccd32da3\n+https://conda.anaconda.org/conda-forge/noarch/ptyprocess-0.7.0-pyhd3deb0d_0.tar.bz2#359eeb6536da0e687af562ed265ec263\n+https://conda.anaconda.org/conda-forge/noarch/pure_eval-0.2.2-pyhd8ed1ab_0.tar.bz2#6784285c7e55cb7212efabc79e4c2883\n+https://conda.anaconda.org/conda-forge/noarch/pycparser-2.22-pyhd8ed1ab_0.conda#844d9eb3b43095b031874477f7d70088\n+https://conda.anaconda.org/conda-forge/noarch/pygments-2.18.0-pyhd8ed1ab_0.conda#b7f5c092b8f9800150d998a71b76d5a1\n+https://conda.anaconda.org/conda-forge/noarch/pyjwt-2.8.0-pyhd8ed1ab_1.conda#74f76d4868dbba5870f2cf1d9b12d8f3\n+https://conda.anaconda.org/conda-forge/noarch/pysocks-1.7.1-pyha2e5f31_6.tar.bz2#2a7de29fb590ca14b5243c4c812c8025\n+https://conda.anaconda.org/conda-forge/noarch/python-fastjsonschema-2.19.1-pyhd8ed1ab_0.conda#4d3ceee3af4b0f9a1f48f57176bf8625\n+https://conda.anaconda.org/conda-forge/noarch/python-json-logger-2.0.7-pyhd8ed1ab_0.conda#a61bf9ec79426938ff785eb69dbb1960\n+https://conda.anaconda.org/conda-forge/noarch/pytz-2024.1-pyhd8ed1ab_0.conda#3eeeeb9e4827ace8c0c1419c85d590ad\n+https://conda.anaconda.org/conda-forge/linux-64/rpds-py-0.18.1-py312h4413252_0.conda#73da42918aaeb87d5618f82e2ac18d1f\n+https://conda.anaconda.org/conda-forge/linux-64/ruamel.yaml.clib-0.2.8-py312h98912ed_0.conda#05f31c2a79ba61df8d6d903ce4a4ce7b\n+https://conda.anaconda.org/conda-forge/noarch/send2trash-1.8.3-pyh0d859eb_0.conda#778594b20097b5a948c59e50ae42482a\n+https://conda.anaconda.org/conda-forge/noarch/setuptools-69.5.1-pyhd8ed1ab_0.conda#7462280d81f639363e6e63c81276bd9e\n+https://conda.anaconda.org/conda-forge/noarch/six-1.16.0-pyh6c4a22f_0.tar.bz2#e5f25f8dbc060e9a8d912e432202afc2\n+https://conda.anaconda.org/conda-forge/noarch/sniffio-1.3.1-pyhd8ed1ab_0.conda#490730480d76cf9c8f8f2849719c6e2b\n+https://conda.anaconda.org/conda-forge/noarch/soupsieve-2.5-pyhd8ed1ab_1.conda#3f144b2c34f8cb5a9abd9ed23a39c561\n+https://conda.anaconda.org/conda-forge/noarch/tomli-2.0.1-pyhd8ed1ab_0.tar.bz2#5844808ffab9ebdb694585b50ba02a96\n+https://conda.anaconda.org/conda-forge/linux-64/tornado-6.4-py312h98912ed_0.conda#e8332e534dca8c5c12c8352e0a23501c\n+https://conda.anaconda.org/conda-forge/noarch/traitlets-5.14.3-pyhd8ed1ab_0.conda#3df84416a021220d8b5700c613af2dc5\n+https://conda.anaconda.org/conda-forge/noarch/typing_extensions-4.11.0-pyha770c72_0.conda#6ef2fc37559256cf682d8b3375e89b80\n+https://conda.anaconda.org/conda-forge/noarch/wcwidth-0.2.13-pyhd8ed1ab_0.conda#68f0738df502a14213624b288c60c9ad\n+https://conda.anaconda.org/conda-forge/noarch/webencodings-0.5.1-pyhd8ed1ab_2.conda#daf5160ff9cde3a468556965329085b9\n+https://conda.anaconda.org/conda-forge/noarch/websocket-client-1.8.0-pyhd8ed1ab_0.conda#f372c576b8774922da83cda2b12f9d29\n+https://conda.anaconda.org/conda-forge/noarch/wheel-0.43.0-pyhd8ed1ab_1.conda#0b5293a157c2b5cd513dd1b03d8d3aae\n+https://conda.anaconda.org/conda-forge/noarch/widgetsnbextension-4.0.10-pyhd8ed1ab_0.conda#521f489e3babeddeec638c2add7e9e64\n+https://conda.anaconda.org/conda-forge/linux-64/zeromq-4.3.5-h75354e8_4.conda#03cc8d9838ad9dd0060ab532e81ccb21\n+https://conda.anaconda.org/conda-forge/noarch/zipp-3.17.0-pyhd8ed1ab_0.conda#2e4d6bc0b14e10f895fc6791a7d9b26a\n+https://conda.anaconda.org/conda-forge/noarch/anyio-3.7.1-pyhd8ed1ab_0.conda#7b517e7a6f0790337906c055aa97ca49\n+https://conda.anaconda.org/conda-forge/noarch/asttokens-2.4.1-pyhd8ed1ab_0.conda#5f25798dcefd8252ce5f9dc494d5f571\n+https://conda.anaconda.org/conda-forge/noarch/babel-2.14.0-pyhd8ed1ab_0.conda#9669586875baeced8fc30c0826c3270e\n+https://conda.anaconda.org/conda-forge/noarch/beautifulsoup4-4.12.3-pyha770c72_0.conda#332493000404d8411859539a5a630865\n+https://conda.anaconda.org/conda-forge/noarch/bleach-6.1.0-pyhd8ed1ab_0.conda#0ed9d7c0e9afa7c025807a9a8136ea3e\n+https://conda.anaconda.org/conda-forge/linux-64/cffi-1.16.0-py312hf06ca03_0.conda#56b0ca764ce23cc54f3f7e2a7b970f6d\n+https://conda.anaconda.org/conda-forge/noarch/comm-0.2.2-pyhd8ed1ab_0.conda#948d84721b578d426294e17a02e24cbb\n+https://conda.anaconda.org/conda-forge/noarch/importlib-metadata-7.1.0-pyha770c72_0.conda#0896606848b2dc5cebdf111b6543aa04\n+https://conda.anaconda.org/conda-forge/noarch/importlib_resources-6.4.0-pyhd8ed1ab_0.conda#c5d3907ad8bd7bf557521a1833cf7e6d\n+https://conda.anaconda.org/conda-forge/noarch/jedi-0.19.1-pyhd8ed1ab_0.conda#81a3be0b2023e1ea8555781f0ad904a2\n+https://conda.anaconda.org/conda-forge/noarch/jinja2-3.1.4-pyhd8ed1ab_0.conda#7b86ecb7d3557821c649b3c31e3eb9f2\n+https://conda.anaconda.org/conda-forge/linux-64/jupyter_core-5.7.2-py312h7900ff3_0.conda#eee5a2e3465220ed87196bbb5665f420\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_pygments-0.3.0-pyhd8ed1ab_0.conda#3f0915b1fb2252ab73686a533c5f9d3f\n+https://conda.anaconda.org/conda-forge/noarch/matplotlib-inline-0.1.7-pyhd8ed1ab_0.conda#779345c95648be40d22aaa89de7d4254\n+https://conda.anaconda.org/conda-forge/noarch/pexpect-4.9.0-pyhd8ed1ab_0.conda#629f3203c99b32e0988910c93e77f3b6\n+https://conda.anaconda.org/conda-forge/noarch/pip-24.0-pyhd8ed1ab_0.conda#f586ac1e56c8638b64f9c8122a7b8a67\n+https://conda.anaconda.org/conda-forge/noarch/prompt-toolkit-3.0.42-pyha770c72_0.conda#0bf64bf10eee21f46ac83c161917fa86\n+https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.9.0-pyhd8ed1ab_0.conda#2cf4264fffb9e6eff6031c5b6884d61c\n+https://conda.anaconda.org/conda-forge/linux-64/pyzmq-26.0.3-py312h8fd38d8_0.conda#27efa6d21e98bcab4585a6b913df7625\n+https://conda.anaconda.org/conda-forge/noarch/referencing-0.35.1-pyhd8ed1ab_0.conda#0fc8b52192a8898627c3efae1003e9f6\n+https://conda.anaconda.org/conda-forge/linux-64/ruamel.yaml-0.18.6-py312h98912ed_0.conda#a99a06a875138829ef65f44bbe2c30ca\n+https://conda.anaconda.org/conda-forge/noarch/terminado-0.18.1-pyh0d859eb_0.conda#efba281bbdae5f6b0a1d53c6d4a97c93\n+https://conda.anaconda.org/conda-forge/noarch/tinycss2-1.3.0-pyhd8ed1ab_0.conda#8662629d9a05f9cff364e31ca106c1ac\n+https://conda.anaconda.org/conda-forge/noarch/typing-extensions-4.11.0-hd8ed1ab_0.conda#471e3988f8ca5e9eb3ce6be7eac3bcee\n+https://conda.anaconda.org/conda-forge/noarch/urllib3-2.2.1-pyhd8ed1ab_0.conda#08807a87fa7af10754d46f63b368e016\n+https://conda.anaconda.org/conda-forge/linux-64/argon2-cffi-bindings-21.2.0-py312h98912ed_4.conda#00536e0a1734dcde9815fe227f32fc5a\n+https://conda.anaconda.org/conda-forge/linux-64/cryptography-42.0.7-py312hbcc2302_0.conda#7bc0e1aae21b2e82d03959931f4294f0\n+https://conda.anaconda.org/conda-forge/noarch/jsonschema-specifications-2023.12.1-pyhd8ed1ab_0.conda#a0e4efb5f35786a05af4809a2fb1f855\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_client-7.4.9-pyhd8ed1ab_0.conda#5cbf9a31a19d4ef9103adb7d71fd45fd\n+https://conda.anaconda.org/conda-forge/noarch/mako-1.3.5-pyhd8ed1ab_0.conda#29fddbfa0e2361636a98de4f46ead2ac\n+https://conda.anaconda.org/conda-forge/noarch/requests-2.32.1-pyhd8ed1ab_0.conda#20ce56db6c7aae9e6654b358ecbfc470\n+https://conda.anaconda.org/conda-forge/linux-64/sqlalchemy-2.0.30-py312h9a8786e_0.conda#76b17aead4627038aef36febaa0a5f7d\n+https://conda.anaconda.org/conda-forge/noarch/stack_data-0.6.2-pyhd8ed1ab_0.conda#e7df0fdd404616638df5ece6e69ba7af\n+https://conda.anaconda.org/conda-forge/noarch/alembic-1.13.1-pyhd8ed1ab_1.conda#7b7b0062b0de9f3f71502d31215fcbbb\n+https://conda.anaconda.org/conda-forge/noarch/argon2-cffi-23.1.0-pyhd8ed1ab_0.conda#3afef1f55a1366b4d3b6a0d92e2235e4\n+https://conda.anaconda.org/conda-forge/noarch/ipython-8.24.0-pyh707e725_0.conda#1fb1f1fcbe053a762748dbf0ae4cfd0d\n+https://conda.anaconda.org/conda-forge/noarch/jsonschema-4.22.0-pyhd8ed1ab_0.conda#b9661a4b1200d6bc7d8a4cdafdc91468\n+https://conda.anaconda.org/conda-forge/noarch/oauthlib-3.2.2-pyhd8ed1ab_0.tar.bz2#8f882b197fd9c4941a787926baea4868\n+https://conda.anaconda.org/conda-forge/noarch/pyopenssl-24.0.0-pyhd8ed1ab_0.conda#b50aec2c744a5c493c09cce9e2e7533e\n+https://conda.anaconda.org/conda-forge/noarch/certipy-0.1.3-py_0.tar.bz2#23486713ef5712923e7c57cae609b22e\n+https://conda.anaconda.org/conda-forge/noarch/ipykernel-6.29.3-pyhd33586a_0.conda#e0deff12c601ce5cb7476f93718f3168\n+https://conda.anaconda.org/conda-forge/noarch/ipywidgets-8.1.2-pyhd8ed1ab_1.conda#34072973a80ea997df2ee52c0f6fef78\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_telemetry-0.1.0-pyhd8ed1ab_1.tar.bz2#bb9ebdb6d5aa2622484aff1faceee181\n+https://conda.anaconda.org/conda-forge/noarch/nbformat-5.10.4-pyhd8ed1ab_0.conda#0b57b5368ab7fc7cdc9e3511fa867214\n+https://conda.anaconda.org/conda-forge/noarch/jupyterhub-base-3.1.1-pyh2a2186d_0.conda#78fc5dfd4577a7ba172fa65eb64b6afb\n+https://conda.anaconda.org/conda-forge/noarch/nbclient-0.10.0-pyhd8ed1ab_0.conda#15b51397e0fe8ea7d7da60d83eb76ebc\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-core-7.16.4-pyhd8ed1ab_0.conda#43d9cd74e3950ab09cbddf36f1706b9f\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_server-1.24.0-pyhd8ed1ab_0.conda#7f0d2ec2d4954188ff23503f39823409\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-pandoc-7.16.4-hd8ed1ab_0.conda#391934bd1a79990c23df1d1809ddc821\n+https://conda.anaconda.org/conda-forge/noarch/jupyter-resource-usage-0.7.1-pyhd8ed1ab_0.conda#62ed54dc748d19d0cf056a3f90cc8f12\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_server-2.27.1-pyhd8ed1ab_0.conda#d97923b777ce837cf67e7858ac600834\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-7.16.4-hd8ed1ab_0.conda#c9d64b8a7ee8e6bdbf0e7d8aa7f39601\n+https://conda.anaconda.org/conda-forge/noarch/notebook-shim-0.2.4-pyhd8ed1ab_0.conda#3d85618e2c97ab896b5b5e298d32b5b3\n+https://conda.anaconda.org/conda-forge/noarch/nbclassic-1.0.0-pyhb4ecaf3_1.conda#a0be31e9bd84d6eae87cdbf74c56b90b\n+https://conda.anaconda.org/conda-forge/noarch/notebook-6.5.7-pyha770c72_0.conda#f81a6fe643390df9347984644727d796\n+https://conda.anaconda.org/conda-forge/noarch/jupyter-offlinenotebook-0.2.2-pyh1d7be83_0.tar.bz2#fe55056ce4bc4bd4953ba440270735fb\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab-3.5.3-pyhd8ed1ab_0.conda#69f71bc3d176b3ad3d9564a32bd049b8\n+https://conda.anaconda.org/conda-forge/noarch/jupyterhub-singleuser-3.1.1-pyh2a2186d_0.conda#6d0a3394efe9ff7c0ad1ed03a6ca3720\ndiff --git a/repo2docker/buildpacks/conda/environment.py-3.12-linux-aarch64.lock b/repo2docker/buildpacks/conda/environment.py-3.12-linux-aarch64.lock\nnew file mode 100644\nindex 000000000..26a5084ce\n--- /dev/null\n+++ b/repo2docker/buildpacks/conda/environment.py-3.12-linux-aarch64.lock\n@@ -0,0 +1,151 @@\n+# AUTO GENERATED FROM environment.py-3.12.yml, DO NOT MANUALLY MODIFY\n+# Frozen on 2024-05-21 07:52:51 UTC\n+# Generated by conda-lock.\n+# platform: linux-aarch64\n+# input_hash: 491fc7a1f5c9466e7bd91af66c076976c6b35633496e179564141fdb96a7c34a\n+@EXPLICIT\n+https://conda.anaconda.org/conda-forge/linux-aarch64/ca-certificates-2024.2.2-hcefe29a_0.conda#57c226edb90c4e973b9b7503537dd339\n+https://conda.anaconda.org/conda-forge/linux-aarch64/ld_impl_linux-aarch64-2.40-hba4e955_0.conda#b55c1cb33c63d23b542fa53f24541e56\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libgomp-13.2.0-he277a41_7.conda#1d1691ec9e5be799f86310fa38f00b9f\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libstdcxx-ng-13.2.0-h3f4de04_7.conda#2a54872c7fab2db99b0074212d8efe64\n+https://conda.anaconda.org/conda-forge/linux-aarch64/pandoc-3.2-h8af1aa0_0.conda#7aaeabb3bef6483bc8437e541809027c\n+https://conda.anaconda.org/conda-forge/linux-aarch64/python_abi-3.12-4_cp312.conda#6c09f8e580146d88f649780cebed01de\n+https://conda.anaconda.org/conda-forge/noarch/tzdata-2024a-h0c530f3_0.conda#161081fc7cec0bfda0d86d7cb595f8d8\n+https://conda.anaconda.org/conda-forge/linux-aarch64/_openmp_mutex-4.5-2_gnu.tar.bz2#6168d71addc746e8f2b8d57dfd2edcea\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libgcc-ng-13.2.0-he277a41_7.conda#01c5b27ce46f50abab2dc8454842c792\n+https://conda.anaconda.org/conda-forge/linux-aarch64/bzip2-1.0.8-h31becfc_5.conda#a64e35f01e0b7a2a152eca87d33b9c87\n+https://conda.anaconda.org/conda-forge/linux-aarch64/icu-73.2-h787c7f5_0.conda#9d3c29d71f28452a2e843aff8cbe09d2\n+https://conda.anaconda.org/conda-forge/linux-aarch64/keyutils-1.6.1-h4e544f5_0.tar.bz2#1f24853e59c68892452ef94ddd8afd4b\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libexpat-2.6.2-h2f0025b_0.conda#1b9f46b804a2c3c5d7fd6a80b77c35f9\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libffi-3.4.2-h3557bc0_5.tar.bz2#dddd85f4d52121fab0a8b099c5e06501\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libnsl-2.0.1-h31becfc_0.conda#c14f32510f694e3185704d89967ec422\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libsodium-1.0.18-hb9de7d4_1.tar.bz2#d09ab3c60eebb6f14eb4d07e172775cc\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libuuid-2.38.1-hb4cce97_0.conda#000e30b09db0b7c775b21695dff30969\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libuv-1.48.0-h31becfc_0.conda#97f754b22f63a943345bd807e1d51e01\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libxcrypt-4.4.36-h31becfc_1.conda#b4df5d7d4b63579d081fd3a4cf99740e\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libzlib-1.2.13-h31becfc_5.conda#b213aa87eea9491ef7b129179322e955\n+https://conda.anaconda.org/conda-forge/linux-aarch64/ncurses-6.5-h0425590_0.conda#38362af7bfac0efef69675acee564458\n+https://conda.anaconda.org/conda-forge/linux-aarch64/openssl-3.3.0-h31becfc_0.conda#36ca60a3afaf2ea2c460daeebd67430e\n+https://conda.anaconda.org/conda-forge/linux-aarch64/xz-5.2.6-h9cdd2b7_0.tar.bz2#83baad393a31d59c20b63ba4da6592df\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libedit-3.1.20191231-he28a2e2_2.tar.bz2#29371161d77933a54fccf1bb66b96529\n+https://conda.anaconda.org/conda-forge/linux-aarch64/libsqlite-3.45.3-h194ca79_0.conda#fb35b8afbe9e92467ac7b5608d60b775\n+https://conda.anaconda.org/conda-forge/linux-aarch64/readline-8.2-h8fc344f_1.conda#105eb1e16bf83bfb2eb380a48032b655\n+https://conda.anaconda.org/conda-forge/linux-aarch64/tk-8.6.13-h194ca79_0.conda#f75105e0585851f818e0009dd1dde4dc\n+https://conda.anaconda.org/conda-forge/linux-aarch64/zlib-1.2.13-h31becfc_5.conda#96866c7301479abaf8308c50958c71a4\n+https://conda.anaconda.org/conda-forge/linux-aarch64/krb5-1.21.2-hc419048_0.conda#55b51af37bf6fdcfe06f140e62e8c8db\n+https://conda.anaconda.org/conda-forge/linux-aarch64/nodejs-18.20.2-ha5f1e90_0.conda#0be733836fc7820443c22f24ae8d9bf3\n+https://conda.anaconda.org/conda-forge/linux-aarch64/python-3.12.3-h43d1f9e_0_cpython.conda#dc93ad1d2ba17ebc948bf5434ffa864b\n+https://conda.anaconda.org/conda-forge/noarch/async_generator-1.10-py_0.tar.bz2#d56c596e61b1c4952acf0a9920856c12\n+https://conda.anaconda.org/conda-forge/noarch/attrs-23.2.0-pyh71513ae_0.conda#5e4c0743c70186509d1412e03c2d8dfa\n+https://conda.anaconda.org/conda-forge/noarch/blinker-1.8.2-pyhd8ed1ab_0.conda#cf85c002319c15e9721934104aaa1137\n+https://conda.anaconda.org/conda-forge/linux-aarch64/brotli-python-1.1.0-py312h2aa54b4_1.conda#7253fd6feb797007a3d290bbcfd23a84\n+https://conda.anaconda.org/conda-forge/noarch/certifi-2024.2.2-pyhd8ed1ab_0.conda#0876280e409658fc6f9e75d035960333\n+https://conda.anaconda.org/conda-forge/noarch/charset-normalizer-3.3.2-pyhd8ed1ab_0.conda#7f4a9e3fcff3f6356ae99244a014da6a\n+https://conda.anaconda.org/conda-forge/linux-aarch64/debugpy-1.8.1-py312h2aa54b4_0.conda#2893bc781f67a580ad93f32f56c921e7\n+https://conda.anaconda.org/conda-forge/noarch/decorator-5.1.1-pyhd8ed1ab_0.tar.bz2#43afe5ab04e35e17ba28649471dd7364\n+https://conda.anaconda.org/conda-forge/noarch/defusedxml-0.7.1-pyhd8ed1ab_0.tar.bz2#961b3a227b437d82ad7054484cfa71b2\n+https://conda.anaconda.org/conda-forge/noarch/entrypoints-0.4-pyhd8ed1ab_0.tar.bz2#3cf04868fee0a029769bd41f4b2fbf2d\n+https://conda.anaconda.org/conda-forge/noarch/exceptiongroup-1.2.0-pyhd8ed1ab_2.conda#8d652ea2ee8eaee02ed8dc820bc794aa\n+https://conda.anaconda.org/conda-forge/noarch/executing-2.0.1-pyhd8ed1ab_0.conda#e16be50e378d8a4533b989035b196ab8\n+https://conda.anaconda.org/conda-forge/linux-aarch64/greenlet-3.0.3-py312h2aa54b4_0.conda#68e5a7d27bbe600bc098175e25509351\n+https://conda.anaconda.org/conda-forge/noarch/idna-3.7-pyhd8ed1ab_0.conda#c0cc1420498b17414d8617d0b9f506ca\n+https://conda.anaconda.org/conda-forge/noarch/ipython_genutils-0.2.0-py_1.tar.bz2#5071c982548b3a20caf70462f04f5287\n+https://conda.anaconda.org/conda-forge/noarch/json5-0.9.25-pyhd8ed1ab_0.conda#5d8c241a9261e720a34a07a3e1ac4109\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_widgets-3.0.10-pyhd8ed1ab_0.conda#16b73b2c4ff7dda8bbecf88aadfe2027\n+https://conda.anaconda.org/conda-forge/linux-aarch64/markupsafe-2.1.5-py312h9ef2f89_0.conda#a00135adde3dfe19b9962c3c767c2129\n+https://conda.anaconda.org/conda-forge/noarch/mistune-3.0.2-pyhd8ed1ab_0.conda#5cbee699846772cc939bef23a0d524ed\n+https://conda.anaconda.org/conda-forge/noarch/nest-asyncio-1.6.0-pyhd8ed1ab_0.conda#6598c056f64dc8800d40add25e4e2c34\n+https://conda.anaconda.org/conda-forge/noarch/packaging-24.0-pyhd8ed1ab_0.conda#248f521b64ce055e7feae3105e7abeb8\n+https://conda.anaconda.org/conda-forge/noarch/pamela-1.1.0-pyh1a96a4e_0.conda#dce93ccf07159017c69c6d9cea2d5d61\n+https://conda.anaconda.org/conda-forge/noarch/pandocfilters-1.5.0-pyhd8ed1ab_0.tar.bz2#457c2c8c08e54905d6954e79cb5b5db9\n+https://conda.anaconda.org/conda-forge/noarch/parso-0.8.4-pyhd8ed1ab_0.conda#81534b420deb77da8833f2289b8d47ac\n+https://conda.anaconda.org/conda-forge/noarch/pickleshare-0.7.5-py_1003.tar.bz2#415f0ebb6198cc2801c73438a9fb5761\n+https://conda.anaconda.org/conda-forge/noarch/pkgutil-resolve-name-1.3.10-pyhd8ed1ab_1.conda#405678b942f2481cecdb3e010f4925d9\n+https://conda.anaconda.org/conda-forge/noarch/platformdirs-4.2.2-pyhd8ed1ab_0.conda#6f6cf28bf8e021933869bae3f84b8fc9\n+https://conda.anaconda.org/conda-forge/noarch/prometheus_client-0.20.0-pyhd8ed1ab_0.conda#9a19b94034dd3abb2b348c8b93388035\n+https://conda.anaconda.org/conda-forge/linux-aarch64/psutil-5.9.8-py312hdd3e373_0.conda#1a87f588762a48fb42311b0c40fa1920\n+https://conda.anaconda.org/conda-forge/noarch/ptyprocess-0.7.0-pyhd3deb0d_0.tar.bz2#359eeb6536da0e687af562ed265ec263\n+https://conda.anaconda.org/conda-forge/noarch/pure_eval-0.2.2-pyhd8ed1ab_0.tar.bz2#6784285c7e55cb7212efabc79e4c2883\n+https://conda.anaconda.org/conda-forge/noarch/pycparser-2.22-pyhd8ed1ab_0.conda#844d9eb3b43095b031874477f7d70088\n+https://conda.anaconda.org/conda-forge/noarch/pygments-2.18.0-pyhd8ed1ab_0.conda#b7f5c092b8f9800150d998a71b76d5a1\n+https://conda.anaconda.org/conda-forge/noarch/pyjwt-2.8.0-pyhd8ed1ab_1.conda#74f76d4868dbba5870f2cf1d9b12d8f3\n+https://conda.anaconda.org/conda-forge/noarch/pysocks-1.7.1-pyha2e5f31_6.tar.bz2#2a7de29fb590ca14b5243c4c812c8025\n+https://conda.anaconda.org/conda-forge/noarch/python-fastjsonschema-2.19.1-pyhd8ed1ab_0.conda#4d3ceee3af4b0f9a1f48f57176bf8625\n+https://conda.anaconda.org/conda-forge/noarch/python-json-logger-2.0.7-pyhd8ed1ab_0.conda#a61bf9ec79426938ff785eb69dbb1960\n+https://conda.anaconda.org/conda-forge/noarch/pytz-2024.1-pyhd8ed1ab_0.conda#3eeeeb9e4827ace8c0c1419c85d590ad\n+https://conda.anaconda.org/conda-forge/linux-aarch64/rpds-py-0.18.1-py312heb99873_0.conda#cdc3eaa8b1373393c4d42f437ce4b603\n+https://conda.anaconda.org/conda-forge/linux-aarch64/ruamel.yaml.clib-0.2.8-py312hdd3e373_0.conda#7d6fe36395d184fd7cfa4469c722339f\n+https://conda.anaconda.org/conda-forge/noarch/send2trash-1.8.3-pyh0d859eb_0.conda#778594b20097b5a948c59e50ae42482a\n+https://conda.anaconda.org/conda-forge/noarch/setuptools-69.5.1-pyhd8ed1ab_0.conda#7462280d81f639363e6e63c81276bd9e\n+https://conda.anaconda.org/conda-forge/noarch/six-1.16.0-pyh6c4a22f_0.tar.bz2#e5f25f8dbc060e9a8d912e432202afc2\n+https://conda.anaconda.org/conda-forge/noarch/sniffio-1.3.1-pyhd8ed1ab_0.conda#490730480d76cf9c8f8f2849719c6e2b\n+https://conda.anaconda.org/conda-forge/noarch/soupsieve-2.5-pyhd8ed1ab_1.conda#3f144b2c34f8cb5a9abd9ed23a39c561\n+https://conda.anaconda.org/conda-forge/noarch/tomli-2.0.1-pyhd8ed1ab_0.tar.bz2#5844808ffab9ebdb694585b50ba02a96\n+https://conda.anaconda.org/conda-forge/linux-aarch64/tornado-6.4-py312h9ef2f89_0.conda#694f216cc566b84a983e976b4fb6f569\n+https://conda.anaconda.org/conda-forge/noarch/traitlets-5.14.3-pyhd8ed1ab_0.conda#3df84416a021220d8b5700c613af2dc5\n+https://conda.anaconda.org/conda-forge/noarch/typing_extensions-4.11.0-pyha770c72_0.conda#6ef2fc37559256cf682d8b3375e89b80\n+https://conda.anaconda.org/conda-forge/noarch/wcwidth-0.2.13-pyhd8ed1ab_0.conda#68f0738df502a14213624b288c60c9ad\n+https://conda.anaconda.org/conda-forge/noarch/webencodings-0.5.1-pyhd8ed1ab_2.conda#daf5160ff9cde3a468556965329085b9\n+https://conda.anaconda.org/conda-forge/noarch/websocket-client-1.8.0-pyhd8ed1ab_0.conda#f372c576b8774922da83cda2b12f9d29\n+https://conda.anaconda.org/conda-forge/noarch/wheel-0.43.0-pyhd8ed1ab_1.conda#0b5293a157c2b5cd513dd1b03d8d3aae\n+https://conda.anaconda.org/conda-forge/noarch/widgetsnbextension-4.0.10-pyhd8ed1ab_0.conda#521f489e3babeddeec638c2add7e9e64\n+https://conda.anaconda.org/conda-forge/linux-aarch64/zeromq-4.3.5-h28faeed_4.conda#ae8c0108b4d6a5981bce484eeaaf1b55\n+https://conda.anaconda.org/conda-forge/noarch/zipp-3.17.0-pyhd8ed1ab_0.conda#2e4d6bc0b14e10f895fc6791a7d9b26a\n+https://conda.anaconda.org/conda-forge/noarch/anyio-3.7.1-pyhd8ed1ab_0.conda#7b517e7a6f0790337906c055aa97ca49\n+https://conda.anaconda.org/conda-forge/noarch/asttokens-2.4.1-pyhd8ed1ab_0.conda#5f25798dcefd8252ce5f9dc494d5f571\n+https://conda.anaconda.org/conda-forge/noarch/babel-2.14.0-pyhd8ed1ab_0.conda#9669586875baeced8fc30c0826c3270e\n+https://conda.anaconda.org/conda-forge/noarch/beautifulsoup4-4.12.3-pyha770c72_0.conda#332493000404d8411859539a5a630865\n+https://conda.anaconda.org/conda-forge/noarch/bleach-6.1.0-pyhd8ed1ab_0.conda#0ed9d7c0e9afa7c025807a9a8136ea3e\n+https://conda.anaconda.org/conda-forge/linux-aarch64/cffi-1.16.0-py312hf3c74c0_0.conda#2b087fcfbe35a1bb081e1723b8c6362a\n+https://conda.anaconda.org/conda-forge/noarch/comm-0.2.2-pyhd8ed1ab_0.conda#948d84721b578d426294e17a02e24cbb\n+https://conda.anaconda.org/conda-forge/noarch/importlib-metadata-7.1.0-pyha770c72_0.conda#0896606848b2dc5cebdf111b6543aa04\n+https://conda.anaconda.org/conda-forge/noarch/importlib_resources-6.4.0-pyhd8ed1ab_0.conda#c5d3907ad8bd7bf557521a1833cf7e6d\n+https://conda.anaconda.org/conda-forge/noarch/jedi-0.19.1-pyhd8ed1ab_0.conda#81a3be0b2023e1ea8555781f0ad904a2\n+https://conda.anaconda.org/conda-forge/noarch/jinja2-3.1.4-pyhd8ed1ab_0.conda#7b86ecb7d3557821c649b3c31e3eb9f2\n+https://conda.anaconda.org/conda-forge/linux-aarch64/jupyter_core-5.7.2-py312h996f985_0.conda#a7ef8076152a1490e2e8a519e81660d7\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_pygments-0.3.0-pyhd8ed1ab_0.conda#3f0915b1fb2252ab73686a533c5f9d3f\n+https://conda.anaconda.org/conda-forge/noarch/matplotlib-inline-0.1.7-pyhd8ed1ab_0.conda#779345c95648be40d22aaa89de7d4254\n+https://conda.anaconda.org/conda-forge/noarch/pexpect-4.9.0-pyhd8ed1ab_0.conda#629f3203c99b32e0988910c93e77f3b6\n+https://conda.anaconda.org/conda-forge/noarch/pip-24.0-pyhd8ed1ab_0.conda#f586ac1e56c8638b64f9c8122a7b8a67\n+https://conda.anaconda.org/conda-forge/noarch/prompt-toolkit-3.0.42-pyha770c72_0.conda#0bf64bf10eee21f46ac83c161917fa86\n+https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.9.0-pyhd8ed1ab_0.conda#2cf4264fffb9e6eff6031c5b6884d61c\n+https://conda.anaconda.org/conda-forge/linux-aarch64/pyzmq-26.0.3-py312h7059f03_0.conda#5f5c7c8e6281711957bfbfdd3b7dcaa5\n+https://conda.anaconda.org/conda-forge/noarch/referencing-0.35.1-pyhd8ed1ab_0.conda#0fc8b52192a8898627c3efae1003e9f6\n+https://conda.anaconda.org/conda-forge/linux-aarch64/ruamel.yaml-0.18.6-py312hdd3e373_0.conda#675a11ab58c2461d33d37275d117dcd2\n+https://conda.anaconda.org/conda-forge/noarch/terminado-0.18.1-pyh0d859eb_0.conda#efba281bbdae5f6b0a1d53c6d4a97c93\n+https://conda.anaconda.org/conda-forge/noarch/tinycss2-1.3.0-pyhd8ed1ab_0.conda#8662629d9a05f9cff364e31ca106c1ac\n+https://conda.anaconda.org/conda-forge/noarch/typing-extensions-4.11.0-hd8ed1ab_0.conda#471e3988f8ca5e9eb3ce6be7eac3bcee\n+https://conda.anaconda.org/conda-forge/noarch/urllib3-2.2.1-pyhd8ed1ab_0.conda#08807a87fa7af10754d46f63b368e016\n+https://conda.anaconda.org/conda-forge/linux-aarch64/argon2-cffi-bindings-21.2.0-py312hdd3e373_4.conda#7aebbb1531055d298dc01002e1286f3a\n+https://conda.anaconda.org/conda-forge/linux-aarch64/cryptography-42.0.7-py312h608731a_0.conda#e8535008b575cd0135c96ca408e88fc6\n+https://conda.anaconda.org/conda-forge/noarch/jsonschema-specifications-2023.12.1-pyhd8ed1ab_0.conda#a0e4efb5f35786a05af4809a2fb1f855\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_client-7.4.9-pyhd8ed1ab_0.conda#5cbf9a31a19d4ef9103adb7d71fd45fd\n+https://conda.anaconda.org/conda-forge/noarch/mako-1.3.5-pyhd8ed1ab_0.conda#29fddbfa0e2361636a98de4f46ead2ac\n+https://conda.anaconda.org/conda-forge/noarch/requests-2.32.1-pyhd8ed1ab_0.conda#20ce56db6c7aae9e6654b358ecbfc470\n+https://conda.anaconda.org/conda-forge/linux-aarch64/sqlalchemy-2.0.30-py312h5adff4d_0.conda#7d8534ca8c6740bdd4affeedd01c17cd\n+https://conda.anaconda.org/conda-forge/noarch/stack_data-0.6.2-pyhd8ed1ab_0.conda#e7df0fdd404616638df5ece6e69ba7af\n+https://conda.anaconda.org/conda-forge/noarch/alembic-1.13.1-pyhd8ed1ab_1.conda#7b7b0062b0de9f3f71502d31215fcbbb\n+https://conda.anaconda.org/conda-forge/noarch/argon2-cffi-23.1.0-pyhd8ed1ab_0.conda#3afef1f55a1366b4d3b6a0d92e2235e4\n+https://conda.anaconda.org/conda-forge/noarch/ipython-8.24.0-pyh707e725_0.conda#1fb1f1fcbe053a762748dbf0ae4cfd0d\n+https://conda.anaconda.org/conda-forge/noarch/jsonschema-4.22.0-pyhd8ed1ab_0.conda#b9661a4b1200d6bc7d8a4cdafdc91468\n+https://conda.anaconda.org/conda-forge/noarch/oauthlib-3.2.2-pyhd8ed1ab_0.tar.bz2#8f882b197fd9c4941a787926baea4868\n+https://conda.anaconda.org/conda-forge/noarch/pyopenssl-24.0.0-pyhd8ed1ab_0.conda#b50aec2c744a5c493c09cce9e2e7533e\n+https://conda.anaconda.org/conda-forge/noarch/certipy-0.1.3-py_0.tar.bz2#23486713ef5712923e7c57cae609b22e\n+https://conda.anaconda.org/conda-forge/noarch/ipykernel-6.29.3-pyhd33586a_0.conda#e0deff12c601ce5cb7476f93718f3168\n+https://conda.anaconda.org/conda-forge/noarch/ipywidgets-8.1.2-pyhd8ed1ab_1.conda#34072973a80ea997df2ee52c0f6fef78\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_telemetry-0.1.0-pyhd8ed1ab_1.tar.bz2#bb9ebdb6d5aa2622484aff1faceee181\n+https://conda.anaconda.org/conda-forge/noarch/nbformat-5.10.4-pyhd8ed1ab_0.conda#0b57b5368ab7fc7cdc9e3511fa867214\n+https://conda.anaconda.org/conda-forge/noarch/jupyterhub-base-3.1.1-pyh2a2186d_0.conda#78fc5dfd4577a7ba172fa65eb64b6afb\n+https://conda.anaconda.org/conda-forge/noarch/nbclient-0.10.0-pyhd8ed1ab_0.conda#15b51397e0fe8ea7d7da60d83eb76ebc\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-core-7.16.4-pyhd8ed1ab_0.conda#43d9cd74e3950ab09cbddf36f1706b9f\n+https://conda.anaconda.org/conda-forge/noarch/jupyter_server-1.24.0-pyhd8ed1ab_0.conda#7f0d2ec2d4954188ff23503f39823409\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-pandoc-7.16.4-hd8ed1ab_0.conda#391934bd1a79990c23df1d1809ddc821\n+https://conda.anaconda.org/conda-forge/noarch/jupyter-resource-usage-0.7.1-pyhd8ed1ab_0.conda#62ed54dc748d19d0cf056a3f90cc8f12\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab_server-2.27.1-pyhd8ed1ab_0.conda#d97923b777ce837cf67e7858ac600834\n+https://conda.anaconda.org/conda-forge/noarch/nbconvert-7.16.4-hd8ed1ab_0.conda#c9d64b8a7ee8e6bdbf0e7d8aa7f39601\n+https://conda.anaconda.org/conda-forge/noarch/notebook-shim-0.2.4-pyhd8ed1ab_0.conda#3d85618e2c97ab896b5b5e298d32b5b3\n+https://conda.anaconda.org/conda-forge/noarch/nbclassic-1.0.0-pyhb4ecaf3_1.conda#a0be31e9bd84d6eae87cdbf74c56b90b\n+https://conda.anaconda.org/conda-forge/noarch/notebook-6.5.7-pyha770c72_0.conda#f81a6fe643390df9347984644727d796\n+https://conda.anaconda.org/conda-forge/noarch/jupyter-offlinenotebook-0.2.2-pyh1d7be83_0.tar.bz2#fe55056ce4bc4bd4953ba440270735fb\n+https://conda.anaconda.org/conda-forge/noarch/jupyterlab-3.5.3-pyhd8ed1ab_0.conda#69f71bc3d176b3ad3d9564a32bd049b8\n+https://conda.anaconda.org/conda-forge/noarch/jupyterhub-singleuser-3.1.1-pyh2a2186d_0.conda#6d0a3394efe9ff7c0ad1ed03a6ca3720\ndiff --git a/repo2docker/buildpacks/conda/environment.py-3.12.yml b/repo2docker/buildpacks/conda/environment.py-3.12.yml\nnew file mode 100644\nindex 000000000..323a83f50\n--- /dev/null\n+++ b/repo2docker/buildpacks/conda/environment.py-3.12.yml\n@@ -0,0 +1,15 @@\n+# AUTO GENERATED FROM environment.yml, DO NOT MANUALLY MODIFY\n+# Generated on 2024-05-21 07:52:44 UTC\n+channels:\n+- conda-forge\n+dependencies:\n+- python=3.12.*\n+- nodejs=18\n+- pip\n+- ipywidgets==8.*     # https://github.com/jupyter-widgets/ipywidgets\n+- jupyter-offlinenotebook==0.2.*     # https://github.com/manics/jupyter-offlinenotebook\n+- jupyter-resource-usage==0.7.*     # https://github.com/jupyter-server/jupyter-resource-usage\n+- jupyter_server==1.*     # https://github.com/jupyter-server/jupyter_server\n+- jupyterhub-singleuser==3.*     # https://github.com/jupyterhub/jupyterhub\n+- jupyterlab==3.*     # https://github.com/jupyterlab/jupyterlab\n+- notebook==6.*     # https://github.com/jupyter/notebook\ndiff --git a/repo2docker/buildpacks/conda/freeze.py b/repo2docker/buildpacks/conda/freeze.py\nindex a3a18b2b0..aa1fed222 100755\n--- a/repo2docker/buildpacks/conda/freeze.py\n+++ b/repo2docker/buildpacks/conda/freeze.py\n@@ -113,7 +113,7 @@ def set_python(py_env_file, py):\n         \"py\",\n         nargs=\"*\",\n         help=\"Python version(s) to update and freeze\",\n-        default=(\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\"),\n+        default=(\"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"),\n     )\n     parser.add_argument(\n         \"platform\",\n", "instance_id": "jupyterhub__repo2docker-1347", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to support Python 3.11 and potentially automate support for future Python releases in a Binder environment. It outlines the goal (supporting newer Python versions), the context (mybinder.org usage), and provides some details on the manual process and the desired automation. However, there are minor ambiguities and missing details. For instance, it lacks specificity on how the automation should interact with conda-forge (e.g., exact API endpoints or update frequency), and there are no explicit constraints or requirements for backward compatibility or testing. Additionally, while alternative options are mentioned (filing requests yearly), there\u2019s no clear prioritization or detailed workflow for implementation. Edge cases, such as handling failed updates or version incompatibilities, are not addressed. Overall, the statement is valid and understandable but leaves some gaps that could lead to implementation questions.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes primarily involve adding support for Python 3.12 by updating environment lock files and modifying a script (`freeze.py`) to include the new version in the default list. The changes are localized to a few files and are mostly mechanical (adding new environment files and updating a tuple of versions). However, the broader goal of automating updates for future Python versions introduces additional complexity, potentially requiring interaction with external APIs (conda-forge) and setting up GitHub Actions for automation. This aspect slightly elevates the difficulty beyond a trivial update but remains manageable.\n\n2. **Technical Concepts Involved**: For the basic task of adding Python 3.11/3.12 support, the required knowledge is straightforward\u2014understanding conda environments, YAML configuration, and basic Python scripting. The automation part requires familiarity with GitHub Actions, API interactions (likely RESTful queries to conda-forge), and possibly cron-based scheduling or event-driven workflows. These concepts are not overly complex for an intermediate developer but do require some additional learning or experience beyond basic scripting.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but implementing automation would require handling scenarios like API downtime, version incompatibilities, or failed environment builds. The current code changes (adding lock files) do not address error handling, but these considerations are implicit in the automation goal. The complexity of edge cases is moderate and does not significantly increase the difficulty at this stage.\n\n4. **Overall Impact and Effort**: Adding support for a new Python version is a low-effort task, as demonstrated by the simplicity of the `freeze.py` change and the generation of lock files. The automation feature, while more involved, is estimated in the problem statement to take \"a few days,\" which aligns with an easy-to-medium difficulty level. It does not impact the core architecture of the system (Binder or repo2docker) but rather extends an existing workflow. The problem does not require deep domain-specific knowledge beyond conda and CI/CD tools.\n\nGiven these factors, a score of 0.35 reflects a task that is slightly more challenging than a basic modification due to the automation aspect but remains within the realm of easy-to-implement features for someone with moderate experience in Python and CI/CD workflows.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG]: retranslating local exception with custom data class hangs in free-threading cpython\n### Required prerequisites\r\n\r\n- [X] Make sure you've read the [documentation](https://pybind11.readthedocs.io). Your issue may be addressed there.\r\n- [X] Search the [issue tracker](https://github.com/pybind/pybind11/issues) and [Discussions](https:/pybind/pybind11/discussions) to verify that this hasn't already been reported. +1 or comment there if it has.\r\n- [X] Consider asking first in the [Gitter chat room](https://gitter.im/pybind/Lobby) or in a [Discussion](https:/pybind/pybind11/discussions/new).\r\n\r\n### What version (or hash if on master) of pybind11 are you using?\r\n\r\n2.13.5 and from source\r\n\r\n### Problem description\r\n\r\nI have a program stuck when retranslating local exception with custom data class using CPython with free-threading mode enabled.\r\n\r\nThis is due to `internals.mutex` being in the locked state when the callback is running. When using custom data, `all_type_info_get_cache` method is called internally which also tries to lock `internals.mutex`.\r\n\r\nSee the repro code below\r\n\r\n### Reproducible example code\r\n\r\nC++ test1.cpp\r\n\r\n```c++\r\n// https://pybind11.readthedocs.io/en/stable/basics.html\r\n// clang++ -O3 -Wall -shared -std=c++11 -fPIC $(python -m pybind11 --includes) test1.cpp -o test1.so\r\n// python -c \"import test1; test1.check()\"\r\n\r\n#include <string>\r\n#include <pybind11/pybind11.h>\r\n#include <pybind11/stl.h>\r\n\r\n\r\nnamespace py = pybind11;\r\n\r\nstruct CustomData {\r\n  CustomData(const std::string & a): a(a) {}\r\n  std::string a;\r\n};\r\n\r\n\r\nstruct CustomError {\r\n  CustomError(const CustomData & message) : message(message) {}\r\n  CustomData message;\r\n};\r\n\r\n\r\nPYBIND11_MODULE(test1, m, py::mod_gil_not_used()) {\r\n  m.doc() = \"test custom exception with free-threading\";\r\n\r\n  m.def(\"check\", [](){\r\n    auto d1 = CustomData(\"abc\");\r\n    throw CustomError(d1);\r\n  });\r\n\r\n  py::class_<CustomData>(m, \"CustomData\", py::module_local())\r\n        .def(py::init<const std::string &>())\r\n        .def_readwrite(\"a\", &CustomData::a);\r\n  py::register_local_exception_translator([](std::exception_ptr p) {\r\n    try {\r\n      if (p)\r\n        std::rethrow_exception(p);\r\n    } catch (const CustomError &e) {\r\n\r\n      printf(\"Handle CustomError exception\\n\");\r\n      auto mod = py::module_::import(\"exceptions1\");\r\n      py::object obj = mod.attr(\"CustomError\");\r\n      printf(\"Before the exception creation\\n\");\r\n\r\n      // Here we can check that internals.mutex is locked: internals.mutex.mutex._bits == 1\r\n      // obj(e.message) calls `all_type_info_get_cache` which would try to lock again internals\r\n\r\n      // // If we unlock internals.mutex then obj(e.message) will work otherwise\r\n      // // execution hangs.\r\n      // auto &internals = py::detail::get_internals();\r\n      // internals.mutex.unlock();\r\n\r\n      py::object obj2 = obj(e.message);\r\n\r\n      // We should lock again if we unlocked it\r\n      // internals.mutex.lock();\r\n\r\n      printf(\"After the exception creation\\n\");\r\n      PyErr_SetObject(PyExc_Exception, obj2.ptr());\r\n\r\n    }\r\n  });\r\n}\r\n```\r\n\r\nPython code: `exceptions1.py`\r\n```python\r\nclass CustomError(Exception):\r\n    def __init__(self, message):\r\n        self.message = message\r\n        super().__init__(message)\r\n\r\n    def __str__(self):\r\n        s = \"[python]: \" + self.message.a\r\n        return s\r\n```\r\n\r\nRun:\r\n```bash\r\npython -c \"import test1; test1.check()\"\r\n```\r\nOutput:\r\n```bash\r\nHandle CustomError exception\r\nBefore the exception creation\r\n```\r\n\r\n\r\n### Is this a regression? Put the last known working version here if it is.\r\n\r\nNot a regression\n", "patch": "diff --git a/.github/workflows/emscripten.yaml b/.github/workflows/emscripten.yaml\nindex fe4cd821c6..18a1ad464a 100644\n--- a/.github/workflows/emscripten.yaml\n+++ b/.github/workflows/emscripten.yaml\n@@ -23,7 +23,7 @@ jobs:\n         submodules: true\n         fetch-depth: 0\n \n-    - uses: pypa/cibuildwheel@v2.20\n+    - uses: pypa/cibuildwheel@v2.21\n       env:\n         PYODIDE_BUILD_EXPORTS: whole_archive\n       with:\ndiff --git a/.github/workflows/pip.yml b/.github/workflows/pip.yml\nindex f241a83a73..77ecfb10bb 100644\n--- a/.github/workflows/pip.yml\n+++ b/.github/workflows/pip.yml\n@@ -92,11 +92,12 @@ jobs:\n     runs-on: ubuntu-latest\n     if: github.event_name == 'release' && github.event.action == 'published'\n     needs: [packaging]\n-    environment: pypi\n+    environment:\n+      name: pypi\n+      url: https://pypi.org/p/pybind11\n     permissions:\n       id-token: write\n       attestations: write\n-      contents: read\n \n     steps:\n     # Downloads all to directories matching the artifact names\n@@ -111,8 +112,10 @@ jobs:\n       uses: pypa/gh-action-pypi-publish@release/v1\n       with:\n         packages-dir: standard/\n+        attestations: true\n \n     - name: Publish global package\n       uses: pypa/gh-action-pypi-publish@release/v1\n       with:\n         packages-dir: global/\n+        attestations: true\ndiff --git a/docs/changelog.rst b/docs/changelog.rst\nindex 0145317743..a91082113f 100644\n--- a/docs/changelog.rst\n+++ b/docs/changelog.rst\n@@ -31,6 +31,37 @@ New Features:\n * The ``array_caster`` in pybind11/stl.h was enhanced to support value types that are not default-constructible.\n   `#5305 <https://github.com/pybind/pybind11/pull/5305>`_\n \n+* Added ``py::warnings`` namespace with ``py::warnings::warn`` and ``py::warnings::new_warning_type`` that provides the interface for Python warnings.\n+  `#5291 <https://github.com/pybind/pybind11/pull/5291>`_\n+\n+Version 2.13.6 (September 13, 2024)\n+-----------------------------------\n+\n+New Features:\n+\n+* A new ``self._pybind11_conduit_v1_()`` method is automatically added to all\n+  ``py::class_``-wrapped types, to enable type-safe interoperability between\n+  different independent Python/C++ bindings systems, including pybind11\n+  versions with different ``PYBIND11_INTERNALS_VERSION``'s. Supported on\n+  pybind11 2.11.2, 2.12.1, and 2.13.6+.\n+  `#5296 <https://github.com/pybind/pybind11/pull/5296>`_\n+\n+\n+Bug fixes:\n+\n+* Using ``__cpp_nontype_template_args`` instead of ``__cpp_nontype_template_parameter_class``.\n+  `#5330 <https://github.com/pybind/pybind11/pull/5330>`_\n+\n+* Properly translate C++ exception to Python exception when creating Python buffer from wrapped object.\n+  `#5324 <https://github.com/pybind/pybind11/pull/5324>`_\n+\n+\n+Documentation:\n+\n+* Adds an answer (FAQ) for \"What is a highly conclusive and simple way to find memory leaks?\".\n+  `#5340 <https://github.com/pybind/pybind11/pull/5340>`_\n+\n+\n Version 2.13.5 (August 22, 2024)\n --------------------------------\n \n@@ -238,6 +269,18 @@ Other:\n * Update docs and noxfile.\n   `#5071 <https://github.com/pybind/pybind11/pull/5071>`_\n \n+Version 2.12.1 (September 13, 2024)\n+-----------------------------------\n+\n+New Features:\n+\n+* A new ``self._pybind11_conduit_v1_()`` method is automatically added to all\n+  ``py::class_``-wrapped types, to enable type-safe interoperability between\n+  different independent Python/C++ bindings systems, including pybind11\n+  versions with different ``PYBIND11_INTERNALS_VERSION``'s. Supported on\n+  pybind11 2.11.2, 2.12.1, and 2.13.6+.\n+  `#5296 <https://github.com/pybind/pybind11/pull/5296>`_\n+\n \n Version 2.12.0 (March 27, 2024)\n -------------------------------\n@@ -413,6 +456,18 @@ Other:\n * An ``assert()`` was added to help Coverty avoid generating a false positive.\n   `#4817 <https://github.com/pybind/pybind11/pull/4817>`_\n \n+Version 2.11.2 (September 13, 2024)\n+-----------------------------------\n+\n+New Features:\n+\n+* A new ``self._pybind11_conduit_v1_()`` method is automatically added to all\n+  ``py::class_``-wrapped types, to enable type-safe interoperability between\n+  different independent Python/C++ bindings systems, including pybind11\n+  versions with different ``PYBIND11_INTERNALS_VERSION``'s. Supported on\n+  pybind11 2.11.2, 2.12.1, and 2.13.6+.\n+  `#5296 <https://github.com/pybind/pybind11/pull/5296>`_\n+\n \n Version 2.11.1 (July 17, 2023)\n ------------------------------\ndiff --git a/include/pybind11/cast.h b/include/pybind11/cast.h\nindex 9cc2dc684e..fdeac33f3e 100644\n--- a/include/pybind11/cast.h\n+++ b/include/pybind11/cast.h\n@@ -1853,9 +1853,9 @@ class argument_loader {\n     using indices = make_index_sequence<sizeof...(Args)>;\n \n     template <typename Arg>\n-    using argument_is_args = std::is_same<intrinsic_t<Arg>, args>;\n+    using argument_is_args = std::is_base_of<args, intrinsic_t<Arg>>;\n     template <typename Arg>\n-    using argument_is_kwargs = std::is_same<intrinsic_t<Arg>, kwargs>;\n+    using argument_is_kwargs = std::is_base_of<kwargs, intrinsic_t<Arg>>;\n     // Get kwargs argument position, or -1 if not present:\n     static constexpr auto kwargs_pos = constexpr_last<argument_is_kwargs, Args...>();\n \ndiff --git a/include/pybind11/detail/exception_translation.h b/include/pybind11/detail/exception_translation.h\nindex 2764180bb0..22ae8a1c94 100644\n--- a/include/pybind11/detail/exception_translation.h\n+++ b/include/pybind11/detail/exception_translation.h\n@@ -50,17 +50,17 @@ inline void try_translate_exceptions() {\n         - delegate translation to the next translator by throwing a new type of exception.\n         */\n \n-    bool handled = with_internals([&](internals &internals) {\n-        auto &local_exception_translators = get_local_internals().registered_exception_translators;\n-        if (detail::apply_exception_translators(local_exception_translators)) {\n-            return true;\n-        }\n-        auto &exception_translators = internals.registered_exception_translators;\n-        if (detail::apply_exception_translators(exception_translators)) {\n-            return true;\n-        }\n-        return false;\n-    });\n+    bool handled = with_exception_translators(\n+        [&](std::forward_list<ExceptionTranslator> &exception_translators,\n+            std::forward_list<ExceptionTranslator> &local_exception_translators) {\n+            if (detail::apply_exception_translators(local_exception_translators)) {\n+                return true;\n+            }\n+            if (detail::apply_exception_translators(exception_translators)) {\n+                return true;\n+            }\n+            return false;\n+        });\n \n     if (!handled) {\n         set_error(PyExc_SystemError, \"Exception escaped from default exception translator!\");\ndiff --git a/include/pybind11/detail/internals.h b/include/pybind11/detail/internals.h\nindex 72ed84ea39..8499c87cff 100644\n--- a/include/pybind11/detail/internals.h\n+++ b/include/pybind11/detail/internals.h\n@@ -41,7 +41,11 @@\n #    elif PY_VERSION_HEX >= 0x030C0000 || defined(_MSC_VER)\n // Version bump for Python 3.12+, before first 3.12 beta release.\n // Version bump for MSVC piggy-backed on PR #4779. See comments there.\n-#        define PYBIND11_INTERNALS_VERSION 5\n+#        ifdef Py_GIL_DISABLED\n+#            define PYBIND11_INTERNALS_VERSION 6\n+#        else\n+#            define PYBIND11_INTERNALS_VERSION 5\n+#        endif\n #    else\n #        define PYBIND11_INTERNALS_VERSION 4\n #    endif\n@@ -179,6 +183,7 @@ static_assert(sizeof(instance_map_shard) % 64 == 0,\n struct internals {\n #ifdef Py_GIL_DISABLED\n     pymutex mutex;\n+    pymutex exception_translator_mutex;\n #endif\n     // std::type_index -> pybind11's type information\n     type_map<type_info *> registered_types_cpp;\n@@ -669,6 +674,19 @@ inline auto with_internals(const F &cb) -> decltype(cb(get_internals())) {\n     return cb(internals);\n }\n \n+template <typename F>\n+inline auto with_exception_translators(const F &cb)\n+    -> decltype(cb(get_internals().registered_exception_translators,\n+                   get_local_internals().registered_exception_translators)) {\n+    auto &internals = get_internals();\n+#ifdef Py_GIL_DISABLED\n+    std::unique_lock<pymutex> lock((internals).exception_translator_mutex);\n+#endif\n+    auto &local_internals = get_local_internals();\n+    return cb(internals.registered_exception_translators,\n+              local_internals.registered_exception_translators);\n+}\n+\n inline std::uint64_t mix64(std::uint64_t z) {\n     // David Stafford's variant 13 of the MurmurHash3 finalizer popularized\n     // by the SplitMix PRNG.\ndiff --git a/include/pybind11/pybind11.h b/include/pybind11/pybind11.h\nindex 795894250c..dabf89e8a3 100644\n--- a/include/pybind11/pybind11.h\n+++ b/include/pybind11/pybind11.h\n@@ -1387,7 +1387,17 @@ class generic_type : public object {\n             } else {\n                 internals.registered_types_cpp[tindex] = tinfo;\n             }\n+\n+            PYBIND11_WARNING_PUSH\n+#if defined(__GNUC__) && __GNUC__ == 12\n+            // When using GCC 12 these warnings are disabled as they trigger\n+            // false positive warnings.  Discussed here:\n+            // https://gcc.gnu.org/bugzilla/show_bug.cgi?id=115824.\n+            PYBIND11_WARNING_DISABLE_GCC(\"-Warray-bounds\")\n+            PYBIND11_WARNING_DISABLE_GCC(\"-Wstringop-overread\")\n+#endif\n             internals.registered_types_py[(PyTypeObject *) m_ptr] = {tinfo};\n+            PYBIND11_WARNING_POP\n         });\n \n         if (rec.bases.size() > 1 || rec.multiple_inheritance) {\n@@ -2915,10 +2925,12 @@ void implicitly_convertible() {\n }\n \n inline void register_exception_translator(ExceptionTranslator &&translator) {\n-    detail::with_internals([&](detail::internals &internals) {\n-        internals.registered_exception_translators.push_front(\n-            std::forward<ExceptionTranslator>(translator));\n-    });\n+    detail::with_exception_translators(\n+        [&](std::forward_list<ExceptionTranslator> &exception_translators,\n+            std::forward_list<ExceptionTranslator> &local_exception_translators) {\n+            (void) local_exception_translators;\n+            exception_translators.push_front(std::forward<ExceptionTranslator>(translator));\n+        });\n }\n \n /**\n@@ -2928,11 +2940,12 @@ inline void register_exception_translator(ExceptionTranslator &&translator) {\n  * the exception.\n  */\n inline void register_local_exception_translator(ExceptionTranslator &&translator) {\n-    detail::with_internals([&](detail::internals &internals) {\n-        (void) internals;\n-        detail::get_local_internals().registered_exception_translators.push_front(\n-            std::forward<ExceptionTranslator>(translator));\n-    });\n+    detail::with_exception_translators(\n+        [&](std::forward_list<ExceptionTranslator> &exception_translators,\n+            std::forward_list<ExceptionTranslator> &local_exception_translators) {\n+            (void) exception_translators;\n+            local_exception_translators.push_front(std::forward<ExceptionTranslator>(translator));\n+        });\n }\n \n /**\ndiff --git a/tools/pybind11NewTools.cmake b/tools/pybind11NewTools.cmake\nindex d3e938fab0..8bac211e14 100644\n--- a/tools/pybind11NewTools.cmake\n+++ b/tools/pybind11NewTools.cmake\n@@ -274,10 +274,6 @@ function(pybind11_add_module target_name)\n     target_link_libraries(${target_name} PRIVATE pybind11::embed)\n   endif()\n \n-  if(MSVC)\n-    target_link_libraries(${target_name} PRIVATE pybind11::windows_extras)\n-  endif()\n-\n   # -fvisibility=hidden is required to allow multiple modules compiled against\n   # different pybind versions to work properly, and for some features (e.g.\n   # py::module_local).  We force it on everything inside the `pybind11`\n", "instance_id": "pybind__pybind11-5384", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a hang occurs when retranslating a local exception with a custom data class in CPython with free-threading mode enabled. It provides a detailed reproducible example in C++ and Python, along with the specific context of the issue (locking of `internals.mutex` during a callback). The goal is implicitly understood as fixing the hang, and the provided code and output help in understanding the problem. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or output after the fix, nor does it mention specific constraints or edge cases beyond the provided example. Additionally, it lacks clarity on whether this issue affects other configurations or versions beyond the specified setup. Thus, while the problem is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code changes involves multiple files and critical components of the pybind11 library, particularly around exception handling and thread safety in free-threading CPython. The changes impact core internals (e.g., `internals.h`, `exception_translation.h`) and require modifications to mutex handling and exception translator logic, which are central to the library's architecture. Second, the technical concepts involved are advanced, including deep knowledge of CPython's free-threading model (GIL-disabled mode), pybind11's internals (mutex management, exception translation), and C++ threading primitives. Third, the problem requires understanding subtle interactions between Python and C++ exception handling, especially in a multi-threaded context, which introduces complex edge cases such as deadlocks or race conditions. While the provided code changes address the issue by introducing a separate mutex for exception translators, implementing and validating this solution demands careful consideration of thread safety and potential performance impacts. Overall, solving this problem requires a deep understanding of the codebase and advanced expertise in Python-C++ bindings and multi-threading, justifying a difficulty score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
